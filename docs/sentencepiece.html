<!DOCTYPE html><html lang="en"><head><title>Help for package sentencepiece</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sentencepiece}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BPEembed'><p>Tokenise and embed text alongside a Sentencepiece and Word2vec model</p></a></li>
<li><a href='#BPEembedder'><p>Build a BPEembed model containing a Sentencepiece and Word2vec model</p></a></li>
<li><a href='#predict.BPEembed'><p>Encode and Decode alongside a BPEembed model</p></a></li>
<li><a href='#read_word2vec'><p>Read a word2vec embedding file</p></a></li>
<li><a href='#sentencepiece'><p>Construct a Sentencepiece model</p></a></li>
<li><a href='#sentencepiece_decode'><p>Decode encoded sequences back to text</p></a></li>
<li><a href='#sentencepiece_download_model'><p>Download a Sentencepiece model</p></a></li>
<li><a href='#sentencepiece_encode'><p>Tokenise text alongside a Sentencepiece model</p></a></li>
<li><a href='#sentencepiece_load_model'><p>Load a Sentencepiece model</p></a></li>
<li><a href='#txt_remove_'><p>Remove prefixed underscore</p></a></li>
<li><a href='#wordpiece_encode'><p>Wordpiece encoding</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Text Tokenization using Byte Pair Encoding and Unigram Modelling</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.3</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jan Wijffels &lt;jwijffels@bnosac.be&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Unsupervised text tokenizer allowing to perform byte pair encoding and unigram modelling. 
    Wraps the 'sentencepiece' library <a href="https://github.com/google/sentencepiece">https://github.com/google/sentencepiece</a> which provides a language independent tokenizer to split text in words and smaller subword units. 
    The techniques are explained in the paper "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing" by Taku Kudo and John Richardson (2018) &lt;<a href="https://doi.org/10.18653%2Fv1%2FD18-2012">doi:10.18653/v1/D18-2012</a>&gt;.
    Provides as well straightforward access to pretrained byte pair encoding models and subword embeddings trained on Wikipedia using 'word2vec', 
    as described in "BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages" by Benjamin Heinzerling and Michael Strube (2018) <a href="http://www.lrec-conf.org/proceedings/lrec2018/pdf/1049.pdf">http://www.lrec-conf.org/proceedings/lrec2018/pdf/1049.pdf</a>.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bnosac/sentencepiece">https://github.com/bnosac/sentencepiece</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.mozilla.org/en-US/MPL/2.0/">MPL-2.0</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.5), stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>tokenizers.bpe, word2vec (&ge; 0.2.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-11 10:45:36 UTC; jwijf</td>
</tr>
<tr>
<td>Author:</td>
<td>Jan Wijffels [aut, cre, cph] (R wrapper),
  BNOSAC [cph] (R wrapper),
  Google Inc. [ctb, cph] (Files at src/sentencepiece/src (Apache License,
    Version 2.0),
  The Abseil Authors [ctb, cph] (Files at src/third_party/absl (Apache
    License, Version 2.0),
  Google Inc. [ctb, cph] (Files at src/third_party/protobuf-lite (BSD-3
    License)),
  Kenton Varda (Google Inc.) [ctb, cph] (Files at
    src/third_party/protobuf-lite: coded_stream.cc, extension_set.cc,
    generated_message_util.cc, generated_message_util.cc,
    message_lite.cc, repeated_field.cc, wire_format_lite.cc,
    zero_copy_stream.cc, zero_copy_stream_impl_lite.cc,
    google/protobuf/extension_set.h,
    google/protobuf/generated_message_util.h,
    google/protobuf/wire_format_lite.h,
    google/protobuf/wire_format_lite_inl.h,
    google/protobuf/message_lite.h, google/protobuf/repeated_field.h,
    google/protobuf/io/coded_stream.h,
    google/protobuf/io/zero_copy_stream_impl_lite.h,
    google/protobuf/io/zero_copy_stream.h,
    google/protobuf/stubs/common.h, google/protobuf/stubs/hash.h,
    google/protobuf/stubs/once.h, google/protobuf/stubs/once.h.org
    (BSD-3 License)),
  Sanjay Ghemawat (Google Inc.) [ctb, cph] (Design of files at
    src/third_party/protobuf-lite: coded_stream.cc, extension_set.cc,
    generated_message_util.cc, generated_message_util.cc,
    message_lite.cc, repeated_field.cc, wire_format_lite.cc,
    zero_copy_stream.cc, zero_copy_stream_impl_lite.cc,
    google/protobuf/extension_set.h,
    google/protobuf/generated_message_util.h,
    google/protobuf/wire_format_lite.h,
    google/protobuf/wire_format_lite_inl.h,
    google/protobuf/message_lite.h, google/protobuf/repeated_field.h,
    google/protobuf/io/coded_stream.h,
    google/protobuf/io/zero_copy_stream_impl_lite.h,
    google/protobuf/io/zero_copy_stream.h (BSD-3 License)),
  Jeff Dean (Google Inc.) [ctb, cph] (Design of files at
    src/third_party/protobuf-lite: coded_stream.cc, extension_set.cc,
    generated_message_util.cc, generated_message_util.cc,
    message_lite.cc, repeated_field.cc, wire_format_lite.cc,
    zero_copy_stream.cc, zero_copy_stream_impl_lite.cc,
    google/protobuf/extension_set.h,
    google/protobuf/generated_message_util.h,
    google/protobuf/wire_format_lite.h,
    google/protobuf/wire_format_lite_inl.h,
    google/protobuf/message_lite.h, google/protobuf/repeated_field.h,
    google/protobuf/io/coded_stream.h,
    google/protobuf/io/zero_copy_stream_impl_lite.h,
    google/protobuf/io/zero_copy_stream.h (BSD-3 License)),
  Laszlo Csomor (Google Inc.) [ctb, cph] (Files at
    src/third_party/protobuf-lite: io_win32.cc,
    google/protobuf/stubs/io_win32.h (BSD-3 License)),
  Wink Saville (Google Inc.) [ctb, cph] (Files at
    src/third_party/protobuf-lite: message_lite.cc,
    google/protobuf/wire_format_lite.h,
    google/protobuf/wire_format_lite_inl.h,
    google/protobuf/message_lite.h (BSD-3 License)),
  Jim Meehan (Google Inc.) [ctb, cph] (Files at
    src/third_party/protobuf-lite: structurally_valid.cc (BSD-3
    License)),
  Chris Atenasio (Google Inc.) [ctb, cph] (Files at
    src/third_party/protobuf-lite: google/protobuf/wire_format_lite.h
    (BSD-3 License)),
  Jason Hsueh (Google Inc.) [ctb, cph] (Files at
    src/third_party/protobuf-lite:
    google/protobuf/io/coded_stream_inl.h (BSD-3 License)),
  Anton Carver (Google Inc.) [ctb, cph] (Files at
    src/third_party/protobuf-lite: google/protobuf/stubs/map_util.h
    (BSD-3 License)),
  Maxim Lifantsev (Google Inc.) [ctb, cph] (Files at
    src/third_party/protobuf-lite: google/protobuf/stubs/mathlimits.h
    (BSD-3 License)),
  Susumu Yata [ctb, cph] (Files at src/third_party/darts_clone (BSD-3
    License),
  Daisuke Okanohara [ctb, cph] (File src/third_party/esaxx/esa.hxx (MIT
    License)),
  Yuta Mori [ctb, cph] (File src/third_party/esaxx/sais.hxx (MIT
    License)),
  Benjamin Heinzerling [ctb, cph] (Files
    data/models/nl.wiki.bpe.vs1000.d25.w2v.txt,
    data/models/nl.wiki.bpe.vs1000.d25.w2v.bin and
    data/models/nl.wiki.bpe.vs1000.model (MIT License))</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-11-13 09:30:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='BPEembed'>Tokenise and embed text alongside a Sentencepiece and Word2vec model</h2><span id='topic+BPEembed'></span>

<h3>Description</h3>

<p>Use a sentencepiece model to tokenise text and get the embeddings of these
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BPEembed(
  file_sentencepiece = x$file_model,
  file_word2vec = x$glove.bin$file_model,
  x,
  normalize = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BPEembed_+3A_file_sentencepiece">file_sentencepiece</code></td>
<td>
<p>the path to the file containing the sentencepiece model</p>
</td></tr>
<tr><td><code id="BPEembed_+3A_file_word2vec">file_word2vec</code></td>
<td>
<p>the path to the file containing the word2vec embeddings</p>
</td></tr>
<tr><td><code id="BPEembed_+3A_x">x</code></td>
<td>
<p>the result of a call to <code><a href="#topic+sentencepiece_download_model">sentencepiece_download_model</a></code>. 
If this is provided, arguments <code>file_sentencepiece</code> and <code>file_word2vec</code> will not be used.</p>
</td></tr>
<tr><td><code id="BPEembed_+3A_normalize">normalize</code></td>
<td>
<p>passed on to <code><a href="word2vec.html#topic+read.wordvectors">read.wordvectors</a></code> to read in <code>file_word2vec</code>. Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class BPEembed which is a list with elements 
</p>

<ul>
<li><p>model: a sentencepiece model as loaded with <code><a href="#topic+sentencepiece_load_model">sentencepiece_load_model</a></code>
</p>
</li>
<li><p>embedding: a matrix with embeddings as loaded with <code><a href="word2vec.html#topic+read.wordvectors">read.wordvectors</a></code>
</p>
</li>
<li><p>dim: the dimension of the embedding
</p>
</li>
<li><p>n: the number of elements in the vocabulary
</p>
</li>
<li><p>file_sentencepiece: the sentencepiece model file
</p>
</li>
<li><p>file_word2vec: the word2vec embedding file
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.BPEembed">predict.BPEembed</a></code>, <code><a href="#topic+sentencepiece_load_model">sentencepiece_load_model</a></code>, <code><a href="#topic+sentencepiece_download_model">sentencepiece_download_model</a></code>, <code><a href="word2vec.html#topic+read.wordvectors">read.wordvectors</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##
## Example loading model from disk
##
folder    &lt;- system.file(package = "sentencepiece", "models")
embedding &lt;- file.path(folder, "nl.wiki.bpe.vs1000.d25.w2v.bin")
model     &lt;- file.path(folder, "nl.wiki.bpe.vs1000.model")
encoder   &lt;- BPEembed(model, embedding)  

## Do tokenisation with the sentencepiece model + embed these
txt    &lt;- c("De eigendomsoverdracht aan de deelstaten is ingewikkeld.",
            "On est d'accord sur le prix de la biere?")
values &lt;- predict(encoder, txt, type = "encode")  
str(values) 
values

txt &lt;- rownames(values[[1]])
predict(encoder, txt, type = "decode") 
txt &lt;- lapply(values, FUN = rownames) 
predict(encoder, txt, type = "decode") 
</code></pre>

<hr>
<h2 id='BPEembedder'>Build a BPEembed model containing a Sentencepiece and Word2vec model</h2><span id='topic+BPEembedder'></span>

<h3>Description</h3>

<p>Build a sentencepiece model on text and build a matching word2vec model on the sentencepiece vocabulary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BPEembedder(
  x,
  tokenizer = c("bpe", "char", "unigram", "word"),
  args = list(vocab_size = 8000, coverage = 0.9999),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BPEembedder_+3A_x">x</code></td>
<td>
<p>a data.frame with columns doc_id and text</p>
</td></tr>
<tr><td><code id="BPEembedder_+3A_tokenizer">tokenizer</code></td>
<td>
<p>character string with the type of sentencepiece tokenizer. Either 'bpe', 'char', 'unigram' or 'word' for Byte Pair Encoding, Character level encoding,
Unigram encoding or pretokenised word encoding. Defaults to 'bpe' (Byte Pair Encoding). Passed on to <code><a href="#topic+sentencepiece">sentencepiece</a></code></p>
</td></tr>
<tr><td><code id="BPEembedder_+3A_args">args</code></td>
<td>
<p>a list of arguments passed on to <code><a href="#topic+sentencepiece">sentencepiece</a></code></p>
</td></tr>
<tr><td><code id="BPEembedder_+3A_...">...</code></td>
<td>
<p>arguments passed on to <code><a href="word2vec.html#topic+word2vec">word2vec</a></code> for training a word2vec model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class BPEembed which is a list with elements 
</p>

<ul>
<li><p>model: a sentencepiece model as loaded with <code><a href="#topic+sentencepiece_load_model">sentencepiece_load_model</a></code>
</p>
</li>
<li><p>embedding: a matrix with embeddings as loaded with <code><a href="word2vec.html#topic+read.wordvectors">read.wordvectors</a></code>
</p>
</li>
<li><p>dim: the dimension of the embedding
</p>
</li>
<li><p>n: the number of elements in the vocabulary
</p>
</li>
<li><p>file_sentencepiece: the sentencepiece model file
</p>
</li>
<li><p>file_word2vec: the word2vec embedding file
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+sentencepiece">sentencepiece</a></code>, <code><a href="word2vec.html#topic+word2vec">word2vec</a></code>, <code><a href="#topic+predict.BPEembed">predict.BPEembed</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tokenizers.bpe)
data(belgium_parliament, package = "tokenizers.bpe")
x     &lt;- subset(belgium_parliament, language %in% "dutch")
model &lt;- BPEembedder(x, tokenizer = "bpe", args = list(vocab_size = 1000),
                     type = "cbow", dim = 20, iter = 10) 
model

txt    &lt;- c("De eigendomsoverdracht aan de deelstaten is ingewikkeld.")
values &lt;- predict(model, txt, type = "encode")  
</code></pre>

<hr>
<h2 id='predict.BPEembed'>Encode and Decode alongside a BPEembed model</h2><span id='topic+predict.BPEembed'></span>

<h3>Description</h3>

<p>Use the sentencepiece model to either
</p>

<ul>
<li><p>encode: tokenise and embed text
</p>
</li>
<li><p>decode: get the untokenised text back of tokenised data
</p>
</li>
<li><p>tokenize: only tokenize alongside the sentencepiece model
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BPEembed'
predict(object, newdata, type = c("encode", "decode", "tokenize"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.BPEembed_+3A_object">object</code></td>
<td>
<p>an object of class BPEembed as returned by <code><a href="#topic+BPEembed">BPEembed</a></code></p>
</td></tr>
<tr><td><code id="predict.BPEembed_+3A_newdata">newdata</code></td>
<td>
<p>a character vector of text to encode or a character vector of encoded tokens to decode or a list of those</p>
</td></tr>
<tr><td><code id="predict.BPEembed_+3A_type">type</code></td>
<td>
<p>character string, either 'encode', 'decode' or 'tokenize'</p>
</td></tr>
<tr><td><code id="predict.BPEembed_+3A_...">...</code></td>
<td>
<p>further arguments passed on to the methods</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>in case type is set to <code>'encode'</code>: a list of matrices containing embeddings of the text which is tokenised with <code><a href="#topic+sentencepiece_encode">sentencepiece_encode</a></code>
</p>
</li>
<li><p>in case type is set to <code>'decode'</code>: a character vector of decoded text as returned by <code><a href="#topic+sentencepiece_decode">sentencepiece_decode</a></code>
</p>
</li>
<li><p>in case type is set to <code>'tokenize'</code>: a tokenised <code><a href="#topic+sentencepiece_encode">sentencepiece_encode</a></code>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+BPEembed">BPEembed</a></code>, <code><a href="#topic+sentencepiece_decode">sentencepiece_decode</a></code>, <code><a href="#topic+sentencepiece_encode">sentencepiece_encode</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>embedding &lt;- system.file(package = "sentencepiece", "models", 
                         "nl.wiki.bpe.vs1000.d25.w2v.bin")
model     &lt;- system.file(package = "sentencepiece", "models", 
                         "nl.wiki.bpe.vs1000.model")    
encoder   &lt;- BPEembed(model, embedding)  

txt      &lt;- c("De eigendomsoverdracht aan de deelstaten is ingewikkeld.",
              "On est d'accord sur le prix de la biere?")
values   &lt;- predict(encoder, txt, type = "encode")  
str(values) 
values

txt &lt;- rownames(values[[1]])
predict(encoder, txt, type = "decode") 
txt &lt;- lapply(values, FUN = rownames) 
predict(encoder, txt, type = "decode") 
txt &lt;- c("De eigendomsoverdracht aan de deelstaten is ingewikkeld.",
         "On est d'accord sur le prix de la biere?")
predict(encoder, txt, type = "tokenize", "subwords") 
predict(encoder, txt, type = "tokenize", "ids")  
</code></pre>

<hr>
<h2 id='read_word2vec'>Read a word2vec embedding file</h2><span id='topic+read_word2vec'></span>

<h3>Description</h3>

<p>Read a word2vec embedding file as a dense matrix. This uses <code><a href="word2vec.html#topic+read.wordvectors">read.wordvectors</a></code> from the word2vec package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_word2vec(
  x,
  type = c("txt", "bin"),
  n = .Machine$integer.max,
  encoding = "UTF-8",
  normalize = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_word2vec_+3A_x">x</code></td>
<td>
<p>path to the file</p>
</td></tr>
<tr><td><code id="read_word2vec_+3A_type">type</code></td>
<td>
<p>either 'bin' or 'txt' indicating the <code>file</code> is a binary file or a text file</p>
</td></tr>
<tr><td><code id="read_word2vec_+3A_n">n</code></td>
<td>
<p>integer, indicating to limit the number of words to read in. Defaults to reading all words.</p>
</td></tr>
<tr><td><code id="read_word2vec_+3A_encoding">encoding</code></td>
<td>
<p>encoding to be assumed for the words. Defaults to 'UTF-8'</p>
</td></tr>
<tr><td><code id="read_word2vec_+3A_normalize">normalize</code></td>
<td>
<p>logical indicating to normalize the embeddings by dividing by the factor (sqrt(sum(x . x) / length(x))). Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with one row per token containing the embedding of the token
</p>


<h3>See Also</h3>

<p><code><a href="word2vec.html#topic+read.wordvectors">read.wordvectors</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>folder    &lt;- system.file(package = "sentencepiece", "models")
embedding &lt;- file.path(folder, "nl.wiki.bpe.vs1000.d25.w2v.bin")
embedding &lt;- read_word2vec(embedding, type = "bin")
head(embedding)
embedding &lt;- file.path(folder, "nl.wiki.bpe.vs1000.d25.w2v.txt")
embedding &lt;- read_word2vec(embedding, type = "txt")
head(embedding, n = 10)
</code></pre>

<hr>
<h2 id='sentencepiece'>Construct a Sentencepiece model</h2><span id='topic+sentencepiece'></span>

<h3>Description</h3>

<p>Construct a Sentencepiece model on text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentencepiece(
  x,
  type = c("bpe", "char", "unigram", "word"),
  vocab_size = 8000,
  coverage = 0.9999,
  model_prefix = "sentencepiece",
  model_dir = tempdir(),
  threads = 1L,
  args,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sentencepiece_+3A_x">x</code></td>
<td>
<p>a character vector of path(s) to the text files containing training data</p>
</td></tr>
<tr><td><code id="sentencepiece_+3A_type">type</code></td>
<td>
<p>either one of 'bpe', 'char', 'unigram' or 'word' for Byte Pair Encoding, Character level encoding,
Unigram encoding or pretokenised word encoding. Defaults to 'bpe' (Byte Pair Encoding).</p>
</td></tr>
<tr><td><code id="sentencepiece_+3A_vocab_size">vocab_size</code></td>
<td>
<p>integer indicating the number of tokens in the final vocabulary. Defaults to 8000.</p>
</td></tr>
<tr><td><code id="sentencepiece_+3A_coverage">coverage</code></td>
<td>
<p>fraction of characters covered by the model. Must be in the range [0, 1]. A good value to use is about 0.9999.</p>
</td></tr>
<tr><td><code id="sentencepiece_+3A_model_prefix">model_prefix</code></td>
<td>
<p>character string with the name of the model. Defaults to 'sentencepiece'.
When executing the function 2 files will be created in the directory specified by <code>model_dir</code>, namely
sentencepiece.model with the model and sentencepiece.vocab containing the vocabulary of the model. 
You can change the name of the model by providing the <code>model_prefix</code> argument.</p>
</td></tr>
<tr><td><code id="sentencepiece_+3A_model_dir">model_dir</code></td>
<td>
<p>directory where the model will be saved. Defaults to the temporary directory (tempdir())</p>
</td></tr>
<tr><td><code id="sentencepiece_+3A_threads">threads</code></td>
<td>
<p>integer indicating number of threads to use when building the model</p>
</td></tr>
<tr><td><code id="sentencepiece_+3A_args">args</code></td>
<td>
<p>character string with arguments passed on to sentencepiece::SentencePieceTrainer::Train (for expert use only)</p>
</td></tr>
<tr><td><code id="sentencepiece_+3A_verbose">verbose</code></td>
<td>
<p>logical indicating to show progress of sentencepiece training. Defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>sentencepiece</code> which is defined at <code><a href="#topic+sentencepiece_load_model">sentencepiece_load_model</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sentencepiece_load_model">sentencepiece_load_model</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tokenizers.bpe)
data(belgium_parliament, package = "tokenizers.bpe")
path   &lt;- "traindata.txt" 
folder &lt;- getwd() 

writeLines(belgium_parliament$text, con = path)


model &lt;- sentencepiece(path, type = "char", 
                       model_dir = folder, verbose = TRUE)
model &lt;- sentencepiece(path, type = "unigram", vocab_size = 20000, 
                       model_dir = folder, verbose = TRUE)
model &lt;- sentencepiece(path, type = "bpe", vocab_size = 4000, 
                       model_dir = folder, verbose = TRUE)

txt &lt;- c("De eigendomsoverdracht aan de deelstaten is ingewikkeld.",
         "On est d'accord sur le prix de la biere?")
sentencepiece_encode(model, x = txt, type = "subwords")
sentencepiece_encode(model, x = txt, type = "ids")


model &lt;- sentencepiece_load_model(file.path(folder, "sentencepiece.model"))
sentencepiece_encode(model, x = txt, type = "subwords")
sentencepiece_encode(model, x = txt, type = "ids")



</code></pre>

<hr>
<h2 id='sentencepiece_decode'>Decode encoded sequences back to text</h2><span id='topic+sentencepiece_decode'></span>

<h3>Description</h3>

<p>Decode a sequence of Sentencepiece ids into text again
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentencepiece_decode(model, x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sentencepiece_decode_+3A_model">model</code></td>
<td>
<p>an object of class <code>sentencepiece</code> as returned by <code><a href="#topic+sentencepiece_load_model">sentencepiece_load_model</a></code> or <code><a href="#topic+sentencepiece">sentencepiece</a></code></p>
</td></tr>
<tr><td><code id="sentencepiece_decode_+3A_x">x</code></td>
<td>
<p>an integer vector of Sentencepiece id's or a list of these</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of detokenised text or if you encoded with <code>nbest</code>, a list of these
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- system.file(package = "sentencepiece", "models", "nl-fr-dekamer.model")
model &lt;- sentencepiece_load_model(file = model)

txt &lt;- c("De eigendomsoverdracht aan de deelstaten is ingewikkeld.",
         "On est d'accord sur le prix de la biere?")
       
x &lt;- sentencepiece_encode(model, x = txt, type = "subwords")
sentencepiece_decode(model, x)
x &lt;- sentencepiece_encode(model, x = txt, type = "ids")
sentencepiece_decode(model, x)

model &lt;- system.file(package = "sentencepiece", "models", 
                     "nl-fr-dekamer-unigram.model")
model &lt;- sentencepiece_load_model(file = model)
x &lt;- sentencepiece_encode(model, x = txt, type = "subwords", nbest = 3)
sentencepiece_decode(model, x)
x &lt;- sentencepiece_encode(model, x = txt, type = "subwords", 
                          nbest = 3, alpha = 0.1)
sentencepiece_decode(model, x)
</code></pre>

<hr>
<h2 id='sentencepiece_download_model'>Download a Sentencepiece model</h2><span id='topic+sentencepiece_download_model'></span>

<h3>Description</h3>

<p>Download pretrained models built on Wikipedia
made available at <a href="https://bpemb.h-its.org">https://bpemb.h-its.org</a> through <a href="https://github.com/bheinzerling/bpemb">https://github.com/bheinzerling/bpemb</a>. 
These models contain Byte Pair Encoded models trained with sentencepiece as well
as Glove embeddings of these Byte Pair subwords. Models for 275 languages are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentencepiece_download_model(
  language,
  vocab_size,
  dim,
  model_dir = system.file(package = "sentencepiece", "models")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sentencepiece_download_model_+3A_language">language</code></td>
<td>
<p>a character string with the language name. This can be either a plain language or a wikipedia shorthand. <br />
Possible values can be found by looking at the examples or typing sentencepiece:::.bpemb$languages <br />
If you provide multi it downloads the multilingual model available at <a href="https://bpemb.h-its.org/multi/">https://bpemb.h-its.org/multi/</a></p>
</td></tr>
<tr><td><code id="sentencepiece_download_model_+3A_vocab_size">vocab_size</code></td>
<td>
<p>integer indicating the number of tokens in the final vocabulary. Defaults to 5000. Possible values depend on the language. To inspect possible values, type sentencepiece:::.bpemb$vocab_sizes and look to your language of your choice.</p>
</td></tr>
<tr><td><code id="sentencepiece_download_model_+3A_dim">dim</code></td>
<td>
<p>dimension of the embedding. Either 25, 50, 100, 200 or 300.</p>
</td></tr>
<tr><td><code id="sentencepiece_download_model_+3A_model_dir">model_dir</code></td>
<td>
<p>path to the location where the model will be downloaded to. Defaults to <code>system.file(package = "sentencepiece", "models")</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with elements 
</p>

<ul>
<li><p>language: the provided language
</p>
</li>
<li><p>wikicode: the wikipedia code of the provided language
</p>
</li>
<li><p>file_model: the path to the downloaded Sentencepiece model
</p>
</li>
<li><p>url: the url where the Sentencepiece model was fetched from
</p>
</li>
<li><p>download_failed: logical, indicating if the download failed
</p>
</li>
<li><p>download_message: a character string with possible download failure information
</p>
</li>
<li><p>glove: a list with elements file_model, url, download_failed and download_message indicating the path to the Glove embeddings in txt format. Only present if the dim argument is provided in the function. Otherwise the embeddings will not be downloaded
</p>
</li>
<li><p>glove.bin: a list with elements file_model, url, download_failed and download_message indicating the path to the Glove embeddings in bin format. Only present if the dim argument is provided in the function. Otherwise the embeddings will not be downloaded
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+sentencepiece_load_model">sentencepiece_load_model</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>path &lt;- getwd()



##
## Download only the tokeniser model
##
dl &lt;- sentencepiece_download_model("Russian", vocab_size = 50000, model_dir = path)
dl &lt;- sentencepiece_download_model("English", vocab_size = 100000, model_dir = path)
dl &lt;- sentencepiece_download_model("French", vocab_size = 25000, model_dir = path)
dl &lt;- sentencepiece_download_model("multi", vocab_size = 320000, model_dir = path)
dl &lt;- sentencepiece_download_model("Vlaams", vocab_size = 1000, model_dir = path)
dl &lt;- sentencepiece_download_model("Dutch", vocab_size = 25000, model_dir = path)
dl &lt;- sentencepiece_download_model("nl", vocab_size = 25000, model_dir = path)
str(dl)
model     &lt;- sentencepiece_load_model(dl$file_model)

##
## Download the tokeniser model + Glove embeddings of Byte Pairs
##
dl &lt;- sentencepiece_download_model("nl", vocab_size = 1000, dim = 50, model_dir = path)
str(dl)
model     &lt;- sentencepiece_load_model(dl$file_model)
embedding &lt;- read_word2vec(dl$glove$file_model)



dl &lt;- sentencepiece_download_model("nl", vocab_size = 1000, dim = 25,
                                   model_dir = tempdir())
str(dl)


</code></pre>

<hr>
<h2 id='sentencepiece_encode'>Tokenise text alongside a Sentencepiece model</h2><span id='topic+sentencepiece_encode'></span>

<h3>Description</h3>

<p>Tokenise text alongside a Sentencepiece model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentencepiece_encode(
  model,
  x,
  type = c("subwords", "ids"),
  nbest = -1L,
  alpha = 0.1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sentencepiece_encode_+3A_model">model</code></td>
<td>
<p>an object of class <code>sentencepiece</code> as returned by <code><a href="#topic+sentencepiece_load_model">sentencepiece_load_model</a></code> or <code><a href="#topic+sentencepiece">sentencepiece</a></code></p>
</td></tr>
<tr><td><code id="sentencepiece_encode_+3A_x">x</code></td>
<td>
<p>a character vector of text (in UTF-8 Encoding)</p>
</td></tr>
<tr><td><code id="sentencepiece_encode_+3A_type">type</code></td>
<td>
<p>a character string, either 'subwords' or 'ids' to get the subwords or the corresponding ids of these subwords as defined in the vocabulary of the model. 
Defaults to 'subwords'.</p>
</td></tr>
<tr><td><code id="sentencepiece_encode_+3A_nbest">nbest</code></td>
<td>
<p>integer indicating the number of segmentations to extract. See the details. The argument is not used if you do not provide a value for it.</p>
</td></tr>
<tr><td><code id="sentencepiece_encode_+3A_alpha">alpha</code></td>
<td>
<p>smoothing parameter to perform subword regularisation. Typical values are 0.1, 0.2 or 0.5. See the details. The argument is not used if you do not provide a value for it or do not provide a value for <code>nbest</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If you specify <code>alpha</code> to perform subword regularisation, keep in mind the following. <br />
When alpha is 0.0, one segmentation is uniformly sampled from the <code>nbest</code> or lattice. 
The best Viterbi segmentation is more likely sampled when setting larger <code>alpha</code> values like 0.1. <br />
</p>

<ul>
<li><p> If you provide a positive value for <code>nbest</code>, approximately samples one segmentation from <code>nbest</code> candidates.
</p>
</li>
<li><p> If you provide a negative value for <code>nbest</code>, samples one segmentation from the hypotheses (Lattice) according to the generation probabilities using forward-filtering and backward-sampling algorithm.
</p>
</li></ul>

<p><code>nbest</code> and <code>alpha</code> correspond respectively to the parameter <code>l</code> and in <code>alpha</code> 
in the paper <a href="https://arxiv.org/abs/1804.10959">https://arxiv.org/abs/1804.10959</a> where (<code>nbest</code> &lt; 0 means l = infinity).<br />
</p>
<p>If the model is a BPE model, <code>alpha</code> is the merge probability <code>p</code> explained in <a href="https://arxiv.org/abs/1910.13267">https://arxiv.org/abs/1910.13267</a>. 
In a BPE model, nbest-based sampling is not supported so the nbest parameter is ignored although 
it still needs to be provided if you want to make use of <code>alpha</code>.
</p>


<h3>Value</h3>

<p>a list with tokenised text, one for each element of <code>x</code> 
unless you provide <code>nbest</code> without providing <code>alpha</code> in which case the result is a list of list of <code>nbest</code> tokenised texts
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- system.file(package = "sentencepiece", "models", "nl-fr-dekamer.model")
model &lt;- sentencepiece_load_model(file = model)

txt &lt;- c("De eigendomsoverdracht aan de deelstaten is ingewikkeld.",
         "On est d'accord sur le prix de la biere?")
sentencepiece_encode(model, x = txt, type = "subwords")
sentencepiece_encode(model, x = txt, type = "ids")

## Examples using subword regularisation
model &lt;- system.file(package = "sentencepiece", "models", "nl-fr-dekamer-unigram.model")
model &lt;- sentencepiece_load_model(file = model)

txt &lt;- c("Goed zo",
         "On est d'accord")
sentencepiece_encode(model, x = txt, type = "subwords", nbest = 4)
sentencepiece_encode(model, x = txt, type = "ids", nbest = 4)
sentencepiece_encode(model, x = txt, type = "subwords", nbest = 2)
sentencepiece_encode(model, x = txt, type = "ids", nbest = 2)
sentencepiece_encode(model, x = txt, type = "subwords", nbest = 1)
sentencepiece_encode(model, x = txt, type = "ids", nbest = 1)
sentencepiece_encode(model, x = txt, type = "subwords", nbest = 4, alpha = 0.1)
sentencepiece_encode(model, x = txt, type = "ids", nbest = 4, alpha = 0.1)
sentencepiece_encode(model, x = txt, type = "subwords", nbest = -1, alpha = 0.1)
sentencepiece_encode(model, x = txt, type = "ids", nbest = -1, alpha = 0.1)
sentencepiece_encode(model, x = txt, type = "subwords", nbest = -1, alpha = 0)
sentencepiece_encode(model, x = txt, type = "ids", nbest = -1, alpha = 0)
</code></pre>

<hr>
<h2 id='sentencepiece_load_model'>Load a Sentencepiece model</h2><span id='topic+sentencepiece_load_model'></span>

<h3>Description</h3>

<p>Load a Sentencepiece model which either was trained with <code><a href="#topic+sentencepiece">sentencepiece</a></code> or which you have found in the wild.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentencepiece_load_model(file = "sentencepiece.model")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sentencepiece_load_model_+3A_file">file</code></td>
<td>
<p>path to the file containing the Sentencepiece model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>sentencepiece</code> which is a list with elements
</p>

<ul>
<li><p>model: an Rcpp pointer to the model
</p>
</li>
<li><p>model_path: the path to the model
</p>
</li>
<li><p>vocab_size: the size of the Sentencepiece vocabulary
</p>
</li>
<li><p>vocabulary: the Sentencepiece vocabulary which is a data.frame with columns id and subword
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- system.file(package = "sentencepiece", "models", "nl-fr-dekamer.model")
model &lt;- sentencepiece_load_model(file = model)

txt &lt;- c("De eigendomsoverdracht aan de deelstaten is ingewikkeld.",
         "On est d'accord sur le prix de la biere?")
sentencepiece_encode(model, x = txt, type = "subwords")
sentencepiece_encode(model, x = txt, type = "ids")
</code></pre>

<hr>
<h2 id='txt_remove_'>Remove prefixed underscore</h2><span id='topic+txt_remove_'></span>

<h3>Description</h3>

<p>Remove prefixed underscore unicode character 'LOWER ONE EIGHTH BLOCK' (U+2581)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_remove_(x, replacement = "")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_remove__+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="txt_remove__+3A_replacement">replacement</code></td>
<td>
<p>character string how to replace the underscore. Defaults to the empty string.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code> where the prefixed underscore is removed
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("\u2581word", "hello", "_regularunderscore")
x
txt_remove_(x)
</code></pre>

<hr>
<h2 id='wordpiece_encode'>Wordpiece encoding</h2><span id='topic+wordpiece_encode'></span>

<h3>Description</h3>

<p>Wordpiece encoding, usefull for BERT-style tokenisation. 
Experimental version mimicing class WordpieceTokenizer from <a href="https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py">https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wordpiece_encode(
  x,
  vocabulary = character(),
  type = c("subwords", "ids"),
  unk_token = "[UNK]",
  max_input_chars_per_word = 100L
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wordpiece_encode_+3A_x">x</code></td>
<td>
<p>a character vector with text which can be splitted based on white space to obtain words</p>
</td></tr>
<tr><td><code id="wordpiece_encode_+3A_vocabulary">vocabulary</code></td>
<td>
<p>a character vector of the vocabulary</p>
</td></tr>
<tr><td><code id="wordpiece_encode_+3A_type">type</code></td>
<td>
<p>a character string, either 'subwords' or 'ids' to get the subwords or the corresponding ids of these subwords as defined in the vocabulary of the model. 
Defaults to 'subwords'.</p>
</td></tr>
<tr><td><code id="wordpiece_encode_+3A_unk_token">unk_token</code></td>
<td>
<p>character string with a value for a token which is not part of the vocabulary. Defaults to '[UNK]'</p>
</td></tr>
<tr><td><code id="wordpiece_encode_+3A_max_input_chars_per_word">max_input_chars_per_word</code></td>
<td>
<p>integer. A word which is longer than this specified number of characters will be set to the unknown token.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of subword tokens
</p>


<h3>Examples</h3>

<pre><code class='language-R'>wordpiece_encode("unaffable", vocabulary = c("un", "##aff", "##able")) 
wordpiece_encode(x = c("unaffable", "unaffableun"), 
                 vocabulary = c("un", "##aff", "##able"))
wordpiece_encode(x = c("unaffable", "unaffableun", "unknown territory"), 
                 vocabulary = c("un", "##aff", "##able", "##un")) 
wordpiece_encode(x = c("unaffable", "unaffableun", "unknown territory"), 
                 vocabulary = c("un", "##aff", "##able", "##un"),
                 type = "ids") 
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
