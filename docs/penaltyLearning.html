<!DOCTYPE html><html><head><title>Help for package penaltyLearning</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {penaltyLearning}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#change.colors'><p>change colors</p></a></li>
<li><a href='#change.labels'><p>change labels</p></a></li>
<li><a href='#changeLabel'><p>changeLabel</p></a></li>
<li><a href='#check_features_targets'><p>check features targets</p></a></li>
<li><a href='#check_target_pred'><p>check target pred</p></a></li>
<li><a href='#coef.IntervalRegression'><p>coef IntervalRegression</p></a></li>
<li><a href='#demo8'>
<p>PeakSegFPOP demo data set</p></a></li>
<li><a href='#featureMatrix'><p>featureMatrix</p></a></li>
<li><a href='#featureVector'><p>featureVector</p></a></li>
<li><a href='#geom_tallrect'><p>geom tallrect</p></a></li>
<li><a href='#GeomTallRect'><p>GeomTallRect</p></a></li>
<li><a href='#IntervalRegressionCV'><p>IntervalRegressionCV</p></a></li>
<li><a href='#IntervalRegressionCVmargin'><p>IntervalRegressionCVmargin</p></a></li>
<li><a href='#IntervalRegressionInternal'><p>IntervalRegressionInternal</p></a></li>
<li><a href='#IntervalRegressionRegularized'><p>IntervalRegressionRegularized</p></a></li>
<li><a href='#IntervalRegressionUnregularized'><p>IntervalRegressionUnregularized</p></a></li>
<li><a href='#labelError'><p>Compute incorrect labels</p></a></li>
<li><a href='#largestContinuousMinimumC'><p>largestContinuousMinimumC</p></a></li>
<li><a href='#largestContinuousMinimumR'><p>largestContinuousMinimumR</p></a></li>
<li><a href='#modelSelection'><p>Compute exact model selection function</p></a></li>
<li><a href='#modelSelectionC'><p>Exact model selection function</p></a></li>
<li><a href='#modelSelectionR'><p>Exact model selection function</p></a></li>
<li><a href='#neuroblastomaProcessed'>
<p>Processed neuroblastoma data set with features and targets</p></a></li>
<li><a href='#notConverging'>
<p>Interval regression problem that was not converging</p></a></li>
<li><a href='#oneSkip'>
<p>oneSkip</p></a></li>
<li><a href='#plot.IntervalRegression'><p>plot IntervalRegression</p></a></li>
<li><a href='#predict.IntervalRegression'><p>predict IntervalRegression</p></a></li>
<li><a href='#print.IntervalRegression'><p>print IntervalRegression</p></a></li>
<li><a href='#ROChange'><p>ROC curve for changepoints</p></a></li>
<li><a href='#squared.hinge'><p>squared hinge</p></a></li>
<li><a href='#targetIntervalResidual'><p>targetIntervalResidual</p></a></li>
<li><a href='#targetIntervalROC'><p>targetIntervalROC</p></a></li>
<li><a href='#targetIntervals'><p>Compute target intervals</p></a></li>
<li><a href='#theme_no_space'><p>theme no space</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Maintainer:</td>
<td>Toby Dylan Hocking &lt;toby.hocking@r-project.org&gt;</td>
</tr>
<tr>
<td>Author:</td>
<td>Toby Dylan Hocking</td>
</tr>
<tr>
<td>Version:</td>
<td>2024.1.25</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Title:</td>
<td>Penalty Learning</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementations of algorithms from 
 Learning Sparse Penalties for Change-point Detection
 using Max Margin Interval Regression, by
 Hocking, Rigaill, Vert, Bach
 <a href="http://proceedings.mlr.press/v28/hocking13.html">http://proceedings.mlr.press/v28/hocking13.html</a>
 published in proceedings of ICML2013.</td>
</tr>
<tr>
<td>Suggests:</td>
<td>neuroblastoma, jointseg, testthat, future, future.apply,
directlabels (&ge; 2017.03.31)</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/tdhock/penaltyLearning">https://github.com/tdhock/penaltyLearning</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tdhock/penaltyLearning/issues">https://github.com/tdhock/penaltyLearning/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table (&ge; 1.9.8), ggplot2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-01 03:41:13 UTC; tdhock</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-01 14:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='change.colors'>change colors</h2><span id='topic+change.colors'></span>

<h3>Description</h3>

<p>character vector of change-point label colors, to be used with
ggplot2::scale_*_manual</p>


<h3>Usage</h3>

<pre><code class='language-R'>"change.colors"</code></pre>

<hr>
<h2 id='change.labels'>change labels</h2><span id='topic+change.labels'></span>

<h3>Description</h3>

<p>data.table of meta-data for label types.</p>


<h3>Usage</h3>

<pre><code class='language-R'>"change.labels"</code></pre>

<hr>
<h2 id='changeLabel'>changeLabel</h2><span id='topic+changeLabel'></span>

<h3>Description</h3>

<p>Describe an annotated region label for supervised change-point detection.</p>


<h3>Usage</h3>

<pre><code class='language-R'>changeLabel(annotation, 
    min.changes, max.changes, 
    color)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="changeLabel_+3A_annotation">annotation</code></td>
<td>
<p>annotation </p>
</td></tr>
<tr><td><code id="changeLabel_+3A_min.changes">min.changes</code></td>
<td>
<p>min.changes </p>
</td></tr>
<tr><td><code id="changeLabel_+3A_max.changes">max.changes</code></td>
<td>
<p>max.changes </p>
</td></tr>
<tr><td><code id="changeLabel_+3A_color">color</code></td>
<td>
<p>color </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='check_features_targets'>check features targets</h2><span id='topic+check_features_targets'></span>

<h3>Description</h3>

<p>stop with an informative error if there is a problem with the
feature or target matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_features_targets(feature.mat, 
    target.mat)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_features_targets_+3A_feature.mat">feature.mat</code></td>
<td>
<p>n x p numeric input feature matrix.</p>
</td></tr>
<tr><td><code id="check_features_targets_+3A_target.mat">target.mat</code></td>
<td>
<p>n x 2 matrix of target interval limits.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>number of observations/rows.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='check_target_pred'>check target pred</h2><span id='topic+check_target_pred'></span>

<h3>Description</h3>

<p>stop with an informative error if there are problems with the
target matrix or predicted values.</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_target_pred(target.mat, 
    pred)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_target_pred_+3A_target.mat">target.mat</code></td>
<td>
<p>target.mat </p>
</td></tr>
<tr><td><code id="check_target_pred_+3A_pred">pred</code></td>
<td>
<p>pred </p>
</td></tr>
</table>


<h3>Value</h3>

<p>number of observations.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='coef.IntervalRegression'>coef IntervalRegression</h2><span id='topic+coef.IntervalRegression'></span>

<h3>Description</h3>

<p>Get the learned coefficients of an IntervalRegression model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'IntervalRegression'
coef(object, 
    ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.IntervalRegression_+3A_object">object</code></td>
<td>
<p>object </p>
</td></tr>
<tr><td><code id="coef.IntervalRegression_+3A_...">...</code></td>
<td>
<p>... </p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric matrix [features x regularizations] of learned weights (on
the original feature scale), can be used for prediction via
cbind(1,features) %*% weights.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='demo8'>
PeakSegFPOP demo data set
</h2><span id='topic+demo8'></span>

<h3>Description</h3>

<p>PeakSegFPOP demo data set with 8 observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("demo8")</code></pre>


<h3>Format</h3>

<p>A list of two objects: feature.mat is an 8 x 36 input feature matrix,
and target.mat is a 8 x 2 output limit matrix.
</p>

<hr>
<h2 id='featureMatrix'>featureMatrix</h2><span id='topic+featureMatrix'></span>

<h3>Description</h3>

<p>Compute a feature matrix (segmentation problems x features).</p>


<h3>Usage</h3>

<pre><code class='language-R'>featureMatrix(data.sequences, 
    problem.vars, data.var)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="featureMatrix_+3A_data.sequences">data.sequences</code></td>
<td>
<p>data.frame of sorted sequences of data to segment.</p>
</td></tr>
<tr><td><code id="featureMatrix_+3A_problem.vars">problem.vars</code></td>
<td>
<p>character vector of columns of <code>data.sequences</code> to treat as
segmentation problem IDs.</p>
</td></tr>
<tr><td><code id="featureMatrix_+3A_data.var">data.var</code></td>
<td>
<p>character vector of length 1 (column of <code>data.sequences</code> to treat as
data to segment).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric feature matrix. Some entries may be missing or infinite;
these columns should be removed before model training.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
test.df &lt;- data.frame(
  id=rep(1:2, each=10),
  x=rnorm(20))
penaltyLearning::featureMatrix(test.df, "id", "x")
if(requireNamespace("neuroblastoma")){
  data(neuroblastoma, package="neuroblastoma", envir=environment())
  one &lt;- subset(neuroblastoma$profiles, profile.id %in% c(1,2))
  f.mat &lt;- penaltyLearning::featureMatrix(
    one, c("profile.id", "chromosome"), "logratio")
}

</code></pre>

<hr>
<h2 id='featureVector'>featureVector</h2><span id='topic+featureVector'></span>

<h3>Description</h3>

<p>Compute a feature vector of constant length which can be used as
an input for supervised penalty learning. The output is a target
interval of log(penalty) values that achieve minimum incorrect
labels (see <code><a href="#topic+targetIntervals">targetIntervals</a></code>).</p>


<h3>Usage</h3>

<pre><code class='language-R'>featureVector(data.vec)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="featureVector_+3A_data.vec">data.vec</code></td>
<td>
<p>numeric vector of ordered data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector of features.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- rnorm(10)
penaltyLearning::featureVector(x)
if(requireNamespace("neuroblastoma")){
  data(neuroblastoma, package="neuroblastoma", envir=environment())
  one &lt;- subset(neuroblastoma$profiles, profile.id=="1" &amp; chromosome=="1")
  (f.vec &lt;- penaltyLearning::featureVector(one$logratio))
}

</code></pre>

<hr>
<h2 id='geom_tallrect'>geom tallrect</h2><span id='topic+geom_tallrect'></span>

<h3>Description</h3>

<p>ggplot2 geom with xmin and xmax aesthetics that covers the entire
y range, useful for clickSelects background elements.</p>


<h3>Usage</h3>

<pre><code class='language-R'>geom_tallrect(mapping = NULL, 
    data = NULL, stat = "identity", 
    position = "identity", 
    ..., na.rm = FALSE, 
    show.legend = NA, 
    inherit.aes = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="geom_tallrect_+3A_mapping">mapping</code></td>
<td>
<p>mapping </p>
</td></tr>
<tr><td><code id="geom_tallrect_+3A_data">data</code></td>
<td>
<p>data </p>
</td></tr>
<tr><td><code id="geom_tallrect_+3A_stat">stat</code></td>
<td>
<p>stat </p>
</td></tr>
<tr><td><code id="geom_tallrect_+3A_position">position</code></td>
<td>
<p>position </p>
</td></tr>
<tr><td><code id="geom_tallrect_+3A_...">...</code></td>
<td>
<p>... </p>
</td></tr>
<tr><td><code id="geom_tallrect_+3A_na.rm">na.rm</code></td>
<td>
<p>na.rm </p>
</td></tr>
<tr><td><code id="geom_tallrect_+3A_show.legend">show.legend</code></td>
<td>
<p>show.legend </p>
</td></tr>
<tr><td><code id="geom_tallrect_+3A_inherit.aes">inherit.aes</code></td>
<td>
<p>inherit.aes </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='GeomTallRect'>GeomTallRect</h2><span id='topic+GeomTallRect'></span>

<h3>Description</h3>

<p>ggproto object for <code><a href="#topic+geom_tallrect">geom_tallrect</a></code></p>


<h3>Usage</h3>

<pre><code class='language-R'>"GeomTallRect"</code></pre>

<hr>
<h2 id='IntervalRegressionCV'>IntervalRegressionCV</h2><span id='topic+IntervalRegressionCV'></span>

<h3>Description</h3>

<p>Use cross-validation to fit an L1-regularized linear interval
regression model by optimizing margin and/or regularization
parameters.
This function repeatedly calls <code><a href="#topic+IntervalRegressionRegularized">IntervalRegressionRegularized</a></code>, and by
default assumes that margin=1. To optimize the margin,
specify the <code>margin.vec</code> parameter
manually, or use <code><a href="#topic+IntervalRegressionCVmargin">IntervalRegressionCVmargin</a></code>
(which takes more computation time
but yields more accurate models).
If the future package is available,
two levels of future_lapply are used
to parallelize on validation.fold and margin.</p>


<h3>Usage</h3>

<pre><code class='language-R'>IntervalRegressionCV(feature.mat, 
    target.mat, n.folds = ifelse(nrow(feature.mat) &lt; 
        10, 3L, 5L), 
    fold.vec = sample(rep(1:n.folds, 
        l = nrow(feature.mat))), 
    verbose = 0, min.observations = 10, 
    reg.type = "min", 
    incorrect.labels.db = NULL, 
    initial.regularization = 0.001, 
    margin.vec = 1, LAPPLY = NULL, 
    check.unlogged = TRUE, 
    ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IntervalRegressionCV_+3A_feature.mat">feature.mat</code></td>
<td>
<p>Numeric feature matrix, n observations x p features.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_target.mat">target.mat</code></td>
<td>
<p>Numeric target matrix, n observations x 2 limits. These should be
real-valued (possibly negative). If your data are interval
censored positive-valued survival times, you need to log them to
obtain <code>target.mat</code>.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_n.folds">n.folds</code></td>
<td>
<p>Number of cross-validation folds.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_fold.vec">fold.vec</code></td>
<td>
<p>Integer vector of fold id numbers.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_verbose">verbose</code></td>
<td>
<p>numeric: 0 for silent, bigger numbers (1 or 2) for more output.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_min.observations">min.observations</code></td>
<td>
<p>stop with an error if there are fewer than this many observations.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_reg.type">reg.type</code></td>
<td>
<p>Either &quot;1sd&quot; or &quot;min&quot; which specifies how the regularization
parameter is chosen during the internal cross-validation
loop. min: first take the mean of the K-CV error functions, then
minimize it (this is the default since it tends to yield the least
test error). 1sd: take the most regularized model with the same
margin which is within one standard deviation of that minimum
(this model is typically a bit less accurate, but much less
complex, so better if you want to interpret the coefficients).</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_incorrect.labels.db">incorrect.labels.db</code></td>
<td>
<p>either NULL or a data.table, which specifies the error function to
compute for selecting the regularization parameter on the
validation set. NULL means to minimize the squared hinge loss,
which measures how far the predicted log(penalty) values are from
the target intervals. If a data.table is specified, its first key
should correspond to the rownames of <code>feature.mat</code>, and columns
min.log.lambda, max.log.lambda, fp, fn, possible.fp, possible.fn;
these will be used with <code><a href="#topic+ROChange">ROChange</a></code> to compute the AUC for each
regularization parameter, and the maximimum will be selected (in
the plot this is negative.auc, which is minimized). This
data.table can be computed via
labelError(modelSelection(<code>...</code>),...)$model.errors &ndash; see
example(<code><a href="#topic+ROChange">ROChange</a></code>). In practice this makes the computation longer,
and it should only result in more accurate models if there are
many labels per data sequence.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_initial.regularization">initial.regularization</code></td>
<td>
<p>Passed to <code><a href="#topic+IntervalRegressionRegularized">IntervalRegressionRegularized</a></code>.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_margin.vec">margin.vec</code></td>
<td>
<p>numeric vector of margin size hyper-parameters. The computation
time is linear in the number of elements of <code>margin.vec</code> &ndash; more
values takes more computation time, but yields slightly more
accurate models (if there is enough data).</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_lapply">LAPPLY</code></td>
<td>
<p>Function to use for parallelization, by default
<code><a href="future.apply.html#topic+future_lapply">future_lapply</a></code> if it is available, otherwise
lapply. For debugging with verbose&gt;0 it is useful to specify
LAPPLY=lapply in order to interactively see messages, before all
parallel processes end.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_check.unlogged">check.unlogged</code></td>
<td>
<p>If TRUE, stop with an error if target matrix is non-negative and
has any big difference in successive quantiles (this is an
indicator that the user probably forgot to log their outputs).</p>
</td></tr>
<tr><td><code id="IntervalRegressionCV_+3A_...">...</code></td>
<td>
<p>passed to <code><a href="#topic+IntervalRegressionRegularized">IntervalRegressionRegularized</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List representing regularized linear model.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(interactive()){
  library(penaltyLearning)
  data("neuroblastomaProcessed", package="penaltyLearning", envir=environment())
  if(require(future)){
    plan(multiprocess)
  }
  set.seed(1)
  i.train &lt;- 1:100
  fit &lt;- with(neuroblastomaProcessed, IntervalRegressionCV(
    feature.mat[i.train,], target.mat[i.train,],
    verbose=0))
  ## When only features and target matrices are specified for
  ## training, the squared hinge loss is used as the metric to
  ## minimize on the validation set.
  plot(fit)
  ## Create an incorrect labels data.table (first key is same as
  ## rownames of feature.mat and target.mat).
  library(data.table)
  errors.per.model &lt;- data.table(neuroblastomaProcessed$errors)
  errors.per.model[, pid.chr := paste0(profile.id, ".", chromosome)]
  setkey(errors.per.model, pid.chr)
  set.seed(1)
  fit &lt;- with(neuroblastomaProcessed, IntervalRegressionCV(
    feature.mat[i.train,], target.mat[i.train,],
    ## The incorrect.labels.db argument is optional, but can be used if
    ## you want to use AUC as the CV model selection criterion.
    incorrect.labels.db=errors.per.model))
  plot(fit)
}

</code></pre>

<hr>
<h2 id='IntervalRegressionCVmargin'>IntervalRegressionCVmargin</h2><span id='topic+IntervalRegressionCVmargin'></span>

<h3>Description</h3>

<p>Use cross-validation to fit an L1-regularized linear interval
regression model by optimizing both margin and regularization
parameters. This function just calls <code><a href="#topic+IntervalRegressionCV">IntervalRegressionCV</a></code> with a
margin.vec parameter that is computed based on the finite target
interval limits. If default parameters are used, this function
should be about 10 times slower than <code><a href="#topic+IntervalRegressionCV">IntervalRegressionCV</a></code>
(since this function computes n.margin=10 models
per regularization parameter whereas <code><a href="#topic+IntervalRegressionCV">IntervalRegressionCV</a></code>
only computes one).
On large (N &gt; 1000 rows) data sets,
this function should yield a model which is a little
more accurate than <code><a href="#topic+IntervalRegressionCV">IntervalRegressionCV</a></code>
(since the margin parameter is optimized).</p>


<h3>Usage</h3>

<pre><code class='language-R'>IntervalRegressionCVmargin(feature.mat, 
    target.mat, log10.diff = 2, 
    n.margin = 10L, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IntervalRegressionCVmargin_+3A_feature.mat">feature.mat</code></td>
<td>
<p>Numeric feature matrix, n observations x p features.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCVmargin_+3A_target.mat">target.mat</code></td>
<td>
<p>Numeric target matrix, n observations x 2 limits.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCVmargin_+3A_log10.diff">log10.diff</code></td>
<td>
<p>Numeric scalar: factors of 10 below the largest finite limit
difference to use as a minimum margin value (difference on the
log10 scale which is used to generate margin parameters). Bigger
values mean a grid of margin parameters with a larger range. For
example if the largest finite limit in <code>target.mat</code> is 26 and the
smallest finite limit is -4 then the largest limit difference is
30, which will be used as the maximum margin parameter. If
<code>log10.diff</code> is the default of 2 then that means the smallest margin
parameter will be 0.3 (two factors of 10 smaller than 30).</p>
</td></tr>
<tr><td><code id="IntervalRegressionCVmargin_+3A_n.margin">n.margin</code></td>
<td>
<p>Integer scalar: number of margin parameters, by default 10.</p>
</td></tr>
<tr><td><code id="IntervalRegressionCVmargin_+3A_...">...</code></td>
<td>
<p>Passed to <code><a href="#topic+IntervalRegressionCV">IntervalRegressionCV</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Model fit list from <code><a href="#topic+IntervalRegressionCV">IntervalRegressionCV</a></code>.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive()){
  library(penaltyLearning)
  data(
    "neuroblastomaProcessed",
    package="penaltyLearning",
    envir=environment())
  if(require(future)){
    plan(multiprocess)
  }
  set.seed(1)
  fit &lt;- with(neuroblastomaProcessed, IntervalRegressionCVmargin(
    feature.mat, target.mat, verbose=1))
  plot(fit)
  print(fit$plot.heatmap)
}
</code></pre>

<hr>
<h2 id='IntervalRegressionInternal'>IntervalRegressionInternal</h2><span id='topic+IntervalRegressionInternal'></span>

<h3>Description</h3>

<p>Solve the squared hinge loss interval regression problem for one
<code>regularization</code> parameter: w* = argmin_w L(w) + <code>regularization</code> *
||w||_1 where L(w) is the average squared hinge loss with respect
to the <code>targets</code>, and ||w||_1 is the L1-norm of the weight vector
(excluding the first element, which is the un-regularized
intercept or bias term). This function performs no scaling of
input <code>features</code>, and is meant for internal use only! To learn a
regression model, try <code><a href="#topic+IntervalRegressionCV">IntervalRegressionCV</a></code> or
<code><a href="#topic+IntervalRegressionUnregularized">IntervalRegressionUnregularized</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>IntervalRegressionInternal(features, 
    targets, initial.param.vec, 
    regularization, threshold = 0.001, 
    max.iterations = 1000, 
    weight.vec = NULL, 
    Lipschitz = NULL, 
    verbose = 2, margin = 1, 
    biggest.crit = 100)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IntervalRegressionInternal_+3A_features">features</code></td>
<td>
<p>Scaled numeric feature matrix (problems x <code>features</code>). The first
column/feature should be all ones and will not be regularized.</p>
</td></tr>
<tr><td><code id="IntervalRegressionInternal_+3A_targets">targets</code></td>
<td>
<p>Numeric target matrix (problems x 2).</p>
</td></tr>
<tr><td><code id="IntervalRegressionInternal_+3A_initial.param.vec">initial.param.vec</code></td>
<td>
<p>initial guess for weight vector (<code>features</code>).</p>
</td></tr>
<tr><td><code id="IntervalRegressionInternal_+3A_regularization">regularization</code></td>
<td>
<p>Degree of L1-regularization.</p>
</td></tr>
<tr><td><code id="IntervalRegressionInternal_+3A_threshold">threshold</code></td>
<td>
<p>When the stopping criterion gets below this <code>threshold</code>, the
algorithm stops and declares the solution as optimal.</p>
</td></tr>
<tr><td><code id="IntervalRegressionInternal_+3A_max.iterations">max.iterations</code></td>
<td>
<p>If the algorithm has not found an optimal solution after this many
iterations, increase <code>Lipschitz</code> constant and <code>max.iterations</code>.</p>
</td></tr>
<tr><td><code id="IntervalRegressionInternal_+3A_weight.vec">weight.vec</code></td>
<td>
<p>A numeric vector of weights for each training example.</p>
</td></tr>
<tr><td><code id="IntervalRegressionInternal_+3A_lipschitz">Lipschitz</code></td>
<td>
<p>A numeric scalar or NULL, which means to compute <code>Lipschitz</code> as the
mean of the squared L2-norms of the rows of the feature matrix.</p>
</td></tr>
<tr><td><code id="IntervalRegressionInternal_+3A_verbose">verbose</code></td>
<td>
<p>Cat messages: for restarts and at the end if &gt;= 1, and for every
iteration if &gt;= 2.</p>
</td></tr>
<tr><td><code id="IntervalRegressionInternal_+3A_margin">margin</code></td>
<td>
<p>Margin size hyper-parameter, default 1.</p>
</td></tr>
<tr><td><code id="IntervalRegressionInternal_+3A_biggest.crit">biggest.crit</code></td>
<td>
<p>Restart FISTA with a bigger <code>Lipschitz</code> (smaller step size) if crit
gets larger than this.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector of scaled weights w of the affine function f_w(X) =
X %*% w for a scaled feature matrix X with the first row entirely
ones.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='IntervalRegressionRegularized'>IntervalRegressionRegularized</h2><span id='topic+IntervalRegressionRegularized'></span>

<h3>Description</h3>

<p>Repeatedly use <code><a href="#topic+IntervalRegressionInternal">IntervalRegressionInternal</a></code> to solve interval
regression problems for a path of regularization parameters. This
function does not perform automatic selection of the
regularization parameter; instead, it returns regression models
for a range of regularization parameters, and it is up to you to
select which one to use. For automatic regularization parameter
selection, use <code><a href="#topic+IntervalRegressionCV">IntervalRegressionCV</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>IntervalRegressionRegularized(feature.mat, 
    target.mat, initial.regularization = 0.001, 
    factor.regularization = 1.2, 
    verbose = 0, margin = 1, 
    ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IntervalRegressionRegularized_+3A_feature.mat">feature.mat</code></td>
<td>
<p>Numeric feature matrix.</p>
</td></tr>
<tr><td><code id="IntervalRegressionRegularized_+3A_target.mat">target.mat</code></td>
<td>
<p>Numeric target matrix.</p>
</td></tr>
<tr><td><code id="IntervalRegressionRegularized_+3A_initial.regularization">initial.regularization</code></td>
<td>
<p>Initial regularization parameter.</p>
</td></tr>
<tr><td><code id="IntervalRegressionRegularized_+3A_factor.regularization">factor.regularization</code></td>
<td>
<p>Increase regularization by this factor after finding an optimal
solution. Or NULL to compute just one model
(<code>initial.regularization</code>).</p>
</td></tr>
<tr><td><code id="IntervalRegressionRegularized_+3A_verbose">verbose</code></td>
<td>
<p>Print messages if &gt;= 1.</p>
</td></tr>
<tr><td><code id="IntervalRegressionRegularized_+3A_margin">margin</code></td>
<td>
<p>Non-negative <code>margin</code> size parameter, default 1.</p>
</td></tr>
<tr><td><code id="IntervalRegressionRegularized_+3A_...">...</code></td>
<td>
<p>Other parameters to pass to <code><a href="#topic+IntervalRegressionInternal">IntervalRegressionInternal</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List representing fit model. You can do
fit$predict(feature.matrix) to get a matrix of predicted log
penalty values. The param.mat is the n.features * n.regularization
numeric matrix of optimal coefficients (on the original scale).</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(interactive()){
  library(penaltyLearning)
  data("neuroblastomaProcessed", package="penaltyLearning", envir=environment())
  i.train &lt;- 1:500
  fit &lt;- with(neuroblastomaProcessed, IntervalRegressionRegularized(
    feature.mat[i.train,], target.mat[i.train,]))
  plot(fit)
}

</code></pre>

<hr>
<h2 id='IntervalRegressionUnregularized'>IntervalRegressionUnregularized</h2><span id='topic+IntervalRegressionUnregularized'></span>

<h3>Description</h3>

<p>Use <code><a href="#topic+IntervalRegressionRegularized">IntervalRegressionRegularized</a></code> with initial.regularization=0
and factor.regularization=NULL, meaning fit one un-regularized
interval regression model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>IntervalRegressionUnregularized(...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IntervalRegressionUnregularized_+3A_...">...</code></td>
<td>
<p>passed to <code><a href="#topic+IntervalRegressionRegularized">IntervalRegressionRegularized</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List representing fit model, see
help(<code><a href="#topic+IntervalRegressionRegularized">IntervalRegressionRegularized</a></code>) for details.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='labelError'>Compute incorrect labels</h2><span id='topic+labelError'></span>

<h3>Description</h3>

<p>Compute incorrect <code>labels</code> for several change-point detection
problems and <code>models</code>. Use this function after having computed
changepoints, loss values, and model selection functions
(see <code><a href="#topic+modelSelection">modelSelection</a></code>). The next step after labelError is typically
computing target intervals of log(penalty) values that predict
changepoints with minimum incorrect <code>labels</code> for each problem (see
<code><a href="#topic+targetIntervals">targetIntervals</a></code>).</p>


<h3>Usage</h3>

<pre><code class='language-R'>labelError(models, labels, 
    changes, change.var = "chromStart", 
    label.vars = c("min", 
        "max"), model.vars = "n.segments", 
    problem.vars = character(0), 
    annotations = change.labels)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="labelError_+3A_models">models</code></td>
<td>
<p>data.frame with one row per (problem,model) combination, typically
the output of modelSelection(...). There is a row for each
changepoint model that could be selected for a particular
segmentation problem. There should be columns <code>problem.vars</code> (for
problem ID) and <code>model.vars</code> (for model complexity).</p>
</td></tr>
<tr><td><code id="labelError_+3A_labels">labels</code></td>
<td>
<p>data.frame with one row per (problem,region). Each label defines a
region in a particular segmentation problem, and a range of
predicted changepoints which are consistent in that region. There
should be a column &quot;annotation&quot; with takes one of the
corresponding values in the annotation column of <code><a href="#topic+change.labels">change.labels</a></code>
(used to determine the range of predicted changepoints which are
consistent). There should also be a columns <code>problem.vars</code> (for
problem ID) and <code>label.vars</code> (for region start/end).</p>
</td></tr>
<tr><td><code id="labelError_+3A_changes">changes</code></td>
<td>
<p>data.frame with one row per (problem,model,change), for each
predicted changepoint (in each model and segmentation
problem). Should have columns <code>problem.vars</code> (for problem ID),
<code>model.vars</code> (for model complexity), and <code>change.var</code> (for changepoint
position).</p>
</td></tr>
<tr><td><code id="labelError_+3A_change.var">change.var</code></td>
<td>
<p>character(length=1): column name of predicted change-point
position in <code>labels</code>. The default &quot;chromStart&quot; is useful for genomic
data with segment start/end positions stored in columns named
chromStart/chromEnd. A predicted changepoint at position X is
interpreted to mean a changepoint between X and X+1.</p>
</td></tr>
<tr><td><code id="labelError_+3A_label.vars">label.vars</code></td>
<td>
<p>character(length=2): column names of start and end positions of
<code>labels</code>, in same units as change-point positions. The default is
c(&quot;min&quot;, &quot;max&quot;). Labeled regions are (start,end] &ndash; open on the
left and closed on the right, so for example a 0changes annotation
between start=10 and end=20 means that any predicted changepoint
at 11, ..., 20 is a false positive.</p>
</td></tr>
<tr><td><code id="labelError_+3A_model.vars">model.vars</code></td>
<td>
<p>character: column names used to identify model complexity. The
default &quot;n.segments&quot; is for change-point <code>models</code> such as in the
jointseg and changepoint packages.</p>
</td></tr>
<tr><td><code id="labelError_+3A_problem.vars">problem.vars</code></td>
<td>
<p>character: column names used to identify data set / segmentation
problem, should be present in all three data tables (<code>models</code>,
<code>labels</code>, <code>changes</code>).</p>
</td></tr>
<tr><td><code id="labelError_+3A_annotations">annotations</code></td>
<td>
<p>data.table with columns annotation, min.changes, max.changes,
possible.fn, possible.fp which is joined to <code>labels</code> in order to
determine how to compute false positives and false negatives for
each annotation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of two data.tables: label.errors has one row for every
combination of <code>models</code> and <code>labels</code>, with status column that
indicates whether or not that model commits an error in that
particular label; model.errors has one row per model, with columns
for computing target intervals and ROC curves (see <code><a href="#topic+targetIntervals">targetIntervals</a></code>
and <code><a href="#topic+ROChange">ROChange</a></code>).</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
label &lt;- function(annotation, min, max){
  data.frame(profile.id=4, chrom="chr14", min, max, annotation)
}
label.df &lt;- rbind(
  label("1change", 70e6, 80e6),
  label("0changes", 20e6, 60e6))
model.df &lt;- data.frame(chrom="chr14", n.segments=1:3)
change.df &lt;- data.frame(chrom="chr14", rbind(
  data.frame(n.segments=2, changepoint=75e6),
  data.frame(n.segments=3, changepoint=c(75e6, 50e6))))
penaltyLearning::labelError(
  model.df, label.df, change.df,
  problem.vars="chrom", # for all three data sets.
  model.vars="n.segments", # for changes and selection.
  change.var="changepoint", # column of changes with breakpoint position.
  label.vars=c("min", "max")) # limit of labels in ann.

</code></pre>

<hr>
<h2 id='largestContinuousMinimumC'>largestContinuousMinimumC</h2><span id='topic+largestContinuousMinimumC'></span>

<h3>Description</h3>

<p>Find the run of minimum <code>cost</code> with the largest <code>size</code>.
This function use a linear time C implementation,
and is meant for internal use.
Use <code><a href="#topic+targetIntervals">targetIntervals</a></code> for real data.</p>


<h3>Usage</h3>

<pre><code class='language-R'>largestContinuousMinimumC(cost, 
    size)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="largestContinuousMinimumC_+3A_cost">cost</code></td>
<td>
<p>numeric vector of <code>cost</code> values.</p>
</td></tr>
<tr><td><code id="largestContinuousMinimumC_+3A_size">size</code></td>
<td>
<p>numeric vector of interval <code>size</code> values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer vector length 2 (start and end of target interval relative
to <code>cost</code> and <code>size</code>).</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(penaltyLearning)
data(neuroblastomaProcessed, envir=environment())
one.problem.error &lt;-
  neuroblastomaProcessed$errors[profile.id=="4" &amp; chromosome=="1"]
indices &lt;- one.problem.error[, largestContinuousMinimumC(
  errors, max.log.lambda-min.log.lambda)]
one.problem.error[indices[["start"]]:indices[["end"]],]

</code></pre>

<hr>
<h2 id='largestContinuousMinimumR'>largestContinuousMinimumR</h2><span id='topic+largestContinuousMinimumR'></span>

<h3>Description</h3>

<p>Find the run of minimum <code>cost</code> with the largest <code>size</code>.
This function uses a two pass R implementation,
and is meant for internal use.
Use <code><a href="#topic+targetIntervals">targetIntervals</a></code> for real data.</p>


<h3>Usage</h3>

<pre><code class='language-R'>largestContinuousMinimumR(cost, 
    size)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="largestContinuousMinimumR_+3A_cost">cost</code></td>
<td>
<p>numeric vector of <code>cost</code> values.</p>
</td></tr>
<tr><td><code id="largestContinuousMinimumR_+3A_size">size</code></td>
<td>
<p>numeric vector of interval <code>size</code> values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer vector length 2 (start and end of target interval relative
to <code>cost</code> and <code>size</code>).</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(penaltyLearning)
data(neuroblastomaProcessed, envir=environment())
one.problem.error &lt;-
  neuroblastomaProcessed$errors[profile.id=="4" &amp; chromosome=="1"]
indices &lt;- one.problem.error[, largestContinuousMinimumR(
  errors, max.log.lambda-min.log.lambda)]
one.problem.error[indices[["start"]]:indices[["end"]],]

</code></pre>

<hr>
<h2 id='modelSelection'>Compute exact model selection function</h2><span id='topic+modelSelection'></span>

<h3>Description</h3>

<p>Given loss.vec L_i, model.complexity K_i, the model selection
function i*(lambda) = argmin_i L_i + lambda*K_i, compute all of
the solutions (i, min.lambda, max.lambda) with i being the
solution for every lambda in (min.lambda, max.lambda). Use this
function after having computed changepoints and <code>loss</code> values for
each model, and before using <code><a href="#topic+labelError">labelError</a></code>. This function uses the
linear time algorithm implemented in C code (<code><a href="#topic+modelSelectionC">modelSelectionC</a></code>).</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelSelection(models, 
    loss = "loss", complexity = "complexity")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modelSelection_+3A_models">models</code></td>
<td>
<p>data.frame with one row per model. There must be at least two
columns models[[loss]] and models[[complexity]], but there can
also be other meta-data columns.</p>
</td></tr>
<tr><td><code id="modelSelection_+3A_loss">loss</code></td>
<td>
<p>character: column name of <code>models</code> to interpret as <code>loss</code> L_i.</p>
</td></tr>
<tr><td><code id="modelSelection_+3A_complexity">complexity</code></td>
<td>
<p>character: column name of <code>models</code> to interpret as <code>complexity</code> K_i.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with a row for each model that can be selected for at
least one lambda value, and the following columns. (min.lambda,
max.lambda) and (min.log.lambda, max.log.lambda) are intervals of
optimal penalty constants, on the original and log scale; the
other columns (and rownames) are taken from <code>models</code>. This should be
used as the <code>models</code> argument of <code><a href="#topic+labelError">labelError</a></code>.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='modelSelectionC'>Exact model selection function</h2><span id='topic+modelSelectionC'></span>

<h3>Description</h3>

<p>Given <code>loss.vec</code> L_i, <code>model.complexity</code> K_i, the model selection
function i*(lambda) = argmin_i L_i + lambda*K_i, compute all of
the solutions (i, min.lambda, max.lambda) with i being the
solution for every lambda in (min.lambda, max.lambda). This
function uses the linear time algorithm implemented in C code.
This function is mostly meant for internal use &ndash; it is instead
recommended to use <code><a href="#topic+modelSelection">modelSelection</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelSelectionC(loss.vec, 
    model.complexity, 
    model.id)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modelSelectionC_+3A_loss.vec">loss.vec</code></td>
<td>
<p>numeric vector: loss L_i</p>
</td></tr>
<tr><td><code id="modelSelectionC_+3A_model.complexity">model.complexity</code></td>
<td>
<p>numeric vector: model complexity K_i</p>
</td></tr>
<tr><td><code id="modelSelectionC_+3A_model.id">model.id</code></td>
<td>
<p>vector: indices i</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with a row for each model that can be selected for at
least one lambda value, and the following columns. (min.lambda,
max.lambda) and (min.log.lambda, max.log.lambda) are intervals of
optimal penalty constants, on the original and log scale;
<code>model.complexity</code> are the K_i values; <code>model.id</code> are the model
identifiers (also used for row names); and model.loss are the C_i
values.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
loss.vec &lt;- c(
  -9.9, -12.8, -19.2, -22.1, -24.5, -26.1, -28.5, -30.1, -32.2, 
  -33.7, -35.2, -36.8, -38.2, -39.5, -40.7, -41.8, -42.8, -43.9, 
  -44.9, -45.8)
seg.vec &lt;- seq_along(loss.vec)
exact.df &lt;- penaltyLearning::modelSelectionC(loss.vec, seg.vec, seg.vec)
## Solve the optimization using grid search.
L.grid &lt;- with(exact.df,{
  seq(min(max.log.lambda)-1,
      max(min.log.lambda)+1,
      l=100)
})
lambda.grid &lt;- exp(L.grid)
kstar.grid &lt;- sapply(lambda.grid, function(lambda){
  crit &lt;- with(exact.df, model.complexity * lambda + model.loss)
  picked &lt;- which.min(crit)
  exact.df$model.id[picked]
})
grid.df &lt;- data.frame(log.lambda=L.grid, segments=kstar.grid)
library(ggplot2)
## Compare the results.
ggplot()+
  ggtitle("grid search (red) agrees with exact path computation (black)")+
  geom_segment(aes(min.log.lambda, model.id,
                   xend=max.log.lambda, yend=model.id),
               data=exact.df)+
  geom_point(aes(log.lambda, segments),
             data=grid.df, color="red", pch=1)+
  ylab("optimal model complexity (segments)")+
  xlab("log(lambda)")

</code></pre>

<hr>
<h2 id='modelSelectionR'>Exact model selection function</h2><span id='topic+modelSelectionR'></span>

<h3>Description</h3>

<p>Given <code>loss.vec</code> L_i, <code>model.complexity</code> K_i, the model selection
function i*(lambda) = argmin_i L_i + lambda*K_i, compute all of
the solutions (i, min.lambda, max.lambda) with i being the
solution for every lambda in (min.lambda, max.lambda). This
function uses the quadratic time algorithm implemented in R code.
This function is mostly meant for internal use and comparison &ndash;
it is instead recommended to use <code><a href="#topic+modelSelection">modelSelection</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelSelectionR(loss.vec, 
    model.complexity, 
    model.id)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modelSelectionR_+3A_loss.vec">loss.vec</code></td>
<td>
<p>numeric vector: loss L_i</p>
</td></tr>
<tr><td><code id="modelSelectionR_+3A_model.complexity">model.complexity</code></td>
<td>
<p>numeric vector: model complexity K_i</p>
</td></tr>
<tr><td><code id="modelSelectionR_+3A_model.id">model.id</code></td>
<td>
<p>vector: indices i</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with a row for each model that can be selected for at
least one lambda value, and the following columns. (min.lambda,
max.lambda) and (min.log.lambda, max.log.lambda) are intervals of
optimal penalty constants, on the original and log scale;
<code>model.complexity</code> are the K_i values; <code>model.id</code> are the model
identifiers (also used for row names); and model.loss are the C_i
values.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
loss.vec &lt;- c(
  -9.9, -12.8, -19.2, -22.1, -24.5, -26.1, -28.5, -30.1, -32.2, 
  -33.7, -35.2, -36.8, -38.2, -39.5, -40.7, -41.8, -42.8, -43.9, 
  -44.9, -45.8)
seg.vec &lt;- seq_along(loss.vec)
penaltyLearning::modelSelectionR(loss.vec, seg.vec, seg.vec)

</code></pre>

<hr>
<h2 id='neuroblastomaProcessed'>
Processed neuroblastoma data set with features and targets
</h2><span id='topic+neuroblastomaProcessed'></span>

<h3>Description</h3>

<p>Features are inputs and targets are outputs for
penalty learning functions like penaltyLearning::IntervalRegressionCV.
data(neuroblastoma, package=&quot;neuroblastoma&quot;) was processed by
computing optimal Gaussian segmentation models from 1 to 20 segments
(cghseg:::segmeanCO or Segmentor3IsBack::Segmentor), then label error
was computed using neuroblastoma$annotations
(penaltyLearning::labelError), then target intervals were
computed (penaltyLearning::targetInterval). Features were also
computed based on neuroblastoma$profiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("neuroblastomaProcessed")</code></pre>


<h3>Format</h3>

<p>List of two matrices: feature.mat is n.observations x n.features, and
target.mat is n.observations x 2, where n.observations=3418 and
n.features=117.
</p>

<hr>
<h2 id='notConverging'>
Interval regression problem that was not converging
</h2><span id='topic+notConverging'></span>

<h3>Description</h3>

<p>A small data set which was diverging using a previous implementation
of IntervalRegressionCV.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("notConverging")</code></pre>


<h3>Format</h3>

<p>A list with names: X.mat are numeric inputs, y.mat are numeric
outputs, fold.vec is an integer vector of fold ID numbers.
</p>


<h3>Source</h3>

<p>github.com/tdhock/neuroblastoma-data, data/H3K4me3_TDH_other/cv/equal_labels/testFolds/3/sampleSelectionGP_erf/5/order.csv
</p>

<hr>
<h2 id='oneSkip'>
oneSkip
</h2><span id='topic+oneSkip'></span>

<h3>Description</h3>

<p>A loss and model complexity function which never selects one of the
models, using a linear penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("oneSkip")</code></pre>


<h3>Format</h3>

<p>A list of two data.frames (input and output).
</p>


<h3>Source</h3>

<p>example(exactModelSelection) in PeakSegDP package.
</p>

<hr>
<h2 id='plot.IntervalRegression'>plot IntervalRegression</h2><span id='topic+plot.IntervalRegression'></span>

<h3>Description</h3>

<p>Plot an IntervalRegression model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'IntervalRegression'
plot(x, 
    ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.IntervalRegression_+3A_x">x</code></td>
<td>
<p>x </p>
</td></tr>
<tr><td><code id="plot.IntervalRegression_+3A_...">...</code></td>
<td>
<p>... </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a ggplot.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='predict.IntervalRegression'>predict IntervalRegression</h2><span id='topic+predict.IntervalRegression'></span>

<h3>Description</h3>

<p>Compute model predictions.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'IntervalRegression'
predict(object, 
    X, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.IntervalRegression_+3A_object">object</code></td>
<td>
<p>object </p>
</td></tr>
<tr><td><code id="predict.IntervalRegression_+3A_x">X</code></td>
<td>
<p>X </p>
</td></tr>
<tr><td><code id="predict.IntervalRegression_+3A_...">...</code></td>
<td>
<p>... </p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric matrix of predicted log(penalty) values.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='print.IntervalRegression'>print IntervalRegression</h2><span id='topic+print.IntervalRegression'></span>

<h3>Description</h3>

<p>print learned model parameters.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'IntervalRegression'
print(x, 
    ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.IntervalRegression_+3A_x">x</code></td>
<td>
<p>x </p>
</td></tr>
<tr><td><code id="print.IntervalRegression_+3A_...">...</code></td>
<td>
<p>... </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='ROChange'>ROC curve for changepoints</h2><span id='topic+ROChange'></span>

<h3>Description</h3>

<p>Compute a Receiver Operating Characteristic curve for a penalty
function.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ROChange(models, predictions, 
    problem.vars = character())</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROChange_+3A_models">models</code></td>
<td>
<p>data.frame describing the number of incorrect labels as a function
of log(lambda), with columns min.log.lambda, max.log.lambda, fp,
fn, possible.fp, possible.fn, etc. This can be computed via
labelError(modelSelection(...), ...)$model.errors &ndash; see examples.</p>
</td></tr>
<tr><td><code id="ROChange_+3A_predictions">predictions</code></td>
<td>
<p>data.frame with a column named pred.log.lambda, the predicted
log(penalty) value for each segmentation problem.</p>
</td></tr>
<tr><td><code id="ROChange_+3A_problem.vars">problem.vars</code></td>
<td>
<p>character: column names used to identify data set / segmentation
problem.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>named list of results:
</p>
<table>
<tr><td><code>roc</code></td>
<td>
<p>a data.table with one row for each point on
the ROC curve</p>
</td></tr>
<tr><td><code>thresholds</code></td>
<td>
<p>two rows of roc which correspond to the
predicted and minimal error thresholds</p>
</td></tr>
<tr><td><code>auc.polygon</code></td>
<td>
<p>a data.table with one row for
each vertex of the polygon used to
compute AUC</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>
<p>numeric Area Under the ROC curve</p>
</td></tr>
<tr><td><code>aum</code></td>
<td>
<p>numeric Area Under Min(FP,FN)</p>
</td></tr>
<tr><td><code>aum.grad</code></td>
<td>
<p>data.table with one row for each prediction, and
columns hi/lo bound for the aum
generalized gradient.</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  library(penaltyLearning)
  library(data.table)

  data(neuroblastomaProcessed, envir=environment())
  ## Get incorrect labels data for one profile.
  pid &lt;- 11
  pro.errors &lt;- neuroblastomaProcessed$errors[
    profile.id==pid][order(chromosome, min.log.lambda)]
  dcast(pro.errors, n.segments ~ chromosome, value.var="errors")
  ## Get the feature that corresponds to the BIC penalty = log(n),
  ## meaning log(penalty) = log(log(n)).
  chr.vec &lt;- paste(c(1:4, 11, 17))
  pid.names &lt;- paste0(pid, ".", chr.vec)
  BIC.feature &lt;- neuroblastomaProcessed$feature.mat[pid.names, "log2.n"]
  pred &lt;- data.table(pred.log.lambda=BIC.feature, chromosome=chr.vec)
  ## edit one prediction so that it ends up having the same threshold
  ## as another one, to illustrate an aum sub-differential with
  ## un-equal lo/hi bounds.
  err.changes &lt;- pro.errors[, {
    .SD[c(NA, diff(errors) != 0), .(min.log.lambda)]
  }, by=chromosome]
  (ch.vec &lt;- err.changes[, structure(min.log.lambda, names=chromosome)])
  other &lt;- "11"
  (diff.other &lt;- ch.vec[[other]]-pred[other, pred.log.lambda, on=.(chromosome)])
  pred["1", pred.log.lambda := ch.vec[["1"]]-diff.other, on=.(chromosome)]
  pred["4", pred.log.lambda := 2, on=.(chromosome)]
  ch.vec[["1"]]-pred["1", pred.log.lambda, on=.(chromosome)]
  result &lt;- ROChange(pro.errors, pred, "chromosome")
  library(ggplot2)
  ## Plot the ROC curves.
  ggplot()+
    geom_path(aes(FPR, TPR), data=result$roc)+
    geom_point(aes(FPR, TPR, color=threshold), data=result$thresholds, shape=1)

  ## Plot the number of incorrect labels as a function of threshold.
  ggplot()+
    geom_segment(aes(
      min.thresh, errors,
      xend=max.thresh, yend=errors),
      data=result$roc)+
    geom_point(aes((min.thresh+max.thresh)/2, errors, color=threshold),
               data=result$thresholds,
               shape=1)+
    xlab("log(penalty) constant added to BIC penalty")

  ## Plot area under Min(FP,FN).
  err.colors &lt;- c(
    "fp"="red",
    "fn"="deepskyblue",
    "min.fp.fn"="black")
  err.sizes &lt;- c(
    "fp"=3,
    "fn"=2,
    "min.fp.fn"=1)
  roc.tall &lt;- melt(result$roc, measure.vars=names(err.colors))
  area.rects &lt;- data.table(
    chromosome="total",
    result$roc[0&lt;min.fp.fn])
  (gg.total &lt;- ggplot()+
     geom_vline(
       xintercept=0,
       color="grey")+
     geom_rect(aes(
       xmin=min.thresh, xmax=max.thresh,
       ymin=0, ymax=min.fp.fn),
       data=area.rects,
       alpha=0.5)+
     geom_text(aes(
       min.thresh, min.fp.fn/2,
       label=sprintf(
         "Area Under Min(FP,FN)=%.3f ",
         result$aum)),
       data=area.rects[1],
       hjust=1,
       color="grey50")+
     geom_segment(aes(
       min.thresh, value,
       xend=max.thresh, yend=value,
       color=variable, size=variable),
       data=data.table(chromosome="total", roc.tall))+
     scale_size_manual(values=err.sizes)+
     scale_color_manual(values=err.colors)+
     theme_bw()+
     theme(panel.grid.minor=element_blank())+
     scale_x_continuous(
       "Prediction threshold")+
     scale_y_continuous(
       "Incorrectly predicted labels",
       breaks=0:10))

  ## Add individual error curves.
  tall.errors &lt;- melt(
    pro.errors[pred, on=.(chromosome)],
    measure.vars=c("fp", "fn"))
  gg.total+
    geom_segment(aes(
      min.log.lambda-pred.log.lambda, value,
      xend=max.log.lambda-pred.log.lambda, yend=value,
      size=variable, color=variable),
      data=tall.errors)+
    facet_grid(chromosome ~ ., scales="free", space="free")+
    theme(panel.spacing=grid::unit(0, "lines"))+
    geom_blank(aes(
      0, errors),
      data=data.table(errors=c(1.5, -0.5)))

  print(result$aum.grad)
  if(interactive()){#this can be too long for CRAN.
    ## Plot how Area Under Min(FP,FN) changes with each predicted value.
    aum.dt &lt;- pred[, {
      data.table(log.pen=seq(0, 4, by=0.5))[, {
        chr &lt;- paste(chromosome)
        new.pred.dt &lt;- data.table(pred)
        new.pred.dt[chr, pred.log.lambda := log.pen, on=.(chromosome)]
        with(
          ROChange(pro.errors, new.pred.dt, "chromosome"),
          data.table(aum))
      }, by=log.pen]
    }, by=chromosome]
    bounds.dt &lt;- melt(
      result$aum.grad,
      measure.vars=c("lo", "hi"),
      variable.name="bound",
      value.name="slope")[pred, on=.(chromosome)]
    bounds.dt[, intercept := result$aum-slope*pred.log.lambda]
    ggplot()+
      geom_abline(aes(
        slope=slope, intercept=intercept),
        size=1,
        data=bounds.dt)+
      geom_text(aes(
        2, 2, label=sprintf("directional derivatives = [%d, %d]", lo, hi)),
        data=result$aum.grad)+
      scale_color_manual(
        values=c(
          predicted="red",
          new="black"))+
      geom_point(aes(
        log.pen, aum, color=type),
        data=data.table(type="new", aum.dt))+
      geom_point(aes(
        pred.log.lambda, result$aum, color=type),
        shape=1,
        data=data.table(type="predicted", pred))+
      theme_bw()+
      theme(panel.spacing=grid::unit(0, "lines"))+
      facet_wrap("chromosome", labeller=label_both)+
      coord_equal()+
      xlab("New log(penalty) value for chromosome")+
      ylab("Area Under Min(FP,FN)
using new log(penalty) for this chromosome
and predicted log(penalty) for others")
  }

</code></pre>

<hr>
<h2 id='squared.hinge'>squared hinge</h2><span id='topic+squared.hinge'></span>

<h3>Description</h3>

<p>The squared hinge loss.</p>


<h3>Usage</h3>

<pre><code class='language-R'>squared.hinge(x, e = 1)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="squared.hinge_+3A_x">x</code></td>
<td>
<p>x </p>
</td></tr>
<tr><td><code id="squared.hinge_+3A_e">e</code></td>
<td>
<p>e </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='targetIntervalResidual'>targetIntervalResidual</h2><span id='topic+targetIntervalResidual'></span>

<h3>Description</h3>

<p>Compute residual of predicted penalties with respect to target
intervals. This function is useful for visualizing the errors in a
plot of log(penalty) versus a feature.</p>


<h3>Usage</h3>

<pre><code class='language-R'>targetIntervalResidual(target.mat, 
    pred)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="targetIntervalResidual_+3A_target.mat">target.mat</code></td>
<td>
<p>n x 2 numeric matrix: target intervals of log(penalty) values that
yield minimal incorrect labels.</p>
</td></tr>
<tr><td><code id="targetIntervalResidual_+3A_pred">pred</code></td>
<td>
<p>numeric vector: predicted log(penalty) values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of n residuals. Predictions that are too high
(above target.mat[,2]) get positive residuals (too few
changepoints), and predictions that are too low (below
target.mat[,1]) get negative residuals.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(penaltyLearning)
library(data.table)
data(neuroblastomaProcessed, envir=environment())
## The BIC model selection criterion is lambda = log(n), where n is
## the number of data points to segment. This implies log(lambda) =
## log(log(n)), which is the log2.n feature.
row.name.vec &lt;- grep(
  "^(4|520)[.]",
  rownames(neuroblastomaProcessed$feature.mat),
  value=TRUE)
feature.mat &lt;- neuroblastomaProcessed$feature.mat[row.name.vec, ]
target.mat &lt;- neuroblastomaProcessed$target.mat[row.name.vec, ]
pred.dt &lt;- data.table(
  row.name=row.name.vec,
  target.mat,
  feature.mat[, "log2.n", drop=FALSE])
pred.dt[, pred.log.lambda := log2.n ]
pred.dt[, residual := targetIntervalResidual(
  cbind(min.L, max.L),
  pred.log.lambda)]
library(ggplot2)
limits.dt &lt;- pred.dt[, data.table(
  log2.n,
  log.penalty=c(min.L, max.L),
  limit=rep(c("min", "max"), each=.N))][is.finite(log.penalty)]
ggplot()+
  geom_abline(slope=1, intercept=0)+
  geom_point(aes(
    log2.n,
    log.penalty,
    fill=limit),
    data=limits.dt,
    shape=21)+
  geom_segment(aes(
    log2.n, pred.log.lambda,
    xend=log2.n, yend=pred.log.lambda-residual),
    data=pred.dt,
    color="red")+
  scale_fill_manual(values=c(min="white", max="black"))

</code></pre>

<hr>
<h2 id='targetIntervalROC'>targetIntervalROC</h2><span id='topic+targetIntervalROC'></span>

<h3>Description</h3>

<p>Compute a ROC curve using a target interval matrix. A prediction
less than the lower limit is considered a false positive (penalty
too small, too many changes), and a prediction greater than the
upper limit is a false negative (penalty too large, too few
changes). WARNING: this ROC curve is less detailed than the one
you get from <code><a href="#topic+ROChange">ROChange</a></code>! Use <code><a href="#topic+ROChange">ROChange</a></code> if possible.</p>


<h3>Usage</h3>

<pre><code class='language-R'>targetIntervalROC(target.mat, 
    pred)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="targetIntervalROC_+3A_target.mat">target.mat</code></td>
<td>
<p>n x 2 numeric matrix: target intervals of log(penalty) values that
yield minimal incorrect labels.</p>
</td></tr>
<tr><td><code id="targetIntervalROC_+3A_pred">pred</code></td>
<td>
<p>numeric vector: predicted log(penalty) values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list describing ROC curves, same as <code><a href="#topic+ROChange">ROChange</a></code>.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(penaltyLearning)
library(data.table)
data(neuroblastomaProcessed, envir=environment())

pid.vec &lt;- c("1", "4")
chr &lt;- 2
incorrect.labels &lt;-
  neuroblastomaProcessed$errors[profile.id%in%pid.vec &amp; chromosome==chr]
pid.chr &lt;- paste0(pid.vec, ".", chr)
target.mat &lt;- neuroblastomaProcessed$target.mat[pid.chr, , drop=FALSE]
pred.dt &lt;- data.table(profile.id=pid.vec, pred.log.lambda=1.5)
roc.list &lt;- list(
  labels=ROChange(incorrect.labels, pred.dt, "profile.id"),
  targets=targetIntervalROC(target.mat, pred.dt$pred.log.lambda))

err &lt;- data.table(incorrect=names(roc.list))[, {
  roc.list[[incorrect]]$roc
}, by=incorrect]
library(ggplot2)
ggplot()+
  ggtitle("incorrect targets is an approximation of incorrect labels")+
  scale_size_manual(values=c(labels=2, targets=1))+
  geom_segment(aes(
    min.thresh, errors,
    color=incorrect,
    size=incorrect,
    xend=max.thresh, yend=errors),
               data=err)

</code></pre>

<hr>
<h2 id='targetIntervals'>Compute target intervals</h2><span id='topic+targetIntervals'></span>

<h3>Description</h3>

<p>Compute target intervals of log(penalty) values that result in
predicted changepoint <code>models</code> with minimum incorrect labels.
Use this function after <code><a href="#topic+labelError">labelError</a></code>, and before IntervalRegression*.</p>


<h3>Usage</h3>

<pre><code class='language-R'>targetIntervals(models, 
    problem.vars)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="targetIntervals_+3A_models">models</code></td>
<td>
<p>data.table with columns errors, min.log.lambda, max.log.lambda,
typically labelError()$model.errors.</p>
</td></tr>
<tr><td><code id="targetIntervals_+3A_problem.vars">problem.vars</code></td>
<td>
<p>character: column names used to identify data set / segmentation
problem.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table with columns <code>problem.vars</code>, one row for each
segmentation problem. The &quot;min.log.lambda&quot;, and &quot;max.log.lambda&quot;
columns give the largest interval of log(penalty) values which
results in the minimum incorrect labels for that problem. This can
be used to create the target.mat parameter of the
IntervalRegression* functions.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.table::setDTthreads(1)

library(penaltyLearning)
data(neuroblastomaProcessed, envir=environment())
targets.dt &lt;- targetIntervals(
  neuroblastomaProcessed$errors,
  problem.vars=c("profile.id", "chromosome"))

</code></pre>

<hr>
<h2 id='theme_no_space'>theme no space</h2><span id='topic+theme_no_space'></span>

<h3>Description</h3>

<p>ggplot2 theme element for no space between panels.</p>


<h3>Usage</h3>

<pre><code class='language-R'>theme_no_space(...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="theme_no_space_+3A_...">...</code></td>
<td>
<p>... </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
