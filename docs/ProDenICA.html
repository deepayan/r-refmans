<!DOCTYPE html><html lang="en"><head><title>Help for package ProDenICA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ProDenICA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#amari'>
<p>Compute the 'Amari' distance between two matrices</p></a></li>
<li><a href='#G1'>
<p>FastICA contrast functions.</p></a></li>
<li><a href='#GPois'>
<p>Fit a tilted Gaussian density via a Poisson GAM</p></a></li>
<li><a href='#ICAorthW'><p>turn a matrix W into an orthogonal matrix</p></a></li>
<li><a href='#mixmat'>
<p>Generate a random mixing matrix with</p>
condition number between 1 and 2</a></li>
<li><a href='#ProDenICA'><p>Product Density Independent Component Analysis</p></a></li>
<li><a href='#ProDenICA-internal'><p>Internal ProDenICA functions</p></a></li>
<li><a href='#rjordan'>
<p>Generate source densities for ICA</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Product Density Estimation for ICA using Tilted Gaussian Density
Estimates</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-02-20</td>
</tr>
<tr>
<td>Author:</td>
<td>Trevor Hastie, Rob Tibshirani</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Trevor Hastie &lt;hastie@stanford.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>gam</td>
</tr>
<tr>
<td>Description:</td>
<td>A direct and flexible method for estimating an ICA model. This approach estimates the densities for each component directly via a tilted Gaussian. The tilt functions are estimated via a GAM Poisson model. Details can be found in "Elements of Statistical Learning (2nd Edition)" in Section 14.7.4.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-02-20 14:11:09 UTC; hastie</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-02-21 08:50:23 UTC</td>
</tr>
</table>
<hr>
<h2 id='amari'>
Compute the 'Amari' distance between two matrices
</h2><span id='topic+amari'></span>

<h3>Description</h3>

<p>The Amari distance is a measure between two nonsingular matrices. Useful for
checking for convergence in ICA algorithms, and for comparing solutions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>amari(V, W, orth = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="amari_+3A_v">V</code></td>
<td>

<p>first matrix
</p>
</td></tr>
<tr><td><code id="amari_+3A_w">W</code></td>
<td>

<p>second matrix
</p>
</td></tr>
<tr><td><code id="amari_+3A_orth">orth</code></td>
<td>

<p>are the matrices orthogonal; default is <code>orth=FALSE</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formula is given in second reference below, page 570.
</p>


<h3>Value</h3>

<p>a numeric distance  metween 0 and 1</p>


<h3>Author(s)</h3>

<p>Trevor Hastie
</p>


<h3>References</h3>

<p>Bach, F. and Jordan, M. (2002). Kernel independent component analysis,
Journal of Machine Learning Research 3: 1-48<br />
Hastie, T., Tibshirani, R. and Friedman, J. (2009) Elements of
Statistical Learning (2nd edition), Springer.<br />
<a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf</a>
</p>


<h3>See Also</h3>

<p><code>ProDenICA</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dist="n" 
N=1024
p=2
A0&lt;-mixmat(p)
s&lt;-scale(cbind(rjordan(dist,N),rjordan(dist,N)))
x &lt;- s %*% A0
###Whiten the data
x &lt;- scale(x, TRUE, FALSE)
sx &lt;- svd(x)	### orthogonalization function
x &lt;- sqrt(N) * sx$u
target &lt;- solve(A0)
target &lt;- diag(sx$d) %*% t(sx$v) %*% target/sqrt(N)
W0 &lt;- matrix(rnorm(2*2), 2, 2)
W0 &lt;- ICAorthW(W0)
W1 &lt;- ProDenICA(x, W0=W0,trace=TRUE,Gfunc=G1)$W
fit=ProDenICA(x, W0=W0,Gfunc=GPois,trace=TRUE, density=TRUE)
W2 &lt;- fit$W
#distance of FastICA from target
amari(W1,target)
#distance of ProDenICA from target
amari(W2,target)
</code></pre>

<hr>
<h2 id='G1'>
FastICA contrast functions.
</h2><span id='topic+G1'></span><span id='topic+G0'></span>

<h3>Description</h3>

<p>contrast functions for computing the negentropy criteria used in
FastICA; see references.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>G1(s, a=1)
G0(s, a=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="G1_+3A_s">s</code></td>
<td>
<p>estimated independent component</p>
</td></tr>
<tr><td><code id="G1_+3A_a">a</code></td>
<td>
<p>additional tuning parameter (only used in <code>G1</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with components
</p>
<table role = "presentation">
<tr><td><code>Gs</code></td>
<td>
<p>contrast function evaluated at values of x. <code>mean(Gs)</code>
is measure of negentropy.</p>
</td></tr>
<tr><td><code>gs</code></td>
<td>
<p>estimated first derivative of <code>Gs</code> at x</p>
</td></tr>
<tr><td><code>gps</code></td>
<td>
<p>estimated second derivative of <code>Gs</code> at x</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Trevor Hastie and Rob Tibshirani
</p>


<h3>References</h3>

<p>Hyvarinen, A., Karhunen, J. and Oja, E. (2001). Independent Component
Analysis, Wiley, New York<br />
Hastie, T. and Tibshirani, R. (2003) <em>Independent Component Analysis
through Product Density Estimation</em> in <em>Advances in Neural Information
Processing Systems 15</em> (Becker, S. and Obermayer, K., eds), MIT Press,
Cambridge, MA. pp 649-656<br />
Hastie, T., Tibshirani, R. and Friedman, J. (2009) Elements of
Statistical Learning (2nd edition), Springer.<br />
<a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf</a>
</p>


<h3>See Also</h3>

<p><code>GPois</code> and <code>ProDenICA</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=2
### Can use letters a-r below for dist
dist="n" 
N=1024
A0&lt;-mixmat(p)
s&lt;-scale(cbind(rjordan(dist,N),rjordan(dist,N)))
x &lt;- s %*% A0
fit=ProDenICA(x,Gfunc=G1, whiten=TRUE)
</code></pre>

<hr>
<h2 id='GPois'>
Fit a tilted Gaussian density via a Poisson GAM</h2><span id='topic+GPois'></span>

<h3>Description</h3>

<p>This is a contrast method for <code>ProDenICA</code>. It fits a tilted
Gaussian density estimate by multiplying the Gaussian density by an
exponential tilt function using a cubic smoothing spline
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GPois(x, df = 6, B = 500, order = 1, widen = 1.2, density.return = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GPois_+3A_x">x</code></td>
<td>
<p>vector of real values</p>
</td></tr>
<tr><td><code id="GPois_+3A_df">df</code></td>
<td>
<p>degrees of freedom for the smoothing-spline fit; default is 6</p>
</td></tr>
<tr><td><code id="GPois_+3A_b">B</code></td>
<td>
<p>number of grid points for density estimate; default is 500</p>
</td></tr>
<tr><td><code id="GPois_+3A_order">order</code></td>
<td>
<p>A robustness parameter to avoid responding to outliers in
<code>x</code>. The range of <code>x</code> is estimated by the  <code>order</code>th
and <code>n-order+1</code>th order statistics. Default is <code>order=1</code></p>
</td></tr>
<tr><td><code id="GPois_+3A_widen">widen</code></td>
<td>
<p>an expansion factor to widen the range of <code>x</code>;
default is <code>widen=1.2</code></p>
</td></tr>
<tr><td><code id="GPois_+3A_density.return">density.return</code></td>
<td>
<p>logical variable, with default <code>FALSE</code>. If
<code>density.return=TRUE</code>, the estimated density is returned</p>
</td></tr>
<tr><td><code id="GPois_+3A_...">...</code></td>
<td>
<p>additional arguments to GAM;  typically not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Section 14.7.4
of 'Elements of Statistical Learning (Hastie, Tibshirani and Friedman,
2009, 2nd Edition)' for details</p>


<h3>Value</h3>

<p>a list with components
</p>
<table role = "presentation">
<tr><td><code>Gs</code></td>
<td>
<p>estimated contrast function, which is the log of the tilting
function, evaluated at the original values of <code>x</code>. <code>mean(Gs)</code>
is measure of negentropy</p>
</td></tr>
<tr><td><code>gs</code></td>
<td>
<p>estimated first derivative of <code>Gs</code> at <code>x</code></p>
</td></tr>
<tr><td><code>gps</code></td>
<td>
<p>estimated second derivative of <code>Gs</code> at <code>x</code></p>
</td></tr>
<tr><td><code>density</code></td>
<td>
<p>if <code>density.return=TRUE</code>, a list with components
<code>$x</code> the grid of B values of <code>x</code>, and <code>$y</code> the estimated
density.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Trevor Hastie and Rob Tibshirani
</p>


<h3>References</h3>

<p>Hastie, T. and Tibshirani, R. (2003) <em>Independent Component Analysis
through Product Density Estimation</em> in <em>Advances in Neural Information
Processing Systems 15</em> (Becker, S. and Obermayer, K., eds), MIT Press,
Cambridge, MA. pp 649-656<br />
Hastie, T., Tibshirani, R. and Friedman, J. (2009) Elements of
Statistical Learning (2nd edition), Springer.<br />
<a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf</a>
</p>


<h3>See Also</h3>

<p><code>ProDenICA</code>, <code>G1</code> and <code>G0</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=2
### Can use letters a-r below for dist
dist="n" 
N=1024
A0&lt;-mixmat(p)
s&lt;-scale(cbind(rjordan(dist,N),rjordan(dist,N)))
x &lt;- s %*% A0
fit=ProDenICA(x,Gfunc=GPois, whiten=TRUE, density=TRUE)
par(mfrow=c(2,1))
plot(fit)
</code></pre>

<hr>
<h2 id='ICAorthW'>turn a matrix W into an orthogonal matrix
</h2><span id='topic+ICAorthW'></span>

<h3>Description</h3>

<p>use the SVD to orthogonalize a matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICAorthW(W)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ICAorthW_+3A_w">W</code></td>
<td>
<p>input matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>simply replace the D matrix of the SVD of W by the identity</p>


<h3>Value</h3>

<p>orthogonalized version of W
</p>


<h3>Note</h3>

<p>If W=UDV', then returns UV'
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie
</p>


<h3>Examples</h3>

<pre><code class='language-R'>W0 &lt;- matrix(rnorm(2*2), 2, 2)
W0 &lt;- ICAorthW(W0)
</code></pre>

<hr>
<h2 id='mixmat'>
Generate a random mixing matrix with
condition number between 1 and 2
</h2><span id='topic+mixmat'></span>

<h3>Description</h3>

<p>A simple function for generating a 'well behaved' random square mixing matrix</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixmat(p = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mixmat_+3A_p">p</code></td>
<td>
<p>dimnesion of matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generates a random matrix by constructing its SVD. The singular values
are drawn from a uniform on [1,2], hence guaranteeing a condition number
between 1 and 2</p>


<h3>Value</h3>

<p>a pxp matrix
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie
</p>

<hr>
<h2 id='ProDenICA'>Product Density Independent Component Analysis</h2><span id='topic+ProDenICA'></span>

<h3>Description</h3>

<p>Fits an ICA model by directly estimating the densities of the
independent components using Poisson GAMs. The densities have the form
of tilted Gaussians, and hense directly estimate the contrast functions
that lead to negentropy measures. This function supports Section 14.7.4
of 'Elements of Statistical Learning (Hastie, Tibshirani and Friedman,
2009, 2nd Edition)'. Models include 'FastICA'. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ProDenICA(x, k = p, W0 = NULL, whiten = FALSE, maxit = 20, thresh = 1e-07,
restarts = 0, trace = FALSE, Gfunc = GPois, eps.rank = 1e-07, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ProDenICA_+3A_x">x</code></td>
<td>
<p>input matrix</p>
</td></tr>
<tr><td><code id="ProDenICA_+3A_k">k</code></td>
<td>
<p>Number of components required, less than or equal to the
number of columns of x</p>
</td></tr>
<tr><td><code id="ProDenICA_+3A_w0">W0</code></td>
<td>
<p>Optional initial matrix (for comparing algorithms)</p>
</td></tr>
<tr><td><code id="ProDenICA_+3A_whiten">whiten</code></td>
<td>
<p>Logical variable - should x be whitened. If TRUE, the SVD of X=UDV' is computed, and U is used (up to
rank(X) columns). Also k is reduced to min(k,rank(X)). If FALSE (default), it is
assumed that the user has pre-whitened x (and if not, the function may
not perform properly)</p>
</td></tr>
<tr><td><code id="ProDenICA_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations; default is 20</p>
</td></tr>
<tr><td><code id="ProDenICA_+3A_thresh">thresh</code></td>
<td>
<p>Convergence threshold, in terms of relative change in
Amari metric; dfault is 1e-7</p>
</td></tr>
<tr><td><code id="ProDenICA_+3A_restarts">restarts</code></td>
<td>
<p>Number of random restarts; default is 0</p>
</td></tr>
<tr><td><code id="ProDenICA_+3A_trace">trace</code></td>
<td>
<p>Trace iterations; default is FALSE</p>
</td></tr>
<tr><td><code id="ProDenICA_+3A_gfunc">Gfunc</code></td>
<td>
<p>Contrast functional which is basis for negentropy
measure. Default is <code>'GPois'</code> which fits a tilted Gaussian density using
a Poisson GAM. Other options are <code>'G1'</code> (cosh negentropy) and
<code>'G0'</code> (kurtosis negentropy)</p>
</td></tr>
<tr><td><code id="ProDenICA_+3A_eps.rank">eps.rank</code></td>
<td>
<p>Threshold for deciding rank of x if option
<code>whiten=TRUE</code>. Any singular value less than <code>eps.thresh</code>
smaller than the largest is treated as zero</p>
</td></tr>
<tr><td><code id="ProDenICA_+3A_...">...</code></td>
<td>
<p>Additional arguments for <code>Gfunc</code> areguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Section 14.7.4
of <em>Elements of Statistical Learning</em> (Hastie, Tibshirani and Friedman,
2009, 2nd Edition)</p>


<h3>Value</h3>

<p>An object of S3 class <code>"ProDenICA"</code> is returned, with the following components:
</p>
<table role = "presentation">
<tr><td><code>W</code></td>
<td>
<p>Orthonormal matrix that takes the whitened version of x to the
independent components</p>
</td></tr>
<tr><td><code>negentropy</code></td>
<td>
<p>The total negentropy measure of this solution</p>
</td></tr>
<tr><td><code>s</code></td>
<td>
<p>the matrix of k independent components</p>
</td></tr>
<tr><td><code>whitner</code></td>
<td>
<p>if <code>whiten=TRUE</code>, the matrix that whitens <code>x</code>,
else <code>NULL</code></p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object</p>
</td></tr>
<tr><td><code>density</code></td>
<td>
<p>If <code>Gfunc=GPois</code>, an list of length <code>k</code> with the density
estimates for each component</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Trevor Hastie and Rob Tibshirani
</p>


<h3>References</h3>

<p>Hastie, T. and Tibshirani, R. (2003) <em>Independent Component Analysis
through Product Density Estimation</em> in <em>Advances in Neural Information
Processing Systems 15</em> (Becker, S. and Obermayer, K., eds), MIT Press,
Cambridge, MA. pp 649-656<br />
Hastie, T., Tibshirani, R. and Friedman, J. (2009) Elements of
Statistical Learning (2nd edition), Springer.<br />
<a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf</a>
</p>


<h3>See Also</h3>

<p><code>GPois</code>, <code>G1</code> and <code>plot</code> method.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=2
### Can use letters a-r below for dist
dist="n" 
N=1024
A0&lt;-mixmat(p)
s&lt;-scale(cbind(rjordan(dist,N),rjordan(dist,N)))
x &lt;- s %*% A0
###Whiten the data
x &lt;- scale(x, TRUE, FALSE)
sx &lt;- svd(x)	### orthogonalization function
x &lt;- sqrt(N) * sx$u
target &lt;- solve(A0)
target &lt;- diag(sx$d) %*% t(sx$v) %*% target/sqrt(N)
W0 &lt;- matrix(rnorm(2*2), 2, 2)
W0 &lt;- ICAorthW(W0)
W1 &lt;- ProDenICA(x, W0=W0,trace=TRUE,Gfunc=G1)$W
fit=ProDenICA(x, W0=W0,Gfunc=GPois,trace=TRUE, density=TRUE)
W2 &lt;- fit$W
#distance of FastICA from target
amari(W1,target)
#distance of ProDenICA from target
amari(W2,target)
par(mfrow=c(2,1))
plot(fit)
</code></pre>

<hr>
<h2 id='ProDenICA-internal'>Internal ProDenICA functions</h2><span id='topic+d.gaussmix'></span><span id='topic+dmix.dexp'></span><span id='topic+modulus'></span><span id='topic+r.gaussmix'></span><span id='topic+rmix.dexp'></span>

<h3>Description</h3>

<p>Internal ProDenICA functions</p>


<h3>Details</h3>

<p>These are not intended for use by users. Thes functions are
used by <code>djordan</code> and <code>rjordan</code>
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie</p>

<hr>
<h2 id='rjordan'>
Generate source densities for ICA
</h2><span id='topic+rjordan'></span><span id='topic+djordan'></span>

<h3>Description</h3>

<p>Functions for generating the source densities used in Bach and Jordan
(2002), and reused in Hastie and Tibshirani (2003)</p>


<h3>Usage</h3>

<pre><code class='language-R'>rjordan(letter, n, ...)
djordan(letter, x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rjordan_+3A_letter">letter</code></td>
<td>
<p>one of the 18 letters <code>a-r</code>; see Figure 14.42 on page 569 of 'Elements of Statistical Learning'
</p>
</td></tr>
<tr><td><code id="rjordan_+3A_n">n</code></td>
<td>
<p>number of samples</p>
</td></tr>
<tr><td><code id="rjordan_+3A_x">x</code></td>
<td>
<p>ordinates at which to compute density</p>
</td></tr>
<tr><td><code id="rjordan_+3A_...">...</code></td>
<td>
<p>place filler for additional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function produces the example densities used in Bach and Jordan
(2002), and copied by Hastie and Tibshirani (2003). They include the
't', uniform, mixtures of exponentials and many mixtures of gaussian
densities. Each are standardized to have mean zero and variance 1.
</p>


<h3>Value</h3>

<p>Either a vector of density values the length of <code>x</code> for
<code>djordan</code>, or a vector of <code>n</code> draws for <code>rjordan</code>
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie
</p>


<h3>References</h3>

<p>Bach, F. and Jordan, M. (2002). Kernel independent component analysis,
Journal of Machine Learning Research 3: 1-48<br />
Hastie, T. and Tibshirani, R. (2003) <em>Independent Component Analysis
through Product Density Estimation</em> in <em>Advances in Neural Information
Processing Systems 15</em> (Becker, S. and Obermayer, K., eds), MIT Press,
Cambridge, MA. pp 649-656<br />
Hastie, T., Tibshirani, R. and Friedman, J. (2009) Elements of
Statistical Learning (2nd edition), Springer.<br />
<a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf</a>
</p>


<h3>See Also</h3>

<p><code>ProDenICA</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dist="n" 
N=1024
s&lt;-scale(cbind(rjordan(dist,N),rjordan(dist,N)))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
