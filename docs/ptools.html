<!DOCTYPE html><html><head><title>Help for package ptools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ptools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bisq_xy'><p>Bisquare weighted sum</p></a></li>
<li><a href='#buff_sp'><p>Creates buffer of sp polygon object</p></a></li>
<li><a href='#check_pois'><p>Checks the fit of a Poisson Distribution</p></a></li>
<li><a href='#count_xy'><p>Count of points in polygon</p></a></li>
<li><a href='#dcount_xy'><p>Count of points within distance of polygon</p></a></li>
<li><a href='#dist_xy'><p>Distance to nearest based on centroid</p></a></li>
<li><a href='#e_test'><p>Poisson E-test</p></a></li>
<li><a href='#hex_area'><p>Get area of hexagon given length of side</p></a></li>
<li><a href='#hex_dim'><p>Get dimensions of hexagon given area</p></a></li>
<li><a href='#hex_wd'><p>Get width of hexagon given height</p></a></li>
<li><a href='#idw_xy'><p>Inverse distance weighted sums</p></a></li>
<li><a href='#kern_xy'><p>Kernel density of nearby areas</p></a></li>
<li><a href='#near_strings1'><p>Strings of Near Repeats</p></a></li>
<li><a href='#near_strings2'><p>Strings of Near Repeats using KDtrees</p></a></li>
<li><a href='#nyc_bor'><p>NYC Boroughs</p></a></li>
<li><a href='#nyc_cafe'><p>NYC Sidewalk Cafes</p></a></li>
<li><a href='#nyc_liq'><p>NYC Alcohol Licenses</p></a></li>
<li><a href='#nyc_shoot'><p>NYPD Open Data on Shootings</p></a></li>
<li><a href='#pai'><p>Predictive Accuracy Index</p></a></li>
<li><a href='#pai_summary'><p>Summary Table for Multiple PAI Stats</p></a></li>
<li><a href='#pois_contour'><p>Checks the fit of a Poisson Distribution</p></a></li>
<li><a href='#powalt'><p>Power for Small Sample Exact Test</p></a></li>
<li><a href='#prep_grid'><p>Creates vector grid cells over study area</p></a></li>
<li><a href='#prep_hexgrid'><p>Creates hexagon grid cells over study area</p></a></li>
<li><a href='#small_samptest'><p>Small Sample Exact Test for Counts in Bins</p></a></li>
<li><a href='#vor_sp'><p>Voronoi tesselation from input points</p></a></li>
<li><a href='#wdd'><p>Estimates the WDD Test</p></a></li>
<li><a href='#wdd_harm'><p>Combines Multiple WDD Tests</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Tools for Poisson Data</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andrew Wheeler &lt;apwheele@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions used for analyzing count data, mostly crime counts. Includes checking difference in two Poisson counts (e-test), checking the fit for a Poisson distribution, small sample tests for counts in bins, Weighted Displacement Difference test (Wheeler and Ratcliffe, 2018) &lt;<a href="https://doi.org/10.1186%2Fs40163-018-0085-5">doi:10.1186/s40163-018-0085-5</a>&gt;, to evaluate crime changes over time in treated/control areas. Additionally includes functions for aggregating spatial data and spatial feature engineering.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/apwheele/ptools">https://github.com/apwheele/ptools</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Imports:</td>
<td>partitions, sp, raster, igraph, RANN, spatstat.geom,
spatstat.utils, sf, stats, methods</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-02-07 18:40:38 UTC; andre</td>
</tr>
<tr>
<td>Author:</td>
<td>Andrew Wheeler <a href="https://orcid.org/0000-0003-2255-1316"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-02-07 19:02:30 UTC</td>
</tr>
</table>
<hr>
<h2 id='bisq_xy'>Bisquare weighted sum</h2><span id='topic+bisq_xy'></span>

<h3>Description</h3>

<p>Given a base X/Y dataset, calculates bisquare weighted sums of points from feature dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bisq_xy(base, feat, bandwidth, weight = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bisq_xy_+3A_base">base</code></td>
<td>
<p>base dataset (eg gridcells), needs to be SpatialPolygonsDataFrame</p>
</td></tr>
<tr><td><code id="bisq_xy_+3A_feat">feat</code></td>
<td>
<p>feature dataset (eg another crime generator), needs to be SpatialPointsDataFrame</p>
</td></tr>
<tr><td><code id="bisq_xy_+3A_bandwidth">bandwidth</code></td>
<td>
<p>distances above this value do not contribute to the bi-square weight</p>
</td></tr>
<tr><td><code id="bisq_xy_+3A_weight">weight</code></td>
<td>
<p>if 1 (default), does not use attribute weights, else pass in string that is the variable name for weights in <code>feat</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This generates bi-square distance weighted sums of features within specified distance of the <code>base</code> centroid.
Bisquare weights are calculated as:
</p>
<p style="text-align: center;"><code class="reqn">w_{ij} = [ 1 - (d_{ij}/b)^2 ]^2 </code>
</p>

<p>where d_ij is the Euclidean distance between the base point and and the feature point. If d &lt; b, then w_ij equals 0. These are then multiplied
and summed so each base point gets a cumulative weighted sum. See the GWR book for a reference.
Uses loops and calculates all pairwise distances, so can be slow for large base and feature datasets. Consider
aggregating/weighting feature dataset if it is too slow. Useful for quantifying features nearby (Groff, 2014), or for egohoods
(e.g. spatial smoothing of demographic info, Hipp &amp; Boessen, 2013).
</p>


<h3>Value</h3>

<p>A vector of bi-square weighted sums
</p>


<h3>References</h3>

<p>Fotheringham, A. S., Brunsdon, C., &amp; Charlton, M. (2003). G<em>eographically weighted regression: the analysis of spatially varying relationships</em>. John Wiley &amp; Sons.
</p>
<p>Groff, E. R. (2014). Quantifying the exposure of street segments to drinking places nearby. <em>Journal of Quantitative Criminology</em>, 30(3), 527-548.
</p>
<p>Hipp, J. R., &amp; Boessen, A. (2013). Egohoods as waves washing across the city: A new measure of “neighborhoods”. Criminology, 51(2), 287-327.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dist_xy">dist_xy()</a></code> for calculating distance to nearest
</p>
<p><code><a href="#topic+count_xy">count_xy()</a></code> for counting points inside polygon
</p>
<p><code><a href="#topic+kern_xy">kern_xy()</a></code> for estimating gaussian density of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+bisq_xy">bisq_xy()</a></code> to estimate bi-square kernel weights of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+idw_xy">idw_xy()</a></code> to estimate inverse distance weights of points for features at base polygon xy coords
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(nyc_cafe); data(nyc_bor)
gr_nyc &lt;- prep_grid(nyc_bor,15000)
gr_nyc$bscafe &lt;- bisq_xy(gr_nyc,nyc_cafe,12000)


</code></pre>

<hr>
<h2 id='buff_sp'>Creates buffer of sp polygon object</h2><span id='topic+buff_sp'></span>

<h3>Description</h3>

<p>Creates buffer of sp polygon object. Intended to replace raster::buffer, which relies on rgeos
</p>


<h3>Usage</h3>

<pre><code class='language-R'>buff_sp(area, radius, dissolve = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="buff_sp_+3A_area">area</code></td>
<td>
<p>SpatialPolygon or SpatialPolygonDataFrame that defines the area</p>
</td></tr>
<tr><td><code id="buff_sp_+3A_radius">radius</code></td>
<td>
<p>scaler for the size of the buffer (in whatever units the polygon is projected in)</p>
</td></tr>
<tr><td><code id="buff_sp_+3A_dissolve">dissolve</code></td>
<td>
<p>boolean (default TRUE), to dissolve into single object, or leave as multiple objects</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Under the hood, this converts sp objects into sf objects and uses <code>st_buffer</code>.
When <code>dissolve=TRUE</code>, it uses <code>st_union(area)</code> and then buffers.
</p>


<h3>Value</h3>

<p>A SpatialPolygonDataFrame object (when dissolve=FALSE), or a SpatialPolygon object (when dissolve=TRUE)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sp) #for sp plot methods
# large grid cells
data(nyc_bor)
res &lt;- buff_sp(nyc_bor,7000)
plot(nyc_bor)
plot(res,border='BLUE',add=TRUE)

# When dissolve=FALSE, still returns individual units
# that can overlap
res2 &lt;- buff_sp(nyc_bor,7000,dissolve=FALSE)
plot(res2)

</code></pre>

<hr>
<h2 id='check_pois'>Checks the fit of a Poisson Distribution</h2><span id='topic+check_pois'></span>

<h3>Description</h3>

<p>Provides a frequency table to check the fit of a Poisson distribution to empirical data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_pois(counts, min_val, max_val, pred, silent = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_pois_+3A_counts">counts</code></td>
<td>
<p>vector of counts, e.g. c(0,5,1,3,4,6)</p>
</td></tr>
<tr><td><code id="check_pois_+3A_min_val">min_val</code></td>
<td>
<p>scaler minimum value to generate the grid of results, e.g. <code>0</code></p>
</td></tr>
<tr><td><code id="check_pois_+3A_max_val">max_val</code></td>
<td>
<p>scaler maximum value to generate the grid of results, e.g. <code>max(counts)</code></p>
</td></tr>
<tr><td><code id="check_pois_+3A_pred">pred</code></td>
<td>
<p>can either be a scaler, e.g. <code>mean(counts)</code>, or a vector (e.g. predicted values from a Poisson regression)</p>
</td></tr>
<tr><td><code id="check_pois_+3A_silent">silent</code></td>
<td>
<p>boolean, do not print mean/var stat messages, only applies when passing scaler for pred (default <code>FALSE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given either a scaler mean to test the fit, or a set of predictions (e.g. varying means predicted from a model), checks whether the data fits a given Poisson distribution over a specified set of integers. That is it builds a table of integer counts, and calculates the observed vs the expected distribution according to Poisson. Useful for checking any obvious deviations.
</p>


<h3>Value</h3>

<p>A dataframe with columns
</p>

<ul>
<li> <p><code>Int</code>, the integer value
</p>
</li>
<li> <p><code>Freq</code>, the total observed counts within that Integer value
</p>
</li>
<li> <p><code>PoisF</code>, the expected counts according to a Poisson distribution with mean/pred specified
</p>
</li>
<li> <p><code>ResidF</code>, the residual from <code>Freq - PoisF</code>
</p>
</li>
<li> <p><code>Prop</code>, the observed proportion of that integer (0-100 scale)
</p>
</li>
<li> <p><code>PoisD</code>, the expected proportion of that integer (0-100 scale)
</p>
</li>
<li> <p><code>ResidD</code>, the residual from <code>Prop - PoisD</code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Example use for constant over the whole sample
set.seed(10)
lambda &lt;- 0.2
x &lt;- rpois(10000,lambda)
pfit &lt;- check_pois(x,0,max(x),mean(x))
print(pfit)
# 82% zeroes is not zero inflated -- expected according to Poisson!

# Example use if you have varying predictions, eg after Poisson regression
n &lt;- 10000
ru &lt;- runif(n,0,10)
x &lt;- rpois(n,lambda=ru)
check_pois(x, 0, 23, ru)

# If you really want to do a statistical test of fit
chi_stat &lt;- sum((pfit$Freq - pfit$PoisF)^2/pfit$PoisF)
df &lt;- length(pfit$Freq) - 2
stats::dchisq(chi_stat, df) #p-value
# I prefer evaluating specific integers though (e.g. zero-inflated, longer-tails, etc.)

</code></pre>

<hr>
<h2 id='count_xy'>Count of points in polygon</h2><span id='topic+count_xy'></span>

<h3>Description</h3>

<p>Given a base X/Y dataset, calculates number of feature points that fall inside
</p>


<h3>Usage</h3>

<pre><code class='language-R'>count_xy(base, feat, weight = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="count_xy_+3A_base">base</code></td>
<td>
<p>base dataset (eg gridcells), needs to be SpatialPolygonsDataFrame</p>
</td></tr>
<tr><td><code id="count_xy_+3A_feat">feat</code></td>
<td>
<p>feature dataset (eg another crime generator), needs to be SpatialPointsDataFrame</p>
</td></tr>
<tr><td><code id="count_xy_+3A_weight">weight</code></td>
<td>
<p>if 1 (default), does not use weights, else pass in string that is the variable name for weights in <code>feat</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This generates a count (or weighted count) of features inside of the base areas. Both should be projected in the same units.
Uses <code>sp::over()</code> methods in the function.
</p>


<h3>Value</h3>

<p>A vector of counts (or weighted sums)
</p>


<h3>References</h3>

<p>Wheeler, A. P. (2019). Quantifying the local and spatial effects of alcohol outlets on crime. <em>Crime &amp; Delinquency</em>, 65(6), 845-871.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dist_xy">dist_xy()</a></code> for calculating distance to nearest
</p>
<p><code><a href="#topic+dcount_xy">dcount_xy()</a></code> for counting points within distance of base polygon
</p>
<p><code><a href="#topic+kern_xy">kern_xy()</a></code> for estimating gaussian density of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+bisq_xy">bisq_xy()</a></code> to estimate bi-square kernel weights of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+idw_xy">idw_xy()</a></code> to estimate inverse distance weights of points for features at base polygon xy coords
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(nyc_liq); data(nyc_bor)
gr_nyc &lt;- prep_grid(nyc_bor,10000)
gr_nyc$liq_cnt &lt;- count_xy(gr_nyc,nyc_liq)
gr_nyc$table_cnt &lt;- count_xy(gr_nyc,nyc_cafe,'SWC_TABLES')
head(gr_nyc@data)
sp::spplot(gr_nyc,zcol='liq_cnt')


</code></pre>

<hr>
<h2 id='dcount_xy'>Count of points within distance of polygon</h2><span id='topic+dcount_xy'></span>

<h3>Description</h3>

<p>Given a base X/Y dataset, calculates number of feature points that are within particular distance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcount_xy(base, feat, d, weight = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dcount_xy_+3A_base">base</code></td>
<td>
<p>base dataset (eg gridcells), needs to be SpatialPolygonsDataFrame</p>
</td></tr>
<tr><td><code id="dcount_xy_+3A_feat">feat</code></td>
<td>
<p>feature dataset (eg another crime generator), needs to be SpatialPointsDataFrame</p>
</td></tr>
<tr><td><code id="dcount_xy_+3A_d">d</code></td>
<td>
<p>scaler distance to count (based on polygon boundary for base, not centroid)</p>
</td></tr>
<tr><td><code id="dcount_xy_+3A_weight">weight</code></td>
<td>
<p>if 1 (default), does not use weights, else pass in string that is the variable name for weights in <code>feat</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This generates a count (or weighted count) of features within specified distance of the <code>base</code> <em>polygon</em> border.
Both should be projected in the same units. Uses <code>raster::buffer()</code> on <code>feat</code> dataset (which calls <code>rgeos</code>) and <code>sp::over</code> functions.
</p>


<h3>Value</h3>

<p>A vector of counts (or weighted sums)
</p>


<h3>References</h3>

<p>Groff, E. R. (2014). Quantifying the exposure of street segments to drinking places nearby. <em>Journal of Quantitative Criminology</em>, 30(3), 527-548.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dist_xy">dist_xy()</a></code> for calculating distance to nearest
</p>
<p><code><a href="#topic+count_xy">count_xy()</a></code> for counting points inside polygon
</p>
<p><code><a href="#topic+kern_xy">kern_xy()</a></code> for estimating gaussian density of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+bisq_xy">bisq_xy()</a></code> to estimate bi-square kernel weights of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+idw_xy">idw_xy()</a></code> to estimate inverse distance weights of points for features at base polygon xy coords
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(nyc_cafe); data(nyc_bor)
gr_nyc &lt;- prep_grid(nyc_bor,15000)
gr_nyc$dcafe_8k &lt;- dcount_xy(gr_nyc,nyc_cafe,8000)
head(gr_nyc@data)


</code></pre>

<hr>
<h2 id='dist_xy'>Distance to nearest based on centroid</h2><span id='topic+dist_xy'></span>

<h3>Description</h3>

<p>Given a base X/Y dataset, calculates distance to nearest for another feature X/Y dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist_xy(base, feat, bxy = c("x", "y"), fxy = c("x", "y"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist_xy_+3A_base">base</code></td>
<td>
<p>base dataset (eg gridcells)</p>
</td></tr>
<tr><td><code id="dist_xy_+3A_feat">feat</code></td>
<td>
<p>feature dataset (eg another crime generator)</p>
</td></tr>
<tr><td><code id="dist_xy_+3A_bxy">bxy</code></td>
<td>
<p>vector of strings that define what the base xy fields are defined as, defaults <code>c('x','y')</code></p>
</td></tr>
<tr><td><code id="dist_xy_+3A_fxy">fxy</code></td>
<td>
<p>vector of strings that define what the base xy fields are defined as, defaults <code>c('x','y')</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This generates a distance to nearest, based on the provided x/y coordinates (so if using polygons pass the centroid).
This uses kd-trees from RANN, so should be reasonably fast. But I do no projection checking, that is on you. You should not
use this with spherical coordinates. Useful for feature engineering for crime generators.
</p>


<h3>Value</h3>

<p>A vector of distances from base dataset xy to the nearest feature xy
</p>


<h3>References</h3>

<p>Caplan, J. M., Kennedy, L. W., &amp; Miller, J. (2011). Risk terrain modeling: Brokering criminological theory and GIS methods for crime forecasting. <em>Justice Quarterly</em>, 28(2), 360-381.
</p>
<p>Wheeler, A. P., &amp; Steenbeek, W. (2021). Mapping the risk terrain for crime using machine learning. <em>Journal of Quantitative Criminology</em>, 37(2), 445-480.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+count_xy">count_xy()</a></code> for counting points inside of base polygon
</p>
<p><code><a href="#topic+dcount_xy">dcount_xy()</a></code> for counting points within distance of base polygon
</p>
<p><code><a href="#topic+kern_xy">kern_xy()</a></code> for estimating gaussian density of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+bisq_xy">bisq_xy()</a></code> for estimate bi-square kernel of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+idw_xy">idw_xy()</a></code> for estimate inverese distance weighted of points for features at base polygon xy coords
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(nyc_bor); data(nyc_cafe)
gr_nyc &lt;- prep_grid(nyc_bor,15000,clip_level=0.3)
gr_nyc$dist_cafe &lt;- dist_xy(gr_nyc,nyc_cafe)
head(gr_nyc@data)
sp::spplot(gr_nyc,zcol='dist_cafe')


</code></pre>

<hr>
<h2 id='e_test'>Poisson E-test</h2><span id='topic+e_test'></span>

<h3>Description</h3>

<p>Tests differences in two Poisson means or rates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>e_test(k1, k2, n1 = 1, n2 = 1, d = 0, eps = 1e-20, silent = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="e_test_+3A_k1">k1</code></td>
<td>
<p>scaler Poisson count</p>
</td></tr>
<tr><td><code id="e_test_+3A_k2">k2</code></td>
<td>
<p>scaler Poisson count</p>
</td></tr>
<tr><td><code id="e_test_+3A_n1">n1</code></td>
<td>
<p>scaler divisor for k1 (e.g. rate per unit time or per area), default 1</p>
</td></tr>
<tr><td><code id="e_test_+3A_n2">n2</code></td>
<td>
<p>scaler divisor for k2 (e.g. rate per unit time or per area), default 1</p>
</td></tr>
<tr><td><code id="e_test_+3A_d">d</code></td>
<td>
<p>scaler amends the null test by a constant amount, default 0</p>
</td></tr>
<tr><td><code id="e_test_+3A_eps">eps</code></td>
<td>
<p>scaler where to terminate sum in testing larger deviations, default 1e-20</p>
</td></tr>
<tr><td><code id="e_test_+3A_silent">silent</code></td>
<td>
<p>boolean if TRUE, does not print error messages</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This e-test tests the differences in two Poisson counts or rates. The null is more formally:
</p>
<p style="text-align: center;"><code class="reqn">k_1/n_1 = k_2/n_2 + d</code>
</p>

<p>Note, I would be wary using the test for Poisson counts over 100 (the tail approximation in the sums will have issues, as the PMF is so spread out). (It is also the case with <em>very</em> large k's, e.g. <code>e_test(4000,4000)</code> my function could run out of memory.) In that case may use the n arguments to make it a rate per some unit time (which can change the p-value, although for smaller counts/rates should be very close).
</p>


<h3>Value</h3>

<p>A scaler p-value. Will return -1 if inputs don't make sense and print an error message, e.g. <code>e_test(0,0)</code> is undefined and will return a -1.
</p>


<h3>References</h3>

<p>Krishnamoorthy, K., &amp; Thomson, J. (2004). A more powerful test for comparing two Poisson means. <em>Journal of Statistical Planning and Inference</em>, 119(1), 23-35.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wdd">wdd()</a></code>, can use that function for a normal based approximation to the difference in Poisson means as well as pre/post designs
</p>


<h3>Examples</h3>

<pre><code class='language-R'># For small N, changes in rates should result in same p-value minus floating point differences
e_test(3,0)
e_test(3,0,2,2)

# Not defined
e_test(0,0) #returns -1 and prints warning

# The same rates
e_test(20,10,4,2)
e_test(10,5,2,1) #not quite the same

# Order of counts/rates should not matter
e_test(6,2) #second example from Krishnamoorthy article
e_test(2,6) #when d=0, can switch arguments and get the same p-value

# These are not the same however, due to how the variance estimates work
e_test(3,2)
e_test(3,1,d=1)
</code></pre>

<hr>
<h2 id='hex_area'>Get area of hexagon given length of side</h2><span id='topic+hex_area'></span>

<h3>Description</h3>

<p>The length of the side is half of the length from vertex to vertex (so height in <code>geom_hex</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hex_area(side)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hex_area_+3A_side">side</code></td>
<td>
<p>scaler</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For use with ggplot and <code>geom_hex</code> binwidth arguments, which expects arguments in width/height. I want hexagons in maps to be a specific area. See <a href="https://andrewpwheeler.com/2019/08/07/making-a-hexbin-map-in-ggplot/">this blog post</a> for a specific use case with ggplot.
</p>


<h3>Value</h3>

<p>A scaler for the width
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hex_wd">hex_wd()</a></code> for estimating the width given the height
<code><a href="#topic+hex_dim">hex_dim()</a></code> for estimating width/height given area
</p>


<h3>Examples</h3>

<pre><code class='language-R'>area_check &lt;- 1000
wh &lt;- hex_dim(area_check^2)   #e.g. a square kilometer if spatial units are in meters
area &lt;- hex_area(wh[1]/2)       #inverse operation
all.equal(area_check,sqrt(area))
wi &lt;- hex_wd(wh[1])
all.equal(wh[2],wi)

</code></pre>

<hr>
<h2 id='hex_dim'>Get dimensions of hexagon given area</h2><span id='topic+hex_dim'></span>

<h3>Description</h3>

<p>Get dimensions of hexagon given area
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hex_dim(area)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hex_dim_+3A_area">area</code></td>
<td>
<p>scaler</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For use with ggplot and <code>geom_hex</code> binwidth arguments, which expects arguments in width/height. I want hexagons in maps to be a specific area. See <a href="https://andrewpwheeler.com/2019/08/07/making-a-hexbin-map-in-ggplot/">this blog post</a> for a specific use case with ggplot.
</p>


<h3>Value</h3>

<p>a vector with two elements, first element is the height (vertex to vertex), the second element is the width (side to side)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hex_wd">hex_wd()</a></code> for estimating the width given the height
<code><a href="#topic+hex_area">hex_area()</a></code> for estimating the area given side length
</p>


<h3>Examples</h3>

<pre><code class='language-R'>area_check &lt;- 1000
wh &lt;- hex_dim(area_check^2)   #e.g. a square kilometer if spatial units are in meters
area &lt;- hex_area(wh[1]/2)       #inverse operation
all.equal(area_check,sqrt(area))
wi &lt;- hex_wd(wh[1])
all.equal(wh[2],wi)

</code></pre>

<hr>
<h2 id='hex_wd'>Get width of hexagon given height</h2><span id='topic+hex_wd'></span>

<h3>Description</h3>

<p>Get width of hexagon given height
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hex_wd(height)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hex_wd_+3A_height">height</code></td>
<td>
<p>scaler</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For use with ggplot and <code>geom_hex</code> binwidth arguments, which expects arguments in width/height. I want hexagons in maps to be a specific area. See <a href="https://andrewpwheeler.com/2019/08/07/making-a-hexbin-map-in-ggplot/">this blog post</a> for a specific use case with ggplot.
</p>


<h3>Value</h3>

<p>A scaler for the width
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hex_area">hex_area()</a></code> for estimating the area given side length
<code><a href="#topic+hex_dim">hex_dim()</a></code> for estimating width/height given area
</p>


<h3>Examples</h3>

<pre><code class='language-R'>area_check &lt;- 1000
wh &lt;- hex_dim(area_check^2)   #e.g. a square kilometer if spatial units are in meters
area &lt;- hex_area(wh[1]/2)       #inverse operation
all.equal(area_check,sqrt(area))
wi &lt;- hex_wd(wh[1])
all.equal(wh[2],wi)

</code></pre>

<hr>
<h2 id='idw_xy'>Inverse distance weighted sums</h2><span id='topic+idw_xy'></span>

<h3>Description</h3>

<p>Given a base X/Y dataset, calculates clipped inverse distance weighted sums of points from feature dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>idw_xy(base, feat, clip = 1, weight = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="idw_xy_+3A_base">base</code></td>
<td>
<p>base dataset (eg gridcells), needs to be SpatialPolygonsDataFrame</p>
</td></tr>
<tr><td><code id="idw_xy_+3A_feat">feat</code></td>
<td>
<p>feature dataset (eg another crime generator), needs to be SpatialPointsDataFrame</p>
</td></tr>
<tr><td><code id="idw_xy_+3A_clip">clip</code></td>
<td>
<p>scaler minimum value for weight, default <code>1</code> (so weights cannot be below 0)</p>
</td></tr>
<tr><td><code id="idw_xy_+3A_weight">weight</code></td>
<td>
<p>if 1 (default), does not use weights, else pass in string that is the variable name for weights in <code>feat</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This generates a inverse distance weighted sum of features within specified distance of the <code>base</code> centroid.
Weights are clipped to never be below <code>clip</code> value, which prevents division by 0 (or division by a very small distance number)
Uses loops and calculates all pairwise distances, so can be slow for large base and feature datasets. Consider
aggregating/weighting feature dataset if it is too slow. Useful for quantifying features nearby (Groff, 2014), or for egohoods
(e.g. spatial smoothing of demographic info, Hipp &amp; Boessen, 2013).
</p>


<h3>Value</h3>

<p>A vector of IDW weighted sums
</p>


<h3>References</h3>

<p>Groff, E. R. (2014). Quantifying the exposure of street segments to drinking places nearby. <em>Journal of Quantitative Criminology</em>, 30(3), 527-548.
</p>
<p>Hipp, J. R., &amp; Boessen, A. (2013). Egohoods as waves washing across the city: A new measure of “neighborhoods”. Criminology, 51(2), 287-327.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dist_xy">dist_xy()</a></code> for calculating distance to nearest
</p>
<p><code><a href="#topic+count_xy">count_xy()</a></code> for counting points inside polygon
</p>
<p><code><a href="#topic+kern_xy">kern_xy()</a></code> for estimating gaussian density of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+bisq_xy">bisq_xy()</a></code> to estimate bi-square kernel weights of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+idw_xy">idw_xy()</a></code> to estimate inverse distance weights of points for features at base polygon xy coords
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(nyc_cafe); data(nyc_bor)
gr_nyc &lt;- prep_grid(nyc_bor,15000)
gr_nyc$idwcafe &lt;- idw_xy(gr_nyc,nyc_cafe)
head(gr_nyc@data)


</code></pre>

<hr>
<h2 id='kern_xy'>Kernel density of nearby areas</h2><span id='topic+kern_xy'></span>

<h3>Description</h3>

<p>Given a base X/Y dataset, calculates guassian kernel density for nearby points in feat dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kern_xy(base, feat, bandwidth, weight = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kern_xy_+3A_base">base</code></td>
<td>
<p>base dataset (eg gridcells), needs to be SpatialPolygonsDataFrame or SpatialPointsDataFrame</p>
</td></tr>
<tr><td><code id="kern_xy_+3A_feat">feat</code></td>
<td>
<p>feature dataset (eg another crime generator), needs to be SpatialPointsDataFrame</p>
</td></tr>
<tr><td><code id="kern_xy_+3A_bandwidth">bandwidth</code></td>
<td>
<p>scaler bandwidth for the normal KDE</p>
</td></tr>
<tr><td><code id="kern_xy_+3A_weight">weight</code></td>
<td>
<p>if 1 (default), does not use weights, else pass in string that is the variable name for weights in <code>feat</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This generates a density of nearby features at particular control points (specified by <code>base</code>). Useful for risk terrain
style feature engineering given nearby crime generators. Loops through all pairwise distances (and uses <code>dnorm()</code>). So will be slow
for large base + feature datasets (although should be OK memory wise). Consider aggregating/weighting data if <code>feat</code> is very large.
</p>


<h3>Value</h3>

<p>A vector of densities (or weighted densities)
</p>


<h3>References</h3>

<p>Caplan, J. M., Kennedy, L. W., &amp; Miller, J. (2011). Risk terrain modeling: Brokering criminological theory and GIS methods for crime forecasting. <em>Justice Quarterly</em>, 28(2), 360-381.
</p>
<p>Wheeler, A. P., &amp; Steenbeek, W. (2021). Mapping the risk terrain for crime using machine learning. <em>Journal of Quantitative Criminology</em>, 37(2), 445-480.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dist_xy">dist_xy()</a></code> for calculating distance to nearest
</p>
<p><code><a href="#topic+count_xy">count_xy()</a></code> for counting points inside polygon
</p>
<p><code><a href="#topic+kern_xy">kern_xy()</a></code> for estimating gaussian density of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+bisq_xy">bisq_xy()</a></code> to estimate bi-square kernel weights of points for features at base polygon xy coords
</p>
<p><code><a href="#topic+idw_xy">idw_xy()</a></code> to estimate inverse distance weights of points for features at base polygon xy coords
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(nyc_cafe); data(nyc_bor)
gr_nyc &lt;- prep_grid(nyc_bor,15000)
gr_nyc$kdecafe_5k &lt;- kern_xy(gr_nyc,nyc_cafe,8000)
head(gr_nyc@data)
sp::spplot(gr_nyc,zcol='kdecafe_5k')


</code></pre>

<hr>
<h2 id='near_strings1'>Strings of Near Repeats</h2><span id='topic+near_strings1'></span>

<h3>Description</h3>

<p>Identifies cases that are nearby each other in space/time
</p>


<h3>Usage</h3>

<pre><code class='language-R'>near_strings1(dat, id, x, y, tim, DistThresh, TimeThresh)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="near_strings1_+3A_dat">dat</code></td>
<td>
<p>data frame</p>
</td></tr>
<tr><td><code id="near_strings1_+3A_id">id</code></td>
<td>
<p>string for id variable in data frame (should be unique)</p>
</td></tr>
<tr><td><code id="near_strings1_+3A_x">x</code></td>
<td>
<p>string for variable that has the x coordinates</p>
</td></tr>
<tr><td><code id="near_strings1_+3A_y">y</code></td>
<td>
<p>string for variable that has the y coordinates</p>
</td></tr>
<tr><td><code id="near_strings1_+3A_tim">tim</code></td>
<td>
<p>string for variable that has the time stamp (should be numeric or datetime)</p>
</td></tr>
<tr><td><code id="near_strings1_+3A_distthresh">DistThresh</code></td>
<td>
<p>scaler for distance threshold (in whatever units x/y are in)</p>
</td></tr>
<tr><td><code id="near_strings1_+3A_timethresh">TimeThresh</code></td>
<td>
<p>scaler for time threshold (in whatever units tim is in)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns strings of cases nearby in space and time. Useful for near-repeat analysis, or to
identify potentially duplicate cases. This particular function is memory safe, although uses loops and will be
approximately <code class="reqn">O(n^2)</code> time (or more specifically <code>choose(n,2)</code>). Tests I have done
<a href="https://andrewpwheeler.com/2017/04/12/identifying-near-repeat-crime-strings-in-r-or-python/">on my machine</a>
5k rows take only ~10 seconds, but ~100k rows takes around 12 minutes with this code.
</p>


<h3>Value</h3>

<p>A data frame that contains the ids as row.names, and two columns:
</p>

<ul>
<li> <p><code>CompId</code>, a unique identifier that lets you collapse original cases together
</p>
</li>
<li> <p><code>CompNum</code>, the number of linked cases inside of a component
</p>
</li></ul>



<h3>References</h3>

<p>Wheeler, A. P., Riddell, J. R., &amp; Haberman, C. P. (2021). Breaking the chain: How arrests reduce the probability of near repeat crimes. <em>Criminal Justice Review</em>, 46(2), 236-258.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+near_strings2">near_strings2()</a></code>, which uses kdtrees, so should be faster with larger data frames, although still may run out of memory, and is not 100% guaranteed to return all nearby strings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simplified example showing two clusters
s &lt;- c(0,0,0,4,4)
ccheck &lt;- c(1,1,1,2,2)
dat &lt;- data.frame(x=1:5,y=0,
                  ti=s,
                  id=1:5)
res1 &lt;- near_strings1(dat,'id','x','y','ti',2,1)
print(res1)

#Full nyc_shoot data with this function takes ~40 seconds
library(sp)
data(nyc_shoot)
nyc_shoot$id &lt;- 1:nrow(nyc_shoot) #incident ID can have dups
mh &lt;- nyc_shoot[nyc_shoot$BORO == 'MANHATTAN',]
print(Sys.time())
res &lt;- near_strings1(mh@data,id='id',x='X_COORD_CD',y='Y_COORD_CD',
                      tim='OCCUR_DATE',DistThresh=1500,TimeThresh=3)
print(Sys.time()) #3k shootings takes only ~1 second on my machine


</code></pre>

<hr>
<h2 id='near_strings2'>Strings of Near Repeats using KDtrees</h2><span id='topic+near_strings2'></span>

<h3>Description</h3>

<p>Identifies cases that are nearby each other in space/time
</p>


<h3>Usage</h3>

<pre><code class='language-R'>near_strings2(dat, id, x, y, tim, DistThresh, TimeThresh, k = 300, eps = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="near_strings2_+3A_dat">dat</code></td>
<td>
<p>data frame</p>
</td></tr>
<tr><td><code id="near_strings2_+3A_id">id</code></td>
<td>
<p>string for id variable in data frame (should be unique)</p>
</td></tr>
<tr><td><code id="near_strings2_+3A_x">x</code></td>
<td>
<p>string for variable that has the x coordinates</p>
</td></tr>
<tr><td><code id="near_strings2_+3A_y">y</code></td>
<td>
<p>string for variable that has the y coordinates</p>
</td></tr>
<tr><td><code id="near_strings2_+3A_tim">tim</code></td>
<td>
<p>string for variable that has the time stamp (should be numeric or datetime)</p>
</td></tr>
<tr><td><code id="near_strings2_+3A_distthresh">DistThresh</code></td>
<td>
<p>scaler for distance threshold (in whatever units x/y are in)</p>
</td></tr>
<tr><td><code id="near_strings2_+3A_timethresh">TimeThresh</code></td>
<td>
<p>scaler for time threshold (in whatever units tim is in)</p>
</td></tr>
<tr><td><code id="near_strings2_+3A_k">k</code></td>
<td>
<p>the k for the max number of neighbors to grab in the nn2 function in RANN package</p>
</td></tr>
<tr><td><code id="near_strings2_+3A_eps">eps</code></td>
<td>
<p>the nn2 function returns &lt;=, so to return less (like <code>near_strings1()</code>), needs a small fudge factor</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns strings of cases nearby in space and time. Useful for near-repeat analysis, or to
identify potentially duplicate cases. This particular function uses kdtrees (from the RANN library).
For very large data frames, this will run quite a bit faster than <code>near_strings1</code> (although still may run out of memory).
And it is not 100% guaranteed to grab all of the pairs. Tests I have done
<a href="https://andrewpwheeler.com/2017/04/12/identifying-near-repeat-crime-strings-in-r-or-python/">on my machine</a>
~100k rows takes around 2 minutes with this code.
</p>


<h3>Value</h3>

<p>A data frame that contains the ids as row.names, and two columns:
</p>

<ul>
<li> <p><code>CompId</code>, a unique identifier that lets you collapse original cases together
</p>
</li>
<li> <p><code>CompNum</code>, the number of linked cases inside of a component
</p>
</li></ul>



<h3>References</h3>

<p>Wheeler, A. P., Riddell, J. R., &amp; Haberman, C. P. (2021). Breaking the chain: How arrests reduce the probability of near repeat crimes. <em>Criminal Justice Review</em>, 46(2), 236-258.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+near_strings1">near_strings1()</a></code>, which uses loops but is guaranteed to get all pairs of cases and should be memory safe.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simplified example showing two clusters
s &lt;- c(0,0,0,4,4)
ccheck &lt;- c(1,1,1,2,2)
dat &lt;- data.frame(x=1:5,y=0,
                  ti=s,
                  id=1:5)
res1 &lt;- near_strings2(dat,'id','x','y','ti',2,1)
print(res1)


# This runs faster than near_strings1
library(sp)
nyc_shoot$id &lt;- 1:nrow(nyc_shoot)  #incident ID can have dups
print(Sys.time())
res &lt;- near_strings2(nyc_shoot@data,id='id',x='X_COORD_CD',y='Y_COORD_CD',
                     tim='OCCUR_DATE',DistThresh=1500,TimeThresh=3)
print(Sys.time()) #around 4 seconds on my machine
head(res)


</code></pre>

<hr>
<h2 id='nyc_bor'>NYC Boroughs</h2><span id='topic+nyc_bor'></span>

<h3>Description</h3>

<p>Spatial file for New York City Borough outlines without water areas
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nyc_bor
</code></pre>


<h3>Format</h3>

<p>A SpatialPolygonsDataFrame object of the NYC Boroughs. This is projected (same coordinates as shootings). See the <a href="https://www.nyc.gov/site/planning/data-maps/open-data.page">Bytes of the Big Apple</a> for any details on the file.
</p>


<h3>Source</h3>


<ul>
<li> <p><a href="https://www1.nyc.gov/assets/planning/download/zip/data-maps/open-data/nybb_21c.zip">https://www1.nyc.gov/assets/planning/download/zip/data-maps/open-data/nybb_21c.zip</a>
</p>
</li></ul>


<hr>
<h2 id='nyc_cafe'>NYC Sidewalk Cafes</h2><span id='topic+nyc_cafe'></span>

<h3>Description</h3>

<p>Point locations for sidewalk cafes in NYC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nyc_cafe
</code></pre>


<h3>Format</h3>

<p>A SpatialPointsDataFrame with point locations Sidwalk cafes in NYC. Note currently
includes only active license locations. Current N around 400 and none in Staten Island.
</p>


<h3>Source</h3>


<ul>
<li> <p><a href="https://data.cityofnewyork.us/Business/Sidewalk-Caf-Licenses-and-Applications/qcdj-rwhu">https://data.cityofnewyork.us/Business/Sidewalk-Caf-Licenses-and-Applications/qcdj-rwhu</a>
</p>
</li></ul>


<hr>
<h2 id='nyc_liq'>NYC Alcohol Licenses</h2><span id='topic+nyc_liq'></span>

<h3>Description</h3>

<p>Point locations for alcohol locations inside NYC boroughs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nyc_liq
</code></pre>


<h3>Format</h3>

<p>A SpatialPointsDataFrame with point locations for alcohol licenses inside of NYC.
Note that some of these are not the actual sales place, but another address for the business.
Currently over 18,000 addresses.
</p>


<h3>Source</h3>


<ul>
<li> <p><a href="https://data.ny.gov/Economic-Development/Liquor-Authority-Current-List-of-Active-Licenses/hrvs-fxs2">https://data.ny.gov/Economic-Development/Liquor-Authority-Current-List-of-Active-Licenses/hrvs-fxs2</a>
</p>
</li></ul>


<hr>
<h2 id='nyc_shoot'>NYPD Open Data on Shootings</h2><span id='topic+nyc_shoot'></span>

<h3>Description</h3>

<p>Shootings recorded from the New York City Police Department from 2006 to current.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nyc_shoot
</code></pre>


<h3>Format</h3>

<p>A SpatialPointsDataFrame with currently over 20k rows and 21 fields, including date/time and address level geocoordinates for the event. Data from 2006 to currently. See the info on Socrata for the field name codebook.
</p>


<h3>Source</h3>


<ul>
<li> <p><a href="https://data.cityofnewyork.us/Public-Safety/NYPD-Shooting-Incident-Data-Year-To-Date-/5ucz-vwe8">https://data.cityofnewyork.us/Public-Safety/NYPD-Shooting-Incident-Data-Year-To-Date-/5ucz-vwe8</a> for current
</p>
</li>
<li> <p><a href="https://data.cityofnewyork.us/Public-Safety/NYPD-Shooting-Incident-Data-Historic-/833y-fsy8">https://data.cityofnewyork.us/Public-Safety/NYPD-Shooting-Incident-Data-Historic-/833y-fsy8</a> for historical
</p>
</li></ul>


<hr>
<h2 id='pai'>Predictive Accuracy Index</h2><span id='topic+pai'></span>

<h3>Description</h3>

<p>Given a set of predictions and observed counts, returns the PAI (predictive accuracy index),
PEI (predictive efficiency index), and the RRI (recovery rate index)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pai(dat, count, pred, area, other = c())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pai_+3A_dat">dat</code></td>
<td>
<p>data frame with the predictions, observed counts, and area sizes (can be a vector of ones)</p>
</td></tr>
<tr><td><code id="pai_+3A_count">count</code></td>
<td>
<p>character specifying the column name for the observed counts (e.g. the out of sample crime counts)</p>
</td></tr>
<tr><td><code id="pai_+3A_pred">pred</code></td>
<td>
<p>character specifying the column name for the predicted counts (e.g. predictions based on a model)</p>
</td></tr>
<tr><td><code id="pai_+3A_area">area</code></td>
<td>
<p>character specifying the column name for the area sizes (could also be street segment distances, see Drawve &amp; Wooditch, 2019)</p>
</td></tr>
<tr><td><code id="pai_+3A_other">other</code></td>
<td>
<p>vector of strings for any other column name you want to keep (e.g. an ID variable), defaults to empty <code>c()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given predictions over an entire sample, this returns a dataframe with the sorted best PAI (sorted by density of predicted counts per area).
PAI is defined as:
</p>
<p style="text-align: center;"><code class="reqn">PAI = \frac{c_t/C}{a_t/A}</code>
</p>

<p>Where the numerator is the percent of crimes in cumulative t areas, and the denominator is the percent of the area encompassed.
PEI is the observed PAI divided by the best possible PAI if you were a perfect oracle, so is scaled between 0 and 1.
RRI is <code>predicted/observed</code>, so if you have very bad predictions can return Inf or undefined!
See Wheeler &amp; Steenbeek (2019) for the definitions of the different metrics.
User note, PEI may behave funny with different sized areas.
</p>


<h3>Value</h3>

<p>A dataframe with the columns:
</p>

<ul>
<li> <p><code>Order</code>, The order of the resulting rankings
</p>
</li>
<li> <p><code>Count</code>, the counts for the original crimes you specified
</p>
</li>
<li> <p><code>Pred</code>, the original predictions
</p>
</li>
<li> <p><code>Area</code>, the area for the units of analysis
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;Cum*&#8288;</code>, the cumulative totals for Count/Pred/Area
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;PCum*&#8288;</code>, the proportion cumulative totals, e.g. <code>CumCount/sum(Count)</code>
</p>
</li>
<li> <p><code>PAI</code>, the PAI stat
</p>
</li>
<li> <p><code>PEI</code>, the PEI stat
</p>
</li>
<li> <p><code>RRI</code>, the RRI stat (probably should analyze/graph the <code>log(RRI)</code>)!
</p>
</li></ul>

<p>Plus any additional variables specified by <code>other</code> at the end of the dataframe.
</p>


<h3>References</h3>

<p>Drawve, G., &amp; Wooditch, A. (2019). A research note on the methodological and theoretical considerations for assessing crime forecasting accuracy with the predictive accuracy index. <em>Journal of Criminal Justice</em>, 64, 101625.
</p>
<p>Wheeler, A. P., &amp; Steenbeek, W. (2021). Mapping the risk terrain for crime using machine learning. <em>Journal of Quantitative Criminology</em>, 37(2), 445-480.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pai_summary">pai_summary()</a></code> for a summary table of metrics for multiple pai tables given fixed N thresholds
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Making some very simple fake data
crime_dat &lt;- data.frame(id=1:6,
                        obs=c(6,7,3,2,1,0),
                        pred=c(8,4,4,2,1,0))
crime_dat$const &lt;- 1
p1 &lt;- pai(crime_dat,'obs','pred','const')
print(p1)

# Combining multiple predictions, making
# A nice table
crime_dat$rand &lt;- sample(crime_dat$obs,nrow(crime_dat),FALSE)
p2 &lt;- pai(crime_dat,'obs','rand','const')
pai_summary(list(p1,p2),c(1,3,5),c('one','two'))

</code></pre>

<hr>
<h2 id='pai_summary'>Summary Table for Multiple PAI Stats</h2><span id='topic+pai_summary'></span>

<h3>Description</h3>

<p>Takes a list of multiple PAI summary tables (for different predictions) and returns summaries at fixed area thresholds
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pai_summary(pai_list, thresh, labs, wide = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pai_summary_+3A_pai_list">pai_list</code></td>
<td>
<p>list of data frames that have the PAI stats from the <code>pai</code> function</p>
</td></tr>
<tr><td><code id="pai_summary_+3A_thresh">thresh</code></td>
<td>
<p>vector of area numbers to select, e.g. 10 would select the top 10 areas, c(10,100) would select the top 10 and the top 100 areas</p>
</td></tr>
<tr><td><code id="pai_summary_+3A_labs">labs</code></td>
<td>
<p>vector of characters that specifies the labels for each PAI dataframe, should be the same length as <code>pai_list</code></p>
</td></tr>
<tr><td><code id="pai_summary_+3A_wide">wide</code></td>
<td>
<p>boolean, if TRUE (default), returns data frame in wide format. Else returns summaries in long format</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given predictions over an entire sample, this returns a dataframe with the sorted best PAI (sorted by density of predicted counts per area).
PAI is defined as:
</p>
<p style="text-align: center;"><code class="reqn">PAI = \frac{c_t/C}{a_t/A}</code>
</p>

<p>Where the numerator is the percent of crimes in cumulative t areas, and the denominator is the percent of the area encompassed.
PEI is the observed PAI divided by the best possible PAI if you were a perfect oracle, so is scaled between 0 and 1.
RRI is <code>predicted/observed</code>, so if you have very bad predictions can return Inf or undefined!
See Wheeler &amp; Steenbeek (2019) for the definitions of the different metrics.
User note, PEI may behave funny with different sized areas.
</p>


<h3>Value</h3>

<p>A dataframe with the PAI/PEI/RRI, and cumulative crime/predicted counts, for each original table
</p>


<h3>References</h3>

<p>Drawve, G., &amp; Wooditch, A. (2019). A research note on the methodological and theoretical considerations for assessing crime forecasting accuracy with the predictive accuracy index. <em>Journal of Criminal Justice</em>, 64, 101625.
</p>
<p>Wheeler, A. P., &amp; Steenbeek, W. (2021). Mapping the risk terrain for crime using machine learning. <em>Journal of Quantitative Criminology</em>, 37(2), 445-480.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pai">pai()</a></code> for a summary table of metrics for multiple pai tables given fixed N thresholds
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Making some very simple fake data
crime_dat &lt;- data.frame(id=1:6,
                        obs=c(6,7,3,2,1,0),
                        pred=c(8,4,4,2,1,0))
crime_dat$const &lt;- 1
p1 &lt;- pai(crime_dat,'obs','pred','const')
print(p1)

# Combining multiple predictions, making
# A nice table
crime_dat$rand &lt;- sample(crime_dat$obs,nrow(crime_dat),FALSE)
p2 &lt;- pai(crime_dat,'obs','rand','const')
pai_summary(list(p1,p2),c(1,3,5),c('one','two'))

</code></pre>

<hr>
<h2 id='pois_contour'>Checks the fit of a Poisson Distribution</h2><span id='topic+pois_contour'></span>

<h3>Description</h3>

<p>Provides contours (for use in graphs) to show changes in Poisson counts in a pre vs post period.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pois_contour(
  pre_crime,
  post_crime,
  lev = c(-3, 0, 3),
  lr = 5,
  hr = max(pre_crime) * 1.05,
  steps = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pois_contour_+3A_pre_crime">pre_crime</code></td>
<td>
<p>vector of crime counts in the pre period</p>
</td></tr>
<tr><td><code id="pois_contour_+3A_post_crime">post_crime</code></td>
<td>
<p>vector of crime counts in the post period</p>
</td></tr>
<tr><td><code id="pois_contour_+3A_lev">lev</code></td>
<td>
<p>vector of Poisson Z-scores to draw the contours at, defaults to <code>c(-3,0,3)</code></p>
</td></tr>
<tr><td><code id="pois_contour_+3A_lr">lr</code></td>
<td>
<p>scaler lower limit for where to draw the contour lines, defaults to <code>5</code></p>
</td></tr>
<tr><td><code id="pois_contour_+3A_hr">hr</code></td>
<td>
<p>scaler upper limit for where to draw the contour lines, defaults to <code>max(pre_crime)*1.05</code></p>
</td></tr>
<tr><td><code id="pois_contour_+3A_steps">steps</code></td>
<td>
<p>scaler how dense to make the lines, defaults to 1000 steps</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Provides a set of contour lines to show whether increases/decreases in Poisson counts between two periods
are outside of those expected by chance according to the Poisson distribution based on the normal approximation.
Meant to be used in subsequent graphs. Note the approximation breaks down at smaller N values, so below 5 is not
typically recommended.
</p>


<h3>Value</h3>

<p>A dataframe with columns
</p>

<ul>
<li> <p><code>x</code>, the integer value
</p>
</li>
<li> <p><code>y</code>, the y-value in the graph for expected changes (will not be below 0)
</p>
</li>
<li> <p><code>levels</code>, the associated Z-score level
</p>
</li></ul>



<h3>References</h3>

<p>Drake, G., Wheeler, A., Kim, D.-Y., Phillips, S. W., &amp; Mendolera, K. (2021). <em>The Impact of COVID-19 on the Spatial Distribution of Shooting Violence in Buffalo, NY</em>. CrimRxiv. https://doi.org/10.21428/cb6ab371.e187aede
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example use with NYC Shooting Data pre/post Covid lockdowns
# Prepping the NYC shooting data
data(nyc_shoot)
begin_date &lt;- as.Date('03/01/2020', format="%m/%d/%Y")
nyc_shoot$Pre &lt;- ifelse(nyc_shoot$OCCUR_DATE &lt; begin_date,1,0)
nyc_shoot$Post &lt;- nyc_shoot$Pre*-1 + 1
# Note being lazy, some of these PCTs have changed over time
pct_tot &lt;- aggregate(cbind(Pre,Post) ~ PRECINCT, data=nyc_shoot@data, FUN=sum)
cont_lines &lt;- pois_contour(pct_tot$Pre,pct_tot$Post)
# Now making an ugly graph
sp &lt;- split(cont_lines,cont_lines$levels)
plot(pct_tot$Pre,pct_tot$Post)
for (s in sp){
  lines(s$x,s$y,lty=2)
}
# Can see it is slightly overdispersed, but pretty close!
# See https://andrewpwheeler.com/2021/02/02/the-spatial-dispersion-of-nyc-shootings-in-2020/
# For a nicer example using ggplot

</code></pre>

<hr>
<h2 id='powalt'>Power for Small Sample Exact Test</h2><span id='topic+powalt'></span>

<h3>Description</h3>

<p>A helper function to calculate power for different alternative distributions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>powalt(SST, p_alt, a = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="powalt_+3A_sst">SST</code></td>
<td>
<p>a small_samptest object created with the small_samptest function</p>
</td></tr>
<tr><td><code id="powalt_+3A_p_alt">p_alt</code></td>
<td>
<p>vector of alternative probabilities to calculate power for</p>
</td></tr>
<tr><td><code id="powalt_+3A_a">a</code></td>
<td>
<p>scaler, alpha level for power estimate, default 0.05</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This construct a null distribution for small sample statistics for N counts in M bins. Example use cases are to see if a repeat offender have a proclivity to commit crimes on a particular day of the week (see the referenced paper). It can also be used for Benford's analysis of leading/trailing digits for small samples.
</p>


<h3>Value</h3>

<p>A PowerSmallSamp object with slots for:
</p>

<ul>
<li> <p><code>permutations</code>, a dataframe that contains the exact probabilities and test statistic values for every possible permutation
</p>
</li>
<li> <p><code>power</code>, the estimated power of the scenario
</p>
</li>
<li> <p><code>alternative</code>, the alternative distribution of probabilities specified
</p>
</li>
<li> <p><code>null</code>, the null distribution (taken from the SST object)
</p>
</li>
<li> <p><code>alpha</code>, the specified alpha level
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+small_samptest">small_samptest()</a></code> for generating the SST object needed to estimate the power
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Counts for different days of the week
d &lt;- c(3,1,2,0,0,0,0) #format N observations in M bins
res &lt;- small_samptest(d=d,type="G")
# Power if someone only commits crime on 4 days of the week
alt_p &lt;- c(1/4,1/4,1/4,1/4,0,0,0)
rp &lt;- powalt(res,alt_p) #need to use previously created SST object
print(rp)

# Example for Benfords analysis
f &lt;- 1:9
p_fd &lt;- log10(1 + (1/f)) #first digit probabilities
#check data from Nigrini page 84
checks &lt;- c(1927.48,27902.31,86241.90,72117.46,81321.75,97473.96,
           93249.11,89658.17,87776.89,92105.83,79949.16,87602.93,
           96879.27,91806.47,84991.67,90831.83,93766.67,88338.72,
           94639.49,83709.28,96412.21,88432.86,71552.16)
# To make example run a bit faster
checks &lt;- checks[1:10]
# extracting the first digits
fd &lt;- substr(format(checks,trim=TRUE),1,1)
tot &lt;- table(factor(fd, levels=paste(f)))
resG &lt;- small_samptest(d=tot,p=p_fd,type="Chi")
# Lets look at alt under equal probabilities (very conservative)
alt_equal &lt;- rep(1/length(p_fd),length(p_fd))
powalt(resG,alt_equal)
</code></pre>

<hr>
<h2 id='prep_grid'>Creates vector grid cells over study area</h2><span id='topic+prep_grid'></span>

<h3>Description</h3>

<p>Creates grid cells of given size over particular study area.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prep_grid(outline, size, clip_level = 0, point_over = NULL, point_n = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prep_grid_+3A_outline">outline</code></td>
<td>
<p>SpatialPolygon or SpatialPolygonDataFrame that defines the area to draw grid cells over</p>
</td></tr>
<tr><td><code id="prep_grid_+3A_size">size</code></td>
<td>
<p>scaler for the size of the grid cells (one side), in whatever units the outline is in</p>
</td></tr>
<tr><td><code id="prep_grid_+3A_clip_level">clip_level</code></td>
<td>
<p>, you can clip grid cells if they are not entirely inside the outlined area, defaults to <code>0</code>
so any cells at least touching are included</p>
</td></tr>
<tr><td><code id="prep_grid_+3A_point_over">point_over</code></td>
<td>
<p>default <code>NULL</code>, but can pass in SpatialPoints and will only include grid cells that have at least one point</p>
</td></tr>
<tr><td><code id="prep_grid_+3A_point_n">point_n</code></td>
<td>
<p>default 0, only used if passing in <code>point_over</code>. Will return only grid cells with greater than <code>point_n</code> points</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This generates a vector grid over the study area of interest. Intentionally working with vector data for use with
other feature engineering helper functions (that can pass in X/Y).
</p>


<h3>Value</h3>

<p>A SpatialPolygonDataFrame object with columns
</p>

<ul>
<li> <p><code>id</code>, integer id value (not the same as row.names!)
</p>
</li>
<li> <p><code>x</code>, x centroid of grid cell
</p>
</li>
<li> <p><code>y</code>, y centroid of grid cell
</p>
</li>
<li> <p><code>cover</code>, proportion that grid cell is covered by <code>outline</code>
</p>
</li>
<li> <p><code>count</code>, optional (only if you pass in <code>point_over</code>)
</p>
</li></ul>



<h3>References</h3>

<p>Wheeler, A. P. (2018). The effect of 311 calls for service on crime in DC at microplaces. <em>Crime &amp; Delinquency</em>, 64(14), 1882-1903.
</p>
<p>Wheeler, A. P., &amp; Steenbeek, W. (2021). Mapping the risk terrain for crime using machine learning. <em>Journal of Quantitative Criminology</em>, 37(2), 445-480.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sp) #for sp plot methods
# large grid cells
data(nyc_bor)
res &lt;- prep_grid(nyc_bor,5000)
plot(nyc_bor)
plot(res,border='BLUE',add=TRUE)

# clipping so majority of grid is inside outline
res &lt;- prep_grid(nyc_bor,2000,clip_level=0.5)
plot(nyc_bor)
plot(res,border='BLUE',add=TRUE)

# only grid cells that have at least one shooting
data(nyc_shoot)
res &lt;- prep_grid(nyc_bor,2000,clip_level=0,nyc_shoot)
plot(nyc_bor)
plot(res,border='RED',add=TRUE)

</code></pre>

<hr>
<h2 id='prep_hexgrid'>Creates hexagon grid cells over study area</h2><span id='topic+prep_hexgrid'></span>

<h3>Description</h3>

<p>Creates hexagon grid cells of given area over particular study area.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prep_hexgrid(outline, area, clip_level = 0, point_over = NULL, point_n = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prep_hexgrid_+3A_outline">outline</code></td>
<td>
<p>SpatialPolygon or SpatialPolygonDataFrame that defines the area to draw hexgrid cells over</p>
</td></tr>
<tr><td><code id="prep_hexgrid_+3A_area">area</code></td>
<td>
<p>scaler for the area of the grid cells in whatever units the outline is in</p>
</td></tr>
<tr><td><code id="prep_hexgrid_+3A_clip_level">clip_level</code></td>
<td>
<p>, you can clip grid cells if they are not entirely inside the outlined area, defaults to <code>0</code>
so any cells at least touching are included. Specify as proportion (so should not be greater than 1!)</p>
</td></tr>
<tr><td><code id="prep_hexgrid_+3A_point_over">point_over</code></td>
<td>
<p>default <code>NULL</code>, but can pass in SpatialPoints and will only include grid cells that have at least one point</p>
</td></tr>
<tr><td><code id="prep_hexgrid_+3A_point_n">point_n</code></td>
<td>
<p>default 0, only used if passing in <code>point_over</code>. Will return only grid cells with greater than <code>point_n</code> points</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This generates a vector hex grid over the study area of interest. Hexgrids are sometimes preferred over square grid cells to
prevent aliasing like artifacts in maps (runs of particular values).
</p>


<h3>Value</h3>

<p>A SpatialPolygonDataFrame object with columns
</p>

<ul>
<li> <p><code>id</code>, integer id value (not the same as row.names!)
</p>
</li>
<li> <p><code>x</code>, x centroid of grid cell
</p>
</li>
<li> <p><code>y</code>, y centroid of grid cell
</p>
</li>
<li> <p><code>cover</code>, optional (only if clip_level &gt; 0) proportion that grid cell is covered by <code>outline</code>
</p>
</li>
<li> <p><code>count</code>, optional (only if you pass in <code>point_over</code>), total N of points over
</p>
</li></ul>



<h3>References</h3>

<p>Circo, G. M., &amp; Wheeler, A. P. (2021). Trauma Center Drive Time Distances and Fatal Outcomes among Gunshot
Wound Victims. <em>Applied Spatial Analysis and Policy</em>, 14(2), 379-393.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sp) #for sp plot methods
#Base example, some barely touch
hnyc &lt;- prep_hexgrid(nyc_bor,area=20000^2)
plot(hnyc)
plot(nyc_bor,border='red',add=TRUE)
#Example clipping hexagons that have dongle hexagons
hex_clip &lt;- prep_hexgrid(nyc_bor,area=20000^2,clip_level=0.3)
plot(hex_clip,border='blue')
plot(nyc_bor,border='red',add=TRUE)
summary(hnyc)

#Example clipping hexagons with no overlap crimes
hnyc &lt;- prep_hexgrid(nyc_bor,area=4000^2,point_over=nyc_shoot)
plot(hnyc)
plot(nyc_shoot,pch='.',add=TRUE)


</code></pre>

<hr>
<h2 id='small_samptest'>Small Sample Exact Test for Counts in Bins</h2><span id='topic+small_samptest'></span>

<h3>Description</h3>

<p>Small sample test statistic for counts of N items in bins with particular probability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>small_samptest(d, p = rep(1/length(d), length(d)), type = "G", cdf = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="small_samptest_+3A_d">d</code></td>
<td>
<p>vector of counts, e.g. c(0,2,1,3,1,4,0) for counts of crimes in days of the week</p>
</td></tr>
<tr><td><code id="small_samptest_+3A_p">p</code></td>
<td>
<p>vector of baseline probabilities, defaults to equal probabilities in each bin</p>
</td></tr>
<tr><td><code id="small_samptest_+3A_type">type</code></td>
<td>
<p>string specifying &quot;G&quot; for likelihhood ratio G stat (the default), &quot;V&quot; for Kuipers test (for circular data), &quot;KS&quot; for Komolgrov-Smirnov test, and &quot;Chi&quot; for Chi-square test</p>
</td></tr>
<tr><td><code id="small_samptest_+3A_cdf">cdf</code></td>
<td>
<p>if <code>FALSE</code> (the default) generates a new permutation vector (using <code>exactProb</code>), else pass it a final probability dataset previously created</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This construct a null distribution for small sample statistics for N counts in M bins. Example use cases are to see if a repeat offender have a proclivity to commit crimes on a particular day of the week (see the referenced paper). It can also be used for Benford's analysis of leading/trailing digits for small samples. Referenced paper shows G test tends to have the most power, although with circular data may consider Kuiper's test.
</p>


<h3>Value</h3>

<p>A small_sampletest object with slots for:
</p>

<ul>
<li> <p><code>CDF</code>, a dataframe that contains the exact probabilities and test statistic values for every possible permutation
</p>
</li>
<li> <p><code>probabilities</code>, the null probabilities you specified
</p>
</li>
<li> <p><code>data</code>, the observed counts you specified
</p>
</li>
<li> <p><code>test</code>, the type of test conducted (e.g. G, KS, Chi, etc.)
</p>
</li>
<li> <p><code>test_stat</code>, the test statistic for the observed data
</p>
</li>
<li> <p><code>p_value</code>, the p-value for the observed stat based on the exact null distribution
</p>
</li>
<li> <p><code>AggregateStatistics</code>, here is a reduced form aggregate table for the CDF/p-value calculation
</p>
</li></ul>

<p>If you wish to save the object, you may want to get rid of the CDF part, it can be quite large. It will have a total of <code>choose(n+n-1,m-1)</code> total rows, where m is the number of bins and n is the total counts. So if you have 10 crimes in 7 days of the week, it will result in a dataframe with <code>choose(7 + 10 - 1,7-1)</code>, which is 8008 rows.
Currently I keep the CDF part though to make it easier to calculate power for a particular test
</p>


<h3>References</h3>

<p>Nigrini, M. J. (2012). <em>Benford's Law: Applications for forensic accounting, auditing, and fraud detection</em>. John Wiley &amp; Sons.
</p>
<p>Wheeler, A. P. (2016). Testing Serial Crime Events for Randomness in Day-of-Week Patterns with Small Samples. <em>Journal of Investigative Psychology and Offender Profiling</em>, 13(2), 148-165.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+powalt">powalt()</a></code> for calculating power of a test under alternative
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Counts for different days of the week
d &lt;- c(3,1,1,0,0,1,1) #format N observations in M bins
res &lt;- small_samptest(d=d,type="G")
print(res)

# Example for Benfords analysis
f &lt;- 1:9
p_fd &lt;- log10(1 + (1/f)) #first digit probabilities
#check data from Nigrini page 84
checks &lt;- c(1927.48,27902.31,86241.90,72117.46,81321.75,97473.96,
           93249.11,89658.17,87776.89,92105.83,79949.16,87602.93,
           96879.27,91806.47,84991.67,90831.83,93766.67,88338.72,
           94639.49,83709.28,96412.21,88432.86,71552.16)
# To make example run a bit faster
c1 &lt;- checks[1:10]
#extracting the first digits
fd &lt;- substr(format(c1,trim=TRUE),1,1)
tot &lt;- table(factor(fd, levels=paste(f)))
resG &lt;- small_samptest(d=tot,p=p_fd,type="Chi")
resG

#Can reuse the cdf table if you have the same number of observations
c2 &lt;- checks[11:20]
fd2 &lt;- substr(format(c2,trim=TRUE),1,1)
t2 &lt;- table(factor(fd2, levels=paste(f)))
resG2 &lt;- small_samptest(d=t2,p=p_fd,type="Chi",cdf=resG$CDF)

</code></pre>

<hr>
<h2 id='vor_sp'>Voronoi tesselation from input points</h2><span id='topic+vor_sp'></span>

<h3>Description</h3>

<p>Given an outline and feature points, calculates Voronoi areas
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vor_sp(outline, feat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vor_sp_+3A_outline">outline</code></td>
<td>
<p>object that can be coerced to a spatstat window via <code>as.owin</code> (so SpatialPolygonDataFrame, SpatialPolygon, owin)</p>
</td></tr>
<tr><td><code id="vor_sp_+3A_feat">feat</code></td>
<td>
<p>A SpatialPointsDataFrame object (if duplicate X/Y coordinates will get errors)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Outline should be a single polygon area. Uses spatstats <code>dirichlet</code> and window to compute the Voronoi tesselation.
Will generate errors if feat has duplicate X/Y points. Useful to create areas for other functions,
such as <code>dcount_xy()</code> or <code>count_xy()</code>. Common spatial unit of analysis used in crime research when using points (e.g. intersections
and street midpoints).
</p>


<h3>Value</h3>

<p>A SpatialPolygonsDataFrame object, including the dataframe for all the info in the orignal <code>feat@data</code> dataframe.
</p>


<h3>References</h3>

<p>Wheeler, A. P. (2018). The effect of 311 calls for service on crime in DC at microplaces. <em>Crime &amp; Delinquency</em>, 64(14), 1882-1903.
</p>
<p>Wheeler, A. P. (2019). Quantifying the local and spatial effects of alcohol outlets on crime. <em>Crime &amp; Delinquency</em>, 65(6), 845-871.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sp) # for sample/coordinates
data(nyc_bor)
nyc_buff &lt;- buff_sp(nyc_bor,50000)
po &lt;- sp::spsample(nyc_buff,20,'hexagonal')
po$id &lt;- 1:dim(coordinates(po))[1] # turns into SpatialDataFrame
vo &lt;- vor_sp(nyc_buff,po)
plot(vo)
plot(nyc_buff,border='RED',lwd=3, add=TRUE)


</code></pre>

<hr>
<h2 id='wdd'>Estimates the WDD Test</h2><span id='topic+wdd'></span>

<h3>Description</h3>

<p>Estimates the weighted displacement difference test from <a href="https://crimesciencejournal.biomedcentral.com/articles/10.1186/s40163-018-0085-5">Wheeler &amp; Ratcliffe, <em>A simple weighted displacement difference test to evaluate place based crime interventions</em>, Crime Science</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wdd(
  control,
  treated,
  disp_control = c(0, 0),
  disp_treated = c(0, 0),
  time_weights = c(1, 1),
  area_weights = c(1, 1, 1, 1),
  alpha = 0.1,
  silent = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wdd_+3A_control">control</code></td>
<td>
<p>vector with counts in pre,post for control area</p>
</td></tr>
<tr><td><code id="wdd_+3A_treated">treated</code></td>
<td>
<p>vector with counts in pre,post for treated area</p>
</td></tr>
<tr><td><code id="wdd_+3A_disp_control">disp_control</code></td>
<td>
<p>vector with counts in pre,post for displacement control area (default <code>c(0,0)</code>)</p>
</td></tr>
<tr><td><code id="wdd_+3A_disp_treated">disp_treated</code></td>
<td>
<p>vector with counts in pre,post for displacement treated area (default <code>c(0,0)</code>)</p>
</td></tr>
<tr><td><code id="wdd_+3A_time_weights">time_weights</code></td>
<td>
<p>vector with weights for time periods for pre/post (default <code>c(1,1)</code>)</p>
</td></tr>
<tr><td><code id="wdd_+3A_area_weights">area_weights</code></td>
<td>
<p>vector with weights for different sized areas, order is c(control,treated,disp_control,disp_treated) (default <code>c(1,1,1,1)</code>)</p>
</td></tr>
<tr><td><code id="wdd_+3A_alpha">alpha</code></td>
<td>
<p>scaler alpha level for confidence interval (default <code>0.1</code>)</p>
</td></tr>
<tr><td><code id="wdd_+3A_silent">silent</code></td>
<td>
<p>boolean set to TRUE if you do not want anything printed out (default FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The wdd (weighted displacement difference) test is an extensions to differences-in-differences when observed count data pre/post in treated control areas. The test statistic (ignoring displacement areas and weights) is:
</p>
<p style="text-align: center;"><code class="reqn">WDD = \Delta T - \Delta Ct</code>
</p>

<p>where <code class="reqn">\Delta T = T_1 - T_0</code>, the post time period count minus the pre time period count for the treated areas. And ditto for the control areas, Ct. The variance is calculated as:
</p>
<p style="text-align: center;"><code class="reqn">T_1 + T_0 + Ct_1 + Ct_0</code>
</p>

<p>that is this test uses the normal approximation to the Poisson distribution to calculate the standard error for the WDD. So beware if using very tiny counts &ndash; this approximation is less likely to be applicable (or count data that is Poisson, e.g. very overdispersed).
</p>
<p>This function also incorporates weights for different examples, such as differing <a href="https://andrewpwheeler.com/2021/01/09/the-wdd-test-with-different-pre-post-time-periods/">pre/post time periods</a> (e.g. 2 years in pre and 1 year in post), or <a href="https://andrewpwheeler.com/2021/02/23/the-wdd-test-with-different-area-sizes/">different area sizes</a> (e.g. a one square mile area vs a two square mile area). The subsequent test statistic can then be interpreted as changes per unit time or changes per unit area (e.g. density) or both per time and density.
</p>


<h3>Value</h3>

<p>A length 9 vector with names:
</p>

<ul>
<li> <p><code>Est_Local</code> and <code>SE_Local</code>, the WDD and its standard error for the local estimates
</p>
</li>
<li> <p><code>Est_Displace</code> and <code>SE_Displace</code>, the WDD and its standard error for the displacement areas
</p>
</li>
<li> <p><code>Est_Total</code> and <code>SE_Total</code>, the WDD and its standard error for the combined local/displacement areas
</p>
</li>
<li> <p><code>Z</code>, the Z-score for the total estimate
</p>
</li>
<li><p> and the lower and upper confidence intervals, <code>LowCI</code> and <code>HighCI</code>, for whatever alpha level you specified for the total estimate.
</p>
</li></ul>



<h3>References</h3>

<p>Wheeler, A. P., &amp; Ratcliffe, J. H. (2018). A simple weighted displacement difference test to evaluate place based crime interventions. <em>Crime Science</em>, 7(1), 1-9.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wdd_harm">wdd_harm()</a></code> for aggregating multiple WDD tests into one metric (e.g. based on crime harm weights)
<code><a href="#topic+e_test">e_test()</a></code> for checking the difference in two Poisson means
</p>


<h3>Examples</h3>

<pre><code class='language-R'># No weights and no displacement
cont &lt;- c(20,20); treat &lt;- c(20,10)
wdd(cont,treat)

# With Displacement stats
disptreat &lt;- c(30,20); dispcont &lt;- c(30,30)
wdd(cont,treat,dispcont,disptreat)

# With different time periods for pre/post
wdd(cont,treat,time_weights=c(2,1))

# With different area sizes
wdd(cont,treat,dispcont,disptreat,area_weights=c(2,1.5,3,3.2))

# You can technically use this even without pre (so just normal based approximation)
# just put in 0's for the pre data (so does not factor into variance)
res_test &lt;- wdd(c(0,20),c(0,10))
twotail_p &lt;- pnorm(res_test['Z'])*2
print(twotail_p) #~0.068
# e-test is very similar
e_test(20,10) #~0.069
</code></pre>

<hr>
<h2 id='wdd_harm'>Combines Multiple WDD Tests</h2><span id='topic+wdd_harm'></span>

<h3>Description</h3>

<p>Combines multiple weighted displacement difference tests into one final weighted harm metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wdd_harm(est, se, weight, alpha = 0.1, silent = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wdd_harm_+3A_est">est</code></td>
<td>
<p>vector with WDD estimates (e.g. difference in crime counts for treated vs controls)</p>
</td></tr>
<tr><td><code id="wdd_harm_+3A_se">se</code></td>
<td>
<p>vector with standard errors for WDD estimates</p>
</td></tr>
<tr><td><code id="wdd_harm_+3A_weight">weight</code></td>
<td>
<p>vector with weights to aggregate results</p>
</td></tr>
<tr><td><code id="wdd_harm_+3A_alpha">alpha</code></td>
<td>
<p>scaler alpha level for confidence interval (default <code>0.1</code>)</p>
</td></tr>
<tr><td><code id="wdd_harm_+3A_silent">silent</code></td>
<td>
<p>boolean, do not print stat messages (default <code>FALSE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This test combines multiple wdd estimates with different weights. Created to <a href="https://andrewpwheeler.com/2020/11/19/amending-the-wdd-test-to-incorporate-harm-weights/">combine tests for crime harm weights</a>.
</p>


<h3>Value</h3>

<p>A length 5 vector with names:
</p>

<ul>
<li> <p><code>HarmEst</code>, the combined harm estimate
</p>
</li>
<li> <p><code>SE_HarmEst</code> its standard error
</p>
</li>
<li> <p><code>Z</code>, the Z-score
</p>
</li>
<li><p> and the lower and upper confidence intervals, <code>LowCI</code> and <code>HighCI</code>, for whatever alpha level you specified.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+wdd">wdd()</a></code> for estimating the individual wdd outcomes
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Creating wdd tests for three different crimes and combining
rob &lt;- wdd(c(20,20),c(20,10))
burg &lt;- wdd(c(30,30),c(25,20))
theft &lt;- wdd(c(80,60),c(70,20))
dat = data.frame(rbind(rob,burg,theft))
# passing those columns now to the wdd_harm function
harm_weights &lt;- c(10,5,1)
wdd_harm(dat$Est_Local,dat$SE_Local,harm_weights)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
