<!DOCTYPE html><html><head><title>Help for package BayesFluxR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BayesFluxR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.install_pkg'><p>Installs Julia packages if needed</p></a></li>
<li><a href='#.julia_project_status'><p>Obtain the status of the current Julia project</p></a></li>
<li><a href='#.set_seed'><p>Set a seed both in Julia and R</p></a></li>
<li><a href='#.using'><p>Loads Julia packages</p></a></li>
<li><a href='#bayes_by_backprop'><p>Use Bayes By Backprop to find Variational Approximation to BNN.</p></a></li>
<li><a href='#BayesFluxR_setup'><p>Set up of the Julia environment needed for BayesFlux</p></a></li>
<li><a href='#BayesFluxR-package'><p>BayesFluxR: Implementation of Bayesian Neural Networks</p></a></li>
<li><a href='#BNN'><p>Create a Bayesian Neural Network</p></a></li>
<li><a href='#BNN.totparams'><p>Obtain the total parameters of the BNN</p></a></li>
<li><a href='#Chain'><p>Chain various layers together to form a network</p></a></li>
<li><a href='#Dense'><p>Create a Dense layer with 'in_size' inputs and 'out_size' outputs using 'act' activation function</p></a></li>
<li><a href='#find_mode'><p>Find the MAP of a BNN using SGD</p></a></li>
<li><a href='#Gamma'><p>Create a Gamma Prior</p></a></li>
<li><a href='#get_random_symbol'><p>Creates a random string that is used as variable in julia</p></a></li>
<li><a href='#initialise.allsame'><p>Initialises all parameters of the network, all hyper parameters</p>
of the prior and all additional parameters
of the likelihood by drawing random values from 'dist'.</a></li>
<li><a href='#InverseGamma'><p>Create an Inverse-Gamma Prior</p></a></li>
<li><a href='#likelihood.feedforward_normal'><p>Use a Normal likelihood for a Feedforward network</p></a></li>
<li><a href='#likelihood.feedforward_tdist'><p>Use  a t-Distribution likelihood for a Feedforward network</p></a></li>
<li><a href='#likelihood.seqtoone_normal'><p>Use a Normal likelihood for a seq-to-one recurrent network</p></a></li>
<li><a href='#likelihood.seqtoone_tdist'><p>Use a T-likelihood for a seq-to-one recurrent network.</p></a></li>
<li><a href='#LSTM'><p>Create an LSTM layer with 'in_size' input size, and 'out_size' hidden state size</p></a></li>
<li><a href='#madapter.DiagCov'><p>Use the diagonal of sample covariance matrix as inverse mass matrix.</p></a></li>
<li><a href='#madapter.FixedMassMatrix'><p>Use a fixed mass matrix</p></a></li>
<li><a href='#madapter.FullCov'><p>Use the full covariance matrix as inverse mass matrix</p></a></li>
<li><a href='#madapter.RMSProp'><p>Use RMSProp to adapt the inverse mass matrix.</p></a></li>
<li><a href='#mcmc'><p>Sample from a BNN using MCMC</p></a></li>
<li><a href='#Normal'><p>Create a Normal Prior</p></a></li>
<li><a href='#opt.ADAM'><p>ADAM optimiser</p></a></li>
<li><a href='#opt.Descent'><p>Standard gradient descent</p></a></li>
<li><a href='#opt.RMSProp'><p>RMSProp optimiser</p></a></li>
<li><a href='#posterior_predictive'><p>Draw from the posterior predictive distribution</p></a></li>
<li><a href='#prior_predictive'><p>Sample from the prior predictive of a Bayesian Neural Network</p></a></li>
<li><a href='#prior.gaussian'><p>Use an isotropic Gaussian prior</p></a></li>
<li><a href='#prior.mixturescale'><p>Scale Mixture of Gaussian Prior</p></a></li>
<li><a href='#RNN'><p>Create a RNN layer with 'in_size' input, 'out_size' hidden state and 'act' activation function</p></a></li>
<li><a href='#sadapter.Const'><p>Use a constant stepsize in mcmc</p></a></li>
<li><a href='#sadapter.DualAverage'><p>Use Dual Averaging like in STAN to tune stepsize</p></a></li>
<li><a href='#sampler.AdaptiveMH'><p>Adaptive Metropolis Hastings as introduced in</p></a></li>
<li><a href='#sampler.GGMC'><p>Gradient Guided Monte Carlo</p></a></li>
<li><a href='#sampler.HMC'><p>Standard Hamiltonian Monte Carlo (Hybrid Monte Carlo).</p></a></li>
<li><a href='#sampler.SGLD'><p>Stochastic Gradient Langevin Dynamics as proposed in Welling, M., &amp; Teh, Y. W.</p>
(n.d.). Bayesian Learning via Stochastic Gradient Langevin Dynamics. 8.</a></li>
<li><a href='#sampler.SGNHTS'><p>Stochastic Gradient Nose-Hoover Thermostat as proposed in</p></a></li>
<li><a href='#summary.BNN'><p>Print a summary of a BNN</p></a></li>
<li><a href='#tensor_embed_mat'><p>Embed a matrix of timeseries into a tensor</p></a></li>
<li><a href='#to_bayesplot'><p>Convert draws array to conform with 'bayesplot'</p></a></li>
<li><a href='#Truncated'><p>Truncates a Distribution</p></a></li>
<li><a href='#vi.get_samples'><p>Draw samples form a variational family.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Implementation of Bayesian Neural Networks</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.3</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Enrico Wegner &lt;e.wegner@student.maastrichtuniversity.nl&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of 'BayesFlux.jl' for R; It extends the famous 
             'Flux.jl' machine learning library to Bayesian Neural Networks. 
             The goal is not to have the fastest production ready 
             library, but rather to allow more people to be able 
             to use and research on Bayesian Neural Networks. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>JuliaCall (&ge; 0.17.5), stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-10 13:12:02 UTC; enricowegner</td>
</tr>
<tr>
<td>Author:</td>
<td>Enrico Wegner [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-10 13:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.install_pkg'>Installs Julia packages if needed</h2><span id='topic+.install_pkg'></span>

<h3>Description</h3>

<p>Installs Julia packages if needed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.install_pkg(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".install_pkg_+3A_...">...</code></td>
<td>
<p>strings of package names</p>
</td></tr>
</table>

<hr>
<h2 id='.julia_project_status'>Obtain the status of the current Julia project</h2><span id='topic+.julia_project_status'></span>

<h3>Description</h3>

<p>Obtain the status of the current Julia project
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.julia_project_status()
</code></pre>

<hr>
<h2 id='.set_seed'>Set a seed both in Julia and R</h2><span id='topic+.set_seed'></span>

<h3>Description</h3>

<p>Set a seed both in Julia and R
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.set_seed(seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".set_seed_+3A_seed">seed</code></td>
<td>
<p>seed to be used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  .set_seed(123)

## End(Not run)
</code></pre>

<hr>
<h2 id='.using'>Loads Julia packages</h2><span id='topic+.using'></span>

<h3>Description</h3>

<p>Loads Julia packages
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.using(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".using_+3A_...">...</code></td>
<td>
<p>strings of package names</p>
</td></tr>
</table>

<hr>
<h2 id='bayes_by_backprop'>Use Bayes By Backprop to find Variational Approximation to BNN.</h2><span id='topic+bayes_by_backprop'></span>

<h3>Description</h3>

<p>This was proposed in Blundell, C., Cornebise, J., Kavukcuoglu, K., &amp; Wierstra,
D. (2015, June). Weight uncertainty in neural network. In International
conference on machine learning (pp. 1613-1622). PMLR.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayes_by_backprop(
  bnn,
  batchsize,
  epochs,
  mc_samples = 1,
  opt = opt.ADAM(),
  n_samples_convergence = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayes_by_backprop_+3A_bnn">bnn</code></td>
<td>
<p>a BNN obtained using <code><a href="#topic+BNN">BNN</a></code></p>
</td></tr>
<tr><td><code id="bayes_by_backprop_+3A_batchsize">batchsize</code></td>
<td>
<p>batch size</p>
</td></tr>
<tr><td><code id="bayes_by_backprop_+3A_epochs">epochs</code></td>
<td>
<p>number of epochs to run for</p>
</td></tr>
<tr><td><code id="bayes_by_backprop_+3A_mc_samples">mc_samples</code></td>
<td>
<p>samples to use in each iteration for the MC approximation
usually one is enough.</p>
</td></tr>
<tr><td><code id="bayes_by_backprop_+3A_opt">opt</code></td>
<td>
<p>An optimiser. These all start with 'opt.'. See for example <code><a href="#topic+opt.ADAM">opt.ADAM</a></code></p>
</td></tr>
<tr><td><code id="bayes_by_backprop_+3A_n_samples_convergence">n_samples_convergence</code></td>
<td>
<p>At the end of each iteration convergence is checked using this
many MC samples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing
</p>

<ul>
<li><p> 'juliavar' - julia variable storing VI
</p>
</li>
<li><p> 'juliacode' - julia representation of function call
</p>
</li>
<li><p> 'params' - variational family parameters for each iteration
</p>
</li>
<li><p> 'losses' - BBB loss in each iteration
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(RNN(5, 1))
  like &lt;- likelihood.seqtoone_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  data &lt;- matrix(rnorm(10*1000), ncol = 10)
  # Choosing sequences of length 10 and predicting one period ahead
  tensor &lt;- tensor_embed_mat(data, 10+1)
  x &lt;- tensor[1:10, , , drop = FALSE]
  # Last value in each sequence is the target value
  y &lt;- tensor[11,,]
  bnn &lt;- BNN(x, y, like, prior, init)
  vi &lt;- bayes_by_backprop(bnn, 100, 100)
  vi_samples &lt;- vi.get_samples(vi, n = 1000)

## End(Not run)

</code></pre>

<hr>
<h2 id='BayesFluxR_setup'>Set up of the Julia environment needed for BayesFlux</h2><span id='topic+BayesFluxR_setup'></span>

<h3>Description</h3>

<p>This will set up a new Julia environment in the current working
directory or another folder if provided. This environment will
then be set with all Julia dependencies needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BayesFluxR_setup(
  pkg_check = TRUE,
  nthreads = 4,
  seed = NULL,
  env_path = getwd(),
  installJulia = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BayesFluxR_setup_+3A_pkg_check">pkg_check</code></td>
<td>
<p>(Default=TRUE) Check whether needed Julia packages
are installed</p>
</td></tr>
<tr><td><code id="BayesFluxR_setup_+3A_nthreads">nthreads</code></td>
<td>
<p>(Default=4) How many threads to make available to Julia</p>
</td></tr>
<tr><td><code id="BayesFluxR_setup_+3A_seed">seed</code></td>
<td>
<p>Seed to be used.</p>
</td></tr>
<tr><td><code id="BayesFluxR_setup_+3A_env_path">env_path</code></td>
<td>
<p>The path to were the Julia environment should be created.
By default, this is the current working directory.</p>
</td></tr>
<tr><td><code id="BayesFluxR_setup_+3A_installjulia">installJulia</code></td>
<td>
<p>(Default=TRUE) Whether to install Julia</p>
</td></tr>
<tr><td><code id="BayesFluxR_setup_+3A_...">...</code></td>
<td>
<p>Other parameters passed on to <code><a href="JuliaCall.html#topic+julia_setup">julia_setup</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Time consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)

## End(Not run)
</code></pre>

<hr>
<h2 id='BayesFluxR-package'>BayesFluxR: Implementation of Bayesian Neural Networks</h2><span id='topic+BayesFluxR'></span><span id='topic+BayesFluxR-package'></span>

<h3>Description</h3>

<p>Implementation of 'BayesFlux.jl' for R; It extends the famous 'Flux.jl' machine learning library to Bayesian Neural Networks. The goal is not to have the fastest production ready library, but rather to allow more people to be able to use and research on Bayesian Neural Networks.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Enrico Wegner <a href="mailto:e.wegner@student.maastrichtuniversity.nl">e.wegner@student.maastrichtuniversity.nl</a>
</p>

<hr>
<h2 id='BNN'>Create a Bayesian Neural Network</h2><span id='topic+BNN'></span>

<h3>Description</h3>

<p>Create a Bayesian Neural Network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BNN(x, y, like, prior, init)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BNN_+3A_x">x</code></td>
<td>
<p>For a Feedforward structure, this must be a matrix of dimensions
variables x observations; For a recurrent structure, this must be a
tensor of dimensions sequence_length x number_variables x number_sequences;
In general, the last dimension is always the dimension over which will be batched.</p>
</td></tr>
<tr><td><code id="BNN_+3A_y">y</code></td>
<td>
<p>A vector or matrix with observations.</p>
</td></tr>
<tr><td><code id="BNN_+3A_like">like</code></td>
<td>
<p>Likelihood; See for example <code><a href="#topic+likelihood.feedforward_normal">likelihood.feedforward_normal</a></code></p>
</td></tr>
<tr><td><code id="BNN_+3A_prior">prior</code></td>
<td>
<p>Prior; See for example <code><a href="#topic+prior.gaussian">prior.gaussian</a></code></p>
</td></tr>
<tr><td><code id="BNN_+3A_init">init</code></td>
<td>
<p>Initialiser; See for example <code><a href="#topic+initialise.allsame">initialise.allsame</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following content
</p>

<ul>
<li><p> 'juliavar' - the julia variable containing the BNN
</p>
</li>
<li><p> 'juliacode' - the string representation of the BNN
</p>
</li>
<li><p> 'x' - x
</p>
</li>
<li><p> 'juliax' - julia variable holding x
</p>
</li>
<li><p> 'y' - y
</p>
</li>
<li><p> 'juliay' - julia variable holding y
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sampler &lt;- sampler.SGLD()
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)


</code></pre>

<hr>
<h2 id='BNN.totparams'>Obtain the total parameters of the BNN</h2><span id='topic+BNN.totparams'></span>

<h3>Description</h3>

<p>Obtain the total parameters of the BNN
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BNN.totparams(bnn)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BNN.totparams_+3A_bnn">bnn</code></td>
<td>
<p>A BNN formed using <code><a href="#topic+BNN">BNN</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The total number of parameters in the BNN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  BNN.totparams(bnn)

## End(Not run)

</code></pre>

<hr>
<h2 id='Chain'>Chain various layers together to form a network</h2><span id='topic+Chain'></span>

<h3>Description</h3>

<p>Chain various layers together to form a network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Chain(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Chain_+3A_...">...</code></td>
<td>
<p>Comma separated layers</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following content
</p>

<ul>
<li><p> juliavar - the julia variable containing the network
</p>
</li>
<li><p> specification - the string representation of the network
</p>
</li>
<li><p> nc - the julia variable for the network constructor
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  Chain(LSTM(5, 5))
  Chain(RNN(5, 5, "tanh"))
  Chain(Dense(1, 5))

## End(Not run)


</code></pre>

<hr>
<h2 id='Dense'>Create a Dense layer with 'in_size' inputs and 'out_size' outputs using 'act' activation function</h2><span id='topic+Dense'></span>

<h3>Description</h3>

<p>Create a Dense layer with 'in_size' inputs and 'out_size' outputs using 'act' activation function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Dense(in_size, out_size, act = c("identity", "sigmoid", "tanh", "relu"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Dense_+3A_in_size">in_size</code></td>
<td>
<p>Input size</p>
</td></tr>
<tr><td><code id="Dense_+3A_out_size">out_size</code></td>
<td>
<p>Output size</p>
</td></tr>
<tr><td><code id="Dense_+3A_act">act</code></td>
<td>
<p>Activation function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following content
</p>

<ul>
<li><p> in_size - Input Size
</p>
</li>
<li><p> out_size - Output Size
</p>
</li>
<li><p> activation - Activation Function
</p>
</li>
<li><p> julia - Julia code representing the Layer
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 5, "relu"))

## End(Not run)

</code></pre>

<hr>
<h2 id='find_mode'>Find the MAP of a BNN using SGD</h2><span id='topic+find_mode'></span>

<h3>Description</h3>

<p>Find the MAP of a BNN using SGD
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_mode(bnn, optimiser, batchsize, epochs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_mode_+3A_bnn">bnn</code></td>
<td>
<p>a BNN obtained using <code><a href="#topic+BNN">BNN</a></code></p>
</td></tr>
<tr><td><code id="find_mode_+3A_optimiser">optimiser</code></td>
<td>
<p>an optimiser. These start with 'opt.'.
See for example <code><a href="#topic+opt.ADAM">opt.ADAM</a></code></p>
</td></tr>
<tr><td><code id="find_mode_+3A_batchsize">batchsize</code></td>
<td>
<p>batch size</p>
</td></tr>
<tr><td><code id="find_mode_+3A_epochs">epochs</code></td>
<td>
<p>number of epochs to run for</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a vector. Use <code><a href="#topic+posterior_predictive">posterior_predictive</a></code>
to obtain a prediction using this MAP estimate.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  find_mode(bnn, opt.RMSProp(), 10, 100)

## End(Not run)

</code></pre>

<hr>
<h2 id='Gamma'>Create a Gamma Prior</h2><span id='topic+Gamma'></span>

<h3>Description</h3>

<p>Creates a Gamma prior in Julia using Distributions.jl
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Gamma(shape = 2, scale = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Gamma_+3A_shape">shape</code></td>
<td>
<p>shape parameter</p>
</td></tr>
<tr><td><code id="Gamma_+3A_scale">scale</code></td>
<td>
<p>scale parameter</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following content
</p>

<ul>
<li><p> juliavar - julia variable containing the distribution
</p>
</li>
<li><p> juliacode - julia code used to create the distribution
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))

## End(Not run)

</code></pre>

<hr>
<h2 id='get_random_symbol'>Creates a random string that is used as variable in julia</h2><span id='topic+get_random_symbol'></span>

<h3>Description</h3>

<p>Creates a random string that is used as variable in julia
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_random_symbol()
</code></pre>

<hr>
<h2 id='initialise.allsame'>Initialises all parameters of the network, all hyper parameters
of the prior and all additional parameters
of the likelihood by drawing random values from 'dist'.</h2><span id='topic+initialise.allsame'></span>

<h3>Description</h3>

<p>Initialises all parameters of the network, all hyper parameters
of the prior and all additional parameters
of the likelihood by drawing random values from 'dist'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initialise.allsame(dist, like, prior)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="initialise.allsame_+3A_dist">dist</code></td>
<td>
<p>A distribution; See for example <code><a href="#topic+Normal">Normal</a></code></p>
</td></tr>
<tr><td><code id="initialise.allsame_+3A_like">like</code></td>
<td>
<p>A likelihood; See for example <code><a href="#topic+likelihood.feedforward_normal">likelihood.feedforward_normal</a></code></p>
</td></tr>
<tr><td><code id="initialise.allsame_+3A_prior">prior</code></td>
<td>
<p>A prior; See for example <code><a href="#topic+prior.gaussian">prior.gaussian</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following
</p>

<ul>
<li><p> 'juliavar' - julia variable storing the initialiser
</p>
</li>
<li><p> 'juliacode' - julia code used to create the initialiser
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  BNN.totparams(bnn)

## End(Not run)

</code></pre>

<hr>
<h2 id='InverseGamma'>Create an Inverse-Gamma Prior</h2><span id='topic+InverseGamma'></span>

<h3>Description</h3>

<p>Creates and Inverse Gamma prior in Julia using Distributions.jl
</p>


<h3>Usage</h3>

<pre><code class='language-R'>InverseGamma(shape = 2, scale = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="InverseGamma_+3A_shape">shape</code></td>
<td>
<p>shape parameter</p>
</td></tr>
<tr><td><code id="InverseGamma_+3A_scale">scale</code></td>
<td>
<p>scale parameter</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following content
</p>

<ul>
<li><p> juliavar - julia variable containing the distribution
</p>
</li>
<li><p> juliacode - julia code used to create the distribution
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Gamma">Gamma</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, InverseGamma(2.0, 0.5))

## End(Not run)
</code></pre>

<hr>
<h2 id='likelihood.feedforward_normal'>Use a Normal likelihood for a Feedforward network</h2><span id='topic+likelihood.feedforward_normal'></span>

<h3>Description</h3>

<p>This creates a likelihood of the form
</p>
<p style="text-align: center;"><code class="reqn">y_i \sim Normal(net(x_i), \sigma)\;\forall i=1,...,N</code>
</p>

<p>where the <code class="reqn">x_i</code> is fed through the network in a standard feedforward way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>likelihood.feedforward_normal(chain, sig_prior)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="likelihood.feedforward_normal_+3A_chain">chain</code></td>
<td>
<p>Network structure obtained using <code>link{Chain}</code></p>
</td></tr>
<tr><td><code id="likelihood.feedforward_normal_+3A_sig_prior">sig_prior</code></td>
<td>
<p>A prior distribution for sigma defined using
<code><a href="#topic+Gamma">Gamma</a></code>, <code>link{InverGamma}</code>,
<code><a href="#topic+Truncated">Truncated</a></code>, <code><a href="#topic+Normal">Normal</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following
</p>

<ul>
<li><p> juliavar - julia variable containing the likelihood
</p>
</li>
<li><p> juliacode - julia code used to create the likelihood
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  BNN.totparams(bnn)

## End(Not run)

</code></pre>

<hr>
<h2 id='likelihood.feedforward_tdist'>Use  a t-Distribution likelihood for a Feedforward network</h2><span id='topic+likelihood.feedforward_tdist'></span>

<h3>Description</h3>

<p>This creates a likelihood of the form
</p>
<p style="text-align: center;"><code class="reqn">\frac{y_i - net(x_i)}{\sigma} \sim T_\nu\;\forall i=1,...,N</code>
</p>

<p>where the <code class="reqn">x_i</code> is fed through the network in the standard feedforward way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>likelihood.feedforward_tdist(chain, sig_prior, nu = 30)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="likelihood.feedforward_tdist_+3A_chain">chain</code></td>
<td>
<p>Network structure obtained using <code>link{Chain}</code></p>
</td></tr>
<tr><td><code id="likelihood.feedforward_tdist_+3A_sig_prior">sig_prior</code></td>
<td>
<p>A prior distribution for sigma defined using
<code><a href="#topic+Gamma">Gamma</a></code>, <code>link{InverGamma}</code>,
<code><a href="#topic+Truncated">Truncated</a></code>, <code><a href="#topic+Normal">Normal</a></code></p>
</td></tr>
<tr><td><code id="likelihood.feedforward_tdist_+3A_nu">nu</code></td>
<td>
<p>DF of TDist</p>
</td></tr>
</table>


<h3>Value</h3>

<p>see <code><a href="#topic+likelihood.feedforward_normal">likelihood.feedforward_normal</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_tdist(net, Gamma(2.0, 0.5), nu=8)
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  BNN.totparams(bnn)

## End(Not run)

</code></pre>

<hr>
<h2 id='likelihood.seqtoone_normal'>Use a Normal likelihood for a seq-to-one recurrent network</h2><span id='topic+likelihood.seqtoone_normal'></span>

<h3>Description</h3>

<p>This creates a likelihood of the form
</p>
<p style="text-align: center;"><code class="reqn">y_i \sim Normal(net(x_i), \sigma), i=1,...,N</code>
</p>

<p>Here <code class="reqn">x_i</code> is a subsequence which will be fed through the recurrent
network to obtain the final output <code class="reqn">net(x_i) = \hat{y}_i</code>. Thus, if
one has a single time series, and splits the single time series into subsequences
of length K which are then used to predict the next output of the time series, then
each <code class="reqn">x_i</code> consists of K consecutive observations of the time series. In a sense
one constraints the maximum memory length of the network this way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>likelihood.seqtoone_normal(chain, sig_prior)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="likelihood.seqtoone_normal_+3A_chain">chain</code></td>
<td>
<p>Network structure obtained using <code>link{Chain}</code></p>
</td></tr>
<tr><td><code id="likelihood.seqtoone_normal_+3A_sig_prior">sig_prior</code></td>
<td>
<p>A prior distribution for sigma defined using
<code><a href="#topic+Gamma">Gamma</a></code>, <code>link{InverGamma}</code>,
<code><a href="#topic+Truncated">Truncated</a></code>, <code><a href="#topic+Normal">Normal</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>see <code><a href="#topic+likelihood.feedforward_normal">likelihood.feedforward_normal</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(RNN(5, 1))
  like &lt;- likelihood.seqtoone_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- array(rnorm(5*100*10), dim=c(10,5,100))
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  BNN.totparams(bnn)

## End(Not run)

</code></pre>

<hr>
<h2 id='likelihood.seqtoone_tdist'>Use a T-likelihood for a seq-to-one recurrent network.</h2><span id='topic+likelihood.seqtoone_tdist'></span>

<h3>Description</h3>

<p>See <code><a href="#topic+likelihood.seqtoone_normal">likelihood.seqtoone_normal</a></code> and <code><a href="#topic+likelihood.feedforward_tdist">likelihood.feedforward_tdist</a></code>
for details,
</p>


<h3>Usage</h3>

<pre><code class='language-R'>likelihood.seqtoone_tdist(chain, sig_prior, nu = 30)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="likelihood.seqtoone_tdist_+3A_chain">chain</code></td>
<td>
<p>Network structure obtained using <code>link{Chain}</code></p>
</td></tr>
<tr><td><code id="likelihood.seqtoone_tdist_+3A_sig_prior">sig_prior</code></td>
<td>
<p>A prior distribution for sigma defined using
<code><a href="#topic+Gamma">Gamma</a></code>, <code>link{InverGamma}</code>,
<code><a href="#topic+Truncated">Truncated</a></code>, <code><a href="#topic+Normal">Normal</a></code></p>
</td></tr>
<tr><td><code id="likelihood.seqtoone_tdist_+3A_nu">nu</code></td>
<td>
<p>DF of TDist</p>
</td></tr>
</table>


<h3>Value</h3>

<p>see <code><a href="#topic+likelihood.feedforward_normal">likelihood.feedforward_normal</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(RNN(5, 1))
  like &lt;- likelihood.seqtoone_tdist(net, Gamma(2.0, 0.5), nu=5)
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- array(rnorm(5*100*10), dim=c(10,5,100))
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  BNN.totparams(bnn)

## End(Not run)

</code></pre>

<hr>
<h2 id='LSTM'>Create an LSTM layer with 'in_size' input size, and 'out_size' hidden state size</h2><span id='topic+LSTM'></span>

<h3>Description</h3>

<p>Create an LSTM layer with 'in_size' input size, and 'out_size' hidden state size
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LSTM(in_size, out_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LSTM_+3A_in_size">in_size</code></td>
<td>
<p>Input size</p>
</td></tr>
<tr><td><code id="LSTM_+3A_out_size">out_size</code></td>
<td>
<p>Output size</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following content
</p>

<ul>
<li><p> in_size - Input Size
</p>
</li>
<li><p> out_size - Output Size
</p>
</li>
<li><p> julia - Julia code representing the Layer
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Dense">Dense</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(LSTM(5, 5))

## End(Not run)

</code></pre>

<hr>
<h2 id='madapter.DiagCov'>Use the diagonal of sample covariance matrix as inverse mass matrix.</h2><span id='topic+madapter.DiagCov'></span>

<h3>Description</h3>

<p>Use the diagonal of sample covariance matrix as inverse mass matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>madapter.DiagCov(adapt_steps, windowlength, kappa = 0.5, epsilon = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="madapter.DiagCov_+3A_adapt_steps">adapt_steps</code></td>
<td>
<p>Number of adaptation steps</p>
</td></tr>
<tr><td><code id="madapter.DiagCov_+3A_windowlength">windowlength</code></td>
<td>
<p>Lookback window length for calculation of covariance</p>
</td></tr>
<tr><td><code id="madapter.DiagCov_+3A_kappa">kappa</code></td>
<td>
<p>How much to shrink towards the identity</p>
</td></tr>
<tr><td><code id="madapter.DiagCov_+3A_epsilon">epsilon</code></td>
<td>
<p>Small value to add to diagonal so as to avoid numerical
non-pos-def problem</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list containing 'juliavar' and 'juliacode' and all given arguments.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  madapter &lt;- madapter.DiagCov(100, 10)
  sampler &lt;- sampler.GGMC(madapter = madapter)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='madapter.FixedMassMatrix'>Use a fixed mass matrix</h2><span id='topic+madapter.FixedMassMatrix'></span>

<h3>Description</h3>

<p>Use a fixed mass matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>madapter.FixedMassMatrix(mat = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="madapter.FixedMassMatrix_+3A_mat">mat</code></td>
<td>
<p>(Default=NULL); inverse mass matrix; If 'NULL', then
identity matrix will be used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with 'juliavar' and 'juliacode' and given matrix or 'NULL'
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  madapter &lt;- madapter.FixedMassMatrix()
  sampler &lt;- sampler.GGMC(madapter = madapter)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)


  # Providing a non-sense weight matrix
  weight_matrix &lt;- matrix(runif(BNN.totparams(bnn)^2, 0, 1),
                          nrow = BNN.totparams(bnn))
  madapter2 &lt;- madapter.FixedMassMatrix(weight_matrix)
  sampler2 &lt;- sampler.GGMC(madapter = madapter2)
  ch2 &lt;- mcmc(bnn, 10, 1000, sampler2)

## End(Not run)

</code></pre>

<hr>
<h2 id='madapter.FullCov'>Use the full covariance matrix as inverse mass matrix</h2><span id='topic+madapter.FullCov'></span>

<h3>Description</h3>

<p>Use the full covariance matrix as inverse mass matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>madapter.FullCov(adapt_steps, windowlength, kappa = 0.5, epsilon = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="madapter.FullCov_+3A_adapt_steps">adapt_steps</code></td>
<td>
<p>Number of adaptation steps</p>
</td></tr>
<tr><td><code id="madapter.FullCov_+3A_windowlength">windowlength</code></td>
<td>
<p>Lookback window length for calculation of covariance</p>
</td></tr>
<tr><td><code id="madapter.FullCov_+3A_kappa">kappa</code></td>
<td>
<p>How much to shrink towards the identity</p>
</td></tr>
<tr><td><code id="madapter.FullCov_+3A_epsilon">epsilon</code></td>
<td>
<p>Small value to add to diagonal so as to avoid numerical
non-pos-def problem</p>
</td></tr>
</table>


<h3>Value</h3>

<p>see <code><a href="#topic+madapter.DiagCov">madapter.DiagCov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  madapter &lt;- madapter.FullCov(100, 10)
  sampler &lt;- sampler.GGMC(madapter = madapter)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)
</code></pre>

<hr>
<h2 id='madapter.RMSProp'>Use RMSProp to adapt the inverse mass matrix.</h2><span id='topic+madapter.RMSProp'></span>

<h3>Description</h3>

<p>Use RMSProp as a preconditions/mass matrix adapter. This was proposed in Li, C.,
Chen, C., Carlson, D., &amp; Carin, L. (2016, February). Preconditioned stochastic
gradient Langevin dynamics for deep neural networks. In Thirtieth AAAI
Conference on Artificial Intelligence for the use in SGLD and related methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>madapter.RMSProp(adapt_steps, lambda = 1e-05, alpha = 0.99)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="madapter.RMSProp_+3A_adapt_steps">adapt_steps</code></td>
<td>
<p>number of adaptation steps</p>
</td></tr>
<tr><td><code id="madapter.RMSProp_+3A_lambda">lambda</code></td>
<td>
<p>see above paper</p>
</td></tr>
<tr><td><code id="madapter.RMSProp_+3A_alpha">alpha</code></td>
<td>
<p>see above paper</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with 'juliavar' and 'juliacode' and all given arguments
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  madapter &lt;- madapter.RMSProp(100)
  sampler &lt;- sampler.GGMC(madapter = madapter)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='mcmc'>Sample from a BNN using MCMC</h2><span id='topic+mcmc'></span>

<h3>Description</h3>

<p>Sample from a BNN using MCMC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc(
  bnn,
  batchsize,
  numsamples,
  sampler = sampler.SGLD(stepsize_a = 1),
  continue_sampling = FALSE,
  start_value = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_+3A_bnn">bnn</code></td>
<td>
<p>A BNN obtained using <code><a href="#topic+BNN">BNN</a></code></p>
</td></tr>
<tr><td><code id="mcmc_+3A_batchsize">batchsize</code></td>
<td>
<p>batchsize to use; Most samplers allow for batching.
For some, theoretical justifications are missing (HMC)</p>
</td></tr>
<tr><td><code id="mcmc_+3A_numsamples">numsamples</code></td>
<td>
<p>Number of mcmc samples</p>
</td></tr>
<tr><td><code id="mcmc_+3A_sampler">sampler</code></td>
<td>
<p>Sampler to use; See for example <code><a href="#topic+sampler.SGLD">sampler.SGLD</a></code> and
all other samplers start with 'sampler.' and are thus easy to identity.</p>
</td></tr>
<tr><td><code id="mcmc_+3A_continue_sampling">continue_sampling</code></td>
<td>
<p>Do not start new sampling, but rather continue sampling
For this, numsamples must be greater than the already sampled number.</p>
</td></tr>
<tr><td><code id="mcmc_+3A_start_value">start_value</code></td>
<td>
<p>Values to start from. By default these will be
sampled using the initialiser in 'bnn'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the 'samples' and the 'sampler' used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sampler &lt;- sampler.SGNHTS(1e-3)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='Normal'>Create a Normal Prior</h2><span id='topic+Normal'></span>

<h3>Description</h3>

<p>Creates a Normal prior in Julia using Distributions.jl. This can
then be truncated using <code><a href="#topic+Truncated">Truncated</a></code> to obtain a prior
that could then be used as a variance prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Normal(mu = 0, sigma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Normal_+3A_mu">mu</code></td>
<td>
<p>Mean</p>
</td></tr>
<tr><td><code id="Normal_+3A_sigma">sigma</code></td>
<td>
<p>Standard Deviation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>see <code><a href="#topic+Gamma">Gamma</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Truncated(Normal(0, 0.5), 0, Inf))

## End(Not run)

</code></pre>

<hr>
<h2 id='opt.ADAM'>ADAM optimiser</h2><span id='topic+opt.ADAM'></span>

<h3>Description</h3>

<p>ADAM optimiser
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opt.ADAM(eta = 0.001, beta = c(0.9, 0.999), eps = 1e-08)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="opt.ADAM_+3A_eta">eta</code></td>
<td>
<p>stepsize</p>
</td></tr>
<tr><td><code id="opt.ADAM_+3A_beta">beta</code></td>
<td>
<p>momentum decays; must be a list of length 2</p>
</td></tr>
<tr><td><code id="opt.ADAM_+3A_eps">eps</code></td>
<td>
<p>Flux does not document this</p>
</td></tr>
</table>


<h3>Value</h3>

<p>see <code><a href="#topic+opt.Descent">opt.Descent</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  find_mode(bnn, opt.ADAM(), 10, 100)

## End(Not run)

</code></pre>

<hr>
<h2 id='opt.Descent'>Standard gradient descent</h2><span id='topic+opt.Descent'></span>

<h3>Description</h3>

<p>Standard gradient descent
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opt.Descent(eta = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="opt.Descent_+3A_eta">eta</code></td>
<td>
<p>stepsize</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list containing
</p>

<ul>
<li><p> 'julivar' - julia variable holding the optimiser
</p>
</li>
<li><p> 'juliacode' - string representation
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  find_mode(bnn, opt.Descent(1e-5), 10, 100)

## End(Not run)

</code></pre>

<hr>
<h2 id='opt.RMSProp'>RMSProp optimiser</h2><span id='topic+opt.RMSProp'></span>

<h3>Description</h3>

<p>RMSProp optimiser
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opt.RMSProp(eta = 0.001, rho = 0.9, eps = 1e-08)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="opt.RMSProp_+3A_eta">eta</code></td>
<td>
<p>learning rate</p>
</td></tr>
<tr><td><code id="opt.RMSProp_+3A_rho">rho</code></td>
<td>
<p>momentum</p>
</td></tr>
<tr><td><code id="opt.RMSProp_+3A_eps">eps</code></td>
<td>
<p>not documented by Flux</p>
</td></tr>
</table>


<h3>Value</h3>

<p>see <code><a href="#topic+opt.Descent">opt.Descent</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  find_mode(bnn, opt.RMSProp(), 10, 100)

## End(Not run)

</code></pre>

<hr>
<h2 id='posterior_predictive'>Draw from the posterior predictive distribution</h2><span id='topic+posterior_predictive'></span>

<h3>Description</h3>

<p>Draw from the posterior predictive distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posterior_predictive(bnn, posterior_samples, x = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="posterior_predictive_+3A_bnn">bnn</code></td>
<td>
<p>a BNN obtained using <code>link{BNN}</code></p>
</td></tr>
<tr><td><code id="posterior_predictive_+3A_posterior_samples">posterior_samples</code></td>
<td>
<p>a vector or matrix containing posterior
samples. This can be obtained using <code><a href="#topic+mcmc">mcmc</a></code>, or <code><a href="#topic+bayes_by_backprop">bayes_by_backprop</a></code>
or <code><a href="#topic+find_mode">find_mode</a></code>.</p>
</td></tr>
<tr><td><code id="posterior_predictive_+3A_x">x</code></td>
<td>
<p>input variables. If 'NULL' (default), training values will be used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix whose columns are the posterior predictive draws.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sampler &lt;- sampler.SGLD()
  ch &lt;- mcmc(bnn, 10, 1000, sampler)
  pp &lt;- posterior_predictive(bnn, ch$samples)

## End(Not run)

</code></pre>

<hr>
<h2 id='prior_predictive'>Sample from the prior predictive of a Bayesian Neural Network</h2><span id='topic+prior_predictive'></span>

<h3>Description</h3>

<p>Sample from the prior predictive of a Bayesian Neural Network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prior_predictive(bnn, n = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prior_predictive_+3A_bnn">bnn</code></td>
<td>
<p>BNN obtained using <code><a href="#topic+BNN">BNN</a></code></p>
</td></tr>
<tr><td><code id="prior_predictive_+3A_n">n</code></td>
<td>
<p>Number of samples</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix of prior predictive samples; Columns are the different samples
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  pp &lt;- prior_predictive(bnn, n = 10)

## End(Not run)

</code></pre>

<hr>
<h2 id='prior.gaussian'>Use an isotropic Gaussian prior</h2><span id='topic+prior.gaussian'></span>

<h3>Description</h3>

<p>Use a Multivariate Gaussian prior for all network parameters.
Covariance matrix is set to be equal 'sigma * I' with 'I' being
the identity matrix. Mean is zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prior.gaussian(chain, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prior.gaussian_+3A_chain">chain</code></td>
<td>
<p>Chain obtained using <code><a href="#topic+Chain">Chain</a></code></p>
</td></tr>
<tr><td><code id="prior.gaussian_+3A_sigma">sigma</code></td>
<td>
<p>Standard deviation of Gaussian prior</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the following
</p>

<ul>
<li><p> 'juliavar' the julia variable used to store the prior
</p>
</li>
<li><p> 'juliacode' the julia code
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sampler &lt;- sampler.SGLD()
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='prior.mixturescale'>Scale Mixture of Gaussian Prior</h2><span id='topic+prior.mixturescale'></span>

<h3>Description</h3>

<p>Uses a scale mixture of Gaussian for each network parameter. That is,
the prior is given by
</p>
<p style="text-align: center;"><code class="reqn">\pi_1 Normal(0, sigma1) + (1-\pi_1) Normal(0, sigma2)</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>prior.mixturescale(chain, sigma1, sigma2, pi1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prior.mixturescale_+3A_chain">chain</code></td>
<td>
<p>Chain obtained using <code><a href="#topic+Chain">Chain</a></code></p>
</td></tr>
<tr><td><code id="prior.mixturescale_+3A_sigma1">sigma1</code></td>
<td>
<p>Standard deviation of first Gaussian</p>
</td></tr>
<tr><td><code id="prior.mixturescale_+3A_sigma2">sigma2</code></td>
<td>
<p>Standard deviation of second Gaussian</p>
</td></tr>
<tr><td><code id="prior.mixturescale_+3A_pi1">pi1</code></td>
<td>
<p>Weight of first Gaussian</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the following
</p>

<ul>
<li><p> 'juliavar' the julia variable used to store the prior
</p>
</li>
<li><p> 'juliacode' the julia code
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.mixturescale(net, 10, 0.1, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sampler &lt;- sampler.SGLD()
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='RNN'>Create a RNN layer with 'in_size' input, 'out_size' hidden state and 'act' activation function</h2><span id='topic+RNN'></span>

<h3>Description</h3>

<p>Create a RNN layer with 'in_size' input, 'out_size' hidden state and 'act' activation function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RNN(in_size, out_size, act = c("sigmoid", "tanh", "identity", "relu"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RNN_+3A_in_size">in_size</code></td>
<td>
<p>Input size</p>
</td></tr>
<tr><td><code id="RNN_+3A_out_size">out_size</code></td>
<td>
<p>Output size</p>
</td></tr>
<tr><td><code id="RNN_+3A_act">act</code></td>
<td>
<p>Activation function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following content
</p>

<ul>
<li><p> in_size - Input Size
</p>
</li>
<li><p> out_size - Output Size
</p>
</li>
<li><p> activation - Activation Function
</p>
</li>
<li><p> julia - Julia code representing the Layer
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Dense">Dense</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(RNN(5, 5, "tanh"))

## End(Not run)

</code></pre>

<hr>
<h2 id='sadapter.Const'>Use a constant stepsize in mcmc</h2><span id='topic+sadapter.Const'></span>

<h3>Description</h3>

<p>Use a constant stepsize in mcmc
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sadapter.Const(l)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sadapter.Const_+3A_l">l</code></td>
<td>
<p>stepsize</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with 'juliavar', 'juliacode' and the given arguments
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sadapter &lt;- sadapter.Const(1e-5)
  sampler &lt;- sampler.GGMC(sadapter = sadapter)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='sadapter.DualAverage'>Use Dual Averaging like in STAN to tune stepsize</h2><span id='topic+sadapter.DualAverage'></span>

<h3>Description</h3>

<p>Use Dual Averaging like in STAN to tune stepsize
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sadapter.DualAverage(
  adapt_steps,
  initial_stepsize = 1,
  target_accept = 0.65,
  gamma = 0.05,
  t0 = 10,
  kappa = 0.75
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sadapter.DualAverage_+3A_adapt_steps">adapt_steps</code></td>
<td>
<p>number of adaptation steps</p>
</td></tr>
<tr><td><code id="sadapter.DualAverage_+3A_initial_stepsize">initial_stepsize</code></td>
<td>
<p>initial stepsize</p>
</td></tr>
<tr><td><code id="sadapter.DualAverage_+3A_target_accept">target_accept</code></td>
<td>
<p>target acceptance ratio</p>
</td></tr>
<tr><td><code id="sadapter.DualAverage_+3A_gamma">gamma</code></td>
<td>
<p>See STAN manual NUTS paper</p>
</td></tr>
<tr><td><code id="sadapter.DualAverage_+3A_t0">t0</code></td>
<td>
<p>See STAN manual or NUTS paper</p>
</td></tr>
<tr><td><code id="sadapter.DualAverage_+3A_kappa">kappa</code></td>
<td>
<p>See STAN manual or NUTS paper</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with 'juliavar', 'juliacode', and all given arguments
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sadapter &lt;- sadapter.DualAverage(100)
  sampler &lt;- sampler.GGMC(sadapter = sadapter)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='sampler.AdaptiveMH'>Adaptive Metropolis Hastings as introduced in</h2><span id='topic+sampler.AdaptiveMH'></span>

<h3>Description</h3>

<p>Haario, H., Saksman, E., &amp; Tamminen, J. (2001). An adaptive Metropolis
algorithm. Bernoulli, 223-242.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampler.AdaptiveMH(bnn, t0, sd, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampler.AdaptiveMH_+3A_bnn">bnn</code></td>
<td>
<p>BNN obtained using <code><a href="#topic+BNN">BNN</a></code></p>
</td></tr>
<tr><td><code id="sampler.AdaptiveMH_+3A_t0">t0</code></td>
<td>
<p>Number of iterators before covariance adaptation will be started.
Also the lookback period for covariance adaptation.</p>
</td></tr>
<tr><td><code id="sampler.AdaptiveMH_+3A_sd">sd</code></td>
<td>
<p>Tuning parameter; See paper</p>
</td></tr>
<tr><td><code id="sampler.AdaptiveMH_+3A_eps">eps</code></td>
<td>
<p>Used for numerical reasons. Increase this if pos-def-error thrown.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with 'juliavar', 'juliacode', and all given arguments
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sampler &lt;- sampler.AdaptiveMH(bnn, 10, 1)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='sampler.GGMC'>Gradient Guided Monte Carlo</h2><span id='topic+sampler.GGMC'></span>

<h3>Description</h3>

<p>Proposed in Garriga-Alonso, A., &amp; Fortuin, V. (2021). Exact langevin dynamics
with stochastic gradients. arXiv preprint arXiv:2102.01691.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampler.GGMC(
  beta = 0.1,
  l = 1,
  sadapter = sadapter.DualAverage(1000),
  madapter = madapter.FixedMassMatrix(),
  steps = 3
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampler.GGMC_+3A_beta">beta</code></td>
<td>
<p>See paper</p>
</td></tr>
<tr><td><code id="sampler.GGMC_+3A_l">l</code></td>
<td>
<p>stepsize</p>
</td></tr>
<tr><td><code id="sampler.GGMC_+3A_sadapter">sadapter</code></td>
<td>
<p>Stepsize adapter; Not used in original paper</p>
</td></tr>
<tr><td><code id="sampler.GGMC_+3A_madapter">madapter</code></td>
<td>
<p>Mass adapter; Not used in ogirinal paper</p>
</td></tr>
<tr><td><code id="sampler.GGMC_+3A_steps">steps</code></td>
<td>
<p>Number of steps before accept/reject</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with 'juliavar', 'juliacode' and all provided arguments.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sadapter &lt;- sadapter.DualAverage(100)
  sampler &lt;- sampler.GGMC(sadapter = sadapter)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='sampler.HMC'>Standard Hamiltonian Monte Carlo (Hybrid Monte Carlo).</h2><span id='topic+sampler.HMC'></span>

<h3>Description</h3>

<p>Allows for the use of stochastic gradients, but the validity of doing so is not clear.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampler.HMC(
  l,
  path_len,
  sadapter = sadapter.DualAverage(1000),
  madapter = madapter.FixedMassMatrix()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampler.HMC_+3A_l">l</code></td>
<td>
<p>stepsize</p>
</td></tr>
<tr><td><code id="sampler.HMC_+3A_path_len">path_len</code></td>
<td>
<p>number of leapfrog steps</p>
</td></tr>
<tr><td><code id="sampler.HMC_+3A_sadapter">sadapter</code></td>
<td>
<p>Stepsize adapter</p>
</td></tr>
<tr><td><code id="sampler.HMC_+3A_madapter">madapter</code></td>
<td>
<p>Mass adapter</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is motivated by parts of the discussion in
Neal, R. M. (1996). Bayesian Learning for Neural Networks (Vol. 118). Springer
New York. https://doi.org/10.1007/978-1-4612-0745-0
</p>


<h3>Value</h3>

<p>a list with 'juliavar', 'juliacode', and all given arguments
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sadapter &lt;- sadapter.DualAverage(100)
  sampler &lt;- sampler.HMC(1e-3, 3, sadapter = sadapter)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='sampler.SGLD'>Stochastic Gradient Langevin Dynamics as proposed in Welling, M., &amp; Teh, Y. W.
(n.d.). Bayesian Learning via Stochastic Gradient Langevin Dynamics. 8.</h2><span id='topic+sampler.SGLD'></span>

<h3>Description</h3>

<p>Stepsizes will be adapted according to
</p>
<p style="text-align: center;"><code class="reqn">a(b+t)^{-\gamma}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>sampler.SGLD(
  stepsize_a = 0.1,
  stepsize_b = 0,
  stepsize_gamma = 0.55,
  min_stepsize = -Inf
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampler.SGLD_+3A_stepsize_a">stepsize_a</code></td>
<td>
<p>See eq. above</p>
</td></tr>
<tr><td><code id="sampler.SGLD_+3A_stepsize_b">stepsize_b</code></td>
<td>
<p>See eq. above</p>
</td></tr>
<tr><td><code id="sampler.SGLD_+3A_stepsize_gamma">stepsize_gamma</code></td>
<td>
<p>see eq. above</p>
</td></tr>
<tr><td><code id="sampler.SGLD_+3A_min_stepsize">min_stepsize</code></td>
<td>
<p>Do not decrease stepsize beyond this</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with 'juliavar', 'juliacode', and all given arguments
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sampler &lt;- sampler.SGLD()
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='sampler.SGNHTS'>Stochastic Gradient Nose-Hoover Thermostat as proposed in</h2><span id='topic+sampler.SGNHTS'></span>

<h3>Description</h3>

<p>Proposed in Leimkuhler, B., &amp; Shang, X. (2016). Adaptive thermostats for noisy
gradient systems. SIAM Journal on Scientific Computing, 38(2), A712-A736.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampler.SGNHTS(
  l,
  sigmaA = 1,
  xi = 1,
  mu = 1,
  madapter = madapter.FixedMassMatrix()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampler.SGNHTS_+3A_l">l</code></td>
<td>
<p>Stepsize</p>
</td></tr>
<tr><td><code id="sampler.SGNHTS_+3A_sigmaa">sigmaA</code></td>
<td>
<p>Diffusion factor</p>
</td></tr>
<tr><td><code id="sampler.SGNHTS_+3A_xi">xi</code></td>
<td>
<p>Thermostat</p>
</td></tr>
<tr><td><code id="sampler.SGNHTS_+3A_mu">mu</code></td>
<td>
<p>Free parameter of thermostat</p>
</td></tr>
<tr><td><code id="sampler.SGNHTS_+3A_madapter">madapter</code></td>
<td>
<p>Mass Adapter; Not used in original paper and thus
has no theoretical backing</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is similar to SGNHT as proposed in
Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R. D., &amp; Neven, H. (2014).
Bayesian sampling using stochastic gradient thermostats. Advances in neural
information processing systems, 27.
</p>


<h3>Value</h3>

<p>a list with 'juliavar', 'juliacode' and all arguments provided
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sampler &lt;- sampler.SGNHTS(1e-3)
  ch &lt;- mcmc(bnn, 10, 1000, sampler)

## End(Not run)

</code></pre>

<hr>
<h2 id='summary.BNN'>Print a summary of a BNN</h2><span id='topic+summary.BNN'></span>

<h3>Description</h3>

<p>Print a summary of a BNN
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.BNN_+3A_object">object</code></td>
<td>
<p>A BNN created using <code><a href="#topic+BNN">BNN</a></code></p>
</td></tr>
<tr><td><code id="summary.BNN_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>

<hr>
<h2 id='tensor_embed_mat'>Embed a matrix of timeseries into a tensor</h2><span id='topic+tensor_embed_mat'></span>

<h3>Description</h3>

<p>This is used when working with recurrent networks, especially in
the case of seq-to-one modelling. Creates overlapping subsequences
of the data with length 'len_seq'. Returned dimensions are seq_len x num_vars x num_subsequences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensor_embed_mat(mat, len_seq)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tensor_embed_mat_+3A_mat">mat</code></td>
<td>
<p>Matrix of time series</p>
</td></tr>
<tr><td><code id="tensor_embed_mat_+3A_len_seq">len_seq</code></td>
<td>
<p>subsequence length</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tensor of dimension: len_seq x num_vars x num_subsequences
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(RNN(5, 1))
  like &lt;- likelihood.seqtoone_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  data &lt;- matrix(rnorm(5*1000), ncol = 5)
  # Choosing sequences of length 10 and predicting one period ahead
  tensor &lt;- tensor_embed_mat(data, 10+1)
  x &lt;- tensor[1:10, , , drop = FALSE]
  # Last value in each sequence is the target value
  y &lt;- tensor[11,1,]
  bnn &lt;- BNN(x, y, like, prior, init)
  BNN.totparams(bnn)

## End(Not run)

</code></pre>

<hr>
<h2 id='to_bayesplot'>Convert draws array to conform with 'bayesplot'</h2><span id='topic+to_bayesplot'></span>

<h3>Description</h3>

<p>BayesFluxR returns draws in a matrix of dimension
params x draws. This cannot be used with the 'bayesplot' package
which expects an array of dimensions draws x  chains x params.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to_bayesplot(ch, param_names = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="to_bayesplot_+3A_ch">ch</code></td>
<td>
<p>Chain of draws obtained using <code><a href="#topic+mcmc">mcmc</a></code></p>
</td></tr>
<tr><td><code id="to_bayesplot_+3A_param_names">param_names</code></td>
<td>
<p>If 'NULL', the parameter names will be of the
form 'param_1', 'param_2', etc. If 'param_names' is a string,
the parameter names will start with the string with the number
of the parameter attached to it. If 'param_names' is a vector, it
has to provide a name for each paramter in the chain.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an array of dimensions draws x chains x params.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  x &lt;- matrix(rnorm(5*100), nrow = 5)
  y &lt;- rnorm(100)
  bnn &lt;- BNN(x, y, like, prior, init)
  sampler &lt;- sampler.SGLD()
  ch &lt;- mcmc(bnn, 10, 1000, sampler)
  ch &lt;- to_bayesplot(ch)
  library(bayesplot)
  mcmc_intervals(ch, pars = paste0("param_", 1:10))

## End(Not run)
</code></pre>

<hr>
<h2 id='Truncated'>Truncates a Distribution</h2><span id='topic+Truncated'></span>

<h3>Description</h3>

<p>Truncates a Julia Distribution between 'lower' and 'upper'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Truncated(dist, lower, upper)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Truncated_+3A_dist">dist</code></td>
<td>
<p>A Julia Distribution created using <code><a href="#topic+Gamma">Gamma</a></code>,
<code><a href="#topic+InverseGamma">InverseGamma</a></code> ...</p>
</td></tr>
<tr><td><code id="Truncated_+3A_lower">lower</code></td>
<td>
<p>lower bound</p>
</td></tr>
<tr><td><code id="Truncated_+3A_upper">upper</code></td>
<td>
<p>upper bound</p>
</td></tr>
</table>


<h3>Value</h3>

<p>see <code><a href="#topic+Gamma">Gamma</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(Dense(5, 1))
  like &lt;- likelihood.feedforward_normal(net, Truncated(Normal(0, 0.5), 0, Inf))

## End(Not run)

</code></pre>

<hr>
<h2 id='vi.get_samples'>Draw samples form a variational family.</h2><span id='topic+vi.get_samples'></span>

<h3>Description</h3>

<p>Draw samples form a variational family.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi.get_samples(vi, n = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi.get_samples_+3A_vi">vi</code></td>
<td>
<p>obtained using <code><a href="#topic+bayes_by_backprop">bayes_by_backprop</a></code></p>
</td></tr>
<tr><td><code id="vi.get_samples_+3A_n">n</code></td>
<td>
<p>number of samples</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix whose columns are draws from the variational posterior
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ## Needs previous call to `BayesFluxR_setup` which is time
  ## consuming and requires Julia and BayesFlux.jl
  BayesFluxR_setup(installJulia=TRUE, seed=123)
  net &lt;- Chain(RNN(5, 1))
  like &lt;- likelihood.seqtoone_normal(net, Gamma(2.0, 0.5))
  prior &lt;- prior.gaussian(net, 0.5)
  init &lt;- initialise.allsame(Normal(0, 0.5), like, prior)
  data &lt;- matrix(rnorm(10*1000), ncol = 10)
  # Choosing sequences of length 10 and predicting one period ahead
  tensor &lt;- tensor_embed_mat(data, 10+1)
  x &lt;- tensor[1:10, , , drop = FALSE]
  # Last value in each sequence is the target value
  y &lt;- tensor[11,,]
  bnn &lt;- BNN(x, y, like, prior, init)
  vi &lt;- bayes_by_backprop(bnn, 100, 100)
  vi_samples &lt;- vi.get_samples(vi, n = 1000)
  pp &lt;- posterior_predictive(bnn, vi_samples)

## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
