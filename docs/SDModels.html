<!DOCTYPE html><html lang="en"><head><title>Help for package SDModels</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SDModels}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#copy.SDForest'><p>Copy a forest</p></a></li>
<li><a href='#copy.SDTree'><p>Copy a tree</p></a></li>
<li><a href='#cvSDTree'><p>Cross-validation for the SDTree</p></a></li>
<li><a href='#f_four'><p>Function of x on a fourier basis</p></a></li>
<li><a href='#fromList.SDForest'><p>SDForest fromList method</p></a></li>
<li><a href='#fromList.SDTree'><p>SDTree fromList method</p></a></li>
<li><a href='#get_cp_seq.SDForest'><p>Get the sequence of complexity parameters of an SDForest</p></a></li>
<li><a href='#get_cp_seq.SDTree'><p>Get the sequence of complexity parameters of an SDTree</p></a></li>
<li><a href='#get_Q'><p>Estimation of spectral transformation</p></a></li>
<li><a href='#get_W'><p>Estimation of anchor transformation</p></a></li>
<li><a href='#mergeForest'><p>Merge two forests</p></a></li>
<li><a href='#partDependence'><p>Partial dependence</p></a></li>
<li><a href='#plot.partDependence'><p>Plot partial dependence</p></a></li>
<li><a href='#plot.paths'><p>Visualize the paths of an SDTree or SDForest</p></a></li>
<li><a href='#plot.SDTree'><p>Plot SDTree</p></a></li>
<li><a href='#plotOOB'><p>Visualize the out-of-bag performance of an SDForest</p></a></li>
<li><a href='#predict_individual_fj'><p>Predictions of individual component functions for SDAM</p></a></li>
<li><a href='#predict.SDAM'><p>Predictions for SDAM</p></a></li>
<li><a href='#predict.SDForest'><p>Predictions for the SDForest</p></a></li>
<li><a href='#predict.SDTree'><p>Predictions for the SDTree</p></a></li>
<li><a href='#predictOOB'><p>Out-of-bag predictions for the SDForest</p></a></li>
<li><a href='#print.partDependence'><p>Print partDependence</p></a></li>
<li><a href='#print.SDAM'><p>Print SDAM</p></a></li>
<li><a href='#print.SDForest'><p>Print SDForest</p></a></li>
<li><a href='#print.SDTree'><p>Print a SDTree</p></a></li>
<li><a href='#prune.SDForest'><p>Prune an SDForest</p></a></li>
<li><a href='#prune.SDTree'><p>Prune an SDTree</p></a></li>
<li><a href='#regPath.SDForest'><p>Calculate the regularization path of an SDForest</p></a></li>
<li><a href='#regPath.SDTree'><p>Calculate the regularization path of an SDTree</p></a></li>
<li><a href='#SDAM'><p>Spectrally Deconfounded Additive Models</p></a></li>
<li><a href='#SDForest'><p>Spectrally Deconfounded Random Forests</p></a></li>
<li><a href='#SDTree'><p>Spectrally Deconfounded Tree</p></a></li>
<li><a href='#simulate_data_nonlinear'><p>Simulate data with linear confounding and non-linear causal effect</p></a></li>
<li><a href='#simulate_data_step'><p>Simulate data with linear confounding and causal effect following a step-function</p></a></li>
<li><a href='#stabilitySelection.SDForest'><p>Calculate the stability selection of an SDForest</p></a></li>
<li><a href='#toList.SDForest'><p>SDForest toList method</p></a></li>
<li><a href='#toList.SDTree'><p>SDTree toList method</p></a></li>
<li><a href='#varImp.SDAM'><p>Extract Variable importance for SDAM</p></a></li>
<li><a href='#varImp.SDForest'><p>Extract variable importance of an SDForest</p></a></li>
<li><a href='#varImp.SDTree'><p>Extract variable importance of an SDTree</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Spectrally Deconfounded Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.4</td>
</tr>
<tr>
<td>Description:</td>
<td>Screen for and analyze non-linear sparse direct effects in the presence of unobserved confounding using the spectral deconfounding techniques (Ćevid, Bühlmann, and Meinshausen (2020)&lt;jmlr.org/papers/v21/19-545.html&gt;, Guo, Ćevid, and Bühlmann (2022) &lt;<a href="https://doi.org/10.1214%2F21-AOS2152">doi:10.1214/21-AOS2152</a>&gt;). These methods have been shown to be a good estimate for the true direct effect if we observe many covariates, e.g., high-dimensional settings, and we have fairly dense confounding. Even if the assumptions are violated, it seems like there is not much to lose, and the deconfounded models will, in general, estimate a function closer to the true one than classical least squares optimization. 'SDModels' provides functions SDAM() for Spectrally Deconfounded Additive Models (Scheidegger, Guo, and Bühlmann (2025) &lt;<a href="https://doi.org/10.1145%2F3711116">doi:10.1145/3711116</a>&gt;) and SDForest() for Spectrally Deconfounded Random Forests (Ulmer, Scheidegger, and Bühlmann (2025) &lt;<a href="https://doi.org/10.48550%2FarXiv.2502.03969">doi:10.48550/arXiv.2502.03969</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>data.tree, DiagrammeR, doParallel, ggplot2, GPUmatrix,
gridExtra, locatexec, parallel, pbapply, Rdpack, tidyr, fda,
grplasso, rlang</td>
</tr>
<tr>
<td>Suggests:</td>
<td>plotly, datasets, rpart, knitr, rmarkdown, ranger, HDclassif,
qpdf, igraph, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://markusul.github.io/SDModels/">https://markusul.github.io/SDModels/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/markusul/SDModels/issues">https://github.com/markusul/SDModels/issues</a></td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-18 10:55:30 UTC; maulmer</td>
</tr>
<tr>
<td>Author:</td>
<td>Markus Ulmer <a href="https://orcid.org/0000-0001-7783-8475"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph],
  Cyrill Scheidegger
    <a href="https://orcid.org/0009-0005-2851-1384"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Markus Ulmer &lt;markus.ulmer@stat.math.ethz.ch&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-19 14:30:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='copy.SDForest'>Copy a forest</h2><span id='topic+copy.SDForest'></span><span id='topic+copy'></span>

<h3>Description</h3>

<p>Returns a copy of the forest object.
Might be useful if you want to keep the original forest in comparison to the pruned forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDForest'
copy(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="copy.SDForest_+3A_object">object</code></td>
<td>
<p>an SDForest object</p>
</td></tr>
<tr><td><code id="copy.SDForest_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A copy of the SDForest object
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prune">prune</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
X &lt;- matrix(rnorm(10 * 20), nrow = 10)
Y &lt;- rnorm(10)
fit &lt;- SDForest(x = X, y = Y, nTree = 2, cp = 0.5)
fit2 &lt;- copy(fit)
</code></pre>

<hr>
<h2 id='copy.SDTree'>Copy a tree</h2><span id='topic+copy.SDTree'></span>

<h3>Description</h3>

<p>Returns a copy of the tree object. 
Might be useful if you want to keep the original tree in comparison to the pruned tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDTree'
copy(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="copy.SDTree_+3A_object">object</code></td>
<td>
<p>an SDTree object</p>
</td></tr>
<tr><td><code id="copy.SDTree_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A copy of the SDTree object
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prune">prune</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
X &lt;- matrix(rnorm(10 * 20), nrow = 10)
Y &lt;- rnorm(10)
fit &lt;- SDTree(x = X, y = Y, cp = 0.5)
fit2 &lt;- copy(fit)
</code></pre>

<hr>
<h2 id='cvSDTree'>Cross-validation for the SDTree</h2><span id='topic+cvSDTree'></span>

<h3>Description</h3>

<p>Estimates the optimal complexity parameter for the SDTree using cross-validation. 
The transformations are estimated for each training set and validation set 
separately to ensure independence of the validation set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvSDTree(
  formula = NULL,
  data = NULL,
  x = NULL,
  y = NULL,
  max_leaves = NULL,
  cp = 0,
  min_sample = 5,
  mtry = NULL,
  fast = TRUE,
  Q_type = "trim",
  trim_quantile = 0.5,
  q_hat = 0,
  Qf = NULL,
  A = NULL,
  gamma = 0.5,
  gpu = FALSE,
  mem_size = 1e+07,
  max_candidates = 100,
  nfolds = 3,
  cp_seq = NULL,
  mc.cores = 1,
  Q_scale = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cvSDTree_+3A_formula">formula</code></td>
<td>
<p>Object of class <code>formula</code> or describing the model to fit 
of the form <code>y ~ x1 + x2 + ...</code> where <code>y</code> is a numeric response and 
<code>x1, x2, ...</code> are vectors of covariates. Interactions are not supported.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code> containing the variables in the model.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_x">x</code></td>
<td>
<p>Predictor data, alternative to <code>formula</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_y">y</code></td>
<td>
<p>Response vector, alternative to <code>formula</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_max_leaves">max_leaves</code></td>
<td>
<p>Maximum number of leaves for the grown tree.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_cp">cp</code></td>
<td>
<p>Complexity parameter, minimum loss decrease to split a node. 
A split is only performed if the loss decrease is larger than <code>cp * initial_loss</code>, 
where <code>initial_loss</code> is the loss of the initial estimate using only a stump.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_min_sample">min_sample</code></td>
<td>
<p>Minimum number of observations per leaf. 
A split is only performed if both resulting leaves have at least 
<code>min_sample</code> observations.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_mtry">mtry</code></td>
<td>
<p>Number of randomly selected covariates to consider for a split, 
if <code>NULL</code> all covariates are available for each split.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_fast">fast</code></td>
<td>
<p>If <code>TRUE</code>, only the optimal splits in the new leaves are 
evaluated and the previously optimal splits and their potential loss-decrease are reused. 
If <code>FALSE</code> all possible splits in all the leaves are reevaluated after every split.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_q_type">Q_type</code></td>
<td>
<p>Type of deconfounding, one of 'trim', 'pca', 'no_deconfounding'. 
'trim' corresponds to the Trim transform (Ćevid et al. 2020) 
as implemented in the Doubly debiased lasso (Guo et al. 2022), 
'pca' to the PCA transformation(Paul et al. 2008). 
See <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_trim_quantile">trim_quantile</code></td>
<td>
<p>Quantile for Trim transform, 
only needed for trim and DDL_trim, see <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_q_hat">q_hat</code></td>
<td>
<p>Assumed confounding dimension, only needed for pca, 
see <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_qf">Qf</code></td>
<td>
<p>Spectral transformation, if <code>NULL</code> 
it is internally estimated using <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_a">A</code></td>
<td>
<p>Numerical Anchor of class <code>matrix</code>. See <code><a href="#topic+get_W">get_W</a></code>.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_gamma">gamma</code></td>
<td>
<p>Strength of distributional robustness, <code class="reqn">\gamma \in [0, \infty]</code>. 
See <code><a href="#topic+get_W">get_W</a></code>.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_gpu">gpu</code></td>
<td>
<p>If <code>TRUE</code>, the calculations are performed on the GPU. 
If it is properly set up.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_mem_size">mem_size</code></td>
<td>
<p>Amount of split candidates that can be evaluated at once.
This is a trade-off between memory and speed can be decreased if either
the memory is not sufficient or the gpu is to small.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_max_candidates">max_candidates</code></td>
<td>
<p>Maximum number of split points that are 
proposed at each node for each covariate.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds for cross-validation. 
It is recommended to not use more than 5 folds if the number of covariates 
is larger than the number of observations. In this case the spectral 
transformation could differ to much if the validation data is 
substantially smaller than the training data.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_cp_seq">cp_seq</code></td>
<td>
<p>Sequence of complexity parameters cp to compare using cross-validation, 
if <code>NULL</code> a sequence from 0 to 0.6 with stepsize 0.002 is used.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_mc.cores">mc.cores</code></td>
<td>
<p>Number of cores to use for parallel computation.</p>
</td></tr>
<tr><td><code id="cvSDTree_+3A_q_scale">Q_scale</code></td>
<td>
<p>Should data be scaled to estimate the spectral transformation? 
Default is <code>TRUE</code> to not reduce the signal of high variance covariates, 
and we do not know of a scenario where this hurts.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing
</p>
<table role = "presentation">
<tr><td><code>cp_min</code></td>
<td>
<p>The optimal complexity parameter.</p>
</td></tr>
<tr><td><code>cp_table</code></td>
<td>
<p>A table containing the complexity parameter, 
the mean and the standard deviation of the loss on the validation sets for the 
complexity parameters. If multiple complexity parameters result in the same loss, 
only the one with the largest complexity parameter is shown.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Guo Z, Ćevid D, Bühlmann P (2022).
&ldquo;Doubly debiased lasso: High-dimensional inference under hidden confounding.&rdquo;
<em>The Annals of Statistics</em>, <b>50</b>(3).
ISSN 0090-5364, <a href="https://doi.org/10.1214/21-AOS2152">doi:10.1214/21-AOS2152</a>.<br /><br /> Paul D, Bair E, Hastie T, Tibshirani R (2008).
&ldquo;“Preconditioning” for feature selection and regression in high-dimensional problems.&rdquo;
<em>The Annals of Statistics</em>, <b>36</b>(4).
ISSN 0090-5364, <a href="https://doi.org/10.1214/009053607000000578">doi:10.1214/009053607000000578</a>.<br /><br /> Ćevid D, Bühlmann P, Meinshausen N (2020).
&ldquo;Spectral Deconfounding via Perturbed Sparse Linear Models.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>21</b>(1).
ISSN 1532-4435, <a href="http://jmlr.org/papers/v21/19-545.html">http://jmlr.org/papers/v21/19-545.html</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDTree">SDTree</a></code> <code><a href="#topic+prune.SDTree">prune.SDTree</a></code> <code><a href="#topic+regPath.SDTree">regPath.SDTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 50
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n, 0, 5)
cp &lt;- cvSDTree(x = X, y = y, Q_type = 'no_deconfounding')
cp
</code></pre>

<hr>
<h2 id='f_four'>Function of x on a fourier basis</h2><span id='topic+f_four'></span>

<h3>Description</h3>

<p>Function of x on a fourier basis with a subset of covariates 
having a causal effect on Y using the parameters beta.
The function is given by:
</p>
<p style="text-align: center;"><code class="reqn">f(x_i) = \sum_{j = 1}^p 1_{j \in js} \sum_{k = 1}^K (\beta_{j, k}^{(1)} \cos(0.2 k x_j) +
\beta_{j, k}^{(2)} \sin(0.2 k x_j))</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>f_four(x, beta, js)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_four_+3A_x">x</code></td>
<td>
<p>a vector of covariates</p>
</td></tr>
<tr><td><code id="f_four_+3A_beta">beta</code></td>
<td>
<p>the parameter vector for the function f(X)</p>
</td></tr>
<tr><td><code id="f_four_+3A_js">js</code></td>
<td>
<p>the indices of the causal covariates in X</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of the function f(x)
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+simulate_data_nonlinear">simulate_data_nonlinear</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
# simulation of confounded data
sim_data &lt;- simulate_data_nonlinear(q = 2, p = 150, n = 100, m = 2)
X &lt;- sim_data$X
j &lt;- sim_data$j[1]
apply(X, 1, function(x) f_four(x, sim_data$beta, j))

</code></pre>

<hr>
<h2 id='fromList.SDForest'>SDForest fromList method</h2><span id='topic+fromList.SDForest'></span><span id='topic+fromList'></span>

<h3>Description</h3>

<p>Converts the trees in an SDForest object from
class <code>list</code> to class <code>Node</code> (Glur 2023).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDForest'
fromList(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fromList.SDForest_+3A_object">object</code></td>
<td>
<p>an SDForest object with the trees in list format</p>
</td></tr>
<tr><td><code id="fromList.SDForest_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an SDForest object with the trees in Node format
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Glur C (2023).
&ldquo;data.tree: General Purpose Hierarchical Data Structure.&rdquo;
<a href="https://CRAN.R-project.org/package=data.tree">https://CRAN.R-project.org/package=data.tree</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fromList">fromList</a></code> <code><a href="#topic+fromList.SDTree">fromList.SDTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5, nTree = 2)
fromList(toList(model))
</code></pre>

<hr>
<h2 id='fromList.SDTree'>SDTree fromList method</h2><span id='topic+fromList.SDTree'></span>

<h3>Description</h3>

<p>Converts the tree in an SDTree object from
class <code>list</code> to class <code>Node</code> (Glur 2023).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDTree'
fromList(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fromList.SDTree_+3A_object">object</code></td>
<td>
<p>an SDTree object with the tree in list format</p>
</td></tr>
<tr><td><code id="fromList.SDTree_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an SDTree object with the tree in Node format
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Glur C (2023).
&ldquo;data.tree: General Purpose Hierarchical Data Structure.&rdquo;
<a href="https://CRAN.R-project.org/package=data.tree">https://CRAN.R-project.org/package=data.tree</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+toList">toList</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDTree(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5)
fromList(toList(model))
</code></pre>

<hr>
<h2 id='get_cp_seq.SDForest'>Get the sequence of complexity parameters of an SDForest</h2><span id='topic+get_cp_seq.SDForest'></span><span id='topic+get_cp_seq'></span>

<h3>Description</h3>

<p>This function extracts the sequence of complexity parameters of an SDForest that
result in changes of the SDForest if pruned. Only cp values that differ
in the first three digits after the decimal point are returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDForest'
get_cp_seq(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_cp_seq.SDForest_+3A_object">object</code></td>
<td>
<p>an SDForest object</p>
</td></tr>
<tr><td><code id="get_cp_seq.SDForest_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sequence of complexity parameters
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regPath">regPath</a></code> <code><a href="#topic+stabilitySelection">stabilitySelection</a></code> 
<code><a href="#topic+get_cp_seq.SDTree">get_cp_seq.SDTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', cp = 0, nTree = 2)
get_cp_seq(model)
</code></pre>

<hr>
<h2 id='get_cp_seq.SDTree'>Get the sequence of complexity parameters of an SDTree</h2><span id='topic+get_cp_seq.SDTree'></span>

<h3>Description</h3>

<p>This function extracts the sequence of complexity parameters of an SDTree that 
result in changes of the tree structure if pruned. Only cp values that differ
in the first three digits after the decimal point are returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDTree'
get_cp_seq(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_cp_seq.SDTree_+3A_object">object</code></td>
<td>
<p>an SDTree object</p>
</td></tr>
<tr><td><code id="get_cp_seq.SDTree_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sequence of complexity parameters
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regPath">regPath</a></code> <code><a href="#topic+stabilitySelection">stabilitySelection</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDTree(x = X, y = y, Q_type = 'no_deconfounding', cp = 0)
get_cp_seq(model)
</code></pre>

<hr>
<h2 id='get_Q'>Estimation of spectral transformation</h2><span id='topic+get_Q'></span>

<h3>Description</h3>

<p>Estimates the spectral transformation Q for spectral deconfounding by 
shrinking the leading singular values of the covariates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_Q(X, type, trim_quantile = 0.5, q_hat = 0, gpu = FALSE, scaling = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_Q_+3A_x">X</code></td>
<td>
<p>Numerical covariates of class <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="get_Q_+3A_type">type</code></td>
<td>
<p>Type of deconfounding, one of 'trim', 'pca', 'no_deconfounding'. 
'trim' corresponds to the Trim transform (Ćevid et al. 2020) 
as implemented in the Doubly debiased lasso (Guo et al. 2022), 
'pca' to the PCA transformation(Paul et al. 2008) 
and 'no_deconfounding' to the Identity.</p>
</td></tr>
<tr><td><code id="get_Q_+3A_trim_quantile">trim_quantile</code></td>
<td>
<p>Quantile for Trim transform, only needed for trim.</p>
</td></tr>
<tr><td><code id="get_Q_+3A_q_hat">q_hat</code></td>
<td>
<p>Assumed confounding dimension, only needed for pca.</p>
</td></tr>
<tr><td><code id="get_Q_+3A_gpu">gpu</code></td>
<td>
<p>If <code>TRUE</code>, the calculations are performed on the GPU. 
If it is properly set up.</p>
</td></tr>
<tr><td><code id="get_Q_+3A_scaling">scaling</code></td>
<td>
<p>Whether X should be scaled before calculating the spectral transformation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Q of class <code>matrix</code>, the spectral transformation matrix.
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Guo Z, Ćevid D, Bühlmann P (2022).
&ldquo;Doubly debiased lasso: High-dimensional inference under hidden confounding.&rdquo;
<em>The Annals of Statistics</em>, <b>50</b>(3).
ISSN 0090-5364, <a href="https://doi.org/10.1214/21-AOS2152">doi:10.1214/21-AOS2152</a>.<br /><br /> Paul D, Bair E, Hastie T, Tibshirani R (2008).
&ldquo;“Preconditioning” for feature selection and regression in high-dimensional problems.&rdquo;
<em>The Annals of Statistics</em>, <b>36</b>(4).
ISSN 0090-5364, <a href="https://doi.org/10.1214/009053607000000578">doi:10.1214/009053607000000578</a>.<br /><br /> Ćevid D, Bühlmann P, Meinshausen N (2020).
&ldquo;Spectral Deconfounding via Perturbed Sparse Linear Models.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>21</b>(1).
ISSN 1532-4435, <a href="http://jmlr.org/papers/v21/19-545.html">http://jmlr.org/papers/v21/19-545.html</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
X &lt;- matrix(rnorm(50 * 20), nrow = 50)
Q_trim &lt;- get_Q(X, 'trim')
Q_pca &lt;- get_Q(X, 'pca', q_hat = 5)
Q_plain &lt;- get_Q(X, 'no_deconfounding')
</code></pre>

<hr>
<h2 id='get_W'>Estimation of anchor transformation</h2><span id='topic+get_W'></span>

<h3>Description</h3>

<p>Estimates the anchor transformation for the Anchor-Objective. 
The anchor transformation is <code class="reqn">W = I-(1-\sqrt{\gamma}))\Pi_A</code>, 
where <code class="reqn">\Pi_A = A(A^TA)^{-1}A^T</code>. For <code class="reqn">\gamma = 1</code> this is just the identity. 
For <code class="reqn">\gamma = 0</code> this corresponds to residuals after orthogonal projecting onto A.
For large <code class="reqn">\gamma</code> this is close to the orthogonal projection onto A, scaled by <code class="reqn">\gamma</code>.
The estimator <code class="reqn">\text{argmin}_f ||W(Y - f(X))||^2</code> corresponds to the Anchor-Regression Estimator 
(Rothenhäusler et al. 2021), (Bühlmann 2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_W(A, gamma, intercept = FALSE, gpu = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_W_+3A_a">A</code></td>
<td>
<p>Numerical Anchor of class <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="get_W_+3A_gamma">gamma</code></td>
<td>
<p>Strength of distributional robustness, <code class="reqn">\gamma \in [0, \infty]</code>.</p>
</td></tr>
<tr><td><code id="get_W_+3A_intercept">intercept</code></td>
<td>
<p>Logical, whether to include an intercept in the anchor.</p>
</td></tr>
<tr><td><code id="get_W_+3A_gpu">gpu</code></td>
<td>
<p>If <code>TRUE</code>, the calculations are performed on the GPU. 
If it is properly set up.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>W of class <code>matrix</code>, the anchor transformation matrix.
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Bühlmann P (2020).
&ldquo;Invariance, Causality and Robustness.&rdquo;
<em>Statistical Science</em>, <b>35</b>(3).
ISSN 0883-4237, <a href="https://doi.org/10.1214/19-STS721">doi:10.1214/19-STS721</a>.<br /><br /> Rothenhäusler D, Meinshausen N, Bühlmann P, Peters J (2021).
&ldquo;Anchor Regression: Heterogeneous Data Meet Causality.&rdquo;
<em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, <b>83</b>(2), 215&ndash;246.
ISSN 1369-7412, <a href="https://doi.org/10.1111/rssb.12398">doi:10.1111/rssb.12398</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 50
X &lt;- matrix(rnorm(n * 1), nrow = n)
Y &lt;- 3 * X + rnorm(n)
W &lt;- get_W(X, gamma = 0)
resid &lt;- W %*% Y
</code></pre>

<hr>
<h2 id='mergeForest'>Merge two forests</h2><span id='topic+mergeForest'></span>

<h3>Description</h3>

<p>This function merges two forests. 
The trees are combined and the variable importance is 
calculated as a weighted average of the two forests. 
If the forests are trained on the same data, 
the predictions and oob_predictions are combined as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mergeForest(fit1, fit2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mergeForest_+3A_fit1">fit1</code></td>
<td>
<p>first <code>SDForest</code> object</p>
</td></tr>
<tr><td><code id="mergeForest_+3A_fit2">fit2</code></td>
<td>
<p>second <code>SDForest</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>merged <code>SDForest</code> object
set.seed(1)
n &lt;- 50
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
fit1 &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', nTree = 5, cp = 0.5)
fit2 &lt;- SDForest(x = X, y = y, nTree = 5, cp = 0.5)
mergeForest(fit1, fit2)
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>

<hr>
<h2 id='partDependence'>Partial dependence</h2><span id='topic+partDependence'></span>

<h3>Description</h3>

<p>This function calculates the partial dependence of a model on a single variable.
For that predictions are made for all observations in the dataset while varying 
the value of the variable of interest. The overall partial effect is the average
of all predictions. (Friedman 2001)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partDependence(object, j, X = NULL, subSample = NULL, mc.cores = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="partDependence_+3A_object">object</code></td>
<td>
<p>A model object that has a predict method that takes newdata as argument 
and returns predictions.</p>
</td></tr>
<tr><td><code id="partDependence_+3A_j">j</code></td>
<td>
<p>The variable for which the partial dependence should be calculated.
Either the column index of the variable in the dataset or the name of the variable.</p>
</td></tr>
<tr><td><code id="partDependence_+3A_x">X</code></td>
<td>
<p>The dataset on which the partial dependence should be calculated.
Should contain the same variables as the dataset used to train the model.
If NULL, tries to extract the dataset from the model object.</p>
</td></tr>
<tr><td><code id="partDependence_+3A_subsample">subSample</code></td>
<td>
<p>Number of samples to draw from the original data for the empirical 
partial dependence. If NULL, all the observations are used.</p>
</td></tr>
<tr><td><code id="partDependence_+3A_mc.cores">mc.cores</code></td>
<td>
<p>Number of cores to use for parallel computation. 
Parallel computing is only supported for unix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>partDependence</code> containing
</p>
<table role = "presentation">
<tr><td><code>preds_mean</code></td>
<td>
<p>The average prediction for each value of the variable of interest.</p>
</td></tr>
<tr><td><code>x_seq</code></td>
<td>
<p>The sequence of values for the variable of interest.</p>
</td></tr>
<tr><td><code>preds</code></td>
<td>
<p>The predictions for each value of the variable of interest for each observation.</p>
</td></tr>
<tr><td><code>j</code></td>
<td>
<p>The name of the variable of interest.</p>
</td></tr>
<tr><td><code>xj</code></td>
<td>
<p>The values of the variable of interest in the dataset.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Friedman JH (2001).
&ldquo;Greedy Function Approximation: A Gradient Boosting Machine.&rdquo;
<em>The Annals of Statistics</em>, <b>29</b>(5), 1189&ndash;1232.
ISSN 00905364, <a href="http://www.jstor.org/stable/2699986">http://www.jstor.org/stable/2699986</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDForest">SDForest</a></code>, <code><a href="#topic+SDTree">SDTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- rnorm(100)
y &lt;- sign(x) * 3 + rnorm(100)
model &lt;- SDTree(x = x, y = y, Q_type = 'no_deconfounding')
pd &lt;- partDependence(model, 1, X = x, subSample = 10)
plot(pd)
</code></pre>

<hr>
<h2 id='plot.partDependence'>Plot partial dependence</h2><span id='topic+plot.partDependence'></span>

<h3>Description</h3>

<p>This function plots the partial dependence of a model on a single variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'partDependence'
plot(x, n_examples = 19, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.partDependence_+3A_x">x</code></td>
<td>
<p>An object of class <code>partDependence</code> returned by <code><a href="#topic+partDependence">partDependence</a></code>.</p>
</td></tr>
<tr><td><code id="plot.partDependence_+3A_n_examples">n_examples</code></td>
<td>
<p>Number of examples to plot in addition to the average prediction.</p>
</td></tr>
<tr><td><code id="plot.partDependence_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+partDependence">partDependence</a></code>
set.seed(1)
x &lt;- rnorm(10)
y &lt;- sign(x) * 3 + rnorm(10)
model &lt;- SDTree(x = x, y = y, Q_type = 'no_deconfounding', cp = 0.5)
pd &lt;- partDependence(model, 1, X = x)
plot(pd)
</p>

<hr>
<h2 id='plot.paths'>Visualize the paths of an SDTree or SDForest</h2><span id='topic+plot.paths'></span>

<h3>Description</h3>

<p>This function visualizes the variable importance of an SDTree or SDForest
for different complexity parameters. Both the regularization path and
the stability selection path can be visualized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'paths'
plot(x, plotly = FALSE, selection = NULL, sqrt_scale = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.paths_+3A_x">x</code></td>
<td>
<p>A <code>paths</code> object</p>
</td></tr>
<tr><td><code id="plot.paths_+3A_plotly">plotly</code></td>
<td>
<p>If TRUE the plot is returned interactive using plotly. Might be slow for large data.</p>
</td></tr>
<tr><td><code id="plot.paths_+3A_selection">selection</code></td>
<td>
<p>A vector of indices of the covariates to be plotted. 
Can be used to plot only a subset of the covariates in case of many covariates.</p>
</td></tr>
<tr><td><code id="plot.paths_+3A_sqrt_scale">sqrt_scale</code></td>
<td>
<p>If TRUE the y-axis is on a square root scale.</p>
</td></tr>
<tr><td><code id="plot.paths_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot</code> object with the variable importance for different regularization.
If the <code>path</code> object includes a cp_min value, a black dashed line is
added to indicate the out-of-bag optimal variable selection.
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regPath">regPath</a></code> <code><a href="#topic+stabilitySelection">stabilitySelection</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + sign(X[, 2]) + rnorm(n)
model &lt;- SDTree(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5)
paths &lt;- regPath(model)
plot(paths)

plot(paths, plotly = TRUE)

</code></pre>

<hr>
<h2 id='plot.SDTree'>Plot SDTree</h2><span id='topic+plot.SDTree'></span>

<h3>Description</h3>

<p>Plot the SDTree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDTree'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.SDTree_+3A_x">x</code></td>
<td>
<p>Fitted object of class <code>SDTree</code>.</p>
</td></tr>
<tr><td><code id="plot.SDTree_+3A_...">...</code></td>
<td>
<p>Further arguments for DiagrammeR::render_graph()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>graph object from DiagrammeR::render_graph()
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDTree">SDTree</a></code>
set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDTree(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5)
plot(model)
</p>

<hr>
<h2 id='plotOOB'>Visualize the out-of-bag performance of an SDForest</h2><span id='topic+plotOOB'></span>

<h3>Description</h3>

<p>This function visualizes the out-of-bag performance of an SDForest
for different complexity parameters. Can be used to choose the optimal
complexity parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotOOB(object, sqrt_scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotOOB_+3A_object">object</code></td>
<td>
<p>A paths object with loss_path <code>matrix</code> 
with the out-of-bag performance for each complexity parameter.</p>
</td></tr>
<tr><td><code id="plotOOB_+3A_sqrt_scale">sqrt_scale</code></td>
<td>
<p>If TRUE the x-axis is on a square root scale.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regPath.SDForest">regPath.SDForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + sign(X[, 2]) + rnorm(n)
model &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5)
paths &lt;- regPath(model)
plotOOB(paths)
</code></pre>

<hr>
<h2 id='predict_individual_fj'>Predictions of individual component functions for SDAM</h2><span id='topic+predict_individual_fj'></span>

<h3>Description</h3>

<p>Predicts the contribution of an individual component j using a fitted SDAM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_individual_fj(object, j, newdata = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_individual_fj_+3A_object">object</code></td>
<td>
<p>Fitted object of class <code>SDAM</code>.</p>
</td></tr>
<tr><td><code id="predict_individual_fj_+3A_j">j</code></td>
<td>
<p>Which component to evaluate.</p>
</td></tr>
<tr><td><code id="predict_individual_fj_+3A_newdata">newdata</code></td>
<td>
<p>New test data of class <code>data.frame</code> containing
the covariates for which to predict the response.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions for fj evaluated at Xjnew.
</p>


<h3>Author(s)</h3>

<p>Cyrill Scheidegger
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDAM">SDAM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
X &lt;- matrix(rnorm(10 * 5), ncol = 5)
Y &lt;- sin(X[, 1]) -  X[, 2] + rnorm(10)
model &lt;- SDAM(x = X, y = Y, Q_type = "trim", trim_quantile = 0.5, nfold = 2, n_K = 1)
predict_individual_fj(model, j = 1)
</code></pre>

<hr>
<h2 id='predict.SDAM'>Predictions for SDAM</h2><span id='topic+predict.SDAM'></span>

<h3>Description</h3>

<p>Predicts the response for new data using a fitted SDAM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDAM'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.SDAM_+3A_object">object</code></td>
<td>
<p>Fitted object of class <code>SDAM</code>.</p>
</td></tr>
<tr><td><code id="predict.SDAM_+3A_newdata">newdata</code></td>
<td>
<p>New test data of class <code>data.frame</code> containing
the covariates for which to predict the response.</p>
</td></tr>
<tr><td><code id="predict.SDAM_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions for the new data.
</p>


<h3>Author(s)</h3>

<p>Cyrill Scheidegger
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDAM">SDAM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
X &lt;- matrix(rnorm(10 * 5), ncol = 5)
Y &lt;- sin(X[, 1]) -  X[, 2] + rnorm(10)
model &lt;- SDAM(x = X, y = Y, Q_type = "trim", trim_quantile = 0.5, nfold = 2, n_K = 1)
predict(model, newdata = data.frame(X))
</code></pre>

<hr>
<h2 id='predict.SDForest'>Predictions for the SDForest</h2><span id='topic+predict.SDForest'></span>

<h3>Description</h3>

<p>Predicts the response for new data using a fitted SDForest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDForest'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.SDForest_+3A_object">object</code></td>
<td>
<p>Fitted object of class <code>SDForest</code>.</p>
</td></tr>
<tr><td><code id="predict.SDForest_+3A_newdata">newdata</code></td>
<td>
<p>New test data of class <code>data.frame</code> containing
the covariates for which to predict the response.</p>
</td></tr>
<tr><td><code id="predict.SDForest_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions for the new data.
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDForest">SDForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 50
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', nTree = 5, cp = 0.5)
predict(model, newdata = data.frame(X))
</code></pre>

<hr>
<h2 id='predict.SDTree'>Predictions for the SDTree</h2><span id='topic+predict.SDTree'></span>

<h3>Description</h3>

<p>Predicts the response for new data using a fitted SDTree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDTree'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.SDTree_+3A_object">object</code></td>
<td>
<p>Fitted object of class <code>SDTree</code>.</p>
</td></tr>
<tr><td><code id="predict.SDTree_+3A_newdata">newdata</code></td>
<td>
<p>New test data of class <code>data.frame</code> containing 
the covariates for which to predict the response.</p>
</td></tr>
<tr><td><code id="predict.SDTree_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions for the new data.
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDTree">SDTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDTree(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5)
predict(model, newdata = data.frame(X))
</code></pre>

<hr>
<h2 id='predictOOB'>Out-of-bag predictions for the SDForest</h2><span id='topic+predictOOB'></span>

<h3>Description</h3>

<p>Predicts the response for the training data 
using only the trees in the SDForest 
that were not trained on the observation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictOOB(object, X = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictOOB_+3A_object">object</code></td>
<td>
<p>Fitted object of class <code>SDForest</code>.</p>
</td></tr>
<tr><td><code id="predictOOB_+3A_x">X</code></td>
<td>
<p>Covariates of the training data.
If <code>NULL</code>, the data saved in the object is used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of out-of-bag predictions for the training data.
#' set.seed(1)
n &lt;- 50
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', nTree = 5, cp = 0.5)
predictOOB(model)
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDForest">SDForest</a></code> <code><a href="#topic+prune.SDForest">prune.SDForest</a></code> <code><a href="#topic+plotOOB">plotOOB</a></code>
</p>

<hr>
<h2 id='print.partDependence'>Print partDependence</h2><span id='topic+print.partDependence'></span>

<h3>Description</h3>

<p>Print contents of the partDependence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'partDependence'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.partDependence_+3A_x">x</code></td>
<td>
<p>Fitted object of class <code>partDependence</code>.</p>
</td></tr>
<tr><td><code id="print.partDependence_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+partDependence">partDependence</a></code>, <code><a href="#topic+plot.partDependence">plot.partDependence</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- rnorm(10)
y &lt;- sign(x) * 3 + rnorm(10)
model &lt;- SDTree(x = x, y = y, Q_type = 'no_deconfounding', cp = 0.5)
pd &lt;- partDependence(model, 1, X = x)
print(pd)
</code></pre>

<hr>
<h2 id='print.SDAM'>Print SDAM</h2><span id='topic+print.SDAM'></span>

<h3>Description</h3>

<p>Print number of covariates and number of active covariates for SDAM object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDAM'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.SDAM_+3A_x">x</code></td>
<td>
<p>Fitted object of class <code>SDAM</code>.</p>
</td></tr>
<tr><td><code id="print.SDAM_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Author(s)</h3>

<p>Cyrill Scheidegger
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDAM">SDAM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
X &lt;- matrix(rnorm(10 * 5), ncol = 5)
Y &lt;- sin(X[, 1]) -  X[, 2] + rnorm(10)
model &lt;- SDAM(x = X, y = Y, Q_type = "trim", trim_quantile = 0.5, nfold = 2, n_K = 1)
print(model)
</code></pre>

<hr>
<h2 id='print.SDForest'>Print SDForest</h2><span id='topic+print.SDForest'></span>

<h3>Description</h3>

<p>Print contents of the SDForest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDForest'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.SDForest_+3A_x">x</code></td>
<td>
<p>Fitted object of class <code>SDForest</code>.</p>
</td></tr>
<tr><td><code id="print.SDForest_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDForest">SDForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 50
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', nTree = 5, cp = 0.5)
print(model)
</code></pre>

<hr>
<h2 id='print.SDTree'>Print a SDTree</h2><span id='topic+print.SDTree'></span>

<h3>Description</h3>

<p>Print contents of the SDTree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDTree'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.SDTree_+3A_x">x</code></td>
<td>
<p>Fitted object of class <code>SDTree</code>.</p>
</td></tr>
<tr><td><code id="print.SDTree_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDTree">SDTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDTree(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5)
print(model)
</code></pre>

<hr>
<h2 id='prune.SDForest'>Prune an SDForest</h2><span id='topic+prune.SDForest'></span><span id='topic+prune'></span>

<h3>Description</h3>

<p>Prunes all trees in the forest and re-calculates the out-of-bag predictions and performance measures.
The training data is needed to calculate the out-of-bag statistics. Note that the forest is pruned in place.
If you intend to keep the original forest, make a copy of it before pruning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDForest'
prune(object, cp, X = NULL, Y = NULL, Q = NULL, pred = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prune.SDForest_+3A_object">object</code></td>
<td>
<p>an SDForest object</p>
</td></tr>
<tr><td><code id="prune.SDForest_+3A_cp">cp</code></td>
<td>
<p>Complexity parameter, the higher the value the more nodes are pruned.</p>
</td></tr>
<tr><td><code id="prune.SDForest_+3A_x">X</code></td>
<td>
<p>The training data, if NULL the data from the forest object is used.</p>
</td></tr>
<tr><td><code id="prune.SDForest_+3A_y">Y</code></td>
<td>
<p>The training response variable, if NULL the data from the forest object is used.</p>
</td></tr>
<tr><td><code id="prune.SDForest_+3A_q">Q</code></td>
<td>
<p>The transformation function, if NULL the data from the forest object is used.</p>
</td></tr>
<tr><td><code id="prune.SDForest_+3A_pred">pred</code></td>
<td>
<p>If TRUE the predictions are calculated, if FALSE only the out-of-bag statistics are calculated.
This can set to FALSE to save computation time if only the out-of-bag statistics are needed.</p>
</td></tr>
<tr><td><code id="prune.SDForest_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A pruned SDForest object
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+copy">copy</a></code> <code><a href="#topic+prune.SDTree">prune.SDTree</a></code> <code><a href="#topic+regPath">regPath</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
X &lt;- matrix(rnorm(10 * 20), nrow = 10)
Y &lt;- rnorm(10)
fit &lt;- SDForest(x = X, y = Y, nTree = 2)
pruned_fit &lt;- prune(copy(fit), 0.2)
</code></pre>

<hr>
<h2 id='prune.SDTree'>Prune an SDTree</h2><span id='topic+prune.SDTree'></span>

<h3>Description</h3>

<p>Removes all nodes that did not improve the loss by more than cp times the initial loss. 
Either by themselves or by one of their successors. Note that the tree is pruned in place.
If you intend to keep the original tree, make a copy of it before pruning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDTree'
prune(object, cp, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prune.SDTree_+3A_object">object</code></td>
<td>
<p>an SDTree object</p>
</td></tr>
<tr><td><code id="prune.SDTree_+3A_cp">cp</code></td>
<td>
<p>Complexity parameter, the higher the value the more nodes are pruned.</p>
</td></tr>
<tr><td><code id="prune.SDTree_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A pruned SDTree object
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+copy">copy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
X &lt;- matrix(rnorm(10 * 20), nrow = 10)
Y &lt;- rnorm(10)
tree &lt;- SDTree(x = X, y = Y)
pruned_tree &lt;- prune(copy(tree), 0.2)
tree
pruned_tree
</code></pre>

<hr>
<h2 id='regPath.SDForest'>Calculate the regularization path of an SDForest</h2><span id='topic+regPath.SDForest'></span><span id='topic+regPath'></span>

<h3>Description</h3>

<p>This function calculates the variable importance of an SDForest
and the out-of-bag performance for different complexity parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDForest'
regPath(object, cp_seq = NULL, X = NULL, Y = NULL, Q = NULL, copy = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regPath.SDForest_+3A_object">object</code></td>
<td>
<p>an SDForest object</p>
</td></tr>
<tr><td><code id="regPath.SDForest_+3A_cp_seq">cp_seq</code></td>
<td>
<p>A sequence of complexity parameters.
If NULL, the sequence is calculated automatically using only relevant values.</p>
</td></tr>
<tr><td><code id="regPath.SDForest_+3A_x">X</code></td>
<td>
<p>The training data, if NULL the data from the forest object is used.</p>
</td></tr>
<tr><td><code id="regPath.SDForest_+3A_y">Y</code></td>
<td>
<p>The training response variable, if NULL the data from the forest object is used.</p>
</td></tr>
<tr><td><code id="regPath.SDForest_+3A_q">Q</code></td>
<td>
<p>The transformation matrix, if NULL the data from the forest object is used.</p>
</td></tr>
<tr><td><code id="regPath.SDForest_+3A_copy">copy</code></td>
<td>
<p>Whether the tree should be copied for the regularization path.
If FALSE, the pruning is done in place and will change the SDForest.
This might be reasonable, if the SDForest is to large to copy.</p>
</td></tr>
<tr><td><code id="regPath.SDForest_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>paths</code> containing
</p>
<table role = "presentation">
<tr><td><code>cp</code></td>
<td>
<p>The sequence of complexity parameters.</p>
</td></tr>
<tr><td><code>varImp_path</code></td>
<td>
<p>A <code>matrix</code> with the variable importance
for each complexity parameter.</p>
</td></tr>
<tr><td><code>loss_path</code></td>
<td>
<p>A <code>matrix</code> with the out-of-bag performance
for each complexity parameter.</p>
</td></tr>
<tr><td><code>cp_min</code></td>
<td>
<p>The complexity parameter with the lowest out-of-bag performance.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Path type</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.paths">plot.paths</a></code> <code><a href="#topic+plotOOB">plotOOB</a></code> <code><a href="#topic+regPath.SDTree">regPath.SDTree</a></code> <code><a href="#topic+prune">prune</a></code> <code><a href="#topic+get_cp_seq">get_cp_seq</a></code> <code><a href="#topic+SDForest">SDForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + sign(X[, 2]) + rnorm(n)
model &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5)
paths &lt;- regPath(model)
plotOOB(paths)
plot(paths)

plot(paths, plotly = TRUE)


</code></pre>

<hr>
<h2 id='regPath.SDTree'>Calculate the regularization path of an SDTree</h2><span id='topic+regPath.SDTree'></span>

<h3>Description</h3>

<p>This function calculates the variable importance of an SDTree
for different complexity parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDTree'
regPath(object, cp_seq = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regPath.SDTree_+3A_object">object</code></td>
<td>
<p>an SDTree object</p>
</td></tr>
<tr><td><code id="regPath.SDTree_+3A_cp_seq">cp_seq</code></td>
<td>
<p>A sequence of complexity parameters.
If NULL, the sequence is calculated automatically using only relevant values.</p>
</td></tr>
<tr><td><code id="regPath.SDTree_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>paths</code> containing
</p>
<table role = "presentation">
<tr><td><code>cp</code></td>
<td>
<p>The sequence of complexity parameters.</p>
</td></tr>
<tr><td><code>varImp_path</code></td>
<td>
<p>A <code>matrix</code> with the variable importance
for each complexity parameter.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Path type</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.paths">plot.paths</a></code> <code><a href="#topic+prune">prune</a></code> <code><a href="#topic+get_cp_seq">get_cp_seq</a></code> <code><a href="#topic+SDTree">SDTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + sign(X[, 2]) + rnorm(n)
model &lt;- SDTree(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5)
paths &lt;- regPath(model)
plot(paths)

plot(paths, plotly = TRUE)

</code></pre>

<hr>
<h2 id='SDAM'>Spectrally Deconfounded Additive Models</h2><span id='topic+SDAM'></span>

<h3>Description</h3>

<p>Estimate high-dimensional additive models using spectral deconfounding (Scheidegger et al. 2025).
The covariates are expanded into B-spline basis functions. A spectral
transformation is used to remove bias arising from hidden confounding and
a group lasso objective is minimized to enforce component-wise sparsity.
Optimal number of basis functions per component and sparsity penalty are
chosen by cross validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SDAM(
  formula = NULL,
  data = NULL,
  x = NULL,
  y = NULL,
  Q_type = "trim",
  trim_quantile = 0.5,
  q_hat = 0,
  nfolds = 5,
  cv_method = "1se",
  n_K = 4,
  n_lambda1 = 10,
  n_lambda2 = 20,
  Q_scale = TRUE,
  ind_lin = NULL,
  mc.cores = 1,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SDAM_+3A_formula">formula</code></td>
<td>
<p>Object of class <code>formula</code> or describing the model to fit 
of the form <code>y ~ x1 + x2 + ...</code> where <code>y</code> is a numeric response and 
<code>x1, x2, ...</code> are vectors of covariates. Interactions are not supported.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code> containing the variables in the model.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_x">x</code></td>
<td>
<p>Matrix of covariates, alternative to <code>formula</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_y">y</code></td>
<td>
<p>Vector of responses, alternative to <code>formula</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_q_type">Q_type</code></td>
<td>
<p>Type of deconfounding, one of 'trim', 'pca', 'no_deconfounding'. 
'trim' corresponds to the Trim transform (Ćevid et al. 2020) 
as implemented in the Doubly debiased lasso (Guo et al. 2022), 
'pca' to the PCA transformation(Paul et al. 2008). 
See <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_trim_quantile">trim_quantile</code></td>
<td>
<p>Quantile for Trim transform, 
only needed for trim, see <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_q_hat">q_hat</code></td>
<td>
<p>Assumed confounding dimension, only needed for pca, 
see <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds for cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_cv_method">cv_method</code></td>
<td>
<p>The method for selecting the regularization parameter during cross-validation.
One of &quot;min&quot; (minimum cv-loss) and &quot;1se&quot; (one-standard-error rule) Default is &quot;1se&quot;.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_n_k">n_K</code></td>
<td>
<p>The number of candidate values for the number of basis functions for B-splines. Default is 4.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_n_lambda1">n_lambda1</code></td>
<td>
<p>The number of candidate values for the regularization parameter in the initial cross-validation step. Default is 10.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_n_lambda2">n_lambda2</code></td>
<td>
<p>The number of candidate values for the regularization parameter in the second stage of cross-validation
(once the optimal number of basis function K is decided, a second stage of cross-validation for the regularization parameter
is performed on a finer grid). Default is 20.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_q_scale">Q_scale</code></td>
<td>
<p>Should data be scaled to estimate the spectral transformation? 
Default is <code>TRUE</code> to not reduce the signal of high variance covariates.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_ind_lin">ind_lin</code></td>
<td>
<p>A vector of indices specifying which covariates to model linearly (i.e. not expanded into basis function).
Default is 'NULL'.</p>
</td></tr>
<tr><td><code id="SDAM_+3A_mc.cores">mc.cores</code></td>
<td>
<p>Number of cores to use for parallel processing, if <code>mc.cores &gt; 1</code>
the cross validation is parallelized. Default is '1'. (only supported for unix)</p>
</td></tr>
<tr><td><code id="SDAM_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code> fitting information is shown.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class 'SDAM' containing the following elements:
</p>
<table role = "presentation">
<tr><td><code>X</code></td>
<td>
<p>The original design matrix.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>The number of covariates in 'X'.</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>The intercept term of the fitted model.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>A vector of the number of basis functions for each covariate,
where 1 corresponds to a linear term. The entries of the vector will mostly by
the same, but some entries might be lower if the corresponding component of
X contains only few unique values.</p>
</td></tr>
<tr><td><code>breaks</code></td>
<td>
<p>A list of breakpoints used for the B-splines. Used to reconstruct the B-spline basis functions.</p>
</td></tr>
<tr><td><code>coefs</code></td>
<td>
<p>A list of coefficients for the B-spline basis functions for each component.</p>
</td></tr>
<tr><td><code>active</code></td>
<td>
<p>A vector of active covariates that contribute to the model.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cyrill Scheidegger
</p>


<h3>References</h3>

<p>Guo Z, Ćevid D, Bühlmann P (2022).
&ldquo;Doubly debiased lasso: High-dimensional inference under hidden confounding.&rdquo;
<em>The Annals of Statistics</em>, <b>50</b>(3).
ISSN 0090-5364, <a href="https://doi.org/10.1214/21-AOS2152">doi:10.1214/21-AOS2152</a>.<br /><br /> Paul D, Bair E, Hastie T, Tibshirani R (2008).
&ldquo;“Preconditioning” for feature selection and regression in high-dimensional problems.&rdquo;
<em>The Annals of Statistics</em>, <b>36</b>(4).
ISSN 0090-5364, <a href="https://doi.org/10.1214/009053607000000578">doi:10.1214/009053607000000578</a>.<br /><br /> Scheidegger C, Guo Z, Bühlmann P (2025).
&ldquo;Spectral Deconfounding for High-Dimensional Sparse Additive Models.&rdquo;
<em>ACM / IMS J. Data Sci.</em>.
<a href="https://doi.org/10.1145/3711116">doi:10.1145/3711116</a>.<br /><br /> Ćevid D, Bühlmann P, Meinshausen N (2020).
&ldquo;Spectral Deconfounding via Perturbed Sparse Linear Models.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>21</b>(1).
ISSN 1532-4435, <a href="http://jmlr.org/papers/v21/19-545.html">http://jmlr.org/papers/v21/19-545.html</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_Q">get_Q</a></code>, <code><a href="#topic+predict.SDAM">predict.SDAM</a></code>, <code><a href="#topic+varImp.SDAM">varImp.SDAM</a></code>, 
<code><a href="#topic+predict_individual_fj">predict_individual_fj</a></code>, <code><a href="#topic+partDependence">partDependence</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
X &lt;- matrix(rnorm(10 * 5), ncol = 5)
Y &lt;- sin(X[, 1]) -  X[, 2] + rnorm(10)
model &lt;- SDAM(x = X, y = Y, Q_type = "trim", trim_quantile = 0.5, nfold = 2, n_K = 1)



library(HDclassif)
data(wine)
names(wine) &lt;- c("class", "alcohol", "malicAcid", "ash", "alcalinityAsh", "magnesium", 
                 "totPhenols", "flavanoids", "nonFlavPhenols", "proanthocyanins", 
                 "colIntens", "hue", "OD", "proline")
wine &lt;- log(wine)

# estimate model
# do not use class in the model and restrict proline to be linear 
model &lt;- SDAM(alcohol ~ -class + ., wine, ind_lin = "proline", nfold = 3)

# extract variable importance
varImp(model)

# most important variable
mostImp &lt;- names(which.max(varImp(model)))
mostImp

# predict for individual Xj
predJ &lt;- predict_individual_fj(object = model, j = mostImp)
plot(wine[, mostImp], predJ, 
     xlab = paste0("log ", mostImp), ylab = "log alcohol")

# partial dependece
plot(partDependence(model, mostImp))

# predict 
predict(model, newdata = wine[42, ])

## alternative function call
mod_none &lt;- SDAM(x = as.matrix(wine[1:10, -c(1, 2)]), y = wine$alcohol[1:10], 
                 Q_type = "no_deconfounding", nfolds = 2, n_K = 4, 
                 n_lambda1 = 4, n_lambda2 = 8)


</code></pre>

<hr>
<h2 id='SDForest'>Spectrally Deconfounded Random Forests</h2><span id='topic+SDForest'></span>

<h3>Description</h3>

<p>Estimate regression Random Forest using spectral deconfounding.
The spectrally deconfounded Random Forest (SDForest) combines SDTrees in the same way, 
as in the original Random Forest (Breiman 2001).
The idea is to combine multiple regression trees into an ensemble in order to 
decrease variance and get a smooth function. Ensembles work best if the different 
models are independent of each other. To decorrelate the regression trees as much 
as possible from each other, we have two mechanisms. The first one is bagging 
(Breiman 1996), where we train each regression 
tree on an independent bootstrap sample of the observations, e.g., we draw a 
random sample of size <code class="reqn">n</code> with replacement from the observations. 
The second mechanic to decrease the correlation is that only a random subset 
of the covariates is available for each split. Before each split, 
we sample <code class="reqn">\text{mtry} \leq p</code> from all the covariates and choose the one 
that reduces the loss the most only from those.
</p>
<p style="text-align: center;"><code class="reqn">\widehat{f(X)} = \frac{1}{N_{tree}} \sum_{t = 1}^{N_{tree}} SDTree_t(X)</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>SDForest(
  formula = NULL,
  data = NULL,
  x = NULL,
  y = NULL,
  nTree = 100,
  cp = 0,
  min_sample = 5,
  mtry = NULL,
  mc.cores = 1,
  Q_type = "trim",
  trim_quantile = 0.5,
  q_hat = 0,
  Qf = NULL,
  A = NULL,
  gamma = 7,
  max_size = NULL,
  gpu = FALSE,
  return_data = TRUE,
  mem_size = 1e+07,
  leave_out_ind = NULL,
  envs = NULL,
  nTree_leave_out = NULL,
  nTree_env = NULL,
  max_candidates = 100,
  Q_scale = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SDForest_+3A_formula">formula</code></td>
<td>
<p>Object of class <code>formula</code> or describing the model to fit 
of the form <code>y ~ x1 + x2 + ...</code> where <code>y</code> is a numeric response and 
<code>x1, x2, ...</code> are vectors of covariates. Interactions are not supported.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code> containing the variables in the model.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_x">x</code></td>
<td>
<p>Matrix of covariates, alternative to <code>formula</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_y">y</code></td>
<td>
<p>Vector of responses, alternative to <code>formula</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_ntree">nTree</code></td>
<td>
<p>Number of trees to grow.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_cp">cp</code></td>
<td>
<p>Complexity parameter, minimum loss decrease to split a node. 
A split is only performed if the loss decrease is larger than <code>cp * initial_loss</code>, 
where <code>initial_loss</code> is the loss of the initial estimate using only a stump.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_min_sample">min_sample</code></td>
<td>
<p>Minimum number of observations per leaf. 
A split is only performed if both resulting leaves have at least 
<code>min_sample</code> observations.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_mtry">mtry</code></td>
<td>
<p>Number of randomly selected covariates to consider for a split, 
if <code>NULL</code> half of the covariates are available for each split. 
<code class="reqn">\text{mtry} = \lfloor \frac{p}{2} \rfloor</code></p>
</td></tr>
<tr><td><code id="SDForest_+3A_mc.cores">mc.cores</code></td>
<td>
<p>Number of cores to use for parallel processing,
if <code>mc.cores &gt; 1</code> the trees are estimated in parallel.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_q_type">Q_type</code></td>
<td>
<p>Type of deconfounding, one of 'trim', 'pca', 'no_deconfounding'. 
'trim' corresponds to the Trim transform (Ćevid et al. 2020) 
as implemented in the Doubly debiased lasso (Guo et al. 2022), 
'pca' to the PCA transformation(Paul et al. 2008). 
See <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_trim_quantile">trim_quantile</code></td>
<td>
<p>Quantile for Trim transform, 
only needed for trim, see <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_q_hat">q_hat</code></td>
<td>
<p>Assumed confounding dimension, only needed for pca, 
see <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_qf">Qf</code></td>
<td>
<p>Spectral transformation, if <code>NULL</code> 
it is internally estimated using <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_a">A</code></td>
<td>
<p>Numerical Anchor of class <code>matrix</code>. See <code><a href="#topic+get_W">get_W</a></code>.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_gamma">gamma</code></td>
<td>
<p>Strength of distributional robustness, <code class="reqn">\gamma \in [0, \infty]</code>. 
See <code><a href="#topic+get_W">get_W</a></code>.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_max_size">max_size</code></td>
<td>
<p>Maximum number of observations used for a bootstrap sample.
If <code>NULL</code> n samples with replacement are drawn.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_gpu">gpu</code></td>
<td>
<p>If <code>TRUE</code>, the calculations are performed on the GPU. 
If it is properly set up.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_return_data">return_data</code></td>
<td>
<p>If <code>TRUE</code>, the training data is returned in the output.
This is needed for <code><a href="#topic+prune.SDForest">prune.SDForest</a></code>, <code><a href="#topic+regPath.SDForest">regPath.SDForest</a></code>, 
and for <code><a href="#topic+mergeForest">mergeForest</a></code>.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_mem_size">mem_size</code></td>
<td>
<p>Amount of split candidates that can be evaluated at once.
This is a trade-off between memory and speed can be decreased if either
the memory is not sufficient or the gpu is to small.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_leave_out_ind">leave_out_ind</code></td>
<td>
<p>Indices of observations that should not be used for training.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_envs">envs</code></td>
<td>
<p>Vector of environments of class <code>factor</code> 
which can be used for stratified tree fitting.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_ntree_leave_out">nTree_leave_out</code></td>
<td>
<p>Number of trees that should be estimated while leaving
one of the environments out. Results in number of environments times number of trees.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_ntree_env">nTree_env</code></td>
<td>
<p>Number of trees that should be estimated for each environment.
Results in number of environments times number of trees.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_max_candidates">max_candidates</code></td>
<td>
<p>Maximum number of split points that are 
proposed at each node for each covariate.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_q_scale">Q_scale</code></td>
<td>
<p>Should data be scaled to estimate the spectral transformation? 
Default is <code>TRUE</code> to not reduce the signal of high variance covariates, 
and we do not know of a scenario where this hurts.</p>
</td></tr>
<tr><td><code id="SDForest_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code> fitting information is shown.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>SDForest</code> containing:
</p>
<table role = "presentation">
<tr><td><code>predictions</code></td>
<td>
<p>Vector of predictions for each observation.</p>
</td></tr>
<tr><td><code>forest</code></td>
<td>
<p>List of SDTree objects.</p>
</td></tr>
<tr><td><code>var_names</code></td>
<td>
<p>Names of the covariates.</p>
</td></tr>
<tr><td><code>oob_loss</code></td>
<td>
<p>Out-of-bag loss. MSE</p>
</td></tr>
<tr><td><code>oob_SDloss</code></td>
<td>
<p>Out-of-bag loss using the spectral transformation.</p>
</td></tr>
<tr><td><code>var_importance</code></td>
<td>
<p>Variable importance.
The variable importance is calculated as the sum of the decrease in the loss function 
resulting from all splits that use a covariate for each tree. 
The mean of the variable importance of all trees results in the variable importance for the forest.</p>
</td></tr>
<tr><td><code>oob_ind</code></td>
<td>
<p>List of indices of trees that did not contain the observation in the training set.</p>
</td></tr>
<tr><td><code>oob_predictions</code></td>
<td>
<p>Out-of-bag predictions.</p>
</td></tr>
</table>
<p>If <code>return_data</code> is <code>TRUE</code> the following are also returned:
</p>
<table role = "presentation">
<tr><td><code>X</code></td>
<td>
<p>Matrix of covariates.</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>Vector of responses.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>Spectral transformation.</p>
</td></tr>
</table>
<p>If <code>envs</code> is provided the following are also returned:
</p>
<table role = "presentation">
<tr><td><code>envs</code></td>
<td>
<p>Vector of environments.</p>
</td></tr>
<tr><td><code>nTree_env</code></td>
<td>
<p>Number of trees for each environment.</p>
</td></tr>
<tr><td><code>ooEnv_ind</code></td>
<td>
<p>List of indices of trees that did not contain the observation or the same environment in the training set
for each observation.</p>
</td></tr>
<tr><td><code>ooEnv_loss</code></td>
<td>
<p>Out-of-bag loss using only trees that did not contain the observation or the same environment.</p>
</td></tr>
<tr><td><code>ooEnv_SDloss</code></td>
<td>
<p>Out-of-bag loss using the spectral transformation and only trees that did not contain the observation
or the same environment.</p>
</td></tr>
<tr><td><code>ooEnv_predictions</code></td>
<td>
<p>Out-of-bag predictions using only trees that did not contain the observation or the same environment.</p>
</td></tr>
<tr><td><code>nTree_leave_out</code></td>
<td>
<p>If environments are left out, the environment for each tree, that was left out.</p>
</td></tr>
<tr><td><code>nTree_env</code></td>
<td>
<p>If environments are provided, the environment each tree is trained with.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Breiman L (1996).
&ldquo;Bagging predictors.&rdquo;
<em>Machine Learning</em>, <b>24</b>(2), 123&ndash;140.
ISSN 0885-6125, <a href="https://doi.org/10.1007/BF00058655">doi:10.1007/BF00058655</a>.<br /><br /> Breiman L (2001).
&ldquo;Random Forests.&rdquo;
<em>Machine Learning</em>, <b>45</b>(1), 5&ndash;32.
ISSN 08856125, <a href="https://doi.org/10.1023/A%3A1010933404324">doi:10.1023/A:1010933404324</a>.<br /><br /> Guo Z, Ćevid D, Bühlmann P (2022).
&ldquo;Doubly debiased lasso: High-dimensional inference under hidden confounding.&rdquo;
<em>The Annals of Statistics</em>, <b>50</b>(3).
ISSN 0090-5364, <a href="https://doi.org/10.1214/21-AOS2152">doi:10.1214/21-AOS2152</a>.<br /><br /> Paul D, Bair E, Hastie T, Tibshirani R (2008).
&ldquo;“Preconditioning” for feature selection and regression in high-dimensional problems.&rdquo;
<em>The Annals of Statistics</em>, <b>36</b>(4).
ISSN 0090-5364, <a href="https://doi.org/10.1214/009053607000000578">doi:10.1214/009053607000000578</a>.<br /><br /> Ćevid D, Bühlmann P, Meinshausen N (2020).
&ldquo;Spectral Deconfounding via Perturbed Sparse Linear Models.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>21</b>(1).
ISSN 1532-4435, <a href="http://jmlr.org/papers/v21/19-545.html">http://jmlr.org/papers/v21/19-545.html</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_Q">get_Q</a></code>, <code><a href="#topic+get_W">get_W</a></code>, <code><a href="#topic+SDTree">SDTree</a></code>, 
<code><a href="#topic+simulate_data_nonlinear">simulate_data_nonlinear</a></code>, <code><a href="#topic+regPath">regPath</a></code>, 
<code><a href="#topic+stabilitySelection">stabilitySelection</a></code>, <code><a href="#topic+prune">prune</a></code>, <code><a href="#topic+partDependence">partDependence</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 50
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', nTree = 5, cp = 0.5)
predict(model, newdata = data.frame(X))


set.seed(42)
# simulation of confounded data
sim_data &lt;- simulate_data_nonlinear(q = 2, p = 150, n = 100, m = 2)
X &lt;- sim_data$X
Y &lt;- sim_data$Y
train_data &lt;- data.frame(X, Y)
# causal parents of y
sim_data$j

# comparison to classical random forest
fit_ranger &lt;- ranger::ranger(Y ~ ., train_data, importance = 'impurity')

fit &lt;- SDForest(x = X, y = Y, nTree = 10, Q_type = 'pca', q_hat = 2)
fit &lt;- SDForest(Y ~ ., nTree = 10, train_data)
fit

# comparison of variable importance
imp_ranger &lt;- fit_ranger$variable.importance
imp_sdf &lt;- fit$var_importance
imp_col &lt;- rep('black', length(imp_ranger))
imp_col[sim_data$j] &lt;- 'red'

plot(imp_ranger, imp_sdf, col = imp_col, pch = 20,
     xlab = 'ranger', ylab = 'SDForest', 
     main = 'Variable Importance')

# check regularization path of variable importance
path &lt;- regPath(fit)
# out of bag error for different regularization
plotOOB(path)
plot(path)

# detection of causal parent using stability selection
stablePath &lt;- stabilitySelection(fit)
plot(stablePath)

# pruning of forest according to optimal out-of-bag performance
fit &lt;- prune(fit, cp = path$cp_min)

# partial functional dependence of y on the most important covariate
most_imp &lt;- which.max(fit$var_importance)
dep &lt;- partDependence(fit, most_imp)
plot(dep, n_examples = 100)

</code></pre>

<hr>
<h2 id='SDTree'>Spectrally Deconfounded Tree</h2><span id='topic+SDTree'></span>

<h3>Description</h3>

<p>Estimates a regression tree using spectral deconfounding. 
A regression tree is part of the function class of step functions
<code class="reqn">f(X) = \sum_{m = 1}^M 1_{\{X \in R_m\}} c_m</code>, where (<code class="reqn">R_m</code>) with 
<code class="reqn">m = 1, \ldots, M</code> are regions dividing the space of <code class="reqn">\mathbb{R}^p</code> 
into <code class="reqn">M</code> rectangular parts. Each region has response level <code class="reqn">c_m \in \mathbb{R}</code>.
For the training data, we can write the step function as <code class="reqn">f(\mathbf{X}) = \mathcal{P} c</code> 
where <code class="reqn">\mathcal{P} \in \{0, 1\}^{n \times M}</code> is an indicator matrix encoding 
to which region an observation belongs and <code class="reqn">c \in \mathbb{R}^M</code> is a vector 
containing the levels corresponding to the different regions. This function then minimizes
</p>
<p style="text-align: center;"><code class="reqn">(\hat{\mathcal{P}}, \hat{c}) = \text{argmin}_{\mathcal{P}' \in \{0, 1\}^{n \times M}, c' \in \mathbb{R}^ {M}} \frac{||Q(\mathbf{Y} - \mathcal{P'} c')||_2^2}{n}</code>
</p>

<p>We find <code class="reqn">\hat{\mathcal{P}}</code> by using the tree structure and repeated splitting of the leaves, 
similar to the original cart algorithm (Breiman et al. 2017).
Since comparing all possibilities for <code class="reqn">\mathcal{P}</code> is impossible, we let a tree grow greedily. 
Given the current tree, we iterate over all leaves and all possible splits. 
We choose the one that reduces the spectral loss the most and estimate after each split 
all the leave estimates
<code class="reqn">\hat{c} = \text{argmin}_{c' \in \mathbb{R}^M} \frac{||Q\mathbf{Y} - Q\mathcal{P} c'||_2^2}{n}</code> 
which is just a linear regression problem. This is repeated until the loss decreases 
less than a minimum loss decrease after a split. 
The minimum loss decrease equals a cost-complexity parameter <code class="reqn">cp</code> times 
the initial loss when only an overall mean is estimated. 
The cost-complexity parameter <code class="reqn">cp</code> controls the complexity of a regression tree 
and acts as a regularization parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SDTree(
  formula = NULL,
  data = NULL,
  x = NULL,
  y = NULL,
  max_leaves = NULL,
  cp = 0.01,
  min_sample = 5,
  mtry = NULL,
  fast = TRUE,
  Q_type = "trim",
  trim_quantile = 0.5,
  q_hat = 0,
  Qf = NULL,
  A = NULL,
  gamma = 0.5,
  gpu = FALSE,
  mem_size = 1e+07,
  max_candidates = 100,
  Q_scale = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SDTree_+3A_formula">formula</code></td>
<td>
<p>Object of class <code>formula</code> or describing the model to fit 
of the form <code>y ~ x1 + x2 + ...</code> where <code>y</code> is a numeric response and 
<code>x1, x2, ...</code> are vectors of covariates. Interactions are not supported.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code> containing the variables in the model.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_x">x</code></td>
<td>
<p>Matrix of covariates, alternative to <code>formula</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_y">y</code></td>
<td>
<p>Vector of responses, alternative to <code>formula</code> and <code>data</code>.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_max_leaves">max_leaves</code></td>
<td>
<p>Maximum number of leaves for the grown tree.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_cp">cp</code></td>
<td>
<p>Complexity parameter, minimum loss decrease to split a node. 
A split is only performed if the loss decrease is larger than <code>cp * initial_loss</code>, 
where <code>initial_loss</code> is the loss of the initial estimate using only a stump.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_min_sample">min_sample</code></td>
<td>
<p>Minimum number of observations per leaf. 
A split is only performed if both resulting leaves have at least 
<code>min_sample</code> observations.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_mtry">mtry</code></td>
<td>
<p>Number of randomly selected covariates to consider for a split, 
if <code>NULL</code> all covariates are available for each split.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_fast">fast</code></td>
<td>
<p>If <code>TRUE</code>, only the optimal splits in the new leaves are 
evaluated and the previously optimal splits and their potential loss-decrease are reused. 
If <code>FALSE</code> all possible splits in all the leaves are reevaluated after every split.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_q_type">Q_type</code></td>
<td>
<p>Type of deconfounding, one of 'trim', 'pca', 'no_deconfounding'. 
'trim' corresponds to the Trim transform (Ćevid et al. 2020) 
as implemented in the Doubly debiased lasso (Guo et al. 2022), 
'pca' to the PCA transformation(Paul et al. 2008). 
See <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_trim_quantile">trim_quantile</code></td>
<td>
<p>Quantile for Trim transform, 
only needed for trim, see <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_q_hat">q_hat</code></td>
<td>
<p>Assumed confounding dimension, only needed for pca, 
see <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_qf">Qf</code></td>
<td>
<p>Spectral transformation, if <code>NULL</code> 
it is internally estimated using <code><a href="#topic+get_Q">get_Q</a></code>.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_a">A</code></td>
<td>
<p>Numerical Anchor of class <code>matrix</code>. See <code><a href="#topic+get_W">get_W</a></code>.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_gamma">gamma</code></td>
<td>
<p>Strength of distributional robustness, <code class="reqn">\gamma \in [0, \infty]</code>. 
See <code><a href="#topic+get_W">get_W</a></code>.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_gpu">gpu</code></td>
<td>
<p>If <code>TRUE</code>, the calculations are performed on the GPU. 
If it is properly set up.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_mem_size">mem_size</code></td>
<td>
<p>Amount of split candidates that can be evaluated at once.
This is a trade-off between memory and speed can be decreased if either
the memory is not sufficient or the gpu is to small.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_max_candidates">max_candidates</code></td>
<td>
<p>Maximum number of split points that are 
proposed at each node for each covariate.</p>
</td></tr>
<tr><td><code id="SDTree_+3A_q_scale">Q_scale</code></td>
<td>
<p>Should data be scaled to estimate the spectral transformation? 
Default is <code>TRUE</code> to not reduce the signal of high variance covariates, 
and we do not know of a scenario where this hurts.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>SDTree</code> containing
</p>
<table role = "presentation">
<tr><td><code>predictions</code></td>
<td>
<p>Predictions for the training set.</p>
</td></tr>
<tr><td><code>tree</code></td>
<td>
<p>The estimated tree of class <code>Node</code> from (Glur 2023). 
The tree contains the information about all the splits and the resulting estimates.</p>
</td></tr>
<tr><td><code>var_names</code></td>
<td>
<p>Names of the covariates in the training data.</p>
</td></tr>
<tr><td><code>var_importance</code></td>
<td>
<p>Variable importance of the covariates. 
The variable importance is calculated as the sum of the decrease in the loss 
function resulting from all splits that use this covariate.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Breiman L, Friedman JH, Olshen RA, Stone CJ (2017).
<em>Classification And Regression Trees</em>.
Routledge.
ISBN 9781315139470, <a href="https://doi.org/10.1201/9781315139470">doi:10.1201/9781315139470</a>.<br /><br /> Glur C (2023).
&ldquo;data.tree: General Purpose Hierarchical Data Structure.&rdquo;
<a href="https://CRAN.R-project.org/package=data.tree">https://CRAN.R-project.org/package=data.tree</a>.<br /><br /> Guo Z, Ćevid D, Bühlmann P (2022).
&ldquo;Doubly debiased lasso: High-dimensional inference under hidden confounding.&rdquo;
<em>The Annals of Statistics</em>, <b>50</b>(3).
ISSN 0090-5364, <a href="https://doi.org/10.1214/21-AOS2152">doi:10.1214/21-AOS2152</a>.<br /><br /> Paul D, Bair E, Hastie T, Tibshirani R (2008).
&ldquo;“Preconditioning” for feature selection and regression in high-dimensional problems.&rdquo;
<em>The Annals of Statistics</em>, <b>36</b>(4).
ISSN 0090-5364, <a href="https://doi.org/10.1214/009053607000000578">doi:10.1214/009053607000000578</a>.<br /><br /> Ćevid D, Bühlmann P, Meinshausen N (2020).
&ldquo;Spectral Deconfounding via Perturbed Sparse Linear Models.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>21</b>(1).
ISSN 1532-4435, <a href="http://jmlr.org/papers/v21/19-545.html">http://jmlr.org/papers/v21/19-545.html</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+simulate_data_nonlinear">simulate_data_nonlinear</a></code>, <code><a href="#topic+regPath.SDTree">regPath.SDTree</a></code>, 
<code><a href="#topic+prune.SDTree">prune.SDTree</a></code>, <code><a href="#topic+partDependence">partDependence</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDTree(x = X, y = y, cp = 0.5)


set.seed(42)
# simulation of confounded data
sim_data &lt;- simulate_data_step(q = 2, p = 15, n = 100, m = 2)
X &lt;- sim_data$X
Y &lt;- sim_data$Y
train_data &lt;- data.frame(X, Y)
# causal parents of y
sim_data$j

tree_plain_cv &lt;- cvSDTree(Y ~ ., train_data, Q_type = "no_deconfounding")
tree_plain &lt;- SDTree(Y ~ ., train_data, Q_type = "no_deconfounding", cp = 0)

tree_causal_cv &lt;- cvSDTree(Y ~ ., train_data)
tree_causal &lt;- SDTree(y = Y, x = X, cp = 0)

# check regularization path of variable importance
path &lt;- regPath(tree_causal)
plot(path)

tree_plain &lt;- prune(tree_plain, cp = tree_plain_cv$cp_min)
tree_causal &lt;- prune(tree_causal, cp = tree_causal_cv$cp_min)
plot(tree_causal)
plot(tree_plain)

</code></pre>

<hr>
<h2 id='simulate_data_nonlinear'>Simulate data with linear confounding and non-linear causal effect</h2><span id='topic+simulate_data_nonlinear'></span>

<h3>Description</h3>

<p>Simulation of data from a confounded non-linear model. 
The data generating process is given by:
</p>
<p style="text-align: center;"><code class="reqn">Y = f(X) + \delta^T H  + \nu</code>
</p>

<p style="text-align: center;"><code class="reqn">X = \Gamma^T H + E</code>
</p>

<p>where <code class="reqn">f(X)</code> is a random function on the fourier basis
with a subset of size m covariates <code class="reqn">X_j</code> having a causal effect on <code class="reqn">Y</code>.
</p>
<p style="text-align: center;"><code class="reqn">f(x_i) = \sum_{j = 1}^p 1_{j \in js} \sum_{k = 1}^K (\beta_{j, k}^{(1)} \cos(0.2 k x_j) + 
\beta_{j, k}^{(2)} \sin(0.2 k x_j))</code>
</p>

<p><code class="reqn">E</code>, <code class="reqn">\nu</code> are random error terms and 
<code class="reqn">H \in \mathbb{R}^{n \times q}</code> is a matrix of random confounding covariates.
<code class="reqn">\Gamma \in \mathbb{R}^{q \times p}</code> and <code class="reqn">\delta \in \mathbb{R}^{q}</code> are random coefficient vectors.
For the simulation, all the above parameters are drawn from a standard normal distribution, except for 
<code class="reqn">\nu</code> which is drawn from a normal distribution with standard deviation 0.1.
The parameters <code class="reqn">\beta</code> are drawn from a uniform distribution between -1 and 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_data_nonlinear(q, p, n, m, K = 2, eff = NULL, fixEff = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simulate_data_nonlinear_+3A_q">q</code></td>
<td>
<p>number of confounding covariates in H</p>
</td></tr>
<tr><td><code id="simulate_data_nonlinear_+3A_p">p</code></td>
<td>
<p>number of covariates in X</p>
</td></tr>
<tr><td><code id="simulate_data_nonlinear_+3A_n">n</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code id="simulate_data_nonlinear_+3A_m">m</code></td>
<td>
<p>number of covariates with a causal effect on Y</p>
</td></tr>
<tr><td><code id="simulate_data_nonlinear_+3A_k">K</code></td>
<td>
<p>number of fourier basis functions K <code class="reqn">K \in \mathbb{N}</code>, e.g. complexity of causal function</p>
</td></tr>
<tr><td><code id="simulate_data_nonlinear_+3A_eff">eff</code></td>
<td>
<p>the number of affected covariates in X by the confounding, if NULL all covariates are affected</p>
</td></tr>
<tr><td><code id="simulate_data_nonlinear_+3A_fixeff">fixEff</code></td>
<td>
<p>if eff is smaller than p: If fixEff = TRUE, the causal parents 
are always affected by confounding if fixEff = FALSE, affected covariates are chosen completely at random.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the simulated data:
</p>
<table role = "presentation">
<tr><td><code>X</code></td>
<td>
<p>a matrix of covariates</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>a vector of responses</p>
</td></tr>
<tr><td><code>f_X</code></td>
<td>
<p>a vector of the true function f(X)</p>
</td></tr>
<tr><td><code>j</code></td>
<td>
<p>the indices of the causal covariates in X</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>the parameter vector for the function f(X), see <code><a href="#topic+f_four">f_four</a></code></p>
</td></tr>
<tr><td><code>H</code></td>
<td>
<p>the matrix of confounding covariates</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+f_four">f_four</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
# simulation of confounded data
sim_data &lt;- simulate_data_nonlinear(q = 2, p = 150, n = 100, m = 2)
X &lt;- sim_data$X
Y &lt;- sim_data$Y

</code></pre>

<hr>
<h2 id='simulate_data_step'>Simulate data with linear confounding and causal effect following a step-function</h2><span id='topic+simulate_data_step'></span>

<h3>Description</h3>

<p>Simulation of data from a confounded non-linear model. Where the non-linear function is a random regression tree.
The data generating process is given by:
</p>
<p style="text-align: center;"><code class="reqn">Y = f(X) + \delta^T H + \nu</code>
</p>

<p style="text-align: center;"><code class="reqn">X = \Gamma^T H + E</code>
</p>

<p>where <code class="reqn">f(X)</code> is a random regression tree with <code class="reqn">m</code> random splits of the data. 
Resulting in a random step-function with <code class="reqn">m+1</code> levels, i.e. leaf-levels.
</p>
<p style="text-align: center;"><code class="reqn">f(x_i) = \sum_{k = 1}^K 1_{\{x_i \in R_k\}} c_k</code>
</p>

<p><code class="reqn">E</code>, <code class="reqn">\nu</code> are random error terms and 
<code class="reqn">H \in \mathbb{R}^{n \times q}</code> is a matrix of random confounding covariates.
<code class="reqn">\Gamma \in \mathbb{R}^{q \times p}</code> and <code class="reqn">\delta \in \mathbb{R}^{q}</code> are random coefficient vectors.
For the simulation, all the above parameters are drawn from a standard normal distribution, except for 
<code class="reqn">\delta</code> which is drawn from a normal distribution with standard deviation 10.
The leaf levels <code class="reqn">c_k</code> are drawn from a uniform distribution between -50 and 50.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_data_step(q, p, n, m, make_tree = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simulate_data_step_+3A_q">q</code></td>
<td>
<p>number of confounding covariates in H</p>
</td></tr>
<tr><td><code id="simulate_data_step_+3A_p">p</code></td>
<td>
<p>number of covariates in X</p>
</td></tr>
<tr><td><code id="simulate_data_step_+3A_n">n</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code id="simulate_data_step_+3A_m">m</code></td>
<td>
<p>number of covariates with a causal effect on Y</p>
</td></tr>
<tr><td><code id="simulate_data_step_+3A_make_tree">make_tree</code></td>
<td>
<p>Whether the random regression tree should be returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the simulated data:
</p>
<table role = "presentation">
<tr><td><code>X</code></td>
<td>
<p>a <code>matrix</code> of covariates</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>a <code>vector</code> of responses</p>
</td></tr>
<tr><td><code>f_X</code></td>
<td>
<p>a <code>vector</code> of the true function f(X)</p>
</td></tr>
<tr><td><code>j</code></td>
<td>
<p>the indices of the causal covariates in X</p>
</td></tr>
<tr><td><code>tree</code></td>
<td>
<p>If <code>make_tree</code>, the random regression tree of class 
<code>Node</code> from (Glur 2023)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Glur C (2023).
&ldquo;data.tree: General Purpose Hierarchical Data Structure.&rdquo;
<a href="https://CRAN.R-project.org/package=data.tree">https://CRAN.R-project.org/package=data.tree</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+simulate_data_nonlinear">simulate_data_nonlinear</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
# simulation of confounded data
sim_data &lt;- simulate_data_step(q = 2, p = 15, n = 100, m = 2)
X &lt;- sim_data$X
Y &lt;- sim_data$Y
</code></pre>

<hr>
<h2 id='stabilitySelection.SDForest'>Calculate the stability selection of an SDForest</h2><span id='topic+stabilitySelection.SDForest'></span><span id='topic+stabilitySelection'></span>

<h3>Description</h3>

<p>This function calculates the stability selection of an SDForest
(Meinshausen and Bühlmann 2010).
Stability selection is calculated as the fraction of trees in the forest
that select a variable for a split at each complexity parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDForest'
stabilitySelection(object, cp_seq = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stabilitySelection.SDForest_+3A_object">object</code></td>
<td>
<p>an SDForest object</p>
</td></tr>
<tr><td><code id="stabilitySelection.SDForest_+3A_cp_seq">cp_seq</code></td>
<td>
<p>A sequence of complexity parameters.
If NULL, the sequence is calculated automatically using only relevant values.</p>
</td></tr>
<tr><td><code id="stabilitySelection.SDForest_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>paths</code> containing
</p>
<table role = "presentation">
<tr><td><code>cp</code></td>
<td>
<p>The sequence of complexity parameters.</p>
</td></tr>
<tr><td><code>varImp_path</code></td>
<td>
<p>A <code>matrix</code> with the stability selection
for each complexity parameter.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Path type</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Meinshausen N, Bühlmann P (2010).
&ldquo;Stability Selection.&rdquo;
<em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, <b>72</b>(4), 417&ndash;473.
ISSN 1369-7412, <a href="https://doi.org/10.1111/j.1467-9868.2010.00740.x">doi:10.1111/j.1467-9868.2010.00740.x</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.paths">plot.paths</a></code> <code><a href="#topic+regPath">regPath</a></code> <code><a href="#topic+prune">prune</a></code> <code><a href="#topic+get_cp_seq">get_cp_seq</a></code> <code><a href="#topic+SDForest">SDForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + sign(X[, 2]) + rnorm(n)
model &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', nTree = 2, cp = 0.5)
paths &lt;- stabilitySelection(model)
plot(paths)

plot(paths, plotly = TRUE)

</code></pre>

<hr>
<h2 id='toList.SDForest'>SDForest toList method</h2><span id='topic+toList.SDForest'></span><span id='topic+toList'></span>

<h3>Description</h3>

<p>Converts the trees in an SDForest object from
class <code>Node</code> (Glur 2023) to class <code>list</code>.
This makes it substantially easier to save the forest to disk.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDForest'
toList(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="toList.SDForest_+3A_object">object</code></td>
<td>
<p>an SDForest object with the trees in Node format</p>
</td></tr>
<tr><td><code id="toList.SDForest_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an SDForest object with the trees in list format
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Glur C (2023).
&ldquo;data.tree: General Purpose Hierarchical Data Structure.&rdquo;
<a href="https://CRAN.R-project.org/package=data.tree">https://CRAN.R-project.org/package=data.tree</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fromList">fromList</a></code> <code><a href="#topic+toList.SDTree">toList.SDTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDForest(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5, nTree = 2)
toList(model)
</code></pre>

<hr>
<h2 id='toList.SDTree'>SDTree toList method</h2><span id='topic+toList.SDTree'></span>

<h3>Description</h3>

<p>Converts the tree in an SDTree object from 
class <code>Node</code> (Glur 2023) to class <code>list</code>.
This makes it substantially easier to save the tree to disk.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDTree'
toList(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="toList.SDTree_+3A_object">object</code></td>
<td>
<p>an SDTree object with the tree in Node format</p>
</td></tr>
<tr><td><code id="toList.SDTree_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an SDTree object with the tree in list format
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>References</h3>

<p>Glur C (2023).
&ldquo;data.tree: General Purpose Hierarchical Data Structure.&rdquo;
<a href="https://CRAN.R-project.org/package=data.tree">https://CRAN.R-project.org/package=data.tree</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fromList">fromList</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 10
X &lt;- matrix(rnorm(n * 5), nrow = n)
y &lt;- sign(X[, 1]) * 3 + rnorm(n)
model &lt;- SDTree(x = X, y = y, Q_type = 'no_deconfounding', cp = 0.5)
toList(model)
</code></pre>

<hr>
<h2 id='varImp.SDAM'>Extract Variable importance for SDAM</h2><span id='topic+varImp.SDAM'></span>

<h3>Description</h3>

<p>This function extracts the variable importance of an SDAM.
The variable importance is calculated as the empirical squared L2 norm of fj. 
The measure is not standardized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDAM'
varImp(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varImp.SDAM_+3A_object">object</code></td>
<td>
<p>an <code>SDAM</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of variable importance
</p>


<h3>Author(s)</h3>

<p>Cyrill Scheidegger
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDAM">SDAM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
X &lt;- matrix(rnorm(10 * 5), ncol = 5)
Y &lt;- sin(X[, 1]) -  X[, 2] + rnorm(10)
model &lt;- SDAM(x = X, y = Y, Q_type = "trim", trim_quantile = 0.5, nfold = 2)
varImp(model)
</code></pre>

<hr>
<h2 id='varImp.SDForest'>Extract variable importance of an SDForest</h2><span id='topic+varImp.SDForest'></span><span id='topic+varImp'></span>

<h3>Description</h3>

<p>This function extracts the variable importance of an SDForest.
The variable importance is calculated as the sum of the decrease in the loss function 
resulting from all splits that use a covariate for each tree. 
The mean of the variable importance of all trees results in the variable importance for the forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDForest'
varImp(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varImp.SDForest_+3A_object">object</code></td>
<td>
<p>an SDForest object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector of variable importance
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+varImp.SDTree">varImp.SDTree</a></code> <code><a href="#topic+SDForest">SDForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
fit &lt;- SDForest(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, 
                 iris, nTree = 10, cp = 0.5)
varImp(fit)
</code></pre>

<hr>
<h2 id='varImp.SDTree'>Extract variable importance of an SDTree</h2><span id='topic+varImp.SDTree'></span>

<h3>Description</h3>

<p>This function extracts the variable importance of an SDTree. 
The variable importance is calculated as the sum of the decrease in the loss 
function resulting from all splits that use this covariate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SDTree'
varImp(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varImp.SDTree_+3A_object">object</code></td>
<td>
<p>an SDTree object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector of variable importance
</p>


<h3>Author(s)</h3>

<p>Markus Ulmer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+varImp.SDForest">varImp.SDForest</a></code> <code><a href="#topic+SDTree">SDTree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
tree &lt;- SDTree(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, iris, cp = 0.5)
varImp(tree)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
