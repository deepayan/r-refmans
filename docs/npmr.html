<!DOCTYPE html><html lang="en"><head><title>Help for package npmr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {npmr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#npmr-package'>
<p>Nuclear penalized multinomial regression</p></a></li>
<li><a href='#cv.npmr'>
<p>Cross-validated nuclear penalized multinomial regression</p></a></li>
<li><a href='#logL'>
<p>Log-likelihood for multinomial regression model</p></a></li>
<li><a href='#npmr'>
<p>Nuclear penalized multinomial regression</p></a></li>
<li><a href='#nuclear'>
<p>Nuclear norm of a matrix</p></a></li>
<li><a href='#objective'>
<p>NPMR objective function</p></a></li>
<li><a href='#objectiveFast'>
<p>Shortcut computation for NPMR objective function</p></a></li>
<li><a href='#PGDnpmr'>
<p>Proximal gradient descent for nuclear penalized multinomial regression</p></a></li>
<li><a href='#plot.cv.npmr'>
<p>Visualize the regression coefficient matrix fit by cross-validated NPMR</p></a></li>
<li><a href='#plot.npmr'>
<p>Visualize the regression coefficient matrix fit by cross-validated NPMR</p></a></li>
<li><a href='#predict.cv.npmr'>
<p>Make predictions from a &ldquo;cv.npmr&rdquo; object</p></a></li>
<li><a href='#predict.npmr'>
<p>Make predictions from a &ldquo;npmr&rdquo; object</p></a></li>
<li><a href='#print.cv.npmr'>
<p>summarize a &quot;cv.npmr&quot; object</p></a></li>
<li><a href='#print.npmr'>
<p>Summarize a &quot;npmr&quot; object</p></a></li>
<li><a href='#prox'>
<p>Proximal operator for nuclear norm</p></a></li>
<li><a href='#Vowel'>
<p>Vowel Recognition</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Nuclear Penalized Multinomial Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-11</td>
</tr>
<tr>
<td>Author:</td>
<td>Scott Powers, Trevor Hastie, Robert Tibshirani</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Scott Powers &lt;saberpowers@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Fit multinomial logistic regression with a penalty on the nuclear
    norm of the estimated regression coefficient matrix, using proximal
    gradient descent.</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-11 17:54:53 UTC; sp161</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-11 18:13:18 UTC</td>
</tr>
</table>
<hr>
<h2 id='npmr-package'>
Nuclear penalized multinomial regression
</h2><span id='topic+npmr-package'></span>

<h3>Description</h3>

<p>As an alternative to an l1- or l2-penalty on multinomial logistic regression,
this package fits multinomial regression with a penalty on the nuclear norm
of the fitted regression coefficient matrix. The result is often a matrix of
reduced rank, leveraging structure among the response classes so that the
likelihood of one class informs the likelihood of other classes. Proximal
gradient descent is used to solve the NPMR optimization problem.
</p>


<h3>Details</h3>

<p>The primary functions in the package are <code><a href="#topic+npmr">npmr</a></code>, which solves
nuclear penalized multinomial regression for a sequence of input values for the
regularization parameter lambda, and <code><a href="#topic+cv.npmr">cv.npmr</a></code>, which chooses the
optimal value of the regularization parameter lambda via cross validation. Both
<code><a href="#topic+npmr">npmr</a></code> and <code><a href="#topic+cv.npmr">cv.npmr</a></code> have predict and plot methods.
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>
<p>Maintainer: Scott Powers &lt;sspowers@stanford.edu&gt;
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#   Fit NPMR to simulated data

K = 5
n = 1000
m = 10000
p = 10
r = 2

# Simulated training data
set.seed(8369)
A = matrix(rnorm(p*r), p, r)
C = matrix(rnorm(K*r), K, r)
B = tcrossprod(A, C)            # low-rank coefficient matrix
X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
eta = X 
P = exp(eta)/rowSums(exp(eta))
Y = t(apply(P, 1, rmultinom, n = 1, size = 1))

# Simulate test data
Xtest = matrix(rnorm(m*p), m, p)
etatest = Xtest 
Ptest = exp(etatest)/rowSums(exp(etatest))
Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))

# Fit NPMR for a sequence of lambda values without CV:
fit2 = npmr(X, Y, lambda = exp(seq(7, -2)))

# Print the NPMR fit:
fit2

# Produce a biplot:
plot(fit2, lambda = 20)

# Compute mean test error using the predict function (for each value of lambda):
getloss = function(pred, Y) {
    -mean(log(rowSums(Y*pred)))
}
apply(predict(fit2, Xtest), 3, getloss, Ytest)
</code></pre>

<hr>
<h2 id='cv.npmr'>
Cross-validated nuclear penalized multinomial regression
</h2><span id='topic+cv.npmr'></span>

<h3>Description</h3>

<p>Divide the training data into folds. Hold out each fold and fit NPMR for a
range of regularization values on the remaining data, testing the result on the
held-out fold. After the optimal value of the regularization parameter is
determined, fit NPMR with this tuning parameter to the whole training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.npmr(X, Y, lambda = exp(seq(7, -2)), s = 0.1/max(X), eps = 1e-06,
    group = NULL, accelerated = TRUE, B.init = NULL, b.init = NULL,
    foldid = NULL, nfolds = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.npmr_+3A_x">X</code></td>
<td>

<p>Covariate matrix. May be in sparse form from <code>Matrix</code> package
</p>
</td></tr>
<tr><td><code id="cv.npmr_+3A_y">Y</code></td>
<td>

<p>Multinomial reponse. May be (1) a vector of length equal to nrow(X), which
will be interpreted as a factor, with levels representing response classes;
or (2) a matrix with nrow(Y) = nrow(X), and each row has exactly one 1
representing the response class for that observation, with the remaining
entries of the row being zero.
</p>
</td></tr>
<tr><td><code id="cv.npmr_+3A_lambda">lambda</code></td>
<td>

<p>Vector of regularization parameter values for penalizing nuclear norm.
Default is a wide range of values. We suggest that the user choose this
sequence via trial and error. If the model takes too long to fit, try
larger values of lambda. If cross validation error is strictly increasing
or strictly decreasing over the range of lambda specified, try extending
the range in the direction of the smallest cross validation error.
</p>
</td></tr>
<tr><td><code id="cv.npmr_+3A_s">s</code></td>
<td>

<p>Step size for proximal gradient descent
</p>
</td></tr>
<tr><td><code id="cv.npmr_+3A_eps">eps</code></td>
<td>

<p>Convergence threshold. When relative change in the objective function after
an interation drops below this threshold, algorithm halts.
</p>
</td></tr>
<tr><td><code id="cv.npmr_+3A_group">group</code></td>
<td>

<p>Vector of length equal to number of variables (ncol(X) and nrow(B)).
Variables in the same group indexed by a POSITIVE integer will be penalized
together (the nuclear norm of the sub-matrix of the regression coefficients
will be penalized). Variables without positive integers will NOT be
penalized. Default is NULL, which means there are no sub-groups; nuclear
norm of entire coefficient matrix is penalized.
</p>
</td></tr>
<tr><td><code id="cv.npmr_+3A_accelerated">accelerated</code></td>
<td>

<p>Logical. Should accelerated proximal gradient descent be used? Default is
TRUE.
</p>
</td></tr>
<tr><td><code id="cv.npmr_+3A_b.init">B.init</code></td>
<td>

<p>Initial value of the regression coefficient matrix for proximal gradient
descent
</p>
</td></tr>
<tr><td><code id="cv.npmr_+3A_b.init">b.init</code></td>
<td>

<p>Initial value of the regression intercept vector for proximal gradient
descent
</p>
</td></tr>
<tr><td><code id="cv.npmr_+3A_foldid">foldid</code></td>
<td>

<p>Vector of length equal to nrow(X). Specifies folds for cross
validation.
</p>
</td></tr>
<tr><td><code id="cv.npmr_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of folds for cross validation. Ignored if <code>foldid</code>
is specified. Default is 10.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &ldquo;cv.npmr&rdquo; with values:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>

<p>the call that produced this object
</p>
</td></tr>
<tr><td><code>error</code></td>
<td>

<p>A vector of total cross validation error for each value of <code>lambda</code>
</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>

<p>An object of class <code><a href="#topic+npmr">npmr</a></code> fitted to the entire training data
using <code>lambda.min</code>
</p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>

<p>The value of lambda with minimum cross validation error
</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>

<p>The input sequence of regularization parameter values for which cross
validation error was calculated
</p>
</td></tr>
<tr><td><code>n</code></td>
<td>

<p>number of rows in the input covariate matrix <code>X</code>
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npmr">npmr</a></code>, <code><a href="#topic+predict.cv.npmr">predict.cv.npmr</a></code>, <code><a href="#topic+print.cv.npmr">print.cv.npmr</a></code>,
<code><a href="#topic+plot.cv.npmr">plot.cv.npmr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#   Fit NPMR to simulated data

K = 5
n = 1000
m = 10000
p = 10
r = 2

# Simulated training data
set.seed(8369)
A = matrix(rnorm(p*r), p, r)
C = matrix(rnorm(K*r), K, r)
B = tcrossprod(A, C)            # low-rank coefficient matrix
X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
eta = X 
P = exp(eta)/rowSums(exp(eta))
Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
fold = sample(rep(1:10, length = nrow(X)))

# Simulate test data
Xtest = matrix(rnorm(m*p), m, p)
etatest = Xtest 
Ptest = exp(etatest)/rowSums(exp(etatest))
Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))

# Fit NPMR for a sequence of lambda values without CV:
fit2 = cv.npmr(X, Y, lambda = exp(seq(7, -2)), foldid = fold)

# Print the NPMR fit:
fit2

# Produce a biplot:
plot(fit2)

# Compute mean test error using the predict function:
-mean(log(rowSums(Ytest*predict(fit2, Xtest))))
</code></pre>

<hr>
<h2 id='logL'>
Log-likelihood for multinomial regression model
</h2><span id='topic+logL'></span>

<h3>Description</h3>

<p>Computes the log-likelihood of the fitted regression parameters given the data
observed. Intended for internal use only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logL(B, b, X, Y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logL_+3A_b">B</code></td>
<td>

<p>Regression coefficient matrix
</p>
</td></tr>
<tr><td><code id="logL_+3A_b">b</code></td>
<td>

<p>Regression intercept vector
</p>
</td></tr>
<tr><td><code id="logL_+3A_x">X</code></td>
<td>

<p>Covariate matrix
</p>
</td></tr>
<tr><td><code id="logL_+3A_y">Y</code></td>
<td>

<p>Multinomial response matrix
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The log-likelihood of B and b given X and Y
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nuclear">nuclear</a></code>, <code><a href="#topic+objective">objective</a></code>
</p>

<hr>
<h2 id='npmr'>
Nuclear penalized multinomial regression
</h2><span id='topic+npmr'></span>

<h3>Description</h3>

<p>Fit a multinomial logistic regression model for a sequence of regularization
parameters penalizing the nuclear norm of the regression coefficient matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npmr(X, Y, lambda = exp(seq(7, -2)), s = 0.1/max(X), eps = 1e-06, group = NULL,
    accelerated = TRUE, B.init = NULL, b.init = NULL, quiet = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="npmr_+3A_x">X</code></td>
<td>

<p>Covariate matrix. May be in sparse form from <code>Matrix</code> package
</p>
</td></tr>
<tr><td><code id="npmr_+3A_y">Y</code></td>
<td>

<p>Multinomial reponse. May be (1) a vector of length equal to nrow(X), which
will be interpreted as a factor, with levels representing response classes;
or (2) a matrix with nrow(Y) = nrow(X), and each row has exactly one 1
representing the response class for that observation, with the remaining
entries of the row being zero.
</p>
</td></tr>
<tr><td><code id="npmr_+3A_lambda">lambda</code></td>
<td>

<p>Vector of regularization parameter values for penalizing nuclear norm.
Default is a wide range of values. We suggest that the user choose this
sequence via trial and error. If the model takes too long to fit, try
larger values of lambda.
</p>
</td></tr>
<tr><td><code id="npmr_+3A_s">s</code></td>
<td>

<p>Step size for proximal gradient descent
</p>
</td></tr>
<tr><td><code id="npmr_+3A_eps">eps</code></td>
<td>

<p>Convergence threshold. When relative change in the objective function after
an interation drops below this threshold, algorithm halts.
</p>
</td></tr>
<tr><td><code id="npmr_+3A_group">group</code></td>
<td>

<p>Vector of length equal to number of variables (ncol(X) and nrow(B)).
Variables in the same group indexed by a POSITIVE integer will be penalized
together (the nuclear norm of the sub-matrix of the regression coefficients
will be penalized). Variables without positive integers will NOT be
penalized. Default is NULL, which means there are no sub-groups; nuclear
norm of entire coefficient matrix is penalized.
</p>
</td></tr>
<tr><td><code id="npmr_+3A_accelerated">accelerated</code></td>
<td>

<p>Logical. Should accelerated proximal gradient descent be used? Default is
TRUE.
</p>
</td></tr>
<tr><td><code id="npmr_+3A_b.init">B.init</code></td>
<td>

<p>Initial value of the regression coefficient matrix for proximal gradient
descent
</p>
</td></tr>
<tr><td><code id="npmr_+3A_b.init">b.init</code></td>
<td>

<p>Initial value of the regression intercept vector for proximal gradient
descent
</p>
</td></tr>
<tr><td><code id="npmr_+3A_quiet">quiet</code></td>
<td>

<p>Logical. Should output be silenced? If not, print the value of the
objective function after each step of proximal gradient descent. Perhaps
useful for debugging. Default is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In multinomial regression (in contrast with Gaussian regression or logistic
regression) there is a matrix of regression coefficients, not just a vector.
NPMR fits a logistic multinomial regression with a penalty on the nuclear norm
of this regression coefficient matrix B. Specifically, the objective is
</p>
<p>-loglik(B,b|X,Y) + lambda*||B||_*
</p>
<p>where ||.||_* denotes the nuclear norm. This implementation solves the problem
using proximal gradient descent, which iteratively steps in the direction of
the negative gradient of the loss function and soft-thresholds the singular
values of the result.
</p>
<p>This function makes available the option, through the groups
argument, of dividing the regression coefficient matrix into sub-matrices (by
row) and penalizing the sum of the nuclear norms of these submatrices. Rows
(correspond to variables) can be given no penalty in this way.
</p>


<h3>Value</h3>

<p>An object of class &ldquo;npmr&rdquo; with values:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>

<p>the call that produced this object
</p>
</td></tr>
<tr><td><code>B</code></td>
<td>

<p>A 3-dimensional array, with dimensions
(<code>ncol(X), ncol(Y), length(lambda)</code>).
For each lambda, this array stores the regression coefficient matrix which
solves the NPMR optimization problem for that value of lambda.
</p>
</td></tr>
<tr><td><code>b</code></td>
<td>

<p>A matrix with <code>ncol(Y)</code> rows and <code>length(lambda)</code> columns. Each
column stores the regression intercept vector which solves the NPMR
optimization problem for that value of lambda.
</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>

<p>A vector of length equal to the length of <code>lambda</code>, giving the value
value of the objective for the solution corresponding to each value of
lambda.
</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>

<p>The input sequence of values for the regularization parameter, for each of
which NPMR has been solved.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.npmr">cv.npmr</a></code>, <code><a href="#topic+predict.npmr">predict.npmr</a></code>, <code><a href="#topic+print.npmr">print.npmr</a></code>,
<code><a href="#topic+plot.npmr">plot.npmr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#   Fit NPMR to simulated data

K = 5
n = 1000
m = 10000
p = 10
r = 2

# Simulated training data
set.seed(8369)
A = matrix(rnorm(p*r), p, r)
C = matrix(rnorm(K*r), K, r)
B = tcrossprod(A, C)            # low-rank coefficient matrix
X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
eta = X 
P = exp(eta)/rowSums(exp(eta))
Y = t(apply(P, 1, rmultinom, n = 1, size = 1))

# Simulate test data
Xtest = matrix(rnorm(m*p), m, p)
etatest = Xtest 
Ptest = exp(etatest)/rowSums(exp(etatest))
Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))

# Fit NPMR for a sequence of lambda values without CV:
fit2 = npmr(X, Y, lambda = exp(seq(7, -2)))

# Print the NPMR fit:
fit2

# Produce a biplot:
plot(fit2, lambda = 20)

# Compute mean test error using the predict function (for each value of lambda):
getloss = function(pred, Y) {
    -mean(log(rowSums(Y*pred)))
}
apply(predict(fit2, Xtest), 3, getloss, Ytest)
</code></pre>

<hr>
<h2 id='nuclear'>
Nuclear norm of a matrix
</h2><span id='topic+nuclear'></span>

<h3>Description</h3>

<p>Returns the nuclear norm of a matrix, which is the sum of its singular values,
obtained through a singular value decomposition. Intended for internal use
only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nuclear(B)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nuclear_+3A_b">B</code></td>
<td>

<p>a matrix
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the nuclear norm of the matrix
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logL">logL</a></code>, <code><a href="#topic+objective">objective</a></code>, <code><a href="#topic+objectiveFast">objectiveFast</a></code>
</p>

<hr>
<h2 id='objective'>
NPMR objective function
</h2><span id='topic+objective'></span>

<h3>Description</h3>

<p>Return the objective function of the data and the fitted parameters for nuclear
penalized multinomial regression. The objective is the sum of the negative
log-likelihood and the product of the regularization parameter and nuclear norm
of the fitted regression coefficient matrix. Intended for internal use only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objective(B, b, X, Y, lambda)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="objective_+3A_b">B</code></td>
<td>

<p>fitted regression coefficient matrix
</p>
</td></tr>
<tr><td><code id="objective_+3A_b">b</code></td>
<td>

<p>fitted regression intercept vector
</p>
</td></tr>
<tr><td><code id="objective_+3A_x">X</code></td>
<td>

<p>covariate matrix
</p>
</td></tr>
<tr><td><code id="objective_+3A_y">Y</code></td>
<td>

<p>multinomial response matrix
</p>
</td></tr>
<tr><td><code id="objective_+3A_lambda">lambda</code></td>
<td>

<p>regularization parameter (maybe be a vector of values)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of objective values for the NPMR optimization problem, one for each
value of <code>lambda</code>
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logL">logL</a></code>, <code><a href="#topic+nuclear">nuclear</a></code>, <code><a href="#topic+objectiveFast">objectiveFast</a></code>
</p>

<hr>
<h2 id='objectiveFast'>
Shortcut computation for NPMR objective function
</h2><span id='topic+objectiveFast'></span>

<h3>Description</h3>

<p>Computes the objective function for NPMR more quickly than <code>objective</code> by
leveraging pre-computed fitted values, which is the bottleneck in
computing the objective. Intended for internal use only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objectiveFast(B, P, W, lambda)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="objectiveFast_+3A_b">B</code></td>
<td>

<p>fitted regression coefficient matrix
</p>
</td></tr>
<tr><td><code id="objectiveFast_+3A_p">P</code></td>
<td>

<p>matrix of fitted multinomial class probabilities
</p>
</td></tr>
<tr><td><code id="objectiveFast_+3A_w">W</code></td>
<td>

<p>vector containing indices of <code>P</code> which correspond to observed data
</p>
</td></tr>
<tr><td><code id="objectiveFast_+3A_lambda">lambda</code></td>
<td>

<p>regularization parameter (maybe be a vector of values)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of objective values for the NPMR optimization problem, one for each
value of <code>lambda</code>
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+objective">objective</a></code>, <code><a href="#topic+nuclear">nuclear</a></code>, <code><a href="#topic+PGDnpmr">PGDnpmr</a></code>
</p>

<hr>
<h2 id='PGDnpmr'>
Proximal gradient descent for nuclear penalized multinomial regression
</h2><span id='topic+PGDnpmr'></span>

<h3>Description</h3>

<p>Iterates steps of proximal gradient descent until convergence, by repeatedly
taking steps in the direction of the negative of the gradient and
soft-thresholding the singular values of the result. Intended for internal use
only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PGDnpmr(B, b, X, Y, lambda, s, group = NULL, accelerated = TRUE, eps = 1e-07,
    maxit = 1e+05, quiet = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PGDnpmr_+3A_b">B</code></td>
<td>

<p>Initial regression coefficient matrix
</p>
</td></tr>
<tr><td><code id="PGDnpmr_+3A_b">b</code></td>
<td>

<p>Initial intercept vector
</p>
</td></tr>
<tr><td><code id="PGDnpmr_+3A_x">X</code></td>
<td>

<p>Covariate matrix. May be in sparse form from <code>Matrix</code> package
</p>
</td></tr>
<tr><td><code id="PGDnpmr_+3A_y">Y</code></td>
<td>

<p>Response matrix. Each row has exactly one 1 indicating response category
for that observation. All other entries are zero.
</p>
</td></tr>
<tr><td><code id="PGDnpmr_+3A_lambda">lambda</code></td>
<td>

<p>Vector of regularization parameter values for penalizing nuclear norm
</p>
</td></tr>
<tr><td><code id="PGDnpmr_+3A_s">s</code></td>
<td>

<p>Step size for proximal gradient descent
</p>
</td></tr>
<tr><td><code id="PGDnpmr_+3A_group">group</code></td>
<td>

<p>Vector of length equal to number of variables (ncol(X) and nrow(B)).
Variables in the same group indexed by a POSITIVE integer will be penalized
together (the nuclear norm of the sub-matrix of the regression coefficients
will be penalized). Variables without positive integers will NOT be
penalized. Default is NULL, which means there are no sub-groups; nuclear
norm of entire coefficient matrix is penalized.
</p>
</td></tr>
<tr><td><code id="PGDnpmr_+3A_accelerated">accelerated</code></td>
<td>

<p>Logical. Should accelerated proximal gradient descent be used? Default is
TRUE.
</p>
</td></tr>
<tr><td><code id="PGDnpmr_+3A_eps">eps</code></td>
<td>

<p>Convergence threshold. When relative change in the objective function after
an interation drops below this threshold, algorithm halts.
</p>
</td></tr>
<tr><td><code id="PGDnpmr_+3A_maxit">maxit</code></td>
<td>

<p>Maximum number of iterations for proximal gradient descent.
</p>
</td></tr>
<tr><td><code id="PGDnpmr_+3A_quiet">quiet</code></td>
<td>

<p>Logical. Should output be silenced? If not, print the value of the
objective function after each step of proximal gradient descent. Perhaps
useful for debugging. Default is TRUE.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>B</code></td>
<td>

<p>Optimal value of the regression coeffient matrix at convergence
</p>
</td></tr>
<tr><td><code>b</code></td>
<td>

<p>Optimal value of the regression intercept vector at convergence
</p>
</td></tr>
<tr><td><code>objectivePath</code></td>
<td>

<p>Vector showing the value of the objective function at each step in proximal
gradient descent
</p>
</td></tr>
<tr><td><code>time</code></td>
<td>

<p>Time taken until convergence
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npmr">npmr</a></code>, <code><a href="#topic+prox">prox</a></code>, <code><a href="#topic+objective">objective</a></code>,
<code><a href="#topic+objectiveFast">objectiveFast</a></code>
</p>

<hr>
<h2 id='plot.cv.npmr'>
Visualize the regression coefficient matrix fit by cross-validated NPMR
</h2><span id='topic+plot.cv.npmr'></span>

<h3>Description</h3>

<p>Plots features (in orange) by their weights on the first two latent variables
in the singular value decomposition of the regression coefficient matrix. Plots
response classes (as blue arrows) by their loadings on the first two latent
variables. Does this for the regression coefficient matrix fit with the
value of lambda that led to the minimum cross validation error among all those
tried.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.npmr'
plot(x, feature.names = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.cv.npmr_+3A_x">x</code></td>
<td>

<p>an object of class <code><a href="#topic+cv.npmr">cv.npmr</a></code>
</p>
</td></tr>
<tr><td><code id="plot.cv.npmr_+3A_feature.names">feature.names</code></td>
<td>

<p>logical. Should the names of the covariates be used in the plot? If FALSE,
use standard plotting symbol (<code>pch=1</code>) instead.
</p>
</td></tr>
<tr><td><code id="plot.cv.npmr_+3A_...">...</code></td>
<td>

<p>additional arguments to be passed to <code>plot</code>
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.npmr">cv.npmr</a></code>, <code><a href="#topic+plot.npmr">plot.npmr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#   Fit NPMR to simulated data

K = 5
n = 1000
m = 10000
p = 10
r = 2

# Simulated training data
set.seed(8369)
A = matrix(rnorm(p*r), p, r)
C = matrix(rnorm(K*r), K, r)
B = tcrossprod(A, C)            # low-rank coefficient matrix
X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
eta = X 
P = exp(eta)/rowSums(exp(eta))
Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
fold = sample(rep(1:10, length = nrow(X)))

# Simulate test data
Xtest = matrix(rnorm(m*p), m, p)
etatest = Xtest 
Ptest = exp(etatest)/rowSums(exp(etatest))
Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))

# Fit NPMR for a sequence of lambda values without CV:
fit2 = cv.npmr(X, Y, lambda = exp(seq(7, -2)), foldid = fold)

# Produce a biplot:
plot(fit2)
</code></pre>

<hr>
<h2 id='plot.npmr'>
Visualize the regression coefficient matrix fit by cross-validated NPMR
</h2><span id='topic+plot.npmr'></span>

<h3>Description</h3>

<p>Plots features (in orange) by their weights on the first two latent variables
in the singular value decomposition of the regression coefficient matrix. Plots
response classes (as blue arrows) by their loadings on the first two latent
variables. Does this for the regression coefficient matrix fit with the
value of lambda closest among all those tried to the value of <code>lambda</code>
specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'npmr'
plot(x, lambda, feature.names = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.npmr_+3A_x">x</code></td>
<td>

<p>an object of class <code><a href="#topic+npmr">npmr</a></code>
</p>
</td></tr>
<tr><td><code id="plot.npmr_+3A_lambda">lambda</code></td>
<td>

<p>a single regularization parameter value
</p>
</td></tr>
<tr><td><code id="plot.npmr_+3A_feature.names">feature.names</code></td>
<td>

<p>logical. Should the names of the covariates be used in the plot? If FALSE,
use standard plotting symbol (<code>pch=1</code>) instead.
</p>
</td></tr>
<tr><td><code id="plot.npmr_+3A_...">...</code></td>
<td>

<p>additional arguments to be passed to <code>plot</code>
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npmr">npmr</a></code>, <code><a href="#topic+plot.cv.npmr">plot.cv.npmr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#   Fit NPMR to simulated data

K = 5
n = 1000
m = 10000
p = 10
r = 2

# Simulated training data
set.seed(8369)
A = matrix(rnorm(p*r), p, r)
C = matrix(rnorm(K*r), K, r)
B = tcrossprod(A, C)            # low-rank coefficient matrix
X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
eta = X 
P = exp(eta)/rowSums(exp(eta))
Y = t(apply(P, 1, rmultinom, n = 1, size = 1))

# Simulate test data
Xtest = matrix(rnorm(m*p), m, p)
etatest = Xtest 
Ptest = exp(etatest)/rowSums(exp(etatest))
Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))

# Fit NPMR for a sequence of lambda values without CV:
fit2 = npmr(X, Y, lambda = exp(seq(7, -2)))

# Produce a biplot:
plot(fit2, lambda = 20)
</code></pre>

<hr>
<h2 id='predict.cv.npmr'>
Make predictions from a &ldquo;cv.npmr&rdquo; object
</h2><span id='topic+predict.cv.npmr'></span>

<h3>Description</h3>

<p>Return predicted reponse class probabilities from a cross-validated NPMR model,
using the value of the regularization parameter that led to the minimum
cross validation error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.npmr'
predict(object, newx, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.cv.npmr_+3A_object">object</code></td>
<td>

<p>an object of class <code><a href="#topic+cv.npmr">cv.npmr</a></code>
</p>
</td></tr>
<tr><td><code id="predict.cv.npmr_+3A_newx">newx</code></td>
<td>

<p>covariate matrix on which for which to make response class probability
predictions. Must have same number of columns as <code>X</code> used original to
fit <code>object</code>.
</p>
</td></tr>
<tr><td><code id="predict.cv.npmr_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix giving the predicted probability that each row of <code>newx</code> belongs
to each class, corresponding the value of the regularization parameter that led
to minimum cross validation error.
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.npmr">cv.npmr</a></code>, <code><a href="#topic+predict.npmr">predict.npmr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#   Fit NPMR to simulated data

K = 5
n = 1000
m = 10000
p = 10
r = 2

# Simulated training data
set.seed(8369)
A = matrix(rnorm(p*r), p, r)
C = matrix(rnorm(K*r), K, r)
B = tcrossprod(A, C)            # low-rank coefficient matrix
X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
eta = X 
P = exp(eta)/rowSums(exp(eta))
Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
fold = sample(rep(1:10, length = nrow(X)))

# Simulate test data
Xtest = matrix(rnorm(m*p), m, p)
etatest = Xtest 
Ptest = exp(etatest)/rowSums(exp(etatest))
Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))

# Fit NPMR for a sequence of lambda values without CV:
fit2 = cv.npmr(X, Y, lambda = exp(seq(7, -2)), foldid = fold)

# Compute mean test error using the predict function:
-mean(log(rowSums(Ytest*predict(fit2, Xtest))))
</code></pre>

<hr>
<h2 id='predict.npmr'>
Make predictions from a &ldquo;npmr&rdquo; object
</h2><span id='topic+predict.npmr'></span>

<h3>Description</h3>

<p>Return predicted reponse class probabilities from a fitted NPMR model,
for each value of lambda on which the NPMR model was originally fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'npmr'
predict(object, newx, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.npmr_+3A_object">object</code></td>
<td>

<p>an object of class <code><a href="#topic+npmr">npmr</a></code>
</p>
</td></tr>
<tr><td><code id="predict.npmr_+3A_newx">newx</code></td>
<td>

<p>covariate matrix on which for which to make response class probability
predictions. Must have same number of columns as <code>X</code> used original to
fit <code>object</code>.
</p>
</td></tr>
<tr><td><code id="predict.npmr_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 3-dimensional array, with dimensions
(<code>nrow(newx), ncol(Y), length(lambda)</code>).
For each lambda, this array stores for that value of lambda the predicted
response class probabilites for each observation.
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npmr">npmr</a></code>, <code><a href="#topic+predict.cv.npmr">predict.cv.npmr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#   Fit NPMR to simulated data

K = 5
n = 1000
m = 10000
p = 10
r = 2

# Simulated training data
set.seed(8369)
A = matrix(rnorm(p*r), p, r)
C = matrix(rnorm(K*r), K, r)
B = tcrossprod(A, C)            # low-rank coefficient matrix
X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
eta = X 
P = exp(eta)/rowSums(exp(eta))
Y = t(apply(P, 1, rmultinom, n = 1, size = 1))

# Simulate test data
Xtest = matrix(rnorm(m*p), m, p)
etatest = Xtest 
Ptest = exp(etatest)/rowSums(exp(etatest))
Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))

# Fit NPMR for a sequence of lambda values without CV:
fit2 = npmr(X, Y, lambda = exp(seq(7, -2)))

# Compute mean test error using the predict function (for each value of lambda):
getloss = function(pred, Y) {
    -mean(log(rowSums(Y*pred)))
}
apply(predict(fit2, Xtest), 3, getloss, Ytest)
</code></pre>

<hr>
<h2 id='print.cv.npmr'>
summarize a &quot;cv.npmr&quot; object
</h2><span id='topic+print.cv.npmr'></span>

<h3>Description</h3>

<p>Print (1) the call that produced the <code><a href="#topic+cv.npmr">cv.npmr</a></code> object;
(2) the value of the regularization parameter lambda that led to the
minimum cross validation error; (3) the rank of the fitted regression
coefficient matrix; and (4) the per-observation cross validation error using
the optimal lambda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.npmr'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.cv.npmr_+3A_x">x</code></td>
<td>

<p>an object of class <code><a href="#topic+cv.npmr">cv.npmr</a></code>
</p>
</td></tr>
<tr><td><code id="print.cv.npmr_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.npmr">cv.npmr</a></code>, <code><a href="#topic+print.npmr">print.npmr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#   Fit NPMR to simulated data

K = 5
n = 1000
m = 10000
p = 10
r = 2

# Simulated training data
set.seed(8369)
A = matrix(rnorm(p*r), p, r)
C = matrix(rnorm(K*r), K, r)
B = tcrossprod(A, C)            # low-rank coefficient matrix
X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
eta = X 
P = exp(eta)/rowSums(exp(eta))
Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
fold = sample(rep(1:10, length = nrow(X)))

# Simulate test data
Xtest = matrix(rnorm(m*p), m, p)
etatest = Xtest 
Ptest = exp(etatest)/rowSums(exp(etatest))
Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))

# Fit NPMR for a sequence of lambda values without CV:
fit2 = cv.npmr(X, Y, lambda = exp(seq(7, -2)), foldid = fold)

# Print the NPMR fit:
fit2
</code></pre>

<hr>
<h2 id='print.npmr'>
Summarize a &quot;npmr&quot; object
</h2><span id='topic+print.npmr'></span>

<h3>Description</h3>

<p>Print the call that produced the <code><a href="#topic+npmr">npmr</a></code> object and a dataframe
showing, for each value of the regularization parameter on
which the NPMR object was fit, the rank of the resulting regression coefficient
matrix and the corresponding value of the NPMR objective function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'npmr'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.npmr_+3A_x">x</code></td>
<td>

<p>an object of class <code><a href="#topic+npmr">npmr</a></code>
</p>
</td></tr>
<tr><td><code id="print.npmr_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Rob Tibshirani (2016). &ldquo;Nuclear penalized
multinomial regression with an application to predicting at bat outcomes in
baseball.&rdquo; In prep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npmr">npmr</a></code>, <code><a href="#topic+print.cv.npmr">print.cv.npmr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#   Fit NPMR to simulated data

K = 5
n = 1000
m = 10000
p = 10
r = 2

# Simulated training data
set.seed(8369)
A = matrix(rnorm(p*r), p, r)
C = matrix(rnorm(K*r), K, r)
B = tcrossprod(A, C)            # low-rank coefficient matrix
X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
eta = X 
P = exp(eta)/rowSums(exp(eta))
Y = t(apply(P, 1, rmultinom, n = 1, size = 1))

# Simulate test data
Xtest = matrix(rnorm(m*p), m, p)
etatest = Xtest 
Ptest = exp(etatest)/rowSums(exp(etatest))
Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))

# Fit NPMR for a sequence of lambda values without CV:
fit2 = npmr(X, Y, lambda = exp(seq(7, -2)))

# Print the NPMR fit:
fit2
</code></pre>

<hr>
<h2 id='prox'>
Proximal operator for nuclear norm
</h2><span id='topic+prox'></span>

<h3>Description</h3>

<p>Return the value of the proximal operator of the nuclear norm (scaled by
<code>threshold</code>) applied to a matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prox(B, threshold, group)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prox_+3A_b">B</code></td>
<td>

<p>matrix
</p>
</td></tr>
<tr><td><code id="prox_+3A_threshold">threshold</code></td>
<td>

<p>scaling factor applied to the nuclear norm. In proximal gradient descent for
NPMR, this is the product of the stepsize and the regularization parameter
lambda
</p>
</td></tr>
<tr><td><code id="prox_+3A_group">group</code></td>
<td>

<p>Vector of length equal to number of variables, i.e. nrow(B).
Variables in the same group indexed by a POSITIVE integer will be penalized
together (the nuclear norm of the sub-matrix of the regression coefficients
will be penalized). Variables without positive integers will NOT be
penalized. Default is NULL, which means there are no sub-groups; nuclear
norm of entire coefficient matrix is penalized.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of the proximal operator of the nuclear norm (scaled by
<code>threshold</code>) applied to <code>B</code>
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Rob Tibshirani
</p>


<h3>References</h3>

<p>Neal Parikh and Stephen Boyd (2013) &ldquo;Proximal algorithms.&rdquo; Foundations and
Trends in Optimization 1, 3:123-231.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nuclear">nuclear</a></code>, <code><a href="#topic+PGDnpmr">PGDnpmr</a></code>
</p>

<hr>
<h2 id='Vowel'>
Vowel Recognition
</h2><span id='topic+Vowel'></span>

<h3>Description</h3>

<p>Speaker independent recognition of the eleven steady state vowels
of British English using a specified training set of lpc derived log area
ratios.
</p>


<h3>Format</h3>

<p>A data frame with 990 observations on the following 12 variables.
</p>

<dl>
<dt><code>y</code></dt><dd><p>Class label indicating vowel spoken</p>
</dd>
<dt><code>subset</code></dt><dd><p>a factor with levels <code>test</code> <code>train</code></p>
</dd>
<dt><code>x.1</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.2</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.3</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.4</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.5</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.6</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.7</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.8</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.9</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.10</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p>The speech signals were low pass filtered at 4.7kHz and then digitised to 12
bits with a 10kHz sampling rate.  Twelfth order linear predictive analysis was
carried out on six 512 sample Hamming windowed segments from the steady part
of the vowel.  The reflection coefficients were used to calculate 10 log area
parameters, giving a 10 dimensional input space.  For a general introduction
to speech processing and an explanation of this technique see Rabiner and
Schafer [RabinerSchafer78].
</p>
<p>Each speaker thus yielded six frames of speech from eleven vowels.  This gave
528 frames from the eight speakers used to train the networks and 462 frames
from the seven speakers used to test the networks.
</p>
<p>The eleven vowels, along with words demonstrating their sound, are:
i (heed)
I (hid)
E (head)
A (had)
a: (hard)
Y (hud)
O (hod)
C: (hoard)
U (hood)
u: (who'd)
3: (heard)
</p>


<h3>Source</h3>

<p>https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/
</p>


<h3>References</h3>

<p>D. H. Deterding, 1989, University of Cambridge, &quot;Speaker Normalisation for Automatic Speech Recognition&quot;, submitted for PhD.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Vowel)
summary(Vowel)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
