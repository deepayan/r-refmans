<!DOCTYPE html><html lang="en"><head><title>Help for package LogicReg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {LogicReg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cumhaz'><p>Cumulative hazard transformation</p></a></li>
<li><a href='#eval.logreg'><p>Evaluate a Logic Regression tree</p></a></li>
<li><a href='#frame.logreg'><p>Constructs a data frame for one or more Logic Regression</p>
models</a></li>
<li><a href='#LogicReg-internal'><p>Internal logic regression functions</p></a></li>
<li><a href='#logreg'><p>Logic Regression</p></a></li>
<li><a href='#logreg.anneal.control'><p>Control for Logic Regression</p></a></li>
<li><a href='#logreg.mc.control'><p>Control for Logic Regression</p></a></li>
<li><a href='#logreg.myown'><p>Writing your own Logic Regression scoring function</p></a></li>
<li><a href='#logreg.savefit1'><p>Sample results for Logic Regression</p></a></li>
<li><a href='#logreg.testdat'><p>Test data for Logic Regression</p></a></li>
<li><a href='#logreg.tree.control'><p>Control for logreg</p></a></li>
<li><a href='#logregmodel'><p>Format of class logregmodel</p></a></li>
<li><a href='#logregtree'><p>Format of class logregtree</p></a></li>
<li><a href='#plot.logreg'><p>Plots for Logic Regression</p></a></li>
<li><a href='#plot.logregmodel'><p>Plots for Logic Regression</p></a></li>
<li><a href='#plot.logregtree'><p>A plot of one Logic Regression tree.</p></a></li>
<li><a href='#predict.logreg'><p>Predicted values Logic Regression</p></a></li>
<li><a href='#print.logreg'><p>Prints Logic Regression Output</p></a></li>
<li><a href='#print.logregmodel'><p>Prints Logic Regression Formula</p></a></li>
<li><a href='#print.logregtree'><p>Prints Logic Regression Formula</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.6.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-07-24</td>
</tr>
<tr>
<td>Title:</td>
<td>Logic Regression</td>
</tr>
<tr>
<td>Author:</td>
<td>Charles Kooperberg &lt;clk@fredhutch.org&gt; and Ingo Ruczinski &lt;ingo@jhu.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Charles Kooperberg &lt;clk@fredhutch.org&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10), survival</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, graphics, utils, grDevices</td>
</tr>
<tr>
<td>Description:</td>
<td>Routines for fitting Logic Regression models. Logic Regression is described
	in Ruczinski, Kooperberg, and LeBlanc (2003) &lt;<a href="https://doi.org/10.1198%2F1061860032238">doi:10.1198/1061860032238</a>&gt;. Monte
        Carlo Logic Regression is described in and Kooperberg and Ruczinski (2005)
        &lt;<a href="https://doi.org/10.1002%2Fgepi.20042">doi:10.1002/gepi.20042</a>&gt;.</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-07-24 22:00:09 UTC; clk</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-08 23:10:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='cumhaz'>Cumulative hazard transformation</h2><span id='topic+cumhaz'></span>

<h3>Description</h3>

<p>Transforms survival times using the cumulative hazard function. </p>


<h3>Usage</h3>

<pre><code class='language-R'>cumhaz(y, d)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cumhaz_+3A_y">y</code></td>
<td>
<p>vector of nonnegative survival times</p>
</td></tr>
<tr><td><code id="cumhaz_+3A_d">d</code></td>
<td>
<p>vector of censoring indicators, should be the same length
as <code>y</code>. If <code>d</code> is missing the data is assumed to be uncensored.</p>
</td></tr></table>


<h3>Value</h3>

<p>A vector of transformed survival times.</p>


<h3>Note</h3>

<p>The primary use of doing a cumulative
hazard transformation is that after such a transformation, exponential
survival models yield results that are often very much comparable to
proportional hazards models. In our implementation of
Logic Regression, however, exponential
survival models run much faster than proportional hazards models when there
are no continuous separate covariates.
</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.testdat)
#
# this is not survival data, but it shows the functionality
yy &lt;- cumhaz(exp(logreg.testdat[,1]), logreg.testdat[, 2])
# then we would use
# logreg(resp=yy, cens=logreg.testdat[,2], type=5, ...
# insted of
# logreg(resp=logreg.testdat[,1], cens=logreg.testdat[,2], type=4, ...
</code></pre>

<hr>
<h2 id='eval.logreg'>Evaluate a Logic Regression tree</h2><span id='topic+eval.logreg'></span>

<h3>Description</h3>

<p>This function evaluates a logic tree, typically a part
of an object generated by <code>logreg</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>eval.logreg(ltree, data)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="eval.logreg_+3A_ltree">ltree</code></td>
<td>
<p>an object of class <code>logregmodel</code> or an object of class
<code>logregtree</code>.
Typically this object will be part of the result of an object of class
<code>logreg</code>, generated with <code>select = 1</code> (single model fit),
<code>select = 2</code> (multiple model fit), or
<code>select = 6</code> (greedy stepwise fit).</p>
</td></tr>
<tr><td><code id="eval.logreg_+3A_data">data</code></td>
<td>
<p>a data frame on which the logic tree is to be
evaluated. <code>data</code> should be binary, and have the same number of
columns as the <code>bin</code> component of the original <code>logreg</code>
fit.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A binary vector with length equal to the number of rows of
<code>data</code>; a 1 corresponds to cases for which <code>ltree</code> was
<code>TRUE</code> and a 0 corresponds to cases for which  <code>ltree</code> was
<code>FALSE</code> if <code>ltree</code> was an object of class <code>logregtree</code>
or the <code>trees</code> component of such an object. Otherwise a matrix
with one column for each tree in <code>ltree</code>.</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a></p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+logregtree">logregtree</a></code>,
<code><a href="#topic+logregmodel">logregmodel</a></code>,
<code><a href="#topic+frame.logreg">frame.logreg</a></code>,
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit1)
# myanneal &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 1000)
# logreg.savefit1 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21], 
#                type = 2, select = 1, ntrees = 2, anneal.control = myanneal)
tree1 &lt;- eval.logreg(logreg.savefit1$model$trees[[1]], logreg.savefit1$binary)
tree2 &lt;- eval.logreg(logreg.savefit1$model$trees[[2]], logreg.savefit1$binary)
alltrees &lt;- eval.logreg(logreg.savefit1$model, logreg.savefit1$binary)
</code></pre>

<hr>
<h2 id='frame.logreg'>Constructs a data frame for one or more Logic Regression
models</h2><span id='topic+frame.logreg'></span>

<h3>Description</h3>

<p>Evaluates all components of one or more Logic Regression
models fitted by a single call to <code>logreg</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>frame.logreg(fit, msz, ntr, newbin, newresp, newsep, newcens, newweight)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="frame.logreg_+3A_fit">fit</code></td>
<td>
<p>object of class <code>logreg</code>, that resulted from applying
the function <code>logreg</code> with 
<code>select = 1</code> (single model fit),
<code>select = 2</code> (multiple model fit), or
<code>select = 6</code> (greedy stepwise fit).</p>
</td></tr>
<tr><td><code id="frame.logreg_+3A_msz">msz</code></td>
<td>
<p>if <code>frame.logreg</code> is executed on an object of class
<code>logreg</code>, that resulted from applying the function <code>logreg</code>
with <code>select = 2</code> (multiple model fit)
or <code>select = 6</code> (greedy stepwise fit) all logic trees for all
fitted models are returned. To restrict the model size and the number
of trees to some models, specify <code>msz</code> and <code>ntr</code>
(for <code>select = 2</code>) or just <code>msz</code> (for <code>select = 6</code>).</p>
</td></tr>
<tr><td><code id="frame.logreg_+3A_ntr">ntr</code></td>
<td>
<p>see <code>msz</code>.</p>
</td></tr>
<tr><td><code id="frame.logreg_+3A_newbin">newbin</code></td>
<td>
<p>binary predictors to evaluate the logic trees at.  If
<code>newbin</code> is omitted, the original (training) data is used.</p>
</td></tr>
<tr><td><code id="frame.logreg_+3A_newresp">newresp</code></td>
<td>
<p>the response. If <code>newbin</code> is omitted, the
original (training) response is used. If <code>newbin</code> is specified
and <code>newresp</code> is omitted, the resulting
data frame will not have a
response column.</p>
</td></tr>
<tr><td><code id="frame.logreg_+3A_newsep">newsep</code></td>
<td>
<p>separate (linear) predictors. If <code>newbin</code> is
omitted, the original (training) predictors are used, even if
<code>newsep</code> is specified.</p>
</td></tr>
<tr><td><code id="frame.logreg_+3A_newweight">newweight</code></td>
<td>
<p>case weights. If <code>newbin</code> is omitted, the
original (training) weights are used. If <code>newbin</code> is specified
and <code>newweight</code> is omitted, the weights are taken to be 1.</p>
</td></tr>
<tr><td><code id="frame.logreg_+3A_newcens">newcens</code></td>
<td>
<p>censoring indicator. For proportional hazards models
and exponential survival models
only.  If <code>newbin</code> is omitted, the original (training) censoring
indicators are used. If <code>newbin</code> is specified and <code>newcens</code>
is omitted, the censoring indicators are taken to be 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calls <code>eval.logreg</code>. </p>


<h3>Value</h3>

<p>A data frame. The first column is the response, later columns
are weights, censoring indicator, separate predictors (all of which
are only provided if they are relevant) and all logic trees. Column
names should be transparent.</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a></p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+eval.logreg">eval.logreg</a></code>,
<code><a href="#topic+predict.logreg">predict.logreg</a></code>,
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit1,logreg.savefit2,logreg.savefit6)
#
# fit a single mode
# myanneal &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 1000)
# logreg.savefit1 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21], 
#               type = 2, select = 1, ntrees = 2, anneal.control = myanneal)
frame1 &lt;- frame.logreg(logreg.savefit1)
#
# a complete sequence
# myanneal2 &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 0)
# logreg.savefit2 &lt;- logreg(select = 2, ntrees = c(1,2), nleaves =c(1,7), 
#               oldfit = logreg.savefit1, anneal.control = myanneal2)
frame2 &lt;- frame.logreg(logreg.savefit2)
#
# a greedy sequence
# logreg.savefit6 &lt;- logreg(select = 6, ntrees = 2, nleaves =c(1,12), oldfit = logreg.savefit1)
frame6  &lt;- frame.logreg(logreg.savefit6, msz = 3:5) # restrict the size

</code></pre>

<hr>
<h2 id='LogicReg-internal'>Internal logic regression functions</h2><span id='topic+clogreg'></span>

<h3>Description</h3>

<p>Internal logic regression functions</p>


<h3>Details</h3>

<p>These are not intended for use by users.</p>


<h3>Author(s)</h3>

<p>Charles Kooperberg</p>

<hr>
<h2 id='logreg'>Logic Regression</h2><span id='topic+logreg'></span>

<h3>Description</h3>

<p>Fit one or a series of Logic Regression models,  carry
out cross-validation or permutation tests for such models,
or fit Monte Carlo Logic Regression models.
</p>
<p>Logic regression is a (generalized) regression methodology that is
primarily applied when most of the covariates in the data to be
analyzed are binary. The goal of logic regression is to find
predictors that are Boolean (logical) combinations of the original
predictors.  Currently the Logic Regression methodology has scoring
functions for linear regression (residual sum of squares), logistic
regression (deviance), classification (misclassification), 
proportional hazards models (partial likelihood),
and exponential survival models (log-likelihood). A feature of the
Logic Regression methodology is that it is easily possible to extend
the method to write ones own scoring function if you have a different
scoring function.  <code>logreg.myown</code> contains information on how to
do so.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logreg(resp, bin, sep, wgt, cens, type, select, ntrees, nleaves, 
       penalty, seed, kfold, nrep, oldfit, anneal.control, tree.control,
       mc.control)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logreg_+3A_resp">resp</code></td>
<td>
<p>vector with the response variables. Let <code>n1</code> be the
length of this column.</p>
</td></tr>
<tr><td><code id="logreg_+3A_bin">bin</code></td>
<td>
<p>matrix or data frame with binary data.  Let <code>n2</code> be
the number of columns of this object.  <code>bin</code> should have
<code>n1</code> rows.</p>
</td></tr>
<tr><td><code id="logreg_+3A_sep">sep</code></td>
<td>
<p>(optional) matrix or data frame that is fitted additively
in the logic regression model.  <code>sep</code> should have <code>n1</code>
rows. When exponential survival models (<code>type = 5</code>) are
used, the additive predictors have to be binary. When
logistic regression models (<code>type = 3</code>) are used <code>logreg</code>
is much faster when all additive predictors are binary.</p>
</td></tr>
<tr><td><code id="logreg_+3A_wgt">wgt</code></td>
<td>
<p>(optional) vector of length <code>n1</code> with case weights;
default is <code>rep(1,n1)</code>.</p>
</td></tr>
<tr><td><code id="logreg_+3A_cens">cens</code></td>
<td>
<p>(optional) an indicator variable with censoring
indicators if <code>type</code> equals 4 (proportional hazards model)
or 5 (exponential survival model); default is <code>rep(1,n1)</code>.</p>
</td></tr>
<tr><td><code id="logreg_+3A_type">type</code></td>
<td>
<p>type of model to be fit: (1) classification, (2)
regression, (3) logistic regression, (4) proportional hazards model
(Cox regression), (5) exponential
survival model, or (0) your own scoring function. If <code>type = 0</code>,
the code needs to be recompiled, uncompiled <code>type = 0</code>
results in a constant score of 0, which may be useful
to generate a sample from the prior when <code>select = 7</code> (Monte Carlo
Logic Regression).</p>
</td></tr>
<tr><td><code id="logreg_+3A_select">select</code></td>
<td>
<p>type of model selection to be carried out: (1) fit a
single model, (2) fit multiple models, (3) cross-validation, (4)
null-model permutation test, (5) conditional permutation test,
(6) a greedy stepwise algorithm, or (7) Monte Carlo Logic Regression
(using MCMC). See
details below.</p>
</td></tr>
<tr><td><code id="logreg_+3A_ntrees">ntrees</code></td>
<td>
<p>number of logic trees to be fit. A single number if you
select to fit a single model (<code>select = 1</code>), carry out the
null-model permutation test (<code>select = 4</code>), carry out greedy
stepwise selection  (<code>select = 6</code>) or,
select using MCMC (<code>select = 7</code>), or a range
(e.g. <code>c(ntreeslow,ntreeshigh)</code>) for any of the other selection
options.  In our applications, we usually ended up with models having
between one and four trees. In general, fitting one and two trees in
the initial exploratory analysis is a good idea.</p>
</td></tr>
<tr><td><code id="logreg_+3A_nleaves">nleaves</code></td>
<td>
<p>maximum number of leaves to be fit in all trees
combined.  A single number if you select to fit a single model
(<code>select = 1</code>) carry out the null-model permutation test
(<code>select = 4</code>), carry out greedy
stepwise selection  (<code>select = 6</code>) or,
select using MCMC (<code>select = 7</code>),
or a range (e.g. <code>c(nleaveslow,nleaveshigh)</code>)
for any of the other selection options.  If <code>select</code> is 1, 4, 6, or 7,
the default is <code>-1</code>, which is expanded to become <code>ntrees * tree.control\$treesize
</code>.</p>
</td></tr>
<tr><td><code id="logreg_+3A_penalty">penalty</code></td>
<td>
<p>specifying the penalty parameter allows you to
penalize the score of larger models. The penalty takes the form
<code>penalty</code> times the number of leaves in the model.
(For some score functions, we compute
average scores: the penalty is naturally
adjusted.)  Thus <code>penalty = 2</code> is somewhat comparable to
AIC. Note, however, that there is no relation between the size of the
model and the number of parameters (dimension) of the model, as is
usual the case for AIC like penalties. 
<code>penalty</code> is only relevant when <code>select = 1</code>.</p>
</td></tr>
<tr><td><code id="logreg_+3A_seed">seed</code></td>
<td>
<p>a seed for the random number generator.  The random seed
is taken to be <code>abs(seed)</code>. \ For
the cross-validation version, if  <code>seed &lt; 0</code> the sequence of
the cases is not permuted for division in training-test sets. This
is useful if you already permuted the sequence, and wish to compare
results with other approaches, or if there is a relation between the
sequence of the cases, for example for a matched case-control study.</p>
</td></tr>
<tr><td><code id="logreg_+3A_kfold">kfold</code></td>
<td>
<p>the number of groups the cases are randomly assigned
to. In turn, the model is trained on <code>(kfold - 1)</code> of those
groups, and scored on the group left out. Common choices
are <code>kfold = 5</code> and <code>kfold = 10</code>. Only
relevant for cross-validation (<code>select = 3</code>).  </p>
</td></tr>
<tr><td><code id="logreg_+3A_nrep">nrep</code></td>
<td>
<p>the number of runs on permuted data for each model size.
We recommend first running this program with a small number of
repetitions (e.g. 10 or 25) before sending off a big job.  Only
relevant for the null-model test (<code>select = 4</code>) or the
permutation test (<code>select = 5</code>).</p>
</td></tr>
<tr><td><code id="logreg_+3A_oldfit">oldfit</code></td>
<td>
<p>object of class <code>logreg</code>, typically the result of a
previous call to <code>logreg</code>.  All options that are not specified
default to the value used in <code>oldfit</code>.
For the permutation test (<code>select = 5</code>) an
<code>oldfit</code> object obtained with <code>select = 2</code> (fit multiple
models) is mandatory, as the best models of each size need to be in
place.</p>
</td></tr>
<tr><td><code id="logreg_+3A_anneal.control">anneal.control</code></td>
<td>
<p>simulated annealing parameters - best set using
the function <code>logreg.anneal.control</code>.</p>
</td></tr>
<tr><td><code id="logreg_+3A_tree.control">tree.control</code></td>
<td>
<p>several secondary parameters - best set using the
function <code>logreg.tree.control</code>.</p>
</td></tr>
<tr><td><code id="logreg_+3A_mc.control">mc.control</code></td>
<td>
<p>Markov chain Monte Carlo parameters - best set using the
function <code>logreg.mc.control</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Logic Regression is an adaptive regression methodology that attempts
to construct predictors as Boolean combinations of binary covariates.
</p>
<p>In most regression problems a model is developed that only relates the
main effects (the predictors or transformations thereof) to the
response.  Although interactions between predictors are considered
sometimes as well, those interactions are usually kept simple
(two- to three-way interactions at most).  But often, especially when
all predictors are binary, the interaction between many predictors is
what causes the differences in response. This issue often arises in
the analysis of SNP microarray data or in data mining problems.  Given
a set of binary predictors X, we try to create new, better predictors
for the response by considering combinations of those binary
predictors. For example, if the response is binary as well (which is
not required in general), we attempt to find decision rules such as
&ldquo;if X1, X2, X3 and X4 are true&rdquo;, or &ldquo;X5 or X6 but not X7 are
true&rdquo;, then the response is more likely to be in class 0. In other
words, we try to find Boolean statements involving the binary
predictors that enhance the prediction for the response.  In more
specific terms: Let X1,...,Xk be binary predictors, and let Y be a
response variable.  We try to fit regression models of the form
g(E[Y]) = b0 + b1 L1+ ...+ bn Ln, where Lj is a Boolean expression of
the predictors X, such as Lj=[(X2 or X4c) and X7].  The above
framework includes many forms of regression, such as linear regression
(g(E[Y])=E[Y]) and logistic regression (g(E[Y])=log(E[Y]/(1-E[Y]))).
For every model type, we define a score function that reflects the
&ldquo;quality&rdquo; of the model under consideration. For example, for linear
regression the score could be the residual sum of squares and for
logistic regression the score could be the deviance. We try to find
the Boolean expressions in the regression model that minimize the
scoring function associated with this model type, estimating the
parameters bj simultaneously with the Boolean expressions Lj. In
general, any type of model can be considered, as long as a scoring
function can be defined. For example, we also implemented the Cox
proportional hazards model, using the partial likelihood as the score.
</p>
<p>Since the number of possible Logic Models we can construct for a given
set of predictors is huge, we have to rely on some search algorithms
to help us find the best scoring models.  We define the move set by a
set of standard operations such as splitting and pruning the tree
(similar to the terminology introduced by Breiman et al (1984) for
CART).  We investigated two types of algorithms: a greedy and a
simulated annealing algorithm. While the greedy algorithm is very
fast, it does not always find a good scoring model. The simulated
annealing algorithm usually does, but computationally it is more
expensive.  Since we have to be certain to find good scoring models,
we usually carry out simulated annealing for our case studies.
However, as usual, the best scoring model generally over-fits the
data, and methods to separate signal and noise are needed.  To assess
the over-fitting of large models, we offer the option to fit a model
of a specific size. For the model selection itself we developed and
implemented permutation tests and tests using cross-validation. If
sufficient data is available, an analysis using a training and a test
set can also be carried out.  These tests are rather complicated, so
we will not go into detail here and refer you to Ruczinski I,
Kooperberg C, LeBlanc ML (2003), cited below. 
</p>
<p>There are two alternatives to the simulated annealing algorithm. One
is a stepwise greedy selection of models. This is when setting
<code>select = 6</code>, and yields a sequence of models from size 1 through
a maximum size. At each time among all the models that are one larger
than the current model the best model is selected, yielding a sequence
of models of different sizes. Usually these models are not the best 
possible, and, if the simulated annealing chain is long enough, you
should expect that the models selected using <code>select = 2</code> are better.
</p>
<p>The second alternative is to run a Markov Chain Monte Carlo (MCMC) algorithm.
This is what is done in Monte Carlo Logic Regression. The algorithm used
is a reversible jump MCMC algorithm, due to Green (1995). Other than
the length of the Markov chain, the only parameter that needs to be set
is a parameter for the geometric prior on model size. Other than in many
MCMC problems, the goal in Monte Carlo Logic Regression is not to yield
one single best predicting model, but rather to provide summaries of
all models. These are exactly the elements that are shown above as
the output when <code>select = 7</code>.
</p>
<p><b>MONITORING</b>
</p>
<p>The help file for <code>logreg.anneal.control</code>, contains more
information on how to monitor the simulated annealing optimization for
logreg. Here is some general information.
</p>
<p><b>Find the best scoring model of any size</b> <code>(select = 1)</code>
</p>
<p>During the iterations the following information is printed out:
</p>

<table>
<tr>
 <td style="text-align: right;">
log-temp </td><td style="text-align: right;"> current score </td><td style="text-align: right;">  best score </td><td style="text-align: right;">  acc / </td><td style="text-align: right;"> rej / </td><td style="text-align: right;"> sing  </td><td style="text-align: right;"> current parameters  </td>
</tr>
<tr>
 <td style="text-align: right;">
  -1.000 </td><td style="text-align: right;">  1.494 </td><td style="text-align: right;">  1.494 </td><td style="text-align: right;">   0(  0) </td><td style="text-align: right;">    0 </td><td style="text-align: right;">  0 </td><td style="text-align: right;"> 2.88   -1.99    0.00</td>
</tr>
<tr>
 <td style="text-align: right;">
  -1.120 </td><td style="text-align: right;">  1.150 </td><td style="text-align: right;">  1.043 </td><td style="text-align: right;"> 655( 54) </td><td style="text-align: right;">  220 </td><td style="text-align: right;"> 71 </td><td style="text-align: right;"> 3.63    0.15   -1.82</td>
</tr>
<tr>
 <td style="text-align: right;">
  -1.240 </td><td style="text-align: right;">  1.226 </td><td style="text-align: right;">  1.043 </td><td style="text-align: right;"> 555( 49) </td><td style="text-align: right;">  316 </td><td style="text-align: right;"> 80 </td><td style="text-align: right;"> 3.83    0.05   -1.71</td>
</tr>
<tr>
 <td style="text-align: right;">
   ... </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  -2.320 </td><td style="text-align: right;">  0.988 </td><td style="text-align: right;">  0.980 </td><td style="text-align: right;"> 147( 36) </td><td style="text-align: right;">  759 </td><td style="text-align: right;"> 58 </td><td style="text-align: right;"> 3.00   -2.11    1.11</td>
</tr>
<tr>
 <td style="text-align: right;">
  -2.440 </td><td style="text-align: right;">  0.982 </td><td style="text-align: right;">  0.980 </td><td style="text-align: right;">  25( 31) </td><td style="text-align: right;">  884 </td><td style="text-align: right;"> 60 </td><td style="text-align: right;"> 2.89   -2.12    1.24</td>
</tr>
<tr>
 <td style="text-align: right;">
  -2.560 </td><td style="text-align: right;">  0.988 </td><td style="text-align: right;">  0.979 </td><td style="text-align: right;">  35( 61) </td><td style="text-align: right;">  850 </td><td style="text-align: right;"> 51 </td><td style="text-align: right;"> 3.00   -2.11    1.11</td>
</tr>
<tr>
 <td style="text-align: right;">
   ... </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  -3.760 </td><td style="text-align: right;">  0.964 </td><td style="text-align: right;">  0.964 </td><td style="text-align: right;">   2( 22) </td><td style="text-align: right;">  961 </td><td style="text-align: right;"> 15 </td><td style="text-align: right;"> 2.57   -2.15    1.55</td>
</tr>
<tr>
 <td style="text-align: right;">
  -3.880 </td><td style="text-align: right;">  0.964 </td><td style="text-align: right;">  0.964 </td><td style="text-align: right;">   0( 17) </td><td style="text-align: right;">  961 </td><td style="text-align: right;"> 22 </td><td style="text-align: right;"> 2.57   -2.15    1.55</td>
</tr>
<tr>
 <td style="text-align: right;">
  -4.000 </td><td style="text-align: right;">  0.964 </td><td style="text-align: right;">  0.964 </td><td style="text-align: right;">   0( 13) </td><td style="text-align: right;">  970 </td><td style="text-align: right;"> 17 </td><td style="text-align: right;"> 2.57   -2.15    1.55</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p><em>log-temp:</em><br /> 
logarithm (base 10) of the temperature at the last iteration before the print out.<br />
<em>current score:</em><br /> 
the score after the last iterations.<br />
<em>best score:</em><br /> 
the single lowest score seen at any iteration.<br />
<em>acc:</em><br /> 
the number of proposed moves that were accepted since the last print
out for which the model changed, within parenthesis, the number of
those that were identical in score to the move before acceptance.<br />
<em>rej:</em><br /> 
the number of proposed moves that gave numerically acceptable results,
but were rejected by the simulated annealing algorithm since the last
print out.<br />
<em>sing:</em><br /> 
the number of proposed moves that were rejected because they gave
numerically unacceptable results, for example because they yielded a
singular system.<br />
<em>current parameters:</em><br /> 
the values of the coefficients (first for the intercept, then for the
linear (separate) components, then for the logic trees).
</p>
<p>This information can be used to judge the convergence of the simulated
annealing algorithm, as described in the help file of
<code>logreg.anneal.control</code>.  Typically we want (i) the number of
acceptances to be high in the beginning, (ii) the number of
acceptances with different scores to be low at the end, and (iii) the
number of iterations when the fraction of acceptances is moderate to
be as large as possible.
</p>
<p><b>Find the best scoring models for various sizes</b>
<code>(select = 2)</code>
</p>
<p>During the iterations the same information as for <em>find the best
scoring model of any size</em>, for each size model considered.
</p>
<p><b>Carry out cross-validation for model selection</b>
<code>(select = 3)</code>
</p>
<p>Information about the simulated annealing as described above can be
printed out. Otherwise, during the cross-validation process
information like
</p>
<p><code>
Step  5 of 10 [ 2 trees; 4 leaves] The CV score is   1.127  1.120  1.052  1.122
</code>
</p>
<p>The first of the four scores is the <em>training</em>-set score on the
current validation sample, the second score is the average of all the
<em>training</em>-set scores that have been processed for this model
size, the third is the <em>test</em>-set score on the current
validation sample, and the fourth score is the average of all the
<em>test</em>-set scores that have been processed for this model size.
Typically we would prefer the model with the lowest test-set score
average over all cross-validation steps.
</p>
<p><b>Carry out a permutation test to check for signal in the data</b>
<code>(select = 4)</code>
</p>
<p>Information about the simulated annealing as described above can be
printed out. Otherwise, first the score of the model of size 0
(typically only fitting an intercept) and the score of the best model
are printed out. Then during the permutation lines like
</p>
<p><code>
Permutation number   21  out of   50  has score       1.47777
</code>
</p>
<p>are printed. Each score is the result of fitting a logic tree model,
on data where the response has been permuted.  Typically we would
believe that there is signal in the data if most permutations have
worse (higher) scores than the best model, but we may believe that
there is substantial over-fitting going on if these permutation scores
are much better (lower) than the score of the model of size 0.
</p>
<p><b>Carry out a permutation test for model selection</b>
<code>(select = 5)</code>
</p>
<p>To be able to run this option, an object of class <code>logreg</code> that
was run with <code>(select = 2)</code> needs to be in place.  Information
about the simulated annealing as described above can be printed
out. Otherwise, lines like
</p>
<p><code>
Permutation number    8  out of   25  has score       1.00767    model size  3 with 2 tree(s)</code>
</p>
<p>are printed.  We can compare these scores to the tree of the same size
and the best tree.  If the scores are about the same as the one for
the best tree, we think that the &ldquo;true&rdquo; model size may be the one
that is being tested, while if the scores are much worse, the true
model size may be larger. The comparison with the model of the same
size suggests us again how much over-fitting may be going on.
<code>plot.logreg</code> generates informative histograms.
</p>
<p><b>Greedy stepwise selection of Logic Regression models</b>
<code>(select = 6)</code>
</p>
<p>The scores of the best greedy models of each size are printed. 
</p>
<p><b>Monte Carlo Logic Regression</b>
<code>(select = 7)</code>
</p>
<p>A status line is printed every so many iterations. This information
is probably not very useful, other than that it helps you figure out how
far the code is.
</p>
<p><b>PARAMETERS</b>
</p>
<p>As Logic Regression is written in Fortran 77 some parameters had to be
hard coded in. The default values of these parameters are
</p>
<p>maximum number of columns in the input file: 1000<br />
maximum number of leaves in a logic tree: 128<br />
maximum number of logic trees: 5<br />
maximum number of separate parameters: 50<br />
maximum number of total parameters(separate + trees): 55<br />
</p>
<p>If these parameters are not large enough (an error message will let you
know this), you need to reset them in <b>slogic.f</b> and recompile.
In particular, the statements defining these parameters are 
</p>
<p><code>PARAMETER (LGCn2MAX   =  1000)</code><br />
<code>PARAMETER (LGCnknMAX  =   128)</code><br /> 
<code>PARAMETER (LGCntrMAX  =     5)</code><br /> 
<code>PARAMETER (LGCnsepMAX =    50)</code><br /> 
<code>PARAMETER (LGCbetaMAX =    55)</code><br /> 
</p>
<p>The unfortunate thing is that you will have to change these parameter
definitions several times in the code. So search until you have found
them all.</p>


<h3>Value</h3>

<p>An object of the class <code>logreg</code>. This contains virtually all
input parameters, and in addition<br />
</p>
<p>If <code>select = 1:</code>
</p>
<p>an object of class <code>logregmodel</code>: the Logic
Regression model. This model contains a list of <code>ntrees</code> objects
of class <code>logregtree</code>.<br />
</p>
<p>If <code>select = 2</code> or <code>select = 6</code>:
</p>
<p><code>nmodels:</code> 
the number of models fitted.<br />
<code>allscores:</code> 
a matrix with 3 columns, containing the scores of all models. Column 1
contains the score, column 2 the number of leaves and column 3 the
number of trees.<br />
<code>alltrees:</code> 
a list with <code>nmodels</code> objects of class <code>logregmodel</code>.<br />
</p>
<p>If <code>select = 3:</code>
</p>
<p><code>cvscores:</code>
a matrix with the results of cross validation. The <code>train.ave</code>
and <code>test.ave</code> columns for train and test contain running
averages of the scores for individual validation sets. As such these
scores are of most interest for the rows where <code>k=kfold</code>.<br />
</p>
<p>If <code>select = 4:</code>
</p>
<p><code>nullscore:</code>
score of the null-model.<br />
<code>bestscore:</code> 
score of the best model.<br />
<code>randscores:</code>
scores of the permutations; vector of length <code>nrep</code>.<br />
</p>
<p>If <code>select = 5:</code>
</p>
<p><code>bestscore:</code> 
score of the best model.<br />
<code>randscores:</code>
scores of the permutations; each column corresponds to one model
size.<br />
</p>
<p>If <code>select = 7:</code>
</p>
<p><code>size:</code> a matrix with two columns, indicating which size models
were fit how often.<br />
<code>single:</code> a vector with as many elements as there are binary
predictors. <code>single[i]</code> shows how often predictor  <code>i</code> is
in any of the MCMC models. Note that when a predictor is twice in the
same model, it is only counted once, thus, in particular,
<code>sum(size[,1]*size[,2]</code> will typically be slightly larger
than <code>sum(single)</code>.<br />
<code>double:</code> square matrix with as size the number of binary predictors.
<code>double[i,j]</code> shows how often predictors <code>i</code> and <code>j</code> are
in the same tree of the same MCMC model if
<code>i&gt;j</code>, if <code>i&lt;=j</code> <code>double[i,j]</code> equals zero. Note that
for models
with several logic trees two predictors can both be in the model but not
be in the same tree.<br />
<code>triple:</code> square 3D array with as size the number of binary predictors.
See <code>double</code>, but here <code>triple[i,j,k]</code> shows how often 
three predictors are jointly in one logic tree.<br />
In addition, the file
<code>slogiclisting.tmp</code> 
in the current working directory can be
created. This file contains a compact listing of all models visited. Column 1:
proportional to the log posterior probability of the model; column 2: score
(log-likelihood); column 3: how often was this model visited, column 4 through 3 + maximum number
of leaves: summary of the first tree, if there are two trees,
column 4 + maximum number of leaves through 3 + twice the maximum number
of leaves contains the second tree, and so on.<br />
In this compact notation, leaves are in the same sequence as the rows in
a <code>logregtree</code> object; a zero means that the leave is empty, a
1000 means an &ldquo;and&rdquo; and a 2000 an &ldquo;or&rdquo;, any other positive number
means a predictor and a negative number means &ldquo;not&rdquo; that predictor.<br />
The <code>mc.control</code> element <code>output</code> can be used to surppress the
creation of
<code>double</code>, <code>triple</code>, and/or <code>slogiclisting.tmp</code>. There doesn't
seem to be too much use in surppressing <code>double</code>. Surpressing
<code>triple</code> speeds up computations a bit (in particular on
machines with limited memory when there are many binary predictors), and reduces the size of
both the code and the object, surppressing <code>slogiclisting.tmp</code>
saves the creation of a possibly very large file, which can slow
down the code considerably. See <code>logreg.mc.control</code> for details.
</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>
<p>Kooperberg C, Ruczinski I, LeBlanc ML, Hsu L (2001). Sequence Analysis
using Logic Regression, <em>Genetic Epidemiology</em>, <b>21</b>,
S626-S631.
</p>
<p>Kooperberg C, Ruczinki I (2005). Identifying interacting SNPs using
Monte Carlo Logic Regression, <em>Genetic Epidemiology</em>, <b>28</b>, 157-170.
</p>
<p>Selected chapters from the dissertation of Ingo Ruczinski, available from
<a href="https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf">https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+eval.logreg">eval.logreg</a></code>,
<code><a href="#topic+frame.logreg">frame.logreg</a></code>,
<code><a href="#topic+plot.logreg">plot.logreg</a></code>,
<code><a href="#topic+print.logreg">print.logreg</a></code>,
<code><a href="#topic+predict.logreg">predict.logreg</a></code>,
<code><a href="#topic+logregtree">logregtree</a></code>,
<code><a href="#topic+plot.logregtree">plot.logregtree</a></code>,
<code><a href="#topic+print.logregtree">print.logregtree</a></code>,
<code><a href="#topic+logregmodel">logregmodel</a></code>,
<code><a href="#topic+plot.logregtree">plot.logregtree</a></code>,
<code><a href="#topic+print.logregtree">print.logregtree</a></code>,
<code><a href="#topic+logreg.myown">logreg.myown</a></code>,
<code><a href="#topic+logreg.anneal.control">logreg.anneal.control</a></code>,
<code><a href="#topic+logreg.tree.control">logreg.tree.control</a></code>, 
<code><a href="#topic+logreg.mc.control">logreg.mc.control</a></code>,
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit1,logreg.savefit2,logreg.savefit3,logreg.savefit4,
     logreg.savefit5,logreg.savefit6,logreg.savefit7,logreg.testdat)

myanneal &lt;- logreg.anneal.control(start = -1, end = -4, iter = 500, update = 100)
# in practie we would use 25000 iterations or far more - the use of 500 is only
# to have the examples run fast
## Not run: myanneal &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 500)
fit1 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21], type = 2,
               select = 1, ntrees = 2, anneal.control = myanneal)
# the best score should be in the 0.95-1.10 range
plot(fit1)
# you'll probably see X1-X4 as well as a few noise predictors
# use logreg.savefit1 for the results with 25000 iterations
 plot(logreg.savefit1)
 print(logreg.savefit1)
 z &lt;- predict(logreg.savefit1)
 plot(z, logreg.testdat[,1]-z, xlab="fitted values", ylab="residuals")
# there are some streaks, thanks to the very discrete predictions
#
# a bit less output
 myanneal2 &lt;- logreg.anneal.control(start = -1, end = -4, iter = 500, update = 0)
# in practie we would use 25000 iterations or more - the use of 500 is only 
# to have the examples run fast
## Not run: myanneal2 &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 0)
#
# fit multiple models
 fit2 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21], type = 2,
               select = 2, ntrees = c(1,2), nleaves =c(1,7), anneal.control = myanneal2)
# equivalent
## Not run: fit2 &lt;- logreg(select = 2, ntrees = c(1,2), nleaves =c(1,7), oldfit = fit1,
                anneal.control = myanneal2)
## End(Not run)
 plot(fit2)
# use logreg.savefit2 for the results with 25000 iterations
 plot(logreg.savefit2)
 print(logreg.savefit2)
# After an initial steep decline, the scores only get slightly better
# for models with more than four leaves and two trees. 
#
# cross validation
 fit3 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21], type = 2,
                select = 3, ntrees = c(1,2), nleaves=c(1,7), anneal.control = myanneal2)
# equivalent
## Not run: fit3 &lt;- logreg(select = 3, oldfit = fit2)
 plot(fit3)
# use logreg.savefit3 for the results with 25000 iterations
 plot(logreg.savefit3)
# 4 leaves, 2 trees should top
# null model test
 fit4 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21], type = 2,
                select = 4, ntrees = 2, anneal.control = myanneal2)
# equivalent
## Not run: fit4 &lt;- logreg(select = 4, anneal.control = myanneal2, oldfit = fit1)
 plot(fit4)
# use logreg.savefit4 for the results with 25000 iterations
plot(logreg.savefit4)
# A histogram of the 25 scores obtained from the permutation test. Also shown
# are the scores for the best scoring model with one logic tree, and the null
# model (no tree). Since the permutation scores are not even close to the score
# of the best model with one tree (fit on the original data), there is overwhelming
# evidence against the null hypothesis that there was no signal in the data. 
 fit5 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21], type = 2,
            select = 5, ntrees = c(1,2), nleaves=c(1,7), anneal.control = myanneal2,
            nrep = 10, oldfit = fit2)
# equivalent
## Not run: fit5 &lt;- logreg(select = 5, nrep = 10, oldfit = fit2)
 plot(fit5)
# use logreg.savefit5 for the results with 25000 iterations and 25 permutations
 plot(logreg.savefit5)
# The permutation scores improve until we condition on a model with two trees and
# four leaves, and then do not change very much anymore. This indicates that the
# best model has indeed four leaves.
#
# greedy selection
 fit6 &lt;- logreg(select = 6, ntrees = 2, nleaves =c(1,12), oldfit = fit1)
 plot(fit6)
# use logreg.savefit6 for the results with 25000 iterations
 plot(logreg.savefit6)
#
# Monte Carlo Logic Regression
fit7 &lt;- logreg(select = 7, oldfit = fit1, mc.control=
               logreg.mc.control(nburn=500, niter=2500, hyperpars=log(2), output=-2))
# we need many more iterations for reasonable results
## Not run: logreg.savefit7 &lt;- logreg(select = 7, oldfit = fit1, mc.control=
               logreg.mc.control(nburn=1000, niter=100000, hyperpars=log(2)))
## End(Not run)
#
plot(fit7)
# use logreg.savefit7 for the results with 100,000 iterations
plot(logreg.savefit7)

</code></pre>

<hr>
<h2 id='logreg.anneal.control'>Control for Logic Regression</h2><span id='topic+logreg.anneal.control'></span>

<h3>Description</h3>

<p>Control of simulated annealing parameters needed in
<code>logreg</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>logreg.anneal.control(start=0, end=0, iter=0, earlyout=0, update=0)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logreg.anneal.control_+3A_start">start</code></td>
<td>
<p>the upper temperature (on a log10 scale) in the annealing
chain. I.e. if <code>start = 3</code>, the annealing chain starts at
temperature 1000. The acceptance function is the usual
<code>min(1,exp(-diff(scores)/temp))</code>, so any temperature larger than what
would be expected as possible differences between any two models
pretty much generates a random walk in the beginning, and means that
you need to wait longer on results. A too low starting temperature
means that the chain may end up in a local optimal (rather than global
optimal) solution.  If you select both <code>start</code> and <code>end</code> the
default of 0, the program will attempt to find reasonable numbers
itself (it is known to be only moderately successful in this though).</p>
</td></tr>
<tr><td><code id="logreg.anneal.control_+3A_end">end</code></td>
<td>
<p>the lower temperature (on a log10 scale) in the annealing
chain. I.e. if <code>end</code> is -2, the annealing chain ends at
temperature 0.01. If this temperature is very low one can use the
early out possibility listed below, as otherwise the chain may run
longer than desired!</p>
</td></tr>
<tr><td><code id="logreg.anneal.control_+3A_iter">iter</code></td>
<td>
<p>the total number of iterations in the annealing chain.
This is the total over all annealing chains, not the number of
iterations of a chain at a given temperature. If this number is too
small the chain may not find a good (the best) solution, if the chain
is too long the program may take long...</p>
</td></tr>
<tr><td><code id="logreg.anneal.control_+3A_earlyout">earlyout</code></td>
<td>
<p>if the <code>end</code> temperature is very low, the simulated
annealing algorithm may not move any more, but one still needs to wait
on all possible moves being evaluated (and rejected)!  An early out
possibility is offered. If during consecutive five blocks of
<code>earlyout</code> iterations, in each block 10 or fewer moves are
accepted (for which the score changes), the program terminates.  This
is a desirable option after one is convinced the program otherwise
runs fine: it can be dangerous on the first run.</p>
</td></tr>
<tr><td><code id="logreg.anneal.control_+3A_update">update</code></td>
<td>
<p>every how many iterations there should be an update of
the scores. I.e. if <code>update = 1000</code>, a score will get printed
every 1000 iterations. So if <code>iter = 100000</code> iterations, there
will be 100 updates on your screen.  If you <code>update = 0</code>, a one
line summary for each fitted model is printed.  If <code>update = -1</code>,
there is virtually no printed output.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing arguments take defaults. If the argument <code>start</code> is a
list with arguments <code>start</code>, <code>end</code>, <code>iter</code>,
<code>earlyout</code>, and <code>update</code>, those values take precedent of
directly specified values.
</p>
<p>This is a rough outline how the automated simulated annealing works:
The algorithm starts running at a very high temperature, and decreases
the temperature until the acceptance ratio of moves is below a certain
threshold (in the neighborhood of 95%). At this point we run longer
chains at fixed temperatures, and stop the search when the last &quot;n&quot;
consecutive moves have been rejected. If you think that the search was
either not sufficiently long or excessively long (both of which can
very well happen since it is pretty much impossible to specify default
values that are appropriate for all sorts of data and models), you can
over-write the default values.
</p>
<p>If you want more detailed information continue reading....
</p>
<p>These are some more detailed suggestions on how to set the parameters
for the beginning temperature, end temperature and number of
iterations for the Logic Regression routine. Note that if <code>start</code>
temperature and <code>end</code> temperature are both zero, the routine uses
its default values. The number of iterations <code>iter</code> is irrelevant
in this case. In our opinion, the default values are OK, but not
great, and you can usually do better if you're willing to invest time
in learning how to set the parameters.
</p>
<p>The starting temperature is the log(10) value of <code>start</code> -
i.e., if <code>start</code> is 2
it means iterations start at a temperature of 100. The
<code>end</code> temperature is again the log(10) value. The number of iterations
are equidistant on a log-scale.
</p>
<p>Considerations in setting these parameters.....
</p>
<p>1) <code>start</code> temperature. If this is too high you're &quot;wasting time&quot;, as
the algorithm is effectively just making a random walk at high
temperatures.  If the starting temperature is too low, you may already
be in a (too) localized region of the search space, and never reach a
good solution.  Typically a starting temperature that gives you 90%
or so acceptances (ignoring the rejected attempts, see below) is
good. Better a bit too high than too low.  But don't waste too much
time.
</p>
<p>2) <code>end</code> temperature. By the time that you reach the 
<code>end</code> temperature the
number of accepted iterations should be only a few per 1000, and the
best score should no longer change. Even zero acceptances is fine. If
there are many more acceptances, lower <code>end</code>. If there
are zero acceptances for many cycles in a row, raise it a bit. You can
set a lower <code>end</code> temperature than needed using the <code>earlyout</code>
test: if
in 5 consecutive cycles of 1000 iterations there are fewer than a
specified number of acceptances per cycle, the program terminates.
</p>
<p>3) number of iterations. What really counts is the number of
iterations in the &quot;crunch time&quot;, when the number of acceptances is,
say, more than 5% but fewer than 40% of the iterations.  If you
print summary statistics in blocks of 1000, you want to see as many
blocks with such acceptance numbers as possible. Obviously within what
is reasonable.
</p>
<p>Here are two examples, with my analysis....
</p>
<p>(A) <code>logreg.anneal.control(start = 2, end = 1, iter = 50000, update = 1000)
</code>
</p>
<p>The first few lines are (cutting of some of the last columns...)
</p>

<table>
<tr>
 <td style="text-align: right;">
log-temp </td><td style="text-align: right;"> current score </td><td style="text-align: right;">  best score </td><td style="text-align: right;">  acc / </td><td style="text-align: right;"> rej / </td><td style="text-align: right;"> sing  </td><td style="text-align: right;"> current parameters  </td>
</tr>
<tr>
 <td style="text-align: right;">
   2.000 </td><td style="text-align: right;"> 1198.785 </td><td style="text-align: right;"> 1198.785 </td><td style="text-align: right;">   0     </td><td style="text-align: right;"> 0   </td><td style="text-align: right;"> 0   </td><td style="text-align: right;"> 0.508 -0.368  -0.144 </td>
</tr>
<tr>
 <td style="text-align: right;">
   1.980 </td><td style="text-align: right;"> 1197.962 </td><td style="text-align: right;"> 1175.311 </td><td style="text-align: right;"> 719(18) </td><td style="text-align: right;"> 34  </td><td style="text-align: right;"> 229 </td><td style="text-align: right;"> 1.273 -0.275  -0.109 </td>
</tr>
<tr>
 <td style="text-align: right;">
   1.960 </td><td style="text-align: right;"> 1197.909 </td><td style="text-align: right;"> 1168.159 </td><td style="text-align: right;"> 722(11) </td><td style="text-align: right;"> 38  </td><td style="text-align: right;"> 229 </td><td style="text-align: right;"> 0.416 -0.345  -0.173 </td>
</tr>
<tr>
 <td style="text-align: right;">
   1.940 </td><td style="text-align: right;"> 1181.545 </td><td style="text-align: right;"> 1168.159 </td><td style="text-align: right;"> 715(19) </td><td style="text-align: right;"> 35  </td><td style="text-align: right;"> 231 </td><td style="text-align: right;"> 0.416 -0.345  -0.173 </td>
</tr>
<tr>
 <td style="text-align: right;">
   ... </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;">  </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
   1.020 </td><td style="text-align: right;"> 1198.258 </td><td style="text-align: right;"> 1167.578 </td><td style="text-align: right;"> 663(16) </td><td style="text-align: right;"> 128 </td><td style="text-align: right;"> 193 </td><td style="text-align: right;"> 1.685 -0.216  -0.024 </td>
</tr>
<tr>
 <td style="text-align: right;">
   1.000 </td><td style="text-align: right;"> 1198.756 </td><td style="text-align: right;"> 1167.578 </td><td style="text-align: right;"> 641(23) </td><td style="text-align: right;"> 104 </td><td style="text-align: right;"> 232 </td><td style="text-align: right;"> 1.685 -0.216  -0.024 </td>
</tr>
<tr>
 <td style="text-align: right;">
   1.000 </td><td style="text-align: right;"> 1198.756 </td><td style="text-align: right;"> 1167.578 </td><td style="text-align: right;"> 1( 0)   </td><td style="text-align: right;"> 0   </td><td style="text-align: right;"> 0   </td><td style="text-align: right;"> 1.685 -0.216  -0.024 </td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>Ignore the last line. This one is just showing a refitting of the best
model.  Otherwise, this suggests
(i) <code>end</code> is ***way*** too high, as there are still have
more than 600 acceptances in blocks of 1000. It is hard to judge what
<code>end</code> should be from this run.
(ii) The initial number of acceptances is really high
<code>(719+18)/(719+18+34))=95%</code> - but when <code>1.00</code> is reached it's at about
85%.  One could  change <code>start</code> to 1, or keep it at 2 and play it save.
</p>
<p>(B) <code>logreg.anneal.control(start = 2, end = -2, iter = 50000, update = 1000)</code>
-  different dataset/problem
</p>
<p>The first few lines are 
</p>

<table>
<tr>
 <td style="text-align: right;">
log-temp </td><td style="text-align: right;"> current score </td><td style="text-align: right;">  best score </td><td style="text-align: right;">  acc / </td><td style="text-align: right;"> rej / </td><td style="text-align: right;"> sing  </td><td style="text-align: right;"> current parameters  </td>
</tr>
<tr>
 <td style="text-align: right;">
   2.000 </td><td style="text-align: right;">   1198.785 </td><td style="text-align: right;"> 1198.785 </td><td style="text-align: right;">    0( 0) </td><td style="text-align: right;">  0 </td><td style="text-align: right;">  0 </td><td style="text-align: right;"> 0.50847  -0.36814 </td>
</tr>
<tr>
 <td style="text-align: right;">  
   1.918 </td><td style="text-align: right;">   1189.951 </td><td style="text-align: right;"> 1172.615 </td><td style="text-align: right;">  634(23) </td><td style="text-align: right;"> 22 </td><td style="text-align: right;">   322 </td><td style="text-align: right;"> 0.38163  -0.28031  </td>
</tr>
<tr>
 <td style="text-align: right;">
   1.837 </td><td style="text-align: right;">   1191.542 </td><td style="text-align: right;"> 1166.739 </td><td style="text-align: right;">  651(24) </td><td style="text-align: right;"> 32 </td><td style="text-align: right;">   293 </td><td style="text-align: right;"> 1.75646  -0.22451 </td>
</tr>
<tr>
 <td style="text-align: right;"> 
   1.755 </td><td style="text-align: right;">   1191.907 </td><td style="text-align: right;"> 1162.902 </td><td style="text-align: right;">  613(30) </td><td style="text-align: right;"> 20 </td><td style="text-align: right;">   337 </td><td style="text-align: right;"> 1.80210  -0.32276  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>The last few are
</p>

<table>
<tr>
 <td style="text-align: right;">
log-temp </td><td style="text-align: right;"> current score </td><td style="text-align: right;">  best score </td><td style="text-align: right;">  acc / </td><td style="text-align: right;"> rej / </td><td style="text-align: right;"> sing  </td><td style="text-align: right;"> current parameters  </td>
</tr>
<tr>
 <td style="text-align: right;">
  -1.837 </td><td style="text-align: right;">   1132.731 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">    0(18) </td><td style="text-align: right;">  701 </td><td style="text-align: right;">  281 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">
  -1.918 </td><td style="text-align: right;">   1132.731 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">    0(25) </td><td style="text-align: right;">  676 </td><td style="text-align: right;">  299 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">
  -2.000 </td><td style="text-align: right;">   1132.731 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">    0(17) </td><td style="text-align: right;">  718 </td><td style="text-align: right;">  265 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">
  -2.000 </td><td style="text-align: right;">   1132.731 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">    0( 0) </td><td style="text-align: right;">  0 </td><td style="text-align: right;">  1 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>But there really weren't any acceptances since
</p>

<table>
<tr>
 <td style="text-align: right;">
log-temp </td><td style="text-align: right;"> current score </td><td style="text-align: right;">  best score </td><td style="text-align: right;">  acc / </td><td style="text-align: right;"> rej / </td><td style="text-align: right;"> sing  </td><td style="text-align: right;"> current parameters  </td>
</tr>
<tr>
 <td style="text-align: right;">
  -0.449 </td><td style="text-align: right;">   1133.622 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">    4(21) </td><td style="text-align: right;">  875 </td><td style="text-align: right;">  100 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">
  -0.531 </td><td style="text-align: right;">   1133.622 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">    0(19) </td><td style="text-align: right;">  829 </td><td style="text-align: right;">  152 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">
  -0.612 </td><td style="text-align: right;">   1133.622 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">    0(33) </td><td style="text-align: right;">  808 </td><td style="text-align: right;">  159 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>Going down from 400 to fewer than 10 acceptances went pretty fast....
</p>

<table>
<tr>
 <td style="text-align: right;">
log-temp </td><td style="text-align: right;"> current score </td><td style="text-align: right;">  best score </td><td style="text-align: right;">  acc / </td><td style="text-align: right;"> rej / </td><td style="text-align: right;"> sing  </td><td style="text-align: right;"> current parameters  </td>
</tr>
<tr>
 <td style="text-align: right;">
   0.776 </td><td style="text-align: right;">   1182.156 </td><td style="text-align: right;"> 1156.354 </td><td style="text-align: right;">  464(31) </td><td style="text-align: right;">  258 </td><td style="text-align: right;">  247 </td><td style="text-align: right;"> 1.00543  -0.26602 </td>
</tr>
<tr>
 <td style="text-align: right;">  
   0.694 </td><td style="text-align: right;">   1168.504 </td><td style="text-align: right;"> 1150.931 </td><td style="text-align: right;">  306(17) </td><td style="text-align: right;">  355 </td><td style="text-align: right;">  322 </td><td style="text-align: right;"> 1.56695  -0.43351 </td>
</tr>
<tr>
 <td style="text-align: right;"> 
   0.612 </td><td style="text-align: right;">   1167.747 </td><td style="text-align: right;"> 1150.931 </td><td style="text-align: right;">  230(38) </td><td style="text-align: right;">  383 </td><td style="text-align: right;">  349 </td><td style="text-align: right;"> 1.56695  -0.43351 </td>
</tr>
<tr>
 <td style="text-align: right;">  
   0.531 </td><td style="text-align: right;">   1162.085 </td><td style="text-align: right;"> 1145.920 </td><td style="text-align: right;">  124(12) </td><td style="text-align: right;">  571 </td><td style="text-align: right;">  293 </td><td style="text-align: right;"> 1.15376  -0.15223 </td>
</tr>
<tr>
 <td style="text-align: right;">   
   0.449 </td><td style="text-align: right;">   1143.841 </td><td style="text-align: right;"> 1142.321 </td><td style="text-align: right;">   63(20) </td><td style="text-align: right;">  590 </td><td style="text-align: right;">  327 </td><td style="text-align: right;"> 2.20150  -0.43795 </td>
</tr>
<tr>
 <td style="text-align: right;">   
   0.367 </td><td style="text-align: right;">   1176.152 </td><td style="text-align: right;"> 1142.321 </td><td style="text-align: right;">  106(21) </td><td style="text-align: right;">  649 </td><td style="text-align: right;">  224 </td><td style="text-align: right;"> 2.20150  -0.43795 </td>
</tr>
<tr>
 <td style="text-align: right;">   
   0.286 </td><td style="text-align: right;">   1138.384 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">   62(18) </td><td style="text-align: right;">  731 </td><td style="text-align: right;">  189 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">   
   0.204 </td><td style="text-align: right;">   1138.224 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">   11(27) </td><td style="text-align: right;">  823 </td><td style="text-align: right;">  139 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">  
   0.122 </td><td style="text-align: right;">   1150.370 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">   15(12) </td><td style="text-align: right;">  722 </td><td style="text-align: right;">  251 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">   
   0.041 </td><td style="text-align: right;">   1144.536 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">   30(19) </td><td style="text-align: right;">  789 </td><td style="text-align: right;">  162 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">   
  -0.041 </td><td style="text-align: right;">   1137.898 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">   21(25) </td><td style="text-align: right;">  911 </td><td style="text-align: right;"> 43 </td><td style="text-align: right;"> 0.00513  -0.45994 </td>
</tr>
<tr>
 <td style="text-align: right;">  
  -0.122 </td><td style="text-align: right;">   1139.403 </td><td style="text-align: right;"> 1131.866 </td><td style="text-align: right;">   12(30) </td><td style="text-align: right;">  883 </td><td style="text-align: right;"> 75 </td><td style="text-align: right;"> 0.00513  -0.45994  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>What does this tell me -
(i) <code>start</code> was probably a bit high - no real harm
done,
(ii) <code>end</code> was lower than needed. Since there really
weren't any acceptances after 10log(T) was about (<code>-0.5</code>), an ending
log-temperature of (<code>-1</code>) would have been fine,
(iii) there were far too few runs. The crunch time didn't take more
than about 10 cycles (10000 iterations). You see that this is the time
the &quot;best model&quot; decreased quite a bit - from 1156 to 1131. I would
want to spend considerably more than 10000 iterations during this
period for a larger problem (how many depends very much on the size of
the problem). So, I'd pick (A)<code>logreg.anneal.control(start = 2,
end = -1, iter = 200000, update = 5000)</code>.  Since the total range is
reduced from <code>2-(-2)=4</code> to <code>2-(-1)=3</code>, over a range of 10log temperatures
of 1 there will be <code>200000/3=67000</code> rather than <code>50000/4=12500</code>
iterations.  I would repeat this run a couple of times.
</p>
<p>In general I may sometimes run several models, and check the scores of
the best models.  If those are all the same, I'm very happy, if
they're similar but not identical, it's OK, though I may run one or
two longer chains. If they're very different, something is wrong.  For
the permutation test and cross-validation I am usually less picky on
convergence.
</p>


<h3>Value</h3>

<p>A list with arguments <code>start</code>, <code>end</code>, <code>iter</code>,
<code>earlyout</code>, and <code>update</code>, that can be used as the value of
the argument <code>anneal.control</code> of<code>logreg</code>.</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>
<p>Selected chapters from the dissertation of Ingo Ruczinski, available from
<a href="https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf">https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+logreg.mc.control">logreg.mc.control</a></code>,
<code><a href="#topic+logreg.tree.control">logreg.tree.control</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>myannealcontrol &lt;- logreg.anneal.control(start = 2, end = -2, iter = 50000, update = 1000)
</code></pre>

<hr>
<h2 id='logreg.mc.control'>Control for Logic Regression</h2><span id='topic+logreg.mc.control'></span>

<h3>Description</h3>

<p>Control of MCMC annealing parameters needed in
<code>logreg</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>logreg.mc.control(nburn=1000, niter=25000, hyperpars=0, update=0,
       output=4)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logreg.mc.control_+3A_nburn">nburn</code></td>
<td>
<p>number of burn in MCMC iterations that are ignored 
when computing summaries</p>
</td></tr>
<tr><td><code id="logreg.mc.control_+3A_niter">niter</code></td>
<td>
<p>number of MCMC iterations that are used to compute
summary statistics</p>
</td></tr>
<tr><td><code id="logreg.mc.control_+3A_hyperpars">hyperpars</code></td>
<td>
<p>hyperparameters. The code allows up to 10 such
parameters, but currently only one is used. In particular,
<code>log(P(size=k)/P(size=k+1))</code> equals <code>hyperpars[1]</code>, where
P is the prior on model size. Since a maximum model size (specified
in <code>logreg</code> is being used, <code>hyperpars[1]</code> can even be 
smaller than 0.</p>
</td></tr>
<tr><td><code id="logreg.mc.control_+3A_update">update</code></td>
<td>
<p>every how many iterations there should be an update of
the scores. I.e. if <code>update = 1000</code>, a score will get printed
every 1000 iterations. So if <code>iter = 100000</code> iterations, there
will be 100 updates on your screen.  If <code>update = 0</code>, a one
line summary for each fitted model is printed.  If <code>update = -1</code>,
there is virtually no printed output.</p>
</td></tr>
<tr><td><code id="logreg.mc.control_+3A_output">output</code></td>
<td>
<p>If <code>abs(output) &gt; 1</code> bivariate statistics
are gathered, if <code>abs(output) &gt; 2</code> trivariate  statistics
are also gathered, otherwise only univariate statistics are gathered. If
<code>output &gt; 0</code>
all fitted models are saved in a text file &ldquo;slogiclisting.tmp&rdquo;,
if <code>output &lt; 0</code> this does not happen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Considerations for setting <code>nburn</code> and <code>niter</code> are as for any
MCMC problem. In our experience Logic Regression mixes quickly, and
a real small <code>nburn</code> (1000, for example) suffices. If there are
many trees and large models <code>niter</code> may need to be large.
</p>
<p>A more detailed description of the output options can be found
in the helpfile of <code>logreg</code>.</p>


<h3>Value</h3>

<p>A list with arguments <code>nburn</code>, <code>niter</code>, <code>hyperpars</code>,
<code>update</code>, and <code>output</code>, that can be used as the value of
the argument <code>mc.control</code> of <code>logreg</code>.</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>
<p>Kooperberg C, Ruczinki I (2005). Identifying interacting SNPs using
Monte Carlo Logic Regression, <em>Genetic Epidemiology</em>, <b>28</b>, 157-170.</p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+logreg.tree.control">logreg.tree.control</a></code>,
<code><a href="#topic+logreg.anneal.control">logreg.anneal.control</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mymccontrol &lt;- logreg.mc.control(nburn = 500, niter = 500000, update = 25000,
hyperpars = log(2), output = -2)
</code></pre>

<hr>
<h2 id='logreg.myown'>Writing your own Logic Regression scoring function</h2><span id='topic+logreg.myown'></span>

<h3>Description</h3>

<p>Help file for writing your own scoring function for
<code>logreg</code>!</p>


<h3>Usage</h3>

<pre><code class='language-R'>logreg.myown()</code></pre>


<h3>Details</h3>

<p>You can write your own scoring function for <code>logreg</code>!  This may
be useful if you have a model other than those which we already
programmed in.
</p>
<p>Essentially you need to provide two routines in the file
<b>Myownscoring.f</b>:<br />
(i) A routine <em>Myownfitting</em> which fits your model: it
provides a coefficient (beta) for each of the logic trees and provides
a score of how good the model is. Low scores are good. (So add a minus
sign if your score is a log-likelihood.)<br />
(ii) A routine <em>Myownscoring</em> which - given the betas -
provides the score of your model. [If you don't use cross-validation,
this second routine is not needed, though some dummy routine to
satisfy the compiler should still be provided.]
</p>
<p>After recompilation, you can fit your model using the option <code>
type = 0</code> in <code>logreg</code>.  Below we give an example for a version
of the My.own functions for conditional logistic regression which are
also provided as <b>inst/condlogic.ff</b> when you downloaded the files.
</p>
<p><b>PROGRAMMING DETAILS</b>
</p>
<p>Below is a list of variables that are passed on. Most of them are as
you expect - response, predictors (binary ones and continuous ones),
number of cases, number of predictors.  In addition there are two
columns - <code>dcph</code> and <code>weight</code> - that can either be used to pass on an
auxiliary variable for each case (discrete for 
<code>dcph</code> and continuous for
<code>weight</code>), or even some overall auxiliary variables - as these numbers
are not used anywhere else.  If you do not need any of the variables -
just ignore them!
</p>
<p><code>prtr</code>:<br /> 
the predictions of the logic trees in the current model: this is an
integer matrix of size <code>n1</code> times <code>ntr</code> - although only the
first <code>nop</code> columns contain useful information.
<br />
<code>rsp</code>:<br />
the response variable: this is a real (single precision) vector of
length <code>n1</code>.  
<br />
<code>dcph</code>: <br />
censor times: this is an integer vector of length <code>n1</code> this could
be used as an auxiliary (integer) vector - as it is just passed on.
(There is no check that this is a 0/1 variable, when you use your own
scoring function.)  For example, you could use this to pass on
something like cluster membership.
<br />
<code>weight</code>:<br />
weights for the cases this is a real vector of length <code>n1</code>.  this
could be used as an auxiliary (real) vector - as it is just passed on.
There is no check that these numbers are positive, when you choose
your own scoring function.
<br />
<code>ordrs</code>:<br /> 
the order (by response size) of the cases This is an integer vector of
length <code>n1</code>. For the case with the smallest response this one is
1, for the second smallest 2, and so on. Ties are resolved arbitrary.
Always computed, although only used for proportional hazards
models. Use it as you wish.
<br />
<code>n1</code>:<br /> 
the total number of cases in the data.
<br />
<code>ntr</code>:<br /> 
the number of logic trees ALLOWED in the tree.
<br />
<code>nop</code>:<br /> 
the number of logic trees in the CURRENT model.  The subroutines
should work if <code>nop</code> is 0.
<br />
<code>wh</code>:<br /> 
the index of the tree that has been edited in the last move - i.e. the
column of <code>prtr</code> that has changes since the last call.
<br />
<code>nsep</code>:<br /> 
number of variables that get fit a separate parameter The subroutines
should work if <code>nsep</code> is 0.
<br />
<code>seps</code>:<br /> 
array of the above variables - this is a single precision matrix of
size <code>nsep</code> times <code>n1</code>. Note that <code>seps</code> and
<code>prtr</code> are stored in different directions.
</p>
<p>For <em>Myownfitting</em> you should return:<br />
<code>betas</code>:<br /> 
a vector of parameters of the model that you fit.  <code>betas(0)</code>
should be the parameter for the intercept <code>betas(1:nsep)</code> should
be the parameters for the continuous variables in seps
<code>betas((nsep+1):(nsep+nop))</code> should be the parameters for the
binary trees in prtr if you have more parameters, use <code>dcph</code>, or
<code>weight</code>; these variables will not be printed however.
<br />
<code>score</code>:<br /> 
whatever score you assign to your model small should be good
(i.e. deviance or -log.likelihood).
<br />
<code>reject</code>:<br /> 
an indicator whether or not to reject the proposed move *regardless*
of the score (for example when an iteration necessary to determine the
score failed to converge (0 = move is OK ; 1 = reject move) set this
one to 0 if there is no such condition.
</p>
<p>You are allowed to change the values of dcph, and weight.
</p>
<p>For <em>Myownscoring</em> additional input is:<br />
<code>betas</code>:
the coefficients
</p>
<p>You should return:<br />
<code>score</code>:
whatever score you assign to your model small should be good
(i.e. deviance or -log.likelihood). If the model &quot;crashes&quot;, you should
simply return a very large number.
</p>
<p>While we try to prevent that models are singular, it is possible that
for your model a single or degenerate model is passed on for
evaluation. For Myownfitting you can pass the model back with
<code>reject = 1</code>, for Myownscoring you can pass it on with a very large
value for <code>score</code>.  Currently Myownscoring.f contains empty frames for
the scoring functions; condlogic.ff contains an example with
conditional logistic regression.
</p>
<p>The logic regression program is written in Fortran 77.
</p>
<p><b>CONDITIONAL LOGISTIC REGRESSION</b>
</p>
<p>A function for a conditional logistic regression score function is
attached as an example function on how to write your own scoring
function for Logic Regression.  Obviously you can also use it if you
have conditional logistic data.
</p>
<p>Conditional logistic regression is common model fitting technique for
matched case-control studies, in which each case is matched with one
or more controls. (In conditional logistic regression several cases
could be matched to several controls, in the implementation provided
here only one case can be matched with each group of controls.)
Conditional logistic regression models are parameterized like regular
logistic regression models, except that the intercept is no longer
identifiable. See, for example, Breslow and Day - Volume 1 (1990,
Statistical Methods in Cancer Research, International Agency for
Research on Cancer, Lyon) for details.  Conditional logistic
regression models are most easily fit using a stratified proportional
hazards model (if there is one-to-one case-control matching it can
also be fit using logistic regression, but that method breaks down if
there is more than one control per case). Each group of a case and
controls is one stratum. All cases get an arbitrarily event time of
1.00, and all controls get a censoring time of 2.00.
</p>
<p>In our implementation we use the response column to indicate the
matching. For all controls this column is 0, for a case it is k,
indicating that the next k records are the matched controls for the
current case. Thus, we order our cases so that each case is followed
by its controls. Cases with a negative response are put in a stratum
-1, which is not used in any computations.  This has implications for
cross-validation. See below.
</p>
<p>In <em>Myownfitting</em> and <em>Myownscoring</em> we first allocate
various vectors (strata, index, censoring variable) that are local, as
well as some work arrays that are used by our fitting routines.  (We
need to set some of the parameters for that, see the help page of
<code>logreg</code> for details.)  We then define <code>idx(j)=j</code> for
<code>j=1,n1</code>, and we define the <code>strata</code> and <code>delta</code>
vectors.  We use slightly modified versions of the proportional
hazards routines that are already used otherwise in the Logic
Regression program, to include stratification. After the model is
fitted, we assign minus the partial likelihood to <code>score(1)</code> and
(for <em>Myownfitting</em>) we pass on the betas.
</p>
<p>Recompile after replacing <em>Myownscoring.f</em> by
<em>condlogic.ff</em>
</p>
<p>The permutation and null model versions are not directly usable (we
could do some permutation tests, but they require more programming),
but we can use cross-validation. Obviously we should keep cases and
controls match. To that extend, we would run permutation with a
negative seed (see <code>logreg</code>) and we would take care ourselves
that case-control groups are in a random order, and that every block
has the same number of records. We achieve the later by adding some
records with response -1.  In particular, suppose that we have 19
pairs of case- (single) control data, and that we want to do 3-fold
cross validation. We would permute the sequence of the 19 pairs, and
add two records with response -1 after the 13th pair, and two records
with -1 at the end of the file, so that the total data file would have
42 records.
</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>
<p>Kooperberg C, Ruczinski I, LeBlanc ML, Hsu L (2001). Sequence Analysis
using Logic Regression, <em>Genetic Epidemiology</em>, <b>21</b>,
S626-S631.
</p>
<p>Selected chapters from the dissertation of Ingo Ruczinski, available from
<a href="https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf">https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>logreg.myown()      # displays this help file
help(logreg.myown)  # equivalent
</code></pre>

<hr>
<h2 id='logreg.savefit1'>Sample results for Logic Regression</h2><span id='topic+logreg.savefit1'></span><span id='topic+logreg.savefit2'></span><span id='topic+logreg.savefit3'></span><span id='topic+logreg.savefit4'></span><span id='topic+logreg.savefit5'></span><span id='topic+logreg.savefit6'></span><span id='topic+logreg.savefit7'></span>

<h3>Description</h3>

<p>The <code>logreg.savefit</code> objects are the results of fitting
<code><a href="#topic+logreg">logreg</a></code> with various options. The examples in
the functions of the LogicReg packages all use far fewer
iterations than is needed. (The number of iterations was reduced
to provide quick results for bug-checking.) The number of
iterations in the <code>logreg.savefit</code> objects are more
reasonable (though they would still be small for larger
problems). Otherwise the arguments used to fit the
<code>logreg.savefit</code> objects are identical as those used
in the examples of <code><a href="#topic+logreg">logreg</a></code>. The
<code>logreg.savefit</code> objects are used for examples involving
things like plotting, printing, and predicting.
</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit1)
print(logreg.savefit1$call)
data(logreg.savefit2)
print(logreg.savefit2$call)
data(logreg.savefit3)
print(logreg.savefit3$call)
data(logreg.savefit4)
print(logreg.savefit4$call)
data(logreg.savefit5)
print(logreg.savefit5$call)
data(logreg.savefit6)
print(logreg.savefit6$call)
data(logreg.savefit7)
print(logreg.savefit7$call)
</code></pre>

<hr>
<h2 id='logreg.testdat'>Test data for Logic Regression</h2><span id='topic+logreg.testdat'></span>

<h3>Description</h3>

<p>logreg.testdat has 500 cases, and 21 columns. Column 1 is the response
Y, column k+1, k=1,...,20 is (binary) predictor Xk.  Each predictor Xk
is simulated as an independent Bernoulli(pk) random variables, with
success probabilities pk between 0.1 and 0.9. The response variable is
simulated from the model
</p>
<p>Y = 3 + 1 L1 - 2 L2 + N(0,1),
</p>
<p>where L1=(X1 or X2) and L2=(X3 or X4). So the task is to use the
linear model in the logic regression framework to find L1 and L2.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.testdat)
</code></pre>

<hr>
<h2 id='logreg.tree.control'>Control for logreg</h2><span id='topic+logreg.tree.control'></span>

<h3>Description</h3>

<p>Control of various secondary parameters of tree shape
needed in <code>logreg</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>logreg.tree.control(treesize=8, opers=1, minmass=0, n1)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logreg.tree.control_+3A_treesize">treesize</code></td>
<td>
<p>specify the maximum number of allowed leaves per
logic tree.  Allowing one leave means that the tree is (at most) a
simple predictor, two leaves allows for trees such as (X1 or X2) or
(not X3 and X4). Four, eight and sixteen leaves allow for two, three
or four levels of operators.  To be able to interpret the results, do
not choose too many leaves. Since the model selection techniques
usually trim down the trees, it is recommend to allow at least four or
eight leaves per tree. </p>
</td></tr>
<tr><td><code id="logreg.tree.control_+3A_opers">opers</code></td>
<td>
<p>The default is to allow both &quot;and&quot; and &quot;or&quot; operators in
the logic trees. If the interest is in logic statements in disjunctive
normal form, use only one of the two operator types. Choose 1 for
both operators, 2 for only &quot;and&quot; and 3 for only &quot;or&quot;. </p>
</td></tr>
<tr><td><code id="logreg.tree.control_+3A_minmass">minmass</code></td>
<td>
<p>specify the minimum number of cases for which any tree
needs to be 1 and for which any tree needs to be 0 to be considered as
a logic tree in the model. This is to prevent that <code>logreg</code>, will
select trees with, for example, 999 1s and one 0 out of 1000 cases.
The default is to take 5% of the cases or 15, whatever is less. </p>
</td></tr>
<tr><td><code id="logreg.tree.control_+3A_n1">n1</code></td>
<td>
<p>if you specify the sample size <code>n1</code>, it is checked
that <code>minmass</code> is smaller than <code>n1/4</code>.  This option is used
by <code>logreg</code>, but is likely not useful for direct use. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing arguments take defaults. If the argument
<code>treesize</code> is a list with arguments <code>treesize</code>,
<code>opers</code>, and <code>minmass</code>, those values take precedent of
directly specified values. </p>


<h3>Value</h3>

<p>A list with components <code>treesize</code>, <code>opers</code>, and
<code>minmass</code>, that can be used as the value of the argument
<code>tree.control</code> of <code>logreg</code>. </p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>
<p>Selected chapters from the dissertation of Ingo Ruczinski, available from
<a href="https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf">https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+logreg.anneal.control">logreg.anneal.control</a></code>,
<code><a href="#topic+logreg.mc.control">logreg.mc.control</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>mytreecontrol &lt;- logreg.tree.control(treesize = 16, minmass = 10)
</code></pre>

<hr>
<h2 id='logregmodel'>Format of class logregmodel</h2><span id='topic+logregmodel'></span>

<h3>Description</h3>

<p>This help file contains a description of the format of
class logregmodel. </p>


<h3>Usage</h3>

<pre><code class='language-R'>logregmodel()</code></pre>


<h3>Value</h3>

<p>An object of class logregtree has the following components:
</p>
<table role = "presentation">
<tr><td><code>ntrees</code></td>
<td>
<p>the number of trees in the current model.</p>
</td></tr>
<tr><td><code>nleaves</code></td>
<td>
<p>the number of leaves for the fitted model.</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>the coefficients for this model.</p>
</td></tr>
<tr><td><code>score</code></td>
<td>
<p>the score of the fitted model.</p>
</td></tr>
<tr><td><code>trees</code></td>
<td>
<p>a list of <code>ntrees</code> objects of class
<code>logregtree</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>
<p>Kooperberg C, Ruczinski I, LeBlanc ML, Hsu L (2001). Sequence Analysis
using Logic Regression, <em>Genetic Epidemiology</em>, <b>21</b>,
S626-S631.
</p>
<p>Selected chapters from the dissertation of Ingo Ruczinski, available from
<a href="https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf">https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+plot.logregmodel">plot.logregmodel</a></code>,
<code><a href="#topic+print.logregmodel">print.logregmodel</a></code>,
<code><a href="#topic+logregtree">logregtree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>logregmodel()       # displays this help file
help(logregmodel)   # equivalent
</code></pre>

<hr>
<h2 id='logregtree'>Format of class logregtree</h2><span id='topic+logregtree'></span>

<h3>Description</h3>

<p>This help file contains a description of the format of
class logregtree. </p>


<h3>Usage</h3>

<pre><code class='language-R'>logregtree()</code></pre>


<h3>Details</h3>

<p>When storing trees, we number the location of the nodes using the
following scheme (this is an example for a tree with at most 8
<em>terminal</em> nodes, but the generalization should be obvious):
</p>

<table>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> 1</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> 2</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> 3</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> 4</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> 5</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> 6</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> 7</td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
8</td><td style="text-align: center;"> </td><td style="text-align: center;"> 9</td><td style="text-align: center;"> </td><td style="text-align: center;"> 10</td><td style="text-align: center;"> </td><td style="text-align: center;"> 11</td><td style="text-align: center;"> </td><td style="text-align: center;"> 
12</td><td style="text-align: center;"> </td><td style="text-align: center;"> 13</td><td style="text-align: center;"> </td><td style="text-align: center;"> 14</td><td style="text-align: center;"> </td><td style="text-align: center;"> 15</td>
</tr>
<tr>
 <td style="text-align: center;">
</td>
</tr>

</table>

<p>Each node may or may not be present in the current tree. If it is
present, it can contain an operator (&ldquo;and&rdquo; or &ldquo;or&rdquo;), in which case
it has to child nodes, or it can contain a variable, in which case the
node is a terminal node. It is also possible that the node does not
exist (as the user only specifies the maximum tree size, not the tree
size that is actually fitted).
</p>
<p>Output files have one line for each node. Each line contains 5
numbers:
</p>

<ol>
<li>
<p>the node number.
</p>
</li>
<li>
<p>does this node contain an &ldquo;and&rdquo; (1), an &ldquo;or&rdquo; (2), a variable (3),
or is the node empty (0).
</p>
</li>
<li>
<p>if the node contains a variable, which one is it; e.g. if this number
is 3 the node contains X3.
</p>
</li>
<li>
<p>if the node contains a variable, does it contain the regular variable
(0) or its complement (1)
</p>
</li>
<li>
<p>is the node empty (0) or not (1) (this information is redundant with
the second number)</p>
</li></ol>

<p><b>Example</b>
</p>

<table>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> AND</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> OR</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> OR</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> OR</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> OR</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> X20</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> OR</td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: center;">
X17</td><td style="text-align: center;"> </td><td style="text-align: center;"> X12</td><td style="text-align: center;"> </td><td style="text-align: center;"> X3</td><td style="text-align: center;"> </td><td style="text-align: center;"> 
X13c</td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> </td><td style="text-align: center;"> X2</td><td style="text-align: center;"> </td><td style="text-align: center;"> X1</td>
</tr>
<tr>
 <td style="text-align: center;">
</td>
</tr>

</table>

<p>is represented as
</p>

<table>
<tr>
 <td style="text-align: right;">
1 </td><td style="text-align: right;">  1 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
2 </td><td style="text-align: right;">  2 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
3 </td><td style="text-align: right;">  2 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
4 </td><td style="text-align: right;">  2 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
5 </td><td style="text-align: right;">  2 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
6 </td><td style="text-align: right;">  3 </td><td style="text-align: right;">  20 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
7 </td><td style="text-align: right;">  2 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
8 </td><td style="text-align: right;">  3 </td><td style="text-align: right;">  17 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
9 </td><td style="text-align: right;">  3 </td><td style="text-align: right;">  12 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
10 </td><td style="text-align: right;">  3 </td><td style="text-align: right;">   3 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
11 </td><td style="text-align: right;">  3 </td><td style="text-align: right;">  13 </td><td style="text-align: right;">   1 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
12 </td><td style="text-align: right;">  0 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   0</td>
</tr>
<tr>
 <td style="text-align: right;">
13 </td><td style="text-align: right;">  0 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   0</td>
</tr>
<tr>
 <td style="text-align: right;">
14 </td><td style="text-align: right;">  3 </td><td style="text-align: right;">   2 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
15 </td><td style="text-align: right;">  3 </td><td style="text-align: right;">   1 </td><td style="text-align: right;">   0 </td><td style="text-align: right;">   1</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>



<h3>Value</h3>

<p>An object of class logregtree is typically a substructure of an object
of the class <code>logregmodel</code>.  It will typically be the result of
using the fitting function <code>logreg</code>.  An object of class
logictree has the following components:
</p>
<table role = "presentation">
<tr><td><code>whichtree</code></td>
<td>
<p>the sequence number of the current tree within the
model.</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>the coefficients of this tree.</p>
</td></tr>
<tr><td><code>trees</code></td>
<td>
<p>a matrix (data.frame) with five columns; see below for
the format.</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>
<p>Selected chapters from the dissertation of Ingo Ruczinski, available from
<a href="https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf">https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+plot.logregtree">plot.logregtree</a></code>,
<code><a href="#topic+print.logregtree">print.logregtree</a></code>,
<code><a href="#topic+logregmodel">logregmodel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>logregtree()       # displays this help file
help(logregtree)   # equivalent
</code></pre>

<hr>
<h2 id='plot.logreg'>Plots for Logic Regression</h2><span id='topic+plot.logreg'></span>

<h3>Description</h3>

<p>Makes plots for objects fitted by <code>logreg</code>. </p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'logreg'
plot(x, pscript=FALSE, title=TRUE, ...)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.logreg_+3A_x">x</code></td>
<td>
<p>object of class <code>logreg</code>, typically the result of the
function <code>logreg</code>. </p>
</td></tr>
<tr><td><code id="plot.logreg_+3A_pscript">pscript</code></td>
<td>
<p>if <code>TRUE</code> all plots will be stored in postscript
files with distinct names.</p>
</td></tr>
<tr><td><code id="plot.logreg_+3A_title">title</code></td>
<td>
<p>if <code>TRUE</code> this generates a title for some plots,
typically listing the number of trees and the model size. </p>
</td></tr> 
<tr><td><code id="plot.logreg_+3A_...">...</code></td>
<td>
<p>graphical parameters can be given as arguments to plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The type of the plots generated depends on the value of
<code>x$select</code>.
</p>
<p>if <code>select = 1</code> the fitted trees for the results of
<em>find the best scoring model of any size</em> are plotted;
</p>
<p>if <code>select = 2</code> or <code>select = 6</code> the fitted trees are plotted, as well as a
graph of the scores for various model sizes versus model size for the
results of <em>find the best scoring models for various sizes</em>,
or <em>fit a sequence of logic regression models using a stepwise
greedy algorithm</em>;
</p>
<p>if <code>select = 3</code> training and test set scores as a function
of model size for the results of <em>carry out cross-validation for
model selection</em> are plotted;
</p>
<p>if <code>select = 4</code> a histogram of the permutation scores with
various important values highlighted for the results of <em>carry
out a permutation test to check for signal in the data</em> are plotted;
</p>
<p>if <code>select = 5</code> a series of histograms of the permutation
scores with various important values highlighted for the results of
<em>carry out a permutation test for model selection</em> are plotted;
</p>
<p>if  <code>select = 7</code> a histogram of the size frequency of the fitted models,
a histogram of the frequency that predictors are marginally in the model,
and (if that information was collected) an image plot
for the observed frequency that predictors were jointly in the model, and
an image plot of the observed/expected ratio of that joint frequency.
See Kooperberg and Ruczinski (2004) for the definition of the expected frequency.
</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>
<p>Kooperberg C, Ruczinki I (2005). Identifying interacting SNPs using
Monte Carlo Logic Regression, <em>Genetic Epidemiology</em>, <b>28</b>, 157-170.
</p>
<p>Selected chapters from the dissertation of Ingo Ruczinski, available from
<a href="https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf">https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>, 
<code><a href="#topic+plot.logregmodel">plot.logregmodel</a></code>, 
<code><a href="#topic+plot.logregtree">plot.logregtree</a></code>, 
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit1,logreg.savefit2,logreg.savefit3,logreg.savefit4,
     logreg.savefit5,logreg.savefit6,logreg.savefit7)
#
# fit a single model
# myanneal &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 1000)
# logreg.savefit1 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21],
#                type = 2, select = 1, ntrees = 2, anneal.control = myanneal)
# the best score should be in the 0.96-0.98 range
plot(logreg.savefit1)
#
# fit multiple models
# myanneal2 &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 0)
# logreg.savefit2 &lt;- logreg(select = 2, ntrees = c(1,2), nleaves =c(1,7), 
#                oldfit = logreg.savefit1, anneal.control = myanneal2)
plot(logreg.savefit2)
# After an initial steep decline, the scores only get slightly better
# for models with more than four leaves and two trees. 
#
# cross validation
# logreg.savefit3 &lt;- logreg(select = 3, oldfit = logreg.savefit2)
plot(logreg.savefit3)
# 4 leaves, 2 trees should give the best test set score
#
# null model test
# logreg.savefit4 &lt;- logreg(select = 4, anneal.control = myanneal2, oldfit = logreg.savefit1)
plot(logreg.savefit4)
# A histogram of the 25 scores obtained from the permutation test. Also shown
# are the scores for the best scoring model with one logic tree, and the null
# model (no tree). As the permutation scores are not even close to the score
# of the best model with one tree (fit on the original data), there is strong
# evidence against the null hypothesis that there was no signal in the data. 
#
# Permutation tests
# logreg.savefit5 &lt;- logreg(select = 5, oldfit = logreg.savefit2)
plot(logreg.savefit5)
# The permutation scores improve until we condition on a model with two
# trees and four leaves, and then do not change very much anymore. This 
# indicates that the best model has indeed four leaves.
#
# a greedy sequence
# logreg.savefit6 &lt;- logreg(select = 6, ntrees = 2, nleaves =c(1,12), oldfit = logreg.savefit1)
plot(logreg.savefit6)
# Monte Carlo Logic Regression
# logreg.savefit7 &lt;- logreg(select = 7, oldfit = logreg.savefit1, mc.control=
#                logreg.mc.control(nburn=1000, niter=100000, hyperpars=log(2)))
plot(logreg.savefit7)
</code></pre>

<hr>
<h2 id='plot.logregmodel'>Plots for Logic Regression</h2><span id='topic+plot.logregmodel'></span>

<h3>Description</h3>

<p>Makes plots for an object of class <code>logregmodel</code>
fitted by <code>logreg</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'logregmodel'
plot(x, pscript=FALSE, title=TRUE, nms, ...)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.logregmodel_+3A_x">x</code></td>
<td>
<p>object of class <code>logregmodel</code>, typically a part of an
object of class <code>logreg</code>, which is the result of the function
<code>logreg</code>. </p>
</td></tr>
<tr><td><code id="plot.logregmodel_+3A_pscript">pscript</code></td>
<td>
<p>if <code>TRUE</code> all plots will be stored in postscript
files with distinct names. </p>
</td></tr>
<tr><td><code id="plot.logregmodel_+3A_title">title</code></td>
<td>
<p>if <code>TRUE</code> this generates a title for some plots,
typically listing the number of trees and the model size. </p>
</td></tr>
<tr><td><code id="plot.logregmodel_+3A_nms">nms</code></td>
<td>
<p>names of variables. If <code>nms</code> is provided variable names will
be plotted, otherwise indices will be used. </p>
</td></tr>
<tr><td><code id="plot.logregmodel_+3A_...">...</code></td>
<td>
<p>graphical parameters can be given as arguments to plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The fitted trees are plotted. </p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>
<p>Selected chapters from the dissertation of Ingo Ruczinski, available from
<a href="https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf">https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+logregmodel">logregmodel</a></code>,
<code><a href="#topic+plot.logreg">plot.logreg</a></code>,
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit1)
# myanneal &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 1000)
# logreg.savefit1 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21],
#                type = 2, select = 1, ntrees = 2, anneal.control = myanneal)
# plot(logreg.savefit1)
plot(logreg.savefit1$model) # does the same
</code></pre>

<hr>
<h2 id='plot.logregtree'>A plot of one Logic Regression tree.</h2><span id='topic+plot.logregtree'></span>

<h3>Description</h3>

<p>Makes a plot of one Logic Regression tree, fitted by
<code>logreg</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'logregtree'
plot(x, nms, full=TRUE, and.or.cx=1.0, leaf.sz=1.0, 
                leaf.txt.cx=1.0, coef.cx=1.0, indents=rep(0,4), coef=TRUE,
                coef.rd=4, ...)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.logregtree_+3A_x">x</code></td>
<td>
<p>an object of class <code>logregtree</code>, or the <code>trees</code>
component of such an object.  Typically this object will be part of
the result of an object of class <code>logreg</code>, generated with
<code>select = 1</code> (single model fit) or <code>select = 2</code> (multiple
model fit). </p>
</td></tr>
<tr><td><code id="plot.logregtree_+3A_nms">nms</code></td>
<td>
<p>names of variables. If nms is provided variable names will
be plotted, otherwise indices will be used. </p>
</td></tr>
<tr><td><code id="plot.logregtree_+3A_full">full</code></td>
<td>
<p>if <code>TRUE</code>, the tree occupies the entire window with
margins specified by <code>indents</code>. </p>
</td></tr>
<tr><td><code id="plot.logregtree_+3A_and.or.cx">and.or.cx</code></td>
<td>
<p>character expansion (size) for the operators
and/or.</p>
</td></tr>
<tr><td><code id="plot.logregtree_+3A_leaf.sz">leaf.sz</code></td>
<td>
<p>character expansion for the size of the leaves. </p>
</td></tr>
<tr><td><code id="plot.logregtree_+3A_leaf.txt.cx">leaf.txt.cx</code></td>
<td>
<p>character expansion for the text in the leaves. </p>
</td></tr>
<tr><td><code id="plot.logregtree_+3A_coef.cx">coef.cx</code></td>
<td>
<p>character expansion for the coefficient string. </p>
</td></tr>
<tr><td><code id="plot.logregtree_+3A_indents">indents</code></td>
<td>
<p>indents for plot - bottom, left, top, right. </p>
</td></tr>
<tr><td><code id="plot.logregtree_+3A_coef">coef</code></td>
<td>
<p>if <code>TRUE</code>, the coefficient of the tree is plotted.</p>
</td></tr>
<tr><td><code id="plot.logregtree_+3A_coef.rd">coef.rd</code></td>
<td>
<p>controls how many digits of the above coefficient are 
displayed.</p>
</td></tr>
<tr><td><code id="plot.logregtree_+3A_...">...</code></td>
<td>
<p>graphical parameters can be given as arguments to plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function makes a plot of one logic tree. The character
expansion terms (<code>and.or.cx, leaf.sz, leaf.txt.cx, coef.cx</code>) defaults of
1.0 are chosen to generate a pretty plot of a single tree with up to
eight leaves (4 levels deep). To plot more than one tree, or trees of
different complexity, scale accordingly. </p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.
</p>
<p>Selected chapters from the dissertation of Ingo Ruczinski, available from
<a href="https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf">https://research.fredhutch.org/content/dam/stripe/kooperberg/ingophd-logic.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+frame.logreg">frame.logreg</a></code>,
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit2)
# 
# myanneal2 &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 0)
# logreg.savefit2 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21],
#                type = 2, select = 2, ntrees = c(1,2), nleaves =c(1,7),
#                anneal.control = myanneal2)
for(i in 1:logreg.savefit2$nmodels) for(j in 1:logreg.savefit2$alltrees[[i]]$ntrees[1]){
   plot.logregtree(logreg.savefit2$alltrees[[i]]$trees[[j]])
   title(main=paste("model",i,"tree",j))
}
</code></pre>

<hr>
<h2 id='predict.logreg'>Predicted values Logic Regression</h2><span id='topic+predict.logreg'></span>

<h3>Description</h3>

<p>Computes predicted values for one or more Logic
Regression models that were fitted by a single call to <code>logreg</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'logreg'
predict(object, msz, ntr, newbin, newsep, ...)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.logreg_+3A_object">object</code></td>
<td>
<p>Object of class <code>logreg</code>, that resulted from
applying the function <code>logreg</code> 
with
<code>select = 1</code> (single model fit),
<code>select = 2</code> (multiple model fit), or
<code>select = 6</code> (greedy stepwise fit).</p>
</td></tr>
<tr><td><code id="predict.logreg_+3A_msz">msz</code></td>
<td>
<p>if <code>predict.logreg</code> is executed on an object of class
<code>logreg</code>, that resulted from applying the function <code>logreg</code>
with <code>select = 2</code> (multiple model fit)
or <code>select = 6</code> (greedy stepwise fit) all logic trees for all
fitted models are returned. To restrict the model size and the number
of trees to some models, specify <code>msz</code> and <code>ntr</code>
(for <code>select = 2</code>) or just <code>msz</code> (for <code>select = 6</code>).</p>
</td></tr>
<tr><td><code id="predict.logreg_+3A_ntr">ntr</code></td>
<td>
<p>see <code>msz</code></p>
</td></tr>
<tr><td><code id="predict.logreg_+3A_newbin">newbin</code></td>
<td>
<p>binary predictors to evaluate the logic trees at.  If
<code>newbin</code> is omitted, the original (training) data is used. </p>
</td></tr>
<tr><td><code id="predict.logreg_+3A_newsep">newsep</code></td>
<td>
<p>separate (linear) predictors. If <code>newbin</code> is
omitted, the original (training) predictors are used, even if <code>
newsep</code> is specified. </p>
</td></tr>
<tr><td><code id="predict.logreg_+3A_...">...</code></td>
<td>
<p>other options are ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calls <code>frame.logreg</code>.</p>


<h3>Value</h3>

<p>If <code>object$select = 1</code>, a vector with fitted values,
otherwise a data frame with fitted values, where columns correspond to
models. </p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.</p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>,
<code><a href="#topic+frame.logreg">frame.logreg</a></code>,
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit1,logreg.savefit2,logreg.savefit6,logreg.testdat)
#
# myanneal &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 1000)
# logreg.savefit1 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21], type = 2,
#                select = 1, ntrees = 2, anneal.control = myanneal)
z1 &lt;- predict(logreg.savefit1)
plot(z1, logreg.testdat[,1]-z1, xlab="fitted values", ylab="residuals")
# myanneal2 &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 0)
# logreg.savefit2 &lt;- logreg(select = 2, nleaves =c(1,7), oldfit = logreg.savefit1,
#                anneal.control = myanneal2)
z2  &lt;- predict(logreg.savefit2)
# logreg.savefit6 &lt;- logreg(select = 6, ntrees = 2, nleaves =c(1,12), oldfit = logreg.savefit1)
z6 &lt;- predict(logreg.savefit6, msz = 3:5)

</code></pre>

<hr>
<h2 id='print.logreg'>Prints Logic Regression Output</h2><span id='topic+print.logreg'></span>

<h3>Description</h3>

<p>Prints formulas for objects fitted by <code>logreg</code>. </p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'logreg'
print(x, nms, notnms, pstyle, ...)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.logreg_+3A_x">x</code></td>
<td>
<p>object of class <code>logreg</code>, typically the result of the
function <code>logreg</code>. </p>
</td></tr>
<tr><td><code id="print.logreg_+3A_nms">nms</code></td>
<td>
<p>names of variables. If <code>nms</code> is provided variable names will
be printted, otherwise <code>x$binnames</code> will be used. If that does
not exist indices will be used.</p>
</td></tr>
<tr><td><code id="print.logreg_+3A_notnms">notnms</code></td>
<td>
<p>names of complements of the variables. If 
<code>notnms</code> is not provided &ldquo;not&rdquo; will be added before the
variable names.</p>
</td></tr>
<tr><td><code id="print.logreg_+3A_pstyle">pstyle</code></td>
<td>
<p>parenthesis style. If <code>pstyle = 1</code> (the default)
rules are more compact than if <code>pstyle = 2</code>.</p>
</td></tr>
<tr><td><code id="print.logreg_+3A_...">...</code></td>
<td>
<p>other options are ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>x$select</code> equals 1 or 2 the fitted logic rule(s)
are generated as a text string. Scores, and if
<code>x$select</code> equals 2 or 6 modelsizes, are also provided. 
If
<code>x$select</code> equals 4 or 5 a summary of the permutation test(s) is printed.
If
<code>x$select</code> equals 3 a summary of the cross validation is printed.
If <code>x$select</code> is equal to 7 an error message is generated.
</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.</p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>, 
<code><a href="#topic+print.logregmodel">print.logregmodel</a></code>, 
<code><a href="#topic+print.logregtree">print.logregtree</a></code>, 
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit1,logreg.savefit2,logreg.savefit3,logreg.savefit4,
     logreg.savefit5,logreg.savefit6)
#
# fit a single model
# myanneal &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 1000)
# logreg.savefit1 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21],
#                type = 2, select = 1, ntrees = 2, anneal.control = myanneal)
# the best score should be in the 0.96-0.98 range
print(logreg.savefit1)
#
# fit multiple models
# myanneal2 &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 0)
# logreg.savefit2 &lt;- logreg(select = 2, ntrees = c(1,2), nleaves =c(1,7),
#                oldfit = logreg.savefit1, anneal.control = myanneal2)
print(logreg.savefit2)
# After an initial steep decline, the scores only get slightly better
# for models with more than four leaves and two trees.
#
# cross validation
# logreg.savefit3 &lt;- logreg(select = 3, oldfit = logreg.savefit2)
print(logreg.savefit3)
# 4 leaves, 2 trees should give the best test set score
#
# null model test
# logreg.savefit4 &lt;- logreg(select = 4, anneal.control = myanneal2, oldfit = logreg.savefit1)
print(logreg.savefit4)
# A summary of the permutation test
#
# Permutation tests
# logreg.savefit5 &lt;- logreg(select = 5, oldfit = logreg.savefit2)
print(logreg.savefit5)
# A table summarizing the permutation tests
#
# a greedy sequence
# logreg.savefit6 &lt;- logreg(select = 6, ntrees = 2, nleaves =c(1,12), oldfit = logreg.savefit1)
print(logreg.savefit6)
</code></pre>

<hr>
<h2 id='print.logregmodel'>Prints Logic Regression Formula</h2><span id='topic+print.logregmodel'></span>

<h3>Description</h3>

<p>Prints formulas for objects fitted by <code>logreg</code>. </p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'logregmodel'
print(x, nms, notnms, pstyle, ...)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.logregmodel_+3A_x">x</code></td>
<td>
<p>object of class <code>logregmodel</code>, typically a part of an
object of class <code>logreg</code>, which is the result of the function
<code>logreg</code>. </p>
</td></tr>
<tr><td><code id="print.logregmodel_+3A_nms">nms</code></td>
<td>
<p>names of variables. If <code>nms</code> is provided variable names will
be printed, otherwise 
indices will be used.</p>
</td></tr>
<tr><td><code id="print.logregmodel_+3A_notnms">notnms</code></td>
<td>
<p>names of complements of the variables. If 
<code>notnms</code> is not provided &ldquo;not&rdquo; will be added before the
variable names.</p>
</td></tr>
<tr><td><code id="print.logregmodel_+3A_pstyle">pstyle</code></td>
<td>
<p>parenthesis style. If <code>pstyle = 1</code> (the default)
rules are more compact than if <code>pstyle = 2</code>.</p>
</td></tr>
<tr><td><code id="print.logregmodel_+3A_...">...</code></td>
<td>
<p>other options are ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A text representation of the model will be printed.
</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.</p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>, 
<code><a href="#topic+logregmodel">logregmodel</a></code>, 
<code><a href="#topic+print.logreg">print.logreg</a></code>, 
<code><a href="#topic+print.logregtree">print.logregtree</a></code>, 
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit1)
#
# myanneal &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 1000)
# logreg.savefit1 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21],
#                type = 2, select = 1, ntrees = 2, anneal.control = myanneal)
print(logreg.savefit1$model) 
</code></pre>

<hr>
<h2 id='print.logregtree'>Prints Logic Regression Formula</h2><span id='topic+print.logregtree'></span>

<h3>Description</h3>

<p>Prints formulas for objects fitted by <code>logreg</code>. </p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'logregtree'
print(x, nms, notnms, pstyle, ...)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.logregtree_+3A_x">x</code></td>
<td>
<p>object of class <code>logregtree</code>, typically a part of an
object of class <code>logreg</code>, which is the result of the function
<code>logreg</code>, or a matrix with five columns
(see <code>logregtree</code>). </p>
</td></tr>
<tr><td><code id="print.logregtree_+3A_nms">nms</code></td>
<td>
<p>names of variables. If <code>nms</code> is provided variable names will
be printed, otherwise 
indices will be used.</p>
</td></tr>
<tr><td><code id="print.logregtree_+3A_notnms">notnms</code></td>
<td>
<p>names of complements of the variables. If 
<code>notnms</code> is not provided &ldquo;not&rdquo; will be added before the
variable names.</p>
</td></tr>
<tr><td><code id="print.logregtree_+3A_pstyle">pstyle</code></td>
<td>
<p>parenthesis style. If <code>pstyle = 1</code> (the default)
rules are more compact than if <code>pstyle = 2</code>.</p>
</td></tr>
<tr><td><code id="print.logregtree_+3A_...">...</code></td>
<td>
<p>other options are ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A text representation of the tree will be printed.
</p>


<h3>Author(s)</h3>

<p>Ingo Ruczinski <a href="mailto:ingo@jhu.edu">ingo@jhu.edu</a> and
Charles Kooperberg <a href="mailto:clk@fredhutch.org">clk@fredhutch.org</a>.
</p>


<h3>References</h3>

<p>Ruczinski I, Kooperberg C, LeBlanc ML (2003).  Logic Regression,
<em>Journal of Computational and Graphical Statistics</em>, <b>12</b>, 475-511.
</p>
<p>Ruczinski I, Kooperberg C, LeBlanc ML (2002).  Logic Regression -
methods and software.  <em>Proceedings of the MSRI workshop on
Nonlinear Estimation and Classification</em> (Eds: D. Denison, M. Hansen,
C. Holmes, B. Mallick, B. Yu), Springer: New York, 333-344.</p>


<h3>See Also</h3>

<p><code><a href="#topic+logreg">logreg</a></code>, 
<code><a href="#topic+logregtree">logregtree</a></code>, 
<code><a href="#topic+print.logreg">print.logreg</a></code>, 
<code><a href="#topic+print.logregmodel">print.logregmodel</a></code>, 
<code><a href="#topic+logreg.testdat">logreg.testdat</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(logreg.savefit1)
#
# myanneal &lt;- logreg.anneal.control(start = -1, end = -4, iter = 25000, update = 1000)
# logreg.savefit1 &lt;- logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21],
#                type = 2, select = 1, ntrees = 2, anneal.control = myanneal)
print(logreg.savefit1$model$trees[[1]]) 
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
