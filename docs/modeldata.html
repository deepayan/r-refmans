<!DOCTYPE html><html><head><title>Help for package modeldata</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {modeldata}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ad_data'><p>Alzheimer's disease data</p></a></li>
<li><a href='#ames'><p>Ames Housing Data</p></a></li>
<li><a href='#attrition'><p>Job attrition</p></a></li>
<li><a href='#biomass'><p>Biomass data</p></a></li>
<li><a href='#bivariate'><p>Example bivariate classification data</p></a></li>
<li><a href='#car_prices'><p>Kelly Blue Book resale data for 2005 model year GM cars</p></a></li>
<li><a href='#cells'><p>Cell body segmentation</p></a></li>
<li><a href='#check_times'><p>Execution time data</p></a></li>
<li><a href='#chem_proc_yield'><p>Chemical manufacturing process data set</p></a></li>
<li><a href='#Chicago'><p>Chicago ridership data</p></a></li>
<li><a href='#concrete'><p>Compressive strength of concrete mixtures</p></a></li>
<li><a href='#covers'><p>Raw cover type data</p></a></li>
<li><a href='#credit_data'><p>Credit data</p></a></li>
<li><a href='#crickets'><p>Rates of Cricket Chirps</p></a></li>
<li><a href='#deliveries'><p>Food Delivery Time Data</p></a></li>
<li><a href='#drinks'><p>Sample time series data</p></a></li>
<li><a href='#grants'><p>Grant acceptance data</p></a></li>
<li><a href='#hepatic_injury_qsar'><p>Predicting hepatic injury from chemical information</p></a></li>
<li><a href='#hotel_rates'><p>Daily Hotel Rate Data</p></a></li>
<li><a href='#hpc_cv'><p>Class probability predictions</p></a></li>
<li><a href='#hpc_data'><p>High-performance computing system data</p></a></li>
<li><a href='#ischemic_stroke'><p>Clinical data used to predict ischemic stroke</p></a></li>
<li><a href='#leaf_id_flavia'><p>Leaf identification data (Flavia)</p></a></li>
<li><a href='#lending_club'><p>Loan data</p></a></li>
<li><a href='#meats'><p>Fat, water and protein content of meat samples</p></a></li>
<li><a href='#mlc_churn'><p>Customer churn data</p></a></li>
<li><a href='#modeldata-package'><p>modeldata: Data Sets Useful for Modeling Examples</p></a></li>
<li><a href='#oils'><p>Fatty acid composition of commercial oils</p></a></li>
<li><a href='#parabolic'><p>Parabolic class boundary data</p></a></li>
<li><a href='#pathology'><p>Liver pathology data</p></a></li>
<li><a href='#pd_speech'><p>Parkinson's disease speech classification data set</p></a></li>
<li><a href='#penguins'><p>Palmer Station penguin data</p></a></li>
<li><a href='#permeability_qsar'><p>Predicting permeability from chemical information</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#Sacramento'><p>Sacramento CA home prices</p></a></li>
<li><a href='#scat'><p>Morphometric data on scat</p></a></li>
<li><a href='#sim_classification'><p>Simulate datasets</p></a></li>
<li><a href='#small_fine_foods'><p>Fine foods example data</p></a></li>
<li><a href='#Smithsonian'><p>Smithsonian museums</p></a></li>
<li><a href='#solubility_test'><p>Solubility predictions from MARS model</p></a></li>
<li><a href='#stackoverflow'><p>Annual Stack Overflow Developer Survey Data</p></a></li>
<li><a href='#steroidogenic_toxicity'><p>Predicting steroidogenic toxicity with assay data</p></a></li>
<li><a href='#tate_text'><p>Tate Gallery modern artwork metadata</p></a></li>
<li><a href='#taxi'><p>Chicago taxi data set</p></a></li>
<li><a href='#two_class_dat'><p>Two class data</p></a></li>
<li><a href='#two_class_example'><p>Two class predictions</p></a></li>
<li><a href='#wa_churn'><p>Watson churn data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Data Sets Useful for Modeling Examples</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Data sets used for demonstrating or testing model-related
    packages are contained in this package.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://modeldata.tidymodels.org">https://modeldata.tidymodels.org</a>,
<a href="https://github.com/tidymodels/modeldata">https://github.com/tidymodels/modeldata</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tidymodels/modeldata/issues">https://github.com/tidymodels/modeldata/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, MASS, purrr, rlang, tibble</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, testthat (&ge; 3.0.0), ggplot2</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>tidyverse/tidytemplate, tidymodels/tidymodels</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LazyDataCompression:</td>
<td>xz</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3.9000</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-21 16:23:38 UTC; max</td>
</tr>
<tr>
<td>Author:</td>
<td>Max Kuhn [aut, cre],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Max Kuhn &lt;max@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-21 17:30:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='ad_data'>Alzheimer's disease data</h2><span id='topic+ad_data'></span>

<h3>Description</h3>

<p>Alzheimer's disease data
</p>


<h3>Details</h3>

<p>Craig-Schapiro et al. (2011) describe a clinical study of 333 patients,
including some with mild (but well-characterized) cognitive impairment as
well as healthy individuals. CSF samples were taken from all subjects. The
goal of the study was to determine if subjects in the early states of
impairment could be differentiated from cognitively healthy individuals.
Data collected on each subject included:
</p>

<ul>
<li><p> Demographic characteristics such as age and gender
</p>
</li>
<li><p> Apolipoprotein E genotype
</p>
</li>
<li><p> Protein measurements of Abeta, Tau, and a phosphorylated version of Tau (called pTau)
</p>
</li>
<li><p> Proteinmeasurements of 124 exploratory biomarkers, and
</p>
</li>
<li><p> Clinical dementia scores
</p>
</li></ul>

<p>For these analyses, we have converted the scores to two classes: impaired
and healthy. The goal of this analysis is to create classification models
using the demographic and assay data to predict which patients have early
stages of disease.
</p>


<h3>Value</h3>

<table>
<tr><td><code>ad_data</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn, M., Johnson, K. (2013) <em>Applied Predictive Modeling</em>, Springer.
</p>
<p>Craig-Schapiro R, Kuhn M, Xiong C, Pickering EH, Liu J, Misko TP, et al.
(2011) Multiplexed Immunoassay Panel Identifies Novel CSF Biomarkers for
Alzheimer's Disease Diagnosis and Prognosis. PLoS ONE 6(4): e18850.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ad_data)
str(ad_data)
</code></pre>

<hr>
<h2 id='ames'>Ames Housing Data</h2><span id='topic+ames'></span>

<h3>Description</h3>

<p>A data set from De Cock (2011) has 82 fields were recorded for 2,930
properties in Ames IA. This version is copies from the <code>AmesHousing</code> package
but does not include a few quality columns that appear to be outcomes
rather than predictors.
</p>


<h3>Details</h3>

<p>See this links for the sources below for more information as well as
<code>?AmesHousing::make_ames</code>.
</p>
<p>For these data, the training materials typically use:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

set.seed(4595)
data_split &lt;- initial_split(ames, strata = "Sale_Price")
ames_train &lt;- training(data_split)
ames_test  &lt;- testing(data_split)

set.seed(2453)
ames_folds&lt;- vfold_cv(ames_train)
</pre></div>


<h3>Value</h3>

<table>
<tr><td><code>ames</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>De Cock, D. (2011). &quot;Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project,&quot; <em>Journal of Statistics Education</em>,  Volume 19, Number 3.
</p>
<p><a href="http://jse.amstat.org/v19n3/decock/DataDocumentation.txt">http://jse.amstat.org/v19n3/decock/DataDocumentation.txt</a>
</p>
<p><a href="http://jse.amstat.org/v19n3/decock.pdf">http://jse.amstat.org/v19n3/decock.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ames)
str(ames)
</code></pre>

<hr>
<h2 id='attrition'>Job attrition</h2><span id='topic+attrition'></span>

<h3>Description</h3>

<p>Job attrition
</p>


<h3>Details</h3>

<p>These data are from the IBM Watson Analytics Lab.
The website describes the data with &ldquo;Uncover the
factors that lead to employee attrition and explore important
questions such as &lsquo;show me a breakdown of distance
from home by job role and attrition&rsquo; or &lsquo;compare
average monthly income by education and attrition&rsquo;. This is a
fictional data set created by IBM data scientists.&rdquo;. There
are 1470 rows.
</p>


<h3>Value</h3>

<table>
<tr><td><code>attrition</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The IBM Watson Analytics Lab website https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(attrition)
str(attrition)
</code></pre>

<hr>
<h2 id='biomass'>Biomass data</h2><span id='topic+biomass'></span>

<h3>Description</h3>

<p>Ghugare et al (2014) contains a data set where different biomass fuels are
characterized by the amount of certain molecules (carbon, hydrogen, oxygen,
nitrogen, and sulfur) and the corresponding higher heating value (HHV).
These data are from their Table S.2 of the Supplementary Materials
</p>


<h3>Value</h3>

<table>
<tr><td><code>biomass</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Ghugare, S. B., Tiwary, S., Elangovan, V., and Tambe, S. S. (2013).
Prediction of Higher Heating Value of Solid Biomass Fuels Using Artificial
Intelligence Formalisms. <em>BioEnergy Research</em>, 1-12.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(biomass)
str(biomass)
</code></pre>

<hr>
<h2 id='bivariate'>Example bivariate classification data</h2><span id='topic+bivariate'></span><span id='topic+bivariate_train'></span><span id='topic+bivariate_test'></span><span id='topic+bivariate_val'></span>

<h3>Description</h3>

<p>Example bivariate classification data
</p>


<h3>Details</h3>

<p>These data are a simplified version of the segmentation data contained
in <code>caret</code>. There are three columns: <code>A</code> and <code>B</code> are predictors and the column
<code>Class</code> is a factor with levels &quot;One&quot; and &quot;Two&quot;. There are three data sets:
one for training (n = 1009), validation (n = 300), and testing (n = 710).
</p>


<h3>Value</h3>

<table>
<tr><td><code>bivariate_train</code>, <code>bivariate_test</code>, <code>bivariate_val</code></td>
<td>
<p>tibbles</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(bivariate)
str(bivariate_train)
str(bivariate_val)
str(bivariate_test)
</code></pre>

<hr>
<h2 id='car_prices'>Kelly Blue Book resale data for 2005 model year GM cars</h2><span id='topic+car_prices'></span>

<h3>Description</h3>

<p>Kuiper (2008) collected data on Kelly Blue Book resale data for 804 GM cars (2005 model year).
</p>


<h3>Value</h3>

<table>
<tr><td><code>car_prices</code></td>
<td>
<p>data frame of the suggested retail price (column <code>Price</code>) and various
characteristics of each car (columns <code>Mileage</code>, <code>Cylinder</code>, <code>Doors</code>, <code>Cruise</code>,
<code>Sound</code>, <code>Leather</code>, <code>Buick</code>, <code>Cadillac</code>, <code>Chevy</code>, <code>Pontiac</code>, <code>Saab</code>,
<code>Saturn</code>, <code>convertible</code>, <code>coupe</code>, <code>hatchback</code>, <code>sedan</code> and <code>wagon</code>)</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuiper, S. (2008). Introduction to Multiple Regression: How Much Is Your Car Worth?,
<em>Journal of Statistics Education</em>, Vol. 16
<a href="http://jse.amstat.org/jse_archive.htm#2008">http://jse.amstat.org/jse_archive.htm#2008</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(car_prices)
str(car_prices)
</code></pre>

<hr>
<h2 id='cells'>Cell body segmentation</h2><span id='topic+cells'></span>

<h3>Description</h3>

<p>Hill, LaPan, Li and Haney (2007) develop models to predict which cells in a
high content screen were well segmented.  The data consists of 119 imaging
measurements on 2019. The original analysis used 1009 for training and 1010
as a test set (see the column called <code>case</code>).
</p>


<h3>Details</h3>

<p>The outcome class is contained in a factor variable called <code>class</code> with
levels &quot;PS&quot; for poorly segmented and &quot;WS&quot; for well segmented.
</p>
<p>The raw data used in the paper can be found at the Biomedcentral website.
The version
contained in <code>cells</code> is modified. First, several discrete
versions of some of the predictors (with the suffix &quot;Status&quot;) were removed.
Second, there are several skewed predictors with minimum values of zero
(that would benefit from some transformation, such as the log). A constant
value of 1 was added to these fields: <code>avg_inten_ch_2</code>,
<code>fiber_align_2_ch_3</code>, <code>fiber_align_2_ch_4</code>, <code>spot_fiber_count_ch_4</code> and
<code>total_inten_ch_2</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>cells</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Hill, LaPan, Li and Haney (2007). Impact of image segmentation on
high-content screening data quality for SK-BR-3 cells, <em>BMC
Bioinformatics</em>, Vol. 8, pg. 340,
<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340">https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cells)
str(cells)
</code></pre>

<hr>
<h2 id='check_times'>Execution time data</h2><span id='topic+check_times'></span>

<h3>Description</h3>

<p>These data were collected from the CRAN web page for 13,626 R
packages. The time to complete the standard package checking
routine was collected In some cases, the package checking
process is stopped due to errors and these data are treated as
censored. It is less than 1 percent.
</p>


<h3>Details</h3>

<p>As predictors, the associated package source code were
downloaded and parsed to create predictors, including
</p>

<ul>
<li> <p><code>authors</code>: The number of authors in the author field.
</p>
</li>
<li> <p><code>imports</code>: The number of imported packages.
</p>
</li>
<li> <p><code>suggests</code>: The number of packages suggested.
</p>
</li>
<li> <p><code>depends</code>: The number of hard dependencies.
</p>
</li>
<li> <p><code>Roxygen</code>: a binary indicator for whether Roxygen was used
for documentation.
</p>
</li>
<li> <p><code>gh</code>: a binary indicator for whether the URL field contained
a GitHub link.
</p>
</li>
<li> <p><code>rforge</code>: a binary indicator for whether the URL field
contained a link to R-forge.
</p>
</li>
<li> <p><code>descr</code>: The number of characters (or, in some cases, bytes)
in the description field.
</p>
</li>
<li> <p><code>r_count</code>: The number of R files in the R directory.
</p>
</li>
<li> <p><code>r_size</code>: The total disk size of the R files.
</p>
</li>
<li> <p><code>ns_import</code>: Estimated number of imported functions or methods.
</p>
</li>
<li> <p><code>ns_export</code>: Estimated number of exported functions or methods.
</p>
</li>
<li> <p><code>s3_methods</code>: Estimated number of S3 methods.
</p>
</li>
<li> <p><code>s4_methods</code>: Estimated number of S4 methods.
</p>
</li>
<li> <p><code>doc_count</code>: How many Rmd or Rnw files in the vignettes
directory.
</p>
</li>
<li> <p><code>doc_size</code>: The disk size of the Rmd or Rnw files.
</p>
</li>
<li> <p><code>src_count</code>: The number of files in the <code>src</code> directory.
</p>
</li>
<li> <p><code>src_size</code>: The size on disk of files in the <code>src</code> directory.
</p>
</li>
<li> <p><code>data_count</code>  The number of files in the <code>data</code> directory.
</p>
</li>
<li> <p><code>data_size</code>: The size on disk of files in the <code>data</code> directory.
</p>
</li>
<li> <p><code>testthat_count</code>: The number of files in the <code>testthat</code>
directory.
</p>
</li>
<li> <p><code>testthat_size</code>: The size on disk of files in the <code>testthat</code>
directory.
</p>
</li>
<li> <p><code>check_time</code>: The time (in seconds) to run <code style="white-space: pre;">&#8288;R CMD check&#8288;</code>
using the &quot;r-devel-windows-ix86+x86_64' flavor.
</p>
</li>
<li> <p><code>status</code>: An indicator for whether the tests completed.
</p>
</li></ul>

<p>Data were collected on 2019-01-20.
</p>


<h3>Value</h3>

<table>
<tr><td><code>check_times</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(check_times)
str(check_times)
</code></pre>

<hr>
<h2 id='chem_proc_yield'>Chemical manufacturing process data set</h2><span id='topic+chem_proc_yield'></span>

<h3>Description</h3>

<p>A data set that models yield as a function of biological material predictors
and chemical structure predictors.
</p>


<h3>Details</h3>

<p>This data set contains information about a chemical manufacturing
process, in which the goal is to understand the relationship between
the process and the resulting final product yield.  Raw material in
this process is put through a sequence of 27 steps to generate the
final pharmaceutical product.  The starting material is generated from
a biological unit and has a range of quality and characteristics.  The
objective in this project was to develop a model to predict percent
yield of the manufacturing process.  The data set consisted of 177
samples of biological material for which 57 characteristics were
measured.  Of the 57 characteristics, there were 12 measurements of
the biological starting material, and 45 measurements of the
manufacturing process.  The process variables included measurements
such as temperature, drying time, washing time, and concentrations of
by-products at various steps.  Some of the process measurements can
be controlled, while others are observed.  Predictors are continuous,
count, categorical; some are correlated, and some contain missing
values.  Samples are not independent because sets of samples come from
the same batch of biological starting material.
</p>
<p>Columns:
</p>

<ul>
<li> <p><code>yield</code>:  numeric
</p>
</li>
<li> <p><code>bio_material_01</code> - <code>bio_material_12</code>:  numeric
</p>
</li>
<li> <p><code>man_proc_01</code> - <code>man_proc_45</code>:  numeric
</p>
</li></ul>



<h3>Value</h3>

<table>
<tr><td><code>chem_proc_yield</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn, Max, and Kjell Johnson. <em>Applied predictive modeling</em>. New York:
Springer, 2013.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(chem_proc_yield)
str(chem_proc_yield)

</code></pre>

<hr>
<h2 id='Chicago'>Chicago ridership data</h2><span id='topic+Chicago'></span><span id='topic+stations'></span>

<h3>Description</h3>

<p>Chicago ridership data
</p>


<h3>Details</h3>

<p>These data are from Kuhn and Johnson (2020) and contain an
<em>abbreviated</em> training set for modeling the number of people (in thousands)
who enter the Clark and Lake L station.
</p>
<p>The <code>date</code> column corresponds to the current date. The columns with station
names (<code>Austin</code> through <code>California</code>) are a <em>sample</em> of the columns used in
the original analysis (for file size reasons). These are 14 day lag
variables (i.e. <code style="white-space: pre;">&#8288;date - 14 days&#8288;</code>). There are columns related to weather and
sports team schedules.
</p>
<p>The station at 35th and Archer is contained in the column <code>Archer_35th</code> to
make it a valid R column name.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Chicago</code></td>
<td>
<p>a tibble</p>
</td></tr> <tr><td><code>stations</code></td>
<td>
<p>a vector of station names</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn and Johnson (2020), <em>Feature Engineering and Selection</em>,
Chapman and Hall/CRC . <a href="https://bookdown.org/max/FES/">https://bookdown.org/max/FES/</a> and
<a href="https://github.com/topepo/FES">https://github.com/topepo/FES</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Chicago)
str(Chicago)
stations
</code></pre>

<hr>
<h2 id='concrete'>Compressive strength of concrete mixtures</h2><span id='topic+concrete'></span>

<h3>Description</h3>

<p>Yeh (2006) describes an aggregated data set for experimental designs used to
test the compressive strength of concrete mixtures. The data are used by
Kuhn and Johnson (2013).
</p>


<h3>Value</h3>

<table>
<tr><td><code>concrete</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Yeh I (2006). &quot;Analysis of Strength of Concrete Using Design of Experiments
and Neural Networks.&quot; <em>Journal of Materials in Civil Engineering</em>, 18, 597-604.
</p>
<p>Kuhn, M., Johnson, K. (2013) <em>Applied Predictive Modeling</em>, Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(concrete)
str(concrete)
</code></pre>

<hr>
<h2 id='covers'>Raw cover type data</h2><span id='topic+covers'></span>

<h3>Description</h3>

<p>These data are raw data describing different types of forest cover-types
from the UCI Machine Learning Database (see link below). There is one
column in the data that has a few difference pieces of textual
information (of variable lengths).
</p>


<h3>Value</h3>

<table>
<tr><td><code>covers</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(covers)
str(covers)
</code></pre>

<hr>
<h2 id='credit_data'>Credit data</h2><span id='topic+credit_data'></span>

<h3>Description</h3>

<p>These data are from the website of Dr. Lluís A. Belanche Muñoz by way of a
github repository of Dr. Gaston Sanchez. One data point is a missing outcome
was removed from the original data.
</p>


<h3>Value</h3>

<table>
<tr><td><code>credit_data</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>https://github.com/gastonstat/CreditScoring,
http://bit.ly/2kkBFrk
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(credit_data)
str(credit_data)
</code></pre>

<hr>
<h2 id='crickets'>Rates of Cricket Chirps</h2><span id='topic+crickets'></span>

<h3>Description</h3>

<p>These data are from from McDonald (2009), by way of Mangiafico (2015), on
the relationship between the ambient temperature and the rate of cricket
chirps per minute. Data were collected for two species: <em>O. exclamationis</em>
and <em>O. niveus</em>. The data are contained in a data frame called <code>crickets</code> with
a total of 31 data points.
</p>


<h3>Value</h3>

<table>
<tr><td><code>crickets</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Mangiafico, S. 2015. &quot;An R Companion for the Handbook of Biological
Statistics.&quot; <a href="https://rcompanion.org/handbook/">https://rcompanion.org/handbook/</a>.
</p>
<p>McDonald, J. 2009. <em>Handbook of Biological Statistics</em>. Sparky House Publishing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(crickets)
str(crickets)
</code></pre>

<hr>
<h2 id='deliveries'>Food Delivery Time Data</h2><span id='topic+deliveries'></span>

<h3>Description</h3>

<p>Food Delivery Time Data
</p>


<h3>Details</h3>

<p>These data are from a study of food delivery times (i.e., the time from the
initial order to receiving the food) for a single resturant. The data
contains 10,012 orders from a specific restaurant. The predictors include:
</p>

<ul>
<li><p> The time, in decimal hours, of the order.
</p>
</li>
<li><p> The day of the week for the order.
</p>
</li>
<li><p> The approximate distance between the restaurant and the delivery
location.
</p>
</li>
<li><p> A set of 27 predictors that count the number of distinct menu items
in the order.
</p>
</li></ul>

<p>No times are censored.
</p>


<h3>Value</h3>

<table>
<tr><td><code>deliveries</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(deliveries)
str(deliveries)
</code></pre>

<hr>
<h2 id='drinks'>Sample time series data</h2><span id='topic+drinks'></span>

<h3>Description</h3>

<p>Sample time series data
</p>


<h3>Details</h3>

<p>Drink sales. The exact name of the series from FRED is:
&quot;Merchant Wholesalers, Except Manufacturers' Sales Branches and Offices
Sales: Nondurable Goods: Beer, Wine, and Distilled Alcoholic Beverages Sales&quot;
</p>


<h3>Value</h3>

<table>
<tr><td><code>drinks</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The Federal Reserve Bank of St. Louis website https://fred.stlouisfed.org/series/S4248SM144NCEN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(drinks)
str(drinks)
</code></pre>

<hr>
<h2 id='grants'>Grant acceptance data</h2><span id='topic+grants'></span><span id='topic+grants_other'></span><span id='topic+grants_test'></span><span id='topic+grants_2008'></span>

<h3>Description</h3>

<p>A data set related to the success or failure of academic grants.
</p>


<h3>Details</h3>

<p>The data are discussed in Kuhn and Johnson (2013):
</p>
<p>&quot;These data are from a 2011 Kaggle competition sponsored by the University
of Melbourne where there was interest in predicting whether or not a grant
application would be accepted. Since public funding of grants had decreased
over time, triaging grant applications based on their likelihood of success
could be important for estimating the amount of potential funding to the
university. In addition to predicting grant success, the university sought
to understand factors that were important in predicting success.&quot;
</p>
<p>The data ranged from 2005 and 2008 and the data spending strategy was
driven by the date of the grant. Kuhn and Johnson (2013) describe:
</p>
<p>&quot;The compromise taken here is to build models on the pre-2008 data and
tune them by evaluating a random sample of 2,075 grants from 2008. Once the
optimal parameters are determined, final model is built using these
parameters and the entire training set (i.e., the data prior to 2008 and the
additional 2,075 grants). A small holdout set of 518 grants from 2008 will
be used to ensure that no gross methodology errors occur from repeatedly
evaluating the 2008 data during model tuning. In the text, this set of
samples is called the 2 0 0 8 holdout set. This small set of year 2008
grants will be referred to as the test set and will not be evaluated until
set of candidate models are identified.&quot;
</p>
<p>To emulate this, <code>grants_other</code> contains the training (pre-2008, n = 6,633)
and holdout/validation data (2008, n = 1,557). <code>grants_test</code> has 518 grant
samples from 2008. The object <code>grants_2008</code> is an integer vector that can
be used to separate the modeling with the holdout/validation sets.
</p>


<h3>Value</h3>

<table>
<tr><td><code>grants_other</code>, <code>grants_test</code>, <code>grants_2008</code></td>
<td>
<p>two tibbles and an integer
vector of data points used for training</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn and Johnson (2013). <em>Applied Predictive Modeling</em>. Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(grants)
str(grants_other)
str(grants_test)
str(grants_2008)
</code></pre>

<hr>
<h2 id='hepatic_injury_qsar'>Predicting hepatic injury from chemical information</h2><span id='topic+hepatic_injury_qsar'></span>

<h3>Description</h3>

<p>A quantitative structure-activity relationship (QSAR) data set to predict
when a molecule has risk associated with liver function.
</p>


<h3>Details</h3>

<p>This  data set was used to develop a model for predicting compounds'
probability of causing hepatic injury (i.e. liver damage). This data set
consisted of 281 unique compounds; 376 predictors were measured or computed
for each. The response was categorical (either &quot;none&quot;, &quot;mild&quot;, or &quot;severe&quot;),
and was highly unbalanced.
</p>
<p>This kind of response often occurs in pharmaceutical data because companies
steer away from creating molecules that have undesirable characteristics.
Therefore, well-behaved molecules often greatly outnumber undesirable
molecules. The predictors consisted of measurements from 184 biological
screens and 192 chemical feature predictors. The biological predictors
represent activity for each screen and take values between 0 and 10 with a
mode of 4. The chemical feature predictors represent counts of important
sub-structures as well as measures of physical properties that are thought to
be associated with hepatic injury.
</p>
<p>Columns:
</p>

<ul>
<li> <p><code>class</code>:  ordered and factor (levels: 'none', 'mild', and 'severe')
</p>
</li>
<li> <p><code>bio_assay_001</code> - <code>bio_assay_184</code>:  numeric
</p>
</li>
<li> <p><code>chem_fp_001</code> - <code>chem_fp_192</code>:  numeric
</p>
</li></ul>



<h3>Value</h3>

<table>
<tr><td><code>hepatic_injury_qsar</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn, Max, and Kjell Johnson. <em>Applied predictive modeling</em>. New York:
Springer, 2013.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hepatic_injury_qsar)
str(hepatic_injury_qsar)

</code></pre>

<hr>
<h2 id='hotel_rates'>Daily Hotel Rate Data</h2><span id='topic+hotel_rates'></span>

<h3>Description</h3>

<p>A data set to predict the average daily rate for a hotel in Lisbon Portugal.
</p>


<h3>Details</h3>

<p>Data are originally described in Antonio, de Almeida, and Nunes (2019).
This version of the data is filtered for one hotel (the &quot;Resort Hotel&quot;) and
is intended as regression data set for predicting the average daily rate for
a room. The data are post-2016; the 2016 data were used to have a predictor
for the historical daily rates. See the <code>hotel_rates.R</code> file in the
<code>data-raw</code> directory of the package to understand other filters used when
creating this version of the data.
</p>
<p>The <code>agent</code> and <code>company</code> fields were changed from random characters to use
a set of random names.
</p>
<p>The outcome column is <code>avg_price_per_room</code>.
</p>


<h4>License</h4>

<p>No license was given for the data; See the reference below for source.
</p>



<h3>Source</h3>

<p><a href="https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-02-11">https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-02-11</a>
</p>


<h3>References</h3>

<p>Antonio, N., de Almeida, A., and Nunes, L. (2019). Hotel booking demand
datasets. <em>Data in Brief</em>, 22, 41-49.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
str(hotel_rates)

## End(Not run)
</code></pre>

<hr>
<h2 id='hpc_cv'>Class probability predictions</h2><span id='topic+hpc_cv'></span>

<h3>Description</h3>

<p>Class probability predictions
</p>


<h3>Details</h3>

<p>This data frame contains the predicted classes and
class probabilities for a linear discriminant analysis model fit
to the HPC data set from Kuhn and Johnson (2013). These data are
the assessment sets from a 10-fold cross-validation scheme. The
data column columns for the true class (<code>obs</code>), the class
prediction (<code>pred</code>) and columns for each class probability
(columns <code>VF</code>, <code>F</code>, <code>M</code>, and <code>L</code>). Additionally, a column for
the resample indicator is included.
</p>


<h3>Value</h3>

<table>
<tr><td><code>hpc_cv</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn, M., Johnson, K. (2013) <em>Applied Predictive
Modeling</em>, Springer
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hpc_cv)
str(hpc_cv)
</code></pre>

<hr>
<h2 id='hpc_data'>High-performance computing system data</h2><span id='topic+hpc_data'></span>

<h3>Description</h3>

<p>Kuhn and Johnson (2013) describe a data set where characteristics of unix
jobs were used to classify there completion times as either very fast
(1 min or less, <code>VF</code>), fast (1–50 min, <code>F</code>), moderate (5–30 min, <code>M</code>), or
long (greater than 30 min, <code>L</code>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>hpc_data</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn, M., Johnson, K. (2013) <em>Applied Predictive Modeling</em>, Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(hpc_data)
str(hpc_data)
</code></pre>

<hr>
<h2 id='ischemic_stroke'>Clinical data used to predict ischemic stroke</h2><span id='topic+ischemic_stroke'></span>

<h3>Description</h3>

<p>A data set to predict a binary outcome using imaging and patient data.
</p>


<h3>Details</h3>

<p>These data were gathered to predict patient risk for ischemic stroke. A
historical set of patients with a range of carotid artery blockages were
selected. The data consisted of 126 patients, 44 of which had blockages
greater than 70%. All patients had undergone Computed Tomography Angiography
(CTA) to generate a detailed three-dimensional visualization and
characterization of the blockage. These images were then analyzed in order to
compute several features related to the disease, including: percent stenosis,
arterial wall thickness, and tissue characteristics such as lipid-rich
necrotic core and calcification.
</p>
<p>The group of patients in this study also had follow-up information on
whether or not a stroke occurred at a subsequent point in time. The data for
each patient also included commonly collected clinical characteristics for
risk of stroke such as whether or not the patient had atrial fibrillation,
coronary artery disease, and a history of smoking. Demographics of gender and
age were included as well. These readily available risk factors can be
thought of as another potentially useful predictor set that can be evaluated.
In fact, this set of predictors should be evaluated first to assess their
ability to predict stroke since these predictors are easy to collect, are
acquired at patient presentation, and do not require an expensive imaging
technique.
</p>
<p>Columns:
</p>

<ul>
<li> <p><code>stroke</code>:  factor (levels: 'yes' and 'no')
</p>
</li>
<li> <p><code>nascet_scale</code>:  numeric
</p>
</li>
<li> <p><code>calc_vol</code>:  numeric
</p>
</li>
<li> <p><code>calc_vol_prop</code>:  numeric
</p>
</li>
<li> <p><code>matx_vol</code>:  numeric
</p>
</li>
<li> <p><code>matx_vol_prop</code>:  numeric
</p>
</li>
<li> <p><code>lrnc_vol</code>:  numeric
</p>
</li>
<li> <p><code>lrnc_vol_prop</code>:  numeric
</p>
</li>
<li> <p><code>max_calc_area</code>:  numeric
</p>
</li>
<li> <p><code>max_calc_area_prop</code>:  numeric
</p>
</li>
<li> <p><code>max_dilation_by_area</code>:  numeric
</p>
</li>
<li> <p><code>max_matx_area</code>:  numeric
</p>
</li>
<li> <p><code>max_matx_area_prop</code>:  numeric
</p>
</li>
<li> <p><code>max_lrnc_area</code>:  numeric
</p>
</li>
<li> <p><code>max_lrnc_area_prop</code>:  numeric
</p>
</li>
<li> <p><code>max_max_wall_thickness</code>:  numeric
</p>
</li>
<li> <p><code>max_remodeling_ratio</code>:  numeric
</p>
</li>
<li> <p><code>max_stenosis_by_area</code>:  numeric
</p>
</li>
<li> <p><code>max_wall_area</code>:  numeric
</p>
</li>
<li> <p><code>wall_vol</code>:  numeric
</p>
</li>
<li> <p><code>max_stenosis_by_diameter</code>:  numeric
</p>
</li>
<li> <p><code>age</code>:  integer
</p>
</li>
<li> <p><code>male</code>:  integer
</p>
</li>
<li> <p><code>smoking_history</code>:  integer
</p>
</li>
<li> <p><code>atrial_fibrillation</code>:  integer
</p>
</li>
<li> <p><code>coronary_artery_disease</code>:  integer
</p>
</li>
<li> <p><code>diabetes_history</code>:  integer
</p>
</li>
<li> <p><code>hypercholesterolemia_history</code>:  integer
</p>
</li>
<li> <p><code>hypertension_history</code>:  integer
</p>
</li></ul>



<h3>Value</h3>

<table>
<tr><td><code>ischemic_stroke</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn, Max, and Kjell Johnson. <em>Feature Engineering and Selection: A Practical
Approach for Predictive Models</em>. Chapman and Hall/CRC, 2019.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ischemic_stroke)
str(ischemic_stroke)

</code></pre>

<hr>
<h2 id='leaf_id_flavia'>Leaf identification data (Flavia)</h2><span id='topic+leaf_id_flavia'></span>

<h3>Description</h3>

<p>Image analysis of leaves to predict species.
</p>


<h3>Details</h3>

<p>From the original manuscript: &quot;The Flavia dataset contains 1907 leaf images.
There are 32 different species and each has 50-77 images. Scanners and
digital cameras are used to acquire the leaf images on a plain background.
The isolated leaf images contain blades only, without a petiole. These leaf
images are collected from the most common plants in Yangtze, Delta,
China. Those leaves were sampled on the campus of the Nanjing University and
the Sun Yat-Sen arboretum, Nanking, China.&quot;
</p>
<p>The reference below has details information on the features used for
prediction.
</p>
<p>Columns:
</p>

<ul>
<li> <p><code>species</code>:  factor (32 levels)
</p>
</li>
<li> <p><code>apex</code>:  factor (9 levels)
</p>
</li>
<li> <p><code>base</code>:  factor (6 levels)
</p>
</li>
<li> <p><code>shape</code>:  factor (5 levels)
</p>
</li>
<li> <p><code>denate_edge</code>:  factor (levels: 'no' and 'yes')
</p>
</li>
<li> <p><code>lobed_edge</code>:  factor (levels: 'no' and 'yes')
</p>
</li>
<li> <p><code>smooth_edge</code>:  factor (levels: 'no' and 'yes')
</p>
</li>
<li> <p><code>toothed_edge</code>:  factor (levels: 'no' and 'yes')
</p>
</li>
<li> <p><code>undulate_edge</code>:  factor (levels: 'no' and 'yes')
</p>
</li>
<li> <p><code>outlying_polar</code>:  numeric
</p>
</li>
<li> <p><code>skewed_polar</code>:  numeric
</p>
</li>
<li> <p><code>clumpy_polar</code>:  numeric
</p>
</li>
<li> <p><code>sparse_polar</code>:  numeric
</p>
</li>
<li> <p><code>striated_polar</code>:  numeric
</p>
</li>
<li> <p><code>convex_polar</code>:  numeric
</p>
</li>
<li> <p><code>skinny_polar</code>:  numeric
</p>
</li>
<li> <p><code>stringy_polar</code>:  numeric
</p>
</li>
<li> <p><code>monotonic_polar</code>:  numeric
</p>
</li>
<li> <p><code>outlying_contour</code>:  numeric
</p>
</li>
<li> <p><code>skewed_contour</code>:  numeric
</p>
</li>
<li> <p><code>clumpy_contour</code>:  numeric
</p>
</li>
<li> <p><code>sparse_contour</code>:  numeric
</p>
</li>
<li> <p><code>striated_contour</code>:  numeric
</p>
</li>
<li> <p><code>convex_contour</code>:  numeric
</p>
</li>
<li> <p><code>skinny_contour</code>:  numeric
</p>
</li>
<li> <p><code>stringy_contour</code>:  numeric
</p>
</li>
<li> <p><code>monotonic_contour</code>:  numeric
</p>
</li>
<li> <p><code>num_max_ponits</code>:  numeric
</p>
</li>
<li> <p><code>num_min_points</code>:  numeric
</p>
</li>
<li> <p><code>diameter</code>:  numeric
</p>
</li>
<li> <p><code>area</code>:  numeric
</p>
</li>
<li> <p><code>perimeter</code>:  numeric
</p>
</li>
<li> <p><code>physiological_length</code>:  numeric
</p>
</li>
<li> <p><code>physiological_width</code>:  numeric
</p>
</li>
<li> <p><code>aspect_ratio</code>:  numeric
</p>
</li>
<li> <p><code>rectangularity</code>:  numeric
</p>
</li>
<li> <p><code>circularity</code>:  numeric
</p>
</li>
<li> <p><code>compactness</code>:  numeric
</p>
</li>
<li> <p><code>narrow_factor</code>:  numeric
</p>
</li>
<li> <p><code>perimeter_ratio_diameter</code>:  numeric
</p>
</li>
<li> <p><code>perimeter_ratio_length</code>:  numeric
</p>
</li>
<li> <p><code>perimeter_ratio_lw</code>:  numeric
</p>
</li>
<li> <p><code>num_convex_points</code>:  numeric
</p>
</li>
<li> <p><code>perimeter_convexity</code>:  numeric
</p>
</li>
<li> <p><code>area_convexity</code>:  numeric
</p>
</li>
<li> <p><code>area_ratio_convexity</code>:  numeric
</p>
</li>
<li> <p><code>equivalent_diameter</code>:  numeric
</p>
</li>
<li> <p><code>eccentriciry</code>:  numeric
</p>
</li>
<li> <p><code>contrast</code>:  numeric
</p>
</li>
<li> <p><code>correlation_texture</code>:  numeric
</p>
</li>
<li> <p><code>inverse_difference_moments</code>:  numeric
</p>
</li>
<li> <p><code>entropy</code>:  numeric
</p>
</li>
<li> <p><code>mean_red_val</code>:  numeric
</p>
</li>
<li> <p><code>mean_green_val</code>:  numeric
</p>
</li>
<li> <p><code>mean_blue_val</code>:  numeric
</p>
</li>
<li> <p><code>std_red_val</code>:  numeric
</p>
</li>
<li> <p><code>std_green_val</code>:  numeric
</p>
</li>
<li> <p><code>std_blue_val</code>:  numeric
</p>
</li>
<li> <p><code>correlation</code>:  numeric
</p>
</li></ul>



<h3>Value</h3>

<table>
<tr><td><code>leaf_id_flavia</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Lakshika, Jayani PG, and Thiyanga S. Talagala. &quot;Computer-aided interpretable
features for leaf image classification.&quot; <em>arXiv preprint</em> arXiv:2106.08077
(2021).
</p>
<p><a href="https://github.com/SMART-Research/leaffeatures_paper">https://github.com/SMART-Research/leaffeatures_paper</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(leaf_id_flavia)
str(leaf_id_flavia)

</code></pre>

<hr>
<h2 id='lending_club'>Loan data</h2><span id='topic+lending_club'></span>

<h3>Description</h3>

<p>Loan data
</p>


<h3>Details</h3>

<p>These data were downloaded from the Lending Club
access site (see below) and are from the first quarter of 2016.
A subset of the rows and variables are included here. The
outcome is in the variable <code>Class</code> and is either &quot;good&quot; (meaning
that the loan was fully paid back or currently on-time) or &quot;bad&quot;
(charged off, defaulted, of 21-120 days late). A data dictionary
can be found on the source website.
</p>


<h3>Value</h3>

<table>
<tr><td><code>lending_club</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Lending Club Statistics https://www.lendingclub.com/info/download-data.action
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lending_club)
str(lending_club)
</code></pre>

<hr>
<h2 id='meats'>Fat, water and protein content of meat samples</h2><span id='topic+meats'></span>

<h3>Description</h3>

<p>&quot;These data are recorded on a Tecator Infratec Food and Feed Analyzer
working in the wavelength range 850 - 1050 nm by the Near Infrared
Transmission (NIT) principle. Each sample contains finely chopped pure meat
with different moisture, fat and protein contents.
</p>


<h3>Details</h3>

<p>If results from these data are used in a publication we want you to mention
the instrument and company name (Tecator) in the publication.  In addition,
please send a preprint of your article to
</p>
<p>Karin Thente, Tecator AB, Box 70, S-263 21 Hoganas, Sweden
</p>
<p>The data are available in the public domain with no responsibility from the
original data source. The data can be redistributed as long as this
permission note is attached.&quot;
</p>
<p>&quot;For each meat sample the data consists of a 100 channel spectrum of
absorbances and the contents of moisture (water), fat and protein.  The
absorbance is -log10 of the transmittance measured by the spectrometer. The
three contents, measured in percent, are determined by analytic chemistry.&quot;
</p>
<p>Included here are the training, monitoring and test sets.
</p>


<h3>Value</h3>

<table>
<tr><td><code>meats</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data(meats)
str(meats)
</code></pre>

<hr>
<h2 id='mlc_churn'>Customer churn data</h2><span id='topic+mlc_churn'></span>

<h3>Description</h3>

<p>A data set from the MLC++ machine learning software for modeling customer
churn. There are 19 predictors, mostly numeric: <code>state</code> (categorical),
<code>account_length</code> <code>area_code</code> <code>international_plan</code> (yes/no),
<code>voice_mail_plan</code> (yes/no), <code>number_vmail_messages</code>
<code>total_day_minutes</code> <code>total_day_calls</code> <code>total_day_charge</code>
<code>total_eve_minutes</code> <code>total_eve_calls</code> <code>total_eve_charge</code>
<code>total_night_minutes</code> <code>total_night_calls</code>
<code>total_night_charge</code> <code>total_intl_minutes</code>
<code>total_intl_calls</code> <code>total_intl_charge</code>, and
<code>number_customer_service_calls</code>.
</p>


<h3>Details</h3>

<p>The outcome is contained in a column called <code>churn</code> (also yes/no).
A note in one of the source files states that the data are &quot;artificial based
on claims similar to real world&quot;.
</p>


<h3>Value</h3>

<table>
<tr><td><code>mlc_churn</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Originally at <code style="white-space: pre;">&#8288;http://www.sgi.com/tech/mlc/&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mlc_churn)
str(mlc_churn)
</code></pre>

<hr>
<h2 id='modeldata-package'>modeldata: Data Sets Useful for Modeling Examples</h2><span id='topic+modeldata'></span><span id='topic+modeldata-package'></span>

<h3>Description</h3>

<p>Data sets used for demonstrating or testing model-related packages are contained in this package.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Max Kuhn <a href="mailto:max@posit.co">max@posit.co</a>
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Posit Software, PBC [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://modeldata.tidymodels.org">https://modeldata.tidymodels.org</a>
</p>
</li>
<li> <p><a href="https://github.com/tidymodels/modeldata">https://github.com/tidymodels/modeldata</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tidymodels/modeldata/issues">https://github.com/tidymodels/modeldata/issues</a>
</p>
</li></ul>


<hr>
<h2 id='oils'>Fatty acid composition of commercial oils</h2><span id='topic+oils'></span>

<h3>Description</h3>

<p>Fatty acid concentrations of commercial oils were measured using gas
chromatography.  The data is used to predict the type of oil.  Note that
only the known oils are in the data set. Also, the authors state that there
are 95 samples of known oils. However, we count 96 in Table 1 (pgs.  33-35).
</p>


<h3>Value</h3>

<table>
<tr><td><code>oils</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Brodnjak-Voncina et al. (2005). Multivariate data analysis in
classification of vegetable oils characterized by the content of fatty
acids, <em>Chemometrics and Intelligent Laboratory Systems</em>, Vol.
75:31-45.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(oils)
str(oils)
</code></pre>

<hr>
<h2 id='parabolic'>Parabolic class boundary data</h2><span id='topic+parabolic'></span>

<h3>Description</h3>

<p>Parabolic class boundary data
</p>


<h3>Details</h3>

<p>These data were simulated. There are two correlated predictors and
two classes in the factor outcome.
</p>


<h3>Value</h3>

<table>
<tr><td><code>parabolic</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(parabolic)
str(parabolic)
</code></pre>

<hr>
<h2 id='pathology'>Liver pathology data</h2><span id='topic+pathology'></span>

<h3>Description</h3>

<p>Liver pathology data
</p>


<h3>Details</h3>

<p>These data have the results of a <em>x</em>-ray examination
to determine whether liver is abnormal or not (in the <code>scan</code>
column) versus the more extensive pathology results that
approximate the truth (in <code>pathology</code>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>pathology</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Altman, D.G., Bland, J.M. (1994) &ldquo;Diagnostic tests 1:
sensitivity and specificity,&rdquo; <em>British Medical Journal</em>,
vol 308, 1552.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(pathology)
str(pathology)
</code></pre>

<hr>
<h2 id='pd_speech'>Parkinson's disease speech classification data set</h2><span id='topic+pd_speech'></span>

<h3>Description</h3>

<p>Parkinson's disease speech classification data set
</p>


<h3>Details</h3>

<p>From the UCI ML archive, the description is &quot;The data used in this
study were gathered from 188 patients with PD (107 men and 81 women) with
ages ranging from 33 to 87 (65.1 p/m 10.9) at the Department of Neurology
in Cerrahpaşa Faculty of Medicine, Istanbul University. The control group
consists of 64 healthy individuals (23 men and 41 women) with ages varying
between 41 and 82 (61.1 p/m 8.9). During the data collection process,
the microphone is set to 44.1 KHz and following the physician's examination,
the sustained phonation of the vowel <code style="white-space: pre;">&#8288;/a/&#8288;</code> was collected from each subject
with three repetitions.&quot;
</p>
<p>The data here are averaged over the replicates.
</p>


<h3>Value</h3>

<table>
<tr><td><code>pd_speech</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>UCI ML repository (data) https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification#,
</p>
<p>Sakar et al (2019), &quot;A comparative analysis of speech signal processing
algorithms for Parkinson’s disease classification and the use of the tunable
Q-factor wavelet transform&quot;, <em>Applied Soft Computing</em>, V74, pg 255-263.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(pd_speech)
str(pd_speech)
</code></pre>

<hr>
<h2 id='penguins'>Palmer Station penguin data</h2><span id='topic+penguins'></span>

<h3>Description</h3>

<p>A data set from Gorman, Williams, and Fraser (2014) containing measurements
from different types of penguins. This version of the data was retrieved from
Allison Horst's <code>palmerpenguins</code> package on 2020-06-22.
</p>


<h3>Value</h3>

<table>
<tr><td><code>penguins</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism
and Environmental Variability within a Community of Antarctic Penguins
(<em>Genus Pygoscelis</em>). PLoS ONE 9(3): e90081.
<a href="https://doi.org/10.1371/journal.pone.0090081">doi:10.1371/journal.pone.0090081</a>
</p>
<p><a href="https://github.com/allisonhorst/palmerpenguins">https://github.com/allisonhorst/palmerpenguins</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(penguins)
str(penguins)
</code></pre>

<hr>
<h2 id='permeability_qsar'>Predicting permeability from chemical information</h2><span id='topic+permeability_qsar'></span>

<h3>Description</h3>

<p>A quantitative structure-activity relationship (QSAR) data set to predict
when a molecule can permeate cells.
</p>


<h3>Details</h3>

<p>This pharmaceutical data set was used to develop a model for predicting
compounds' permeability. In short, permeability is the measure of a
molecule's ability to cross a membrane. The body, for example, has notable
membranes between the body and brain, known as the blood-brain barrier, and
between the gut and body in the intestines. These membranes help the body
guard critical regions from receiving undesirable or detrimental substances.
For an orally taken drug to be effective in the brain, it first must pass
through the intestinal wall and then must pass through the blood-brain
barrier in order to be present for the desired neurological target.
Therefore, a compound's ability to permeate relevant biological membranes
is critically important to understand early in the drug discovery process.
Compounds that appear to be effective for a particular disease in research
screening experiments, but appear to be poorly permeable may need to be
altered in order improve permeability, and thus the compound's ability to
reach the desired target. Identifying permeability problems can help guide
chemists towards better molecules.
</p>
<p>Permeability assays such as PAMPA and Caco-2 have been developed to help
measure compounds' permeability (Kansy et al, 1998). These screens are
effective at quantifying a compound's permeability, but the assay is
expensive labor intensive. Given a sufficient number of compounds that have
been screened, we could develop a predictive model for permeability in an
attempt to potentially reduce the need for the assay. In this project there
were 165 unique compounds; 1107 molecular fingerprints were determined for
each. A molecular fingerprint is a binary sequence of numbers that
represents the presence or absence of a specific molecular sub-structure.
The response is highly skewed, the predictors are sparse (15.5% are present),
and many predictors are strongly associated.
</p>
<p>Columns:
</p>

<ul>
<li> <p><code>permeability</code>: numeric
</p>
</li>
<li> <p><code>chem_fp_0001</code> - <code>chem_fp_1107</code>: numeric
</p>
</li></ul>



<h3>Value</h3>

<table>
<tr><td><code>permeability_qsar</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn, Max, and Kjell Johnson. <em>Applied predictive modeling</em>. New York:
Springer, 2013.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(permeability_qsar)
str(permeability_qsar)

</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>dplyr</dt><dd><p><code><a href="dplyr.html#topic+reexports">%&gt;%</a></code></p>
</dd>
</dl>

<hr>
<h2 id='Sacramento'>Sacramento CA home prices</h2><span id='topic+Sacramento'></span>

<h3>Description</h3>

<p>This data frame contains house and sale price data for 932 homes in
Sacramento CA.  The original data were obtained from the website for the
SpatialKey software. From their website: &quot;The Sacramento real estate
transactions file is a list of 985 real estate transactions in the
Sacramento area reported over a five-day period, as reported by the
Sacramento Bee.&quot; Google was used to fill in missing/incorrect data.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Sacramento</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>SpatialKey website:
<a href="https://support.spatialkey.com/spatialkey-sample-csv-data/">https://support.spatialkey.com/spatialkey-sample-csv-data/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Sacramento)
str(Sacramento)
</code></pre>

<hr>
<h2 id='scat'>Morphometric data on scat</h2><span id='topic+scat'></span>

<h3>Description</h3>

<p>Reid (2015) collected data on animal feses in coastal California. The data
consist of DNA verified species designations as well as fields related to
the time and place of the collection and the scat itself. The data are on
the three main species.
</p>


<h3>Value</h3>

<table>
<tr><td><code>scat</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Reid, R. E. B. (2015). A morphometric modeling approach to
distinguishing among bobcat, coyote and gray fox scats. <em>Wildlife
Biology</em>, 21(5), 254-262
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(scat)
str(scat)
</code></pre>

<hr>
<h2 id='sim_classification'>Simulate datasets</h2><span id='topic+sim_classification'></span><span id='topic+sim_regression'></span><span id='topic+sim_noise'></span><span id='topic+sim_logistic'></span><span id='topic+sim_multinomial'></span>

<h3>Description</h3>

<p>These functions can be used to generate simulated data for supervised
(classification and regression) and unsupervised modeling applications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_classification(
  num_samples = 100,
  method = "caret",
  intercept = -5,
  num_linear = 10,
  keep_truth = FALSE
)

sim_regression(
  num_samples = 100,
  method = "sapp_2014_1",
  std_dev = NULL,
  factors = FALSE,
  keep_truth = FALSE
)

sim_noise(
  num_samples,
  num_vars,
  cov_type = "exchangeable",
  outcome = "none",
  num_classes = 2,
  cov_param = 0
)

sim_logistic(num_samples, eqn, correlation = 0, keep_truth = FALSE)

sim_multinomial(
  num_samples,
  eqn_1,
  eqn_2,
  eqn_3,
  correlation = 0,
  keep_truth = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_classification_+3A_num_samples">num_samples</code></td>
<td>
<p>Number of data points to simulate.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_method">method</code></td>
<td>
<p>A character string for the simulation method. For
classification, the single current option is &quot;caret&quot;. For regression,
values can be &quot;sapp_2014_1&quot;, &quot;sapp_2014_2&quot;, &quot;van_der_laan_2007_1&quot;, or
&quot;van_der_laan_2007_2&quot;. See Details below.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_intercept">intercept</code></td>
<td>
<p>The intercept for the linear predictor.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_num_linear">num_linear</code></td>
<td>
<p>Number of diminishing linear effects.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_keep_truth">keep_truth</code></td>
<td>
<p>A logical: should the true outcome value be retained for
the data? If so, the column name is <code>.truth</code>.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_std_dev">std_dev</code></td>
<td>
<p>Gaussian distribution standard deviation for residuals.
Default values are shown below in Details.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_factors">factors</code></td>
<td>
<p>A single logical for whether the binary indicators should be
encoded as factors or not.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_num_vars">num_vars</code></td>
<td>
<p>Number of noise predictors to create.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_cov_type">cov_type</code></td>
<td>
<p>The multivariate normal correlation structure of the
predictors. Possible values are &quot;exchangeable&quot; and &quot;toeplitz&quot;.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_outcome">outcome</code></td>
<td>
<p>A single character string for what type of independent outcome
should be simulated (if any). The default value of &quot;none&quot; produces no extra
columns. Using &quot;classification&quot; will generate a <code>class</code> column with
<code>num_classes</code> values, equally distributed. A value of &quot;regression&quot; results
in an <code>outcome</code> column that contains independent standard normal values.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_num_classes">num_classes</code></td>
<td>
<p>When <code>outcome = "classification"</code>, the number of classes
to simulate.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_cov_param">cov_param</code></td>
<td>
<p>A single numeric value for the exchangeable correlation
value or the base of the Toeplitz structure. See Details below.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_eqn">eqn</code>, <code id="sim_classification_+3A_eqn_1">eqn_1</code>, <code id="sim_classification_+3A_eqn_2">eqn_2</code>, <code id="sim_classification_+3A_eqn_3">eqn_3</code></td>
<td>
<p>An R expression or  (one sided) formula that
only involves variables <code>A</code> and <code>B</code> that is used to compute the linear
predictor. External objects should not be used as symbols; see the examples
below on how to use external objects in the equations.</p>
</td></tr>
<tr><td><code id="sim_classification_+3A_correlation">correlation</code></td>
<td>
<p>A single numeric value for the correlation between variables
<code>A</code> and <code>B</code>.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Specific Regression and Classification methods</h4>

<p>These functions provide several supervised simulation methods (and one
unsupervised). Learn more by <code>method</code>:
</p>


<h5><code>method = "caret"</code></h5>

<p>This is a simulated classification problem with two classes, originally
implemented in <code><a href="caret.html#topic+twoClassSim">caret::twoClassSim()</a></code> with all numeric predictors. The
predictors are simulated in different sets. First, two multivariate normal
predictors (denoted here as <code>two_factor_1</code> and <code>two_factor_2</code>) are created
with a correlation of about 0.65. They change the log-odds using main
effects and an interaction:
</p>
<pre>  intercept - 4 * two_factor_1 + 4 * two_factor_2 + 2 * two_factor_1 * two_factor_2 </pre>
<p>The intercept is a parameter for the simulation and can be used to control
the amount of class imbalance.
</p>
<p>The second set of effects are linear with coefficients that alternate signs
and have a sequence of values between 2.5 and 0.25. For example, if there
were four predictors in this set, their contribution to the log-odds would
be
</p>
<pre>  -2.5 * linear_1 + 1.75 * linear_2 -1.00 * linear_3 + 0.25 * linear_4</pre>
<p>(Note that these column names may change based on the value of <code>num_linear</code>).
</p>
<p>The third set is a nonlinear function of a single predictor ranging between
<code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> called <code>non_linear_1</code> here:
</p>
<pre>  (non_linear_1^3) + 2 * exp(-6 * (non_linear_1 - 0.3)^2) </pre>
<p>The fourth set of informative predictors are copied from one of Friedman's
systems and use two more predictors (<code>non_linear_2</code> and <code>non_linear_3</code>):
</p>
<pre>  2 * sin(non_linear_2 * non_linear_3) </pre>
<p>All of these effects are added up to model the log-odds.
</p>



<h5><code>method = "sapp_2014_1"</code></h5>

<p>This regression simulation is from Sapp et al. (2014). There are 20
independent Gaussian random predictors with mean zero and a variance of 9.
The prediction equation is:
</p>
<pre>
  predictor_01 + sin(predictor_02) + log(abs(predictor_03)) +
   predictor_04^2 + predictor_05 * predictor_06 +
   ifelse(predictor_07 * predictor_08 * predictor_09 &lt; 0, 1, 0) +
   ifelse(predictor_10 &gt; 0, 1, 0) + predictor_11 * ifelse(predictor_11 &gt; 0, 1, 0) +
   sqrt(abs(predictor_12)) + cos(predictor_13) + 2 * predictor_14 + abs(predictor_15) +
   ifelse(predictor_16 &lt; -1, 1, 0) + predictor_17 * ifelse(predictor_17 &lt; -1, 1, 0) -
   2 * predictor_18 - predictor_19 * predictor_20
</pre>
<p>The error is Gaussian with mean zero and variance 9.
</p>



<h5><code>method = "sapp_2014_2"</code></h5>

<p>This regression simulation is also from Sapp et al. (2014). There are 200
independent Gaussian predictors with mean zero and variance 16. The
prediction equation has an intercept of one and identical linear effects of
<code>log(abs(predictor))</code>.
</p>
<p>The error is Gaussian with mean zero and variance 25.
</p>



<h5><code>method = "van_der_laan_2007_1"</code></h5>

<p>This is a regression simulation from van der Laan et al. (2007) with ten
random Bernoulli variables that have a 40% probability of being a value of
one. The true regression equation is:
</p>
<pre>
  2 * predictor_01 * predictor_10 + 4 * predictor_02 * predictor_07 +
    3 * predictor_04 * predictor_05 - 5 * predictor_06 * predictor_10 +
    3 * predictor_08 * predictor_09 + predictor_01 * predictor_02 * predictor_04 -
    2 * predictor_07 * (1 - predictor_06) * predictor_02 * predictor_09 -
    4 * (1 - predictor_10) * predictor_01 * (1 - predictor_04)
</pre>
<p>The error term is standard normal.
</p>



<h5><code>method = "van_der_laan_2007_2"</code></h5>

<p>This is another regression simulation from van der Laan et al. (2007)  with
twenty Gaussians with mean zero and variance 16. The prediction equation is:
</p>
<pre>
  predictor_01 * predictor_02 + predictor_10^2 - predictor_03 * predictor_17 -
    predictor_15 * predictor_04 + predictor_09 * predictor_05 + predictor_19 -
    predictor_20^2 + predictor_09 * predictor_08
</pre>
<p>The error term is also Gaussian with mean zero and variance 16.
</p>



<h5><code>method = "hooker_2004"</code></h5>

<p>Hooker (2004) and Sorokina <em>at al</em> (2008) used the following:
</p>
<pre>
    pi ^ (predictor_01 * predictor_02) * sqrt( 2 * predictor_03 ) -
    asin(predictor_04) + log(predictor_03  + predictor_05) -
   (predictor_09 / predictor_10) * sqrt (predictor_07 / predictor_08) -
    predictor_02 * predictor_07
</pre>
<p>Predictors 1, 2, 3, 6, 7, and 9 are standard uniform while the others are
uniform on <code style="white-space: pre;">&#8288;[0.6, 1.0]&#8288;</code>. The errors are normal with mean zero and default
standard deviation of 0.25.
</p>




<h4><code>sim_noise()</code></h4>

<p>This function simulates a number of random normal variables with mean zero.
The values can be independent if <code>cov_param = 0</code>. Otherwise the values are
multivariate normal with non-diagonal covariance matrices. For
<code>cov_type = "exchangeable"</code>, the structure has unit variances and covariances
of <code>cov_param</code>. With <code>cov_type = "toeplitz"</code>, the covariances have an
exponential pattern (see example below).
</p>



<h4>Logistic simulation</h4>

<p><code>sim_logistic()</code> provides a flexible interface to simulating a logistic
regression model with two multivariate normal variables <code>A</code> and <code>B</code> (with
zero mean, unit variances and correlation determined by the <code>correlation</code>
argument).
</p>
<p>For example, using <code>eqn = A + B</code> would specify that the true probability of
the event was
</p>
<pre>
   prob = 1 / (1 + exp(A + B))
</pre>
<p>The class levels for the outcome column are <code>"one"</code> and <code>"two"</code>.
</p>



<h4>Multinomial simulation</h4>

<p><code>sim_multinomial()</code> can generate data with classes <code>"one"</code>, <code>"two"</code>, and
<code>"three"</code> based on the values in arguments <code>eqn_1</code>, <code>eqn_2</code>, and <code>eqn_3</code>,
respectfully. Like <code><a href="#topic+sim_logistic">sim_logistic()</a></code> these equations use predictors <code>A</code> and
<code>B</code>.
</p>
<p>The individual equations are evaluated and exponentiated. After this, their
values are, for each row of data, normalized to add up to one. These
probabilities are them passed to <code><a href="stats.html#topic+Multinom">stats::rmultinom()</a></code> to generate the outcome
values.
</p>



<h3>References</h3>

<p>Van der Laan, M. J., Polley, E. C., &amp; Hubbard, A. E. (2007). Super learner.
<em>Statistical applications in genetics and molecular biology</em>, 6(1).
DOI: 10.2202/1544-6115.1309.
</p>
<p>Sapp, S., van der Laan, M. J., &amp; Canny, J. (2014). Subsemble: an ensemble
method for combining subset-specific algorithm fits. <em>Journal of applied
statistics</em>, 41(6), 1247-1259. DOI: 10.1080/02664763.2013.864263
</p>
<p>Hooker, G. (2004, August). Discovering additive structure in black box
functions. In <em>Proceedings of the tenth ACM SIGKDD international conference
on Knowledge discovery and data mining</em> (pp. 575-580).
DOI: 10.1145/1014052.1014122
</p>
<p>Sorokina, D., Caruana, R., Riedewald, M., &amp; Fink, D. (2008, July). Detecting
statistical interactions with additive groves of trees. In <em>Proceedings of
the 25th international conference on Machine learning</em> (pp. 1000-1007).
DOI: 10.1145/1390156.1390282
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
sim_regression(100)
sim_classification(100)

# Flexible logistic regression simulation
if (rlang::is_installed("ggplot2")) {
  library(dplyr)
  library(ggplot2)

  sim_logistic(1000, ~ .1 + 2 * A - 3 * B + 1 * A *B, corr = .7) %&gt;%
    ggplot(aes(A, B, col = class)) +
    geom_point(alpha = 1/2) +
    coord_equal()

  f_xor &lt;- ~ 10 * xor(A &gt; 0, B &lt; 0)
  # or
  f_xor &lt;- rlang::expr(10 * xor(A &gt; 0, B &lt; 0))

  sim_logistic(1000, f_xor, keep_truth = TRUE) %&gt;%
    ggplot(aes(A, B, col = class)) +
    geom_point(alpha = 1/2) +
    coord_equal() +
    theme_bw()
}

## How to use external symbols:

a_coef &lt;- 2
# splice the value in using rlang's !! operator
lp_eqn &lt;- rlang::expr(!!a_coef * A+B)
lp_eqn
sim_logistic(5, lp_eqn)

# Flexible multinomial regression simulation
if (rlang::is_installed("ggplot2")) {

}
</code></pre>

<hr>
<h2 id='small_fine_foods'>Fine foods example data</h2><span id='topic+small_fine_foods'></span><span id='topic+training_data'></span><span id='topic+testing_data'></span>

<h3>Description</h3>

<p>Fine foods example data
</p>


<h3>Details</h3>

<p>These data are from Amazon, who describe it as &quot;This dataset consists of
reviews of fine foods from amazon. The data span a period of more than 10
years, including all ~500,000 reviews up to October 2012. Reviews include
product and user information, ratings, and a plaintext review.&quot;
</p>
<p>A subset of the data are contained here and are split into a training and
test set. The training set sampled 10 products and retained all of their
individual reviews. Since the reviews within these products are correlated,
we recommend resampling the data using a leave-one-product-out approach. The
test set sampled 500 products that were not included in the training set
and selected a single review at random for each.
</p>
<p>There is a column for the product, a column for the text of the review, and
a factor column for a class variable. The outcome is whether the reviewer
gave the product a 5-star rating or not.
</p>


<h3>Value</h3>

<table>
<tr><td><code>training_data</code>, <code>testing_data</code></td>
<td>
<p>tibbles</p>
</td></tr>
</table>


<h3>Source</h3>

<p>https://snap.stanford.edu/data/web-FineFoods.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(small_fine_foods)
str(training_data)
str(testing_data)
</code></pre>

<hr>
<h2 id='Smithsonian'>Smithsonian museums</h2><span id='topic+Smithsonian'></span>

<h3>Description</h3>

<p>Geocodes for the Smithsonian museums (circa 2018).
</p>


<h3>Value</h3>

<table>
<tr><td><code>Smithsonian</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>https://en.wikipedia.org/wiki/List_of_Smithsonian_museums
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Smithsonian)
str(Smithsonian)
</code></pre>

<hr>
<h2 id='solubility_test'>Solubility predictions from MARS model</h2><span id='topic+solubility_test'></span>

<h3>Description</h3>

<p>Solubility predictions from MARS model
</p>


<h3>Details</h3>

<p>For the solubility data in Kuhn and Johnson (2013),
these data are the test set results for the MARS model. The
observed solubility (in column <code>solubility</code>) and the model
results (<code>prediction</code>) are contained in the data.
</p>


<h3>Value</h3>

<table>
<tr><td><code>solubility_test</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn, M., Johnson, K. (2013) <em>Applied Predictive
Modeling</em>, Springer
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(solubility_test)
str(solubility_test)
</code></pre>

<hr>
<h2 id='stackoverflow'>Annual Stack Overflow Developer Survey Data</h2><span id='topic+stackoverflow'></span>

<h3>Description</h3>

<p>Annual Stack Overflow Developer Survey Data
</p>


<h3>Details</h3>

<p>These data are a collection of 5,594 data points collected on
developers. These data could be used to try to predict who works remotely
(as used in the source listed below).
</p>


<h3>Value</h3>

<table>
<tr><td><code>stackoverflow</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Julia Silge, <em>Supervised Machine Learning Case Studies in R</em>
</p>
<p><a href="https://supervised-ml-course.netlify.com/chapter2">https://supervised-ml-course.netlify.com/chapter2</a>
</p>
<p>Raw data: <a href="https://insights.stackoverflow.com/survey/">https://insights.stackoverflow.com/survey/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(stackoverflow)
str(stackoverflow)
</code></pre>

<hr>
<h2 id='steroidogenic_toxicity'>Predicting steroidogenic toxicity with assay data</h2><span id='topic+steroidogenic_toxicity'></span>

<h3>Description</h3>

<p>A set of <em>in vitro</em> assays are used to quantify the risk of reproductive
toxicity via the disruption of steroidogenic pathways.
</p>


<h3>Details</h3>

<p>H295R cells were used to measure the effect with two sets of assay results.
The first includes a set of protein measurements on: cytochrome P450 enzymes
(&quot;cyp&quot;s), STAR, and 3BHSD2. The second include hormone measurements for
DHEA, progesterone, testosterone, and cortisol.
</p>
<p>Columns:
</p>

<ul>
<li> <p><code>class</code>: factor (levels: 'toxic' and 'nontoxic')
</p>
</li>
<li> <p><code>cyp_11a1</code>: numeric
</p>
</li>
<li> <p><code>cyp_11b1</code>: numeric
</p>
</li>
<li> <p><code>cyp_11b2</code>: numeric
</p>
</li>
<li> <p><code>cyp_17a1</code>: numeric
</p>
</li>
<li> <p><code>cyp_19a1</code>: numeric
</p>
</li>
<li> <p><code>cyp_21a1</code>: numeric
</p>
</li>
<li> <p><code>hsd3b2</code>: numeric
</p>
</li>
<li> <p><code>star</code>: numeric
</p>
</li>
<li> <p><code>progesterone</code>: numeric
</p>
</li>
<li> <p><code>testosterone</code>: numeric
</p>
</li>
<li> <p><code>dhea</code>: numeric
</p>
</li>
<li> <p><code>cortisol</code>: numeric
</p>
</li></ul>



<h3>Value</h3>

<p>A tibble with columns
</p>

<ul>
<li> <p><code>class</code>: factor(levels: toxic and nontoxic)
</p>
</li>
<li> <p><code>cyp_11a1</code>: numeric
</p>
</li>
<li> <p><code>cyp_11b1</code>: numeric
</p>
</li>
<li> <p><code>cyp_11b2</code>: numeric
</p>
</li>
<li> <p><code>cyp_17a1</code>: numeric
</p>
</li>
<li> <p><code>cyp_19a1</code>: numeric
</p>
</li>
<li> <p><code>cyp_21a1</code>: numeric
</p>
</li>
<li> <p><code>hsd3b2</code>: numeric
</p>
</li>
<li> <p><code>star</code>: numeric
</p>
</li>
<li> <p><code>progesterone</code>: numeric
</p>
</li>
<li> <p><code>testosterone</code>: numeric
</p>
</li>
<li> <p><code>dhea</code>: numeric
</p>
</li>
<li> <p><code>cortisol</code>: numeric
</p>
</li></ul>



<h3>Source</h3>

<p>Maglich, J. M., Kuhn, M., Chapin, R. E., &amp; Pletcher, M. T. (2014). More than
just hormones: H295R cells as predictors of reproductive toxicity.
<em>Reproductive Toxicology</em>, 45, 77-86.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(steroidogenic_toxicity)
str(steroidogenic_toxicity)

</code></pre>

<hr>
<h2 id='tate_text'>Tate Gallery modern artwork metadata</h2><span id='topic+tate_text'></span>

<h3>Description</h3>

<p>Metadata such as artist, title, and year created for recent artworks owned
by the Tate Gallery. Only artworks created during or after 1990 are
included, and the metadata source was last updated in 2014. The Tate Gallery
provides these data but requests users to be respectful of their
<a href="https://github.com/tategallery/collection#usage-guidelines-for-open-data">guidelines for use</a>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>tate_text</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>


<ul>
<li> <p><a href="https://github.com/tategallery/collection">https://github.com/tategallery/collection</a>
</p>
</li>
<li> <p><a href="https://www.tate.org.uk/">https://www.tate.org.uk/</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(tate_text)
str(tate_text)
</code></pre>

<hr>
<h2 id='taxi'>Chicago taxi data set</h2><span id='topic+taxi'></span>

<h3>Description</h3>

<p>A data set containing information on a subset of taxi trips in the city
of Chicago in 2022.
</p>


<h3>Details</h3>

<p>The source data are originally described on the linked City of Chicago
data portal. The data exported here are a pre-processed subset motivated by
the modeling problem of predicting whether a rider will tip or not.
</p>

<dl>
<dt>tip</dt><dd><p>Whether the rider left a tip. A factor with levels
&quot;yes&quot; and &quot;no&quot;.</p>
</dd>
<dt>distance</dt><dd><p>The trip distance, in odometer miles.</p>
</dd>
<dt>company</dt><dd><p>The taxi company, as a factor. Companies that occurred
few times were binned as &quot;other&quot;.</p>
</dd>
<dt>local</dt><dd><p>Whether the trip's starting and ending locations are in the
same community. See the source data for community area values.</p>
</dd>
<dt>dow</dt><dd><p>The day of the week in which the trip began, as a
factor.</p>
</dd>
<dt>month</dt><dd><p>The month in which the trip began, as a factor.</p>
</dd>
<dt>hour</dt><dd><p>The hour of the day in which the trip began, as a
numeric.</p>
</dd>
</dl>



<h3>Value</h3>

<p>tibble
</p>


<h3>Source</h3>

<p><a href="https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew">https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
taxi

</code></pre>

<hr>
<h2 id='two_class_dat'>Two class data</h2><span id='topic+two_class_dat'></span>

<h3>Description</h3>

<p>Two class data
</p>


<h3>Details</h3>

<p>There are artifical data with two predictors (<code>A</code> and <code>B</code>) and
a factor outcome variable (<code>Class</code>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>two_class_dat</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(two_class_dat)
str(two_class_dat)
</code></pre>

<hr>
<h2 id='two_class_example'>Two class predictions</h2><span id='topic+two_class_example'></span>

<h3>Description</h3>

<p>Two class predictions
</p>


<h3>Details</h3>

<p>These data are a test set form a model built for two
classes (&quot;Class1&quot; and &quot;Class2&quot;). There are columns for the true
and predicted classes and column for the probabilities for each
class.
</p>


<h3>Value</h3>

<table>
<tr><td><code>two_class_example</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(two_class_example)
str(two_class_example)
</code></pre>

<hr>
<h2 id='wa_churn'>Watson churn data</h2><span id='topic+wa_churn'></span>

<h3>Description</h3>

<p>Watson churn data
</p>


<h3>Details</h3>

<p>These data were downloaded from the IBM Watson site
(see below) in September 2018. The data contain a factor for
whether a customer churned or not. Alternatively, the <code>tenure</code>
column presumably contains information on how long the customer
has had an account. A survival analysis can be done on this
column using the <code>churn</code> outcome as the censoring information. A
data dictionary can be found on the source website.
</p>


<h3>Value</h3>

<table>
<tr><td><code>wa_churn</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>IBM Watson Analytics https://ibm.co/2sOvyvy
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wa_churn)
str(wa_churn)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
