<!DOCTYPE html><html><head><title>Help for package literanger</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {literanger}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#literanger-package'><p><span class="pkg">literanger</span>: Random Forests for Multiple Imputation based on 'ranger'.</p></a></li>
<li><a href='#predict.literanger'><p>Literanger prediction</p></a></li>
<li><a href='#train'><p>Train forest using ranger for multiple imputation algorithms.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Random Forests for Multiple Imputation Based on 'ranger'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.2</td>
</tr>
<tr>
<td>Description:</td>
<td>An updated implementation of R package 'ranger' by Wright et al,
    (2017) &lt;<a href="https://doi.org/10.18637%2Fjss.v077.i01">doi:10.18637/jss.v077.i01</a>&gt; for training and predicting from random
    forests, particularly suited to high-dimensional data, and for embedding in
    'Multiple Imputation by Chained Equations' (MICE) by van Buuren (2007)
    &lt;<a href="https://doi.org/10.1177%2F0962280206074463">doi:10.1177/0962280206074463</a>&gt;. Ensembles of classification and regression
    trees are currently supported. Sparse data of class 'dgCMatrix' (R package
    'Matrix') can be directly analyzed. Conventional bagged predictions are
    available alongside an efficient prediction for MICE via the algorithm
    proposed by Doove et al (2014) &lt;<a href="https://doi.org/10.1016%2Fj.csda.2013.10.025">doi:10.1016/j.csda.2013.10.025</a>&gt;. Survival
    and probability forests are not supported in the update, nor is data of
    class 'gwaa.data' (R package 'GenABEL'); use the original 'ranger' package
    for these analyses.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/stephematician/literanger/issues">https://github.com/stephematician/literanger/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/stephematician/literanger">https://github.com/stephematician/literanger</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>Matrix (&ge; 1.5.3), testthat (&ge; 3.0.0), tibble (&ge; 3.2.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>cpp11 (&ge; 0.4.3)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-07-11 13:15:08 UTC; stephematician</td>
</tr>
<tr>
<td>Author:</td>
<td>Stephen Wade <a href="https://orcid.org/0000-0002-2573-9683"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Marvin N Wright [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Stephen Wade &lt;stephematician@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-07-13 21:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='literanger-package'><span class="pkg">literanger</span>: Random Forests for Multiple Imputation based on 'ranger'.</h2><span id='topic+literanger'></span><span id='topic+literanger-package'></span>

<h3>Description</h3>

<p>'literanger' is an adaption of the 'ranger' R package for training and
predicting from random forest models within multiple imputation algorithms.
ranger is a fast implementation of random forests (Breiman, 2001) or
recursive partitioning, particularly suited for high dimensional data
(Wright et al, 2017a). literanger enables random forests to be embedded in
the fully conditional specification framework for multiple imputation known
as &quot;Multiple Imputation via Chained Equations&quot; (Van Buuren 2007).
</p>


<h3>Details</h3>

<p>literanger trains classification and regression forests. The trained forest
retains information about the in-bag responses in each terminal node, thus
facilitating computationally efficient prediction within multiple imputation
with random forests proposed by Doove et al (2014). This multiple imputation
algorithm has better predictive distribution properties than competing
approaches which use predictive mean matching. Alternatively, the usual
bagged prediction may be used as in the imputation algorithm called
'missForest' (Stekhoven et al, 2014).
</p>
<p>Classification and regression forests are implemented as in the original
Random Forest (Breiman, 2001) or using extremely randomized trees (Geurts et
al, 2006). 'data.frame', 'matrix', and sparse matrices ('dgCmatrix') are
supported.
</p>
<p>Split selection may be based on improvement in metrics such as the variance,
Gini impurity, beta log-likelihood (Weinhold et al, 2019), Hellinger distance
(Cieslak et al, 2012) or maximally selected rank statistics (Wright et
al, 2017b).
</p>
<p>See <a href="https://github.com/imbs-hl/ranger">https://github.com/imbs-hl/ranger</a> for the development version of ranger
or <a href="https://github.com/stephematician/literanger">https://github.com/stephematician/literanger</a> for development version of
this package.
</p>
<p>For alternative approaches to multiple imputation that employ random forests,
see 'missRanger' <a href="https://cran.r-project.org/package=missRanger">https://cran.r-project.org/package=missRanger</a> and
'miceRanger' <a href="https://cran.r-project.org/package=miceRanger">https://cran.r-project.org/package=miceRanger</a>, which use
predictive mean matching combined with the original 'ranger' algorithm.
</p>
<p>This package was adapted from the 'ranger' package for R Statistical
Software. The C++ core is provided under the same license terms as the
original C++ core in the 'ranger' package, namely the MIT license
<a href="https://www.r-project.org/Licenses/MIT">https://www.r-project.org/Licenses/MIT</a>. The wrappers in this package around
the core are licensed under the same terms of the corresponding components in
the 'ranger' R package, namely the GPL3 license
<a href="https://www.r-project.org/Licenses/GPL-3">https://www.r-project.org/Licenses/GPL-3</a>, <a href="http://www.gnu.org/licenses/">http://www.gnu.org/licenses/</a>.
</p>


<h3>License</h3>

<p>'literanger' was adapted from the 'ranger' package for R statistical
software. ranger was authored by Marvin N. Wright with the GNU General Public
License version 3 for the R package (interface), while the C++ core of ranger
has the MIT license. The adaptation was performed by Stephen Wade in 2023.
literanger carries the same license, terms, and permissions as ranger,
including the GNU General Public License 3 for the R package interface, and
the MIT license for the C++ core.
</p>
<p>License statement for C++ core of ranger:
</p>
<div class="sourceCode"><pre>MIT License

Copyright (c) [2014-2018] [Marvin N. Wright]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the “Software”), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</pre></div>
<p>License statement for the ranger R package interface:
</p>
<div class="sourceCode"><pre>Ranger is free software: you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation, either version 3 of the License, or (at your option)
any later version.

Ranger is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with Ranger. If not, see &lt;http://www.gnu.org/licenses/&gt;.

Written by:
Marvin N. Wright
Institut für Medizinische Biometrie und Statistik
Universität zu Lübeck
Ratzeburger Allee 160
23562 Lübeck
http://www.imbs-luebeck.d
</pre></div>


<h3>Author(s)</h3>

<p>Stephen Wade <a href="mailto:stephematician@gmail.com">stephematician@gmail.com</a>, Marvin N Wright (original
'ranger' package)
</p>


<h3>References</h3>


<ul>
<li><p> Breiman, L. (2001). Random forests. <em>Machine Learning</em>, 45, 5-32.
<a href="https://doi.org/10.1023/A%3A1010933404324">doi:10.1023/A:1010933404324</a>.
</p>
</li>
<li><p> Cieslak, D. A., Hoens, T. R., Chawla, N. V., &amp; Kegelmeyer, W. P. (2012).
Hellinger distance decision trees are robust and skew-insensitive. <em>Data
Mining and Knowledge Discovery</em>, 24, 136-158.
<a href="https://doi.org/10.1007/s10618-011-0222-1">doi:10.1007/s10618-011-0222-1</a>.
</p>
</li>
<li><p> Doove, L. L., Van Buuren, S., &amp; Dusseldorp, E. (2014). Recursive
partitioning for missing data imputation in the presence of interaction
effects. <em>Computational Statistics &amp; Data Analysis</em>, 72, 92-104.
<a href="https://doi.org/10.1016/j.csda.2013.10.025">doi:10.1016/j.csda.2013.10.025</a>.
</p>
</li>
<li><p> Geurts, P., Ernst, D., &amp; Wehenkel, L. (2006). Extremely randomized trees.
<em>Machine Learning</em>, 63, 3-42. <a href="https://doi.org/10.1007/s10994-006-6226-1">doi:10.1007/s10994-006-6226-1</a>.
</p>
</li>
<li><p> Stekhoven, D.J. and Buehlmann, P. (2012). MissForest&ndash;non-parametric
missing value imputation for mixed-type data. <em>Bioinformatics</em>, 28(1),
112-118. <a href="https://doi.org/10.1093/bioinformatics/btr597">doi:10.1093/bioinformatics/btr597</a>.
</p>
</li>
<li><p> Van Buuren, S. (2007). Multiple imputation of discrete and continuous
data by fully conditional specification. <em>Statistical Methods in Medical
Research</em>, 16(3), 219-242. <a href="https://doi.org/10.1177/0962280206074463">doi:10.1177/0962280206074463</a>.
</p>
</li>
<li><p> Weinhold, L., Schmid, M., Wright, M. N., &amp; Berger, M. (2019). A random
forest approach for modeling bounded outcomes. <em>arXiv preprint</em>,
arXiv:1901.06211. <a href="https://doi.org/10.48550/arXiv.1901.06211">doi:10.48550/arXiv.1901.06211</a>.
</p>
</li>
<li><p> Wright, M. N., &amp; Ziegler, A. (2017a). ranger: A Fast Implementation of
Random Forests for High Dimensional Data in C++ and R. <em>Journal of
Statistical Software</em>, 77, 1-17. <a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>.
</p>
</li>
<li><p> Wright, M. N., Dankowski, T., &amp; Ziegler, A. (2017b). Unbiased split
variable selection for random survival forests using maximally selected
rank statistics. <em>Statistics in medicine</em>, 36(8), 1272-1284.
<a href="https://doi.org/10.1002/sim.7212">doi:10.1002/sim.7212</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/stephematician/literanger">https://github.com/stephematician/literanger</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/stephematician/literanger/issues">https://github.com/stephematician/literanger/issues</a>
</p>
</li></ul>


<hr>
<h2 id='predict.literanger'>Literanger prediction</h2><span id='topic+predict.literanger'></span>

<h3>Description</h3>

<p>'literanger' provides different types of prediction that may be used in
multiple imputation algorithms with random forests. The usual prediction is
the 'bagged' prediction, the most frequent value (or the mean) of the in-bag
samples in a terminal node. Doove et al (2014) propose a prediction that
better matches the predictive distribution as needed for multiple imputation;
take a random draw from the observations in the terminal node from a randomly
drawn tree in the forest for each predicted value needed. Alternatively, the
usual most-frequent-value or mean of the in-bag responses can be used as in
missForest (Stekhoven et al, 2014) or miceRanger
<a href="https://cran.r-project.org/package=miceRanger">https://cran.r-project.org/package=miceRanger</a> and missRanger
<a href="https://cran.r-project.org/package=missRanger">https://cran.r-project.org/package=missRanger</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'literanger'
predict(
  object,
  newdata = NULL,
  prediction_type = c("bagged", "inbag", "nodes"),
  seed = 1L + sample.int(n = .Machine$integer.max - 1L, size = 1),
  n_thread = 0,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.literanger_+3A_object">object</code></td>
<td>
<p>A trained random forest <code>literanger</code> object.</p>
</td></tr>
<tr><td><code id="predict.literanger_+3A_newdata">newdata</code></td>
<td>
<p>Data of class <code>data.frame</code>, <code>matrix</code>, or <code>dgCMatrix</code>
(Matrix), for the latter two; must have column names; all predictors named in
<code>object$predictor_names</code> must be present.</p>
</td></tr>
<tr><td><code id="predict.literanger_+3A_prediction_type">prediction_type</code></td>
<td>
<p>Name of the prediction algorithm; &quot;bagged&quot; is the
most-frequent value among in-bag samples for classification, or the mean of
in-bag responses for regression; &quot;inbag&quot; predicts by drawing one in-bag
response from a random tree for each row; &quot;nodes&quot; (currently unsupported)
returns the node keys (ids) of the terminal node from every tree for each
row.</p>
</td></tr>
<tr><td><code id="predict.literanger_+3A_seed">seed</code></td>
<td>
<p>Random seed, an integer between 1 and <code>.Machine$integer.max</code>.
Default generates the seed from <code>R</code>, set to <code>0</code> to ignore the <code>R</code> seed and
use a C++ <code>std::random_device</code>.</p>
</td></tr>
<tr><td><code id="predict.literanger_+3A_n_thread">n_thread</code></td>
<td>
<p>Number of threads. Default is determined by system, typically
the number of cores.</p>
</td></tr>
<tr><td><code id="predict.literanger_+3A_verbose">verbose</code></td>
<td>
<p>Show computation status and estimated runtime.</p>
</td></tr>
<tr><td><code id="predict.literanger_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Forests trained by literanger retain information about the in-bag responses
in each terminal node, thus facilitating efficient predictions within a
variation on multiple imputation proposed by Doove et al (2014). This type of
prediction can be selected by setting <code>prediction_type="inbag"</code>, or the usual
prediction for classification and regression forests, the most-frequent-value
and mean of in bag samples respectively, is given by setting
<code>prediction_type="bagged"</code>.
</p>
<p>A list is returned. The <code>values</code> item contains the predicted classes or
values (classification and regression forests, respectively). Factor levels
are returned as factors with the levels as per the original training data.
</p>
<p>Compared to the original package ranger, literanger excludes certain
features:
</p>

<ul>
<li><p> Probability, survival, and quantile regression forests.
</p>
</li>
<li><p> Support for class gwaa.data.
</p>
</li>
<li><p> Standard error estimation.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class <code>literanger_prediction</code> with elements:
</p>

<dl>
<dt><code>values</code></dt><dd><p>Predicted (drawn) classes/value for classification and
regression.</p>
</dd>
<dt><code>tree_type</code></dt><dd><p>Number of trees.</p>
</dd>
<dt><code>seed</code></dt><dd><p>The seed supplied to the C++ library.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Stephen Wade <a href="mailto:stephematician@gmail.com">stephematician@gmail.com</a>, Marvin N Wright (original
ranger package)
</p>


<h3>References</h3>


<ul>
<li><p> Doove, L. L., Van Buuren, S., &amp; Dusseldorp, E. (2014). Recursive
partitioning for missing data imputation in the presence of interaction
effects. <em>Computational Statistics &amp; Data Analysis</em>, 72, 92-104.
<a href="https://doi.org/10.1016/j.csda.2013.10.025">doi:10.1016/j.csda.2013.10.025</a>.
</p>
</li>
<li><p> Stekhoven, D.J. and Buehlmann, P. (2012). MissForest&ndash;non-parametric
missing value imputation for mixed-type data. <em>Bioinformatics</em>, 28(1),
112-118. <a href="https://doi.org/10.1093/bioinformatics/btr597">doi:10.1093/bioinformatics/btr597</a>.
</p>
</li>
<li><p> Wright, M. N., &amp; Ziegler, A. (2017a). ranger: A Fast Implementation of
Random Forests for High Dimensional Data in C++ and R. <em>Journal of
Statistical Software</em>, 77, 1-17. <a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+train">train</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Classification forest
train_idx &lt;- sample(nrow(iris), 2/3 * nrow(iris))
iris_train &lt;- iris[ train_idx, ]
iris_test  &lt;- iris[-train_idx, ]
rf_iris &lt;- train(data=iris_train, response_name="Species")
pred_iris_bagged &lt;- predict(rf_iris, newdata=iris_test,
                            prediction_type="bagged")
pred_iris_inbag  &lt;- predict(rf_iris, newdata=iris_test,
                            prediction_type="inbag")
# compare bagged vs actual test values
table(iris_test$Species, pred_iris_bagged$values)
# compare bagged prediction vs in-bag draw
table(pred_iris_bagged$values, pred_iris_inbag$values)

</code></pre>

<hr>
<h2 id='train'>Train forest using ranger for multiple imputation algorithms.</h2><span id='topic+train'></span>

<h3>Description</h3>

<p>'literanger' trains random forests for use in multiple imputation problems
via an adaptation of the 'ranger' R package. ranger is a fast
implementation of random forests (Breiman, 2001) or recursive partitioning,
particularly suited for high dimensional data (Wright et al, 2017a).
literanger supports prediction used in algorithms such as &quot;Multiple
Imputation via Chained Equations&quot; (Van Buuren, 2007).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train(
  data = NULL,
  response_name = character(),
  predictor_names = character(),
  x = NULL,
  y = NULL,
  case_weights = numeric(),
  classification = NULL,
  n_tree = 10,
  replace = TRUE,
  sample_fraction = ifelse(replace, 1, 0.632),
  n_try = NULL,
  draw_predictor_weights = numeric(),
  names_of_always_draw = character(),
  split_rule = NULL,
  max_depth = 0,
  min_split_n_sample = 0,
  min_leaf_n_sample = 0,
  unordered_predictors = NULL,
  response_weights = numeric(),
  n_random_split = 1,
  alpha = 0.5,
  min_prop = 0.1,
  seed = 1L + sample.int(n = .Machine$integer.max - 1L, size = 1),
  save_memory = FALSE,
  n_thread = 0,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code>, <code>matrix</code>, or <code>dgCMatrix</code>
(Matrix), for the latter two; must have column names.</p>
</td></tr>
<tr><td><code id="train_+3A_response_name">response_name</code></td>
<td>
<p>Name of response (dependent) variable if <code>data</code> was
provided.</p>
</td></tr>
<tr><td><code id="train_+3A_predictor_names">predictor_names</code></td>
<td>
<p>Names of predictor (independent) variables if <code>data</code>
was provided; default is all variables that are not the response.</p>
</td></tr>
<tr><td><code id="train_+3A_x">x</code></td>
<td>
<p>Predictor data (independent variables), alternative interface to
<code>data</code> and <code>response_name</code>.</p>
</td></tr>
<tr><td><code id="train_+3A_y">y</code></td>
<td>
<p>Response vector (dependent variable), alternative interface to
<code>data</code> and <code>response_name</code>.</p>
</td></tr>
<tr><td><code id="train_+3A_case_weights">case_weights</code></td>
<td>
<p>Weights for sampling of training observations.
Observations with larger weights will be selected with higher probability
in the bootstrap (or sub-sampled) samples for each tree.</p>
</td></tr>
<tr><td><code id="train_+3A_classification">classification</code></td>
<td>
<p>Set to <code>TRUE</code> to grow a classification forest if the
response is numeric (including if data is a matrix), else, a regression
forest is grown.</p>
</td></tr>
<tr><td><code id="train_+3A_n_tree">n_tree</code></td>
<td>
<p>Number of trees (default 10).</p>
</td></tr>
<tr><td><code id="train_+3A_replace">replace</code></td>
<td>
<p>Sample with replacement to train each tree.</p>
</td></tr>
<tr><td><code id="train_+3A_sample_fraction">sample_fraction</code></td>
<td>
<p>Fraction of observations to sample to train each tree.
Default is 1 for sampling with replacement and 0.632 for sampling without
replacement. For classification, this can be a vector of class-specific
values.</p>
</td></tr>
<tr><td><code id="train_+3A_n_try">n_try</code></td>
<td>
<p>Number of variables (predictors) to draw that are candidates for
splitting each node by. Default is the (rounded down) square root of the
number of predictors. Alternatively, a single argument function returning an
integer, given the number of predictors.</p>
</td></tr>
<tr><td><code id="train_+3A_draw_predictor_weights">draw_predictor_weights</code></td>
<td>
<p>For predictor-drawing weights shared by all
trees; a numeric vector of <em>non-negative</em> weights for each predictor. For
tree-specific predictor-drawing weights; a list of size <code>n_tree</code> containing
(non-negative) vectors with length equal to the number of predictors.</p>
</td></tr>
<tr><td><code id="train_+3A_names_of_always_draw">names_of_always_draw</code></td>
<td>
<p>Character vector with predictor (variable) names
to be selected <em>in addition</em> to the <code>n_try</code> predictors drawn as candidates to
split by.</p>
</td></tr>
<tr><td><code id="train_+3A_split_rule">split_rule</code></td>
<td>
<p>Splitting rule. For classification estimation &quot;gini&quot;,
&quot;extratrees&quot; or &quot;hellinger&quot; with default &quot;gini&quot;. For regression &quot;variance&quot;,
&quot;extratrees&quot;, &quot;maxstat&quot; or &quot;beta&quot; with default &quot;variance&quot;.</p>
</td></tr>
<tr><td><code id="train_+3A_max_depth">max_depth</code></td>
<td>
<p>Maximal tree depth. A value of NULL or 0 (the default)
corresponds to unlimited depth, 1 to tree stumps (1 split per tree).</p>
</td></tr>
<tr><td><code id="train_+3A_min_split_n_sample">min_split_n_sample</code></td>
<td>
<p>Minimal number of in-bag samples a node must have
in order to be split. Default 1 for classification and 5 for regression.</p>
</td></tr>
<tr><td><code id="train_+3A_min_leaf_n_sample">min_leaf_n_sample</code></td>
<td>
<p>Minimum number of in-bag samples in a leaf node.</p>
</td></tr>
<tr><td><code id="train_+3A_unordered_predictors">unordered_predictors</code></td>
<td>
<p>Handling of unordered factor predictors. One of
&quot;ignore&quot;, &quot;order&quot; and &quot;partition&quot;. For the &quot;extratrees&quot; splitting rule the
default is &quot;partition&quot; for all other splitting rules &quot;ignore&quot;.</p>
</td></tr>
<tr><td><code id="train_+3A_response_weights">response_weights</code></td>
<td>
<p>Classification only: Weights for the response classes
(in order of the factor levels) in the splitting rule e.g. cost-sensitive
learning. Weights are also used by each tree to determine majority vote.</p>
</td></tr>
<tr><td><code id="train_+3A_n_random_split">n_random_split</code></td>
<td>
<p>&quot;extratrees&quot; split metric only: Number of random splits
to consider for each candidate splitting variable, default is 1.</p>
</td></tr>
<tr><td><code id="train_+3A_alpha">alpha</code></td>
<td>
<p>&quot;maxstat&quot; splitting rule only: Significance threshold to allow
splitting, default is 0.5, must be in the interval <code style="white-space: pre;">&#8288;(0,1]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="train_+3A_min_prop">min_prop</code></td>
<td>
<p>&quot;maxstat&quot; splitting rule only: Lower quantile of covariate
distribution to be considered for splitting, default is 0.1, must be in the
interval <code style="white-space: pre;">&#8288;[0,0.5]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="train_+3A_seed">seed</code></td>
<td>
<p>Random seed, an integer between 1 and <code>.Machine$integer.max</code>.
Default generates the seed from <code>R</code>, set to <code>0</code> to ignore the <code>R</code> seed and
use a C++ <code>std::random_device</code>.</p>
</td></tr>
<tr><td><code id="train_+3A_save_memory">save_memory</code></td>
<td>
<p>Use memory saving (but slower) splitting mode. Warning:
This option slows down the tree growing, use only if you encounter memory
problems.</p>
</td></tr>
<tr><td><code id="train_+3A_n_thread">n_thread</code></td>
<td>
<p>Number of threads. Default is determined by system, typically
the number of cores.</p>
</td></tr>
<tr><td><code id="train_+3A_verbose">verbose</code></td>
<td>
<p>Show computation status and estimated runtime.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>literanger trains classification and regression forests using the original
Random Forest (Breiman, 2001) or extremely randomized trees (Geurts et
al, 2006) algorithms. The trained forest retains information about the in-bag
responses in each terminal node, thus facilitating a variation on the
algorithm for multiple imputation with random forests proposed by Doove et
al (2014). This algorithm should match the predictive distribution more
closely than using predictive mean matching.
</p>
<p>The default split metric for classification trees is the Gini impurity, which
can be extended to use the extra-randomized trees rule (Geurts et al, 2006).
For binary responses, the Hellinger distance metric may be used instead
(Cieslak et al, 2012).
</p>
<p>The default split metric for regression trees is the estimated variance,
which can be extended to include the extra-randomized trees rule, too.
Alternatively, the beta log-likelihood (Wright et al, 2017b) or maximally
selected rank statistics (Wright et al, 2019) are available.
</p>
<p>When the <code>data</code> and <code>response_name</code> arguments are supplied the response
variable is identified by its corresponding column name. The type of response
may be used to determine the type of tree. If the response is a factor then
classification trees are used. If the response is numeric then regression
trees are used. The <code>classification</code> argument can be used to override the
default tree type when the response is numeric. Alternatively, use <code>x</code> and
<code>y</code> arguments to specify response and predictor; this can avoid conversions
and save memory. If memory usage issues persist, consider setting
<code>save_memory=TRUE</code> but be aware that this option slows down the tree growing.
</p>
<p>The <code>min_split_n_sample</code> rule can be used to control the minimum number of
in-bag samples required to split a node; thus, as in the original algorithm,
nodes with fewer samples than <code>min_split_n_sample</code> are possible. To put a
floor under the number of samples per node, the <code>min_leaf_n_sample</code>
argument is used.
</p>
<p>When drawing candidate predictors for splitting a node on, the predictors
identified by <code>names_of_always_draw</code> are included <em>in addition</em> to the
<code>n_try</code> predictors that are randomly drawn. Another way to modify the way
predictors are selected is via the <code>draw_predictor_weights</code> argument, which
are normalised and interpreted as probabilities when drawing candidates. The
weights are assigned <em>in the order they appear in the data</em>. Weights assigned
by <code>draw_predictor_weights</code> to variables in <code>names_of_always_draw</code> are
ignored. The usage of <code>draw_predictor_weights</code> can increase the computation
times for large forests.
</p>
<p>Unordered-factor predictors can be handled in 3 different ways by using
<code>unordered_predictors</code>:
</p>

<ul>
<li><p> For &quot;ignore&quot; all factors are regarded ordered;
</p>
</li>
<li><p> For &quot;partition&quot; all possible 2-partitions are considered for splitting.
</p>
</li>
<li><p> For &quot;order&quot; and 2-class classification the factor levels are ordered by
their proportion falling in the second class, for regression by their
mean response, as described in Hastie et al. (2009), chapter 9.2.4. For
multi-class classification the factor levels are ordered by the first
principal component of the weighted covariance matrix of the contingency
table (Coppersmith et al, 1999).
</p>
</li></ul>

<p>The use of &quot;order&quot; is recommended, as it computationally fast and
can handle an unlimited number of factor levels. Note that the factors are
only reordered once and not again in each split.
</p>
<p>Compared to the original package ranger, literanger excludes certain
features:
</p>

<ul>
<li><p> Formula interface.
</p>
</li>
<li><p> Probability, survival, and quantile regression forests.
</p>
</li>
<li><p> Support for class gwaa.data.
</p>
</li>
<li><p> Measures of variable importance.
</p>
</li>
<li><p> Regularisation of importance.
</p>
</li>
<li><p> Access to in-bag data via R.
</p>
</li>
<li><p> Support for user-specified hold-out data.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class <code>literanger</code> with elements:
</p>

<dl>
<dt><code>predictor_names</code></dt><dd><p>The names of the predictor variables in the
model.</p>
</dd>
<dt><code>names_of_unordered</code></dt><dd><p>The names of predictors that are unordered.</p>
</dd>
<dt><code>tree_type</code></dt><dd><p>The type of tree in the forest.</p>
</dd>
<dt><code>n_tree</code></dt><dd><p>The number of trees that were trained.</p>
</dd>
<dt><code>n_try</code></dt><dd><p>The number of predictors drawn as candidates for each
split.</p>
</dd>
<dt><code>split_rule</code></dt><dd><p>The name of the split metric used.</p>
</dd>
<dt><code>max_depth</code></dt><dd><p>The maximum allowed depth of a tree in the forest.</p>
</dd>
<dt><code>min_metric_decrease</code></dt><dd><p>The minimum decrease in the metric for an
acceptable split (equal to negative @p alpha for maximally selected
rank statistics, else zero).</p>
</dd>
<dt><code>min_split_n_sample</code></dt><dd><p>The minimum number of in-bag samples in a node
prior to splitting.</p>
</dd>
<dt><code>min_leaf_n_sample</code></dt><dd><p>The minimum number of in-bag samples in a leaf
node.</p>
</dd>
<dt><code>seed</code></dt><dd><p>The seed supplied to the C++ library.</p>
</dd>
<dt><code>oob_error</code></dt><dd><p>The misclassification rate or the mean square error
using out-of-bag samples.</p>
</dd>
<dt><code>cpp11_ptr</code></dt><dd><p>An external pointer to the trained forest. DO NOT
MODIFY.</p>
</dd>
<dt><code>response_values</code></dt><dd><p>Classification only: the values of the response in
the order they appear in the data.</p>
</dd>
<dt><code>response_levels</code></dt><dd><p>Classification only: the labels for the response
in the order they appear in the data.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Stephen Wade <a href="mailto:stephematician@gmail.com">stephematician@gmail.com</a>, Marvin N Wright (original
ranger package)
</p>


<h3>References</h3>


<ul>
<li><p> Breiman, L. (2001). Random forests. <em>Machine Learning</em>, 45, 5-32.
<a href="https://doi.org/10.1023/A%3A1010933404324">doi:10.1023/A:1010933404324</a>.
</p>
</li>
<li><p> Cieslak, D. A., Hoens, T. R., Chawla, N. V., &amp; Kegelmeyer, W. P. (2012).
Hellinger distance decision trees are robust and skew-insensitive. <em>Data
Mining and Knowledge Discovery</em>, 24, 136-158.
<a href="https://doi.org/10.1007/s10618-011-0222-1">doi:10.1007/s10618-011-0222-1</a>.
</p>
</li>
<li><p> Coppersmith, D., Hong, S. J., &amp; Hosking, J. R. (1999). Partitioning
nominal attributes in decision trees. <em>Data Mining and Knowledge
Discovery</em>, 3, 197-217. <a href="https://doi.org/10.1023/A%3A1009869804967">doi:10.1023/A:1009869804967</a>.
</p>
</li>
<li><p> Doove, L. L., Van Buuren, S., &amp; Dusseldorp, E. (2014). Recursive
partitioning for missing data imputation in the presence of interaction
effects. <em>Computational Statistics &amp; Data Analysis</em>, 72, 92-104.
<a href="https://doi.org/10.1016/j.csda.2013.10.025">doi:10.1016/j.csda.2013.10.025</a>.
</p>
</li>
<li><p> Geurts, P., Ernst, D., &amp; Wehenkel, L. (2006). Extremely randomized trees.
<em>Machine Learning</em>, 63, 3-42. <a href="https://doi.org/10.1007/s10994-006-6226-1">doi:10.1007/s10994-006-6226-1</a>.
</p>
</li>
<li><p> Hastie, T., Tibshirani, R., Friedman, J. H., &amp; Friedman, J. H. (2009).
The elements of statistical learning: data mining, inference, and
prediction (Vol. 2). New York: Springer.
<a href="https://doi.org/10.1007/978-0-387-21606-5">doi:10.1007/978-0-387-21606-5</a>.
</p>
</li>
<li><p> Van Buuren, S. (2007). Multiple imputation of discrete and continuous
data by fully conditional specification. <em>Statistical Methods in Medical
Research</em>, 16(3), 219-242. <a href="https://doi.org/10.1177/0962280206074463">doi:10.1177/0962280206074463</a>.
</p>
</li>
<li><p> Weinhold, L., Schmid, M., Wright, M. N., &amp; Berger, M. (2019). A random
forest approach for modeling bounded outcomes. <em>arXiv preprint</em>,
arXiv:1901.06211. <a href="https://doi.org/10.48550/arXiv.1901.06211">doi:10.48550/arXiv.1901.06211</a>.
</p>
</li>
<li><p> Wright, M. N., &amp; Ziegler, A. (2017a). ranger: A Fast Implementation of
Random Forests for High Dimensional Data in C++ and R. <em>Journal of
Statistical Software</em>, 77, 1-17. <a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>.
</p>
</li>
<li><p> Wright, M. N., Dankowski, T., &amp; Ziegler, A. (2017b). Unbiased split
variable selection for random survival forests using maximally selected
rank statistics. <em>Statistics in medicine</em>, 36(8), 1272-1284.
<a href="https://doi.org/10.1002/sim.7212">doi:10.1002/sim.7212</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.literanger">predict.literanger</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Classification forest with default settings
train(data=iris, response_name="Species")

## Prediction
train_idx &lt;- sample(nrow(iris), 2/3 * nrow(iris))
iris_train &lt;- iris[train_idx, ]
iris_test &lt;- iris[-train_idx, ]
rg_iris &lt;- train(data=iris_train, response_name="Species")
pred_iris &lt;- predict(rg_iris, newdata=iris_test)
table(iris_test$Species, pred_iris$values)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
