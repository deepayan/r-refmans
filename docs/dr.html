<!DOCTYPE html><html lang="en"><head><title>Help for package dr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ais'><p>Australian institute of sport data</p></a></li>
<li><a href='#banknote'><p>Swiss banknote data</p></a></li>
<li><a href='#coord.hyp.basis'><p> Internal function to find the basis of a subspace</p></a></li>
<li><a href='#dr'><p>Main function for dimension reduction regression</p></a></li>
<li><a href='#dr.coordinate.test'><p>Dimension reduction tests</p></a></li>
<li><a href='#dr.directions'><p> Directions selected by dimension reduction regressiosn</p></a></li>
<li><a href='#dr.permutation.test'><p>Permutation tests of dimension for dr</p></a></li>
<li><a href='#dr.pvalue'><p> Compute the Chi-square approximations to a weighted</p>
sum of Chi-square(1) random variables.</a></li>
<li><a href='#dr.slices'><p> Divide a vector into slices of approximately equal size</p></a></li>
<li><a href='#dr.weights'><p> Estimate weights for elliptical symmetry</p></a></li>
<li><a href='#dr.x'><p> Accessor functions for data in dr objects</p></a></li>
<li><a href='#drop1.dr'><p> Sequential fitting of coordinate tests using a dr object</p></a></li>
<li><a href='#mussels'><p>Mussels' muscles data</p></a></li>
<li><a href='#plot.dr'><p> Basic plot of a dr object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>3.0.10</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-07-31</td>
</tr>
<tr>
<td>Title:</td>
<td>Methods for Dimension Reduction for Regression</td>
</tr>
<tr>
<td>Author:</td>
<td>Sanford Weisberg &lt;sandy@umn.edu&gt;,</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Sanford Weisberg &lt;sandy@umn.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>MASS</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats,graphics</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions, methods, and datasets for fitting dimension
 reduction regression, using slicing (methods SAVE and SIR), Principal
 Hessian Directions (phd, using residuals and the response), and an
 iterative IRE.  Partial methods, that condition on categorical
 predictors are also available.  A variety of tests, and stepwise
 deletion of predictors, is also included.  Also included is
 code for computing permutation tests of dimension.  Adding additional
 methods of estimating dimension is straightforward.
 For documentation, see the vignette in the package.   With version 3.0.4,
 the arguments for dr.step have been modified.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://cran.r-project.org/package=dr">https://cran.r-project.org/package=dr</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2015-08-03 18:59:02 UTC; sandy</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2015-08-04 00:25:42</td>
</tr>
</table>
<hr>
<h2 id='ais'>Australian institute of sport data</h2><span id='topic+ais'></span>

<h3>Description</h3>

<p>Data on 102 male and 100 female athletes collected at the Australian
Institute of Sport.
</p>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>Sex</dt><dd>
<p>(0 = male or 1 = female)
</p>
</dd>
<dt>Ht</dt><dd>
<p>height (cm)
</p>
</dd>
<dt>Wt</dt><dd>
<p>weight (kg)
</p>
</dd>
<dt>LBM</dt><dd>
<p>lean body mass
</p>
</dd>
<dt>RCC</dt><dd>
<p>red cell count
</p>
</dd>
<dt>WCC</dt><dd>
<p>white cell count
</p>
</dd>
<dt>Hc</dt><dd>
<p>Hematocrit
</p>
</dd>
<dt>Hg</dt><dd>
<p>Hemoglobin
</p>
</dd>
<dt>Ferr</dt><dd>
<p>plasma ferritin concentration
</p>
</dd>
<dt>BMI</dt><dd>
<p>body mass index, weight/(height)**2
</p>
</dd>
<dt>SSF</dt><dd>
<p>sum of skin folds
</p>
</dd>
<dt>Bfat</dt><dd>
<p>Percent body fat
</p>
</dd>
<dt>Label</dt><dd>
<p>Case Labels
</p>
</dd>
<dt>Sport</dt><dd>
<p>Sport
</p>
</dd>
</dl>



<h3>Source</h3>

<p>Ross Cunningham and Richard Telford</p>


<h3>References</h3>

<p>S. Weisberg (2005).  <em>Applied Linear Regression</em>, 3rd edition.  New York:
Wiley, Section 6.4</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ais)
</code></pre>

<hr>
<h2 id='banknote'>Swiss banknote data</h2><span id='topic+banknote'></span>

<h3>Description</h3>

<p>Six measurements made on 100 genuine
Swiss banknotes and 100 counterfeit ones.
</p>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>Length</dt><dd>
<p>Length of bill, mm
</p>
</dd>
<dt>Left</dt><dd>
<p>Width of left edge, mm
</p>
</dd>
<dt>Right</dt><dd>
<p>Width of right edge, mm
</p>
</dd>
<dt>Bottom</dt><dd>
<p>Bottom margin width, mm
</p>
</dd>
<dt>Top</dt><dd>
<p>Top margin width, mm
</p>
</dd>
<dt>Diagonal</dt><dd>
<p>Length of image diagonal, mm
</p>
</dd>
<dt>Y</dt><dd>
<p>0 = genuine, 1 = counterfeit
</p>
</dd>
</dl>



<h3>Source</h3>

<p>Flury, B. and Riedwyl, H. (1988). <em>Multivariate Statistics: A practical approach.</em> London: Chapman &amp; Hall.</p>


<h3>References</h3>

<p>Weisberg, S. (2005).  <em>Applied Linear Regression</em>, 3rd edition. New York: Wiley, Problem 12.5.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(banknote)
</code></pre>

<hr>
<h2 id='coord.hyp.basis'> Internal function to find the basis of a subspace </h2><span id='topic+coord.hyp.basis'></span>

<h3>Description</h3>

<p>If a dimension reduction object has <code class="reqn">p</code> dimension, returns a <code class="reqn">p \times r</code>
matrix whose columns space a subspace specified by a formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coord.hyp.basis(object, spec, which = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="coord.hyp.basis_+3A_object">object</code></td>
<td>
<p> A <code>dr</code> object </p>
</td></tr>
<tr><td><code id="coord.hyp.basis_+3A_spec">spec</code></td>
<td>
<p> A one-sided formula, see below </p>
</td></tr>
<tr><td><code id="coord.hyp.basis_+3A_which">which</code></td>
<td>
<p> either +1 or <code class="reqn">-1</code>; see below </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The workings of this function is best explained by an example.  Suppose
the <code>dr</code> object was created with the formula <code>y~x1+x2+x3+x4</code>, so
we have <code class="reqn">p=4</code> predictors.  A matrix that spans the subspace of 
<code class="reqn">R^4</code> specified
by Span(x1,x2,x3,x4) is simply the identity matrix of order 4.
</p>
<p>This function will return a subset of the columns of this identity matrix,
as determined by <code>spec</code>.  For example, if <code>spec = ~.-(x3+x4)</code>, the 
function returns the columns corresponding to x1 and x2 if <code>which=+1</code> or the
columns corresponding to x3 and x4 if which=<code class="reqn">-1</code>.  Similarly, if 
<code>spec=~x1+x2</code>, the same matrices will be returned.
</p>


<h3>Value</h3>

<p>A matrix corresponding to the value of <code>spec</code> and <code>which</code> given.
</p>


<h3>Author(s)</h3>

<p>Sanford Weisberg, sandy@stat.umn.edu</p>


<h3>See Also</h3>

 <p><code><a href="#topic+dr.coordinate.test">dr.coordinate.test</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ais)
s1 &lt;- dr(LBM~log(Ht)+log(Wt)+log(RCC)+log(WCC)+log(Hc)+log(Hg),
         data=ais,method="sir")
coord.hyp.basis(s1,~.-log(Wt)-log(Hg))
 </code></pre>

<hr>
<h2 id='dr'>Main function for dimension reduction regression</h2><span id='topic+dr'></span><span id='topic+dr.compute'></span>

<h3>Description</h3>

<p>This is the main function in the dr package.  It creates objects of class
dr to estimate the central (mean) subspace and perform tests concerning
its dimension.  Several helper functions that require a dr object can then
be applied to the output from this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dr (formula, data, subset, group=NULL, na.action = na.fail, weights, ...)
    
dr.compute (x, y, weights, group=NULL, method = "sir", chi2approx="bx",...)
 </code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dr_+3A_formula">formula</code></td>
<td>
<p>a two-sided formula like <code>y~x1+x2+x3</code>, where the left-side
variable is a vector or a matrix of the response variable(s), and the right-hand side
variables represent the predictors.  While any legal formula in the Rogers-Wilkinson
notation can appear, dimension reduction methods generally expect the predictors to be
numeric, not factors, with no nesting.   Full rank models are
recommended, although rank deficient models are permitted.
</p>
<p>The left-hand side of the formula will generally be a single vector, but it
can also be a matrix, such as <code>cbind(y1+y2)~x1+x2+x3</code> if the <code>method</code>
is <code>"save"</code> or <code>"sir"</code>.  Both of these methods are based on slicing,
and for the multivariate case slices are determined by slicing on all the
columns of the left-hand side variables.  
</p>
</td></tr>
<tr><td><code id="dr_+3A_data">data</code></td>
<td>
<p> an optional data frame containing the variables in the model.
By default the variables are taken from the environment from
which &lsquo;dr&rsquo; is called.</p>
</td></tr>
<tr><td><code id="dr_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td></tr>
<tr><td><code id="dr_+3A_group">group</code></td>
<td>
<p>If used, this argument specifies a grouping variable so that 
dimension reduction is done separately for each distinct level.  This is
implemented only when <code>method</code> is one of <code>"sir"</code>, 
<code>"save"</code>, or <code>"ire"</code>.  This argument must be a one-sided formula.
For example, <code>~Location</code> would fit separately for each level of the variable
<code>Location</code>.  The formula <code>~A:B</code> would fit separately for each combination of
<code>A</code> and <code>B</code>, provided that both have been declared factors.</p>
</td></tr>
<tr><td><code id="dr_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used where appropriate.  In the
context of dimension reduction methods, weights are used to obtain
elliptical symmetry, not constant variance. 

</p>
</td></tr>
<tr><td><code id="dr_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain &lsquo;NA&rsquo;s.  The default is &lsquo;na.fail,&rsquo; which will stop calculations.
The option 'na.omit' is also permitted, but it may not work correctly when
weights are used.</p>
</td></tr>
<tr><td><code id="dr_+3A_x">x</code></td>
<td>
<p>The design matrix.  This will be computed from the formula by <code>dr</code> and then
passed to <code>dr.compute</code>, or you can create it yourself.</p>
</td></tr>
<tr><td><code id="dr_+3A_y">y</code></td>
<td>
<p>The response vector or matrix</p>
</td></tr>
<tr><td><code id="dr_+3A_method">method</code></td>
<td>
<p>This character string specifies the method of fitting.  The options
include <code>"sir"</code>, <code>"save"</code>, <code>"phdy"</code>, <code>"phdres"</code> and 
<code>"ire"</code>.  Each method may have its own additional arguments, or its
own defaults; see the details below for more information.</p>
</td></tr>
<tr><td><code id="dr_+3A_chi2approx">chi2approx</code></td>
<td>
<p>Several dr methods compute significance levels using 
statistics that are asymptotically distributed as a linear combination of
<code class="reqn">\chi^2(1)</code> random variables.  This keyword chooses the method for
computing the chi2approx, either <code>"bx"</code>, the default for a method
suggested by Bentler and Xie (2000) or <code>"wood"</code> for a method proposed
by Wood (1989).</p>
</td></tr>
<tr><td><code id="dr_+3A_...">...</code></td>
<td>
<p>For <code>dr</code>, all additional 
arguments passed to <code>dr.compute</code>.  For 
<code>dr.compute</code>, additional 
arguments may be required for particular dimension reduction method.  For
example, 
<code>nslices</code> is the number of slices used by <code>"sir"</code> and <code>"save"</code>.
<code>numdir</code> is the maximum number of directions to compute, with
default equal to 4. Other methods may have other defaults.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The general regression problem studies <code class="reqn">F(y|x)</code>, the conditional
distribution of a response <code class="reqn">y</code> given a set of predictors <code class="reqn">x</code>.  
This function provides methods for estimating the dimension and central
subspace of a general regression problem.  That is, we want to find a 
<code class="reqn">p \times d</code> matrix <code class="reqn">B</code> of minimal rank <code class="reqn">d</code> such that 
</p>
<p style="text-align: center;"><code class="reqn">F(y|x)=F(y|B'x)</code>
</p>
  
<p>Both the dimension <code class="reqn">d</code> and the subspace
<code class="reqn">R(B)</code> are unknown.  These methods make few assumptions.  Many methods
are based on the inverse distribution, <code class="reqn">F(x|y)</code>.  
</p>
<p>For the methods <code>"sir"</code>, <code>"save"</code>, <code>"phdy"</code> and 
<code>"phdres"</code>, a kernel matrix <code class="reqn">M</code> is estimated such that the 
column space of <code class="reqn">M</code> should be close to the central subspace 
<code class="reqn">R(B)</code>.  The eigenvectors corresponding to the <code>d</code> largest 
eigenvalues of <code class="reqn">M</code> provide an estimate of <code class="reqn">R(B)</code>.
</p>
<p>For the method <code>"ire"</code>, subspaces are estimated by minimizing 
an objective function.
</p>
<p>Categorical predictors can be included using the <code>groups</code> 
argument, with the methods <code>"sir"</code>, <code>"save"</code> and 
<code>"ire"</code>, using the ideas from Chiaromonte, Cook and Li (2002).
</p>
<p>The primary output from this method is (1) a set of vectors whose 
span estimates <code>R(B)</code>; and various tests concerning the 
dimension <code>d</code>.  
</p>
<p>Weights can be used, essentially to specify the relative 
frequency of each case in the data.  Empirical weights that make 
the contours of the weighted sample closer to elliptical can be 
computed using <code>dr.weights</code>.  
This will usually result in zero weight for some 
cases.  The function will set zero estimated weights to missing.
</p>


<h3>Value</h3>

<p>dr returns an object that inherits from dr (the name of the type is the
value of the <code>method</code> argument), with attributes:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>The design matrix</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>The response vector</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>The weights used, normalized to add to n.</p>
</td></tr>
<tr><td><code>qr</code></td>
<td>
<p>QR factorization of x.</p>
</td></tr>
<tr><td><code>cases</code></td>
<td>
<p>Number of cases used.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The initial call to <code>dr</code>.</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p>A matrix that depends on the method of computing.  The column space
of M should be close to the central subspace.</p>
</td></tr>
<tr><td><code>evalues</code></td>
<td>
<p>The eigenvalues of M (or squared singular values if M is not
symmetric).</p>
</td></tr>
<tr><td><code>evectors</code></td>
<td>
<p>The eigenvectors of M (or of M'M if M is not square and
symmetric) ordered according to the eigenvalues.</p>
</td></tr>
<tr><td><code>chi2approx</code></td>
<td>
<p>Value of the input argument of this name.</p>
</td></tr>
<tr><td><code>numdir</code></td>
<td>
<p>The maximum number of directions to be found.  The output
value of numdir may be smaller than the input value.</p>
</td></tr>
<tr><td><code>slice.info</code></td>
<td>
<p>output from 'sir.slice', used by sir and save.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the dimension reduction method used.</p>
</td></tr> 
<tr><td><code>terms</code></td>
<td>
<p>same as terms attribute in lm or glm.  Needed to make <code>update</code>
work correctly.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>If method=<code>"save"</code>, then <code>A</code> is a three dimensional array needed to
compute test statistics.</p>
</td></tr>  
</table>


<h3>Author(s)</h3>

<p>Sanford Weisberg, &lt;sandy@stat.umn.edu&gt;.</p>


<h3>References</h3>

 
<p>Bentler, P. M. and Xie, J. (2000), Corrections to test statistics in 
principal Hessian directions.  <em>Statistics and Probability 
Letters</em>, 47, 381-389.  Approximate p-values.
</p>
<p>Cook, R. D. (1998).  <em>Regression Graphics</em>.  New York:  Wiley.  
This book provides the basic results for dimension reduction 
methods, including detailed discussion of the methods <code>"sir"</code>, 
<code>"phdy"</code> and <code>"phdres"</code>.
</p>
<p>Cook, R. D. (2004). Testing predictor contributions in sufficient 
dimension reduction. <em>Annals of Statistics</em>, 32, 1062-1092.  
Introduced marginal coordinate tests.
</p>
<p>Cook, R. D. and Nachtsheim, C. (1994), Reweighting to achieve 
elliptically contoured predictors in regression.  <em>Journal of 
the American Statistical Association</em>, 89, 592&ndash;599.  Describes the 
weighting scheme used by <code><a href="#topic+dr.weights">dr.weights</a></code>.
</p>
<p>Cook, R. D. and Ni, L. (2004). Sufficient dimension reduction via 
inverse regression:  A minimum discrrepancy approach, <em>Journal 
of the American Statistical Association</em>, 100, 410-428. The 
<code>"ire"</code> is described in this paper.
</p>
<p>Cook, R. D. and Weisberg, S. (1999).  <em>Applied Regression 
Including Computing and Graphics</em>, New York:  Wiley, 
<a href="http://www.stat.umn.edu/arc">http://www.stat.umn.edu/arc</a>.  The program <code>arc</code> described 
in this book also computes most of the dimension reduction methods 
described here.
</p>
<p>Chiaromonte, F., Cook, R. D. and Li, B. (2002). Sufficient dimension 
reduction in regressions with categorical predictors. Ann. Statist. 
30 475-497.  Introduced grouping, or conditioning on factors.
</p>
<p>Shao, Y., Cook, R. D. and Weisberg (2007).  Marginal tests with 
sliced average variance estimation.  <em>Biometrika</em>.  Describes 
the tests used for <code>"save"</code>. 
</p>
<p>Wen, X. and Cook, R. D. (2007).  Optimal Sufficient Dimension 
Reduction in Regressions with Categorical Predictors, <em>Journal 
of Statistical Inference and Planning</em>.   This paper extends the 
<code>"ire"</code> method to grouping.  
</p>
<p>Wood, A. T. A. (1989) An <code class="reqn">F</code> approximation to the distribution 
of a linear combination of chi-squared variables. 
<em>Communications in Statistics: Simulation and Computation</em>, 18, 
1439-1456.  Approximations for p-values. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ais)
# default fitting method is "sir"
s0 &lt;- dr(LBM~log(SSF)+log(Wt)+log(Hg)+log(Ht)+log(WCC)+log(RCC)+
  log(Hc)+log(Ferr),data=ais) 
# Refit, using a different function for slicing to agree with arc.
summary(s1 &lt;- update(s0,slice.function=dr.slices.arc))
# Refit again, using save, with 10 slices; the default is max(8,ncol+3)
summary(s2&lt;-update(s1,nslices=10,method="save"))
# Refit, using phdres.  Tests are different for phd, and not
# Fit using phdres; output is similar for phdy, but tests are not justifiable. 
summary(s3&lt;- update(s1,method="phdres"))
# fit using ire:
summary(s4 &lt;- update(s1,method="ire"))
# fit using Sex as a grouping variable.  
s5 &lt;- update(s4,group=~Sex)
</code></pre>

<hr>
<h2 id='dr.coordinate.test'>Dimension reduction tests</h2><span id='topic+dr.test'></span><span id='topic+dr.coordinate.test'></span><span id='topic+dr.joint.test'></span><span id='topic+dr.joint.test.default'></span><span id='topic+dr.joint.test.ire'></span><span id='topic+dr.joint.test.pire'></span><span id='topic+dr.coordinate.test.default'></span><span id='topic+dr.coordinate.test.sir'></span><span id='topic+dr.coordinate.test.save'></span><span id='topic+dr.coordinate.test.ire'></span><span id='topic+dr.coordinate.test.pire'></span>

<h3>Description</h3>

<p>Functions to compute various tests concerning the dimension of a
central subspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dr.test(object, numdir, ...)

dr.coordinate.test(object, hypothesis,d,chi2approx,...)

## S3 method for class 'ire'
dr.joint.test(object, hypothesis, d = NULL,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dr.coordinate.test_+3A_object">object</code></td>
<td>
<p> The name of an object returned by a call to <code>dr</code>.</p>
</td></tr>
<tr><td><code id="dr.coordinate.test_+3A_hypothesis">hypothesis</code></td>
<td>
<p> A specification of the null hypothesis to be tested by
the coordinate hypothesis.  See details below for options.</p>
</td></tr>
<tr><td><code id="dr.coordinate.test_+3A_d">d</code></td>
<td>
<p>For conditional coordinate hypotheses, specify the dimension of
the central mean subspace, typically 1, 2 or possibly 3.  If left at the
default, tests are unconditional.</p>
</td></tr>
<tr><td><code id="dr.coordinate.test_+3A_numdir">numdir</code></td>
<td>
<p>The maximum dimension to consider.  If not set defaults to 4.</p>
</td></tr>
<tr><td><code id="dr.coordinate.test_+3A_chi2approx">chi2approx</code></td>
<td>
<p>Approximation method for p.values of linear combination
of <code class="reqn">\chi^2(1)</code> random variables.  Choices are from <code>c("bx","wood")</code>,
for the Bentler-Xie and Wood approximatations, respectively. The default is either &quot;bx&quot;
or the value set in the call that created the dr object. </p>
</td></tr>
<tr><td><code id="dr.coordinate.test_+3A_...">...</code></td>
<td>
<p>Additional arguments.  None are currently available.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dr.test</code> returns marginal dimension tests.  
<code>dr.coordinate.test</code> returns marginal dimension tests (Cook, 2004)
if <code>d=NULL</code> or conditional dimension tests if <code>d</code> is a
positive integer giving the assumed dimension of the central
subspace.  The function <code>dr.joint.test</code> tests the coordinate
hypothesis and dimension simultaneously.  It is defined only for 
ire, and is used to compute the conditional coordinate test.
</p>
<p>As an example, suppose we have created a <code>dr</code> object 
using the formula
<code>y ~ x1 + x2 + x3 + x4</code>.    
The marginal coordinate hypothesis defined by Cook (2004) tests 
the hypothesis that <code>y</code> is independent of some of the 
predictors given the other predictors.  For example, one could test 
whether <code>x4</code> could be dropped from the problem by testing <code>y</code> 
independent of <code>x4</code> given <code>x1,x2,x3</code>.
</p>
<p>The hypothesis to be tested is determined by the argument <code>hypothesis</code>.
The argument <code>hypothesis = ~.-x4</code> would test the hypothesis of the last
paragraph.  Alternatively, <code>hypothesis = ~x1+x2+x3</code> would
fit the same hypothesis.
</p>
<p>More generally, if <code>H</code> is a <code class="reqn">p \times q</code> 
rank <code class="reqn">q</code> matrix, and 
<code class="reqn">P(H)</code> is the projection
on the column space of <code>H</code>, then specifying <code>hypothesis = H</code> will test the 
hypothesis that <code class="reqn">Y</code> is independent of <code class="reqn">(I-P(H))X | P(H)X</code>.
</p>


<h3>Value</h3>

<p>Returns a list giving the value of the test statistic and an asymptotic
p.value computed from
the test statistic.  For SIR objects, the p.value is computed in two ways.  The
<em>general test</em>, indicated by <code>p.val(Gen)</code> in the output, assumes only 
that the predictors are linearly related.  The <em>restricted test</em>, indicated 
by <code>p.val(Res)</code> in the output, assumes in addition to the linearity condition
that a constant covariance condition holds; see Cook (2004) for more information
on these assumptions.  In either case, the asymptotic distribution is a linear
combination of Chi-squared random variables.  The function specified by the
<code>chi2approx</code> approximates this linear combination by a single Chi-squared 
variable.
</p>
<p>For SAVE objects, two p.values are also returned.  <code>p.val(Nor)</code> assumes
predictors are normally distributed, in which case the test statistic is asympotically
Chi-sqaured with the number of df shown.  Assuming general linearly related
predictors we again get an asymptotic linear combination of Chi-squares that
leads to <code>p.val(Gen)</code>.
</p>
<p>For IRE and PIRE, the tests
statistics have an asymptotic <code class="reqn">\chi^2</code> distribution, so the
value of <code>chi2approx</code> is not relevant.
</p>


<h3>Author(s)</h3>

<p>Yongwu Shao for SIR and SAVE and Sanford Weisberg
for all methods, &lt;sandy@stat.umn.edu&gt;</p>


<h3>References</h3>

<p> Cook, R. D. (2004).  Testing predictor contributions in 
sufficient dimension reduction.  <em>Annals of Statistics</em>, 32, 1062-1092.
</p>
<p>Cook, R. D. and Ni, L.
(2004).  Sufficient dimension reduction via inverse regression:  A minimum
discrrepancy approach, <em>Journal of the American Statistical Association</em>,
100, 410-428.
</p>
<p>Cook, R. D. and Weisberg, S. (1999). <em>Applied Regression Including
Computing and Graphics</em>.  Hoboken NJ:  Wiley.
</p>
<p>Shao, Y., Cook, R. D. and Weisberg, S. (2007, in press).  Marginal tests with 
sliced average variance estimation.  <em>Biometrika</em>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+drop1.dr">drop1.dr</a></code>, <code><a href="#topic+coord.hyp.basis">coord.hyp.basis</a></code>,
<code><a href="#topic+dr.step">dr.step</a></code>,
<code><a href="#topic+dr.pvalue">dr.pvalue</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>#  This will match Table 5 in Cook (2004).  
data(ais)
# To make this idential to Arc (Cook and Weisberg, 1999), need to modify slices to match.
summary(s1 &lt;- dr(LBM~log(SSF)+log(Wt)+log(Hg)+log(Ht)+log(WCC)+log(RCC)+log(Hc)+log(Ferr),
  data=ais,method="sir",slice.function=dr.slices.arc,nslices=8))
dr.coordinate.test(s1,~.-log(Hg))
#The following nearly reproduces Table 5 in Cook (2004)
drop1(s1,chi2approx="wood",update=FALSE)
drop1(s1,d=2,chi2approx="wood",update=FALSE)
drop1(s1,d=3,chi2approx="wood",update=FALSE)
</code></pre>

<hr>
<h2 id='dr.directions'> Directions selected by dimension reduction regressiosn </h2><span id='topic+dr.direction'></span><span id='topic+dr.directions'></span><span id='topic+dr.direction.default'></span><span id='topic+dr.direction.ire'></span><span id='topic+dr.basis'></span><span id='topic+dr.basis.default'></span><span id='topic+dr.basis.ire'></span>

<h3>Description</h3>

<p>Dimension reduction regression returns a set of up to <code class="reqn">p</code> orthogonal direction
vectors each of length <code class="reqn">p</code>, the first <code class="reqn">d</code> of which are estimates a basis of a 
<code class="reqn">d</code> dimensional central subspace.  The function returns the estimated directions 
in the original <code class="reqn">n</code> dimensional space for plotting.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dr.direction(object, which, x)
dr.directions(object, which, x)
## Default S3 method:
dr.direction(object, which=NULL,x=dr.x(object))

dr.basis(object,numdir)

## S3 method for class 'ire'
dr.basis(object,numdir=length(object$result))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dr.directions_+3A_object">object</code></td>
<td>
<p> a dimension reduction regression object created by dr. </p>
</td></tr>
<tr><td><code id="dr.directions_+3A_which">which</code></td>
<td>
<p> select the directions wanted, default is all directions.
If method is <code>ire</code>, then the directions depend on the value of the
dimension you select.  If omitted, select all directions. </p>
</td></tr>
<tr><td><code id="dr.directions_+3A_numdir">numdir</code></td>
<td>
<p>The number of basis vectors to return</p>
</td></tr>
<tr><td><code id="dr.directions_+3A_x">x</code></td>
<td>
<p>select the X matrix, the default is <code>dr.x(object)</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Dimension reduction regression is used to estimate a basis of the central
subspace or mean central subspace of a regression.  If there are <code class="reqn">p</code> 
predictors, the dimension of the central subspace is less than or equal to
<code class="reqn">p</code>.  These two functions, <code>dr.basis</code> and <code>dr.direction</code>, 
return vectors that describe the central subspace in various ways.
</p>
<p>Consder <code>dr.basis</code> first.  If you set <code>numdir=3</code>, for example, this
method will return a <code class="reqn">p</code> by 3 matrix whose columns span the estimated
three dimensional central subspace.  For all methods except for <code>ire</code>,
this simply returns the first three columns of <code>object$evectors</code>.  For
the <code>ire</code> method, this returns the three vectors determined by a 
three-dimensional solution. Call this matrix <code class="reqn">C</code>.  The basis is 
determined by back-transforming from centered and scaled predictors to
the scale of the original predictors, and then renormalizing the vectors
to have length one.  These vectors are orthogonal in the inner
product determined by Var(X).
</p>
<p>The <code>dr.direction</code> method return <code class="reqn">XC</code>, the same space but now a
subspace of the original <code class="reqn">n</code>-dimensional space.  These vectors are
appropriate for plotting.   
</p>


<h3>Value</h3>

<p>Both functions return a matrix:  for <code>dr.direction</code>, the matrix has n rows and
numdir columns, and for <code>dr.basis</code> it has p rows and numdir columns.
</p>


<h3>Author(s)</h3>

<p>Sanford Weisberg &lt;sandy@stat.umn.edu&gt; </p>


<h3>References</h3>

<p> See R. D. Cook (1998).  Regression Graphics.  New York:  Wiley. </p>


<h3>See Also</h3>

  <p><code><a href="#topic+dr">dr</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ais)
#fit dimension reduction using sir
m1 &lt;- dr(LBM~Wt+Ht+RCC+WCC, method="sir", nslices = 8, data=ais)
summary(m1)
dr.basis(m1)
dr.directions(m1)
</code></pre>

<hr>
<h2 id='dr.permutation.test'>Permutation tests of dimension for dr</h2><span id='topic+dr.permutation.test'></span><span id='topic+dr.permutation.test.statistic'></span>

<h3>Description</h3>

<p>Approximates marginal dimension test significance levels for sir, save,
and phd by sampling from the permutation distribution.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dr.permutation.test(object, npermute=50,numdir=object$numdir)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dr.permutation.test_+3A_object">object</code></td>
<td>
<p>a dimension reduction regression object created by dr</p>
</td></tr>
<tr><td><code id="dr.permutation.test_+3A_npermute">npermute</code></td>
<td>
<p>number of permutations to compute, default is 50</p>
</td></tr>
<tr><td><code id="dr.permutation.test_+3A_numdir">numdir</code></td>
<td>
<p>maximum permitted value of the dimension, 
with the default from the object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The method approximates significance levels of the marginal dimension
tests based on a permutation test.  The algorithm:  (1) permutes the
rows of the predictor but not the response; (2) computes marginal
dimension tests for the permuted data; (3) obtains significane levels 
by comparing the observed statsitics to the permutation distribution.
</p>
<p>The method is not implemented for ire.
</p>


<h3>Value</h3>

<p>Returns an object of type &lsquo;dr.permutation.test&rsquo; that can be printed or
summarized to give the summary of the test.
</p>


<h3>Author(s)</h3>

<p> Sanford Weisberg, sandy@stat.umn.edu</p>


<h3>References</h3>

<p>See www.stat.umn.edu/arc/addons.html, and then select the article
on dimension reduction regression or inverse regression.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+dr">dr</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ais)
attach(ais)  # the Australian athletes data
#fit dimension reduction regression using sir
m1 &lt;- dr(LBM~Wt+Ht+RCC+WCC, method="sir", nslices = 8)
summary(m1)
dr.permutation.test(m1,npermute=100)
plot(m1)
</code></pre>

<hr>
<h2 id='dr.pvalue'> Compute the Chi-square approximations to a weighted
sum of Chi-square(1) random variables. </h2><span id='topic+dr.pvalue'></span><span id='topic+wood.pvalue'></span><span id='topic+bentlerxie.pvalue'></span>

<h3>Description</h3>

<p>Returns an approximate quantile for a weighted sum of independent
<code class="reqn">\chi^2(1)</code> random variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dr.pvalue(coef,f,chi2approx=c("bx","wood"),...)

bentlerxie.pvalue(coef, f)

wood.pvalue(coef, f, tol=0.0, print=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dr.pvalue_+3A_coef">coef</code></td>
<td>
<p> a vector of nonnegative weights </p>
</td></tr>
<tr><td><code id="dr.pvalue_+3A_f">f</code></td>
<td>
<p> Observed value of the statistic </p>
</td></tr>
<tr><td><code id="dr.pvalue_+3A_chi2approx">chi2approx</code></td>
<td>
<p>Which approximation should be used?</p>
</td></tr>
<tr><td><code id="dr.pvalue_+3A_tol">tol</code></td>
<td>
<p>tolerance for Wood's method.</p>
</td></tr>
<tr><td><code id="dr.pvalue_+3A_print">print</code></td>
<td>
<p>Printed output for Wood's method</p>
</td></tr>
<tr><td><code id="dr.pvalue_+3A_...">...</code></td>
<td>
<p>Arguments passed from <code>dr.pvalue</code> to 
wood.pvalue.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For Bentler-Xie, we approximate <code class="reqn">f</code> by <code class="reqn">c \chi^2(d)</code> for values of <code class="reqn">c</code>
and <code class="reqn">d</code> computed by the function.  The Wood approximation is more 
complicated.</p>


<h3>Value</h3>

<p>Returns a data.frame with four named components:
</p>
<table role = "presentation">
<tr><td><code>test</code></td>
<td>
<p>The input argument <code>f</code>.</p>
</td></tr>
<tr><td><code>test.adj</code></td>
<td>
<p>For Bentler-Xie, returns <code class="reqn">cf</code>; for Wood, returns <code>NA</code>.</p>
</td></tr>
<tr><td><code>df.adj</code></td>
<td>
<p>For Bentler-Xie, returns <code class="reqn">d</code>; for Wood, returns <code>NA</code>.</p>
</td></tr>
<tr><td><code>pval.adj</code></td>
<td>
<p>Approximate p.value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Sanford Weisberg &lt;sandy@stat.umn.edu&gt; </p>


<h3>References</h3>

<p> Peter M. Bentler and Jun Xie (2000), Corrections to test
statistics in principal Hessian directions.  <em>Statistics and
Probability Letters</em>, 47, 381-389. 
</p>
<p>Wood, Andrew T. A. (1989)
An <code class="reqn">F</code> approximation to the distribution of a linear combination of 
chi-squared variables.
<em>Communications in Statistics: Simulation and Computation</em>, 18, 1439-1456.</p>

<hr>
<h2 id='dr.slices'> Divide a vector into slices of approximately equal size </h2><span id='topic+dr.slices'></span><span id='topic+dr.slices.arc'></span>

<h3>Description</h3>

<p>Divides a vector into slices of approximately equal size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dr.slices(y, nslices)

dr.slices.arc(y, nslices)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dr.slices_+3A_y">y</code></td>
<td>
<p>a vector of length <code class="reqn">n</code> or an <code class="reqn">n \times p</code> matrix</p>
</td></tr>
<tr><td><code id="dr.slices_+3A_nslices">nslices</code></td>
<td>
<p>the number of slices, no larger than <code class="reqn">n</code>, or a vector of
<code class="reqn">p</code> numbers giving the number of slices in each direction.  If <code class="reqn">y</code> 
has <code class="reqn">p</code> 
columns and nslices is a number, then the number of slices in each direction
is the smallest integer greater than the p-th root of nslices.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code class="reqn">y</code> is an n-vector, order <code class="reqn">y</code>.  The goal for the number of observations per slice
is <code class="reqn">m</code>, the smallest integer in nslices/n. Allocate the first <code class="reqn">m</code> observations to
slice 1.  If there are duplicates in <code class="reqn">y</code>, keep adding observations to the first
slice until the next value of <code class="reqn">y</code> is not equal to the largest value in the 
first slice.  Allocate the next <code class="reqn">m</code> values to the next slice, and again check 
for ties.  Continue until all values are allocated to a slice.  This does not
guarantee that nslices will be obtained, nor does it guarantee an equal number
of observations per slice.  This method of choosing slices is invariant under
rescaling, but not under multiplication by <code class="reqn">-1</code>, so the slices of <code class="reqn">y</code> will not
be the same as the slices of <code class="reqn">-y</code>.  This function was rewritten for Version 2.0.4 of
this package, and will no longer give exactly the same results as the program Arc.  If you
want to duplicate Arc, use the function <code>dr.slice.arc</code>, as illustrated in the 
example below.
</p>
<p>If <code class="reqn">y</code> is a matrix of p columns, slice the first column as described above.  Then,
within each of the slices determined for the first column, slice based on the
second column, so that each of the &ldquo;cells&rdquo; has approximately the same number
of observations.  Continue through all the columns.  This method is not
invariant under reordering of the columns, or under multiplication by <code class="reqn">-1</code>.
</p>


<h3>Value</h3>

<p>Returns a named list with three elements as follows:
</p>
<table role = "presentation">
<tr><td><code>slice.indicator</code></td>
<td>
<p>ordered eigenvectors that describe the estimates of the 
dimension reduction subspace</p>
</td></tr>
<tr><td><code>nslices</code></td>
<td>
<p>Gives the actual number of slices produced, which may be smaller 
than the number requested.</p>
</td></tr>
<tr><td><code>slice.sizes</code></td>
<td>
<p>The number of observations in each slice.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Sanford Weisberg, &lt;sandy@stat.umn.edu&gt; </p>


<h3>References</h3>

<p>R. D. Cook and S. Weisberg (1999), <em>Applied Regression Including
Computing and Graphics</em>, New York:  Wiley. </p>


<h3>See Also</h3>

  <p><code><a href="#topic+dr">dr</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'> 
data(ais)
summary(s1 &lt;- dr(LBM~log(SSF)+log(Wt)+log(Hg)+log(Ht)+log(WCC)+log(RCC)+
                 log(Hc)+log(Ferr), data=ais,method="sir",nslices=8))
# To make this idential to ARC, need to modify slices to match.
summary(s2 &lt;- update(s1,slice.info=dr.slices.arc(ais$LBM,8)))
</code></pre>

<hr>
<h2 id='dr.weights'> Estimate weights for elliptical symmetry</h2><span id='topic+dr.weights'></span>

<h3>Description</h3>

<p>This function estimate weights to apply to the rows of a data matrix to
make the resulting weighted matrix as close to elliptically symmetric
as possible.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dr.weights(formula, data = list(), subset, na.action = na.fail, 
    sigma=1, nsamples=NULL, ...) 

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dr.weights_+3A_formula">formula</code></td>
<td>
<p>A one-sided or two-sided formula.  The right hand side is
used to define the design matrix.</p>
</td></tr> 
<tr><td><code id="dr.weights_+3A_data">data</code></td>
<td>
<p>An optional data frame.</p>
</td></tr>
<tr><td><code id="dr.weights_+3A_subset">subset</code></td>
<td>
<p>A list of cases to be used in computing the weights.</p>
</td></tr>
<tr><td><code id="dr.weights_+3A_na.action">na.action</code></td>
<td>
<p>The default is na.fail, to prohibit computations.  
If set to na.omit, the function will return a list of weights of the 
wrong length for use with dr.</p>
</td></tr>
<tr><td><code id="dr.weights_+3A_nsamples">nsamples</code></td>
<td>
<p>The weights are determined by random sampling from a
data-determined normal distribution.  This controls the number of samples.  The
default is 10 times the number of cases.</p>
</td></tr>
<tr><td><code id="dr.weights_+3A_sigma">sigma</code></td>
<td>
<p>Scale factor, set to one by default; see the paper by
Cook and Nachtsheim for more information on choosing this parameter.</p>
</td></tr>
<tr><td><code id="dr.weights_+3A_...">...</code></td>
<td>
<p>Arguments are passed to <code><a href="MASS.html#topic+cov.rob">cov.rob</a></code> to compute a
robust estimate of the covariance matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic outline is:  (1) Estimate a mean m and covariance matrix S using a
possibly robust method; (2) For each iteration, obtain a random vector
from N(m,sigma*S).  Add 1 to a counter for observation i if the i-th row
of the data matrix is closest to the random vector; (3) return as weights
the sample faction allocated to each observation.  If you set the keyword
<code>weights.only</code> to <code>T</code> on the call to <code>dr</code>, then only the
list of weights will be returned.</p>


<h3>Value</h3>

<p>Returns a list of <code class="reqn">n</code> weights, some of which may be zero.  
</p>


<h3>Author(s)</h3>

<p>Sanford Weisberg, sandy@stat.umn.edu</p>


<h3>References</h3>

<p> R. D. Cook and C. Nachtsheim (1994), Reweighting to achieve
elliptically contoured predictors in regression.  Journal of the American
Statistical Association, 89, 592&ndash;599.</p>


<h3>See Also</h3>

<p><code><a href="#topic+dr">dr</a></code>, <code><a href="MASS.html#topic+cov.rob">cov.rob</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ais)
w1 &lt;- dr.weights(~ Ht +Wt +RCC, data = ais)
m1 &lt;- dr(LBM~Ht+Wt+RCC,data=ais,weights=w1)
</code></pre>

<hr>
<h2 id='dr.x'> Accessor functions for data in dr objects </h2><span id='topic+dr.x'></span><span id='topic+dr.y'></span><span id='topic+dr.z'></span>

<h3>Description</h3>

<p>Accessor functions for dr objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dr.x(object)
dr.y(object)
dr.z(object) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dr.x_+3A_object">object</code></td>
<td>
<p> An object that inherits from <code>dr</code>. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a component of a dr object.  <code>dr.x</code> returns the predictor
matrix reduced to full rank by dropping trailing columns; <code>dr.y</code>
returns the response vector/matrix, and <code>dr.z</code> returns the centered
and scaled predictor matrix.
</p>


<h3>Author(s)</h3>

<p> Sanford Weisberg, &lt;sandy@stat.umn.edu&gt; </p>


<h3>See Also</h3>

  <p><code><a href="#topic+dr">dr</a></code>. </p>

<hr>
<h2 id='drop1.dr'> Sequential fitting of coordinate tests using a dr object </h2><span id='topic+drop1.dr'></span><span id='topic+dr.step'></span>

<h3>Description</h3>

<p>This function implements backward elimination using a <code>dr</code> object for which
a <code>dr.coordinate.test</code> is defined, currently for SIR SAVE, IRE and PIRE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dr.step(object,scope=NULL,d=NULL,minsize=2,stop=0,trace=1,...)

## S3 method for class 'dr'
drop1(object, scope = NULL,  update=TRUE,
test="general",trace=1,...)


</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="drop1.dr_+3A_object">object</code></td>
<td>
<p> A <code>dr</code> object for which <code>dr.coordinate.test</code> 
is defined, for <code>method</code> equal to one of <code>sir</code>, <code>save</code>
or <code>ire</code>.</p>
</td></tr>
<tr><td><code id="drop1.dr_+3A_scope">scope</code></td>
<td>
<p>A one sided formula specifying predictors that will never
be removed.</p>
</td></tr>
<tr><td><code id="drop1.dr_+3A_d">d</code></td>
<td>
<p>To use <em>conditional coordinate tests</em>, specify the dimension
of the central (mean) subspace.  The default is <code>NULL</code>, meaning no
conditioning. This is currently available only for methods <code>sir</code>, 
<code>save</code> without categorical predictors, or for 
<code>ire</code> with or without categorical predictors.</p>
</td></tr>
<tr><td><code id="drop1.dr_+3A_minsize">minsize</code></td>
<td>
<p>Minimum subset size, must be greater than or equal to 2.</p>
</td></tr>
<tr><td><code id="drop1.dr_+3A_stop">stop</code></td>
<td>
<p>Set stopping criterion:  continue removing variables until
the p-value for the next variable to be removed is less than stop.  The
default is stop = 0.</p>
</td></tr>
<tr><td><code id="drop1.dr_+3A_update">update</code></td>
<td>
<p>If true, the <code>update</code> method is used to return a
<code>dr</code> object obtained from <code>object</code> by updating the formula to
drop the variable with the largest p.value.  This can significantly slow
the computations for IRE but has little effect on SAVE and SIR.</p>
</td></tr>
<tr><td><code id="drop1.dr_+3A_test">test</code></td>
<td>
<p> Type of test to be used for selecting the next predictor
to remove for <code>method="save"</code> only. <code>"normal"</code> assumes normal 
predictors, <code>"general"</code> assumes elliptically contoured predictors. 
For other methods, this argument is ignored.</p>
</td></tr>
<tr><td><code id="drop1.dr_+3A_trace">trace</code></td>
<td>
<p>If positive, print informative output at each step, the default.
If trace is 0 or false, suppress all printing.</p>
</td></tr>
<tr><td><code id="drop1.dr_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>dr.coordinate.test</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose a <code>dr</code> object has <code class="reqn">p=a+b</code> predictors, with <code class="reqn">a</code> predictors specified in the <code>scope</code> statement.  
<code>drop1</code> will compute either marginal coordinate tests (if <code>d=NULL</code>)
or conditional marginal coordinate tests (if <code>d</code> is positive) for dropping each of the <code>b</code> predictors not in the scope, and return p.values.
The result is an object created from the original object with the predictor
with the largest p.value removed.
</p>
<p><code>dr.step</code> will call <code>drop1.dr</code> repeatedly until 
<code class="reqn">\max(a,d+1)</code> predictors remain.  </p>


<h3>Value</h3>

<p>As a side effect, 
a data frame of labels, tests, df, and p.values is printed.  If
<code>update=TRUE</code>, a  <code>dr</code>
object is returned with the predictor with the largest p.value removed.
</p>


<h3>Author(s)</h3>

<p>Sanford Weisberg, &lt;sandy@stat.umn.edu&gt;, based on the 
<code>drop1</code> generic function in the
base R. The <code>dr.step</code> function is also similar to <code>step</code> in 
base R.</p>


<h3>References</h3>

<p>Cook, R. D. (2004).  Testing predictor contributions in 
sufficient dimension reduction.  <em>Annals of Statistics</em>, 32, 1062-1092.
</p>
<p>Shao, Y., Cook, R. D. and Weisberg (2007).  Marginal tests with 
sliced average variance estimation.  <em>Biometrika</em>.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+dr.coordinate.test">dr.coordinate.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ais)
# To make this idential to ARC, need to modify slices to match by
# using slice.info=dr.slices.arc() rather than nslices=8
summary(s1 &lt;- dr(LBM~log(SSF)+log(Wt)+log(Hg)+log(Ht)+log(WCC)+log(RCC)+
                 log(Hc)+log(Ferr), data=ais,method="sir",
                 slice.method=dr.slices.arc,nslices=8)) 
# The following will almost duplicate information in Table 5 of Cook (2004).
# Slight differences occur because a different approximation for the
# sum of independent chi-square(1) random variables is used:
ans1 &lt;- drop1(s1)
ans2 &lt;- drop1(s1,d=2)
ans3 &lt;- drop1(s1,d=3)
# remove predictors stepwise until we run out of variables to drop.
dr.step(s1,scope=~log(Wt)+log(Ht))
</code></pre>

<hr>
<h2 id='mussels'>Mussels' muscles data</h2><span id='topic+mussels'></span>

<h3>Description</h3>

<p>Data were furnished by Mike Camden, Wellington Polytechnic, Wellington,
New Zealand.  Horse mussels, (Atrinia), were sampled from the Marlborough
Sounds.  The response is the mussels' Muscle Mass.
</p>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>H</dt><dd>
<p>Shell height in mm
</p>
</dd>
<dt>L</dt><dd>
<p>Shell length in mm
</p>
</dd>
<dt>M</dt><dd>
<p>Muscle mass in g
</p>
</dd>
<dt>S</dt><dd>
<p>Shell mass in g
</p>
</dd>
<dt>W</dt><dd>
<p>Shell width in mm
</p>
</dd>
</dl>



<h3>Source</h3>

<p>R. D. Cook and S. Weisberg (1999). <em>Applied
Statistics Including Computing and Graphics</em>.  New York:  Wiley.</p>

<hr>
<h2 id='plot.dr'> Basic plot of a dr object </h2><span id='topic+plot.dr'></span>

<h3>Description</h3>

<p>Plots selected direction vectors determined by a dimension reduction regression fit.
By default, the <code>pairs</code> function is used for plotting, but the user can use any
other graphics command that is appropriate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dr'
plot(x, which = 1:x$numdir, mark.by.y = FALSE, plot.method = pairs, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.dr_+3A_x">x</code></td>
<td>
<p>The name of an object of class dr, a dimension reduction regression object </p>
</td></tr>
<tr><td><code id="plot.dr_+3A_which">which</code></td>
<td>
<p>selects the directions to be plotted</p>
</td></tr>
<tr><td><code id="plot.dr_+3A_mark.by.y">mark.by.y</code></td>
<td>
<p> if TRUE, color points according to the value of the response, otherwise,
do not color points but include the response as a variable in the plot.</p>
</td></tr>
<tr><td><code id="plot.dr_+3A_plot.method">plot.method</code></td>
<td>
<p>the name of a function for the plotting.  The default is <code>pairs</code>. </p>
</td></tr>
<tr><td><code id="plot.dr_+3A_...">...</code></td>
<td>
<p> arguments passed to the plot.method. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a graph.
</p>


<h3>Author(s)</h3>

<p>Sanford Weisberg, &lt;sandy@stat.umn.edu&gt;.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ais)
# default fitting method is "sir"
s0 &lt;- dr(LBM~log(SSF)+log(Wt)+log(Hg)+log(Ht)+log(WCC)+log(RCC)+
  log(Hc)+log(Ferr),data=ais)
plot(s0)
plot(s0,mark.by.y=TRUE)
 </code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
