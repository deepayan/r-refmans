<!DOCTYPE html><html><head><title>Help for package syuzhet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="/home/deepayan/Rinstall/R-devel/lib/R/doc/html/R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {syuzhet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#get_dct_transform'><p>Discrete Cosine Transformation with Reverse Transform.</p></a></li>
<li><a href='#get_nrc_sentiment'><p>Get Emotions and Valence from NRC Dictionary</p></a></li>
<li><a href='#get_nrc_values'><p>Summarize NRC Values</p></a></li>
<li><a href='#get_percentage_values'><p>Chunk a Text and Get Means</p></a></li>
<li><a href='#get_sent_values'><p>Assigns Sentiment Values</p></a></li>
<li><a href='#get_sentences'><p>Sentence Tokenization</p></a></li>
<li><a href='#get_sentiment'><p>Get Sentiment Values for a String</p></a></li>
<li><a href='#get_sentiment_dictionary'><p>Sentiment Dictionaries</p></a></li>
<li><a href='#get_stanford_sentiment'><p>Get Sentiment from the Stanford Tagger</p></a></li>
<li><a href='#get_text_as_string'><p>Load Text from a File</p></a></li>
<li><a href='#get_tokens'><p>Word Tokenization</p></a></li>
<li><a href='#get_transformed_values'><p>Fourier Transform and Reverse Transform Values</p></a></li>
<li><a href='#mixed_messages'><p>Mixed Messages</p></a></li>
<li><a href='#rescale'><p>Vector Value Rescaling</p></a></li>
<li><a href='#rescale_x_2'><p>Bi-Directional x and y axis Rescaling</p></a></li>
<li><a href='#simple_plot'><p>Plots simple and rolling shapes overlayed</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Extracts Sentiment and Sentiment-Derived Plot Arcs from Text</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.7</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-8-11</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matthew Jockers &lt;mjockers@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Extracts sentiment and sentiment-derived plot arcs
    from text using a variety of sentiment dictionaries conveniently
    packaged for consumption by R users.  Implemented dictionaries include
    "syuzhet" (default) developed in the Nebraska Literary Lab
    "afinn" developed by Finn Årup Nielsen, "bing" developed by Minqing Hu
    and Bing Liu, and "nrc" developed by Mohammad, Saif M. and Turney, Peter D.
    Applicable references are available in README.md and in the documentation
    for the "get_sentiment" function.  The package also provides a hack for
    implementing Stanford's coreNLP sentiment parser. The package provides
    several methods for plot arc normalization.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mjockers/syuzhet">https://github.com/mjockers/syuzhet</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>textshape (&ge; 1.3.0), NLP, zoo, dtt, stats, graphics, dplyr,
tidyr, rlang</td>
</tr>
<tr>
<td>Suggests:</td>
<td>devtools, knitr, pander, parallel, readxl, rmarkdown,
stringr, testthat (&ge; 0.9.1)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-11 21:24:23 UTC; matthewjockers</td>
</tr>
<tr>
<td>Author:</td>
<td>Matthew Jockers [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-11 22:10:02 UTC</td>
</tr>
<tr>
<td>Built:</td>
<td>R 4.4.0; ; 2024-01-02 08:01:22 UTC; unix</td>
</tr>
</table>
<hr>
<h2 id='get_dct_transform'>Discrete Cosine Transformation with Reverse Transform.</h2><span id='topic+get_dct_transform'></span>

<h3>Description</h3>

<p>Converts input values into a standardized
set of filtered and reverse transformed values for
easy plotting and/or comparison.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_dct_transform(
  raw_values,
  low_pass_size = 5,
  x_reverse_len = 100,
  scale_vals = FALSE,
  scale_range = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_dct_transform_+3A_raw_values">raw_values</code></td>
<td>
<p>the raw sentiment values
calculated for each sentence</p>
</td></tr>
<tr><td><code id="get_dct_transform_+3A_low_pass_size">low_pass_size</code></td>
<td>
<p>The number of components
to retain in the low pass filtering. Default = 5</p>
</td></tr>
<tr><td><code id="get_dct_transform_+3A_x_reverse_len">x_reverse_len</code></td>
<td>
<p>the number of values to return via decimation. Default = 100</p>
</td></tr>
<tr><td><code id="get_dct_transform_+3A_scale_vals">scale_vals</code></td>
<td>
<p>Logical determines whether or not to normalize the values using the scale function  Default = FALSE.  If TRUE, values will be scaled by subtracting the means and scaled by dividing by their standard deviations.  See ?scale</p>
</td></tr>
<tr><td><code id="get_dct_transform_+3A_scale_range">scale_range</code></td>
<td>
<p>Logical determines whether or not to scale the values from -1 to +1.  Default = FALSE.  If set to TRUE, the lowest value in the vector will be set to -1 and the highest values set to +1 and all the values scaled accordingly in between.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The transformed values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>s_v &lt;- get_sentences("I begin this story with a neutral statement.
Now I add a statement about how much I despise cats.  
I am allergic to them. I hate them. Basically this is a very silly test. But I do love dogs!")
raw_values &lt;- get_sentiment(s_v, method = "syuzhet")
dct_vals &lt;- get_dct_transform(raw_values)
plot(dct_vals, type="l", ylim=c(-0.1,.1))
</code></pre>

<hr>
<h2 id='get_nrc_sentiment'>Get Emotions and Valence from NRC Dictionary</h2><span id='topic+get_nrc_sentiment'></span>

<h3>Description</h3>

<p>Calls the NRC sentiment dictionary to calculate the presence of eight different emotions and their corresponding valence in a text file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_nrc_sentiment(
  char_v,
  cl = NULL,
  language = "english",
  lowercase = TRUE,
  lexicon = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_nrc_sentiment_+3A_char_v">char_v</code></td>
<td>
<p>A character vector</p>
</td></tr>
<tr><td><code id="get_nrc_sentiment_+3A_cl">cl</code></td>
<td>
<p>Optional, for parallel analysis</p>
</td></tr>
<tr><td><code id="get_nrc_sentiment_+3A_language">language</code></td>
<td>
<p>A string</p>
</td></tr>
<tr><td><code id="get_nrc_sentiment_+3A_lowercase">lowercase</code></td>
<td>
<p>should tokens be converted to lowercase. Default equals TRUE</p>
</td></tr>
<tr><td><code id="get_nrc_sentiment_+3A_lexicon">lexicon</code></td>
<td>
<p>a custom lexicon provided by the user and formatted as a data frame containing two columns labeled as &quot;word&quot; and &quot;sentiment&quot;. The &quot;sentiment&quot; column must indicate either the valence of the word (using either the term &quot;positive&quot; or &quot;negative&quot;) or the emotional category of the word, using one of the following terms: &quot;anger&quot;, &quot;anticipation&quot;, &quot;disgust&quot;, &quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;surprise&quot;, &quot;trust&quot;.  For example: the English word &quot;abandon&quot; may appear in your lexicon twice, first with a emotional category of &quot;fear&quot; and again with a value of &quot;negative.&quot;  Not all words necessarily need to have a valence indicator.  See example section below</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame where each row represents a sentence From the original file.  The columns include one for each emotion type as well as a positive or negative valence. The ten columns are as follows: &quot;anger&quot;, &quot;anticipation&quot;, &quot;disgust&quot;, &quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;surprise&quot;, &quot;trust&quot;, &quot;negative&quot;, &quot;positive.&quot;
</p>


<h3>References</h3>

<p>Saif Mohammad and Peter Turney.  &quot;Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon.&quot; In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California.  See: http://saifmohammad.com/WebPages/lexicons.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
my_lexicon &lt;- data.frame(
word = c("love","love", "hate", "hate"), 
sentiment = c("positive", "joy", "negative", "anger")
)
my_example_text &lt;- "I am in love with R programming.  
  I hate writing code in C."
s_v &lt;- get_sentences(my_example_text)
get_nrc_sentiment(s_v, lexicon=my_lexicon)
</code></pre>

<hr>
<h2 id='get_nrc_values'>Summarize NRC Values</h2><span id='topic+get_nrc_values'></span>

<h3>Description</h3>

<p>Access the NRC dictionary to compute emotion types and
valence for a set of words in the input vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_nrc_values(word_vector, language = "english", lexicon = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_nrc_values_+3A_word_vector">word_vector</code></td>
<td>
<p>A character vector.</p>
</td></tr>
<tr><td><code id="get_nrc_values_+3A_language">language</code></td>
<td>
<p>A string</p>
</td></tr>
<tr><td><code id="get_nrc_values_+3A_lexicon">lexicon</code></td>
<td>
<p>A data frame with at least the columns &quot;word&quot;, &quot;sentiment&quot; and &quot;value&quot;. If NULL, internal data will be taken.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of values for the emotions and valence
detected in the input vector.
</p>

<hr>
<h2 id='get_percentage_values'>Chunk a Text and Get Means</h2><span id='topic+get_percentage_values'></span>

<h3>Description</h3>

<p>Chunks text into 100 Percentage based segments and calculates means.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_percentage_values(raw_values, bins = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_percentage_values_+3A_raw_values">raw_values</code></td>
<td>
<p>Raw sentiment values</p>
</td></tr>
<tr><td><code id="get_percentage_values_+3A_bins">bins</code></td>
<td>
<p>The number of bins to split the input vector.
Default is 100 bins.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of mean values from each chunk
</p>

<hr>
<h2 id='get_sent_values'>Assigns Sentiment Values</h2><span id='topic+get_sent_values'></span>

<h3>Description</h3>

<p>Assigns sentiment values to words based on preloaded dictionary. The default is the syuzhet dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_sent_values(char_v, method = "syuzhet", lexicon = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_sent_values_+3A_char_v">char_v</code></td>
<td>
<p>A string</p>
</td></tr>
<tr><td><code id="get_sent_values_+3A_method">method</code></td>
<td>
<p>A string indicating which sentiment dictionary to use</p>
</td></tr>
<tr><td><code id="get_sent_values_+3A_lexicon">lexicon</code></td>
<td>
<p>A data frame with with at least two columns named word and value. Works with &quot;nrc&quot; or &quot;custom&quot; method.  If using custom method, you must load a custom lexicon as a data frame with aforementioend columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single numerical value (positive or negative)
based on the assessed sentiment in the string
</p>

<hr>
<h2 id='get_sentences'>Sentence Tokenization</h2><span id='topic+get_sentences'></span>

<h3>Description</h3>

<p>Parses a string into a vector of sentences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_sentences(text_of_file, fix_curly_quotes = TRUE, as_vector = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_sentences_+3A_text_of_file">text_of_file</code></td>
<td>
<p>A Text String</p>
</td></tr>
<tr><td><code id="get_sentences_+3A_fix_curly_quotes">fix_curly_quotes</code></td>
<td>
<p>logical.  If <code>TRUE</code> curly quotes will be 
converted to ASCII representation before splitting.</p>
</td></tr>
<tr><td><code id="get_sentences_+3A_as_vector">as_vector</code></td>
<td>
<p>If <code>TRUE</code> the result is unlisted.  If <code>FALSE</code>
the result stays as a list of the original text string elements split into 
sentences.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A Character Vector of Sentences
</p>


<h3>Examples</h3>

<pre><code class='language-R'>(x &lt;- c(paste0(
    "Mr. Brown comes! He says hello. i give him coffee.  i will ",
    "go at 5 p. m. eastern time.  Or somewhere in between!go there"
),
paste0(
    "Marvin K. Mooney Will You Please Go Now!", "The time has come.",
    "The time has come. The time is now. Just go. Go. GO!",
    "I don't care how."
)))

get_sentences(x)
get_sentences(x, as_vector = FALSE)


</code></pre>

<hr>
<h2 id='get_sentiment'>Get Sentiment Values for a String</h2><span id='topic+get_sentiment'></span>

<h3>Description</h3>

<p>Iterates over a vector of strings and returns sentiment values based on user supplied method. The default method, &quot;syuzhet&quot; is a custom sentiment dictionary developed in the Nebraska Literary Lab.  The default dictionary should be better tuned to fiction as the terms were extracted from a collection of 165,000 human coded sentences taken from a small corpus of contemporary novels.   
At the time of this release, Syuzhet will only work with languages that use Latin character sets.  This effectively means that &quot;Arabic&quot;, &quot;Bengali&quot;, &quot;Chinese_simplified&quot;, &quot;Chinese_traditional&quot;, &quot;Greek&quot;, &quot;Gujarati&quot;, &quot;Hebrew&quot;, &quot;Hindi&quot;, &quot;Japanese&quot;, &quot;Marathi&quot;, &quot;Persian&quot;, &quot;Russian&quot;, &quot;Tamil&quot;, &quot;Telugu&quot;, &quot;Thai&quot;, &quot;Ukranian&quot;, &quot;Urdu&quot;, &quot;Yiddish&quot; are not supported even though these languages are part of the extended NRC dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_sentiment(
  char_v,
  method = "syuzhet",
  path_to_tagger = NULL,
  cl = NULL,
  language = "english",
  lexicon = NULL,
  regex = "[^A-Za-z']+",
  lowercase = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_sentiment_+3A_char_v">char_v</code></td>
<td>
<p>A vector of strings for evaluation.</p>
</td></tr>
<tr><td><code id="get_sentiment_+3A_method">method</code></td>
<td>
<p>A string indicating which sentiment method to use. Options include &quot;syuzhet&quot;, &quot;bing&quot;, &quot;afinn&quot;, &quot;nrc&quot; and &quot;stanford.&quot;  See references for more detail on methods.</p>
</td></tr>
<tr><td><code id="get_sentiment_+3A_path_to_tagger">path_to_tagger</code></td>
<td>
<p>local path to location of Stanford CoreNLP package</p>
</td></tr>
<tr><td><code id="get_sentiment_+3A_cl">cl</code></td>
<td>
<p>Optional, for parallel sentiment analysis.</p>
</td></tr>
<tr><td><code id="get_sentiment_+3A_language">language</code></td>
<td>
<p>A string. Only works for &quot;nrc&quot; method</p>
</td></tr>
<tr><td><code id="get_sentiment_+3A_lexicon">lexicon</code></td>
<td>
<p>a data frame with at least two columns labeled &quot;word&quot; and &quot;value.&quot;</p>
</td></tr>
<tr><td><code id="get_sentiment_+3A_regex">regex</code></td>
<td>
<p>A regular expression for splitting words.  Default is &quot;[^A-Za-z']+&quot;</p>
</td></tr>
<tr><td><code id="get_sentiment_+3A_lowercase">lowercase</code></td>
<td>
<p>should tokens be converted to lowercase. Default equals TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Return value is a numeric vector of sentiment values, one value for each input sentence.
</p>


<h3>References</h3>

<p>Bing Liu, Minqing Hu and Junsheng Cheng. &quot;Opinion Observer: Analyzing and Comparing Opinions on the Web.&quot; Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan.
</p>
<p>Minqing Hu and Bing Liu. &quot;Mining and Summarizing Customer Reviews.&quot; Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, Washington, USA.  See: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon
</p>
<p>Saif Mohammad and Peter Turney.  &quot;Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon.&quot; In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California.  See: http://saifmohammad.com/WebPages/lexicons.html
</p>
<p>Finn Årup Nielsen. &quot;A new ANEW: Evaluation of a word list for sentiment analysis in microblogs&quot;, Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts':Big things come in small packages 718 in CEUR Workshop Proceedings : 93-98. 2011 May. http://arxiv.org/abs/1103.2903. See: http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010
</p>
<p>Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60.  See: http://nlp.stanford.edu/software/corenlp.shtml
</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng and Christopher Potts.  &quot;Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank Conference on Empirical Methods in Natural Language Processing&quot; (EMNLP 2013).  See: http://nlp.stanford.edu/sentiment/
</p>

<hr>
<h2 id='get_sentiment_dictionary'>Sentiment Dictionaries</h2><span id='topic+get_sentiment_dictionary'></span>

<h3>Description</h3>

<p>Get the sentiment dictionaries used in <span class="pkg">syuzhet</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_sentiment_dictionary(dictionary = "syuzhet", language = "english")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_sentiment_dictionary_+3A_dictionary">dictionary</code></td>
<td>
<p>A string indicating which sentiment dictionary to return.  Options include &quot;syuzhet&quot;, &quot;bing&quot;, &quot;afinn&quot;, and &quot;nrc&quot;.</p>
</td></tr>
<tr><td><code id="get_sentiment_dictionary_+3A_language">language</code></td>
<td>
<p>A string indicating the language to choose if using the NRC dictionary and a language other than English</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="base.html#topic+data.frame">data.frame</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_sentiment_dictionary()
get_sentiment_dictionary('bing')
get_sentiment_dictionary('afinn')
get_sentiment_dictionary('nrc', language = "spanish")
</code></pre>

<hr>
<h2 id='get_stanford_sentiment'>Get Sentiment from the Stanford Tagger</h2><span id='topic+get_stanford_sentiment'></span>

<h3>Description</h3>

<p>Call the Stanford Sentiment tagger with a
vector of strings.  The Stanford tagger automatically
detects sentence boundaries and treats each sentence as a 
distinct instance to measure. As a result, the vector 
that gets returned will not be the same length as the
input vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_stanford_sentiment(text_vector, path_to_stanford_tagger)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_stanford_sentiment_+3A_text_vector">text_vector</code></td>
<td>
<p>A vector of strings</p>
</td></tr>
<tr><td><code id="get_stanford_sentiment_+3A_path_to_stanford_tagger">path_to_stanford_tagger</code></td>
<td>
<p>a local file path indicating 
where the coreNLP package is installed.</p>
</td></tr>
</table>

<hr>
<h2 id='get_text_as_string'>Load Text from a File</h2><span id='topic+get_text_as_string'></span>

<h3>Description</h3>

<p>Loads a file as a single text string.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_text_as_string(path_to_file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_text_as_string_+3A_path_to_file">path_to_file</code></td>
<td>
<p>file path</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector of length 1 containing the text of the file in the path_to_file argument.
</p>

<hr>
<h2 id='get_tokens'>Word Tokenization</h2><span id='topic+get_tokens'></span>

<h3>Description</h3>

<p>Parses a string into a vector of word tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_tokens(text_of_file, pattern = "\\W", lowercase = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_tokens_+3A_text_of_file">text_of_file</code></td>
<td>
<p>A Text String</p>
</td></tr>
<tr><td><code id="get_tokens_+3A_pattern">pattern</code></td>
<td>
<p>A regular expression for token breaking</p>
</td></tr>
<tr><td><code id="get_tokens_+3A_lowercase">lowercase</code></td>
<td>
<p>should tokens be converted to lowercase. Default equals TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A Character Vector of Words
</p>

<hr>
<h2 id='get_transformed_values'>Fourier Transform and Reverse Transform Values</h2><span id='topic+get_transformed_values'></span>

<h3>Description</h3>

<p>Please Note: This function is maintained for legacy purposes.  Users should consider using get_dct_transform() instead. Converts input values into a standardized set of filtered and reverse transformed values for easy plotting and/or comparison.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_transformed_values(
  raw_values,
  low_pass_size = 2,
  x_reverse_len = 100,
  padding_factor = 2,
  scale_vals = FALSE,
  scale_range = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_transformed_values_+3A_raw_values">raw_values</code></td>
<td>
<p>the raw sentiment values calculated for each sentence</p>
</td></tr>
<tr><td><code id="get_transformed_values_+3A_low_pass_size">low_pass_size</code></td>
<td>
<p>The number of components to retain in the low pass filtering. Default = 3</p>
</td></tr>
<tr><td><code id="get_transformed_values_+3A_x_reverse_len">x_reverse_len</code></td>
<td>
<p>the number of values to return. Default = 100</p>
</td></tr>
<tr><td><code id="get_transformed_values_+3A_padding_factor">padding_factor</code></td>
<td>
<p>the amount of zero values to pad raw_values with, as a factor of the size of raw_values. Default = 2.</p>
</td></tr>
<tr><td><code id="get_transformed_values_+3A_scale_vals">scale_vals</code></td>
<td>
<p>Logical determines whether or not to normalize the values using the scale function  Default = FALSE.  If TRUE, values will be scaled by subtracting the means and scaled by dividing by their standard deviations.  See ?scale</p>
</td></tr>
<tr><td><code id="get_transformed_values_+3A_scale_range">scale_range</code></td>
<td>
<p>Logical determines whether or not to scale the values from -1 to +1.  Default = FALSE.  If set to TRUE, the lowest value in the vector will be set to -1 and the highest values set to +1 and all the values scaled accordingly in between.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The transformed values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>s_v &lt;- get_sentences("I begin this story with a neutral statement. 
Now I add a statement about how much I despise cats. 
I am allergic to them. 
Basically this is a very silly test.")
raw_values &lt;- get_sentiment(s_v, method = "bing")
get_transformed_values(raw_values)
</code></pre>

<hr>
<h2 id='mixed_messages'>Mixed Messages</h2><span id='topic+mixed_messages'></span>

<h3>Description</h3>

<p>This function calculates the &quot;emotional entropy&quot; of a string based on the amount of conflicting valence. Emotional entropy is a measure of unpredictability and surprise based on the consistency or inconsistency of the emotional language in a given string. A string with conflicting emotional language may be said to express a &quot;mixed message.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixed_messages(string, remove_neutral = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixed_messages_+3A_string">string</code></td>
<td>
<p>A string of words</p>
</td></tr>
<tr><td><code id="mixed_messages_+3A_remove_neutral">remove_neutral</code></td>
<td>
<p>Logical indicating whether or not to remove words with neutral valence before computing the emotional entropy of the string.  Default is TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="base.html#topic+vector">vector</a></code> containing two named values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text_v &lt;- "That's the love and the hate of it" 
mixed_messages(text_v) # [1] 1.0 0.5 = high (1.0, 0.5) entropy
mixed_messages(text_v, TRUE)
# Example of a predictable message i.e. no surprise
text_v &lt;- "I absolutley love, love, love it." 
mixed_messages(text_v) # [1] 0 0 = low entropy e.g. totally consistent emotion, i.e. no surprise
mixed_messages(text_v, FALSE)
# A more realistic example with a lot of mixed emotion.
text_v &lt;- "I loved the way he looked at me but I hated that he was no longer my lover"
mixed_messages(text_v) # [1] 0.91829583 0.05101644 pretty high entropy.
mixed_messages(text_v, FALSE)
# A more realistic example without a lot of mixed emotion.
text_v &lt;- "I loved the way he looked at me and I was happy that he was my lover."
mixed_messages(text_v) # [1] 0 0 low entropy, no surprise.
mixed_messages(text_v, FALSE)
# An urealistic example with a lot of mixed emotion.
text_v &lt;- "I loved, hated and despised the way he looked at me and 
I was happy as hell that he was my white hot lover."
mixed_messages(text_v)
mixed_messages(text_v, FALSE)
</code></pre>

<hr>
<h2 id='rescale'>Vector Value Rescaling</h2><span id='topic+rescale'></span>

<h3>Description</h3>

<p>Rescale Transformed values from -1 to 1
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rescale(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rescale_+3A_x">x</code></td>
<td>
<p>A vector of values</p>
</td></tr>
</table>

<hr>
<h2 id='rescale_x_2'>Bi-Directional x and y axis Rescaling</h2><span id='topic+rescale_x_2'></span>

<h3>Description</h3>

<p>Rescales input values to two scales (0 to 1 and  -1 to 1) on the y-axis and also creates a scaled vector of x axis values from 0 to 1.  This function is useful for plotting and plot comparison.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rescale_x_2(v)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rescale_x_2_+3A_v">v</code></td>
<td>
<p>A vector of values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of three vectors (x, y, z).  x is a vector of values from 0 to 1 equal in length to the input vector v. y is a scaled (from 0 to 1) vector of the input values equal in length to the input vector v. z is a scaled (from -1 to +1) vector of the input values equal in length to the input vector v.
</p>

<hr>
<h2 id='simple_plot'>Plots simple and rolling shapes overlayed</h2><span id='topic+simple_plot'></span>

<h3>Description</h3>

<p>A simple function for comparing three smoothers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simple_plot(
  raw_values,
  title = "Syuzhet Plot",
  legend_pos = "top",
  lps = 10,
  window = 0.1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simple_plot_+3A_raw_values">raw_values</code></td>
<td>
<p>the raw sentiment values
calculated for each sentence</p>
</td></tr>
<tr><td><code id="simple_plot_+3A_title">title</code></td>
<td>
<p>for resulting image</p>
</td></tr>
<tr><td><code id="simple_plot_+3A_legend_pos">legend_pos</code></td>
<td>
<p>position for legend</p>
</td></tr>
<tr><td><code id="simple_plot_+3A_lps">lps</code></td>
<td>
<p>size of the low pass filter. I.e. the number of low frequency components to retain</p>
</td></tr>
<tr><td><code id="simple_plot_+3A_window">window</code></td>
<td>
<p>size of the rolling window for the rolling mean expressed as a percentage.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
