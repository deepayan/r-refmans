<!DOCTYPE html><html><head><title>Help for package drclust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {drclust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#apseudoF'><p>pseudoF (pF or Calinski-Harabsz) index for choosing k in partitioning models</p></a></li>
<li><a href='#centree'><p>Ward-dendrogeam of centroids of partitioning models</p></a></li>
<li><a href='#cluster'><p>classification variable</p></a></li>
<li><a href='#CronbachAlpha'><p>Cronbach Alpha</p></a></li>
<li><a href='#disfa'><p>Disjoint Factor Analysis</p></a></li>
<li><a href='#dispca'><p>Disjoint Principal Components Analysis</p></a></li>
<li><a href='#doublekm'><p>Double k-means Clustering</p></a></li>
<li><a href='#dpcakm'><p>Clustering with Disjoint Principal Components Analysis</p></a></li>
<li><a href='#dpseudoF'><p>double pseudoF (Calinski-Harabsz) index</p></a></li>
<li><a href='#factkm'><p>Factorial k-means</p></a></li>
<li><a href='#heatm'><p>Heatmap of a partition in a reduced subspace</p></a></li>
<li><a href='#kaiserCrit'><p>Selecting the number of principal components to be extracted from a dataset</p></a></li>
<li><a href='#mrand'><p>Adjusted Rand Index</p></a></li>
<li><a href='#redkm'><p>k-means on a reduced subspace</p></a></li>
<li><a href='#silhouette'><p>Silhouette</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Simultaneous Clustering and (or) Dimensionality Reduction</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-04-19</td>
</tr>
<tr>
<td>Description:</td>
<td>Methods for simultaneous clustering and dimensionality reduction such as: Double k-means, Reduced k-means, Factorial k-means, Clustering with Disjoint PCA but also methods for exclusively dimensionality reduction: Disjoint PCA, Disjoint FA. The statistical methods implemented refer to the following articles: de Soete G., Carroll J. (1994) "K-means clustering in a low-dimensional Euclidean space" &lt;<a href="https://doi.org/10.1007%2F978-3-642-51175-2_24">doi:10.1007/978-3-642-51175-2_24</a>&gt; ; Vichi M. (2001) "Double k-means Clustering for Simultaneous Classification of Objects and Variables" &lt;<a href="https://doi.org/10.1007%2F978-3-642-59471-7_6">doi:10.1007/978-3-642-59471-7_6</a>&gt; ; Vichi M., Kiers H.A.L. (2001) "Factorial k-means analysis for two-way data" &lt;<a href="https://doi.org/10.1016%2FS0167-9473%2800%2900064-5">doi:10.1016/S0167-9473(00)00064-5</a>&gt; ; Vichi M., Saporta G. (2009) "Clustering and disjoint principal component analysis" &lt;<a href="https://doi.org/10.1016%2Fj.csda.2008.05.028">doi:10.1016/j.csda.2008.05.028</a>&gt; ; Vichi M. (2017) "Disjoint factor analysis with cross-loadings" &lt;<a href="https://doi.org/10.1007%2Fs11634-016-0263-9">doi:10.1007/s11634-016-0263-9</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, RcppArmadillo, fpc, cluster, factoextra, pheatmap</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-19 15:48:27 UTC; Ionel</td>
</tr>
<tr>
<td>Author:</td>
<td>Ionel Prunila [aut, cre],
  Maurizio Vichi [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ionel Prunila &lt;ionel.prunila@uniroma1.it&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-22 18:22:47 UTC</td>
</tr>
</table>
<hr>
<h2 id='apseudoF'>pseudoF (pF or Calinski-Harabsz) index for choosing k in partitioning models</h2><span id='topic+apseudoF'></span>

<h3>Description</h3>

<p>Calculates and plots the CH index for k = 2, ..., maxK. The function provides an interval wide (2tol*pF) so that the choice of K is less conservative. Instead of just choosing the maximum pF, if it exists, picks the value such that its upper bound is larger than max pF.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>apseudoF(data, maxK, tol, model, Q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="apseudoF_+3A_data">data</code></td>
<td>
<p>Units x variables numeric data matrix.</p>
</td></tr>
<tr><td><code id="apseudoF_+3A_maxk">maxK</code></td>
<td>
<p>Maximum number of clusters for the units to be tested.</p>
</td></tr>
<tr><td><code id="apseudoF_+3A_tol">tol</code></td>
<td>
<p>Approximation value. It is half of the length of theinterval put for each pF. 0 &lt;= tol &lt; 1. Its default value is 0.05.</p>
</td></tr>
<tr><td><code id="apseudoF_+3A_model">model</code></td>
<td>
<p>Partitioning Models to run for each value of k. (1 = doublekm; 2 = redkm; 3 = factkm; 4 = dpcakm)</p>
</td></tr>
<tr><td><code id="apseudoF_+3A_q">Q</code></td>
<td>
<p>Number of principal components w.r.t. variables selected for the maxK -1 partitions to be tested.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>bestK</code></td>
<td>
<p>best value of K (scalar).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Calinski T., Harabasz J. (1974) &quot;A dendrite method for cluster analysis&quot; &lt;doi:10.1080/03610927408827101&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

apF &lt;- apseudoF(iris, maxK=10, tol = 0.05, model = 3, Q = 2)

</code></pre>

<hr>
<h2 id='centree'>Ward-dendrogeam of centroids of partitioning models</h2><span id='topic+centree'></span>

<h3>Description</h3>

<p>Plots the Ward-dendrogram of the centroids of a partitioning model. The plot is useful as a diagnosis tool for the choice o the number of clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>centree(drclust_out)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="centree_+3A_drclust_out">drclust_out</code></td>
<td>
<p>Output of either doublekm, redkm, factkm or dpcakm.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>centroids-dkm</code></td>
<td>
<p>Centroids x centroids distance matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Ward J. H. (1963) &quot;Hierarchical Grouping to Optimize an Objective Function&quot; &lt;doi:10.1080/01621459.1963.10500845&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

dc_out &lt;- dpcakm(iris, 20, 3)
d &lt;- centree(dc_out)

</code></pre>

<hr>
<h2 id='cluster'>classification variable</h2><span id='topic+cluster'></span>

<h3>Description</h3>

<p>Recodes the binary and row-stochastic membership matrix U into the classification variable (similar to the &quot;cluster&quot; output returned by kmeans()).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster(U)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_+3A_u">U</code></td>
<td>
<p>Binary and row-stochastic matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>cl</code></td>
<td>
<p>vector of length n indicating, for each element, the index of the cluster to which it has been assigned.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

# standardizing the data
iris &lt;- scale(iris)

# double k-means with 3 unit-clusters and 2 components for the variables
p1 &lt;- redkm(iris, K = 3, Q = 2)
cl &lt;- cluster(p1$U)

</code></pre>

<hr>
<h2 id='CronbachAlpha'>Cronbach Alpha</h2><span id='topic+CronbachAlpha'></span>

<h3>Description</h3>

<p>Computes the Cronbach Alpha index on a units x variables data matrix. It measures the internal reliability, i.e., the propensity of J variables of a data matrix (n units x J variables) to be concordantly correlated with a single factor (composite indicator).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CronbachAlpha(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CronbachAlpha_+3A_x">X</code></td>
<td>
<p>Units x variables numeric data matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>as</code></td>
<td>
<p>Cronbach's Alpha</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Cronbach L. J. (1951) &quot;Coefficient alpha and the internal structure of tests&quot; &lt;doi:10.1007/BF02310555&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

# standardizing the data
iris &lt;- scale(iris)

# compute Cronbach's Alpha
as &lt;- CronbachAlpha(iris)
</code></pre>

<hr>
<h2 id='disfa'>Disjoint Factor Analysis</h2><span id='topic+disfa'></span>

<h3>Description</h3>

<p>Performs disjoint factor analysis, i.e., a Factor Analysis with a simple structure. In fact, each factor is defined by a disjoint subset of variables, resulting thus, in a simplified, easier to interpret loading matrix A and factors. Estimation is carried out via Maximum Likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>disfa(X, Q, Rndstart, verbose, maxiter, tol, constr, prep, print)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="disfa_+3A_x">X</code></td>
<td>
<p>Units x variables numeric data matrix.</p>
</td></tr>
<tr><td><code id="disfa_+3A_q">Q</code></td>
<td>
<p>Number of factors.</p>
</td></tr>
<tr><td><code id="disfa_+3A_rndstart">Rndstart</code></td>
<td>
<p>Number of runs to be performed (Defaults is 20).</p>
</td></tr>
<tr><td><code id="disfa_+3A_verbose">verbose</code></td>
<td>
<p>Outputs basic summary statistics for each run (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
<tr><td><code id="disfa_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations allowed (if convergence is not yet reached. Default is 100).</p>
</td></tr>
<tr><td><code id="disfa_+3A_tol">tol</code></td>
<td>
<p>Tolerance threshold (maximum difference between the values of the objective function of two consecutive iterations such that convergence is assumed. Default is 1e-6).</p>
</td></tr>
<tr><td><code id="disfa_+3A_constr">constr</code></td>
<td>
<p>is a vector of length J = nr. of variables, pre-specifying to which cluster some of the variables must be assigned. Each component of the vector can assume integer values from 1 o Q (See example for more details), or 0 if no constraint on the variable is imposed (i.e., it will be assigned based on the plain algorithm).</p>
</td></tr>
<tr><td><code id="disfa_+3A_prep">prep</code></td>
<td>
<p>Pre-processing of the data. 1 performs the z-score transform (default choice); 2 performs the min-max transform; 0 leaves the data un-pre-processed.</p>
</td></tr>
<tr><td><code id="disfa_+3A_print">print</code></td>
<td>
<p>Prints summary statistics of the performed method (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list of estimates and some descriptive quantities of the final results.
</p>
<table>
<tr><td><code>V</code></td>
<td>
<p>Variables x factors membership matrix (binary and row-stochastic). Each row is a dummy variable indicating to which cluster each variable has been assigned.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>Variables x components loading matrix.</p>
</td></tr>
<tr><td><code>Psi</code></td>
<td>
<p>Specific variance of each observed variable, not accounted for by the common factors (matrix).</p>
</td></tr>
<tr><td><code>discrepancy</code></td>
<td>
<p>Value of the objective function, to be minimized. Difference between the observed and estimated covariance matrices (scalar).</p>
</td></tr>
<tr><td><code>RMSEA</code></td>
<td>
<p>Adjusted Root Mean Squared Error (scalar).</p>
</td></tr>
<tr><td><code>AIC</code></td>
<td>
<p>Aikake Information Criterion (scalar).</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>Bayesian Information Criterion (scalar).</p>
</td></tr>
<tr><td><code>GFI</code></td>
<td>
<p>Goodness of Fit Index (scalar).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Vichi M. (2017) &quot;Disjoint factor analysis with cross-loadings&quot; &lt;doi:10.1007/s11634-016-0263-9&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

# No constraint on variables
out &lt;- disfa(iris, Q = 2)

# Constraint: the first two variables must contribute to the same factor.
outc &lt;- disfa(iris, Q = 2, constr = c(1,1,0,0))

</code></pre>

<hr>
<h2 id='dispca'>Disjoint Principal Components Analysis</h2><span id='topic+dispca'></span>

<h3>Description</h3>

<p>Performs disjoint PCA, that is, a simplified version of PCA. Computes each one of the Q principal components from a different subset of the J variables (resulting thus, in a simplified, easier to interpret loading matrix A).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dispca(X, Q, Rndstart, verbose, maxiter, tol, prep, print, constr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dispca_+3A_x">X</code></td>
<td>
<p>Units x variables numeric data matrix.</p>
</td></tr>
<tr><td><code id="dispca_+3A_q">Q</code></td>
<td>
<p>Number of factors.</p>
</td></tr>
<tr><td><code id="dispca_+3A_rndstart">Rndstart</code></td>
<td>
<p>Number of runs to be performed (Defaults is 20).</p>
</td></tr>
<tr><td><code id="dispca_+3A_verbose">verbose</code></td>
<td>
<p>Outputs basic summary statistics for each run (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
<tr><td><code id="dispca_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations allowed (if convergence is not yet reached. Default is 100).</p>
</td></tr>
<tr><td><code id="dispca_+3A_tol">tol</code></td>
<td>
<p>Tolerance threshold (maximum difference between the values of the objective function of two consecutive iterations such that convergence is assumed). Default is 1e-6.</p>
</td></tr>
<tr><td><code id="dispca_+3A_prep">prep</code></td>
<td>
<p>Pre-processing of the data. 1 performs the z-score transform (default choice); 2 performs the min-max transform; 0 leaves the data un-pre-processed.</p>
</td></tr>
<tr><td><code id="dispca_+3A_print">print</code></td>
<td>
<p>Prints summary statistics of the results (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
<tr><td><code id="dispca_+3A_constr">constr</code></td>
<td>
<p>is a vector of length J = nr. of variables, pre-specifying to which cluster some of the variables must be assigned. Each component of the vector can assume integer values from 1 o Q (See example for more details), or 0 if no constraint on the variable is imposed (i.e., it will be assigned based on the plain algorithm).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list of estimates and some descriptive quantities of the final results.
</p>
<table>
<tr><td><code>V</code></td>
<td>
<p>Variables x factors membership matrix (binary and row-stochastic). Each row is a dummy variable indicating to which cluster it has been assigned.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>Variables x components loading matrix.</p>
</td></tr>
<tr><td><code>betweenss</code></td>
<td>
<p>Amount of deviance captured by the model (scalar).</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>
<p>total amount of deviance (scalar).</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>Number of variables assigned to each column-cluster (vector).</p>
</td></tr>
<tr><td><code>loop</code></td>
<td>
<p>The index of the (best) run from which the results have been chosen.</p>
</td></tr>
<tr><td><code>it</code></td>
<td>
<p>the number of iterations performed during the (best) run.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Vichi M., Saporta G. (2009) &quot;Clustering and disjoint principal component analysis&quot; &lt;doi:10.1016/j.csda.2008.05.028&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

# No constraint on variables
out &lt;- dispca(iris, Q = 2)

# Constraint: the first two variables must contribute to the same factor.
outc &lt;- dispca(iris, Q = 2, constr = c(1,1,0,0))
</code></pre>

<hr>
<h2 id='doublekm'>Double k-means Clustering</h2><span id='topic+doublekm'></span>

<h3>Description</h3>

<p>Performs simultaneous <em>k</em>-means partitioning on units and variables (rows and columns of the data matrix).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>doublekm(Xs, K, Q, Rndstart, verbose, maxiter, tol, prep, print)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="doublekm_+3A_xs">Xs</code></td>
<td>
<p>Units x variables numeric data matrix.</p>
</td></tr>
<tr><td><code id="doublekm_+3A_k">K</code></td>
<td>
<p>Number of clusters for the units.</p>
</td></tr>
<tr><td><code id="doublekm_+3A_q">Q</code></td>
<td>
<p>Number of clusters for the variables.</p>
</td></tr>
<tr><td><code id="doublekm_+3A_rndstart">Rndstart</code></td>
<td>
<p>Number of runs to be performed (Defaults is 20).</p>
</td></tr>
<tr><td><code id="doublekm_+3A_verbose">verbose</code></td>
<td>
<p>Outputs basic summary statistics for each run (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
<tr><td><code id="doublekm_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations allowed (if convergence is not yet reached. Default is 100).</p>
</td></tr>
<tr><td><code id="doublekm_+3A_tol">tol</code></td>
<td>
<p>Tolerance threshold. It is the maximum difference between the values of the objective function of two consecutive iterations such that convergence is assumed (default is 1e-6).</p>
</td></tr>
<tr><td><code id="doublekm_+3A_prep">prep</code></td>
<td>
<p>Pre-processing of the data. 1 performs the z-score transform (default choice); 2 performs the min-max transform; 0 leaves the data un-pre-processed.</p>
</td></tr>
<tr><td><code id="doublekm_+3A_print">print</code></td>
<td>
<p>Prints summary statistics of the results (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list of estimates and some descriptive quantities of the final results.
</p>
<table>
<tr><td><code>U</code></td>
<td>
<p>Units x clusters membership matrix (binary and row-stochastic). Each row is a dummy variable indicating to which unit-cluster each unit has been assigned.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>Variables x clusters membership matrix (binary and row-stochastic). Each row is a dummy variable indicating to which variable-cluster each variable has been assigned.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>K x Q matrix of centers containing the row means expressed in terms of column means.</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>
<p>The total sum of squares (scalar).</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>Vector of within-row-cluster sum of squares, one component per cluster.</p>
</td></tr>
<tr><td><code>columnwise_withinss</code></td>
<td>
<p>Vector of within-column-cluster sum of squares, one component per cluster.</p>
</td></tr>
<tr><td><code>betweenss</code></td>
<td>
<p>Amount of deviance captured by the model (scalar).</p>
</td></tr>
<tr><td><code>K-size</code></td>
<td>
<p>Number of units assigned to each row-cluster (vector).</p>
</td></tr>
<tr><td><code>Q-size</code></td>
<td>
<p>Number of variables assigned to each column-cluster (vector).</p>
</td></tr>
<tr><td><code>pseudoF</code></td>
<td>
<p>Calinski-Harabasz index of the resulting (row-) partition (scalar).</p>
</td></tr>
<tr><td><code>loop</code></td>
<td>
<p>The index of the (best) run from which the results have been chosen.</p>
</td></tr>
<tr><td><code>it</code></td>
<td>
<p>the number of iterations performed during the (best) run.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Vichi M. (2001) &quot;Double k-means Clustering for Simultaneous Classification of Objects and Variables&quot; &lt;doi:10.1007/978-3-642-59471-7_6&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

# double k-means with 3 unit-clusters and 2 variable-clusters
out &lt;- doublekm(iris, K = 3, Q = 2)

</code></pre>

<hr>
<h2 id='dpcakm'>Clustering with Disjoint Principal Components Analysis</h2><span id='topic+dpcakm'></span>

<h3>Description</h3>

<p>Performs simultaneously k-means partitioning on units and disjoint PCA on the variables, computing each principal component from a different subset of variables. The result is a simplified, easier to interpret loading matrix A, 
the principal components and the clustering. The reduced subspace is identified by the centroids.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dpcakm(X, K, Q, Rndstart, verbose, maxiter, tol, constr, print, prep)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dpcakm_+3A_x">X</code></td>
<td>
<p>Units x variables numeric data matrix.</p>
</td></tr>
<tr><td><code id="dpcakm_+3A_k">K</code></td>
<td>
<p>Number of clusters for the units.</p>
</td></tr>
<tr><td><code id="dpcakm_+3A_q">Q</code></td>
<td>
<p>Number of principal components.</p>
</td></tr>
<tr><td><code id="dpcakm_+3A_rndstart">Rndstart</code></td>
<td>
<p>Number of runs to be performed (Defaults is 20).</p>
</td></tr>
<tr><td><code id="dpcakm_+3A_verbose">verbose</code></td>
<td>
<p>Outputs basic summary statistics for each run (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
<tr><td><code id="dpcakm_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations allowed (if convergence is not yet reached. Default is 100).</p>
</td></tr>
<tr><td><code id="dpcakm_+3A_tol">tol</code></td>
<td>
<p>Tolerance threshold (maximum difference between the values of the objective function of two consecutive iterations such that convergence is assumed. Default is 1e-6).</p>
</td></tr>
<tr><td><code id="dpcakm_+3A_constr">constr</code></td>
<td>
<p>is a vector of length J = nr. of variables, pre-specifying to which cluster some of the variables must be assigned. Each component of the vector can assume integer values from 1 o Q = nr. of variable-cluster / principal components (See examples for more details), or 0 if no constraint on the variable is imposed (i.e., it will be assigned based on the plain algorithm).</p>
</td></tr>
<tr><td><code id="dpcakm_+3A_print">print</code></td>
<td>
<p>Prints summary statistics of the results (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
<tr><td><code id="dpcakm_+3A_prep">prep</code></td>
<td>
<p>Pre-processing of the data. 1 performs the z-score transform (default choice); 2 performs the min-max transform; 0 leaves the data un-pre-processed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list of estimates and some descriptive quantities of the final results.
</p>
<table>
<tr><td><code>V</code></td>
<td>
<p>Variables x factors membership matrix (binary and row-stochastic). Each row is a dummy variable indicating to which cluster each variable has been assigned.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>Units x clusters membership matrix (binary and row-stochastic). Each row is a dummy variable indicating to which cluster each unit has been assigned.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>Variables x components loading matrix.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>K x Q matrix of centers containing the row means expressed in the reduced space of Q principal components.</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>
<p>The total sum of squares (scalar).</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>Vector of within-cluster sum of squares, one component per cluster.</p>
</td></tr>
<tr><td><code>betweenss</code></td>
<td>
<p>Amount of deviance captured by the model (scalar).</p>
</td></tr>
<tr><td><code>K-size</code></td>
<td>
<p>Number of units assigned to each row-cluster (vector).</p>
</td></tr>
<tr><td><code>Q-size</code></td>
<td>
<p>Number of variables assigned to each column-cluster (vector).</p>
</td></tr>
<tr><td><code>pseudoF</code></td>
<td>
<p>Calinski-Harabasz index of the resulting partition (scalar).</p>
</td></tr>
<tr><td><code>loop</code></td>
<td>
<p>The index of the (best) run from which the results have been chosen.</p>
</td></tr>
<tr><td><code>it</code></td>
<td>
<p>the number of iterations performed during the (best) run.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Vichi M., Saporta G. (2009) &quot;Clustering and disjoint principal component analysis&quot; &lt;doi:10.1016/j.csda.2008.05.028&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

# No constraint on variables
out &lt;- dpcakm(iris, K = 3, Q = 2, Rndstart = 5)

# Constraint: the first two variables must contribute to the same factor.
outc &lt;- dpcakm(iris, K = 3, Q = 2, Rndstart = 5,constr = c(1,1,0,0))
</code></pre>

<hr>
<h2 id='dpseudoF'>double pseudoF (Calinski-Harabsz) index</h2><span id='topic+dpseudoF'></span>

<h3>Description</h3>

<p>A pseudoF version for double partitioning, for the choice of the number of clusters of the units and variables (rows and columns of the data matrix). It is a diagnostic tool for inspecting simultaneously the optimal number of unit-clusters and variable-clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dpseudoF(data, maxK, maxQ)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dpseudoF_+3A_data">data</code></td>
<td>
<p>Units x variables numeric data matrix.</p>
</td></tr>
<tr><td><code id="dpseudoF_+3A_maxk">maxK</code></td>
<td>
<p>Maximum number of clusters for the units to be tested.</p>
</td></tr>
<tr><td><code id="dpseudoF_+3A_maxq">maxQ</code></td>
<td>
<p>Maximum number of clusters for the variables to be tested.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>dpseudoF</code></td>
<td>
<p>matrix containing the pF value for each pair of K and Q within the specified range</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>R. Rocci, M. Vichi (2008)&quot; Two-mode multi-partitioning&quot; &lt;doi:10.1016/j.csda.2007.06.025&gt;
</p>
<p>T. Calinski &amp; J. Harabasz (1974). A dendrite method for cluster analysis. Communications in Statistics, 3:1, 1-27
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

dpeudoF &lt;- dpseudoF(iris, maxK=10, maxQ = 3)

</code></pre>

<hr>
<h2 id='factkm'>Factorial k-means</h2><span id='topic+factkm'></span>

<h3>Description</h3>

<p>Performs simultaneously k-means partitioning on units and principal component analysis on the variables. 
Identifies the best partition in a Least-Squares sense in the best reduced space of the data. Both the data 
and the centroids are used to identify the best Least-Squares reduced subspace, where also their distances is measured.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factkm(X, K, Q, Rndstart, verbose, maxiter, tol, rot, prep, print)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factkm_+3A_x">X</code></td>
<td>
<p>Units x variables numeric data matrix.</p>
</td></tr>
<tr><td><code id="factkm_+3A_k">K</code></td>
<td>
<p>Number of clusters for the units.</p>
</td></tr>
<tr><td><code id="factkm_+3A_q">Q</code></td>
<td>
<p>Number of principal components w.r.t. variables.</p>
</td></tr>
<tr><td><code id="factkm_+3A_rndstart">Rndstart</code></td>
<td>
<p>Number of runs to be performed (Defaults is 20).</p>
</td></tr>
<tr><td><code id="factkm_+3A_verbose">verbose</code></td>
<td>
<p>Outputs basic summary statistics for each run (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
<tr><td><code id="factkm_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations allowed (if convergence is not yet reached. Default is 100).</p>
</td></tr>
<tr><td><code id="factkm_+3A_tol">tol</code></td>
<td>
<p>Tolerance threshold (maximum difference in the values of the objective function of two consecutive iterations such that convergence is assumed. Default is 1e-6).</p>
</td></tr>
<tr><td><code id="factkm_+3A_rot">rot</code></td>
<td>
<p>performs varimax rotation of axes obtained via PCA. (=1 enabled; =0 disabled, default option)</p>
</td></tr>
<tr><td><code id="factkm_+3A_prep">prep</code></td>
<td>
<p>Pre-processing of the data. 1 performs the z-score transform (default choice); 2 performs the min-max transform; 0 leaves the data un-pre-processed.</p>
</td></tr>
<tr><td><code id="factkm_+3A_print">print</code></td>
<td>
<p>Prints summary statistics of the results (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list of estimates and some descriptive quantities of the final results.
</p>
<table>
<tr><td><code>U</code></td>
<td>
<p>Units x clusters membership matrix (binary and row-stochastic). Each row is a dummy variable indicating to which cluster each unit has been assigned.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>Variables x components loading matrix (orthonormal).</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>K x Q matrix of centers containing the row means expressed in the reduced space of Q principal components.</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>
<p>The total sum of squares.</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>Vector of within-cluster sum of squares, one component per cluster.</p>
</td></tr>
<tr><td><code>betweenss</code></td>
<td>
<p>amount of deviance captured by the model.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>Number of units assigned to each cluster.</p>
</td></tr>
<tr><td><code>pseudoF</code></td>
<td>
<p>Calinski-Harabasz index of the resulting partition.</p>
</td></tr>
<tr><td><code>loop</code></td>
<td>
<p>The index of the (best) run from which the results have been chosen.</p>
</td></tr>
<tr><td><code>it</code></td>
<td>
<p>the number of iterations performed during the (best) run.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Vichi M., Kiers H.A.L. (2001) &quot;Factorial k-means analysis for two-way data&quot; &lt;doi:10.1016/S0167-9473(00)00064-5&gt;
</p>
<p>Kaiser H.F. (1958) &quot;The varimax criterion for analytic rotation in factor analysis&quot; &lt;doi:10.1007/BF02289233&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

# factorial k-means with 3 unit-clusters and 2 components for the variables
out &lt;- factkm(iris, K = 3, Q = 2, Rndstart = 15, verbose = 0, maxiter = 100, tol = 1e-7, rot = 1)

</code></pre>

<hr>
<h2 id='heatm'>Heatmap of a partition in a reduced subspace</h2><span id='topic+heatm'></span>

<h3>Description</h3>

<p>Plots the heatmap of a partition on a reduced subspace obtained via either: doublekm, redkm, factkm or dpcakm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heatm(data, drclust_out)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="heatm_+3A_data">data</code></td>
<td>
<p>Units x variables data matrix.</p>
</td></tr>
<tr><td><code id="heatm_+3A_drclust_out">drclust_out</code></td>
<td>
<p>Out of either doublekm, redkm, factkm or dpcakm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Kolde R. (2019) &quot;pheatmap: Pretty Heatmaps&quot; &lt;https://cran.r-project.org/web/packages/pheatmap/index.html&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

# standardizing the data
iris &lt;- scale(iris)

# applying a clustering algorithm
drclust_out &lt;- dpcakm(iris, 20, 3)

# obtain a heatmap based on the output of the clustering algorithm and the data
h &lt;- heatm(iris, drclust_out)

</code></pre>

<hr>
<h2 id='kaiserCrit'>Selecting the number of principal components to be extracted from a dataset</h2><span id='topic+kaiserCrit'></span>

<h3>Description</h3>

<p>Selects the optimal number of principal components to be extracted from a dataset based on Kaiser's criterion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kaiserCrit(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kaiserCrit_+3A_data">data</code></td>
<td>
<p>Units x variables data matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>bestQ</code></td>
<td>
<p>Number of components to be extracted (scalar).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Kaiser H. F. (1960) &quot;The Application of Electronic Computers to Factor Analysis&quot; &lt;doi:10.1177/001316446002000&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- scale(as.matrix(iris[,-5])) 

# Apply the Kaiser rule
h &lt;- kaiserCrit(iris)

</code></pre>

<hr>
<h2 id='mrand'>Adjusted Rand Index</h2><span id='topic+mrand'></span>

<h3>Description</h3>

<p>Performs the Adjusted Rand Index on a confusion matrix (row-by-column product of two partition-matrices). ARI is a measure of the similarity between two data clusterings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mrand(N)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mrand_+3A_n">N</code></td>
<td>
<p>Confusion matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>mri</code></td>
<td>
<p>Adjusted Rand Index of a confusion matrix (scalar).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Rand W. M. (1971) &quot;Objective criteria for the evaluation of clustering methods&quot; &lt;doi:10.2307/2284239&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

# standardizing the data
iris &lt;- scale(iris)

# double k-means with 3 unit-clusters and 2 components for the variables
p1 &lt;- redkm(iris, K = 3, Q = 2, Rndstart = 10)
p2 &lt;- doublekm(iris, K=3, Q=2, Rndstart = 10)
mri &lt;- mrand(t(p1$U)%*%p2$U)
</code></pre>

<hr>
<h2 id='redkm'>k-means on a reduced subspace</h2><span id='topic+redkm'></span>

<h3>Description</h3>

<p>Performs simultaneously k-means partitioning on units and principal component analysis on the variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>redkm(X, K, Q, Rndstart, verbose, maxiter, tol, rot, prep, print)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="redkm_+3A_x">X</code></td>
<td>
<p>Units x variables numeric data matrix.</p>
</td></tr>
<tr><td><code id="redkm_+3A_k">K</code></td>
<td>
<p>Number of clusters for the units.</p>
</td></tr>
<tr><td><code id="redkm_+3A_q">Q</code></td>
<td>
<p>Number of principal components w.r.t. variables.</p>
</td></tr>
<tr><td><code id="redkm_+3A_rndstart">Rndstart</code></td>
<td>
<p>Number of runs to be performed (Defaults is 20).</p>
</td></tr>
<tr><td><code id="redkm_+3A_verbose">verbose</code></td>
<td>
<p>Outputs basic summary statistics for each run (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
<tr><td><code id="redkm_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations allowed (if convergence is not yet reached. Default is 100).</p>
</td></tr>
<tr><td><code id="redkm_+3A_tol">tol</code></td>
<td>
<p>Tolerance threshold (maximum difference between the values of the objective function of two consecutive iterations such that convergence is assumed. Default is 1e-6).</p>
</td></tr>
<tr><td><code id="redkm_+3A_rot">rot</code></td>
<td>
<p>performs varimax rotation of axes obtained via PCA. (=1 enabled; =0 disabled, default option)</p>
</td></tr>
<tr><td><code id="redkm_+3A_prep">prep</code></td>
<td>
<p>Pre-processing of the data. 1 performs the z-score transform (default choice); 2 performs the min-max transform; 0 leaves the data un-pre-processed.</p>
</td></tr>
<tr><td><code id="redkm_+3A_print">print</code></td>
<td>
<p>Tolerancestats summary statistics of the performed method (1 = enabled; 0 = disabled, default option).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list of estimates and some descriptive quantities of the final results.
</p>
<table>
<tr><td><code>U</code></td>
<td>
<p>Units x clusters membership matrix (binary and row-stochastic). Each row is a dummy variable indicating to which cluster each unit has been assigned.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>Variables x components loading matrix (orthonormal).</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>K x Q matrix of centers containing the row means expressed in the reduced space of Q principal components.</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>
<p>The total sum of squares (scalar).</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>Vector of within-cluster sum of squares, one component per cluster.</p>
</td></tr>
<tr><td><code>betweenss</code></td>
<td>
<p>Amount of deviance captured by the model (scalar).</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>Number of units assigned to each cluster (vector).</p>
</td></tr>
<tr><td><code>pseudoF</code></td>
<td>
<p>Calinski-Harabasz index of the resulting partition (scalar).</p>
</td></tr>
<tr><td><code>loop</code></td>
<td>
<p>The index of the (best) run from which the results have been chosen.</p>
</td></tr>
<tr><td><code>it</code></td>
<td>
<p>the number of iterations performed during the (best) run.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>de Soete G., Carroll J. (1994) &quot;K-means clustering in a low-dimensional Euclidean space&quot; &lt;doi:10.1007/978-3-642-51175-2_24&gt;
</p>
<p>Kaiser H.F. (1958) &quot;The varimax criterion for analytic rotation in factor analysis&quot; &lt;doi:10.1007/BF02289233&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

# reduced k-means with 3 unit-clusters and 2 components for the variables
out &lt;- redkm(iris, K = 3, Q = 2, Rndstart = 15, verbose = 0, maxiter = 100, tol = 1e-7, rot = 1)

</code></pre>

<hr>
<h2 id='silhouette'>Silhouette</h2><span id='topic+silhouette'></span>

<h3>Description</h3>

<p>Computes and plots the silhouette of a partition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>silhouette(data, drclust_out)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="silhouette_+3A_data">data</code></td>
<td>
<p>Units x variables data matrix.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_drclust_out">drclust_out</code></td>
<td>
<p>Out of either doublekm, redkm, factkm or dpcakm.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>cl.silhouette</code></td>
<td>
<p>Silhouette index for the given partition, for each object (matrix).</p>
</td></tr>
<tr><td><code>fe.silhouette</code></td>
<td>
<p>Factoextra silhouette graphical object</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ionel Prunila, Maurizio Vichi
</p>


<h3>References</h3>

<p>Rousseeuw P. J. (1987) &quot;Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis&quot; &lt;doi:10.1016/0377-0427(87)90125-7&gt;
</p>
<p>Maechler M. et al. (2023) &quot;cluster: Cluster Analysis Basics and Extensions&quot; &lt;https://CRAN.R-project.org/package=cluster&gt;
</p>
<p>Kassambara A. (2022) &quot;factoextra: Extract and Visualize the Results of Multivariate Data Analyses&quot; &lt;https://cran.r-project.org/web/packages/factoextra/index.html&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data 
# Loading the numeric variables of iris data
iris &lt;- as.matrix(iris[,-5]) 

#standardizing the data
iris &lt;- scale(iris)

#applying a clustering algorithm
drclust_out &lt;- dpcakm(iris, 20, 3)

#silhouette based on the data and the output of the clustering algorithm
d &lt;- silhouette(iris, drclust_out)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
