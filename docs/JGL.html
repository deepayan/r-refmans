<!DOCTYPE html><html><head><title>Help for package JGL</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {JGL}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#crit'>
<p>Calculate the critical value of the FGL objective funciton.</p></a></li>
<li><a href='#example.data'>
<p>Toy two-class gene expression dataset.</p></a></li>
<li><a href='#gcrit'>
<p>Calculate the critical value of the GGL objective funciton.</p></a></li>
<li><a href='#JGL'>
<p>Joint Graphical Lasso</p></a></li>
<li><a href='#JGL-internal'><p>Internal JGL functions</p></a></li>
<li><a href='#JGL-package'>
<p>Joint Graphical Lasso</p></a></li>
<li><a href='#net.degree'>
<p>List the degree of every node in all classes.</p></a></li>
<li><a href='#net.edges'>
<p>List the edges in a network</p></a></li>
<li><a href='#net.hubs'>
<p>Get degrees of most connected nodes.</p></a></li>
<li><a href='#net.neighbors'>
<p>Get network neighbors of a node</p></a></li>
<li><a href='#screen.fgl'>
<p>Quickly identify connected features in the solution to FGL</p></a></li>
<li><a href='#screen.ggl'>
<p>Quickly identify connected features in the solution to GGL</p></a></li>
<li><a href='#subnetworks'>
<p>Identify subnetwork membership</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Performs the Joint Graphical Lasso for Sparse Inverse Covariance
Estimation on Multiple Classes</td>
</tr>
<tr>
<td>Version:</td>
<td>2.3.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-04</td>
</tr>
<tr>
<td>Author:</td>
<td>Patrick Danaher</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Patrick Danaher &lt;pdanaher@uw.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The Joint Graphical Lasso is a generalized method for
        estimating Gaussian graphical models/ sparse inverse covariance
        matrices/ biological networks on multiple classes of data.  We
        solve JGL under two penalty functions: The Fused Graphical
        Lasso (FGL), which employs a fused penalty to encourage inverse
        covariance matrices to be similar across classes, and the Group
        Graphical Lasso (GGL), which encourages similar network
        structure between classes.  FGL is recommended over GGL for
        most applications. Reference: Danaher P, Wang P, Witten DM. (2013) 
        &lt;<a href="https://doi.org/10.1111%2Frssb.12033">doi:10.1111/rssb.12033</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>igraph</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-19 00:02:36 UTC; pdanaher</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-19 05:50:06 UTC</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
</table>
<hr>
<h2 id='crit'>
Calculate the critical value of the FGL objective funciton.  
</h2><span id='topic+crit'></span>

<h3>Description</h3>

<p>crit() calculates the critical value of the FGL objective funciton.  It is used to confirm that the FGL algorithm is converging.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crit(theta, S, n, lam1, lam2, penalize.diagonal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crit_+3A_theta">theta</code></td>
<td>

<p>A list of pXp inverse covariance matrices.  
</p>
</td></tr>
<tr><td><code id="crit_+3A_s">S</code></td>
<td>

<p>A list of pXp empirical covariance matrices.  
</p>
</td></tr>
<tr><td><code id="crit_+3A_n">n</code></td>
<td>

<p>A vector of sample sizes to attribute to each of the K data matrices.  n controls the relative weights of the classes: for example, with n==c(1,1), each class's theta will be penalized equally.  
</p>
</td></tr>
<tr><td><code id="crit_+3A_lam1">lam1</code></td>
<td>

<p>The tuning parameter for the graphical lasso penalty.
</p>
</td></tr>
<tr><td><code id="crit_+3A_lam2">lam2</code></td>
<td>

<p>The tuning parameter for the fused lasso penalty.  
</p>
</td></tr>
<tr><td><code id="crit_+3A_penalize.diagonal">penalize.diagonal</code></td>
<td>

<p>Logical value determing whether the graphical lasso penalty should also be applied to the diagonal of the inverse covariance matrices.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A function called by FGL to calculate the critical value of the objective function.  
</p>


<h3>Value</h3>

<p>crit, the critical value of the list of inverse covariance matrices.  
</p>


<h3>Author(s)</h3>

<p>Patrick Danaher
</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>

<hr>
<h2 id='example.data'>
Toy two-class gene expression dataset.
</h2><span id='topic+example.data'></span>

<h3>Description</h3>

<p>A dataset with 200 genes and 2 classes of data, each with 100 observations.  The two classes' data matrices are stored in a list.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(example.data)</code></pre>


<h3>Format</h3>

<p>The format is:
List of 2
$ : num [1:100, 1:200] 0.395 -2.03 -1.704 -0.469 1.75 ...
..- attr(*, &quot;dimnames&quot;)=List of 2
.. ..$ : NULL
.. ..$ : chr [1:200] &quot;gene 1&quot; &quot;gene 2&quot; &quot;gene 3&quot; &quot;gene 4&quot; ...
$ : num [1:100, 1:200] -1.548 1.45 -0.812 -0.589 0.69 ...
..- attr(*, &quot;dimnames&quot;)=List of 2
.. ..$ : NULL
.. ..$ : chr [1:200] &quot;gene 1&quot; &quot;gene 2&quot; &quot;gene 3&quot; &quot;gene 4&quot; ...
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(example.data)
str(example.data) 
</code></pre>

<hr>
<h2 id='gcrit'>
Calculate the critical value of the GGL objective funciton.  
</h2><span id='topic+gcrit'></span>

<h3>Description</h3>

<p>gcrit() calculates the critical value of the GGL objective funciton.  It is used to confirm that the GGL algorithm is converging.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gcrit(theta, S, n, lam1, lam2, penalize.diagonal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gcrit_+3A_theta">theta</code></td>
<td>

<p>A list of pXp inverse covariance matrices.  
</p>
</td></tr>
<tr><td><code id="gcrit_+3A_s">S</code></td>
<td>

<p>A list of pXp empirical covariance matrices.  
</p>
</td></tr>
<tr><td><code id="gcrit_+3A_n">n</code></td>
<td>

<p>A vector of sample sizes to attribute to each of the K data matrices.  n controls the relative weights of the classes: for example, with n==c(1,1), each class's theta will be penalized equally.  
</p>
</td></tr>
<tr><td><code id="gcrit_+3A_lam1">lam1</code></td>
<td>

<p>The tuning parameter for the graphical lasso penalty.
</p>
</td></tr>
<tr><td><code id="gcrit_+3A_lam2">lam2</code></td>
<td>

<p>The tuning parameter for the group lasso penalty.
</p>
</td></tr>
<tr><td><code id="gcrit_+3A_penalize.diagonal">penalize.diagonal</code></td>
<td>

<p>Logical, determining whether the penalties will be applied to the diagonal elements of the theta matrices.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A function called by GGL to calculate the critical value of the objective function.  
</p>


<h3>Value</h3>

<p>crit, the critical value of the list of inverse covariance matrices.  
</p>


<h3>Author(s)</h3>

<p>Patrick Danaher</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>

<hr>
<h2 id='JGL'>
Joint Graphical Lasso
</h2><span id='topic+JGL'></span>

<h3>Description</h3>

<p>Solve the Joint Graphical Lasso
</p>


<h3>Usage</h3>

<pre><code class='language-R'>JGL(Y,penalty="fused",lambda1,lambda2,rho=1,weights="equal",penalize.diagonal=FALSE,
maxiter=500,tol=1e-5,warm=NULL,return.whole.theta=FALSE, screening="fast",
truncate = 1e-5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="JGL_+3A_y">Y</code></td>
<td>

<p>A list of nXp data matrices.   
</p>
</td></tr>
<tr><td><code id="JGL_+3A_penalty">penalty</code></td>
<td>

<p>Determines whether lambda2 controls a &quot;fused&quot; or &quot;group&quot; lasso penalty.  Must take value &quot;fused&quot; or &quot;group&quot;.
</p>
</td></tr>
<tr><td><code id="JGL_+3A_lambda1">lambda1</code></td>
<td>

<p>The tuning parameter for the graphical lasso penalty.
</p>
</td></tr>
<tr><td><code id="JGL_+3A_lambda2">lambda2</code></td>
<td>

<p>The tuning parameter for the fused or group lasso penalty.
</p>
</td></tr>
<tr><td><code id="JGL_+3A_rho">rho</code></td>
<td>

<p>A step size parameter.  Large values decrease step size.
</p>
</td></tr>
<tr><td><code id="JGL_+3A_weights">weights</code></td>
<td>

<p>Determines the putative sample size of each class's data.  Allowed values: a vector with 
length equal to the number of classes; &quot;equal&quot;, giving each class weight 1; &quot;sample.size&quot;,
giving each class weight corresponding to its sample size.
</p>
</td></tr>
<tr><td><code id="JGL_+3A_penalize.diagonal">penalize.diagonal</code></td>
<td>

<p>If penalty==&quot;fused&quot;, determines whether lambda1 is applied to the diagonal of theta.
If penalty==&quot;group&quot;, determines whether lambda1 and lambda2 are applied to the diagonal of theta.
</p>
</td></tr>
<tr><td><code id="JGL_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations.
</p>
</td></tr>
<tr><td><code id="JGL_+3A_tol">tol</code></td>
<td>

<p>Determines convergence criterion.  
</p>
</td></tr>
<tr><td><code id="JGL_+3A_warm">warm</code></td>
<td>

<p>Input a warm start to theta in the form of a K-length list of pXp matrices.
</p>
</td></tr>
<tr><td><code id="JGL_+3A_return.whole.theta">return.whole.theta</code></td>
<td>

<p>If TRUE, each class's inverse covariance matrix is returned whole.  If FALSE, the inverse covariance
matrix is only returned over the connected nodes, and only the diagonal of the matrix is returned over
the unconnected nodes.  
</p>
</td></tr>
<tr><td><code id="JGL_+3A_screening">screening</code></td>
<td>

<p>&quot;fast&quot; or &quot;memory.efficient&quot;.  Use of &quot;fast&quot; is recommended unless the number of features prohibits
storage of a pXp matrix.  For very high dimension data, screening=&quot;memory.efficient&quot; will allow a 
solution with a much longer computation time.
</p>
</td></tr>
<tr><td><code id="JGL_+3A_truncate">truncate</code></td>
<td>

<p>Defaults to 1e-5.  At convergence, all values of theta below this number will be set to zero.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can solve both the Fused Graphical Lasso and the Group Graphical Lasso.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>theta</code></td>
<td>
<p>A list of the estimated inverse covariance matrices, over all nodes if 
return.whole.theta==TRUE and over only the connected nodes if return.whole.theta==FALSE</p>
</td></tr>
<tr><td><code>diag.theta.unconnected</code></td>
<td>
<p>Returned only if return.whole.theta==FALSE.  A list of vectors, 
each vector the estimated diagonal of an inverse covariance matrix over the unconnected nodes.</p>
</td></tr>
<tr><td><code>connected</code></td>
<td>
<p>A logical vector identifying whether each node is connected.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Patrick Danaher
</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance 
estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load an example dataset with K=two classes, p=200 features, and n=100 samples per class:
data(example.data)
str(example.data)
## run fgl:
fgl.results = JGL(Y=example.data,penalty="fused",lambda1=.25,lambda2=.1)
str(fgl.results)
print.jgl(fgl.results)
## run ggl:
ggl.results = JGL(Y=example.data,penalty="group",lambda1=.15,lambda2=.2,return.whole.theta=TRUE)
str(ggl.results)
print.jgl(ggl.results)
</code></pre>

<hr>
<h2 id='JGL-internal'>Internal JGL functions</h2><span id='topic+print.jgl'></span><span id='topic+plot.jgl'></span>

<h3>Description</h3>

<p>Internal JGL functions</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'jgl'
plot(x,...)
## S3 method for class 'jgl'
print(x,...)
</code></pre>


<h3>Author(s)</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten</p>

<hr>
<h2 id='JGL-package'>
Joint Graphical Lasso
</h2><span id='topic+JGL-package'></span>

<h3>Description</h3>

<p>Runs the Fused Graphical Lasso and the Group Graphical Lasso for network estimation and sparse inverse covariance estimation across multiple classes of data.  
</p>


<h3>Details</h3>

<p>The Fused Graphical Lasso (FGL) and the Group Graphical Lasso (GGL) are two methods for estimating sparse inverse covariance matrices that are similar across classes.  
A motivating example is the analysis of gene expression data from tumor and healthy cells: FGL and GGL allow joint estimation of gene expression conditional dependency networks in both cancer and healthy cells
FGL is recommended over GGL for most purposes.  The function JGL can implement either of these methods.
</p>
<p>The JGL package includes a number of functions to help analyze estimated networks: subnetworks(), net.degree(), net.edges(), net.hubs(), net.neighbors(), print.jgl() and plot.jgl().  These functions rely on the igraph package.  
</p>
<p>A large number of other functions are called by the above functions, and are not generally useful when called by the user.  
</p>


<h3>Author(s)</h3>

<p>Patrick Danaher
</p>
<p>Maintainer: Patrick Danaher - pdanaher at uw dot edu
</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load an example dataset with K=two classes, p=200 features, and n=100 samples per class:
data(example.data)
str(example.data)
## run FGL:
fgl.results = JGL(Y=example.data,penalty="fused",lambda1=.25,lambda2=.1)
str(fgl.results)
print.jgl(fgl.results)
## get subnetwork membership of FGL results:
subnetworks(fgl.results$theta)
</code></pre>

<hr>
<h2 id='net.degree'>
List the degree of every node in all classes.  
</h2><span id='topic+net.degree'></span>

<h3>Description</h3>

<p>For each class, lists the degree of every node.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>net.degree(theta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="net.degree_+3A_theta">theta</code></td>
<td>

<p>A list of pXp matrices, each an estimated sparse inverse covariance matrix.  (For example, the result of FGL or GGL.)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>degree, a list of p-length vectors, each giving the degree of all p nodes in the network for the corresponding class.</p>


<h3>Author(s)</h3>

<p>Patrick Danaher</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load an example dataset with K=two classes, p=200 features, and n=100 samples per class:
data(example.data)
str(example.data)
## run fgl:
fgl.results = JGL(Y=example.data,penalty="fused",lambda1=.25,lambda2=.1)
## get degree list:
net.degree(fgl.results$theta)

</code></pre>

<hr>
<h2 id='net.edges'>
List the edges in a network
</h2><span id='topic+net.edges'></span>

<h3>Description</h3>

<p>For each class, list every pair of connected nodes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>net.edges(theta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="net.edges_+3A_theta">theta</code></td>
<td>

<p>A list of pXp matrices, each an estimated sparse inverse covariance matrix.  (For example, the result of FGL or GGL.)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>edges, a K-length list, each element of the list an igraph.es object detailing all pairs of connected nodes in the class.   
</p>


<h3>Author(s)</h3>

<p>Patrick Danaher
</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load an example dataset with K=two classes, p=200 features, and n=100 samples per class:
data(example.data)
str(example.data)
## run fgl:
fgl.results = JGL(Y=example.data,penalty="fused",lambda1=.25,lambda2=.1)
## get edges list:
net.edges(fgl.results$theta)

</code></pre>

<hr>
<h2 id='net.hubs'>
Get degrees of most connected nodes.
</h2><span id='topic+net.hubs'></span>

<h3>Description</h3>

<p>List the degrees of the most connected nodes in each class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>net.hubs(theta, nhubs = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="net.hubs_+3A_theta">theta</code></td>
<td>

<p>A list of pXp matrices, each an estimated sparse inverse covariance matrix.  (For example, the result of FGL or GGL.)
</p>
</td></tr>
<tr><td><code id="net.hubs_+3A_nhubs">nhubs</code></td>
<td>

<p>The number of hubs to be identified.  net.hubs() will list the degree of the nhubs most connected nodes in each class.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>hubs, a list of length K, each element of which is a vector giving the degree of the most connected nodes in the corresponding class.</p>


<h3>Author(s)</h3>

<p>Patrick Danaher
</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load an example dataset with K=two classes, p=200 features, and n=100 samples per class:
data(example.data)
str(example.data)
## run fgl:
fgl.results = JGL(Y=example.data,penalty="fused",lambda1=.25,lambda2=.1)
## get hubs list:
net.hubs(fgl.results$theta)
</code></pre>

<hr>
<h2 id='net.neighbors'>
Get network neighbors of a node
</h2><span id='topic+net.neighbors'></span>

<h3>Description</h3>

<p>For each class, returns the names of the nodes connected to a given node.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>net.neighbors(theta, index)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="net.neighbors_+3A_theta">theta</code></td>
<td>

<p>A list of pXp matrices, each an estimated sparse inverse covariance matrix.  (For example, the result of FGL or GGL.)
</p>
</td></tr>
<tr><td><code id="net.neighbors_+3A_index">index</code></td>
<td>

<p>The row number of the node to be investigated.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>neighbors, a list of length K, each element of which is a vector of the row names of the nodes neighboring the node of interest.
</p>


<h3>Author(s)</h3>

<p>Patrick Danaher
</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load an example dataset with K=two classes, p=200 features, and n=100 samples per class:
data(example.data)
str(example.data)
## run fgl:
fgl.results = JGL(Y=example.data,penalty="fused",lambda1=.25,lambda2=.1,return.whole.theta=TRUE)
## get neighbors of gene 195:
net.neighbors(fgl.results$theta,index=195)

</code></pre>

<hr>
<h2 id='screen.fgl'>
Quickly identify connected features in the solution to FGL
</h2><span id='topic+screen.fgl'></span>

<h3>Description</h3>

<p>Applies the FGL screening rule to identify (before running FGL) which features are connected (have degree &gt; 0 in any class) or unconnected in the solution.  
screen.fgl returns exactly the right list of connected nodes when K=2.  When K is larger than 2, screen.fgl applies a weaker condition that screens out many, but not all unconnected nodes.  
This algorithm is set up to be memory-efficient, but not fast: it can be applied to very large dimension datasets, but it will take time to run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen.fgl(Y, lambda1, lambda2, weights = "equal")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen.fgl_+3A_y">Y</code></td>
<td>

<p>A list of nXp data matrices.
</p>
</td></tr>
<tr><td><code id="screen.fgl_+3A_lambda1">lambda1</code></td>
<td>

<p>The tuning parameter for the graphical lasso penalty.  Must be greater than or equal to 0.  
</p>
</td></tr>
<tr><td><code id="screen.fgl_+3A_lambda2">lambda2</code></td>
<td>

<p>The tuning parameter for the fused lasso penalty.  Must be greater than or equal to 0.  
</p>
</td></tr>
<tr><td><code id="screen.fgl_+3A_weights">weights</code></td>
<td>

<p>The weights to assign to each class.  The higher a class's weights, the weaker the effect of the penalties on its estimated inverse covariance matrix.  If &quot;equal&quot;, the classes are weighted equally, regardless of sample size.  If &quot;sample.size&quot;, the classes are weighted by sample size.  Custom weightings are achievable by entering a vector of K weights.  
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>connected, a logical vector identifying the connected nodes.
</p>


<h3>Author(s)</h3>

<p>Patrick Danaher
</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load an example dataset with K=two classes, p=200 features, and n=100 samples per class:
data(example.data)
str(example.data)
## which nodes will be connected?
screen.fgl(example.data,lambda1=.2,lambda2=.1,weights="equal")

</code></pre>

<hr>
<h2 id='screen.ggl'>
Quickly identify connected features in the solution to GGL
</h2><span id='topic+screen.ggl'></span>

<h3>Description</h3>

<p>Applies the GGL screening rule to identify (before running GGL) which features are connected (have degree &gt; 0 in any class) in the solution.  This algorithm is set up to be memory-efficient, but not fast: it can be applied to very large dimension datasets, but it will take time to run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen.ggl(Y, lambda1, lambda2, weights = "equal")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen.ggl_+3A_y">Y</code></td>
<td>

<p>A list of nXp data matrices.
</p>
</td></tr>
<tr><td><code id="screen.ggl_+3A_lambda1">lambda1</code></td>
<td>

<p>The tuning parameter for the graphical lasso penalty.  Must be greater than or equal to 0.  
</p>
</td></tr>
<tr><td><code id="screen.ggl_+3A_lambda2">lambda2</code></td>
<td>

<p>The tuning parameter for the group lasso penalty.  Must be greater than or equal to 0.  
</p>
</td></tr>
<tr><td><code id="screen.ggl_+3A_weights">weights</code></td>
<td>

<p>The weights to assign to each class.  The higher a class's weights, the weaker the effect of the penalties on its estimated inverse covariance matrix.  If &quot;equal&quot;, the classes are weighted equally, regardless of sample size.  If &quot;sample.size&quot;, the classes are weighted by sample size.  Custom weightings are achievable by entering a vector of K weights.  
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>connected, a logical vector identifying the connected nodes.
</p>


<h3>Author(s)</h3>

<p>Patrick Danaher
</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load an example dataset with K=two classes, p=200 features, and n=100 samples per class:
data(example.data)
str(example.data)
## which nodes will be connected?
screen.ggl(example.data,lambda1=.3,lambda2=.3,weights="equal")

</code></pre>

<hr>
<h2 id='subnetworks'>
Identify subnetwork membership
</h2><span id='topic+subnetworks'></span>

<h3>Description</h3>

<p>For each class, returns lists of all features belonging to subnetworks.  (A subnetwork is defined as a collection of features C for which theta[C,!C]==0, and within which no further subnetworks can be identified.  In other words, a block in the block diagonal structure of theta, or a set of features that can be connected through theta's edges.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subnetworks(theta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subnetworks_+3A_theta">theta</code></td>
<td>

<p>A list of pXp matrices, each an estimated sparse inverse covariance matrix.  (For example, the result of FGL or GGL.)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list length K, each element of which is a list of subnetworks in class K.  Each subnetwork is represented as a vector of feature names.</p>


<h3>Author(s)</h3>

<p>Patrick Danaher
</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load an example dataset with K=two classes, p=200 features, and n=100 samples per class:
data(example.data)
str(example.data)
## run fgl:
fgl.results = JGL(Y=example.data,penalty="fused",lambda1=.25,lambda2=.1)
## identify subnetworks
subnetworks(fgl.results$theta)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
