<!DOCTYPE html><html><head><title>Help for package reglogit</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {reglogit}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#pima'>
<p>Pima Indian Data</p></a></li>
<li><a href='#predict.reglogit'>
<p>Prediction for regularized (polychotomous) logistic regression models</p></a></li>
<li><a href='#reglogit'>
<p>Gibbs sampling for regularized logistic regression</p></a></li>
<li><a href='#reglogit-internal'><p>Internal reglogit Functions</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Simulation-Based Regularized Logistic Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2-7</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-04-21</td>
</tr>
<tr>
<td>Author:</td>
<td>Robert B. Gramacy &lt;rbg@vt.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Robert B. Gramacy &lt;rbg@vt.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14.0), methods, mvtnorm, boot, Matrix</td>
</tr>
<tr>
<td>Suggests:</td>
<td>plgp</td>
</tr>
<tr>
<td>Description:</td>
<td>Regularized (polychotomous) logistic regression 
  by Gibbs sampling. The package implements subtly different 
  MCMC schemes with varying efficiency depending on the data type 
  (binary v. binomial, say) and the desired estimator (regularized maximum
  likelihood, or Bayesian maximum a posteriori/posterior mean, etc.) through a 
  unified interface. For details, see Gramacy &amp; Polson (2012 &lt;<a href="https://doi.org/10.1214%2F12-BA719">doi:10.1214/12-BA719</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://bobby.gramacy.com/r_packages/reglogit/">https://bobby.gramacy.com/r_packages/reglogit/</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-24 13:21:55 UTC; bobby</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-25 07:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='pima'>
Pima Indian Data
</h2><span id='topic+pima'></span>

<h3>Description</h3>

<p>A population of women who were at least 21 years old,
of Pima Indian heritage and living near Phoenix, Arizona, 
was tested for diabetes according to World Health Organization 
criteria. The data were collected by the US National Institute 
of Diabetes and Digestive and Kidney Diseases. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(pima)</code></pre>


<h3>Format</h3>

<p>A data frame with 768 observations on the following 9 variables.
</p>

<dl>
<dt><code>npreg</code></dt><dd><p>number of pregnancies</p>
</dd>
<dt><code>glu</code></dt><dd><p>plasma glucose concentration in an oral glucose
tolerance test</p>
</dd>
<dt><code>bp</code></dt><dd><p>diastolic blood pressure (mm Hg)</p>
</dd>
<dt><code>skin</code></dt><dd><p>tricepts skin fold thickness (mm)</p>
</dd>
<dt><code>serum</code></dt><dd><p>2-hour serum insulin (mu U/ml)</p>
</dd>
<dt><code>bmi</code></dt><dd><p>mody mass index (weight in kg/(height in m)^2)</p>
</dd>
<dt><code>ped</code></dt><dd><p>diabetes pedigree function</p>
</dd>
<dt><code>age</code></dt><dd><p>age in years</p>
</dd>
<dt><code>y</code></dt><dd><p>classification label: <code>1</code> for diabetic</p>
</dd>
</dl>



<h3>Source</h3>

<p>Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C. and
Johannes, R. S. (1988) 
<em>Using the ADAP learning algorithm to forecast the onset of
diabetes mellitus. In Proceedings of the Symposium on Computer 
Applications in Medical Care (Washington, 1988)</em>, ed. 
R. A. Greenes, pp. 261-265. Los Alamitos, CA: IEEE Computer Society Press.
</p>
<p>Ripley, B.D. (1996) <em>Pattern Recognition and Neural Networks.</em>
Cambridge: Cambridge University Press.
</p>
<p>UCI Machine Learning Repository<br />
<a href="http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes">http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(pima)
## see reglogit documentation for an example using this data
</code></pre>

<hr>
<h2 id='predict.reglogit'>
Prediction for regularized (polychotomous) logistic regression models
</h2><span id='topic+predict.reglogit'></span><span id='topic+predict.regmlogit'></span>

<h3>Description</h3>

<p>Sampling from the posterior predictive distribution of a regularized (multinomial)
logistic regression fit, including entropy information for variability assessment
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'reglogit'
predict(object, XX, burnin = round(0.1 * nrow(object$beta)), ...)
## S3 method for class 'regmlogit'
predict(object, XX, burnin = round(0.1 * dim(object$beta)[1]), ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.reglogit_+3A_object">object</code></td>
<td>

<p>a <code>"reglogit"</code>-class object or a <code>"regmlogit"</code>-class object, depending
on whether binary or polychotomous methods were used for fitting
</p>
</td></tr>
<tr><td><code id="predict.reglogit_+3A_xx">XX</code></td>
<td>

<p>a <code>matrix</code> of predictive locations where <code>ncol(XX) == object$ncol(XX)</code>.
</p>
</td></tr>
<tr><td><code id="predict.reglogit_+3A_burnin">burnin</code></td>
<td>

<p>a scalar positive <code>integer</code> indicate the number of samples of <code>object$beta</code>
to discard as burn-in;  the default is 10% of the number of samples
</p>
</td></tr>
<tr><td><code id="predict.reglogit_+3A_...">...</code></td>
<td>
<p> For compatibility with generic <code><a href="stats.html#topic+predict">predict</a></code> method; not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Applies the logit transformation (<code>reglogit</code>) or multinomial logit (<code>regmlogit</code>)
to convert samples of the linear predictor at <code>XX</code> into a samples from a predictive
posterior probability distribution.  The raw probabilties, averages (posterior means), 
entropies, and posterior mean casses (arg-max of the average probabilities) are returned.
</p>


<h3>Value</h3>

<p>The output is a <code>list</code> with components explained below. For
<code>predict.regmlogit</code> everyhing (except entropy) is expanded by one
dimension into an <code>array</code> or <code>matrix</code> as appropriate.
</p>
<table>
<tr><td><code>p</code></td>
<td>
<p> a <code>nrow(XX) x (T-burnin)</code> sized <code>matrix</code> of
probabilities (of class 1) from the posterior predictive distribution. </p>
</td></tr>
<tr><td><code>mp</code></td>
<td>
<p> a vector of average probablities calculated over the rows of <code>p</code></p>
</td></tr>
<tr><td><code>pc</code></td>
<td>
<p> class labels formed by rouding (or arg max for <code>predict.regmlogit</code>) 
the values in <code>mp</code></p>
</td></tr>
<tr><td><code>ent</code></td>
<td>
<p> The posterior mean entropy given the probabilities in <code>mp</code> </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>R.B. Gramacy, N.G. Polson. &ldquo;Simulation-based regularized
logistic regression&rdquo;. (2012) Bayesian Analysis, 7(3), p567-590; 
arXiv:1005.3430; <a href="https://arxiv.org/abs/1005.3430">https://arxiv.org/abs/1005.3430</a>
</p>
<p>C. Holmes, K. Held (2006). &ldquo;Bayesian Auxilliary Variable Models for
Binary and Multinomial Regression&rdquo;. Bayesian Analysis, 1(1), p145-168. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+reglogit">reglogit</a></code> and <code><a href="#topic+regmlogit">regmlogit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see reglogit for a full example of binary classifiction complete with
## sampling from the posterior predictive distribution.  

## the example here is for polychotomous classification and prediction

## Not run: 
library(plgp)
x &lt;- seq(-2, 2, length=40)
X &lt;- expand.grid(x, x)
C &lt;- exp2d.C(X)
xx &lt;- seq(-2, 2, length=100)
XX &lt;- expand.grid(xx, xx)
CC &lt;- exp2d.C(XX)

## build cubically-expanded design matrix (with interactions)
Xe &lt;- cbind(X, X[,1]^2, X[,2]^2, X[,1]*X[,2],
            X[,1]^3, X[,2]^3, X[,1]^2*X[,2], X[,2]^2*X[,1],
            (X[,1]*X[,2])^2)

## perform MCMC
T &lt;- 1000
out &lt;- regmlogit(T, C, Xe, nu=6, normalize=TRUE)

## create predictive (cubically-expanded) design matrix
XX &lt;- as.matrix(XX)
XXe &lt;- cbind(XX, XX[,1]^2, XX[,2]^2, XX[,1]*XX[,2],
             XX[,1]^3, XX[,2]^3, XX[,1]^2*XX[,2], XX[,2]^2*XX[,1],
             (XX[,1]*XX[,2])^2)

## predict class labels
p &lt;- predict(out, XXe)

## make an image of the predictive surface
cols &lt;- c(gray(0.85), gray(0.625), gray(0.4))
par(mfrow=c(1,3))
image(xx, xx, matrix(CC, ncol=length(xx)), col=cols, main="truth")
image(xx, xx, matrix(p$c, ncol=length(xx)), col=cols, main="predicted")
image(xx, xx, matrix(p$ent, ncol=length(xx)), col=heat.colors(128),
      main="entropy")

## End(Not run)
</code></pre>

<hr>
<h2 id='reglogit'>
Gibbs sampling for regularized logistic regression
</h2><span id='topic+reglogit'></span><span id='topic+regmlogit'></span>

<h3>Description</h3>

<p>Regularized (multinomial) logistic regression
by Gibbs sampling implementing subtly different 
MCMC schemes with varying efficiency depending on the data type 
(binary v. binomial, say) and the desired estimator (regularized maximum
likelihood, or Bayesian maximum a posteriori/posterior mean, etc.) through a 
unified interface.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reglogit(T, y, X, N = NULL, flatten = FALSE, sigma = 1, nu = 1,
      kappa = 1, icept = TRUE, normalize = TRUE, zzero = TRUE, 
      powerprior = TRUE, kmax = 442, bstart = NULL, lt = NULL, 
      nup = list(a = 2, b = 0.1), save.latents = FALSE, verb = 100)
regmlogit(T, y, X, flatten = FALSE, sigma = 1, nu = 1, kappa = 1, 
      icept=TRUE, normalize = TRUE, zzero = TRUE, powerprior = TRUE, 
      kmax = 442, bstart = NULL, lt = NULL, nup = list(a=2, b=0.1),
      save.latents = FALSE, verb=100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reglogit_+3A_t">T</code></td>
<td>

<p>a positive integer scalar specifying the number of MCMC rounds
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_y">y</code></td>
<td>

<p><code>reglogit</code> requires <code>logical</code> classification labels for Bernoulli 
data, or countsfor Binomial data; for the latter, <code>N</code> must also be specified.
<code>regmlogit</code> requires positive integer class labeels in <code>1:C</code> where
<code>C</code> is the number of classes.
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_x">X</code></td>
<td>

<p>a design <code>matrix</code> of predictors; can be a typical (dense) <code>matrix</code>
or a sparse <code><a href="Matrix.html#topic+Matrix">Matrix</a></code> object.  When the design matrix
is sparse (and is stored sparsely), this can produce a ~3x-faster execution via
a more efficient update for the beta parameter.  But when it is not sparse
(but is stored sparsely) the execution could be much slower
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_n">N</code></td>
<td>

<p>an optional integer vector of total numbers of replicate trials 
for each <code>X</code>-<code>y</code>, i.e., for Binomial data instead of Bernoulli
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_flatten">flatten</code></td>
<td>

<p>a scalar <code>logical</code> that is only specified for Binomial data.  It
indicates if pre-processing code should flatten the Binomial
likelihood into a Bernoulli likelihood
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_sigma">sigma</code></td>
<td>

<p>weights on the regression coefficients in the lasso penalty.  The
default of <code>1</code> is sensible when <code>normalize = TRUE</code> since
then the estimator for <code>beta</code> is equivariant under rescaling
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_nu">nu</code></td>
<td>

<p>a non-negative scalar indicating the initial value of the penalty
parameter
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_kappa">kappa</code></td>
<td>

<p>a positive scalar specifying the multiplicity;  <code>kappa = 1</code>
provides samples from the Bayesian posterior distribution.  Larger
values of <code>kappa</code> facilitates a simulated annealing approach
to obtaining a regularized point estimator
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_icept">icept</code></td>
<td>

<p>a scalar <code>logical</code> indicating if an (implicit) intercept should
be included in the model
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_normalize">normalize</code></td>
<td>

<p>a scalar logical which, if <code>TRUE</code>, causes each variable is standardized
to have unit L2-norm, otherwise it is left alone 
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_zzero">zzero</code></td>
<td>

<p>a scalar <code>logical</code> indicating if the latent <code>z</code> variables to be
sampled.  Therefore this indicator specifies if the cdf
representation (<code>zzero = FALSE</code>) or pdf representation
(otherwise) should be used
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_powerprior">powerprior</code></td>
<td>

<p>a scalar <code>logical</code> indicating if the prior should be powered up
with multiplicity parameter <code>kappa</code> as well as the likelihood
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_kmax">kmax</code></td>
<td>

<p>a positive integer indicating the number replacing infinity in the
sum for mixing density in the generative expression for
<code>lambda</code>
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_bstart">bstart</code></td>
<td>

<p>an optional vector of length <code>p = ncol(X)</code> specifying initial
values for the regression coefficients <code>beta</code>.   Otherwise
standard normal deviates are used
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_lt">lt</code></td>
<td>

<p>an optional vector of length <code>n = nrow(X)</code> of initial values
for the <code>lambda</code> latent
variables.  Otherwise a vector of ones is used.
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_nup">nup</code></td>
<td>

<p>prior parameters <code>=list(a, b)</code> for the inverse Gamma distribution
prior for <code>nu</code>, or <code>NULL</code>, which causes <code>nu</code> to be fixed
</p>
</td></tr>
<tr><td><code id="reglogit_+3A_save.latents">save.latents</code></td>
<td>
<p> a scalar <code>logical</code> indicating wether or not
a trace of latent <code>z</code>, <code>lambda</code> and <code>omega</code> values should be saved
for each iteration.  Specify <code>save.latents=TRUE</code> for very large <code>X</code>
in order to reduce memory swapping on low-RAM machines </p>
</td></tr>
<tr><td><code id="reglogit_+3A_verb">verb</code></td>
<td>

<p>A positive integer indicating the number of MCMC rounds after which
a progress statement is printed.  Giving <code>verb = 0</code> causes no
statements to be printed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are the main functions in the package.  They support an omnibus
framework for simulation-based regularized logistic regression.  The
default arguments invoke a Gibbs sampling algorithm to sample from the
posterior distribution of a logistic regression model with
lasso-type (double-exponential) priors.  See the paper by Gramacy &amp;
Polson (2012) for details.  Both cdf and pdf implementations are
provided, which use slightly different latent variable
representations, resulting in slightly different Gibbs samplers.  These
methods extend the un-regularized methods of Holmes &amp; Held (2006)
</p>
<p>The <code>kappa</code> parameter facilitates simulated annealing (SA)
implementations in order to help find the MAP, and other point
estimators.  The actual SA algorithm is not provided in the package.
However, it is easy to string calls to this function, using the
outputs from one call as inputs to another, in order to establish a SA
schedule for increasing kappa values.
</p>
<p>The <code>regmlogit</code> function is a wrapper around the Gibbs sampler
inside <code>reglogit</code>, invoking <code>C-1</code> linked chains for <code>C</code>
classes, extending the polychotomous regression scheme outlined by 
Holmes &amp; Held (2006).  For an example with <code>regmlogit</code>, see
<code><a href="#topic+predict.regmlogit">predict.regmlogit</a></code>
</p>


<h3>Value</h3>

<p>The output is a <code>list</code> object of type <code>"reglogit"</code> or 
<code>"regmlogit"</code> containing a subset of the following fields;
for <code>"regmlogit"</code> everyhing is expanded by one dimension into
an <code>array</code> or <code>matrix</code> as appropriate.
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p> the input design <code>matrix</code>, possible adjusted by
normalization or intercept </p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p> the input response variable </p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p> a <code>matrix</code> of <code>T</code> sampled regression
coefficients on the original input scale </p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p> if <code>zzero = FALSE</code> a <code>matrix</code> of latent
variables for the hierarchical cdf representation of the likelihood </p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p> a <code>matrix</code> of latent variables for the
hierarchical (cdf or pdf) representation of the likelihood </p>
</td></tr>
<tr><td><code>lpost</code></td>
<td>
<p> a vector of log posterior probabilities of the parameters </p>
</td></tr>
<tr><td><code>map</code></td>
<td>
<p> the <code>list</code> containing the maximum a' posterior
parameters; <code>out$map$beta</code> is on the original scale of the data  </p>
</td></tr>
<tr><td><code>kappa</code></td>
<td>
<p> the input multiplicity parameter </p>
</td></tr>
<tr><td><code>omega</code></td>
<td>
<p> a <code>matrix</code> of latent variables for the
regularization prior</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>R.B. Gramacy, N.G. Polson. &ldquo;Simulation-based regularized
logistic regression&rdquo;. (2012) Bayesian Analysis, 7(3), p567-590; 
arXiv:1005.3430; <a href="https://arxiv.org/abs/1005.3430">https://arxiv.org/abs/1005.3430</a>
</p>
<p>C. Holmes, K. Held (2006). &ldquo;Bayesian Auxilliary Variable Models for
Binary and Multinomial Regression&rdquo;. Bayesian Analysis, 1(1), p145-168. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.reglogit">predict.reglogit</a></code>, <code><a href="#topic+predict.regmlogit">predict.regmlogit</a></code>, 
<code><a href="monomvn.html#topic+blasso">blasso</a></code> and <code><a href="monomvn.html#topic+regress">regress</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load in the pima indian data
data(pima)
X &lt;- as.matrix(pima[,-9])
y &lt;- as.numeric(pima[,9])

## pre-normalize to match the comparison in the paper
one &lt;- rep(1, nrow(X))
normx &lt;- sqrt(drop(one %*% (X^2)))
X &lt;- scale(X, FALSE, normx)

## compare to the GLM fit
fit.logit &lt;- glm(y~X, family=binomial(link="logit"))
bstart &lt;- fit.logit$coef

## do the Gibbs sampling
T &lt;- 300 ## set low for CRAN checks; increase to &gt;= 1000 for better results
out6 &lt;- reglogit(T, y, X, nu=6, nup=NULL, bstart=bstart, normalize=FALSE)

## plot the posterior distribution of the coefficients
burnin &lt;- (1:(T/10)) 
boxplot(out6$beta[-burnin,], main="nu=6, kappa=1", ylab="posterior",
        xlab="coefficients", bty="n", names=c("mu", paste("b", 1:8, sep="")))
abline(h=0, lty=2)

## add in GLM fit and MAP with legend
points(bstart, col=2, pch=17)
points(out6$map$beta, pch=19, col=3)
legend("topright", c("MLE", "MAP"), col=2:3, pch=c(17,19))

## simple prediction
p6 &lt;- predict(out6, XX=X)
## hit rate
mean(p6$c == y)

##
## for a polychotomous example, with prediction, 
## see ? predict.regmlogit
##

## Not run: 
## now with kappa=10
out10 &lt;- reglogit(T, y, X, kappa=10, nu=6, nup=NULL, bstart=bstart, 
                            normalize=FALSE)

## plot the posterior distribution of the coefficients
par(mfrow=c(1,2))
boxplot(out6$beta[-burnin,], main="nu=6, kappa=1",  ylab="posterior",
        xlab="coefficients", bty="n",  names=c("mu", paste("b", 1:8, sep="")))
abline(h=0, lty=2) 
points(bstart, col=2, pch=17)
points(out6$map$beta, pch=19, col=3)
legend("topright", c("MLE", "MAP"), col=2:3, pch=c(17,19))
boxplot(out10$beta[-burnin,], main="nu=6, kappa=10",  ylab="posterior",
        xlab="coefficients", bty="n",  names=c("mu", paste("b", 1:8, sep="")))
abline(h=0, lty=2)
## add in GLM fit and MAP with legend
points(bstart, col=2, pch=17)
points(out10$map$beta, pch=19, col=3)
legend("topright", c("MLE", "MAP"), col=2:3, pch=c(17,19))

## End(Not run)

##
## now some binomial data
##

## Not run: 
## synthetic data generation
library(boot)
N &lt;- rep(20, 100)
beta &lt;- c(2, -3, 2, -4, 0, 0, 0, 0, 0)
X &lt;- matrix(runif(length(N)*length(beta)), ncol=length(beta))
eta &lt;- drop(1 + X %*% beta)
p &lt;- inv.logit(eta)
y &lt;- rbinom(length(N), N, p)

## run the Gibbs sampler for the logit -- uses the fast Binomial
## version; for a comparison, try flatten=FALSE
out &lt;- reglogit(T, y, X, N)

## plot the posterior distribution of the coefficients
boxplot(out$beta[-burnin,], main="binomial data",  ylab="posterior", 
       xlab="coefficients", bty="n",
       names=c("mu", paste("b", 1:ncol(X), sep="")))
abline(h=0, lty=2)

## add in GLM fit, the MAP fit, the truth, and a legend
fit.logit &lt;- glm(y/N~X, family=binomial(link="logit"), weights=N)
points(fit.logit$coef, col=2, pch=17)
points(c(1, beta), col=4, pch=16)
points(out$map$beta, pch=19, col=3)
legend("topright", c("MLE", "MAP", "truth"), col=2:4, pch=c(17,19,16))

## also try specifying a larger kappa value to pin down the MAP

## End(Not run)
</code></pre>

<hr>
<h2 id='reglogit-internal'>Internal reglogit Functions</h2><span id='topic+draw.beta'></span><span id='topic+draw.nu'></span><span id='topic+draw.z'></span><span id='topic+draw.omega'></span><span id='topic+draw.lambda'></span><span id='topic+my.rinvgauss'></span><span id='topic+calc.lpost'></span><span id='topic+preprocess'></span><span id='topic+z.dRUM'></span><span id='topic+beta.dRUM'></span><span id='topic+gibbs.dRUM'></span><span id='topic+calc.Cs'></span><span id='topic+calc.mlpost'></span><span id='topic+mpreprocess'></span><span id='topic+rmultnorm'></span>

<h3>Description</h3>

<p>Internal <span class="pkg">reglogit</span> functions
</p>


<h3>Details</h3>

<p>These functions are primarily provided to facilitate the
demos, and to provide for a cleaner exposition.  We encourage
users to inspect their contents in order to help develop 
similar functions to suit their particular needs
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
