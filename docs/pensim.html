<!DOCTYPE html><html><head><title>Help for package pensim</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pensim}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#beer.exprs'>
<p>Lung adenocarcinoma microarray expression data of Beer et al. (2002)</p></a></li>
<li><a href='#beer.survival'>
<p>Survival data for Beer et al. (2002) lung adenocarcinoma study</p></a></li>
<li><a href='#create.data'>
<p>simulate correlated predictors with time-to-event or binary outcome</p></a></li>
<li><a href='#opt.nested.crossval'><p>Parallelized calculation of cross-validated risk score</p>
predictions from L1/L2/Elastic Net penalized regression.</a></li>
<li><a href='#opt.splitval'><p>Parallelized calculation of split training/test set predictions from L1/L2/Elastic</p>
Net penalized regression.</a></li>
<li><a href='#opt1D'><p>Parallelized repeated tuning of Lasso or Ridge penalty parameter</p></a></li>
<li><a href='#opt2D'>
<p>Parallelized, two-dimensional tuning of Elastic Net L1/L2 penalties</p></a></li>
<li><a href='#pensim-package'>
<p>Functions and data for simulation of high-dimensional data and parallelized repeated penalized regression</p></a></li>
<li><a href='#scan.l1l2'>
<p>Function calculate cross-validated likelihood on a regular grid of L1/L2 penalties</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Simulation of High-Dimensional Data and Parallelized Repeated
Penalized Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.6</td>
</tr>
<tr>
<td>Author:</td>
<td>Levi Waldron &lt;lwaldron.research@gmail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Levi Waldron &lt;lwaldron.research@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, parallel, penalized, MASS</td>
</tr>
<tr>
<td>Suggests:</td>
<td>survivalROC, survival, rmarkdown, knitr</td>
</tr>
<tr>
<td>Description:</td>
<td>Simulation of continuous, correlated high-dimensional data with 
    time to event or binary response, and parallelized functions for Lasso, 
    Ridge, and Elastic Net penalized regression with repeated starts and 
    two-dimensional tuning of the Elastic Net.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://waldronlab.io/pensim/">https://waldronlab.io/pensim/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/waldronlab/pensim/issues">https://github.com/waldronlab/pensim/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-12-06 12:10:19 UTC; levi</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-12-09 00:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='beer.exprs'>
Lung adenocarcinoma microarray expression data of Beer et al. (2002)
</h2><span id='topic+beer.exprs'></span>

<h3>Description</h3>

<p>Lung adenocarcinomas were profiled by Beer et al. (2002) using Affymetrix hu6800 microarrays.  The data here were normalized from raw .CEL files by RMAExpress (v0.3).  The expression matrix contains expression data for 86 patients with 7,129 probe sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(beer.exprs)
</code></pre>


<h3>Format</h3>

<p>A data frame with 7129 probe sets (rows) for 86 patients (columns)
</p>


<h3>Source</h3>

<p>Beer DG, Kardia SL, Huang C, Giordano TJ, Levin AM, Misek DE, Lin L, Chen G, Gharib TG, Thomas DG, Lizyness ML, Kuick R, Hayasaka S, Taylor JM, Iannettoni MD, Orringer MB, Hanash S: Gene-expression profiles predict survival of patients with lung adenocarcinoma. Nat Med 2002, 8:816-824. 
</p>


<h3>References</h3>

<p>Irizarry, R.A., et al. (2003) Summaries of Affymetrix GeneChip probe level data, Nucl. Acids Res., 31, e15+-e15+.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beer.exprs)
mysd &lt;- apply(beer.exprs, 1, sd)

beer.subset &lt;- as.matrix(beer.exprs[rank(-mysd) &lt;= 100, ])  
heatmap(beer.subset)
</code></pre>

<hr>
<h2 id='beer.survival'>
Survival data for Beer et al. (2002) lung adenocarcinoma study
</h2><span id='topic+beer.survival'></span>

<h3>Description</h3>

<p>Overall survival time for 86 lung adenocarcinoma patients, with 62 of
the 86 events being censored.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(beer.survival)</code></pre>


<h3>Format</h3>

<p>A data frame with 86 observations on the following 2 variables.
</p>

<dl>
<dt><code>status</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>os</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Source</h3>

<p>Beer DG, Kardia SL, Huang C, Giordano TJ, Levin AM, Misek DE, Lin L,
Chen G, Gharib TG, Thomas DG, Lizyness ML, Kuick R, Hayasaka S, Taylor
JM, Iannettoni MD, Orringer MB, Hanash S: Gene-expression pr
ofiles predict survival of patients with lung adenocarcinoma. Nat Med 2002, 8:816-824.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beer.survival)
library(survival)
surv.obj &lt;- with(beer.survival, Surv(os, status))
surv.obj.rev &lt;- with(beer.survival, Surv(os, 1-status))
survfit(surv.obj.rev~1)  #reverse KM estimate of follow-up time (months)
(my.survfit &lt;- survfit(surv.obj~1))  ##KM estimate of survival
plot(my.survfit, xlab="Time (months)", 
     ylab="KM estimate of overall survival")
legend("bottomright", lty=c(1, 2), pch=-1,
       legend=c("KM estimate", "95 percent confidence interval"))
</code></pre>

<hr>
<h2 id='create.data'>
simulate correlated predictors with time-to-event or binary outcome
</h2><span id='topic+create.data'></span>

<h3>Description</h3>

<p>This function creates multiple groups of predictor variables which may
be correlated within each group, and binary or survival time (without
censoring) response according to specified weights of the predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create.data(nvars = c(100, 100, 100, 100, 600),
            cors = c(0.8, 0, 0.8, 0, 0),
            associations = c(0.5, 0.5, 0.3, 0.3, 0),
            firstonly = c(TRUE, FALSE, TRUE, FALSE, FALSE),
            nsamples = 100,
            censoring = "none",
            labelswapprob = 0,
            response = "timetoevent",
            basehaz = 0.2,
            logisticintercept = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create.data_+3A_nvars">nvars</code></td>
<td>
<p>integer vector giving the number of variables of each
variable type.  The number of variable types is equal to the length
of this vector.</p>
</td></tr>
<tr><td><code id="create.data_+3A_cors">cors</code></td>
<td>
<p>integer vector of the same length as nvars, giving the population pairwise Pearson
correlation within each group.</p>
</td></tr>
<tr><td><code id="create.data_+3A_associations">associations</code></td>
<td>
<p>integer vector of the same length as nvars, giving the associations of each
type with outcome</p>
</td></tr>
<tr><td><code id="create.data_+3A_firstonly">firstonly</code></td>
<td>
<p>logical vector of the same length as nvars, specifying whether only the first variable
of each type is associated with outcome (TRUE) or all variables of that
type (FALSE)</p>
</td></tr>
<tr><td><code id="create.data_+3A_nsamples">nsamples</code></td>
<td>
<p>an integer giving the number of observations</p>
</td></tr>
<tr><td><code id="create.data_+3A_censoring">censoring</code></td>
<td>
<p>&quot;none&quot; for no censoring, or a vector of length two c(a,b)
for uniform U(a,b) censoring.</p>
</td></tr>
<tr><td><code id="create.data_+3A_labelswapprob">labelswapprob</code></td>
<td>
<p>This provides an option to add uncertainty to
binary outcomes by randomly switching labels with probability
labelswapprob.  The probability of a label being swapped is
independent for each observation.  The value is ignored if
response is &quot;timetoevent&quot;</p>
</td></tr>
<tr><td><code id="create.data_+3A_response">response</code></td>
<td>
<p>either &quot;timetoevent&quot; or &quot;binary&quot;</p>
</td></tr>
<tr><td><code id="create.data_+3A_basehaz">basehaz</code></td>
<td>
<p>baseline hazard, used for &quot;timetoevent&quot;</p>
</td></tr>
<tr><td><code id="create.data_+3A_logisticintercept">logisticintercept</code></td>
<td>
<p>intercept which is added to X%*%Beta for &quot;binary&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function simulates &quot;predictor&quot; variables in one or more groups, which are standard
normally distributed.  The user can specify the population correlation
within each variable group, the association of each variable group to
outcome, and whether the first or all variables of that type should be
associated with outcome.  The simulated response variable can be time to event
with an exponential distribution, or binary survival with a logistic distribution.
</p>


<h3>Value</h3>

<p>Returns a list with items:
</p>
<table>
<tr><td><code>summary</code></td>
<td>
<p>a summary of the variable
types produced</p>
</td></tr>
<tr><td><code>associations</code></td>
<td>
<p>weights of each variable in
computing the outcome</p>
</td></tr>
<tr><td><code>covariance</code></td>
<td>
<p>covariance matrix used for
generating potentially correlated random predictors</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>dataframe containing the predictors and response.
Response is the last column for binary outcome (&quot;outcome&quot;), and the
last two columns for timetoevent outcome (&quot;time&quot; and &quot;cens&quot;)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Depends on the MASS package for correlated random number generation</p>


<h3>Author(s)</h3>

<p>Levi Waldron et al.</p>


<h3>References</h3>

<p>Waldron L., Pintilie M., Tsao M.-S., Shepherd F. A., Huttenhower C.*, and Jurisica I.*   Optimized
application of penalized regression methods to diverse genomic
data. (2010). Under review.  (*equal contribution)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##binary outcome example
set.seed(9)
x &lt;-
  create.data(
    nvars = c(15, 5),
    cors = c(0, 0.8),
    associations = c(0, 2),
    firstonly = c(TRUE, TRUE),
    nsamples = 50,
    response = "binary",
    logisticintercept = 0.5
  )
summary(x)
x$summary
model &lt;- glm(outcome ~ .,  data = x$data, family = binomial)
summary(model)
dat &lt;- t(as.matrix(x$data[, -match("outcome", colnames(x$data))]))
heatmap(dat, ColSideColors = ifelse(x$data$outcome == 0, "black", "white"))

##censored survival outcome example:
set.seed(1)
x &lt;- create.data(
  nvars = c(15, 5),
  cors = c(0, 0.8),
  associations = c(0, 2),
  firstonly = c(TRUE, TRUE),
  nsamples = 50,
  censoring = c(2, 10),
  response = "timetoevent"
)
sum(x$data$cens == 0) / nrow(x$data)  #34 percent censoring

library(survival)
surv.obj &lt;- Surv(x$data$time, x$data$cens)
plot(survfit(surv.obj ~ 1), ylab = "Survival probability", xlab = "time")
</code></pre>

<hr>
<h2 id='opt.nested.crossval'>Parallelized calculation of cross-validated risk score
predictions from L1/L2/Elastic Net penalized regression.
</h2><span id='topic+opt.nested.crossval'></span>

<h3>Description</h3>

<p>calculates risk score predictions by a nested cross-validation, using
the optL1 and optL2 functions of the penalized R package for
regression.  In the outer level of cross-validation, samples are split
into training and test samples.  Model parameters are tuned by cross-validation within
training samples only.
</p>
<p>By setting nprocessors &gt; 1, the outer cross-validation is split between multiple processors.
</p>
<p>The functions support z-score scaling of training data, and application
of these scaling and shifting coefficients to the test data.  It also
supports repeated tuning of the penalty parameters and selection of the
model with greatest cross-validated likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opt.nested.crossval(outerfold=10, nprocessors=1, cl=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="opt.nested.crossval_+3A_outerfold">outerfold</code></td>
<td>

<p>number of folds in outer cross-validation (the level used for validation)
</p>
</td></tr>
<tr><td><code id="opt.nested.crossval_+3A_nprocessors">nprocessors</code></td>
<td>

<p>An integer number of processors to use.  If specified in 
opt.nested.crossval, iterations of the outer cross-validation are
sent to different processors.  If specified in opt.splitval,
repeated starts for the penalty tuning are sent to different processors.
</p>
</td></tr>
<tr><td><code id="opt.nested.crossval_+3A_cl">cl</code></td>
<td>

<p>Optional cluster object created with the makeCluster() function of
the parallel package.  If this is not set, pensim calls
makeCluster(nprocessors, type=&quot;SOCK&quot;).   Setting this parameter
can enable parallelization in more diverse scenarios than multi-core
desktops; see the documentation for the parallel package.  Note that if
cl is user-defined, this function will not automatically run
parallel::stopCluster() to shut down the cluster.
</p>
</td></tr>
<tr><td><code id="opt.nested.crossval_+3A_...">...</code></td>
<td>

<p>optFUN (either &quot;opt1D&quot; or &quot;opt2D&quot;), scaling (TRUE to z-score
training data then apply the same shift and scale factors to test
data, FALSE for no scaling) are passed onto the opt.splitval
function.  Additional arguments are required, to be passed to the
optL1 or optL2 function of the penalized R package.  See those help
pages, and it may be desirable to test these arguments directly on
optL1 or optL2 before using this more CPU-consuming and complex
function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates cross-validated risk score predictions,
tuning a penalized regression model using the optL1 or optL2 functions
of the penalized R package, for each iteration of the
cross-validation.  Tuning is done by cross-validation in the training
samples only.  Test samples are scaled using the shift and scale
factors determined from the training samples.  parameter.  If
nprocessors &gt; 1, it uses the SNOW package for parallelization,
dividing the iterations of the outer cross-validation among the
specified number of processors.
</p>
<p>Some arguments MUST be passed (through the ... arguments) but which
are documented for the functions in which they are used.  These
include, from the opt.splitval function:
</p>
<p>optFUN=&quot;opt1D&quot; for Lasso or Ridge regression, or &quot;opt2D&quot; for Elastic
Net.  See the help pages for opt1D and opt2D for additional
arguments associated with these functions.
</p>
<p>scaling=TRUE to scale each feature (column) of the training sample to
z-scores.  These same scaling and shifting factors are applied to the
test data.  If FALSE, no scaling is done.  Note that only data in the
penalized argument are scaled, not the optional unpenalized argument
(see documentation for opt1D, opt2D, or cvl from the penalized package
for descriptions of the penalized and unpenalized arguments).
Alternatively, the standardize=TRUE argument to the penalized package
functions can be used to do scaling internally.
</p>
<p>nsim=50 this number specifies the number of times to repeat tuning of
the penalty parameters on different data foldings for the
cross-validation.
</p>
<p>setpen=&quot;L1&quot; or &quot;L2&quot; : if optFUN=&quot;opt1D&quot;, this sets regression type to
LASSO or Ridge, respectively.  See ?opt1D.
</p>
<p>L1range, L2range, dofirst, L1gridsize, L2gridsize: options for Elastic
Net regression if optFUN=&quot;opt2D&quot;.  See ?opt2D.
</p>


<h3>Value</h3>

<p>Returns a vector of cross-validated continuous risk score predictions.
</p>


<h3>Note</h3>

<p>Depends on the R packages: penalized, parallel
</p>


<h3>Author(s)</h3>

<p>Levi Waldron et al.
</p>


<h3>References</h3>

<p>Waldron L, Pintilie M, Tsao M-S, Shepherd FA, Huttenhower C*, Jurisica
I*: Optimized application of penalized regression methods to diverse
genomic data. Bioinformatics 2011, 27:3399-3406.  (*equal contribution)
</p>


<h3>See Also</h3>

<p>opt.splitval
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beer.exprs)
data(beer.survival)

##select just 100 genes to speed computation:
set.seed(1)
beer.exprs.sample &lt;- beer.exprs[sample(1:nrow(beer.exprs), 100), ]

gene.quant &lt;- apply(beer.exprs.sample, 1, quantile, probs = 0.75)
dat.filt &lt;- beer.exprs.sample[gene.quant &gt; log2(100), ]
gene.iqr &lt;- apply(dat.filt, 1, IQR)
dat.filt &lt;- as.matrix(dat.filt[gene.iqr &gt; 0.5, ])
dat.filt &lt;- t(dat.filt)
dat.filt &lt;- data.frame(dat.filt)

library(survival)
surv.obj &lt;- Surv(beer.survival$os, beer.survival$status)

## First, test the regression arguments using functions from
## the penalized package.  I use maxlambda1=5 here to ensure at least
## one non-zero coefficient.
testfit &lt;- penalized::optL1(
  response = surv.obj,
  maxlambda1 = 3,
  penalized = dat.filt,
  fold = 2,
  positive = FALSE,
  standardize = TRUE,
  trace = TRUE
)

## Now pass these arguments to opt.nested.splitval() for cross-validated
## calculation and assessment of risk scores, with the additional
## arguments:
##    outerfold and nprocessors (?opt.nested.crossval)
##    optFUN and scaling (?opt.splitval)
##    setpen and nsim (?opt1D)

## Ideally nsim would be 50, and outerfold and fold would be 10, but the
## values below speed computation 200x compared to these recommended
## values.  Note that here we are using the standardize=TRUE argument of
## optL1 rather than the scaling=TRUE argument of opt.splitval.  These
## two approaches to scaling are roughly equivalent, but the scaling
## approaches are not the same (scaling=TRUE does z-score,
## standardize=TRUE scales to unit central L2 norm), and results will
## not be identical.  Also, using standardize=TRUE scales variables but
## provides coeffients for the original scale, whereas using
## scaling=TRUE scales variables in the training set then applies the
## same scales to the test set.
set.seed(1)
## In this example I use two processors:
preds &lt;-
  pensim::opt.nested.crossval(
    outerfold = 2,
    nprocessors = 1,
    #opt.nested.crossval arguments
    optFUN = "opt1D",
    scaling = FALSE,
    #opt.splitval arguments
    setpen = "L1",
    nsim = 1,
    #opt1D arguments
    response = surv.obj,
    #rest are penalized::optl1 arguments
    penalized = dat.filt,
    fold = 2,
    maxlambda1 = 5,
    positive = FALSE,
    standardize = TRUE,
    trace = FALSE
  )

## We probably also want the coefficients from the model fit on all the
## data, for future use:
beer.coefs &lt;- pensim::opt1D(
  setpen = "L1",
  nsim = 1,
  maxlambda1 = 5,
  response = surv.obj,
  penalized = dat.filt,
  fold = 2,
  positive = FALSE,
  standardize = TRUE,
  trace = FALSE
)

## We can also include unpenalized covariates, if desired.
## Note that when keeping only one variable for a penalized or
## unpenalized covariate, indexing a dataframe like [1] instead of doing
## [,1] preserves the variable name.  With [,1] the variable name gets
## converted to "".

beer.coefs &lt;- pensim::opt1D(
  setpen = "L1",
  nsim = 1,
  maxlambda1 = 5,
  response = surv.obj,
  penalized = dat.filt[-1],
  # This is equivalent to dat.filt[, -1]
  unpenalized = dat.filt[1],
  fold = 2,
  positive = FALSE,
  standardize = TRUE,
  trace = FALSE
)
## (note the non-zero first coefficient this time, due to it being unpenalized).

## Summarization and plotting.
preds.dichot &lt;- preds &gt; median(preds)

coxfit.continuous &lt;- coxph(surv.obj ~ preds)
coxfit.dichot &lt;- coxph(surv.obj ~ preds.dichot)
summary(coxfit.continuous)
summary(coxfit.dichot)

nobs &lt;- length(preds)
cutoff &lt;- 12
if (requireNamespace("survivalROC", quietly = TRUE)) {
preds.roc &lt;-
  survivalROC::survivalROC(
    Stime = beer.survival$os,
    status = beer.survival$status,
    marker = preds,
    predict.time = cutoff,
    span = 0.25 * nobs ^ (-0.20)
  )
 plot(
  preds.roc$FP,
  preds.roc$TP,
  type = "l",
  xlim = c(0, 1),
  ylim = c(0, 1),
  lty = 2,
  xlab = paste("FP", "\n", "AUC = ", round(preds.roc$AUC, 3)),
  ylab = "TP",
  main = "LASSO predictions\n ROC curve at 12 months"
 )
 abline(0, 1)
 }
</code></pre>

<hr>
<h2 id='opt.splitval'>Parallelized calculation of split training/test set predictions from L1/L2/Elastic
Net penalized regression.
</h2><span id='topic+opt.splitval'></span>

<h3>Description</h3>

<p>uses a single training/test split to train a penalized
regression model in the training samples, then use the model to
calculate values of the linear risk score in the test samples.  This
function is used by opt.nested.crossval, but can also be used on its own.
</p>
<p>This function support z-score scaling of training data, and application
of these scaling and shifting coefficients to the test data.  It also
supports repeated tuning of the penalty parameters and selection of the
model with greatest cross-validated likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opt.splitval(optFUN="opt1D",testset="equal",scaling=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="opt.splitval_+3A_optfun">optFUN</code></td>
<td>

<p>&quot;opt1D&quot; for Lasso or Ridge regression, &quot;opt2D&quot; for Elastic Net.  See
the help pages for these functions for additional arguments.
</p>
</td></tr>
<tr><td><code id="opt.splitval_+3A_testset">testset</code></td>
<td>

<p>For the opt.splitval function ONLY.
&quot;equal&quot; for randomly assigned equal training and test sets, or an
integer vector defining the positions of the test samples in the
response, penalized, and unpenalized arguments which are passed to
the optL1, optL2, or cvl functions of the penalized R package.
</p>
</td></tr>
<tr><td><code id="opt.splitval_+3A_scaling">scaling</code></td>
<td>

<p>If TRUE, each feature (column) of the training samples (in
matrix/dataframe specified by the penalized argument) are scaled to
z-scores, then these scaling and shifting factors are applied to the
test data.
If FALSE, no scaling is done.
</p>
</td></tr>
<tr><td><code id="opt.splitval_+3A_...">...</code></td>
<td>

<p>Additional arguments are required, to be passed to the
optL1 or optL2 function of the penalized R package.  See those help
pages, and it may be desirable to test these arguments directly on
optL1 or optL2 before using this more CPU-consuming and complex
function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function does split sample model training and testing for a
single split of the data, using the optL1 or optL2 functions
of the penalized R package, for each
iteration of the cross-validation.  Scaling of the test samples is
done independently, using scale factors determined from the training
samples.  Repeated starts of model training can be parallelized as
documented in the opt1D and opt2D functions.  This function is used
for nested cross-validation by the opt.nested.crossval function.
</p>


<h3>Value</h3>

<p>Returns a vector of cross-validated continuous risk score predictions.
</p>


<h3>Note</h3>

<p>Depends on the R packages: penalized, parallel, rlecuyer
</p>


<h3>Author(s)</h3>

<p>Levi Waldron et al.
</p>


<h3>References</h3>

<p>Waldron L, Pintilie M, Tsao M-S, Shepherd FA, Huttenhower C*, Jurisica
I*: Optimized application of penalized regression methods to diverse
genomic data. Bioinformatics 2011, 27:3399-3406.  (*equal contribution)
</p>


<h3>See Also</h3>

<p>opt1D, opt2D, opt.nested.crossval
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beer.exprs)
data(beer.survival)

## select just 250 genes to speed computation:
set.seed(1)
beer.exprs.sample &lt;- beer.exprs[sample(1:nrow(beer.exprs), 250), ]

gene.quant &lt;- apply(beer.exprs.sample, 1, quantile, probs = 0.75)
dat.filt &lt;- beer.exprs.sample[gene.quant &gt; log2(100),]
gene.iqr &lt;- apply(dat.filt, 1, IQR)
dat.filt &lt;- as.matrix(dat.filt[gene.iqr &gt; 0.5,])
dat.filt &lt;- t(dat.filt)

library(survival)
surv.obj &lt;- Surv(beer.survival$os, beer.survival$status)

## Single split training/test evaluation.  Ideally nsim would be 50 and
## fold=10, but this requires 100x more resources.
set.seed(1)
preds50 &lt;- opt.splitval(
  optFUN = "opt1D",
  scaling = TRUE,
  testset = "equal",
  setpen = "L1",
  nsim = 1,
  nprocessors = 1,
  response = surv.obj,
  penalized = dat.filt,
  fold = 5,
  positive = FALSE,
  standardize = FALSE,
  trace = FALSE
)

preds50.dichot &lt;- preds50 &gt; median(preds50)

surv.obj.50 &lt;-
  surv.obj[match(names(preds50), rownames(beer.survival))]
coxfit50.continuous &lt;- coxph(surv.obj.50 ~ preds50)
coxfit50.dichot &lt;- coxph(surv.obj.50 ~ preds50.dichot)
summary(coxfit50.continuous)
summary(coxfit50.dichot)
</code></pre>

<hr>
<h2 id='opt1D'>Parallelized repeated tuning of Lasso or Ridge penalty parameter
</h2><span id='topic+opt1D'></span>

<h3>Description</h3>

<p>This function is a wrapper to the optL1 and optL2 functions of the
penalized R package, useful for parallelized repeated tuning of the
penalty parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opt1D(nsim = 50, nprocessors = 1, setpen = "L1", cl = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="opt1D_+3A_nsim">nsim</code></td>
<td>

<p>Number of times to repeat the simulation (around 50 is suggested)
</p>
</td></tr>
<tr><td><code id="opt1D_+3A_nprocessors">nprocessors</code></td>
<td>

<p>An integer number of processors to use.
</p>
</td></tr>
<tr><td><code id="opt1D_+3A_setpen">setpen</code></td>
<td>

<p>Either &quot;L1&quot; (Lasso) or &quot;L2&quot; (Ridge) penalty
</p>
</td></tr>
<tr><td><code id="opt1D_+3A_cl">cl</code></td>
<td>

<p>Optional cluster object created with the makeCluster() function of
the parallel package.  If this is not set, pensim calls
makeCluster(nprocessors, type=&quot;SOCK&quot;).   Setting this parameter
can enable parallelization in more diverse scenarios than multi-core
desktops; see the documentation for the parallel package.  Note that if
cl is user-defined, this function will not automatically run
parallel::stopCluster() to shut down the cluster.
</p>
</td></tr>
<tr><td><code id="opt1D_+3A_...">...</code></td>
<td>

<p>arguments passed on to optL1 or optL2 function of the penalized R package
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function sets up a SNOW (Simple Network of Workstations) &quot;sock&quot;
cluster to parallelize the task of repeated tunings the L1 or L2 penalty
parameter.  Tuning of the penalty parameters is done by the optL1 or
optL2 functions of the penalized R package.
</p>


<h3>Value</h3>

<p>Returns a matrix with the following columns:
</p>
<table>
<tr><td><code>L1 (or L2)</code></td>
<td>
<p>optimized value of the penalty parameter</p>
</td></tr>
<tr><td><code>cvl</code></td>
<td>
<p>optimized cross-validated likelihood</p>
</td></tr>
<tr><td><code>coef_1</code>, <code>coef_2</code>, <code>...</code>, <code>coef_n</code></td>
<td>
<p>argmax coefficients for the model
with this value of the tuning parameter</p>
</td></tr>
</table>
<p>The matrix contains one row for each repeat of the regression.
</p>


<h3>Note</h3>

<p>Depends on the R packages: penalized, parallel, rlecuyer
</p>


<h3>Author(s)</h3>

<p>Levi Waldron et al.
</p>


<h3>References</h3>

<p>Waldron L, Pintilie M, Tsao M-S, Shepherd FA, Huttenhower C*, Jurisica
I*: Optimized application of penalized regression methods to diverse
genomic data. Bioinformatics 2011, 27:3399-3406.  (*equal contribution)
</p>


<h3>See Also</h3>

<p>optL1, optL2
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beer.exprs)
data(beer.survival)

##select just 100 genes to speed computation:
set.seed(1)
beer.exprs.sample &lt;- beer.exprs[sample(1:nrow(beer.exprs), 100),]

gene.quant &lt;- apply(beer.exprs.sample, 1, quantile, probs = 0.75)
dat.filt &lt;- beer.exprs.sample[gene.quant &gt; log2(100),]
gene.iqr &lt;- apply(dat.filt, 1, IQR)
dat.filt &lt;- as.matrix(dat.filt[gene.iqr &gt; 0.5,])
dat.filt &lt;- t(dat.filt)

##define training and test sets
set.seed(1)
trainingset &lt;- sample(rownames(dat.filt), round(nrow(dat.filt) / 2))
testset &lt;-
  rownames(dat.filt)[!rownames(dat.filt) %in% trainingset]

dat.training &lt;- data.frame(dat.filt[trainingset, ])
pheno.training &lt;- beer.survival[trainingset, ]

library(survival)
surv.training &lt;- Surv(pheno.training$os, pheno.training$status)

dat.test &lt;- data.frame(dat.filt[testset, ])
all.equal(colnames(dat.training), colnames(dat.test))
pheno.test &lt;- beer.survival[testset, ]
surv.test &lt;- Surv(pheno.test$os, pheno.test$status)

##ideally nsim should be on the order of 50,  but this slows computation
##50x without parallelization.
set.seed(1)
output &lt;-
  pensim::opt1D(
    nsim = 1,
    nprocessors = 1,
    setpen = "L2",
    response = surv.training,
    penalized = dat.training,
    fold = 3,
    positive = FALSE,
    standardize = TRUE,
    minlambda2 = 1,
    maxlambda2 = 100
  )

cc &lt;- output[which.max(output[, "cvl"]),-(1:2)]  #coefficients
sum(abs(cc) &gt; 0)  #count non-zero coefficients

preds.training &lt;- as.matrix(dat.training) %*% cc
preds.training.median &lt;- median(preds.training)
preds.training.dichot &lt;-
  ifelse(preds.training &gt; preds.training.median, "high risk", "low risk")
preds.training.dichot &lt;-
  factor(preds.training.dichot[, 1], levels = c("low risk", "high risk"))
preds.test &lt;- as.matrix(dat.test) %*% cc
preds.test.dichot &lt;-
  ifelse(preds.test &gt; preds.training.median, "high risk", "low risk")
preds.test.dichot &lt;-
  factor(preds.test.dichot[, 1], levels = c("low risk", "high risk"))

coxphfit.training &lt;- coxph(surv.training ~ preds.training.dichot)
survfit.training &lt;- survfit(surv.training ~ preds.training.dichot)
summary(coxphfit.training)
coxphfit.test &lt;- coxph(surv.test ~ preds.test.dichot)
survfit.test &lt;- survfit(surv.test ~ preds.test.dichot)
summary(coxphfit.test)

(p.training &lt;-
    signif(summary(coxphfit.training)$logtest[3], 2))  #likelihood ratio test
(hr.training &lt;- signif(summary(coxphfit.training)$conf.int[1], 2))
(hr.lower.training &lt;- summary(coxphfit.training)$conf.int[3])
(hr.upper.training &lt;- summary(coxphfit.training)$conf.int[4])
par(mfrow = c(1, 2))
plot(
  survfit.training,
  col = c("black", "red"),
  conf.int = FALSE,
  xlab = "Months",
  main = "TRAINING",
  ylab = "Overall survival"
)
xmax &lt;- par("usr")[2] - 50
text(
  x = xmax,
  y = 0.4,
  lab = paste("HR=", hr.training),
  pos = 2
)
text(
  x = xmax,
  y = 0.3,
  lab = paste("p=", p.training, "", sep = ""),
  pos = 2
)
tmp &lt;- summary(preds.training.dichot)
text(
  x = c(xmax, xmax),
  y = c(0.2, 0.1),
  lab = paste(tmp, names(tmp)),
  col = 1:2,
  pos = 2
)
(p.test &lt;-
    signif(summary(coxphfit.test)$logtest[3], 2))  #likelihood ratio test
(hr.test &lt;- signif(summary(coxphfit.test)$conf.int[1], 2))
(hr.lower.test &lt;- summary(coxphfit.test)$conf.int[3])
(hr.upper.test &lt;- summary(coxphfit.test)$conf.int[4])
plot(
  survfit.test,
  col = c("black", "red"),
  conf.int = FALSE,
  xlab = "Months",
  main = "TEST"
)
text(
  x = xmax,
  y = 0.4,
  lab = paste("HR=", hr.test),
  pos = 2
)
text(
  x = xmax,
  y = 0.3,
  lab = paste("p=", p.test, "", sep = ""),
  pos = 2
)
tmp &lt;- summary(preds.test.dichot)
text(
  x = c(xmax, xmax),
  y = c(0.2, 0.1),
  lab = paste(tmp, names(tmp)),
  col = 1:2,
  pos = 2
)
</code></pre>

<hr>
<h2 id='opt2D'>
Parallelized, two-dimensional tuning of Elastic Net L1/L2 penalties
</h2><span id='topic+opt2D'></span>

<h3>Description</h3>

<p>This function implements parallelized two-dimensional optimization of Elastic Net
penalty parameters.  This is accomplished by scanning a regular grid
of L1/L2 penalties, then using the top five CVL penalty combinations
from this grid as starting points for the convex optimization problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opt2D(nsim,
      L1range = c(0.001, 100),
      L2range = c(0.001, 100),
      dofirst = "both",
      nprocessors = 1,
      L1gridsize = 10, L2gridsize = 10,
      cl = NULL,
      ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="opt2D_+3A_nsim">nsim</code></td>
<td>

<p>Number of times to repeat the simulation (around 50 is suggested)
</p>
</td></tr>
<tr><td><code id="opt2D_+3A_l1range">L1range</code></td>
<td>

<p>numeric vector of length two, giving minimum and maximum constraints
on the L1 penalty
</p>
</td></tr>
<tr><td><code id="opt2D_+3A_l2range">L2range</code></td>
<td>

<p>numeric vector of length two, giving minimum and maximum constraints
on the L2 penalty
</p>
</td></tr>
<tr><td><code id="opt2D_+3A_dofirst">dofirst</code></td>
<td>

<p>&quot;L1&quot; to optimize L1 followed by L2, &quot;L2&quot; to optimize L2 followed by
L1, or &quot;both&quot; to optimize both simultaneously in a two-dimensional optimization.
</p>
</td></tr>
<tr><td><code id="opt2D_+3A_nprocessors">nprocessors</code></td>
<td>

<p>An integer number of processors to use.
</p>
</td></tr>
<tr><td><code id="opt2D_+3A_l1gridsize">L1gridsize</code></td>
<td>

<p>Number of values of the L1 penalty in the regular grid of L1/L2 penalties
</p>
</td></tr>
<tr><td><code id="opt2D_+3A_l2gridsize">L2gridsize</code></td>
<td>

<p>Number of values of the L2 penalty in the regular grid of L1/L2 penalties
</p>
</td></tr>
<tr><td><code id="opt2D_+3A_cl">cl</code></td>
<td>

<p>Optional cluster object created with the makeCluster() function of
the parallel package.  If this is not set, pensim calls
makeCluster(nprocessors, type=&quot;SOCK&quot;).   Setting this parameter
can enable parallelization in more diverse scenarios than multi-core
desktops; see the documentation for the parallel package.  Note that if
cl is user-defined, this function will not automatically run
parallel::stopCluster() to shut down the cluster.
</p>
</td></tr>
<tr><td><code id="opt2D_+3A_...">...</code></td>
<td>

<p>arguments passed on to optL1 and optL2 (dofirst=&quot;L1&quot; or &quot;L2&quot;), or
cvl (dofirst=&quot;both&quot;) functions of the penalized R package
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function sets up a SNOW (Simple Network of Workstations) &quot;sock&quot;
cluster to parallelize the task of repeated tunings the Elastic Net
penalty parameters.  Three methods are implemented, as described by
Waldron et al. (2011): lambda1 followed by lambda2 (lambda1-lambda2),
lambda2 followed by lambda1 (lambda2-lambda1), and lambda1 with
lambda2 simultaneously (lambda1+lambda2).  Tuning of the penalty
parameters is done by the optL1 or optL2 functions of the penalized R
package.
</p>


<h3>Value</h3>

<p>Returns a matrix with the following columns:
</p>
<table>
<tr><td><code>L1</code></td>
<td>
<p>optimized value of the L1 penalty parameter</p>
</td></tr>
<tr><td><code>L2</code></td>
<td>
<p>optimized value of the L2 penalty parameter</p>
</td></tr>
<tr><td><code>cvl</code></td>
<td>
<p>optimized cross-validated likelihood</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>0 if the optimization converged, non-zero otherwise
(see stats:optim for details)</p>
</td></tr>
<tr><td><code>fncalls</code></td>
<td>
<p>number of calls to cvl function during optimization</p>
</td></tr>
<tr><td><code>coef_1</code>, <code>coef_2</code>, <code>...</code>, <code>coef_n</code></td>
<td>
<p>argmax coefficients for the model
with this value of the tuning parameter</p>
</td></tr>
</table>
<p>The matrix contains one row for each repeat of the regression.
</p>


<h3>Note</h3>

<p>Depends on the R packages: penalized, parallel, rlecuyer
</p>


<h3>Author(s)</h3>

<p>Levi Waldron et al.
</p>


<h3>References</h3>

<p>Waldron L, Pintilie M, Tsao M-S, Shepherd FA, Huttenhower C*, Jurisica
I*: Optimized application of penalized regression methods to diverse
genomic data. Bioinformatics 2011, 27:3399-3406.  (*equal contribution)
</p>


<h3>See Also</h3>

<p>optL1, optL2, cvl
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beer.exprs)
data(beer.survival)

## Select just 100 genes to speed computation:
set.seed(1)
beer.exprs.sample &lt;- beer.exprs[sample(1:nrow(beer.exprs), 100),]

## Apply an unreasonably strict gene filter here to speed computation
## time for the Elastic Net example.
gene.quant &lt;- apply(beer.exprs.sample, 1, quantile, probs = 0.75)
dat.filt &lt;- beer.exprs.sample[gene.quant &gt; log2(150),]
gene.iqr &lt;- apply(dat.filt, 1, IQR)
dat.filt &lt;- as.matrix(dat.filt[gene.iqr &gt; 1,])
dat.filt &lt;- t(dat.filt)

## Define training and test sets
set.seed(9)
trainingset &lt;- sample(rownames(dat.filt), round(nrow(dat.filt) / 2))
testset &lt;-
  rownames(dat.filt)[!rownames(dat.filt) %in% trainingset]

dat.training &lt;- data.frame(dat.filt[trainingset,])
pheno.training &lt;- beer.survival[trainingset,]

library(survival)
surv.training &lt;- Surv(pheno.training$os, pheno.training$status)

dat.test &lt;- data.frame(dat.filt[testset,])
all.equal(colnames(dat.training), colnames(dat.test))
pheno.test &lt;- beer.survival[testset,]
surv.test &lt;- Surv(pheno.test$os, pheno.test$status)

set.seed(1)
##ideally set nsim=50, fold=10, but this takes 100x longer.
system.time(
  output &lt;- opt2D(
    nsim = 1,
    L1range = c(0.1, 1),
    L2range = c(20, 1000),
    dofirst = "both",
    nprocessors = 1,
    response = surv.training,
    penalized = dat.training,
    fold = 5,
    positive = FALSE,
    standardize = TRUE
  )
)

cc &lt;- output[which.max(output[, "cvl"]),-1:-5]
output[which.max(output[, "cvl"]), 1:5]  #small L1, large L2
sum(abs(cc) &gt; 0)  #number of non-zero coefficients

preds.training &lt;- as.matrix(dat.training) %*% cc
preds.training.median &lt;- median(preds.training)
preds.training.dichot &lt;-
  ifelse(preds.training &gt; preds.training.median, "high risk", "low risk")
preds.training.dichot &lt;-
  factor(preds.training.dichot[, 1], levels = c("low risk", "high risk"))
preds.test &lt;- as.matrix(dat.test) %*% cc
preds.test.dichot &lt;-
  ifelse(preds.test &gt; preds.training.median, "high risk", "low risk")
preds.test.dichot &lt;-
  factor(preds.test.dichot[, 1], levels = c("low risk", "high risk"))

coxphfit.training &lt;- coxph(surv.training ~ preds.training.dichot)
survfit.training &lt;- survfit(surv.training ~ preds.training.dichot)
summary(coxphfit.training)
coxphfit.test &lt;- coxph(surv.test ~ preds.test.dichot)
survfit.test &lt;- survfit(surv.test ~ preds.test.dichot)
summary(coxphfit.test)

(p.training &lt;-
    signif(summary(coxphfit.training)$logtest[3], 2))  #likelihood ratio test
(hr.training &lt;- signif(summary(coxphfit.training)$conf.int[1], 2))
(hr.lower.training &lt;- summary(coxphfit.training)$conf.int[3])
(hr.upper.training &lt;- summary(coxphfit.training)$conf.int[4])
par(mfrow = c(1, 2))
plot(
  survfit.training,
  col = c("black", "red"),
  conf.int = FALSE,
  xlab = "Months",
  main = "TRAINING",
  ylab = "Overall survival"
)
xmax &lt;- par("usr")[2] - 50
text(
  x = xmax,
  y = 0.4,
  lab = paste("HR=", hr.training),
  pos = 2
)
text(
  x = xmax,
  y = 0.3,
  lab = paste("p=", p.training, "", sep = ""),
  pos = 2
)
tmp &lt;- summary(preds.training.dichot)
text(
  x = xmax,
  y = c(0.2, 0.1),
  lab = paste(tmp, names(tmp)),
  col = 1:2,
  pos = 2
)
## Now the test set.
## in the test set,  HR=1.7 is not significant - not surprising with the
## overly strict non-specific pre-filter (IQR&gt;1,  75th percentile &gt; log2(150)
(p.test &lt;-
    signif(summary(coxphfit.test)$logtest[3], 2))  #likelihood ratio test
(hr.test &lt;- signif(summary(coxphfit.test)$conf.int[1], 2))
(hr.lower.test &lt;- summary(coxphfit.test)$conf.int[3])
(hr.upper.test &lt;- summary(coxphfit.test)$conf.int[4])
plot(
  survfit.test,
  col = c("black",  "red"),
  conf.int = FALSE,
  xlab = "Months",
  main = "TEST"
)
text(
  x = xmax,
  y = 0.4,
  lab = paste("HR=", hr.test),
  pos = 2
)
text(
  x = xmax,
  y = 0.3,
  lab = paste("p=", p.test, "", sep = ""),
  pos = 2
)
tmp &lt;- summary(preds.test.dichot)
text(
  x = xmax,
  y = c(0.2, 0.1),
  lab = paste(tmp, names(tmp)),
  col = 1:2,
  pos = 2
)
</code></pre>

<hr>
<h2 id='pensim-package'>
Functions and data for simulation of high-dimensional data and parallelized repeated penalized regression
</h2><span id='topic+pensim-package'></span><span id='topic+pensim'></span>

<h3>Description</h3>

<p>Simulation of continuous, correlated high-dimensional data with
time-to-event or binary response, and parallelized functions for Lasso,
Ridge, and Elastic Net penalized regression model training and
validation by split-sample or nested cross-validation.  See the help
page for opt.nested.crossval() for the most extensive usage examples.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> pensim</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;=2)</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Model training and validation by Lasso, Ridge, and Elastic Net penalized
regression.   This package also contains a function for simulation of correlated
high-dimensional data with binary or time-to-event response.
</p>


<h3>Author(s)</h3>

<p>Levi Waldron
</p>
<p>Maintainer: Levi Waldron &lt;lwaldron.research@gmail.com&gt;
</p>


<h3>References</h3>

<p>Waldron L, Pintilie M, Tsao M-S, Shepherd FA, Huttenhower C*, Jurisica
I*: Optimized application of penalized regression methods to diverse
genomic data. Bioinformatics 2011, 27:3399-3406.  (*equal contribution)
</p>


<h3>See Also</h3>

<p>penalized-package
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(9)
## 
## create some data,  with one of a group of five correlated variables
## having an association with the binary outcome:
## 
x &lt;- create.data(
  nvars = c(10, 3),
  cors = c(0, 0.8),
  associations = c(0, 2),
  firstonly = c(TRUE, TRUE),
  nsamples = 50,
  response = "binary",
  logisticintercept = 0.5
)
x$summary
##
##predictor data frame and binary response vector
##
pen.data &lt;- x$data[, -match("outcome", colnames(x$data))]
response &lt;- x$data[, match("outcome", colnames(x$data))]
## lasso regression.  Note that epsilon=1e-2 is passed onto optL1,  and
## reduces the precision of the tuning compared to the default 1e-10.
output &lt;-
  opt1D(
    nsim = 1,
    nprocessors = 1,
    penalized = pen.data,
    response = response,
    epsilon = 1e-2
  )
cc &lt;-
  output[which.max(output[, "cvl"]), -1:-3]  ##non-zero b.* are true positives
</code></pre>

<hr>
<h2 id='scan.l1l2'>
Function calculate cross-validated likelihood on a regular grid of L1/L2 penalties
</h2><span id='topic+scan.l1l2'></span>

<h3>Description</h3>

<p>This function generates a grid of values of L1/L2 penalties,
then calculated cross-validated likelihood at each point on the grid.
The grid can be regular (linear progression of the penalty values), or
polynomial (finer grid for small penalty values, and coarser grid for
larger penalty values).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scan.l1l2(L1range = c(0.1, 100.1),
          L2range = c(0.1, 100.1),
          L1.ngrid = 50,
          L2.ngrid = 50,
          nprocessors = 1,
          polydegree = 1,
          cl = NULL,
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scan.l1l2_+3A_l1range">L1range</code></td>
<td>

<p>numeric vector of length two, giving minimum and maximum constraints
on the L1 penalty
</p>
</td></tr>
<tr><td><code id="scan.l1l2_+3A_l2range">L2range</code></td>
<td>

<p>numeric vector of length two, giving minimum and maximum constraints
on the L2 penalty
</p>
</td></tr>
<tr><td><code id="scan.l1l2_+3A_l1.ngrid">L1.ngrid</code></td>
<td>

<p>Number of values of the L1 penalty in the regular grid of L1/L2 penalties
</p>
</td></tr>
<tr><td><code id="scan.l1l2_+3A_l2.ngrid">L2.ngrid</code></td>
<td>

<p>Number of values of the L2 penalty in the regular grid of L1/L2 penalties
</p>
</td></tr>
<tr><td><code id="scan.l1l2_+3A_nprocessors">nprocessors</code></td>
<td>

<p>An integer number of processors to use.
</p>
</td></tr>
<tr><td><code id="scan.l1l2_+3A_polydegree">polydegree</code></td>
<td>

<p>power of the polynomial on which the L1/L2 penalty values are fit.
ie if polydegree=2, penalty values could be y=x^2, x=1,2,3,..., so y=1,4,9,...
</p>
</td></tr>
<tr><td><code id="scan.l1l2_+3A_cl">cl</code></td>
<td>

<p>Optional cluster object created with the makeCluster() function of
the parallel package.  If this is not set, pensim calls
makeCluster(nprocessors, type=&quot;SOCK&quot;).   Setting this parameter
can enable parallelization in more diverse scenarios than multi-core
desktops; see the documentation for the parallel package.  Note that if
cl is user-defined, this function will not automatically run
parallel::stopCluster() to shut down the cluster.
</p>
</td></tr>
<tr><td><code id="scan.l1l2_+3A_...">...</code></td>
<td>

<p>arguments passed on to cvl function of the penalized R package
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function sets up a SNOW (Simple Network of Workstations) &quot;sock&quot;
cluster to parallelize the task of scanning a grid of penalty values
to search for suitable starting values for two-dimensional
optimization of the Elastic Net.
</p>


<h3>Value</h3>

<table>
<tr><td><code>cvl</code></td>
<td>
<p>matrix of cvl values along the grid</p>
</td></tr>
<tr><td><code>L1range</code></td>
<td>
<p>range of L1 penalties to scan</p>
</td></tr>
<tr><td><code>L2range</code></td>
<td>
<p>range of L2 penalties to scan</p>
</td></tr>
<tr><td><code>xlab</code></td>
<td>
<p>A text string indicating the range of L1 penalties</p>
</td></tr>
<tr><td><code>ylab</code></td>
<td>
<p>A text string giving the range of L2 penalties</p>
</td></tr>
<tr><td><code>zlab</code></td>
<td>
<p>A text string giving the range of cvl values</p>
</td></tr>
<tr><td><code>note</code></td>
<td>
<p>A note to the user that rows of cvl correspond to values of lambda1, columns to lambda2</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Depends on the R packages: penalized, parallel, rlecuyer
</p>


<h3>Author(s)</h3>

<p>Levi Waldron et al.
</p>


<h3>References</h3>

<p>Waldron L, Pintilie M, Tsao M-S, Shepherd FA, Huttenhower C*, Jurisica
I*: Optimized application of penalized regression methods to diverse
genomic data. Bioinformatics 2011, 27:3399-3406.  (*equal contribution)
</p>


<h3>See Also</h3>

<p>cvl
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beer.exprs)
data(beer.survival)

##select just 250 genes to speed computation:
set.seed(1)
beer.exprs.sample &lt;- beer.exprs[sample(1:nrow(beer.exprs), 250), ]

gene.quant &lt;- apply(beer.exprs.sample, 1, quantile, probs = 0.75)
dat.filt &lt;- beer.exprs.sample[gene.quant &gt; log2(150), ]
gene.iqr &lt;- apply(dat.filt, 1, IQR)
dat.filt &lt;- as.matrix(dat.filt[gene.iqr &gt; 1, ])
dat.filt &lt;- t(dat.filt)

## Define training and test sets
set.seed(9)
trainingset &lt;- sample(rownames(dat.filt), round(nrow(dat.filt) / 2))
testset &lt;- rownames(dat.filt)[!rownames(dat.filt) %in% trainingset]

dat.training &lt;- data.frame(dat.filt[trainingset, ])
pheno.training &lt;- beer.survival[trainingset, ]

library(survival)
surv.training &lt;- Surv(pheno.training$os, pheno.training$status)

dat.test &lt;- data.frame(dat.filt[testset, ])
all.equal(colnames(dat.training), colnames(dat.test))
pheno.test &lt;- beer.survival[testset, ]
surv.test &lt;- Surv(pheno.test$os, pheno.test$status)

set.seed(9)
system.time(
  output &lt;- scan.l1l2(
    L1range = c(0.2, 3.2),
    L2range = c(2, 30),
    L1.ngrid = 10,
    L2.ngrid = 10,
    polydegree = 1,
    nprocessors = 1,
    response = surv.training,
    penalized = dat.training,
    fold = 4,
    positive = FALSE,
    standardize = TRUE
  )
)

##Note that the cvl surface is not smooth because a different folding of
##the data was used for each cvl calculation
image(
  x = seq(output$L1range[1], output$L1range[2], length.out = nrow(output$cvl)),
  y = seq(output$L2range[1], output$L2range[2], length.out = ncol(output$cvl)),
  z = output$cvl,
  xlab = "lambda1",
  ylab = "lambda2",
  main = "red is higher cross-validated likelihood"
)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
