<!DOCTYPE html><html><head><title>Help for package SeaVal</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SeaVal}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#SeaVal-package'><p>SeaVal: Validation of Seasonal Weather Forecasts</p></a></li>
<li><a href='#add_climatology'><p>Add climatology to a data table</p></a></li>
<li><a href='#add_country'><p>Same as add_country_names</p></a></li>
<li><a href='#add_country_names'><p>Add country names to a data table with lon/lat coordinates</p></a></li>
<li><a href='#add_tercile_cat'><p>Add a tercile-category column to a data table</p></a></li>
<li><a href='#add_tercile_probs'><p>Add tercile probabilities to ensemble forecasts</p></a></li>
<li><a href='#are_all_elements_within_eps'><p>Check if all elements of x are within tolerance eps of any element in y</p></a></li>
<li><a href='#by_cols_ens_fc_score'><p>Auxiliary function</p></a></li>
<li><a href='#by_cols_terc_fc_score'><p>Auxiliary function</p></a></li>
<li><a href='#by_cols_terc_fc_score_sp'><p>Auxiliary function</p></a></li>
<li><a href='#checks_ens_fc_score'><p>Auxiliary function for scores of ensemble forecasts.</p></a></li>
<li><a href='#checks_terc_fc_score'><p>Auxiliary function for scores for tercile forecasts.</p></a></li>
<li><a href='#chirps_dir'><p>CHIRPS directory</p></a></li>
<li><a href='#chirps_monthly'><p>Monthly mean precipitation</p></a></li>
<li><a href='#chirps_ver_map_quantiles'><p>Calculates and saves the quantiles of CHIRPS data required for verification maps.</p></a></li>
<li><a href='#climatology_ens_forecast'><p>Returns a leave-one-year-out climatology-based ensemble forecast</p></a></li>
<li><a href='#climatology_threshold_exceedence'><p>Get climatological prediction for exceedence probabilities.</p></a></li>
<li><a href='#combine'><p>Combine two data tables</p></a></li>
<li><a href='#complete_regular_grid'><p>Expand Regular Spatial Grid</p></a></li>
<li><a href='#convert_monthly_to_seasonal'><p>Convert a data table from monthly to seasonal format</p></a></li>
<li><a href='#CPA'><p>Coefficients of Predictive Ability</p></a></li>
<li><a href='#create_diagram_by_level'><p>Auxiliary function to simplify grouping for diagrams</p></a></li>
<li><a href='#CRPS'><p>Continuous Ranked Probability Score</p></a></li>
<li><a href='#crps_aux'><p>Auxiliary function for calculating crps.</p></a></li>
<li><a href='#crps_aux_esc'><p>Auxiliary function for calculating crps with ensemble size correction by Ferro et al. 2008.</p></a></li>
<li><a href='#CRPSS'><p>Continuous Ranked Probability Skill Score</p></a></li>
<li><a href='#data_dir'><p>Auxiliary function to access and change the directory used to load and save data.</p></a></li>
<li><a href='#delete_redundant_files'><p>Auxiliary function cleaning out the directories, called at the end of the CHIRPS download.</p></a></li>
<li><a href='#dimvars'><p>Get dimension variables</p></a></li>
<li><a href='#disc_score_dt'><p>Generalized Discrimination score</p></a></li>
<li><a href='#DISS'><p>Generalized discrimination score</p></a></li>
<li><a href='#download_chirps_monthly'><p>Download monthly CHIRPS-data</p></a></li>
<li><a href='#download_chirps_monthly_high'><p>Auxiliary function called by download_chirps_monthly</p></a></li>
<li><a href='#download_chirps_monthly_low'><p>Auxiliary function called by download_chirps_monthly</p></a></li>
<li><a href='#download_chirps_prelim_aux'><p>Auxiliary function for downloading the preliminary CHIRPS monthly data</p></a></li>
<li><a href='#dt_to_netcdf'><p>Write a netcdf from a long data table</p></a></li>
<li><a href='#EA_country_names'><p>Get names of countries in east Africa</p></a></li>
<li><a href='#ecmwf_monthly'><p>Monthly mean precipitation forecast example dataset</p></a></li>
<li><a href='#EIR'><p>Effective Interest Rate</p></a></li>
<li><a href='#fc_cols'><p>Forecast column names</p></a></li>
<li><a href='#get_mask'><p>Function to create a mask of dry regions from CHIRPS</p></a></li>
<li><a href='#get_quantiles'><p>Calculate quantiles from a data table</p></a></li>
<li><a href='#get_terciles'><p>get terciles from a data table</p></a></li>
<li><a href='#ggplot_dt'><p>plotting function for spatial data</p></a></li>
<li><a href='#GHA_extent'><p>GHA-bounding-box</p></a></li>
<li><a href='#gha_plot'><p>Plotting function with different map for Greater Horn of Africa</p></a></li>
<li><a href='#grid_info'><p>Retrieve spatial grid information from a data table</p></a></li>
<li><a href='#HS'><p>Hit score</p></a></li>
<li><a href='#HSS'><p>Hit Skill Score</p></a></li>
<li><a href='#IGS'><p>Ignorance Score</p></a></li>
<li><a href='#IGSS'><p>Ignorance Skill score</p></a></li>
<li><a href='#indicator_times_value_aux'><p>Auxiliary function for multiplying two numbers such that 0 x infty is 0. Needed for the ignorance score: 0log(0) should be 0.</p></a></li>
<li><a href='#load_chirps'><p>Function for loading CHIRPS (monthly) data.</p></a></li>
<li><a href='#lt_cols'><p>Data table column names that are recognized as leadtime</p></a></li>
<li><a href='#MB'><p>Multicategory Brier score</p></a></li>
<li><a href='#MBS'><p>Multicategory Brier Skill score</p></a></li>
<li><a href='#modify_dt_map_plotting'><p>Auxiliary function for checking dimensions for map-plotting</p></a></li>
<li><a href='#MSD_to_YM'><p>Converts time given as 'months since date' (MSD) into years and months (YM)</p></a></li>
<li><a href='#MSE'><p>Mean Square Error of ensemble forecasts.</p></a></li>
<li><a href='#MSES'><p>Mean Square Error Skill score</p></a></li>
<li><a href='#netcdf_to_dt'><p>function for converting netcdfs to long data tables.</p></a></li>
<li><a href='#obs_cols'><p>Observation column names</p></a></li>
<li><a href='#obs_dimvars'><p>Auxiliary function returning observation dimvars.</p></a></li>
<li><a href='#PCC'><p>Pearson Correlation Coefficient</p></a></li>
<li><a href='#profit_graph'><p>(Accumulative) profit graphs</p></a></li>
<li><a href='#REL'><p>Reliability score</p></a></li>
<li><a href='#rel_diag'><p>Reliability Diagrams for tercile forecasts</p></a></li>
<li><a href='#rel_diag_vec'><p>Reliability diagram from vectors of probabilities and observations</p></a></li>
<li><a href='#RES'><p>Resolution score</p></a></li>
<li><a href='#restrict_to_country'><p>restricts data to a specified country</p></a></li>
<li><a href='#restrict_to_GHA'><p>restricts data to the Greater Horn of Africa</p></a></li>
<li><a href='#ROC_curve'><p>ROC curve for tercile forecasts</p></a></li>
<li><a href='#roc_curve_vec'><p>ROC curves</p></a></li>
<li><a href='#roc_score_vec'><p>ROC score (AUC)</p></a></li>
<li><a href='#ROCS'><p>ROC-score/Area Under Curve(AUC)</p></a></li>
<li><a href='#round_probs'><p>auxiliary function for rounding probabilities</p></a></li>
<li><a href='#RPS'><p>Ranked Probability score</p></a></li>
<li><a href='#RPSS'><p>Ranked Probability skill score</p></a></li>
<li><a href='#run_dimension_check_ens_fc_score'><p>Auxiliary Function</p></a></li>
<li><a href='#run_dimension_check_terc_forecast'><p>Auxiliary Function</p></a></li>
<li><a href='#season_strings_to_int'><p>Auxiliary function for decoding season-strings</p></a></li>
<li><a href='#set_spatial_grid'><p>Set Spatial Grid Attributes to a Data Table</p></a></li>
<li><a href='#space_dimvars'><p>Auxiliary function</p></a></li>
<li><a href='#SRC'><p>Compute the slope of the reliability curve</p></a></li>
<li><a href='#tc_cols'><p>Tercile column names</p></a></li>
<li><a href='#tendency_diag'><p>Tendency diagram from a data table containing tercile forecasts.</p></a></li>
<li><a href='#tercile_plot'><p>Function for plotting terciles</p></a></li>
<li><a href='#tfc_from_efc'><p>Get tercile probability forecast from ensemble forecasts</p></a></li>
<li><a href='#tfc_gha_plot'><p>Plotting function with different map for Greater Horn of Africa</p></a></li>
<li><a href='#tfc_plot'><p>plotting function for tercile forecasts</p></a></li>
<li><a href='#time_dimvars'><p>Auxiliary function</p></a></li>
<li><a href='#upscale_chirps'><p>Upscales monthly CHIRPS data to a coarser grid</p></a></li>
<li><a href='#upscale_regular_lon_lat'><p>Function for matching data between different grids</p></a></li>
<li><a href='#ver_map'><p>Plot a verification map of percentiles</p></a></li>
<li><a href='#ver_map_chirps'><p>Plot a verification map of percentiles based on precomputed CHIRPS quantiles.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Validation of Seasonal Weather Forecasts</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides tools for processing and evaluating seasonal weather forecasts, 
    with an emphasis on tercile forecasts. We follow the World Meteorological Organization's 
    "Guidance on Verification of Operational Seasonal Climate Forecasts", 
    S.J.Mason (2018, ISBN: 978-92-63-11220-0, URL: <a href="https://library.wmo.int/idurl/4/56227">https://library.wmo.int/idurl/4/56227</a>). 
    The development was supported by the European Union’s Horizon 2020 research and innovation 
    programme under grant agreement no. 869730 (CONFER).
    A comprehensive online tutorial is available at <a href="https://seasonalforecastingengine.github.io/SeaValDoc/">https://seasonalforecastingengine.github.io/SeaValDoc/</a>.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://seasonalforecastingengine.github.io/SeaValDoc/">https://seasonalforecastingengine.github.io/SeaValDoc/</a>,
<a href="https://github.com/SeasonalForecastingEngine/SeaVal">https://github.com/SeasonalForecastingEngine/SeaVal</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10), data.table, ggplot2</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggnewscale, ggplotify, lifecycle, maps, ncdf4, patchwork,
RColorBrewer, scales, stringr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Collate:</td>
<td>'auxiliary_functions.R' 'chirps.R' 'data.R' 'diagrams.R'
'ICPAC_temp.R' 'ncdf_to_dt.R' 'plotting.R' 'quantiles.R'
'scores.R' 'seasonal.R' 'SeaVal-package.R' 'spatial_grids.R'
'upscaling.R' 'utils.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-14 15:00:01 UTC; claudio</td>
</tr>
<tr>
<td>Author:</td>
<td>Claudio Heinrich-Mertsching
    <a href="https://orcid.org/0000-0003-3581-6416"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre,
    cph],
  Celine Cunen [ctb],
  Michael Scheuerer [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Claudio Heinrich-Mertsching &lt;claudio.heinrich@hotmail.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-14 15:20:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='SeaVal-package'>SeaVal: Validation of Seasonal Weather Forecasts</h2><span id='topic+SeaVal'></span><span id='topic+SeaVal-package'></span>

<h3>Description</h3>

<p>Provides tools for processing and evaluating seasonal weather forecasts, with an emphasis on tercile forecasts. We follow the World Meteorological Organization's &quot;Guidance on Verification of Operational Seasonal Climate Forecasts&quot;, S.J.Mason (2018, ISBN: 978-92-63-11220-0, URL: <a href="https://library.wmo.int/idurl/4/56227">https://library.wmo.int/idurl/4/56227</a>). The development was supported by the European Union’s Horizon 2020 research and innovation programme under grant agreement no. 869730 (CONFER). A comprehensive online tutorial is available at <a href="https://seasonalforecastingengine.github.io/SeaValDoc/">https://seasonalforecastingengine.github.io/SeaValDoc/</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Claudio Heinrich-Mertsching <a href="mailto:claudio.heinrich@hotmail.de">claudio.heinrich@hotmail.de</a> (<a href="https://orcid.org/0000-0003-3581-6416">ORCID</a>) [copyright holder]
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Celine Cunen <a href="mailto:celine@nr.no">celine@nr.no</a> [contributor]
</p>
</li>
<li><p> Michael Scheuerer <a href="mailto:scheuerer@nr.no">scheuerer@nr.no</a> [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://seasonalforecastingengine.github.io/SeaValDoc/">https://seasonalforecastingengine.github.io/SeaValDoc/</a>
</p>
</li>
<li> <p><a href="https://github.com/SeasonalForecastingEngine/SeaVal">https://github.com/SeasonalForecastingEngine/SeaVal</a>
</p>
</li></ul>


<hr>
<h2 id='add_climatology'>Add climatology to a data table</h2><span id='topic+add_climatology'></span>

<h3>Description</h3>

<p>The climatology is the average over years (and members for ensemble forecases),
taken separately for each month, season, and coordinate.
By default, the average is taken over all years in the data table,
but you can change this using the years-argument.
By default, climatologies (averages) are calculated for each column that is not
recognized as dimension variable and does not contain characters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_climatology(dt, data_cols = NULL, years = NULL, by = dimvars(dt))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_climatology_+3A_dt">dt</code></td>
<td>
<p>the data table.</p>
</td></tr>
<tr><td><code id="add_climatology_+3A_data_cols">data_cols</code></td>
<td>
<p>For which columns do you want to derive the climatology?
The default i</p>
</td></tr>
<tr><td><code id="add_climatology_+3A_years">years</code></td>
<td>
<p>The average over which years should be considered as climatology.
The default is all years in dt.</p>
</td></tr>
<tr><td><code id="add_climatology_+3A_by">by</code></td>
<td>
<p>column names to group by.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The provided data table with an extra climatology column
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = add_climatology(chirps_monthly)
</code></pre>

<hr>
<h2 id='add_country'>Same as add_country_names</h2><span id='topic+add_country'></span>

<h3>Description</h3>

<p>This is a synonyme for <code><a href="#topic+add_country_names">add_country_names</a></code>.
Following a more intuitive naming convention, that is more in-line
with <code>add_climatology</code> and <code>add_tercile_cat</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_country(dt, regions = EA_country_names())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_country_+3A_dt">dt</code></td>
<td>
<p>the data table.</p>
</td></tr>
<tr><td><code id="add_country_+3A_regions">regions</code></td>
<td>
<p>Character vector of country names for which shapefiles are loaded.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The provided data table with an extra column with country names
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = add_country(chirps_monthly)

</code></pre>

<hr>
<h2 id='add_country_names'>Add country names to a data table with lon/lat coordinates</h2><span id='topic+add_country_names'></span>

<h3>Description</h3>

<p>Takes a data table with lon/lat coordinates and adds a column
'country' to it, containing the name of the country, the coordinate belongs to.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_country_names(dt, regions = EA_country_names())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_country_names_+3A_dt">dt</code></td>
<td>
<p>the data table.</p>
</td></tr>
<tr><td><code id="add_country_names_+3A_regions">regions</code></td>
<td>
<p>Character vector of country names for which shapefiles are loaded.
By default, countries in East Africa are loaded, see <code><a href="#topic+EA_country_names">EA_country_names</a></code>.
If you set regions = '.', the entire world is loaded, but this makes the function slower.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The provided data table with an extra column with country names
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = add_country_names(chirps_monthly)

</code></pre>

<hr>
<h2 id='add_tercile_cat'>Add a tercile-category column to a data table</h2><span id='topic+add_tercile_cat'></span>

<h3>Description</h3>

<p>Given a data table with multiple years of data, this function derives the tercile category per year.
It first derives terciles for the data and then returns, for each row, a -1 if the data falls into
the lowest tercile, 0 if it falls between 1st and second tercile, and +1 if it falls above the third tercile.
Allows grouping by levels (e.g. months and location-coordinates): Tercile categories are derived separately
for each level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_tercile_cat(
  dt,
  datacol = NULL,
  years = NULL,
  by = setdiff(dimvars(dt), c("year", "member"))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_tercile_cat_+3A_dt">dt</code></td>
<td>
<p>the data table.</p>
</td></tr>
<tr><td><code id="add_tercile_cat_+3A_datacol">datacol</code></td>
<td>
<p>Name of the column where the data is stored. If NULL, the function guesses.</p>
</td></tr>
<tr><td><code id="add_tercile_cat_+3A_years">years</code></td>
<td>
<p>Optional, if provided only these years are used for establishing climatology terciles.</p>
</td></tr>
<tr><td><code id="add_tercile_cat_+3A_by">by</code></td>
<td>
<p>names of columns to group by.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The provided data table with an extra column tercile_cat
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dt = add_tercile_cat(chirps_monthly)

</code></pre>

<hr>
<h2 id='add_tercile_probs'>Add tercile probabilities to ensemble forecasts</h2><span id='topic+add_tercile_probs'></span>

<h3>Description</h3>

<p>Adds columns 'below', 'normal' and 'above', containing predicted tercile probabilities, to a data table with ensemble forecasts.
The predicted probability is always the fraction of members ending up in the respective tercile.
The data table should either already have a column 'tercile_cat' (added by <code>add_tercile_cat</code>),
or <code>add_tercile_cat</code> will be run first.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_tercile_probs(dt, f = NULL, by = setdiff(dimvars(dt), "member"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_tercile_probs_+3A_dt">dt</code></td>
<td>
<p>the data table.</p>
</td></tr>
<tr><td><code id="add_tercile_probs_+3A_f">f</code></td>
<td>
<p>name of the column containing the forecast.</p>
</td></tr>
<tr><td><code id="add_tercile_probs_+3A_by">by</code></td>
<td>
<p>names of columns to group by</p>
</td></tr>
<tr><td><code id="add_tercile_probs_+3A_...">...</code></td>
<td>
<p>passed on to <code>add_tercile_cat</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The provided data table, with added columns 'above', 'normal', and 'below'
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dt = add_tercile_probs(ecmwf_monthly)


</code></pre>

<hr>
<h2 id='are_all_elements_within_eps'>Check if all elements of x are within tolerance eps of any element in y</h2><span id='topic+are_all_elements_within_eps'></span>

<h3>Description</h3>

<p>Auxiliary function, used for checking whether spatial grids are regular, with allowing for rounding errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>are_all_elements_within_eps(x, y, eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="are_all_elements_within_eps_+3A_x">x</code></td>
<td>
<p>A numeric vector, sorted in increasing order.</p>
</td></tr>
<tr><td><code id="are_all_elements_within_eps_+3A_y">y</code></td>
<td>
<p>A numeric vector, sorted in increasing order.</p>
</td></tr>
<tr><td><code id="are_all_elements_within_eps_+3A_eps">eps</code></td>
<td>
<p>The tolerance within which we consider two values to be equal.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A boolean value, TRUE if all x are in y within tolerance eps, FALSE otherwise.
</p>

<hr>
<h2 id='by_cols_ens_fc_score'>Auxiliary function</h2><span id='topic+by_cols_ens_fc_score'></span>

<h3>Description</h3>

<p>returns the default column names to group by when calculating scores of
ensemble forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>by_cols_ens_fc_score(dt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="by_cols_ens_fc_score_+3A_dt">dt</code></td>
<td>
<p>optional. You can provide a data table, then the function
returns the names of grouping variables in this data table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of characters with the column names.
</p>

<hr>
<h2 id='by_cols_terc_fc_score'>Auxiliary function</h2><span id='topic+by_cols_terc_fc_score'></span>

<h3>Description</h3>

<p>returns the default column names to group by when calculating scores for
tercile forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>by_cols_terc_fc_score(dt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="by_cols_terc_fc_score_+3A_dt">dt</code></td>
<td>
<p>optional. You can provide a data table, then the function returns
the names of grouping variables in this data table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of characters with the column names.
</p>

<hr>
<h2 id='by_cols_terc_fc_score_sp'>Auxiliary function</h2><span id='topic+by_cols_terc_fc_score_sp'></span>

<h3>Description</h3>

<p>Gets column names to group by when calculating scores for tercile forecasts.
Some tercile forecasts, such as ROC score or SRC (slope of reliability curve)
require many data points and should therefore be pooled in space.
This auxiliary function returns the default column names to group by for these scores.
The suffix _sp stands for spatial pooling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>by_cols_terc_fc_score_sp(dt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="by_cols_terc_fc_score_sp_+3A_dt">dt</code></td>
<td>
<p>optional. You can provide a data table, then the function returns the names
of grouping variables in this data table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of characters with the column names.
</p>

<hr>
<h2 id='checks_ens_fc_score'>Auxiliary function for scores of ensemble forecasts.</h2><span id='topic+checks_ens_fc_score'></span>

<h3>Description</h3>

<p>Checks whether the data table contains columns with names that are not allowed,
or whether it is missing columns that are required.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checks_ens_fc_score()
</code></pre>

<hr>
<h2 id='checks_terc_fc_score'>Auxiliary function for scores for tercile forecasts.</h2><span id='topic+checks_terc_fc_score'></span>

<h3>Description</h3>

<p>Checks whether the data table contains columns with names that are not allowed,
or whether it is missing columns that are required.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checks_terc_fc_score()
</code></pre>

<hr>
<h2 id='chirps_dir'>CHIRPS directory</h2><span id='topic+chirps_dir'></span>

<h3>Description</h3>

<p>Auxiliary function to access/set the directory for loading and saving CHIRPS data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chirps_dir(dir = file.path(data_dir(), "CHIRPS"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chirps_dir_+3A_dir">dir</code></td>
<td>
<p>The directory</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The directory path.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive()){chirps_dir()}

</code></pre>

<hr>
<h2 id='chirps_monthly'>Monthly mean precipitation</h2><span id='topic+chirps_monthly'></span>

<h3>Description</h3>

<p>This dataset contains observed monthly mean precipitation for the greater horn of Africa,
for November - December 1991-2020. The unit of precipitation is mm/day. It also contains the tercile category, where -1
means below normal rainfall (lowest tercile for this location and month), 0
is normal and 1 is above normal.The data source is CHIRPS-blended, upscaled
to a half-degree grid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(chirps_monthly)
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.table</code> (inherits from <code>data.frame</code>) with 209040 rows and 6 columns.
</p>


<h3>Source</h3>

<p><a href="http://iridl.ldeo.columbia.edu/SOURCES/.UCSB/.CHIRPS/.v2p0/.monthly/.global/.precipitation/">http://iridl.ldeo.columbia.edu/SOURCES/.UCSB/.CHIRPS/.v2p0/.monthly/.global/.precipitation/</a>
</p>

<hr>
<h2 id='chirps_ver_map_quantiles'>Calculates and saves the quantiles of CHIRPS data required for verification maps.</h2><span id='topic+chirps_ver_map_quantiles'></span>

<h3>Description</h3>

<p>Calculates and saves the quantiles of CHIRPS data required for verification maps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chirps_ver_map_quantiles(
  clim_period = 1991:2020,
  version = "UCSB",
  resolution = "low",
  CHIRPS_dir = chirps_dir(),
  seasons = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chirps_ver_map_quantiles_+3A_clim_period">clim_period</code></td>
<td>
<p>which years should be considered for the quantiles.</p>
</td></tr>
<tr><td><code id="chirps_ver_map_quantiles_+3A_version">version</code></td>
<td>
<p>which version of CHIRPS, 'UCSB' or 'ICPAC'? Can be a vector with both.</p>
</td></tr>
<tr><td><code id="chirps_ver_map_quantiles_+3A_resolution">resolution</code></td>
<td>
<p>If this is set to 'high', the quantiles are also calculated for high-resolution CHIRPS data. This is not nicely implemented right now and will take a lot of memory and time.</p>
</td></tr>
<tr><td><code id="chirps_ver_map_quantiles_+3A_chirps_dir">CHIRPS_dir</code></td>
<td>
<p>directory the CHIRPS data is stored in.</p>
</td></tr>
<tr><td><code id="chirps_ver_map_quantiles_+3A_seasons">seasons</code></td>
<td>
<p>Are we plotting for seasonal or monthly forecasts?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data table with quantiles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: chirps_ver_map_quantiles()

</code></pre>

<hr>
<h2 id='climatology_ens_forecast'>Returns a leave-one-year-out climatology-based ensemble forecast</h2><span id='topic+climatology_ens_forecast'></span>

<h3>Description</h3>

<p>for a given year, the ensemble forecast simply consists of the observations in all other years.
This is essentially an auxiliary function for computing skill scores relative to climatology.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>climatology_ens_forecast(obs_dt, by = setdiff(dimvars(obs_dt), "year"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="climatology_ens_forecast_+3A_obs_dt">obs_dt</code></td>
<td>
<p>Data table containing observations, must contain a column 'year'.</p>
</td></tr>
<tr><td><code id="climatology_ens_forecast_+3A_by">by</code></td>
<td>
<p>character vector containing the column names of the grouping variables, e.g. <code>c('month','lon','lat')</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Long data table with the typical ensemble-forecast looks, i.e. containing a column 'member'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dt = climatology_ens_forecast(chirps_monthly)


</code></pre>

<hr>
<h2 id='climatology_threshold_exceedence'>Get climatological prediction for exceedence probabilities.</h2><span id='topic+climatology_threshold_exceedence'></span>

<h3>Description</h3>

<p>The climatological prediction for exceedence probabilities is the fraction of observed years where the observation exceeded the threshold.
It's calculated from leave-one-year-out climatology.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>climatology_threshold_exceedence(
  obs_dt,
  o = "prec",
  by = setdiff(dimvars(obs_dt), "year"),
  thresholds = c(200, 300, 350, 400)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="climatology_threshold_exceedence_+3A_obs_dt">obs_dt</code></td>
<td>
<p>Data table containing observations.</p>
</td></tr>
<tr><td><code id="climatology_threshold_exceedence_+3A_o">o</code></td>
<td>
<p>column name of the observation. Mostly observed precipitation in mm.</p>
</td></tr>
<tr><td><code id="climatology_threshold_exceedence_+3A_by">by</code></td>
<td>
<p>By which columns should be grouped?</p>
</td></tr>
<tr><td><code id="climatology_threshold_exceedence_+3A_thresholds">thresholds</code></td>
<td>
<p>vector of thresholds for which the exceedence probabilities should be derived.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data table with the climatological probabilities of exceedence for the provided thresholds.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dt = climatology_threshold_exceedence(chirps_monthly)


</code></pre>

<hr>
<h2 id='combine'>Combine two data tables</h2><span id='topic+combine'></span>

<h3>Description</h3>

<p>Function for combining two data tables, e.g. with predictions and observations.
This is a user-friendly wrapper for <code><a href="data.table.html#topic+merge">merge</a></code>. It guesses the columns to merge by (the dimension variables
contained in both data tables) and adds some warnings when merges are attempted that are likely not correctly specified by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine(dt1, dt2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combine_+3A_dt1">dt1</code></td>
<td>
<p>first data table</p>
</td></tr>
<tr><td><code id="combine_+3A_dt2">dt2</code></td>
<td>
<p>second data table</p>
</td></tr>
<tr><td><code id="combine_+3A_...">...</code></td>
<td>
<p>passed on to data.table::merge</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The merged data table
</p>


<h3>Examples</h3>

<pre><code class='language-R'># merge ECMWF-forecasts and CHIRPS observations:
dt = ecmwf_monthly[month == 11]
setnames(dt,'prec','forecast') # forecasts and observations both have a column 'prec'
dt_new = combine(dt,chirps_monthly)

</code></pre>

<hr>
<h2 id='complete_regular_grid'>Expand Regular Spatial Grid</h2><span id='topic+complete_regular_grid'></span>

<h3>Description</h3>

<p>First checks whether the spatial coordinates in a data table are part of a <em>regular grid</em>.
If they are, the function returns the smallest <em>regular complete grid</em> including all coordinates.
See <code><a href="#topic+set_spatial_grid">set_spatial_grid</a></code> for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>complete_regular_grid(dt)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="complete_regular_grid_+3A_dt">dt</code></td>
<td>
<p>A data table object containing the spatial grid with coordinates.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the completed spatial grid. Has the grid-attribute.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  dt = data.table(lon = c(1, 2, 3), lat = c(1, 2, 3))
  completed_grid = complete_regular_grid(dt)
  print(completed_grid)


</code></pre>

<hr>
<h2 id='convert_monthly_to_seasonal'>Convert a data table from monthly to seasonal format</h2><span id='topic+convert_monthly_to_seasonal'></span>

<h3>Description</h3>

<p>Converts monthly to seasonal data. The function default values are set for precipitation. In particular, default behavior is to <em>sum</em>
values over all months contained in a season. If you want to average instead (for temperature, for example), you can change the aggregation function <code>FUN</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_monthly_to_seasonal(
  dt,
  vars = NULL,
  by = NULL,
  FUN = sum,
  ...,
  seasons = c("MAM", "JJAS", "OND"),
  only_complete_seasons = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_monthly_to_seasonal_+3A_dt">dt</code></td>
<td>
<p>A data table containing the values for conversion.</p>
</td></tr>
<tr><td><code id="convert_monthly_to_seasonal_+3A_vars">vars</code></td>
<td>
<p>Character vector of names of the columns containing the values for conversion. Default is to try converting everything that is not contained in <code>by</code> and that is not recognized as tercile category (see <code><a href="#topic+tc_cols">tc_cols()</a></code>) or
tercile forecast (<code>'below'</code>, <code>'normal'</code>, <code>'above'</code>).</p>
</td></tr>
<tr><td><code id="convert_monthly_to_seasonal_+3A_by">by</code></td>
<td>
<p>Character vector of column names to group by. Separate values are derived for each unique combination of values in <code>by</code>.
Defaults to all <code><a href="#topic+dimvars">dimvars()</a></code> that are not <code>'month'</code>, which is usually what you want.</p>
</td></tr>
<tr><td><code id="convert_monthly_to_seasonal_+3A_fun">FUN</code></td>
<td>
<p>function for aggregation.</p>
</td></tr>
<tr><td><code id="convert_monthly_to_seasonal_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>FUN</code>, for example <code>na.rm</code>.</p>
</td></tr>
<tr><td><code id="convert_monthly_to_seasonal_+3A_seasons">seasons</code></td>
<td>
<p>Vector of character strings specifying seasons. See details. Defaults to <code>c('MAM', 'JJAS', 'OND')</code>, which are the seasons considered in the COFs for the Greater Horn of Africa.</p>
</td></tr>
<tr><td><code id="convert_monthly_to_seasonal_+3A_only_complete_seasons">only_complete_seasons</code></td>
<td>
<p>Logical. If <code>TRUE</code>, only values are kept for which we have data for all months. For example, no <code>OND</code> values would be derived if the data for December is missing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that it is impossible to derive seasonal tercile categories from monthly ones (and similar for seasonal tercile forecasts). For getting these, you should convert to seasonal
<em>before</em> deriving the tercile categories or forecasts, e.g. by using <code><a href="#topic+add_tercile_cat">add_tercile_cat()</a></code> or <code><a href="#topic+tfc_from_efc">tfc_from_efc()</a></code>.
</p>
<p>Seasons are provided as sequences of capitalized initial characters of the months they contain, e.g. <code>'MAM'</code> for March-April-May.
They can have any length from 1 to 12 months and are allowed to overlap and wrap over the end of the year
(for example, you can provide <code>seasons = c('OND', 'NDJ')</code> to derive values for October-December and November-January).
If a season includes months from 2 years, it gets assigned the year of its starting month. For example, <code>season = 'NDJ'</code> and <code>year = 2000</code> refers to values for the season November 2000 to January 2001.
</p>
<p>Factor- or Character-valued columns cannot be aggregated to seasonal values. If <code>vars</code> contains columns that are factor- or character-valued, it checks whether they take a unique value for each grouping level
provided in <code>by</code>. If yes, they are kept, else they are discarded. A typical case where this is useful is when your data table contains country names (see <code><a href="#topic+add_country">add_country()</a></code>).
The grouping levels usually include <code>'lon'</code>, <code>'lat'</code>, so there is only one country name per grouping level and the name is kept.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# returns empty data table, because the example data does not contain data for a full season:
dt = convert_monthly_to_seasonal(chirps_monthly)

# remove terc_cat first to avoid the warning,
# and set season to the months we actually have data for:
dt2 = convert_monthly_to_seasonal(copy(chirps_monthly)[,terc_cat := NULL], seasons = c('ND'))
print(dt2)

# season OND, get monthly averages rather than sums, and force the function to derive values
# even though we do not have October-data:
dt3 = convert_monthly_to_seasonal(chirps_monthly,
                                  seasons = c('OND'),
                                  FUN = mean,
                                  only_complete_seasons = FALSE)
print(dt3)

</code></pre>

<hr>
<h2 id='CPA'>Coefficients of Predictive Ability</h2><span id='topic+CPA'></span>

<h3>Description</h3>

<p>Function for calculating coefficients of predictive ability (CPAs) of ensemble mean forecasts stored in long data tables:#'
Can also handle point forecasts.
Warning: This metric always needs several years of data since the ranks on which it is based are calculated across multi-year samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CPA(
  dt,
  f,
  o = "obs",
  by = by_cols_ens_fc_score(dt),
  pool = "year",
  mem = "member",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CPA_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="CPA_+3A_f">f</code></td>
<td>
<p>column name of the prediction.</p>
</td></tr>
<tr><td><code id="CPA_+3A_o">o</code></td>
<td>
<p>column name of the observations.</p>
</td></tr>
<tr><td><code id="CPA_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt. A separate CPA is computed for each value of the grouping variables.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="CPA_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged. Needs to contain 'year' per warning above.</p>
</td></tr>
<tr><td><code id="CPA_+3A_mem">mem</code></td>
<td>
<p>Number of column containing the number of the ensemble member.</p>
</td></tr>
<tr><td><code id="CPA_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If True, a simple test whether the dimensions match up is conducted:
The data table should only have one row for each level of c(by,pool,mem)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(fc = 1:4,obs = c(4,4,7,7),member = c(1,2,1,2),year = c(1999,1999,2000,2000))
CPA(dt,f = 'fc')
</code></pre>

<hr>
<h2 id='create_diagram_by_level'>Auxiliary function to simplify grouping for diagrams</h2><span id='topic+create_diagram_by_level'></span>

<h3>Description</h3>

<p>Only works for functions that return a single plot if by == NULL.
This is not the case for some functions plotting results for all three categories, e.g. reliability diagrams or ROC curves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_diagram_by_level(FUN, by, dt, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_diagram_by_level_+3A_fun">FUN</code></td>
<td>
<p>The name of the function creating the diagram</p>
</td></tr>
<tr><td><code id="create_diagram_by_level_+3A_by">by</code></td>
<td>
<p>Column names in dt to group by</p>
</td></tr>
<tr><td><code id="create_diagram_by_level_+3A_dt">dt</code></td>
<td>
<p>data table (cannot be part of ..., because a sub-data-table is passed to FUN)</p>
</td></tr>
<tr><td><code id="create_diagram_by_level_+3A_...">...</code></td>
<td>
<p>arguments passed to FUN</p>
</td></tr>
</table>

<hr>
<h2 id='CRPS'>Continuous Ranked Probability Score</h2><span id='topic+CRPS'></span>

<h3>Description</h3>

<p>Taking CRPSs of ensemble forecasts stored in long data tables:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CRPS(
  dt,
  f,
  o = "obs",
  by = by_cols_ens_fc_score(),
  pool = "year",
  mem = "member",
  dim.check = TRUE,
  ens_size_correction = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CRPS_+3A_dt">dt</code></td>
<td>
<p>Data table containing predictions and observations.</p>
</td></tr>
<tr><td><code id="CRPS_+3A_f">f</code></td>
<td>
<p>column name of the forecasts. May not be called <code>'f'</code></p>
</td></tr>
<tr><td><code id="CRPS_+3A_o">o</code></td>
<td>
<p>column name of the observations.</p>
</td></tr>
<tr><td><code id="CRPS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="CRPS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) over which is averaged. Typically just 'year'.</p>
</td></tr>
<tr><td><code id="CRPS_+3A_mem">mem</code></td>
<td>
<p>Name of the column identifying the ensemble member.</p>
</td></tr>
<tr><td><code id="CRPS_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If True, a simple test whether the dimensions match up is conducted:
The data table should only have one row for each level of c(by,pool,mem)</p>
</td></tr>
<tr><td><code id="CRPS_+3A_ens_size_correction">ens_size_correction</code></td>
<td>
<p>logical. If TRUE, the CRPS is corrected for sample size (see Ferro et al. 2008: 'On the effect of ensemble size on the discrete and continuous
ranked probability scores'). This is slower, but you should do it if you compare ensembles of different size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(fc = 1:4,obs = c(4,4,7,7),member = c(1,2,1,2),year = c(1999,1999,2000,2000))
CRPS(dt,f = 'fc')


</code></pre>

<hr>
<h2 id='crps_aux'>Auxiliary function for calculating crps.</h2><span id='topic+crps_aux'></span>

<h3>Description</h3>

<p>Mostly copy-paste from <code>scoringRules:::crps_edf</code>. Adjusted to the data table format, where the observation is a vector of the same length as the ensemble forecast,
but is just repeated (which is why only <code>y[1]</code>) is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crps_aux(y, dat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crps_aux_+3A_y">y</code></td>
<td>
<p>vector of length m with m identical entries, the observation</p>
</td></tr>
<tr><td><code id="crps_aux_+3A_dat">dat</code></td>
<td>
<p>vector of length m containing the m ensemble forecasts</p>
</td></tr>
</table>

<hr>
<h2 id='crps_aux_esc'>Auxiliary function for calculating crps with ensemble size correction by Ferro et al. 2008.</h2><span id='topic+crps_aux_esc'></span>

<h3>Description</h3>

<p>Mostly copy-paste from <code>scoringRules::crps_edf</code>. Adjusted to the data table format, where the observation is a vector of the same length as the ensemble forecast,
but is just repeated (which is why only <code>y[1]</code>) is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crps_aux_esc(y, dat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crps_aux_esc_+3A_y">y</code></td>
<td>
<p>vector of length m with m identical entries, the observation</p>
</td></tr>
<tr><td><code id="crps_aux_esc_+3A_dat">dat</code></td>
<td>
<p>vector of length m containing the m ensemble forecasts</p>
</td></tr>
</table>

<hr>
<h2 id='CRPSS'>Continuous Ranked Probability Skill Score</h2><span id='topic+CRPSS'></span>

<h3>Description</h3>

<p>Function for taking CRPS skill scores of ensemble forecasts stored in long data tables.
The skill score needs a climatological forecast as reference. This is so far always based on the leave-one-year-out climatology.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CRPSS(dt, f, o = "obs", by = by_cols_ens_fc_score(), pool = c("year"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CRPSS_+3A_dt">dt</code></td>
<td>
<p>Data table containing predictions and observations.</p>
</td></tr>
<tr><td><code id="CRPSS_+3A_f">f</code></td>
<td>
<p>column name of the prediction.</p>
</td></tr>
<tr><td><code id="CRPSS_+3A_o">o</code></td>
<td>
<p>column name of the observations.</p>
</td></tr>
<tr><td><code id="CRPSS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt. A separate CRPS is computed for each value of the grouping variables.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="CRPSS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged. Needs to contain 'year' since the reference climatology forecast is leave-one-year-out.</p>
</td></tr>
<tr><td><code id="CRPSS_+3A_...">...</code></td>
<td>
<p>passed on to CRPS_ens_fc, in particular mem and dim.check</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(fc = 1:4,obs = c(4,4,7,7),member = c(1,2,1,2),year = c(1999,1999,2000,2000))
CRPSS(dt,f = 'fc')


</code></pre>

<hr>
<h2 id='data_dir'>Auxiliary function to access and change the directory used to load and save data.</h2><span id='topic+data_dir'></span>

<h3>Description</h3>

<p>The package allows to download and organize CHIRPS data. This function specifies the directory where
the data is stored. The first time this function is called, it asks the user to configure the directory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_dir(set_dir = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data_dir_+3A_set_dir">set_dir</code></td>
<td>
<p>logical. Set this to TRUE if you have to redefine your data directory.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The current data directory as string.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive()){
data_dir()
}
</code></pre>

<hr>
<h2 id='delete_redundant_files'>Auxiliary function cleaning out the directories, called at the end of the CHIRPS download.</h2><span id='topic+delete_redundant_files'></span>

<h3>Description</h3>

<p>Auxiliary function cleaning out the directories, called at the end of the CHIRPS download.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_redundant_files(dir)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="delete_redundant_files_+3A_dir">dir</code></td>
<td>
<p>the directory of the high dimensional CHIRPS data.</p>
</td></tr>
</table>

<hr>
<h2 id='dimvars'>Get dimension variables</h2><span id='topic+dimvars'></span>

<h3>Description</h3>

<p>The function returns all names currently considered dimension variables.
Following the logic of netcdfs, data tables usually have columns specifying coordinates
(or dimvars) and other columns containing data for these dimvars. Dimension variables can be spatial
or temporal coordinates, or the lead time of a forecast or the member in an ensemble forecast, etc...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dimvars(dt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dimvars_+3A_dt">dt</code></td>
<td>
<p>Optional data table. If a data table is provided only the dimvars of the data table are returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of characters with the column names considered dimvars.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dimvars()

</code></pre>

<hr>
<h2 id='disc_score_dt'>Generalized Discrimination score</h2><span id='topic+disc_score_dt'></span>

<h3>Description</h3>

<p>Calculate the Generalized discrimination score from a data.table with data belonging to a single group (as defined by the
by variable in the DISS function), for example a single location and month.
Formula (5a) from Mason&amp;2018 is used in the calculation. Mostly auxiliary function for the DISS function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>disc_score_dt(year, obs, pB, pN, pA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="disc_score_dt_+3A_year">year</code></td>
<td>
<p>a vector of pool variables, typically year.</p>
</td></tr>
<tr><td><code id="disc_score_dt_+3A_obs">obs</code></td>
<td>
<p>a vector of observations the observation column, needs to contain -1 if it falls into the first category,
0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="disc_score_dt_+3A_pb">pB</code></td>
<td>
<p>a vector of probabilities for the first category.</p>
</td></tr>
<tr><td><code id="disc_score_dt_+3A_pn">pN</code></td>
<td>
<p>a vector of probabilities for the second category.</p>
</td></tr>
<tr><td><code id="disc_score_dt_+3A_pa">pA</code></td>
<td>
<p>a vector of probabilities for the third category.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>disc_score_dt(year = 1999:2001,
             obs = c(-1,0,0),
             pB = c(0.5,0.3,0),
             pN = c(0.3,0.3,0.7),
             pA = c(0.2,0.4,0.3))

</code></pre>

<hr>
<h2 id='DISS'>Generalized discrimination score</h2><span id='topic+DISS'></span>

<h3>Description</h3>

<p>A generalisation of the ROC score for more than two categories.
This score is not proper, but can be used to assess the discrimination of a tercile forecast.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DISS(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score_sp(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DISS_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="DISS_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="DISS_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in obs_dt, or in dt if obs_dt = NULL). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="DISS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="DISS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="DISS_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                year = 1:3)
print(dt)
DISS(dt)
</code></pre>

<hr>
<h2 id='download_chirps_monthly'>Download monthly CHIRPS-data</h2><span id='topic+download_chirps_monthly'></span>

<h3>Description</h3>

<p>Download CHIRPS monthly data for the GHA-region and save it as netcdfs.
The data is downloaded either from the IRI data library or from ICPAC (depending on <code>version</code>), because these data library allows to subset before downloading,
unlike the original source at UCSB.
As of Feb 2022, the entire CHIRPS-monthly data for the GHA-region is roughly 800MB on disk.
The original spatial resolution of CHIRPS is 0.05 degree lon/lat. However, for many applications a coarser resolution is perfectly fine.
The function therefore offers the option to also create and save a coarser, upscaled version of the CHIRPS data that allows much faster data processing.
Alternatively you can also ONLY save the upscaled version to save disk space (roughly 8MB on disk).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_chirps_monthly(
  resolution = "both",
  update = TRUE,
  version = "UCSB",
  years = NULL,
  months = NULL,
  extent = GHA_extent(),
  timeout_limit = 300,
  upscale_grid = data.table(expand.grid(lon = seq(extent[1], extent[2], 0.5), lat =
    seq(extent[3], extent[4], 0.5)))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="download_chirps_monthly_+3A_resolution">resolution</code></td>
<td>
<p>Shall the data be upscaled? Takes one of three arguments:
</p>

<ul>
<li> <p><strong>'both'</strong> (the default) downloads and saves the data on full resolution and additionally derives an upscaled version. Both will be available later.
</p>
</li>
<li> <p><strong>'high'</strong> downloads and saves on original resolution, but does not upscale.
</p>
</li>
<li> <p><strong>'low'</strong> (for saving disk space) downloads the original resolution, upscales immediately and only saves the upscaled version.
</p>
</li></ul>
</td></tr>
<tr><td><code id="download_chirps_monthly_+3A_update">update</code></td>
<td>
<p>Logical, if TRUE, previously created files are skipped.</p>
</td></tr>
<tr><td><code id="download_chirps_monthly_+3A_version">version</code></td>
<td>
<p>Should be 'UCSB' (for University of California Santa Barbara, the original source of CHIRPS) or 'ICPAC' (for downloading the ICPAC version CHIRPS blended)</p>
</td></tr>
<tr><td><code id="download_chirps_monthly_+3A_years">years</code>, <code id="download_chirps_monthly_+3A_months">months</code></td>
<td>
<p>Which years and months do you want to load? NULL loads everything there is.</p>
</td></tr>
<tr><td><code id="download_chirps_monthly_+3A_extent">extent</code></td>
<td>
<p>vector of length four (xmin,xmax,ymin,ymax), restricting the spatial area.</p>
</td></tr>
<tr><td><code id="download_chirps_monthly_+3A_timeout_limit">timeout_limit</code></td>
<td>
<p>how many seconds (per file, i.e. per yearmonth) before the download is aborted?</p>
</td></tr>
<tr><td><code id="download_chirps_monthly_+3A_upscale_grid">upscale_grid</code></td>
<td>
<p>The coarse grid to which the data is upscaled (only used when resolution is either 'both' or 'high'). Only change this if you know what you are doing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive()){
download_chirps_monthly(years = 2020, months = 1)
}

</code></pre>

<hr>
<h2 id='download_chirps_monthly_high'>Auxiliary function called by download_chirps_monthly</h2><span id='topic+download_chirps_monthly_high'></span>

<h3>Description</h3>

<p>Auxiliary function called by download_chirps_monthly
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_chirps_monthly_high(
  update,
  version,
  years,
  months,
  extent,
  timeout_limit,
  save_dir = file.path(chirps_dir(), version)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="download_chirps_monthly_high_+3A_update">update</code>, <code id="download_chirps_monthly_high_+3A_version">version</code>, <code id="download_chirps_monthly_high_+3A_years">years</code>, <code id="download_chirps_monthly_high_+3A_months">months</code>, <code id="download_chirps_monthly_high_+3A_extent">extent</code>, <code id="download_chirps_monthly_high_+3A_timeout_limit">timeout_limit</code></td>
<td>
<p>see <code>download_chirps_monthly</code>.</p>
</td></tr>
<tr><td><code id="download_chirps_monthly_high_+3A_save_dir">save_dir</code></td>
<td>
<p>directory where the chirps data is stored.</p>
</td></tr>
</table>

<hr>
<h2 id='download_chirps_monthly_low'>Auxiliary function called by download_chirps_monthly</h2><span id='topic+download_chirps_monthly_low'></span>

<h3>Description</h3>

<p>Auxiliary function called by download_chirps_monthly
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_chirps_monthly_low(
  update,
  version,
  years,
  months,
  extent,
  timeout_limit,
  upscale_grid,
  root_dir = file.path(chirps_dir(), version)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="download_chirps_monthly_low_+3A_update">update</code>, <code id="download_chirps_monthly_low_+3A_version">version</code>, <code id="download_chirps_monthly_low_+3A_years">years</code>, <code id="download_chirps_monthly_low_+3A_months">months</code>, <code id="download_chirps_monthly_low_+3A_extent">extent</code>, <code id="download_chirps_monthly_low_+3A_timeout_limit">timeout_limit</code></td>
<td>
<p>see <code>download_chirps_monthly</code>.</p>
</td></tr>
<tr><td><code id="download_chirps_monthly_low_+3A_upscale_grid">upscale_grid</code></td>
<td>
<p>To which grid shall we upscale? Needs a data table with lon/lat columns</p>
</td></tr>
<tr><td><code id="download_chirps_monthly_low_+3A_root_dir">root_dir</code></td>
<td>
<p>directory where the high-dimensional chirps data would be stored. The upscaled data is then stored in root_dir/upscaled/.</p>
</td></tr>
</table>

<hr>
<h2 id='download_chirps_prelim_aux'>Auxiliary function for downloading the preliminary CHIRPS monthly data</h2><span id='topic+download_chirps_prelim_aux'></span>

<h3>Description</h3>

<p>This data becomes available earlier, but it has to be downloaded from UCSB.
The function checks whether the non-preliminary version exists and only downloads otherwise.
Annoyingly, the grid of UCBS and IRIDL are shifted against each other. Therefore this function also interpolates the UCSB data to the IRIDL grid, which makes it a bit slower.
In particular, everything will crash if you have never downloaded a non-preliminary file and try to download a preliminary one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_chirps_prelim_aux(
  years,
  months,
  extent,
  timeout_limit = 300,
  nonprelim_dir = file.path(chirps_dir(), "monthly"),
  save_dir = file.path(nonprelim_dir, "prelim")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="download_chirps_prelim_aux_+3A_years">years</code></td>
<td>
<p>years for which you want to download</p>
</td></tr>
<tr><td><code id="download_chirps_prelim_aux_+3A_months">months</code></td>
<td>
<p>months for which you want to download</p>
</td></tr>
<tr><td><code id="download_chirps_prelim_aux_+3A_extent">extent</code></td>
<td>
<p>Spatial window for downloading</p>
</td></tr>
<tr><td><code id="download_chirps_prelim_aux_+3A_timeout_limit">timeout_limit</code></td>
<td>
<p>How many seconds before download is aborted.</p>
</td></tr>
<tr><td><code id="download_chirps_prelim_aux_+3A_nonprelim_dir">nonprelim_dir</code></td>
<td>
<p>Directory where the non-preliminary CHIRPS data is stored.</p>
</td></tr>
<tr><td><code id="download_chirps_prelim_aux_+3A_save_dir">save_dir</code></td>
<td>
<p>Directory where the function stores the preliminary data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nothing
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive()){
download_chirps_prelim_aux(years = 2023, months = 10)
}

</code></pre>

<hr>
<h2 id='dt_to_netcdf'>Write a netcdf from a long data table</h2><span id='topic+dt_to_netcdf'></span>

<h3>Description</h3>

<p>This function writes a netcdf from a long data table, the usual data format in SeaVal.
If not specified, it guesses (based on column names) which columns contain dimension variables and which contain variables.
The function currently does not support writing netcdfs with multiple variables that have different sets of dimension variables!
</p>
<p>It allows to store character columns in netcdfs (essentially labelling them as integers and storing a legend).
This legend is automatically interpreted when the netcdf is read with <code><a href="#topic+netcdf_to_dt">netcdf_to_dt()</a></code>, but is also humanly readable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dt_to_netcdf(
  dt,
  nc_out,
  vars = NULL,
  units = NULL,
  dim_vars = dimvars(dt),
  dim_var_units = NULL,
  check = interactive(),
  description = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dt_to_netcdf_+3A_dt">dt</code></td>
<td>
<p>a data.table</p>
</td></tr>
<tr><td><code id="dt_to_netcdf_+3A_nc_out">nc_out</code></td>
<td>
<p>File name (including path) of the netcdf to write.</p>
</td></tr>
<tr><td><code id="dt_to_netcdf_+3A_vars">vars</code></td>
<td>
<p>names of columns in dt containing variables. If this is NULL, the function guesses and asks for confirmation.</p>
</td></tr>
<tr><td><code id="dt_to_netcdf_+3A_units">units</code></td>
<td>
<p>character vector containing the units for vars (in the same order). If this is NULL (default), the user is prompted for input.</p>
</td></tr>
<tr><td><code id="dt_to_netcdf_+3A_dim_vars">dim_vars</code></td>
<td>
<p>names of columns in dt containing dimension variables. If this is NULL, the function guesses and asks for confirmation.</p>
</td></tr>
<tr><td><code id="dt_to_netcdf_+3A_dim_var_units">dim_var_units</code></td>
<td>
<p>character vector containing the units for dim_vars (in the same order). If this is NULL (default), the user is prompted for input (except for lon/lat).</p>
</td></tr>
<tr><td><code id="dt_to_netcdf_+3A_check">check</code></td>
<td>
<p>If check is TRUE, the function asks the user whether an existing file should be overwritten, and whether the correct dimvars have been guessed.</p>
</td></tr>
<tr><td><code id="dt_to_netcdf_+3A_description">description</code></td>
<td>
<p>For adding a global attribute 'Description' as a string.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>none.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>example_dt = data.table(lon = 1:3, month = 4:6, prec = 7:9)
file_name = tempfile()
dt_to_netcdf(dt = example_dt, nc_out = file_name,
             vars = "prec", units = "mm",
             dim_vars = c("lon","month"), dim_var_units = c("degree longitude","month"))



</code></pre>

<hr>
<h2 id='EA_country_names'>Get names of countries in east Africa</h2><span id='topic+EA_country_names'></span>

<h3>Description</h3>

<p>This is an auxiliary function used in <code><a href="#topic+add_country_names">add_country_names</a></code>, so only these names are recognized
by default.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EA_country_names()
</code></pre>


<h3>Value</h3>

<p>A character-vector of country names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>EA_country_names()
</code></pre>

<hr>
<h2 id='ecmwf_monthly'>Monthly mean precipitation forecast example dataset</h2><span id='topic+ecmwf_monthly'></span>

<h3>Description</h3>

<p>This is a small example dataset containing hindcasts of monthly mean precipitation for illustration purposes.
The forecasts are contained for the entire GHA-region, for November and December 2018-2020.
The forecasts are issued by the ECMWF SEAS 5 model and initialized in August.
The unit of precipitation is mm/day. Only the first 3 ensemble members are
provided. The dataset also contains tercile probability forecasts, which are
derived from the full 51 member ensemble. The probability for a tercile for a
given year, month and location is always computed as the fraction of ensemble
members falling into that tercile, computed from all ensemble predictions for
the month and location under consideration.
This dataset was generated using Copernicus Climate Change Service information (2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ecmwf_monthly)
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.table</code> (inherits from <code>data.frame</code>) with 37224 rows and 9 columns.
</p>


<h3>Source</h3>

<p><a href="https://cds.climate.copernicus.eu">https://cds.climate.copernicus.eu</a>
</p>

<hr>
<h2 id='EIR'>Effective Interest Rate</h2><span id='topic+EIR'></span>

<h3>Description</h3>

<p>This score is suitable for tercile category forecasts. Using log2 for now (?). According to Mason, the averaging here
should be over many years at a single locations and for discrete time-periods (so Mason prefers to take the average after
averaging over different locations, but I keep it like this for now).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EIR(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EIR_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="EIR_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="EIR_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in <code>obs_dt</code>, or in dt if <code>obs_dt = NULL</code>). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="EIR_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="EIR_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="EIR_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
EIR(dt)
</code></pre>

<hr>
<h2 id='fc_cols'>Forecast column names</h2><span id='topic+fc_cols'></span>

<h3>Description</h3>

<p>returns the columns names that are recognized as (ensemble-) forecast values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fc_cols(dt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fc_cols_+3A_dt">dt</code></td>
<td>
<p>optional data table. If provided, the function guesses which column contains the forecast values. Else it returns all recognized forecast column names.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector with column names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fc_cols()

</code></pre>

<hr>
<h2 id='get_mask'>Function to create a mask of dry regions from CHIRPS</h2><span id='topic+get_mask'></span>

<h3>Description</h3>

<p>A gridpoint is masked for a given season (either 'MAM', 'JJAS' or 'OND'), if, on average, less than 10% of the annual total of rainfall
occur during the season. This function loads CHIRPS data, and derives this mask as a data table of lon, lat coordinates, only containing
the coordinates that shouldn't be masked. You can apply the mask to an existing data table using dt = combine(dt,mask).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_mask(
  season,
  clim_years = 1990:2020,
  version = "UCSB",
  resolution = "low",
  us = (resolution == "low")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_mask_+3A_season">season</code></td>
<td>
<p>For which season do you want to calculate the mask? Needs to be either 'MAM', 'JJAS' or 'OND'.</p>
</td></tr>
<tr><td><code id="get_mask_+3A_clim_years">clim_years</code></td>
<td>
<p>Numeric vector of years. Which years should be used to establish the mask?</p>
</td></tr>
<tr><td><code id="get_mask_+3A_version">version</code>, <code id="get_mask_+3A_resolution">resolution</code>, <code id="get_mask_+3A_us">us</code></td>
<td>
<p>Passed to <code><a href="#topic+load_chirps">load_chirps</a></code>. Which CHIRPS version do you want to use and on what resolution?</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive()) get_mask('MAM')

</code></pre>

<hr>
<h2 id='get_quantiles'>Calculate quantiles from a data table</h2><span id='topic+get_quantiles'></span>

<h3>Description</h3>

<p>The quantiles are saved in/returned as a list with the following elements:
</p>

<ul>
<li><p> dt - A data table with quantiles for each level of by (not the same as the input-dt).
</p>
</li>
<li><p> quantiles - the vector of quantiles that were used.
</p>
</li>
<li><p> group - a data table containing the levels the quantiles are grouped over, e.g. all years the quantiles are calculated over.
</p>
</li>
<li><p> data_col_name - the name of data_col, see below, so that you know what the quantiles actually were computed from.
</p>
</li>
<li><p> description - the description string, if provided.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>get_quantiles(
  dt,
  data_col = setdiff(names(dt), dimvars(dt))[1],
  qqs = c(10, 20, 33, 67, 80, 90),
  by = setdiff(dimvars(dt), c("year", "member")),
  description = NULL,
  save_file = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_quantiles_+3A_dt">dt</code></td>
<td>
<p>Data table containing the data.</p>
</td></tr>
<tr><td><code id="get_quantiles_+3A_data_col">data_col</code></td>
<td>
<p>The name of the column in dt containing the data for which the quantiles are derived. By default the first column that is not a dimension variable is selected.</p>
</td></tr>
<tr><td><code id="get_quantiles_+3A_qqs">qqs</code></td>
<td>
<p>Vector of quantiles. If one of them is larger 1 they are interpreted as percent. Default is the quantiles used in the verification maps.</p>
</td></tr>
<tr><td><code id="get_quantiles_+3A_by">by</code></td>
<td>
<p>Column names in dt. Levels by which the quantiles are calculated</p>
</td></tr>
<tr><td><code id="get_quantiles_+3A_description">description</code></td>
<td>
<p>Optional description string.</p>
</td></tr>
<tr><td><code id="get_quantiles_+3A_save_file">save_file</code></td>
<td>
<p>Optional name of save file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing if save_file is provided. Otherwise the list described above
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_quantiles(chirps_monthly)

</code></pre>

<hr>
<h2 id='get_terciles'>get terciles from a data table</h2><span id='topic+get_terciles'></span>

<h3>Description</h3>

<p>This function wraps <code><a href="#topic+get_quantiles">get_quantiles</a></code> with the fixed quantiles 0.33 and 0.67.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_terciles(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_terciles_+3A_...">...</code></td>
<td>
<p>passed on to <code><a href="#topic+get_quantiles">get_quantiles</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>See <code><a href="#topic+get_quantiles">get_quantiles</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# takes a few seconds:
get_terciles(chirps_monthly)


</code></pre>

<hr>
<h2 id='ggplot_dt'>plotting function for spatial data</h2><span id='topic+ggplot_dt'></span>

<h3>Description</h3>

<p>Plots spatial data from a data.table. The data table needs to contain columns named 'lon' and 'lat'. The grid needs to be regular.
If spatial data is contained for several levels (e.g. mutliple times or multiple ensemble members), only the data for the first level is plotted.
By default, the first column that is not recognized as a dimension variable is plotted, see <code>data_col</code>. For the most common data-columns, reasonable
color scales are selected automatically.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggplot_dt(
  dt,
  data_col = NULL,
  mn = NULL,
  discrete_cs = FALSE,
  rr = NULL,
  low = NULL,
  mid = NULL,
  high = NULL,
  name = data_col,
  midpoint = NULL,
  breaks = NULL,
  na.value = "gray75",
  oob = NULL,
  guide = guide_colorbar(barwidth = 0.5, barheight = 10),
  ...,
  binwidth = NULL,
  bin_midpoint = midpoint,
  add_map = TRUE,
  extent = NULL,
  expand.x = c(0, 0),
  expand.y = c(0, 0),
  dimension_check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggplot_dt_+3A_dt">dt</code></td>
<td>
<p>Data table containing the data for plotting.</p>
</td></tr>
<tr><td><code id="ggplot_dt_+3A_data_col">data_col</code></td>
<td>
<p>The name of the column in dt containing the data for plotting. If nothing is provided (the default), the first column that is not a dimension variable or 'member' is selected.</p>
</td></tr>
<tr><td><code id="ggplot_dt_+3A_mn">mn</code></td>
<td>
<p>optional plot title</p>
</td></tr>
<tr><td><code id="ggplot_dt_+3A_discrete_cs">discrete_cs</code></td>
<td>
<p>Logical. Should the color scale be discretized?</p>
</td></tr>
<tr><td><code id="ggplot_dt_+3A_rr">rr</code>, <code id="ggplot_dt_+3A_low">low</code>, <code id="ggplot_dt_+3A_mid">mid</code>, <code id="ggplot_dt_+3A_high">high</code>, <code id="ggplot_dt_+3A_name">name</code>, <code id="ggplot_dt_+3A_breaks">breaks</code>, <code id="ggplot_dt_+3A_na.value">na.value</code>, <code id="ggplot_dt_+3A_oob">oob</code>, <code id="ggplot_dt_+3A_guide">guide</code>, <code id="ggplot_dt_+3A_...">...</code></td>
<td>
<p>Arguments for the color scale, passed to scale_fill_gradient2 or scale_fill_steps2 (depending on whether discrete_cs == TRUE).
rr replaces limits (specifying the range of the color scale) for consistency with the older plotting functions from the PostProcessing package.
<code>na.value</code> specifies the color of missing values. <code>oob</code> specifies the treatment of out-of-boundary values, i.e. values beyond the
limits.
The ... argument can in particular be used to customize the legend/colorbar using the function <code>guide_colorbar</code>,
see https://ggplot2.tidyverse.org/reference/guide_colourbar.html. Moreover, when <code>discrete_cs == TRUE</code> you can pass the arguments <code>n.breaks,breaks</code> to customize the scale.
If you use n.breaks you might also want to set nice.breaks = FALSE, see ?scale_fill_steps2.</p>
</td></tr>
<tr><td><code id="ggplot_dt_+3A_midpoint">midpoint</code></td>
<td>
<p>midpoint of the color scale, passed to <code>scale_fill_gradient2</code> or <code>scale_fill_steps2</code> (depending on whether <code>discrete_cs == TRUE</code>).
If you set it to NULL (the default), the midpoint is set to the center of the data range (or the center of rr, if provided), such that the entire color scale is used.
Note that this default differs from the default behavior of <code>scale_fill_gradient2</code> or <code>scale_fill_steps2</code>.
Specifying the midpoint can often be a convenient way to force a color scale with only two colors (for example, by setting it
to the minimum or maximum of your data).</p>
</td></tr>
<tr><td><code id="ggplot_dt_+3A_binwidth">binwidth</code>, <code id="ggplot_dt_+3A_bin_midpoint">bin_midpoint</code></td>
<td>
<p>only used when <code>discrete_cs == TRUE</code>. Normally, the breaks for the discrete colorscale are specified by n.breaks (which is not reliable,
since they're adjusted to be 'nice'), or by specifying the breaks explicitly (which is often tedious). This gives you a third option, namely specifying how far the breaks
should be apart, and specifying the centerpoint for one of the bins (default is midpoint, or the center of rr if midpoint is not provided). For example, if your color scale shows
percentages and you'd like 4 categories, ranging from white to red, this is easiest achieved by <code>binwidth = 25, midpoint = 12.5</code>.</p>
</td></tr>
<tr><td><code id="ggplot_dt_+3A_add_map">add_map</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>, mostly for internal use. Set to <code>FALSE</code> to remove borders (e.g. if you want to add them yourself from a shapefile).</p>
</td></tr>
<tr><td><code id="ggplot_dt_+3A_extent">extent</code></td>
<td>
<p>An optional four-element vector in the order xmin,xmax,ymin,ymax for specifying the spatial extent of the plot. Default is to fit the extent to the data.</p>
</td></tr>
<tr><td><code id="ggplot_dt_+3A_expand.x">expand.x</code>, <code id="ggplot_dt_+3A_expand.y">expand.y</code></td>
<td>
<p>vectors with two entries to be added to xlims/ylims of the plot. E.g. expand.x = c(-0.5,0.5)
expands the plot by half a longitude both on the right and left hand side</p>
</td></tr>
<tr><td><code id="ggplot_dt_+3A_dimension_check">dimension_check</code></td>
<td>
<p>Logical. By default the function checks that there are not multiple values per coordinate
(and subsets to the first level if there are several, e.g. to the first year and month (by appearance in <code>dt</code>) if <code>dt</code> contains data for several years and months).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a ggplot object.
</p>


<h3>Author(s)</h3>

<p>Claudio Heinrich
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ex_dt = chirps_monthly[lat &lt;0 &amp; month == 12 &amp; year == 2020]
pp = ggplot_dt(ex_dt)
if(interactive()) plot(pp)

</code></pre>

<hr>
<h2 id='GHA_extent'>GHA-bounding-box</h2><span id='topic+GHA_extent'></span>

<h3>Description</h3>

<p>Returns a lon/lat bounding box for the greater horn of Africa region. Format is c(xmin,xmax,ymin,ymax), as for raster::extent
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GHA_extent()
</code></pre>


<h3>Value</h3>

<p>A numeric vectorof length 4.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>GHA_extent()
</code></pre>

<hr>
<h2 id='gha_plot'>Plotting function with different map for Greater Horn of Africa</h2><span id='topic+gha_plot'></span><span id='topic+ggplot_dt_shf'></span><span id='topic+ggplot_dt_gha_map'></span>

<h3>Description</h3>

<p>This essentially wraps <code><a href="#topic+ggplot_dt">ggplot_dt</a></code>, but uses a different map for borders.
The map is part of the package and is the one currently used during GHACOFs at ICPAC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gha_plot(..., expand.x = c(-0.5, 0.5), expand.y = c(-0.5, 2))

ggplot_dt_shf(...)

ggplot_dt_gha_map(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gha_plot_+3A_...">...</code>, <code id="gha_plot_+3A_expand.x">expand.x</code>, <code id="gha_plot_+3A_expand.y">expand.y</code></td>
<td>
<p>passed to <code><a href="#topic+ggplot_dt">ggplot_dt</a></code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>dt = chirps_monthly[lon %between% c(30,40) &amp; lat &lt; 0 &amp; month == 11 &amp; year == 2020]
pp = gha_plot(dt)
if(interactive()) plot(pp)

</code></pre>

<hr>
<h2 id='grid_info'>Retrieve spatial grid information from a data table</h2><span id='topic+grid_info'></span>

<h3>Description</h3>

<p>This function prints out spatial grid information from a data table. If the grid-attribute does not exist
<code><a href="#topic+set_spatial_grid">set_spatial_grid</a></code> is called first.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grid_info(dt)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grid_info_+3A_dt">dt</code></td>
<td>
<p>A data table</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function does not return a value; instead, it prints a message to the console
with the grid information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 dt = data.table(lon = runif(10), lat = runif(10))
 grid_info(dt)
</code></pre>

<hr>
<h2 id='HS'>Hit score</h2><span id='topic+HS'></span>

<h3>Description</h3>

<p>This score is suitable for tercile category forecasts. This score is the frequency at which the highest probability category actually
happens. The function also provides the frequency at which the second-highest probability category, and lowest probability category,
actually happens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HS(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HS_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="HS_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="HS_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in <code>obs_dt</code>, or in <code>dt</code> if <code>obs_dt = NULL</code>). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="HS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="HS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="HS_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
HS(dt)
</code></pre>

<hr>
<h2 id='HSS'>Hit Skill Score</h2><span id='topic+HSS'></span>

<h3>Description</h3>

<p>This score is suitable for tercile category forecasts. The skill score is the difference between the hit scores
for the categories with the highest and lowest probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HSS(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HSS_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="HSS_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="HSS_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in <code>obs_dt</code>, or in dt if <code>obs_dt = NULL</code>). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="HSS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="HSS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="HSS_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                year = 1999:2001)
print(dt)
HSS(dt)
</code></pre>

<hr>
<h2 id='IGS'>Ignorance Score</h2><span id='topic+IGS'></span>

<h3>Description</h3>

<p>This score is suitable for tercile category forecasts. Using log2 for now (?).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IGS(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IGS_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="IGS_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="IGS_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in <code>obs_dt</code>, or in dt if <code>obs_dt = NULL</code>). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="IGS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="IGS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="IGS_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
IGS(dt)
</code></pre>

<hr>
<h2 id='IGSS'>Ignorance Skill score</h2><span id='topic+IGSS'></span>

<h3>Description</h3>

<p>This score is suitable for tercile category forecasts. Using log2 for now (?). This is the &quot;usual&quot; skill score
(not the effective interest rate).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IGSS(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IGSS_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="IGSS_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="IGSS_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in <code>obs_dt</code>, or in <code>dt</code> if <code>obs_dt = NULL</code>). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="IGSS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="IGSS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="IGSS_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
IGSS(dt)
</code></pre>

<hr>
<h2 id='indicator_times_value_aux'>Auxiliary function for multiplying two numbers such that 0 x infty is 0. Needed for the ignorance score: 0log(0) should be 0.</h2><span id='topic+indicator_times_value_aux'></span>

<h3>Description</h3>

<p>Auxiliary function for multiplying two numbers such that 0 x infty is 0. Needed for the ignorance score: 0log(0) should be 0.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>indicator_times_value_aux(indicator, value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="indicator_times_value_aux_+3A_indicator">indicator</code></td>
<td>
<p>logical input vector</p>
</td></tr>
<tr><td><code id="indicator_times_value_aux_+3A_value">value</code></td>
<td>
<p>numeric input vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>indicator x value with 0*infty = 0
</p>

<hr>
<h2 id='load_chirps'>Function for loading CHIRPS (monthly) data.</h2><span id='topic+load_chirps'></span>

<h3>Description</h3>

<p>The data has to be previously downloaded, see <code>download_chirps_monthly</code>. The resulting data table contains precip in unit mm/day.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_chirps(
  years = NULL,
  months = NULL,
  version = "UCSB",
  resolution = "low",
  us = (resolution == "low"),
  load_prelim = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_chirps_+3A_years">years</code>, <code id="load_chirps_+3A_months">months</code></td>
<td>
<p>Optional subset of years and months you want to load. The default is to load everything that has been downloaded locally.
You can update your local CHIRPS download by calling download_chirps_monthly</p>
</td></tr>
<tr><td><code id="load_chirps_+3A_version">version</code></td>
<td>
<p>Either 'UCSB' to load the original version from UCSB or 'ICPAC' to load CHIRPS blended (both need to be downloaded first).</p>
</td></tr>
<tr><td><code id="load_chirps_+3A_resolution">resolution</code></td>
<td>
<p>Either 'low' for loading the coarser upscaled version, or 'high' for loading the data on full resolution</p>
</td></tr>
<tr><td><code id="load_chirps_+3A_us">us</code></td>
<td>
<p>logical. If TRUE, the upscaled version is loaded. Takes precedence over resolution.</p>
</td></tr>
<tr><td><code id="load_chirps_+3A_load_prelim">load_prelim</code></td>
<td>
<p>logical. Should preliminary data be loaded? Note that the preliminary data is always from UCSB, not from ICPAC.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the derived data table
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive()){
load_chirps()
}
</code></pre>

<hr>
<h2 id='lt_cols'>Data table column names that are recognized as leadtime</h2><span id='topic+lt_cols'></span>

<h3>Description</h3>

<p>Data table column names that are recognized as leadtime
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lt_cols()
</code></pre>


<h3>Examples</h3>

<pre><code class='language-R'>lt_cols()
</code></pre>

<hr>
<h2 id='MB'>Multicategory Brier score</h2><span id='topic+MB'></span>

<h3>Description</h3>

<p>This score is suitable for tercile category forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MB(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MB_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="MB_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="MB_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in <code>obs_dt</code>, or in dt if <code>obs_dt = NULL</code>). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="MB_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="MB_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="MB_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
MB(dt)
</code></pre>

<hr>
<h2 id='MBS'>Multicategory Brier Skill score</h2><span id='topic+MBS'></span>

<h3>Description</h3>

<p>This score is suitable for tercile category forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MBS(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MBS_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="MBS_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="MBS_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in <code>obs_dt</code>, or in dt if <code>obs_dt = NULL</code>). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="MBS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="MBS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="MBS_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
MBS(dt)
</code></pre>

<hr>
<h2 id='modify_dt_map_plotting'>Auxiliary function for checking dimensions for map-plotting</h2><span id='topic+modify_dt_map_plotting'></span>

<h3>Description</h3>

<p>Auxiliary function for checking dimensions for map-plotting
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modify_dt_map_plotting(dt, data_col)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modify_dt_map_plotting_+3A_dt">dt</code></td>
<td>
<p>Data table containing the data for plotting</p>
</td></tr>
<tr><td><code id="modify_dt_map_plotting_+3A_data_col">data_col</code></td>
<td>
<p>Name of column containing the data for plotting</p>
</td></tr>
</table>

<hr>
<h2 id='MSD_to_YM'>Converts time given as 'months since date' (MSD) into years and months (YM)</h2><span id='topic+MSD_to_YM'></span>

<h3>Description</h3>

<p>Converts time given as 'months since date' (MSD) into years and months (YM)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MSD_to_YM(dt, timecol = "time", origin = "1981-01-01")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MSD_to_YM_+3A_dt">dt</code></td>
<td>
<p>a data table.</p>
</td></tr>
<tr><td><code id="MSD_to_YM_+3A_timecol">timecol</code></td>
<td>
<p>name of the column containing the time.</p>
</td></tr>
<tr><td><code id="MSD_to_YM_+3A_origin">origin</code></td>
<td>
<p>The time column contains time in the format month since which date?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data table with two new columns 'month' and 'year', the timecol is deleted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = MSD_to_YM(data.table(time = 0:12))
</code></pre>

<hr>
<h2 id='MSE'>Mean Square Error of ensemble forecasts.</h2><span id='topic+MSE'></span>

<h3>Description</h3>

<p>Derives the MSE of ensemble forecasts stored in long data tables. Can also handle point forecast.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MSE(
  dt,
  f,
  o = "obs",
  by = by_cols_ens_fc_score(),
  pool = "year",
  mem = "member",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MSE_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="MSE_+3A_f">f</code></td>
<td>
<p>column name of the prediction.</p>
</td></tr>
<tr><td><code id="MSE_+3A_o">o</code></td>
<td>
<p>column name of the observations.</p>
</td></tr>
<tr><td><code id="MSE_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="MSE_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="MSE_+3A_mem">mem</code></td>
<td>
<p>Name of the column identifying the ensemble member. Only used if check_dimension is TRUE. Is NULL for a point forecast.</p>
</td></tr>
<tr><td><code id="MSE_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If True, a simple test whether the dimensions match up is conducted:
The data table should only have one row for each level of c(by,pool,mem)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(fc = 1:4,obs = c(4,4,7,7),member = c(1,2,1,2),year = c(1999,1999,2000,2000))
MSE(dt,f = 'fc')
</code></pre>

<hr>
<h2 id='MSES'>Mean Square Error Skill score</h2><span id='topic+MSES'></span>

<h3>Description</h3>

<p>Function for taking MSE skill scores of ensemble forecasts stored in long data tables.
Can also handle point forecasts.
The skill score needs a climatological forecast as reference. This is so far always based on the leave-one-year-out climatology.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MSES(dt, f, o = "obs", by = by_cols_ens_fc_score(), pool = c("year"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MSES_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="MSES_+3A_f">f</code></td>
<td>
<p>column name of the prediction.</p>
</td></tr>
<tr><td><code id="MSES_+3A_o">o</code></td>
<td>
<p>column name of the observations.</p>
</td></tr>
<tr><td><code id="MSES_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt. A separate MSE is computed for each value of the grouping variables.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="MSES_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged. Needs to contain 'year' since the reference climatology forecast is leave-one-year-out.</p>
</td></tr>
<tr><td><code id="MSES_+3A_...">...</code></td>
<td>
<p>passed on to MSE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(fc = 1:4,obs = c(4,4,7,7),member = c(1,2,1,2),year = c(1999,1999,2000,2000))
MSES(dt,f = 'fc')

</code></pre>

<hr>
<h2 id='netcdf_to_dt'>function for converting netcdfs to long data tables.</h2><span id='topic+netcdf_to_dt'></span>

<h3>Description</h3>

<p>The function converts netcdfs into long data.tables.
Be aware that the data table can be much larger in memory, especially if you have many dimension variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>netcdf_to_dt(
  nc,
  vars = NULL,
  verbose = 2,
  trymerge = TRUE,
  subset_list = NULL,
  keep_nas = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="netcdf_to_dt_+3A_nc">nc</code></td>
<td>
<p>Either a character string with the name of the .nc file (including path), or an object of type ncdf4.</p>
</td></tr>
<tr><td><code id="netcdf_to_dt_+3A_vars">vars</code></td>
<td>
<p>Which variables should be read from the netcdf? Either a character vector of variable names, or an
integer vector of variable indices. If this is NULL, all variables are read.</p>
</td></tr>
<tr><td><code id="netcdf_to_dt_+3A_verbose">verbose</code></td>
<td>
<p>Either 0, 1 or 2. How much information should be printed?
The default (2) is to print the entire netcdf information (as output by <code>ncdf4::nc_open</code>), 1 just prints the units for all variables, 0 (or any other input)
prints nothing.</p>
</td></tr>
<tr><td><code id="netcdf_to_dt_+3A_trymerge">trymerge</code></td>
<td>
<p>logical. If TRUE, a single data table containing all variables is returned, else a list of data
tables, one for each variable. The latter is much more memory efficient if you have multiple variables depending
on different dimensions.</p>
</td></tr>
<tr><td><code id="netcdf_to_dt_+3A_subset_list">subset_list</code></td>
<td>
<p>A named list for reading only subsets of the data. Currently only 'rectangle subsetting' is provided, i.e. you can provide two limit values for each dimension and everything between
will be read. The names of the pages of subset_list must correspond to the names of dimension variables in the netcdf, and each page should contain a (two-element-)range vector.
For example, subsetting a global dataset to just East Africa could look like this: subset_list = list(latitude = c(-15,25),longitude = c(20,55)).
Non-rectangular subsetting during reading a netcdf seems to be difficult, see ncvar_get. Every dimension variable not named in subset_list is read entirely.</p>
</td></tr>
<tr><td><code id="netcdf_to_dt_+3A_keep_nas">keep_nas</code></td>
<td>
<p>Should missing values be kept? If FALSE (the default), missing values are not included in the returned data table.
If this is set to TRUE, the data table is constructed from the full data-cube (meaning its number of rows is the product of the length of the dimension variables, even if many coordinates
have missing data). This makes the returned data table potentially much larger and is almost never an advantage. It is only allowed, because it can make complex bookkeeping tasks easier
(specifically upscaling many CHIRPS-netcdfs with the same coordinates while saving the upscaling weights in a matrix).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table if <code>trymerge == TRUE</code> or else a list of data tables.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># filename of example-netcdf file:
fn = system.file("extdata", "example.nc", package="SeaVal")

dt = netcdf_to_dt(fn)
print(dt)


</code></pre>

<hr>
<h2 id='obs_cols'>Observation column names</h2><span id='topic+obs_cols'></span>

<h3>Description</h3>

<p>Note that this function guesses column names for observed precip, not observed tercile category.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>obs_cols(dt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="obs_cols_+3A_dt">dt</code></td>
<td>
<p>optional data table. If provided, the function guesses which column contains the observations. Else it returns all recognized observation column names.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector with column names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>obs_cols()

</code></pre>

<hr>
<h2 id='obs_dimvars'>Auxiliary function returning observation dimvars.</h2><span id='topic+obs_dimvars'></span>

<h3>Description</h3>

<p>Observation dimvars are column names in a data table that resemble coordinates for which only one observation may exist.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>obs_dimvars(dt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="obs_dimvars_+3A_dt">dt</code></td>
<td>
<p>optional. You can provide a data table, then the function returns the names of coordinate columns in this data table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector with column names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>obs_dimvars

</code></pre>

<hr>
<h2 id='PCC'>Pearson Correlation Coefficient</h2><span id='topic+PCC'></span>

<h3>Description</h3>

<p>Function for calculating Pearson correlation coefficients (PCCs) of ensemble mean forecasts stored in long data tables.
Can also handle point forecasts.
This metric always needs several years of data since the means and standard deviations are calculated across time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PCC(
  dt,
  f,
  o = "obs",
  by = by_cols_ens_fc_score(dt),
  pool = "year",
  mem = "member",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PCC_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="PCC_+3A_f">f</code></td>
<td>
<p>column name of the prediction.</p>
</td></tr>
<tr><td><code id="PCC_+3A_o">o</code></td>
<td>
<p>column name of the observations.</p>
</td></tr>
<tr><td><code id="PCC_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt. A separate PCC is computed for each value of the grouping variables.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="PCC_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged. Needs to contain 'year' per warning above.</p>
</td></tr>
<tr><td><code id="PCC_+3A_mem">mem</code></td>
<td>
<p>Name of the column identifying the ensemble member. Only used if check_dimension is TRUE. Is NULL for a point forecast.</p>
</td></tr>
<tr><td><code id="PCC_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If True, a simple test whether the dimensions match up is conducted:
The data table should only have one row for each level of c(by,pool,mem)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(fc = 1:4,obs = c(4,4,7,7),member = c(1,2,1,2),year = c(1999,1999,2000,2000))
PCC(dt,f = 'fc')

</code></pre>

<hr>
<h2 id='profit_graph'>(Accumulative) profit graphs</h2><span id='topic+profit_graph'></span>

<h3>Description</h3>

<p>These graphs really only make sense if you have 50 or less observations.
Typical application would be when you compare seasonal mean forecasts to station data for a single location.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>profit_graph(
  dt,
  accumulative = TRUE,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = NULL,
  pool = setdiff(dimvars(dt), by),
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="profit_graph_+3A_dt">dt</code></td>
<td>
<p>Data table containing tercile forecasts</p>
</td></tr>
<tr><td><code id="profit_graph_+3A_accumulative">accumulative</code></td>
<td>
<p>Logic. Should the accumulative profit be plotted or the profit per forecast?</p>
</td></tr>
<tr><td><code id="profit_graph_+3A_f">f</code></td>
<td>
<p>column names of the prediction columns</p>
</td></tr>
<tr><td><code id="profit_graph_+3A_o">o</code></td>
<td>
<p>column name of the observation column</p>
</td></tr>
<tr><td><code id="profit_graph_+3A_by">by</code></td>
<td>
<p>column names of grouping variables. Default is NULL.</p>
</td></tr>
<tr><td><code id="profit_graph_+3A_pool">pool</code></td>
<td>
<p>column names of pooling variables (used for the dimension check). Default is all dimvars.</p>
</td></tr>
<tr><td><code id="profit_graph_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function checks whether the columns in by and pool span the entire data table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of gg objects which can be plotted by ggpubr::ggarrange (for example)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
p1 = profit_graph(dt)
p2 = profit_graph(dt,accumulative = FALSE)

if(interactive()){
plot(p1)
plot(p2)
}

</code></pre>

<hr>
<h2 id='REL'>Reliability score</h2><span id='topic+REL'></span>

<h3>Description</h3>

<p>Computes both the reliability component of the Brier score or reliability component of the Ignorance score.
Mason claims to prefer the ignorance score version, but this has a very high chance of being NA. Mason writes that the
scores are unstable for single locations and that one should pool over many locations.
Requires the specification of probability bins. One score for each category (below, normal, above) and
also the sum of the scores.
</p>
<p>Values close to 0 indicate reliable forecasts. Higher values mean less reliable forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>REL(
  dt,
  bins = c(0.3, 0.35001),
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="REL_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="REL_+3A_bins">bins</code></td>
<td>
<p>probability bins, defaults to (&quot;&lt;30&quot;, &quot;30-35&quot;,&quot;&gt;35&quot;) which is given as c(0.30, 0.35001).</p>
</td></tr>
<tr><td><code id="REL_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="REL_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in obs_dt, or in dt if obs_dt = NULL). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="REL_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="REL_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="REL_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                year = 1:3)
print(dt)
REL(dt)

</code></pre>

<hr>
<h2 id='rel_diag'>Reliability Diagrams for tercile forecasts</h2><span id='topic+rel_diag'></span>

<h3>Description</h3>

<p>Creates reliability diagrams from a data table containing tercile forecasts
It wraps <code>rel_diag_vec</code>, see <code>?rel_diag_vec</code> for more details.
about the output diagrams. The output format is very much inspired by Figure 5 of Mason&amp;2018. By default, 4 diagrams are drawn,
one for each the prediction of above-, normal- and below-values, plus one for  all forecasts together.
You can provide a 'by' argument to obtain separate reliability diagrams for different values of the by-columns. E.g., when you data table contains
a column named 'season', you can set by = 'season'. Then, the function will output a list of 16 diagrams, 4 for each season.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rel_diag(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = NULL,
  pool = setdiff(dimvars(dt), by),
  binwidth = 0.05,
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rel_diag_+3A_dt">dt</code></td>
<td>
<p>Data table containing tercile forecasts</p>
</td></tr>
<tr><td><code id="rel_diag_+3A_f">f</code></td>
<td>
<p>column names of the prediction columns</p>
</td></tr>
<tr><td><code id="rel_diag_+3A_o">o</code></td>
<td>
<p>column name of the observation column</p>
</td></tr>
<tr><td><code id="rel_diag_+3A_by">by</code></td>
<td>
<p>column names of grouping variables. Default is to not group.</p>
</td></tr>
<tr><td><code id="rel_diag_+3A_pool">pool</code></td>
<td>
<p>column names of pooling variables (used for the dimension check). Default is all dimvars.</p>
</td></tr>
<tr><td><code id="rel_diag_+3A_binwidth">binwidth</code></td>
<td>
<p>bin width for discretizing probabilities.</p>
</td></tr>
<tr><td><code id="rel_diag_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function checks whether the columns in by and pool span the entire data table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of gg objects which can be plotted by ggpubr::ggarrange (for example)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
pp = rel_diag(dt)
if(interactive()) plot(pp)


</code></pre>

<hr>
<h2 id='rel_diag_vec'>Reliability diagram from vectors of probabilities and observations</h2><span id='topic+rel_diag_vec'></span>

<h3>Description</h3>

<p>The probabilities have to be rounded beforehand (see <code>round_probs</code>), because the diagram draws a point for each level of the probabilities. The diagram includes a histogram indicating
the forecast relative frequency for each probability bin. The diagram shows the reliability curve and the diagonal for reference.
Moreover, it shows a regression line fitted by weighted linear regression where the forecast relative frequencies are used as weights.
A horizontal and vertical line indicate the frequency of observation = TRUE over the entire dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rel_diag_vec(discrete_probs, obs, slope_only = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rel_diag_vec_+3A_discrete_probs">discrete_probs</code></td>
<td>
<p>Vector of (rounded) probabilites.</p>
</td></tr>
<tr><td><code id="rel_diag_vec_+3A_obs">obs</code></td>
<td>
<p>Vector of logical observations.</p>
</td></tr>
<tr><td><code id="rel_diag_vec_+3A_slope_only">slope_only</code></td>
<td>
<p>logical. If set to TRUE, only the slope of the reliability curve is returned</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A gg object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>discrete_probs = seq(0,1,length.out = 5)
obs = c(FALSE,FALSE,TRUE,TRUE,TRUE)
pp = rel_diag_vec(discrete_probs,obs)
if(interactive()) plot(pp)


</code></pre>

<hr>
<h2 id='RES'>Resolution score</h2><span id='topic+RES'></span>

<h3>Description</h3>

<p>Computes both the resolution component of the Brier score or resolution component of the Ignorance score.
Mason claims to prefer the ignorance score version, but this has a very high chance of being NA (much higher
than for the full ignorance score itself, I think we should drop it for that reason). Mason writes that the
scores are unstable for single locations and that one should pool over many locations.
Requires the specification of probability bins. One score for each category (below, normal, above) and
also the sum of the scores.
Values close to 0 means low resolution. Higher values mean higher resolution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RES(
  dt,
  bins = c(0.3, 0.35001),
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RES_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="RES_+3A_bins">bins</code></td>
<td>
<p>probability bins, defaults to c(&quot;&lt;30&quot;, &quot;30-35&quot;,&quot;&gt;35&quot;)</p>
</td></tr>
<tr><td><code id="RES_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="RES_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in obs_dt, or in dt if obs_dt = NULL). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="RES_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="RES_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="RES_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                year = 1:3)
print(dt)
RES(dt)
</code></pre>

<hr>
<h2 id='restrict_to_country'>restricts data to a specified country</h2><span id='topic+restrict_to_country'></span>

<h3>Description</h3>

<p>Restricts a dataset to one or more countries, specified by their names. If you have lon/lat data and don't know
which countries these coordinates belong to, see <code><a href="#topic+add_country_names">add_country_names</a></code>. Can restrict data to a rectangle around a given country
as well (usually looks nicer for plotting).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>restrict_to_country(dt, ct, rectangle = FALSE, tol = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="restrict_to_country_+3A_dt">dt</code></td>
<td>
<p>the data table.</p>
</td></tr>
<tr><td><code id="restrict_to_country_+3A_ct">ct</code></td>
<td>
<p>name of the country, or vector containing multiple country names</p>
</td></tr>
<tr><td><code id="restrict_to_country_+3A_rectangle">rectangle</code></td>
<td>
<p>logical. If FALSE (default), the data is restricted to the gridcells for which the centerpoint lies within the selected country (e.g. for computing mean scores for a country).
If TRUE, the data is kept for a rectangle containing the entire country, therefore also containing gridpoints outside the country. This is the preferred option for plotting data
for a specific country.</p>
</td></tr>
<tr><td><code id="restrict_to_country_+3A_tol">tol</code></td>
<td>
<p>Only used when <code>rectangle == TRUE</code>. A tolerance value for widening the plotting window, making things look a bit nicer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the data table, restricted to the selected country
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example data:
ex_dt = chirps_monthly[lat &lt; 0 &amp; month == 11 &amp; year == 2020]
dt = restrict_to_country(ex_dt,'Kenya')

</code></pre>

<hr>
<h2 id='restrict_to_GHA'>restricts data to the Greater Horn of Africa</h2><span id='topic+restrict_to_GHA'></span><span id='topic+restrict_to_confer_region'></span>

<h3>Description</h3>

<p>Wraps <code><a href="#topic+restrict_to_country">restrict_to_country</a></code>, and restricts to the GHA-region usually considered in CONFER, see <code><a href="#topic+EA_country_names">EA_country_names</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>restrict_to_GHA(dt, ...)

restrict_to_confer_region(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="restrict_to_GHA_+3A_dt">dt</code></td>
<td>
<p>the data table.</p>
</td></tr>
<tr><td><code id="restrict_to_GHA_+3A_...">...</code></td>
<td>
<p>passed on to <code><a href="#topic+restrict_to_country">restrict_to_country</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the data table, restricted to the selected country
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ex_dt = chirps_monthly[lat &lt; 0 &amp; month == 11 &amp; year == 2020]
dt = restrict_to_GHA(ex_dt)

</code></pre>

<hr>
<h2 id='ROC_curve'>ROC curve for tercile forecasts</h2><span id='topic+ROC_curve'></span>

<h3>Description</h3>

<p>Creates ROC curves from a data table containing tercile forecasts. It wraps <code>roc_curve_vec</code>.
By default, 4 ROC-curves are drawn, one for each the prediction of above-, normal- and below-values, plus one for all forecasts together.
You can provide a 'by' argument to obtain separate ROC-curves for different values of the by-columns. E.g., when your data table contains
a column named 'season', you can set by = 'season'. Then, the function will output a list of 16 ROC-curvess, 4 for each season.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ROC_curve(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = NULL,
  pool = setdiff(dimvars(dt), by),
  interpolate = TRUE,
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROC_curve_+3A_dt">dt</code></td>
<td>
<p>Data table containing tercile forecasts</p>
</td></tr>
<tr><td><code id="ROC_curve_+3A_f">f</code></td>
<td>
<p>column names of the prediction columns</p>
</td></tr>
<tr><td><code id="ROC_curve_+3A_o">o</code></td>
<td>
<p>column name of the observation column</p>
</td></tr>
<tr><td><code id="ROC_curve_+3A_by">by</code></td>
<td>
<p>column names of grouping variables. Default is to not group.</p>
</td></tr>
<tr><td><code id="ROC_curve_+3A_pool">pool</code></td>
<td>
<p>column names of pooling variables (used for the dimension check). Default is all dimvars.</p>
</td></tr>
<tr><td><code id="ROC_curve_+3A_interpolate">interpolate</code></td>
<td>
<p>Logical. If TRUE, the curve connects the dots making up the ROC curve (which looks nicer), if not a step function is drawn (which is closer to the mathematical definition of the ROC curve).</p>
</td></tr>
<tr><td><code id="ROC_curve_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function checks whether the columns in by and pool span the entire data table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of gg objects which can be plotted by <code>ggpubr::ggarrange</code> (for example)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
pp = ROC_curve(dt)
if(interactive()) plot(pp)

</code></pre>

<hr>
<h2 id='roc_curve_vec'>ROC curves</h2><span id='topic+roc_curve_vec'></span>

<h3>Description</h3>

<p>Plot the ROC-curve for a vector of probabilities and corresponding observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_curve_vec(probs, obs, interpolate = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_curve_vec_+3A_probs">probs</code></td>
<td>
<p>vector with probabilities (between 0 and 1)</p>
</td></tr>
<tr><td><code id="roc_curve_vec_+3A_obs">obs</code></td>
<td>
<p>vector with categorical observations</p>
</td></tr>
<tr><td><code id="roc_curve_vec_+3A_interpolate">interpolate</code></td>
<td>
<p>logical. If TRUE the ROC-curve is interpolated and drawn as a continuous function. Otherwise it is drawn as a step function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a gg object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>probs = seq(0,1,length.out = 5)
obs = c(FALSE,FALSE,TRUE,FALSE,TRUE)
pp = roc_curve_vec(probs,obs)
if(interactive()) plot(pp)

</code></pre>

<hr>
<h2 id='roc_score_vec'>ROC score (AUC)</h2><span id='topic+roc_score_vec'></span>

<h3>Description</h3>

<p>Calculates the area under curve (AUC) or ROC-score from a vector of probabilities and corresponding observations.
Formula (1a) from Mason&amp;2018 is used in the calculation, corresponding to trapezoidal interpolation.
This is mostly an auxiliary function for the ROCS function, but also used in the ROC-diagram function, where the AUC is added to the diagrams.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_score_vec(probs, obs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_score_vec_+3A_probs">probs</code></td>
<td>
<p>vector with probabilities (between 0 and 1)</p>
</td></tr>
<tr><td><code id="roc_score_vec_+3A_obs">obs</code></td>
<td>
<p>vector with categorical observations (as TRUE/FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric. The ROC score.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>roc_score_vec(probs = c(0.1,0.6,0.3,0.4),
             obs = c(FALSE,TRUE,TRUE,FALSE))
</code></pre>

<hr>
<h2 id='ROCS'>ROC-score/Area Under Curve(AUC)</h2><span id='topic+ROCS'></span>

<h3>Description</h3>

<p>This score is not proper, but can be used to assess the resolution of a tercile forecast.
The ROC score requires more datapoints to be robust than e.g. the ignorance or Brier score. Therefore the default is to pool the data in space and only calculate one score per season.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ROCS(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score_sp(dt),
  pool = c("year", space_dimvars(dt)),
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROCS_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="ROCS_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="ROCS_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in obs_dt, or in dt if obs_dt = NULL). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="ROCS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="ROCS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="ROCS_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
ROCS(dt)

</code></pre>

<hr>
<h2 id='round_probs'>auxiliary function for rounding probabilities</h2><span id='topic+round_probs'></span>

<h3>Description</h3>

<p>takes a vector of probabilities (between 0 and 1) and rounds them to the scale specified by binwidth. This is used for reliability diagrams,
where one point is drawn for each bin. 0 is always at the center of the first interval for rounding:
E.g. if binwidth = 0.05 (the default), then probabilities up to 0.025 are rounded to 0, probs between 0.025 and 0.075 are rounded to 0.05, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>round_probs(probs, binwidth = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="round_probs_+3A_probs">probs</code></td>
<td>
<p>vector of probabilities (between 0 and 1, not percent)</p>
</td></tr>
<tr><td><code id="round_probs_+3A_binwidth">binwidth</code></td>
<td>
<p>width of the bins for rounding.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with rounded probabilities
</p>


<h3>Examples</h3>

<pre><code class='language-R'>round_probs(c(0.001,0.7423))
</code></pre>

<hr>
<h2 id='RPS'>Ranked Probability score</h2><span id='topic+RPS'></span>

<h3>Description</h3>

<p>This score is suitable for tercile category forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RPS(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RPS_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="RPS_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="RPS_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in obs_dt, or in dt if obs_dt = NULL). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="RPS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="RPS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="RPS_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                year = 1:3)
print(dt)
RPS(dt)
</code></pre>

<hr>
<h2 id='RPSS'>Ranked Probability skill score</h2><span id='topic+RPSS'></span>

<h3>Description</h3>

<p>This score is suitable for tercile category forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RPSS(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score(),
  pool = "year",
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RPSS_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="RPSS_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="RPSS_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in obs_dt, or in dt if obs_dt = NULL). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="RPSS_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="RPSS_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="RPSS_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>
<p>@examples
dt = data.table(below = c(0.5,0.3,0),
normal = c(0.3,0.3,0.7),
above = c(0.2,0.4,0.3),
tc_cat = c(-1,0,0),
year = 1:3)
print(dt)
RPSS(dt)
</p>

<hr>
<h2 id='run_dimension_check_ens_fc_score'>Auxiliary Function</h2><span id='topic+run_dimension_check_ens_fc_score'></span>

<h3>Description</h3>

<p>called inside functions that calculate scores
for ensemble forecasts. Checks whether the provided data table has the right format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_dimension_check_ens_fc_score()
</code></pre>

<hr>
<h2 id='run_dimension_check_terc_forecast'>Auxiliary Function</h2><span id='topic+run_dimension_check_terc_forecast'></span>

<h3>Description</h3>

<p>called inside functions that calculate scores
for ensemble forecasts. Checks whether the provided data table has the right format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_dimension_check_terc_forecast()
</code></pre>

<hr>
<h2 id='season_strings_to_int'>Auxiliary function for decoding season-strings</h2><span id='topic+season_strings_to_int'></span>

<h3>Description</h3>

<p>Auxiliary function for decoding season-strings
</p>


<h3>Usage</h3>

<pre><code class='language-R'>season_strings_to_int(seasons = c("MAM", "JJAS", "OND"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="season_strings_to_int_+3A_seasons">seasons</code></td>
<td>
<p>A character vector of season strings, see <code><a href="#topic+convert_monthly_to_seasonal">convert_monthly_to_seasonal()</a></code> for details</p>
</td></tr>
</table>

<hr>
<h2 id='set_spatial_grid'>Set Spatial Grid Attributes to a Data Table</h2><span id='topic+set_spatial_grid'></span>

<h3>Description</h3>

<p>This function creates the spatial grid attribute for a data table.
If the data table already has such an attribute, missing information is filled in.
In particular, the function checks whether a grid is regular, allowing for rounding errors in the grid coordinates, see details below.
By default the grid coordinates are rounded to a regular grid if they are very close to being regular.
While this sounds dangerous, it is almost always desirable to treat coordinates like that when working with data tables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_spatial_grid(
  dt,
  coor_cns = NULL,
  check_regular = TRUE,
  regular_tolerance = 1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_spatial_grid_+3A_dt">dt</code></td>
<td>
<p>A data table object.</p>
</td></tr>
<tr><td><code id="set_spatial_grid_+3A_coor_cns">coor_cns</code></td>
<td>
<p>Optional character vector of length two indicating the names of the spatial coordinates
within the data table in order <code>x</code>,<code>y</code>. Default (<code>NULL</code>) makes the function guess based on column names.</p>
</td></tr>
<tr><td><code id="set_spatial_grid_+3A_check_regular">check_regular</code></td>
<td>
<p>A logical indicating whether to check for regularity of the grid. This should essentially always be done but can be suppressed for speed.
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="set_spatial_grid_+3A_regular_tolerance">regular_tolerance</code></td>
<td>
<p>Value &gt;= 0 specifying the amount of rounding error we allow for still recognizing a grid as regular.
Given in percent of the minimum of <code>dx</code> and <code>dy</code>. Default is 1. Based on this value coordinates are rounded to the smallest after-comma-digit making them regular,
as long as this rounding introduces less error than <code>min(dx,dy)*regular_tolerance/100</code>.
Set this to <code>NULL</code> if you are absolutely certain that you don't want to round/change the grid. Doing this or decreasing this below 1 is not recommended, see details below.</p>
</td></tr>
<tr><td><code id="set_spatial_grid_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the grid information is printed out (by a call to <code><a href="#topic+grid_info">grid_info</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The grid attribute is a named list with (some of) the following pages:
</p>

<ul>
<li><p><code>coor_cns</code>: Character vector of length two specifying the names of the data-table-columns containing the spatial grids (in order x,y).
</p>
</li>
<li><p><code style="white-space: pre;">&#8288;x,y&#8288;</code>: Numeric vectors of all unique x- and y-coordinates in increasing order (NAs not included).
</p>
</li>
<li><p><code>regular</code>: Logical. Is the grid <em>regular</em>? See details below.
</p>
</li>
<li><p><code style="white-space: pre;">&#8288;dx,dy&#8288;</code>: Step sizes of the regular grid (only contained if <code>regular = TRUE</code>). By convention we set <code>dx</code> to 9999 if only one x-coordinate is present, likewise for <code>dy</code>.
</p>
</li>
<li><p><code>complete</code>: Logical. Is the regular grid <em>complete</em>? See details below.
</p>
</li></ul>

<p>We call a grid <em>regular</em> if there is a coordinate <code style="white-space: pre;">&#8288;(x0,y0)&#8288;</code> and positive values <code>dx</code>, <code>dy</code>,
such that each coordinate of the grid can be written as <code style="white-space: pre;">&#8288;(x0 + n*dx,y0 + m*dy)&#8288;</code> for integers <code>n</code>,<code>m</code>.
Importantly, a regular grid does not need to be &quot;a complete rectangle&quot;, we allow for missing coordinates, see details below.
We call it a <em>regular complete grid</em> if the grid contains these numbers for all integers <code>n</code>, <code>m</code> between some limits <code>n_min</code> and <code>n_max</code>,
respectively <code>m_min</code>, <code>m_max</code>.
</p>
<p>Checking regularity properly is a difficult problem, because we allow for missing coordinates
in the grid and allow for rounding errors.
For the treatment of rounding errors it is not recommended to set <code>regular_tolerance</code> to <code>NULL</code> or a very small value
(e.g. 0.1 or smaller). In this case, grids that are regular in praxis are frequently not recognized as regular:
Take for example the three x-coordinates 1, 1.5001, 2.4999. They are supposed to be rounded to 1 digit after
the comma and then the grid is regular with <code>dx = 0.5</code>. However, if <code>regular_tolerance</code> is NULL, the grid will be marked as irregular.
Similarly, if <code>regular_tolerance</code> is too small, the function is not allowed to make rounding errors of 0.0001
and the grid will also not be recognized as regular.
</p>
<p>When it comes to the issue of missing values in the grid, we are (deliberately) a bit sloppy and only check whether
the coordinates are part of a grid with <code>dx</code> being the minimum <code>x</code>-difference between two coordinates,
and similar <code>dy</code>. This may not detect regularity, when we have data that is sparse on a regular grid.
An example would be the three lon/lat coordinates <code>c(0,0)</code>, <code>c(2,0)</code>, <code>c(5,0)</code>. They clearly lie on the regular integer-lon/lat-
grid. However, the grid would show as not regular, because <code>dx</code> is not checked for smaller values than 2.
This choice is on purpose, since for most applications grids with many (or mostly) holes should be treated as irregular (e.g. plotting, upscaling, etc.).
The most important case of regular but not complete grids is gridded data that is restricted to a certain region, e.g. a country
or restricted to land. This is what we think of when we think of a regular incomplete grid, and for such data the check works perfectly.
</p>
<p>Note that at the very bottom it is the definition of regularity itself that is a bit tricky:
If we allow <code>dx</code>, <code>dy</code> to go all the way down to the machine-delta,
then pretty much any set of coordinates represented in a computer is part of a regular grid.
This hints at testing and detecting regularity actually depending on how small you're willing to make your <code>dx</code>,<code>dy</code>.
An example in 1 dimension: consider the three 1-dimensional coordinates <code>0</code>, <code>1</code>, and <code>m/n</code>, with <code>m</code> and <code>n</code> integers
without common divisors and <code>m&gt;n</code>. It is not difficult to see that these coordinates are part of a regular grid and that the
largest <code>dx</code> for detecting this is 1/n. This shows that you can have very small coordinate sets that are in theory regular, but their regularity
can be arbitrarily hard to detect. An example of a grid that is truely not regular are the three <code>x</code>-coordinates 0,1,a with a irrational.
</p>


<h3>Value</h3>

<p>Nothing, the attributes of dt are set in the parent environment. Moreover, the grid coordinates may be rounded If <code>regular</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(lon = 1:4, lat = rep(1:2,each = 2), some_data = runif(4))
print(dt)
attr(dt,'grid')

set_spatial_grid(dt)
attr(dt,'grid')

</code></pre>

<hr>
<h2 id='space_dimvars'>Auxiliary function</h2><span id='topic+space_dimvars'></span>

<h3>Description</h3>

<p>returns all column names indicating a spatial coordinate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>space_dimvars(dt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="space_dimvars_+3A_dt">dt</code></td>
<td>
<p>optional. You can provide a data table, then the function returns the names of spatial coordinate columns in this data table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector with column names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>space_dimvars()
</code></pre>

<hr>
<h2 id='SRC'>Compute the slope of the reliability curve</h2><span id='topic+SRC'></span>

<h3>Description</h3>

<p>Values below 1 indicate a lack of resolution or overconfidence, 1 is perfect, above means underconfident.
This score requires more datapoints to be robust than e.g. the ignorance or Brier score. Therefore the default is to pool the data in space and only calculate one score per season.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SRC(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = by_cols_terc_fc_score_sp(dt),
  pool = c("year", space_dimvars(dt)),
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SRC_+3A_dt">dt</code></td>
<td>
<p>Data table containing the predictions.</p>
</td></tr>
<tr><td><code id="SRC_+3A_f">f</code></td>
<td>
<p>column names of the prediction.</p>
</td></tr>
<tr><td><code id="SRC_+3A_o">o</code></td>
<td>
<p>column name of the observations (either in obs_dt, or in dt if obs_dt = NULL). The observation column needs to
contain -1 if it falls into the first category (corresponding to <code>fcs[1]</code>), 0 for the second and 1 for the third category.</p>
</td></tr>
<tr><td><code id="SRC_+3A_by">by</code></td>
<td>
<p>column names of grouping variables, all of which need to be columns in dt.
Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in dt.</p>
</td></tr>
<tr><td><code id="SRC_+3A_pool">pool</code></td>
<td>
<p>column name(s) for the variable(s) along which is averaged, typically just 'year'.</p>
</td></tr>
<tr><td><code id="SRC_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function tests whether the data table contains only one row per coordinate-level, as should be the case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table with the scores
</p>
<p>@examples
dt = data.table(below = c(0.5,0.3,0),
normal = c(0.3,0.3,0.7),
above = c(0.2,0.4,0.3),
tc_cat = c(-1,0,0),
year = 1:3)
print(dt)
SRC(dt)
</p>

<hr>
<h2 id='tc_cols'>Tercile column names</h2><span id='topic+tc_cols'></span>

<h3>Description</h3>

<p>which column names are interpreted as observed tercile categories
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tc_cols(dt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tc_cols_+3A_dt">dt</code></td>
<td>
<p>optional data table. If provided, the function guesses which column contains the observations. Else it returns all recognized column names.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector with column names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc_cols()

</code></pre>

<hr>
<h2 id='tendency_diag'>Tendency diagram from a data table containing tercile forecasts.</h2><span id='topic+tendency_diag'></span>

<h3>Description</h3>

<p>Tendency diagram from a data table containing tercile forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tendency_diag(
  dt,
  f = c("below", "normal", "above"),
  o = tc_cols(dt),
  by = NULL,
  pool = setdiff(dimvars(dt), by),
  dim.check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tendency_diag_+3A_dt">dt</code></td>
<td>
<p>Data table containing tercile forecasts</p>
</td></tr>
<tr><td><code id="tendency_diag_+3A_f">f</code></td>
<td>
<p>column names of the prediction columns</p>
</td></tr>
<tr><td><code id="tendency_diag_+3A_o">o</code></td>
<td>
<p>column name of the observation column</p>
</td></tr>
<tr><td><code id="tendency_diag_+3A_by">by</code></td>
<td>
<p>column names of grouping variables. Default is to not group.</p>
</td></tr>
<tr><td><code id="tendency_diag_+3A_pool">pool</code></td>
<td>
<p>column names of pooling variables (used for the dimension check). Default is all dimvars.</p>
</td></tr>
<tr><td><code id="tendency_diag_+3A_dim.check">dim.check</code></td>
<td>
<p>Logical. If TRUE, the function checks whether the columns in by and pool span the entire data table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If by == NULL a gg object, otherwise a list of gg objects that can be plotted by ggpubr::ggarrange (for example)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = data.table(below = c(0.5,0.3,0),
                normal = c(0.3,0.3,0.7),
                above = c(0.2,0.4,0.3),
                tc_cat = c(-1,0,0),
                lon = 1:3)
print(dt)
pp = tendency_diag(dt)
if(interactive()) plot(pp)
</code></pre>

<hr>
<h2 id='tercile_plot'>Function for plotting terciles</h2><span id='topic+tercile_plot'></span>

<h3>Description</h3>

<p>Function for plotting terciles
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tercile_plot(
  dt,
  data_col = tc_cols(dt),
  mn = NULL,
  low = "orange",
  mid = "cyan",
  high = "green1",
  name = "",
  labels = c("Wetter", "Average", "Drier"),
  na.value = "white",
  extent = NULL,
  expand.x = c(-0.5, 0.5),
  expand.y = c(-0.5, 2),
  dimension_check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tercile_plot_+3A_dt">dt</code></td>
<td>
<p>data table</p>
</td></tr>
<tr><td><code id="tercile_plot_+3A_data_col">data_col</code></td>
<td>
<p>Name of the column containing the observed tercile category</p>
</td></tr>
<tr><td><code id="tercile_plot_+3A_mn">mn</code></td>
<td>
<p>optional title for the plot.</p>
</td></tr>
<tr><td><code id="tercile_plot_+3A_low">low</code>, <code id="tercile_plot_+3A_mid">mid</code>, <code id="tercile_plot_+3A_high">high</code></td>
<td>
<p>colors for the three categories</p>
</td></tr>
<tr><td><code id="tercile_plot_+3A_name">name</code></td>
<td>
<p>optional title for the colorscale</p>
</td></tr>
<tr><td><code id="tercile_plot_+3A_labels">labels</code></td>
<td>
<p>How to label the three categories</p>
</td></tr>
<tr><td><code id="tercile_plot_+3A_na.value">na.value</code></td>
<td>
<p>How to color missing values</p>
</td></tr>
<tr><td><code id="tercile_plot_+3A_extent">extent</code></td>
<td>
<p>Optional vector of length 4 specifying the plotting borders in order xmin, xmax, ymin, ymax.</p>
</td></tr>
<tr><td><code id="tercile_plot_+3A_expand.x">expand.x</code>, <code id="tercile_plot_+3A_expand.y">expand.y</code></td>
<td>
<p>How far should the plotting borders be extended (beyond the data range)?</p>
</td></tr>
<tr><td><code id="tercile_plot_+3A_dimension_check">dimension_check</code></td>
<td>
<p>Logical. By default the function checks that there are not multiple values per coordinate
(and subsets to the first level if there are several, e.g. to the first year and month (by appearance in <code>dt</code>) if <code>dt</code> contains data for several years and months).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>dt = chirps_monthly[month == 12 &amp; lat &lt;0 &amp; year == 2018]
p = tercile_plot(dt = dt)
if(interactive()) plot(p)


</code></pre>

<hr>
<h2 id='tfc_from_efc'>Get tercile probability forecast from ensemble forecasts</h2><span id='topic+tfc_from_efc'></span>

<h3>Description</h3>

<p>The function takes a data table containing ensemble predictions and reduces it to predicted tercile probabilities.
The data table should either have a column 'tercile_cat' or it will be generated in the process (by <code><a href="#topic+add_tercile_cat">add_tercile_cat</a></code>).
In particular, if you don't know the tercile category of the ensemble predictions, your data table should contain hindcasts as well,
such that the tercile categories are calculated correctly.
The probability for 'below', for example, is the fraction of ensemble members predicting below normal (for this coordinate).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfc_from_efc(dt, by = setdiff(dimvars(dt), "member"), keep_cols = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfc_from_efc_+3A_dt">dt</code></td>
<td>
<p>The data table.</p>
</td></tr>
<tr><td><code id="tfc_from_efc_+3A_by">by</code></td>
<td>
<p>Names of columns to group by.</p>
</td></tr>
<tr><td><code id="tfc_from_efc_+3A_keep_cols">keep_cols</code></td>
<td>
<p>A vector of column names that you want to keep. Column names in by are kept automatically.</p>
</td></tr>
<tr><td><code id="tfc_from_efc_+3A_...">...</code></td>
<td>
<p>passed on to <code><a href="#topic+add_tercile_probs">add_tercile_probs</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new data table with tercile forecasts
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test_dt = ecmwf_monthly[lat &lt; 0 &amp; month == 11]
tfc = tfc_from_efc(test_dt)


</code></pre>

<hr>
<h2 id='tfc_gha_plot'>Plotting function with different map for Greater Horn of Africa</h2><span id='topic+tfc_gha_plot'></span>

<h3>Description</h3>

<p>This function wraps <code><a href="#topic+tfc_plot">tfc_plot()</a></code>, but uses a different map for borders.
The map is part of the package and is the one currently used during GHACOFs at ICPAC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfc_gha_plot(
  ...,
  expand.x = c(-0.5, 0.5),
  expand.y = c(-0.5, 2),
  showplot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfc_gha_plot_+3A_...">...</code>, <code id="tfc_gha_plot_+3A_expand.x">expand.x</code>, <code id="tfc_gha_plot_+3A_expand.y">expand.y</code>, <code id="tfc_gha_plot_+3A_showplot">showplot</code></td>
<td>
<p>passed to <code><a href="#topic+tfc_plot">tfc_plot()</a></code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>dt = tfc_from_efc(ecmwf_monthly[month == 11 &amp; lat &lt; 0])
pp = tfc_gha_plot(dt[year == 2018], expand.y = c(0.5,0.5))
if(interactive()) plot(pp)

</code></pre>

<hr>
<h2 id='tfc_plot'>plotting function for tercile forecasts</h2><span id='topic+tfc_plot'></span>

<h3>Description</h3>

<p>Plots spatial maps of tercile forecasts. Requires a data table with three columns <code>"below"</code>, <code>"normal"</code>, <code>"above"</code>
which sum to 1. For each gridpoint only the highest of the three values is plotted, so there are three colorscales.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfc_plot(
  dt,
  discrete_cs = TRUE,
  rmax = NULL,
  below = "brown",
  normal = "gold",
  above = "forestgreen",
  na.value = "gray75",
  cs_names = c("below", "normal", "above"),
  oob = NULL,
  guide_barwidth = grid::unit(0.01, units = "npc"),
  guide_barheight = grid::unit(0.15, units = "npc"),
  legend_horizontal = FALSE,
  binwidth = "auto",
  add_map = TRUE,
  extent = NULL,
  expand.x = c(0, 0),
  expand.y = c(0, 0),
  showplot = TRUE,
  dimension_check = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfc_plot_+3A_dt">dt</code></td>
<td>
<p>Data table containing the data for plotting.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_discrete_cs">discrete_cs</code></td>
<td>
<p>Logical. Do you want to use discrete color scales (default) or not.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_rmax">rmax</code></td>
<td>
<p>Optional value to fix the range of the colorscale (lower limit is always 0.33).</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_below">below</code>, <code id="tfc_plot_+3A_normal">normal</code>, <code id="tfc_plot_+3A_above">above</code></td>
<td>
<p>Colors to use for the three categories. Default is <code>'brown'</code>, <code>'gold'</code>, <code>'forestgreen'</code>.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_na.value">na.value</code></td>
<td>
<p>Color to use for missing value. Default is <code>'gray75'</code>.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_cs_names">cs_names</code></td>
<td>
<p>Character vector of length three giving the legend titles for the below-, normal-, and above category.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_oob">oob</code></td>
<td>
<p>Behavior for data above <code>r_max</code>. Passed to <code><a href="ggplot2.html#topic+scale_colour_continuous">ggplot2::scale_fill_continuous()</a></code> if <code>discrete_cs == FALSE</code> or else to <code><a href="ggplot2.html#topic+scale_steps">ggplot2::scale_fill_steps()</a></code>.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_guide_barwidth">guide_barwidth</code>, <code id="tfc_plot_+3A_guide_barheight">guide_barheight</code></td>
<td>
<p>value to specify the width and height of the color guide. Are flipped if <code>legend_horizontal</code> is <code>TRUE</code>. Use <code>units(...,"npc")</code> to make it work across all output devices.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_legend_horizontal">legend_horizontal</code></td>
<td>
<p>Logical. Set to <code>TRUE</code> to show the legend horizontally underneath the plot.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_binwidth">binwidth</code></td>
<td>
<p>Width of the steps when a discrete colorscale is used.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_add_map">add_map</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>, mostly for internal use. Set to <code>FALSE</code> to remove borders (e.g. if you want to add them yourself from a shapefile).</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_extent">extent</code></td>
<td>
<p>An optional four-element vector in the order xmin,xmax,ymin,ymax for specifying the spatial extent of the plot. Default is to fit the extent to the data.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_expand.x">expand.x</code>, <code id="tfc_plot_+3A_expand.y">expand.y</code></td>
<td>
<p>vectors with two entries to be added to xlims/ylims of the plot. E.g. expand.x = c(-0.5,0.5)
expands the plot by half a longitude both on the right and left hand side.</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_showplot">showplot</code></td>
<td>
<p>Logical. Should the plot be displayed at the end?</p>
</td></tr>
<tr><td><code id="tfc_plot_+3A_dimension_check">dimension_check</code></td>
<td>
<p>Logical. By default the function checks that there are not multiple values per coordinate
(and subsets to the first level if there are several, e.g. to the first year and month (by appearance in <code>dt</code>) if <code>dt</code> contains data for several years and months).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a ggplot object.
</p>


<h3>Author(s)</h3>

<p>Claudio Heinrich
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#dt = tfc_from_efc(ecmwf_monthly[month == 11 &amp; lat &lt; 0])
#pp = tfc_plot(dt[year == 2018])
#if(interactive()) plot(pp)

</code></pre>

<hr>
<h2 id='time_dimvars'>Auxiliary function</h2><span id='topic+time_dimvars'></span>

<h3>Description</h3>

<p>returns all column names indicating a temporal coordinate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>time_dimvars(dt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="time_dimvars_+3A_dt">dt</code></td>
<td>
<p>optional. You can provide a data table, then the function returns the names of temporal coordinate columns in this data table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector with column names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>time_dimvars()

</code></pre>

<hr>
<h2 id='upscale_chirps'>Upscales monthly CHIRPS data to a coarser grid</h2><span id='topic+upscale_chirps'></span>

<h3>Description</h3>

<p>this is mostly auxiliary and called from download_chirps_monthly.
Uses the function upscale_regular_lon_lat, but derives the weights for upscaling only once for efficiency and avoids simultaneous loading of all CHIRPS data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>upscale_chirps(
  update = TRUE,
  years = NULL,
  months = NULL,
  upscale_grid = data.table(expand.grid(lon = seq(GHA_extent()[1], GHA_extent()[2], 0.5),
    lat = seq(GHA_extent()[3], GHA_extent()[4], 0.5))),
  root_dir = NULL,
  version = "UCSB",
  us_dir = file.path(root_dir, "upscaled")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="upscale_chirps_+3A_update">update</code></td>
<td>
<p>Logical, if TRUE, files that have already been upscaled are skipped</p>
</td></tr>
<tr><td><code id="upscale_chirps_+3A_years">years</code>, <code id="upscale_chirps_+3A_months">months</code></td>
<td>
<p>Which years and months do you want to upscale? NULL upscales everything there is (except if update is TRUE).</p>
</td></tr>
<tr><td><code id="upscale_chirps_+3A_upscale_grid">upscale_grid</code></td>
<td>
<p>A regular lon/lat grid for upscaling. Defaults to half degrees.</p>
</td></tr>
<tr><td><code id="upscale_chirps_+3A_root_dir">root_dir</code></td>
<td>
<p>directory where the high-resolution file is stored.</p>
</td></tr>
<tr><td><code id="upscale_chirps_+3A_version">version</code></td>
<td>
<p>Version specifier, should be 'UCSB' or 'ICPAC'. The latter only works if you have access to CHIRPS blended.</p>
</td></tr>
<tr><td><code id="upscale_chirps_+3A_us_dir">us_dir</code></td>
<td>
<p>Directory where the low-resolution file will be stored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive()){
upscale_chirps()
}

</code></pre>

<hr>
<h2 id='upscale_regular_lon_lat'>Function for matching data between different grids</h2><span id='topic+upscale_regular_lon_lat'></span>

<h3>Description</h3>

<p>Upscales data from one regular lon-lat grid to another lon-lat grid that is coarser or of the same resolution.
It uses conservative interpolation (rather than bilinear interpolation) which is the better choice for upscaling, see details below.
If the fine grid and coarse grid are of the same resolution but shifted, results are (almost) identical to bilinear interpolation
(almost because bilinear interpolation does not account for the fact that grid cells get smaller towards the pole, which this function does).
</p>
<p>The function addresses the following major challenges:
</p>

<ul>
<li><p> The fine grid does not need to be nested in the coarse grid, creating different partial overlap scenarios.
Therefore, the value of each fine grid cell may contribute to multiple (up to four) coarse grid cells.
</p>
</li>
<li><p> Grid cell area varies with latitude, grid cells at the equator are much larger than at the poles.
This affects the contribution of grid cells (grid cells closer to the pole contribute less to the coarse grid cell average).
</p>
</li>
<li><p> Frequently, it is required to upscale <em>repeated</em> data between the same grids, for example when you want to upscale observations for many different years.
In this case, the calculation of grid cell overlaps is only done once, and not repeated every time.
</p>
</li>
<li><p> For coarse grid cells that are only partially covered, a minimal required fraction of coverage can be specified.
</p>
</li>
<li><p> It is memory efficient: Naive merging of data tables or distance-based matching of grid cells is avoided, since it results in
unnecessary large lookup tables that may not fit into memory when both your fine and your coarse grid are high-resolution.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>upscale_regular_lon_lat(
  dt,
  coarse_grid,
  uscols,
  bycols = setdiff(dimvars(dt), c("lon", "lat")),
  save_weights = NULL,
  req_frac_of_coverage = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="upscale_regular_lon_lat_+3A_dt">dt</code></td>
<td>
<p>data table containing the data you want to upscale.</p>
</td></tr>
<tr><td><code id="upscale_regular_lon_lat_+3A_coarse_grid">coarse_grid</code></td>
<td>
<p>data table containing lons/lats of the grid you want to upscale to.</p>
</td></tr>
<tr><td><code id="upscale_regular_lon_lat_+3A_uscols">uscols</code></td>
<td>
<p>column name(s) of the data you want to upscale (can take multiple columns at once, but assumes that the different columns have missing values at the same position).</p>
</td></tr>
<tr><td><code id="upscale_regular_lon_lat_+3A_bycols">bycols</code></td>
<td>
<p>optional column names for grouping if you have repeated data on the same grid, e.g. use bycols = 'date' if your data table contains observations for many dates on the same grid (and the column specifying the date is in fact called 'date').</p>
</td></tr>
<tr><td><code id="upscale_regular_lon_lat_+3A_save_weights">save_weights</code></td>
<td>
<p>optional file name for saving the weights for upscaling. Used for the CHIRPS data.</p>
</td></tr>
<tr><td><code id="upscale_regular_lon_lat_+3A_req_frac_of_coverage">req_frac_of_coverage</code></td>
<td>
<p>Numeric value between 0 and 1. All coarse grid cells with less coverage than this value get assigned a missing value. In particular, setting this to 0 (the default) means a value is assigned to each coarse grid cell
that overlaps with at least one fine grid cell. Setting this to 1 means only coarse grid cells are kept for which we have full coverage.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bilinear interpolation is generally not appropriate for mapping data from finer to coarser grids.
The reason is that in BI, the value of a coarse grid cell only depends on the four fine grid cells surrounding its center coordinate,
even though many fine grid cells may overlap the coarse grid cell).
Conservative interpolation calculates the coarse grid cell value by averaging all fine grid cells overlapping with it, weighted by
the fraction of overlap. This is the appropriate way of upscaling when predictions and observations constitute grid point averages, which is usually the case (Göber et al. 2008).
</p>
<p>The grids are assumed to be <em>regular</em>, but are not required to be <em>complete</em> (see <code><a href="#topic+set_spatial_grid">set_spatial_grid</a></code>).
The function is faster when missing-data grid points are not contained in <code>dt</code> (then fewer grid points need to be matched).
</p>


<h3>Value</h3>

<p>A data table with the upscaled values.
</p>


<h3>References</h3>

<p>Göber, M., Ervin Z., and Richardson, D.S. (2008): <em>&quot;Could a perfect model ever satisfy a naïve forecaster? On grid box mean versus point verification.&quot;</em> Meteorological Applications: A journal of forecasting, practical applications, training techniques and modelling 15, no. 3 (2008): 359-365.
</p>

<hr>
<h2 id='ver_map'>Plot a verification map of percentiles</h2><span id='topic+ver_map'></span>

<h3>Description</h3>

<p>For each location, the map shows whether the observed value was normal, below, or above. This makes it possible to visually compare to the usual tercile forecsst
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ver_map(
  dt,
  o = obs_cols(dt),
  yy = dt[, max(year)],
  climatology_period = unique(dt[, year]),
  out_file = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ver_map_+3A_dt">dt</code></td>
<td>
<p>input data table. This has to contain the observations for the year to plot, as well as for many other years (which are used to calculate the climatological reference).
The data table should have coumns named <code>lon</code>, <code>lat</code>, <code>year</code>, and an observation column, the name of which is passed as value of <code>o</code> to the function, see below.
For each level of <code>lon</code>, <code>lat</code>, and <code>year</code>, the table should only contain one row (this is checked by the function).</p>
</td></tr>
<tr><td><code id="ver_map_+3A_o">o</code></td>
<td>
<p>name of the column containing the observation.</p>
</td></tr>
<tr><td><code id="ver_map_+3A_yy">yy</code></td>
<td>
<p>The year for which to show the verification map. Defaults to the last year available in dt</p>
</td></tr>
<tr><td><code id="ver_map_+3A_climatology_period">climatology_period</code></td>
<td>
<p>which years should the climatology be calculated on? Defaults to all years (except <code>yy</code>) in <code>dt</code></p>
</td></tr>
<tr><td><code id="ver_map_+3A_out_file">out_file</code></td>
<td>
<p>optional path and file name (including valid filetype, like .pdf or .png) for saving the file. If not provided, the function just shows the plot in the running R session.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a gg object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# takes some time:
pp = ver_map(chirps_monthly[month == 11],yy = 2018)
if(interactive()) plot(pp)

</code></pre>

<hr>
<h2 id='ver_map_chirps'>Plot a verification map of percentiles based on precomputed CHIRPS quantiles.</h2><span id='topic+ver_map_chirps'></span>

<h3>Description</h3>

<p>The quantiles should be computed and saved by the function <code>chirps_ver_map_quantiles</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ver_map_chirps(
  mm = month(Sys.Date() - 60),
  yy = year(Sys.Date() - 60),
  version = "UCSB",
  resolution = "low",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ver_map_chirps_+3A_yy">yy</code>, <code id="ver_map_chirps_+3A_mm">mm</code></td>
<td>
<p>The year and month for which to show the verification map. Defaults to the month 60 days ago (in order to avoid using preliminary data).</p>
</td></tr>
<tr><td><code id="ver_map_chirps_+3A_version">version</code></td>
<td>
<p>which CHIRPS version to use.</p>
</td></tr>
<tr><td><code id="ver_map_chirps_+3A_resolution">resolution</code></td>
<td>
<p>Spatial resolution, 'high' or 'low'</p>
</td></tr>
<tr><td><code id="ver_map_chirps_+3A_...">...</code></td>
<td>
<p>passed on to ver_map.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A gg object
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # takes a while:
if(interactive()) ver_map_chirps(mm = 12,yy = 2022)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
