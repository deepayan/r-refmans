<!DOCTYPE html><html lang="en"><head><title>Help for package RobustPrediction</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RobustPrediction}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#RobustPrediction'><p>Package Title: Robust Tuning and Training for Cross-Source Prediction</p></a></li>
<li><a href='#sample_data_extern'><p>Sample External Validation Data Subset</p></a></li>
<li><a href='#sample_data_train'><p>Sample Training Data Subset</p></a></li>
<li><a href='#tuneandtrain'><p>Tune and Train Classifier</p></a></li>
<li><a href='#tuneandtrainExt'><p>Tune and Train Classifier by Tuning Method Ext</p></a></li>
<li><a href='#tuneandtrainExtBoost'><p>Tune and Train External Boosting</p></a></li>
<li><a href='#tuneandtrainExtLasso'><p>Tune and Train External Lasso</p></a></li>
<li><a href='#tuneandtrainExtRF'><p>Tune and Train External Random Forest</p></a></li>
<li><a href='#tuneandtrainExtRidge'><p>Tune and Train External Ridge</p></a></li>
<li><a href='#tuneandtrainExtSVM'><p>Tune and Train External SVM</p></a></li>
<li><a href='#tuneandtrainInt'><p>Tune and Train by tuning method Int</p></a></li>
<li><a href='#tuneandtrainIntBoost'><p>Tune and Train Internal Boosting</p></a></li>
<li><a href='#tuneandtrainIntLasso'><p>Tune and Train Internal Lasso</p></a></li>
<li><a href='#tuneandtrainIntRF'><p>Tune and Train Internal Random Forest</p></a></li>
<li><a href='#tuneandtrainIntRidge'><p>Tune and Train Internal Ridge</p></a></li>
<li><a href='#tuneandtrainIntSVM'><p>Tune and Train Internal SVM</p></a></li>
<li><a href='#tuneandtrainRobustTuneC'><p>Tune and Train Classifier by Tuning Method RobustTuneC</p></a></li>
<li><a href='#tuneandtrainRobustTuneCBoost'><p>Tune and Train RobustTuneC Boosting</p></a></li>
<li><a href='#tuneandtrainRobustTuneCLasso'><p>Tune and Train RobustTuneC Lasso</p></a></li>
<li><a href='#tuneandtrainRobustTuneCRF'><p>Tune and Train RobustTuneC Random Forest</p></a></li>
<li><a href='#tuneandtrainRobustTuneCRidge'><p>Tune and Train RobustTuneC Ridge</p></a></li>
<li><a href='#tuneandtrainRobustTuneCSVM'><p>Tune and Train RobustTuneC Support Vector Machine (SVM)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Robust Tuning and Training for Cross-Source Prediction</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.7</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yuting He &lt;yutingh19@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides robust parameter tuning and model training for predictive models applied across data sources where the data distribution varies slightly from source to source. This package implements three primary tuning methods: cross-validation-based internal tuning, external tuning, and the 'RobustTuneC' method. External tuning includes a conservative option where parameters are tuned internally on the training data and validating on an external dataset, providing a slightly pessimistic estimate. It supports Lasso, Ridge, Random Forest, Boosting, and Support Vector Machine classifiers. Currently, only binary classification is supported. The response variable must be the first column of the dataset and a factor with exactly two levels. The tuning methods are based on the paper by Nicole Ellenbach, Anne-Laure Boulesteix, Bernd Bischl, Kristian Unger, and Roman Hornung (2021) "Improved Outcome Prediction Across Data Sources Through Robust Parameter Tuning" &lt;<a href="https://doi.org/10.1007%2Fs00357-020-09368-z">doi:10.1007/s00357-020-09368-z</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet, mboost, mlr, ranger, e1071, pROC</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/Yuting-He/RobustPrediction">https://github.com/Yuting-He/RobustPrediction</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-12-16 16:13:19 UTC; yutin</td>
</tr>
<tr>
<td>Author:</td>
<td>Yuting He [aut, cre],
  Nicole Ellenbach [ctb],
  Roman Hornung [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-12-16 16:50:11 UTC</td>
</tr>
</table>
<hr>
<h2 id='RobustPrediction'>Package Title: Robust Tuning and Training for Cross-Source Prediction</h2><span id='topic+RobustPrediction'></span><span id='topic+RobustPrediction-package'></span>

<h3>Description</h3>

<p>This package provides robust parameter tuning and predictive modeling techniques, useful for situations 
where prediction across different data sources is important and the data distribution varies slightly from source to source.
</p>


<h3>Details</h3>

<p>The 'RobustPrediction' package helps users build and tune classifiers using the methods  
'RobustTuneC' method, internal, or external tuning method. The package supports the following classifiers: 
boosting, lasso, ridge, random forest, and support vector machine(SVM). It is intended for scenarios 
where parameter tuning across data sources is important.
</p>
<p>The 'RobustPrediction' package provides comprehensive tools for robust parameter tuning 
and predictive modeling, particularly for cross-source prediction tasks. 
</p>
<p>The package includes functions for tuning model parameters using three methods:
- **Internal tuning**: Standard cross-validation on the training data to select the best parameters.
- **External tuning**: Parameter tuning based on an external dataset that is independent of the training data. This method 
has two variants controlled by the <code>estperf</code> argument:
- **Standard external tuning (<code>estperf = FALSE</code>)**: Parameters are tuned directly using the external dataset. 
This is the default approach and provides a straightforward method for selecting optimal parameters based on external data.
- **Conservative external tuning (<code>estperf = TRUE</code>)**: Internal tuning is first performed on the training data, 
and then the model is evaluated on the external dataset. This approach provides a more conservative (slightly pessimistic) 
AUC estimate, as described by Ellenbach et al. (2021). For the most accurate performance evaluation, 
it is recommended to use a second external dataset.
- **RobustTuneC**: A method designed to combine internal and external tuning for better performance in cross-source scenarios.
</p>
<p>The package supports Lasso, Ridge, Random Forest, Boosting, and SVM classifiers. 
These models can be trained and tuned using the provided methods, and the package includes 
the model's AUC (Area Under the Curve) value to help users evaluate prediction performance.
</p>
<p>It is particularly useful when the data to be predicted comes from a different source than the training data, 
where variability between datasets may require more robust parameter tuning techniques. The methods provided in 
this package may help reduce overfitting the training data distribution and improve model generalization across 
different data sources.
</p>


<h3>Dependencies</h3>

<p>This package requires the following packages: <code>glmnet</code>, <code>mboost</code>, <code>mlr</code>, 
<code>pROC</code>, <code>ranger</code>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Yuting He <a href="mailto:yutingh19@gmail.com">yutingh19@gmail.com</a>
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Nicole Ellenbach [contributor]
</p>
</li>
<li><p> Roman Hornung [contributor]
</p>
</li></ul>



<h3>References</h3>

<p>Ellenbach, N., Boulesteix, A.-L., Bischl, B., Unger, K., &amp; Hornung, R. (2021). 
Improved outcome prediction across data sources through robust parameter tuning. 
<em>Journal of Classification</em>, <em>38</em>, 212-231. 
&lt;doi:10.1007/s00357-020-09368-z&gt;.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/Yuting-He/RobustPrediction">https://github.com/Yuting-He/RobustPrediction</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Example usage:
data(sample_data_train)
data(sample_data_extern)
res &lt;- tuneandtrain(sample_data_train, sample_data_extern, tuningmethod = "robusttunec", 
  classifier = "lasso")

</code></pre>

<hr>
<h2 id='sample_data_extern'>Sample External Validation Data Subset</h2><span id='topic+sample_data_extern'></span>

<h3>Description</h3>

<p>This dataset, named 'sample_data_extern', is a subset of publicly available microarray data from the HG-U133PLUS2 chip. 
It contains expression levels of 200 genes across 50 samples, used primarily as an external validation set 
in robust feature selection studies. 
The data has been sourced from the ArrayExpress repository and has been referenced in several research articles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_data_extern
</code></pre>


<h3>Format</h3>

<p>A data frame with 50 observations and 201 variables, including:
</p>

<dl>
<dt>y</dt><dd><p>Factor. The response variable.</p>
</dd>
<dt>236694_at</dt><dd><p>Numeric. Expression level of gene 236694_at.</p>
</dd>
<dt>222356_at</dt><dd><p>Numeric. Expression level of gene 222356_at.</p>
</dd>
<dt>1554125_a_at</dt><dd><p>Numeric. Expression level of gene 1554125_a_at.</p>
</dd>
<dt>232823_at</dt><dd><p>Numeric. Expression level of gene 232823_at.</p>
</dd>
<dt>205766_at</dt><dd><p>Numeric. Expression level of gene 205766_at.</p>
</dd>
<dt>1560446_at</dt><dd><p>Numeric. Expression level of gene 1560446_at.</p>
</dd>
<dt>202565_s_at</dt><dd><p>Numeric. Expression level of gene 202565_s_at.</p>
</dd>
<dt>234887_at</dt><dd><p>Numeric. Expression level of gene 234887_at.</p>
</dd>
<dt>209687_at</dt><dd><p>Numeric. Expression level of gene 209687_at.</p>
</dd>
<dt>221592_at</dt><dd><p>Numeric. Expression level of gene 221592_at.</p>
</dd>
<dt>1570123_at</dt><dd><p>Numeric. Expression level of gene 1570123_at.</p>
</dd>
<dt>241368_at</dt><dd><p>Numeric. Expression level of gene 241368_at.</p>
</dd>
<dt>243324_x_at</dt><dd><p>Numeric. Expression level of gene 243324_x_at.</p>
</dd>
<dt>224046_s_at</dt><dd><p>Numeric. Expression level of gene 224046_s_at.</p>
</dd>
<dt>202775_s_at</dt><dd><p>Numeric. Expression level of gene 202775_s_at.</p>
</dd>
<dt>216332_at</dt><dd><p>Numeric. Expression level of gene 216332_at.</p>
</dd>
<dt>1569545_at</dt><dd><p>Numeric. Expression level of gene 1569545_at.</p>
</dd>
<dt>205946_at</dt><dd><p>Numeric. Expression level of gene 205946_at.</p>
</dd>
<dt>203547_at</dt><dd><p>Numeric. Expression level of gene 203547_at.</p>
</dd>
<dt>243239_at</dt><dd><p>Numeric. Expression level of gene 243239_at.</p>
</dd>
<dt>234245_at</dt><dd><p>Numeric. Expression level of gene 234245_at.</p>
</dd>
<dt>210832_x_at</dt><dd><p>Numeric. Expression level of gene 210832_x_at.</p>
</dd>
<dt>224549_x_at</dt><dd><p>Numeric. Expression level of gene 224549_x_at.</p>
</dd>
<dt>236628_at</dt><dd><p>Numeric. Expression level of gene 236628_at.</p>
</dd>
<dt>214848_at</dt><dd><p>Numeric. Expression level of gene 214848_at.</p>
</dd>
<dt>1553015_a_at</dt><dd><p>Numeric. Expression level of gene 1553015_a_at.</p>
</dd>
<dt>1554199_at</dt><dd><p>Numeric. Expression level of gene 1554199_at.</p>
</dd>
<dt>1557636_a_at</dt><dd><p>Numeric. Expression level of gene 1557636_a_at.</p>
</dd>
<dt>1558511_s_at</dt><dd><p>Numeric. Expression level of gene 1558511_s_at.</p>
</dd>
<dt>1561713_at</dt><dd><p>Numeric. Expression level of gene 1561713_at.</p>
</dd>
<dt>1561883_at</dt><dd><p>Numeric. Expression level of gene 1561883_at.</p>
</dd>
<dt>1568720_at</dt><dd><p>Numeric. Expression level of gene 1568720_at.</p>
</dd>
<dt>1569168_at</dt><dd><p>Numeric. Expression level of gene 1569168_at.</p>
</dd>
<dt>1569443_s_at</dt><dd><p>Numeric. Expression level of gene 1569443_s_at.</p>
</dd>
<dt>1570103_at</dt><dd><p>Numeric. Expression level of gene 1570103_at.</p>
</dd>
<dt>200916_at</dt><dd><p>Numeric. Expression level of gene 200916_at.</p>
</dd>
<dt>201554_x_at</dt><dd><p>Numeric. Expression level of gene 201554_x_at.</p>
</dd>
<dt>202371_at</dt><dd><p>Numeric. Expression level of gene 202371_at.</p>
</dd>
<dt>204481_at</dt><dd><p>Numeric. Expression level of gene 204481_at.</p>
</dd>
<dt>205831_at</dt><dd><p>Numeric. Expression level of gene 205831_at.</p>
</dd>
<dt>207061_at</dt><dd><p>Numeric. Expression level of gene 207061_at.</p>
</dd>
<dt>207423_s_at</dt><dd><p>Numeric. Expression level of gene 207423_s_at.</p>
</dd>
<dt>209896_s_at</dt><dd><p>Numeric. Expression level of gene 209896_s_at.</p>
</dd>
<dt>212646_at</dt><dd><p>Numeric. Expression level of gene 212646_at.</p>
</dd>
<dt>214068_at</dt><dd><p>Numeric. Expression level of gene 214068_at.</p>
</dd>
<dt>217727_x_at</dt><dd><p>Numeric. Expression level of gene 217727_x_at.</p>
</dd>
<dt>221103_s_at</dt><dd><p>Numeric. Expression level of gene 221103_s_at.</p>
</dd>
<dt>221785_at</dt><dd><p>Numeric. Expression level of gene 221785_at.</p>
</dd>
<dt>224207_x_at</dt><dd><p>Numeric. Expression level of gene 224207_x_at.</p>
</dd>
<dt>228257_at</dt><dd><p>Numeric. Expression level of gene 228257_at.</p>
</dd>
<dt>228877_at</dt><dd><p>Numeric. Expression level of gene 228877_at.</p>
</dd>
<dt>231173_at</dt><dd><p>Numeric. Expression level of gene 231173_at.</p>
</dd>
<dt>231328_s_at</dt><dd><p>Numeric. Expression level of gene 231328_s_at.</p>
</dd>
<dt>231639_at</dt><dd><p>Numeric. Expression level of gene 231639_at.</p>
</dd>
<dt>232221_x_at</dt><dd><p>Numeric. Expression level of gene 232221_x_at.</p>
</dd>
<dt>232349_x_at</dt><dd><p>Numeric. Expression level of gene 232349_x_at.</p>
</dd>
<dt>232849_at</dt><dd><p>Numeric. Expression level of gene 232849_at.</p>
</dd>
<dt>233601_at</dt><dd><p>Numeric. Expression level of gene 233601_at.</p>
</dd>
<dt>234403_at</dt><dd><p>Numeric. Expression level of gene 234403_at.</p>
</dd>
<dt>234585_at</dt><dd><p>Numeric. Expression level of gene 234585_at.</p>
</dd>
<dt>234650_at</dt><dd><p>Numeric. Expression level of gene 234650_at.</p>
</dd>
<dt>234897_s_at</dt><dd><p>Numeric. Expression level of gene 234897_s_at.</p>
</dd>
<dt>236071_at</dt><dd><p>Numeric. Expression level of gene 236071_at.</p>
</dd>
<dt>236689_at</dt><dd><p>Numeric. Expression level of gene 236689_at.</p>
</dd>
<dt>238551_at</dt><dd><p>Numeric. Expression level of gene 238551_at.</p>
</dd>
<dt>239414_at</dt><dd><p>Numeric. Expression level of gene 239414_at.</p>
</dd>
<dt>241034_at</dt><dd><p>Numeric. Expression level of gene 241034_at.</p>
</dd>
<dt>241131_at</dt><dd><p>Numeric. Expression level of gene 241131_at.</p>
</dd>
<dt>241897_at</dt><dd><p>Numeric. Expression level of gene 241897_at.</p>
</dd>
<dt>242611_at</dt><dd><p>Numeric. Expression level of gene 242611_at.</p>
</dd>
<dt>244805_at</dt><dd><p>Numeric. Expression level of gene 244805_at.</p>
</dd>
<dt>244866_at</dt><dd><p>Numeric. Expression level of gene 244866_at.</p>
</dd>
<dt>32259_at</dt><dd><p>Numeric. Expression level of gene 32259_at.</p>
</dd>
<dt>1552264_a_at</dt><dd><p>Numeric. Expression level of gene 1552264_a_at.</p>
</dd>
<dt>1552880_at</dt><dd><p>Numeric. Expression level of gene 1552880_at.</p>
</dd>
<dt>1553186_x_at</dt><dd><p>Numeric. Expression level of gene 1553186_x_at.</p>
</dd>
<dt>1553372_at</dt><dd><p>Numeric. Expression level of gene 1553372_at.</p>
</dd>
<dt>1553438_at</dt><dd><p>Numeric. Expression level of gene 1553438_at.</p>
</dd>
<dt>1554299_at</dt><dd><p>Numeric. Expression level of gene 1554299_at.</p>
</dd>
<dt>1554362_at</dt><dd><p>Numeric. Expression level of gene 1554362_at.</p>
</dd>
<dt>1554491_a_at</dt><dd><p>Numeric. Expression level of gene 1554491_a_at.</p>
</dd>
<dt>1555098_a_at</dt><dd><p>Numeric. Expression level of gene 1555098_a_at.</p>
</dd>
<dt>1555990_at</dt><dd><p>Numeric. Expression level of gene 1555990_at.</p>
</dd>
<dt>1556034_s_at</dt><dd><p>Numeric. Expression level of gene 1556034_s_at.</p>
</dd>
<dt>1556822_s_at</dt><dd><p>Numeric. Expression level of gene 1556822_s_at.</p>
</dd>
<dt>1556824_at</dt><dd><p>Numeric. Expression level of gene 1556824_at.</p>
</dd>
<dt>1557278_s_at</dt><dd><p>Numeric. Expression level of gene 1557278_s_at.</p>
</dd>
<dt>1558603_at</dt><dd><p>Numeric. Expression level of gene 1558603_at.</p>
</dd>
<dt>1558890_at</dt><dd><p>Numeric. Expression level of gene 1558890_at.</p>
</dd>
<dt>1560791_at</dt><dd><p>Numeric. Expression level of gene 1560791_at.</p>
</dd>
<dt>1561083_at</dt><dd><p>Numeric. Expression level of gene 1561083_at.</p>
</dd>
<dt>1561364_at</dt><dd><p>Numeric. Expression level of gene 1561364_at.</p>
</dd>
<dt>1561553_at</dt><dd><p>Numeric. Expression level of gene 1561553_at.</p>
</dd>
<dt>1562523_at</dt><dd><p>Numeric. Expression level of gene 1562523_at.</p>
</dd>
<dt>1562613_at</dt><dd><p>Numeric. Expression level of gene 1562613_at.</p>
</dd>
<dt>1563351_at</dt><dd><p>Numeric. Expression level of gene 1563351_at.</p>
</dd>
<dt>1563473_at</dt><dd><p>Numeric. Expression level of gene 1563473_at.</p>
</dd>
<dt>1566780_at</dt><dd><p>Numeric. Expression level of gene 1566780_at.</p>
</dd>
<dt>1567257_at</dt><dd><p>Numeric. Expression level of gene 1567257_at.</p>
</dd>
<dt>1569664_at</dt><dd><p>Numeric. Expression level of gene 1569664_at.</p>
</dd>
<dt>1569882_at</dt><dd><p>Numeric. Expression level of gene 1569882_at.</p>
</dd>
<dt>1570252_at</dt><dd><p>Numeric. Expression level of gene 1570252_at.</p>
</dd>
<dt>201089_at</dt><dd><p>Numeric. Expression level of gene 201089_at.</p>
</dd>
<dt>201261_x_at</dt><dd><p>Numeric. Expression level of gene 201261_x_at.</p>
</dd>
<dt>202052_s_at</dt><dd><p>Numeric. Expression level of gene 202052_s_at.</p>
</dd>
<dt>202236_s_at</dt><dd><p>Numeric. Expression level of gene 202236_s_at.</p>
</dd>
<dt>202948_at</dt><dd><p>Numeric. Expression level of gene 202948_at.</p>
</dd>
<dt>203080_s_at</dt><dd><p>Numeric. Expression level of gene 203080_s_at.</p>
</dd>
<dt>203211_s_at</dt><dd><p>Numeric. Expression level of gene 203211_s_at.</p>
</dd>
<dt>203218_at</dt><dd><p>Numeric. Expression level of gene 203218_at.</p>
</dd>
<dt>203236_s_at</dt><dd><p>Numeric. Expression level of gene 203236_s_at.</p>
</dd>
<dt>203347_s_at</dt><dd><p>Numeric. Expression level of gene 203347_s_at.</p>
</dd>
<dt>203960_s_at</dt><dd><p>Numeric. Expression level of gene 203960_s_at.</p>
</dd>
<dt>204609_at</dt><dd><p>Numeric. Expression level of gene 204609_at.</p>
</dd>
<dt>204806_x_at</dt><dd><p>Numeric. Expression level of gene 204806_x_at.</p>
</dd>
<dt>204949_at</dt><dd><p>Numeric. Expression level of gene 204949_at.</p>
</dd>
<dt>204979_s_at</dt><dd><p>Numeric. Expression level of gene 204979_s_at.</p>
</dd>
<dt>205823_at</dt><dd><p>Numeric. Expression level of gene 205823_at.</p>
</dd>
<dt>205902_at</dt><dd><p>Numeric. Expression level of gene 205902_at.</p>
</dd>
<dt>205967_at</dt><dd><p>Numeric. Expression level of gene 205967_at.</p>
</dd>
<dt>206186_at</dt><dd><p>Numeric. Expression level of gene 206186_at.</p>
</dd>
<dt>207151_at</dt><dd><p>Numeric. Expression level of gene 207151_at.</p>
</dd>
<dt>207379_at</dt><dd><p>Numeric. Expression level of gene 207379_at.</p>
</dd>
<dt>207440_at</dt><dd><p>Numeric. Expression level of gene 207440_at.</p>
</dd>
<dt>207883_s_at</dt><dd><p>Numeric. Expression level of gene 207883_s_at.</p>
</dd>
<dt>208277_at</dt><dd><p>Numeric. Expression level of gene 208277_at.</p>
</dd>
<dt>208280_at</dt><dd><p>Numeric. Expression level of gene 208280_at.</p>
</dd>
<dt>209224_s_at</dt><dd><p>Numeric. Expression level of gene 209224_s_at.</p>
</dd>
<dt>209561_at</dt><dd><p>Numeric. Expression level of gene 209561_at.</p>
</dd>
<dt>209630_s_at</dt><dd><p>Numeric. Expression level of gene 209630_s_at.</p>
</dd>
<dt>210118_s_at</dt><dd><p>Numeric. Expression level of gene 210118_s_at.</p>
</dd>
<dt>210342_s_at</dt><dd><p>Numeric. Expression level of gene 210342_s_at.</p>
</dd>
<dt>211566_x_at</dt><dd><p>Numeric. Expression level of gene 211566_x_at.</p>
</dd>
<dt>211756_at</dt><dd><p>Numeric. Expression level of gene 211756_at.</p>
</dd>
<dt>212170_at</dt><dd><p>Numeric. Expression level of gene 212170_at.</p>
</dd>
<dt>212494_at</dt><dd><p>Numeric. Expression level of gene 212494_at.</p>
</dd>
<dt>213118_at</dt><dd><p>Numeric. Expression level of gene 213118_at.</p>
</dd>
<dt>214475_x_at</dt><dd><p>Numeric. Expression level of gene 214475_x_at.</p>
</dd>
<dt>214834_at</dt><dd><p>Numeric. Expression level of gene 214834_at.</p>
</dd>
<dt>215718_s_at</dt><dd><p>Numeric. Expression level of gene 215718_s_at.</p>
</dd>
<dt>216283_s_at</dt><dd><p>Numeric. Expression level of gene 216283_s_at.</p>
</dd>
<dt>217206_at</dt><dd><p>Numeric. Expression level of gene 217206_at.</p>
</dd>
<dt>217557_s_at</dt><dd><p>Numeric. Expression level of gene 217557_s_at.</p>
</dd>
<dt>217577_at</dt><dd><p>Numeric. Expression level of gene 217577_at.</p>
</dd>
<dt>218152_at</dt><dd><p>Numeric. Expression level of gene 218152_at.</p>
</dd>
<dt>218252_at</dt><dd><p>Numeric. Expression level of gene 218252_at.</p>
</dd>
<dt>219714_s_at</dt><dd><p>Numeric. Expression level of gene 219714_s_at.</p>
</dd>
<dt>220506_at</dt><dd><p>Numeric. Expression level of gene 220506_at.</p>
</dd>
<dt>220889_s_at</dt><dd><p>Numeric. Expression level of gene 220889_s_at.</p>
</dd>
<dt>221204_s_at</dt><dd><p>Numeric. Expression level of gene 221204_s_at.</p>
</dd>
<dt>221795_at</dt><dd><p>Numeric. Expression level of gene 221795_at.</p>
</dd>
<dt>222048_at</dt><dd><p>Numeric. Expression level of gene 222048_at.</p>
</dd>
<dt>223142_s_at</dt><dd><p>Numeric. Expression level of gene 223142_s_at.</p>
</dd>
<dt>223439_at</dt><dd><p>Numeric. Expression level of gene 223439_at.</p>
</dd>
<dt>223673_at</dt><dd><p>Numeric. Expression level of gene 223673_at.</p>
</dd>
<dt>224363_at</dt><dd><p>Numeric. Expression level of gene 224363_at.</p>
</dd>
<dt>224512_s_at</dt><dd><p>Numeric. Expression level of gene 224512_s_at.</p>
</dd>
<dt>224690_at</dt><dd><p>Numeric. Expression level of gene 224690_at.</p>
</dd>
<dt>224936_at</dt><dd><p>Numeric. Expression level of gene 224936_at.</p>
</dd>
<dt>225334_at</dt><dd><p>Numeric. Expression level of gene 225334_at.</p>
</dd>
<dt>225713_at</dt><dd><p>Numeric. Expression level of gene 225713_at.</p>
</dd>
<dt>225839_at</dt><dd><p>Numeric. Expression level of gene 225839_at.</p>
</dd>
<dt>226041_at</dt><dd><p>Numeric. Expression level of gene 226041_at.</p>
</dd>
<dt>226093_at</dt><dd><p>Numeric. Expression level of gene 226093_at.</p>
</dd>
<dt>226543_at</dt><dd><p>Numeric. Expression level of gene 226543_at.</p>
</dd>
<dt>227695_at</dt><dd><p>Numeric. Expression level of gene 227695_at.</p>
</dd>
<dt>228295_at</dt><dd><p>Numeric. Expression level of gene 228295_at.</p>
</dd>
<dt>228548_at</dt><dd><p>Numeric. Expression level of gene 228548_at.</p>
</dd>
<dt>229234_at</dt><dd><p>Numeric. Expression level of gene 229234_at.</p>
</dd>
<dt>229658_at</dt><dd><p>Numeric. Expression level of gene 229658_at.</p>
</dd>
<dt>229725_at</dt><dd><p>Numeric. Expression level of gene 229725_at.</p>
</dd>
<dt>230252_at</dt><dd><p>Numeric. Expression level of gene 230252_at.</p>
</dd>
<dt>230471_at</dt><dd><p>Numeric. Expression level of gene 230471_at.</p>
</dd>
<dt>231149_s_at</dt><dd><p>Numeric. Expression level of gene 231149_s_at.</p>
</dd>
<dt>231556_at</dt><dd><p>Numeric. Expression level of gene 231556_at.</p>
</dd>
<dt>231754_at</dt><dd><p>Numeric. Expression level of gene 231754_at.</p>
</dd>
<dt>232011_s_at</dt><dd><p>Numeric. Expression level of gene 232011_s_at.</p>
</dd>
<dt>233030_at</dt><dd><p>Numeric. Expression level of gene 233030_at.</p>
</dd>
<dt>234161_at</dt><dd><p>Numeric. Expression level of gene 234161_at.</p>
</dd>
<dt>235050_at</dt><dd><p>Numeric. Expression level of gene 235050_at.</p>
</dd>
<dt>235094_at</dt><dd><p>Numeric. Expression level of gene 235094_at.</p>
</dd>
<dt>235278_at</dt><dd><p>Numeric. Expression level of gene 235278_at.</p>
</dd>
<dt>235671_at</dt><dd><p>Numeric. Expression level of gene 235671_at.</p>
</dd>
<dt>235952_at</dt><dd><p>Numeric. Expression level of gene 235952_at.</p>
</dd>
<dt>236158_at</dt><dd><p>Numeric. Expression level of gene 236158_at.</p>
</dd>
<dt>236181_at</dt><dd><p>Numeric. Expression level of gene 236181_at.</p>
</dd>
<dt>237055_at</dt><dd><p>Numeric. Expression level of gene 237055_at.</p>
</dd>
<dt>237768_x_at</dt><dd><p>Numeric. Expression level of gene 237768_x_at.</p>
</dd>
<dt>238897_at</dt><dd><p>Numeric. Expression level of gene 238897_at.</p>
</dd>
<dt>239160_at</dt><dd><p>Numeric. Expression level of gene 239160_at.</p>
</dd>
<dt>239998_at</dt><dd><p>Numeric. Expression level of gene 239998_at.</p>
</dd>
<dt>240254_at</dt><dd><p>Numeric. Expression level of gene 240254_at.</p>
</dd>
<dt>240612_at</dt><dd><p>Numeric. Expression level of gene 240612_at.</p>
</dd>
<dt>240692_at</dt><dd><p>Numeric. Expression level of gene 240692_at.</p>
</dd>
<dt>240822_at</dt><dd><p>Numeric. Expression level of gene 240822_at.</p>
</dd>
<dt>240842_at</dt><dd><p>Numeric. Expression level of gene 240842_at.</p>
</dd>
<dt>241331_at</dt><dd><p>Numeric. Expression level of gene 241331_at.</p>
</dd>
<dt>241598_at</dt><dd><p>Numeric. Expression level of gene 241598_at.</p>
</dd>
<dt>241927_x_at</dt><dd><p>Numeric. Expression level of gene 241927_x_at.</p>
</dd>
<dt>242405_at</dt><dd><p>Numeric. Expression level of gene 242405_at.</p>
</dd>
</dl>



<h3>Details</h3>

<p>This dataset was extracted from a larger dataset available on ArrayExpress and is used as an external validation set 
for feature selection tasks and other machine learning applications in bioinformatics.
</p>


<h3>Source</h3>

<p>The original dataset can be found on ArrayExpress: <a href="https://www.ebi.ac.uk/arrayexpress">https://www.ebi.ac.uk/arrayexpress</a>
</p>


<h3>References</h3>

<p>Ellenbach, N., Boulesteix, A.L., Bischl, B., et al. (2021). 
Improved Outcome Prediction Across Data Sources Through Robust Parameter Tuning. <em>Journal of Classification</em>, 
38, 212–231. <a href="https://doi.org/10.1007/s00357-020-09368-z">doi:10.1007/s00357-020-09368-z</a>.
</p>
<p>Hornung, R., Causeur, D., Bernau, C., Boulesteix, A.L. (2017). 
Improving cross-study prediction through addon batch effect adjustment or addon normalization. <em>Bioinformatics</em>, 
33(3), 397–404. <a href="https://doi.org/10.1093/bioinformatics/btw650">doi:10.1093/bioinformatics/btw650</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load the dataset
data(sample_data_extern)

# View the first few rows of the dataset
head(sample_data_extern)

# Summary of the dataset
summary(sample_data_extern)
</code></pre>

<hr>
<h2 id='sample_data_train'>Sample Training Data Subset</h2><span id='topic+sample_data_train'></span>

<h3>Description</h3>

<p>This dataset, named 'sample_data_train', is a subset of publicly available microarray data from the HG-U133PLUS2 chip. 
It contains expression levels of 200 genes across 50 samples, used primarily as a training set in robust 
feature selection studies. 
The data has been sourced from the ArrayExpress repository and has been referenced in several research articles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_data_train
</code></pre>


<h3>Format</h3>

<p>A data frame with 50 observations and 201 variables, including:
</p>

<dl>
<dt>y</dt><dd><p>Factor. The response variable.</p>
</dd>
<dt>236694_at</dt><dd><p>Numeric. Expression level of gene 236694_at.</p>
</dd>
<dt>222356_at</dt><dd><p>Numeric. Expression level of gene 222356_at.</p>
</dd>
<dt>1554125_a_at</dt><dd><p>Numeric. Expression level of gene 1554125_a_at.</p>
</dd>
<dt>232823_at</dt><dd><p>Numeric. Expression level of gene 232823_at.</p>
</dd>
<dt>205766_at</dt><dd><p>Numeric. Expression level of gene 205766_at.</p>
</dd>
<dt>1560446_at</dt><dd><p>Numeric. Expression level of gene 1560446_at.</p>
</dd>
<dt>202565_s_at</dt><dd><p>Numeric. Expression level of gene 202565_s_at.</p>
</dd>
<dt>234887_at</dt><dd><p>Numeric. Expression level of gene 234887_at.</p>
</dd>
<dt>209687_at</dt><dd><p>Numeric. Expression level of gene 209687_at.</p>
</dd>
<dt>221592_at</dt><dd><p>Numeric. Expression level of gene 221592_at.</p>
</dd>
<dt>1570123_at</dt><dd><p>Numeric. Expression level of gene 1570123_at.</p>
</dd>
<dt>241368_at</dt><dd><p>Numeric. Expression level of gene 241368_at.</p>
</dd>
<dt>243324_x_at</dt><dd><p>Numeric. Expression level of gene 243324_x_at.</p>
</dd>
<dt>224046_s_at</dt><dd><p>Numeric. Expression level of gene 224046_s_at.</p>
</dd>
<dt>202775_s_at</dt><dd><p>Numeric. Expression level of gene 202775_s_at.</p>
</dd>
<dt>216332_at</dt><dd><p>Numeric. Expression level of gene 216332_at.</p>
</dd>
<dt>1569545_at</dt><dd><p>Numeric. Expression level of gene 1569545_at.</p>
</dd>
<dt>205946_at</dt><dd><p>Numeric. Expression level of gene 205946_at.</p>
</dd>
<dt>203547_at</dt><dd><p>Numeric. Expression level of gene 203547_at.</p>
</dd>
<dt>243239_at</dt><dd><p>Numeric. Expression level of gene 243239_at.</p>
</dd>
<dt>234245_at</dt><dd><p>Numeric. Expression level of gene 234245_at.</p>
</dd>
<dt>210832_x_at</dt><dd><p>Numeric. Expression level of gene 210832_x_at.</p>
</dd>
<dt>224549_x_at</dt><dd><p>Numeric. Expression level of gene 224549_x_at.</p>
</dd>
<dt>236628_at</dt><dd><p>Numeric. Expression level of gene 236628_at.</p>
</dd>
<dt>214848_at</dt><dd><p>Numeric. Expression level of gene 214848_at.</p>
</dd>
<dt>1553015_a_at</dt><dd><p>Numeric. Expression level of gene 1553015_a_at.</p>
</dd>
<dt>1554199_at</dt><dd><p>Numeric. Expression level of gene 1554199_at.</p>
</dd>
<dt>1557636_a_at</dt><dd><p>Numeric. Expression level of gene 1557636_a_at.</p>
</dd>
<dt>1558511_s_at</dt><dd><p>Numeric. Expression level of gene 1558511_s_at.</p>
</dd>
<dt>1561713_at</dt><dd><p>Numeric. Expression level of gene 1561713_at.</p>
</dd>
<dt>1561883_at</dt><dd><p>Numeric. Expression level of gene 1561883_at.</p>
</dd>
<dt>1568720_at</dt><dd><p>Numeric. Expression level of gene 1568720_at.</p>
</dd>
<dt>1569168_at</dt><dd><p>Numeric. Expression level of gene 1569168_at.</p>
</dd>
<dt>1569443_s_at</dt><dd><p>Numeric. Expression level of gene 1569443_s_at.</p>
</dd>
<dt>1570103_at</dt><dd><p>Numeric. Expression level of gene 1570103_at.</p>
</dd>
<dt>200916_at</dt><dd><p>Numeric. Expression level of gene 200916_at.</p>
</dd>
<dt>201554_x_at</dt><dd><p>Numeric. Expression level of gene 201554_x_at.</p>
</dd>
<dt>202371_at</dt><dd><p>Numeric. Expression level of gene 202371_at.</p>
</dd>
<dt>204481_at</dt><dd><p>Numeric. Expression level of gene 204481_at.</p>
</dd>
<dt>205831_at</dt><dd><p>Numeric. Expression level of gene 205831_at.</p>
</dd>
<dt>207061_at</dt><dd><p>Numeric. Expression level of gene 207061_at.</p>
</dd>
<dt>207423_s_at</dt><dd><p>Numeric. Expression level of gene 207423_s_at.</p>
</dd>
<dt>209896_s_at</dt><dd><p>Numeric. Expression level of gene 209896_s_at.</p>
</dd>
<dt>212646_at</dt><dd><p>Numeric. Expression level of gene 212646_at.</p>
</dd>
<dt>214068_at</dt><dd><p>Numeric. Expression level of gene 214068_at.</p>
</dd>
<dt>217727_x_at</dt><dd><p>Numeric. Expression level of gene 217727_x_at.</p>
</dd>
<dt>221103_s_at</dt><dd><p>Numeric. Expression level of gene 221103_s_at.</p>
</dd>
<dt>221785_at</dt><dd><p>Numeric. Expression level of gene 221785_at.</p>
</dd>
<dt>224207_x_at</dt><dd><p>Numeric. Expression level of gene 224207_x_at.</p>
</dd>
<dt>228257_at</dt><dd><p>Numeric. Expression level of gene 228257_at.</p>
</dd>
<dt>228877_at</dt><dd><p>Numeric. Expression level of gene 228877_at.</p>
</dd>
<dt>231173_at</dt><dd><p>Numeric. Expression level of gene 231173_at.</p>
</dd>
<dt>231328_s_at</dt><dd><p>Numeric. Expression level of gene 231328_s_at.</p>
</dd>
<dt>231639_at</dt><dd><p>Numeric. Expression level of gene 231639_at.</p>
</dd>
<dt>232221_x_at</dt><dd><p>Numeric. Expression level of gene 232221_x_at.</p>
</dd>
<dt>232349_x_at</dt><dd><p>Numeric. Expression level of gene 232349_x_at.</p>
</dd>
<dt>232849_at</dt><dd><p>Numeric. Expression level of gene 232849_at.</p>
</dd>
<dt>233601_at</dt><dd><p>Numeric. Expression level of gene 233601_at.</p>
</dd>
<dt>234403_at</dt><dd><p>Numeric. Expression level of gene 234403_at.</p>
</dd>
<dt>234585_at</dt><dd><p>Numeric. Expression level of gene 234585_at.</p>
</dd>
<dt>234650_at</dt><dd><p>Numeric. Expression level of gene 234650_at.</p>
</dd>
<dt>234897_s_at</dt><dd><p>Numeric. Expression level of gene 234897_s_at.</p>
</dd>
<dt>236071_at</dt><dd><p>Numeric. Expression level of gene 236071_at.</p>
</dd>
<dt>236689_at</dt><dd><p>Numeric. Expression level of gene 236689_at.</p>
</dd>
<dt>238551_at</dt><dd><p>Numeric. Expression level of gene 238551_at.</p>
</dd>
<dt>239414_at</dt><dd><p>Numeric. Expression level of gene 239414_at.</p>
</dd>
<dt>241034_at</dt><dd><p>Numeric. Expression level of gene 241034_at.</p>
</dd>
<dt>241131_at</dt><dd><p>Numeric. Expression level of gene 241131_at.</p>
</dd>
<dt>241897_at</dt><dd><p>Numeric. Expression level of gene 241897_at.</p>
</dd>
<dt>242611_at</dt><dd><p>Numeric. Expression level of gene 242611_at.</p>
</dd>
<dt>244805_at</dt><dd><p>Numeric. Expression level of gene 244805_at.</p>
</dd>
<dt>244866_at</dt><dd><p>Numeric. Expression level of gene 244866_at.</p>
</dd>
<dt>32259_at</dt><dd><p>Numeric. Expression level of gene 32259_at.</p>
</dd>
<dt>1552264_a_at</dt><dd><p>Numeric. Expression level of gene 1552264_a_at.</p>
</dd>
<dt>1552880_at</dt><dd><p>Numeric. Expression level of gene 1552880_at.</p>
</dd>
<dt>1553186_x_at</dt><dd><p>Numeric. Expression level of gene 1553186_x_at.</p>
</dd>
<dt>1553372_at</dt><dd><p>Numeric. Expression level of gene 1553372_at.</p>
</dd>
<dt>1553438_at</dt><dd><p>Numeric. Expression level of gene 1553438_at.</p>
</dd>
<dt>1554299_at</dt><dd><p>Numeric. Expression level of gene 1554299_at.</p>
</dd>
<dt>1554362_at</dt><dd><p>Numeric. Expression level of gene 1554362_at.</p>
</dd>
<dt>1554491_a_at</dt><dd><p>Numeric. Expression level of gene 1554491_a_at.</p>
</dd>
<dt>1555098_a_at</dt><dd><p>Numeric. Expression level of gene 1555098_a_at.</p>
</dd>
<dt>1555990_at</dt><dd><p>Numeric. Expression level of gene 1555990_at.</p>
</dd>
<dt>1556034_s_at</dt><dd><p>Numeric. Expression level of gene 1556034_s_at.</p>
</dd>
<dt>1556822_s_at</dt><dd><p>Numeric. Expression level of gene 1556822_s_at.</p>
</dd>
<dt>1556824_at</dt><dd><p>Numeric. Expression level of gene 1556824_at.</p>
</dd>
<dt>1557278_s_at</dt><dd><p>Numeric. Expression level of gene 1557278_s_at.</p>
</dd>
<dt>1558603_at</dt><dd><p>Numeric. Expression level of gene 1558603_at.</p>
</dd>
<dt>1558890_at</dt><dd><p>Numeric. Expression level of gene 1558890_at.</p>
</dd>
<dt>1560791_at</dt><dd><p>Numeric. Expression level of gene 1560791_at.</p>
</dd>
<dt>1561083_at</dt><dd><p>Numeric. Expression level of gene 1561083_at.</p>
</dd>
<dt>1561364_at</dt><dd><p>Numeric. Expression level of gene 1561364_at.</p>
</dd>
<dt>1561553_at</dt><dd><p>Numeric. Expression level of gene 1561553_at.</p>
</dd>
<dt>1562523_at</dt><dd><p>Numeric. Expression level of gene 1562523_at.</p>
</dd>
<dt>1562613_at</dt><dd><p>Numeric. Expression level of gene 1562613_at.</p>
</dd>
<dt>1563351_at</dt><dd><p>Numeric. Expression level of gene 1563351_at.</p>
</dd>
<dt>1563473_at</dt><dd><p>Numeric. Expression level of gene 1563473_at.</p>
</dd>
<dt>1566780_at</dt><dd><p>Numeric. Expression level of gene 1566780_at.</p>
</dd>
<dt>1567257_at</dt><dd><p>Numeric. Expression level of gene 1567257_at.</p>
</dd>
<dt>1569664_at</dt><dd><p>Numeric. Expression level of gene 1569664_at.</p>
</dd>
<dt>1569882_at</dt><dd><p>Numeric. Expression level of gene 1569882_at.</p>
</dd>
<dt>1570252_at</dt><dd><p>Numeric. Expression level of gene 1570252_at.</p>
</dd>
<dt>201089_at</dt><dd><p>Numeric. Expression level of gene 201089_at.</p>
</dd>
<dt>201261_x_at</dt><dd><p>Numeric. Expression level of gene 201261_x_at.</p>
</dd>
<dt>202052_s_at</dt><dd><p>Numeric. Expression level of gene 202052_s_at.</p>
</dd>
<dt>202236_s_at</dt><dd><p>Numeric. Expression level of gene 202236_s_at.</p>
</dd>
<dt>202948_at</dt><dd><p>Numeric. Expression level of gene 202948_at.</p>
</dd>
<dt>203080_s_at</dt><dd><p>Numeric. Expression level of gene 203080_s_at.</p>
</dd>
<dt>203211_s_at</dt><dd><p>Numeric. Expression level of gene 203211_s_at.</p>
</dd>
<dt>203218_at</dt><dd><p>Numeric. Expression level of gene 203218_at.</p>
</dd>
<dt>203236_s_at</dt><dd><p>Numeric. Expression level of gene 203236_s_at.</p>
</dd>
<dt>203347_s_at</dt><dd><p>Numeric. Expression level of gene 203347_s_at.</p>
</dd>
<dt>203960_s_at</dt><dd><p>Numeric. Expression level of gene 203960_s_at.</p>
</dd>
<dt>204609_at</dt><dd><p>Numeric. Expression level of gene 204609_at.</p>
</dd>
<dt>204806_x_at</dt><dd><p>Numeric. Expression level of gene 204806_x_at.</p>
</dd>
<dt>204949_at</dt><dd><p>Numeric. Expression level of gene 204949_at.</p>
</dd>
<dt>204979_s_at</dt><dd><p>Numeric. Expression level of gene 204979_s_at.</p>
</dd>
<dt>205823_at</dt><dd><p>Numeric. Expression level of gene 205823_at.</p>
</dd>
<dt>205902_at</dt><dd><p>Numeric. Expression level of gene 205902_at.</p>
</dd>
<dt>205967_at</dt><dd><p>Numeric. Expression level of gene 205967_at.</p>
</dd>
<dt>206186_at</dt><dd><p>Numeric. Expression level of gene 206186_at.</p>
</dd>
<dt>207151_at</dt><dd><p>Numeric. Expression level of gene 207151_at.</p>
</dd>
<dt>207379_at</dt><dd><p>Numeric. Expression level of gene 207379_at.</p>
</dd>
<dt>207440_at</dt><dd><p>Numeric. Expression level of gene 207440_at.</p>
</dd>
<dt>207883_s_at</dt><dd><p>Numeric. Expression level of gene 207883_s_at.</p>
</dd>
<dt>208277_at</dt><dd><p>Numeric. Expression level of gene 208277_at.</p>
</dd>
<dt>208280_at</dt><dd><p>Numeric. Expression level of gene 208280_at.</p>
</dd>
<dt>209224_s_at</dt><dd><p>Numeric. Expression level of gene 209224_s_at.</p>
</dd>
<dt>209561_at</dt><dd><p>Numeric. Expression level of gene 209561_at.</p>
</dd>
<dt>209630_s_at</dt><dd><p>Numeric. Expression level of gene 209630_s_at.</p>
</dd>
<dt>210118_s_at</dt><dd><p>Numeric. Expression level of gene 210118_s_at.</p>
</dd>
<dt>210342_s_at</dt><dd><p>Numeric. Expression level of gene 210342_s_at.</p>
</dd>
<dt>211566_x_at</dt><dd><p>Numeric. Expression level of gene 211566_x_at.</p>
</dd>
<dt>211756_at</dt><dd><p>Numeric. Expression level of gene 211756_at.</p>
</dd>
<dt>212170_at</dt><dd><p>Numeric. Expression level of gene 212170_at.</p>
</dd>
<dt>212494_at</dt><dd><p>Numeric. Expression level of gene 212494_at.</p>
</dd>
<dt>213118_at</dt><dd><p>Numeric. Expression level of gene 213118_at.</p>
</dd>
<dt>214475_x_at</dt><dd><p>Numeric. Expression level of gene 214475_x_at.</p>
</dd>
<dt>214834_at</dt><dd><p>Numeric. Expression level of gene 214834_at.</p>
</dd>
<dt>215718_s_at</dt><dd><p>Numeric. Expression level of gene 215718_s_at.</p>
</dd>
<dt>216283_s_at</dt><dd><p>Numeric. Expression level of gene 216283_s_at.</p>
</dd>
<dt>217206_at</dt><dd><p>Numeric. Expression level of gene 217206_at.</p>
</dd>
<dt>217557_s_at</dt><dd><p>Numeric. Expression level of gene 217557_s_at.</p>
</dd>
<dt>217577_at</dt><dd><p>Numeric. Expression level of gene 217577_at.</p>
</dd>
<dt>218152_at</dt><dd><p>Numeric. Expression level of gene 218152_at.</p>
</dd>
<dt>218252_at</dt><dd><p>Numeric. Expression level of gene 218252_at.</p>
</dd>
<dt>219714_s_at</dt><dd><p>Numeric. Expression level of gene 219714_s_at.</p>
</dd>
<dt>220506_at</dt><dd><p>Numeric. Expression level of gene 220506_at.</p>
</dd>
<dt>220889_s_at</dt><dd><p>Numeric. Expression level of gene 220889_s_at.</p>
</dd>
<dt>221204_s_at</dt><dd><p>Numeric. Expression level of gene 221204_s_at.</p>
</dd>
<dt>221795_at</dt><dd><p>Numeric. Expression level of gene 221795_at.</p>
</dd>
<dt>222048_at</dt><dd><p>Numeric. Expression level of gene 222048_at.</p>
</dd>
<dt>223142_s_at</dt><dd><p>Numeric. Expression level of gene 223142_s_at.</p>
</dd>
<dt>223439_at</dt><dd><p>Numeric. Expression level of gene 223439_at.</p>
</dd>
<dt>223673_at</dt><dd><p>Numeric. Expression level of gene 223673_at.</p>
</dd>
<dt>224363_at</dt><dd><p>Numeric. Expression level of gene 224363_at.</p>
</dd>
<dt>224512_s_at</dt><dd><p>Numeric. Expression level of gene 224512_s_at.</p>
</dd>
<dt>224690_at</dt><dd><p>Numeric. Expression level of gene 224690_at.</p>
</dd>
<dt>224936_at</dt><dd><p>Numeric. Expression level of gene 224936_at.</p>
</dd>
<dt>225334_at</dt><dd><p>Numeric. Expression level of gene 225334_at.</p>
</dd>
<dt>225713_at</dt><dd><p>Numeric. Expression level of gene 225713_at.</p>
</dd>
<dt>225839_at</dt><dd><p>Numeric. Expression level of gene 225839_at.</p>
</dd>
<dt>226041_at</dt><dd><p>Numeric. Expression level of gene 226041_at.</p>
</dd>
<dt>226093_at</dt><dd><p>Numeric. Expression level of gene 226093_at.</p>
</dd>
<dt>226543_at</dt><dd><p>Numeric. Expression level of gene 226543_at.</p>
</dd>
<dt>227695_at</dt><dd><p>Numeric. Expression level of gene 227695_at.</p>
</dd>
<dt>228295_at</dt><dd><p>Numeric. Expression level of gene 228295_at.</p>
</dd>
<dt>228548_at</dt><dd><p>Numeric. Expression level of gene 228548_at.</p>
</dd>
<dt>229234_at</dt><dd><p>Numeric. Expression level of gene 229234_at.</p>
</dd>
<dt>229658_at</dt><dd><p>Numeric. Expression level of gene 229658_at.</p>
</dd>
<dt>229725_at</dt><dd><p>Numeric. Expression level of gene 229725_at.</p>
</dd>
<dt>230252_at</dt><dd><p>Numeric. Expression level of gene 230252_at.</p>
</dd>
<dt>230471_at</dt><dd><p>Numeric. Expression level of gene 230471_at.</p>
</dd>
<dt>231149_s_at</dt><dd><p>Numeric. Expression level of gene 231149_s_at.</p>
</dd>
<dt>231556_at</dt><dd><p>Numeric. Expression level of gene 231556_at.</p>
</dd>
<dt>231754_at</dt><dd><p>Numeric. Expression level of gene 231754_at.</p>
</dd>
<dt>232011_s_at</dt><dd><p>Numeric. Expression level of gene 232011_s_at.</p>
</dd>
<dt>233030_at</dt><dd><p>Numeric. Expression level of gene 233030_at.</p>
</dd>
<dt>234161_at</dt><dd><p>Numeric. Expression level of gene 234161_at.</p>
</dd>
<dt>235050_at</dt><dd><p>Numeric. Expression level of gene 235050_at.</p>
</dd>
<dt>235094_at</dt><dd><p>Numeric. Expression level of gene 235094_at.</p>
</dd>
<dt>235278_at</dt><dd><p>Numeric. Expression level of gene 235278_at.</p>
</dd>
<dt>235671_at</dt><dd><p>Numeric. Expression level of gene 235671_at.</p>
</dd>
<dt>235952_at</dt><dd><p>Numeric. Expression level of gene 235952_at.</p>
</dd>
<dt>236158_at</dt><dd><p>Numeric. Expression level of gene 236158_at.</p>
</dd>
<dt>236181_at</dt><dd><p>Numeric. Expression level of gene 236181_at.</p>
</dd>
<dt>237055_at</dt><dd><p>Numeric. Expression level of gene 237055_at.</p>
</dd>
<dt>237768_x_at</dt><dd><p>Numeric. Expression level of gene 237768_x_at.</p>
</dd>
<dt>238897_at</dt><dd><p>Numeric. Expression level of gene 238897_at.</p>
</dd>
<dt>239160_at</dt><dd><p>Numeric. Expression level of gene 239160_at.</p>
</dd>
<dt>239998_at</dt><dd><p>Numeric. Expression level of gene 239998_at.</p>
</dd>
<dt>240254_at</dt><dd><p>Numeric. Expression level of gene 240254_at.</p>
</dd>
<dt>240612_at</dt><dd><p>Numeric. Expression level of gene 240612_at.</p>
</dd>
<dt>240692_at</dt><dd><p>Numeric. Expression level of gene 240692_at.</p>
</dd>
<dt>240822_at</dt><dd><p>Numeric. Expression level of gene 240822_at.</p>
</dd>
<dt>240842_at</dt><dd><p>Numeric. Expression level of gene 240842_at.</p>
</dd>
<dt>241331_at</dt><dd><p>Numeric. Expression level of gene 241331_at.</p>
</dd>
<dt>241598_at</dt><dd><p>Numeric. Expression level of gene 241598_at.</p>
</dd>
<dt>241927_x_at</dt><dd><p>Numeric. Expression level of gene 241927_x_at.</p>
</dd>
<dt>242405_at</dt><dd><p>Numeric. Expression level of gene 242405_at.</p>
</dd>
</dl>



<h3>Details</h3>

<p>This dataset was extracted from a larger dataset available on ArrayExpress. It is used as a training set 
for feature selection tasks and other machine learning applications in bioinformatics.
</p>


<h3>Source</h3>

<p>The original dataset can be found on ArrayExpress: <a href="https://www.ebi.ac.uk/arrayexpress">https://www.ebi.ac.uk/arrayexpress</a>
</p>


<h3>References</h3>

<p>Ellenbach, N., Boulesteix, A.L., Bischl, B., et al. (2021). 
Improved Outcome Prediction Across Data Sources Through Robust Parameter Tuning. <em>Journal of Classification</em>, 
38, 212–231. <a href="https://doi.org/10.1007/s00357-020-09368-z">doi:10.1007/s00357-020-09368-z</a>.
</p>
<p>Hornung, R., Causeur, D., Bernau, C., Boulesteix, A.L. (2017). 
Improving cross-study prediction through addon batch effect adjustment or addon normalization. <em>Bioinformatics</em>, 
33(3), 397–404. <a href="https://doi.org/10.1093/bioinformatics/btw650">doi:10.1093/bioinformatics/btw650</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load the dataset:
data(sample_data_train)

# Dimension of the dataset:
dim(sample_data_train)

# View the first rows of the dataset:
head(sample_data_train)
</code></pre>

<hr>
<h2 id='tuneandtrain'>Tune and Train Classifier</h2><span id='topic+tuneandtrain'></span>

<h3>Description</h3>

<p>This function tunes and trains a classifier using a specified tuning method. Depending on the method chosen, 
the function will either perform RobustTuneC, external tuning, or internal tuning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrain(data, dataext = NULL, tuningmethod, classifier, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrain_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable, 
which must be a factor for classification tasks. The remaining columns should be the predictor variables. 
Ensure that the data is properly formatted, with no missing values.</p>
</td></tr>
<tr><td><code id="tuneandtrain_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data, required only for the tuning methods 
&quot;robusttunec&quot; and &quot;ext&quot;. Similar to the 'data' argument, the first column should be the response variable (factor), 
and the remaining columns should be the predictors. If 'tuningmethod = &quot;int&quot;', this parameter is ignored.</p>
</td></tr>
<tr><td><code id="tuneandtrain_+3A_tuningmethod">tuningmethod</code></td>
<td>
<p>A character string specifying which tuning approach to use. Options are:
</p>

<ul>
<li><p> &quot;robusttunec&quot;: Uses robust tuning that combines internal and external validation for parameter selection.
</p>
</li>
<li><p> &quot;ext&quot;: Uses external validation data for tuning the parameters.
</p>
</li>
<li><p> &quot;int&quot;: Internal cross-validation is used to tune the parameters without any external data.
</p>
</li></ul>
</td></tr>
<tr><td><code id="tuneandtrain_+3A_classifier">classifier</code></td>
<td>
<p>A character string specifying which classifier to use. Options include:
</p>

<ul>
<li><p> &quot;boosting&quot;: Boosting algorithms for improving weak classifiers.
</p>
</li>
<li><p> &quot;rf&quot;: Random Forest for robust decision tree-based models.
</p>
</li>
<li><p> &quot;lasso&quot;: Lasso regression for feature selection and regularization.
</p>
</li>
<li><p> &quot;ridge&quot;: Ridge regression for regularization.
</p>
</li>
<li><p> &quot;svm&quot;: Support Vector Machines for high-dimensional classification.
</p>
</li></ul>
</td></tr>
<tr><td><code id="tuneandtrain_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed to the specific tuning and training functions. These can include 
options such as the number of trees for Random Forest, the number of folds for cross-validation, or hyperparameters 
specific to the chosen classifier.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the results of the tuning and training process, which typically includes:
</p>

<ul>
<li><p> Best hyperparameters selected during the tuning process.
</p>
</li>
<li><p> The final trained model.
</p>
</li>
<li><p> Performance metrics (AUC) on the training or validation data, depending on the tuning method.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage: Robust tuning with Ridge classifier
result_boosting &lt;- tuneandtrain(sample_data_train, sample_data_extern, 
  tuningmethod = "robusttunec", classifier = "ridge")
result_boosting$best_lambda
result_boosting$best_model
result_boosting$final_auc

# Example usage: Internal cross-validation with Lasso classifier
result_lasso &lt;- tuneandtrain(sample_data_train, tuningmethod = "int", 
  classifier = "lasso", maxit = 120000, nlambda = 200, nfolds = 5)
result_lasso$best_lambda
result_lasso$best_model
result_lasso$final_auc
result_lasso$active_set_Train
</code></pre>

<hr>
<h2 id='tuneandtrainExt'>Tune and Train Classifier by Tuning Method Ext</h2><span id='topic+tuneandtrainExt'></span>

<h3>Description</h3>

<p>This function tunes and trains a classifier using an external validation dataset. Based on the specified classifier, 
the function selects and runs the appropriate tuning and training process. The external validation data is used 
to optimize the model's hyperparameters and improve generalization performance across datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainExt(data, dataext, classifier, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainExt_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables. Ensure that the data is properly formatted, 
with no missing values.</p>
</td></tr>
<tr><td><code id="tuneandtrainExt_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables. The external data is used for tuning hyperparameters to 
avoid overfitting on the training data.</p>
</td></tr>
<tr><td><code id="tuneandtrainExt_+3A_classifier">classifier</code></td>
<td>
<p>A character string specifying the classifier to use. Must be one of the following:
</p>

<ul>
<li><p> &quot;boosting&quot; for gradient boosting models.
</p>
</li>
<li><p> &quot;rf&quot; for Random Forest.
</p>
</li>
<li><p> &quot;lasso&quot; for Lasso regression (for feature selection and regularization).
</p>
</li>
<li><p> &quot;ridge&quot; for Ridge regression (for regularization).
</p>
</li>
<li><p> &quot;svm&quot; for Support Vector Machines (SVM).
</p>
</li></ul>
</td></tr>
<tr><td><code id="tuneandtrainExt_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to the specific classifier function. These may include hyperparameters 
such as the number of trees for Random Forest, regularization parameters for Lasso/Ridge, or kernel settings for SVM.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the results from the classifier's tuning and training process. The returned object typically includes:
</p>

<ul>
<li> <p><code>best_model</code>: The final trained model using the best hyperparameters.
</p>
</li>
<li> <p><code>best_hyperparams</code>: The optimal hyperparameters found during the tuning process.
</p>
</li>
<li> <p><code>final_auc</code>: Performance metrics (AUC) of the final model.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage with Lasso
result_lasso &lt;- tuneandtrainExt(sample_data_train, sample_data_extern, classifier = "lasso",
  maxit = 120000, nlambda = 100)
result_lasso$best_lambda
result_lasso$best_model
result_lasso$final_auc
result_lasso$active_set_Train

# Example usage with Ridge
result_ridge &lt;- tuneandtrainExt(sample_data_train, sample_data_extern, 
  classifier = "ridge", maxit = 120000, nlambda = 100)
result_ridge$best_lambda
result_ridge$best_model
result_ridge$final_auc
</code></pre>

<hr>
<h2 id='tuneandtrainExtBoost'>Tune and Train External Boosting</h2><span id='topic+tuneandtrainExtBoost'></span>

<h3>Description</h3>

<p>This function tunes and trains a Boosting classifier using the <code>mboost::glmboost</code> function.
It provides two strategies for tuning the number of boosting iterations (<code>mstop</code>) based on 
the <code>estperf</code> argument:
</p>

<ul>
<li><p> When <code>estperf = FALSE</code> (default): Hyperparameters are tuned using the external validation dataset. 
The <code>mstop</code> value that gives the highest AUC on the external dataset is selected as the best model.
However, no AUC value is returned in this case, as per best practices.
</p>
</li>
<li><p> When <code>estperf = TRUE</code>: Hyperparameters are tuned internally using the training dataset. 
The model is then validated on the external dataset to provide a conservative (slightly pessimistic) AUC estimate.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainExtBoost(
  data,
  dataext,
  estperf = FALSE,
  mstop_seq = seq(5, 1000, by = 5),
  nu = 0.1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainExtBoost_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtBoost_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the response variable 
(factor), and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtBoost_+3A_estperf">estperf</code></td>
<td>
<p>A logical value indicating whether to use internal tuning with external validation (<code>TRUE</code>) 
or external tuning (<code>FALSE</code>). Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtBoost_+3A_mstop_seq">mstop_seq</code></td>
<td>
<p>A numeric vector specifying the sequence of boosting iterations to evaluate. 
Default is <code>seq(5, 1000, by = 5)</code>.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtBoost_+3A_nu">nu</code></td>
<td>
<p>A numeric value specifying the learning rate for boosting. Default is <code>0.1</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>

<ul>
<li> <p><code>best_mstop</code>: The optimal number of boosting iterations determined during the tuning process.
</p>
</li>
<li> <p><code>best_model</code>: The trained Boosting model using the selected <code>mstop</code>.
</p>
</li>
<li> <p><code>est_auc</code>: The AUC value evaluated on the external dataset. This is only returned when <code>estperf = TRUE</code>, 
providing a conservative (slightly pessimistic) estimate of the model's performance.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage with external tuning (default)
mstop_seq &lt;- seq(50, 500, by = 50)
result &lt;- tuneandtrainExtBoost(sample_data_train, sample_data_extern, 
  mstop_seq = mstop_seq, nu = 0.1)
print(result$best_mstop)         # Optimal mstop
print(result$best_model)         # Trained Boosting model
# Note: est_auc is not returned when estperf = FALSE

# Example usage with internal tuning and external validation
result_internal &lt;- tuneandtrainExtBoost(sample_data_train, sample_data_extern, 
  estperf = TRUE, mstop_seq = mstop_seq, nu = 0.1)
print(result_internal$best_mstop) # Optimal mstop
print(result_internal$best_model) # Trained Boosting model
print(result_internal$est_auc)    # AUC on external validation dataset
</code></pre>

<hr>
<h2 id='tuneandtrainExtLasso'>Tune and Train External Lasso</h2><span id='topic+tuneandtrainExtLasso'></span>

<h3>Description</h3>

<p>This function tunes and trains a Lasso classifier using the <code>glmnet</code> package. 
It provides two strategies for tuning hyperparameters based on the <code>estperf</code> argument:
</p>

<ul>
<li><p> When <code>estperf = FALSE</code> (default): Hyperparameters are tuned using the external validation dataset. 
The lambda value that gives the highest AUC on the external dataset is selected as the best model.
However, no AUC value is returned in this case, as per best practices.
</p>
</li>
<li><p> When <code>estperf = TRUE</code>: Hyperparameters are tuned internally using the training dataset. 
The model is then validated on the external dataset to provide a conservative (slightly pessimistic) AUC estimate.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainExtLasso(
  data,
  dataext,
  estperf = FALSE,
  maxit = 120000,
  nlambda = 100
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainExtLasso_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtLasso_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the response variable 
(factor), and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtLasso_+3A_estperf">estperf</code></td>
<td>
<p>A logical value indicating whether to use internal tuning with external validation (<code>TRUE</code>) 
or external tuning (<code>FALSE</code>). Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtLasso_+3A_maxit">maxit</code></td>
<td>
<p>An integer specifying the maximum number of iterations. Default is 120000.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtLasso_+3A_nlambda">nlambda</code></td>
<td>
<p>An integer specifying the number of lambda values to use in the Lasso model. Default is 100.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>

<ul>
<li> <p><code>best_lambda</code>: The optimal lambda value determined during the tuning process.
</p>
</li>
<li> <p><code>best_model</code>: The trained Lasso model using the selected lambda value.
</p>
</li>
<li> <p><code>est_auc</code>: The AUC value evaluated on the external dataset. This is only returned when <code>estperf = TRUE</code>, 
providing a conservative (slightly pessimistic) estimate of the model's performance.
</p>
</li>
<li> <p><code>active_set_Train</code>: The number of active coefficients (non-zero) in the model trained on the training dataset.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage with external tuning (default)
result &lt;- tuneandtrainExtLasso(sample_data_train, sample_data_extern, maxit = 120000, nlambda = 100)
print(result$best_lambda)
print(result$best_model)
print(result$active_set_Train)

# Example usage with internal tuning and external validation
result_internal &lt;- tuneandtrainExtLasso(sample_data_train, sample_data_extern, 
  estperf = TRUE, maxit = 120000, nlambda = 100)
print(result_internal$best_lambda)
print(result_internal$best_model)
print(result_internal$est_auc)
print(result_internal$active_set_Train)
</code></pre>

<hr>
<h2 id='tuneandtrainExtRF'>Tune and Train External Random Forest</h2><span id='topic+tuneandtrainExtRF'></span>

<h3>Description</h3>

<p>This function tunes and trains a Random Forest classifier using the <code>ranger</code> package. 
It provides two strategies for tuning the <code>min.node.size</code> parameter based on the <code>estperf</code> argument:
</p>

<ul>
<li><p> When <code>estperf = FALSE</code> (default): Hyperparameters are tuned using the external validation dataset. 
The <code>min.node.size</code> value that gives the highest AUC on the external dataset is selected as the best model.
However, no AUC value is returned in this case, as per best practices.
</p>
</li>
<li><p> When <code>estperf = TRUE</code>: Hyperparameters are tuned internally using the training dataset. 
The model is then validated on the external dataset to provide a conservative (slightly pessimistic) AUC estimate.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainExtRF(data, dataext, estperf = FALSE, num.trees = 500)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainExtRF_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtRF_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the response 
variable (factor), and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtRF_+3A_estperf">estperf</code></td>
<td>
<p>A logical value indicating whether to use internal tuning with external validation (<code>TRUE</code>) 
or external tuning (<code>FALSE</code>). Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtRF_+3A_num.trees">num.trees</code></td>
<td>
<p>An integer specifying the number of trees in the Random Forest. Default is 500.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>

<ul>
<li> <p><code>best_min_node_size</code>: The optimal <code>min.node.size</code> value determined during the tuning process.
</p>
</li>
<li> <p><code>best_model</code>: The trained Random Forest model using the selected <code>min.node.size</code>.
</p>
</li>
<li> <p><code>est_auc</code>: The AUC value evaluated on the external dataset. This is only returned when <code>estperf = TRUE</code>, 
providing a conservative (slightly pessimistic) estimate of the model's performance.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage with external tuning (default)
result &lt;- tuneandtrainExtRF(sample_data_train, sample_data_extern, num.trees = 500)
print(result$best_min_node_size)  # Optimal min.node.size
print(result$best_model)          # Trained Random Forest model
# Note: est_auc is not returned when estperf = FALSE

# Example usage with internal tuning and external validation
result_internal &lt;- tuneandtrainExtRF(sample_data_train, sample_data_extern, 
  estperf = TRUE, num.trees = 500)
print(result_internal$best_min_node_size)  # Optimal min.node.size
print(result_internal$best_model)          # Trained Random Forest model
print(result_internal$est_auc)             # AUC on external validation dataset

</code></pre>

<hr>
<h2 id='tuneandtrainExtRidge'>Tune and Train External Ridge</h2><span id='topic+tuneandtrainExtRidge'></span>

<h3>Description</h3>

<p>This function tunes and trains a Ridge classifier using the <code>glmnet</code> package. 
It provides two strategies for tuning the regularization parameter <code>lambda</code> based on the <code>estperf</code> argument:
</p>

<ul>
<li><p> When <code>estperf = FALSE</code> (default): Hyperparameters are tuned using the external validation dataset. 
The <code>lambda</code> value that gives the highest AUC on the external dataset is selected as the best model.
However, no AUC value is returned in this case, as per best practices.
</p>
</li>
<li><p> When <code>estperf = TRUE</code>: Hyperparameters are tuned internally using the training dataset. 
The model is then validated on the external dataset to provide a conservative (slightly pessimistic) AUC estimate.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainExtRidge(
  data,
  dataext,
  estperf = FALSE,
  maxit = 120000,
  nlambda = 100
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainExtRidge_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtRidge_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the response 
variable (factor), and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtRidge_+3A_estperf">estperf</code></td>
<td>
<p>A logical value indicating whether to use internal tuning with external validation (<code>TRUE</code>) 
or external tuning (<code>FALSE</code>). Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtRidge_+3A_maxit">maxit</code></td>
<td>
<p>An integer specifying the maximum number of iterations. Default is 120000.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtRidge_+3A_nlambda">nlambda</code></td>
<td>
<p>An integer specifying the number of lambda values to use in the Ridge model. Default is 100.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>

<ul>
<li> <p><code>best_lambda</code>: The optimal <code>lambda</code> value determined during the tuning process.
</p>
</li>
<li> <p><code>best_model</code>: The trained Ridge model using the selected <code>lambda</code>.
</p>
</li>
<li> <p><code>est_auc</code>: The AUC value evaluated on the external dataset. This is only returned when <code>estperf = TRUE</code>, 
providing a conservative (slightly pessimistic) estimate of the model's performance.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage with external tuning (default)
result &lt;- tuneandtrainExtRidge(sample_data_train, sample_data_extern, maxit = 120000, nlambda = 100)
print(result$best_lambda)       # Optimal lambda
print(result$best_model)        # Final trained model
# Note: est_auc is not returned when estperf = FALSE

# Example usage with internal tuning and external validation
result_internal &lt;- tuneandtrainExtRidge(sample_data_train, sample_data_extern, 
  estperf = TRUE, maxit = 120000, nlambda = 100)
print(result_internal$best_lambda)  # Optimal lambda
print(result_internal$best_model)   # Final trained model
print(result_internal$est_auc)      # AUC on external validation dataset
</code></pre>

<hr>
<h2 id='tuneandtrainExtSVM'>Tune and Train External SVM</h2><span id='topic+tuneandtrainExtSVM'></span>

<h3>Description</h3>

<p>This function tunes and trains a Support Vector Machine (SVM) classifier using the <code>mlr</code> package. 
It provides two strategies for tuning the cost parameter based on the <code>estperf</code> argument:
</p>

<ul>
<li><p> When <code>estperf = FALSE</code> (default): Hyperparameters are tuned using the external validation dataset. 
The <code>cost</code> value that gives the highest AUC on the external dataset is selected as the best model.
However, no AUC value is returned in this case, as per best practices.
</p>
</li>
<li><p> When <code>estperf = TRUE</code>: Hyperparameters are tuned internally using the training dataset. 
The model is then validated on the external dataset to provide a conservative (slightly pessimistic) AUC estimate.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainExtSVM(
  data,
  dataext,
  estperf = FALSE,
  kernel = "linear",
  cost_seq = 2^(-15:15),
  scale = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainExtSVM_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtSVM_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the response 
variable (factor), and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtSVM_+3A_estperf">estperf</code></td>
<td>
<p>A logical value indicating whether to use internal tuning with external validation (<code>TRUE</code>) 
or external tuning (<code>FALSE</code>). Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtSVM_+3A_kernel">kernel</code></td>
<td>
<p>A character string specifying the kernel type to be used in the SVM. Default is <code>"linear"</code>.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtSVM_+3A_cost_seq">cost_seq</code></td>
<td>
<p>A numeric vector specifying the sequence of cost values to evaluate. Default is <code>2^(-15:15)</code>.</p>
</td></tr>
<tr><td><code id="tuneandtrainExtSVM_+3A_scale">scale</code></td>
<td>
<p>A logical value indicating whether to scale the predictor variables. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>

<ul>
<li> <p><code>best_cost</code>: The optimal cost value determined during the tuning process.
</p>
</li>
<li> <p><code>best_model</code>: The trained SVM model using the selected <code>cost</code>.
</p>
</li>
<li> <p><code>est_auc</code>: The AUC value evaluated on the external dataset. This is only returned when <code>estperf = TRUE</code>, 
providing a conservative (slightly pessimistic) estimate of the model's performance.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage with external tuning (default)
result &lt;- tuneandtrainExtSVM(sample_data_train, sample_data_extern, kernel = "linear", 
  cost_seq = 2^(-15:15), scale = FALSE)
print(result$best_cost)        # Optimal cost
print(result$best_model)       # Final trained model
# Note: est_auc is not returned when estperf = FALSE

# Example usage with internal tuning and external validation
result_internal &lt;- tuneandtrainExtSVM(sample_data_train, sample_data_extern, 
  estperf = TRUE, kernel = "linear", cost_seq = 2^(-15:15), scale = FALSE)
print(result_internal$best_cost)  # Optimal cost
print(result_internal$best_model) # Final trained model
print(result_internal$est_auc)    # AUC on external validation dataset

</code></pre>

<hr>
<h2 id='tuneandtrainInt'>Tune and Train by tuning method Int</h2><span id='topic+tuneandtrainInt'></span>

<h3>Description</h3>

<p>This function tunes and trains a specified classifier using internal cross-validation. The classifier is specified 
by the 'classifier' argument, and the function delegates to the appropriate tuning and training function based 
on this choice.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainInt(data, classifier, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainInt_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainInt_+3A_classifier">classifier</code></td>
<td>
<p>A character string specifying the classifier to use. 
Must be one of 'boosting', 'rf', 'lasso', 'ridge', 'svm'.</p>
</td></tr>
<tr><td><code id="tuneandtrainInt_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to the specific classifier function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the results from the specific classifier's tuning and training process. 
The list typically includes:
</p>

<ul>
<li> <p><code>best_hyperparams</code>: The best hyperparameters selected by cross-validation.
</p>
</li>
<li> <p><code>best_model</code>: The final trained model using the selected hyperparameters.
</p>
</li>
<li> <p><code>final_auc</code>: Cross-validation results (AUC).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)

# Example usage with Lasso
result_lasso &lt;- tuneandtrainInt(sample_data_train, classifier = "lasso",
  maxit = 120000, nlambda = 100)
result_lasso$best_lambda
result_lasso$best_model
result_lasso$final_auc
result_lasso$active_set_Train

# Example usage with Ridge
result_ridge &lt;- tuneandtrainInt(sample_data_train, classifier = "ridge", 
  maxit = 120000, nlambda = 100)
result_ridge$best_lambda
result_ridge$best_model
result_ridge$final_auc
</code></pre>

<hr>
<h2 id='tuneandtrainIntBoost'>Tune and Train Internal Boosting</h2><span id='topic+tuneandtrainIntBoost'></span>

<h3>Description</h3>

<p>This function tunes and trains a Boosting classifier using the <code>mboost</code> package. The function 
evaluates a sequence of boosting iterations on the training dataset using internal cross-validation 
and selects the best model based on the Area Under the Curve (AUC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainIntBoost(data, mstop_seq = seq(5, 1000, by = 5), nu = 0.1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainIntBoost_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntBoost_+3A_mstop_seq">mstop_seq</code></td>
<td>
<p>A numeric vector of boosting iterations to be evaluated. Default is a sequence 
from 5 to 1000 with a step of 5.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntBoost_+3A_nu">nu</code></td>
<td>
<p>A numeric value for the learning rate. Default is 0.1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs K-fold cross-validation on the training dataset, where the number of boosting 
iterations (<code>mstop</code>) is tuned to maximize the AUC. The optimal number of boosting iterations is selected, 
and the final model is trained on the entire training dataset.
</p>


<h3>Value</h3>

<p>A list containing the best number of boosting iterations ('best_mstop') and
the final Boosting classifier model ('best_model').
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Load sample data
data(sample_data_train)

# Example usage
mstop_seq &lt;- seq(5, 5000, by = 5)
result &lt;- tuneandtrainIntBoost(sample_data_train, mstop_seq, nu = 0.1)
result$best_mstop
result$best_model

</code></pre>

<hr>
<h2 id='tuneandtrainIntLasso'>Tune and Train Internal Lasso</h2><span id='topic+tuneandtrainIntLasso'></span>

<h3>Description</h3>

<p>This function tunes and trains a Lasso classifier using the <code>glmnet</code> package. The function 
performs internal cross-validation to evaluate a sequence of lambda (regularization) values and 
selects the best model based on the Area Under the Curve (AUC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainIntLasso(data, maxit = 120000, nlambda = 200, nfolds = 5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainIntLasso_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntLasso_+3A_maxit">maxit</code></td>
<td>
<p>An integer specifying the maximum number of iterations. Default is 120000.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntLasso_+3A_nlambda">nlambda</code></td>
<td>
<p>An integer specifying the number of lambda values to use in the Lasso model. Default is 200.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntLasso_+3A_nfolds">nfolds</code></td>
<td>
<p>An integer specifying the number of folds for cross-validation. Default is 5.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function trains a logistic Lasso model on the training dataset using cross-validation. 
The lambda value that results in the highest AUC during cross-validation is chosen as the best model, 
and the final model is trained on the full training dataset with this optimal lambda value.
</p>


<h3>Value</h3>

<p>A list containing the best lambda value ('best_lambda'), the final trained model ('best_model'), 
and the number of active coefficients ('active_set_Train').
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)

# Example usage
result &lt;- tuneandtrainIntLasso(sample_data_train, maxit = 120000, nlambda = 200, nfolds = 5)
result$best_lambda
result$best_model
result$active_set_Train
</code></pre>

<hr>
<h2 id='tuneandtrainIntRF'>Tune and Train Internal Random Forest</h2><span id='topic+tuneandtrainIntRF'></span>

<h3>Description</h3>

<p>This function tunes and trains a Random Forest classifier using the <code>ranger</code> package with internal cross-validation. 
The function evaluates a sequence of <code>min.node.size</code> values on the training dataset and selects 
the best model based on the Area Under the Curve (AUC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainIntRF(data, num.trees = 500, nfolds = 5, seed = 123)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainIntRF_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntRF_+3A_num.trees">num.trees</code></td>
<td>
<p>An integer specifying the number of trees in the Random Forest. Default is 500.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntRF_+3A_nfolds">nfolds</code></td>
<td>
<p>An integer specifying the number of folds for cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntRF_+3A_seed">seed</code></td>
<td>
<p>An integer specifying the random seed for reproducibility. Default is 123.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Random Forest constructs multiple decision trees and aggregates their predictions. 
The <code>min.node.size</code> parameter controls the minimum number of samples in each terminal node, affecting model complexity. 
This function performs cross-validation within the training dataset to evaluate the impact of different <code>min.node.size</code> values. 
The <code>min.node.size</code> value that results in the highest AUC is selected as the best model.
</p>


<h3>Value</h3>

<p>A list containing the best 'min.node.size' value ('best_min_node_size') and
the final trained model ('best_model').
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Load sample data
data(sample_data_train)

# Example usage
result &lt;- tuneandtrainIntRF(sample_data_train, num.trees = 500, nfolds = 5, seed = 123)
result$best_min_node_size
result$best_model

</code></pre>

<hr>
<h2 id='tuneandtrainIntRidge'>Tune and Train Internal Ridge</h2><span id='topic+tuneandtrainIntRidge'></span>

<h3>Description</h3>

<p>This function tunes and trains a Ridge classifier using the <code>glmnet</code> package. The function 
evaluates a sequence of lambda (regularization) values using internal cross-validation and selects 
the best model based on the Area Under the Curve (AUC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainIntRidge(
  data,
  maxit = 120000,
  nlambda = 200,
  nfolds = 5,
  seed = 123
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainIntRidge_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntRidge_+3A_maxit">maxit</code></td>
<td>
<p>An integer specifying the maximum number of iterations. Default is 120000.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntRidge_+3A_nlambda">nlambda</code></td>
<td>
<p>An integer specifying the number of lambda values to use in the Ridge model. Default is 200.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntRidge_+3A_nfolds">nfolds</code></td>
<td>
<p>An integer specifying the number of folds for cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntRidge_+3A_seed">seed</code></td>
<td>
<p>An integer specifying the random seed for reproducibility. Default is 123.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function trains a logistic Ridge regression model on the training dataset and performs cross-validation 
to select the best lambda value. The lambda value that gives the highest AUC on the training dataset during 
cross-validation is chosen as the best model.
</p>


<h3>Value</h3>

<p>A list containing the best lambda value ('best_lambda') and the final trained model ('best_model').
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)

# Example usage
result &lt;- tuneandtrainIntRidge(sample_data_train, maxit = 120000, 
  nlambda = 200, nfolds = 5, seed = 123)
result$best_lambda
result$best_model

</code></pre>

<hr>
<h2 id='tuneandtrainIntSVM'>Tune and Train Internal SVM</h2><span id='topic+tuneandtrainIntSVM'></span>

<h3>Description</h3>

<p>This function tunes and trains a Support Vector Machine (SVM) classifier using the <code>mlr</code> package. 
The function evaluates a sequence of cost values using internal cross-validation and selects 
the best model based on the Area Under the Curve (AUC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainIntSVM(
  data,
  kernel = "linear",
  cost_seq = 2^(-15:15),
  scale = FALSE,
  nfolds = 5,
  seed = 123
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainIntSVM_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntSVM_+3A_kernel">kernel</code></td>
<td>
<p>A character string specifying the kernel type to be used in the SVM. Default is &quot;linear&quot;.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntSVM_+3A_cost_seq">cost_seq</code></td>
<td>
<p>A numeric vector of cost values to be evaluated. Default is '2^(-15:15)'.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntSVM_+3A_scale">scale</code></td>
<td>
<p>A logical indicating whether to scale the predictor variables. Default is FALSE.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntSVM_+3A_nfolds">nfolds</code></td>
<td>
<p>An integer specifying the number of folds for cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="tuneandtrainIntSVM_+3A_seed">seed</code></td>
<td>
<p>An integer specifying the random seed for reproducibility. Default is 123.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In Support Vector Machines, the <code>cost</code> parameter controls the trade-off between 
achieving a low training error and a low testing error. 
This function trains an SVM model on the training dataset, performs cross-validation, and 
selects the cost value that results in the highest AUC. The final model is then trained using the optimal 
cost value, and the performance is reported based on the AUC.
</p>


<h3>Value</h3>

<p>A list containing the best cost value ('best_cost') and the final trained model ('best_model').
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Load sample data
data(sample_data_train)

# Example usage
result &lt;- tuneandtrainIntSVM(
  sample_data_train,
  kernel = "linear",
  cost_seq = 2^(-15:15),
  scale = FALSE,
  nfolds = 5,
  seed = 123
)
result$best_cost
result$best_model

</code></pre>

<hr>
<h2 id='tuneandtrainRobustTuneC'>Tune and Train Classifier by Tuning Method RobustTuneC</h2><span id='topic+tuneandtrainRobustTuneC'></span>

<h3>Description</h3>

<p>This function tunes and trains a specified classifier using the &quot;RobustTuneC&quot; method and the provided data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainRobustTuneC(data, dataext, classifier, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainRobustTuneC_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneC_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the 
response variable (factor), and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneC_+3A_classifier">classifier</code></td>
<td>
<p>A character string specifying the classifier to use. Must be one of the following:
</p>

<ul>
<li><p> &quot;boosting&quot; for Boosting classifiers.
</p>
</li>
<li><p> &quot;rf&quot; for Random Forest.
</p>
</li>
<li><p> &quot;lasso&quot; for Lasso regression.
</p>
</li>
<li><p> &quot;ridge&quot; for Ridge regression.
</p>
</li>
<li><p> &quot;svm&quot; for Support Vector Machines.
</p>
</li></ul>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneC_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to the specific classifier function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the results from the specific classifier's tuning and training process, 
the returned object typically includes:
</p>

<ul>
<li> <p><code>best_hyperparams</code>: The best hyperparameters selected through the RobustTuneC method.
</p>
</li>
<li> <p><code>best_model</code>: The final trained model based on the best hyperparameters.
</p>
</li>
<li> <p><code>final_auc</code>: Performance metrics (AUC) of the final model.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage with Lasso
result_lasso &lt;- tuneandtrainRobustTuneC(sample_data_train, sample_data_extern, classifier = "lasso",
  maxit = 120000, nlambda = 100)
result_lasso$best_lambda
result_lasso$best_model
result_lasso$final_auc
result_lasso$active_set_Train

# Example usage with Ridge
result_ridge &lt;- tuneandtrainRobustTuneC(sample_data_train, sample_data_extern, 
  classifier = "ridge", maxit = 120000, nlambda = 100)
result_ridge$best_lambda
result_ridge$best_model
result_ridge$final_auc
</code></pre>

<hr>
<h2 id='tuneandtrainRobustTuneCBoost'>Tune and Train RobustTuneC Boosting</h2><span id='topic+tuneandtrainRobustTuneCBoost'></span>

<h3>Description</h3>

<p>This function tunes and trains a Boosting classifier using the <code>mboost::glmboost</code> function 
and the &quot;RobustTuneC&quot; method. The function performs K-fold cross-validation on the training dataset 
and evaluates a sequence of boosting iterations (<code>mstop</code>) based on the Area Under the Curve (AUC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainRobustTuneCBoost(
  data,
  dataext,
  K = 5,
  mstop_seq = seq(5, 1000, by = 5),
  nu = 0.1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainRobustTuneCBoost_+3A_data">data</code></td>
<td>
<p>Training data as a data frame. The first column should be the response variable.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCBoost_+3A_dataext">dataext</code></td>
<td>
<p>External validation data as a data frame. The first column should be the response variable.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCBoost_+3A_k">K</code></td>
<td>
<p>Number of folds to use in cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCBoost_+3A_mstop_seq">mstop_seq</code></td>
<td>
<p>A sequence of boosting iterations to consider. Default is a sequence starting at 5 and 
increasing by 5 each time, up to 1000.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCBoost_+3A_nu">nu</code></td>
<td>
<p>Learning rate for the boosting algorithm. Default is 0.1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After cross-validation, the best <code>mstop</code> value is selected based on the AUC, and the final Boosting 
model is trained using this optimal <code>mstop</code>. The external validation dataset is then used to calculate 
the final AUC and assess the model performance.
</p>


<h3>Value</h3>

<p>A list containing the best number of boosting iterations ('best_mstop'), 
the final trained model ('best_model'), and the chosen c value('best_c').
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load the sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage with the sample data
mstop_seq &lt;- seq(50, 500, by = 50)
result &lt;- tuneandtrainRobustTuneCBoost(sample_data_train, sample_data_extern, mstop_seq = mstop_seq)
result$best_mstop
result$best_model
result$best_c
</code></pre>

<hr>
<h2 id='tuneandtrainRobustTuneCLasso'>Tune and Train RobustTuneC Lasso</h2><span id='topic+tuneandtrainRobustTuneCLasso'></span>

<h3>Description</h3>

<p>This function tunes and trains a Lasso classifier using the <code>glmnet</code> package and the &quot;RobustTuneC&quot; method.
The function uses K-fold cross-validation to evaluate a sequence of lambda (regularization) values and selects 
the best model based on the Area Under the Curve (AUC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainRobustTuneCLasso(
  data,
  dataext,
  K = 5,
  maxit = 120000,
  nlambda = 100
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainRobustTuneCLasso_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCLasso_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the response 
variable (factor), and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCLasso_+3A_k">K</code></td>
<td>
<p>Number of folds to use in cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCLasso_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations. Default is 120000.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCLasso_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of lambda values to use for cross-validation. Default is 100.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function trains a logistic Lasso model using the training dataset and validates it through cross-validation.
After selecting the best lambda value based on the training data, the model is then applied to an external validation dataset
to compute the final AUC. The lambda value that results in the highest AUC on the external validation dataset is chosen as the best model.
</p>


<h3>Value</h3>

<p>A list containing the best lambda value ('best_lambda'), the final trained model ('best_model'), 
the number of active coefficients ('active_set_Train'), and the chosen c value('best_c').
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage
result &lt;- tuneandtrainRobustTuneCLasso(sample_data_train, sample_data_extern, 
  K = 5, maxit = 120000, nlambda = 100)
result$best_lambda
result$best_model
result$best_c
</code></pre>

<hr>
<h2 id='tuneandtrainRobustTuneCRF'>Tune and Train RobustTuneC Random Forest</h2><span id='topic+tuneandtrainRobustTuneCRF'></span>

<h3>Description</h3>

<p>This function tunes and trains a Random Forest classifier using the <code>ranger</code> package and the &quot;RobustTuneC&quot; method. 
The function uses K-fold cross-validation to evaluate different <code>min.node.size</code> values on the training dataset 
and selects the best model based on the Area Under the Curve (AUC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainRobustTuneCRF(data, dataext, K = 5, num.trees = 500)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainRobustTuneCRF_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCRF_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the response 
variable (factor), and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCRF_+3A_k">K</code></td>
<td>
<p>Number of folds to use in cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCRF_+3A_num.trees">num.trees</code></td>
<td>
<p>An integer specifying the number of trees to grow in the Random Forest. Default is 500.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Random Forest constructs multiple decision trees and aggregates their predictions. 
The <code>min.node.size</code> parameter controls the minimum number of samples in each terminal node, affecting model complexity. 
This function evaluates the <code>min.node.size</code> values through cross-validation and then applies the best model to an 
external validation dataset. The <code>min.node.size</code> value that results in the highest AUC on the validation dataset is selected.
</p>


<h3>Value</h3>

<p>A list containing the best minimum node size ('best_min_node_size'), 
the final trained model ('best_model'), and the chosen c value('best_c').
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage
result &lt;- tuneandtrainRobustTuneCRF(sample_data_train, sample_data_extern, K = 5, num.trees = 500)
result$best_min_node_size
result$best_model
result$best_c

</code></pre>

<hr>
<h2 id='tuneandtrainRobustTuneCRidge'>Tune and Train RobustTuneC Ridge</h2><span id='topic+tuneandtrainRobustTuneCRidge'></span>

<h3>Description</h3>

<p>This function tunes and trains a Ridge classifier using the <code>glmnet</code> package with the &quot;RobustTuneC&quot; method.
The function evaluates a sequence of lambda (regularization) values using K-fold cross-validation (K specified by the user) 
on the training dataset and selects the best model based on Area Under the Curve (AUC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainRobustTuneCRidge(
  data,
  dataext,
  K = 5,
  maxit = 120000,
  nlambda = 100
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainRobustTuneCRidge_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCRidge_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the response 
variable (factor), and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCRidge_+3A_k">K</code></td>
<td>
<p>Number of folds to use in cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCRidge_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations. Default is 120000.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCRidge_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of lambda values to use for cross-validation. Default is 100.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function first performs K-fold cross-validation on the training dataset to select the best lambda value based on AUC. 
Then, the model is further validated on an external dataset, and the lambda value that provides the best performance on 
the external dataset is chosen as the final model. The Ridge regression is fitted using the selected lambda value, and 
the final model's performance is evaluated using AUC on the external validation dataset.
</p>


<h3>Value</h3>

<p>A list containing the best lambda value ('best_lambda'), the final trained model ('best_model'), 
and the chosen c value('best_c').
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage
result &lt;- tuneandtrainRobustTuneCRidge(sample_data_train, sample_data_extern, 
  K = 5, maxit = 120000, nlambda = 100)
result$best_lambda
result$best_model
result$best_c
</code></pre>

<hr>
<h2 id='tuneandtrainRobustTuneCSVM'>Tune and Train RobustTuneC Support Vector Machine (SVM)</h2><span id='topic+tuneandtrainRobustTuneCSVM'></span>

<h3>Description</h3>

<p>This function tunes and trains a Support Vector Machine (SVM) classifier using the &quot;RobustTuneC&quot; method. 
It performs K-fold cross-validation (with K specified by the user) to select the best model based on 
the Area Under the Curve (AUC) metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneandtrainRobustTuneCSVM(
  data,
  dataext,
  K = 5,
  seed = 123,
  kernel = "linear",
  cost_seq = 2^(-15:15),
  scale = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneandtrainRobustTuneCSVM_+3A_data">data</code></td>
<td>
<p>A data frame containing the training data. The first column should be the response variable (factor), 
and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCSVM_+3A_dataext">dataext</code></td>
<td>
<p>A data frame containing the external validation data. The first column should be the response variable (factor), and the remaining columns should be the predictor variables.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCSVM_+3A_k">K</code></td>
<td>
<p>Number of folds to use in cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCSVM_+3A_seed">seed</code></td>
<td>
<p>An integer specifying the random seed for reproducibility. Default is 123.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCSVM_+3A_kernel">kernel</code></td>
<td>
<p>A character string specifying the kernel type to be used in the SVM. 
It can be &quot;linear&quot;, &quot;polynomial&quot;, &quot;radial&quot;, or &quot;sigmoid&quot;. Default is &quot;linear&quot;.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCSVM_+3A_cost_seq">cost_seq</code></td>
<td>
<p>A numeric vector of cost values to be evaluated. Default is '2^(-15:15)'.</p>
</td></tr>
<tr><td><code id="tuneandtrainRobustTuneCSVM_+3A_scale">scale</code></td>
<td>
<p>A logical value indicating whether to scale the predictor variables. Default is 'FALSE'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In Support Vector Machines, the <code>cost</code> parameter controls the trade-off between achieving 
a low training error and a low testing error. 
This function trains an SVM model on the training dataset, performs cross-validation to evaluate different 
<code>cost</code> values, and selects the one that yields the highest AUC. 
The final model is trained using the optimal cost value, and its performance is reported using the AUC metric 
on the external validation dataset.
</p>


<h3>Value</h3>

<p>A list containing the best cost value ('best_cost'), the final trained model ('best_model'), 
and the chosen c value('best_c').
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Load sample data
data(sample_data_train)
data(sample_data_extern)

# Example usage
result &lt;- tuneandtrainRobustTuneCSVM(sample_data_train, sample_data_extern, K = 5, seed = 123, 
                                     kernel = "linear", cost_seq = 2^(-15:15), scale = FALSE)
result$best_cost
result$best_model
result$best_c

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
