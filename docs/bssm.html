<!DOCTYPE html><html><head><title>Help for package bssm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bssm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ar1_lg'><p>Univariate Gaussian model with AR(1) latent process</p></a></li>
<li><a href='#ar1_ng'><p>Non-Gaussian model with AR(1) latent process</p></a></li>
<li><a href='#as_bssm'><p>Convert KFAS Model to bssm Model</p></a></li>
<li><a href='#as_draws_df.mcmc_output'><p>Convert <code>run_mcmc</code> Output to <code>draws_df</code> Format</p></a></li>
<li><a href='#as.data.frame.mcmc_output'><p>Convert MCMC Output to data.frame</p></a></li>
<li><a href='#asymptotic_var'><p>Asymptotic Variance of IS-type Estimators</p></a></li>
<li><a href='#bootstrap_filter'><p>Bootstrap Filtering</p></a></li>
<li><a href='#bsm_lg'><p>Basic Structural (Time Series) Model</p></a></li>
<li><a href='#bsm_ng'><p>Non-Gaussian Basic Structural (Time Series) Model</p></a></li>
<li><a href='#bssm'><p>Bayesian Inference of State Space Models</p></a></li>
<li><a href='#check_diagnostics'><p>Quick Diagnostics Checks for <code>run_mcmc</code> Output</p></a></li>
<li><a href='#cpp_example_model'><p>Example C++ Codes for Non-Linear and SDE Models</p></a></li>
<li><a href='#drownings'><p>Deaths by drowning in Finland in 1969-2019</p></a></li>
<li><a href='#ekf'><p>(Iterated) Extended Kalman Filtering</p></a></li>
<li><a href='#ekf_smoother'><p>Extended Kalman Smoothing</p></a></li>
<li><a href='#ekpf_filter'><p>Extended Kalman Particle Filtering</p></a></li>
<li><a href='#estimate_ess'><p>Effective Sample Size for IS-type Estimators</p></a></li>
<li><a href='#exchange'><p>Pound/Dollar daily exchange rates</p></a></li>
<li><a href='#expand_sample'><p>Expand the Jump Chain representation</p></a></li>
<li><a href='#fast_smoother'><p>Kalman Smoothing</p></a></li>
<li><a href='#fitted.mcmc_output'><p>Fitted for State Space Model</p></a></li>
<li><a href='#gaussian_approx'><p>Gaussian Approximation of Non-Gaussian/Non-linear State Space Model</p></a></li>
<li><a href='#iact'><p>Integrated Autocorrelation Time</p></a></li>
<li><a href='#importance_sample'><p>Importance Sampling from non-Gaussian State Space Model</p></a></li>
<li><a href='#kfilter'><p>Kalman Filtering</p></a></li>
<li><a href='#logLik.lineargaussian'><p>Extract Log-likelihood of a State Space Model of class <code>bssm_model</code></p></a></li>
<li><a href='#negbin_model'><p>Estimated Negative Binomial Model of Helske and Vihola (2021)</p></a></li>
<li><a href='#negbin_series'><p>Simulated Negative Binomial Time Series Data</p></a></li>
<li><a href='#particle_smoother'><p>Particle Smoothing</p></a></li>
<li><a href='#plot.mcmc_output'><p>Trace and Density Plots for <code>mcmc_output</code></p></a></li>
<li><a href='#poisson_series'><p>Simulated Poisson Time Series Data</p></a></li>
<li><a href='#post_correct'><p>Run Post-correction for Approximate MCMC using <code class="reqn">\psi</code>-APF</p></a></li>
<li><a href='#predict.mcmc_output'><p>Predictions for State Space Models</p></a></li>
<li><a href='#print.mcmc_output'><p>Print Results from MCMC Run</p></a></li>
<li><a href='#run_mcmc'><p>Bayesian Inference of State Space Models</p></a></li>
<li><a href='#sim_smoother'><p>Simulation Smoothing</p></a></li>
<li><a href='#ssm_mlg'><p>General multivariate linear Gaussian state space models</p></a></li>
<li><a href='#ssm_mng'><p>General Non-Gaussian State Space Model</p></a></li>
<li><a href='#ssm_nlg'><p>General multivariate nonlinear Gaussian state space models</p></a></li>
<li><a href='#ssm_sde'><p>Univariate state space model with continuous SDE dynamics</p></a></li>
<li><a href='#ssm_ulg'><p>General univariate linear-Gaussian state space models</p></a></li>
<li><a href='#ssm_ung'><p>General univariate non-Gaussian state space model</p></a></li>
<li><a href='#suggest_N'><p>Suggest Number of Particles for <code class="reqn">\psi</code>-APF Post-correction</p></a></li>
<li><a href='#summary.mcmc_output'><p>Summary Statistics of Posterior Samples</p></a></li>
<li><a href='#svm'><p>Stochastic Volatility Model</p></a></li>
<li><a href='#ukf'><p>Unscented Kalman Filtering</p></a></li>
<li><a href='#uniform_prior'><p>Prior objects for bssm models</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Inference of Non-Linear and Non-Gaussian State Space
Models</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Efficient methods for Bayesian inference of state space models 
    via Markov chain Monte Carlo (MCMC) based on parallel 
    importance sampling type weighted estimators 
    (Vihola, Helske, and Franks, 2020, &lt;<a href="https://doi.org/10.1111%2Fsjos.12492">doi:10.1111/sjos.12492</a>&gt;), 
    particle MCMC, and its delayed acceptance version. 
    Gaussian, Poisson, binomial, negative binomial, and Gamma
    observation densities and basic stochastic volatility models 
    with linear-Gaussian state dynamics, as well as general non-linear Gaussian 
    models and discretised diffusion models are supported. 
    See Helske and Vihola (2021, &lt;<a href="https://doi.org/10.32614%2FRJ-2021-103">doi:10.32614/RJ-2021-103</a>&gt;) for details.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, ggplot2 (&ge; 2.0.0), KFAS (&ge; 1.2.1), knitr (&ge; 1.11),
MASS, rmarkdown (&ge; 0.8.1), ramcmc, sde, sitmo, testthat</td>
</tr>
<tr>
<td>Imports:</td>
<td>bayesplot, checkmate, coda (&ge; 0.18-1), diagis, dplyr,
posterior, Rcpp (&ge; 0.12.3), rlang, tidyr</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>ramcmc, Rcpp, RcppArmadillo, sitmo</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>pandoc (&gt;= 1.12.3, needed for vignettes)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/helske/bssm/issues">https://github.com/helske/bssm/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/helske/bssm">https://github.com/helske/bssm</a></td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-27 10:43:10 UTC; jvhels</td>
</tr>
<tr>
<td>Author:</td>
<td>Jouni Helske <a href="https://orcid.org/0000-0001-7130-793X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Matti Vihola <a href="https://orcid.org/0000-0002-8041-7222"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jouni Helske &lt;jouni.helske@iki.fi&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-27 12:00:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='ar1_lg'>Univariate Gaussian model with AR(1) latent process</h2><span id='topic+ar1_lg'></span>

<h3>Description</h3>

<p>Constructs a simple Gaussian model where the state dynamics
follow an AR(1) process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ar1_lg(y, rho, sigma, mu, sd_y, beta, xreg = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ar1_lg_+3A_y">y</code></td>
<td>
<p>A vector or a <code>ts</code> object of observations.</p>
</td></tr>
<tr><td><code id="ar1_lg_+3A_rho">rho</code></td>
<td>
<p>A prior for autoregressive coefficient.
Should be an object of class <code>bssm_prior</code>.</p>
</td></tr>
<tr><td><code id="ar1_lg_+3A_sigma">sigma</code></td>
<td>
<p>A prior for the standard deviation of noise of the AR-process.
Should be an object of class <code>bssm_prior</code></p>
</td></tr>
<tr><td><code id="ar1_lg_+3A_mu">mu</code></td>
<td>
<p>A fixed value or a prior for the stationary mean of the latent
AR(1) process. Should be an object of class <code>bssm_prior</code> or scalar
value defining a fixed mean such as 0.</p>
</td></tr>
<tr><td><code id="ar1_lg_+3A_sd_y">sd_y</code></td>
<td>
<p>A prior for the standard deviation of observation equation.</p>
</td></tr>
<tr><td><code id="ar1_lg_+3A_beta">beta</code></td>
<td>
<p>A prior for the regression coefficients.
Should be an object of class <code>bssm_prior</code> or <code>bssm_prior_list</code>
(in case of multiple coefficients) or missing in case of no covariates.</p>
</td></tr>
<tr><td><code id="ar1_lg_+3A_xreg">xreg</code></td>
<td>
<p>A matrix containing covariates with number of rows matching the
length of <code>y</code>. Can also be <code>ts</code>, <code>mts</code> or similar object
convertible to matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>ar1_lg</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
mu &lt;- 2
rho &lt;- 0.7
sd_y &lt;- 0.1
sigma &lt;- 0.5
beta &lt;- -1
x &lt;- rnorm(30)
z &lt;- y &lt;- numeric(30)
z[1] &lt;- rnorm(1, mu, sigma / sqrt(1 - rho^2))
y[1] &lt;- rnorm(1, beta * x[1] + z[1], sd_y)
for(i in 2:30) {
  z[i] &lt;- rnorm(1, mu * (1 - rho) + rho * z[i - 1], sigma)
  y[i] &lt;- rnorm(1, beta * x[i] + z[i], sd_y)
}
model &lt;- ar1_lg(y, rho = uniform(0.5, -1, 1), 
  sigma = halfnormal(1, 10), mu = normal(0, 0, 1), 
  sd_y = halfnormal(1, 10), 
  xreg = x,  beta = normal(0, 0, 1))
out &lt;- run_mcmc(model, iter = 2e4)
summary(out, return_se = TRUE)

</code></pre>

<hr>
<h2 id='ar1_ng'>Non-Gaussian model with AR(1) latent process</h2><span id='topic+ar1_ng'></span>

<h3>Description</h3>

<p>Constructs a simple non-Gaussian model where the state dynamics follow an
AR(1) process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ar1_ng(y, rho, sigma, mu, distribution, phi, u, beta, xreg = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ar1_ng_+3A_y">y</code></td>
<td>
<p>A vector or a <code>ts</code> object of observations.</p>
</td></tr>
<tr><td><code id="ar1_ng_+3A_rho">rho</code></td>
<td>
<p>A prior for autoregressive coefficient.
Should be an object of class <code>bssm_prior</code>.</p>
</td></tr>
<tr><td><code id="ar1_ng_+3A_sigma">sigma</code></td>
<td>
<p>A prior for the standard deviation of noise of the AR-process.
Should be an object of class <code>bssm_prior</code></p>
</td></tr>
<tr><td><code id="ar1_ng_+3A_mu">mu</code></td>
<td>
<p>A fixed value or a prior for the stationary mean of the latent
AR(1) process. Should be an object of class <code>bssm_prior</code> or scalar
value defining a fixed mean such as 0.</p>
</td></tr>
<tr><td><code id="ar1_ng_+3A_distribution">distribution</code></td>
<td>
<p>Distribution of the observed time series. Possible
choices are <code>"poisson"</code>, <code>"binomial"</code>, <code>"gamma"</code>, and
<code>"negative binomial"</code>.</p>
</td></tr>
<tr><td><code id="ar1_ng_+3A_phi">phi</code></td>
<td>
<p>Additional parameter relating to the non-Gaussian distribution.
For negative binomial distribution this is the dispersion term, for gamma
distribution this is the shape parameter, and for other distributions this
is ignored. Should an object of class <code>bssm_prior</code> or
a positive scalar.</p>
</td></tr>
<tr><td><code id="ar1_ng_+3A_u">u</code></td>
<td>
<p>A vector of positive constants for non-Gaussian models. For
Poisson, gamma, and negative binomial distribution, this corresponds to the
offset term. For binomial, this is the number of trials.</p>
</td></tr>
<tr><td><code id="ar1_ng_+3A_beta">beta</code></td>
<td>
<p>A prior for the regression coefficients.
Should be an object of class <code>bssm_prior</code> or <code>bssm_prior_list</code>
(in case of multiple coefficients) or missing in case of no covariates.</p>
</td></tr>
<tr><td><code id="ar1_ng_+3A_xreg">xreg</code></td>
<td>
<p>A matrix containing covariates with number of rows matching the
length of <code>y</code>. Can also be <code>ts</code>, <code>mts</code> or similar object
convertible to matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>ar1_ng</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- ar1_ng(discoveries, rho = uniform(0.5,-1,1), 
  sigma = halfnormal(0.1, 1), mu = normal(0, 0, 1), 
  distribution = "poisson")
out &lt;- run_mcmc(model, iter = 1e4, mcmc_type = "approx",
  output_type = "summary")
  
ts.plot(cbind(discoveries, exp(out$alphahat)), col = 1:2)

set.seed(1)
n &lt;- 30
phi &lt;- 2
rho &lt;- 0.9
sigma &lt;- 0.1
beta &lt;- 0.5
u &lt;- rexp(n, 0.1)
x &lt;- rnorm(n)
z &lt;- y &lt;- numeric(n)
z[1] &lt;- rnorm(1, 0, sigma / sqrt(1 - rho^2))
y[1] &lt;- rnbinom(1, mu = u * exp(beta * x[1] + z[1]), size = phi)
for(i in 2:n) {
  z[i] &lt;- rnorm(1, rho * z[i - 1], sigma)
  y[i] &lt;- rnbinom(1, mu = u * exp(beta * x[i] + z[i]), size = phi)
}

model &lt;- ar1_ng(y, rho = uniform_prior(0.9, 0, 1), 
  sigma = gamma_prior(0.1, 2, 10), mu = 0., 
  phi = gamma_prior(2, 2, 1), distribution = "negative binomial",
  xreg = x, beta = normal_prior(0.5, 0, 1), u = u)

</code></pre>

<hr>
<h2 id='as_bssm'>Convert KFAS Model to bssm Model</h2><span id='topic+as_bssm'></span>

<h3>Description</h3>

<p>Converts <code>SSModel</code> object of <code>KFAS</code> package to general <code>bssm</code>
model of type <code>ssm_ulg</code>, <code>ssm_mlg</code>, <code>ssm_ung</code> or
<code>ssm_mng</code>. As <code>KFAS</code> supports formula syntax for defining
e.g. regression and cyclic components it maybe sometimes easier to define
the model with <code>KFAS::SSModel</code> and then convert for the bssm style with
<code>as_bssm</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_bssm(model, kappa = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_bssm_+3A_model">model</code></td>
<td>
<p>Object of class <code>SSModel</code>.</p>
</td></tr>
<tr><td><code id="as_bssm_+3A_kappa">kappa</code></td>
<td>
<p>For <code>SSModel</code> object, a prior variance for initial state
used to replace exact diffuse elements of the original model.</p>
</td></tr>
<tr><td><code id="as_bssm_+3A_...">...</code></td>
<td>
<p>Additional arguments to model building functions of <code>bssm</code>
(such as prior and updating functions, C, and D).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>ssm_ulg</code>, <code>ssm_mlg</code>, <code>ssm_ung</code> or
<code>ssm_mng</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("KFAS")
  model_KFAS &lt;- SSModel(Nile ~
    SSMtrend(1, Q = 2, P1 = 1e4), H = 2)
  model_bssm &lt;- as_bssm(model_KFAS)  
  logLik(model_KFAS)
  logLik(model_bssm)

</code></pre>

<hr>
<h2 id='as_draws_df.mcmc_output'>Convert <code>run_mcmc</code> Output to <code>draws_df</code> Format</h2><span id='topic+as_draws_df.mcmc_output'></span><span id='topic+as_draws'></span><span id='topic+as_draws_df'></span><span id='topic+as_draws.mcmc_output'></span>

<h3>Description</h3>

<p>Converts MCMC output from <code>run_mcmc</code> call to a
<code>draws_df</code> format of the <code>posterior</code> package. This enables the use
of diagnostics and plotting methods of <code>posterior</code> and <code>bayesplot</code>
packages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcmc_output'
as_draws_df(x, times, states, ...)

## S3 method for class 'mcmc_output'
as_draws(x, times, states, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_draws_df.mcmc_output_+3A_x">x</code></td>
<td>
<p>An object of class <code>mcmc_output</code>.</p>
</td></tr>
<tr><td><code id="as_draws_df.mcmc_output_+3A_times">times</code></td>
<td>
<p>A vector of indices defining which time points to return?
Default is all. If 0, no samples for the states are extracted.</p>
</td></tr>
<tr><td><code id="as_draws_df.mcmc_output_+3A_states">states</code></td>
<td>
<p>A vector of indices defining which states to return.
Default is all. If 0, no samples for the states are extracted.</p>
</td></tr>
<tr><td><code id="as_draws_df.mcmc_output_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>draws_df</code> object.
</p>


<h3>Note</h3>

<p>The jump chain representation is automatically expanded by
<code>as_draws</code>, but if <code>run_mcmc</code> used IS-MCMC method, the output
contains additional <code>weight</code> column corresponding to the IS-weights
(without counts), which is ignored by <code>posterior</code> and <code>bayesplot</code>,
i.e. those results correspond to approximate MCMC.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
model &lt;- bsm_lg(Nile,
  sd_y = tnormal(init = 100, mean = 100, sd = 100, min = 0),
  sd_level = tnormal(init = 50, mean = 50, sd = 100, min = 0),
  a1 = 1000, P1 = 500^2)

fit1 &lt;- run_mcmc(model, iter = 2000)
draws &lt;- as_draws(fit1)
head(draws, 4)
estimate_ess(draws$sd_y)
summary(fit1, return_se = TRUE)

# More chains:
model$theta[] &lt;- c(50, 150) # change initial value
fit2 &lt;- run_mcmc(model, iter = 2000, verbose = FALSE)
model$theta[] &lt;- c(150, 50) # change initial value
fit3 &lt;- run_mcmc(model, iter = 2000, verbose = FALSE)

# it is actually enough to transform first mcmc_output to draws object,
# rest are transformed automatically inside bind_draws
draws &lt;- posterior::bind_draws(as_draws(fit1),
  as_draws(fit2), as_draws(fit3), along = "chain")

posterior::rhat(draws$sd_y)

</code></pre>

<hr>
<h2 id='as.data.frame.mcmc_output'>Convert MCMC Output to data.frame</h2><span id='topic+as.data.frame.mcmc_output'></span>

<h3>Description</h3>

<p>Converts the MCMC output of <code><a href="#topic+run_mcmc">run_mcmc</a></code> to <code>data.frame</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcmc_output'
as.data.frame(
  x,
  row.names,
  optional,
  variable = c("theta", "states"),
  times,
  states,
  expand = TRUE,
  use_times = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.data.frame.mcmc_output_+3A_x">x</code></td>
<td>
<p>Object of class <code>mcmc_output</code> from <code><a href="#topic+run_mcmc">run_mcmc</a></code>.</p>
</td></tr>
<tr><td><code id="as.data.frame.mcmc_output_+3A_row.names">row.names</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="as.data.frame.mcmc_output_+3A_optional">optional</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="as.data.frame.mcmc_output_+3A_variable">variable</code></td>
<td>
<p>Return samples of <code>"theta"</code> (default) or
<code>"states"</code>?</p>
</td></tr>
<tr><td><code id="as.data.frame.mcmc_output_+3A_times">times</code></td>
<td>
<p>A vector of indices. In case of states,
what time points to return? Default is all.</p>
</td></tr>
<tr><td><code id="as.data.frame.mcmc_output_+3A_states">states</code></td>
<td>
<p>A vector of indices. In case of states,
what states to return? Default is all.</p>
</td></tr>
<tr><td><code id="as.data.frame.mcmc_output_+3A_expand">expand</code></td>
<td>
<p>Should the jump-chain be expanded?
Defaults to <code>TRUE</code>.
For <code>expand = FALSE</code> and always for IS-MCMC,
the resulting data.frame contains variable weight (= counts * IS-weights).</p>
</td></tr>
<tr><td><code id="as.data.frame.mcmc_output_+3A_use_times">use_times</code></td>
<td>
<p>If <code>TRUE</code> (default), transforms the values of the time
variable to match the ts attribute of the input to define. If <code>FALSE</code>,
time is based on the indexing starting from 1.</p>
</td></tr>
<tr><td><code id="as.data.frame.mcmc_output_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code>as_draws</code> which converts the output for
<code>as_draws</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("poisson_series")
model &lt;- bsm_ng(y = poisson_series, 
sd_slope = halfnormal(0.1, 0.1), 
sd_level = halfnormal(0.1, 1),
  distribution = "poisson")
  
out &lt;- run_mcmc(model, iter = 2000, particles = 10)
head(as.data.frame(out, variable = "theta"))
head(as.data.frame(out, variable = "state"))

# don't expand the jump chain:
head(as.data.frame(out, variable = "theta", expand = FALSE))

# IS-weighted version:
out_is &lt;- run_mcmc(model, iter = 2000, particles = 10, 
  mcmc_type  = "is2")
head(as.data.frame(out_is, variable = "theta"))

</code></pre>

<hr>
<h2 id='asymptotic_var'>Asymptotic Variance of IS-type Estimators</h2><span id='topic+asymptotic_var'></span>

<h3>Description</h3>

<p>The asymptotic variance MCMCSE^2 is based on Corollary 1
of Vihola et al. (2020) from weighted samples from IS-MCMC. The default
method is based on the integrated autocorrelation time (IACT) by Sokal
(1997) which seem to work well for reasonable problems, but it is also
possible to use the Geyer's method as implemented in <code>ess_mean</code> of the
<code>posterior</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>asymptotic_var(x, w, method = "sokal")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asymptotic_var_+3A_x">x</code></td>
<td>
<p>A numeric vector of samples.</p>
</td></tr>
<tr><td><code id="asymptotic_var_+3A_w">w</code></td>
<td>
<p>A numeric vector of weights. If missing, set to 1 (i.e. no
weighting is assumed).</p>
</td></tr>
<tr><td><code id="asymptotic_var_+3A_method">method</code></td>
<td>
<p>Method for computing IACT. Default is <code>"sokal"</code>,
other option <code>"geyer"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single numeric value of asymptotic variance estimate.
</p>


<h3>References</h3>

<p>Vihola M, Helske J, Franks J. (2020). Importance sampling type estimators
based on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 1-38. https://doi.org/10.1111/sjos.12492
</p>
<p>Sokal A. (1997). Monte Carlo Methods in Statistical Mechanics: Foundations
and New Algorithms.
In: DeWitt-Morette C, Cartier P, Folacci A (eds) Functional Integration.
NATO ASI Series (Series B: Physics), vol 361. Springer, Boston, MA.
https://doi.org/10.1007/978-1-4899-0319-8_6
</p>
<p>Gelman, A, Carlin J B, Stern H S, Dunson, D B, Vehtari A, Rubin D B. (2013).
Bayesian Data Analysis, Third Edition. Chapman and Hall/CRC.
</p>
<p>Vehtari A, Gelman A, Simpson D, Carpenter B, Bürkner P-C. (2021).
Rank-normalization, folding, and localization: An improved Rhat for
assessing convergence of MCMC. Bayesian analysis, 16(2):667-718.
https://doi.org/10.1214/20-BA1221
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 1e4 
x &lt;- numeric(n)
phi &lt;- 0.7
for(t in 2:n) x[t] &lt;- phi * x[t-1] + rnorm(1)
w &lt;- rexp(n, 0.5 * exp(0.001 * x^2))
# different methods:
asymptotic_var(x, w, method = "sokal")
asymptotic_var(x, w, method = "geyer")

data("negbin_model")
# can be obtained directly with summary method
d &lt;- suppressWarnings(as_draws(negbin_model))
sqrt(asymptotic_var(d$sd_level, d$weight))

</code></pre>

<hr>
<h2 id='bootstrap_filter'>Bootstrap Filtering</h2><span id='topic+bootstrap_filter'></span><span id='topic+bootstrap_filter.lineargaussian'></span><span id='topic+bootstrap_filter.nongaussian'></span><span id='topic+bootstrap_filter.ssm_nlg'></span><span id='topic+bootstrap_filter.ssm_sde'></span>

<h3>Description</h3>

<p>Function <code>bootstrap_filter</code> performs a bootstrap filtering with
stratification resampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_filter(model, particles, ...)

## S3 method for class 'lineargaussian'
bootstrap_filter(
  model,
  particles,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)

## S3 method for class 'nongaussian'
bootstrap_filter(
  model,
  particles,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)

## S3 method for class 'ssm_nlg'
bootstrap_filter(
  model,
  particles,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)

## S3 method for class 'ssm_sde'
bootstrap_filter(
  model,
  particles,
  L,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrap_filter_+3A_model">model</code></td>
<td>
<p>A model object of class <code>bssm_model</code>.</p>
</td></tr>
<tr><td><code id="bootstrap_filter_+3A_particles">particles</code></td>
<td>
<p>Number of particles as a positive integer. Suitable values
depend on the model and the data, and while larger values provide more
accurate estimates, the run time also increases with respect to the
number of particles, so it is generally a good idea to test the filter first
with a small number of particles, e.g., less than 100.</p>
</td></tr>
<tr><td><code id="bootstrap_filter_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="bootstrap_filter_+3A_seed">seed</code></td>
<td>
<p>Seed for the C++ RNG (positive integer).</p>
</td></tr>
<tr><td><code id="bootstrap_filter_+3A_l">L</code></td>
<td>
<p>Positive integer defining the discretization level for SDE models.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with samples (<code>alpha</code>) from the filtering distribution and
corresponding weights (<code>weights</code>), as well as filtered and predicted
states and corresponding covariances (<code>at</code>, <code>att</code>, <code>Pt</code>,
<code>Ptt</code>), and estimated log-likelihood (<code>logLik</code>).
</p>


<h3>References</h3>

<p>Gordon, NJ, Salmond, DJ, Smith, AFM (1993) Novel approach to
nonlinear/non-Gaussian Bayesian state estimation. IEE Proceedings F,
140(2), p. 107-113.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- cumsum(rnorm(50))
y &lt;- rnorm(50, x, 0.5) 
model &lt;- bsm_lg(y, sd_y = 0.5, sd_level = 1, P1 = 1)
  
out &lt;- bootstrap_filter(model, particles = 1000)
ts.plot(cbind(y, x, out$att), col = 1:3)
ts.plot(cbind(kfilter(model)$att, out$att), col = 1:3)

data("poisson_series")
model &lt;- bsm_ng(poisson_series, sd_level = 0.1, sd_slope = 0.01, 
  P1 = diag(1, 2), distribution = "poisson")
  
out &lt;- bootstrap_filter(model, particles = 100)
ts.plot(cbind(poisson_series, exp(out$att[, 1])), col = 1:2)

</code></pre>

<hr>
<h2 id='bsm_lg'>Basic Structural (Time Series) Model</h2><span id='topic+bsm_lg'></span>

<h3>Description</h3>

<p>Constructs a basic structural model with local level or local trend
component and seasonal component.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bsm_lg(
  y,
  sd_y,
  sd_level,
  sd_slope,
  sd_seasonal,
  beta,
  xreg = NULL,
  period,
  a1 = NULL,
  P1 = NULL,
  D = NULL,
  C = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bsm_lg_+3A_y">y</code></td>
<td>
<p>A vector or a <code>ts</code> object of observations.</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_sd_y">sd_y</code></td>
<td>
<p>Standard deviation of the noise of observation equation.
Should be an object of class <code>bssm_prior</code> or scalar
value defining a known value such as 0.</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_sd_level">sd_level</code></td>
<td>
<p>Standard deviation of the noise of level equation.
Should be an object of class <code>bssm_prior</code> or scalar
value defining a known value such as 0.</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_sd_slope">sd_slope</code></td>
<td>
<p>Standard deviation of the noise of slope equation.
Should be an object of class <code>bssm_prior</code>, scalar
value defining a known value such as 0, or missing, in which case the slope
term is omitted from the model.</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_sd_seasonal">sd_seasonal</code></td>
<td>
<p>Standard deviation of the noise of seasonal equation.
Should be an object of class <code>bssm_prior</code>, scalar
value defining a known value such as 0, or missing, in which case the
seasonal term is omitted from the model.</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_beta">beta</code></td>
<td>
<p>A prior for the regression coefficients.
Should be an object of class <code>bssm_prior</code> or <code>bssm_prior_list</code>
(in case of multiple coefficients) or missing in case of no covariates.</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_xreg">xreg</code></td>
<td>
<p>A matrix containing covariates with number of rows matching the
length of <code>y</code>. Can also be <code>ts</code>, <code>mts</code> or similar object
convertible to matrix.</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_period">period</code></td>
<td>
<p>Length of the seasonal pattern.
Must be a positive value greater than 2 and less than the length of the
input time series. Default is <code>frequency(y)</code>,
which can also return non-integer value (in which case error is given).</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_a1">a1</code></td>
<td>
<p>Prior means for the initial states (level, slope, seasonals).
Defaults to vector of zeros.</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_p1">P1</code></td>
<td>
<p>Prior covariance matrix for the initial states (level, slope,
seasonals).Default is diagonal matrix with 100 on the diagonal.</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_d">D</code></td>
<td>
<p>Intercept terms for observation equation, given as a length n
numeric vector or a scalar in case of time-invariant intercept.</p>
</td></tr>
<tr><td><code id="bsm_lg_+3A_c">C</code></td>
<td>
<p>Intercept terms for state equation, given as a m times n matrix
or m times 1 matrix in case of time-invariant intercept.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>bsm_lg</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 50
x &lt;- rnorm(n)
level &lt;- numeric(n)
level[1] &lt;- rnorm(1)
for (i in 2:n) level[i] &lt;- rnorm(1, -0.2 + level[i-1], sd = 0.1)
y &lt;- rnorm(n, 2.1 + x + level)
model &lt;- bsm_lg(y, sd_y = halfnormal(1, 5), sd_level = 0.1, a1 = level[1], 
  P1 = matrix(0, 1, 1), xreg = x, beta = normal(1, 0, 1),
  D = 2.1, C = matrix(-0.2, 1, 1))
  
ts.plot(cbind(fast_smoother(model), level), col = 1:2)

prior &lt;- uniform(0.1 * sd(log10(UKgas)), 0, 1)
# period here is redundant as frequency(UKgas) = 4
model_UKgas &lt;- bsm_lg(log10(UKgas), sd_y = prior, sd_level =  prior,
  sd_slope =  prior, sd_seasonal =  prior, period = 4)

# Note small number of iterations for CRAN checks
mcmc_out &lt;- run_mcmc(model_UKgas, iter = 5000)
summary(mcmc_out, return_se = TRUE)
# Use the summary method from coda:
summary(expand_sample(mcmc_out, "theta"))$stat
mcmc_out$theta[which.max(mcmc_out$posterior), ]
sqrt((fit &lt;- StructTS(log10(UKgas), type = "BSM"))$coef)[c(4, 1:3)]

</code></pre>

<hr>
<h2 id='bsm_ng'>Non-Gaussian Basic Structural (Time Series) Model</h2><span id='topic+bsm_ng'></span>

<h3>Description</h3>

<p>Constructs a non-Gaussian basic structural model with local level or
local trend component, a seasonal component, and regression component
(or subset of these components).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bsm_ng(
  y,
  sd_level,
  sd_slope,
  sd_seasonal,
  sd_noise,
  distribution,
  phi,
  u,
  beta,
  xreg = NULL,
  period,
  a1 = NULL,
  P1 = NULL,
  C = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bsm_ng_+3A_y">y</code></td>
<td>
<p>A vector or a <code>ts</code> object of observations.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_sd_level">sd_level</code></td>
<td>
<p>Standard deviation of the noise of level equation.
Should be an object of class <code>bssm_prior</code> or scalar
value defining a known value such as 0.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_sd_slope">sd_slope</code></td>
<td>
<p>Standard deviation of the noise of slope equation.
Should be an object of class <code>bssm_prior</code>, scalar
value defining a known value such as 0, or missing, in which case the slope
term is omitted from the model.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_sd_seasonal">sd_seasonal</code></td>
<td>
<p>Standard deviation of the noise of seasonal equation.
Should be an object of class <code>bssm_prior</code>, scalar
value defining a known value such as 0, or missing, in which case the
seasonal term is omitted from the model.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_sd_noise">sd_noise</code></td>
<td>
<p>A prior for the standard deviation of the additional noise
term to be added to linear predictor, defined as an object of class
<code>bssm_prior</code>. If missing, no additional noise term is used.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_distribution">distribution</code></td>
<td>
<p>Distribution of the observed time series. Possible
choices are <code>"poisson"</code>, <code>"binomial"</code>, <code>"gamma"</code>, and
<code>"negative binomial"</code>.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_phi">phi</code></td>
<td>
<p>Additional parameter relating to the non-Gaussian distribution.
For negative binomial distribution this is the dispersion term, for gamma
distribution this is the shape parameter, and for other distributions this
is ignored. Should an object of class <code>bssm_prior</code> or
a positive scalar.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_u">u</code></td>
<td>
<p>A vector of positive constants for non-Gaussian models. For
Poisson, gamma, and negative binomial distribution, this corresponds to the
offset term. For binomial, this is the number of trials.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_beta">beta</code></td>
<td>
<p>A prior for the regression coefficients.
Should be an object of class <code>bssm_prior</code> or <code>bssm_prior_list</code>
(in case of multiple coefficients) or missing in case of no covariates.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_xreg">xreg</code></td>
<td>
<p>A matrix containing covariates with number of rows matching the
length of <code>y</code>. Can also be <code>ts</code>, <code>mts</code> or similar object
convertible to matrix.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_period">period</code></td>
<td>
<p>Length of the seasonal pattern.
Must be a positive value greater than 2 and less than the length of the
input time series. Default is <code>frequency(y)</code>,
which can also return non-integer value (in which case error is given).</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_a1">a1</code></td>
<td>
<p>Prior means for the initial states (level, slope, seasonals).
Defaults to vector of zeros.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_p1">P1</code></td>
<td>
<p>Prior covariance matrix for the initial states (level, slope,
seasonals).Default is diagonal matrix with 100 on the diagonal.</p>
</td></tr>
<tr><td><code id="bsm_ng_+3A_c">C</code></td>
<td>
<p>Intercept terms for state equation, given as a m x n or m x 1
matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>bsm_ng</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Same data as in Vihola, Helske, Franks (2020)
data(poisson_series)
s &lt;- sd(log(pmax(0.1, poisson_series)))
model &lt;- bsm_ng(poisson_series, sd_level = uniform(0.115, 0, 2 * s),
 sd_slope = uniform(0.004, 0, 2 * s), P1 = diag(0.1, 2), 
 distribution = "poisson")


out &lt;- run_mcmc(model, iter = 1e5, particles = 10)
summary(out, variable = "theta", return_se = TRUE)
# should be about 0.093 and 0.016
summary(out, variable = "states", return_se = TRUE, 
 states = 1, times = c(1, 100))
# should be about -0.075, 2.618


model &lt;- bsm_ng(Seatbelts[, "VanKilled"], distribution = "poisson",
  sd_level = halfnormal(0.01, 1),
  sd_seasonal = halfnormal(0.01, 1),
  beta = normal(0, 0, 10),
  xreg = Seatbelts[, "law"],
  # default values, just for illustration
  period = 12L,
  a1 = rep(0, 1 + 11), # level + period - 1 seasonal states
  P1 = diag(1, 12),
  C = matrix(0, 12, 1),
  u = rep(1, nrow(Seatbelts)))


set.seed(123)
mcmc_out &lt;- run_mcmc(model, iter = 5000, particles = 10, mcmc_type = "da")
mcmc_out$acceptance_rate
theta &lt;- expand_sample(mcmc_out, "theta")
plot(theta)
summary(theta)

library("ggplot2")
ggplot(as.data.frame(theta[,1:2]), aes(x = sd_level, y = sd_seasonal)) +
  geom_point() + stat_density2d(aes(fill = ..level.., alpha = ..level..),
  geom = "polygon") + scale_fill_continuous(low = "green", high = "blue") +
  guides(alpha = "none")

# Traceplot using as.data.frame method for MCMC output
library("dplyr")
as.data.frame(mcmc_out) |&gt; 
  filter(variable == "sd_level") |&gt; 
  ggplot(aes(y = value, x = iter)) + geom_line()
  

# Model with slope term and additional noise to linear predictor to capture 
# excess variation   
model2 &lt;- bsm_ng(Seatbelts[, "VanKilled"], distribution = "poisson",
  sd_level = halfnormal(0.01, 1),
  sd_seasonal = halfnormal(0.01, 1),
  beta = normal(0, 0, 10),
  xreg = Seatbelts[, "law"],
  sd_slope = halfnormal(0.01, 0.1),
  sd_noise = halfnormal(0.01, 1))

# instead of extra noise term, model using negative binomial distribution:
model3 &lt;- bsm_ng(Seatbelts[, "VanKilled"], 
  distribution = "negative binomial",
  sd_level = halfnormal(0.01, 1),
  sd_seasonal = halfnormal(0.01, 1),
  beta = normal(0, 0, 10),
  xreg = Seatbelts[, "law"],
  sd_slope = halfnormal(0.01, 0.1),
  phi = gamma_prior(1, 5, 5)) 

</code></pre>

<hr>
<h2 id='bssm'>Bayesian Inference of State Space Models</h2><span id='topic+bssm'></span><span id='topic+bssm-package'></span>

<h3>Description</h3>

<p>This package contains functions for efficient Bayesian inference of state
space models (SSMs). For details, see the package vignette and the R Journal
paper.
</p>


<h3>Details</h3>

<p>The model is assumed to be either
</p>

<ul>
<li><p> Exponential family state space model, where the state equation is linear
Gaussian, and the conditional observation density is either Gaussian,
Poisson, binomial, negative binomial or Gamma density.
</p>
</li>
<li><p> Basic stochastic volatility model.
</p>
</li>
<li><p> General non-linear model with Gaussian noise terms.
</p>
</li>
<li><p> Model with continuous SDE dynamics.
</p>
</li></ul>

<p>Missing values in response series are allowed as per SSM theory and can be
automatically predicted, but there can be no missing values in the system
matrices of the model.
</p>
<p>The package contains multiple functions for building the model:
</p>

<ul>
<li> <p><code>bsm_lg</code> for basic univariate structural time series model (BSM),
<code>ar1</code> for univariate noisy AR(1) process, and <code>ssm_ulg</code> and <code>ssm_mlg</code> for
arbitrary linear gaussian model with univariate/multivariate
observations.
</p>
</li>
<li><p> The non-Gaussian versions (where observations are non-Gaussian) of the
above models can be constructed using the functions <code>bsm_ng</code>, <code>ar1_ng</code>,
<code>ssm_ung</code> and <code>ssm_mng</code>.
</p>
</li>
<li><p> An univariate stochastic volatility model can be defined using a function
<code>svm</code>.
</p>
</li>
<li><p> For non-linear models, user must define the model using C++ snippets and
the the function <code>ssm_nlg</code>. See details in the <code>growth_model</code> vignette.
</p>
</li>
<li><p> Diffusion models can be defined with the function <code>ssm_sde</code>, again using
the C++ snippets. See <code>sde_model</code> vignette for details.
</p>
</li></ul>

<p>See the corresponding functions for some examples and details.
</p>
<p>After building the model, the model can be estimated via <code>run_mcmc</code>
function. The documentation of this function gives some examples. The
<code>bssm</code> package includes several MCMC sampling and sequential Monte
Carlo methods for models outside classic linear-Gaussian framework. For
definitions of the currently supported models and methods, usage of the
package as well as some theory behind the novel IS-MCMC and
<code class="reqn">\psi</code>-APF algorithms, see Helske and Vihola (2021), Vihola,
Helske, Franks (2020), and the package vignettes.
</p>
<p>The output of the <code>run_mcmc</code> can be analysed by extracting the posterior
samples of the latent states and hyperparameters using <code>as.data.frame</code>,
<code>as_draws</code>, <code>expand_sample</code>, and <code>summary</code> methods, as well as <code>fitted</code> and
<code>predict</code> methods. Some MCMC diagnostics checks are available via
<code>check_diagnostics</code> function, some of which are also provided via the print
method of the <code>run_mcmc</code> output. Functionality of the <code>ggplot2</code> and
<code>bayesplot</code>, can be used to visualize the posterior draws or their summary
statistics, and further diagnostics checks can be performed with the help of
the <code>posterior</code> and <code>coda</code> packages.
</p>


<h3>References</h3>

<p>Helske J, Vihola M (2021). bssm: Bayesian Inference of Non-linear and
Non-Gaussian State Space Models in R. The R Journal (2021) 13:2, 578-589.
https://doi.org/10.32614/RJ-2021-103
</p>
<p>Vihola, M, Helske, J, Franks, J. (2020). Importance sampling type estimators
based on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 1-38. https://doi.org/10.1111/sjos.12492
</p>
<p>H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag
New York, 2016.
</p>
<p>Gabry J, Mahr T (2022). “bayesplot: Plotting for Bayesian Models.” R package
version 1.9.0, https://mc-stan.org/bayesplot.
</p>
<p>Bürkner P, Gabry J, Kay M, Vehtari A (2022). “posterior: Tools for Working
with Posterior Distributions.” R package version 1.2.1,
https://mc-stan.org/posterior.
</p>
<p>Martyn Plummer, Nicky Best, Kate Cowles and Karen Vines (2006). CODA:
Convergence Diagnosis and Output Analysis for MCMC, R News, vol 6, 7-11.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create a local level model (latent random walk + noise) to the Nile
# dataset using the bsm_lg function:
model &lt;- bsm_lg(Nile,
  sd_y = tnormal(init = 100, mean = 100, sd = 100, min = 0),
  sd_level = tnormal(init = 50, mean = 50, sd = 100, min = 0),
  a1 = 1000, P1 = 500^2)

# the priors for the unknown paramters sd_y and sd_level were defined
# as trunctated normal distributions, see ?bssm_prior for details

# Run the MCMC for 2000 iterations (notice the small number of iterations to
# comply with the CRAN's check requirements)
fit &lt;- run_mcmc(model, iter = 2000)

# Some diagnostics checks:
check_diagnostics(fit)

# print some summary information:
fit

# traceplots:
plot(fit)

# extract the summary statistics for state variable
sumr &lt;- summary(fit,variable = "states")

# visualize
library("ggplot2")
ggplot(sumr, aes(time, Mean)) +
    geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`),alpha = 0.25) +
    geom_line() +
    theme_bw()

</code></pre>

<hr>
<h2 id='check_diagnostics'>Quick Diagnostics Checks for <code>run_mcmc</code> Output</h2><span id='topic+check_diagnostics'></span>

<h3>Description</h3>

<p>Prints out the acceptance rate, smallest effective sample sizes (ESS) and
largest Rhat values for a quick first check that the sampling worked. For
further checks, see e.g. <code>bayesplot</code> and <code>coda</code> packages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_diagnostics(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_diagnostics_+3A_x">x</code></td>
<td>
<p>Results object of class <code>mcmc_output</code> from
<code><a href="#topic+run_mcmc">run_mcmc</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For methods other than IS-MCMC, the estimates are based on the improved
diagnostics from the <code>posterior</code> package.For IS-MCMC, these Rhat,
bulk-ESS, and tail-ESS estimates are based on the approximate posterior
which should look reasonable, otherwise the IS-correction does not make much
sense. For IS-MCMC, ESS estimates based on a weighted posterior are also
computed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 30
phi &lt;- 2
rho &lt;- 0.9
sigma &lt;- 0.1
beta &lt;- 0.5
u &lt;- rexp(n, 0.1)
x &lt;- rnorm(n)
z &lt;- y &lt;- numeric(n)
z[1] &lt;- rnorm(1, 0, sigma / sqrt(1 - rho^2))
y[1] &lt;- rnbinom(1, mu = u * exp(beta * x[1] + z[1]), size = phi)
for(i in 2:n) {
  z[i] &lt;- rnorm(1, rho * z[i - 1], sigma)
  y[i] &lt;- rnbinom(1, mu = u * exp(beta * x[i] + z[i]), size = phi)
}

model &lt;- ar1_ng(y, rho = uniform_prior(0.9, 0, 1), 
  sigma = gamma_prior(0.1, 2, 10), mu = 0., 
  phi = gamma_prior(2, 2, 1), distribution = "negative binomial",
  xreg = x, beta = normal_prior(0.5, 0, 1), u = u)
  
out &lt;- run_mcmc(model, iter = 1000, particles = 10)
check_diagnostics(out)
</code></pre>

<hr>
<h2 id='cpp_example_model'>Example C++ Codes for Non-Linear and SDE Models</h2><span id='topic+cpp_example_model'></span>

<h3>Description</h3>

<p>Example C++ Codes for Non-Linear and SDE Models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpp_example_model(example, return_code = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpp_example_model_+3A_example">example</code></td>
<td>
<p>Name of the example model.
Run <code>cpp_example_model("abc")</code> to get the names of possible models.</p>
</td></tr>
<tr><td><code id="cpp_example_model_+3A_return_code">return_code</code></td>
<td>
<p>If TRUE, will not compile the model but only returns the
corresponding code.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns pointers to the C++ snippets defining the model, or in case
of <code>return_code = TRUE</code>, returns the example code without compiling.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cpp_example_model("sde_poisson_OU", return_code = TRUE)

</code></pre>

<hr>
<h2 id='drownings'>Deaths by drowning in Finland in 1969-2019</h2><span id='topic+drownings'></span>

<h3>Description</h3>

<p>Dataset containing number of deaths by drowning in Finland in 1969-2019,
corresponding population sizes (in hundreds of thousands), and
yearly average summer temperatures (June to August), based on simple
unweighted average of three weather stations: Helsinki (Southern Finland),
Jyvaskyla (Central Finland), and Sodankyla (Northern Finland).
</p>


<h3>Format</h3>

<p>A time series object containing 51 observations.
</p>


<h3>Source</h3>

<p>Statistics Finland
<a href="https://stat.fi/tup/tilastotietokannat/index_en.html">https://stat.fi/tup/tilastotietokannat/index_en.html</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("drownings")
model &lt;- bsm_ng(drownings[, "deaths"], u = drownings[, "population"],
  xreg = drownings[, "summer_temp"], distribution = "poisson",
  beta = normal(0, 0, 1),
  sd_level = gamma_prior(0.1,2, 10), sd_slope = gamma_prior(0, 2, 10))

fit &lt;- run_mcmc(model, iter = 5000,
  output_type = "summary", mcmc_type = "approx")
fit
ts.plot(model$y/model$u, exp(fit$alphahat[, 1]), col = 1:2)
</code></pre>

<hr>
<h2 id='ekf'>(Iterated) Extended Kalman Filtering</h2><span id='topic+ekf'></span>

<h3>Description</h3>

<p>Function <code>ekf</code> runs the (iterated) extended Kalman filter for the given
non-linear Gaussian model of class <code>ssm_nlg</code>,
and returns the filtered estimates and one-step-ahead predictions of the
states <code class="reqn">\alpha_t</code> given the data up to time <code class="reqn">t</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ekf(model, iekf_iter = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ekf_+3A_model">model</code></td>
<td>
<p>Model of class <code>ssm_nlg</code>.</p>
</td></tr>
<tr><td><code id="ekf_+3A_iekf_iter">iekf_iter</code></td>
<td>
<p>Non-negative integer. The default zero corresponds to
normal EKF, whereas <code>iekf_iter &gt; 0</code> corresponds to iterated EKF
with <code>iekf_iter</code> iterations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the log-likelihood,
one-step-ahead predictions <code>at</code> and filtered
estimates <code>att</code> of states, and the corresponding variances <code>Pt</code> and
<code>Ptt</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Takes a while on CRAN
set.seed(1)
mu &lt;- -0.2
rho &lt;- 0.7
sigma_y &lt;- 0.1
sigma_x &lt;- 1
x &lt;- numeric(50)
x[1] &lt;- rnorm(1, mu, sigma_x / sqrt(1 - rho^2))
for(i in 2:length(x)) {
  x[i] &lt;- rnorm(1, mu * (1 - rho) + rho * x[i - 1], sigma_x)
}
y &lt;- rnorm(50, exp(x), sigma_y)

pntrs &lt;- cpp_example_model("nlg_ar_exp")

model_nlg &lt;- ssm_nlg(y = y, a1 = pntrs$a1, P1 = pntrs$P1, 
  Z = pntrs$Z_fn, H = pntrs$H_fn, T = pntrs$T_fn, R = pntrs$R_fn, 
  Z_gn = pntrs$Z_gn, T_gn = pntrs$T_gn,
  theta = c(mu= mu, rho = rho, 
    log_sigma_x = log(sigma_x), log_sigma_y = log(sigma_y)), 
  log_prior_pdf = pntrs$log_prior_pdf,
  n_states = 1, n_etas = 1, state_names = "state")

out_ekf &lt;- ekf(model_nlg, iekf_iter = 0)
out_iekf &lt;- ekf(model_nlg, iekf_iter = 5)
ts.plot(cbind(x, out_ekf$att, out_iekf$att), col = 1:3)

</code></pre>

<hr>
<h2 id='ekf_smoother'>Extended Kalman Smoothing</h2><span id='topic+ekf_smoother'></span><span id='topic+ekf_fast_smoother'></span>

<h3>Description</h3>

<p>Function <code>ekf_smoother</code> runs the (iterated) extended Kalman smoother
for the given non-linear Gaussian model of class <code>ssm_nlg</code>,
and returns the smoothed estimates of the states and the corresponding
variances. Function <code>ekf_fast_smoother</code> computes only smoothed
estimates of the states.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ekf_smoother(model, iekf_iter = 0)

ekf_fast_smoother(model, iekf_iter = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ekf_smoother_+3A_model">model</code></td>
<td>
<p>Model of class <code>ssm_nlg</code>.</p>
</td></tr>
<tr><td><code id="ekf_smoother_+3A_iekf_iter">iekf_iter</code></td>
<td>
<p>Non-negative integer. The default zero corresponds to
normal EKF, whereas <code>iekf_iter &gt; 0</code> corresponds to iterated EKF
with <code>iekf_iter</code> iterations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the log-likelihood,
smoothed state estimates <code>alphahat</code>, and the corresponding variances
<code>Vt</code> and <code>Ptt</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Takes a while on CRAN
set.seed(1)
mu &lt;- -0.2
rho &lt;- 0.7
sigma_y &lt;- 0.1
sigma_x &lt;- 1
x &lt;- numeric(50)
x[1] &lt;- rnorm(1, mu, sigma_x / sqrt(1 - rho^2))
for(i in 2:length(x)) {
  x[i] &lt;- rnorm(1, mu * (1 - rho) + rho * x[i - 1], sigma_x)
}
y &lt;- rnorm(length(x), exp(x), sigma_y)

pntrs &lt;- cpp_example_model("nlg_ar_exp")

model_nlg &lt;- ssm_nlg(y = y, a1 = pntrs$a1, P1 = pntrs$P1, 
  Z = pntrs$Z_fn, H = pntrs$H_fn, T = pntrs$T_fn, R = pntrs$R_fn, 
  Z_gn = pntrs$Z_gn, T_gn = pntrs$T_gn,
  theta = c(mu= mu, rho = rho, 
    log_sigma_x = log(sigma_x), log_sigma_y = log(sigma_y)), 
  log_prior_pdf = pntrs$log_prior_pdf,
  n_states = 1, n_etas = 1, state_names = "state")

out_ekf &lt;- ekf_smoother(model_nlg, iekf_iter = 0)
out_iekf &lt;- ekf_smoother(model_nlg, iekf_iter = 1)
ts.plot(cbind(x, out_ekf$alphahat, out_iekf$alphahat), col = 1:3)

</code></pre>

<hr>
<h2 id='ekpf_filter'>Extended Kalman Particle Filtering</h2><span id='topic+ekpf_filter'></span><span id='topic+ekpf_filter.ssm_nlg'></span>

<h3>Description</h3>

<p>Function <code>ekpf_filter</code> performs a extended Kalman particle filtering
with stratification resampling, based on Van Der Merwe et al (2001).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ekpf_filter(model, particles, ...)

## S3 method for class 'ssm_nlg'
ekpf_filter(
  model,
  particles,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ekpf_filter_+3A_model">model</code></td>
<td>
<p>Model of class <code>ssm_nlg</code>.</p>
</td></tr>
<tr><td><code id="ekpf_filter_+3A_particles">particles</code></td>
<td>
<p>Number of particles as a positive integer. Suitable values
depend on the model and the data, and while larger values provide more
accurate estimates, the run time also increases with respect to the
number of particles, so it is generally a good idea to test the filter first
with a small number of particles, e.g., less than 100.</p>
</td></tr>
<tr><td><code id="ekpf_filter_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="ekpf_filter_+3A_seed">seed</code></td>
<td>
<p>Seed for the C++ RNG (positive integer).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing samples, filtered estimates and the
corresponding covariances, weights, and an estimate of log-likelihood.
</p>


<h3>References</h3>

<p>Van Der Merwe, R., Doucet, A., De Freitas, N., &amp; Wan, E. A.
(2001). The unscented particle filter. In Advances in neural
information processing systems (pp. 584-590).
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Takes a while
set.seed(1)
n &lt;- 50
x &lt;- y &lt;- numeric(n)
y[1] &lt;- rnorm(1, exp(x[1]), 0.1)
for(i in 1:(n-1)) {
 x[i+1] &lt;- rnorm(1, sin(x[i]), 0.1)
 y[i+1] &lt;- rnorm(1, exp(x[i+1]), 0.1)
}

pntrs &lt;- cpp_example_model("nlg_sin_exp")

model_nlg &lt;- ssm_nlg(y = y, a1 = pntrs$a1, P1 = pntrs$P1, 
  Z = pntrs$Z_fn, H = pntrs$H_fn, T = pntrs$T_fn, R = pntrs$R_fn, 
  Z_gn = pntrs$Z_gn, T_gn = pntrs$T_gn,
  theta = c(log_H = log(0.1), log_R = log(0.1)), 
  log_prior_pdf = pntrs$log_prior_pdf,
  n_states = 1, n_etas = 1, state_names = "state")

out &lt;- ekpf_filter(model_nlg, particles = 100)
ts.plot(cbind(x, out$at[1:n], out$att[1:n]), col = 1:3)

</code></pre>

<hr>
<h2 id='estimate_ess'>Effective Sample Size for IS-type Estimators</h2><span id='topic+estimate_ess'></span>

<h3>Description</h3>

<p>Computes the effective sample size (ESS) based on weighted posterior
samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_ess(x, w, method = "sokal")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_ess_+3A_x">x</code></td>
<td>
<p>A numeric vector of samples.</p>
</td></tr>
<tr><td><code id="estimate_ess_+3A_w">w</code></td>
<td>
<p>A numeric vector of weights. If missing, set to 1 (i.e. no
weighting is assumed).</p>
</td></tr>
<tr><td><code id="estimate_ess_+3A_method">method</code></td>
<td>
<p>Method for computing the ESS. Default is <code>"sokal"</code>, other
option are <code>"geyer"</code> (see also <code>asymptotic_var</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The asymptotic variance MCMCSE^2 is based on Corollary 1 of
Vihola et al. (2020) which is used to compute an estimate for the ESS
using the identity ESS(x) = var(x) / MCMCSE^2 where var(x) is the
posterior variance of x assuming independent samples.
</p>


<h3>Value</h3>

<p>A single numeric value of effective sample size estimate.
</p>


<h3>References</h3>

<p>Vihola, M, Helske, J, Franks, J. (2020). Importance sampling type estimators
based on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 1-38. https://doi.org/10.1111/sjos.12492
</p>
<p>Sokal A. (1997). Monte Carlo Methods in Statistical Mechanics: Foundations
and New Algorithms.
In: DeWitt-Morette C, Cartier P, Folacci A (eds) Functional Integration.
NATO ASI Series (Series B: Physics), vol 361. Springer, Boston, MA.
https://doi.org/10.1007/978-1-4899-0319-8_6
</p>
<p>Gelman, A, Carlin J B, Stern H S, Dunson, D B, Vehtari A, Rubin D B. (2013).
Bayesian Data Analysis, Third Edition. Chapman and Hall/CRC.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 1e4 
x &lt;- numeric(n)
phi &lt;- 0.7
for(t in 2:n) x[t] &lt;- phi * x[t-1] + rnorm(1)
w &lt;- rexp(n, 0.5 * exp(0.001 * x^2))
# different methods:
estimate_ess(x, w, method = "sokal")
estimate_ess(x, w, method = "geyer")

</code></pre>

<hr>
<h2 id='exchange'>Pound/Dollar daily exchange rates</h2><span id='topic+exchange'></span>

<h3>Description</h3>

<p>Dataset containing daily log-returns from 1/10/81-28/6/85 as in Durbin and
Koopman (2012).
</p>


<h3>Format</h3>

<p>A vector of length 945.
</p>


<h3>Source</h3>

<p>The data used to be available on the www.ssfpack.com/DKbook.html but
this page is does not seem to be available anymore.
</p>


<h3>References</h3>

<p>James Durbin, Siem Jan Koopman (2012).
Time Series Analysis by State Space Methods. Oxford University Press.
https://doi.org/10.1093/acprof:oso/9780199641178.001.0001
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Don't test on CRAN as complains about parallelisation
data("exchange")
model &lt;- svm(exchange, rho = uniform(0.97,-0.999,0.999),
 sd_ar = halfnormal(0.175, 2), mu = normal(-0.87, 0, 2))

out &lt;- particle_smoother(model, particles = 500)
plot.ts(cbind(model$y, exp(out$alphahat)))

</code></pre>

<hr>
<h2 id='expand_sample'>Expand the Jump Chain representation</h2><span id='topic+expand_sample'></span>

<h3>Description</h3>

<p>The MCMC algorithms of <code>bssm</code> use a jump chain representation where we
store the accepted values and the number of times we stayed in the current
value. Although this saves bit memory and is especially convenient for
IS-corrected  MCMC, sometimes we want to have the usual sample paths
(for example for drawing traceplots).
Function <code>expand_sample</code> returns the expanded sample based on the
counts (in form of <code>coda::mcmc</code> object. Note that for
the IS-MCMC the expanded sample corresponds to the approximate posterior,
i.e., the weights are ignored.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expand_sample(x, variable = "theta", times, states, by_states = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expand_sample_+3A_x">x</code></td>
<td>
<p>Output from <code><a href="#topic+run_mcmc">run_mcmc</a></code>.</p>
</td></tr>
<tr><td><code id="expand_sample_+3A_variable">variable</code></td>
<td>
<p>Expand parameters <code>"theta"</code> or states <code>"states"</code>.</p>
</td></tr>
<tr><td><code id="expand_sample_+3A_times">times</code></td>
<td>
<p>A vector of indices. In case of states,
what time points to expand? Default is all.</p>
</td></tr>
<tr><td><code id="expand_sample_+3A_states">states</code></td>
<td>
<p>A vector of indices. In case of states,
what states to expand? Default is all.</p>
</td></tr>
<tr><td><code id="expand_sample_+3A_by_states">by_states</code></td>
<td>
<p>If <code>TRUE</code> (default), return list by states.
Otherwise by time.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This functions is mostly for backwards compatibility, methods
<code>as.data.frame</code> and <code>as_draws</code> produce likely more convenient
output.
</p>


<h3>Value</h3>

<p>An object of class <code>"mcmc"</code> of the <code>coda</code> package.
</p>


<h3>See Also</h3>

<p><code>as.data.frame.mcmc_output</code> and <code>as_draws.mcmc_output</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 50
x &lt;- cumsum(rnorm(n))
y &lt;- rnorm(n, x)
model &lt;- bsm_lg(y, sd_y = gamma_prior(1, 2, 2), 
  sd_level = gamma_prior(1, 2, 2))
fit &lt;- run_mcmc(model, iter = 1e4)
# Traceplots for theta
plot.ts(expand_sample(fit, variable = "theta"))
# Traceplot for x_5
plot.ts(expand_sample(fit, variable = "states", times = 5, 
  states = 1)$level)
</code></pre>

<hr>
<h2 id='fast_smoother'>Kalman Smoothing</h2><span id='topic+fast_smoother'></span><span id='topic+fast_smoother.lineargaussian'></span><span id='topic+smoother'></span><span id='topic+smoother.lineargaussian'></span>

<h3>Description</h3>

<p>Methods for Kalman smoothing of the states. Function <code>fast_smoother</code>
computes only smoothed estimates of the states, and function
<code>smoother</code> computes also smoothed variances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_smoother(model, ...)

## S3 method for class 'lineargaussian'
fast_smoother(model, ...)

smoother(model, ...)

## S3 method for class 'lineargaussian'
smoother(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fast_smoother_+3A_model">model</code></td>
<td>
<p>Model to be approximated. Should be of class
<code>bsm_ng</code>, <code>ar1_ng</code> <code>svm</code>,
<code>ssm_ung</code>, or <code>ssm_mng</code>, or <code>ssm_nlg</code>, i.e. non-gaussian or
non-linear <code>bssm_model</code>.</p>
</td></tr>
<tr><td><code id="fast_smoother_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For non-Gaussian models, the smoothing is based on the approximate Gaussian
model.
</p>


<h3>Value</h3>

<p>Matrix containing the smoothed estimates of states, or a list
with the smoothed states and the variances.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- bsm_lg(Nile, 
  sd_level = tnormal(120, 100, 20, min = 0),
  sd_y = tnormal(50, 50, 25, min = 0),
  a1 = 1000, P1 = 200)
ts.plot(cbind(Nile, fast_smoother(model)), col = 1:2)
model &lt;- bsm_lg(Nile, 
  sd_y = tnormal(120, 100, 20, min = 0),
  sd_level = tnormal(50, 50, 25, min = 0),
  a1 = 1000, P1 = 500^2)

out &lt;- smoother(model)
ts.plot(cbind(Nile, out$alphahat), col = 1:2)
ts.plot(sqrt(out$Vt[1, 1, ]))
</code></pre>

<hr>
<h2 id='fitted.mcmc_output'>Fitted for State Space Model</h2><span id='topic+fitted.mcmc_output'></span>

<h3>Description</h3>

<p>Returns summary statistics from the posterior predictive
distribution of the mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcmc_output'
fitted(object, model, probs = c(0.025, 0.975), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitted.mcmc_output_+3A_object">object</code></td>
<td>
<p>Results object of class <code>mcmc_output</code> from
<code><a href="#topic+run_mcmc">run_mcmc</a></code> based on the input model.</p>
</td></tr>
<tr><td><code id="fitted.mcmc_output_+3A_model">model</code></td>
<td>
<p>A <code>bssm_model</code> object.</p>
</td></tr>
<tr><td><code id="fitted.mcmc_output_+3A_probs">probs</code></td>
<td>
<p>Numeric vector defining the quantiles of interest. Default is
<code>c(0.025, 0.975)</code>.</p>
</td></tr>
<tr><td><code id="fitted.mcmc_output_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>prior &lt;- uniform(0.1 * sd(log10(UKgas)), 0, 1)
model &lt;- bsm_lg(log10(UKgas), sd_y = prior, sd_level =  prior,
  sd_slope =  prior, sd_seasonal =  prior, period = 4)
fit &lt;- run_mcmc(model, iter = 1e4)
res &lt;- fitted(fit, model) 
head(res)

</code></pre>

<hr>
<h2 id='gaussian_approx'>Gaussian Approximation of Non-Gaussian/Non-linear State Space Model</h2><span id='topic+gaussian_approx'></span><span id='topic+gaussian_approx.nongaussian'></span><span id='topic+gaussian_approx.ssm_nlg'></span>

<h3>Description</h3>

<p>Returns the approximating Gaussian model which has the same conditional
mode of p(alpha|y, theta) as the original model.
This function is rarely needed itself, and is mainly available for
testing and debugging purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_approx(model, max_iter, conv_tol, ...)

## S3 method for class 'nongaussian'
gaussian_approx(model, max_iter = 100, conv_tol = 1e-08, ...)

## S3 method for class 'ssm_nlg'
gaussian_approx(model, max_iter = 100, conv_tol = 1e-08, iekf_iter = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gaussian_approx_+3A_model">model</code></td>
<td>
<p>Model to be approximated. Should be of class
<code>bsm_ng</code>, <code>ar1_ng</code> <code>svm</code>,
<code>ssm_ung</code>, or <code>ssm_mng</code>, or <code>ssm_nlg</code>, i.e. non-gaussian or
non-linear <code>bssm_model</code>.</p>
</td></tr>
<tr><td><code id="gaussian_approx_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations as a positive integer.
Default is 100 (although typically only few iterations are needed).</p>
</td></tr>
<tr><td><code id="gaussian_approx_+3A_conv_tol">conv_tol</code></td>
<td>
<p>Positive tolerance parameter. Default is 1e-8. Approximation
is claimed to be converged when the mean squared difference of the modes of
is less than <code>conv_tol</code>.</p>
</td></tr>
<tr><td><code id="gaussian_approx_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="gaussian_approx_+3A_iekf_iter">iekf_iter</code></td>
<td>
<p>For non-linear models, non-negative number of iterations in
iterated EKF (defaults to 0, i.e. normal EKF). Used only for models of class
<code>ssm_nlg</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns linear-Gaussian SSM of class <code>ssm_ulg</code> or
<code>ssm_mlg</code> which has the same conditional mode of p(alpha|y, theta) as
the original model.
</p>


<h3>References</h3>

<p>Koopman, SJ and Durbin J (2012). Time Series Analysis by State Space
Methods. Second edition. Oxford: Oxford University Press.
</p>
<p>Vihola, M, Helske, J, Franks, J. (2020). Importance sampling type estimators
based on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 1-38. https://doi.org/10.1111/sjos.12492
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("poisson_series")
model &lt;- bsm_ng(y = poisson_series, sd_slope = 0.01, sd_level = 0.1,
  distribution = "poisson")
out &lt;- gaussian_approx(model)
for(i in 1:7)
 cat("Number of iterations used: ", i, ", y[1] = ",
   gaussian_approx(model, max_iter = i, conv_tol = 0)$y[1], "\n", sep ="")
   
</code></pre>

<hr>
<h2 id='iact'>Integrated Autocorrelation Time</h2><span id='topic+iact'></span>

<h3>Description</h3>

<p>Estimates the integrated autocorrelation time (IACT) based on Sokal (1997).
Note that the estimator is not particularly good for very short series x
(say &lt; 100), but that is not very practical for MCMC applications anyway.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iact(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iact_+3A_x">x</code></td>
<td>
<p>A numeric vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single numeric value of IACT estimate.
</p>


<h3>References</h3>

<p>Sokal A. (1997) Monte Carlo Methods in Statistical Mechanics: Foundations
and New Algorithms.
In: DeWitt-Morette C., Cartier P., Folacci A. (eds) Functional Integration.
NATO ASI Series (Series B: Physics), vol 361. Springer, Boston, MA.
https://doi.org/10.1007/978-1-4899-0319-8_6
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 1000
x &lt;- numeric(n)
phi &lt;- 0.8
for(t in 2:n) x[t] &lt;- phi * x[t-1] + rnorm(1)
iact(x)
</code></pre>

<hr>
<h2 id='importance_sample'>Importance Sampling from non-Gaussian State Space Model</h2><span id='topic+importance_sample'></span><span id='topic+importance_sample.nongaussian'></span>

<h3>Description</h3>

<p>Returns <code>nsim</code> samples from the approximating Gaussian model with
corresponding (scaled) importance weights.
Probably mostly useful for comparing KFAS and bssm packages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>importance_sample(model, nsim, use_antithetic, max_iter, conv_tol, seed, ...)

## S3 method for class 'nongaussian'
importance_sample(
  model,
  nsim,
  use_antithetic = TRUE,
  max_iter = 100,
  conv_tol = 1e-08,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="importance_sample_+3A_model">model</code></td>
<td>
<p>Model of class <code>bsm_ng</code>, <code>ar1_ng</code> <code>svm</code>,
<code>ssm_ung</code>, or <code>ssm_mng</code>.</p>
</td></tr>
<tr><td><code id="importance_sample_+3A_nsim">nsim</code></td>
<td>
<p>Number of samples (positive integer). Suitable values
depend on the model and the data, and while larger values provide more
accurate estimates, the run time also increases with respect to to the
number of samples, so it is generally a good idea to test the filter first
with a small number of samples, e.g., less than 100.</p>
</td></tr>
<tr><td><code id="importance_sample_+3A_use_antithetic">use_antithetic</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), use antithetic
variable for location in simulation smoothing. Ignored for <code>ssm_mng</code>
models.</p>
</td></tr>
<tr><td><code id="importance_sample_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations as a positive integer.
Default is 100 (although typically only few iterations are needed).</p>
</td></tr>
<tr><td><code id="importance_sample_+3A_conv_tol">conv_tol</code></td>
<td>
<p>Positive tolerance parameter. Default is 1e-8. Approximation
is claimed to be converged when the mean squared difference of the modes of
is less than <code>conv_tol</code>.</p>
</td></tr>
<tr><td><code id="importance_sample_+3A_seed">seed</code></td>
<td>
<p>Seed for the C++ RNG (positive integer).</p>
</td></tr>
<tr><td><code id="importance_sample_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data("sexratio", package = "KFAS")
model &lt;- bsm_ng(sexratio[, "Male"], sd_level = 0.001, 
  u = sexratio[, "Total"],
  distribution = "binomial")

imp &lt;- importance_sample(model, nsim = 1000)

est &lt;- matrix(NA, 3, nrow(sexratio))
for(i in 1:ncol(est)) {
  est[, i] &lt;- diagis::weighted_quantile(exp(imp$alpha[i, 1, ]), imp$weights, 
    prob = c(0.05,0.5,0.95))
}

ts.plot(t(est),lty = c(2,1,2))

</code></pre>

<hr>
<h2 id='kfilter'>Kalman Filtering</h2><span id='topic+kfilter'></span><span id='topic+kfilter.lineargaussian'></span><span id='topic+kfilter.nongaussian'></span>

<h3>Description</h3>

<p>Function <code>kfilter</code> runs the Kalman filter for the given model,
and returns the filtered estimates and one-step-ahead predictions of the
states <code class="reqn">\alpha_t</code> given the data up to time <code class="reqn">t</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kfilter(model, ...)

## S3 method for class 'lineargaussian'
kfilter(model, ...)

## S3 method for class 'nongaussian'
kfilter(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kfilter_+3A_model">model</code></td>
<td>
<p>Model of class <code>lineargaussian</code>, <code>nongaussian</code> or
<code>ssm_nlg</code>.</p>
</td></tr>
<tr><td><code id="kfilter_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For non-Gaussian models, the filtering is based on the approximate
Gaussian model.
</p>


<h3>Value</h3>

<p>List containing the log-likelihood
(approximate in non-Gaussian case), one-step-ahead predictions <code>at</code>
and filtered estimates <code>att</code> of states, and the corresponding
variances <code>Pt</code> and <code>Ptt</code> up to the time point n+1 where n is the
length of the input time series.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bootstrap_filter">bootstrap_filter</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- cumsum(rnorm(20))
y &lt;- x + rnorm(20, sd = 0.1)
model &lt;- bsm_lg(y, sd_level = 1, sd_y = 0.1)
ts.plot(cbind(y, x, kfilter(model)$att), col = 1:3)
</code></pre>

<hr>
<h2 id='logLik.lineargaussian'>Extract Log-likelihood of a State Space Model of class <code>bssm_model</code></h2><span id='topic+logLik.lineargaussian'></span><span id='topic+logLik.nongaussian'></span><span id='topic+logLik.ssm_nlg'></span><span id='topic+logLik.ssm_sde'></span>

<h3>Description</h3>

<p>Computes the log-likelihood of a state space model defined by <code>bssm</code>
package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lineargaussian'
logLik(object, ...)

## S3 method for class 'nongaussian'
logLik(
  object,
  particles,
  method = "psi",
  max_iter = 100,
  conv_tol = 1e-08,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)

## S3 method for class 'ssm_nlg'
logLik(
  object,
  particles,
  method = "bsf",
  max_iter = 100,
  conv_tol = 1e-08,
  iekf_iter = 0,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)

## S3 method for class 'ssm_sde'
logLik(
  object,
  particles,
  L,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logLik.lineargaussian_+3A_object">object</code></td>
<td>
<p>Model of class <code>bssm_model</code>.</p>
</td></tr>
<tr><td><code id="logLik.lineargaussian_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="logLik.lineargaussian_+3A_particles">particles</code></td>
<td>
<p>Number of samples for particle filter
(non-negative integer). If 0, approximate log-likelihood is returned either
based on the Gaussian approximation or EKF, depending on the <code>method</code>
argument.</p>
</td></tr>
<tr><td><code id="logLik.lineargaussian_+3A_method">method</code></td>
<td>
<p>Sampling method. For Gaussian and non-Gaussian models with
linear dynamics,options are <code>"bsf"</code> (bootstrap particle filter, default
for non-linear models) and <code>"psi"</code> (<code class="reqn">\psi</code>-APF, the default for
other models). For-nonlinear models option <code>"ekf"</code>
uses EKF/IEKF-based particle filter (or just EKF/IEKF approximation in the
case of <code>particles = 0</code>).</p>
</td></tr>
<tr><td><code id="logLik.lineargaussian_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations used in Gaussian approximation,
as a positive integer.
Default is 100 (although typically only few iterations are needed).</p>
</td></tr>
<tr><td><code id="logLik.lineargaussian_+3A_conv_tol">conv_tol</code></td>
<td>
<p>Positive tolerance parameter used in Gaussian approximation.
Default is 1e-8.</p>
</td></tr>
<tr><td><code id="logLik.lineargaussian_+3A_seed">seed</code></td>
<td>
<p>Seed for the C++ RNG (positive integer).</p>
</td></tr>
<tr><td><code id="logLik.lineargaussian_+3A_iekf_iter">iekf_iter</code></td>
<td>
<p>Non-negative integer. If zero (default), first
approximation for non-linear Gaussian models is obtained from extended
Kalman filter. If <code>iekf_iter &gt; 0</code>, iterated extended Kalman filter is
used with <code>iekf_iter</code> iterations.</p>
</td></tr>
<tr><td><code id="logLik.lineargaussian_+3A_l">L</code></td>
<td>
<p>Integer  defining the discretization level defined as (2^L).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric value.
</p>


<h3>References</h3>

<p>Durbin, J., &amp; Koopman, S. (2002). A Simple and Efficient Simulation
Smoother for State Space Time Series Analysis. Biometrika, 89(3), 603-615.
</p>
<p>Shephard, N., &amp; Pitt, M. (1997). Likelihood Analysis of
Non-Gaussian Measurement Time Series. Biometrika, 84(3), 653-667.
</p>
<p>Gordon, NJ, Salmond, DJ, Smith, AFM (1993).
Novel approach to nonlinear/non-Gaussian Bayesian state estimation.
IEE Proceedings-F, 140, 107-113.
</p>
<p>Vihola, M, Helske, J, Franks, J. Importance sampling type estimators
based on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 2020; 1-38. https://doi.org/10.1111/sjos.12492
</p>
<p>Van Der Merwe, R, Doucet, A, De Freitas, N,  Wan, EA (2001).
The unscented particle filter.
In Advances in neural information processing systems, p 584-590.
</p>
<p>Jazwinski, A 1970. Stochastic Processes and Filtering Theory.
Academic Press.
</p>
<p>Kitagawa, G (1996). Monte Carlo filter and smoother for non-Gaussian
nonlinear state space models.
Journal of Computational and Graphical Statistics, 5, 1-25.
</p>


<h3>See Also</h3>

<p>particle_smoother
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
model &lt;- ssm_ulg(y = c(1,4,3), Z = 1, H = 1, T = 1, R = 1)
logLik(model)
model &lt;- ssm_ung(y = c(1,4,3), Z = 1, T = 1, R = 0.5, P1 = 2,
  distribution = "poisson")
  
model2 &lt;- bsm_ng(y = c(1,4,3), sd_level = 0.5, P1 = 2,
  distribution = "poisson")
  
logLik(model, particles = 0)
logLik(model2, particles = 0)
logLik(model, particles = 10, seed = 1)
logLik(model2, particles = 10, seed = 1)
</code></pre>

<hr>
<h2 id='negbin_model'>Estimated Negative Binomial Model of Helske and Vihola (2021)</h2><span id='topic+negbin_model'></span>

<h3>Description</h3>

<p>This model was used in Helske and Vihola (2021), but with larger number of
iterations. Here only 2000 iterations were used in order to reduce the size
of the model object in CRAN.
</p>


<h3>Format</h3>

<p>A object of class <code>mcmc_output</code>.
</p>


<h3>References</h3>

<p>Helske J, Vihola M (2021). bssm: Bayesian Inference of Non-linear and
Non-Gaussian State Space Models in R. The R Journal (2021) 13:2, 578-589.
https://doi.org/10.32614/RJ-2021-103
</p>


<h3>Examples</h3>

<pre><code class='language-R'># reproducing the model:
data("negbin_series")
# Construct model for bssm
bssm_model &lt;- bsm_ng(negbin_series[, "y"],
  xreg = negbin_series[, "x"],
  beta = normal(0, 0, 10),
  phi = halfnormal(1, 10),
  sd_level = halfnormal(0.1, 1),
  sd_slope = halfnormal(0.01, 0.1),
  a1 = c(0, 0), P1 = diag(c(10, 0.1)^2),
  distribution = "negative binomial")


# In the paper we used 60000 iterations with first 10000 as burnin
fit_bssm &lt;- run_mcmc(bssm_model, iter = 2000, particles = 10, seed = 1)
fit_bssm

</code></pre>

<hr>
<h2 id='negbin_series'>Simulated Negative Binomial Time Series Data</h2><span id='topic+negbin_series'></span>

<h3>Description</h3>

<p>See example for code for reproducing the data. This was used in
Helske and Vihola (2021).
</p>


<h3>Format</h3>

<p>A time series <code>mts</code> object with 200 time points and two series.
</p>


<h3>References</h3>

<p>Helske J, Vihola M (2021). bssm: Bayesian Inference of Non-linear and
Non-Gaussian State Space Models in R. The R Journal (2021) 13:2, 578-589.
https://doi.org/10.32614/RJ-2021-103
</p>


<h3>See Also</h3>

<p><code>negbin_model</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The data was generated as follows:
set.seed(123)
n &lt;- 200
sd_level &lt;- 0.1
drift &lt;- 0.01
beta &lt;- -0.9
phi &lt;- 5

level &lt;- cumsum(c(5, drift + rnorm(n - 1, sd = sd_level)))
x &lt;- 3 + (1:n) * drift + sin(1:n + runif(n, -1, 1))
y &lt;- rnbinom(n, size = phi, mu = exp(beta * x + level))

</code></pre>

<hr>
<h2 id='particle_smoother'>Particle Smoothing</h2><span id='topic+particle_smoother'></span><span id='topic+particle_smoother.lineargaussian'></span><span id='topic+particle_smoother.nongaussian'></span><span id='topic+particle_smoother.ssm_nlg'></span><span id='topic+particle_smoother.ssm_sde'></span>

<h3>Description</h3>

<p>Function <code>particle_smoother</code> performs particle smoothing
based on either bootstrap particle filter (Gordon et al. 1993),
<code class="reqn">\psi</code>-auxiliary particle filter (<code class="reqn">\psi</code>-APF) (Vihola et al. 2020),
extended Kalman particle filter (Van Der Merwe et al. 2001),
or its version based on iterated EKF (Jazwinski, 1970).
The smoothing phase is based on the filter-smoother algorithm by
Kitagawa (1996).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>particle_smoother(model, particles, ...)

## S3 method for class 'lineargaussian'
particle_smoother(
  model,
  particles,
  method = "psi",
  seed = sample(.Machine$integer.max, size = 1),
  ...
)

## S3 method for class 'nongaussian'
particle_smoother(
  model,
  particles,
  method = "psi",
  seed = sample(.Machine$integer.max, size = 1),
  max_iter = 100,
  conv_tol = 1e-08,
  ...
)

## S3 method for class 'ssm_nlg'
particle_smoother(
  model,
  particles,
  method = "bsf",
  seed = sample(.Machine$integer.max, size = 1),
  max_iter = 100,
  conv_tol = 1e-08,
  iekf_iter = 0,
  ...
)

## S3 method for class 'ssm_sde'
particle_smoother(
  model,
  particles,
  L,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="particle_smoother_+3A_model">model</code></td>
<td>
<p>A model object of class <code>bssm_model</code>.</p>
</td></tr>
<tr><td><code id="particle_smoother_+3A_particles">particles</code></td>
<td>
<p>Number of particles as a positive integer. Suitable values
depend on the model, the data, and the chosen algorithm. While larger values
provide more accurate estimates, the run time also increases with respect to
the number of particles, so it is generally a good idea to test the filter
first with a small number of particles, e.g., less than 100.</p>
</td></tr>
<tr><td><code id="particle_smoother_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="particle_smoother_+3A_method">method</code></td>
<td>
<p>Choice of particle filter algorithm.
For Gaussian and non-Gaussian models with linear dynamics,
options are <code>"bsf"</code> (bootstrap particle filter, default for
non-linear models)
and <code>"psi"</code> (<code class="reqn">\psi</code>-APF, the default for other models), and
for non-linear models option <code>"ekf"</code> (extended Kalman particle filter)
is also available.</p>
</td></tr>
<tr><td><code id="particle_smoother_+3A_seed">seed</code></td>
<td>
<p>Seed for the C++ RNG (positive integer).</p>
</td></tr>
<tr><td><code id="particle_smoother_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations used in Gaussian approximation,
as a positive integer.
Default is 100 (although typically only few iterations are needed).</p>
</td></tr>
<tr><td><code id="particle_smoother_+3A_conv_tol">conv_tol</code></td>
<td>
<p>Positive tolerance parameter used in Gaussian approximation.
Default is 1e-8.</p>
</td></tr>
<tr><td><code id="particle_smoother_+3A_iekf_iter">iekf_iter</code></td>
<td>
<p>Non-negative integer. If zero (default), first
approximation for non-linear Gaussian models is obtained from extended
Kalman filter. If <code>iekf_iter &gt; 0</code>, iterated extended Kalman filter is
used with <code>iekf_iter</code> iterations.</p>
</td></tr>
<tr><td><code id="particle_smoother_+3A_l">L</code></td>
<td>
<p>Positive integer defining the discretization level for SDE model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See one of the vignettes for <code class="reqn">\psi</code>-APF in case of nonlinear models.
</p>


<h3>Value</h3>

<p>List with samples (<code>alpha</code>) from the smoothing distribution
and corresponding weights (<code>weights</code>),
as well as smoothed means and covariances (<code>alphahat</code> and <code>Vt</code>)
of the states and
estimated log-likelihood (<code>logLik</code>).
</p>


<h3>References</h3>

<p>Gordon, NJ, Salmond, DJ, Smith, AFM (1993).
Novel approach to nonlinear/non-Gaussian Bayesian state estimation.
IEE Proceedings-F, 140, 107-113.
https://doi.org/10.1049/ip-f-2.1993.0015
</p>
<p>Vihola, M, Helske, J, Franks, J. Importance sampling type estimators
based on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 2020; 1-38.
https://doi.org/10.1111/sjos.12492
</p>
<p>Van Der Merwe, R, Doucet, A, De Freitas, N,  Wan, EA (2001).
The unscented particle filter.
In Advances in neural information processing systems, p 584-590.
</p>
<p>Jazwinski, A 1970. Stochastic Processes and Filtering Theory.
Academic Press.
</p>
<p>Kitagawa, G (1996). Monte Carlo filter and smoother for non-Gaussian
nonlinear state space models.
Journal of Computational and Graphical Statistics, 5, 1-25.
https://doi.org/10.2307/1390750
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- cumsum(rnorm(100))
y &lt;- rnorm(100, x)
model &lt;- ssm_ulg(y, Z = 1, T = 1, R = 1, H = 1, P1 = 1)
system.time(out &lt;- particle_smoother(model, particles = 1000))
# same with simulation smoother:
system.time(out2 &lt;- sim_smoother(model, particles = 1000, 
  use_antithetic = TRUE))
ts.plot(out$alphahat, rowMeans(out2), col = 1:2)

</code></pre>

<hr>
<h2 id='plot.mcmc_output'>Trace and Density Plots for <code>mcmc_output</code></h2><span id='topic+plot.mcmc_output'></span>

<h3>Description</h3>

<p>Plots the trace and density plots of the hyperparameters theta from the MCMC
run by <code><a href="#topic+run_mcmc">run_mcmc</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcmc_output'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.mcmc_output_+3A_x">x</code></td>
<td>
<p>Object of class <code>mcmc_output</code> from <code><a href="#topic+run_mcmc">run_mcmc</a></code>.</p>
</td></tr>
<tr><td><code id="plot.mcmc_output_+3A_...">...</code></td>
<td>
<p>Further arguments to <a href="bayesplot.html#topic+MCMC-combos">bayesplot::mcmc_combo</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For further visualization (of the states), you can extract the posterior
samples with <code>as.data.frame</code> and <code>as_draws</code> methods to be used for example
with the <code>bayesplot</code> or <code>ggplot2</code> packages.
</p>


<h3>Value</h3>

<p>The output object from <a href="bayesplot.html#topic+MCMC-combos">bayesplot::mcmc_combo</a>.
</p>


<h3>Note</h3>

<p>For IS-MCMC, these plots correspond to the approximate (non-weighted)
samples
.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+check_diagnostics">check_diagnostics</a></code> for a quick diagnostics statistics
of the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("negbin_model")
# Note the very small number of iterations, so the plots look bad
plot(negbin_model)
</code></pre>

<hr>
<h2 id='poisson_series'>Simulated Poisson Time Series Data</h2><span id='topic+poisson_series'></span>

<h3>Description</h3>

<p>See example for code for reproducing the data. This was used in
Vihola, Helske, Franks (2020).
</p>


<h3>Format</h3>

<p>A vector of length 100.
</p>


<h3>References</h3>

<p>Vihola, M, Helske, J, Franks, J (2020). Importance sampling type
estimators based on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 1-38. https://doi.org/10.1111/sjos.12492
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The data was generated as follows:
set.seed(321)
slope &lt;- cumsum(c(0, rnorm(99, sd = 0.01)))
y &lt;- rpois(100, exp(cumsum(slope + c(0, rnorm(99, sd = 0.1)))))
</code></pre>

<hr>
<h2 id='post_correct'>Run Post-correction for Approximate MCMC using <code class="reqn">\psi</code>-APF</h2><span id='topic+post_correct'></span>

<h3>Description</h3>

<p>Function <code>post_correct</code> updates previously obtained approximate MCMC
output with post-correction weights leading to asymptotically exact
weighted posterior, and returns updated MCMC output where components
<code>weights</code>, <code>posterior</code>, <code>alpha</code>, <code>alphahat</code>, and
<code>Vt</code> are updated (depending on the original output type).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>post_correct(
  model,
  mcmc_output,
  particles,
  threads = 1L,
  is_type = "is2",
  seed = sample(.Machine$integer.max, size = 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="post_correct_+3A_model">model</code></td>
<td>
<p>Model of class <code>nongaussian</code> or <code>ssm_nlg</code>.</p>
</td></tr>
<tr><td><code id="post_correct_+3A_mcmc_output">mcmc_output</code></td>
<td>
<p>An output from <code>run_mcmc</code> used to compute the MAP
estimate of theta.
While the intended use assumes this is from approximate MCMC, it is not
actually checked, i.e., it is also possible to input previous
(asymptotically) exact output.</p>
</td></tr>
<tr><td><code id="post_correct_+3A_particles">particles</code></td>
<td>
<p>Number of particles for <code class="reqn">\psi</code>-APF (positive integer).
Suitable values depend on the model and the data, but often relatively
small value less than say 50 is enough. See also <code>suggest_N</code></p>
</td></tr>
<tr><td><code id="post_correct_+3A_threads">threads</code></td>
<td>
<p>Number of parallel threads (positive integer, default is 1).</p>
</td></tr>
<tr><td><code id="post_correct_+3A_is_type">is_type</code></td>
<td>
<p>Type of IS-correction. Possible choices are
<code>"is3"</code> for simple importance sampling (weight is computed for each
MCMC iteration independently),
<code>"is2"</code> for jump chain importance sampling type weighting (default), or
<code>"is1"</code> for importance sampling type weighting where the number of
particles used forweight computations is proportional to the length of the
jump chain block.</p>
</td></tr>
<tr><td><code id="post_correct_+3A_seed">seed</code></td>
<td>
<p>Seed for the C++ RNG (positive integer).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The original object of class <code>mcmc_output</code> with updated
weights, log-posterior values and state samples or summaries (depending on
the <code>mcmc_output$mcmc_type</code>).
</p>


<h3>References</h3>

<p>Doucet A, Pitt M K, Deligiannidis G, Kohn R (2018).
Efficient implementation of Markov chain Monte Carlo when using an unbiased
likelihood estimator. Biometrika, 102, 2, 295-313,
https://doi.org/10.1093/biomet/asu075
</p>
<p>Vihola M, Helske J, Franks J (2020). Importance sampling type estimators
based on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 1-38. https://doi.org/10.1111/sjos.12492
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 300
x1 &lt;- sin((2 * pi / 12) * 1:n)
x2 &lt;- cos((2 * pi / 12) * 1:n)
alpha &lt;- numeric(n)
alpha[1] &lt;- 0
rho &lt;- 0.7
sigma &lt;- 2
mu &lt;- 1
for(i in 2:n) {
  alpha[i] &lt;- rnorm(1, mu * (1 - rho) + rho * alpha[i-1], sigma)
}
u &lt;- rpois(n, 50)
y &lt;- rbinom(n, size = u, plogis(0.5 * x1 + x2 + alpha))

ts.plot(y / u)

model &lt;- ar1_ng(y, distribution = "binomial", 
  rho = uniform(0.5, -1, 1), sigma = gamma_prior(1, 2, 0.001),
  mu = normal(0, 0, 10),
  xreg = cbind(x1,x2), beta = normal(c(0, 0), 0, 5),
  u = u)

out_approx &lt;- run_mcmc(model, mcmc_type = "approx", 
  local_approx = FALSE, iter = 50000)

out_is2 &lt;- post_correct(model, out_approx, particles = 30,
  threads = 2)
out_is2$time

summary(out_approx, return_se = TRUE)
summary(out_is2, return_se = TRUE)

# latent state
library("dplyr")
library("ggplot2")
state_approx &lt;- as.data.frame(out_approx, variable = "states") |&gt; 
  group_by(time) |&gt;
  summarise(mean = mean(value))
  
state_exact &lt;- as.data.frame(out_is2, variable = "states") |&gt; 
  group_by(time) |&gt;
  summarise(mean = weighted.mean(value, weight))

dplyr::bind_rows(approx = state_approx, 
  exact = state_exact, .id = "method") |&gt;
  filter(time &gt; 200) |&gt;
ggplot(aes(time, mean, colour = method)) + 
  geom_line() + 
  theme_bw()

# posterior means
p_approx &lt;- predict(out_approx, model, type = "mean", 
  nsim = 1000, future = FALSE) |&gt; 
  group_by(time) |&gt;
  summarise(mean = mean(value))
p_exact &lt;- predict(out_is2, model, type = "mean", 
  nsim = 1000, future = FALSE) |&gt; 
  group_by(time) |&gt;
  summarise(mean = mean(value))

dplyr::bind_rows(approx = p_approx, 
  exact = p_exact, .id = "method") |&gt;
  filter(time &gt; 200) |&gt;
ggplot(aes(time, mean, colour = method)) + 
  geom_line() + 
  theme_bw() 

</code></pre>

<hr>
<h2 id='predict.mcmc_output'>Predictions for State Space Models</h2><span id='topic+predict.mcmc_output'></span><span id='topic+predict'></span>

<h3>Description</h3>

<p>Draw samples from the posterior predictive distribution for future
time points given the posterior draws of hyperparameters <code class="reqn">\theta</code> and
latent state <code class="reqn">alpha_{n+1}</code> returned by <code>run_mcmc</code>.
Function can also be used to draw samples from the posterior predictive
distribution <code class="reqn">p(\tilde y_1, \ldots, \tilde y_n | y_1,\ldots, y_n)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcmc_output'
predict(
  object,
  model,
  nsim,
  type = "response",
  future = TRUE,
  seed = sample(.Machine$integer.max, size = 1),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.mcmc_output_+3A_object">object</code></td>
<td>
<p>Results object of class <code>mcmc_output</code> from
<code><a href="#topic+run_mcmc">run_mcmc</a></code>.</p>
</td></tr>
<tr><td><code id="predict.mcmc_output_+3A_model">model</code></td>
<td>
<p>A <code>bssm_model</code> object.
Should have same structure and class as the original model which was used in
<code>run_mcmc</code>, in order to plug the posterior samples of the model
parameters to the right places.
It is also possible to input the original model for obtaining predictions
for past time points. In this case, set argument
<code>future</code> to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="predict.mcmc_output_+3A_nsim">nsim</code></td>
<td>
<p>Positive integer defining number of samples to draw. Should be
less than or equal to <code>sum(object$counts)</code> i.e. the number of samples
in the MCMC output. Default is to use all the samples.</p>
</td></tr>
<tr><td><code id="predict.mcmc_output_+3A_type">type</code></td>
<td>
<p>Type of predictions. Possible choices are
<code>"mean"</code> <code>"response"</code>, or  <code>"state"</code> level.</p>
</td></tr>
<tr><td><code id="predict.mcmc_output_+3A_future">future</code></td>
<td>
<p>Default is <code>TRUE</code>, in which case predictions are for the
future, using posterior samples of (theta, alpha_T+1) i.e. the
posterior samples of hyperparameters and latest states.
Otherwise it is assumed that <code>model</code> corresponds to the original model.</p>
</td></tr>
<tr><td><code id="predict.mcmc_output_+3A_seed">seed</code></td>
<td>
<p>Seed for the C++ RNG (positive integer). Note that this affects
only the C++ side, and <code>predict</code> also uses R side RNG for subsampling,
so for replicable results you should call <code>set.seed</code> before
<code>predict</code>.</p>
</td></tr>
<tr><td><code id="predict.mcmc_output_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame consisting of samples from the predictive
posterior distribution.
</p>


<h3>See Also</h3>

<p><code>fitted</code> for in-sample predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("graphics")
y &lt;- log10(JohnsonJohnson)
prior &lt;- uniform(0.01, 0, 1)
model &lt;- bsm_lg(window(y, end = c(1974, 4)), sd_y = prior,
  sd_level = prior, sd_slope = prior, sd_seasonal = prior)

mcmc_results &lt;- run_mcmc(model, iter = 5000)
future_model &lt;- model
future_model$y &lt;- ts(rep(NA, 25), 
  start = tsp(model$y)[2] + 2 * deltat(model$y), 
  frequency = frequency(model$y))
# use "state" for illustrative purposes, we could use type = "mean" directly
pred &lt;- predict(mcmc_results, model = future_model, type = "state", 
  nsim = 1000)

library("dplyr")
sumr_fit &lt;- as.data.frame(mcmc_results, variable = "states") |&gt;
  group_by(time, iter) |&gt; 
  mutate(signal = 
      value[variable == "level"] + 
      value[variable == "seasonal_1"]) |&gt;
  group_by(time) |&gt;
  summarise(mean = mean(signal), 
    lwr = quantile(signal, 0.025), 
    upr = quantile(signal, 0.975))

sumr_pred &lt;- pred |&gt; 
  group_by(time, sample) |&gt;
  mutate(signal = 
      value[variable == "level"] + 
      value[variable == "seasonal_1"]) |&gt;
  group_by(time) |&gt;
  summarise(mean = mean(signal),
    lwr = quantile(signal, 0.025), 
    upr = quantile(signal, 0.975)) 
    
# If we used type = "mean", we could do
# sumr_pred &lt;- pred |&gt; 
#   group_by(time) |&gt;
#   summarise(mean = mean(value),
#     lwr = quantile(value, 0.025), 
#     upr = quantile(value, 0.975)) 
    
library("ggplot2")
rbind(sumr_fit, sumr_pred) |&gt; 
  ggplot(aes(x = time, y = mean)) + 
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
   fill = "#92f0a8", alpha = 0.25) +
  geom_line(colour = "#92f0a8") +
  theme_bw() + 
  geom_point(data = data.frame(
    mean = log10(JohnsonJohnson), 
    time = time(JohnsonJohnson)))

# Posterior predictions for past observations:
yrep &lt;- predict(mcmc_results, model = model, type = "response", 
  future = FALSE, nsim = 1000)
meanrep &lt;- predict(mcmc_results, model = model, type = "mean", 
  future = FALSE, nsim = 1000)
  
sumr_yrep &lt;- yrep |&gt; 
  group_by(time) |&gt;
  summarise(earnings = mean(value),
    lwr = quantile(value, 0.025), 
    upr = quantile(value, 0.975)) |&gt;
  mutate(interval = "Observations")

sumr_meanrep &lt;- meanrep |&gt; 
  group_by(time) |&gt;
  summarise(earnings = mean(value),
    lwr = quantile(value, 0.025), 
    upr = quantile(value, 0.975)) |&gt;
  mutate(interval = "Mean")
    
rbind(sumr_meanrep, sumr_yrep) |&gt; 
  mutate(interval = 
    factor(interval, levels = c("Observations", "Mean"))) |&gt;
  ggplot(aes(x = time, y = earnings)) + 
  geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), 
   alpha = 0.75) +
  theme_bw() + 
  geom_point(data = data.frame(
    earnings = model$y, 
    time = time(model$y)))    


</code></pre>

<hr>
<h2 id='print.mcmc_output'>Print Results from MCMC Run</h2><span id='topic+print.mcmc_output'></span>

<h3>Description</h3>

<p>Prints some basic summaries from the MCMC run by <code><a href="#topic+run_mcmc">run_mcmc</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcmc_output'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.mcmc_output_+3A_x">x</code></td>
<td>
<p>Object of class <code>mcmc_output</code> from <code><a href="#topic+run_mcmc">run_mcmc</a></code>.</p>
</td></tr>
<tr><td><code id="print.mcmc_output_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data("negbin_model")
print(negbin_model)
</code></pre>

<hr>
<h2 id='run_mcmc'>Bayesian Inference of State Space Models</h2><span id='topic+run_mcmc'></span><span id='topic+run_mcmc.lineargaussian'></span><span id='topic+run_mcmc.nongaussian'></span><span id='topic+run_mcmc.ssm_nlg'></span><span id='topic+run_mcmc.ssm_sde'></span>

<h3>Description</h3>

<p>Adaptive Markov chain Monte Carlo simulation for SSMs using
Robust Adaptive Metropolis algorithm by Vihola (2012). Several different
MCMC sampling schemes are implemented, see parameter
arguments, package vignette, Vihola, Helske, Franks (2020) and Helske and
Vihola (2021) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_mcmc(model, ...)

## S3 method for class 'lineargaussian'
run_mcmc(
  model,
  iter,
  output_type = "full",
  burnin = floor(iter/2),
  thin = 1,
  gamma = 2/3,
  target_acceptance = 0.234,
  S,
  end_adaptive_phase = FALSE,
  threads = 1,
  seed = sample(.Machine$integer.max, size = 1),
  verbose,
  ...
)

## S3 method for class 'nongaussian'
run_mcmc(
  model,
  iter,
  particles,
  output_type = "full",
  mcmc_type = "is2",
  sampling_method = "psi",
  burnin = floor(iter/2),
  thin = 1,
  gamma = 2/3,
  target_acceptance = 0.234,
  S,
  end_adaptive_phase = FALSE,
  local_approx = TRUE,
  threads = 1,
  seed = sample(.Machine$integer.max, size = 1),
  max_iter = 100,
  conv_tol = 1e-08,
  verbose,
  ...
)

## S3 method for class 'ssm_nlg'
run_mcmc(
  model,
  iter,
  particles,
  output_type = "full",
  mcmc_type = "is2",
  sampling_method = "bsf",
  burnin = floor(iter/2),
  thin = 1,
  gamma = 2/3,
  target_acceptance = 0.234,
  S,
  end_adaptive_phase = FALSE,
  threads = 1,
  seed = sample(.Machine$integer.max, size = 1),
  max_iter = 100,
  conv_tol = 1e-08,
  iekf_iter = 0,
  verbose,
  ...
)

## S3 method for class 'ssm_sde'
run_mcmc(
  model,
  iter,
  particles,
  output_type = "full",
  mcmc_type = "is2",
  L_c,
  L_f,
  burnin = floor(iter/2),
  thin = 1,
  gamma = 2/3,
  target_acceptance = 0.234,
  S,
  end_adaptive_phase = FALSE,
  threads = 1,
  seed = sample(.Machine$integer.max, size = 1),
  verbose,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_mcmc_+3A_model">model</code></td>
<td>
<p>Model of class <code>bssm_model</code>.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_iter">iter</code></td>
<td>
<p>A positive integer defining the total number of MCMC iterations.
Suitable value depends on the model, data, and the choice of specific
algorithms (<code>mcmc_type</code> and <code>sampling_method</code>). As increasing
<code>iter</code> also increases run time, it is is generally good idea to first
test the performance with a small values, e.g., less than 10000.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_output_type">output_type</code></td>
<td>
<p>Either <code>"full"</code>
(default, returns posterior samples from the posterior
<code class="reqn">p(\alpha, \theta | y)</code>), <code>"theta"</code> (for marginal posterior of
theta), or <code>"summary"</code> (return the mean and variance estimates of the
states and posterior samples of theta). See details.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_burnin">burnin</code></td>
<td>
<p>A positive integer defining the length of the burn-in period
which is disregarded from the results. Defaults to <code>iter / 2</code>.
Note that all MCMC algorithms of <code>bssm</code> use adaptive MCMC during the
burn-in period in order to find good proposal distribution.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_thin">thin</code></td>
<td>
<p>A positive integer defining the thinning rate. All the MCMC
algorithms in <code>bssm</code> use the jump chain representation (see refs),
and the thinning is applied to these blocks. Defaults to 1.
For IS-corrected methods, larger value can also be
statistically more effective. Note: With <code>output_type = "summary"</code>,
the thinning does not affect the computations of the summary statistics in
case of pseudo-marginal methods.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_gamma">gamma</code></td>
<td>
<p>Tuning parameter for the adaptation of RAM algorithm. Must be
between 0 and 1.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_target_acceptance">target_acceptance</code></td>
<td>
<p>Target acceptance rate for MCMC. Defaults to 0.234.
Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_s">S</code></td>
<td>
<p>Matrix defining the initial value for the lower triangular matrix
of the RAM algorithm, so that the covariance matrix of the Gaussian proposal
distribution is <code class="reqn">SS'</code>. Note that for some parameters
(currently the standard deviation, dispersion, and autoregressive parameters
of the BSM and AR(1) models) the sampling is done in unconstrained parameter
space, i.e. internal_theta = log(theta) (and logit(rho) or AR coefficient).</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_end_adaptive_phase">end_adaptive_phase</code></td>
<td>
<p>Logical, if <code>TRUE</code>, S is held fixed after the
burnin period. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_threads">threads</code></td>
<td>
<p>Number of threads for state simulation. Positive integer
(default is 1).
Note that parallel computing is only used in the post-correction phase of
IS-MCMC and when sampling the states in case of (approximate) Gaussian
models.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_seed">seed</code></td>
<td>
<p>Seed for the C++ RNG (positive integer).</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, prints a progress bar to the console. If
missing, defined by <code>rlang::is_interactive</code>.
Set to <code>FALSE</code> if number of iterations is less than 50.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_particles">particles</code></td>
<td>
<p>A positive integer defining the number of state samples per
MCMC iteration for models other than linear-Gaussian models.
Ignored if <code>mcmc_type</code> is <code>"approx"</code> or <code>"ekf"</code>. Suitable
values depend on the model, the data, <code>mcmc_type</code> and
<code>sampling_method</code>. While large values provide more
accurate estimates, the run time also increases with respect to to the
number of particles, so it is generally a good idea to test the run time
firstwith a small number of particles, e.g., less than 100.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_mcmc_type">mcmc_type</code></td>
<td>
<p>What type of MCMC algorithm should be used for models other
than linear-Gaussian models? Possible choices are
<code>"pm"</code> for pseudo-marginal MCMC,
<code>"da"</code> for delayed acceptance version of PMCMC ,
<code>"approx"</code> for approximate inference based on the Gaussian
approximation of the model,
<code>"ekf"</code> for approximate inference using extended Kalman filter
(for <code>ssm_nlg</code>),
or one of the three importance sampling type weighting schemes:
<code>"is3"</code> for simple importance sampling (weight is computed for each
MCMC iteration independently),
<code>"is2"</code> for jump chain importance sampling type weighting (default), or
<code>"is1"</code> for importance sampling type weighting where the number of
particles used for
weight computations is proportional to the length of the jump chain block.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_sampling_method">sampling_method</code></td>
<td>
<p>Method for state sampling when for models other than
linear-Gaussian models. If <code>"psi"</code>, <code class="reqn">\psi</code>-APF is used (default).
If <code>"spdk"</code>, non-sequential importance sampling
based on Gaussian approximation is used. If <code>"bsf"</code>, bootstrap filter
is used. If <code>"ekf"</code>, particle filter based on EKF-proposals are used
(only for <code>ssm_nlg</code> models).</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_local_approx">local_approx</code></td>
<td>
<p>If <code>TRUE</code> (default), Gaussian approximation
needed for some of the methods is performed at each iteration.
If <code>FALSE</code>, approximation is updated only once at the start of the
MCMC using the initial model.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations used in Gaussian approximation,
as a positive integer.
Default is 100 (although typically only few iterations are needed).</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_conv_tol">conv_tol</code></td>
<td>
<p>Positive tolerance parameter used in Gaussian approximation.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_iekf_iter">iekf_iter</code></td>
<td>
<p>Non-negative integer. The default zero corresponds to
normal EKF, whereas <code>iekf_iter &gt; 0</code> corresponds to iterated EKF
with <code>iekf_iter</code> iterations. Used only for models of class
<code>ssm_nlg</code>.</p>
</td></tr>
<tr><td><code id="run_mcmc_+3A_l_c">L_c</code>, <code id="run_mcmc_+3A_l_f">L_f</code></td>
<td>
<p>For <code>ssm_sde</code> models, Positive integer values defining
the discretization levels for first and second stages (defined as 2^L).
For pseudo-marginal methods (<code>"pm"</code>), maximum of these is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For linear-Gaussian models, option <code>"summary"</code> does not simulate
states directly but computes the posterior means and variances of states
using fast Kalman smoothing. This is slightly faster,
more memory efficient and more accurate than calculations based on
simulation smoother. In other cases, the means and
covariances are computed using the full output of particle filter
instead of subsampling one of these as in case of
<code>output_type = "full"</code>. The states are sampled up to the time point n+1
where n is the length of the input time series i.e. the last values are
one-step-ahead predictions. (for predicting further, see
<code>?predict.mcmc_output</code>).
</p>
<p>Initial values for the sampling are taken from the model object
(<code>model$theta</code>). If you want to continue from previous run, you can
reconstruct your original model by plugging in the previously obtained
parameters to <code>model$theta</code>, providing the S matrix for the RAM
algorithm and setting <code>burnin = 0</code>. See example. Note however, that
this is not identical as running all the iterations once, due to the
RNG &quot;discontinuity&quot; and because even without burnin bssm does include
&quot;theta_0&quot; i.e. the initial theta in the final chain (even with
<code>burnin=0</code>).
</p>


<h3>Value</h3>

<p>An object of class <code>mcmc_output</code>.
</p>


<h3>References</h3>

<p>Vihola M (2012). Robust adaptive Metropolis algorithm with
coerced acceptance rate. Statistics and Computing, 22(5), p 997-1008.
https://doi.org/10.1007/s11222-011-9269-5
</p>
<p>Vihola, M, Helske, J, Franks, J (2020). Importance sampling type
estimators based on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 1-38. https://doi.org/10.1111/sjos.12492
</p>
<p>Helske J, Vihola M (2021). bssm: Bayesian Inference of Non-linear and
Non-Gaussian State Space Models in R. The R Journal (2021) 13:2, 578-589.
https://doi.org/10.32614/RJ-2021-103
</p>
<p>Vihola, M, Helske, J, Franks, J. Importance sampling type estimators based
on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 2020; 1-38. https://doi.org/10.1111/sjos.12492
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- ar1_lg(LakeHuron, rho = uniform(0.5,-1,1),
  sigma = halfnormal(1, 10), mu = normal(500, 500, 500),
  sd_y = halfnormal(1, 10))

mcmc_results &lt;- run_mcmc(model, iter = 2e4)
summary(mcmc_results, return_se = TRUE)

sumr &lt;- summary(mcmc_results, variable = "states")
library("ggplot2")
ggplot(sumr, aes(time, Mean)) +
  geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`), alpha = 0.25) +
  geom_line() + theme_bw() +
  geom_point(data = data.frame(Mean = LakeHuron, time = time(LakeHuron)),
    col = 2)

# Continue from the previous run
model$theta[] &lt;- mcmc_results$theta[nrow(mcmc_results$theta), ]
run_more &lt;- run_mcmc(model, S = mcmc_results$S, iter = 1000, burnin = 0)

set.seed(1)
n &lt;- 50
slope &lt;- cumsum(c(0, rnorm(n - 1, sd = 0.001)))
level &lt;- cumsum(slope + c(0, rnorm(n - 1, sd = 0.2)))
y &lt;- rpois(n, exp(level))
poisson_model &lt;- bsm_ng(y,
  sd_level = halfnormal(0.01, 1),
  sd_slope = halfnormal(0.01, 0.1),
  P1 = diag(c(10, 0.1)), distribution = "poisson")

# Note small number of iterations for CRAN checks
mcmc_out &lt;- run_mcmc(poisson_model, iter = 1000, particles = 10,
  mcmc_type = "da")
summary(mcmc_out, what = "theta", return_se = TRUE)

set.seed(123)
n &lt;- 50
sd_level &lt;- 0.1
drift &lt;- 0.01
beta &lt;- -0.9
phi &lt;- 5

level &lt;- cumsum(c(5, drift + rnorm(n - 1, sd = sd_level)))
x &lt;- 3 + (1:n) * drift + sin(1:n + runif(n, -1, 1))
y &lt;- rnbinom(n, size = phi, mu = exp(beta * x + level))

model &lt;- bsm_ng(y, xreg = x,
  beta = normal(0, 0, 10),
  phi = halfnormal(1, 10),
  sd_level = halfnormal(0.1, 1),
  sd_slope = halfnormal(0.01, 0.1),
  a1 = c(0, 0), P1 = diag(c(10, 0.1)^2),
  distribution = "negative binomial")

# run IS-MCMC
# Note small number of iterations for CRAN checks
fit &lt;- run_mcmc(model, iter = 4000,
  particles = 10, mcmc_type = "is2", seed = 1)

# extract states
d_states &lt;- as.data.frame(fit, variable = "states", time = 1:n)

library("dplyr")
library("ggplot2")

 # compute summary statistics
level_sumr &lt;- d_states |&gt;
  filter(variable == "level") |&gt;
  group_by(time) |&gt;
  summarise(mean = diagis::weighted_mean(value, weight),
    lwr = diagis::weighted_quantile(value, weight,
      0.025),
    upr = diagis::weighted_quantile(value, weight,
      0.975))

# visualize
level_sumr |&gt; ggplot(aes(x = time, y = mean)) +
  geom_line() +
  geom_line(aes(y = lwr), linetype = "dashed", na.rm = TRUE) +
  geom_line(aes(y = upr), linetype = "dashed", na.rm = TRUE) +
  theme_bw() +
  theme(legend.title = element_blank()) +
  xlab("Time") + ylab("Level")

# theta
d_theta &lt;- as.data.frame(fit, variable = "theta")
ggplot(d_theta, aes(x = value)) +
 geom_density(aes(weight = weight), adjust = 2, fill = "#92f0a8") +
 facet_wrap(~ variable, scales = "free") +
 theme_bw()


# Bivariate Poisson model:

set.seed(1)
x &lt;- cumsum(c(3, rnorm(19, sd = 0.5)))
y &lt;- cbind(
  rpois(20, exp(x)),
  rpois(20, exp(x)))

prior_fn &lt;- function(theta) {
  # half-normal prior using transformation
  dnorm(exp(theta), 0, 1, log = TRUE) + theta # plus jacobian term
}

update_fn &lt;- function(theta) {
  list(R = array(exp(theta), c(1, 1, 1)))
}

model &lt;- ssm_mng(y = y, Z = matrix(1,2,1), T = 1,
  R = 0.1, P1 = 1, distribution = "poisson",
  init_theta = log(0.1),
  prior_fn = prior_fn, update_fn = update_fn)

# Note small number of iterations for CRAN checks
out &lt;- run_mcmc(model, iter = 4000, mcmc_type = "approx")

sumr &lt;- as.data.frame(out, variable = "states") |&gt;
  group_by(time) |&gt; mutate(value = exp(value)) |&gt;
  summarise(mean = mean(value),
    ymin = quantile(value, 0.05), ymax = quantile(value, 0.95))
ggplot(sumr, aes(time, mean)) +
geom_ribbon(aes(ymin = ymin, ymax = ymax),alpha = 0.25) +
geom_line() +
geom_line(data = data.frame(mean = y[, 1], time = 1:20),
  colour = "tomato") +
geom_line(data = data.frame(mean = y[, 2], time = 1:20),
  colour = "tomato") +
theme_bw()

</code></pre>

<hr>
<h2 id='sim_smoother'>Simulation Smoothing</h2><span id='topic+sim_smoother'></span><span id='topic+sim_smoother.lineargaussian'></span><span id='topic+sim_smoother.nongaussian'></span>

<h3>Description</h3>

<p>Function <code>sim_smoother</code> performs simulation smoothing i.e. simulates
the states from the conditional distribution <code class="reqn">p(\alpha | y, \theta)</code>
for linear-Gaussian models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_smoother(model, nsim, seed, use_antithetic = TRUE, ...)

## S3 method for class 'lineargaussian'
sim_smoother(
  model,
  nsim = 1,
  seed = sample(.Machine$integer.max, size = 1),
  use_antithetic = TRUE,
  ...
)

## S3 method for class 'nongaussian'
sim_smoother(
  model,
  nsim = 1,
  seed = sample(.Machine$integer.max, size = 1),
  use_antithetic = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_smoother_+3A_model">model</code></td>
<td>
<p>Model of class <code>bsm_lg</code>, <code>ar1_lg</code>
<code>ssm_ulg</code>, or <code>ssm_mlg</code>, or one of the non-gaussian models
<code>bsm_ng</code>, <code>ar1_ng</code> <code>svm</code>,
<code>ssm_ung</code>, or <code>ssm_mng</code>.</p>
</td></tr>
<tr><td><code id="sim_smoother_+3A_nsim">nsim</code></td>
<td>
<p>Number of samples (positive integer). Suitable values
depend on the model and the data, and while larger values provide more
accurate estimates, the run time also increases with respect to to the
number of samples, so it is generally a good idea to test the filter first
with a small number of samples, e.g., less than 100.</p>
</td></tr>
<tr><td><code id="sim_smoother_+3A_seed">seed</code></td>
<td>
<p>Seed for the C++ RNG (positive integer).</p>
</td></tr>
<tr><td><code id="sim_smoother_+3A_use_antithetic">use_antithetic</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), use antithetic
variable for location in simulation smoothing. Ignored for <code>ssm_mng</code>
models.</p>
</td></tr>
<tr><td><code id="sim_smoother_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For non-Gaussian/non-linear models, the simulation is based on the
approximating Gaussian model.
</p>


<h3>Value</h3>

<p>An array containing the generated samples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># only missing data, simulates from prior
model &lt;- bsm_lg(rep(NA, 25), sd_level = 1, 
  sd_y = 1)
# use antithetic variable for location
sim &lt;- sim_smoother(model, nsim = 4, use_antithetic = TRUE, seed = 1)
ts.plot(sim[, 1, ])
cor(sim[, 1, ])
</code></pre>

<hr>
<h2 id='ssm_mlg'>General multivariate linear Gaussian state space models</h2><span id='topic+ssm_mlg'></span>

<h3>Description</h3>

<p>Construct an object of class <code>ssm_mlg</code> by directly defining the
corresponding terms of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssm_mlg(
  y,
  Z,
  H,
  T,
  R,
  a1 = NULL,
  P1 = NULL,
  init_theta = numeric(0),
  D = NULL,
  C = NULL,
  state_names,
  update_fn = default_update_fn,
  prior_fn = default_prior_fn
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssm_mlg_+3A_y">y</code></td>
<td>
<p>Observations as multivariate time series or matrix with
dimensions n x p.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_z">Z</code></td>
<td>
<p>System matrix Z of the observation equation as p x m matrix or
p x m x n array.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_h">H</code></td>
<td>
<p>Lower triangular matrix H of the observation. Either a scalar or
a vector of length n.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_t">T</code></td>
<td>
<p>System matrix T of the state equation. Either a m x m matrix or a
m x m x n array.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_r">R</code></td>
<td>
<p>Lower triangular matrix R the state equation. Either a m x k matrix
or a m x k x n array.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_a1">a1</code></td>
<td>
<p>Prior mean for the initial state as a vector of length m.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_p1">P1</code></td>
<td>
<p>Prior covariance matrix for the initial state as m x m matrix.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_init_theta">init_theta</code></td>
<td>
<p>Initial values for the unknown hyperparameters theta
(i.e. unknown variables excluding latent state variables).</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_d">D</code></td>
<td>
<p>Intercept terms for observation equation, given as a p x n matrix.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_c">C</code></td>
<td>
<p>Intercept terms for state equation, given as m x n matrix.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_state_names">state_names</code></td>
<td>
<p>A character vector defining the names of the states.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_update_fn">update_fn</code></td>
<td>
<p>A function which returns list of updated model
components given input vector theta. This function should take only one
vector argument which is used to create list with elements named as
<code>Z</code>, <code>H</code>, <code>T</code>, <code>R</code>, <code>a1</code>, <code>P1</code>, <code>D</code>, and
<code>C</code>, where each element matches the dimensions of the original model
It's best to check the internal dimensions with <code>str(model_object)</code> as
the dimensions of input arguments can differ from the final dimensions.
If any of these components is missing, it is assumed to be constant wrt.
theta.</p>
</td></tr>
<tr><td><code id="ssm_mlg_+3A_prior_fn">prior_fn</code></td>
<td>
<p>A function which returns log of prior density
given input vector theta.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The general multivariate linear-Gaussian model is defined using the
following observational and state equations:
</p>
<p style="text-align: center;"><code class="reqn">y_t = D_t + Z_t \alpha_t + H_t \epsilon_t, 
(\textrm{observation equation})</code>
</p>

<p style="text-align: center;"><code class="reqn">\alpha_{t+1} = C_t + T_t \alpha_t + R_t \eta_t, 
(\textrm{transition equation})</code>
</p>

<p>where <code class="reqn">\epsilon_t \sim N(0, I_p)</code>, <code class="reqn">\eta_t \sim N(0, I_k)</code> and
<code class="reqn">\alpha_1 \sim N(a_1, P_1)</code> independently of each other.
Here p is the number of time series and k is the number of disturbance terms
(which can be less than m, the number of states).
</p>
<p>The <code>update_fn</code> function should take only one
vector argument which is used to create list with elements named as
<code>Z</code>, <code>H</code> <code>T</code>, <code>R</code>, <code>a1</code>, <code>P1</code>, <code>D</code>,
and <code>C</code>,
where each element matches the dimensions of the original model.
If any of these components is missing, it is assumed to be
constant wrt. theta.
Note that while you can input say R as m x k matrix for <code>ssm_mlg</code>,
<code>update_fn</code> should return R as m x k x 1 in this case.
It might be useful to first construct the model without updating function
</p>


<h3>Value</h3>

<p>An object of class <code>ssm_mlg</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("GlobalTemp", package = "KFAS")
model_temp &lt;- ssm_mlg(GlobalTemp, H = matrix(c(0.15,0.05,0, 0.05), 2, 2), 
  R = 0.05, Z = matrix(1, 2, 1), T = 1, P1 = 10,
  state_names = "temperature",
  # using default values, but being explicit for testing purposes
  D = matrix(0, 2, 1), C = matrix(0, 1, 1))
ts.plot(cbind(model_temp$y, smoother(model_temp)$alphahat), col = 1:3)

</code></pre>

<hr>
<h2 id='ssm_mng'>General Non-Gaussian State Space Model</h2><span id='topic+ssm_mng'></span>

<h3>Description</h3>

<p>Construct an object of class <code>ssm_mng</code> by directly defining the
corresponding terms of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssm_mng(
  y,
  Z,
  T,
  R,
  a1 = NULL,
  P1 = NULL,
  distribution,
  phi = 1,
  u,
  init_theta = numeric(0),
  D = NULL,
  C = NULL,
  state_names,
  update_fn = default_update_fn,
  prior_fn = default_prior_fn
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssm_mng_+3A_y">y</code></td>
<td>
<p>Observations as multivariate time series or matrix with dimensions
n x p.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_z">Z</code></td>
<td>
<p>System matrix Z of the observation equation as p x m matrix or
p x m x n array.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_t">T</code></td>
<td>
<p>System matrix T of the state equation. Either a m x m matrix or a
m x m x n array.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_r">R</code></td>
<td>
<p>Lower triangular matrix R the state equation. Either a m x k
matrix or a
m x k x n array.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_a1">a1</code></td>
<td>
<p>Prior mean for the initial state as a vector of length m.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_p1">P1</code></td>
<td>
<p>Prior covariance matrix for the initial state as m x m matrix.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_distribution">distribution</code></td>
<td>
<p>A vector of distributions of the observed series.
Possible choices are
<code>"poisson"</code>, <code>"binomial"</code>, <code>"negative binomial"</code>,
<code>"gamma"</code>, and <code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_phi">phi</code></td>
<td>
<p>Additional parameters relating to the non-Gaussian distributions.
For negative binomial distribution this is the dispersion term, for
gamma distribution this is the shape parameter, for Gaussian this is
standard deviation, and for other distributions this is ignored.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_u">u</code></td>
<td>
<p>A matrix of positive constants for non-Gaussian models
(of same dimensions as y). For Poisson,  gamma, and negative binomial
distribution, this corresponds to the offset term. For binomial, this is the
number of trials (and as such should be integer(ish)).</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_init_theta">init_theta</code></td>
<td>
<p>Initial values for the unknown hyperparameters theta
(i.e. unknown variables excluding latent state variables).</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_d">D</code></td>
<td>
<p>Intercept terms for observation equation, given as p x n matrix.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_c">C</code></td>
<td>
<p>Intercept terms for state equation, given as m x n matrix.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_state_names">state_names</code></td>
<td>
<p>A character vector defining the names of the states.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_update_fn">update_fn</code></td>
<td>
<p>A function which returns list of updated model
components given input vector theta. This function should take only one
vector argument which is used to create list with elements named as
<code>Z</code>, <code>T</code>, <code>R</code>, <code>a1</code>, <code>P1</code>, <code>D</code>, <code>C</code>, and
<code>phi</code>, where each element matches the dimensions of the original model.
If any of these components is missing, it is assumed to be constant wrt.
theta. It's best to check the internal dimensions with
<code>str(model_object)</code> as the dimensions of input arguments can differ
from the final dimensions.</p>
</td></tr>
<tr><td><code id="ssm_mng_+3A_prior_fn">prior_fn</code></td>
<td>
<p>A function which returns log of prior density
given input vector theta.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The general multivariate non-Gaussian model is defined using the following
observational and state equations:
</p>
<p style="text-align: center;"><code class="reqn">p^i(y^i_t | D_t + Z_t \alpha_t), (\textrm{observation equation})</code>
</p>

<p style="text-align: center;"><code class="reqn">\alpha_{t+1} = C_t + T_t \alpha_t + R_t \eta_t, 
(\textrm{transition equation})</code>
</p>

<p>where <code class="reqn">\eta_t \sim N(0, I_k)</code> and
<code class="reqn">\alpha_1 \sim N(a_1, P_1)</code> independently of each other, and
<code class="reqn">p^i(y_t | .)</code> is either Poisson, binomial, gamma, Gaussian, or
negative binomial distribution for each observation series <code class="reqn">i=1,...,p</code>.
Here k is the number of disturbance terms (which can be less than m,
the number of states).
</p>


<h3>Value</h3>

<p>An object of class <code>ssm_mng</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
set.seed(1)
n &lt;- 20
x &lt;- cumsum(rnorm(n, sd = 0.5))
phi &lt;- 2
y &lt;- cbind(
  rgamma(n, shape = phi, scale = exp(x) / phi),
  rbinom(n, 10, plogis(x)))

Z &lt;- matrix(1, 2, 1)
T &lt;- 1
R &lt;- 0.5
a1 &lt;- 0
P1 &lt;- 1

update_fn &lt;- function(theta) {
  list(R = array(theta[1], c(1, 1, 1)), phi = c(theta[2], 1))
}

prior_fn &lt;- function(theta) {
  ifelse(all(theta &gt; 0), sum(dnorm(theta, 0, 1, log = TRUE)), -Inf)
}

model &lt;- ssm_mng(y, Z, T, R, a1, P1, phi = c(2, 1), 
  init_theta = c(0.5, 2), 
  distribution = c("gamma", "binomial"),
  u = cbind(1, rep(10, n)),
  update_fn = update_fn, prior_fn = prior_fn,
  state_names = "random_walk",
  # using default values, but being explicit for testing purposes
  D = matrix(0, 2, 1), C = matrix(0, 1, 1))

# smoothing based on approximating gaussian model
ts.plot(cbind(y, fast_smoother(model)), 
  col = 1:3, lty = c(1, 1, 2))

</code></pre>

<hr>
<h2 id='ssm_nlg'>General multivariate nonlinear Gaussian state space models</h2><span id='topic+ssm_nlg'></span>

<h3>Description</h3>

<p>Constructs an object of class <code>ssm_nlg</code> by defining the corresponding
terms of the observation and state equation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssm_nlg(
  y,
  Z,
  H,
  T,
  R,
  Z_gn,
  T_gn,
  a1,
  P1,
  theta,
  known_params = NA,
  known_tv_params = matrix(NA),
  n_states,
  n_etas,
  log_prior_pdf,
  time_varying = rep(TRUE, 4),
  state_names = paste0("state", 1:n_states)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssm_nlg_+3A_y">y</code></td>
<td>
<p>Observations as multivariate time series (or matrix) of length
<code class="reqn">n</code>.</p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_z">Z</code>, <code id="ssm_nlg_+3A_h">H</code>, <code id="ssm_nlg_+3A_t">T</code>, <code id="ssm_nlg_+3A_r">R</code></td>
<td>
<p>An external pointers (object of class <code>externalptr</code>)
for the C++ functions which define the corresponding model functions.</p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_z_gn">Z_gn</code>, <code id="ssm_nlg_+3A_t_gn">T_gn</code></td>
<td>
<p>An external pointers (object of class <code>externalptr</code>)
for the C++ functions which define the gradients of the corresponding model
functions.</p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_a1">a1</code></td>
<td>
<p>Prior mean for the initial state as object of class
<code>externalptr</code></p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_p1">P1</code></td>
<td>
<p>Prior covariance matrix for the initial state as object of class
<code>externalptr</code></p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_theta">theta</code></td>
<td>
<p>Parameter vector passed to all model functions.</p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_known_params">known_params</code></td>
<td>
<p>A vector of known parameters passed to all model
functions.</p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_known_tv_params">known_tv_params</code></td>
<td>
<p>A matrix of known parameters passed to all model
functions.</p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_n_states">n_states</code></td>
<td>
<p>Number of states in the model (positive integer).</p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_n_etas">n_etas</code></td>
<td>
<p>Dimension of the noise term of the transition equation
(positive integer).</p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_log_prior_pdf">log_prior_pdf</code></td>
<td>
<p>An external pointer (object of class
<code>externalptr</code>) for the C++ function which
computes the log-prior density given theta.</p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_time_varying">time_varying</code></td>
<td>
<p>Optional logical vector of length 4, denoting whether
the values of
Z, H, T, and R vary with respect to time variable (given identical states).
If used, this can speed up some computations.</p>
</td></tr>
<tr><td><code id="ssm_nlg_+3A_state_names">state_names</code></td>
<td>
<p>A character vector containing names for the states.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The nonlinear Gaussian model is defined as
</p>
<p style="text-align: center;"><code class="reqn">y_t = Z(t, \alpha_t, \theta) + H(t, \theta) \epsilon_t, 
(\textrm{observation equation})</code>
</p>

<p style="text-align: center;"><code class="reqn">\alpha_{t+1} = T(t, \alpha_t, \theta) + R(t, \theta)\eta_t, 
(\textrm{transition equation})</code>
</p>

<p>where <code class="reqn">\epsilon_t \sim N(0, I_p)</code>, <code class="reqn">\eta_t \sim N(0, I_m)</code> and
<code class="reqn">\alpha_1 \sim N(a_1, P_1)</code> independently of each other, and functions
<code class="reqn">Z, H, T, R</code> can depend on <code class="reqn">\alpha_t</code> and parameter vector
<code class="reqn">\theta</code>.
</p>
<p>Compared to other models, these general models need a bit more effort from
the user, as you must provide the several small C++ snippets which define the
model structure. See examples in the vignette and <code>cpp_example_model</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>ssm_nlg</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Takes a while on CRAN
set.seed(1)
n &lt;- 50
x &lt;- y &lt;- numeric(n)
y[1] &lt;- rnorm(1, exp(x[1]), 0.1)
for(i in 1:(n-1)) {
 x[i+1] &lt;- rnorm(1, sin(x[i]), 0.1)
 y[i+1] &lt;- rnorm(1, exp(x[i+1]), 0.1)
}

pntrs &lt;- cpp_example_model("nlg_sin_exp")

model_nlg &lt;- ssm_nlg(y = y, a1 = pntrs$a1, P1 = pntrs$P1, 
  Z = pntrs$Z_fn, H = pntrs$H_fn, T = pntrs$T_fn, R = pntrs$R_fn, 
  Z_gn = pntrs$Z_gn, T_gn = pntrs$T_gn,
  theta = c(log_H = log(0.1), log_R = log(0.1)), 
  log_prior_pdf = pntrs$log_prior_pdf,
  n_states = 1, n_etas = 1, state_names = "state")

out &lt;- ekf(model_nlg, iekf_iter = 100)
ts.plot(cbind(x, out$at[1:n], out$att[1:n]), col = 1:3)

</code></pre>

<hr>
<h2 id='ssm_sde'>Univariate state space model with continuous SDE dynamics</h2><span id='topic+ssm_sde'></span>

<h3>Description</h3>

<p>Constructs an object of class <code>ssm_sde</code> by defining the functions for
the drift, diffusion and derivative of diffusion terms of univariate SDE,
as well as the log-density of observation equation. We assume that the
observations are measured at integer times (missing values are allowed).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssm_sde(
  y,
  drift,
  diffusion,
  ddiffusion,
  obs_pdf,
  prior_pdf,
  theta,
  x0,
  positive
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssm_sde_+3A_y">y</code></td>
<td>
<p>Observations as univariate time series (or vector) of length
<code class="reqn">n</code>.</p>
</td></tr>
<tr><td><code id="ssm_sde_+3A_drift">drift</code>, <code id="ssm_sde_+3A_diffusion">diffusion</code>, <code id="ssm_sde_+3A_ddiffusion">ddiffusion</code></td>
<td>
<p>An external pointers for the C++ functions
which
define the drift, diffusion and derivative of diffusion functions of SDE.</p>
</td></tr>
<tr><td><code id="ssm_sde_+3A_obs_pdf">obs_pdf</code></td>
<td>
<p>An external pointer for the C++ function which
computes the observational log-density given the the states and parameter
vector theta.</p>
</td></tr>
<tr><td><code id="ssm_sde_+3A_prior_pdf">prior_pdf</code></td>
<td>
<p>An external pointer for the C++ function which
computes the prior log-density given the parameter vector theta.</p>
</td></tr>
<tr><td><code id="ssm_sde_+3A_theta">theta</code></td>
<td>
<p>Parameter vector passed to all model functions.</p>
</td></tr>
<tr><td><code id="ssm_sde_+3A_x0">x0</code></td>
<td>
<p>Fixed initial value for SDE at time 0.</p>
</td></tr>
<tr><td><code id="ssm_sde_+3A_positive">positive</code></td>
<td>
<p>If <code>TRUE</code>, positivity constraint is
forced by <code>abs</code> in Milstein scheme.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As in case of <code>ssm_nlg</code> models, these general models need a bit more
effort from the user, as you must provide the several small C++ snippets
which define the model structure. See vignettes for an example and
<code>cpp_example_model</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>ssm_sde</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 # Takes a while on CRAN
library("sde")
set.seed(1)
# theta_0 = rho = 0.5
# theta_1 = nu = 2
# theta_2 = sigma = 0.3
x &lt;- sde.sim(t0 = 0, T = 50, X0 = 1, N = 50,
       drift = expression(0.5 * (2 - x)),
       sigma = expression(0.3),
       sigma.x = expression(0))
y &lt;- rpois(50, exp(x[-1]))

# source c++ snippets
pntrs &lt;- cpp_example_model("sde_poisson_OU")

sde_model &lt;- ssm_sde(y, pntrs$drift, pntrs$diffusion,
 pntrs$ddiffusion, pntrs$obs_density, pntrs$prior,
 c(rho = 0.5, nu = 2, sigma = 0.3), 1, positive = FALSE)

est &lt;- particle_smoother(sde_model, L = 12, particles = 500)

ts.plot(cbind(x, est$alphahat, 
  est$alphahat - 2*sqrt(c(est$Vt)), 
  est$alphahat + 2*sqrt(c(est$Vt))), 
  col = c(2, 1, 1, 1), lty = c(1, 1, 2, 2))


# Takes time with finer mesh, parallelization with IS-MCMC helps a lot
out &lt;- run_mcmc(sde_model, L_c = 4, L_f = 8, 
  particles = 50, iter = 2e4,
  threads = 4L)


</code></pre>

<hr>
<h2 id='ssm_ulg'>General univariate linear-Gaussian state space models</h2><span id='topic+ssm_ulg'></span>

<h3>Description</h3>

<p>Construct an object of class <code>ssm_ulg</code> by directly defining the
corresponding terms of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssm_ulg(
  y,
  Z,
  H,
  T,
  R,
  a1 = NULL,
  P1 = NULL,
  init_theta = numeric(0),
  D = NULL,
  C = NULL,
  state_names,
  update_fn = default_update_fn,
  prior_fn = default_prior_fn
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssm_ulg_+3A_y">y</code></td>
<td>
<p>Observations as time series (or vector) of length <code class="reqn">n</code>.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_z">Z</code></td>
<td>
<p>System matrix Z of the observation equation. Either a
vector of length m,
a m x n matrix, or object which can be coerced to such.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_h">H</code></td>
<td>
<p>A vector of standard deviations. Either a scalar or a vector of
length n.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_t">T</code></td>
<td>
<p>System matrix T of the state equation. Either a m x m matrix or a
m x m x n array, or object which can be coerced to such.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_r">R</code></td>
<td>
<p>Lower triangular matrix R the state equation. Either
a m x k matrix or a m x k x n array, or object which can be coerced to such.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_a1">a1</code></td>
<td>
<p>Prior mean for the initial state as a vector of length m.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_p1">P1</code></td>
<td>
<p>Prior covariance matrix for the initial state as m x m matrix.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_init_theta">init_theta</code></td>
<td>
<p>Initial values for the unknown hyperparameters theta
(i.e. unknown variables excluding latent state variables).</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_d">D</code></td>
<td>
<p>Intercept terms <code class="reqn">D_t</code> for the observations equation, given as a
scalar or vector of length n.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_c">C</code></td>
<td>
<p>Intercept terms <code class="reqn">C_t</code> for the state equation, given as a
m times 1 or m times n matrix.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_state_names">state_names</code></td>
<td>
<p>A character vector defining the names of the states.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_update_fn">update_fn</code></td>
<td>
<p>A function which returns list of updated model
components given input vector theta. This function should take only one
vector argument which is used to create list with elements named as
<code>Z</code>, <code>H</code>, <code>T</code>, <code>R</code>, <code>a1</code>, <code>P1</code>, <code>D</code>, and
<code>C</code>, where each element matches the dimensions of the original model
It's best to check the internal dimensions with <code>str(model_object)</code> as
the dimensions of input arguments can differ from the final dimensions.
If any of these components is missing, it is assumed to be constant wrt.
theta.</p>
</td></tr>
<tr><td><code id="ssm_ulg_+3A_prior_fn">prior_fn</code></td>
<td>
<p>A function which returns log of prior density
given input vector theta.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The general univariate linear-Gaussian model is defined using the following
observational and state equations:
</p>
<p style="text-align: center;"><code class="reqn">y_t = D_t + Z_t \alpha_t + H_t \epsilon_t, 
(\textrm{observation equation})</code>
</p>

<p style="text-align: center;"><code class="reqn">\alpha_{t+1} = C_t + T_t \alpha_t + R_t \eta_t, 
(\textrm{transition equation})</code>
</p>

<p>where <code class="reqn">\epsilon_t \sim N(0, 1)</code>, <code class="reqn">\eta_t \sim N(0, I_k)</code> and
<code class="reqn">\alpha_1 \sim N(a_1, P_1)</code> independently of each other.
Here k is the number of disturbance terms which can be less than m, the
number of states.
</p>
<p>The <code>update_fn</code> function should take only one
vector argument which is used to create list with elements named as
<code>Z</code>, <code>H</code> <code>T</code>, <code>R</code>, <code>a1</code>, <code>P1</code>, <code>D</code>,
and <code>C</code>,
where each element matches the dimensions of the original model.
If any of these components is missing, it is assumed to be constant wrt.
theta.
Note that while you can input say R as m x k matrix for <code>ssm_ulg</code>,
<code>update_fn</code> should return R as m x k x 1 in this case.
It might be useful to first construct the model without updating function
and then check the expected structure of the model components from the
output.
</p>


<h3>Value</h3>

<p>An object of class <code>ssm_ulg</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Regression model with time-varying coefficients
set.seed(1)
n &lt;- 100
x1 &lt;- rnorm(n)
x2 &lt;- rnorm(n)
b1 &lt;- 1 + cumsum(rnorm(n, sd = 0.5))
b2 &lt;- 2 + cumsum(rnorm(n, sd = 0.1))
y &lt;- 1 + b1 * x1 + b2 * x2 + rnorm(n, sd = 0.1)

Z &lt;- rbind(1, x1, x2)
H &lt;- 0.1
T &lt;- diag(3)
R &lt;- diag(c(0, 1, 0.1))
a1 &lt;- rep(0, 3)
P1 &lt;- diag(10, 3)

# updates the model given the current values of the parameters
update_fn &lt;- function(theta) {
  R &lt;- diag(c(0, theta[1], theta[2]))
  dim(R) &lt;- c(3, 3, 1)
  list(R = R, H = theta[3])
}
# prior for standard deviations as half-normal(1)
prior_fn &lt;- function(theta) {
  if(any(theta &lt; 0)) {
    log_p &lt;- -Inf 
  } else {
    log_p &lt;- sum(dnorm(theta, 0, 1, log = TRUE))
  }
  log_p
}

model &lt;- ssm_ulg(y, Z, H, T, R, a1, P1, 
  init_theta = c(1, 0.1, 0.1), 
  update_fn = update_fn, prior_fn = prior_fn, 
  state_names = c("level", "b1", "b2"),
  # using default values, but being explicit for testing purposes
  C = matrix(0, 3, 1), D = numeric(1))

out &lt;- run_mcmc(model, iter = 5000)
out
sumr &lt;- summary(out, variable = "state", times = 1:n)
sumr$true &lt;- c(b1, b2, rep(1, n))
library(ggplot2)
ggplot(sumr, aes(x = time, y = Mean)) +
geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`), alpha = 0.5) +
geom_line() + 
geom_line(aes(y = true), colour = "red") + 
facet_wrap(~ variable, scales = "free") +
theme_bw()

# Perhaps easiest way to construct a general SSM for bssm is to use the 
# model building functionality of KFAS:
library("KFAS")

model_kfas &lt;- SSModel(log(drivers) ~ SSMtrend(1, Q = 5e-4)+
  SSMseasonal(period = 12, sea.type = "trigonometric", Q = 0) +
 log(PetrolPrice) + law, data = Seatbelts, H = 0.005)

# use as_bssm function for conversion, kappa defines the 
# prior variance for diffuse states
model_bssm &lt;- as_bssm(model_kfas, kappa = 100)

# define updating function for parameter estimation
# we can use SSModel and as_bssm functions here as well
# (for large model it is more efficient to do this 
# "manually" by constructing only necessary matrices,
# i.e., in this case  a list with H and Q)

prior_fn &lt;- function(theta) {
  if(any(theta &lt; 0)) -Inf else sum(dnorm(theta, 0, 0.1, log = TRUE))
}
 
update_fn &lt;- function(theta) {
  
  model_kfas &lt;- SSModel(log(drivers) ~ SSMtrend(1, Q = theta[1]^2)+
    SSMseasonal(period = 12, 
      sea.type = "trigonometric", Q = theta[2]^2) +
    log(PetrolPrice) + law, data = Seatbelts, H = theta[3]^2)
  
  # the bssm_model object is essentially list so this is fine
  as_bssm(model_kfas, kappa = 100, init_theta = init_theta,
    update_fn = update_fn, prior_fn = prior_fn) 
}

init_theta &lt;- rep(1e-2, 3)
names(init_theta) &lt;- c("sd_level", "sd_seasonal", "sd_y")

model_bssm &lt;- update_fn(init_theta)


out &lt;- run_mcmc(model_bssm, iter = 10000, burnin = 5000) 
out

# Above the regression coefficients are modelled as 
# time-invariant latent states. 
# Here is an alternative way where we use variable D so that the
# coefficients are part of parameter vector theta. Note however that the 
# first option often preferable in order to keep the dimension of theta low.

updatefn2 &lt;- function(theta) {
  # note no PetrolPrice or law variables here
  model_kfas2 &lt;- SSModel(log(drivers) ~ SSMtrend(1, Q = theta[1]^2)+
    SSMseasonal(period = 12, sea.type = "trigonometric", Q = theta[2]^2), 
    data = Seatbelts, H = theta[3]^2)
  
  X &lt;- model.matrix(~ -1 + law + log(PetrolPrice), data = Seatbelts)
  D &lt;- t(X %*% theta[4:5])
  as_bssm(model_kfas2, D = D, kappa = 100)
}
prior2 &lt;- function(theta) {
 if(any(theta[1:3] &lt; 0)) {
  -Inf
 } else {
   sum(dnorm(theta[1:3], 0, 0.1, log = TRUE)) +
   sum(dnorm(theta[4:5], 0, 10, log = TRUE))
 }
}
init_theta &lt;- c(rep(1e-2, 3), 0, 0)
names(init_theta) &lt;- c("sd_level", "sd_seasonal", "sd_y", "law", "Petrol")
model_bssm2 &lt;- updatefn2(init_theta)
model_bssm2$theta &lt;- init_theta
model_bssm2$prior_fn &lt;- prior2
model_bssm2$update_fn &lt;- updatefn2

out2 &lt;- run_mcmc(model_bssm2, iter = 10000, burnin = 5000) 
out2

</code></pre>

<hr>
<h2 id='ssm_ung'>General univariate non-Gaussian state space model</h2><span id='topic+ssm_ung'></span>

<h3>Description</h3>

<p>Construct an object of class <code>ssm_ung</code> by directly defining the
corresponding terms of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssm_ung(
  y,
  Z,
  T,
  R,
  a1 = NULL,
  P1 = NULL,
  distribution,
  phi = 1,
  u,
  init_theta = numeric(0),
  D = NULL,
  C = NULL,
  state_names,
  update_fn = default_update_fn,
  prior_fn = default_prior_fn
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssm_ung_+3A_y">y</code></td>
<td>
<p>Observations as time series (or vector) of length <code class="reqn">n</code>.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_z">Z</code></td>
<td>
<p>System matrix Z of the observation equation. Either a
vector of length m,
a m x n matrix, or object which can be coerced to such.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_t">T</code></td>
<td>
<p>System matrix T of the state equation. Either a m x m matrix or a
m x m x n array, or object which can be coerced to such.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_r">R</code></td>
<td>
<p>Lower triangular matrix R the state equation. Either
a m x k matrix or a m x k x n array, or object which can be coerced to such.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_a1">a1</code></td>
<td>
<p>Prior mean for the initial state as a vector of length m.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_p1">P1</code></td>
<td>
<p>Prior covariance matrix for the initial state as m x m matrix.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_distribution">distribution</code></td>
<td>
<p>Distribution of the observed time series. Possible
choices are <code>"poisson"</code>, <code>"binomial"</code>, <code>"gamma"</code>, and
<code>"negative binomial"</code>.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_phi">phi</code></td>
<td>
<p>Additional parameter relating to the non-Gaussian distribution.
For negative binomial distribution this is the dispersion term, for gamma
distribution this is the shape parameter, and for other distributions this
is ignored. Should an object of class <code>bssm_prior</code> or
a positive scalar.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_u">u</code></td>
<td>
<p>A vector of positive constants for non-Gaussian models. For
Poisson, gamma, and negative binomial distribution, this corresponds to the
offset term. For binomial, this is the number of trials.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_init_theta">init_theta</code></td>
<td>
<p>Initial values for the unknown hyperparameters theta
(i.e. unknown variables excluding latent state variables).</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_d">D</code></td>
<td>
<p>Intercept terms <code class="reqn">D_t</code> for the observations equation, given as a
scalar or vector of length n.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_c">C</code></td>
<td>
<p>Intercept terms <code class="reqn">C_t</code> for the state equation, given as a
m times 1 or m times n matrix.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_state_names">state_names</code></td>
<td>
<p>A character vector defining the names of the states.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_update_fn">update_fn</code></td>
<td>
<p>A function which returns list of updated model
components given input vector theta. This function should take only one
vector argument which is used to create list with elements named as
<code>Z</code>, <code>T</code>, <code>R</code>, <code>a1</code>, <code>P1</code>, <code>D</code>, <code>C</code>, and
<code>phi</code>, where each element matches the dimensions of the original model.
If any of these components is missing, it is assumed to be constant wrt.
theta. It's best to check the internal dimensions with
<code>str(model_object)</code> as the dimensions of input arguments can differ
from the final dimensions.</p>
</td></tr>
<tr><td><code id="ssm_ung_+3A_prior_fn">prior_fn</code></td>
<td>
<p>A function which returns log of prior density
given input vector theta.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The general univariate non-Gaussian model is defined using the following
observational and state equations:
</p>
<p style="text-align: center;"><code class="reqn">p(y_t | D_t + Z_t \alpha_t), (\textrm{observation equation})</code>
</p>

<p style="text-align: center;"><code class="reqn">\alpha_{t+1} = C_t + T_t \alpha_t + R_t \eta_t, 
(\textrm{transition equation})</code>
</p>

<p>where <code class="reqn">\eta_t \sim N(0, I_k)</code> and
<code class="reqn">\alpha_1 \sim N(a_1, P_1)</code> independently of each other,
and <code class="reqn">p(y_t | .)</code> is either Poisson, binomial, gamma, or
negative binomial distribution.
Here k is the number of disturbance terms which can be less than m,
the number of states.
</p>
<p>The <code>update_fn</code> function should take only one
vector argument which is used to create list with elements named as
<code>Z</code>, <code>phi</code> <code>T</code>, <code>R</code>, <code>a1</code>, <code>P1</code>, <code>D</code>,
and <code>C</code>,
where each element matches the dimensions of the original model.
If any of these components is missing, it is assumed to be constant
wrt. theta.
Note that while you can input say R as m x k matrix for <code>ssm_ung</code>,
<code>update_fn</code> should return R as m x k x 1 in this case.
It might be useful to first construct the model without updating function
and then check the expected structure of the model components from
the output.
</p>


<h3>Value</h3>

<p>An object of class <code>ssm_ung</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("drownings", package = "bssm")
model &lt;- ssm_ung(drownings[, "deaths"], Z = 1, T = 1, R = 0.2, 
  a1 = 0, P1 = 10, distribution = "poisson", u = drownings[, "population"])

# approximate results based on Gaussian approximation
out &lt;- smoother(model)
ts.plot(cbind(model$y / model$u, exp(out$alphahat)), col = 1:2)
</code></pre>

<hr>
<h2 id='suggest_N'>Suggest Number of Particles for <code class="reqn">\psi</code>-APF Post-correction</h2><span id='topic+suggest_N'></span>

<h3>Description</h3>

<p>Function <code>estimate_N</code> estimates suitable number particles needed for
accurate post-correction of approximate MCMC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>suggest_N(
  model,
  theta,
  candidates = seq(10, 100, by = 10),
  replications = 100,
  seed = sample(.Machine$integer.max, size = 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="suggest_N_+3A_model">model</code></td>
<td>
<p>Model of class <code>nongaussian</code> or <code>ssm_nlg</code>.</p>
</td></tr>
<tr><td><code id="suggest_N_+3A_theta">theta</code></td>
<td>
<p>A vector of theta corresponding to the model, at which point
the standard deviation of the log-likelihood is computed. Typically MAP
estimate from the (approximate) MCMC run. Can also be an output from
<code>run_mcmc</code> which is then used to compute the MAP
estimate of theta.</p>
</td></tr>
<tr><td><code id="suggest_N_+3A_candidates">candidates</code></td>
<td>
<p>A vector of positive integers containing the candidate
number of particles to test. Default is <code>seq(10, 100, by = 10)</code>.</p>
</td></tr>
<tr><td><code id="suggest_N_+3A_replications">replications</code></td>
<td>
<p>Positive integer, how many replications should be used
for computing the standard deviations? Default is 100.</p>
</td></tr>
<tr><td><code id="suggest_N_+3A_seed">seed</code></td>
<td>
<p>Seed for the C++ RNG  (positive integer).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>suggest_N</code> estimates the standard deviation of the
logarithm of the post-correction weights at approximate MAP of theta,
using various particle sizes and suggest smallest number of particles
which still leads standard deviation less than 1. Similar approach was
suggested in the context of pseudo-marginal MCMC by Doucet et al. (2015),
but see also Section 10.3 in Vihola et al (2020).
</p>


<h3>Value</h3>

<p>List with suggested number of particles <code>N</code> and matrix
containing estimated standard deviations of the log-weights and
corresponding number of particles.
</p>


<h3>References</h3>

<p>Doucet, A, Pitt, MK, Deligiannidis, G, Kohn, R (2015).
Efficient implementation of Markov chain Monte Carlo when using an
unbiased likelihood estimator, Biometrika, 102(2) p. 295-313,
https://doi.org/10.1093/biomet/asu075
</p>
<p>Vihola, M, Helske, J, Franks, J (2020). Importance sampling type estimators
based on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 1-38. https://doi.org/10.1111/sjos.12492
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 300
x1 &lt;- sin((2 * pi / 12) * 1:n)
x2 &lt;- cos((2 * pi / 12) * 1:n)
alpha &lt;- numeric(n)
alpha[1] &lt;- 0
rho &lt;- 0.7
sigma &lt;- 1.2
mu &lt;- 1
for(i in 2:n) {
  alpha[i] &lt;- rnorm(1, mu * (1 - rho) + rho * alpha[i-1], sigma)
}
u &lt;- rpois(n, 50)
y &lt;- rbinom(n, size = u, plogis(0.5 * x1 + x2 + alpha))

ts.plot(y / u)

model &lt;- ar1_ng(y, distribution = "binomial", 
  rho = uniform(0.5, -1, 1), sigma = gamma_prior(1, 2, 0.001),
  mu = normal(0, 0, 10),
  xreg = cbind(x1,x2), beta = normal(c(0, 0), 0, 5),
  u = u)

# theta from earlier approximate MCMC run
# out_approx &lt;- run_mcmc(model, mcmc_type = "approx", 
#   iter = 5000) 
# theta &lt;- out_approx$theta[which.max(out_approx$posterior), ]

theta &lt;- c(rho = 0.64, sigma = 1.16, mu = 1.1, x1 = 0.56, x2 = 1.28)

estN &lt;- suggest_N(model, theta, candidates = seq(10, 50, by = 10),
  replications = 50, seed = 1)
plot(x = estN$results$N, y = estN$results$sd, type = "b")
estN$N

</code></pre>

<hr>
<h2 id='summary.mcmc_output'>Summary Statistics of Posterior Samples</h2><span id='topic+summary.mcmc_output'></span>

<h3>Description</h3>

<p>This functions returns a data frame containing mean, standard deviations,
standard errors, and effective sample size estimates for parameters and
states.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcmc_output'
summary(
  object,
  return_se = FALSE,
  variable = "theta",
  probs = c(0.025, 0.975),
  times,
  states,
  use_times = TRUE,
  method = "sokal",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.mcmc_output_+3A_object">object</code></td>
<td>
<p>Output from <code>run_mcmc</code></p>
</td></tr>
<tr><td><code id="summary.mcmc_output_+3A_return_se">return_se</code></td>
<td>
<p>if <code>FALSE</code> (default), computation of standard
errors and effective sample sizes is omitted (as they can take considerable
time for models with large number of states and time points).</p>
</td></tr>
<tr><td><code id="summary.mcmc_output_+3A_variable">variable</code></td>
<td>
<p>Are the summary statistics computed for either
<code>"theta"</code> (default), <code>"states"</code>, or <code>"both"</code>?</p>
</td></tr>
<tr><td><code id="summary.mcmc_output_+3A_probs">probs</code></td>
<td>
<p>A numeric vector defining the quantiles of interest. Default is
<code>c(0.025, 0.975)</code>.</p>
</td></tr>
<tr><td><code id="summary.mcmc_output_+3A_times">times</code></td>
<td>
<p>A vector of indices. For states, for what time points the
summaries should be computed? Default is all, ignored if
<code>variable = "theta"</code>.</p>
</td></tr>
<tr><td><code id="summary.mcmc_output_+3A_states">states</code></td>
<td>
<p>A vector of indices. For what states the summaries should be
computed?. Default is all, ignored if
<code>variable = "theta"</code>.</p>
</td></tr>
<tr><td><code id="summary.mcmc_output_+3A_use_times">use_times</code></td>
<td>
<p>If <code>TRUE</code> (default), transforms the values of the time
variable to match the ts attribute of the input to define. If <code>FALSE</code>,
time is based on the indexing starting from 1.</p>
</td></tr>
<tr><td><code id="summary.mcmc_output_+3A_method">method</code></td>
<td>
<p>Method for computing integrated autocorrelation time. Default
is <code>"sokal"</code>, other option is <code>"geyer"</code>.</p>
</td></tr>
<tr><td><code id="summary.mcmc_output_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For IS-MCMC two types of standard errors are reported.
SE-IS can be regarded as the square root of independent IS variance,
whereas SE corresponds to the square root of total asymptotic variance
(see Remark 3 of Vihola et al. (2020)).
</p>


<h3>Value</h3>

<p>If <code>variable</code> is <code>"theta"</code> or <code>"states"</code>, a
<code>data.frame</code> object. If <code>"both"</code>, a list of two data frames.
</p>


<h3>References</h3>

<p>Vihola, M, Helske, J, Franks, J. Importance sampling type estimators based
on approximate marginal Markov chain Monte Carlo.
Scand J Statist. 2020; 1-38. https://doi.org/10.1111/sjos.12492
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("negbin_model")
summary(negbin_model, return_se = TRUE, method = "geyer")
summary(negbin_model, times = c(1, 200), prob = c(0.05, 0.5, 0.95))
</code></pre>

<hr>
<h2 id='svm'>Stochastic Volatility Model</h2><span id='topic+svm'></span>

<h3>Description</h3>

<p>Constructs a simple stochastic volatility model with Gaussian errors and
first order autoregressive signal. See the main vignette for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svm(y, mu, rho, sd_ar, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svm_+3A_y">y</code></td>
<td>
<p>A numeric vector or a <code><a href="stats.html#topic+ts">ts</a></code> object of observations.</p>
</td></tr>
<tr><td><code id="svm_+3A_mu">mu</code></td>
<td>
<p>A prior for mu parameter of transition equation.
Should be an object of class <code>bssm_prior</code>.</p>
</td></tr>
<tr><td><code id="svm_+3A_rho">rho</code></td>
<td>
<p>A prior for autoregressive coefficient.
Should be an object of class <code>bssm_prior</code>.</p>
</td></tr>
<tr><td><code id="svm_+3A_sd_ar">sd_ar</code></td>
<td>
<p>A prior for the standard deviation of noise of the AR-process.
Should be an object of class <code>bssm_prior</code>.</p>
</td></tr>
<tr><td><code id="svm_+3A_sigma">sigma</code></td>
<td>
<p>A prior for sigma parameter of observation equation, internally
denoted as phi. Should be an object of class <code>bssm_prior</code>.
Ignored if <code>mu</code> is provided. Note that typically
parametrization using mu is preferred due to better numerical properties and
availability of better Gaussian approximation.
Most notably the global approximation approach does not work with sigma
parameterization as sigma is not a parameter of the resulting approximate
model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>svm</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("exchange")
y &lt;- exchange[1:100] # for faster CRAN check
model &lt;- svm(y, rho = uniform(0.98, -0.999, 0.999),
 sd_ar = halfnormal(0.15, 5), sigma = halfnormal(0.6, 2))

obj &lt;- function(pars) {
   -logLik(svm(y, 
     rho = uniform(pars[1], -0.999, 0.999),
     sd_ar = halfnormal(pars[2], 5),
     sigma = halfnormal(pars[3], 2)), particles = 0)
}
opt &lt;- optim(c(0.98, 0.15, 0.6), obj, 
  lower = c(-0.999, 1e-4, 1e-4),
  upper = c(0.999, 10, 10), method = "L-BFGS-B")
pars &lt;- opt$par
model &lt;- svm(y, 
  rho = uniform(pars[1],-0.999,0.999),
  sd_ar = halfnormal(pars[2], 5),
  sigma = halfnormal(pars[3], 2))

# alternative parameterization  
model2 &lt;- svm(y, rho = uniform(0.98,-0.999, 0.999),
 sd_ar = halfnormal(0.15, 5), mu = normal(0, 0, 1))

obj2 &lt;- function(pars) {
   -logLik(svm(y, 
     rho = uniform(pars[1], -0.999, 0.999),
     sd_ar = halfnormal(pars[2], 5),
     mu = normal(pars[3], 0, 1)), particles = 0)
}
opt2 &lt;- optim(c(0.98, 0.15, 0), obj2, lower = c(-0.999, 1e-4, -Inf),
  upper = c(0.999, 10, Inf), method = "L-BFGS-B")
pars2 &lt;- opt2$par
model2 &lt;- svm(y, 
  rho = uniform(pars2[1],-0.999,0.999),
  sd_ar = halfnormal(pars2[2], 5),
  mu = normal(pars2[3], 0, 1))

# sigma is internally stored in phi
ts.plot(cbind(model$phi * exp(0.5 * fast_smoother(model)), 
  exp(0.5 * fast_smoother(model2))), col = 1:2)

</code></pre>

<hr>
<h2 id='ukf'>Unscented Kalman Filtering</h2><span id='topic+ukf'></span>

<h3>Description</h3>

<p>Function <code>ukf</code> runs the unscented Kalman filter for the given
non-linear Gaussian model of class <code>ssm_nlg</code>,
and returns the filtered estimates and one-step-ahead predictions of the
states <code class="reqn">\alpha_t</code> given the data up to time <code class="reqn">t</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ukf(model, alpha = 0.001, beta = 2, kappa = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ukf_+3A_model">model</code></td>
<td>
<p>Model of class <code>ssm_nlg</code>.</p>
</td></tr>
<tr><td><code id="ukf_+3A_alpha">alpha</code></td>
<td>
<p>Positive tuning parameter of the UKF. Default is 0.001. Smaller
the value, closer the sigma point are to the mean of the state.</p>
</td></tr>
<tr><td><code id="ukf_+3A_beta">beta</code></td>
<td>
<p>Non-negative tuning parameter of the UKF. The default value is
2, which is optimal for Gaussian states.</p>
</td></tr>
<tr><td><code id="ukf_+3A_kappa">kappa</code></td>
<td>
<p>Non-negative tuning parameter of the UKF, which also affects
the spread of sigma points. Default value is 0.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the log-likelihood,
one-step-ahead predictions <code>at</code> and filtered
estimates <code>att</code> of states, and the corresponding variances <code>Pt</code> and
<code>Ptt</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Takes a while on CRAN
set.seed(1)
mu &lt;- -0.2
rho &lt;- 0.7
sigma_y &lt;- 0.1
sigma_x &lt;- 1
x &lt;- numeric(50)
x[1] &lt;- rnorm(1, mu, sigma_x / sqrt(1 - rho^2))
for(i in 2:length(x)) {
  x[i] &lt;- rnorm(1, mu * (1 - rho) + rho * x[i - 1], sigma_x)
}
y &lt;- rnorm(50, exp(x), sigma_y)

pntrs &lt;- cpp_example_model("nlg_ar_exp")

model_nlg &lt;- ssm_nlg(y = y, a1 = pntrs$a1, P1 = pntrs$P1, 
  Z = pntrs$Z_fn, H = pntrs$H_fn, T = pntrs$T_fn, R = pntrs$R_fn, 
  Z_gn = pntrs$Z_gn, T_gn = pntrs$T_gn,
  theta = c(mu= mu, rho = rho, 
    log_sigma_x = log(sigma_x), log_sigma_y = log(sigma_y)), 
  log_prior_pdf = pntrs$log_prior_pdf,
  n_states = 1, n_etas = 1, state_names = "state")

out_iekf &lt;- ekf(model_nlg, iekf_iter = 5)
out_ukf &lt;- ukf(model_nlg, alpha = 0.01, beta = 2, kappa = 1)
ts.plot(cbind(x, out_iekf$att, out_ukf$att), col = 1:3)

</code></pre>

<hr>
<h2 id='uniform_prior'>Prior objects for bssm models</h2><span id='topic+uniform_prior'></span><span id='topic+bssm_prior'></span><span id='topic+bssm_prior_list'></span><span id='topic+uniform'></span><span id='topic+halfnormal_prior'></span><span id='topic+halfnormal'></span><span id='topic+normal_prior'></span><span id='topic+normal'></span><span id='topic+tnormal_prior'></span><span id='topic+tnormal'></span><span id='topic+gamma_prior'></span><span id='topic+gamma'></span>

<h3>Description</h3>

<p>These simple objects of class <code>bssm_prior</code> are used to construct a
prior distributions for the hyperparameters theta for some of the model
objects of <code>bssm</code> package. Note that these priors do not include the
constant terms as they do not affect the sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uniform_prior(init, min, max)

uniform(init, min, max)

halfnormal_prior(init, sd)

halfnormal(init, sd)

normal_prior(init, mean, sd)

normal(init, mean, sd)

tnormal_prior(init, mean, sd, min = -Inf, max = Inf)

tnormal(init, mean, sd, min = -Inf, max = Inf)

gamma_prior(init, shape, rate)

gamma(init, shape, rate)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="uniform_prior_+3A_init">init</code></td>
<td>
<p>Initial value for the parameter, used in initializing the model
components and as a starting values in MCMC.</p>
</td></tr>
<tr><td><code id="uniform_prior_+3A_min">min</code></td>
<td>
<p>Lower bound of the uniform and truncated normal prior.</p>
</td></tr>
<tr><td><code id="uniform_prior_+3A_max">max</code></td>
<td>
<p>Upper bound of the uniform and truncated normal prior.</p>
</td></tr>
<tr><td><code id="uniform_prior_+3A_sd">sd</code></td>
<td>
<p>Positive value defining the standard deviation of the
(underlying i.e. non-truncated) Normal distribution.</p>
</td></tr>
<tr><td><code id="uniform_prior_+3A_mean">mean</code></td>
<td>
<p>Mean of the Normal prior.</p>
</td></tr>
<tr><td><code id="uniform_prior_+3A_shape">shape</code></td>
<td>
<p>Positive shape parameter of the Gamma prior.</p>
</td></tr>
<tr><td><code id="uniform_prior_+3A_rate">rate</code></td>
<td>
<p>Positive rate parameter of the Gamma prior.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently supported priors are
</p>

<ul>
<li><p> uniform prior (<code>uniform()</code>) with a probability density function (pdf)
defined as <code class="reqn">\frac{1}{max - min}</code> for <code class="reqn">min &lt; theta &lt; max</code>.
</p>
</li>
<li><p> normal (<code>normal()</code>), a normal distribution parameterized via mean and
standard deviation, i.e. N(mean, sd^2).
</p>
</li>
<li><p> truncated normal distribution  (<code>tnormal()</code>), a normal distribution
with known truncation points (from below and/or above). Ignoring the
scaling factors, this corresponds to the pdf of N(mean, sd^2) when
<code class="reqn">min &lt; theta &lt; max</code> and zero otherwise.
</p>
</li>
<li><p> half-normal (<code>halfnormal()</code>) with a pdf matching the pdf of the
truncated normal distribution with min=0 and max=inf.
</p>
</li>
<li><p> gamma (<code>gamma</code>), a gamma distribution with shape and rate
parameterization.
</p>
</li></ul>

<p>All parameters are vectorized so for regression coefficient vector beta you
can define prior for example as <code>normal(0, 0, c(10, 20))</code>.
</p>
<p>For the general exponential models, i.e. models built with the <code>ssm_ulg</code>,
<code>ssm_ung</code>, <code>ssm_mlg</code>, and <code>ssm_mng</code>, you can define arbitrary priors by
defining the <code>prior_fn</code> function, which takes the one argument, <code>theta</code>,
corresponding to the hyperparameter vector of the model,
and returns a log-density of the (joint) prior (see the R Journal paper and
e.g. <code>ssm_ulg</code> for examples).  Similarly, the priors for the non-linear
models (<code>ssm_nlg</code>) and SDE models (<code>ssm_sde</code>) are constructed
via C++ snippets (see the vignettes for details).
</p>
<p>The longer name versions of the prior functions with <code>_prior</code> ending
are identical with shorter versions and they are available only to
avoid clash with R's primitive function <code>gamma</code> (other long prior names
are just for consistent naming).
</p>


<h3>Value</h3>

<p>object of class <code>bssm_prior</code> or <code>bssm_prior_list</code> in case
of multiple priors (i.e. multiple regression coefficients).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create uniform prior on [-1, 1] for one parameter with initial value 0.2:
uniform(init = 0.2, min = -1.0, max = 1.0)
# two normal priors at once i.e. for coefficients beta:
normal(init = c(0.1, 2.5), mean = 0.1, sd = c(1.5, 2.8))
# Gamma prior (not run because autotest tests complain)
# gamma(init = 0.1, shape = 2.5, rate = 1.1)
# Same as
gamma_prior(init = 0.1, shape = 2.5, rate = 1.1)
# Half-normal
halfnormal(init = 0.01, sd = 0.1)
# Truncated normal
tnormal(init = 5.2, mean = 5.0, sd = 3.0, min = 0.5, max = 9.5)


# Further examples for diagnostic purposes:
uniform(c(0, 0.2), c(-1.0, 0.001), c(1.0, 1.2))
normal(c(0, 0.2), c(-1.0, 0.001), c(1.0, 1.2))
tnormal(c(2, 2.2), c(-1.0, 0.001), c(1.0, 1.2), c(1.2, 2), 3.3)
halfnormal(c(0, 0.2), c(1.0, 1.2))
# not run because autotest bug
# gamma(c(0.1, 0.2), c(1.2, 2), c(3.3, 3.3))

# longer versions:
uniform_prior(init = c(0, 0.2), min = c(-1.0, 0.001), max = c(1.0, 1.2))
normal_prior(init = c(0, 0.2), mean = c(-1.0, 0.001), sd = c(1.0, 1.2))
tnormal_prior(init = c(2, 2.2), mean = c(-1.0, 0.001), sd = c(1.0, 1.2),
  min = c(1.2, 2), max = 3.3)
halfnormal_prior(init = c(0, 0.2), sd = c(1.0, 1.2))
gamma_prior(init = c(0.1, 0.2), shape = c(1.2, 2), rate = c(3.3, 3.3))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
