<!DOCTYPE html><html lang="en"><head><title>Help for package onlinePCA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {onlinePCA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#onlinePCA-package'>
<p>Online Principal Component Analysis</p></a></li>
<li><a href='#batchpca'><p>Batch PCA</p></a></li>
<li><a href='#bsoipca'>
<p>Block Stochastic Orthononal Iteration (BSOI)</p></a></li>
<li><a href='#ccipca'><p>Candid Covariance-Free Incremental PCA</p></a></li>
<li><a href='#coef2fd'>
<p>Recover functional data from their B-spline coefficients</p></a></li>
<li><a href='#create.basis'>
<p>Create a smooth B-spline basis</p></a></li>
<li><a href='#fd2coef'>
<p>Compute the coefficients of functional data in a B-spline basis</p></a></li>
<li><a href='#ghapca'><p>Generalized Hebbian Algorithm for PCA</p></a></li>
<li><a href='#impute'><p>BLUP Imputation of Missing Values</p></a></li>
<li><a href='#incRpca'><p>Incremental PCA</p></a></li>
<li><a href='#incRpca.block'><p>Incremental PCA with Block Update</p></a></li>
<li><a href='#incRpca.rc'><p>Incremental PCA With Reduced Complexity</p></a></li>
<li><a href='#perturbationRpca'>
<p>Recursive PCA using a rank 1 perturbation method</p></a></li>
<li><a href='#secularRpca'>
<p>Recursive PCA Using Secular Equations</p></a></li>
<li><a href='#sgapca'><p>Stochastic Gradient Ascent PCA</p></a></li>
<li><a href='#snlpca'><p>Subspace Network Learning PCA</p></a></li>
<li><a href='#updateCovariance'>
<p>Update the Sample Covariance Matrix</p></a></li>
<li><a href='#updateMean'>
<p>Update the Sample Mean Vector</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Online Principal Component Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-15</td>
</tr>
<tr>
<td>Author:</td>
<td>David Degras [aut, cre], Herve Cardot [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Degras &lt;ddegrasv@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Online PCA for multivariate and functional data using perturbation methods, low-rank incremental methods, and stochastic optimization methods. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2), RSpectra</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.4), splines, stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://cran.r-project.org/package=onlinePCA">https://cran.r-project.org/package=onlinePCA</a></td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-15 01:07:14 UTC; daviddegras</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-15 04:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='onlinePCA-package'>
Online Principal Component Analysis
</h2><span id='topic+onlinePCA-package'></span>

<h3>Description</h3>

<p>Online PCA algorithms using perturbation methods (<code><a href="#topic+perturbationRpca">perturbationRpca</a></code>), secular equations  (<code><a href="#topic+secularRpca">secularRpca</a></code>), incremental PCA (<code><a href="#topic+incRpca">incRpca</a>, <a href="#topic+incRpca.block">incRpca.block</a>, <a href="#topic+incRpca.rc">incRpca.rc</a></code>), and stochastic optimization (<code><a href="#topic+bsoipca">bsoipca</a></code>,<br /> <code><a href="#topic+ccipca">ccipca</a>, <a href="#topic+ghapca">ghapca</a>, <a href="#topic+sgapca">sgapca</a>, <a href="#topic+snlpca">snlpca</a></code>). <code><a href="#topic+impute">impute</a></code> handles missing data with the regression approach of Brand (2002). <code><a href="#topic+batchpca">batchpca</a></code> performs fast batch (offline) PCA using iterative methods. <code><a href="#topic+create.basis">create.basis</a>, <a href="#topic+coef2fd">coef2fd</a>, <a href="#topic+fd2coef">fd2coef</a></code> respectively create B-spline basis sets for functional data (FD), convert FD to basis coefficients, and convert basis coefficients back to FD. <code><a href="#topic+updateMean">updateMean</a></code> and <code><a href="#topic+updateCovariance">updateCovariance</a></code> update the sample mean and sample covariance.</p>


<h3>Author(s)</h3>

<p>David Degras &lt;ddegrasv@gmail.com&gt;
</p>


<h3>References</h3>

<p>Brand, M. (2002). Incremental singular value decomposition of uncertain data with missing values. <em>European Conference on Computer Vision (ECCV).</em><br />
Gu, M. and Eisenstat, S. C. (1994). A stable and efficient algorithm for the rank-one modification of the symmetric eigenproblem. <em>SIAM Journal of Matrix Analysis and Applications.</em><br />
Hegde et al. (2006) Perturbation-Based Eigenvector Updates for On-Line Principal Components Analysis and Canonical Correlation Analysis. <em>Journal of VLSI Signal Processing</em>. <br />
Oja (1992). Principal components, Minor components, and linear neural networks. <em>Neural Networks.</em><br />
Sanger (1989). Optimal unsupervised learning in a single-layer linear feedforward neural network. <em>Neural Networks.</em><br />
Mitliagkas et al. (2013). Memory limited, streaming PCA. <em>Advances in Neural Information Processing Systems</em>.<br />
Weng et al. (2003). Candid Covariance-free Incremental Principal Component Analysis. <em>IEEE Trans. Pattern Analysis and Machine Intelligence</em>. 
</p>

<hr>
<h2 id='batchpca'>Batch PCA</h2><span id='topic+batchpca'></span>

<h3>Description</h3>

<p>This function performs the PCA of a data matrix or covariance matrix, returning the specified number of principal components (eigenvectors) and eigenvalues.</p>


<h3>Usage</h3>

<pre><code class='language-R'>batchpca(x, q, center, type = c("data","covariance"), byrow = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="batchpca_+3A_x">x</code></td>
<td>
<p>data or covariance matrix</p>
</td></tr>
<tr><td><code id="batchpca_+3A_q">q</code></td>
<td>
<p>number of requested PCs</p>
</td></tr>
<tr><td><code id="batchpca_+3A_center">center</code></td>
<td>
<p>optional centering vector for <code>x</code></p>
</td></tr>
<tr><td><code id="batchpca_+3A_type">type</code></td>
<td>
<p>type of the matrix <code>x</code></p>
</td></tr>
<tr><td><code id="batchpca_+3A_byrow">byrow</code></td>
<td>
<p>Are observation vectors stored in rows (TRUE) or in columns (FALSE)?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The PCA is efficiently computed using the functions <code><a href="RSpectra.html#topic+svds">svds</a></code> or <code><a href="RSpectra.html#topic+eigs_sym">eigs_sym</a></code> of package <code>RSpectra</code>, depending on the argument <code>type</code>. An Implicitly Restarted Arnoldi Method (IRAM) is used in the former case and an Implicitly Restarted Lanczos Method (IRLM) in the latter.<br />
The arguments <code>center</code> and <code>byrow</code> are only in effect if <code>type</code> is <code>"data"</code>. In this case a scaling factor <code class="reqn">1/\sqrt{n}</code> (not <code class="reqn">1/\sqrt{n-1}</code>) 
is applied to <code>x</code> before computing its singular values and vectors, where <code class="reqn">n</code> is the number of observation vectors stored in <code>x</code>.    
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>the first <code>q</code> squared singular values of <code>x</code> if <code>type="data"</code>; the first <code>Q</code> eigenvalues if <code>type="covariance"</code>.</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>the first <code>q</code> PC of <code>x</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p><a href="https://www.arpack.org">https://www.arpack.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
## Simulate data
n &lt;- 1e4
d &lt;- 500
q &lt;- 10
x &lt;- matrix(runif(n*d), n, d)
x &lt;- x %*% diag(sqrt(12*(1:d)))
# The eigenvalues of cov(x) are approximately 1, 2, ..., d
# and the corresponding eigenvectors are approximately  
# the canonical basis of R^p

## PCA computation (from fastest to slowest)
system.time(pca1 &lt;- batchpca(scale(x,scale=FALSE), q, byrow=TRUE))
system.time(pca2 &lt;- batchpca(cov(x), q, type="covariance"))
system.time(pca3 &lt;- eigen(cov(x),TRUE))
system.time(pca4 &lt;- svd(scale(x/sqrt(n-1),scale=FALSE), 0, q))
system.time(pca5 &lt;- prcomp(x))

## End(Not run)
</code></pre>

<hr>
<h2 id='bsoipca'>
Block Stochastic Orthononal Iteration (BSOI)</h2><span id='topic+bsoipca'></span>

<h3>Description</h3>

<p>The online PCA algorithm of Mitliagkas et al. (2013) is a block-wise stochastic variant of the classical power-method.</p>


<h3>Usage</h3>

<pre><code class='language-R'>bsoipca(x, q, U, B, center, byrow = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bsoipca_+3A_x">x</code></td>
<td>
<p>data matrix.</p>
</td></tr>
<tr><td><code id="bsoipca_+3A_q">q</code></td>
<td>
<p>number of PC to compute.</p>
</td></tr>
<tr><td><code id="bsoipca_+3A_u">U</code></td>
<td>
<p>matrix of initial PCs in columns (optional).</p>
</td></tr>
<tr><td><code id="bsoipca_+3A_b">B</code></td>
<td>
<p>size of block updates (optional).</p>
</td></tr>
<tr><td><code id="bsoipca_+3A_center">center</code></td>
<td>
<p>centering vector (optional).</p>
</td></tr>
<tr><td><code id="bsoipca_+3A_byrow">byrow</code></td>
<td>
<p>are the data vectors in <code>x</code> stored in rows (TRUE) or columns (FALSE)?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default value of <code>B</code> is <code class="reqn">floor(n/nblock)</code> with <code class="reqn">n</code> the number of data vectors in <code>x</code>, <code class="reqn">d</code> the number of variables, and <code class="reqn">nblock=ceiling(log(d))</code> the number of blocks.<br />
If <code>U</code> is specified, <code>q</code> defaults to <code>ncol(U)</code>; otherwise the initial PCs are computed from the first block of data and <code>q</code> must be specified explicitly. <br />
Although the algorithm does not give eigenvalues, they can easily be estimated by computing the variance of the data along the PCs.  
</p>


<h3>Value</h3>

<p>A matrix with the <code>q</code> first eigenvectors/PCs in columns.
</p>


<h3>References</h3>

<p>Mitliagkas et al. (2013). Memory limited, streaming PCA. <em>Advances in Neural Information Processing Systems</em>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Simulate Brownian Motion
n &lt;- 100 # number of sample paths
d &lt;- 50 # number of observation points
x &lt;- matrix(rnorm(n*d,sd=1/sqrt(d)),n,d)
x &lt;- t(apply(x,1,cumsum)) # dim(x) = c(100,50)

q &lt;- 10 # number of PC to compute
B &lt;- 20 # block size

## BSOI PCA 
U &lt;- bsoipca(x, q, B=B, byrow=TRUE) # PCs
lambda &lt;- apply(x %*% U, 2, var) # eigenvalues 
</code></pre>

<hr>
<h2 id='ccipca'>Candid Covariance-Free Incremental PCA</h2><span id='topic+ccipca'></span>

<h3>Description</h3>

<p>Stochastic gradient ascent algorithm CCIPCA of Weng et al. (2003). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ccipca(lambda, U, x, n, q = length(lambda), l=2, center, tol = 1e-8, sort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ccipca_+3A_lambda">lambda</code></td>
<td>
<p>vector of eigenvalues.</p>
</td></tr>  
<tr><td><code id="ccipca_+3A_u">U</code></td>
<td>
<p>matrix of eigenvectors (PC) stored in columns.</p>
</td></tr>
<tr><td><code id="ccipca_+3A_x">x</code></td>
<td>
<p>new data vector.</p>
</td></tr>
<tr><td><code id="ccipca_+3A_n">n</code></td>
<td>
<p>sample size before observing <code>x</code>.</p>
</td></tr> 
<tr><td><code id="ccipca_+3A_q">q</code></td>
<td>
<p>number of eigenvectors to compute.</p>
</td></tr>
<tr><td><code id="ccipca_+3A_l">l</code></td>
<td>
<p>'amnesic' parameter.</p>
</td></tr>
<tr><td><code id="ccipca_+3A_center">center</code></td>
<td>
<p>optional centering vector for <code>x</code>.</p>
</td></tr>
<tr><td><code id="ccipca_+3A_tol">tol</code></td>
<td>
<p>numerical tolerance.</p>
</td></tr>
<tr><td><code id="ccipca_+3A_sort">sort</code></td>
<td>
<p>Should the new eigenpairs be sorted?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The 'amnesic' parameter <code>l</code> determines the weight of past observations in the PCA update. If <code>l=0</code>, all observations have  equal weight, which is appropriate for stationary processes. Otherwise, typical values of <code>l</code> range between 2 and 4. 
As <code>l</code> increases, more weight is placed on new observations and less on older ones. For meaningful results, the condition <code>0&lt;=l&lt;n</code> should hold.<br />
The CCIPCA algorithm iteratively updates the PCs while deflating  <code>x</code>. If at some point the Euclidean norm of <code>x</code> becomes less than <code>tol</code>, the algorithm stops to prevent numerical overflow.<br />
If <code>sort</code> is TRUE, the updated eigenpairs are sorted by decreasing eigenvalue. If FALSE, they are not sorted.      
</p>


<h3>Value</h3>

<p>A list with components  
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>updated eigenvalues.</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>updated eigenvectors.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Weng et al. (2003). Candid Covariance-free Incremental Principal Component Analysis. <em>IEEE Trans. Pattern Analysis and Machine Intelligence</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Simulation of Brownian motion
n &lt;- 100 # number of paths
d &lt;- 50	 # number of observation points
q &lt;- 10	 # number of PCs to compute
x &lt;- matrix(rnorm(n*d,sd=1/sqrt(d)), n, d)
x &lt;- t(apply(x,1,cumsum))	

## Initial PCA
n0 &lt;- 50
pca &lt;- princomp(x[1:n0,])
xbar &lt;- pca$center
pca &lt;- list(values=pca$sdev^2, vectors=pca$loadings)

## Incremental PCA
for (i in n0:(n-1))
{	xbar &lt;- updateMean(xbar, x[i+1,], i)
  	pca &lt;- ccipca(pca$values, pca$vectors, x[i+1,], i, q = q, center = xbar) }

# Uncentered PCA
nx1 &lt;- sqrt(sum(x[1,]^2))
pca &lt;- list(values=nx1^2, vectors=as.matrix(x[1,]/nx1))
for (i in n0:(n-1))
  	pca &lt;- ccipca(pca$values, pca$vectors, x[i+1,], i, q = q)

</code></pre>

<hr>
<h2 id='coef2fd'>
Recover functional data from their B-spline coefficients
</h2><span id='topic+coef2fd'></span>

<h3>Description</h3>

<p>This function computes functional data 
from their coefficients in a B-spline basis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coef2fd(beta, basis, byrow = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="coef2fd_+3A_beta">beta</code></td>
<td>
<p>B-spline coefficients</p>
</td></tr>
<tr><td><code id="coef2fd_+3A_basis">basis</code></td>
<td>
<p>object created by <code><a href="#topic+create.basis">create.basis</a></code>
</p>
</td></tr>
<tr><td><code id="coef2fd_+3A_byrow">byrow</code></td>
<td>
<p>are the coefficients of each functional observation
stored in rows (TRUE) or in columns (FALSE)?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of functional data stored in the same format (row or columns) as the coefficients <code>beta</code>.
</p>


<h3>Note</h3>

<p>In view of (online or offline) functional PCA, 
the coefficients <code>beta</code> are left- or right- multiplied 
by <code>M^{-1/2}</code> (depending on their row/column format)) 
before applying the B-spline matrix <code>B</code>, 
with <code>M</code> the Gram matrix associated to <code>B</code>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+create.basis">create.basis</a></code>, <code><a href="#topic+fd2coef">fd2coef</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 100 # number of curves
d &lt;- 500 # number of observation points
grid &lt;- (1:d)/d # observation points
p &lt;- 50 # number of B-spline basis functions 

# Simulate Brownian motion
x &lt;- matrix(rnorm(n*d,sd=1/sqrt(d)),n,d)
x &lt;- t(apply(x,1,cumsum))

# Create B-spline basis 
mybasis &lt;- create.basis(grid, p, 1e-4)

# Compute smooth basis coefficients 
beta &lt;- fd2coef(x, mybasis)

# Recover smooth functional data
x.smooth &lt;- coef2fd(beta, mybasis) 
	
# Standard PCA and Functional PCA
pca &lt;- prcomp(x)
fpca &lt;- prcomp(beta)
</code></pre>

<hr>
<h2 id='create.basis'>
Create a smooth B-spline basis
</h2><span id='topic+create.basis'></span>

<h3>Description</h3>

<p>This function creates a smooth B-spline basis and provides tools to
find the coefficients of functional data in the basis  
and to recover functional data from basis coefficients.</p>


<h3>Usage</h3>

<pre><code class='language-R'>create.basis(x, p, sp = 1e-09, degree = 3, nderiv = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create.basis_+3A_x">x</code></td>
<td>
<p>vector of observation times</p>
</td></tr>
<tr><td><code id="create.basis_+3A_p">p</code></td>
<td>
<p>number of basis functions</p>
</td></tr>
<tr><td><code id="create.basis_+3A_sp">sp</code></td>
<td>
<p>smoothing parameter</p>
</td></tr>
<tr><td><code id="create.basis_+3A_degree">degree</code></td>
<td>
<p>degree of the B splines</p>
</td></tr>
<tr><td><code id="create.basis_+3A_nderiv">nderiv</code></td>
<td>
<p>order of the derivative to penalize for smoothing</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The knots of the B-spline basis are taken as regular quantiles of <code>x</code>. The function output is intended for use with functions 
<code><a href="#topic+coef2fd">coef2fd</a></code> and <code><a href="#topic+fd2coef">fd2coef</a></code>. 
</p>


<h3>Value</h3>

<p>A list with fields
</p>
<table role = "presentation">
<tr><td><code>B</code></td>
<td>
<p>matrix of B-splines evaluated at <code>x</code>
(each column represents a basis function)</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>matrix that maps functional data
to their (smoothed) coefficients of their projection
in the basis set. For the purpose of PCA, the 
coefficients are premultiplied by <code>M^{1/2}</code>, 
where <code>M</code> is the Gram matrix associated with <code>B</code></p>
</td></tr>
<tr><td><code>invsqrtM</code></td>
<td>
<p>matrix <code>M^{-1/2}</code> used to recover functional
# data from their coefficients in the basis set</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+coef2fd">coef2fd</a></code>, <code><a href="#topic+fd2coef">fd2coef</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 100 # number of curves
d &lt;- 500 # number of observation points
grid &lt;- (1:d)/d # observation points
p &lt;- 50 # number of B-spline basis functions 

# Simulate Brownian motion
x &lt;- matrix(rnorm(n*d,sd=1/sqrt(d)),n,d)
x &lt;- t(apply(x,1,cumsum))

# Create B-spline basis 
mybasis &lt;- create.basis(grid, p, 1e-4)

# Compute smooth basis coefficients 
beta &lt;- fd2coef(x, mybasis)

# Recover smooth functional data
x.smooth &lt;- coef2fd(beta, mybasis) 
	
# Standard PCA and Functional PCA
pca &lt;- prcomp(x)
fpca &lt;- prcomp(beta)
</code></pre>

<hr>
<h2 id='fd2coef'>
Compute the coefficients of functional data in a B-spline basis
</h2><span id='topic+fd2coef'></span>

<h3>Description</h3>

<p>This function computes the coefficients of functional data in a B-spline basis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fd2coef(x, basis, byrow = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fd2coef_+3A_x">x</code></td>
<td>
<p>matrix of functional data.</p>
</td></tr>
<tr><td><code id="fd2coef_+3A_basis">basis</code></td>
<td>
<p>object created by <code><a href="#topic+create.basis">create.basis</a></code></p>
</td></tr></table>
<p>.
</p>
<table role = "presentation">
<tr><td><code id="fd2coef_+3A_byrow">byrow</code></td>
<td>
<p>are the functional data stored in rows (TRUE) or in columns (FALSE)?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of B-spline coefficients stored in the same format (row or columns) as functional data.
</p>


<h3>Note</h3>

<p>In view of (online or offline) functional PCA, 
the coefficients are smoothed and premultiplied by <code>M^{1/2}</code>, 
with <code>M</code> the Gram matrix associated to the B-spline matrix.</p>


<h3>See Also</h3>

<p><code><a href="#topic+create.basis">create.basis</a></code>, <code><a href="#topic+coef2fd">coef2fd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 100 # number of curves
d &lt;- 500 # number of observation points
grid &lt;- (1:d)/d # observation points
p &lt;- 50 # number of B-spline basis functions 

# Simulate Brownian motion
x &lt;- matrix(rnorm(n*d,sd=1/sqrt(d)),n,d)
x &lt;- t(apply(x,1,cumsum))

# Create B-spline basis 
mybasis &lt;- create.basis(grid, p, 1e-4)

# Compute smooth basis coefficients 
beta &lt;- fd2coef(x, mybasis)

# Recover smooth functional data
x.smooth &lt;- coef2fd(beta, mybasis) 
	
# Standard PCA and Functional PCA
pca &lt;- prcomp(x)
fpca &lt;- prcomp(beta)
</code></pre>

<hr>
<h2 id='ghapca'>Generalized Hebbian Algorithm for PCA</h2><span id='topic+ghapca'></span>

<h3>Description</h3>

<p>Online PCA with the GHA of Sanger (1989).</p>


<h3>Usage</h3>

<pre><code class='language-R'>ghapca(lambda, U, x, gamma, q = length(lambda), center, sort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ghapca_+3A_lambda">lambda</code></td>
<td>
<p>optional vector of eigenvalues.</p>
</td></tr>  
<tr><td><code id="ghapca_+3A_u">U</code></td>
<td>
<p>matrix of eigenvectors (PC) stored in columns.</p>
</td></tr>
<tr><td><code id="ghapca_+3A_x">x</code></td>
<td>
<p>new data vector.</p>
</td></tr>
<tr><td><code id="ghapca_+3A_gamma">gamma</code></td>
<td>
<p>vector of gain parameters.</p>
</td></tr>
<tr><td><code id="ghapca_+3A_q">q</code></td>
<td>
<p>number of eigenvectors to compute.</p>
</td></tr>
<tr><td><code id="ghapca_+3A_center">center</code></td>
<td>
<p>optional centering vector for <code>x</code>.</p>
</td></tr>
<tr><td><code id="ghapca_+3A_sort">sort</code></td>
<td>
<p>Should the new eigenpairs be sorted?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The vector <code>gamma</code> determines the weight placed on the new data in updating each eigenvector (the first coefficient of <code>gamma</code> corresponds to the first eigenvector, etc). It can be specified as a single positive number or as a vector of length <code>ncol(U)</code>. Larger values of <code>gamma</code> place more weight on <code>x</code> and less on <code>U</code>. A common choice for (the components of) <code>gamma</code> is of the form <code>c/n</code>, with <code>n</code> the sample size and <code>c</code> a suitable positive constant. <br />
If <code>sort</code> is TRUE and <code>lambda</code> is not missing, the updated eigenpairs are sorted by decreasing eigenvalue. Otherwise, they are not sorted.    
</p>


<h3>Value</h3>

<p>A list with components  
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>updated eigenvalues or NULL.</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>updated eigenvectors.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Sanger (1989). Optimal unsupervised learning in a single-layer linear feedforward neural network. <em>Neural Networks.</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sgapca">sgapca</a></code>, <code><a href="#topic+snlpca">snlpca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Initialization
n &lt;- 1e4  # sample size
n0 &lt;- 5e3 # initial sample size
d &lt;- 10   # number of variables
q &lt;- d # number of PC
x &lt;- matrix(runif(n*d), n, d)
x &lt;- x %*% diag(sqrt(12*(1:d)))
# The eigenvalues of X are close to 1, 2, ..., d
# and the corresponding eigenvectors are close to 
# the canonical basis of R^d

## GHA PCA
pca &lt;- princomp(x[1:n0,])
xbar &lt;- pca$center
pca &lt;- list(values=pca$sdev[1:q]^2, vectors=pca$loadings[,1:q])
for (i in (n0+1):n) {
  xbar &lt;- updateMean(xbar, x[i,], i-1)
  pca &lt;- ghapca(pca$values, pca$vectors, x[i,], 2/i, q, xbar)
}
</code></pre>

<hr>
<h2 id='impute'>BLUP Imputation of Missing Values</h2><span id='topic+impute'></span>

<h3>Description</h3>

<p>Missing values of a vector are imputed by best linear unbiased prediction (BLUP) assuming a multivariate normal distribution. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impute(lambda, U, x, center, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="impute_+3A_lambda">lambda</code></td>
<td>
<p>vector of eigenvalues of length <code>q</code>.</p>
</td></tr>  
<tr><td><code id="impute_+3A_u">U</code></td>
<td>
<p>matrix of eigenvectors (principal components) of dimension <code>p * q</code>.</p>
</td></tr>
<tr><td><code id="impute_+3A_x">x</code></td>
<td>
<p>vector of observations of length <code>p</code> with missing entries.</p>
</td></tr>
<tr><td><code id="impute_+3A_center">center</code></td>
<td>
<p>centering vector for <code>x</code>. Default is zero.</p>
</td></tr>
<tr><td><code id="impute_+3A_tol">tol</code></td>
<td>
<p>tolerance in the calculation of the pseudoinverse.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The vector <code>x</code> is assumed to arise from a multivariate normal distribution with mean vector <code>center</code> and covariance matrix <code class="reqn">U diag(lambda) U^T</code>.</p>


<h3>Value</h3>

<p>The imputed vector <code>x</code>.</p>


<h3>References</h3>

<p>Brand, M. (2002). Incremental singular value decomposition of uncertain data with missing values. <em>European Conference on Computer Vision (ECCV).</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(10)
lambda &lt;- c(1,2,5)
U &lt;- qr.Q(qr(matrix(rnorm(30),10,3)))
x &lt;- U %*% diag(sqrt(lambda)) %*% rnorm(3) + rnorm(10, sd =.05)
x.na &lt;- x
x.na[c(1,3,7)] &lt;- NA
x.imputed &lt;- impute(lambda,U,x.na)
cbind(x,x.imputed)
</code></pre>

<hr>
<h2 id='incRpca'>Incremental PCA</h2><span id='topic+incRpca'></span>

<h3>Description</h3>

<p>Online PCA using the incremental SVD method of Brand (2002) and Arora et al. (2012).</p>


<h3>Usage</h3>

<pre><code class='language-R'>incRpca(lambda, U, x, n, f = 1/n, q = length(lambda), center, tol = 1e-7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="incRpca_+3A_lambda">lambda</code></td>
<td>
<p>vector of eigenvalues.</p>
</td></tr>  
<tr><td><code id="incRpca_+3A_u">U</code></td>
<td>
<p>matrix of eigenvectors (principal components) stored in columns.</p>
</td></tr>
<tr><td><code id="incRpca_+3A_x">x</code></td>
<td>
<p>new data vector.</p>
</td></tr>
<tr><td><code id="incRpca_+3A_n">n</code></td>
<td>
<p>sample size before observing <code>x</code>.</p>
</td></tr> 
<tr><td><code id="incRpca_+3A_f">f</code></td>
<td>
<p>forgetting factor: a number in (0,1).</p>
</td></tr> 
<tr><td><code id="incRpca_+3A_q">q</code></td>
<td>
<p>number of eigenvectors to compute.</p>
</td></tr>
<tr><td><code id="incRpca_+3A_center">center</code></td>
<td>
<p>optional centering vector for <code>x</code>.</p>
</td></tr>
<tr><td><code id="incRpca_+3A_tol">tol</code></td>
<td>
<p>numerical tolerance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the Euclidean distance between <code>x</code> and <code>U</code> is more than <code>tol</code>, the number of eigenpairs increases to <code>length(lambda)+1</code> before eventual truncation at order <code>q</code>. Otherwise, the eigenvectors remain unchanged and only the eigenvalues are updated.
<br />  
The forgetting factor <code>f</code> can be interpreted as the inverse of the number of observation vectors effectively used in the PCA: the &quot;memory&quot; of the PCA algorithm goes back <code>1/f</code> observations in the past. For larger values of <code>f</code>, the PCA update gives more relative weight to the new data <code>x</code> and less to the current PCA (<code>lambda,U</code>). For nonstationary processes, <code>f</code> should be closer to 1.<br />
Only one of the arguments <code>n</code> and <code>f</code> needs being specified. If it is <code>n</code>, then <code>f</code> is set to <code>1/n</code> by default (usual PCA of sample covariance matrix where all data points have equal weight). If <code>f</code> is specified, its value overrides any eventual specification of <code>n</code>. 
</p>


<h3>Value</h3>

<p>A list with components  
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>updated eigenvalues in decreasing order.</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>updated eigenvectors.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Arora et al. (2012). Stochastic Optimization for PCA and PLS.  <em>50th Annual Conference on Communication, Control, and Computing (Allerton).</em><br />
Brand, M. (2002). Incremental singular value decomposition of uncertain data with missing values. <em>European Conference on Computer Vision (ECCV).</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Simulate Brownian motion
n &lt;- 100 # number of sample paths
d &lt;- 50	 # number of observation points
q &lt;- 10	 # number of PCs to compute
n0 &lt;- 50 # number of sample paths used for initialization 
x &lt;- matrix(rnorm(n*d,sd=1/sqrt(d)), n, d)
x &lt;- t(apply(x,1,cumsum))	
dim(x) # (100,50)


## Incremental PCA (IPCA, centered)
pca &lt;- prcomp(x[1:n0,]) # initialization
xbar &lt;- pca$center
pca &lt;- list(values=pca$sdev[1:q]^2, vectors=pca$rotation[,1:q])
for (i in (n0+1):n)
{
  xbar &lt;- updateMean(xbar, x[i,], i-1)
  pca &lt;- incRpca(pca$values, pca$vectors, x[i,], i-1, q = q,
		center = xbar)
}

## Incremental PCA (IPCA, uncentered)
pca &lt;- prcomp(x[1:n0,],center=FALSE) # initialization
pca &lt;- list(values = pca$sdev[1:q]^2, vectors = pca$rotation[,1:q])
for (i in (n0+1):n)
  pca &lt;- incRpca(pca$values, pca$vectors, x[i,], i-1, q = q)
</code></pre>

<hr>
<h2 id='incRpca.block'>Incremental PCA with Block Update
</h2><span id='topic+incRpca.block'></span>

<h3>Description</h3>

<p>Sequential Karhunen-Loeve (SKL) algorithm of Levy and Lindenbaum (2000). The PCA can be updated with respect to a data matrix (not just a data vector). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>incRpca.block(x, B, lambda, U, n0 = 0, f, q = length(lambda), center, byrow = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="incRpca.block_+3A_x">x</code></td>
<td>
<p>data matrix</p>
</td></tr>
<tr><td><code id="incRpca.block_+3A_b">B</code></td>
<td>
<p>block size</p>
</td></tr>
<tr><td><code id="incRpca.block_+3A_lambda">lambda</code></td>
<td>
<p>initial eigenvalues (optional)</p>
</td></tr>
<tr><td><code id="incRpca.block_+3A_u">U</code></td>
<td>
<p>initial eigenvectors/PCs (optional)</p>
</td></tr>
<tr><td><code id="incRpca.block_+3A_n0">n0</code></td>
<td>
<p>initial sample size (optional)</p>
</td></tr>
<tr><td><code id="incRpca.block_+3A_f">f</code></td>
<td>
<p>vector of forgetting factors</p>
</td></tr>
<tr><td><code id="incRpca.block_+3A_q">q</code></td>
<td>
<p>number of requested PCs</p>
</td></tr>
<tr><td><code id="incRpca.block_+3A_center">center</code></td>
<td>
<p>centering vector for <code>x</code> (optional)</p>
</td></tr>
<tr><td><code id="incRpca.block_+3A_byrow">byrow</code></td>
<td>
<p>Are the data vectors in <code>x</code> stored in rows (TRUE) or columns (FALSE)?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This incremental PCA algorithm utilizes QR factorization and RSVD. 
It generalizes the algorithm <code><a href="#topic+incRpca">incRpca</a></code> from vector to matrix/block updates. However, <code><a href="#topic+incRpca">incRpca</a></code> should be preferred for vector updates as it is faster.<br />
If <code>lambda</code> and <code>U</code> are specified, they are taken as the initial PCA. Otherwise, the PCA is initialized by the SVD of the first block of data in <code>x</code> (<code>B</code> data vectors). The number <code>n0</code> is the sample size before observing <code>x</code> (default value = 0). The number <code>B</code> is the size of blocks to be used in the PCA updates. Ideally, <code>B</code> should be a divisor of the number of data vectors in <code>x</code> (otherwise the last smaller block is discarded). If <code>U</code> is provided, then <code>B</code> and <code>q</code> default to the number of columns (=PC) of <code>U</code>.<br /> 
The argument <code>f</code> determines the relative weight of current PCA and new data in each block update. Its length should be equal to the number of blocks in <code>x</code>, say <code class="reqn">nblock</code>. If <code>n0</code> and <code>B</code> are provided, then <code>f</code> defaults to <code>B/(n0+(0:(nblock-1)*B))</code>, i.e., the case where all data points have equal weights. The values in <code>f</code> should be in (0,1), with higher values giving more weight to new data and less to the current PCA.  
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>first <code>q</code> eigenvalues.</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>first <code>q</code> eigenvectors/PCs.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Levy, A. and Lindenbaum, M. (2000). Sequential Karhunen-Loeve basis extraction and its application to images. <em>IEEE Transactions on Image Processing</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+incRpca">incRpca</a>,<a href="#topic+incRpca.rc">incRpca.rc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Simulate Brownian Motion
n &lt;- 100 # number of sample paths
d &lt;- 50 # number of observation points
x &lt;- matrix(rnorm(n*d,sd=1/sqrt(d)),n,d)
x &lt;- t(apply(x,1,cumsum)) # dim(x) = c(100,50)
q &lt;- 10 # number of PC to compute
B &lt;- 20 # block size
n0 &lt;- B # initial sample size (if relevant)

## PCA without initial values
res1 &lt;- incRpca.block(t(x), B, q=q) # data vectors in columns
res2 &lt;- incRpca.block(x, B, q=q, byrow=TRUE) # data vectors in rows
all.equal(res1,res2) # TRUE

## PCA with initial values 
svd0 &lt;- svd(x[1:n0,], 0, n0) # use first block for initialization 
lambda &lt;- svd0$d[1:n0]^2/n0 # initial eigenvalues
U &lt;- svd0$v # initial PC
res3 &lt;- incRpca.block(x[-(1:n0),], B, lambda, U, n0, q=q, byrow=TRUE) 
# run PCA with this initialization on rest of the data 
all.equal(res1,res3) # compare with previous PCA: TRUE

## Compare with function incRpca
res4 &lt;- list(values=lambda, vectors=U)
for (i in (n0+1):n)
	res4 &lt;- incRpca(res4$values, res4$vectors, x[i,], i-1, q=q)
B &lt;- 1 # vector update
res5 &lt;- incRpca.block(x[-(1:n0),], B, lambda, U, n0, q=q, byrow=TRUE)
ind &lt;- which(sign(res5$vectors[1,]) != sign(res4$vectors[1,]))
res5$vectors[,ind] &lt;- - res5$vectors[,ind] # align PCs (flip orientation as needed)
all.equal(res4,res5) # TRUE
</code></pre>

<hr>
<h2 id='incRpca.rc'>Incremental PCA With Reduced Complexity</h2><span id='topic+incRpca.rc'></span>

<h3>Description</h3>

<p>The incremental PCA is computed without rotating the updated projection space (Brand, 2002; Arora et al., 2012). Specifically, PCs are specified through a matrix of orthogonal vectors <code>Ut</code> that spans the PC space and a rotation matrix <code>Us</code> such that the PC matrix is <code>UtUs</code>. Given a new data vector, the PCA is updated by adding one column to <code>Ut</code> and recalculating the low-dimensional rotation matrix <code>Us</code>. This reduces complexity and helps preserving orthogonality. Eigenvalues are updated as the usual incremental PCA algorithm.</p>


<h3>Usage</h3>

<pre><code class='language-R'>incRpca.rc(lambda, Ut, Us, x, n, f = 1/n, center, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="incRpca.rc_+3A_lambda">lambda</code></td>
<td>
<p>vector of eigenvalues.</p>
</td></tr>
<tr><td><code id="incRpca.rc_+3A_ut">Ut</code></td>
<td>
<p>matrix of orthogonal vectors stored in columns.</p>
</td></tr>
<tr><td><code id="incRpca.rc_+3A_us">Us</code></td>
<td>
<p>rotation matrix.</p>
</td></tr>
<tr><td><code id="incRpca.rc_+3A_x">x</code></td>
<td>
<p>new data vector.</p>
</td></tr>
<tr><td><code id="incRpca.rc_+3A_n">n</code></td>
<td>
<p>sample size before observing <code>x</code>.</p>
</td></tr>
<tr><td><code id="incRpca.rc_+3A_f">f</code></td>
<td>
<p>forgetting factor: a number in (0,1).</p>
</td></tr>
<tr><td><code id="incRpca.rc_+3A_center">center</code></td>
<td>
<p>optional centering vector for <code>x</code>.</p>
</td></tr>
<tr><td><code id="incRpca.rc_+3A_tol">tol</code></td>
<td>
<p>numerical tolerance for eigenvalues.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For large datasets, this algorithm is considerably faster than its counterpart <code><a href="#topic+incRpca">incRpca</a></code>, reducing the time complexity of each update from <code class="reqn">O(qd^2)</code> to <code class="reqn">O(qd + q^3)</code> flops with <code>d</code> the length of <code>x</code>. A consequence of not rotating the PC basis at each update is that the dimension of the PCA decomposition increases whenever a new observation vector is not entirely contained in the PC space. To keep the number of PCs and eigenvalues from getting too large, it is necessary to multiply the matrices <code class="reqn">U_t</code> and <code class="reqn">U_s</code> at regular time intervals so as to recover the individual PCs and retain only the largest ones.  
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>updated eigenvalues in decreasing order.</p>
</td></tr>
<tr><td><code>Ut</code></td>
<td>
<p>updated projection space.</p>
</td></tr>
<tr><td><code>Us</code></td>
<td>
<p>updated rotation matrix.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Arora et al. (2012). Stochastic Optimization for PCA and PLS.  <em>50th Annual Conference on Communication, Control, and Computing (Allerton).</em><br />
Brand, M. (2002). Incremental singular value decomposition of uncertain data with missing values. <em>European Conference on Computer Vision (ECCV).</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+incRpca">incRpca</a>, <a href="#topic+incRpca.block">incRpca.block</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Data generation
n &lt;- 400 # number of units
d &lt;- 10000 # number of variables
n0 &lt;- 200 # initial sample
q &lt;- 20 # required number of PCs
x &lt;- matrix(rnorm(n*d,sd=1/sqrt(d)),n,d) # data matrix
x &lt;- t(apply(x,1,cumsum)) # standard Brownian motion

# Initial PCA
# Initial PCA
xbar0 &lt;- colMeans(x[1:n0,])
pca0 &lt;- batchpca(x0c, q, center=xbar0, byrow=TRUE)

# Incremental PCA with rotation
xbar &lt;- xbar0
pca1 &lt;- pca0
system.time({
for (i in n0:(n-1)) {
	xbar &lt;- updateMean(xbar, x[i+1,], i)
	pca1 &lt;- incRpca(pca1$values, pca1$vectors, x[i+1,], i, center=xbar)
	}
})

# Incremental PCA without rotation
xbar &lt;- xbar0
pca2 &lt;- list(values=pca0$values, Ut=pca0$vectors, Us=diag(q))

system.time({
for (i in n0:(n-1)) {
	xbar &lt;- updateMean(xbar, x[i+1,], i)
	pca2 &lt;- incRpca.rc(pca2$values, pca2$Ut, pca2$Us, x[i+1,], 
		i, center = xbar)
	# Rotate the PC basis and reduce its size to q every k observations
	if (i %% q == 0 || i == n-1) 
		{ pca2$values &lt;- pca2$values[1:q]
		  pca2$Ut &lt;- pca2$Ut %*% pca2$Us[,1:q]
		  pca2$Us &lt;- diag(q)
	}
}
})

# Check that the results are identical
# Relative differences in eigenvalues
range(pca1$values/pca2$values-1)
# Cosines of angles between eigenvectors 
abs(colSums(pca1$vectors * pca2$Ut))

## End(Not run)
</code></pre>

<hr>
<h2 id='perturbationRpca'>
Recursive PCA using a rank 1 perturbation method</h2><span id='topic+perturbationRpca'></span>

<h3>Description</h3>

<p>This function recursively updates the PCA with respect to a single new data vector, using the (fast) perturbation method of Hegde et al. (2006).</p>


<h3>Usage</h3>

<pre><code class='language-R'>perturbationRpca(lambda, U, x, n, f = 1/n, center, sort = TRUE)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="perturbationRpca_+3A_lambda">lambda</code></td>
<td>
<p>vector of eigenvalues.</p>
</td></tr>
<tr><td><code id="perturbationRpca_+3A_u">U</code></td>
<td>
<p>matrix of eigenvectors (PC) stored in columns.</p>
</td></tr>
<tr><td><code id="perturbationRpca_+3A_x">x</code></td>
<td>
<p>new data vector.</p>
</td></tr>
<tr><td><code id="perturbationRpca_+3A_n">n</code></td>
<td>
<p>sample size before observing <code>x</code>.</p>
</td></tr>
<tr><td><code id="perturbationRpca_+3A_f">f</code></td>
<td>
<p>forgetting factor: a number between 0 and 1.</p>
</td></tr>
<tr><td><code id="perturbationRpca_+3A_center">center</code></td>
<td>
<p>optional centering vector for <code>x</code>.</p>
</td></tr>
<tr><td><code id="perturbationRpca_+3A_sort">sort</code></td>
<td>
<p>Should the eigenpairs be sorted?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The forgetting factor <code>f</code> can be interpreted as the inverse of the number of observation vectors effectively used in the PCA: the &quot;memory&quot; of the PCA algorithm goes back <code>1/f</code> observations in the past. For larger values of <code>f</code>, the PCA update gives more relative weight to the new data <code>x</code> and less to the current PCA (<code>lambda,U</code>). For nonstationary processes, <code>f</code> should be closer to 1.<br />
Only one of the arguments <code>n</code> and <code>f</code> needs being specified. If it is <code>n</code>, then <code>f</code> is set to <code>1/n</code> by default (usual PCA of sample covariance matrix where all data points have equal weight). If <code>f</code> is specified, its value overrides any eventual specification of <code>n</code>. 
<br />
If <code>sort</code> is TRUE, the updated eigenpairs are sorted by decreasing eigenvalue. Otherwise, they are not sorted.  
</p>


<h3>Value</h3>

<p>A list with components 
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>updated eigenvalues.</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>updated eigenvectors.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This perturbation method is based on large sample approximations. It tends to be highly inaccurate for small/medium sized samples and should not be used in this case.</p>


<h3>References</h3>

<p>Hegde et al. (2006) Perturbation-Based Eigenvector Updates for On-Line Principal Components Analysis and Canonical Correlation Analysis. <em>Journal of VLSI Signal Processing</em>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+secularRpca">secularRpca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1e3
n0 &lt;- 5e2
d &lt;- 10
x &lt;- matrix(runif(n*d), n, d)
 x &lt;- x %*% diag(sqrt(12*(1:d)))
# The eigenvalues of cov(x) are approximately equal to 1, 2, ..., d
# and the corresponding eigenvectors are approximately equal to 
# the canonical basis of R^d

## Perturbation-based recursive PCA
# Initialization: use factor 1/n0 (princomp) rather 
# than factor 1/(n0-1) (prcomp) in calculations
pca &lt;- princomp(x[1:n0,], center=FALSE)
xbar &lt;- pca$center
pca &lt;- list(values=pca$sdev^2, vectors=pca$loadings) 

for (i in (n0+1):n) {
	xbar &lt;- updateMean(xbar, x[i,], i-1)
	pca &lt;- perturbationRpca(pca$values, pca$vectors, x[i,], 
		i-1, center=xbar) }
</code></pre>

<hr>
<h2 id='secularRpca'>
Recursive PCA Using Secular Equations</h2><span id='topic+secularRpca'></span>

<h3>Description</h3>

<p>The PCA is recursively updated after observation of a new vector
(rank one modification of the covariance matrix). Eigenvalues are computed as roots of a secular equation. Eigenvectors (principal components) are deduced by explicit calculation (Bunch et al., 1978) or approximated with the method of Gu and Eisenstat (1994).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>secularRpca(lambda, U, x, n, f = 1/n, center, tol = 1e-10, reortho = FALSE) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="secularRpca_+3A_lambda">lambda</code></td>
<td>
<p>vector of eigenvalues.</p>
</td></tr> 
<tr><td><code id="secularRpca_+3A_u">U</code></td>
<td>
<p>matrix of eigenvectors (PCs) stored in columns.</p>
</td></tr>
<tr><td><code id="secularRpca_+3A_x">x</code></td>
<td>
<p>new data vector.</p>
</td></tr> 
<tr><td><code id="secularRpca_+3A_n">n</code></td>
<td>
<p>sample size before observing <code>x</code>.</p>
</td></tr>  
<tr><td><code id="secularRpca_+3A_f">f</code></td>
<td>
<p>forgetting factor: a number in (0,1).</p>
</td></tr>
<tr><td><code id="secularRpca_+3A_center">center</code></td>
<td>
<p>centering vector for <code>x</code> (optional).</p>
</td></tr>
<tr><td><code id="secularRpca_+3A_tol">tol</code></td>
<td>
<p>tolerance for the computation of eigenvalues.</p>
</td></tr>  
<tr><td><code id="secularRpca_+3A_reortho">reortho</code></td>
<td>
<p>if FALSE, eigenvectors are explicitly computed using the method of Bunch et al. (1978). If TRUE, they are approximated with the method of Gu and Eisenstat (1994).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The method of secular equations provides accurate eigenvalues in all but pathological cases. On the other hand, the perturbation method implemented by <code><a href="#topic+perturbationRpca">perturbationRpca</a></code> typically runs much faster but is only accurate for a large sample size <code>n</code>. <br />
The default eigendecomposition method is that of Bunch et al. (1978). This algorithm consists in three stages: initial deflation, nonlinear solution of secular equations, and calculation of eigenvectors.  
The calculation of eigenvectors (PCs) is accurate for the first few eigenvectors but loss of accuracy and orthogonality may occur for the next ones. In contrast the method of Gu and Eisenstat (1994) is robust against small errors in the computation of eigenvalues. It provides eigenvectors that may be less accurate than the default method but for which strict orthogonality is guaranteed. <br />
The forgetting factor <code>f</code> can be interpreted as the inverse of the number of observation vectors effectively used in the PCA: the &quot;memory&quot; of the PCA algorithm goes back <code>1/f</code> observations in the past. For larger values of <code>f</code>, the PCA update gives more relative weight to the new data <code>x</code> and less to the current PCA (<code>lambda,U</code>). For nonstationary processes, <code>f</code> should be closer to 1.<br />
Only one of the arguments <code>n</code> and <code>f</code> needs being specified. If it is <code>n</code>, then <code>f</code> is set to <code>1/n</code> by default (usual PCA of sample covariance matrix where all data points have equal weight). If <code>f</code> is specified, its value overrides any eventual specification of <code>n</code>. 
</p>


<h3>Value</h3>

<p>A list with components  
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>updated eigenvalues in decreasing order.</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>updated eigenvectors (PCs).</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bunch, J.R., Nielsen, C.P., and Sorensen, D.C. (1978). Rank-one modification of the symmetric eigenproblem. <em>Numerische Mathematik.</em><br />
Gu, M. and Eisenstat, S.C. (1994). A stable and efficient algorithm for the rank-one modification of the symmetric eigenproblem. <em>SIAM Journal of Matrix Analysis and Applications.</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perturbationRpca">perturbationRpca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Initial data set
n &lt;- 100	
d &lt;- 50
x &lt;- matrix(runif(n*d),n,d)
xbar &lt;- colMeans(x)
pca0 &lt;- eigen(cov(x))

# New observation
newx &lt;- runif(d)

# Recursive PCA with secular equations
xbar &lt;- updateMean(xbar, newx, n)
pca &lt;- secularRpca(pca0$values, pca0$vectors, newx, n, center = xbar)
</code></pre>

<hr>
<h2 id='sgapca'>Stochastic Gradient Ascent PCA</h2><span id='topic+sgapca'></span>

<h3>Description</h3>

<p>Online PCA with the SGA algorithm of Oja (1992).</p>


<h3>Usage</h3>

<pre><code class='language-R'>sgapca(lambda, U, x, gamma, q = length(lambda), center, 
	type = c("exact", "nn"), sort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sgapca_+3A_lambda">lambda</code></td>
<td>
<p>optional vector of eigenvalues.</p>
</td></tr>  
<tr><td><code id="sgapca_+3A_u">U</code></td>
<td>
<p>matrix of eigenvectors (PC) stored in columns.</p>
</td></tr>
<tr><td><code id="sgapca_+3A_x">x</code></td>
<td>
<p>new data vector.</p>
</td></tr>
<tr><td><code id="sgapca_+3A_gamma">gamma</code></td>
<td>
<p>vector of gain parameters.</p>
</td></tr>
<tr><td><code id="sgapca_+3A_q">q</code></td>
<td>
<p>number of eigenvectors to compute.</p>
</td></tr>
<tr><td><code id="sgapca_+3A_center">center</code></td>
<td>
<p>optional centering vector for <code>x</code>.</p>
</td></tr>
<tr><td><code id="sgapca_+3A_type">type</code></td>
<td>
<p>algorithm implementation: &quot;exact&quot; or &quot;nn&quot; (neural network).</p>
</td></tr>
<tr><td><code id="sgapca_+3A_sort">sort</code></td>
<td>
<p>Should the new eigenpairs be sorted?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The gain vector <code>gamma</code> determines the weight placed on the new data in updating each principal component. The first coefficient of <code>gamma</code> corresponds to the first principal component, etc.. It can be specified as a single positive number (which is recycled by the function) or as a vector of length <code>ncol(U)</code>. For larger values of <code>gamma</code>, more weight is placed on <code>x</code> and less on <code>U</code>. A common choice for (the components of) <code>gamma</code> is of the form <code>c/n</code>, with <code>n</code> the sample size and <code>c</code> a suitable positive constant. <br /> 
The Stochastic Gradient Ascent PCA can be implemented exactly or through a neural network. The latter is less accurate but faster.<br />
If <code>sort</code> is TRUE and <code>lambda</code> is not missing, the updated eigenpairs are sorted by decreasing eigenvalue. Otherwise, they are not sorted.  
</p>


<h3>Value</h3>

<p>A list with components  
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>updated eigenvalues or NULL.</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>updated principal components.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Oja (1992). Principal components, Minor components, and linear neural networks. <em>Neural Networks.</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ghapca">ghapca</a></code>, 
<code><a href="#topic+snlpca">snlpca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Initialization
n &lt;- 1e4  # sample size
n0 &lt;- 5e3 # initial sample size
d &lt;- 10   # number of variables
q &lt;- d # number of PC to compute
x &lt;- matrix(runif(n*d), n, d)
x &lt;- x %*% diag(sqrt(12*(1:d)))
# The eigenvalues of x are close to 1, 2, ..., d
# and the corresponding eigenvectors are close to 
# the canonical basis of R^d

## SGA PCA
xbar &lt;- colMeans(x[1:n0,])
pca &lt;- batchpca(x[1:n0,], q, center=xbar, byrow=TRUE)
for (i in (n0+1):n) {
  xbar &lt;- updateMean(xbar, x[i,], i-1)
  pca &lt;- sgapca(pca$values, pca$vectors, x[i,], 2/i, q, xbar)
}
pca
</code></pre>

<hr>
<h2 id='snlpca'>Subspace Network Learning PCA</h2><span id='topic+snlpca'></span>

<h3>Description</h3>

<p>Online PCA with the SNL algorithm of Oja (1992).</p>


<h3>Usage</h3>

<pre><code class='language-R'>snlpca(lambda, U, x, gamma, q = length(lambda), center, 
	type = c("exact", "nn"), sort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="snlpca_+3A_lambda">lambda</code></td>
<td>
<p>optional vector of eigenvalues.</p>
</td></tr>
<tr><td><code id="snlpca_+3A_u">U</code></td>
<td>
<p>matrix of eigenvectors (PC) stored in columns.</p>
</td></tr>
<tr><td><code id="snlpca_+3A_x">x</code></td>
<td>
<p>new data vector.</p>
</td></tr>
<tr><td><code id="snlpca_+3A_gamma">gamma</code></td>
<td>
<p>vector of learning rates.</p>
</td></tr>
<tr><td><code id="snlpca_+3A_q">q</code></td>
<td>
<p>number of eigenvectors to compute.</p>
</td></tr>
<tr><td><code id="snlpca_+3A_center">center</code></td>
<td>
<p>optional centering vector for <code>x</code>.</p>
</td></tr>
<tr><td><code id="snlpca_+3A_type">type</code></td>
<td>
<p>algorithm implementation: &quot;exact&quot; or &quot;nn&quot; (neural network).</p>
</td></tr>
<tr><td><code id="snlpca_+3A_sort">sort</code></td>
<td>
<p>Should the new eigenpairs be sorted?</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>The vector <code>gamma</code> determines the weight placed on the new data in updating each PC. For larger values of <code>gamma</code>, more weight is placed on <code>x</code> and less on <code>U</code>. A common choice is of the form <code>c/n</code>, with <code>n</code> the sample size and <code>c</code> a suitable positive constant. Argument <code>gamma</code> can be specified as a single positive number (common to all PCs) or as a vector of length <code>q</code>. <br />
If <code>sort</code> is TRUE and <code>lambda</code> is not missing, the updated eigenpairs are sorted by decreasing eigenvalue. Otherwise, they are not sorted.</p>


<h3>Value</h3>

<p>A list with components  
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>updated eigenvalues or NULL.</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>updated (rotated) eigenvectors.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The Subspace Network Learning PCA can be implemented exactly or through a neural network. The latter is less accurate but much faster. Unlike the GHA and SGA algorithms, the SNL algorithm does not consistently estimate  principal components. It provides only the linear space spanned by the PCs.
</p>


<h3>References</h3>

<p>Oja (1992). Principal components, Minor components, and linear neural networks. <em>Neural Networks.</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ghapca">ghapca</a>, <a href="#topic+sgapca">sgapca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Initialization
n &lt;- 1e4  # sample size
n0 &lt;- 5e3 # initial sample size
d &lt;- 10   # number of variables
q &lt;- d # number of PC to compute
x &lt;- matrix(runif(n*d), n, d)
x &lt;- x %*% diag(sqrt(12*(1:d)))
# The eigenvalues of x are close to 1, 2, ..., d
# and the corresponding eigenvectors are close to 
# the canonical basis of R^d

## SNL PCA
xbar &lt;- colMeans(x[1:n0,])
pca &lt;- batchpca(x[1:n0,], q, center=xbar, byrow=TRUE)
for (i in (n0+1):n) {
  xbar &lt;- updateMean(xbar, x[i,], i-1)
  pca &lt;- snlpca(pca$values, pca$vectors, x[i,], 1/i, q, xbar)
}
</code></pre>

<hr>
<h2 id='updateCovariance'>
Update the Sample Covariance Matrix</h2><span id='topic+updateCovariance'></span>

<h3>Description</h3>

<p>This function recursively updates a covariance matrix without entirely recomputing it when new observations arrive.</p>


<h3>Usage</h3>

<pre><code class='language-R'>updateCovariance(C, x, n, xbar, f, byrow = TRUE) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="updateCovariance_+3A_c">C</code></td>
<td>
<p>covariance matrix.</p>
</td></tr>
<tr><td><code id="updateCovariance_+3A_x">x</code></td>
<td>
<p>vector/matrix of new data.</p>
</td></tr>
<tr><td><code id="updateCovariance_+3A_n">n</code></td>
<td>
<p>sample size before observing <code>x</code>.</p>
</td></tr>
<tr><td><code id="updateCovariance_+3A_xbar">xbar</code></td>
<td>
<p>mean vector before observing <code>x</code>.</p>
</td></tr>
<tr><td><code id="updateCovariance_+3A_f">f</code></td>
<td>
<p>forgetting factor: a number beween 0 and 1.</p>
</td></tr>
<tr><td><code id="updateCovariance_+3A_byrow">byrow</code></td>
<td>
<p>Are the observation vectors in <code>x</code> stored in rows?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The forgetting factor <code>f</code> determines the balance between past and present observations in the PCA update: the closer it is to 1 (resp. to 0), the more weight is placed on current (resp. past) observations. At least one of the arguments <code>n</code> and <code>f</code> must be specified. If <code>f</code> is specified, its value overrides the argument <code>n</code>. The default <code>f=1/n</code> corresponds to a stationnary observation process.<br />
The argument <code>byrow</code> should be set to TRUE (default value) if the data vectors in <code>x</code> are stored in rows and to FALSE if they are stored in columns. The function automatically handles the case where <code>x</code> is a single vector.</p>


<h3>Value</h3>

<p>The updated covariance matrix. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+updateMean">updateMean</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1e4
n0 &lt;- 5e3
d &lt;- 10
x &lt;- matrix(runif(n*d), n, d)

## Direct computation of the covariance
C &lt;- cov(x)

## Recursive computation of the covariance
xbar0 &lt;- colMeans(x[1:n0,])
C0 &lt;- cov(x[1:n0,])
Crec &lt;- updateCovariance(C0, x[(n0+1):n,], n0, xbar0)

## Check equality
all.equal(C, Crec)
</code></pre>

<hr>
<h2 id='updateMean'>
Update the Sample Mean Vector
</h2><span id='topic+updateMean'></span>

<h3>Description</h3>

<p>Recursive update of the sample mean vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>updateMean(xbar, x, n, f, byrow = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="updateMean_+3A_xbar">xbar</code></td>
<td>
<p>current mean vector.</p>
</td></tr>
<tr><td><code id="updateMean_+3A_x">x</code></td>
<td>
<p>vector or matrix of new data.</p>
</td></tr>
<tr><td><code id="updateMean_+3A_n">n</code></td>
<td>
<p>sample size before observing <code>x</code>.</p>
</td></tr>
<tr><td><code id="updateMean_+3A_f">f</code></td>
<td>
<p>forgetting factor: a number in (0,1).</p>
</td></tr>
<tr><td><code id="updateMean_+3A_byrow">byrow</code></td>
<td>
<p>Are the observation vectors in <code>x</code> stored in rows (TRUE) or in columns (FALSE)?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The forgetting factor <code>f</code> determines the balance between past and present observations in the PCA update: the closer it is to 1 (resp. to 0), the more weight is placed on current (resp. past) observations. At least one of the arguments <code>n</code> and <code>f</code> must be specified. If <code>f</code> is specified, its value overrides the argument <code>n</code>. For a given argument <code>n</code>, the default value of<code>f</code> is<code class="reqn">k/(n+k)</code>, with <code class="reqn">k</code> the number of new vector observations. This corresponds to a stationnary observation process.<br />
</p>


<h3>Value</h3>

<p>The updated mean vector. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+updateCovariance">updateCovariance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1e4
n0 &lt;- 5e3
d &lt;- 10
x &lt;- matrix(runif(n*d), n, d)

## Direct computation
xbar1 &lt;- colMeans(x)

## Recursive computation
xbar2 &lt;- colMeans(x[1:n0,])
xbar2 &lt;- updateMean(xbar2, x[(n0+1):n,], n0)

## Check equality
all.equal(xbar1, xbar2)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
