<!DOCTYPE html><html lang="en"><head><title>Help for package learner</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {learner}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cv.learner'><p>Cross-validation for LEARNER</p></a></li>
<li><a href='#dat_highsim'><p>Simulated data set: High similarity in the latent spaces</p></a></li>
<li><a href='#dat_modsim'><p>Simulated data set: Moderate similarity in the latent spaces</p></a></li>
<li><a href='#dlearner'><p>Latent space-based transfer learning</p></a></li>
<li><a href='#learner'><p>Latent space-based transfer learning</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Latent Space-Based Transfer Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Sean McGrath &lt;sean.mcgrath514@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements transfer learning methods for low-rank matrix 
    estimation. These methods leverage similarity in the latent row and column 
    spaces between the source and target populations to improve estimation in 
    the target population. The methods include the LatEnt spAce-based tRaNsfer 
    lEaRning (LEARNER) method and the direct projection LEARNER (D-LEARNER) 
    method described by McGrath et al. (2024) &lt;<a href="https://doi.org/10.48550%2FarXiv.2412.20605">doi:10.48550/arXiv.2412.20605</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/stmcg/learner">https://github.com/stmcg/learner</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/stmcg/learner/issues">https://github.com/stmcg/learner/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>ScreeNOT, Rcpp (&ge; 1.0.11), RcppEigen</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-02 22:23:44 UTC; Sean</td>
</tr>
<tr>
<td>Author:</td>
<td>Sean McGrath <a href="https://orcid.org/0000-0002-7281-3516"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Ryan ODea [aut],
  Cenhao Zhu [aut],
  Rui Duan [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-03 10:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cv.learner'>Cross-validation for LEARNER</h2><span id='topic+cv.learner'></span>

<h3>Description</h3>

<p>This function performs k-fold cross-validation to select the nuisance parameters <code class="reqn">(\lambda_1, \lambda_2)</code> for <code><a href="#topic+learner">learner</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.learner(
  Y_source,
  Y_target,
  r,
  lambda_1_all,
  lambda_2_all,
  step_size,
  n_folds = 4,
  n_cores = 1,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.learner_+3A_y_source">Y_source</code></td>
<td>
<p>matrix containing the source population data, as in <code><a href="#topic+learner">learner</a></code></p>
</td></tr>
<tr><td><code id="cv.learner_+3A_y_target">Y_target</code></td>
<td>
<p>matrix containing the target population data, as in <code><a href="#topic+learner">learner</a></code></p>
</td></tr>
<tr><td><code id="cv.learner_+3A_r">r</code></td>
<td>
<p>(optional) integer specifying the rank of the knowledge graphs, as in <code><a href="#topic+learner">learner</a></code></p>
</td></tr>
<tr><td><code id="cv.learner_+3A_lambda_1_all">lambda_1_all</code></td>
<td>
<p>vector of numerics specifying the candidate values of <code class="reqn">\lambda_1</code> (see Details)</p>
</td></tr>
<tr><td><code id="cv.learner_+3A_lambda_2_all">lambda_2_all</code></td>
<td>
<p>vector of numerics specifying the candidate values of <code class="reqn">\lambda_2</code> (see Details)</p>
</td></tr>
<tr><td><code id="cv.learner_+3A_step_size">step_size</code></td>
<td>
<p>numeric scalar specifying the step size for the Newton steps in the numerical optimization algorithm, as in <code><a href="#topic+learner">learner</a></code></p>
</td></tr>
<tr><td><code id="cv.learner_+3A_n_folds">n_folds</code></td>
<td>
<p>an integer specify the number of cross-validation folds. The default is <code>4</code>.</p>
</td></tr>
<tr><td><code id="cv.learner_+3A_n_cores">n_cores</code></td>
<td>
<p>an integer specifying the number of CPU cores in OpenMP parallelization. Parallelization is performed across the different candidate <code class="reqn">(\lambda_1, \lambda_2)</code> pairs. The default is <code>1</code>, i.e., no parallelization.</p>
</td></tr>
<tr><td><code id="cv.learner_+3A_control">control</code></td>
<td>
<p>a list of parameters for controlling the stopping criteria for the numerical optimization algorithm, as in <code><a href="#topic+learner">learner</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given sets of candidate values of <code class="reqn">\lambda_1</code> and <code class="reqn">\lambda_2</code>, this function performs k-fold cross-validation to select the pair <code class="reqn">(\lambda_1, \lambda_2)</code> with the smallest held out error. This function randomly partitions the entries of <code>Y_target</code> into <code class="reqn">k</code> (approximately) equally sized subsamples. The training data sets are obtained by removing one of the <code class="reqn">k</code> subsamples and the corresponding test data sets are based on the held out subsamples. The <code><a href="#topic+learner">learner</a></code> function is applied to each training data set. The held out error is computed by the mean squared error comparing the entries in the test data sets with those imputed from the LEARNER estimates. See McGrath et al. (2024) for further details.
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table role = "presentation">
<tr><td><code>lambda_1_min</code></td>
<td>
<p>value of <code class="reqn">\lambda_1</code> with the smallest MSE</p>
</td></tr>
<tr><td><code>lambda_2_min</code></td>
<td>
<p>value of <code class="reqn">\lambda_2</code> with the smallest MSE</p>
</td></tr>
<tr><td><code>mse_all</code></td>
<td>
<p>matrix containing MSE value for each <code class="reqn">(\lambda_1, \lambda_2)</code> pair. The rows correspond to the <code class="reqn">\lambda_1</code> values, and the columns correspond to the <code class="reqn">\lambda_2</code> values.</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>rank value used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>McGrath, S., Zhu, C,. Guo, M. and Duan, R. (2024). <em>LEARNER: A transfer learning method for low-rank matrix estimation</em>. arXiv preprint	arXiv:2412.20605.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>res &lt;- cv.learner(Y_source = dat_highsim$Y_source,
                  Y_target = dat_highsim$Y_target,
                  lambda_1_all = c(1, 10, 100),
                  lambda_2_all = c(1, 10, 100),
                  step_size = 0.003)



</code></pre>

<hr>
<h2 id='dat_highsim'>Simulated data set: High similarity in the latent spaces</h2><span id='topic+dat_highsim'></span>

<h3>Description</h3>

<p>This data set contains simulated data in the source and target populations where there is a high degree of similarity in the underlying latent spaces between these populations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dat_highsim
</code></pre>


<h3>Format</h3>

<p>A list containing the observed and true matrices in the source and target populations. The list contains the following components:
</p>

<dl>
<dt><code>Y_source</code></dt><dd><p>A matrix of size <code class="reqn">100 \times 50</code> representing the observed source population matrix.</p>
</dd>
<dt><code>Y_target</code></dt><dd><p>A matrix of size <code class="reqn">100 \times 50</code> representing the observed target population matrix.</p>
</dd>
<dt><code>Theta_source</code></dt><dd><p>A matrix of size <code class="reqn">100 \times 50</code> (rank 3) representing the true source population matrix.</p>
</dd>
<dt><code>Theta_target</code></dt><dd><p>A matrix of size <code class="reqn">100 \times 50</code> (rank 3) representing the true target population matrix.</p>
</dd>
</dl>



<h3>Details</h3>

<p>In this data set, there is a high degree of similarity in the latent spaces between the source and target populations. Specifically, the true source population matrix was obtained by reversing the order of the singular values of the true target population matrix.
The observed target population matrix was obtained by adding independent and identically distributed noise to the entries of the true source population matrix. The noise was generated from a normal distribution with mean 0 and standard deviation of 1. The observed source population matrix was generated analogously, where the noise had a standard deviation of 0.5.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dat_modsim">dat_modsim</a></code>
</p>

<hr>
<h2 id='dat_modsim'>Simulated data set: Moderate similarity in the latent spaces</h2><span id='topic+dat_modsim'></span>

<h3>Description</h3>

<p>This data set contains simulated data in the source and target populations where there is a moderate degree of similarity in the underlying latent spaces between these populations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dat_modsim
</code></pre>


<h3>Format</h3>

<p>A list containing the observed and true matrices in the source and target populations. The list contains the following components:
</p>

<dl>
<dt><code>Y_source</code></dt><dd><p>A matrix of size <code class="reqn">100 \times 50</code> representing the observed source population matrix.</p>
</dd>
<dt><code>Y_target</code></dt><dd><p>A matrix of size <code class="reqn">100 \times 50</code> representing the observed target population matrix.</p>
</dd>
<dt><code>Theta_source</code></dt><dd><p>A matrix of size <code class="reqn">100 \times 50</code> (rank 3) representing the true source population matrix.</p>
</dd>
<dt><code>Theta_target</code></dt><dd><p>A matrix of size <code class="reqn">100 \times 50</code> (rank 3) representing the true target population matrix.</p>
</dd>
</dl>



<h3>Details</h3>

<p>In this data set, there is a moderate degree of similarity in the latent spaces between the source and target populations. Specifically, the true source population matrix was obtained by (i) reversing the order of the singular values of the true target population matrix and (ii) adding perturbations to the left and right singular vectors of the true target population matrix.
The observed target population matrix was obtained by adding independent and identically distributed noise to the entries of the true source population matrix. The noise was generated from a normal distribution with mean 0 and standard deviation of 1. The observed source population matrix was generated analogously, where the noise had a standard deviation of 0.5.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dat_modsim">dat_modsim</a></code>
</p>

<hr>
<h2 id='dlearner'>Latent space-based transfer learning</h2><span id='topic+dlearner'></span>

<h3>Description</h3>

<p>This function applies the Direct project LatEnt spAce-based tRaNsfer lEaRning (D-LEARNER) method (McGrath et al. 2024) to leverage data from a source population to improve
estimation of a low rank matrix in an underrepresented target population.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlearner(Y_source, Y_target, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dlearner_+3A_y_source">Y_source</code></td>
<td>
<p>matrix containing the source population data</p>
</td></tr>
<tr><td><code id="dlearner_+3A_y_target">Y_target</code></td>
<td>
<p>matrix containing the target population data</p>
</td></tr>
<tr><td><code id="dlearner_+3A_r">r</code></td>
<td>
<p>(optional) integer specifying the rank of the knowledge graphs. By default, ScreeNOT (Donoho et al. 2023) is applied to the source population knowledge graph to select the rank.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Data and notation:</strong>
</p>
<p>The data consists of a matrix in the target population <code class="reqn">Y_0 \in \mathbb{R}^{p \times q}</code> and the source population <code class="reqn">Y_1 \in \mathbb{R}^{p \times q}</code>.
Let <code class="reqn">\hat{U}_{k} \hat{\Lambda}_{k} \hat{V}_{k}^{\top}</code> denote the truncated singular value decomposition (SVD) of <code class="reqn">Y_k</code>, <code class="reqn">k = 0, 1</code>.
</p>
<p>For <code class="reqn">k = 0, 1</code>, one can view <code class="reqn">Y_k</code> as a noisy version of <code class="reqn">\Theta_k</code>, referred to as the knowledge graph. The target of inference is the target population knowledge graph, <code class="reqn">\Theta_0</code>.
</p>
<p><strong>Estimation:</strong>
</p>
<p>This method estimates <code class="reqn">\Theta_0</code> by <code class="reqn">\hat{U}_{1}^{\top}\hat{U}_{1} Y_0 \hat{V}_{1}^{\top}\hat{V}_{1}</code>.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table role = "presentation">
<tr><td><code>dlearner_estimate</code></td>
<td>
<p>matrix containing the D-LEARNER estimate of the target population knowledge graph.</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>rank value used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Donoho, D., Gavish, M. and Romanov, E. (2023). <em>ScreeNOT: Exact MSE-optimal singular value thresholding in correlated noise</em>. The Annals of Statistics, 51(1), pp.122-148.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>res &lt;- dlearner(Y_source = dat_highsim$Y_source,
                Y_target = dat_highsim$Y_target)

</code></pre>

<hr>
<h2 id='learner'>Latent space-based transfer learning</h2><span id='topic+learner'></span>

<h3>Description</h3>

<p>This function applies the LatEnt spAce-based tRaNsfer lEaRning (LEARNER) method (McGrath et al. 2024) to leverage data from a source population to improve
estimation of a low rank matrix in an underrepresented target population.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learner(Y_source, Y_target, r, lambda_1, lambda_2, step_size, control = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="learner_+3A_y_source">Y_source</code></td>
<td>
<p>matrix containing the source population data</p>
</td></tr>
<tr><td><code id="learner_+3A_y_target">Y_target</code></td>
<td>
<p>matrix containing the target population data</p>
</td></tr>
<tr><td><code id="learner_+3A_r">r</code></td>
<td>
<p>(optional) integer specifying the rank of the knowledge graphs. By default, ScreeNOT (Donoho et al. 2023) is applied to the source population knowledge graph to select the rank.</p>
</td></tr>
<tr><td><code id="learner_+3A_lambda_1">lambda_1</code></td>
<td>
<p>numeric scalar specifying the value of <code class="reqn">\lambda_1</code> (see Details)</p>
</td></tr>
<tr><td><code id="learner_+3A_lambda_2">lambda_2</code></td>
<td>
<p>numeric scalar specifying the value of <code class="reqn">\lambda_2</code> (see Details)</p>
</td></tr>
<tr><td><code id="learner_+3A_step_size">step_size</code></td>
<td>
<p>numeric scalar specifying the step size for the Newton steps in the numerical optimization algorithm</p>
</td></tr>
<tr><td><code id="learner_+3A_control">control</code></td>
<td>
<p>a list of parameters for controlling the stopping criteria for the numerical optimization algorithm. The list may include the following components:
</p>

<table>
<tr>
 <td style="text-align: left;">
<code>max_iter</code> </td><td style="text-align: left;"> integer specifying the maximum number of iterations </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>threshold</code> </td><td style="text-align: left;"> numeric scalar specifying a convergence threshold. The algorithm converges when <code class="reqn">|\epsilon_t - \epsilon_{t-1}| &lt; </code><code>threshold</code>, where <code class="reqn">\epsilon_t</code> denotes the value of the objective function at iteration <code class="reqn">t</code>. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>max_value</code> </td><td style="text-align: left;"> numeric scalar used to specify the maximum value of the objective function allowed before terminating the algorithm. Specifically, the algorithm will terminate if the value of the objective function exceeds <code>max_value</code><code class="reqn">\times \epsilon_0</code>, where <code class="reqn">\epsilon_0</code> denotes the value of the objective function at the initial point. This is used to prevent unnecessary computation time after the optimization algorithm diverges. </td>
</tr>

</table>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Data and notation:</strong>
</p>
<p>The data consists of a matrix in the target population <code class="reqn">Y_0 \in \mathbb{R}^{p \times q}</code> and the source population <code class="reqn">Y_1 \in \mathbb{R}^{p \times q}</code>.
Let <code class="reqn">\hat{U}_{k} \hat{\Lambda}_{k} \hat{V}_{k}^{\top}</code> denote the truncated singular value decomposition (SVD) of <code class="reqn">Y_k</code>, <code class="reqn">k = 0, 1</code>.
</p>
<p>For <code class="reqn">k = 0, 1</code>, one can view <code class="reqn">Y_k</code> as a noisy version of <code class="reqn">\Theta_k</code>, referred to as the knowledge graph. The target of inference is the target population knowledge graph, <code class="reqn">\Theta_0</code>.
</p>
<p><strong>Estimation:</strong>
</p>
<p>This method estimates <code class="reqn">\Theta_0</code> by <code class="reqn">\tilde{U}\tilde{V}^{\top}</code>, where <code class="reqn">(\tilde{U}, \tilde{V})</code> is the solution to the following optimization problem
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{arg\,min}_{U \in \mathbb{R}^{p \times r}, V \in \mathbb{R}^{q \times r}} \big\{ \| U V^{\top} - Y_0 \|_F^2 + \lambda_1\| \mathcal{P}_{\perp}(\hat{U}_{1})U \|_F^2 + \lambda_1\|  \mathcal{P}_{\perp}(\hat{V}_{1})V \|_F^2  + \lambda_2 \| U^{\top} U - V^{\top} V \|_F^2 \big\}</code>
</p>

<p>where <code class="reqn">\mathcal{P}_{\perp}(\hat{U}_{1}) = I - \hat{U}_{1}^{\top}\hat{U}_{1}</code> and <code class="reqn">\mathcal{P}_{\perp}(\hat{V}_{1}) = I - \hat{V}_{1}^{\top}\hat{V}_{1}</code>.
</p>
<p>This function uses an alternating minimization strategy to solve the optimization problem. That is, this approach updates <code class="reqn">U</code> by minimizing the objective function (via a gradient descent step) treating <code class="reqn">V</code> as fixed. Then, <code class="reqn">V</code> is updated treating <code class="reqn">U</code> as fixed. These updates of <code class="reqn">U</code> and <code class="reqn">V</code> are repeated until convergence.
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table role = "presentation">
<tr><td><code>learner_estimate</code></td>
<td>
<p>matrix containing the LEARNER estimate of the target population knowledge graph</p>
</td></tr>
<tr><td><code>objective_values</code></td>
<td>
<p>numeric vector containing the values of the objective function at each iteration</p>
</td></tr>
<tr><td><code>convergence_criterion</code></td>
<td>
<p>integer specifying the criterion that was satisfied for terminating the numerical optimization algorithm. A value of 1 indicates the convergence threshold was satisfied; A value of 2 indicates that the maximum number of iterations was satisfied; A value of 3 indicates that the maximum value of the objective function was satisfied.</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>rank value used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>McGrath, S., Zhu, C,. Guo, M. and Duan, R. (2024). <em>LEARNER: A transfer learning method for low-rank matrix estimation</em>. arXiv preprint	arXiv:2412.20605.
</p>
<p>Donoho, D., Gavish, M. and Romanov, E. (2023). <em>ScreeNOT: Exact MSE-optimal singular value thresholding in correlated noise</em>. The Annals of Statistics, 51(1), pp.122-148.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>res &lt;- learner(Y_source = dat_highsim$Y_source,
               Y_target = dat_highsim$Y_target,
               lambda_1 = 1, lambda_2 = 1,
               step_size = 0.003)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
