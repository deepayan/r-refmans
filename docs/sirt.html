<!DOCTYPE html><html><head><title>Help for package sirt</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sirt}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#sirt-package'>
<p>Supplementary Item Response Theory Models</p></a></li>
<li><a href='#automatic.recode'>
<p>Automatic Method of Finding Keys in a Dataset with Raw Item Responses</p></a></li>
<li><a href='#brm-Methods'>
<p>Functions for the Beta Item Response Model</p></a></li>
<li><a href='#btm'>
<p>Extended Bradley-Terry Model</p></a></li>
<li><a href='#categorize'>
<p>Categorize and Decategorize Variables in a Data Frame</p></a></li>
<li><a href='#ccov.np'>
<p>Nonparametric Estimation of Conditional Covariances of Item Pairs</p></a></li>
<li><a href='#cfa_meas_inv'>
<p>Estimation of a Unidimensional Factor Model under Full and Partial</p>
Measurement Invariance</a></li>
<li><a href='#class.accuracy.rasch'>
<p>Classification Accuracy in the Rasch Model</p></a></li>
<li><a href='#conf.detect'>
<p>Confirmatory DETECT and polyDETECT Analysis</p></a></li>
<li><a href='#data.activity.itempars'>
<p>Item Parameters Cultural Activities</p></a></li>
<li><a href='#data.befki'>
<p>BEFKI Dataset (Schroeders, Schipolowski, &amp; Wilhelm, 2015)</p></a></li>
<li><a href='#data.big5'>
<p>Dataset Big 5 from <span class="pkg">qgraph</span> Package</p></a></li>
<li><a href='#data.bs'>
<p>Datasets from Borg and Staufenbiel (2007)</p></a></li>
<li><a href='#data.eid'>
<p>Examples with Datasets from Eid and Schmidt (2014)</p></a></li>
<li><a href='#data.ess2005'>
<p>Dataset European Social Survey 2005</p></a></li>
<li><a href='#data.g308'>
<p>C-Test Datasets</p></a></li>
<li><a href='#data.inv4gr'>
<p>Dataset for Invariance Testing with 4 Groups</p></a></li>
<li><a href='#data.liking.science'>
<p>Dataset 'Liking For Science'</p></a></li>
<li><a href='#data.long'>
<p>Longitudinal Dataset</p></a></li>
<li><a href='#data.lsem'>
<p>Datasets for Local Structural Equation Models / Moderated Factor Analysis</p></a></li>
<li><a href='#data.math'>
<p>Dataset Mathematics</p></a></li>
<li><a href='#data.mcdonald'>
<p>Some Datasets from McDonald's <em>Test Theory</em> Book</p></a></li>
<li><a href='#data.mixed1'>
<p>Dataset with Mixed Dichotomous and Polytomous Item Responses</p></a></li>
<li><a href='#data.ml'>
<p>Multilevel Datasets</p></a></li>
<li><a href='#data.noharm'>
<p>Datasets for NOHARM Analysis</p></a></li>
<li><a href='#data.pars1.rasch'>
<p>Item Parameters for Three Studies Obtained by 1PL and 2PL Estimation</p></a></li>
<li><a href='#data.pirlsmissing'>
<p>Dataset from PIRLS Study with Missing Responses</p></a></li>
<li><a href='#data.pisaMath'>
<p>Dataset PISA Mathematics</p></a></li>
<li><a href='#data.pisaPars'>
<p>Item Parameters from Two PISA Studies</p></a></li>
<li><a href='#data.pisaRead'>
<p>Dataset PISA Reading</p></a></li>
<li><a href='#data.pw'>
<p>Datasets for Pairwise Comparisons</p></a></li>
<li><a href='#data.ratings'>
<p>Rating Datasets</p></a></li>
<li><a href='#data.raw1'>
<p>Dataset with Raw Item Responses</p></a></li>
<li><a href='#data.read'>
<p>Dataset Reading</p></a></li>
<li><a href='#data.reck'>
<p>Datasets from Reckase' Book <em>Multidimensional Item Response Theory</em></p></a></li>
<li><a href='#data.sirt'>
<p>Some Example Datasets for the <code>sirt</code> Package</p></a></li>
<li><a href='#data.timss'>
<p>Dataset TIMSS Mathematics</p></a></li>
<li><a href='#data.timss07.G8.RUS'>
<p>TIMSS 2007 Grade 8 Mathematics and Science Russia</p></a></li>
<li><a href='#data.trees'>
<p>Dataset Used in Stoyan, Pommerening and Wuensche (2018)</p></a></li>
<li><a href='#data.wide2long'>
<p>Converting a Data Frame from Wide Format in a Long Format</p></a></li>
<li><a href='#detect.index'>
<p>Calculation of the DETECT and polyDETECT Index</p></a></li>
<li><a href='#dif.logistic.regression'>
<p>Differential Item Functioning using Logistic Regression Analysis</p></a></li>
<li><a href='#dif.strata.variance'>
<p>Stratified DIF Variance</p></a></li>
<li><a href='#dif.variance'>
<p>DIF Variance</p></a></li>
<li><a href='#dirichlet.mle'>
<p>Maximum Likelihood Estimation of the Dirichlet Distribution</p></a></li>
<li><a href='#dirichlet.simul'>
<p>Simulation of a Dirichlet Distributed Vectors</p></a></li>
<li><a href='#dmlavaan'>
<p>Comparing Regression Parameters of Different lavaan Models Fitted to the</p>
Same Dataset</a></li>
<li><a href='#eigenvalues.manymatrices'>
<p>Computation of Eigenvalues of Many Symmetric Matrices</p></a></li>
<li><a href='#equating.rasch'>
<p>Equating in the Generalized Logistic Rasch Model</p></a></li>
<li><a href='#equating.rasch.jackknife'>
<p>Jackknife Equating Error in Generalized Logistic Rasch Model</p></a></li>
<li><a href='#expl.detect'>
<p>Exploratory DETECT Analysis</p></a></li>
<li><a href='#f1d.irt'>
<p>Functional Unidimensional Item Response Model</p></a></li>
<li><a href='#fit.isop'>
<p>Fitting the ISOP and ADISOP Model for Frequency Tables</p></a></li>
<li><a href='#fuzcluster'>
<p>Clustering for Continuous Fuzzy Data</p></a></li>
<li><a href='#fuzdiscr'>
<p>Estimation of a Discrete Distribution for Fuzzy Data (Data in Belief Function</p>
Framework)</a></li>
<li><a href='#gom.em'>
<p>Discrete (Rasch) Grade of Membership Model</p></a></li>
<li><a href='#gom.jml'>
<p>Grade of Membership Model (Joint Maximum Likelihood Estimation)</p></a></li>
<li><a href='#greenyang.reliability'>
<p>Reliability for Dichotomous Item Response Data</p>
Using the Method of Green and Yang (2009)</a></li>
<li><a href='#invariance.alignment'>
<p>Alignment Procedure for Linking under Approximate Invariance</p></a></li>
<li><a href='#IRT.mle'>
<p>Person Parameter Estimation</p></a></li>
<li><a href='#isop'>
<p>Fit Unidimensional ISOP and ADISOP Model to Dichotomous</p>
and Polytomous Item Responses</a></li>
<li><a href='#isop.scoring'>
<p>Scoring Persons and Items in the ISOP Model</p></a></li>
<li><a href='#isop.test'>
<p>Testing the ISOP Model</p></a></li>
<li><a href='#latent.regression.em.raschtype'>
<p>Latent Regression Model for the Generalized</p>
Logistic Item Response Model and the Linear Model for Normal Responses</a></li>
<li><a href='#lavaan2mirt'>
<p>Converting a <code>lavaan</code> Model into a <code>mirt</code> Model</p></a></li>
<li><a href='#lc.2raters'>
<p>Latent Class Model for Two Exchangeable Raters and One Item</p></a></li>
<li><a href='#likelihood.adjustment'>
<p>Adjustment and Approximation of Individual Likelihood Functions</p></a></li>
<li><a href='#linking.haberman'>
<p>Linking in the 2PL/Generalized Partial Credit Model</p></a></li>
<li><a href='#linking.haebara'>
<p>Haebara Linking of the 2PL Model for Multiple Studies</p></a></li>
<li><a href='#linking.robust'>
<p>Robust Linking of Item Intercepts</p></a></li>
<li><a href='#lq_fit'>
<p>Fit of a <code class="reqn">L_q</code> Regression Model</p></a></li>
<li><a href='#lsdm'>
<p>Least Squares Distance Method of Cognitive Validation</p></a></li>
<li><a href='#lsem.estimate'>
<p>Local Structural Equation Models (LSEM)</p></a></li>
<li><a href='#lsem.permutationTest'>
<p>Permutation Test for a Local Structural Equation Model</p></a></li>
<li><a href='#lsem.test'>
<p>Test a Local Structural Equation Model Based on Bootstrap</p></a></li>
<li><a href='#marginal.truescore.reliability'>
<p>True-Score Reliability for Dichotomous Data</p></a></li>
<li><a href='#matrixfunctions.sirt'>
<p>Some Matrix Functions</p></a></li>
<li><a href='#mcmc_coef'>
<p>Some Methods for Objects of Class <code>mcmc.list</code></p></a></li>
<li><a href='#mcmc_Rhat'>
<p>Computation of the Rhat Statistic from a Single MCMC Chain</p></a></li>
<li><a href='#mcmc.2pno'>
<p>MCMC Estimation of the Two-Parameter Normal Ogive Item Response Model</p></a></li>
<li><a href='#mcmc.2pno.ml'>
<p>Random Item Response Model / Multilevel IRT Model</p></a></li>
<li><a href='#mcmc.2pnoh'>
<p>MCMC Estimation of the Hierarchical IRT Model for Criterion-Referenced</p>
Measurement</a></li>
<li><a href='#mcmc.3pno.testlet'>
<p>3PNO Testlet Model</p></a></li>
<li><a href='#mcmc.list.descriptives'>
<p>Computation of Descriptive Statistics for a <code>mcmc.list</code> Object</p></a></li>
<li><a href='#mcmclist2coda'>
<p>Write Coda File from an Object of Class <code>mcmc.list</code></p></a></li>
<li><a href='#md.pattern.sirt'>
<p>Response Pattern in a Binary Matrix</p></a></li>
<li><a href='#mgsem'>
<p>Estimation of Multiple-Group Structural Equation Models</p></a></li>
<li><a href='#mirt.specify.partable'>
<p>Specify or modify a Parameter Table in <span class="pkg">mirt</span></p></a></li>
<li><a href='#mirt.wrapper'>
<p>Some Functions for Wrapping with the <span class="pkg">mirt</span> Package</p></a></li>
<li><a href='#mle.pcm.group'>
<p>Maximum Likelihood Estimation of Person or Group Parameters</p>
in the Generalized Partial Credit Model</a></li>
<li><a href='#modelfit.sirt'>
<p>Assessing Model Fit and Local Dependence by Comparing Observed and Expected</p>
Item Pair Correlations</a></li>
<li><a href='#monoreg.rowwise'>
<p>Monotone Regression for Rows or Columns in a Matrix</p></a></li>
<li><a href='#nedelsky-methods'>
<p>Functions for the Nedelsky Model</p></a></li>
<li><a href='#noharm.sirt'>
<p>NOHARM Model in <span class="rlang"><b>R</b></span></p></a></li>
<li><a href='#np.dich'>
<p>Nonparametric Estimation of Item Response Functions</p></a></li>
<li><a href='#parmsummary_extend'>
<p>Includes Confidence Interval in Parameter Summary Table</p></a></li>
<li><a href='#pbivnorm2'>
<p>Cumulative Function for the Bivariate Normal Distribution</p></a></li>
<li><a href='#pcm.conversion'>
<p>Conversion of the Parameterization of the Partial Credit Model</p></a></li>
<li><a href='#pcm.fit'>
<p>Item and Person Fit Statistics for the Partial Credit Model</p></a></li>
<li><a href='#person.parameter.rasch.copula'>
<p>Person Parameter Estimation of the Rasch Copula Model (Braeken, 2011)</p></a></li>
<li><a href='#personfit.stat'>
<p>Person Fit Statistics for the Rasch Model</p></a></li>
<li><a href='#pgenlogis'>
<p>Calculation of Probabilities and Moments for the</p>
Generalized Logistic Item Response Model</a></li>
<li><a href='#plausible.value.imputation.raschtype'>
<p>Plausible Value Imputation in Generalized Logistic Item</p>
Response Model</a></li>
<li><a href='#plot.mcmc.sirt'>
<p>Plot Function for Objects of Class <code>mcmc.sirt</code></p></a></li>
<li><a href='#plot.np.dich'>
<p>Plot Method for Object of Class <code>np.dich</code></p></a></li>
<li><a href='#polychoric2'>
<p>Polychoric Correlation</p></a></li>
<li><a href='#prior_model_parse'>
<p>Parsing a Prior Model</p></a></li>
<li><a href='#prmse.subscores.scales'>
<p>Proportional Reduction of Mean Squared</p>
Error (PRMSE) for Subscale Scores</a></li>
<li><a href='#prob.guttman'>
<p>Probabilistic Guttman Model</p></a></li>
<li><a href='#Q3'>
<p>Estimation of the <code class="reqn">Q_3</code> Statistic (Yen, 1984)</p></a></li>
<li><a href='#Q3.testlet'>
<p><code class="reqn">Q_3</code> Statistic of Yen (1984) for Testlets</p></a></li>
<li><a href='#qmc.nodes'>
<p>Calculation of Quasi Monte Carlo Integration Points</p></a></li>
<li><a href='#R2conquest'>
<p>Running ConQuest From Within <span class="rlang"><b>R</b></span></p></a></li>
<li><a href='#R2noharm'>
<p>Estimation of a NOHARM Analysis from within <span class="rlang"><b>R</b></span></p></a></li>
<li><a href='#R2noharm.EAP'>
<p>EAP Factor Score Estimation</p></a></li>
<li><a href='#R2noharm.jackknife'>
<p>Jackknife Estimation of NOHARM Analysis</p></a></li>
<li><a href='#rasch.copula2'>
<p>Multidimensional IRT Copula Model</p></a></li>
<li><a href='#rasch.evm.pcm'>
<p>Estimation of the Partial Credit Model using the Eigenvector Method</p></a></li>
<li><a href='#rasch.jml'>
<p>Joint Maximum Likelihood (JML) Estimation of the Rasch Model</p></a></li>
<li><a href='#rasch.jml.biascorr'>
<p>Bias Correction of Item Parameters for Joint Maximum Likelihood Estimation</p>
in the Rasch model</a></li>
<li><a href='#rasch.jml.jackknife1'>
<p>Jackknifing the IRT Model Estimated by Joint Maximum Likelihood (JML)</p></a></li>
<li><a href='#rasch.mirtlc'>
<p>Multidimensional Latent Class 1PL and 2PL Model</p></a></li>
<li><a href='#rasch.mml2'>
<p>Estimation of the Generalized Logistic Item Response Model,</p>
Ramsay's Quotient Model, Nonparametric Item Response Model,
Pseudo-Likelihood Estimation and a Missing Data Item Response Model</a></li>
<li><a href='#rasch.pairwise'>
<p>Pairwise Estimation Method of the Rasch Model</p></a></li>
<li><a href='#rasch.pairwise.itemcluster'>
<p>Pairwise Estimation of the Rasch Model for Locally Dependent Items</p></a></li>
<li><a href='#rasch.pml3'>
<p>Pairwise Marginal Likelihood Estimation for the Probit Rasch Model</p></a></li>
<li><a href='#rasch.prox'>
<p>PROX Estimation Method for the Rasch Model</p></a></li>
<li><a href='#rasch.va'>
<p>Estimation of the Rasch Model with Variational Approximation</p></a></li>
<li><a href='#reliability.nonlinearSEM'>
<p>Estimation of Reliability for Confirmatory Factor Analyses</p>
Based on Dichotomous Data</a></li>
<li><a href='#resp_groupwise'>
<p>Creates Group-Wise Item Response Dataset</p></a></li>
<li><a href='#rinvgamma2'>
<p>Inverse Gamma Distribution in Prior Sample Size Parameterization</p></a></li>
<li><a href='#rm.facets'>
<p>Rater Facets Models with Item/Rater Intercepts and Slopes</p></a></li>
<li><a href='#rm.sdt'>
<p>Hierarchical Rater Model Based on Signal Detection Theory (HRM-SDT)</p></a></li>
<li><a href='#rmvn'>
<p>Simulation of a Multivariate Normal Distribution with Exact Moments</p></a></li>
<li><a href='#scale_group_means'>
<p>Scaling of Group Means and Standard Deviations</p></a></li>
<li><a href='#sia.sirt'>
<p>Statistical Implicative Analysis (SIA)</p></a></li>
<li><a href='#sim.qm.ramsay'>
<p>Simulate from Ramsay's Quotient Model</p></a></li>
<li><a href='#sim.rasch.dep'>
<p>Simulation of the Rasch Model with Locally Dependent Responses</p></a></li>
<li><a href='#sim.raschtype'>
<p>Simulate from Generalized Logistic Item Response Model</p></a></li>
<li><a href='#sirt_eigenvalues'>
<p>First Eigenvalues of a Symmetric Matrix</p></a></li>
<li><a href='#sirt-defunct'><p>Defunct <span class="pkg">sirt</span> Functions</p></a></li>
<li><a href='#sirt-utilities'><p>Utility Functions in <span class="pkg">sirt</span></p></a></li>
<li><a href='#smirt'>
<p>Multidimensional Noncompensatory, Compensatory and Partially</p>
Compensatory Item Response Model</a></li>
<li><a href='#stratified.cronbach.alpha'>
<p>Stratified Cronbach's Alpha</p></a></li>
<li><a href='#summary.mcmc.sirt'>
<p>Summary Method for Objects of Class <code>mcmc.sirt</code></p></a></li>
<li><a href='#tam2mirt'>
<p>Converting a fitted <code>TAM</code> Object into a <code>mirt</code> Object</p></a></li>
<li><a href='#testlet.marginalized'>
<p>Marginal Item Parameters from a Testlet (Bifactor) Model</p></a></li>
<li><a href='#tetrachoric2'>
<p>Tetrachoric Correlation Matrix</p></a></li>
<li><a href='#truescore.irt'>
<p>Conversion of Trait Scores <code class="reqn">\theta</code> into</p>
True Scores <code class="reqn">\tau ( \theta )</code></a></li>
<li><a href='#unidim.test.csn'>
<p>Test for Unidimensionality of CSN</p></a></li>
<li><a href='#wle.rasch'>
<p>Weighted Likelihood Estimation of Person Abilities</p></a></li>
<li><a href='#wle.rasch.jackknife'>
<p>Standard Error Estimation of WLE by Jackknifing</p></a></li>
<li><a href='#xxirt'>
<p>User Defined Item Response Model</p></a></li>
<li><a href='#xxirt_createParTable'>
<p>Create Item Response Functions and Item Parameter Table</p></a></li>
<li><a href='#xxirt_createThetaDistribution'>
<p>Creates a User Defined Theta Distribution</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Supplementary Item Response Theory Models</td>
</tr>
<tr>
<td>Version:</td>
<td>4.1-15</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-06 00:05:40</td>
</tr>
<tr>
<td>Author:</td>
<td>Alexander Robitzsch [aut,cre] (&lt;https://orcid.org/0000-0002-8226-3132&gt;)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Alexander Robitzsch &lt;robitzsch@ipn.uni-kiel.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>
    Supplementary functions for item response models aiming
    to complement existing R packages. The functionality includes among others
    multidimensional compensatory and noncompensatory IRT models
    (Reckase, 2009, &lt;<a href="https://doi.org/10.1007%2F978-0-387-89976-3">doi:10.1007/978-0-387-89976-3</a>&gt;), 
    MCMC for hierarchical IRT models and testlet models
    (Fox, 2010, &lt;<a href="https://doi.org/10.1007%2F978-1-4419-0742-4">doi:10.1007/978-1-4419-0742-4</a>&gt;), 
    NOHARM (McDonald, 1982, &lt;<a href="https://doi.org/10.1177%2F014662168200600402">doi:10.1177/014662168200600402</a>&gt;), 
    Rasch copula model (Braeken, 2011, &lt;<a href="https://doi.org/10.1007%2Fs11336-010-9190-4">doi:10.1007/s11336-010-9190-4</a>&gt;;
    Schroeders, Robitzsch &amp; Schipolowski, 2014, &lt;<a href="https://doi.org/10.1111%2Fjedm.12054">doi:10.1111/jedm.12054</a>&gt;),
    faceted and hierarchical rater models (DeCarlo, Kim &amp; Johnson, 2011,
    &lt;<a href="https://doi.org/10.1111%2Fj.1745-3984.2011.00143.x">doi:10.1111/j.1745-3984.2011.00143.x</a>&gt;),
    ordinal IRT model (ISOP; Scheiblechner, 1995, &lt;<a href="https://doi.org/10.1007%2FBF02301417">doi:10.1007/BF02301417</a>&gt;), 
    DETECT statistic (Stout, Habing, Douglas &amp; Kim, 1996, 
    &lt;<a href="https://doi.org/10.1177%2F014662169602000403">doi:10.1177/014662169602000403</a>&gt;), local structural equation modeling 
    (LSEM; Hildebrandt, Luedtke, Robitzsch, Sommer &amp; Wilhelm, 2016,
    &lt;<a href="https://doi.org/10.1080%2F00273171.2016.1142856">doi:10.1080/00273171.2016.1142856</a>&gt;).</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>Imports:</td>
<td>CDM, graphics, methods, parallel, pbapply, Rcpp, stats, TAM,
utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>coda, igraph, lavaan, MASS, Matrix, miceadds, minqa, mirt,
mvtnorm, nloptr, optimx, pbivnorm, pbv, psych, sfsmisc, sm,
survey</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>pbv, Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/alexanderrobitzsch/sirt">https://github.com/alexanderrobitzsch/sirt</a>,
<a href="https://sites.google.com/view/alexander-robitzsch/software">https://sites.google.com/view/alexander-robitzsch/software</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-05 23:13:53 UTC; sunpn563</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-06 00:10:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='sirt-package'>
Supplementary Item Response Theory Models
</h2><span id='topic+sirt-package'></span><span id='topic+sirt'></span>

<h3>Description</h3>


<p>    Supplementary functions for item response models aiming
    to complement existing R packages. The functionality includes among others
    multidimensional compensatory and noncompensatory IRT models
    (Reckase, 2009, &lt;doi:10.1007/978-0-387-89976-3&gt;), 
    MCMC for hierarchical IRT models and testlet models
    (Fox, 2010, &lt;doi:10.1007/978-1-4419-0742-4&gt;), 
    NOHARM (McDonald, 1982, &lt;doi:10.1177/014662168200600402&gt;), 
    Rasch copula model (Braeken, 2011, &lt;doi:10.1007/s11336-010-9190-4&gt;;
    Schroeders, Robitzsch &amp; Schipolowski, 2014, &lt;doi:10.1111/jedm.12054&gt;),
    faceted and hierarchical rater models (DeCarlo, Kim &amp; Johnson, 2011,
    &lt;doi:10.1111/j.1745-3984.2011.00143.x&gt;),
    ordinal IRT model (ISOP; Scheiblechner, 1995, &lt;doi:10.1007/BF02301417&gt;), 
    DETECT statistic (Stout, Habing, Douglas &amp; Kim, 1996, 
    &lt;doi:10.1177/014662169602000403&gt;), local structural equation modeling 
    (LSEM; Hildebrandt, Luedtke, Robitzsch, Sommer &amp; Wilhelm, 2016,
    &lt;doi:10.1080/00273171.2016.1142856&gt;).
</p>


<h3>Details</h3>

<p>The <span class="pkg">sirt</span> package enables the estimation of following models:
</p>

<ul>
<li><p> Multidimensional marginal maximum likelihood estimation (MML)
of generalized logistic Rasch type models using the
generalized logistic link function (Stukel, 1988) can be conducted
with <code><a href="#topic+rasch.mml2">rasch.mml2</a></code> and the argument <code>itemtype="raschtype"</code>.
This model also allows the estimation of the 4PL item
response model (Loken &amp; Rulison, 2010).
Multiple group estimation, latent regression models and
plausible value imputation are supported. In addition, pseudo-likelihood
estimation for fractional item response data can be conducted.
</p>
</li>
<li><p> Multidimensional noncompensatory, compensatory and partially compensatory
item response models for dichotomous item responses (Reckase, 2009) can be estimated
with the <code><a href="#topic+smirt">smirt</a></code> function and the options <code>irtmodel="noncomp"</code>
, <code>irtmodel="comp"</code> and <code>irtmodel="partcomp"</code>.
</p>
</li>
<li><p> The unidimensional quotient model (Ramsay, 1989) can be estimated
using <code><a href="#topic+rasch.mml2">rasch.mml2</a></code> with <code>itemtype="ramsay.qm"</code>.
</p>
</li>
<li><p> Unidimensional nonparametric item response models can be estimated
employing MML estimation (Rossi, Wang &amp; Ramsay, 2002) by making use of
<code><a href="#topic+rasch.mml2">rasch.mml2</a></code> with <code>itemtype="npirt"</code>.
Kernel smoothing for item response function estimation (Ramsay, 1991)
is implemented in <code><a href="#topic+np.dich">np.dich</a></code>.
</p>
</li>
<li><p> The multidimensional IRT copula model (Braeken, 2011) can be applied
for handling local dependencies, see <code><a href="#topic+rasch.copula3">rasch.copula3</a></code>.
</p>
</li>
<li><p> Unidimensional joint maximum likelihood estimation (JML) of the Rasch model
is possible with the <code><a href="#topic+rasch.jml">rasch.jml</a></code> function. Bias correction methods
for item parameters are included in <code><a href="#topic+rasch.jml.jackknife1">rasch.jml.jackknife1</a></code>
and <code><a href="#topic+rasch.jml.biascorr">rasch.jml.biascorr</a></code>.
</p>
</li>
<li><p> The multidimensional latent class Rasch and 2PL model (Bartolucci, 2007)
which employs a discrete trait distribution can be estimated
with <code><a href="#topic+rasch.mirtlc">rasch.mirtlc</a></code>.
</p>
</li>
<li><p> The unidimensional 2PL rater facets model (Lincare, 1994) can be estimated
with <code><a href="#topic+rm.facets">rm.facets</a></code>. A hierarchical rater model based on
signal detection theory (DeCarlo, Kim &amp; Johnson, 2011) can be conducted
with <code><a href="#topic+rm.sdt">rm.sdt</a></code>. A simple latent class model for two exchangeable
raters is implemented in <code><a href="#topic+lc.2raters">lc.2raters</a></code>. See Robitzsch and Steinfeld (2018)
for more details.
</p>
</li>
<li><p> The discrete grade of membership model (Erosheva, Fienberg &amp; Joutard, 2007)
and the Rasch grade of membership model can be estimated by <code><a href="#topic+gom.em">gom.em</a></code>.
</p>
</li>
<li><p> Some hierarchical IRT models and random item models for dichotomous
and normally distributed data (van den Noortgate, de Boeck &amp; Meulders, 2003;
Fox &amp; Verhagen, 2010) can be estimated with <code><a href="#topic+mcmc.2pno.ml">mcmc.2pno.ml</a></code>.
</p>
</li>
<li><p> Unidimensional pairwise conditional likelihood estimation
(PCML; Zwinderman, 1995) is implemented in <code><a href="#topic+rasch.pairwise">rasch.pairwise</a></code> or
<code><a href="#topic+rasch.pairwise.itemcluster">rasch.pairwise.itemcluster</a></code>.
</p>
</li>
<li><p> Unidimensional pairwise marginal likelihood estimation
(PMML; Renard, Molenberghs &amp; Geys, 2004)
can be conducted using <code><a href="#topic+rasch.pml3">rasch.pml3</a></code>. In this function
local dependence can be handled by imposing residual error structure
or omitting item pairs within a dependent item cluster from the
estimation. <br />
The function <code><a href="#topic+rasch.evm.pcm">rasch.evm.pcm</a></code> estimates the multiple group
partial credit model based on the pairwise eigenvector approach
which avoids iterative estimation.
</p>
</li>
<li><p> Some item response models in <span class="pkg">sirt</span> can be estimated via
Markov Chain Monte Carlo (MCMC) methods. In <code><a href="#topic+mcmc.2pno">mcmc.2pno</a></code>
the two-parameter normal ogive model can be estimated. A hierarchical
version of this model (Janssen, Tuerlinckx, Meulders &amp; de Boeck, 2000)
is implemented in <code><a href="#topic+mcmc.2pnoh">mcmc.2pnoh</a></code>. The 3PNO testlet model
(Wainer, Bradlow &amp; Wang, 2007; Glas, 2012) can be estimated with
<code><a href="#topic+mcmc.3pno.testlet">mcmc.3pno.testlet</a></code>.
Some hierarchical IRT models and random item models
(van den Noortgate, de Boeck &amp; Meulders, 2003) can be estimated
with <code><a href="#topic+mcmc.2pno.ml">mcmc.2pno.ml</a></code>.
</p>
</li>
<li><p> For dichotomous response data, the free NOHARM software
(McDonald, 1982, 1997) estimates the multidimensional compensatory 3PL model and the function
<code><a href="#topic+R2noharm">R2noharm</a></code> runs NOHARM from within <span class="rlang"><b>R</b></span>. Note that NOHARM must be
downloaded from <em>http://noharm.niagararesearch.ca/nh4cldl.html</em>
at first. A pure <span class="rlang"><b>R</b></span> implementation of the NOHARM model with some extensions
can be found in <code><a href="#topic+noharm.sirt">noharm.sirt</a></code>.
</p>
</li>
<li><p> The measurement theoretic founded nonparametric item
response models of Scheiblechner (1995, 1999) &ndash; the ISOP
and the ADISOP model &ndash; can be estimated with
<code><a href="#topic+isop.dich">isop.dich</a></code> or <code><a href="#topic+isop.poly">isop.poly</a></code>.
Item scoring within this theory can be conducted with
<code><a href="#topic+isop.scoring">isop.scoring</a></code>.
</p>
</li>
<li><p> The functional unidimensional item response model
(Ip et al., 2013) can be estimated with <code><a href="#topic+f1d.irt">f1d.irt</a></code>.
</p>
</li>
<li><p> The Rasch model can be estimated by variational approximation
(Rijmen &amp; Vomlel, 2008) using <code><a href="#topic+rasch.va">rasch.va</a></code>.
</p>
</li>
<li><p> The unidimensional probabilistic Guttman model (Proctor, 1970) can be
specified with <code><a href="#topic+prob.guttman">prob.guttman</a></code>.
</p>
</li>
<li><p> A jackknife method for the estimation of standard errors of the
weighted likelihood trait estimate (Warm, 1989) is available in
<code><a href="#topic+wle.rasch.jackknife">wle.rasch.jackknife</a></code>.
</p>
</li>
<li><p> Model based reliability for dichotomous data can be calculated by the method
of Green and Yang (2009) with <code><a href="#topic+greenyang.reliability">greenyang.reliability</a></code> and the
marginal true score method of Dimitrov (2003) using the function
<code><a href="#topic+marginal.truescore.reliability">marginal.truescore.reliability</a></code>.
</p>
</li>
<li><p> Essential unidimensionality can be assessed by the DETECT
index (Stout, Habing, Douglas &amp; Kim, 1996), see the function
<code><a href="#topic+conf.detect">conf.detect</a></code>.
</p>
</li>
<li><p> Item parameters from several studies can be linked using the Haberman
method (Haberman, 2009) in <code><a href="#topic+linking.haberman">linking.haberman</a></code>. See also
<code><a href="#topic+equating.rasch">equating.rasch</a></code> and <code><a href="#topic+linking.robust">linking.robust</a></code>.
The alignment procedure (Asparouhov &amp; Muthen, 2013)
<code><a href="#topic+invariance.alignment">invariance.alignment</a></code> is originally for comfirmatory factor
analysis and aims at obtaining approximate invariance.
</p>
</li>
<li><p> Some person fit statistics in the Rasch model (Meijer &amp; Sijtsma, 2001)
are included in <code><a href="#topic+personfit.stat">personfit.stat</a></code>.
</p>
</li>
<li><p> An alternative to the linear logistic test model (LLTM), the
so called least squares distance model for cognitive diagnosis
(LSDM; Dimitrov, 2007), can be estimated with the function
<code><a href="#topic+lsdm">lsdm</a></code>.
</p>
</li>
<li><p> Local structural equation models (LSEM) can be estimated with the
<code><a href="#topic+lsem.estimate">lsem.estimate</a></code> function (Hildebrandt et al., 2016).
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Alexander Robitzsch [aut,cre] (&lt;https://orcid.org/0000-0002-8226-3132&gt;)
</p>
<p>Maintainer: Alexander Robitzsch &lt;robitzsch@ipn.uni-kiel.de&gt;
</p>


<h3>References</h3>

<p>Asparouhov, T., &amp; Muthen, B. (2014). Multiple-group factor analysis alignment.
<em>Structural Equation Modeling, 21</em>(4), 1-14.
<a href="https://doi.org/10.1080/10705511.2014.919210">doi:10.1080/10705511.2014.919210</a>
</p>
<p>Bartolucci, F. (2007). A class of multidimensional
IRT models for testing unidimensionality and clustering
items. <em>Psychometrika, 72</em>, 141-157.
</p>
<p>Braeken, J. (2011). A boundary mixture approach to violations of conditional
independence. <em>Psychometrika, 76</em>(1), 57-76.
<a href="https://doi.org/10.1007/s11336-010-9190-4">doi:10.1007/s11336-010-9190-4</a>
</p>
<p>DeCarlo, T., Kim, Y., &amp; Johnson, M. S. (2011).
A hierarchical rater model for constructed responses, with a signal detection
rater model. <em>Journal of Educational Measurement, 48</em>(3), 333-356.
<a href="https://doi.org/10.1111/j.1745-3984.2011.00143.x">doi:10.1111/j.1745-3984.2011.00143.x</a>
</p>
<p>Dimitrov, D. (2003). Marginal true-score measures and reliability
for binary items as a function of their IRT parameters.
<em>Applied Psychological Measurement, 27</em>, 440-458.
</p>
<p>Dimitrov, D. M. (2007). Least squares distance method of cognitive validation
and analysis for binary items using their item response theory parameters.
<em>Applied Psychological Measurement, 31</em>, 367-387.
</p>
<p>Erosheva, E. A., Fienberg, S. E., &amp; Joutard, C. (2007).
Describing disability through individual-level mixture models
for multivariate binary data. <em>Annals of Applied Statistics,
1</em>, 502-537.
</p>
<p>Fox, J.-P. (2010). <em>Bayesian item response modeling</em>. New York: Springer.
<a href="https://doi.org/10.1007/978-1-4419-0742-4">doi:10.1007/978-1-4419-0742-4</a>
</p>
<p>Fox, J.-P., &amp; Verhagen, A.-J. (2010). Random item effects modeling for
cross-national survey data.
In E. Davidov, P. Schmidt, &amp; J. Billiet (Eds.),
<em>Cross-cultural Analysis: Methods and Applications</em>
(pp. 467-488), London: Routledge Academic.
</p>
<p>Fraser, C., &amp; McDonald, R. P. (1988). NOHARM: Least squares item factor analysis.
<em>Multivariate Behavioral Research, 23</em>, 267-269.
</p>
<p>Glas, C. A. W. (2012). <em>Estimating and testing the extended testlet model.</em>
LSAC Research Report Series, RR 12-03.
</p>
<p>Green, S.B., &amp; Yang, Y. (2009). Reliability of summed item
scores using structural equation modeling: An alternative to
coefficient alpha. <em>Psychometrika, 74</em>, 155-167.
</p>
<p>Haberman, S. J. (2009). <em>Linking parameter estimates derived
from an item response model through separate calibrations</em>.
ETS Research Report ETS RR-09-40. Princeton, ETS.
<a href="https://doi.org/10.1002/j.2333-8504.2009.tb02197.x">doi:10.1002/j.2333-8504.2009.tb02197.x</a>
</p>
<p>Hildebrandt, A., Luedtke, O., Robitzsch, A., Sommer, C., &amp;
Wilhelm, O. (2016). Exploring factor model parameters across continuous variables
with local structural equation models.
<em>Multivariate Behavioral Research, 51</em>(2-3), 257-278.
<a href="https://doi.org/10.1080/00273171.2016.1142856">doi:10.1080/00273171.2016.1142856</a>
</p>
<p>Ip, E. H., Molenberghs, G., Chen, S. H., Goegebeur, Y., &amp;
De Boeck, P. (2013). Functionally unidimensional item
response models for multivariate binary data.
<em>Multivariate Behavioral Research, 48</em>, 534-562.
</p>
<p>Janssen, R., Tuerlinckx, F., Meulders, M., &amp; de Boeck, P. (2000).
A hierarchical IRT model for criterion-referenced measurement.
<em>Journal of Educational and Behavioral Statistics, 25</em>, 285-306.
</p>
<p>Jeon, M., &amp; Rijmen, F. (2016). A modular approach for item response theory modeling with 
the <span class="rlang"><b>R</b></span> package <span class="pkg">flirt</span>. <em>Behavior Research Methods, 48</em>(2), 742-755.
<a href="https://doi.org/10.3758/s13428-015-0606-z">doi:10.3758/s13428-015-0606-z</a>
</p>
<p>Linacre, J. M. (1994). <em>Many-Facet Rasch Measurement</em>.
Chicago: MESA Press.
</p>
<p>Loken, E. &amp; Rulison, K. L. (2010). Estimation of a four-parameter
item response theory model. <em>British Journal of Mathematical
and Statistical Psychology, 63</em>, 509-525.
</p>
<p>McDonald, R. P. (1982). Linear versus nonlinear models in item response theory.
<em>Applied Psychological Measurement, 6</em>(4), 379-396.
<a href="https://doi.org/10.1177/014662168200600402">doi:10.1177/014662168200600402</a>
</p>
<p>McDonald, R. P. (1997). Normal-ogive multidimensional model.
In W. van der Linden &amp; R. K. Hambleton (1997):
<em>Handbook of modern item response theory</em> (pp. 257-269).
New York: Springer.
<a href="https://doi.org/10.1007/978-1-4757-2691-6_15">doi:10.1007/978-1-4757-2691-6_15</a>
</p>
<p>Meijer, R. R., &amp; Sijtsma, K. (2001). Methodology
review: Evaluating person fit. <em>Applied Psychological
Measurement, 25</em>, 107-135.
</p>
<p>Proctor, C. H. (1970). A probabilistic formulation and statistical analysis
for Guttman scaling. <em>Psychometrika, 35</em>, 73-78.
</p>
<p>Ramsay, J. O. (1989). A comparison of three simple test theory models.
<em>Psychometrika, 54</em>, 487-499.
</p>
<p>Ramsay, J. O. (1991). Kernel smoothing approaches to
nonparametric item characteristic curve estimation.
<em>Psychometrika, 56</em>, 611-630.
</p>
<p>Reckase, M. (2009). <em>Multidimensional item response theory</em>.
New York: Springer. <a href="https://doi.org/10.1007/978-0-387-89976-3">doi:10.1007/978-0-387-89976-3</a>
</p>
<p>Renard, D., Molenberghs, G., &amp; Geys, H. (2004). A pairwise likelihood
approach to estimation in multilevel probit models. <em>Computational Statistics
&amp; Data Analysis, 44</em>, 649-667.
</p>
<p>Rijmen, F., &amp; Vomlel, J. (2008). Assessing the performance of
variational methods for mixed logistic regression models.
<em>Journal of Statistical Computation and Simulation, 78</em>, 765-779.
</p>
<p>Robitzsch, A., &amp; Steinfeld, J. (2018). Item response models for human ratings: Overview,
estimation methods, and implementation in R.
<em>Psychological Test and Assessment Modeling, 60</em>(1), 101-139.
</p>
<p>Rossi, N., Wang, X. &amp; Ramsay, J. O. (2002). Nonparametric item response function
estimates with the EM algorithm.
<em>Journal of Educational and Behavioral Statistics, 27</em>, 291-317.
</p>
<p>Rusch, T., Mair, P., &amp; Hatzinger, R. (2013).
<em>Psychometrics with <span class="rlang"><b>R</b></span>: A Review of CRAN Packages for Item Response Theory.</em>
http://epub.wu.ac.at/4010/1/resrepIRThandbook.pdf
</p>
<p>Scheiblechner, H. (1995). Isotonic ordinal
probabilistic models (ISOP). <em>Psychometrika, 60</em>(2), 281-304.
<a href="https://doi.org/10.1007/BF02301417">doi:10.1007/BF02301417</a>
</p>
<p>Scheiblechner, H. (1999). Additive conjoint isotonic
probabilistic models (ADISOP). <em>Psychometrika, 64</em>, 295-316.
</p>
<p>Schroeders, U., Robitzsch, A., &amp; Schipolowski, S. (2014). A comparison of different
psychometric approaches to modeling testlet structures: An example with C-tests.
<em>Journal of Educational Measurement, 51</em>(4), 400-418.
<a href="https://doi.org/10.1111/jedm.12054">doi:10.1111/jedm.12054</a>
</p>
<p>Stout, W., Habing, B., Douglas, J., &amp; Kim, H. R. (1996).
Conditional covariance-based nonparametric multidimensionality assessment.
<em>Applied Psychological Measurement, 20</em>(4), 331-354.
<a href="https://doi.org/10.1177/014662169602000403">doi:10.1177/014662169602000403</a>
</p>
<p>Stukel, T. A. (1988). Generalized logistic models.
<em>Journal of the American Statistical Association, 83</em>(402), 426-431.
<a href="https://doi.org/10.1080/01621459.1988.10478613">doi:10.1080/01621459.1988.10478613</a>
</p>
<p>Uenlue, A., &amp; Yanagida, T. (2011). <span class="rlang"><b>R</b></span> you ready for <span class="rlang"><b>R</b></span>?:
The CRAN psychometrics task view.
<em>British Journal of Mathematical and Statistical Psychology, 64</em>(1), 182-186.
<a href="https://doi.org/10.1348/000711010X519320">doi:10.1348/000711010X519320</a>
</p>
<p>van den Noortgate, W., De Boeck, P., &amp; Meulders, M. (2003).
Cross-classification multilevel logistic models in psychometrics.
<em>Journal of Educational and Behavioral Statistics, 28</em>, 369-386.
</p>
<p>Warm, T. A. (1989). Weighted likelihood estimation of ability in item response theory.
<em>Psychometrika, 54</em>, 427-450.
</p>
<p>Wainer, H., Bradlow, E. T., &amp; Wang, X. (2007).
<em>Testlet response theory and its applications</em>.
Cambridge: Cambridge University Press.
</p>
<p>Zwinderman, A. H. (1995). Pairwise parameter estimation in Rasch models.
<em>Applied Psychological Measurement, 19</em>, 369-375.
</p>


<h3>See Also</h3>

<p>For estimating multidimensional models for polytomous item responses
see the <span class="pkg">mirt</span>, <span class="pkg">flirt</span> (Jeon &amp; Rijmen, 2016) and <span class="pkg">TAM</span> packages.
</p>
<p>For conditional maximum likelihood estimation see the <span class="pkg">eRm</span> package.
</p>
<p>For pairwise estimation likelihood methods (also known as composite
likelihood methods) see <span class="pkg">pln</span> or <span class="pkg">lavaan</span>.
</p>
<p>The estimation of cognitive diagnostic models is possible
using the <span class="pkg">CDM</span> package.
</p>
<p>For the multidimensional latent class IRT model see the <span class="pkg">MultiLCIRT</span>
package which also allows the estimation IRT models with polytomous item responses.
</p>
<p>Latent class analysis can be carried out with <span class="pkg">covLCA</span>, <span class="pkg">poLCA</span>,
<span class="pkg">BayesLCA</span>, <span class="pkg">randomLCA</span> or <span class="pkg">lcmm</span> packages.
</p>
<p>Markov Chain Monte Carlo estimation for item response models can also
be found in the <span class="pkg">MCMCpack</span> package (see the <code>MCMCirt</code> functions
therein).
</p>
<p>See Rusch, Mair and Hatzinger (2013) and Uenlue and Yanagida (2011)
for reviews of psychometrics packages in <span class="rlang"><b>R</b></span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##
##   |-----------------------------------------------------------------|
##   | sirt 0.40-4 (2013-11-26)                                        |
##   | Supplementary Item Response Theory                              |
##   | Maintainer: Alexander Robitzsch &lt;a.robitzsch at bifie.at &gt;      |
##   | https://sites.google.com/site/alexanderrobitzsch/software       |
##   |-----------------------------------------------------------------|
##
##                       _/              _/
##              _/_/_/      _/  _/_/  _/_/_/_/
##           _/_/      _/  _/_/        _/
##              _/_/  _/  _/          _/
##         _/_/_/    _/  _/            _/_/
##
</code></pre>

<hr>
<h2 id='automatic.recode'>
Automatic Method of Finding Keys in a Dataset with Raw Item Responses
</h2><span id='topic+automatic.recode'></span>

<h3>Description</h3>

<p>This function calculates keys of a dataset with raw item responses.
It starts with setting the most frequent category of an item to 1.
Then, in each iteration keys are changed such that the highest
item discrimination is found.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>automatic.recode(data, exclude=NULL, pstart.min=0.6, allocate=200,
    maxiter=20, progress=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="automatic.recode_+3A_data">data</code></td>
<td>

<p>Dataset with raw item responses
</p>
</td></tr>
<tr><td><code id="automatic.recode_+3A_exclude">exclude</code></td>
<td>

<p>Vector with categories to be excluded for searching the key
</p>
</td></tr>
<tr><td><code id="automatic.recode_+3A_pstart.min">pstart.min</code></td>
<td>

<p>Minimum probability for an initial solution of keys.
</p>
</td></tr>
<tr><td><code id="automatic.recode_+3A_allocate">allocate</code></td>
<td>

<p>Maximum number of categories per item. This argument is used in
the function <code>tam.ctt3</code> of the <span class="pkg">TAM</span> package.
</p>
</td></tr>
<tr><td><code id="automatic.recode_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="automatic.recode_+3A_progress">progress</code></td>
<td>

<p>A logical which indicates if iteration progress should be displayed
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>item.stat</code></td>
<td>
<p>Data frame with item name, p value, item discrimination
and the calculated key</p>
</td></tr>
<tr><td><code>data.scored</code></td>
<td>
<p>Scored data frame using calculated keys
in <code>item.stat</code>
</p>
</td></tr>
<tr><td><code>categ.stats</code></td>
<td>
<p>Data frame with statistics for all categories
of all items</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: data.raw1
#############################################################################
data(data.raw1)

# recode data.raw1 and exclude keys 8 and 9 (missing codes) and
# start with initially setting all categories larger than 50 
res1 &lt;- sirt::automatic.recode( data.raw1, exclude=c(8,9), pstart.min=.50 )
# inspect calculated keys
res1$item.stat

#############################################################################
# EXAMPLE 2: data.timssAusTwn from TAM package
#############################################################################

miceadds::library_install("TAM")
data(data.timssAusTwn,package="TAM")
raw.resp &lt;- data.timssAusTwn[,1:11]
res2 &lt;- sirt::automatic.recode( data=raw.resp )

## End(Not run)
</code></pre>

<hr>
<h2 id='brm-Methods'>
Functions for the Beta Item Response Model
</h2><span id='topic+brm.sim'></span><span id='topic+brm.irf'></span>

<h3>Description</h3>

<p>Functions for simulating and estimating the Beta item response model
(Noel &amp; Dauvier, 2007). <code>brm.sim</code> can be used for
simulating the model, <code>brm.irf</code> computes the item response
function. The Beta item response model is estimated as a discrete
version to enable estimation in <em>standard</em> IRT software like
<span class="pkg">mirt</span> or <span class="pkg">TAM</span> packages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># simulating the beta item response model
brm.sim(theta, delta, tau, K=NULL)

# computing the item response function of the beta item response model
brm.irf( Theta, delta, tau, ncat, thdim=1, eps=1E-10 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brm-Methods_+3A_theta">theta</code></td>
<td>

<p>Ability vector of <code class="reqn">\theta</code> values
</p>
</td></tr>
<tr><td><code id="brm-Methods_+3A_delta">delta</code></td>
<td>

<p>Vector of item difficulty parameters
</p>
</td></tr>
<tr><td><code id="brm-Methods_+3A_tau">tau</code></td>
<td>

<p>Vector item dispersion parameters
</p>
</td></tr>
<tr><td><code id="brm-Methods_+3A_k">K</code></td>
<td>

<p>Number of discretized categories. The default is <code>NULL</code> which
means that the simulated item responses are real number values
between 0 and 1. If an integer <code>K</code> chosen, then values
are discretized such that values of 0, 1, ..., <code class="reqn">K</code>-1 arise.
</p>
</td></tr>
<tr><td><code id="brm-Methods_+3A_theta">Theta</code></td>
<td>
<p>Matrix of the ability vector <code class="reqn">\bold{\theta}</code></p>
</td></tr>
<tr><td><code id="brm-Methods_+3A_ncat">ncat</code></td>
<td>
<p>Number of categories</p>
</td></tr>
<tr><td><code id="brm-Methods_+3A_thdim">thdim</code></td>
<td>
<p>Theta dimension in the matrix <code>Theta</code> on
which the item loads.</p>
</td></tr>
<tr><td><code id="brm-Methods_+3A_eps">eps</code></td>
<td>
<p>Nuisance parameter which stabilize probabilities.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The discrete version of the beta item response model is defined as follows.
Assume that for item <code class="reqn">i</code> there are <code class="reqn">K</code> categories resulting in
values <code class="reqn">k=0,1,\dots,K-1</code>. Each value <code class="reqn">k</code> is associated with a
corresponding the transformed value in <code class="reqn">[0,1]</code>, namely
<code class="reqn"> q (k)=1/(2 \cdot K), 1/(2 \cdot K) + 1/K, \ldots,  1 - 1/(2 \cdot K) </code>.
The item response model is defined as
</p>
<p style="text-align: center;"><code class="reqn"> P( X_{pi}=x_{pi} | \theta_p)  \propto
      q( x_{pi} )^{ m_{pi} - 1 } [ 1- q( x_{pi} ) ]^{ n_{pi} - 1 } </code>
</p>

<p>This density is a discrete version of a Beta distribution with
shape parameters <code class="reqn">m_{pi}</code> and <code class="reqn">n_{pi}</code>. These parameters are
defined as
</p>
<p style="text-align: center;"><code class="reqn"> m_{pi}=\mathrm{exp} \left[ ( \theta_p - \delta_i + \tau_i ) / 2 \right]
    \qquad \mbox{and} \qquad
    n_{pi}=\mathrm{exp} \left[ ( - \theta_p + \delta_i + \tau_i ) / 2 \right]
                </code>
</p>

<p>The item response function can also be formulated as
</p>
<p style="text-align: center;"><code class="reqn"> \mathrm{log} \left[ P( X_{pi}=x_{pi} | \theta_p) \right]  \propto
      ( m_{pi} - 1 ) \cdot \mathrm{log} [ q( x_{pi} ) ] +
      ( n_{pi} - 1 )  \cdot \mathrm{log} [ 1- q( x_{pi} ) ]
                      </code>
</p>

<p>The item parameters can be reparameterized as
<code class="reqn"> a_{i}=\mathrm{exp} \left[ ( - \delta_i + \tau_i ) / 2 \right]</code> and
<code class="reqn"> b_{i}=\mathrm{exp} \left[ ( \delta_i + \tau_i ) / 2 \right]</code>.
</p>
<p>Then, the original item parameters can be retrieved by
<code class="reqn">\tau_i=\mathrm{log} ( a_i b_i)</code> and
<code class="reqn">\delta_i=\mathrm{log} ( b_i / a_i)</code>.
Using <code class="reqn"> \gamma _p=\mathrm{exp} ( \theta_p / 2) </code>, we obtain
</p>
<p style="text-align: center;"><code class="reqn"> \mathrm{log} \left[ P( X_{pi}=x_{pi} | \theta_p) \right]  \propto
       a_{i} \gamma_p  \cdot \mathrm{log} [ q( x_{pi} ) ] +
       b_i / \gamma_p   \cdot \mathrm{log} [ 1- q( x_{pi} ) ] -
      \left[ \mathrm{log} q( x_{pi} ) + \mathrm{log} [ 1- q( x_{pi} ) ] \right]
                      </code>
</p>

<p>This formulation enables the specification of the Beta item response
model as a structured latent class model
(see <code><a href="TAM.html#topic+tam.mml.3pl">TAM::tam.mml.3pl</a></code>;
Example 1).
</p>
<p>See Smithson and Verkuilen (2006) for motivations for treating
continuous indicators not as normally distributed variables.
</p>


<h3>Value</h3>

<p>A simulated dataset of item responses if <code>brm.sim</code> is applied.
</p>
<p>A matrix of item response probabilities if <code>brm.irf</code> is applied.
</p>


<h3>References</h3>

<p>Gruen, B., Kosmidis, I., &amp; Zeileis, A. (2012). Extended Beta regression
in <span class="rlang"><b>R</b></span>: Shaken, stirred, mixed, and partitioned.
<em>Journal of Statistical Software, 48</em>(11), 1-25.
<a href="https://doi.org/10.18637/jss.v048.i11">doi:10.18637/jss.v048.i11</a>
</p>
<p>Noel, Y., &amp; Dauvier, B. (2007). A beta item response model for continuous
bounded responses. <em>Applied Psychological Measurement,
31</em>(1), 47-73. <a href="https://doi.org/10.1177/0146621605287691">doi:10.1177/0146621605287691</a>
</p>
<p>Smithson, M., &amp; Verkuilen, J. (2006). A better lemon squeezer?
Maximum-likelihood regression with beta-distributed dependent variables.
<em>Psychological Methods, 11</em>(1), 54-71.
doi: 10.1037/1082-989X.11.1.54
</p>


<h3>See Also</h3>

<p>See also the <span class="pkg">betareg</span> package for fitting Beta regression
regression models in <span class="rlang"><b>R</b></span> (Gruen, Kosmidis &amp; Zeileis, 2012).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Simulated data beta response model
#############################################################################

#*** (1) Simulation of the beta response model
# Table 3 (p. 65) of Noel and Dauvier (2007)
delta &lt;- c( -.942, -.649, -.603, -.398, -.379, .523, .649, .781, .907 )
tau &lt;- c( .382, .166, 1.799, .615, 2.092, 1.988, 1.899, 1.439, 1.057 )
K &lt;- 5        # number of categories for discretization
N &lt;- 500        # number of persons
I &lt;- length(delta) # number of items

set.seed(865)
theta &lt;- stats::rnorm( N )
dat &lt;- sirt::brm.sim( theta=theta, delta=delta, tau=tau, K=K)
psych::describe(dat)

#*** (2) some preliminaries for estimation of the model in mirt
#*** define a mirt function
library(mirt)
Theta &lt;- matrix( seq( -4, 4, len=21), ncol=1 )

# compute item response function
ii &lt;- 1     # item ii=1
b1 &lt;- sirt::brm.irf( Theta=Theta, delta=delta[ii], tau=tau[ii],  ncat=K )
# plot item response functions
graphics::matplot( Theta[,1], b1, type="l" )

#*** defining the beta item response function for estimation in mirt
par &lt;- c( 0, 1,  1)
names(par) &lt;- c( "delta", "tau","thdim")
est &lt;- c( TRUE, TRUE, FALSE )
names(est) &lt;- names(par)
brm.icc &lt;- function( par, Theta, ncat ){
     delta &lt;- par[1]
     tau &lt;- par[2]
     thdim &lt;- par[3]
     probs &lt;- sirt::brm.irf( Theta=Theta, delta=delta, tau=tau,  ncat=ncat,
            thdim=thdim)
     return(probs)
            }
name &lt;- "brm"
# create item response function
brm.itemfct &lt;- mirt::createItem(name, par=par, est=est, P=brm.icc)
#*** define model in mirt
mirtmodel &lt;- mirt::mirt.model("
           F1=1-9
            " )
itemtype &lt;- rep("brm", I )
customItems &lt;- list("brm"=brm.itemfct)

# define parameters to be estimated
mod1.pars &lt;- mirt::mirt(dat, mirtmodel, itemtype=itemtype,
                   customItems=customItems, pars="values")

## Not run: 
#*** (3) estimate beta item response model in mirt
mod1 &lt;- mirt::mirt(dat,mirtmodel, itemtype=itemtype, customItems=customItems,
               pars=mod1.pars, verbose=TRUE  )
# model summaries
print(mod1)
summary(mod1)
coef(mod1)
# estimated coefficients and comparison with simulated data
cbind( sirt::mirt.wrapper.coef( mod1 )$coef, delta, tau )
mirt.wrapper.itemplot(mod1,ask=TRUE)

#---------------------------
# estimate beta item response model in TAM
library(TAM)

# define the skill space: standard normal distribution
TP &lt;- 21                   # number of theta points
theta.k &lt;- diag(TP)
theta.vec &lt;-  seq( -6,6, len=TP)
d1 &lt;- stats::dnorm(theta.vec)
d1 &lt;- d1 / sum(d1)
delta.designmatrix &lt;- matrix( log(d1), ncol=1 )
delta.fixed &lt;- cbind( 1, 1, 1 )

# define design matrix E
E &lt;- array(0, dim=c(I,K,TP,2*I + 1) )
dimnames(E)[[1]] &lt;- items &lt;- colnames(dat)
dimnames(E)[[4]] &lt;- c( paste0( rep( items, each=2 ),
        rep( c("_a","_b" ), I) ), "one" )
for (ii in 1:I){
    for (kk in 1:K){
      for (tt in 1:TP){
        qk &lt;- (2*(kk-1)+1)/(2*K)
        gammap &lt;- exp( theta.vec[tt] / 2 )
        E[ii, kk, tt, 2*(ii-1) + 1 ] &lt;- gammap * log( qk )
        E[ii, kk, tt, 2*(ii-1) + 2 ] &lt;- 1 / gammap * log( 1 - qk )
        E[ii, kk, tt, 2*I+1 ] &lt;- - log(qk) - log( 1 - qk )
                    }
            }
        }
gammaslope.fixed &lt;- cbind( 2*I+1, 1 )
gammaslope &lt;- exp( rep(0,2*I+1) )

# estimate model in TAM
mod2 &lt;- TAM::tam.mml.3pl(resp=dat, E=E,control=list(maxiter=100),
              skillspace="discrete", delta.designmatrix=delta.designmatrix,
              delta.fixed=delta.fixed, theta.k=theta.k, gammaslope=gammaslope,
              gammaslope.fixed=gammaslope.fixed, notA=TRUE )
summary(mod2)

# extract original tau and delta parameters
m1 &lt;- matrix( mod2$gammaslope[1:(2*I) ], ncol=2, byrow=TRUE )
m1 &lt;- as.data.frame(m1)
colnames(m1) &lt;- c("a","b")
m1$delta.TAM &lt;- log( m1$b / m1$a)
m1$tau.TAM &lt;- log( m1$a * m1$b )

# compare estimated parameter
m2 &lt;- cbind( sirt::mirt.wrapper.coef( mod1 )$coef, delta, tau )[,-1]
colnames(m2) &lt;- c(  "delta.mirt", "tau.mirt", "thdim","delta.true","tau.true"   )
m2 &lt;- cbind(m1,m2)
round( m2, 3 )

## End(Not run)
</code></pre>

<hr>
<h2 id='btm'>
Extended Bradley-Terry Model
</h2><span id='topic+btm'></span><span id='topic+summary.btm'></span><span id='topic+predict.btm'></span><span id='topic+btm_sim'></span>

<h3>Description</h3>

<p>The function <code>btm</code> estimates an extended Bradley-Terry model (Hunter, 2004; see Details).
Parameter estimation uses a bias corrected joint maximum likelihood
estimation method based on <code class="reqn">\varepsilon</code>-adjustment (see Bertoli-Barsotti, Lando &amp; Punzo, 2014).
See Details for the algorithm.
</p>
<p>The function <code>btm_sim</code> simulated data from the extended Bradley-Terry model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>btm(data, judge=NULL, ignore.ties=FALSE, fix.eta=NULL, fix.delta=NULL, fix.theta=NULL,
       maxiter=100, conv=1e-04, eps=0.3, wgt.ties=.5)

## S3 method for class 'btm'
summary(object, file=NULL, digits=4,...)

## S3 method for class 'btm'
predict(object, data=NULL, ...)

btm_sim(theta, eta=0, delta=-99, repeated=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="btm_+3A_data">data</code></td>
<td>

<p>Data frame with three columns. The first two columns contain labels
from the units in the pair comparison. The third column contains the
result of the comparison. &quot;1&quot; means that the first units wins, &quot;0&quot; means
that the second unit wins and &quot;0.5&quot; means a draw (a tie).
</p>
</td></tr>
<tr><td><code id="btm_+3A_judge">judge</code></td>
<td>
<p>Optional vector of judge identifiers (if multiple
judges are available)</p>
</td></tr>
<tr><td><code id="btm_+3A_ignore.ties">ignore.ties</code></td>
<td>

<p>Logical indicating whether ties should be ignored.
</p>
</td></tr>
<tr><td><code id="btm_+3A_fix.eta">fix.eta</code></td>
<td>

<p>Numeric value for a fixed <code class="reqn">\eta</code> value
</p>
</td></tr>
<tr><td><code id="btm_+3A_fix.delta">fix.delta</code></td>
<td>

<p>Numeric value for a fixed <code class="reqn">\delta</code> value
</p>
</td></tr>
<tr><td><code id="btm_+3A_fix.theta">fix.theta</code></td>
<td>

<p>A vector with entries for fixed theta values.
</p>
</td></tr>
<tr><td><code id="btm_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="btm_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="btm_+3A_eps">eps</code></td>
<td>

<p>The <code class="reqn">\varepsilon</code> parameter for the <code class="reqn">\varepsilon</code>-adjustment
method (see Bertoli-Barsotti, Lando &amp; Punzo, 2014) which reduces bias
in ability estimates. In case of <code class="reqn">\varepsilon=0</code>, persons with
extreme scores are removed from the pairwise comparison.
</p>
</td></tr>
<tr><td><code id="btm_+3A_wgt.ties">wgt.ties</code></td>
<td>
<p>Weighting parameter for ties, see formula in Details.
The default is .5</p>
</td></tr>
<tr><td><code id="btm_+3A_object">object</code></td>
<td>
<p>Object of class <code>btm</code></p>
</td></tr>
<tr><td><code id="btm_+3A_file">file</code></td>
<td>
<p>Optional file name for sinking the summary into</p>
</td></tr>
<tr><td><code id="btm_+3A_digits">digits</code></td>
<td>
<p>Number of digits after decimal to print</p>
</td></tr>
<tr><td><code id="btm_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed.</p>
</td></tr>
<tr><td><code id="btm_+3A_theta">theta</code></td>
<td>
<p>Vector of abilities</p>
</td></tr>
<tr><td><code id="btm_+3A_eta">eta</code></td>
<td>
<p>Value of <code class="reqn">\eta</code> parameter</p>
</td></tr>
<tr><td><code id="btm_+3A_delta">delta</code></td>
<td>
<p>Value of <code class="reqn">\delta</code> parameter</p>
</td></tr>
<tr><td><code id="btm_+3A_repeated">repeated</code></td>
<td>
<p>Logical indicating whether repeated ratings of dyads (for home advantage
effect) should be simulated</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The extended Bradley-Terry model for the comparison of individuals
<code class="reqn">i</code> and <code class="reqn">j</code> is defined as
</p>
<p style="text-align: center;"><code class="reqn">P(X_{ij}=1 ) \propto \exp( \eta + \theta_i ) </code>
</p>

<p style="text-align: center;"><code class="reqn">P(X_{ij}=0 ) \propto \exp(  \theta_j ) </code>
</p>

<p style="text-align: center;"><code class="reqn">P(X_{ij}=0.5) \propto \exp( \delta + w_T ( \eta  + \theta_i +\theta_j ) ) </code>
</p>

<p>The parameters <code class="reqn">\theta_i</code> denote the abilities, <code class="reqn">\delta</code> is the
tendency of the occurrence of ties and <code class="reqn">\eta</code> is the home-advantage
effect. The weighting parameter <code class="reqn">w_T</code> governs the importance of ties and can be
chosen in the argument <code>wgt.ties</code>.
</p>
<p>A joint maximum likelihood (JML) estimation is applied for simulataneous
estimation of <code class="reqn">\eta</code>, <code class="reqn">\delta</code> and all <code class="reqn">\theta_i</code> parameters.
In the Rasch model, it was shown that JML can result in biased parameter
estimates. The <code class="reqn">\varepsilon</code>-adjustment approach has been proposed
to reduce the bias in parameter estimates (Bertoli-Bersotti, Lando &amp; Punzo, 2014).
This estimation approach is adapted to the Bradley-Terry model in
the <code>btm</code> function. To this end, the likelihood function is
modified for the purpose of bias reduction. It can be easily shown that there
exist sufficient statistics for <code class="reqn">\eta</code>, <code class="reqn">\delta</code> and all <code class="reqn">\theta_i</code>
parameters. In the <code class="reqn">\varepsilon</code>-adjustment approach, the sufficient
statistic for the <code class="reqn">\theta_i</code> parameter is modified. In JML estimation
of the Bradley-Terry model, <code class="reqn">S_i=\sum_{j \ne i} ( x_{ij} + x_{ji} )</code> is
a sufficient statistic for <code class="reqn">\theta_i</code>. Let <code class="reqn">M_i</code> the maximum score
for person <code class="reqn">i</code> which is the number of <code class="reqn">x_{ij}</code> terms appearing in
<code class="reqn">S_i</code>. In the <code class="reqn">\varepsilon</code>-adjustment approach, the sufficient statistic
<code class="reqn">S_i</code> is modified to
</p>
<p style="text-align: center;"><code class="reqn">S_{i, \varepsilon}=\varepsilon +
\frac{M_i - 2 \varepsilon}{M_i} S_i </code>
</p>
<p> and <code class="reqn">S_{i, \varepsilon}</code> instead of
<code class="reqn">S_{i}</code> is used in JML estimation. Hence, original scores <code class="reqn">S_i</code> are
linearly transformed for all persons <code class="reqn">i</code>.
</p>


<h3>Value</h3>

<p>List with following entries
</p>
<table>
<tr><td><code>pars</code></td>
<td>
<p>Parameter summary for <code class="reqn">\eta</code> and <code class="reqn">\delta</code></p>
</td></tr>
<tr><td><code>effects</code></td>
<td>
<p>Parameter estimates for <code class="reqn">\theta</code> and
outfit and infit statistics</p>
</td></tr>
<tr><td><code>summary.effects</code></td>
<td>
<p>Summary of <code class="reqn">\theta</code> parameter estimates</p>
</td></tr>
<tr><td><code>mle.rel</code></td>
<td>
<p>MLE reliability, also known as separation reliability</p>
</td></tr>
<tr><td><code>sepG</code></td>
<td>
<p>Separation index <code class="reqn">G</code></p>
</td></tr>
<tr><td><code>probs</code></td>
<td>
<p>Estimated probabilities</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>Used dataset with integer identifiers</p>
</td></tr>
<tr><td><code>fit_judges</code></td>
<td>
<p>Fit statistics (outfit and infit) for judges if <code>judge</code>
is provided. In addition, average agreement of the rating with the mode of
the ratings is calculated for each judge (at least three ratings per dyad has
to be available for computing the agreement).</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Unstandardized and standardized residuals for each observation</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bertoli-Barsotti, L., Lando, T., &amp; Punzo, A. (2014). Estimating a Rasch Model
via fuzzy empirical probability functions. In D. Vicari, A. Okada, G. Ragozini
&amp; C. Weihs (Eds.). <em>Analysis and Modeling of Complex Data in Behavioral
and Social Sciences</em>. Springer.
<a href="https://doi.org/10.1007/978-3-319-06692-9_4">doi:10.1007/978-3-319-06692-9_4</a>
</p>
<p>Hunter, D. R. (2004). MM algorithms for generalized Bradley-Terry models.
<em>Annals of Statistics, 32</em>, 384-406. doi: 10.1214/aos/1079120141
</p>


<h3>See Also</h3>

<p>See also the <span class="rlang"><b>R</b></span> packages <span class="pkg">BradleyTerry2</span>, <span class="pkg">psychotools</span>,
<span class="pkg">psychomix</span> and <span class="pkg">prefmod</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Bradley-Terry model | data.pw01
#############################################################################

data(data.pw01)

dat &lt;- data.pw01
dat &lt;- dat[, c("home_team", "away_team", "result") ]

# recode results according to needed input
dat$result[ dat$result==0 ] &lt;- 1/2   # code for ties
dat$result[ dat$result==2 ] &lt;- 0     # code for victory of away team

#********************
# Model 1: Estimation with ties and home advantage
mod1 &lt;- sirt::btm( dat)
summary(mod1)

## Not run: 
#*** Model 2: Estimation with ties, no epsilon adjustment
mod2 &lt;- sirt::btm( dat, eps=0)
summary(mod2)

#*** Model 3: Estimation with ties, no epsilon adjustment, weight for ties of .333 which
#    corresponds to the rule of 3 points for a victory and 1 point of a draw in football
mod3 &lt;- sirt::btm( dat, eps=0, wgt.ties=1/3)
summary(mod3)

#*** Model 4: Some fixed abilities
fix.theta &lt;- c("Anhalt Dessau"=-1 )
mod4 &lt;- sirt::btm( dat, eps=0, fix.theta=fix.theta)
summary(mod4)

#*** Model 5: Ignoring ties, no home advantage effect
mod5 &lt;- sirt::btm( dat, ignore.ties=TRUE, fix.eta=0)
summary(mod5)

#*** Model 6: Ignoring ties, no home advantage effect (JML approach and eps=0)
mod6 &lt;- sirt::btm( dat, ignore.ties=TRUE, fix.eta=0, eps=0)
summary(mod5)

#############################################################################
# EXAMPLE 2: Venice chess data
#############################################################################

# See http://www.rasch.org/rmt/rmt113o.htm
# Linacre, J. M. (1997). Paired Comparisons with Standard Rasch Software.
# Rasch Measurement Transactions, 11:3, 584-585.

# dataset with chess games -&gt; "D" denotes a draw (tie)
chessdata &lt;- scan( what="character")
    1D.0..1...1....1.....1......D.......D........1.........1.......... Browne
    0.1.D..0...1....1.....1......D.......1........D.........1......... Mariotti
    .D0..0..1...D....D.....1......1.......1........1.........D........ Tatai
    ...1D1...D...D....1.....D......D.......D........1.........0....... Hort
    ......010D....D....D.....1......D.......1........1.........D...... Kavalek
    ..........00DDD.....D.....D......D.......1........D.........1..... Damjanovic
    ...............00D0DD......D......1.......1........1.........0.... Gligoric
    .....................000D0DD.......D.......1........D.........1... Radulov
    ............................DD0DDD0D........0........0.........1.. Bobotsov
    ....................................D00D00001.........1.........1. Cosulich
    .............................................0D000D0D10..........1 Westerinen
    .......................................................00D1D010000 Zichichi

L &lt;- length(chessdata) / 2
games &lt;- matrix( chessdata, nrow=L, ncol=2, byrow=TRUE )
G &lt;- nchar(games[1,1])
# create matrix with results
results &lt;- matrix( NA, nrow=G, ncol=3 )
for (gg in 1:G){
    games.gg &lt;- substring( games[,1], gg, gg )
    ind.gg &lt;- which( games.gg !="." )
    results[gg, 1:2 ] &lt;- games[ ind.gg, 2]
    results[gg, 3 ] &lt;- games.gg[ ind.gg[1] ]
}
results &lt;- as.data.frame(results)
results[,3] &lt;- paste(results[,3] )
results[ results[,3]=="D", 3] &lt;- 1/2
results[,3] &lt;- as.numeric( results[,3] )

# fit model ignoring draws
mod1 &lt;- sirt::btm( results, ignore.ties=TRUE, fix.eta=0, eps=0 )
summary(mod1)

# fit model with draws
mod2 &lt;- sirt::btm( results, fix.eta=0, eps=0 )
summary(mod2)

#############################################################################
# EXAMPLE 3: Simulated data from the Bradley-Terry model
#############################################################################

set.seed(9098)
N &lt;- 22
theta &lt;- seq(2,-2, len=N)

#** simulate and estimate data without repeated dyads
dat1 &lt;- sirt::btm_sim(theta=theta)
mod1 &lt;- sirt::btm( dat1, ignore.ties=TRUE, fix.delta=-99, fix.eta=0)
summary(mod1)

#*** simulate data with home advantage effect and ties
dat2 &lt;- sirt::btm_sim(theta=theta, eta=.8, delta=-.6, repeated=TRUE)
mod2 &lt;- sirt::btm(dat2)
summary(mod2)

#############################################################################
# EXAMPLE 4: Estimating the Bradley-Terry model with multiple judges
#############################################################################

#*** simulating data with multiple judges
set.seed(987)
N &lt;- 26  # number of objects to be rated
theta &lt;- seq(2,-2, len=N)
s1 &lt;- stats::sd(theta)
dat &lt;- NULL
# judge discriminations which define tendency to provide reliable ratings
discrim &lt;- c( rep(.9,10), rep(.5,2), rep(0,2) )
#=&gt; last four raters provide less reliable ratings

RR &lt;- length(discrim)
for (rr in 1:RR){
    theta1 &lt;- discrim[rr]*theta + stats::rnorm(N, mean=0, sd=s1*sqrt(1-discrim[rr]))
    dat1 &lt;- sirt::btm_sim(theta1)
    dat1$judge &lt;- rr
    dat &lt;- rbind(dat, dat1)
}

#** estimate the Bradley-Terry model and compute judge-specific fit statistics
mod &lt;- sirt::btm( dat[,1:3], judge=paste0("J",100+dat[,4]), fix.eta=0, ignore.ties=TRUE)
summary(mod)

## End(Not run)
</code></pre>

<hr>
<h2 id='categorize'>
Categorize and Decategorize Variables in a Data Frame
</h2><span id='topic+categorize'></span><span id='topic+decategorize'></span>

<h3>Description</h3>

<p>The function <code>categorize</code> defines categories for variables in
a data frame, starting with a user-defined index (e.g. 0 or 1).
Continuous variables can be categorized by defining categories by
discretizing the variables in different quantile groups.
</p>
<p>The function <code>decategorize</code> does the reverse operation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>categorize(dat, categorical=NULL, quant=NULL, lowest=0)

decategorize(dat, categ_design=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="categorize_+3A_dat">dat</code></td>
<td>

<p>Data frame
</p>
</td></tr>
<tr><td><code id="categorize_+3A_categorical">categorical</code></td>
<td>

<p>Vector with variable names which should be converted into categories,
beginning with integer <code>lowest</code>
</p>
</td></tr>
<tr><td><code id="categorize_+3A_quant">quant</code></td>
<td>
<p>Vector with number of classes for each variables.
Variables are categorized among quantiles. The vector must
have names containing variable names.</p>
</td></tr>
<tr><td><code id="categorize_+3A_lowest">lowest</code></td>
<td>

<p>Lowest category index. Default is 0.
</p>
</td></tr>
<tr><td><code id="categorize_+3A_categ_design">categ_design</code></td>
<td>
<p>Data frame containing informations about
categorization which is the output of <code>categorize</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>categorize</code>, it is a list with entries
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>Converted data frame</p>
</td></tr>
<tr><td><code>categ_design</code></td>
<td>
<p>Data frame containing some informations
about categorization</p>
</td></tr>
</table>
<p>For <code>decategorize</code> it is a data frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(mice)
library(miceadds)

#############################################################################
# EXAMPLE 1: Categorize questionnaire data
#############################################################################

data(data.smallscale, package="miceadds")
dat &lt;- data.smallscale

# (0) select dataset
dat &lt;- dat[, 9:20 ]
summary(dat)
categorical &lt;- colnames(dat)[2:6]

# (1) categorize data
res &lt;- sirt::categorize( dat, categorical=categorical )

# (2) multiple imputation using the mice package
dat2 &lt;- res$data
VV &lt;- ncol(dat2)
impMethod &lt;- rep( "sample", VV )    # define random sampling imputation method
names(impMethod) &lt;- colnames(dat2)
imp &lt;- mice::mice( as.matrix(dat2), impMethod=impMethod, maxit=1, m=1 )
dat3 &lt;- mice::complete(imp,action=1)

# (3) decategorize dataset
dat3a &lt;- sirt::decategorize( dat3, categ_design=res$categ_design )

#############################################################################
# EXAMPLE 2: Categorize ordinal and continuous data
#############################################################################

data(data.ma01,package="miceadds")
dat &lt;- data.ma01
summary(dat[,-c(1:2)] )

# define variables to be categorized
categorical &lt;- c("books", "paredu" )
# define quantiles
quant &lt;-  c(6,5,11)
names(quant) &lt;- c("math", "read", "hisei")

# categorize data
res &lt;- sirt::categorize( dat, categorical=categorical, quant=quant)
str(res)

## End(Not run)
</code></pre>

<hr>
<h2 id='ccov.np'>
Nonparametric Estimation of Conditional Covariances of Item Pairs
</h2><span id='topic+ccov.np'></span>

<h3>Description</h3>

<p>This function estimates conditional covariances of itempairs
(Stout, Habing, Douglas &amp; Kim, 1996; Zhang &amp; Stout,
1999a). The function is used for the estimation of the DETECT index.
The <code>ccov.np</code> function has the (default) option to smooth item response
functions (argument <code>smooth</code>) in the computation of conditional covariances
(Douglas, Kim, Habing, &amp; Gao, 1998).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ccov.np(data, score, bwscale=1.1, thetagrid=seq(-3, 3, len=200),
    progress=TRUE, scale_score=TRUE, adjust_thetagrid=TRUE, smooth=TRUE,
    use_sum_score=FALSE, bias_corr=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ccov.np_+3A_data">data</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous responses.
Missing responses are allowed.
</p>
</td></tr>
<tr><td><code id="ccov.np_+3A_score">score</code></td>
<td>

<p>An ability estimate, e.g. the WLE
</p>
</td></tr>
<tr><td><code id="ccov.np_+3A_bwscale">bwscale</code></td>
<td>

<p>Bandwidth factor for calculation of conditional covariance. The bandwidth
used in the estimation is <code>bwscale</code> times <code class="reqn">N^{-1/5}</code>.
</p>
</td></tr>
<tr><td><code id="ccov.np_+3A_thetagrid">thetagrid</code></td>
<td>

<p>A vector which contains theta values where conditional
covariances are evaluated.
</p>
</td></tr>
<tr><td><code id="ccov.np_+3A_progress">progress</code></td>
<td>

<p>Display progress?
</p>
</td></tr>
<tr><td><code id="ccov.np_+3A_scale_score">scale_score</code></td>
<td>
<p>Logical indicating whether <code>score</code>
should be z standardized in advance of the calculation of
conditional covariances
</p>
</td></tr>
<tr><td><code id="ccov.np_+3A_adjust_thetagrid">adjust_thetagrid</code></td>
<td>
<p>Logical indicating whether <code>thetagrid</code> should be
adjusted if observed values in <code>score</code> are outside of <code>thetagrid</code>.
</p>
</td></tr>
<tr><td><code id="ccov.np_+3A_smooth">smooth</code></td>
<td>
<p>Logical indicating whether smoothing should be
applied for conditional covariance estimation</p>
</td></tr>
<tr><td><code id="ccov.np_+3A_use_sum_score">use_sum_score</code></td>
<td>
<p>Logical indicating whether sum score should be used.
With this option, the bias corrected conditional covariance of Zhang and
Stout (1999) is used.</p>
</td></tr>
<tr><td><code id="ccov.np_+3A_bias_corr">bias_corr</code></td>
<td>
<p>Logical indicating whether bias correction (Zhang &amp; Stout, 1999)
should be utilized if <code>use_sum_score=TRUE</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is used in <code><a href="#topic+conf.detect">conf.detect</a></code> and <code><a href="#topic+expl.detect">expl.detect</a></code>.
</p>


<h3>References</h3>

<p>Douglas, J., Kim, H. R., Habing, B., &amp; Gao, F. (1998). Investigating local dependence
with conditional covariance functions.
<em>Journal of Educational and Behavioral Statistics, 23</em>(2), 129-151.
<a href="https://doi.org/10.3102/10769986023002129">doi:10.3102/10769986023002129</a>
</p>
<p>Stout, W., Habing, B., Douglas, J., &amp; Kim, H. R. (1996).
Conditional covariance-based nonparametric multidimensionality assessment.
<em>Applied Psychological Measurement, 20</em>(4), 331-354.
<a href="https://doi.org/10.1177/014662169602000403">doi:10.1177/014662169602000403</a>
</p>
<p>Zhang, J., &amp; Stout, W. (1999). Conditional covariance structure
of generalized compensatory multidimensional items.
<em>Psychometrika, 64</em>(2), 129-152.
<a href="https://doi.org/10.1007/BF02294532">doi:10.1007/BF02294532</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: data.read | different settings for computing conditional covariance
#############################################################################

data(data.read, package="sirt")
dat &lt;- data.read

#* fit Rasch model
mod &lt;- sirt::rasch.mml2(dat)
score &lt;- sirt::wle.rasch(dat=dat, b=mod$item$b)$theta

#* ccov with smoothing
cmod1 &lt;- sirt::ccov.np(data=dat, score=score, bwscale=1.1)
#* ccov without smoothing
cmod2 &lt;- sirt::ccov.np(data=dat, score=score, smooth=FALSE)

#- compare results
100*cbind( cmod1$ccov.table[1:6, "ccov"], cmod2$ccov.table[1:6, "ccov"])

## End(Not run)
</code></pre>

<hr>
<h2 id='cfa_meas_inv'>
Estimation of a Unidimensional Factor Model under Full and Partial
Measurement Invariance
</h2><span id='topic+cfa_meas_inv'></span>

<h3>Description</h3>

<p>Estimates a unidimensional factor model based on the normal distribution fitting
function under full and partial measurement invariance.
Item loadings and item intercepts are successively freed based on the largest
modification index and a chosen significance level <code>alpha</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cfa_meas_inv(dat, group, weights=NULL, alpha=0.01, verbose=FALSE, op=c("~1","=~"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cfa_meas_inv_+3A_dat">dat</code></td>
<td>

<p>Data frame containing items
</p>
</td></tr>
<tr><td><code id="cfa_meas_inv_+3A_group">group</code></td>
<td>

<p>Vector of group identifiers
</p>
</td></tr>
<tr><td><code id="cfa_meas_inv_+3A_weights">weights</code></td>
<td>

<p>Optional vector of sampling weights
</p>
</td></tr>
<tr><td><code id="cfa_meas_inv_+3A_alpha">alpha</code></td>
<td>

<p>Significance level
</p>
</td></tr>
<tr><td><code id="cfa_meas_inv_+3A_verbose">verbose</code></td>
<td>

<p>Logical indicating whether progress should be shown
</p>
</td></tr>
<tr><td><code id="cfa_meas_inv_+3A_op">op</code></td>
<td>
<p>Operators (intercepts or loadings) for which estimation should be freed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with several entries
</p>
<table>
<tr><td><code>pars_mi</code></td>
<td>
<p>Model parameters under full invariance</p>
</td></tr>
<tr><td><code>pars_pi</code></td>
<td>
<p>Model parameters under partial invariance</p>
</td></tr>
<tr><td><code>mod_mi</code></td>
<td>
<p>Fitted model under full invariance</p>
</td></tr>
<tr><td><code>mod_pi</code></td>
<td>
<p>Fitted model under partial invariance</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>More output</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>See also <code><a href="#topic+invariance.alignment">sirt::invariance.alignment</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Factor model under full and partial invariance
#############################################################################

#--- data simulation

set.seed(65)
G &lt;- 3  # number of groups
I &lt;- 5  # number of items
# define lambda and nu parameters
lambda &lt;- matrix(1, nrow=G, ncol=I)
nu &lt;- matrix(0, nrow=G, ncol=I)
err_var &lt;- matrix(1, nrow=G, ncol=I)

# define size of noninvariance
dif &lt;- 1
#- 1st group: N(0,1)
lambda[1,3] &lt;- 1+dif*.4; nu[1,5] &lt;- dif*.5
#- 2nd group: N(0.3,1.5)
gg &lt;- 2 ;
lambda[gg,5] &lt;- 1-.5*dif; nu[gg,1] &lt;- -.5*dif
#- 3nd group: N(.8,1.2)
gg &lt;- 3
lambda[gg,4] &lt;- 1-.7*dif; nu[gg,2] &lt;- -.5*dif
#- define distributions of groups
mu &lt;- c(0,.3,.8)
sigma &lt;- sqrt(c(1,1.5,1.2))
N &lt;- rep(1000,3) # sample sizes per group

#* use simulation function
dat &lt;- sirt::invariance_alignment_simulate(nu, lambda, err_var, mu, sigma, N,
                exact=TRUE)

#--- estimate CFA
mod &lt;- sirt::cfa_meas_inv(dat=dat[,-1], group=dat$group, verbose=TRUE, alpha=0.05)
mod$pars_mi
mod$pars_pi

## End(Not run)
</code></pre>

<hr>
<h2 id='class.accuracy.rasch'>
Classification Accuracy in the Rasch Model
</h2><span id='topic+class.accuracy.rasch'></span>

<h3>Description</h3>

<p>This function computes the classification accuracy in the Rasch model
for the maximum likelihood (person parameter) estimate according
to the method of Rudner (2001).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>class.accuracy.rasch(cutscores, b, meantheta, sdtheta, theta.l, n.sims=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="class.accuracy.rasch_+3A_cutscores">cutscores</code></td>
<td>

<p>Vector of cut scores
</p>
</td></tr>
<tr><td><code id="class.accuracy.rasch_+3A_b">b</code></td>
<td>

<p>Vector of item difficulties
</p>
</td></tr>
<tr><td><code id="class.accuracy.rasch_+3A_meantheta">meantheta</code></td>
<td>

<p>Mean of the trait distribution
</p>
</td></tr>
<tr><td><code id="class.accuracy.rasch_+3A_sdtheta">sdtheta</code></td>
<td>

<p>Standard deviation of the trait distribution
</p>
</td></tr>
<tr><td><code id="class.accuracy.rasch_+3A_theta.l">theta.l</code></td>
<td>

<p>Discretized theta distribution
</p>
</td></tr>
<tr><td><code id="class.accuracy.rasch_+3A_n.sims">n.sims</code></td>
<td>

<p>Number of simulated persons in a data set. The default is 0
which means that no simulation is performed.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>class.stats</code></td>
<td>
<p>Data frame containing classification accuracy statistics. The
column <code>agree0</code> refers to absolute agreement, <code>agree1</code> to
the agreement of at most a difference of one level.
</p>
</td></tr>
<tr><td><code>class.prob</code></td>
<td>
<p>Probability table of classification</p>
</td></tr>
</table>


<h3>References</h3>

<p>Rudner, L.M. (2001). Computing the expected proportions of misclassified examinees.
<em>Practical Assessment, Research &amp; Evaluation, 7</em>(14).
</p>


<h3>See Also</h3>

<p>Classification accuracy of other IRT models
can be obtained with the <span class="rlang"><b>R</b></span> package <span class="pkg">cacIRT</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Reading dataset
#############################################################################
data( data.read, package="sirt")
dat &lt;- data.read

# estimate the Rasch model
mod &lt;- sirt::rasch.mml2( dat )

# estimate classification accuracy (3 levels)
cutscores &lt;- c( -1, .3 )    # cut scores at theta=-1 and theta=.3
sirt::class.accuracy.rasch( cutscores=cutscores, b=mod$item$b,
           meantheta=0,  sdtheta=mod$sd.trait,
           theta.l=seq(-4,4,len=200), n.sims=3000)
  ##   Cut Scores
  ##   [1] -1.0  0.3
  ##
  ##   WLE reliability (by simulation)=0.671
  ##   WLE consistency (correlation between two parallel forms)=0.649
  ##
  ##   Classification accuracy and consistency
  ##              agree0 agree1 kappa consistency
  ##   analytical   0.68  0.990 0.492          NA
  ##   simulated    0.70  0.997 0.489       0.599
  ##
  ##   Probability classification table
  ##               Est_Class1 Est_Class2 Est_Class3
  ##   True_Class1      0.136      0.041      0.001
  ##   True_Class2      0.081      0.249      0.093
  ##   True_Class3      0.009      0.095      0.294
</code></pre>

<hr>
<h2 id='conf.detect'>
Confirmatory DETECT and polyDETECT Analysis
</h2><span id='topic+conf.detect'></span><span id='topic+summary.conf.detect'></span>

<h3>Description</h3>

<p>This function computes the DETECT statistics for dichotomous item responses
and the polyDETECT statistic for polytomous item responses
under a confirmatory specification of item clusters
(Stout, Habing, Douglas &amp; Kim, 1996; Zhang &amp; Stout,
1999a, 1999b; Zhang, 2007; Bonifay, Reise, Scheines, &amp; Meijer, 2015).
</p>
<p>Item responses in a multi-matrix design are allowed (Zhang, 2013).
</p>
<p>An exploratory DETECT analysis can be conducted using the
<code><a href="#topic+expl.detect">expl.detect</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conf.detect(data, score, itemcluster, bwscale=1.1, progress=TRUE,
        thetagrid=seq(-3, 3, len=200), smooth=TRUE, use_sum_score=FALSE, bias_corr=TRUE)

## S3 method for class 'conf.detect'
summary(object, digits=3, file=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conf.detect_+3A_data">data</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous or polytomous responses.
Missing responses are allowed.
</p>
</td></tr>
<tr><td><code id="conf.detect_+3A_score">score</code></td>
<td>

<p>An ability estimate, e.g. the WLE, sum score or mean score
</p>
</td></tr>
<tr><td><code id="conf.detect_+3A_itemcluster">itemcluster</code></td>
<td>

<p>Item cluster for each item. The order of entries must correspond
to the columns in <code>data</code>.
</p>
</td></tr>
<tr><td><code id="conf.detect_+3A_bwscale">bwscale</code></td>
<td>

<p>Bandwidth factor for calculation of conditional covariance
(see <code><a href="#topic+ccov.np">ccov.np</a></code>)
</p>
</td></tr>
<tr><td><code id="conf.detect_+3A_progress">progress</code></td>
<td>

<p>Display progress?
</p>
</td></tr>
<tr><td><code id="conf.detect_+3A_smooth">smooth</code></td>
<td>
<p>Logical indicating whether smoothing should be
applied for conditional covariance estimation</p>
</td></tr>
<tr><td><code id="conf.detect_+3A_thetagrid">thetagrid</code></td>
<td>

<p>A vector which contains theta values where conditional
covariances are evaluated.
</p>
</td></tr>
<tr><td><code id="conf.detect_+3A_use_sum_score">use_sum_score</code></td>
<td>
<p>Logical indicating whether sum score should be used.
With this option, the bias corrected conditional covariance of Zhang and
Stout (1999) is used.</p>
</td></tr>
<tr><td><code id="conf.detect_+3A_bias_corr">bias_corr</code></td>
<td>
<p>Logical indicating whether bias correction (Zhang &amp; Stout, 1999)
should be utilized if <code>use_sum_score=TRUE</code>.</p>
</td></tr>
<tr><td><code id="conf.detect_+3A_object">object</code></td>
<td>
<p>Object of class <code>conf.detect</code></p>
</td></tr>
<tr><td><code id="conf.detect_+3A_digits">digits</code></td>
<td>
<p>Number of digits for rounding in <code>summary</code></p>
</td></tr>
<tr><td><code id="conf.detect_+3A_file">file</code></td>
<td>
<p>Optional file name to be sunk for <code>summary</code></p>
</td></tr>
<tr><td><code id="conf.detect_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The result of DETECT are the indices <code>DETECT</code>, <code>ASSI</code>
and <code>RATIO</code> (see Zhang 2007 for details) calculated
for the options <code>unweighted</code> and <code>weighted</code>.
The option <code>unweighted</code> means that all conditional covariances of
item pairs are equally weighted, <code>weighted</code> means that
these covariances are weighted by the sample size of item pairs.
In case of multi matrix item designs, both types of indices can
differ.
</p>
<p>The classification scheme of these indices are as follows
(Jang &amp; Roussos, 2007; Zhang, 2007):
</p>

<table>
<tr>
 <td style="text-align: left;">
 Strong multidimensionality </td><td style="text-align: left;"> DETECT &gt; 1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
Moderate multidimensionality </td><td style="text-align: left;"> .40 &lt; DETECT &lt; 1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
Weak multidimensionality </td><td style="text-align: left;"> .20 &lt; DETECT &lt; .40 </td>
</tr>
<tr>
 <td style="text-align: left;">
Essential unidimensionality </td><td style="text-align: left;"> DETECT &lt; .20 </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td>
</tr>

</table>


<table>
<tr>
 <td style="text-align: left;">
 Maximum value under simple structure </td><td style="text-align: left;"> ASSI=1 </td><td style="text-align: left;"> RATIO=1 </td>
</tr>
<tr>
 <td style="text-align: left;">
Essential deviation from unidimensionality </td><td style="text-align: left;"> ASSI &gt; .25 </td><td style="text-align: left;">
RATIO &gt; .36 </td>
</tr>
<tr>
 <td style="text-align: left;">
Essential unidimensionality </td><td style="text-align: left;">  ASSI &lt; .25 </td><td style="text-align: left;">
RATIO &lt; .36 </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td>
</tr>

</table>

<p>Note that the expected value of a conditional covariance for an item pair
is negative when a unidimensional model holds. In consequence,
the DETECT index can become negative for unidimensional data
(see Example 3). This can be also seen in the statistic
<code>MCOV100</code> in the value <code>detect</code>.
</p>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>detect</code></td>
<td>
<p>Data frame with statistics DETECT, ASSI, RATIO, MADCOV100
and MCOV100</p>
</td></tr>
<tr><td><code>ccovtable</code></td>
<td>
<p>Individual contributions to conditional covariance</p>
</td></tr>
<tr><td><code>ccov.matrix</code></td>
<td>
<p>Evaluated conditional covariance</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bonifay, W. E., Reise, S. P., Scheines, R., &amp; Meijer, R. R. (2015).
When are multidimensional data unidimensional enough for structural
equation modeling? An evaluation of the DETECT multidimensionality index.
<em>Structural Equation Modeling, 22</em>(4), 504-516.
<a href="https://doi.org/10.1080/10705511.2014.938596">doi:10.1080/10705511.2014.938596</a>
</p>
<p>Jang, E. E., &amp; Roussos, L. (2007). An investigation into the dimensionality
of TOEFL using conditional covariance-based nonparametric approach.
<em>Journal of Educational Measurement, 44</em>(1), 1-21.
<a href="https://doi.org/10.1111/j.1745-3984.2007.00024.x">doi:10.1111/j.1745-3984.2007.00024.x</a>
</p>
<p>Stout, W., Habing, B., Douglas, J., &amp; Kim, H. R. (1996).
Conditional covariance-based nonparametric multidimensionality assessment.
<em>Applied Psychological Measurement, 20</em>(4), 331-354.
<a href="https://doi.org/10.1177/014662169602000403">doi:10.1177/014662169602000403</a>
</p>
<p>Zhang, J. (2007). Conditional covariance theory and DETECT for
polytomous items. <em>Psychometrika, 72</em>(1), 69-91.
<a href="https://doi.org/10.1007/s11336-004-1257-7">doi:10.1007/s11336-004-1257-7</a>
</p>
<p>Zhang, J. (2013). A procedure for dimensionality analyses of
response data from various test designs. <em>Psychometrika, 78</em>(1), 37-58.
<a href="https://doi.org/10.1007/s11336-012-9287-z">doi:10.1007/s11336-012-9287-z</a>
</p>
<p>Zhang, J., &amp; Stout, W. (1999a). Conditional covariance structure
of generalized compensatory multidimensional items.
<em>Psychometrika, 64</em>(2), 129-152.
<a href="https://doi.org/10.1007/BF02294532">doi:10.1007/BF02294532</a>
</p>
<p>Zhang, J., &amp; Stout, W. (1999b). The theoretical DETECT index of
dimensionality and its application to approximate simple structure.
<em>Psychometrika, 64</em>(2), 213-249.
<a href="https://doi.org/10.1007/BF02294536">doi:10.1007/BF02294536</a>
</p>


<h3>See Also</h3>

<p>For a download of the free <em>DIM-Pack</em> software (DIMTEST, DETECT) see
<em>https://psychometrics.onlinehelp.measuredprogress.org/tools/dim/</em>.
</p>
<p>See <code><a href="#topic+expl.detect">expl.detect</a></code> for exploratory DETECT analysis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: TIMSS mathematics data set (dichotomous data)
#############################################################################
data(data.timss)

# extract data
dat &lt;- data.timss$data
dat &lt;- dat[, substring( colnames(dat),1,1)=="M" ]
# extract item informations
iteminfo &lt;- data.timss$item
# estimate Rasch model
mod1 &lt;- sirt::rasch.mml2( dat )
# estimate WLEs
wle1 &lt;- sirt::wle.rasch( dat, b=mod1$item$b )$theta

# DETECT for content domains
detect1 &lt;- sirt::conf.detect( data=dat, score=wle1,
                    itemcluster=iteminfo$Content.Domain )
  ##          unweighted weighted
  ##   DETECT      0.316    0.316
  ##   ASSI        0.273    0.273
  ##   RATIO       0.355    0.355

## Not run: 
# DETECT cognitive domains
detect2 &lt;- sirt::conf.detect( data=dat, score=wle1,
                    itemcluster=iteminfo$Cognitive.Domain )
  ##          unweighted weighted
  ##   DETECT      0.251    0.251
  ##   ASSI        0.227    0.227
  ##   RATIO       0.282    0.282

# DETECT for item format
detect3 &lt;- sirt::conf.detect( data=dat, score=wle1,
                    itemcluster=iteminfo$Format )
  ##          unweighted weighted
  ##   DETECT      0.056    0.056
  ##   ASSI        0.060    0.060
  ##   RATIO       0.062    0.062

# DETECT for item blocks
detect4 &lt;- sirt::conf.detect( data=dat, score=wle1,
                    itemcluster=iteminfo$Block )
  ##          unweighted weighted
  ##   DETECT      0.301    0.301
  ##   ASSI        0.193    0.193
  ##   RATIO       0.339    0.339 
## End(Not run)

# Exploratory DETECT: Application of a cluster analysis employing the Ward method
detect5 &lt;- sirt::expl.detect( data=dat, score=wle1,
                nclusters=10, N.est=nrow(dat)  )
# Plot cluster solution
pl &lt;- graphics::plot( detect5$clusterfit, main="Cluster solution" )
stats::rect.hclust(detect5$clusterfit, k=4, border="red")

## Not run: 
#############################################################################
# EXAMPLE 2: Big 5 data set (polytomous data)
#############################################################################

# attach Big5 Dataset
data(data.big5)

# select 6 items of each dimension
dat &lt;- data.big5
dat &lt;- dat[, 1:30]

# estimate person score by simply using a transformed sum score
score &lt;- stats::qnorm( ( rowMeans( dat )+.5 )  / ( 30 + 1 ) )

# extract item cluster (Big 5 dimensions)
itemcluster &lt;- substring( colnames(dat), 1, 1 )

# DETECT Item cluster
detect1 &lt;- sirt::conf.detect( data=dat, score=score, itemcluster=itemcluster )
  ##        unweighted weighted
  ## DETECT      1.256    1.256
  ## ASSI        0.384    0.384
  ## RATIO       0.597    0.597

# Exploratory DETECT
detect5 &lt;- sirt::expl.detect( data=dat, score=score,
                     nclusters=9, N.est=nrow(dat)  )
  ## DETECT (unweighted)
  ## Optimal Cluster Size is  6  (Maximum of DETECT Index)
  ##   N.Cluster N.items N.est N.val      size.cluster DETECT.est ASSI.est RATIO.est
  ## 1         2      30   500     0              6-24      1.073    0.246     0.510
  ## 2         3      30   500     0           6-10-14      1.578    0.457     0.750
  ## 3         4      30   500     0         6-10-11-3      1.532    0.444     0.729
  ## 4         5      30   500     0        6-8-11-2-3      1.591    0.462     0.757
  ## 5         6      30   500     0       6-8-6-2-5-3      1.610    0.499     0.766
  ## 6         7      30   500     0     6-3-6-2-5-5-3      1.557    0.476     0.740
  ## 7         8      30   500     0   6-3-3-2-3-5-5-3      1.540    0.462     0.732
  ## 8         9      30   500     0 6-3-3-2-3-5-3-3-2      1.522    0.444     0.724

# Plot Cluster solution
pl &lt;- graphics::plot( detect5$clusterfit, main="Cluster solution" )
stats::rect.hclust(detect5$clusterfit, k=6, border="red")

#############################################################################
# EXAMPLE 3: DETECT index for unidimensional data
#############################################################################

set.seed(976)
N &lt;- 1000
I &lt;- 20
b &lt;- sample( seq( -2, 2, len=I) )
dat &lt;- sirt::sim.raschtype( stats::rnorm(N), b=b )

# estimate Rasch model and corresponding WLEs
mod1 &lt;- TAM::tam.mml( dat )
wmod1 &lt;- TAM::tam.wle(mod1)$theta

# define item cluster
itemcluster &lt;- c( rep(1,5), rep(2,I-5) )

# compute DETECT statistic
detect1 &lt;- sirt::conf.detect( data=dat, score=wmod1, itemcluster=itemcluster)
  ##            unweighted weighted
  ##  DETECT        -0.184   -0.184
  ##  ASSI          -0.147   -0.147
  ##  RATIO         -0.226   -0.226
  ##  MADCOV100      0.816    0.816
  ##  MCOV100       -0.786   -0.786

## End(Not run)
</code></pre>

<hr>
<h2 id='data.activity.itempars'>
Item Parameters Cultural Activities
</h2><span id='topic+data.activity.itempars'></span>

<h3>Description</h3>

<p>List with item parameters for cultural activities
of Austrian students for 9 Austrian countries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.activity.itempars)
</code></pre>


<h3>Format</h3>

<p>The format is a list with number of students per group
(<code>N</code>), item loadings (<code>lambda</code>) and
item intercepts (<code>nu</code>):
</p>
<p><code>List of 3</code> <br />
<code> $ N     : 'table' int [1:9(1d)] 2580 5279 15131 14692 5525 11005 7080 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 1</code> <br />
<code>  .. ..$ : chr [1:9] "1" "2" "3" "4" ...</code> <br />
<code> $ lambda: num [1:9, 1:5] 0.423 0.485 0.455 0.437 0.502 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : chr [1:9] "country1" "country2" "country3" "country4" ...</code> <br />
<code>  .. ..$ : chr [1:5] "act1" "act2" "act3" "act4" ...</code> <br />
<code> $ nu    : num [1:9, 1:5] 1.65 1.53 1.7 1.59 1.7 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : chr [1:9] "country1" "country2" "country3" "country4" ...</code> <br />
<code>  .. ..$ : chr [1:5] "act1" "act2" "act3" "act4" ...</code> <br />
</p>

<hr>
<h2 id='data.befki'>
BEFKI Dataset (Schroeders, Schipolowski, &amp; Wilhelm, 2015)
</h2><span id='topic+data.befki'></span><span id='topic+data.befki_resp'></span>

<h3>Description</h3>

<p>The synthetic dataset is based on the standardization sample of the Berlin
Test of Fluid and Crystallized Intelligence (BEFKI, Wilhelm, Schroeders, &amp;
Schipolowski, 2014). The underlying sample consists of N=11,756 students
from all German federal states (except for the smallest one) and all school
types of the general educational system attending Grades 5 to 12. A
detailed description of the study, the sample, and the measure is given in
Schroeders, Schipolowski, and Wilhelm (2015).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.befki)
data(data.befki_resp)
</code></pre>


<h3>Format</h3>


<ul>
<li><p> The dataset <code>data.befki</code> contains 11756 students, nested within
581 classes.
</p>
<p><code>'data.frame':   11756 obs. of  12 variables:</code> <br />
<code> $ idclass: int  1276 1276 1276 1276 1276 1276 1276 1276 1276 1276 ...</code> <br />
<code> $ idstud : int  127601 127602 127603 127604 127605 127606 127607 127608 127609 127610 ...</code> <br />
<code> $ grade  : int  5 5 5 5 5 5 5 5 5 5 ...</code> <br />
<code> $ gym    : int  0 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code> $ female : int  0 1 0 0 0 0 1 0 0 0 ...</code> <br />
<code> $ age    : num  12.2 11.8 11.5 10.8 10.9 ...</code> <br />
<code> $ sci    : num  -3.14 -3.44 -2.62 -2.16 -1.01 -1.91 -1.01 -4.13 -2.16 -3.44 ...</code> <br />
<code> $ hum    : num  -1.71 -1.29 -2.29 -2.48 -0.65 -0.92 -1.71 -2.31 -1.99 -2.48 ...</code> <br />
<code> $ soc    : num  -2.87 -3.35 -3.81 -2.35 -1.32 -1.11 -1.68 -2.96 -2.69 -3.35 ...</code> <br />
<code> $ gfv    : num  -2.25 -2.19 -2.25 -1.17 -2.19 -3.05 -1.7 -2.19 -3.05 -1.7 ...</code> <br />
<code> $ gfn    : num  -2.2 -1.85 -1.85 -1.85 -1.85 -0.27 -1.37 -2.58 -1.85 -3.13 ...</code> <br />
<code> $ gff    : num  -0.91 -0.43 -1.17 -1.45 -0.61 -1.78 -1.17 -1.78 -1.78 -3.87 ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.befki_resp</code> contains response indicators for observed data points
in the dataset <code>data.befki</code>.
</p>
<p><code> num [1:11756, 1:12] 1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> - attr(*, "dimnames")=List of 2</code> <br />
<code>  ..$ : NULL</code> <br />
<code>  ..$ : chr [1:12] "idclass" "idstud" "grade" "gym" ...</code> <br />
</p>
</li></ul>



<h3>Details</h3>

<p>The procedure for generating this dataset is based on a factorization of the
joint distribution. All variables are simulated from unidimensional conditional
parametric regression models including several interaction and quadratic terms.
The multilevel structure is approximated by including
cluster means as predictors in the regression models.
</p>


<h3>Source</h3>

<p>Synthetic dataset
</p>


<h3>References</h3>

<p>Schroeders, U., Schipolowski, S., &amp; Wilhelm, O. (2015). Age-related
changes in the mean and covariance structure of fluid and crystallized
intelligence in childhood and adolescence. <em>Intelligence, 48</em>, 15-29.
<a href="https://doi.org/10.1016/j.intell.2014.10.006">doi:10.1016/j.intell.2014.10.006</a>
</p>
<p>Wilhelm, O., Schroeders, U., &amp; Schipolowski, S. (2014). <em>Berliner Test
zur Erfassung fluider und kristalliner Intelligenz fuer die 8. bis 10.
Jahrgangsstufe</em> [Berlin test of fluid and crystallized intelligence for
grades 8-10]. Goettingen: Hogrefe.
</p>

<hr>
<h2 id='data.big5'>
Dataset Big 5 from <span class="pkg">qgraph</span> Package
</h2><span id='topic+data.big5'></span><span id='topic+data.big5.qgraph'></span>

<h3>Description</h3>

<p>This is a Big 5 dataset from the <span class="pkg">qgraph</span> package (Dolan, Oorts,
Stoel, Wicherts, 2009). It contains 500 subjects on 240 items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.big5)
data(data.big5.qgraph)
</code></pre>


<h3>Format</h3>


<ul>
<li><p> The format of <code>data.big5</code> is: <br />
<code> num [1:500, 1:240] 1 0 0 0 0 1 1 2 0 1 ...</code> <br />
<code> - attr(*, "dimnames")=List of 2</code> <br />
<code>  ..$ : NULL</code> <br />
<code>  ..$ : chr [1:240] "N1" "E2" "O3" "A4" ...</code> <br />
</p>
</li>
<li><p> The format of <code>data.big5.qgraph</code> is: <br />
</p>
<p><code> num [1:500, 1:240] 2 3 4 4 5 2 2 1 4 2 ...</code> <br />
<code> - attr(*, "dimnames")=List of 2</code> <br />
<code>  ..$ : NULL</code> <br />
<code>  ..$ : chr [1:240] "N1" "E2" "O3" "A4" ...</code> <br />
</p>
</li></ul>



<h3>Details</h3>

<p>In these datasets, there exist 48 items for each dimension. The Big 5
dimensions are Neuroticism (<code>N</code>), Extraversion (<code>E</code>),
Openness (<code>O</code>), Agreeableness (<code>A</code>) and
Conscientiousness (<code>C</code>). Note that the <code>data.big5</code> differs from
<code>data.big5.qgraph</code> in a way that original items were recoded into
three categories 0,1 and 2.
</p>


<h3>Source</h3>

<p>See <code>big5</code> in <span class="pkg">qgraph</span> package.
</p>


<h3>References</h3>

<p>Dolan, C. V., Oort, F. J., Stoel, R. D., &amp; Wicherts, J. M. (2009).
Testing measurement invariance in the target rotates multigroup exploratory
factor model. <em>Structural Equation Modeling, 16</em>, 295-314.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# list of needed packages for the following examples
packages &lt;- scan(what="character")
     sirt   TAM   eRm   CDM   mirt  ltm   mokken  psychotools  psychomix
     psych

# load packages. make an installation if necessary
miceadds::library_install(packages)

#############################################################################
# EXAMPLE 1: Unidimensional models openness scale
#############################################################################

data(data.big5)
# extract first 10 openness items
items &lt;- which( substring( colnames(data.big5), 1, 1 )=="O"  )[1:10]
dat &lt;- data.big5[, items ]
I &lt;- ncol(dat)
summary(dat)
  ##   &gt; colnames(dat)
  ##    [1] "O3"  "O8"  "O13" "O18" "O23" "O28" "O33" "O38" "O43" "O48"
# descriptive statistics
psych::describe(dat)

#****************
# Model 1: Partial credit model
#****************

#-- M1a: rm.facets (in sirt)
m1a &lt;- sirt::rm.facets( dat )
summary(m1a)

#-- M1b: tam.mml (in TAM)
m1b &lt;- TAM::tam.mml( resp=dat )
summary(m1b)

#-- M1c: gdm (in CDM)
theta.k &lt;- seq(-6,6,len=21)
m1c &lt;- CDM::gdm( dat, irtmodel="1PL",theta.k=theta.k, skillspace="normal")
summary(m1c)
# compare results with loglinear skillspace
m1c2 &lt;- CDM::gdm( dat, irtmodel="1PL",theta.k=theta.k, skillspace="loglinear")
summary(m1c2)

#-- M1d: PCM (in eRm)
m1d &lt;- eRm::PCM( dat )
summary(m1d)

#-- M1e: gpcm (in ltm)
m1e &lt;- ltm::gpcm( dat, constraint="1PL", control=list(verbose=TRUE))
summary(m1e)

#-- M1f: mirt (in mirt)
m1f &lt;- mirt::mirt( dat, model=1, itemtype="1PL", verbose=TRUE)
summary(m1f)
coef(m1f)

#-- M1g: PCModel.fit (in psychotools)
mod1g &lt;- psychotools::PCModel.fit(dat)
summary(mod1g)
plot(mod1g)

#****************
# Model 2: Generalized partial credit model
#****************

#-- M2a: rm.facets (in sirt)
m2a &lt;- sirt::rm.facets( dat, est.a.item=TRUE)
summary(m2a)
# Note that in rm.facets the mean of item discriminations is fixed to 1

#-- M2b: tam.mml.2pl (in TAM)
m2b &lt;- TAM::tam.mml.2pl( resp=dat, irtmodel="GPCM")
summary(m2b)

#-- M2c: gdm (in CDM)
m2c &lt;- CDM::gdm( dat, irtmodel="2PL",theta.k=seq(-6,6,len=21),
                   skillspace="normal", standardized.latent=TRUE)
summary(m2c)

#-- M2d: gpcm (in ltm)
m2d &lt;- ltm::gpcm( dat, control=list(verbose=TRUE))
summary(m2d)

#-- M2e: mirt (in mirt)
m2e &lt;- mirt::mirt( dat, model=1,  itemtype="GPCM", verbose=TRUE)
summary(m2e)
coef(m2e)

#****************
# Model 3: Nonparametric item response model
#****************

#-- M3a: ISOP and ADISOP model - isop.poly (in sirt)
m3a &lt;- sirt::isop.poly( dat )
summary(m3a)
plot(m3a)

#-- M3b: Mokken scale analysis (in mokken)
# Scalability coefficients
mokken::coefH(dat)
# Assumption of monotonicity
monotonicity.list &lt;- mokken::check.monotonicity(dat)
summary(monotonicity.list)
plot(monotonicity.list)
# Assumption of non-intersecting ISRFs using method restscore
restscore.list &lt;- mokken::check.restscore(dat)
summary(restscore.list)
plot(restscore.list)

#****************
# Model 4: Graded response model
#****************

#-- M4a: mirt (in mirt)
m4a &lt;- mirt::mirt( dat, model=1,  itemtype="graded", verbose=TRUE)
print(m4a)
mirt.wrapper.coef(m4a)

#----  M4b: WLSMV estimation with cfa (in lavaan)
lavmodel &lt;- "F=~ O3__O48
             F ~~ 1*F
                "
# transform lavaan syntax with lavaanify.IRT
lavmodel &lt;- TAM::lavaanify.IRT( lavmodel, items=colnames(dat) )$lavaan.syntax
mod4b &lt;- lavaan::cfa( data=as.data.frame(dat), model=lavmodel, std.lv=TRUE,
                 ordered=colnames(dat),  parameterization="theta")
summary(mod4b, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)
coef(mod4b)

#****************
# Model 5: Normally distributed residuals
#****************

#----  M5a: cfa (in lavaan)
lavmodel &lt;- "F=~ O3__O48
             F ~~ 1*F
             F ~ 0*1
             O3__O48 ~ 1
                "
lavmodel &lt;- TAM::lavaanify.IRT( lavmodel, items=colnames(dat) )$lavaan.syntax
mod5a &lt;- lavaan::cfa( data=as.data.frame(dat), model=lavmodel, std.lv=TRUE,
                 estimator="MLR" )
summary(mod5a, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#----  M5b: mirt (in mirt)

# create user defined function
name &lt;- 'normal'
par &lt;- c("d"=1, "a1"=0.8, "vy"=1)
est &lt;- c(TRUE, TRUE,FALSE)
P.normal &lt;- function(par,Theta,ncat){
     d &lt;- par[1]
     a1 &lt;- par[2]
     vy &lt;- par[3]
     psi &lt;- vy - a1^2
     # expected values given Theta
     mui &lt;- a1*Theta[,1] + d
     TP &lt;- nrow(Theta)
     probs &lt;- matrix( NA, nrow=TP, ncol=ncat )
     eps &lt;- .01
     for (cc in 1:ncat){
        probs[,cc] &lt;- stats::dnorm( cc, mean=mui, sd=sqrt( abs( psi + eps) ) )
                    }
     psum &lt;- matrix( rep(rowSums( probs ),each=ncat), nrow=TP, ncol=ncat, byrow=TRUE)
     probs &lt;- probs / psum
     return(probs)
}

# create item response function
normal &lt;- mirt::createItem(name, par=par, est=est, P=P.normal)
customItems &lt;- list("normal"=normal)
itemtype &lt;- rep( "normal",I)
# define parameters to be estimated
mod5b.pars &lt;- mirt::mirt(dat, 1, itemtype=itemtype,
                   customItems=customItems, pars="values")
ind &lt;- which( mod5b.pars$name=="vy")
vy &lt;- apply( dat, 2, var, na.rm=TRUE )
mod5b.pars[ ind, "value" ] &lt;- vy
ind &lt;- which( mod5b.pars$name=="a1")
mod5b.pars[ ind, "value" ] &lt;- .5* sqrt(vy)
ind &lt;- which( mod5b.pars$name=="d")
mod5b.pars[ ind, "value" ] &lt;- colMeans( dat, na.rm=TRUE )

# estimate model
mod5b &lt;- mirt::mirt(dat, 1, itemtype=itemtype, customItems=customItems,
                 pars=mod5b.pars, verbose=TRUE    )
sirt::mirt.wrapper.coef(mod5b)$coef

# some item plots
    par(ask=TRUE)
plot(mod5b, type='trace', layout=c(1,1))
    par(ask=FALSE)
# Alternatively:
sirt::mirt.wrapper.itemplot(mod5b)

## End(Not run)
</code></pre>

<hr>
<h2 id='data.bs'>
Datasets from Borg and Staufenbiel (2007)
</h2><span id='topic+data.bs'></span><span id='topic+data.bs07a'></span>

<h3>Description</h3>

<p>Datasets of the book of Borg and Staufenbiel (2007)
<em>Lehrbuch Theorien and Methoden der Skalierung</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.bs07a)
</code></pre>


<h3>Format</h3>


<ul>
<li><p> The dataset <code>data.bs07a</code> contains the data
<em>Gefechtsangst</em> (p. 130) and contains 8 of the original 9 items.
The items are symptoms of anxiety in engagement. <br />
<code>GF1</code>: starkes Herzklopfen, <code>GF2</code>: flaues Gefuehl in der
Magengegend, <code>GF3</code>: Schwaechegefuehl, <code>GF4</code>: Uebelkeitsgefuehl,
<code>GF5</code>: Erbrechen, <code>GF6</code>: Schuettelfrost,
<code>GF7</code>: in die Hose urinieren/einkoten, <code>GF9</code>: Gefuehl der
Gelaehmtheit
</p>
<p>The format is
</p>
<p><code>'data.frame':   100 obs. of  9 variables:</code> <br />
<code> $ idpatt: int  44 29 1 3 28 50 50 36 37 25 ...</code> <br />
<code> $ GF1   : int  1 1 1 1 1 0 0 1 1 1 ...</code> <br />
<code> $ GF2   : int  0 1 1 1 1 0 0 1 1 1 ...</code> <br />
<code> $ GF3   : int  0 0 1 1 0 0 0 0 0 1 ...</code> <br />
<code> $ GF4   : int  0 0 1 1 0 0 0 1 0 1 ...</code> <br />
<code> $ GF5   : int  0 0 1 1 0 0 0 0 0 0 ...</code> <br />
<code> $ GF6   : int  1 1 1 1 1 0 0 0 0 0 ...</code> <br />
<code> $ GF7   : num  0 0 1 1 0 0 0 0 0 0 ...</code> <br />
<code> $ GF9   : int  0 0 1 1 1 0 0 0 0 0 ...</code> <br />
</p>
</li>
<li> <p><em>MORE DATASETS</em>
</p>
</li></ul>



<h3>References</h3>

<p>Borg, I., &amp; Staufenbiel, T. (2007).
<em>Lehrbuch Theorie und Methoden der Skalierung</em>.
Bern: Hogrefe.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 07a: Dataset Gefechtsangst
#############################################################################

data(data.bs07a)
dat &lt;- data.bs07a
items &lt;- grep( "GF", colnames(dat), value=TRUE )

#************************
# Model 1: Rasch model
mod1 &lt;- TAM::tam.mml(dat[,items] )
summary(mod1)
IRT.WrightMap(mod1)

#************************
# Model 2: 2PL model
mod2 &lt;- TAM::tam.mml.2pl(dat[,items] )
summary(mod2)

#************************
# Model 3: Latent class analysis (LCA) with two classes
tammodel &lt;- "
ANALYSIS:
  TYPE=LCA;
  NCLASSES(2)
  NSTARTS(5,10)
LAVAAN MODEL:
  F=~ GF1__GF9
  "
mod3 &lt;- TAM::tamaan( tammodel, dat )
summary(mod3)

#************************
# Model 4: LCA with three classes
tammodel &lt;- "
ANALYSIS:
  TYPE=LCA;
  NCLASSES(3)
  NSTARTS(5,10)
LAVAAN MODEL:
  F=~ GF1__GF9
  "
mod4 &lt;- TAM::tamaan( tammodel, dat )
summary(mod4)

#************************
# Model 5: Located latent class model (LOCLCA) with two classes
tammodel &lt;- "
ANALYSIS:
  TYPE=LOCLCA;
  NCLASSES(2)
  NSTARTS(5,10)
LAVAAN MODEL:
  F=~ GF1__GF9
  "
mod5 &lt;- TAM::tamaan( tammodel, dat )
summary(mod5)

#************************
# Model 6: Located latent class model with three classes
tammodel &lt;- "
ANALYSIS:
  TYPE=LOCLCA;
  NCLASSES(3)
  NSTARTS(5,10)
LAVAAN MODEL:
  F=~ GF1__GF9
  "
mod6 &lt;- TAM::tamaan( tammodel, dat )
summary(mod6)

#************************
# Model 7: Probabilistic Guttman model
mod7 &lt;- sirt::prob.guttman( dat[,items] )
summary(mod7)

#-- model comparison
IRT.compareModels( mod1, mod2, mod3, mod4, mod5, mod6, mod7 )

## End(Not run)
</code></pre>

<hr>
<h2 id='data.eid'>
Examples with Datasets from Eid and Schmidt (2014)
</h2><span id='topic+data.eid'></span><span id='topic+data.eid.kap4'></span><span id='topic+data.eid.kap5'></span><span id='topic+data.eid.kap6'></span><span id='topic+data.eid.kap7'></span>

<h3>Description</h3>

<p>Examples with datasets from Eid and Schmidt (2014),
illustrations with several <span class="rlang"><b>R</b></span> packages. The examples
follow closely the online material of Hosoya (2014).
The datasets are completely synthetic datasets which were
resimulated from the originally available data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.eid.kap4)
data(data.eid.kap5)
data(data.eid.kap6)
data(data.eid.kap7)
</code></pre>


<h3>Format</h3>


<ul>
<li>  <p><code>data.eid.kap4</code> is the dataset from Chapter 4.
</p>
<p><code>'data.frame':   193 obs. of  11 variables:</code> <br />
<code> $ sex     : int  0 0 0 0 0 0 1 0 0 1 ...</code> <br />
<code> $ Freude_1: int  1 1 1 0 1 1 1 1 1 1 ...</code> <br />
<code> $ Wut_1   : int  1 1 1 0 1 1 1 1 1 1 ...</code> <br />
<code> $ Angst_1 : int  1 0 0 0 1 1 1 0 1 0 ...</code> <br />
<code> $ Trauer_1: int  1 1 1 0 1 1 1 1 1 1 ...</code> <br />
<code> $ Ueber_1 : int  1 1 1 0 1 1 0 1 1 1 ...</code> <br />
<code> $ Trauer_2: int  0 1 1 1 1 1 1 1 1 0 ...</code> <br />
<code> $ Angst_2 : int  0 0 1 0 0 1 0 0 0 0 ...</code> <br />
<code> $ Wut_2   : int  1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ Ueber_2 : int  1 0 1 0 1 1 1 0 1 1 ...</code> <br />
<code> $ Freude_2: int  1 1 1 0 1 1 1 1 1 1 ...</code> <br />
</p>
</li>
<li> <p><code>data.eid.kap5</code> is the dataset from Chapter 5.
</p>
<p><code>'data.frame':   499 obs. of  7 variables:</code> <br />
<code> $ sex   : int  0 0 0 0 1 1 1 0 0 0 ...</code> <br />
<code> $ item_1: int  2 3 3 2 4 1 0 0 0 2 ...</code> <br />
<code> $ item_2: int  1 1 4 1 3 3 2 1 2 3 ...</code> <br />
<code> $ item_3: int  1 3 3 2 3 3 0 0 0 1 ...</code> <br />
<code> $ item_4: int  2 4 3 4 3 3 3 2 0 2 ...</code> <br />
<code> $ item_5: int  1 3 2 2 0 0 0 0 1 2 ...</code> <br />
<code> $ item_6: int  4 3 4 3 4 3 2 1 1 3 ...</code> <br />
</p>
</li>
<li> <p><code>data.eid.kap6</code> is the dataset from Chapter 6.
</p>
<p><code>'data.frame':   238 obs. of  7 variables:</code> <br />
<code> $ geschl: int  1 1 0 0 0 1 0 1 1 0 ...</code> <br />
<code> $ item_1: int  3 3 3 3 2 0 1 4 3 3 ...</code> <br />
<code> $ item_2: int  2 2 2 2 2 0 2 3 1 3 ...</code> <br />
<code> $ item_3: int  2 2 1 3 2 0 0 3 1 3 ...</code> <br />
<code> $ item_4: int  2 3 3 3 3 0 2 4 3 4 ...</code> <br />
<code> $ item_5: int  1 2 1 2 2 0 1 2 2 2 ...</code> <br />
<code> $ item_6: int  2 2 2 2 2 0 1 2 1 2 ...</code> <br />
</p>
</li>
<li> <p><code>data.eid.kap7</code> is the dataset <em>Emotionale Klarheit</em> from Chapter 7.
</p>
<p><code>'data.frame':   238 obs. of  9 variables:</code> <br />
<code> $ geschl : int  1 0 1 1 0 1 0 1 0 1 ...</code> <br />
<code> $ reakt_1: num  2.13 1.78 1.28 1.82 1.9 1.63 1.73 1.49 1.43 1.27 ...</code> <br />
<code> $ reakt_2: num  1.2 1.73 0.95 1.5 1.99 1.75 1.58 1.71 1.41 0.96 ...</code> <br />
<code> $ reakt_3: num  1.77 1.42 0.76 1.54 2.36 1.84 2.06 1.21 1.75 0.92 ...</code> <br />
<code> $ reakt_4: num  2.18 1.28 1.39 1.82 2.09 2.15 2.1 1.13 1.71 0.78 ...</code> <br />
<code> $ reakt_5: num  1.47 1.7 1.08 1.77 1.49 1.73 1.96 1.76 1.88 1.1 ...</code> <br />
<code> $ reakt_6: num  1.63 0.9 0.82 1.63 1.79 1.37 1.79 1.11 1.27 1.06 ...</code> <br />
<code> $ kla_th1: int  8 11 11 8 10 11 12 5 6 12 ...</code> <br />
<code> $ kla_th2: int  7 11 12 8 10 11 12 5 8 11 ...</code> <br />
</p>
</li></ul>



<h3>Source</h3>

<p>The material and original datasets can be downloaded from
<em>http://www.hogrefe.de/buecher/lehrbuecher/psychlehrbuchplus/lehrbuecher/
testtheorie-und-testkonstruktion/zusatzmaterial/</em>.
</p>


<h3>References</h3>

<p>Eid, M., &amp; Schmidt, K. (2014). <em>Testtheorie und Testkonstruktion</em>.
Goettingen, Hogrefe.
</p>
<p>Hosoya, G. (2014). <em>Einfuehrung in die Analyse testtheoretischer
Modelle mit <span class="rlang"><b>R</b></span></em>. Available at
<em>http://www.hogrefe.de/buecher/lehrbuecher/psychlehrbuchplus/lehrbuecher/testtheorie-und-testkonstruktion/zusatzmaterial/</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
miceadds::library_install("foreign")
#---- load some IRT packages in R
miceadds::library_install("TAM")        # package (a)
miceadds::library_install("mirt")       # package (b)
miceadds::library_install("sirt")       # package (c)
miceadds::library_install("eRm")        # package (d)
miceadds::library_install("ltm")        # package (e)
miceadds::library_install("psychomix")  # package (f)

#############################################################################
# EXAMPLES Ch. 4: Unidimensional IRT models | dichotomous data
#############################################################################

data(data.eid.kap4)
data0 &lt;- data.eid.kap4

# load data
data0 &lt;- foreign::read.spss( linkname, to.data.frame=TRUE, use.value.labels=FALSE)
# extract items
dat &lt;- data0[,2:11]

#*********************************************************
# Model 1: Rasch model
#*********************************************************

#-----------
#-- 1a: estimation with TAM package

# estimation with tam.mml
mod1a &lt;- TAM::tam.mml(dat)
summary(mod1a)

# person parameters in TAM
pp1a &lt;- TAM::tam.wle(mod1a)

# plot item response functions
plot(mod1a,export=FALSE,ask=TRUE)

# Infit and outfit in TAM
itemf1a &lt;- TAM::tam.fit(mod1a)
itemf1a

# model fit
modf1a &lt;- TAM::tam.modelfit(mod1a)
summary(modf1a)

#-----------
#-- 1b: estimation with mirt package

# estimation with mirt
mod1b &lt;- mirt::mirt( dat, 1, itemtype="Rasch")
summary(mod1b)
print(mod1b)

# person parameters
pp1b &lt;- mirt::fscores(mod1b, method="WLE")

# extract coefficients
sirt::mirt.wrapper.coef(mod1b)

# plot item response functions
plot(mod1b, type="trace" )
par(mfrow=c(1,1))

# item fit
itemf1b &lt;- mirt::itemfit(mod1b)
itemf1b

# model fit
modf1b &lt;- mirt::M2(mod1b)
modf1b

#-----------
#-- 1c: estimation with sirt package

# estimation with rasch.mml2
mod1c &lt;- sirt::rasch.mml2(dat)
summary(mod1c)

# person parameters (EAP)
pp1c &lt;- mod1c$person

# plot item response functions
plot(mod1c, ask=TRUE )

# model fit
modf1c &lt;- sirt::modelfit.sirt(mod1c)
summary(modf1c)

#-----------
#-- 1d: estimation with eRm package

# estimation with RM
mod1d &lt;- eRm::RM(dat)
summary(mod1d)

# estimation person parameters
pp1d &lt;- eRm::person.parameter(mod1d)
summary(pp1d)

# plot item response functions
eRm::plotICC(mod1d)

# person-item map
eRm::plotPImap(mod1d)

# item fit
itemf1d &lt;- eRm::itemfit(pp1d)

# person fit
persf1d &lt;- eRm::personfit(pp1d)

#-----------
#-- 1e: estimation with ltm package

# estimation with rasch
mod1e &lt;- ltm::rasch(dat)
summary(mod1e)

# estimation person parameters
pp1e &lt;- ltm::factor.scores(mod1e)

# plot item response functions
plot(mod1e)

# item fit
itemf1e &lt;- ltm::item.fit(mod1e)

# person fit
persf1e &lt;- ltm::person.fit(mod1e)

# goodness of fit with Bootstrap
modf1e &lt;- ltm::GoF.rasch(mod1e,B=20)    # use more bootstrap samples
modf1e

#*********************************************************
# Model 2: 2PL model
#*********************************************************

#-----------
#-- 2a: estimation with TAM package

# estimation
mod2a &lt;- TAM::tam.mml.2pl(dat)
summary(mod2a)

# model fit
modf2a &lt;- TAM::tam.modelfit(mod2a)
summary(modf2a)

# item response functions
plot(mod2a, export=FALSE, ask=TRUE)

# model comparison
anova(mod1a,mod2a)

#-----------
#-- 2b: estimation with mirt package

# estimation
mod2b &lt;- mirt::mirt(dat,1,itemtype="2PL")
summary(mod2b)
print(mod2b)
sirt::mirt.wrapper.coef(mod2b)

# model fit
modf2b &lt;- mirt::M2(mod2b)
modf2b

#-----------
#-- 2c: estimation with sirt package

I &lt;- ncol(dat)
# estimation
mod2c &lt;- sirt::rasch.mml2(dat,est.a=1:I)
summary(mod2c)

# model fit
modf2c &lt;- sirt::modelfit.sirt(mod2c)
summary(modf2c)

#-----------
#-- 2e: estimation with ltm package

# estimation
mod2e &lt;- ltm::ltm(dat ~ z1 )
summary(mod2e)

# item response functions
plot(mod2e)

#*********************************************************
# Model 3: Mixture Rasch model
#*********************************************************

#-----------
#-- 3a: estimation with TAM package

# avoid "_" in column names if the "__" operator is used in
# the tamaan syntax
dat1 &lt;- dat
colnames(dat1) &lt;- gsub("_", "", colnames(dat1) )
# define tamaan model
tammodel &lt;- "
ANALYSIS:
  TYPE=MIXTURE ;
  NCLASSES(2);
  NSTARTS(20,25);   # 20 random starts with 25 initial iterations each
LAVAAN MODEL:
  F=~ Freude1__Freude2
  F ~~ F
ITEM TYPE:
  ALL(Rasch);
    "
mod3a &lt;- TAM::tamaan( tammodel, resp=dat1 )
summary(mod3a)
# extract item parameters
ipars &lt;- mod2$itempartable_MIXTURE[ 1:10, ]
plot( 1:10, ipars[,3], type="o", ylim=range( ipars[,3:4] ), pch=16,
        xlab="Item", ylab="Item difficulty")
lines( 1:10, ipars[,4], type="l", col=2, lty=2)
points( 1:10, ipars[,4],  col=2, pch=2)

#-----------
#-- 3f: estimation with psychomix package

# estimation
mod3f &lt;- psychomix::raschmix( as.matrix(dat), k=2, scores="meanvar")
summary(mod3f)
# plot class-specific item difficulties
plot(mod3f)

#############################################################################
# EXAMPLES Ch. 5: Unidimensional IRT models | polytomous data
#############################################################################

data(data.eid.kap5)
data0 &lt;- data.eid.kap5
# extract items
dat &lt;- data0[,2:7]

#*********************************************************
# Model 1: Partial credit model
#*********************************************************

#-----------
#-- 1a: estimation with TAM package

# estimation with tam.mml
mod1a &lt;- TAM::tam.mml(dat)
summary(mod1a)

# person parameters in TAM
pp1a &lt;- tam.wle(mod1a)

# plot item response functions
plot(mod1a,export=FALSE,ask=TRUE)

# Infit and outfit in TAM
itemf1a &lt;- TAM::tam.fit(mod1a)
itemf1a

# model fit
modf1a &lt;- TAM::tam.modelfit(mod1a)
summary(modf1a)

#-----------
#-- 1b: estimation with mirt package

# estimation with tam.mml
mod1b &lt;- mirt::mirt( dat, 1, itemtype="Rasch")
summary(mod1b)
print(mod1b)
sirt::mirt.wrapper.coef(mod1b)

# plot item response functions
plot(mod1b, type="trace" )
par(mfrow=c(1,1))

# item fit
itemf1b &lt;- mirt::itemfit(mod1b)
itemf1b

#-----------
#-- 1c: estimation with sirt package

# estimation with rm.facets
mod1c &lt;- sirt::rm.facets(dat)
summary(mod1c)
summary(mod1a)

#-----------
#-- 1d: estimation with eRm package

# estimation
mod1d &lt;- eRm::PCM(dat)
summary(mod1d)

# plot item response functions
eRm::plotICC(mod1d)

# person-item map
eRm::plotPImap(mod1d)

# item fit
itemf1d &lt;- eRm::itemfit(pp1d)

#-----------
#-- 1e: estimation with ltm package

# estimation
mod1e &lt;- ltm::gpcm(dat, constraint="1PL")
summary(mod1e)
# plot item response functions
plot(mod1e)

#*********************************************************
# Model 2: Generalized partial credit model
#*********************************************************

#-----------
#-- 2a: estimation with TAM package

# estimation with tam.mml
mod2a &lt;- TAM::tam.mml.2pl(dat, irtmodel="GPCM")
summary(mod2a)

# model fit
modf2a &lt;- TAM::tam.modelfit(mod2a)
summary(modf2a)

#-----------
#-- 2b: estimation with mirt package

# estimation
mod2b &lt;- mirt::mirt( dat, 1, itemtype="gpcm")
summary(mod2b)
print(mod2b)
sirt::mirt.wrapper.coef(mod2b)

#-----------
#-- 2c: estimation with sirt package

# estimation with rm.facets
mod2c &lt;- sirt::rm.facets(dat, est.a.item=TRUE)
summary(mod2c)

#-----------
#-- 2e: estimation with ltm package

# estimation
mod2e &lt;- ltm::gpcm(dat)
summary(mod2e)
plot(mod2e)

## End(Not run)
</code></pre>

<hr>
<h2 id='data.ess2005'>
Dataset European Social Survey 2005
</h2><span id='topic+data.ess2005'></span>

<h3>Description</h3>

<p>This dataset contains item loadings <code class="reqn">\lambda</code> and intercepts <code class="reqn">\nu</code>
for 26 countries for the European Social Survey (ESS 2005;
see Asparouhov &amp; Muthen, 2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.ess2005)
</code></pre>


<h3>Format</h3>

<p>The format of the dataset is:
</p>
<p><code>List of 2</code> <br />
<code> $ lambda: num [1:26, 1:4] 0.688 0.721 0.72 0.687 0.625 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:4] "ipfrule" "ipmodst" "ipbhprp" "imptrad"</code> <br />
<code> $ nu    : num [1:26, 1:4] 3.26 2.52 3.41 2.84 2.79 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:4] "ipfrule" "ipmodst" "ipbhprp" "imptrad"</code> <br />
</p>


<h3>References</h3>

<p>Asparouhov, T., &amp; Muthen, B. (2014). Multiple-group factor analysis alignment.
<em>Structural Equation Modeling, 21</em>(4), 1-14.
<a href="https://doi.org/10.1080/10705511.2014.919210">doi:10.1080/10705511.2014.919210</a>
</p>

<hr>
<h2 id='data.g308'>
C-Test Datasets
</h2><span id='topic+data.g308'></span>

<h3>Description</h3>

<p>Some datasets of C-tests are provided. The dataset <code>data.g308</code>
was used in Schroeders, Robitzsch and Schipolowski (2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.g308)
</code></pre>


<h3>Format</h3>


<ul>
<li><p> The dataset <code>data.g308</code> is a C-test containing 20 items and is
used in Schroeders, Robitzsch and Schipolowski (2014) and is of the
following format <br />
</p>
<p><code>'data.frame':   747 obs. of  21 variables:</code> <br />
<code> $ id    : int  1 2 3 4 5 6 7 8 9 10 ...</code> <br />
<code> $ G30801: int  1 1 1 1 1 0 0 1 1 1 ...</code> <br />
<code> $ G30802: int  1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ G30803: int  1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ G30804: int  1 1 1 1 1 0 1 1 1 1 ...</code> <br />
<code>[...]</code> <br />
<code> $ G30817: int  0 0 0 0 1 0 1 0 1 0 ...</code> <br />
<code> $ G30818: int  0 0 1 0 0 0 0 1 1 0 ...</code> <br />
<code> $ G30819: int  1 1 1 1 0 0 1 1 1 0 ...</code> <br />
<code> $ G30820: int  1 1 1 1 0 0 0 1 1 0 ...</code> <br />
</p>
</li></ul>



<h3>References</h3>

<p>Schroeders, U., Robitzsch, A., &amp; Schipolowski, S. (2014).
A comparison of different psychometric approaches to modeling testlet
structures: An example with C-tests.
<em>Journal of Educational Measurement, 51</em>(4), 400-418.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Dataset G308 from Schroeders et al. (2014)
#############################################################################

data(data.g308)
dat &lt;- data.g308

library(TAM)
library(sirt)

# define testlets
testlet &lt;- c(1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 5, 5, 6, 6, 6)

#****************************************
#*** Model 1: Rasch model
mod1 &lt;- TAM::tam.mml(resp=dat, control=list(maxiter=300, snodes=1500))
summary(mod1)

#****************************************
#*** Model 2: Rasch testlet model

# testlets are dimensions, assign items to Q-matrix
TT &lt;- length(unique(testlet))
Q &lt;- matrix(0, nrow=ncol(dat), ncol=TT + 1)
Q[,1] &lt;- 1 # First dimension constitutes g-factor
for (tt in 1:TT){Q[testlet==tt, tt+1] &lt;- 1}

# In a testlet model, all dimensions are uncorrelated among
# each other, that is, all pairwise correlations are set to 0,
# which can be accomplished with the "variance.fixed" command
variance.fixed &lt;- cbind(t( utils::combn(TT+1,2)), 0)
mod2 &lt;- TAM::tam.mml(resp=dat, Q=Q, variance.fixed=variance.fixed,
            control=list(snodes=1500, maxiter=300))
summary(mod2)

#****************************************
#*** Model 3: Partial credit model

scores &lt;- list()
testlet.names &lt;- NULL
dat.pcm &lt;- NULL
for (tt in 1:max(testlet) ){
   scores[[tt]] &lt;- rowSums (dat[, testlet==tt, drop=FALSE])
   dat.pcm &lt;- c(dat.pcm, list(c(scores[[tt]])))
   testlet.names &lt;- append(testlet.names, paste0("testlet",tt) )
   }
dat.pcm &lt;- as.data.frame(dat.pcm)
colnames(dat.pcm) &lt;- testlet.names
mod3 &lt;- TAM::tam.mml(resp=dat.pcm, control=list(snodes=1500, maxiter=300) )
summary(mod3)

#****************************************
#*** Model 4: Copula model

mod4 &lt;- sirt::rasch.copula2 (dat=dat, itemcluster=testlet)
summary(mod4)

## End(Not run)
</code></pre>

<hr>
<h2 id='data.inv4gr'>
Dataset for Invariance Testing with 4 Groups
</h2><span id='topic+data.inv4gr'></span>

<h3>Description</h3>

<p>Dataset for invariance testing with 4 groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.inv4gr)
</code></pre>


<h3>Format</h3>

<p>A data frame with 4000 observations on the following 12 variables.
The first variable is a group identifier, the other variables are items.
</p>

<dl>
<dt><code>group</code></dt><dd><p>A group identifier</p>
</dd>
<dt><code>I01</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>I02</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>I03</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>I04</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>I05</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>I06</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>I07</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>I08</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>I09</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>I10</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>I11</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Source</h3>

<p>Simulated dataset
</p>

<hr>
<h2 id='data.liking.science'>
Dataset 'Liking For Science'
</h2><span id='topic+data.liking.science'></span>

<h3>Description</h3>

<p>Dataset 'Liking for science' published by Wright and Masters (1982).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.liking.science)
</code></pre>


<h3>Format</h3>

<p>The format is:
</p>
<p><code> num [1:75, 1:24] 1 2 2 1 1 1 2 2 0 2 ...</code> <br />
<code> - attr(*, "dimnames")=List of 2</code> <br />
<code>  ..$ : NULL</code> <br />
<code>  ..$ : chr [1:24] "LS01" "LS02" "LS03" "LS04" ...</code> <br />
</p>


<h3>References</h3>

<p>Wright, B. D., &amp; Masters, G. N. (1982). <em>Rating scale analysis</em>.
Chicago: MESA Press.
</p>

<hr>
<h2 id='data.long'>
Longitudinal Dataset
</h2><span id='topic+data.long'></span>

<h3>Description</h3>

<p>This dataset contains 200 observations on
12 items. 6 items (<code>I1T1</code>, ...,<code>I6T1</code>)
were administered at measurement occasion T1
and 6 items at T2 (<code>I3T2</code>, ..., <code>I8T2</code>). There were 4 anchor items
which were presented at both time points.
The first column in the dataset contains the student identifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.long)
</code></pre>


<h3>Format</h3>

<p>The format of the dataset is
</p>
<p><code>'data.frame':   200 obs. of  13 variables:</code> <br />
<code> $ idstud: int  1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 ...</code> <br />
<code> $ I1T1  : int  1 1 1 1 1 1 1 0 1 1 ...</code> <br />
<code> $ I2T1  : int  0 0 1 1 1 1 0 1 1 1 ...</code> <br />
<code> $ I3T1  : int  1 0 1 1 0 1 0 0 0 0 ...</code> <br />
<code> $ I4T1  : int  1 0 0 1 0 0 0 0 1 1 ...</code> <br />
<code> $ I5T1  : int  1 0 0 1 0 0 0 0 1 0 ...</code> <br />
<code> $ I6T1  : int  1 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code> $ I3T2  : int  1 1 0 0 1 1 1 1 0 1 ...</code> <br />
<code> $ I4T2  : int  1 1 0 0 1 1 0 0 0 1 ...</code> <br />
<code> $ I5T2  : int  1 0 1 1 1 1 1 0 1 1 ...</code> <br />
<code> $ I6T2  : int  1 1 0 0 0 0 0 0 0 1 ...</code> <br />
<code> $ I7T2  : int  1 0 0 0 0 0 0 0 0 1 ...</code> <br />
<code> $ I8T2  : int  0 0 0 0 1 0 0 0 0 0 ...</code> <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(data.long)
dat &lt;- data.long
dat &lt;- dat[,-1]
I &lt;- ncol(dat)

#*************************************************
# Model 1: 2-dimensional Rasch model
#*************************************************
# define Q-matrix
Q &lt;- matrix(0,I,2)
Q[1:6,1] &lt;- 1
Q[7:12,2] &lt;- 1
rownames(Q) &lt;- colnames(dat)
colnames(Q) &lt;- c("T1","T2")

# vector with same items
itemnr &lt;- as.numeric( substring( colnames(dat),2,2) )
# fix mean at T2 to zero
mu.fixed &lt;- cbind( 2,0 )

#--- M1a: rasch.mml2 (in sirt)
mod1a &lt;- sirt::rasch.mml2(dat, Q=Q, est.b=itemnr, mu.fixed=mu.fixed)
summary(mod1a)

#--- M1b: smirt (in sirt)
mod1b &lt;- sirt::smirt(dat, Qmatrix=Q, irtmodel="comp", est.b=itemnr,
                  mu.fixed=mu.fixed )

#--- M1c: tam.mml (in TAM)

# assume equal item difficulty of I3T1 and I3T2, I4T1 and I4T2, ...
# create draft design matrix and modify it
A &lt;- TAM::designMatrices(resp=dat)$A
dimnames(A)[[1]] &lt;- colnames(dat)
  ##   &gt; str(A)
  ##    num [1:12, 1:2, 1:12] 0 0 0 0 0 0 0 0 0 0 ...
  ##    - attr(*, "dimnames")=List of 3
  ##     ..$ : chr [1:12] "Item01" "Item02" "Item03" "Item04" ...
  ##     ..$ : chr [1:2] "Category0" "Category1"
  ##     ..$ : chr [1:12] "I1T1" "I2T1" "I3T1" "I4T1" ...
A1 &lt;- A[,, c(1:6, 11:12 ) ]
A1[7,2,3] &lt;- -1     # difficulty(I3T1)=difficulty(I3T2)
A1[8,2,4] &lt;- -1     # I4T1=I4T2
A1[9,2,5] &lt;- A1[10,2,6] &lt;- -1
dimnames(A1)[[3]] &lt;- substring( dimnames(A1)[[3]],1,2)
  ##   &gt; A1[,2,]
  ##        I1 I2 I3 I4 I5 I6 I7 I8
  ##   I1T1 -1  0  0  0  0  0  0  0
  ##   I2T1  0 -1  0  0  0  0  0  0
  ##   I3T1  0  0 -1  0  0  0  0  0
  ##   I4T1  0  0  0 -1  0  0  0  0
  ##   I5T1  0  0  0  0 -1  0  0  0
  ##   I6T1  0  0  0  0  0 -1  0  0
  ##   I3T2  0  0 -1  0  0  0  0  0
  ##   I4T2  0  0  0 -1  0  0  0  0
  ##   I5T2  0  0  0  0 -1  0  0  0
  ##   I6T2  0  0  0  0  0 -1  0  0
  ##   I7T2  0  0  0  0  0  0 -1  0
  ##   I8T2  0  0  0  0  0  0  0 -1

# estimate model
# set intercept of second dimension (T2) to zero
beta.fixed &lt;- cbind( 1, 2, 0 )
mod1c &lt;- TAM::tam.mml( resp=dat, Q=Q, A=A1, beta.fixed=beta.fixed)
summary(mod1c)

#*************************************************
# Model 2: 2-dimensional 2PL model
#*************************************************

# set variance at T2 to 1
variance.fixed &lt;- cbind(2,2,1)

# M2a: rasch.mml2 (in sirt)
mod2a &lt;- sirt::rasch.mml2(dat, Q=Q, est.b=itemnr, est.a=itemnr, mu.fixed=mu.fixed,
             variance.fixed=variance.fixed, mmliter=100)
summary(mod2a)

#*************************************************
# Model 3: Concurrent calibration by assuming invariant item parameters
#*************************************************

library(mirt)   # use mirt for concurrent calibration
data(data.long)
dat &lt;- data.long[,-1]
I &lt;- ncol(dat)

# create user defined function for between item dimensionality 4PL model
name &lt;- "4PLbw"
par &lt;- c("low"=0,"upp"=1,"a"=1,"d"=0,"dimItem"=1)
est &lt;- c(TRUE, TRUE,TRUE,TRUE,FALSE)
# item response function
irf &lt;- function(par,Theta,ncat){
     low &lt;- par[1]
     upp &lt;- par[2]
     a &lt;- par[3]
     d &lt;- par[4]
     dimItem &lt;- par[5]
     P1 &lt;- low + ( upp - low ) * plogis( a*Theta[,dimItem] + d )
     cbind(1-P1, P1)
}

# create item response function
fourPLbetw &lt;- mirt::createItem(name, par=par, est=est, P=irf)
head(dat)

# create mirt model (use variable names in mirt.model)
mirtsyn &lt;- "
     T1=I1T1,I2T1,I3T1,I4T1,I5T1,I6T1
     T2=I3T2,I4T2,I5T2,I6T2,I7T2,I8T2
     COV=T1*T2,,T2*T2
     MEAN=T1
     CONSTRAIN=(I3T1,I3T2,d),(I4T1,I4T2,d),(I5T1,I5T2,d),(I6T1,I6T2,d),
                 (I3T1,I3T2,a),(I4T1,I4T2,a),(I5T1,I5T2,a),(I6T1,I6T2,a)
        "
# create mirt model
mirtmodel &lt;- mirt::mirt.model( mirtsyn, itemnames=colnames(dat) )
# define parameters to be estimated
mod3.pars &lt;- mirt::mirt(dat, mirtmodel$model, rep( "4PLbw",I),
                   customItems=list("4PLbw"=fourPLbetw), pars="values")
# select dimensions
ind &lt;- intersect( grep("T2",mod3.pars$item), which( mod3.pars$name=="dimItem" ) )
mod3.pars[ind,"value"] &lt;- 2
# set item parameters low and upp to non-estimated
ind &lt;- which( mod3.pars$name %in% c("low","upp") )
mod3.pars[ind,"est"] &lt;- FALSE

# estimate 2PL model
mod3 &lt;- mirt::mirt(dat, mirtmodel$model, itemtype=rep( "4PLbw",I),
                customItems=list("4PLbw"=fourPLbetw), pars=mod3.pars, verbose=TRUE,
                technical=list(NCYCLES=50)  )
mirt.wrapper.coef(mod3)

#****** estimate model in lavaan
library(lavaan)

# specify syntax
lavmodel &lt;- "
             #**** T1
             F1=~ a1*I1T1+a2*I2T1+a3*I3T1+a4*I4T1+a5*I5T1+a6*I6T1
             I1T1 | b1*t1 ; I2T1 | b2*t1 ; I3T1 | b3*t1 ; I4T1 | b4*t1
             I5T1 | b5*t1 ; I6T1 | b6*t1
             F1 ~~ 1*F1
             #**** T2
             F2=~ a3*I3T2+a4*I4T2+a5*I5T2+a6*I6T2+a7*I7T2+a8*I8T2
             I3T2 | b3*t1 ; I4T2 | b4*t1 ; I5T2 | b5*t1 ; I6T2 | b6*t1
             I7T2 | b7*t1 ; I8T2 | b8*t1
             F2 ~~ NA*F2
             F2 ~ 1
             #*** covariance
             F1 ~~ F2
                "
# estimate model using theta parameterization
mod3lav &lt;- lavaan::cfa( data=dat, model=lavmodel,
            std.lv=TRUE, ordered=colnames(dat), parameterization="theta")
summary(mod3lav, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#*************************************************
# Model 4: Linking with items of different item slope groups
#*************************************************

data(data.long)
dat &lt;- data.long
# dataset for T1
dat1 &lt;- dat[, grep( "T1", colnames(dat) ) ]
colnames(dat1) &lt;- gsub("T1","", colnames(dat1) )
# dataset for T2
dat2 &lt;- dat[, grep( "T2", colnames(dat) ) ]
colnames(dat2) &lt;- gsub("T2","", colnames(dat2) )

# 2PL model with slope groups T1
mod1 &lt;- sirt::rasch.mml2( dat1, est.a=c( rep(1,2), rep(2,4) ) )
summary(mod1)

# 2PL model with slope groups T2
mod2 &lt;- sirt::rasch.mml2( dat2, est.a=c( rep(1,4), rep(2,2) ) )
summary(mod2)

#------- Link 1: Haberman Linking
# collect item parameters
dfr1 &lt;- data.frame( "study1", mod1$item$item, mod1$item$a, mod1$item$b )
dfr2 &lt;- data.frame( "study2", mod2$item$item, mod2$item$a, mod2$item$b )
colnames(dfr2) &lt;- colnames(dfr1) &lt;- c("study", "item", "a", "b" )
itempars &lt;- rbind( dfr1, dfr2 )
# Linking
link1 &lt;- sirt::linking.haberman(itempars=itempars)

#------- Link 2: Invariance alignment method
# create objects for invariance.alignment
nu &lt;- rbind( c(mod1$item$thresh,NA,NA), c(NA,NA,mod2$item$thresh) )
lambda &lt;- rbind( c(mod1$item$a,NA,NA), c(NA,NA,mod2$item$a ) )
colnames(lambda) &lt;- colnames(nu) &lt;- paste0("I",1:8)
rownames(lambda) &lt;- rownames(nu) &lt;- c("T1", "T2")
# Linking
link2a &lt;- sirt::invariance.alignment( lambda, nu )
summary(link2a)

## End(Not run)
</code></pre>

<hr>
<h2 id='data.lsem'>
Datasets for Local Structural Equation Models / Moderated Factor Analysis
</h2><span id='topic+data.lsem01'></span><span id='topic+data.lsem02'></span><span id='topic+data.lsem03'></span>

<h3>Description</h3>

<p>Datasets for local structural equation models or moderated factor analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.lsem01)
data(data.lsem02)
data(data.lsem03)
</code></pre>


<h3>Format</h3>


<ul>
<li><p> The dataset <code>data.lsem01</code> has the following structure
</p>
<p><code>'data.frame':   989 obs. of  6 variables:</code> <br />
<code> $ age: num  4 4 4 4 4 4 4 4 4 4 ...</code> <br />
<code> $ v1 : num  1.83 2.38 1.85 4.53 -0.04 4.35 2.38 1.83 4.81 2.82 ...</code> <br />
<code> $ v2 : num  6.06 9.08 7.41 8.24 6.18 7.4 6.54 4.28 6.43 7.6 ...</code> <br />
<code> $ v3 : num  1.42 3.05 6.42 -1.05 -1.79 4.06 -0.17 -2.64 0.84 6.42 ...</code> <br />
<code> $ v4 : num  3.84 4.24 3.24 3.36 2.31 6.07 4 5.93 4.4 3.49 ...</code> <br />
<code> $ v5 : num  7.84 7.51 6.62 8.02 7.12 7.99 7.25 7.62 7.66 7.03 ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.lsem02</code> is a slightly perturbed dataset of the
Woodcock-Johnson  III (WJ-III) Tests of Cognitive Abilities used in Hildebrandt et al.
(2016) and has the following structure
</p>
<p><code>'data.frame':   1129 obs. of  8 variables:</code> <br />
<code> $ age : int  4 4 4 4 4 4 4 4 4 4 ...</code> <br />
<code> $ gcw : num  -3.53 -3.73 -3.77 -3.84 -4.26 -4.6 -3.66 -4.31 -4.46 -3.64 ...</code> <br />
<code> $ gvw : num  -1.98 -1.35 -1.66 -3.24 -1.17 -2.78 -2.97 -3.88 -3.22 -0.68 ...</code> <br />
<code> $ gfw : num  -2.49 -2.41 -4.48 -4.17 -4.43 -5.06 -3.94 -3.66 -3.7 -2.74 ...</code> <br />
<code> $ gsw : num  -4.85 -5.05 -5.66 -4.3 -5.23 -5.63 -4.91 -5.75 -6.29 -5.47 ...</code> <br />
<code> $ gsmw: num  -2.99 -1.13 -4.21 -3.59 -3.79 -4.77 -2.98 -4.48 -2.99 -3.83 ...</code> <br />
<code> $ glrw: num  -2.49 -2.91 -3.45 -2.91 -3.31 -3.78 -3.5 -3.96 -2.97 -3.14 ...</code> <br />
<code> $ gaw : num  -3.22 -3.77 -3.54 -3.6 -3.22 -3.5 -1.27 -2.08 -2.23 -3.25 ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.lsem03</code> is a synthetic dataset of the SON-R application
used in Hueluer et al. (2011) has the following structure
</p>
<p><code>'data.frame':   1027 obs. of  10 variables:</code> <br />
<code> $ id       : num  10001 10002 10003 10004 10005 ...</code> <br />
<code> $ female   : int  0 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code> $ age      : num  2.62 2.65 2.66 2.67 2.68 2.68 2.68 2.69 2.71 2.71 ...</code> <br />
<code> $ age_group: int  1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ p1       : num  -1.98 -1.98 -1.67 -2.29 -1.67 -1.98 -2.29 -1.98 -2.6 -1.67 ...</code> <br />
<code> $ p2       : num  -1.51 -1.51 -0.55 -1.84 -1.51 -1.84 -2.16 -1.84 -2.48 -1.84 ...</code> <br />
<code> $ p3       : num  -1.4 -2.31 -1.1 -2 -1.4 -1.7 -2.31 -1.4 -2.31 -0.79 ...</code> <br />
<code> $ r1       : num  -1.46 -1.14 -0.49 -2.11 -1.46 -1.46 -2.11 -1.46 -2.75 -1.78 ...</code> <br />
<code> $ r2       : num  -2.67 -1.74 0.74 -1.74 -0.81 -1.43 -2.05 -1.43 -1.74 -1.12 ...</code> <br />
<code> $ r3       : num  -1.64 -1.64 -1.64 -0.9 -1.27 -3.11 -2.74 -1.64 -2.37 -1.27 ...</code> <br />
</p>
<p>The subtests  Mosaics (<code>p1</code>), Puzzles (<code>p1</code>), and Patterns (<code>p3</code>)
constitute the performance subscale;
the subtests Categories (<code>r1</code>), Analogies (<code>r2</code>), and
Situations (<code>r3</code>) constitute the reasoning subscale.
</p>
</li></ul>



<h3>References</h3>

<p>Hildebrandt, A., Luedtke, O., Robitzsch, A., Sommer, C., &amp;
Wilhelm, O. (2016). Exploring factor model parameters across continuous variables
with local structural equation models.
<em>Multivariate Behavioral Research, 51</em>(2-3), 257-278.
<a href="https://doi.org/10.1080/00273171.2016.1142856">doi:10.1080/00273171.2016.1142856</a>
</p>
<p>Hueluer, G., Wilhelm, O., &amp; Robitzsch, A. (2011). Intelligence differentiation in
early childhood. <em>Journal of Individual Differences, 32</em>(3), 170-179.
<a href="https://doi.org/10.1027/1614-0001/a000049">doi:10.1027/1614-0001/a000049</a>
</p>

<hr>
<h2 id='data.math'>
Dataset Mathematics
</h2><span id='topic+data.math'></span>

<h3>Description</h3>

<p>This is an example dataset involving Mathematics items for
German fourth graders. Items are classified into several domains and
subdomains (see Section Format).
The dataset contains 664 students on 30 items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.math)</code></pre>


<h3>Format</h3>

<p>The dataset is a list. The list element <code>data</code>
contains the dataset with the demographic variables
student ID (<code>idstud</code>) and a dummy variable
for female students (<code>female</code>). The remaining
variables (starting with <code>M</code> in the name) are
the mathematics items. <br />
The item metadata are included in the list element
<code>item</code> which contains item name (<code>item</code>) and the
testlet label (<code>testlet</code>). An item not included
in a testlet is indicated by <code>NA</code>.
Each item is allocated to one and only competence domain (<code>domain</code>).
<br />
</p>
<p>The format is:
</p>
<p><code>List of 2</code> <br />
<code> $ data:'data.frame':</code> <br />
<code>  ..$ idstud: int [1:664] 1001 1002 1003 ...</code> <br />
<code>  ..$ female: int [1:664] 1 1 0 0 1 1 1 0 0 1 ...</code> <br />
<code>  ..$ MA1   : int [1:664] 1 1 1 0 0 1 1 1 1 1 ...</code> <br />
<code>  ..$ MA2   : int [1:664] 1 1 1 1 1 0 0 0 0 1 ...</code> <br />
<code>  ..$ MA3   : int [1:664] 1 1 0 0 0 0 0 1 0 0 ...</code> <br />
<code>  ..$ MA4   : int [1:664] 0 1 1 1 0 0 1 0 0 0 ...</code> <br />
<code>  ..$ MB1   : int [1:664] 0 1 0 1 0 0 0 0 0 1 ...</code> <br />
<code>  ..$ MB2   : int [1:664] 1 1 1 1 0 1 0 1 0 0 ...</code> <br />
<code>  ..$ MB3   : int [1:664] 1 1 1 1 0 0 0 1 0 1 ...</code> <br />
<code>  [...]</code> <br />
<code>  ..$ MH3   : int [1:664] 1 1 0 1 0 0 1 0 1 0 ...</code> <br />
<code>  ..$ MH4   : int [1:664] 0 1 1 1 0 0 0 0 1 0 ...</code> <br />
<code>  ..$ MI1   : int [1:664] 1 1 0 1 0 1 0 0 1 0 ...</code> <br />
<code>  ..$ MI2   : int [1:664] 1 1 0 0 0 1 1 0 1 1 ...</code> <br />
<code>  ..$ MI3   : int [1:664] 0 1 0 1 0 0 0 0 0 0 ...</code> <br />
<code> $ item:'data.frame':</code> <br />
<code>  ..$ item     : Factor w/ 30 levels "MA1","MA2","MA3",..: 1 2 3 4 5 ...</code> <br />
<code>  ..$ testlet  : Factor w/ 9 levels "","MA","MB","MC",..: 2 2 2 2 3 3  ...</code> <br />
<code>  ..$ domain   : Factor w/ 3 levels "arithmetic","geometry",..: 1 1 1  ...</code> <br />
<code>  ..$ subdomain: Factor w/ 9 levels "","addition",..: 2 2 2 2 7 7  ...</code> <br />
</p>

<hr>
<h2 id='data.mcdonald'>
Some Datasets from McDonald's <em>Test Theory</em> Book
</h2><span id='topic+data.mcdonald.act15'></span><span id='topic+data.mcdonald.LSAT6'></span><span id='topic+data.mcdonald.rape'></span>

<h3>Description</h3>

<p>Some datasets from McDonald (1999), especially related to using
NOHARM for item response modeling. See Examples below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.mcdonald.act15)
data(data.mcdonald.LSAT6)
data(data.mcdonald.rape)
</code></pre>


<h3>Format</h3>


<ul>
<li><p> The format of the ACT15 data <code>data.mcdonald.act15</code> is:
</p>
<p><code> num [1:15, 1:15] 0.49 0.44 0.38 0.3 0.29 0.13 0.23 0.16 0.16 0.23 ...</code> <br />
<code> - attr(*, "dimnames")=List of 2</code> <br />
<code>  ..$ : chr [1:15] "A01" "A02" "A03" "A04" ...</code> <br />
<code>  ..$ : chr [1:15] "A01" "A02" "A03" "A04" ...</code>
</p>
<p>The dataset (which is the product-moment covariance matrix)
is obtained from Ch. 12 in McDonald (1999). <br />
</p>
</li>
<li><p> The format of the LSAT6 data <code>data.mcdonald.LSAT6</code> is:
</p>
<p><code>'data.frame':   1004 obs. of  5 variables:</code> <br />
<code> $ L1: int  0 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code> $ L2: int  0 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code> $ L3: int  0 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code> $ L4: int  0 0 0 0 0 0 0 0 0 1 ...</code> <br />
<code> $ L5: int  0 0 0 1 1 1 1 1 1 0 ...</code>
</p>
<p>The dataset is obtained from Ch. 6 in McDonald (1999). <br />
</p>
</li>
<li><p> The format of the rape myth scale data  <code>data.mcdonald.rape</code> is
</p>
<p><code>List of 2</code> <br />
<code> $ lambda: num [1:2, 1:19] 1.13 0.88 0.85 0.77 0.79 0.55 1.12 1.01 0.99 0.79 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : chr [1:2] "male" "female"</code> <br />
<code>  .. ..$ : chr [1:19] "I1" "I2" "I3" "I4" ...</code> <br />
<code> $ nu    : num [1:2, 1:19] 2.88 1.87 3.12 2.32 2.13 1.43 3.79 2.6 3.01 2.11 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : chr [1:2] "male" "female"</code> <br />
<code>  .. ..$ : chr [1:19] "I1" "I2" "I3" "I4" ...</code> <br />
</p>
<p>The dataset is obtained from Ch. 15 in McDonald (1999).
</p>
</li></ul>



<h3>Source</h3>

<p>Tables in McDonald (1999)
</p>


<h3>References</h3>

<p>McDonald, R. P. (1999). <em>Test theory: A unified treatment</em>.
Psychology Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: LSAT6 data    | Chapter 12 McDonald (1999)
#############################################################################
data(data.mcdonald.act15)

#************
# Model 1: 2-parameter normal ogive model

#++ NOHARM estimation
I &lt;- ncol(dat)
# covariance structure
P.pattern &lt;- matrix( 0, ncol=1, nrow=1 )
P.init &lt;- 1+0*P.pattern
# fix all entries in the loading matrix to 1
F.pattern &lt;- matrix( 1, nrow=I, ncol=1 )
F.init &lt;- F.pattern
# estimate model
mod1a &lt;- sirt::R2noharm( dat=dat, model.type="CFA", F.pattern=F.pattern,
             F.init=F.init, P.pattern=P.pattern, P.init=P.init,
             writename="LSAT6__1dim_2pno", noharm.path=noharm.path, dec="," )
summary(mod1a, logfile="LSAT6__1dim_2pno__SUMMARY")

#++ pairwise marginal maximum likelihood estimation using the probit link
mod1b &lt;- sirt::rasch.pml3( dat, est.a=1:I, est.sigma=FALSE)

#************
# Model 2: 1-parameter normal ogive model

#++ NOHARM estimation
# covariance structure
P.pattern &lt;- matrix( 0, ncol=1, nrow=1 )
P.init &lt;- 1+0*P.pattern
# fix all entries in the loading matrix to 1
F.pattern &lt;- matrix( 2, nrow=I, ncol=1 )
F.init &lt;- 1+0*F.pattern
# estimate model
mod2a &lt;- sirt::R2noharm( dat=dat, model.type="CFA", F.pattern=F.pattern,
                F.init=F.init, P.pattern=P.pattern, P.init=P.init,
                writename="LSAT6__1dim_1pno", noharm.path=noharm.path, dec="," )
summary(mod2a, logfile="LSAT6__1dim_1pno__SUMMARY")

# PMML estimation
mod2b &lt;- sirt::rasch.pml3( dat, est.a=rep(1,I), est.sigma=FALSE )
summary(mod2b)

#************
# Model 3: 3-parameter normal ogive model with fixed guessing parameters

#++ NOHARM estimation
# covariance structure
P.pattern &lt;- matrix( 0, ncol=1, nrow=1 )
P.init &lt;- 1+0*P.pattern
# fix all entries in the loading matrix to 1
F.pattern &lt;- matrix( 1, nrow=I, ncol=1 )
F.init &lt;- 1+0*F.pattern
# estimate model
mod &lt;- sirt::R2noharm( dat=dat, model.type="CFA",  guesses=rep(.2,I),
            F.pattern=F.pattern, F.init=F.init, P.pattern=P.pattern,
            P.init=P.init, writename="LSAT6__1dim_3pno",
            noharm.path=noharm.path, dec="," )
summary(mod, logfile="LSAT6__1dim_3pno__SUMMARY")

#++ logistic link function employed in smirt function
mod1d &lt;- sirt::smirt(dat, Qmatrix=F.pattern, est.a=matrix(1:I,I,1), c.init=rep(.2,I))
summary(mod1d)

#############################################################################
# EXAMPLE 2: ACT15 data    | Chapter 6 McDonald (1999)
#############################################################################
data(data.mcdonald.act15)
pm &lt;- data.mcdonald.act15

#************
# Model 1: 2-dimensional exploratory factor analysis
mod1 &lt;- sirt::R2noharm( pm=pm, n=1000, model.type="EFA", dimensions=2,
             writename="ACT15__efa_2dim", noharm.path=noharm.path, dec="," )
summary(mod1)

#************
# Model 2: 2-dimensional independent clusters basis solution
P.pattern &lt;- matrix(1,2,2)
diag(P.pattern) &lt;- 0
P.init &lt;- 1+0*P.pattern
F.pattern &lt;- matrix(0,15,2)
F.pattern[ c(1:5,11:15),1] &lt;- 1
F.pattern[ c(6:10,11:15),2] &lt;- 1
F.init &lt;- F.pattern

# estimate model
mod2 &lt;- sirt::R2noharm( pm=pm, n=1000,  model.type="CFA", F.pattern=F.pattern,
            F.init=F.init, P.pattern=P.pattern,P.init=P.init,
            writename="ACT15_indep_clusters", noharm.path=noharm.path, dec="," )
summary(mod2)

#************
# Model 3: Hierarchical model

P.pattern &lt;- matrix(0,3,3)
P.init &lt;- P.pattern
diag(P.init) &lt;- 1
F.pattern &lt;- matrix(0,15,3)
F.pattern[,1] &lt;- 1    # all items load on g factor
F.pattern[ c(1:5,11:15),2] &lt;- 1   # Items 1-5 and 11-15 load on first nested factor
F.pattern[ c(6:10,11:15),3] &lt;- 1  # Items 6-10 and 11-15 load on second nested factor
F.init &lt;- F.pattern

# estimate model
mod3 &lt;- sirt::R2noharm( pm=pm, n=1000,  model.type="CFA", F.pattern=F.pattern,
           F.init=F.init, P.pattern=P.pattern, P.init=P.init,
           writename="ACT15_hierarch_model", noharm.path=noharm.path, dec="," )
summary(mod3)

#############################################################################
# EXAMPLE 3: Rape myth scale | Chapter 15 McDonald (1999)
#############################################################################
data(data.mcdonald.rape)
lambda &lt;- data.mcdonald.rape$lambda
nu &lt;- data.mcdonald.rape$nu

# obtain multiplier for factor loadings (Formula 15.5)
k &lt;- sum( lambda[1,] * lambda[2,] ) / sum( lambda[2,]^2 )
  ##   [1] 1.263243

# additive parameter (Formula 15.7)
c &lt;- sum( lambda[2,]*(nu[1,]-nu[2,]) ) / sum( lambda[2,]^2 )
  ##   [1] 1.247697

# SD in the female group
1/k
  ##   [1] 0.7916132

# M in the female group
- c/k
  ##   [1] -0.9876932

# Burt's coefficient of factorial congruence (Formula 15.10a)
sum( lambda[1,] * lambda[2,] ) / sqrt( sum( lambda[1,]^2 ) * sum( lambda[2,]^2 ) )
  ##   [1] 0.9727831

# congruence for mean parameters
sum(  (nu[1,]-nu[2,]) * lambda[2,] ) / sqrt( sum( (nu[1,]-nu[2,])^2 ) * sum( lambda[2,]^2 ) )
  ##   [1] 0.968176

## End(Not run)
</code></pre>

<hr>
<h2 id='data.mixed1'>
Dataset with Mixed Dichotomous and Polytomous Item Responses
</h2><span id='topic+data.mixed1'></span>

<h3>Description</h3>

<p>Dataset with mixed dichotomous and polytomous item responses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.mixed1)</code></pre>


<h3>Format</h3>

<p>A data frame with 1000 observations on the following 37 variables.
</p>
<p><code>'data.frame':   1000 obs. of  37 variables:</code> <br />
<code> $ I01: num  1 1 1 1 1 1 1 0 1 1 ...</code> <br />
<code> $ I02: num  1 1 1 1 1 1 1 1 0 1 ...</code> <br />
<code> [...]</code> <br />
<code> $ I36: num  1 1 1 1 0 0 0 0 1 1 ...</code> <br />
<code> $ I37: num  0 1 1 1 0 1 0 0 1 1 ...</code> <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.mixed1)
apply( data.mixed1, 2, max )
  ##   I01 I02 I03 I04 I05 I06 I07 I08 I09 I10 I11 I12 I13 I14 I15 I16
  ##     1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
  ##   I17 I18 I19 I20 I21 I22 I23 I24 I25 I26 I27 I28 I29 I30 I31 I32
  ##     1   1   1   1   4   4   1   1   1   1   1   1   1   1   1   1
  ##   I33 I34 I35 I36 I37
  ##     1   1   1   1   1
</code></pre>

<hr>
<h2 id='data.ml'>
Multilevel Datasets
</h2><span id='topic+data.ml'></span><span id='topic+data.ml1'></span><span id='topic+data.ml2'></span>

<h3>Description</h3>

<p>Datasets for conducting multilevel IRT analysis. This dataset is used
in the examples of the function <code><a href="#topic+mcmc.2pno.ml">mcmc.2pno.ml</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.ml1)
data(data.ml2)
</code></pre>


<h3>Format</h3>


<ul>
<li>  <p><code>data.ml1</code>
</p>
<p>A data frame with 2000 student observations in 100 classes on 17 variables.
The first variable <code>group</code> contains the class identifier.
The remaining 16 variables are dichotomous test items.
</p>
<p><code>'data.frame':   2000 obs. of  17 variables:</code> <br />
<code> $ group: num  1001 1001 1001 1001 1001 ...</code> <br />
<code> $ X1   : num  1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ X2   : num  1 1 1 0 1 1 1 1 1 1 ...</code> <br />
<code> $ X3   : num  0 1 1 0 1 0 1 0 1 0 ...</code> <br />
<code> $ X4   : num  1 1 1 0 0 1 1 1 1 1 ...</code> <br />
<code> $ X5   : num  0 0 0 1 1 1 0 0 1 1 ...</code> <br />
<code>[...]</code> <br />
<code> $ X16  : num  0 0 1 0 0 0 1 0 0 0 ...</code> <br />
</p>
</li>
<li>  <p><code>data.ml2</code>
</p>
<p>A data frame with 2000 student observations in 100 classes on 6 variables.
The first variable <code>group</code> contains the class identifier.
The remaining 5 variables are polytomous test items.
</p>
<p><code>'data.frame':   2000 obs. of  6 variables:</code> <br />
<code> $ group: num  1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ X1   : num  2 3 4 3 3 3 1 4 4 3 ...</code> <br />
<code> $ X2   : num  2 2 4 3 3 2 2 3 4 3 ...</code> <br />
<code> $ X3   : num  3 4 5 4 2 3 3 4 4 2 ...</code> <br />
<code> $ X4   : num  2 3 3 2 1 3 1 4 4 3 ...</code> <br />
<code> $ X5   : num  2 3 3 2 3 3 1 3 2 2 ...</code> <br />
</p>
</li></ul>


<hr>
<h2 id='data.noharm'>
Datasets for NOHARM Analysis
</h2><span id='topic+data.noharm18'></span><span id='topic+data.noharmExC'></span>

<h3>Description</h3>

<p>Datasets for analyses in NOHARM (see <code><a href="#topic+R2noharm">R2noharm</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.noharmExC)
data(data.noharm18)
</code></pre>


<h3>Format</h3>


<ul>
<li>  <p><code>data.noharmExC</code> <br />
</p>
<p>The format of this dataset is
</p>
<p><code>'data.frame':   300 obs. of  8 variables:</code> <br />
<code> $ C1: int  1 1 1 1 1 0 1 1 1 1 ...</code> <br />
<code> $ C2: int  1 1 1 1 0 1 1 1 1 1 ...</code> <br />
<code> $ C3: int  1 1 1 1 1 0 0 0 1 1 ...</code> <br />
<code> $ C4: int  0 0 1 1 1 1 1 0 1 0 ...</code> <br />
<code> $ C5: int  1 1 1 1 1 0 0 1 1 0 ...</code> <br />
<code> $ C6: int  1 0 0 0 1 0 1 1 0 1 ...</code> <br />
<code> $ C7: int  1 1 0 0 1 1 0 0 0 1 ...</code> <br />
<code> $ C8: int  1 0 1 0 1 0 1 0 1 1 ...</code> <br />
</p>
</li>
<li> <p><code>data.noharm18</code> <br />
</p>
<p>A data frame with 200 observations on the following 18 variables <code>I01</code>,
..., <code>I18</code>. The format is
</p>
<p><code>'data.frame':   200 obs. of  18 variables:</code> <br />
<code> $ I01: int  1 1 1 1 1 0 1 1 0 1 ...</code> <br />
<code> $ I02: int  1 1 0 1 1 0 1 1 1 1 ...</code> <br />
<code> $ I03: int  1 0 0 1 0 0 1 1 0 1 ...</code> <br />
<code> $ I04: int  0 1 0 1 0 0 0 1 1 1 ...</code> <br />
<code> $ I05: int  1 0 0 0 1 0 1 1 0 1 ...</code> <br />
<code> $ I06: int  1 1 0 1 0 0 1 1 0 1 ...</code> <br />
<code> $ I07: int  1 1 1 1 0 1 1 1 1 1 ...</code> <br />
<code> $ I08: int  1 1 1 1 1 1 1 1 0 1 ...</code> <br />
<code> $ I09: int  1 1 1 1 0 0 1 1 0 1 ...</code> <br />
<code> $ I10: int  1 0 0 1 1 0 1 1 0 1 ...</code> <br />
<code> $ I11: int  1 1 1 1 0 0 1 1 0 1 ...</code> <br />
<code> $ I12: int  0 0 0 0 0 1 0 0 0 0 ...</code> <br />
<code> $ I13: int  1 1 1 1 0 1 1 0 1 1 ...</code> <br />
<code> $ I14: int  1 1 1 0 1 0 1 1 0 1 ...</code> <br />
<code> $ I15: int  1 1 1 0 0 1 1 1 0 1 ...</code> <br />
<code> $ I16: int  1 1 0 1 1 0 1 0 1 1 ...</code> <br />
<code> $ I17: int  0 1 0 0 0 0 1 1 0 1 ...</code> <br />
<code> $ I18: int  0 0 0 0 0 0 0 0 1 0 ...</code> <br />
</p>
</li></ul>


<hr>
<h2 id='data.pars1.rasch'>
Item Parameters for Three Studies Obtained by 1PL and 2PL Estimation
</h2><span id='topic+data.pars1.rasch'></span><span id='topic+data.pars1.2pl'></span>

<h3>Description</h3>

<p>The datasets contain item parameters to be prepared for linking
using the function <code><a href="#topic+linking.haberman">linking.haberman</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.pars1.rasch)
data(data.pars1.2pl)
</code></pre>


<h3>Format</h3>


<ul>
<li><p> The format of <code>data.pars1.rasch</code> is:
</p>
<p><code>'data.frame':   22 obs. of  4 variables:</code> <br />
<code> $ study: chr  "study1" "study1" "study1" "study1" ...</code> <br />
<code> $ item : Factor w/ 12 levels "M133","M176",..: 1 2 3 4 5 1 6 7 3 8 ...</code> <br />
<code> $ a    : num  1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ b    : num  -1.5862 0.40762 1.78031 2.00382 0.00862 ...</code>
</p>
<p>Item slopes <code>a</code> are fixed to 1 in 1PL estimation. Item difficulties
are denoted by <code>b</code>. <br />
</p>
</li>
<li><p> The format of <code>data.pars1.2pl</code> is:
</p>
<p><code>'data.frame':   22 obs. of  4 variables:</code> <br />
<code> $ study: chr  "study1" "study1" "study1" "study1" ...</code> <br />
<code> $ item : Factor w/ 12 levels "M133","M176",..: 1 2 3 4 5 1 6 7 3 8 ...</code> <br />
<code> $ a    : num  1.238 0.957 1.83 1.927 2.298 ...</code> <br />
<code> $ b    : num  -1.16607 0.35844 1.06571 1.17159 0.00792 ...</code> <br />
</p>
</li></ul>


<hr>
<h2 id='data.pirlsmissing'>
Dataset from PIRLS Study with Missing Responses
</h2><span id='topic+data.pirlsmissing'></span>

<h3>Description</h3>

<p>This is a dataset of the PIRLS 2011 study for 4th graders for the reading
booklet 13 (the 'PIRLS reader') and 4 countries (Austria, Germany, France, Netherlands).
Missing responses (missing by intention and not reached) are coded by 9.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.pirlsmissing)
</code></pre>


<h3>Format</h3>

<p>A data frame with 3480 observations on the following 38 variables.
</p>
<p>The format is:
</p>
<p><code>'data.frame':   3480 obs. of  38 variables:</code> <br />
<code> $ idstud  : int  1000001 1000002 1000003 1000004 1000005 ...</code> <br />
<code> $ country : Factor w/ 4 levels "AUT","DEU","FRA",..: 1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ studwgt : num  1.06 1.06 1.06 1.06 1.06 ...</code> <br />
<code> $ R31G01M : int  1 1 1 1 1 1 0 1 1 0 ...</code> <br />
<code> $ R31G02C : int  0 9 0 1 0 0 0 0 1 0 ...</code> <br />
<code> $ R31G03M : int  1 1 1 1 0 1 0 0 1 1 ...</code> <br />
<code> [...]</code> <br />
<code> $ R31P15C : int  1 9 0 1 0 0 0 0 1 0 ...</code> <br />
<code> $ R31P16C : int  0 0 0 0 0 0 0 9 0 1 ...</code> <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data.pirlsmissing)
# inspect missing rates
round( colMeans( data.pirlsmissing==9 ), 3 )
  ##    idstud  country  studwgt  R31G01M  R31G02C  R31G03M  R31G04C  R31G05M
  ##     0.000    0.000    0.000    0.009    0.076    0.012    0.203    0.018
  ##   R31G06M  R31G07M R31G08CZ R31G08CA R31G08CB  R31G09M  R31G10C  R31G11M
  ##     0.010    0.020    0.189    0.225    0.252    0.019    0.126    0.023
  ##   R31G12C R31G13CZ R31G13CA R31G13CB R31G13CC  R31G14M  R31P01M  R31P02C
  ##     0.202    0.170    0.198    0.220    0.223    0.074    0.013    0.039
  ##   R31P03C  R31P04M  R31P05C  R31P06C  R31P07C  R31P08M  R31P09C  R31P10M
  ##     0.056    0.012    0.075    0.043    0.074    0.024    0.062    0.025
  ##   R31P11M  R31P12M  R31P13M  R31P14C  R31P15C  R31P16C
  ##     0.027    0.030    0.030    0.126    0.130    0.127
</code></pre>

<hr>
<h2 id='data.pisaMath'>
Dataset PISA Mathematics
</h2><span id='topic+data.pisaMath'></span>

<h3>Description</h3>

<p>This is an example PISA dataset of reading items from the PISA
2009 study of students from Austria. The
dataset contains 565 students who worked on the 11 reading items
from item cluster M3.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.pisaMath)
</code></pre>


<h3>Format</h3>

<p>The dataset is a list. The list element <code>data</code>
contains the dataset with the demographical variables
student ID (<code>idstud</code>), school ID (<code>idschool</code>), a dummy variable for female
students (<code>female</code>), socioeconomic status (<code>hisei</code>)
and migration background (<code>migra</code>). The remaining
variables (starting with <code>M</code> in the name) are
the mathematics items. <br />
The item metadata are included in the list element
<code>item</code> which contains item name (<code>item</code>) and the
testlet label (<code>testlet</code>). An item not included
in a testlet is indicated by <code>NA</code>.
</p>
<p>The format is:
</p>
<p><code>List of 2</code> <br />
<code> $ data:'data.frame':</code> <br />
<code>  ..$ idstud  : num [1:565] 9e+10 9e+10 9e+10 9e+10 9e+10 ...</code> <br />
<code>  ..$ idschool: int [1:565] 900015 900015 900015 900015  ...</code> <br />
<code>  ..$ female  : int [1:565] 0 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code>  ..$ hisei   : num [1:565] -1.16 -1.099 -1.588 -0.365 -1.588 ...</code> <br />
<code>  ..$ migra   : int [1:565] 0 0 0 0 0 0 0 0 0 1 ...</code> <br />
<code>  ..$ M192Q01 : int [1:565] 1 0 1 1 1 1 1 0 0 0 ...</code> <br />
<code>  ..$ M406Q01 : int [1:565] 1 1 1 0 1 0 0 0 1 0 ...</code> <br />
<code>  ..$ M406Q02 : int [1:565] 1 0 0 0 1 0 0 0 1 0 ...</code> <br />
<code>  ..$ M423Q01 : int [1:565] 0 1 0 1 1 1 1 1 1 0 ...</code> <br />
<code>  ..$ M496Q01 : int [1:565] 1 0 0 0 0 0 0 0 1 0 ...</code> <br />
<code>  ..$ M496Q02 : int [1:565] 1 0 0 1 0 1 0 1 1 0 ...</code> <br />
<code>  ..$ M564Q01 : int [1:565] 1 1 1 1 1 1 0 0 1 0 ...</code> <br />
<code>  ..$ M564Q02 : int [1:565] 1 0 1 1 1 0 0 0 0 0 ...</code> <br />
<code>  ..$ M571Q01 : int [1:565] 1 0 0 0 1 0 0 0 0 0 ...</code> <br />
<code>  ..$ M603Q01 : int [1:565] 1 0 0 0 1 0 0 0 0 0 ...</code> <br />
<code>  ..$ M603Q02 : int [1:565] 1 0 0 0 1 0 0 0 1 0 ...</code> <br />
<code> $ item:'data.frame':</code> <br />
<code>  ..$ item   : Factor w/ 11 levels "M192Q01","M406Q01",..: 1 2 3 4  ...</code> <br />
<code>  ..$ testlet: chr [1:11] NA "M406" "M406" NA ...</code> <br />
</p>

<hr>
<h2 id='data.pisaPars'>
Item Parameters from Two PISA Studies
</h2><span id='topic+data.pisaPars'></span>

<h3>Description</h3>

<p>This data frame contains item parameters from two PISA studies.
Because the Rasch model is used, only item difficulties are considered.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.pisaPars)
</code></pre>


<h3>Format</h3>

<p>A data frame with 25 observations on the following 4 variables.
</p>

<dl>
<dt><code>item</code></dt><dd><p>Item names</p>
</dd>
<dt><code>testlet</code></dt><dd><p>Items are arranged in corresponding testlets. These
names are located in this column.</p>
</dd>
<dt><code>study1</code></dt><dd><p>Item difficulties of study 1</p>
</dd>
<dt><code>study2</code></dt><dd><p>Item difficulties of study 2</p>
</dd>
</dl>


<hr>
<h2 id='data.pisaRead'>
Dataset PISA Reading
</h2><span id='topic+data.pisaRead'></span>

<h3>Description</h3>

<p>This is an example PISA dataset of reading items from the PISA
2009 study of students from Austria. The
dataset contains 623 students who worked on the 12 reading items
from item cluster R7.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.pisaRead)
</code></pre>


<h3>Format</h3>

<p>The dataset is a list. The list element <code>data</code>
contains the dataset with the demographical variables
student ID (<code>idstud</code>), school ID (<code>idschool</code>), a dummy variable for female
students (<code>female</code>), socioeconomic status (<code>hisei</code>)
and migration background (<code>migra</code>). The remaining
variables (starting with <code>R</code> in the name) are
the reading items. <br />
The item metadata are included in the list element
<code>item</code> which contains item name (<code>item</code>),
testlet label (<code>testlet</code>), item format (<code>ItemFormat</code>),
text type (<code>TextType</code>) and text aspect (<code>Aspect</code>).
</p>
<p>The format is:
</p>
<p><code>List of 2</code> <br />
<code> $ data:'data.frame':</code> <br />
<code>  ..$ idstud  : num [1:623] 9e+10 9e+10 9e+10 9e+10 9e+10 ...</code> <br />
<code>  ..$ idschool: int [1:623] 900003 900003 900003 900003 ...</code> <br />
<code>  ..$ female  : int [1:623] 1 0 1 0 0 0 1 0 1 0 ...</code> <br />
<code>  ..$ hisei   : num [1:623] -1.16 -0.671 1.286 0.185 1.225 ...</code> <br />
<code>  ..$ migra   : int [1:623] 0 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code>  ..$ R432Q01 : int [1:623] 1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code>  ..$ R432Q05 : int [1:623] 1 1 1 1 1 0 1 1 1 0 ...</code> <br />
<code>  ..$ R432Q06 : int [1:623] 0 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code>  ..$ R456Q01 : int [1:623] 1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code>  ..$ R456Q02 : int [1:623] 1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code>  ..$ R456Q06 : int [1:623] 1 1 1 1 1 1 0 0 1 1 ...</code> <br />
<code>  ..$ R460Q01 : int [1:623] 1 1 0 0 0 0 0 1 1 1 ...</code> <br />
<code>  ..$ R460Q05 : int [1:623] 1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code>  ..$ R460Q06 : int [1:623] 0 1 1 1 1 1 0 0 1 1 ...</code> <br />
<code>  ..$ R466Q02 : int [1:623] 0 1 0 1 1 0 1 0 0 1 ...</code> <br />
<code>  ..$ R466Q03 : int [1:623] 0 0 0 1 0 0 0 1 0 1 ...</code> <br />
<code>  ..$ R466Q06 : int [1:623] 0 1 1 1 1 1 0 1 1 1 ...</code> <br />
<code> $ item:'data.frame':</code> <br />
<code>  ..$ item      : Factor w/ 12 levels "R432Q01","R432Q05",..: 1 2 3 4  ...</code> <br />
<code>  ..$ testlet   : Factor w/ 4 levels "R432","R456",..: 1 1 1 2  ...</code> <br />
<code>  ..$ ItemFormat: Factor w/ 2 levels "CR","MC": 1 1 2 2 1 1 1 2 2 1 ...</code> <br />
<code>  ..$ TextType  : Factor w/ 3 levels "Argumentation",..: 1 1 1 3  ...</code> <br />
<code>  ..$ Aspect    : Factor w/ 3 levels "Access_and_retrieve",..: 2 3 2 1 ...</code> <br />
</p>

<hr>
<h2 id='data.pw'>
Datasets for Pairwise Comparisons
</h2><span id='topic+data.pw01'></span>

<h3>Description</h3>

<p>Some datasets for pairwise comparisons.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.pw01)
</code></pre>


<h3>Format</h3>

<p>The dataset <code>data.pw01</code> contains results of a German football league
from the season 2000/01.
</p>

<hr>
<h2 id='data.ratings'>
Rating Datasets
</h2><span id='topic+data.ratings'></span><span id='topic+data.ratings1'></span><span id='topic+data.ratings2'></span><span id='topic+data.ratings3'></span>

<h3>Description</h3>

<p>Some rating datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.ratings1)
data(data.ratings2)
data(data.ratings3)
</code></pre>


<h3>Format</h3>


<ul>
<li><p> Dataset <code>data.ratings1</code>: <br />
</p>
<p>Data frame with 274 observations containing 5 criteria (<code>k1</code>, ..., <code>k5</code>),
135 students and 7 raters.
</p>
<p><code>'data.frame':   274 obs. of  7 variables:</code> <br />
<code> $ idstud: int  100020106 100020106 100070101 100070101 100100109  ...</code> <br />
<code> $ rater : Factor w/ 16 levels "db01","db02",..: 3 15 5 10 2 1 5 4 1 5 ...</code> <br />
<code> $ k1    : int  1 1 0 1 2 0 1 3 0 0 ...</code> <br />
<code> $ k2    : int  1 1 1 1 1 0 0 3 0 0 ...</code> <br />
<code> $ k3    : int  1 1 1 1 2 0 0 3 1 0 ...</code> <br />
<code> $ k4    : int  1 1 1 2 1 0 0 2 0 1 ...</code> <br />
<code> $ k5    : int  2 2 1 2 0 1 0 3 1 0 ...</code> <br />
</p>
<p>Data from a 2009 Austrian survey of national educational
standards for 8th graders in German language writing.
Variables <code>k1</code> to <code>k5</code> denote several rating
criteria of writing competency. <br />
</p>
</li>
<li><p> Dataset <code>data.ratings2</code>: <br />
</p>
<p>Data frame with 615 observations containing 5 criteria (<code>k1</code>, ..., <code>k5</code>),
178 students and 16 raters.
</p>
<p><code>'data.frame':   615 obs. of  7 variables:</code> <br />
<code> $ idstud: num  1001 1001 1002 1002 1003 ...</code> <br />
<code> $ rater : chr  "R03" "R15" "R05" "R10" ...</code> <br />
<code> $ k1    : int  1 1 0 1 2 0 1 3 3 0 ...</code> <br />
<code> $ k2    : int  1 1 1 1 1 0 0 3 3 0 ...</code> <br />
<code> $ k3    : int  1 1 1 1 2 0 0 3 3 1 ...</code> <br />
<code> $ k4    : int  1 1 1 2 1 0 0 2 2 0 ...</code> <br />
<code> $ k5    : int  2 2 1 2 0 1 0 3 2 1 ...</code> <br />
</p>
</li>
<li><p> Dataset <code>data.ratings3</code>: <br />
</p>
<p>Data frame with 3169 observations containing 4 criteria (<code>crit2</code>, ..., <code>crit6</code>),
561 students and 52 raters.
</p>
<p><code>'data.frame':   3169 obs. of  6 variables:</code> <br />
<code> $ idstud: num  10001 10001 10002 10002 10003 ...</code> <br />
<code> $ rater : num  840 838 842 808 830 845 813 849 809 802 ...</code> <br />
<code> $ crit2 : int  1 3 3 1 2 2 2 2 3 3 ...</code> <br />
<code> $ crit3 : int  2 2 2 2 2 2 2 2 3 3 ...</code> <br />
<code> $ crit4 : int  1 2 2 2 1 1 1 2 2 2 ...</code> <br />
<code> $ crit6 : num  4 4 4 3 4 4 4 4 4 4 ...</code> <br />
</p>
</li></ul>


<hr>
<h2 id='data.raw1'>
Dataset with Raw Item Responses
</h2><span id='topic+data.raw1'></span>

<h3>Description</h3>

<p>Dataset with raw item responses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.raw1)</code></pre>


<h3>Format</h3>

<p>A data frame with raw item responses of 1200 persons on the following 77 items:
</p>
<p><code>'data.frame':   1200 obs. of  77 variables:</code> <br />
<code> $ I101: num  0 0 0 2 0 0 0 0 0 0 ...</code> <br />
<code> $ I102: int  NA NA 2 1 2 1 3 2 NA NA ...</code> <br />
<code> $ I103: int  1 1 NA NA NA NA NA NA 1 1 ...</code> <br />
<code> ...</code> <br />
<code> $ I179: chr  "E" "C" "D" "E" ...</code> <br />
</p>

<hr>
<h2 id='data.read'>
Dataset Reading
</h2><span id='topic+data.read'></span>

<h3>Description</h3>

<p>This dataset contains <code class="reqn">N=328</code> students and <code class="reqn">I=12</code> items measuring reading
competence. All 12 items are arranged into 3 testlets (items with common
text stimulus) labeled as
A, B and C. The allocation of items to testlets is indicated by their
variable names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.read)
</code></pre>


<h3>Format</h3>

<p>A data frame with 328 persons on the following 12 variables.
Rows correspond to persons and columns to items. The following items are
included in <code>data.read</code>:
</p>
<p>Testlet A: <code>A1</code>, <code>A2</code>, <code>A3</code>, <code>A4</code>
</p>
<p>Testlet B: <code>B1</code>, <code>B2</code>, <code>B3</code>, <code>B4</code>
</p>
<p>Testlet C: <code>C1</code>, <code>C2</code>, <code>C3</code>, <code>C4</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(data.read)
dat &lt;- data.read
I &lt;- ncol(dat)

# list of needed packages for the following examples
packages &lt;- scan(what="character")
     eRm  ltm  TAM mRm  CDM  mirt psychotools  IsingFit  igraph  qgraph  pcalg
     poLCA  randomLCA psychomix MplusAutomation lavaan

# load packages. make an installation if necessary
miceadds::library_install(packages)

#*****************************************************
# Model 1: Rasch model
#*****************************************************

#----  M1a: rasch.mml2 (in sirt)
mod1a &lt;- sirt::rasch.mml2(dat)
summary(mod1a)

#----  M1b: smirt (in sirt)
Qmatrix &lt;- matrix(1,nrow=I, ncol=1)
mod1b &lt;- sirt::smirt(dat,Qmatrix=Qmatrix)
summary(mod1b)

#----  M1c: gdm (in CDM)
theta.k &lt;- seq(-6,6,len=21)
mod1c &lt;- CDM::gdm(dat,theta.k=theta.k,irtmodel="1PL", skillspace="normal")
summary(mod1c)

#----  M1d: tam.mml (in TAM)
mod1d &lt;- TAM::tam.mml( resp=dat )
summary(mod1d)

#----  M1e: RM (in eRm)
mod1e &lt;- eRm::RM( dat )
  # eRm uses Conditional Maximum Likelihood (CML) as the estimation method.
summary(mod1e)
eRm::plotPImap(mod1e)

#----  M1f: mrm (in mRm)
mod1f &lt;- mRm::mrm( dat, cl=1)   # CML estimation
mod1f$beta  # item parameters

#----  M1g: mirt (in mirt)
mod1g &lt;- mirt::mirt( dat, model=1, itemtype="Rasch", verbose=TRUE )
print(mod1g)
summary(mod1g)
coef(mod1g)
    # arrange coefficients in nicer layout
sirt::mirt.wrapper.coef(mod1g)$coef

#----  M1h: rasch (in ltm)
mod1h &lt;- ltm::rasch( dat, control=list(verbose=TRUE ) )
summary(mod1h)
coef(mod1h)

#----  M1i: RaschModel.fit (in psychotools)
mod1i &lt;- psychotools::RaschModel.fit(dat)  # CML estimation
summary(mod1i)
plot(mod1i)

#----  M1j: noharm.sirt (in sirt)
Fpatt &lt;- matrix( 0, I, 1 )
Fval &lt;- 1 + 0*Fpatt
Ppatt &lt;- Pval &lt;- matrix(1,1,1)
mod1j &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt, Fpatt=Fpatt, Fval=Fval, Pval=Pval)
summary(mod1j)
  #   Normal-ogive model, multiply item discriminations with constant D=1.7.
  #   The same holds for other examples with noharm.sirt and R2noharm.
plot(mod1j)

#----  M1k: rasch.pml3 (in sirt)
mod1k &lt;- sirt::rasch.pml3( dat=dat)
  #         pairwise marginal maximum likelihood estimation
summary(mod1k)

#----  M1l: running Mplus (using MplusAutomation package)
mplus_path &lt;- "c:/Mplus7/Mplus.exe"    # locate Mplus executable
#****************
  # specify Mplus object
mplusmod &lt;- MplusAutomation::mplusObject(
    TITLE="1PL in Mplus ;",
    VARIABLE=paste0( "CATEGORICAL ARE ", paste0(colnames(dat),collapse=" ") ),
    MODEL="
       ! fix all item loadings to 1
       F1 BY A1@1 A2@1 A3@1 A4@1 ;
       F1 BY B1@1 B2@1 B3@1 B4@1 ;
       F1 BY C1@1 C2@1 C3@1 C4@1 ;
       ! estimate variance
       F1 ;
            ",
    ANALYSIS="ESTIMATOR=MLR;",
    OUTPUT="stand;",
    usevariables=colnames(dat),  rdata=dat )
#****************

  # write Mplus syntax
filename &lt;- "mod1u"   # specify file name
  # create Mplus syntaxes
res2 &lt;- MplusAutomation::mplusModeler(object=mplusmod, dataout=paste0(filename,".dat"),
               modelout=paste0(filename,".inp"), run=0 )
  # run Mplus model
MplusAutomation::runModels( filefilter=paste0(filename,".inp"), Mplus_command=mplus_path)
  # alternatively, the system() command can also be used
  # get results
mod1l &lt;- MplusAutomation::readModels(target=getwd(), filefilter=filename )
mod1l$summaries    # summaries
mod1l$parameters$unstandardized   # parameter estimates

#*****************************************************
# Model 2: 2PL model
#*****************************************************

#----  M2a: rasch.mml2 (in sirt)
mod2a &lt;- sirt::rasch.mml2(dat, est.a=1:I)
summary(mod2a)

#----  M2b: smirt (in sirt)
mod2b &lt;- sirt::smirt(dat,Qmatrix=Qmatrix,est.a="2PL")
summary(mod2b)

#----  M2c: gdm (in CDM)
mod2c &lt;- CDM::gdm(dat,theta.k=theta.k,irtmodel="2PL", skillspace="normal")
summary(mod2c)

#----  M2d: tam.mml (in TAM)
mod2d &lt;- TAM::tam.mml.2pl( resp=dat )
summary(mod2d)

#----  M2e: mirt (in mirt)
mod2e &lt;- mirt::mirt( dat, model=1, itemtype="2PL" )
print(mod2e)
summary(mod2e)
sirt::mirt.wrapper.coef(mod1g)$coef

#----  M2f: ltm (in ltm)
mod2f &lt;- ltm::ltm( dat ~ z1, control=list(verbose=TRUE ) )
summary(mod2f)
coef(mod2f)
plot(mod2f)

#----  M2g: R2noharm (in NOHARM, running from within R using sirt package)
  # define noharm.path where 'NoharmCL.exe' is located
noharm.path &lt;- "c:/NOHARM"
  # covariance matrix
P.pattern &lt;- matrix( 1, ncol=1, nrow=1 )
P.init &lt;- P.pattern
P.init[1,1] &lt;- 1
  # loading matrix
F.pattern &lt;- matrix(1,I,1)
F.init &lt;- F.pattern
  # estimate model
mod2g &lt;- sirt::R2noharm( dat=dat, model.type="CFA", F.pattern=F.pattern,
             F.init=F.init, P.pattern=P.pattern, P.init=P.init,
             writename="ex2g", noharm.path=noharm.path, dec="," )
summary(mod2g)

#----  M2h: noharm.sirt (in sirt)
mod2h &lt;- sirt::noharm.sirt( dat=dat, Ppatt=P.pattern,Fpatt=F.pattern,
              Fval=F.init, Pval=P.init )
summary(mod2h)
plot(mod2h)

#----  M2i: rasch.pml2 (in sirt)
mod2i &lt;- sirt::rasch.pml2(dat, est.a=1:I)
summary(mod2i)

#----  M2j: WLSMV estimation with cfa (in lavaan)
lavmodel &lt;- "F=~ A1+A2+A3+A4+B1+B2+B3+B4+
                        C1+C2+C3+C4"
mod2j &lt;- lavaan::cfa( data=dat, model=lavmodel, std.lv=TRUE, ordered=colnames(dat))
summary(mod2j, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#*****************************************************
# Model 3: 3PL model (note that results can be quite unstable!)
#*****************************************************

#----  M3a: rasch.mml2 (in sirt)
mod3a &lt;- sirt::rasch.mml2(dat, est.a=1:I, est.c=1:I)
summary(mod3a)

#----  M3b: smirt (in sirt)
mod3b &lt;- sirt::smirt(dat,Qmatrix=Qmatrix,est.a="2PL", est.c=1:I)
summary(mod3b)

#----  M3c: mirt (in mirt)
mod3c &lt;- mirt::mirt( dat, model=1, itemtype="3PL", verbose=TRUE)
summary(mod3c)
coef(mod3c)
  # stabilize parameter estimating using informative priors for guessing parameters
mirtmodel &lt;- mirt::mirt.model("
            F=1-12
            PRIOR=(1-12, g, norm, -1.38, 0.25)
            ")
  # a prior N(-1.38,.25) is specified for transformed guessing parameters: qlogis(g)
  # simulate values from this prior for illustration
N &lt;- 100000
logit.g &lt;- stats::rnorm(N, mean=-1.38, sd=sqrt(.5) )
graphics::plot( stats::density(logit.g) )  # transformed qlogis(g)
graphics::plot( stats::density( stats::plogis(logit.g)) )  # g parameters
  # estimate 3PL with priors
mod3c1 &lt;- mirt::mirt(dat, mirtmodel, itemtype="3PL",verbose=TRUE)
coef(mod3c1)
  # In addition, set upper bounds for g parameters of .35
mirt.pars &lt;- mirt::mirt( dat, mirtmodel, itemtype="3PL",  pars="values")
ind &lt;- which( mirt.pars$name=="g" )
mirt.pars[ ind, "value" ] &lt;- stats::plogis(-1.38)
mirt.pars[ ind, "ubound" ] &lt;- .35
  # prior distribution for slopes
ind &lt;- which( mirt.pars$name=="a1" )
mirt.pars[ ind, "prior_1" ] &lt;- 1.3
mirt.pars[ ind, "prior_2" ] &lt;- 2
mod3c2 &lt;- mirt::mirt(dat, mirtmodel, itemtype="3PL",
                pars=mirt.pars,verbose=TRUE, technical=list(NCYCLES=100) )
coef(mod3c2)
sirt::mirt.wrapper.coef(mod3c2)

#----  M3d: ltm (in ltm)
mod3d &lt;- ltm::tpm( dat, control=list(verbose=TRUE), max.guessing=.3)
summary(mod3d)
coef(mod3d) #=&gt; numerical instabilities

#*****************************************************
# Model 4: 3-dimensional Rasch model
#*****************************************************

# define Q-matrix
Q &lt;- matrix( 0, nrow=12, ncol=3 )
Q[ cbind(1:12, rep(1:3,each=4) ) ] &lt;- 1
rownames(Q) &lt;- colnames(dat)
colnames(Q) &lt;- c("A","B","C")

# define nodes
theta.k &lt;- seq(-6,6,len=13)

#----  M4a: smirt (in sirt)
mod4a &lt;- sirt::smirt(dat,Qmatrix=Q,irtmodel="comp", theta.k=theta.k, maxiter=30)
summary(mod4a)

#----  M4b: rasch.mml2 (in sirt)
mod4b &lt;- sirt::rasch.mml2(dat,Q=Q,theta.k=theta.k, mmliter=30)
summary(mod4b)

#----  M4c: gdm (in CDM)
mod4c &lt;- CDM::gdm( dat, irtmodel="1PL", theta.k=theta.k, skillspace="normal",
            Qmatrix=Q, maxiter=30, centered.latent=TRUE )
summary(mod4c)

#----  M4d: tam.mml (in TAM)
mod4d &lt;- TAM::tam.mml( resp=dat, Q=Q, control=list(nodes=theta.k, maxiter=30) )
summary(mod4d)

#----  M4e: R2noharm (in NOHARM, running from within R using sirt package)
noharm.path &lt;- "c:/NOHARM"
  # covariance matrix
P.pattern &lt;- matrix( 1, ncol=3, nrow=3 )
P.init &lt;- 0.8+0*P.pattern
diag(P.init) &lt;- 1
  # loading matrix
F.pattern &lt;- 0*Q
F.init &lt;- Q
  # estimate model
mod4e &lt;- sirt::R2noharm( dat=dat, model.type="CFA", F.pattern=F.pattern,
    F.init=F.init, P.pattern=P.pattern, P.init=P.init,
    writename="ex4e", noharm.path=noharm.path, dec="," )
summary(mod4e)

#----  M4f: mirt (in mirt)
cmodel &lt;- mirt::mirt.model("
     F1=1-4
     F2=5-8
     F3=9-12
     # equal item slopes correspond to the Rasch model
     CONSTRAIN=(1-4, a1), (5-8, a2), (9-12,a3)
     COV=F1*F2, F1*F3, F2*F3
     " )
mod4f &lt;- mirt::mirt(dat, cmodel, verbose=TRUE)
summary(mod4f)

#*****************************************************
# Model 5: 3-dimensional 2PL model
#*****************************************************

#----  M5a: smirt (in sirt)
mod5a &lt;- sirt::smirt(dat,Qmatrix=Q,irtmodel="comp", est.a="2PL", theta.k=theta.k,
                 maxiter=30)
summary(mod5a)

#----  M5b: rasch.mml2 (in sirt)
mod5b &lt;- sirt::rasch.mml2(dat,Q=Q,theta.k=theta.k,est.a=1:12, mmliter=30)
summary(mod5b)

#----  M5c: gdm (in CDM)
mod5c &lt;- CDM::gdm( dat, irtmodel="2PL", theta.k=theta.k, skillspace="loglinear",
            Qmatrix=Q, maxiter=30, centered.latent=TRUE,
            standardized.latent=TRUE)
summary(mod5c)

#----  M5d: tam.mml (in TAM)
mod5d &lt;- TAM::tam.mml.2pl( resp=dat, Q=Q, control=list(nodes=theta.k, maxiter=30) )
summary(mod5d)

#----  M5e: R2noharm (in NOHARM, running from within R using sirt package)
noharm.path &lt;- "c:/NOHARM"
  # covariance matrix
P.pattern &lt;- matrix( 1, ncol=3, nrow=3 )
diag(P.pattern) &lt;- 0
P.init &lt;- 0.8+0*P.pattern
diag(P.init) &lt;- 1
  # loading matrix
F.pattern &lt;- Q
F.init &lt;- Q
  # estimate model
mod5e &lt;- sirt::R2noharm( dat=dat, model.type="CFA", F.pattern=F.pattern,
    F.init=F.init, P.pattern=P.pattern, P.init=P.init,
    writename="ex5e", noharm.path=noharm.path, dec="," )
summary(mod5e)

#----  M5f: mirt (in mirt)
cmodel &lt;- mirt::mirt.model("
   F1=1-4
   F2=5-8
   F3=9-12
   COV=F1*F2, F1*F3, F2*F3
   "  )
mod5f &lt;- mirt::mirt(dat, cmodel, verbose=TRUE)
summary(mod5f)

#*****************************************************
# Model 6: Network models (Graphical models)
#*****************************************************

#----  M6a: Ising model using the IsingFit package (undirected graph)
#        - fit Ising model using the "OR rule" (AND=FALSE)
mod6a &lt;- IsingFit::IsingFit(x=dat, family="binomial", AND=FALSE)
summary(mod6a)
##           Network Density:                 0.29
##    Gamma:                  0.25
##    Rule used:              Or-rule
# plot results
qgraph::qgraph(mod6a$weiadj,fade=FALSE)

#**-- graph estimation using pcalg package

# some packages from Bioconductor must be downloaded at first (if not yet done)
if (FALSE){  # set 'if (TRUE)' if packages should be downloaded
     source("http://bioconductor.org/biocLite.R")
     biocLite("RBGL")
     biocLite("Rgraphviz")
}

#----  M6b: graph estimation based on Pearson correlations
V &lt;- colnames(dat)
n &lt;- nrow(dat)
mod6b &lt;- pcalg::pc(suffStat=list(C=stats::cor(dat), n=n ),
             indepTest=gaussCItest, ## indep.test: partial correlations
             alpha=0.05, labels=V, verbose=TRUE)
plot(mod6b)
# plot in qgraph package
qgraph::qgraph(mod6b, label.color=rep( c( "red", "blue","darkgreen" ), each=4 ),
         edge.color="black")
summary(mod6b)

#----  M6c: graph estimation based on tetrachoric correlations
mod6c &lt;- pcalg::pc(suffStat=list(C=sirt::tetrachoric2(dat)$rho, n=n ),
             indepTest=gaussCItest, alpha=0.05, labels=V, verbose=TRUE)
plot(mod6c)
summary(mod6c)

#----  M6d: Statistical implicative analysis (in sirt)
mod6d &lt;- sirt::sia.sirt(dat, significance=.85 )
  # plot results with igraph and qgraph package
plot( mod6d$igraph.obj, vertex.shape="rectangle", vertex.size=30 )
qgraph::qgraph( mod6d$adj.matrix )

#*****************************************************
# Model 7: Latent class analysis with 3 classes
#*****************************************************

#----  M7a: randomLCA (in randomLCA)
  #        - use two trials of starting values
mod7a &lt;- randomLCA::randomLCA(dat,nclass=3, notrials=2, verbose=TRUE)
summary(mod7a)
plot(mod7a,type="l", xlab="Item")

#----  M7b: rasch.mirtlc (in sirt)
mod7b &lt;- sirt::rasch.mirtlc( dat, Nclasses=3,seed=-30,  nstarts=2 )
summary(mod7b)
matplot( t(mod7b$pjk), type="l", xlab="Item" )

#----  M7c: poLCA (in poLCA)
  #   define formula for outcomes
f7c &lt;- paste0( "cbind(", paste0(colnames(dat),collapse=","), ") ~ 1 " )
dat1 &lt;- as.data.frame( dat + 1 ) # poLCA needs integer values from 1,2,..
mod7c &lt;- poLCA::poLCA( stats::as.formula(f7c),dat1,nclass=3, verbose=TRUE)
plot(mod7c)

#----  M7d: gom.em (in sirt)
  #    - the latent class model is a special grade of membership model
mod7d &lt;- sirt::gom.em( dat, K=3, problevels=c(0,1),  model="GOM"  )
summary(mod7d)

#---- - M7e: mirt (in mirt)
  # define three latent classes
Theta &lt;- diag(3)
  # define mirt model
I &lt;- ncol(dat)  # I=12
mirtmodel &lt;- mirt::mirt.model("
        C1=1-12
        C2=1-12
        C3=1-12
        ")
  # get initial parameter values
mod.pars &lt;- mirt::mirt(dat, model=mirtmodel,  pars="values")
  # modify parameters: only slopes refer to item-class probabilities
set.seed(9976)
  # set starting values for class specific item probabilities
mod.pars[ mod.pars$name=="d","value" ]  &lt;- 0
mod.pars[ mod.pars$name=="d","est" ]  &lt;- FALSE
b1 &lt;- stats::qnorm( colMeans( dat ) )
mod.pars[ mod.pars$name=="a1","value" ]  &lt;- b1
  # random starting values for other classes
mod.pars[ mod.pars$name %in% c("a2","a3"),"value" ]  &lt;- b1 + stats::runif(12*2,-1,1)
mod.pars
  #** define prior for latent class analysis
lca_prior &lt;- function(Theta,Etable){
  # number of latent Theta classes
  TP &lt;- nrow(Theta)
  # prior in initial iteration
  if ( is.null(Etable) ){
    prior &lt;- rep( 1/TP, TP )
  }
  # process Etable (this is correct for datasets without missing data)
  if ( ! is.null(Etable) ){
    # sum over correct and incorrect expected responses
    prior &lt;- ( rowSums(Etable[, seq(1,2*I,2)]) + rowSums(Etable[,seq(2,2*I,2)]) )/I
  }
  prior &lt;- prior / sum(prior)
  return(prior)
}
  #** estimate model
mod7e &lt;- mirt::mirt(dat, mirtmodel, pars=mod.pars, verbose=TRUE,
            technical=list( customTheta=Theta, customPriorFun=lca_prior) )
  # compare estimated results
print(mod7e)
summary(mod7b)
  # The number of estimated parameters is incorrect because mirt does not correctly count
  # estimated parameters from the user customized  prior distribution.
mod7e@nest &lt;- as.integer(sum(mod.pars$est) + 2)  # two additional class probabilities
  # extract log-likelihood
mod7e@logLik
  # compute AIC and BIC
( AIC &lt;- -2*mod7e@logLik+2*mod7e@nest )
( BIC &lt;- -2*mod7e@logLik+log(mod7e@Data$N)*mod7e@nest )
  # RMSEA and SRMSR fit statistic
mirt::M2(mod7e)     # TLI and CFI does not make sense in this example
  #** extract item parameters
sirt::mirt.wrapper.coef(mod7e)
  #** extract class-specific item-probabilities
probs &lt;- apply( coef1[, c("a1","a2","a3") ], 2, stats::plogis )
matplot( probs, type="l", xlab="Item", main="mirt::mirt")
  #** inspect estimated distribution
mod7e@Theta
mod7e@Prior[[1]]

#*****************************************************
# Model 8: Mixed Rasch model with two classes
#*****************************************************

#----  M8a: raschmix (in psychomix)
mod8a &lt;- psychomix::raschmix(data=as.matrix(dat), k=2, scores="saturated")
summary(mod8a)

#----  M8b: mrm (in mRm)
mod8b &lt;- mRm::mrm(data.matrix=dat, cl=2)
mod8b$conv.to.bound
plot(mod8b)
print(mod8b)

#----  M8c: mirt (in mirt)
  #* define theta grid
theta.k &lt;- seq( -5, 5, len=9 )
TP &lt;- length(theta.k)
Theta &lt;- matrix( 0, nrow=2*TP, ncol=4)
Theta[1:TP,1:2] &lt;- cbind(theta.k, 1 )
Theta[1:TP + TP,3:4] &lt;- cbind(theta.k, 1 )
Theta
  # define model
I &lt;- ncol(dat)  # I=12
mirtmodel &lt;- mirt::mirt.model("
        F1a=1-12  # slope Class 1
        F1b=1-12  # difficulty Class 1
        F2a=1-12  # slope Class 2
        F2b=1-12  # difficulty Class 2
        CONSTRAIN=(1-12,a1),(1-12,a3)
        ")
  # get initial parameter values
mod.pars &lt;- mirt::mirt(dat, model=mirtmodel,  pars="values")
  # set starting values for class specific item probabilities
mod.pars[ mod.pars$name=="d","value" ]  &lt;- 0
mod.pars[ mod.pars$name=="d","est" ]  &lt;- FALSE
mod.pars[ mod.pars$name=="a1","value" ]  &lt;- 1
mod.pars[ mod.pars$name=="a3","value" ]  &lt;- 1
  # initial values difficulties
b1 &lt;-  stats::qlogis( colMeans(dat) )
mod.pars[ mod.pars$name=="a2","value" ]  &lt;- b1
mod.pars[ mod.pars$name=="a4","value" ]  &lt;- b1 + stats::runif(I, -1, 1)
  #* define prior for mixed Rasch analysis
mixed_prior &lt;- function(Theta,Etable){
  NC &lt;- 2   # number of theta classes
  TP &lt;- nrow(Theta) / NC
  prior1 &lt;- stats::dnorm( Theta[1:TP,1] )
  prior1 &lt;- prior1 / sum(prior1)
  if ( is.null(Etable) ){   prior &lt;- c( prior1, prior1 ) }
  if ( ! is.null(Etable) ){
    prior &lt;- ( rowSums( Etable[, seq(1,2*I,2)] ) +
                   rowSums( Etable[,seq(2,2*I,2)]) )/I
    a1 &lt;- stats::aggregate( prior, list( rep(1:NC, each=TP) ), sum )
    a1[,2] &lt;- a1[,2] / sum( a1[,2])
    # print some information during estimation
    cat( paste0( " Class proportions: ",
              paste0( round(a1[,2], 3 ), collapse=" " ) ), "\n")
    a1 &lt;- rep( a1[,2], each=TP )
    # specify mixture of two normal distributions
    prior &lt;- a1*c(prior1,prior1)
  }
  prior &lt;- prior / sum(prior)
  return(prior)
}
  #* estimate model
mod8c &lt;- mirt::mirt(dat, mirtmodel, pars=mod.pars, verbose=TRUE,
        technical=list(  customTheta=Theta, customPriorFun=mixed_prior ) )
  # Like in Model 7e, the number of estimated parameters must be included.
mod8c@nest &lt;- as.integer(sum(mod.pars$est) + 1)
      # two class proportions and therefore one probability is freely estimated.
  #* extract item parameters
sirt::mirt.wrapper.coef(mod8c)
  #* estimated distribution
mod8c@Theta
mod8c@Prior

#----  M8d: tamaan (in TAM)

tammodel &lt;- "
ANALYSIS:
  TYPE=MIXTURE ;
  NCLASSES(2);
  NSTARTS(7,20);
LAVAAN MODEL:
  F=~ A1__C4
  F ~~ F
ITEM TYPE:
  ALL(Rasch);
    "
mod8d &lt;- TAM::tamaan( tammodel, resp=dat )
summary(mod8d)
# plot item parameters
I &lt;- 12
ipars &lt;- mod8d$itempartable_MIXTURE[ 1:I, ]
plot( 1:I, ipars[,3], type="o", ylim=range( ipars[,3:4] ), pch=16,
        xlab="Item", ylab="Item difficulty")
lines( 1:I, ipars[,4], type="l", col=2, lty=2)
points( 1:I, ipars[,4],  col=2, pch=2)

#*****************************************************
# Model 9: Mixed 2PL model with two classes
#*****************************************************

#----  M9a: tamaan (in TAM)

tammodel &lt;- "
ANALYSIS:
  TYPE=MIXTURE ;
  NCLASSES(2);
  NSTARTS(10,30);
LAVAAN MODEL:
  F=~ A1__C4
  F ~~ F
ITEM TYPE:
  ALL(2PL);
    "
mod9a &lt;- TAM::tamaan( tammodel, resp=dat )
summary(mod9a)

#*****************************************************
# Model 10: Rasch testlet model
#*****************************************************

#----  M10a: tam.fa (in TAM)
dims &lt;- substring( colnames(dat),1,1 )  # define dimensions
mod10a &lt;- TAM::tam.fa( resp=dat, irtmodel="bifactor1", dims=dims,
                control=list(maxiter=60) )
summary(mod10a)

#----  M10b: mirt (in mirt)
cmodel &lt;- mirt::mirt.model("
        G=1-12
        A=1-4
        B=5-8
        C=9-12
        CONSTRAIN=(1-12,a1), (1-4, a2), (5-8, a3), (9-12,a4)
      ")
mod10b &lt;- mirt::mirt(dat, model=cmodel, verbose=TRUE)
summary(mod10b)
coef(mod10b)
mod10b@logLik   # equivalent is slot( mod10b, "logLik")

#alternatively, using a dimensional reduction approach (faster and better accuracy)
cmodel &lt;- mirt::mirt.model("
      G=1-12
      CONSTRAIN=(1-12,a1), (1-4, a2), (5-8, a3), (9-12,a4)
     ")
item_bundles &lt;- rep(c(1,2,3), each=4)
mod10b1 &lt;- mirt::bfactor(dat, model=item_bundles, model2=cmodel, verbose=TRUE)
coef(mod10b1)

#----  M10c: smirt (in sirt)
  # define Q-matrix
Qmatrix &lt;- matrix(0,12,4)
Qmatrix[,1] &lt;- 1
Qmatrix[ cbind( 1:12, match( dims, unique(dims)) +1 ) ]  &lt;- 1
  # uncorrelated factors
variance.fixed &lt;- cbind( c(1,1,1,2,2,3), c(2,3,4,3,4,4), 0 )
  # estimate model
mod10c &lt;- sirt::smirt( dat, Qmatrix=Qmatrix, irtmodel="comp",
              variance.fixed=variance.fixed, qmcnodes=1000, maxiter=60)
summary(mod10c)

#*****************************************************
# Model 11: Bifactor model
#*****************************************************

#----  M11a: tam.fa (in TAM)
dims &lt;- substring( colnames(dat),1,1 )  # define dimensions
mod11a &lt;- TAM::tam.fa( resp=dat, irtmodel="bifactor2", dims=dims,
                 control=list(maxiter=60) )
summary(mod11a)

#----  M11b: bfactor (in mirt)
dims1 &lt;- match( dims, unique(dims) )
mod11b &lt;- mirt::bfactor(dat, model=dims1, verbose=TRUE)
summary(mod11b)
coef(mod11b)
mod11b@logLik

#----  M11c: smirt (in sirt)
  # define Q-matrix
Qmatrix &lt;- matrix(0,12,4)
Qmatrix[,1] &lt;- 1
Qmatrix[ cbind( 1:12, match( dims, unique(dims)) +1 ) ]  &lt;- 1
  # uncorrelated factors
variance.fixed &lt;- cbind( c(1,1,1,2,2,3), c(2,3,4,3,4,4), 0 )
  # estimate model
mod11c &lt;- sirt::smirt( dat, Qmatrix=Qmatrix, irtmodel="comp", est.a="2PL",
                variance.fixed=variance.fixed, qmcnodes=1000, maxiter=60)
summary(mod11c)

#*****************************************************
# Model 12: Located latent class model: Rasch model with three theta classes
#*****************************************************

# use 10th item as the reference item
ref.item &lt;- 10
# ability grid
theta.k &lt;- seq(-4,4,len=9)

#----  M12a: rasch.mirtlc (in sirt)
mod12a &lt;- sirt::rasch.mirtlc(dat, Nclasses=3, modeltype="MLC1", ref.item=ref.item)
summary(mod12a)

#----  M12b: gdm (in CDM)
theta.k &lt;- seq(-1, 1, len=3)      # initial matrix
b.constraint &lt;- matrix( c(10,1,0), nrow=1,ncol=3)
  # estimate model
mod12b &lt;- CDM::gdm( dat, theta.k=theta.k, skillspace="est", irtmodel="1PL",
              b.constraint=b.constraint, maxiter=200)
summary(mod12b)

#----  M12c: mirt (in mirt)
items &lt;- colnames(dat)
  # define three latent classes
Theta &lt;- diag(3)
  # define mirt model
I &lt;- ncol(dat)  # I=12
mirtmodel &lt;- mirt::mirt.model("
        C1=1-12
        C2=1-12
        C3=1-12
        CONSTRAIN=(1-12,a1),(1-12,a2),(1-12,a3)
        ")
  # get parameters
mod.pars &lt;- mirt(dat, model=mirtmodel,  pars="values")
 # set starting values for class specific item probabilities
mod.pars[ mod.pars$name=="d","value" ]  &lt;- stats::qlogis( colMeans(dat,na.rm=TRUE) )
  # set item difficulty of reference item to zero
ind &lt;- which( ( paste(mod.pars$item)==items[ref.item] ) &amp;
               ( ( paste(mod.pars$name)=="d" ) ) )
mod.pars[ ind,"value" ]  &lt;- 0
mod.pars[ ind,"est" ]  &lt;- FALSE
  # initial values for a1, a2 and a3
mod.pars[ mod.pars$name %in% c("a1","a2","a3"),"value" ]  &lt;- c(-1,0,1)
mod.pars
  #* define prior for latent class analysis
lca_prior &lt;- function(Theta,Etable){
  # number of latent Theta classes
  TP &lt;- nrow(Theta)
  # prior in initial iteration
  if ( is.null(Etable) ){
    prior &lt;- rep( 1/TP, TP )
              }
  # process Etable (this is correct for datasets without missing data)
  if ( ! is.null(Etable) ){
    # sum over correct and incorrect expected responses
    prior &lt;- ( rowSums( Etable[, seq(1,2*I,2)] ) + rowSums( Etable[, seq(2,2*I,2)] ) )/I
            }
  prior &lt;- prior / sum(prior)
  return(prior)
   }
 #* estimate model
mod12c &lt;- mirt(dat, mirtmodel, technical=list(
            customTheta=Theta, customPriorFun=lca_prior),
            pars=mod.pars, verbose=TRUE )
  # estimated parameters from the user customized  prior distribution.
mod12c@nest &lt;- as.integer(sum(mod.pars$est) + 2)
  #* extract item parameters
coef1 &lt;- sirt::mirt.wrapper.coef(mod12c)
  #* inspect estimated distribution
mod12c@Theta
coef1$coef[1,c("a1","a2","a3")]
mod12c@Prior[[1]]

#*****************************************************
# Model 13: Multidimensional model with discrete traits
#*****************************************************
# define Q-Matrix
Q &lt;- matrix( 0, nrow=12,ncol=3)
Q[1:4,1] &lt;- 1
Q[5:8,2] &lt;- 1
Q[9:12,3] &lt;- 1
# define discrete theta distribution with 3 dimensions
Theta &lt;- scan(what="character",nlines=1)
  000 100 010 001 110 101 011 111
Theta &lt;- as.numeric( unlist( lapply( Theta, strsplit, split="")   ) )
Theta &lt;- matrix(Theta, 8, 3, byrow=TRUE )
Theta

#----  Model 13a: din (in CDM)
mod13a &lt;- CDM::din( dat, q.matrix=Q, rule="DINA")
summary(mod13a)
# compare used Theta distributions
cbind( Theta, mod13a$attribute.patt.splitted)

#----  Model 13b: gdm (in CDM)
mod13b &lt;- CDM::gdm( dat, Qmatrix=Q, theta.k=Theta, skillspace="full")
summary(mod13b)

#----  Model 13c: mirt (in mirt)
  # define mirt model
I &lt;- ncol(dat)  # I=12
mirtmodel &lt;- mirt::mirt.model("
        F1=1-4
        F2=5-8
        F3=9-12
        ")
  # get parameters
mod.pars &lt;- mirt(dat, model=mirtmodel,  pars="values")
# starting values d parameters (transformed guessing parameters)
ind &lt;- which(  mod.pars$name=="d"  )
mod.pars[ind,"value"] &lt;- stats::qlogis(.2)
# starting values transformed slipping parameters
ind &lt;- which( ( mod.pars$name %in% paste0("a",1:3)  ) &amp;  ( mod.pars$est ) )
mod.pars[ind,"value"] &lt;- stats::qlogis(.8) - stats::qlogis(.2)
mod.pars

  #* define prior for latent class analysis
lca_prior &lt;- function(Theta,Etable){
  TP &lt;- nrow(Theta)
  if ( is.null(Etable) ){
    prior &lt;- rep( 1/TP, TP )
              }
  if ( ! is.null(Etable) ){
    prior &lt;- ( rowSums( Etable[, seq(1,2*I,2)] ) + rowSums( Etable[, seq(2,2*I,2)] ) )/I
            }
  prior &lt;- prior / sum(prior)
  return(prior)
}
 #* estimate model
mod13c &lt;- mirt(dat, mirtmodel, technical=list(
            customTheta=Theta, customPriorFun=lca_prior),
            pars=mod.pars, verbose=TRUE )
  # estimated parameters from the user customized  prior distribution.
mod13c@nest &lt;- as.integer(sum(mod.pars$est) + 2)
  #* extract item parameters
coef13c &lt;- sirt::mirt.wrapper.coef(mod13c)$coef
  #* inspect estimated distribution
mod13c@Theta
mod13c@Prior[[1]]

 #-* comparisons of estimated  parameters
# extract guessing and slipping parameters from din
dfr &lt;- coef(mod13a)[, c("guess","slip") ]
colnames(dfr) &lt;- paste0("din.",c("guess","slip") )
# estimated parameters from gdm
dfr$gdm.guess &lt;- stats::plogis(mod13b$item$b)
dfr$gdm.slip &lt;- 1 - stats::plogis( rowSums(mod13b$item[,c("b.Cat1","a.F1","a.F2","a.F3")] ) )
# estimated parameters from mirt
dfr$mirt.guess &lt;- stats::plogis( coef13c$d )
dfr$mirt.slip &lt;- 1 - stats::plogis( rowSums(coef13c[,c("d","a1","a2","a3")]) )
# comparison
round(dfr[, c(1,3,5,2,4,6)],3)
  ##      din.guess gdm.guess mirt.guess din.slip gdm.slip mirt.slip
  ##   A1     0.691     0.684      0.686    0.000    0.000     0.000
  ##   A2     0.491     0.489      0.489    0.031    0.038     0.036
  ##   A3     0.302     0.300      0.300    0.184    0.193     0.190
  ##   A4     0.244     0.239      0.240    0.337    0.340     0.339
  ##   B1     0.568     0.579      0.577    0.163    0.148     0.151
  ##   B2     0.329     0.344      0.340    0.344    0.326     0.329
  ##   B3     0.817     0.827      0.825    0.014    0.007     0.009
  ##   B4     0.431     0.463      0.456    0.104    0.089     0.092
  ##   C1     0.188     0.191      0.189    0.013    0.013     0.013
  ##   C2     0.050     0.050      0.050    0.239    0.238     0.239
  ##   C3     0.000     0.002      0.001    0.065    0.065     0.065
  ##   C4     0.000     0.004      0.000    0.212    0.212     0.212

# estimated class sizes
dfr &lt;- data.frame( "Theta"=Theta, "din"=mod13a$attribute.patt$class.prob,
                   "gdm"=mod13b$pi.k, "mirt"=mod13c@Prior[[1]])
# comparison
round(dfr,3)
  ##     Theta.1 Theta.2 Theta.3   din   gdm  mirt
  ##   1       0       0       0 0.039 0.041 0.040
  ##   2       1       0       0 0.008 0.009 0.009
  ##   3       0       1       0 0.009 0.007 0.008
  ##   4       0       0       1 0.394 0.417 0.412
  ##   5       1       1       0 0.011 0.011 0.011
  ##   6       1       0       1 0.017 0.042 0.037
  ##   7       0       1       1 0.042 0.008 0.016
  ##   8       1       1       1 0.480 0.465 0.467

#*****************************************************
# Model 14: DINA model with two skills
#*****************************************************

# define some simple Q-Matrix (does not really make in this application)
Q &lt;- matrix( 0, nrow=12,ncol=2)
Q[1:4,1] &lt;- 1
Q[5:8,2] &lt;- 1
Q[9:12,1:2] &lt;- 1
# define discrete theta distribution with 3 dimensions
Theta &lt;- scan(what="character",nlines=1)
  00 10 01 11
Theta &lt;- as.numeric( unlist( lapply( Theta, strsplit, split="")   ) )
Theta &lt;- matrix(Theta, 4, 2, byrow=TRUE )
Theta

#----  Model 14a: din (in CDM)
mod14a &lt;- CDM::din( dat, q.matrix=Q, rule="DINA")
summary(mod14a)
# compare used Theta distributions
cbind( Theta, mod14a$attribute.patt.splitted)

#----  Model 14b: mirt (in mirt)
  # define mirt model
I &lt;- ncol(dat)  # I=12
mirtmodel &lt;- mirt::mirt.model("
        F1=1-4
        F2=5-8
        (F1*F2)=9-12
        ")
#-&gt; constructions like (F1*F2*F3) are also allowed in mirt.model
  # get parameters
mod.pars &lt;- mirt(dat, model=mirtmodel,  pars="values")
# starting values d parameters (transformed guessing parameters)
ind &lt;- which(  mod.pars$name=="d"  )
mod.pars[ind,"value"] &lt;- stats::qlogis(.2)
# starting values transformed slipping parameters
ind &lt;- which( ( mod.pars$name %in% paste0("a",1:3)  ) &amp;  ( mod.pars$est ) )
mod.pars[ind,"value"] &lt;- stats::qlogis(.8) - stats::qlogis(.2)
mod.pars
 #* use above defined prior lca_prior
 # lca_prior &lt;- function(prior,Etable) ...
 #* estimate model
mod14b &lt;- mirt(dat, mirtmodel, technical=list(
            customTheta=Theta, customPriorFun=lca_prior),
            pars=mod.pars, verbose=TRUE )
  # estimated parameters from the user customized  prior distribution.
mod14b@nest &lt;- as.integer(sum(mod.pars$est) + 2)
  #* extract item parameters
coef14b &lt;- sirt::mirt.wrapper.coef(mod14b)$coef

 #-* comparisons of estimated  parameters
# extract guessing and slipping parameters from din
dfr &lt;- coef(mod14a)[, c("guess","slip") ]
colnames(dfr) &lt;- paste0("din.",c("guess","slip") )
# estimated parameters from mirt
dfr$mirt.guess &lt;- stats::plogis( coef14b$d )
dfr$mirt.slip &lt;- 1 - stats::plogis( rowSums(coef14b[,c("d","a1","a2","a3")]) )
# comparison
round(dfr[, c(1,3,2,4)],3)
  ##      din.guess mirt.guess din.slip mirt.slip
  ##   A1     0.674      0.671    0.030     0.030
  ##   A2     0.423      0.420    0.049     0.050
  ##   A3     0.258      0.255    0.224     0.225
  ##   A4     0.245      0.243    0.394     0.395
  ##   B1     0.534      0.543    0.166     0.164
  ##   B2     0.338      0.347    0.382     0.380
  ##   B3     0.796      0.802    0.016     0.015
  ##   B4     0.421      0.436    0.142     0.140
  ##   C1     0.850      0.851    0.000     0.000
  ##   C2     0.480      0.480    0.097     0.097
  ##   C3     0.746      0.746    0.026     0.026
  ##   C4     0.575      0.577    0.136     0.137

# estimated class sizes
dfr &lt;- data.frame( "Theta"=Theta, "din"=mod13a$attribute.patt$class.prob,
                    "mirt"=mod14b@Prior[[1]])
# comparison
round(dfr,3)
  ##     Theta.1 Theta.2   din  mirt
  ##   1       0       0 0.357 0.369
  ##   2       1       0 0.044 0.049
  ##   3       0       1 0.047 0.031
  ##   4       1       1 0.553 0.551

#*****************************************************
# Model 15: Rasch model with non-normal distribution
#*****************************************************

# A non-normal theta distributed is specified by log-linear smoothing
# the distribution as described in
# Xu, X., &amp; von Davier, M. (2008). Fitting the structured general diagnostic model
# to NAEP data. ETS Research Report ETS RR-08-27. Princeton, ETS.

# define theta grid
theta.k &lt;- matrix( seq(-4,4,len=15), ncol=1 )
# define design matrix for smoothing (up to cubic moments)
delta.designmatrix &lt;- cbind( 1, theta.k, theta.k^2, theta.k^3 )
# constrain item difficulty of fifth item (item B1) to zero
b.constraint &lt;- matrix( c(5,1,0), ncol=3 )

#----  Model 15a: gdm (in CDM)
mod15a &lt;- CDM::gdm( dat, irtmodel="1PL", theta.k=theta.k,
               b.constraint=b.constraint  )
summary(mod15a)
 # plot estimated distribution
graphics::barplot( mod15a$pi.k[,1], space=0, names.arg=round(theta.k[,1],2),
           main="Estimated Skewed Distribution (gdm function)")

#----  Model 15b: mirt (in mirt)
 # define mirt model
mirtmodel &lt;- mirt::mirt.model("
    F=1-12
    ")
 # get parameters
mod.pars &lt;- mirt::mirt(dat, model=mirtmodel, pars="values", itemtype="Rasch")
  # fix variance (just for correct counting of parameters)
mod.pars[ mod.pars$name=="COV_11", "est"] &lt;- FALSE
  # fix item difficulty
ind &lt;- which( ( mod.pars$item=="B1" ) &amp; ( mod.pars$name=="d" ) )
mod.pars[ ind, "value"] &lt;- 0
mod.pars[ ind, "est"] &lt;- FALSE

 # define prior
loglinear_prior &lt;- function(Theta,Etable){
    TP &lt;- nrow(Theta)
    if ( is.null(Etable) ){
    prior &lt;- rep( 1/TP, TP )
           }
    # process Etable (this is correct for datasets without missing data)
    if ( ! is.null(Etable) ){
          # sum over correct and incorrect expected responses
       prior &lt;- ( rowSums( Etable[, seq(1,2*I,2)] ) + rowSums( Etable[, seq(2,2*I,2)] ) )/I
       # smooth prior using the above design matrix and a log-linear model
       # see Xu &amp; von Davier (2008).
       y &lt;- log( prior + 1E-15 )
       lm1 &lt;- lm( y ~ 0 + delta.designmatrix, weights=prior )
       prior &lt;- exp(fitted(lm1))   # smoothed prior
           }
    prior &lt;- prior / sum(prior)
    return(prior)
}

#* estimate model
mod15b &lt;- mirt::mirt(dat, mirtmodel, technical=list(
                customTheta=theta.k, customPriorFun=loglinear_prior ),
                pars=mod.pars, verbose=TRUE )
# estimated parameters from the user customized prior distribution.
mod15b@nest &lt;- as.integer(sum(mod.pars$est) + 3)
#* extract item parameters
coef1 &lt;- sirt::mirt.wrapper.coef(mod15b)$coef

#** compare estimated item parameters
dfr &lt;- data.frame( "gdm"=mod15a$item$b.Cat1, "mirt"=coef1$d )
rownames(dfr) &lt;- colnames(dat)
round(t(dfr),4)
  ##            A1     A2      A3      A4 B1      B2     B3     B4     C1    C2     C3    C4
  ##   gdm  0.9818 0.1538 -0.7837 -1.3197  0 -1.0902 1.6088 -0.170 1.9778 0.006 1.1859 0.135
  ##   mirt 0.9829 0.1548 -0.7826 -1.3186  0 -1.0892 1.6099 -0.169 1.9790 0.007 1.1870 0.136
# compare estimated theta distribution
dfr &lt;- data.frame( "gdm"=mod15a$pi.k, "mirt"=mod15b@Prior[[1]] )
round(t(dfr),4)
  ##        1 2     3     4      5      6      7      8      9     10     11     12     13
  ##   gdm  0 0 1e-04 9e-04 0.0056 0.0231 0.0652 0.1299 0.1881 0.2038 0.1702 0.1129 0.0612
  ##   mirt 0 0 1e-04 9e-04 0.0056 0.0232 0.0653 0.1300 0.1881 0.2038 0.1702 0.1128 0.0611
  ##            14    15
  ##   gdm  0.0279 0.011
  ##   mirt 0.0278 0.011

## End(Not run)
</code></pre>

<hr>
<h2 id='data.reck'>
Datasets from Reckase' Book <em>Multidimensional Item Response Theory</em>
</h2><span id='topic+data.reck'></span><span id='topic+data.reck21'></span><span id='topic+data.reck61DAT1'></span><span id='topic+data.reck61DAT2'></span><span id='topic+data.reck73C1a'></span><span id='topic+data.reck73C1b'></span><span id='topic+data.reck75C2'></span><span id='topic+data.reck78ExA'></span><span id='topic+data.reck79ExB'></span>

<h3>Description</h3>

<p>Some simulated datasets from Reckase (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.reck21)
data(data.reck61DAT1)
data(data.reck61DAT2)
data(data.reck73C1a)
data(data.reck73C1b)
data(data.reck75C2)
data(data.reck78ExA)
data(data.reck79ExB)
</code></pre>


<h3>Format</h3>


<ul>
<li><p> The format of the <code>data.reck21</code> (Table 2.1, p. 45) is:
</p>
<p><code>List of 2</code> <br />
<code> $ data: num [1:2500, 1:50] 0 0 0 1 1 0 0 0 1 0 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:50] "I0001" "I0002" "I0003" "I0004" ...</code> <br />
<code> $ pars:'data.frame':</code> <br />
<code>  ..$ a: num [1:50] 1.83 1.38 1.47 1.53 0.88 0.82 1.02 1.19 1.15 0.18 ...</code> <br />
<code>  ..$ b: num [1:50] 0.91 0.81 0.06 -0.8 0.24 0.99 1.23 -0.47 2.78 -3.85 ...</code> <br />
<code>  ..$ c: num [1:50] 0 0 0 0.25 0.21 0.29 0.26 0.19 0 0.21 ...</code> <br />
</p>
</li>
<li><p> The format of the datasets <code>data.reck61DAT1</code> and
<code>data.reck61DAT2</code> (Table 6.1, p. 153) is
</p>
<p><code>List of 4</code> <br />
<code> $ data : num [1:2500, 1:30] 1 0 0 1 1 0 0 1 1 0 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:30] "A01" "A02" "A03" "A04" ...</code> <br />
<code> $ pars :'data.frame':</code> <br />
<code>  ..$ a1: num [1:30] 0.747 0.46 0.861 1.014 0.552 ...</code> <br />
<code>  ..$ a2: num [1:30] 0.025 0.0097 0.0067 0.008 0.0204 0.0064 0.0861 ...</code> <br />
<code>  ..$ a3: num [1:30] 0.1428 0.0692 0.404 0.047 0.1482 ...</code> <br />
<code>  ..$ d : num [1:30] 0.183 -0.192 -0.466 -0.434 -0.443 ...</code> <br />
<code> $ mu   : num [1:3] -0.4 -0.7 0.1</code> <br />
<code> $ sigma: num [1:3, 1:3] 1.21 0.297 1.232 0.297 0.81 ...</code> <br />
</p>
<p>The dataset <code>data.reck61DAT2</code> has correlated dimensions while
<code>data.reck61DAT1</code> has uncorrelated dimensions.
</p>
</li>
<li><p> Datasets <code>data.reck73C1a</code> and <code>data.reck73C1b</code> use item parameters
from Table 7.3 (p. 188). The dataset <code>C1a</code> has uncorrelated dimensions,
while <code>C1b</code> has perfectly correlated dimensions. The items are sensitive to
3 dimensions. The format of the datasets is
</p>
<p><code>List of 4</code> <br />
<code> $ data : num [1:2500, 1:30] 1 0 1 1 1 0 1 1 1 1 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:30] "A01" "A02" "A03" "A04" ...</code> <br />
<code> $ pars :'data.frame':  30 obs. of  4 variables:</code> <br />
<code>  ..$ a1: num [1:30] 0.747 0.46 0.861 1.014 0.552 ...</code> <br />
<code>  ..$ a2: num [1:30] 0.025 0.0097 0.0067 0.008 0.0204 0.0064 ...</code> <br />
<code>  ..$ a3: num [1:30] 0.1428 0.0692 0.404 0.047 0.1482 ...</code> <br />
<code>  ..$ d : num [1:30] 0.183 -0.192 -0.466 -0.434 -0.443 ...</code> <br />
<code> $ mu   : num [1:3] 0 0 0</code> <br />
<code> $ sigma: num [1:3, 1:3] 0.167 0.236 0.289 0.236 0.334 ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.reck75C2</code> is simulated using item parameters
from Table 7.5 (p. 191). It contains items which are sensitive to only
one dimension but individuals which have abilities in three
uncorrelated dimensions. The format is
</p>
<p><code>List of 4</code> <br />
<code> $ data : num [1:2500, 1:30] 0 0 1 1 1 0 0 1 1 1 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:30] "A01" "A02" "A03" "A04" ...</code> <br />
<code> $ pars :'data.frame':  30 obs. of  4 variables:</code> <br />
<code>  ..$ a1: num [1:30] 0.56 0.48 0.67 0.57 0.54 0.74 0.7 0.59 0.63 0.64 ...</code> <br />
<code>  ..$ a2: num [1:30] 0.62 0.53 0.63 0.69 0.58 0.69 0.75 0.63 0.64 0.64 ...</code> <br />
<code>  ..$ a3: num [1:30] 0.46 0.42 0.43 0.51 0.41 0.48 0.46 0.5 0.51 0.46 ...</code> <br />
<code>  ..$ d : num [1:30] 0.1 0.06 -0.38 0.46 0.14 0.31 0.06 -1.23 0.47 1.06 ...</code> <br />
<code> $ mu   : num [1:3] 0 0 0</code> <br />
<code> $ sigma: num [1:3, 1:3] 1 0 0 0 1 0 0 0 1</code> <br />
</p>
</li>
<li><p> The dataset <code>data.reck78ExA</code> contains simulated item responses
from Table 7.8 (p. 204 ff.). There are three item clusters and
two ability dimensions. The format is
</p>
<p><code>List of 4</code> <br />
<code> $ data : num [1:2500, 1:50] 0 1 1 0 1 0 0 0 0 0 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:50] "A01" "A02" "A03" "A04" ...</code> <br />
<code> $ pars :'data.frame':  50 obs. of  3 variables:</code> <br />
<code>  ..$ a1: num [1:50] 0.889 1.057 1.047 1.178 1.029 ...</code> <br />
<code>  ..$ a2: num [1:50] 0.1399 0.0432 0.016 0.0231 0.2347 ...</code> <br />
<code>  ..$ d : num [1:50] 0.2724 1.2335 -0.0918 -0.2372 0.8471 ...</code> <br />
<code> $ mu   : num [1:2] 0 0</code> <br />
<code> $ sigma: num [1:2, 1:2] 1 0 0 1</code> <br />
</p>
</li>
<li><p> The dataset <code>data.reck79ExB</code> contains simulated item responses
from Table 7.9 (p. 207 ff.). There are three item clusters and
three ability dimensions. The format is
</p>
<p><code>List of 4</code> <br />
<code> $ data : num [1:2500, 1:50] 1 1 0 1 0 0 0 1 1 0 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:50] "A01" "A02" "A03" "A04" ...</code> <br />
<code> $ pars :'data.frame':  50 obs. of  4 variables:</code> <br />
<code>  ..$ a1: num [1:50] 0.895 1.032 1.036 1.163 1.022 ...</code> <br />
<code>  ..$ a2: num [1:50] 0.052 0.132 0.144 0.13 0.165 ...</code> <br />
<code>  ..$ a3: num [1:50] 0.0722 0.1923 0.0482 0.1321 0.204 ...</code> <br />
<code>  ..$ d : num [1:50] 0.2724 1.2335 -0.0918 -0.2372 0.8471 ...</code> <br />
<code> $ mu   : num [1:3] 0 0 0</code> <br />
<code> $ sigma: num [1:3, 1:3] 1 0 0 0 1 0 0 0 1</code> <br />
</p>
</li></ul>



<h3>Source</h3>

<p>Simulated datasets
</p>


<h3>References</h3>

<p>Reckase, M. (2009). <em>Multidimensional item response theory</em>.
New York: Springer.
<a href="https://doi.org/10.1007/978-0-387-89976-3">doi:10.1007/978-0-387-89976-3</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: data.reck21 dataset, Table 2.1, p. 45
#############################################################################
data(data.reck21)

dat &lt;- data.reck21$dat      # extract dataset

# items with zero guessing parameters
guess0 &lt;- c( 1, 2, 3, 9,11,27,30,35,45,49,50 )
I &lt;- ncol(dat)

#***
# Model 1: 3PL estimation using rasch.mml2
est.c &lt;- est.a &lt;- 1:I
est.c[ guess0 ] &lt;- 0
mod1 &lt;- sirt::rasch.mml2( dat, est.a=est.a, est.c=est.c, mmliter=300 )
summary(mod1)

#***
# Model 2: 3PL estimation using smirt
Q &lt;- matrix(1,I,1)
mod2 &lt;- sirt::smirt( dat, Qmatrix=Q, est.a="2PL", est.c=est.c, increment.factor=1.01)
summary(mod2)

#***
# Model 3: estimation in mirt package
library(mirt)
itemtype &lt;- rep("3PL", I )
itemtype[ guess0 ] &lt;- "2PL"
mod3 &lt;- mirt::mirt(dat, 1, itemtype=itemtype, verbose=TRUE)
summary(mod3)

c3 &lt;- unlist( coef(mod3) )[ 1:(4*I) ]
c3 &lt;- matrix( c3, I, 4, byrow=TRUE )
# compare estimates of rasch.mml2, smirt and true parameters
round( cbind( mod1$item$c, mod2$item$c,c3[,3],data.reck21$pars$c ), 2 )
round( cbind( mod1$item$a, mod2$item$a.Dim1,c3[,1], data.reck21$pars$a ), 2 )
round( cbind( mod1$item$b, mod2$item$b.Dim1 / mod2$item$a.Dim1, - c3[,2] / c3[,1],
            data.reck21$pars$b ), 2 )

#############################################################################
# EXAMPLE 2: data.reck61 dataset, Table 6.1, p. 153
#############################################################################

data(data.reck61DAT1)
dat &lt;- data.reck61DAT1$data

#***
# Model 1: Exploratory factor analysis

#-- Model 1a: tam.fa in TAM
library(TAM)
mod1a &lt;- TAM::tam.fa( dat, irtmodel="efa", nfactors=3 )
# varimax rotation
varimax(mod1a$B.stand)

# Model 1b: EFA in NOHARM (Promax rotation)
mod1b &lt;- sirt::R2noharm( dat=dat, model.type="EFA",  dimensions=3,
              writename="reck61__3dim_efa", noharm.path="c:/NOHARM",dec=",")
summary(mod1b)

# Model 1c: EFA with noharm.sirt
mod1c &lt;- sirt::noharm.sirt( dat=dat, dimensions=3  )
summary(mod1c)
plot(mod1c)

# Model 1d: EFA with 2 dimensions in noharm.sirt
mod1d &lt;- sirt::noharm.sirt( dat=dat, dimensions=2  )
summary(mod1d)
plot(mod1d, efa.load.min=.2)   # plot loadings of at least .20

#***
# Model 2: Confirmatory factor analysis

#-- Model 2a: tam.fa in TAM
dims &lt;- c( rep(1,10), rep(3,10), rep(2,10)  )
Qmatrix &lt;- matrix( 0, nrow=30, ncol=3 )
Qmatrix[ cbind( 1:30, dims) ] &lt;- 1
mod2a &lt;- TAM::tam.mml.2pl( dat,Q=Qmatrix,
            control=list( snodes=1000, QMC=TRUE, maxiter=200) )
summary(mod2a)

#-- Model 2b: smirt in sirt
mod2b &lt;- sirt::smirt( dat,Qmatrix=Qmatrix, est.a="2PL", maxiter=20, qmcnodes=1000 )
summary(mod2b)

#-- Model 2c: rasch.mml2 in sirt
mod2c &lt;- sirt::rasch.mml2( dat,Qmatrix=Qmatrix, est.a=1:30,
                mmliter=200, theta.k=seq(-5,5,len=11) )
summary(mod2c)

#-- Model 2d: mirt in mirt
cmodel &lt;- mirt::mirt.model("
     F1=1-10
     F2=21-30
     F3=11-20
     COV=F1*F2, F1*F3, F2*F3 " )
mod2d &lt;- mirt::mirt(dat, cmodel, verbose=TRUE)
summary(mod2d)
coef(mod2d)

#-- Model 2e: CFA in NOHARM
# specify covariance pattern
P.pattern &lt;- matrix( 1, ncol=3, nrow=3 )
P.init &lt;- .4*P.pattern
diag(P.pattern) &lt;- 0
diag(P.init) &lt;- 1
# fix all entries in the loading matrix to 1
F.pattern &lt;- matrix( 0, nrow=30, ncol=3 )
F.pattern[1:10,1] &lt;- 1
F.pattern[21:30,2] &lt;- 1
F.pattern[11:20,3] &lt;- 1
F.init &lt;- F.pattern
# estimate model
mod2e &lt;- sirt::R2noharm( dat=dat, model.type="CFA", P.pattern=P.pattern,
            P.init=P.init, F.pattern=F.pattern, F.init=F.init,
            writename="reck61__3dim_cfa", noharm.path="c:/NOHARM",dec=",")
summary(mod2e)

#-- Model 2f: CFA with noharm.sirt
mod2f &lt;- sirt::noharm.sirt( dat=dat, Fval=F.init, Fpatt=F.pattern,
                 Pval=P.init, Ppatt=P.pattern )
summary(mod2f)

#############################################################################
# EXAMPLE 3: DETECT analysis data.reck78ExA and data.reck79ExB
#############################################################################

data(data.reck78ExA)
data(data.reck79ExB)

#************************
# Example A
dat &lt;- data.reck78ExA$data
#- estimate person score
score &lt;- stats::qnorm( ( rowMeans( dat )+.5 )  / ( ncol(dat) + 1 ) )
#- extract item cluster
itemcluster &lt;- substring( colnames(dat), 1, 1 )
#- confirmatory DETECT Item cluster
detectA &lt;- sirt::conf.detect( data=dat, score=score, itemcluster=itemcluster )
  ##          unweighted weighted
  ##   DETECT      0.571    0.571
  ##   ASSI        0.523    0.523
  ##   RATIO       0.757    0.757

#- exploratory DETECT analysis
detect_explA &lt;- sirt::expl.detect(data=dat, score, nclusters=10, N.est=nrow(dat)/2  )
  ##  Optimal Cluster Size is  5  (Maximum of DETECT Index)
  ##     N.Cluster N.items N.est N.val         size.cluster DETECT.est ASSI.est
  ##   1         2      50  1250  1250                31-19      0.531    0.404
  ##   2         3      50  1250  1250             10-19-21      0.554    0.407
  ##   3         4      50  1250  1250           10-19-14-7      0.630    0.509
  ##   4         5      50  1250  1250         10-19-3-7-11      0.653    0.546
  ##   5         6      50  1250  1250       10-12-7-3-7-11      0.593    0.458
  ##   6         7      50  1250  1250      10-12-7-3-7-9-2      0.604    0.474
  ##   7         8      50  1250  1250    10-12-7-3-3-9-4-2      0.608    0.481
  ##   8         9      50  1250  1250  10-12-7-3-3-5-4-2-4      0.617    0.494
  ##   9        10      50  1250  1250 10-5-7-7-3-3-5-4-2-4      0.592    0.460

# cluster membership
cluster_membership &lt;- detect_explA$itemcluster$cluster3
# Cluster 1:
colnames(dat)[ cluster_membership==1 ]
  ##   [1] "A01" "A02" "A03" "A04" "A05" "A06" "A07" "A08" "A09" "A10"
# Cluster 2:
colnames(dat)[ cluster_membership==2 ]
  ##    [1] "B11" "B12" "B13" "B14" "B15" "B16" "B17" "B18" "B19" "B20" "B21" "B22"
  ##   [13] "B23" "B25" "B26" "B27" "B28" "B29" "B30"
# Cluster 3:
colnames(dat)[ cluster_membership==3 ]
  ##    [1] "B24" "C31" "C32" "C33" "C34" "C35" "C36" "C37" "C38" "C39" "C40" "C41"
  ##   [13] "C42" "C43" "C44" "C45" "C46" "C47" "C48" "C49" "C50"

#************************
# Example B
dat &lt;- data.reck79ExB$data
#- estimate person score
score &lt;- stats::qnorm( ( rowMeans( dat )+.5 )  / ( ncol(dat) + 1 ) )
#- extract item cluster
itemcluster &lt;- substring( colnames(dat), 1, 1 )
#- confirmatory DETECT Item cluster
detectB &lt;- sirt::conf.detect( data=dat, score=score, itemcluster=itemcluster )
  ##          unweighted weighted
  ##   DETECT      0.715    0.715
  ##   ASSI        0.624    0.624
  ##   RATIO       0.855    0.855

#- exploratory DETECT analysis
detect_explB &lt;- sirt::expl.detect(data=dat, score, nclusters=10, N.est=nrow(dat)/2  )
  ##   Optimal Cluster Size is  4  (Maximum of DETECT Index)
  ##
  ##     N.Cluster N.items N.est N.val         size.cluster DETECT.est ASSI.est
  ##   1         2      50  1250  1250                30-20      0.665    0.546
  ##   2         3      50  1250  1250             10-20-20      0.686    0.585
  ##   3         4      50  1250  1250           10-20-8-12      0.728    0.644
  ##   4         5      50  1250  1250         10-6-14-8-12      0.654    0.553
  ##   5         6      50  1250  1250       10-6-14-3-12-5      0.659    0.561
  ##   6         7      50  1250  1250      10-6-14-3-7-5-5      0.664    0.576
  ##   7         8      50  1250  1250     10-6-7-7-3-7-5-5      0.616    0.518
  ##   8         9      50  1250  1250   10-6-7-7-3-5-5-5-2      0.612    0.512
  ##   9        10      50  1250  1250 10-6-7-7-3-5-3-5-2-2      0.613    0.512

## End(Not run)
</code></pre>

<hr>
<h2 id='data.sirt'>
Some Example Datasets for the <code>sirt</code> Package
</h2><span id='topic+data.sirt'></span><span id='topic+data.si01'></span><span id='topic+data.si02'></span><span id='topic+data.si03'></span><span id='topic+data.si04'></span><span id='topic+data.si05'></span><span id='topic+data.si06'></span><span id='topic+data.si07'></span><span id='topic+data.si08'></span><span id='topic+data.si09'></span><span id='topic+data.si10'></span>

<h3>Description</h3>

<p>Some example datasets for the <code>sirt</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.si01)
data(data.si02)
data(data.si03)
data(data.si04)
data(data.si05)
data(data.si06)
data(data.si07)
data(data.si08)
data(data.si09)
data(data.si10)
</code></pre>


<h3>Format</h3>


<ul>
<li><p> The format of the dataset <code>data.si01</code> is:
</p>
<p><code>'data.frame':   1857 obs. of  3 variables:</code> <br />
<code> $ idgroup: int  1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ item1  : int  NA NA NA NA NA NA NA NA NA NA ...</code> <br />
<code> $ item2  : int  4 4 4 4 4 4 4 2 4 4 ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.si02</code> is the Stouffer-Toby-dataset published
in Lindsay, Clogg and Grego (1991; Table 1, p.97, Cross-classification A):
</p>
<p><code>List of 2</code> <br />
<code> $ data   : num [1:16, 1:4] 1 0 1 0 1 0 1 0 1 0 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:4] "I1" "I2" "I3" "I4"</code> <br />
<code> $ weights: num [1:16] 42 1 6 2 6 1 7 2 23 4 ...</code> <br />
</p>
</li>
<li><p> The format of the dataset <code>data.si03</code> (containing item
parameters of two studies) is:
</p>
<p><code>'data.frame':   27 obs. of  3 variables:</code> <br />
<code> $ item    : Factor w/ 27 levels "M1","M10","M11",..: 1 12 21 22 ...</code> <br />
<code> $ b_study1: num  0.297 1.163 0.151 -0.855 -1.653 ...</code> <br />
<code> $ b_study2: num  0.72 1.118 0.351 -0.861 -1.593 ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.si04</code> is adapted from Bartolucci, Montanari
and Pandolfi (2012; Table 4, Table 7). The data contains 4999 persons,
79 items on 5 dimensions. See <code><a href="#topic+rasch.mirtlc">rasch.mirtlc</a></code> for using the
data in an analysis.
</p>
<p><code>List of 3</code> <br />
<code> $ data        : num [1:4999, 1:79] 0 1 1 0 1 1 0 0 1 1 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:79] "A01" "A02" "A03" "A04" ...</code> <br />
<code> $ itempars    :'data.frame':   79 obs. of  4 variables:</code> <br />
<code>  ..$ item      : Factor w/ 79 levels "A01","A02","A03",..: 1 2 3 4 5 6 7 8 9 10 ...</code> <br />
<code>  ..$ dim       : num [1:79] 1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code>  ..$ gamma     : num [1:79] 1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code>  ..$ gamma.beta: num [1:79] -0.189 0.25 0.758 1.695 1.022 ...</code> <br />
<code> $ distribution: num [1:9, 1:7] 1 2 3 4 5 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:7] "class" "A" "B" "C" ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.si05</code> contains double ratings of two
exchangeable raters for three items which are in <code>Ex1</code>, <code>Ex2</code>
and <code>Ex3</code>, respectively.
</p>
<p><code>List of 3</code> <br />
<code> $ Ex1:'data.frame':    199 obs. of  2 variables:</code> <br />
<code>  ..$ C7040: num [1:199] NA 1 0 1 1 0 0 0 1 0 ...</code> <br />
<code>  ..$ C7041: num [1:199] 1 1 0 0 0 0 0 0 1 0 ...</code> <br />
<code> $ Ex2:'data.frame':    2000 obs. of  2 variables:</code> <br />
<code>  ..$ rater1: num [1:2000] 2 0 3 1 2 2 0 0 0 0 ...</code> <br />
<code>  ..$ rater2: num [1:2000] 4 1 3 2 1 0 0 0 0 2 ...</code> <br />
<code> $ Ex3:'data.frame':    2000 obs. of  2 variables:</code> <br />
<code>  ..$ rater1: num [1:2000] 5 1 6 2 3 3 0 0 0 0 ...</code> <br />
<code>  ..$ rater2: num [1:2000] 7 2 6 3 2 1 0 1 0 3 ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.si06</code> contains multiple choice item
responses. The correct alternative is denoted as 0, distractors
are indicated by the codes 1, 2 or 3.
</p>
<p><code>'data.frame':   4441 obs. of  14 variables:</code> <br />
<code> $ WV01: num  0 0 0 0 0 0 0 0 0 3 ...</code> <br />
<code> $ WV02: num  0 0 0 3 0 0 0 0 0 1 ...</code> <br />
<code> $ WV03: num  0 1 0 0 0 0 0 0 0 0 ...</code> <br />
<code> $ WV04: num  0 0 0 0 0 0 0 0 0 1 ...</code> <br />
<code> $ WV05: num  3 1 1 1 0 0 1 1 0 2 ...</code> <br />
<code> $ WV06: num  0 1 3 0 0 0 2 0 0 1 ...</code> <br />
<code> $ WV07: num  0 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code> $ WV08: num  0 1 1 0 0 0 0 0 0 0 ...</code> <br />
<code> $ WV09: num  0 0 0 0 0 0 0 0 0 2 ...</code> <br />
<code> $ WV10: num  1 1 3 0 0 2 0 0 0 0 ...</code> <br />
<code> $ WV11: num  0 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code> $ WV12: num  0 0 0 2 0 0 2 0 0 0 ...</code> <br />
<code> $ WV13: num  3 1 1 3 0 0 3 0 0 0 ...</code> <br />
<code> $ WV14: num  3 1 2 3 0 3 1 3 3 0 ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.si07</code> contains parameters of the empirical illustration
of DeCarlo (2020). The simulation function <code>sim_fun</code> can be used for
simulating data from the IRSDT model (see DeCarlo, 2020)
</p>
<p><code>List of 3</code> <br />
<code> $ pars   :'data.frame':        16 obs. of  3 variables:</code> <br />
<code>  ..$ item: Factor w/ 16 levels "I01","I02","I03",..: 1 2 3 4 5 6 7 8 9 10 ...</code> <br />
<code>  ..$ b   : num [1:16] -1.1 -0.18 1.44 1.78 -1.19 0.45 -1.12 0.33 0.82 -0.43 ...</code> <br />
<code>  ..$ d   : num [1:16] 2.69 4.6 6.1 3.11 3.2 ...</code> <br />
<code> $ trait  :'data.frame':        20 obs. of  2 variables:</code> <br />
<code>  ..$ x   : num [1:20] 0.025 0.075 0.125 0.175 0.225 0.275 0.325 0.375 0.425 0.475 ...</code> <br />
<code>  ..$ prob: num [1:20] 0.0238 0.1267 0.105 0.0594 0.0548 ...</code> <br />
<code> $ sim_fun:function (lambda, b, d, items)  </code> <br />
</p>
</li>
<li><p> The dataset <code>data.si08</code> contains 5 items with respect to knowledge
about lung cancer and the kind of information acquisition (Goodman, 1970;
see also Rasch, Kubinger &amp; Yanagida, 2011).
<code>L1</code>: reading newspapers, <code>L2</code>: listening radio,
<code>L3</code>: reading books and magazines,
<code>L4</code>: attending talks, <code>L5</code>: knowledge about lung cancer
</p>
<p><code>'data.frame':   32 obs. of  6 variables:</code> <br />
<code> $ L1 : num  1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ L2 : num  1 1 1 1 1 1 1 1 0 0 ...</code> <br />
<code> $ L3 : num  1 1 1 1 0 0 0 0 1 1 ...</code> <br />
<code> $ L4 : num  1 1 0 0 1 1 0 0 1 1 ...</code> <br />
<code> $ L5 : num  1 0 1 0 1 0 1 0 1 0 ...</code> <br />
<code> $ wgt: num  23 8 102 67 8 4 35 59 27 18 ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.si09</code> was used in Fischer and Karl (2019) and they
asked employees in a eight countries, to report whether they typically help
other employees (helping behavior, seven items, <code>help</code>) and whether
they make suggestions to improve work conditions and products
(voice behavior, five items, <code>voice</code>). Individuals responded to these items
on a 1-7 Likert-type scale. The dataset was downloaded from <a href="https://osf.io/wkx8c/">https://osf.io/wkx8c/</a>.
</p>
<p><code>'data.frame':   5201 obs. of  13 variables:</code> <br />
<code> $ country: Factor w/ 8 levels "BRA","CAN","KEN",..: 5 5 5 5 5 5 5 5 5 5 ...</code> <br />
<code> $ help1  : int  6 6 5 5 5 6 6 6 4 6 ...</code> <br />
<code> $ help2  : int  3 6 5 6 6 6 6 6 6 7 ...</code> <br />
<code> $ help3  : int  5 6 6 7 7 6 5 6 6 7 ...</code> <br />
<code> $ help4  : int  7 6 5 6 6 7 7 6 6 7 ...</code> <br />
<code> $ help5  : int  5 5 5 6 6 6 6 6 6 7 ...</code> <br />
<code> $ help6  : int  3 4 5 6 6 7 7 6 6 5 ...</code> <br />
<code> $ help7  : int  5 4 4 5 5 7 7 6 6 6 ...</code> <br />
<code> $ voice1 : int  3 6 5 6 4 7 6 6 5 7 ...</code> <br />
<code> $ voice2 : int  3 6 4 7 6 5 6 6 4 7 ...</code> <br />
<code> $ voice3 : int  6 6 5 7 6 5 6 6 6 5 ...</code> <br />
<code> $ voice4 : int  6 6 6 5 5 7 5 6 6 6 ...</code> <br />
<code> $ voice5 : int  6 7 4 7 6 6 6 6 5 7 ...</code> <br />
</p>
</li>
<li><p> The dataset <code>data.si10</code> contains votes of 435 members of the U.S. House of
Representatives, 267 Democrates and 168 Republicans. The dataset was
used by Fop and Murphy (2017).
</p>
<p><code>'data.frame':   435 obs. of  17 variables:</code> <br />
<code> $ party : Factor w/ 2 levels "democrat","republican": 2 2 1 1 1 1 1 2 2 1 ...</code> <br />
<code> $ vote01: num  0 0 NA 0 1 0 0 0 0 1 ...</code> <br />
<code> $ vote02: num  1 1 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ vote03: num  0 0 1 1 1 1 0 0 0 1 ...</code> <br />
<code> $ vote04: num  1 1 NA 0 0 0 1 1 1 0 ...</code> <br />
<code> $ vote05: num  1 1 1 NA 1 1 1 1 1 0 ...</code> <br />
<code> $ vote06: num  1 1 1 1 1 1 1 1 1 0 ...</code> <br />
<code> $ vote07: num  0 0 0 0 0 0 0 0 0 1 ...</code> <br />
<code> $ vote08: num  0 0 0 0 0 0 0 0 0 1 ...</code> <br />
<code> $ vote09: num  0 0 0 0 0 0 0 0 0 1 ...</code> <br />
<code> $ vote10: num  1 0 0 0 0 0 0 0 0 0 ...</code> <br />
<code> $ vote11: num  NA 0 1 1 1 0 0 0 0 0 ...</code> <br />
<code> $ vote12: num  1 1 0 0 NA 0 0 0 1 0 ...</code> <br />
<code> $ vote13: num  1 1 1 1 1 1 NA 1 1 0 ...</code> <br />
<code> $ vote14: num  1 1 1 0 1 1 1 1 1 0 ...</code> <br />
<code> $ vote15: num  0 0 0 0 1 1 1 NA 0 NA ...</code> <br />
<code> $ vote16: num  1 NA 0 1 1 1 1 1 1 NA ...</code> <br />
</p>
</li></ul>



<h3>References</h3>

<p>Bartolucci, F., Montanari, G. E., &amp; Pandolfi, S. (2012).
Dimensionality of the latent structure and item selection via latent
class multidimensional IRT models. <em>Psychometrika, 77</em>(4), 782-802.
<a href="https://doi.org/10.1007/s11336-012-9278-0">doi:10.1007/s11336-012-9278-0</a>
</p>
<p>DeCarlo, L. T. (2020). An item response model for true-false exams based on signal
detection theory. <em>Applied Psychological Measurement, 34</em>(3). 234-248.
<a href="https://doi.org/10.1177/0146621619843823">doi:10.1177/0146621619843823</a>
</p>
<p>Fischer, R., &amp; Karl, J. A. (2019). A primer to (cross-cultural) multi-group invariance
testing possibilities in R.
<em>Frontiers in Psychology | Cultural Psychology, 10</em>:1507.
<a href="https://doi.org/10.3389/fpsyg.2019.01507">doi:10.3389/fpsyg.2019.01507</a>
</p>
<p>Fop, M., &amp; Murphy, T. B. (2018). Variable selection methods for model-based clustering.
<em>Statistics Surveys, 12</em>, 18-65. https://doi.org/10.1214/18-SS119
</p>
<p>Goodman, L. A. (1970). The multivariate analysis of qualitative data: Interactions
among multiple classifications.
<em>Journal of the American Statistical Association, 65</em>(329), 226-256.
<a href="https://doi.org/10.1080/01621459.1970.10481076">doi:10.1080/01621459.1970.10481076</a>
</p>
<p>Lindsay, B., Clogg, C. C., &amp; Grego, J. (1991).
Semiparametric estimation in the Rasch model and related exponential response
models, including a simple latent class model for item analysis.
<em>Journal of the American Statistical Association, 86</em>(413), 96-107.
<a href="https://doi.org/10.1080/01621459.1991.10475008">doi:10.1080/01621459.1991.10475008</a>
</p>
<p>Rasch, D., Kubinger, K. D., &amp; Yanagida, T. (2011).
<em>Statistics in psychology using R and SPSS</em>. New York: Wiley.
<a href="https://doi.org/10.1002/9781119979630">doi:10.1002/9781119979630</a>
</p>


<h3>See Also</h3>

<p>Some free datasets can be obtained from <br />
Psychological questionnaires: http://personality-testing.info/_rawdata/ <br />
PISA 2012: http://pisa2012.acer.edu.au/downloads.php <br />
PIAAC: http://www.oecd.org/site/piaac/publicdataandanalysis.htm <br />
TIMSS 2011: http://timssandpirls.bc.edu/timss2011/international-database.html <br />
ALLBUS: http://www.gesis.org/allbus/allbus-home/
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Nested logit model multiple choice dataset data.si06
#############################################################################

data(data.si06, package="sirt")
dat &lt;- data.si06

#** estimate 2PL nested logit model
library(mirt)
mod1 &lt;- mirt::mirt( dat, model=1, itemtype="2PLNRM", key=rep(0,ncol(dat) ),
            verbose=TRUE  )
summary(mod1)
cmod1 &lt;- sirt::mirt.wrapper.coef(mod1)$coef
cmod1[,-1] &lt;- round( cmod1[,-1], 3)

#** normalize item parameters according Suh and Bolt (2010)
cmod2 &lt;- cmod1

# slope parameters
ind &lt;-  grep("ak",colnames(cmod2))
h1 &lt;- cmod2[,ind ]
cmod2[,ind] &lt;- t( apply( h1, 1, FUN=function(ll){ ll - mean(ll) } ) )
# item intercepts
ind &lt;-  paste0( "d", 0:9 )
ind &lt;- which( colnames(cmod2) %in% ind )
h1 &lt;- cmod2[,ind ]
cmod2[,ind] &lt;- t( apply( h1, 1, FUN=function(ll){ ll - mean(ll) } ) )
cmod2[,-1] &lt;- round( cmod2[,-1], 3)

#############################################################################
# EXAMPLE 2: Item response modle based on signal detection theory (IRSDT model)
#############################################################################

data(data.si07, package="sirt")
data &lt;- data.si07

#-- simulate data
set.seed(98)
N &lt;- 2000 # define sample size
# generate membership scores
lambda &lt;- sample(size=N, x=data$trait$x, prob=data$trait$prob, replace=TRUE)
b &lt;- data$pars$b
d &lt;- data$pars$d
items &lt;- data$pars$item
dat &lt;- data$sim_fun(lambda=lambda, b=b, d=d, items=items)

#- estimate IRSDT model as a grade of membership model with two classes
problevels &lt;- seq( 0.025, 0.975, length=20 )
mod1 &lt;- sirt::gom.em( dat, K=2, problevels=problevels )
summary(mod1)

## End(Not run)
</code></pre>

<hr>
<h2 id='data.timss'>
Dataset TIMSS Mathematics
</h2><span id='topic+data.timss'></span>

<h3>Description</h3>

<p>This datasets contains TIMSS mathematics data from 345 students
on 25 items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.timss)</code></pre>


<h3>Format</h3>

<p>This dataset is a list. <code>data</code> is the dataset containing
student ID (<code>idstud</code>), a dummy variable for female (<code>girl</code>)
and student age (<code>age</code>). The following variables (starting with
<code>M</code> in the variable name are items.
</p>
<p>The format is:
</p>
<p><code>List of 2</code> <br />
<code> $ data:'data.frame':</code> <br />
<code>  ..$ idstud  : num [1:345] 4e+09 4e+09 4e+09 4e+09 4e+09 ...</code> <br />
<code>  ..$ girl    : int [1:345] 0 0 0 0 0 0 0 0 1 0 ...</code> <br />
<code>  ..$ age     : num [1:345] 10.5 10 10.25 10.25 9.92 ...</code> <br />
<code>  ..$ M031286 : int [1:345] 0 0 0 1 1 0 1 0 1 0 ...</code> <br />
<code>  ..$ M031106 : int [1:345] 0 0 0 1 1 0 1 1 0 0 ...</code> <br />
<code>  ..$ M031282 : int [1:345] 0 0 0 1 1 0 1 1 0 0 ...</code> <br />
<code>  ..$ M031227 : int [1:345] 0 0 0 0 1 0 0 0 0 0 ...</code> <br />
<code>    [...]</code> <br />
<code>  ..$ M041203 : int [1:345] 0 0 0 1 1 0 0 0 0 1 ...</code> <br />
<code> $ item:'data.frame':</code> <br />
<code>  ..$ item            : Factor w/ 25 levels "M031045","M031068",..: ...</code> <br />
<code>  ..$ Block           : Factor w/ 2 levels "M01","M02": 1 1 1 1 1 1 ..</code> <br />
<code>  ..$ Format          : Factor w/ 2 levels "CR","MC": 1 1 1 1 2  ...</code> <br />
<code>  ..$ Content.Domain  : Factor w/ 3 levels "Data Display",..: 3 3 3 3  ...</code> <br />
<code>  ..$ Cognitive.Domain: Factor w/ 3 levels "Applying","Knowing",..: 2 3 3 ..</code> <br />
</p>

<hr>
<h2 id='data.timss07.G8.RUS'>
TIMSS 2007 Grade 8 Mathematics and Science Russia
</h2><span id='topic+data.timss07.G8.RUS'></span>

<h3>Description</h3>

<p>This TIMSS 2007 dataset contains item responses of 4472 eigth grade
Russian students in Mathematics and Science.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.timss07.G8.RUS)
</code></pre>


<h3>Format</h3>

<p>The datasets contains raw responses (<code>raw</code>), scored responses
(<code>scored</code>) and item informations (<code>iteminfo</code>).
</p>
<p>The format of the dataset is:
</p>
<p><code>List of 3</code> <br />
<code> $ raw     :'data.frame':</code> <br />
<code>  ..$ idstud  : num [1:4472] 3010101 3010102 3010104 3010105 3010106 ...</code> <br />
<code>  ..$ M022043 : atomic [1:4472] NA 1 4 NA NA NA NA NA NA NA ...</code> <br />
<code>  .. ..- attr(*, "value.labels")=Named num [1:7] 9 6 5 4 3 2 1</code> <br />
<code>  .. .. ..- attr(*, "names")=chr [1:7] "OMITTED" "NOT REACHED" "E" "D*" ...</code> <br />
<code>  [...]</code> <br />
<code>  ..$ M032698 : atomic [1:4472] NA NA NA NA NA NA NA 2 1 NA ...</code> <br />
<code>  .. ..- attr(*, "value.labels")=Named num [1:6] 9 6 4 3 2 1</code> <br />
<code>  .. .. ..- attr(*, "names")=chr [1:6] "OMITTED" "NOT REACHED" "D" "C" ...</code> <br />
<code>  ..$ M032097 : atomic [1:4472] NA NA NA NA NA NA NA 2 3 NA ...</code> <br />
<code>  .. ..- attr(*, "value.labels")=Named num [1:6] 9 6 4 3 2 1</code> <br />
<code>  .. .. ..- attr(*, "names")=chr [1:6] "OMITTED" "NOT REACHED" "D" "C*" ...</code> <br />
<code>  .. [list output truncated]</code> <br />
<code> $ scored  : num [1:4472, 1:443] 3010101 3010102 3010104 3010105 3010106 ...</code> <br />
<code>  ..- attr(*, "dimnames")=List of 2</code> <br />
<code>  .. ..$ : NULL</code> <br />
<code>  .. ..$ : chr [1:443] "idstud" "M022043" "M022046" "M022049" ...</code> <br />
<code> $ iteminfo:'data.frame':</code> <br />
<code>  ..$ item      : Factor w/ 442 levels "M022043","M022046",..: 1 2 3 4 5 6 21 7 8 17 ...</code> <br />
<code>  ..$ content   : Factor w/ 8 levels "Algebra","Biology",..: 7 7 6 1 6 7 4 6 7 7 ...</code> <br />
<code>  ..$ topic     : Factor w/ 49 levels "Algebraic Expression",..: 32 32 41 29  ...</code> <br />
<code>  ..$ cognitive : Factor w/ 3 levels "Applying","Knowing",..: 2 1 3 2 1 1 1 1 2 1 ...</code> <br />
<code>  ..$ item.type : Factor w/ 2 levels "CR","MC": 2 1 2 2 1 2 2 2 2 1 ...</code> <br />
<code>  ..$ N.options : Factor w/ 4 levels "-"," -","4","5": 4 1 3 4 1 4 4 4 3 1 ...</code> <br />
<code>  ..$ key       : Factor w/ 7 levels "-"," -","A","B",..: 6 1 6 7 1 5 5 4 6 1 ...</code> <br />
<code>  ..$ max.points: int [1:442] 1 1 1 1 1 1 1 1 1 2 ...</code> <br />
<code>  ..$ item.label: Factor w/ 432 levels "1 teacher for every 12 students ",..: 58 351 ...</code> <br />
</p>


<h3>Source</h3>

<p>TIMSS 2007 8th Grade, Russian Sample
</p>

<hr>
<h2 id='data.trees'>
Dataset Used in Stoyan, Pommerening and Wuensche (2018)
</h2><span id='topic+data.trees'></span>

<h3>Description</h3>

<p>Dataset used in Stoyan, Pommerening and Wuensche (2018; see also
Pommerening et al., 2018). In the dataset, 15 forest managers classify 387 trees
either as trees to be maintained or as trees to be removed. They assign tree marks,
either 0 or 1, where mark 1 means remove.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data.trees)
</code></pre>


<h3>Format</h3>

<p>The dataset has the following structure. <br />
</p>
<p><code>'data.frame':   387 obs. of  16 variables:</code> <br />
<code> $ Number: int  142 184 9 300 374 42 382 108 125 201 ...</code> <br />
<code> $ FM1   : int  1 1 1 1 1 1 1 1 1 0 ...</code> <br />
<code> $ FM2   : int  1 1 1 0 1 1 1 1 1 1 ...</code> <br />
<code> $ FM3   : int  1 0 1 1 1 1 1 1 1 1 ...</code> <br />
<code> $ FM4   : int  1 1 1 1 1 1 0 1 1 1 ...</code> <br />
<code> $ FM5   : int  1 1 1 1 1 1 0 0 0 1 ...</code> <br />
<code> $ FM6   : int  1 1 1 1 0 1 1 1 1 0 ...</code> <br />
<code> $ FM7   : int  1 0 1 1 0 0 1 0 1 1 ...</code> <br />
<code> $ FM8   : int  1 1 1 1 1 0 0 1 0 1 ...</code> <br />
<code> $ FM9   : int  1 1 0 1 1 1 1 0 1 1 ...</code> <br />
<code> $ FM10  : int  0 1 1 0 1 1 1 1 0 0 ...</code> <br />
<code> $ FM11  : int  1 1 1 1 0 1 1 0 1 0 ...</code> <br />
<code> $ FM12  : int  1 1 1 1 1 1 0 1 0 0 ...</code> <br />
<code> $ FM13  : int  0 1 0 0 1 1 1 1 1 1 ...</code> <br />
<code> $ FM14  : int  1 1 1 1 1 0 1 1 1 1 ...</code> <br />
<code> $ FM15  : int  1 1 0 1 1 0 1 0 0 1 ...</code> <br />
</p>


<h3>Source</h3>

<p><a href="https://www.pommerening.org/wiki/images/d/dc/CoedyBreninSortedforPublication.txt">https://www.pommerening.org/wiki/images/d/dc/CoedyBreninSortedforPublication.txt</a>
</p>


<h3>References</h3>

<p>Pommerening, A., Ramos, C. P., Kedziora, W., Haufe, J., &amp; Stoyan, D. (2018).
Rating experiments in forestry: How much agreement is there in tree marking?
<em>PloS ONE, 13</em>(3), e0194747. <a href="https://doi.org/10.1371/journal.pone.0194747">doi:10.1371/journal.pone.0194747</a>
</p>
<p>Stoyan, D., Pommerening, A., &amp; Wuensche, A. (2018).
Rater classification by means of set-theoretic methods applied to forestry data.
<em>Journal of Environmental Statistics, 8</em>(2), 1-17.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Latent class models, latent trait models, mixed membership models
#############################################################################

data(data.trees, package="sirt")
dat &lt;- data.trees[,-1]
I &lt;- ncol(dat)

#** latent class models with 2, 3, and 4 classes
problevels &lt;- seq( 0, 1, len=2 )
mod02 &lt;- sirt::gom.em(dat, K=2, problevels, model="GOM")
mod03 &lt;- sirt::gom.em(dat, K=3, problevels, model="GOM")
mod04 &lt;- sirt::gom.em(dat, K=4, problevels, model="GOM")

#** grade of membership models
mod11 &lt;- sirt::gom.em(dat, K=2, theta0.k=10*seq(-1,1,len=11), model="GOMnormal")
problevels &lt;- seq( 0, 1, len=3 )
mod12 &lt;- sirt::gom.em(dat, K=2, problevels, model="GOM")
mod13 &lt;- sirt::gom.em(dat, K=3, problevels, model="GOM")
mod14 &lt;- sirt::gom.em(dat, K=4, problevels, model="GOM")
problevels &lt;- seq( 0, 1, len=4 )
mod22 &lt;- sirt::gom.em(dat, K=2, problevels, model="GOM")
mod23 &lt;- sirt::gom.em(dat, K=3, problevels, model="GOM")
mod24 &lt;- sirt::gom.em(dat, K=4, problevels, model="GOM")

#** latent trait models
#- 1PL
mod31 &lt;- sirt::rasch.mml2(dat)
#- 2PL
mod32 &lt;- sirt::rasch.mml2(dat, est.a=1:I)

#- model comparison
IRT.compareModels(mod02, mod03, mod04, mod11, mod12, mod13, mod14,
                     mod22, mod23, mod24, mod31, mod32)

#-- inspect model results
summary(mod12)
round( cbind( mod12$theta.k, mod12$pi.k ),3)

summary(mod13)
round(cbind( mod13$theta.k, mod13$pi.k ),3)

## End(Not run)
</code></pre>

<hr>
<h2 id='data.wide2long'>
Converting a Data Frame from Wide Format in a Long Format
</h2><span id='topic+data.wide2long'></span>

<h3>Description</h3>

<p>Converts a data frame in wide format into long format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data.wide2long(dat, id=NULL, X=NULL, Q=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data.wide2long_+3A_dat">dat</code></td>
<td>

<p>Data frame with item responses and a person identifier if
<code>id !=NULL</code>.
</p>
</td></tr>
<tr><td><code id="data.wide2long_+3A_id">id</code></td>
<td>

<p>An optional string with the variable name of the person identifier.
</p>
</td></tr>
<tr><td><code id="data.wide2long_+3A_x">X</code></td>
<td>
<p>Data frame with person covariates for inclusion in the
data frame of long format
</p>
</td></tr>
<tr><td><code id="data.wide2long_+3A_q">Q</code></td>
<td>
<p>Data frame with item predictors. Item labels must be included
as a column named by <code>"item"</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame in long format
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: data.pisaRead
#############################################################################
miceadds::library_install("lme4")

data(data.pisaRead)
dat &lt;- data.pisaRead$data
Q &lt;- data.pisaRead$item   # item predictors

# define items
items &lt;- colnames(dat)[ substring( colnames(dat), 1, 1 )=="R" ]
dat1 &lt;- dat[, c( "idstud", items ) ]
# matrix with person predictors
X &lt;- dat[, c("idschool", "hisei", "female", "migra") ]

# create dataset in long format
dat.long &lt;- sirt::data.wide2long( dat=dat1, id="idstud", X=X, Q=Q )

#***
# Model 1: Rasch model
mod1 &lt;- lme4::glmer( resp ~ 0 + ( 1 | idstud ) + as.factor(item), data=dat.long,
            family="binomial", verbose=TRUE)
summary(mod1)

#***
# Model 2: Rasch model and inclusion of person predictors
mod2 &lt;- lme4::glmer( resp ~ 0 + ( 1 | idstud ) + as.factor(item) + female + hisei + migra,
           data=dat.long, family="binomial", verbose=TRUE)
summary(mod2)

#***
# Model 3: LLTM
mod3 &lt;- lme4::glmer(resp ~ (1|idstud) + as.factor(ItemFormat) + as.factor(TextType),
            data=dat.long, family="binomial", verbose=TRUE)
summary(mod3)

#############################################################################
# EXAMPLE 2: Rasch model in lme4
#############################################################################

set.seed(765)
N &lt;- 1000  # number of persons
I &lt;- 10    # number of items
b &lt;- seq(-2,2,length=I)
dat &lt;- sirt::sim.raschtype( stats::rnorm(N,sd=1.2), b=b )
dat.long &lt;- sirt::data.wide2long( dat=dat )
#***
# estimate Rasch model with lmer
library(lme4)
mod1 &lt;- lme4::glmer( resp ~ 0 + as.factor( item ) + ( 1 | id_index), data=dat.long,
             verbose=TRUE, family="binomial")
summary(mod1)
  ##   Random effects:
  ##    Groups   Name        Variance Std.Dev.
  ##    id_index (Intercept) 1.454    1.206
  ##   Number of obs: 10000, groups: id_index, 1000
  ##
  ##   Fixed effects:
  ##                        Estimate Std. Error z value Pr(&gt;|z|)
  ##   as.factor(item)I0001  2.16365    0.10541  20.527  &lt; 2e-16 ***
  ##   as.factor(item)I0002  1.66437    0.09400  17.706  &lt; 2e-16 ***
  ##   as.factor(item)I0003  1.21816    0.08700  14.002  &lt; 2e-16 ***
  ##   as.factor(item)I0004  0.68611    0.08184   8.383  &lt; 2e-16 ***
  ##   [...]

## End(Not run)
</code></pre>

<hr>
<h2 id='detect.index'>
Calculation of the DETECT and polyDETECT Index
</h2><span id='topic+detect.index'></span>

<h3>Description</h3>

<p>This function calculated the DETECT and polyDETECT index (Stout, Habing, Douglas
&amp; Kim, 1996; Zhang &amp; Stout, 1999a; Zhang, 2007). At first,
conditional covariances have to be estimated
using the <code><a href="#topic+ccov.np">ccov.np</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>detect.index(ccovtable, itemcluster)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="detect.index_+3A_ccovtable">ccovtable</code></td>
<td>

<p>A value of <code><a href="#topic+ccov.np">ccov.np</a></code>.
</p>
</td></tr>
<tr><td><code id="detect.index_+3A_itemcluster">itemcluster</code></td>
<td>

<p>Item cluster for each item. The order of entries must correspond
to the columns in <code>data</code> (submitted to <code><a href="#topic+ccov.np">ccov.np</a></code>).
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Stout, W., Habing, B., Douglas, J., &amp; Kim, H. R. (1996).
Conditional covariance-based nonparametric multidimensionality assessment.
<em>Applied Psychological Measurement, 20</em>, 331-354.
</p>
<p>Zhang, J., &amp; Stout, W. (1999a). Conditional covariance structure
of generalized compensatory multidimensional items.
<em>Psychometrika, 64</em>, 129-152.
</p>
<p>Zhang, J., &amp; Stout, W. (1999b). The theoretical DETECT index of
dimensionality and its application to approximate simple structure.
<em>Psychometrika, 64</em>, 213-249.
</p>
<p>Zhang, J. (2007). Conditional covariance theory and DETECT for
polytomous items. <em>Psychometrika, 72</em>, 69-91.
</p>


<h3>See Also</h3>

<p>For examples see <code><a href="#topic+conf.detect">conf.detect</a></code>.
</p>

<hr>
<h2 id='dif.logistic.regression'>
Differential Item Functioning using Logistic Regression Analysis
</h2><span id='topic+dif.logistic.regression'></span>

<h3>Description</h3>

<p>This function assesses differential item
functioning using logistic regression analysis (Zumbo, 1999).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dif.logistic.regression(dat, group, score,quant=1.645)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dif.logistic.regression_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous item responses
</p>
</td></tr>
<tr><td><code id="dif.logistic.regression_+3A_group">group</code></td>
<td>

<p>Group identifier
</p>
</td></tr>
<tr><td><code id="dif.logistic.regression_+3A_score">score</code></td>
<td>

<p>Ability estimate, e.g. the WLE.
</p>
</td></tr>
<tr><td><code id="dif.logistic.regression_+3A_quant">quant</code></td>
<td>
<p>Used quantile of the normal distribution
for assessing statistical significance</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Items are classified into A (negligible DIF), B (moderate DIF) and
C (large DIF) levels according to the
ETS classification system (Longford, Holland &amp; Thayer, 1993, p. 175).
See also Monahan, McHorney, Stump and Perkins (2007) for further DIF effect size
classifications.
</p>


<h3>Value</h3>

<p>A data frame with following variables:
</p>
<table>
<tr><td><code>itemnr</code></td>
<td>
<p>Numeric index of the item</p>
</td></tr>
<tr><td><code>sortDIFindex</code></td>
<td>
<p>Rank of item with respect to the uniform DIF
(from negative to positive values)</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Item name</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>Sample size per item</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>Value of <code>group</code> variable for reference group</p>
</td></tr>
<tr><td><code>F</code></td>
<td>
<p>Value of <code>group</code> variable for focal group</p>
</td></tr>
<tr><td><code>nR</code></td>
<td>
<p>Sample size per item in reference group</p>
</td></tr>
<tr><td><code>nF</code></td>
<td>
<p>Sample size per item in focal group</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>Item <code class="reqn">p</code> value</p>
</td></tr>
<tr><td><code>pR</code></td>
<td>
<p>Item <code class="reqn">p</code> value in reference group</p>
</td></tr>
<tr><td><code>pF</code></td>
<td>
<p>Item <code class="reqn">p</code> value in focal group</p>
</td></tr>
<tr><td><code>pdiff</code></td>
<td>
<p>Item <code class="reqn">p</code> value differences</p>
</td></tr>
<tr><td><code>pdiff.adj</code></td>
<td>
<p>Adjusted <code class="reqn">p</code> value difference</p>
</td></tr>
<tr><td><code>uniformDIF</code></td>
<td>
<p>Uniform DIF estimate</p>
</td></tr>
<tr><td><code>se.uniformDIF</code></td>
<td>
<p>Standard error of uniform DIF</p>
</td></tr>
<tr><td><code>t.uniformDIF</code></td>
<td>
<p>The <code class="reqn">t</code> value for uniform DIF</p>
</td></tr>
<tr><td><code>sig.uniformDIF</code></td>
<td>
<p>Significance label for uniform DIF</p>
</td></tr>
<tr><td><code>DIF.ETS</code></td>
<td>
<p>DIF classification according to the ETS classification
system (see Details)</p>
</td></tr>
<tr><td><code>uniform.EBDIF</code></td>
<td>
<p>Empirical Bayes estimate of uniform DIF (Longford,
Holland &amp; Thayer, 1993) which takes degree of DIF standard error
into account</p>
</td></tr>
<tr><td><code>DIF.SD</code></td>
<td>
<p>Value of the DIF standard deviation</p>
</td></tr>
<tr><td><code>nonuniformDIF</code></td>
<td>
<p>Nonuniform DIF estimate</p>
</td></tr>
<tr><td><code>se.nonuniformDIF</code></td>
<td>
<p>Standard error of nonuniform DIF</p>
</td></tr>
<tr><td><code>t.nonuniformDIF</code></td>
<td>
<p>The <code class="reqn">t</code> value for nonuniform DIF</p>
</td></tr>
<tr><td><code>sig.nonuniformDIF</code></td>
<td>
<p>Significance label for nonuniform DIF</p>
</td></tr>
</table>


<h3>References</h3>

<p>Longford, N. T., Holland, P. W., &amp; Thayer, D. T. (1993).
Stability of the MH D-DIF statistics across populations.
In P. W. Holland &amp; H. Wainer (Eds.). <em>Differential
Item Functioning</em> (pp. 171-196). Hillsdale, NJ: Erlbaum.
</p>
<p>Magis, D., Beland, S., Tuerlinckx, F., &amp; De Boeck, P. (2010). A general framework and an
<span class="rlang"><b>R</b></span> package for the detection of dichotomous differential item functioning.
<em>Behavior Research Methods, 42</em>(3), 847-862.
<a href="https://doi.org/10.3758/BRM.42.3.847">doi:10.3758/BRM.42.3.847</a>
</p>
<p>Monahan, P. O., McHorney, C. A., Stump, T. E., &amp; Perkins, A. J. (2007).
Odds ratio, delta, ETS classification, and standardization measures of
DIF magnitude for binary logistic regression.
<em>Journal of Educational and Behavioral Statistics, 32</em>(1), 92-109.
<a href="https://doi.org/10.3102/1076998606298035">doi:10.3102/1076998606298035</a>
</p>
<p>Zumbo, B. D. (1999). <em>A handbook on the theory and methods of differential
item functioning (DIF): Logistic regression modeling as a unitary framework
for binary and Likert-type (ordinal) item scores</em>.
Ottawa ON: Directorate of Human Resources Research and Evaluation,
Department of National Defense.
</p>


<h3>See Also</h3>

<p>For assessing DIF variance see <code><a href="#topic+dif.variance">dif.variance</a></code> and
<code><a href="#topic+dif.strata.variance">dif.strata.variance</a></code>
</p>
<p>See also <code><a href="#topic+rasch.evm.pcm">rasch.evm.pcm</a></code> for assessing differential item
functioning in the partial credit model.
</p>
<p>See the <span class="pkg">difR</span> package for a large collection of DIF detection
methods (Magis, Beland, Tuerlinckx, &amp; De Boeck, 2010).
</p>
<p>For a download of the free <em>DIF-Pack</em> software (SIBTEST, ...) see
<em>http://psychometrictools.measuredprogress.org/home</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Mathematics data | Gender DIF
#############################################################################

data( data.math )
dat &lt;- data.math$data
items &lt;- grep( "M", colnames(dat))

# estimate item parameters and WLEs
mod &lt;- sirt::rasch.mml2( dat[,items] )
wle &lt;- sirt::wle.rasch( dat[,items], b=mod$item$b )$theta

# assess DIF by logistic regression
mod1 &lt;- sirt::dif.logistic.regression( dat=dat[,items], score=wle, group=dat$female)

# calculate DIF variance
dif1 &lt;- sirt::dif.variance( dif=mod1$uniformDIF, se.dif=mod1$se.uniformDIF )
dif1$unweighted.DIFSD
  ## &gt; dif1$unweighted.DIFSD
  ## [1] 0.1963958

# calculate stratified DIF variance
# stratification based on domains
dif2 &lt;- sirt::dif.strata.variance( dif=mod1$uniformDIF, se.dif=mod1$se.uniformDIF,
              itemcluster=data.math$item$domain )
  ## $unweighted.DIFSD
  ## [1] 0.1455916

## Not run: 
#****
# Likelihood ratio test and graphical model test in eRm package
miceadds::library_install("eRm")
# estimate Rasch model
res &lt;- eRm::RM( dat[,items] )
summary(res)
# LR-test with respect to female
lrres &lt;- eRm::LRtest(res, splitcr=dat$female)
summary(lrres)
# graphical model test
eRm::plotGOF(lrres)

#############################################################################
# EXAMPLE 2: Comparison with Mantel-Haenszel test
#############################################################################

library(TAM)
library(difR)

#*** (1) simulate data
set.seed(776)
N &lt;- 1500   # number of persons per group
I &lt;- 12     # number of items
mu2 &lt;- .5   # impact (group difference)
sd2 &lt;- 1.3  # standard deviation group 2

# define item difficulties
b &lt;- seq( -1.5, 1.5, length=I)
# simulate DIF effects
bdif &lt;- scale( stats::rnorm(I, sd=.6 ), scale=FALSE )[,1]
# item difficulties per group
b1 &lt;- b + 1/2 * bdif
b2 &lt;- b - 1/2 * bdif
# simulate item responses
dat1 &lt;- sirt::sim.raschtype( theta=stats::rnorm(N, mean=0, sd=1 ), b=b1 )
dat2 &lt;- sirt::sim.raschtype( theta=stats::rnorm(N, mean=mu2, sd=sd2 ), b=b2 )
dat &lt;- rbind( dat1, dat2 )
group &lt;- rep( c(1,2), each=N ) # define group indicator

#*** (2) scale data
mod &lt;- TAM::tam.mml( dat, group=group )
summary(mod)

#*** (3) extract person parameter estimates
mod_eap &lt;- mod$person$EAP
mod_wle &lt;- tam.wle( mod )$theta

#*********************************
# (4) techniques for assessing differential item functioning

# Model 1: assess DIF by logistic regression and WLEs
dif1 &lt;- sirt::dif.logistic.regression( dat=dat, score=mod_wle, group=group)
# Model 2: assess DIF by logistic regression and EAPs
dif2 &lt;- sirt::dif.logistic.regression( dat=dat, score=mod_eap, group=group)
# Model 3: assess DIF by Mantel-Haenszel statistic
dif3 &lt;- difR::difMH(Data=dat, group=group, focal.name="1",  purify=FALSE )
print(dif3)
  ##  Mantel-Haenszel Chi-square statistic:
  ##
  ##        Stat.    P-value
  ##  I0001  14.5655   0.0001 ***
  ##  I0002 300.3225   0.0000 ***
  ##  I0003   2.7160   0.0993 .
  ##  I0004 191.6925   0.0000 ***
  ##  I0005   0.0011   0.9740
  ##  [...]
  ##  Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
  ##  Detection threshold: 3.8415 (significance level: 0.05)
  ##
  ##  Effect size (ETS Delta scale):
  ##
  ##  Effect size code:
  ##   'A': negligible effect
  ##   'B': moderate effect
  ##   'C': large effect
  ##
  ##        alphaMH deltaMH
  ##  I0001  1.3908 -0.7752 A
  ##  I0002  0.2339  3.4147 C
  ##  I0003  1.1407 -0.3093 A
  ##  I0004  2.8515 -2.4625 C
  ##  I0005  1.0050 -0.0118 A
  ##  [...]
  ##
  ##  Effect size codes: 0 'A' 1.0 'B' 1.5 'C'
  ##   (for absolute values of 'deltaMH')

# recompute DIF parameter from alphaMH
uniformDIF3 &lt;- log(dif3$alphaMH)

# compare different DIF statistics
dfr &lt;- data.frame( "bdif"=bdif, "LR_wle"=dif1$uniformDIF,
        "LR_eap"=dif2$uniformDIF, "MH"=uniformDIF3 )
round( dfr, 3 )
  ##       bdif LR_wle LR_eap     MH
  ##  1   0.236  0.319  0.278  0.330
  ##  2  -1.149 -1.473 -1.523 -1.453
  ##  3   0.140  0.122  0.038  0.132
  ##  4   0.957  1.048  0.938  1.048
  ##  [...]
colMeans( abs( dfr[,-1] - bdif ))
  ##      LR_wle     LR_eap         MH
  ##  0.07759187 0.19085743 0.07501708

## End(Not run)
</code></pre>

<hr>
<h2 id='dif.strata.variance'>
Stratified DIF Variance
</h2><span id='topic+dif.strata.variance'></span>

<h3>Description</h3>

<p>Calculation of stratified DIF variance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dif.strata.variance(dif, se.dif, itemcluster)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dif.strata.variance_+3A_dif">dif</code></td>
<td>

<p>Vector of uniform DIF effects
</p>
</td></tr>
<tr><td><code id="dif.strata.variance_+3A_se.dif">se.dif</code></td>
<td>

<p>Standard error of uniform DIF effects
</p>
</td></tr>
<tr><td><code id="dif.strata.variance_+3A_itemcluster">itemcluster</code></td>
<td>

<p>Vector of item strata
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>stratadif</code></td>
<td>
<p>Summary statistics of DIF effects within item strata</p>
</td></tr>
<tr><td><code>weighted.DIFSD</code></td>
<td>
<p>Weighted DIF standard deviation</p>
</td></tr>
<tr><td><code>unweigted.DIFSD</code></td>
<td>
<p>DIF standard deviation</p>
</td></tr>
</table>


<h3>References</h3>

<p>Longford, N. T., Holland, P. W., &amp; Thayer, D. T. (1993).
Stability of the MH D-DIF statistics across populations.
In P. W. Holland &amp; H. Wainer (Eds.). <em>Differential
Item Functioning</em> (pp. 171-196). Hillsdale, NJ: Erlbaum.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+dif.logistic.regression">dif.logistic.regression</a></code> for examples.
</p>

<hr>
<h2 id='dif.variance'>
DIF Variance
</h2><span id='topic+dif.variance'></span>

<h3>Description</h3>

<p>This function calculates the variance of DIF effects, the so called
DIF variance (Longford, Holland &amp; Thayer, 1993).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dif.variance(dif, se.dif, items=paste("item", 1:length(dif), sep="") )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dif.variance_+3A_dif">dif</code></td>
<td>

<p>Vector of uniform DIF effects
</p>
</td></tr>
<tr><td><code id="dif.variance_+3A_se.dif">se.dif</code></td>
<td>

<p>Standard error of uniform DIF effects
</p>
</td></tr>
<tr><td><code id="dif.variance_+3A_items">items</code></td>
<td>

<p>Optional vector of item names
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>weighted.DIFSD</code></td>
<td>
<p>Weighted DIF standard deviation</p>
</td></tr>
<tr><td><code>unweigted.DIFSD</code></td>
<td>
<p>DIF standard deviation</p>
</td></tr>
<tr><td><code>mean.se.dif</code></td>
<td>
<p>Mean of standard errors of DIF effects</p>
</td></tr>
<tr><td><code>eb.dif</code></td>
<td>
<p>Empirical Bayes estimates of DIF effects</p>
</td></tr>
</table>


<h3>References</h3>

<p>Longford, N. T., Holland, P. W., &amp; Thayer, D. T. (1993).
Stability of the MH D-DIF statistics across populations.
In P. W. Holland &amp; H. Wainer (Eds.). <em>Differential
Item Functioning</em> (pp. 171-196). Hillsdale, NJ: Erlbaum.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+dif.logistic.regression">dif.logistic.regression</a></code> for examples.
</p>

<hr>
<h2 id='dirichlet.mle'>
Maximum Likelihood Estimation of the Dirichlet Distribution
</h2><span id='topic+dirichlet.mle'></span>

<h3>Description</h3>

<p>Maximum likelihood estimation of the parameters of the Dirichlet
distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dirichlet.mle(x, weights=NULL, eps=10^(-5), convcrit=1e-05, maxit=1000,
     oldfac=.3, progress=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dirichlet.mle_+3A_x">x</code></td>
<td>

<p>Data frame with <code class="reqn">N</code> observations and <code class="reqn">K</code> variables
of a Dirichlet distribution
</p>
</td></tr>
<tr><td><code id="dirichlet.mle_+3A_weights">weights</code></td>
<td>
<p>Optional vector of frequency weights</p>
</td></tr>
<tr><td><code id="dirichlet.mle_+3A_eps">eps</code></td>
<td>

<p>Tolerance number which is added to prevent from logarithms of zero
</p>
</td></tr>
<tr><td><code id="dirichlet.mle_+3A_convcrit">convcrit</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="dirichlet.mle_+3A_maxit">maxit</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="dirichlet.mle_+3A_oldfac">oldfac</code></td>
<td>
<p>Convergence acceleration factor. It must be a parameter
between 0 and 1.</p>
</td></tr>
<tr><td><code id="dirichlet.mle_+3A_progress">progress</code></td>
<td>

<p>Display iteration progress?
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>Vector of <code class="reqn">\alpha</code> parameters</p>
</td></tr>
<tr><td><code>alpha0</code></td>
<td>
<p>The concentration parameter <code class="reqn">\alpha_0=\sum_k \alpha_k</code> </p>
</td></tr>
<tr><td><code>xsi</code></td>
<td>
<p>Vector of proportions <code class="reqn">\xi_k=\alpha_k / \alpha_0</code> </p>
</td></tr>
</table>


<h3>References</h3>

<p>Minka, T. P. (2012). <em>Estimating a Dirichlet distribution</em>.
Technical Report.
</p>


<h3>See Also</h3>

<p>For simulating Dirichlet vectors with matrix-wise
<code class="reqn">\bold{\alpha}</code> parameters see <code><a href="#topic+dirichlet.simul">dirichlet.simul</a></code>.
</p>
<p>For a variety of functions concerning the Dirichlet distribution
see the <span class="pkg">DirichletReg</span> package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Simulate and estimate Dirichlet distribution
#############################################################################

# (1) simulate data
set.seed(789)
N &lt;- 200
probs &lt;- c(.5, .3, .2 )
alpha0 &lt;- .5
alpha &lt;- alpha0*probs
alpha &lt;- matrix( alpha, nrow=N, ncol=length(alpha), byrow=TRUE  )
x &lt;- sirt::dirichlet.simul( alpha )

# (2) estimate Dirichlet parameters
dirichlet.mle(x)
  ##   $alpha
  ##   [1] 0.24507708 0.14470944 0.09590745
  ##   $alpha0
  ##   [1] 0.485694
  ##   $xsi
  ##   [1] 0.5045916 0.2979437 0.1974648

## Not run: 
#############################################################################
# EXAMPLE 2: Fitting Dirichlet distribution with frequency weights
#############################################################################

# define observed data
x &lt;- scan( nlines=1)
    1 0   0 1   .5 .5
x &lt;- matrix( x, nrow=3, ncol=2, byrow=TRUE)

# transform observations x into (0,1)
eps &lt;- .01
x &lt;- ( x + eps ) / ( 1 + 2 * eps )

# compare results with likelihood fitting package maxLik
miceadds::library_install("maxLik")
# define likelihood function
dirichlet.ll &lt;- function(param) {
    ll &lt;- sum( weights * log( ddirichlet( x, param ) ) )
    ll
}

#*** weights 10-10-1
weights &lt;- c(10, 10, 1 )
mod1a &lt;- sirt::dirichlet.mle( x, weights=weights )
mod1a
# estimation in maxLik
mod1b &lt;- maxLik::maxLik(loglik, start=c(.5,.5))
print( mod1b )
coef( mod1b )

#*** weights 10-10-10
weights &lt;- c(10, 10, 10 )
mod2a &lt;- sirt::dirichlet.mle( x, weights=weights )
mod2a
# estimation in maxLik
mod2b &lt;- maxLik::maxLik(loglik, start=c(.5,.5))
print( mod2b )
coef( mod2b )

#*** weights 30-10-2
weights &lt;- c(30, 10, 2 )
mod3a &lt;- sirt::dirichlet.mle( x, weights=weights )
mod3a
# estimation in maxLik
mod3b &lt;- maxLik::maxLik(loglik, start=c(.25,.25))
print( mod3b )
coef( mod3b )

## End(Not run)
</code></pre>

<hr>
<h2 id='dirichlet.simul'>
Simulation of a Dirichlet Distributed Vectors
</h2><span id='topic+dirichlet.simul'></span>

<h3>Description</h3>

<p>This function makes random draws from a Dirichlet distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dirichlet.simul(alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dirichlet.simul_+3A_alpha">alpha</code></td>
<td>

<p>A matrix with <code class="reqn">\bold{\alpha}</code> parameters of the
Dirichlet distribution
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with Dirichlet distributed responses
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Simulation with two components
#############################################################################

set.seed(789)
N &lt;- 2000
probs &lt;- c(.7, .3)    # define (extremal) class probabilities

#*** alpha0=.2  -&gt; nearly crisp latent classes
alpha0 &lt;- .2
alpha &lt;- alpha0*probs
alpha &lt;- matrix( alpha, nrow=N, ncol=length(alpha), byrow=TRUE  )
x &lt;- sirt::dirichlet.simul( alpha )
htitle &lt;- expression(paste( alpha[0], "=.2, ", p[1], "=.7"   ) )
hist( x[,1], breaks=seq(0,1,len=20), main=htitle)

#*** alpha0=3 -&gt; strong deviation from crisp membership
alpha0 &lt;- 3
alpha &lt;- alpha0*probs
alpha &lt;- matrix( alpha, nrow=N, ncol=length(alpha), byrow=TRUE  )
x &lt;- sirt::dirichlet.simul( alpha )
htitle &lt;- expression(paste( alpha[0], "=3, ", p[1], "=.7"   ) )
hist( x[,1], breaks=seq(0,1,len=20), main=htitle)

## Not run: 
#############################################################################
# EXAMPLE 2: Simulation with three components
#############################################################################

set.seed(986)
N &lt;- 2000
probs &lt;- c( .5, .35, .15 )

#*** alpha0=.2
alpha0 &lt;- .2
alpha &lt;- alpha0*probs
alpha &lt;- matrix( alpha, nrow=N, ncol=length(alpha), byrow=TRUE  )
x &lt;- sirt::dirichlet.simul( alpha )
htitle &lt;- expression(paste( alpha[0], "=.2, ", p[1], "=.7"   ) )
miceadds::library_install("ade4")
ade4::triangle.plot(x, label=NULL, clabel=1)

#*** alpha0=3
alpha0 &lt;- 3
alpha &lt;- alpha0*probs
alpha &lt;- matrix( alpha, nrow=N, ncol=length(alpha), byrow=TRUE  )
x &lt;- sirt::dirichlet.simul( alpha )
htitle &lt;- expression(paste( alpha[0], "=3, ", p[1], "=.7"   ) )
ade4::triangle.plot(x, label=NULL, clabel=1)

## End(Not run)
</code></pre>

<hr>
<h2 id='dmlavaan'>
Comparing Regression Parameters of Different lavaan Models Fitted to the
Same Dataset
</h2><span id='topic+dmlavaan'></span>

<h3>Description</h3>

<p>The function <code>dmlavaan</code> compares model parameters from different <span class="pkg">lavaan</span>
models fitted to the same dataset. This leads to dependent coefficients.
Statistical inference is either conducted by M-estimation (i.e., robust
sandwich method; <code>method="bootstrap"</code>) or bootstrap (<code>method="bootstrap"</code>).
See Mize et al. (2019) or Weesie (1999) for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmlavaan(fun1, args1, fun2, args2, method="sandwich", R=50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dmlavaan_+3A_fun1">fun1</code></td>
<td>

<p>lavaan function of the first model (e.g., <code>"lavaan"</code>, <code>"cfa"</code>, or
<code>"sem"</code>)
</p>
</td></tr>
<tr><td><code id="dmlavaan_+3A_args1">args1</code></td>
<td>

<p>arguments for lavaan function in the first model
</p>
</td></tr>
<tr><td><code id="dmlavaan_+3A_fun2">fun2</code></td>
<td>

<p>lavaan function of the second model (e.g., <code>"lavaan"</code>, <code>"cfa"</code>, or
<code>"sem"</code>)
</p>
</td></tr>
<tr><td><code id="dmlavaan_+3A_args2">args2</code></td>
<td>

<p>arguments for lavaan function in the second model
</p>
</td></tr>
<tr><td><code id="dmlavaan_+3A_method">method</code></td>
<td>

<p>estimation method for standard errors
</p>
</td></tr>
<tr><td><code id="dmlavaan_+3A_r">R</code></td>
<td>

<p>Number of bootstrap samples
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In bootstrap estimation, a normal approximation is applied in the
computation of confidence intervals. Hence, <code>R</code> could be chosen
relatively small. <br /> <br />
TO DO (not yet implemented): <br />
</p>

<table>
<tr>
 <td style="text-align: left;">
1) </td><td style="text-align: left;"> inclusion of sampling weights </td>
</tr>
<tr>
 <td style="text-align: left;">
2) </td><td style="text-align: left;"> cluster robust standard errors in hierarchical sampling </td>
</tr>
<tr>
 <td style="text-align: left;">
3) </td><td style="text-align: left;"> stratification </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>coef</code></td>
<td>
<p>Model parameters of both models</p>
</td></tr>
<tr><td><code>vcov</code></td>
<td>
<p>Covariance matrix of model parameters of both models</p>
</td></tr>
<tr><td><code>partable</code></td>
<td>
<p>Parameter table containing all univariate model parameters</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>More entries</p>
</td></tr>
</table>


<h3>References</h3>

<p>Mize, T.D., Doan, L., &amp; Long, J.S. (2019).
A general framework for comparing predictions and marginal effects across models.
<em>Sociological Methodology, 49</em>(1), 152-189.
<a href="https://doi.org/10.1177/0081175019852763">doi:10.1177/0081175019852763</a>
</p>
<p>Weesie, J. (1999) Seemingly unrelated estimation and the cluster-adjusted sandwich
estimator. <em>Stata Technical Bulletin, 9</em>, 231-248.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
############################################################################
# EXAMPLE 1: Confirmatory factor analysis with and without fourth item
#############################################################################

#**** simulate data
N &lt;- 200  # number of persons
I &lt;- 4    # number of items

# loadings and error correlations
lam &lt;- seq(.7,.4, len=I)
PSI &lt;- diag( 1-lam^2 )

# define some model misspecification
sd_error &lt;- .1
S1 &lt;- matrix( c( -1.84, 0.39,-0.68, 0.13,
  0.39,-1.31,-0.07,-0.27,
 -0.68,-0.07, 0.90, 1.91,
  0.13,-0.27, 1.91,-0.56 ), nrow=4, ncol=4, byrow=TRUE)
S1 &lt;- ( S1 - mean(S1) ) / sd(S1) * sd_error

Sigma &lt;- lam %*% t(lam) + PSI + S1
dat &lt;- MASS::mvrnorm(n=N, mu=rep(0,I), Sigma=Sigma)
colnames(dat) &lt;- paste0("X",1:4)
dat &lt;- as.data.frame(dat)
rownames(Sigma) &lt;- colnames(Sigma) &lt;- colnames(dat)


#*** define two lavaan models
lavmodel1 &lt;- "F=~ X1 + X2 + X3 + X4"
lavmodel2 &lt;- "F=~ X1 + X2 + X3"

#*** define lavaan estimation arguments and functions
fun2 &lt;- fun1 &lt;- "cfa"
args1 &lt;- list( model=lavmodel1, data=dat, std.lv=TRUE, estimator="MLR")
args2 &lt;- args1
args2$model &lt;- lavmodel2

#* run model comparison
res1 &lt;- sirt::dmlavaan( fun1=fun1, args1=args1, fun2=fun2, args2=args2)

# inspect results
sirt:::print_digits(res1$partable, digits=3)

## End(Not run)
</code></pre>

<hr>
<h2 id='eigenvalues.manymatrices'>
Computation of Eigenvalues of Many Symmetric Matrices
</h2><span id='topic+eigenvalues.manymatrices'></span>

<h3>Description</h3>

<p>This function computes the eigenvalue decomposition of <code class="reqn">N</code>
symmetric positive definite matrices. The eigenvalues are computed
by the Rayleigh quotient method (Lange, 2010, p. 120). In addition,
the inverse matrix can be calculated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eigenvalues.manymatrices(Sigma.all, itermax=10, maxconv=0.001,
    inverse=FALSE )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eigenvalues.manymatrices_+3A_sigma.all">Sigma.all</code></td>
<td>

<p>An <code class="reqn">N \times D^2</code> matrix containing the <code class="reqn">D^2</code> entries
of <code class="reqn">N</code> symmetric matrices of dimension <code class="reqn">D \times D</code>
</p>
</td></tr>
<tr><td><code id="eigenvalues.manymatrices_+3A_itermax">itermax</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="eigenvalues.manymatrices_+3A_maxconv">maxconv</code></td>
<td>

<p>Convergence criterion for convergence of eigenvectors
</p>
</td></tr>
<tr><td><code id="eigenvalues.manymatrices_+3A_inverse">inverse</code></td>
<td>
<p>A logical which indicates if the inverse matrix
shall be calculated</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>lambda</code></td>
<td>
<p>Matrix with eigenvalues</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>An <code class="reqn">N \times D^2</code> Matrix of orthonormal eigenvectors</p>
</td></tr>
<tr><td><code>logdet</code></td>
<td>
<p>Vector of logarithm of determinants</p>
</td></tr>
<tr><td><code>det</code></td>
<td>
<p>Vector of determinants</p>
</td></tr>
<tr><td><code>Sigma.inv</code></td>
<td>
<p>Inverse matrix if <code>inverse=TRUE</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lange, K. (2010). <em>Numerical Analysis for Statisticians</em>.
New York: Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># define matrices
Sigma &lt;- diag(1,3)
Sigma[ lower.tri(Sigma) ] &lt;- Sigma[ upper.tri(Sigma) ] &lt;- c(.4,.6,.8 )
Sigma1 &lt;- Sigma

Sigma &lt;- diag(1,3)
Sigma[ lower.tri(Sigma) ] &lt;- Sigma[ upper.tri(Sigma) ] &lt;- c(.2,.1,.99 )
Sigma2 &lt;- Sigma

# collect matrices in a "super-matrix"
Sigma.all &lt;- rbind( matrix( Sigma1, nrow=1, byrow=TRUE),
                matrix( Sigma2, nrow=1, byrow=TRUE) )
Sigma.all &lt;- Sigma.all[ c(1,1,2,2,1 ), ]

# eigenvalue decomposition
m1 &lt;- sirt::eigenvalues.manymatrices( Sigma.all )
m1

# eigenvalue decomposition for Sigma1
s1 &lt;- svd(Sigma1)
s1
</code></pre>

<hr>
<h2 id='equating.rasch'>
Equating in the Generalized Logistic Rasch Model
</h2><span id='topic+equating.rasch'></span>

<h3>Description</h3>

<p>This function does the linking in the generalized
logistic item response model. Only item difficulties (<code class="reqn">b</code>
item parameters) are allowed. Mean-mean linking and the methods
of Haebara and Stocking-Lord are implemented (Kolen &amp; Brennan, 2004).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equating.rasch(x, y, theta=seq(-4, 4, len=100),
       alpha1=0, alpha2=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="equating.rasch_+3A_x">x</code></td>
<td>

<p>Matrix with two columns: First column items, second column item
difficulties
</p>
</td></tr>
<tr><td><code id="equating.rasch_+3A_y">y</code></td>
<td>

<p>Matrix with two columns: First columns item, second column item
difficulties
</p>
</td></tr>
<tr><td><code id="equating.rasch_+3A_theta">theta</code></td>
<td>

<p>Vector of theta values at which the linking functions
should be evaluated. If a weighting according to a prespecified normal
distribution <code class="reqn">N( \mu,\sigma^2)</code> is aimed, then choose
<code>theta=stats::qnorm( seq(.001, .999, len=100), mean=mu, sd=sigma)</code>
</p>
</td></tr>
<tr><td><code id="equating.rasch_+3A_alpha1">alpha1</code></td>
<td>

<p>Fixed <code class="reqn">\alpha_1</code> parameter in the generalized item
response model
</p>
</td></tr>
<tr><td><code id="equating.rasch_+3A_alpha2">alpha2</code></td>
<td>

<p>Fixed <code class="reqn">\alpha_2</code> parameter in the generalized item
response model
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>B.est</code></td>
<td>
<p>Estimated linking constants according to the methods
<code>Mean.Mean</code> (Mean-mean linking), <code>Haebara</code> (Haebara method)
and <code>Stocking.Lord</code> (Stocking-Lord method).</p>
</td></tr>
<tr><td><code>descriptives</code></td>
<td>
<p>Descriptives of the linking. The linking error
(<code>linkerror</code>) is calculated under the assumption of simple
random sampling of items</p>
</td></tr>
<tr><td><code>anchor</code></td>
<td>
<p>Original and transformed item parameters of anchor items</p>
</td></tr>
<tr><td><code>transf.par</code></td>
<td>
<p>Original and transformed item parameters of all items</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kolen, M. J., &amp; Brennan, R. L. (2004). <em>Test Equating, Scaling, and Linking:
Methods and Practices</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p>For estimating standard errors (due to inference with respect to
the item domain) of this procedure see <code><a href="#topic+equating.rasch.jackknife">equating.rasch.jackknife</a></code>.
</p>
<p>For linking several studies see <code><a href="#topic+linking.haberman">linking.haberman</a></code> or
<code><a href="#topic+invariance.alignment">invariance.alignment</a></code>.
</p>
<p>A robust alternative to mean-mean linking is implemented in
<code><a href="#topic+linking.robust">linking.robust</a></code>.
</p>
<p>For linking under more general item response models
see the <span class="pkg">plink</span> package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Linking item parameters of the PISA study
#############################################################################

data(data.pisaPars)
pars &lt;- data.pisaPars

# linking the two studies with the Rasch model
mod &lt;- sirt::equating.rasch(x=pars[,c("item","study1")], y=pars[,c("item","study2")])
  ##   Mean.Mean    Haebara Stocking.Lord
  ## 1   0.08828 0.08896269    0.09292838

## Not run: 
#*** linking using the plink package
# The plink package is not available on CRAN anymore.
# You can download the package with
# utils::install.packages("plink", repos="http://www2.uaem.mx/r-mirror")
library(plink)
I &lt;- nrow(pars)
pm &lt;- plink::as.poly.mod(I)
# linking parameters
plink.pars1 &lt;- list( "study1"=data.frame( 1, pars$study1, 0 ),
                     "study2"=data.frame( 1, pars$study2, 0 ) )
      # the parameters are arranged in the columns:
      # Discrimination, Difficulty, Guessing Parameter
# common items
common.items &lt;- cbind("study1"=1:I,"study2"=1:I)
# number of categories per item
cats.item &lt;- list( "study1"=rep(2,I), "study2"=rep(2,I))
# convert into plink object
x &lt;- plink::as.irt.pars( plink.pars1, common.items, cat=cats.item,
          poly.mod=list(pm,pm))
# linking using plink: first group is reference group
out &lt;- plink::plink(x, rescale="MS", base.grp=1, D=1.7)
# summary for linking
summary(out)
  ##   -------  group2/group1*  -------
  ##   Linking Constants
  ##
  ##                        A         B
  ##   Mean/Mean     1.000000 -0.088280
  ##   Mean/Sigma    1.000000 -0.088280
  ##   Haebara       1.000000 -0.088515
  ##   Stocking-Lord 1.000000 -0.096610
# extract linked parameters
pars.out &lt;- plink::link.pars(out)

## End(Not run)
</code></pre>

<hr>
<h2 id='equating.rasch.jackknife'>
Jackknife Equating Error in Generalized Logistic Rasch Model
</h2><span id='topic+equating.rasch.jackknife'></span>

<h3>Description</h3>

<p>This function estimates the linking error in linking
based on Jackknife (Monseur &amp; Berezner, 2007).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equating.rasch.jackknife(pars.data, display=TRUE,
   se.linkerror=FALSE, alpha1=0, alpha2=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="equating.rasch.jackknife_+3A_pars.data">pars.data</code></td>
<td>

<p>Data frame with four columns: jackknife unit (1st column),
item parameter study 1 (2nd column), item parameter study 2 (3rd column),
item (4th column)
</p>
</td></tr>
<tr><td><code id="equating.rasch.jackknife_+3A_display">display</code></td>
<td>

<p>Display progress?
</p>
</td></tr>
<tr><td><code id="equating.rasch.jackknife_+3A_se.linkerror">se.linkerror</code></td>
<td>

<p>Compute standard error of the linking error
</p>
</td></tr>
<tr><td><code id="equating.rasch.jackknife_+3A_alpha1">alpha1</code></td>
<td>

<p>Fixed <code class="reqn">\alpha_1</code> parameter in the generalized item
response model
</p>
</td></tr>
<tr><td><code id="equating.rasch.jackknife_+3A_alpha2">alpha2</code></td>
<td>

<p>Fixed <code class="reqn">\alpha_2</code> parameter in the generalized item
response model
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>pars.data</code></td>
<td>
<p>Used item parameters</p>
</td></tr>
<tr><td><code>itemunits</code></td>
<td>
<p>Used units for jackknife</p>
</td></tr>
<tr><td><code>descriptives</code></td>
<td>
<p>Descriptives for Jackknife.
<code>linkingerror.jackknife</code> is the estimated linking error.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Monseur, C., &amp; Berezner, A. (2007). The computation of equating errors
in international surveys in education.
<em>Journal of Applied Measurement, 8</em>, 323-335.
</p>


<h3>See Also</h3>

<p>For more details on linking methods see <code><a href="#topic+equating.rasch">equating.rasch</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Linking errors PISA study
#############################################################################

data(data.pisaPars)
pars &lt;- data.pisaPars

# Linking error: Jackknife unit is the testlet
vars &lt;- c("testlet","study1","study2","item")
res1 &lt;- sirt::equating.rasch.jackknife(pars[, vars])
res1$descriptives
  ##   N.items N.units      shift        SD linkerror.jackknife SE.SD.jackknife
  ## 1      25       8 0.09292838 0.1487387          0.04491197      0.03466309

# Linking error: Jackknife unit is the item
res2 &lt;- sirt::equating.rasch.jackknife(pars[, vars ] )
res2$descriptives
  ##   N.items N.units      shift        SD linkerror.jackknife SE.SD.jackknife
  ## 1      25      25 0.09292838 0.1487387          0.02682839      0.02533327
</code></pre>

<hr>
<h2 id='expl.detect'>
Exploratory DETECT Analysis
</h2><span id='topic+expl.detect'></span>

<h3>Description</h3>

<p>This function estimates the DETECT index (Stout, Habing, Douglas &amp; Kim, 1996;
Zhang &amp; Stout, 1999a, 1999b) in an exploratory way.
Conditional covariances of itempairs are transformed into a distance
matrix such that items are clustered by the hierarchical Ward
algorithm (Roussos, Stout &amp; Marden, 1998). Note that the function will not
provide the same output as the original DETECT software.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expl.detect(data, score, nclusters, N.est=NULL, seed=NULL, bwscale=1.1,
    smooth=TRUE, use_sum_score=FALSE, hclust_method="ward.D", estsample=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expl.detect_+3A_data">data</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous or polytomous responses.
Missing responses are allowed.
</p>
</td></tr>
<tr><td><code id="expl.detect_+3A_score">score</code></td>
<td>

<p>An ability estimate, e.g. the WLE, sum score or mean score
</p>
</td></tr>
<tr><td><code id="expl.detect_+3A_nclusters">nclusters</code></td>
<td>

<p>Maximum number of clusters used in the exploratory analysis
</p>
</td></tr>
<tr><td><code id="expl.detect_+3A_n.est">N.est</code></td>
<td>

<p>Number of students in a (possible) validation of the DETECT index.
<code>N.est</code> students are drawn at random from <code>data</code>.
</p>
</td></tr>
<tr><td><code id="expl.detect_+3A_seed">seed</code></td>
<td>

<p>Random seed
</p>
</td></tr>
<tr><td><code id="expl.detect_+3A_bwscale">bwscale</code></td>
<td>

<p>Bandwidth scale factor
</p>
</td></tr>
<tr><td><code id="expl.detect_+3A_smooth">smooth</code></td>
<td>
<p>Logical indicating whether smoothing should be
applied for conditional covariance estimation</p>
</td></tr>
<tr><td><code id="expl.detect_+3A_use_sum_score">use_sum_score</code></td>
<td>
<p>Logical indicating whether sum score should be used.
With this option, the bias corrected conditional covariance of Zhang and
Stout (1999) is used.</p>
</td></tr>
<tr><td><code id="expl.detect_+3A_hclust_method">hclust_method</code></td>
<td>
<p>Clustering method used as the argument
<code>method</code> in <code><a href="stats.html#topic+hclust">stats::hclust</a></code>.
</p>
</td></tr>
<tr><td><code id="expl.detect_+3A_estsample">estsample</code></td>
<td>
<p>Optional vector of subject indices that defines the
estimation sample</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>detect.unweighted</code></td>
<td>
<p>Unweighted DETECT statistics</p>
</td></tr>
<tr><td><code>detect.weighted</code></td>
<td>
<p>Weighted DETECT statistics. Weighting is done
proportionally to sample sizes of item pairs.</p>
</td></tr>
<tr><td><code>clusterfit</code></td>
<td>
<p>Fit of the cluster method</p>
</td></tr>
<tr><td><code>itemcluster</code></td>
<td>
<p>Cluster allocations</p>
</td></tr>
</table>
<p>use_sum_score
</p>


<h3>References</h3>

<p>Roussos, L. A., Stout, W. F., &amp; Marden, J. I. (1998). Using new proximity
measures with hierarchical cluster analysis to detect multidimensionality.
<em>Journal of Educational Measurement,
35</em>, 1-30.
</p>
<p>Stout, W., Habing, B., Douglas, J., &amp; Kim, H. R. (1996).
Conditional covariance-based nonparametric multidimensionality assessment.
<em>Applied Psychological Measurement, 20</em>, 331-354.
</p>
<p>Zhang, J., &amp; Stout, W. (1999a). Conditional covariance structure
of generalized compensatory multidimensional items,
<em>Psychometrika, 64</em>, 129-152.
</p>
<p>Zhang, J., &amp; Stout, W. (1999b). The theoretical DETECT index of
dimensionality and its application to approximate simple structure,
<em>Psychometrika, 64</em>, 213-249.
</p>


<h3>See Also</h3>

<p>For examples see <code><a href="#topic+conf.detect">conf.detect</a></code>.
</p>

<hr>
<h2 id='f1d.irt'>
Functional Unidimensional Item Response Model
</h2><span id='topic+f1d.irt'></span>

<h3>Description</h3>

<p>Estimates the functional unidimensional item response model for
dichotomous data (Ip, Molenberghs, Chen, Goegebeur &amp; De Boeck, 2013).
Either the IRT model is estimated using a probit link and employing tetrachoric
correlations or item discriminations and intercepts of a pre-estimated multidimensional IRT
model are provided as input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f1d.irt(dat=NULL, nnormal=1000, nfactors=3, A=NULL, intercept=NULL,
    mu=NULL, Sigma=NULL, maxiter=100, conv=10^(-5), progress=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f1d.irt_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous item responses
</p>
</td></tr>
<tr><td><code id="f1d.irt_+3A_nnormal">nnormal</code></td>
<td>

<p>Number of <code class="reqn">\theta_p</code> grid points for approximating the
normal distribution
</p>
</td></tr>
<tr><td><code id="f1d.irt_+3A_nfactors">nfactors</code></td>
<td>

<p>Number of dimensions to be estimated
</p>
</td></tr>
<tr><td><code id="f1d.irt_+3A_a">A</code></td>
<td>

<p>Matrix of item discriminations (if the IRT model is already estimated)
</p>
</td></tr>
<tr><td><code id="f1d.irt_+3A_intercept">intercept</code></td>
<td>

<p>Vector of item intercepts (if the IRT model is already estimated)
</p>
</td></tr>
<tr><td><code id="f1d.irt_+3A_mu">mu</code></td>
<td>
<p>Vector of estimated means. In the default it is assumed
that all means are zero.</p>
</td></tr>
<tr><td><code id="f1d.irt_+3A_sigma">Sigma</code></td>
<td>
<p>Estimated covariance matrix. In the default it is
the identity matrix.</p>
</td></tr>
<tr><td><code id="f1d.irt_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="f1d.irt_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="f1d.irt_+3A_progress">progress</code></td>
<td>

<p>Display progress? The default is <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional unidimensional item response model (F1D model)
for dichotomous item responses is based on a multidimensional model with a
link function <code class="reqn">g</code> (probit or logit):
</p>
<p style="text-align: center;"><code class="reqn"> P( X_{pi}=1 | \bold{\theta}_p )=
g( \sum_d a_{id} \theta_{pd} - d_i ) </code>
</p>

<p>It is assumed that <code class="reqn">\bold{\theta}_p</code> is multivariate normally
distribution with a zero mean vector and identity covariance matrix.
</p>
<p>The F1D model estimates unidimensional item response functions
such that
</p>
<p style="text-align: center;"><code class="reqn"> P( X_{pi}=1 | \theta_p^\ast ) \approx
g \left( a_{i}^\ast \theta_{p}^\ast - d_i^\ast \right) </code>
</p>

<p>The optimization function <code class="reqn">F</code> minimizes the deviations of
the approximation equations
</p>
<p style="text-align: center;"><code class="reqn"> a_{i}^\ast \theta_{p}^\ast - d_i^\ast
\approx \sum_d a_{id} \theta_{pd} - d_i </code>
</p>

<p>The optimization function <code class="reqn">F</code> is defined by
</p>
<p style="text-align: center;"><code class="reqn"> F( \{ a_i^\ast, d_i^\ast \}_i, \{ \theta_p^\ast \}_p )=
    \sum_p \sum_i w_p ( a_{id} \theta_{pd} - d_i-
a_{i}^\ast \theta_{p}^\ast + d_i^\ast )^2 \rightarrow Min! </code>
</p>

<p>All items <code class="reqn">i</code> are equally weighted whereas the ability
distribution of persons <code class="reqn">p</code> are weighted according to the
multivariate normal distribution (using weights <code class="reqn">w_p</code>).
The estimation is conducted using an alternating least squares algorithm
(see Ip et al. 2013 for a different algorithm). The ability distribution
<code class="reqn">\theta_p^\ast</code> of the functional unidimensional model is assumed
to be standardized, i.e. does have a zero mean and a standard deviation of one.
</p>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>item</code></td>
<td>
<p>Data frame with estimated item parameters: Item intercepts
for the functional unidimensional <code class="reqn">a_{i}^\ast</code> (<code>ai.ast</code>) and
the ('ordinary') unidimensional (<code>ai0</code>) item response
model. The same holds for item intercepts <code class="reqn">d_{i}^\ast</code> (<code>di.ast</code> and
<code>di0</code> respectively).
</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with estimated <code class="reqn">\theta_p^\ast</code>
distribution. Locations are <code>theta.ast</code> with corresponding
probabilities in <code>wgt</code>.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>Estimated or provided item discriminations</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>Estimated or provided intercepts</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Used dataset</p>
</td></tr>
<tr><td><code>tetra</code></td>
<td>
<p>Object generated by <code><a href="#topic+tetrachoric2">tetrachoric2</a></code> if <code>dat</code>
is specified as input. This list entry is useful for applying
<code><a href="#topic+greenyang.reliability">greenyang.reliability</a></code>.
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Ip, E. H., Molenberghs, G., Chen, S. H., Goegebeur, Y., &amp;
De Boeck, P. (2013). Functionally unidimensional item
response models for multivariate binary data.
<em>Multivariate Behavioral Research, 48</em>, 534-562.
</p>


<h3>See Also</h3>

<p>For estimation of bifactor models and Green-Yang reliability
based on tetrachoric correlations see <code><a href="#topic+greenyang.reliability">greenyang.reliability</a></code>.
</p>
<p>For estimation of bifactor models based on marginal maximum likelihood
(i.e. full information maximum likelihood) see the
<code><a href="TAM.html#topic+tam.fa">TAM::tam.fa</a></code> function in the <span class="pkg">TAM</span> package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset Mathematics data.math | Exploratory multidimensional model
#############################################################################
data(data.math)
dat &lt;- ( data.math$data )[, -c(1,2) ] # select Mathematics items

#****
# Model 1: Functional unidimensional model based on original data

#++ (1) estimate model with 3 factors
mod1 &lt;- sirt::f1d.irt( dat=dat, nfactors=3)

#++ (2) plot results
     par(mfrow=c(1,2))
# Intercepts
plot( mod1$item$di0, mod1$item$di.ast, pch=16, main="Item Intercepts",
        xlab=expression( paste( d[i], " (Unidimensional Model)" )),
        ylab=expression( paste( d[i], " (Functional Unidimensional Model)" )))
abline( lm(mod1$item$di.ast ~ mod1$item$di0), col=2, lty=2 )
# Discriminations
plot( mod1$item$ai0, mod1$item$ai.ast, pch=16, main="Item Discriminations",
        xlab=expression( paste( a[i], " (Unidimensional Model)" )),
        ylab=expression( paste( a[i], " (Functional Unidimensional Model)" )))
abline( lm(mod1$item$ai.ast ~ mod1$item$ai0), col=2, lty=2 )
     par(mfrow=c(1,1))

#++ (3) estimate bifactor model and Green-Yang reliability
gy1 &lt;- sirt::greenyang.reliability( mod1$tetra, nfactors=3 )

## Not run: 
#****
# Model 2: Functional unidimensional model based on estimated multidimensional
#          item response model

#++ (1) estimate 2-dimensional exploratory factor analysis with 'smirt'
I &lt;- ncol(dat)
Q &lt;- matrix( 1, I,2 )
Q[1,2] &lt;- 0
variance.fixed &lt;- cbind( 1,2,0 )
mod2a &lt;- sirt::smirt( dat, Qmatrix=Q, irtmodel="comp", est.a="2PL",
                variance.fixed=variance.fixed, maxiter=50)
#++ (2) input estimated discriminations and intercepts for
#       functional unidimensional model
mod2b &lt;- sirt::f1d.irt( A=mod2a$a, intercept=mod2a$b )

#############################################################################
# EXAMPLE 2: Dataset Mathematics data.math | Confirmatory multidimensional model
#############################################################################

data(data.math)
library(TAM)

# dataset
dat &lt;- data.math$data
dat &lt;- dat[, grep("M", colnames(dat) ) ]
# extract item informations
iteminfo &lt;- data.math$item
I &lt;- ncol(dat)
# define Q-matrix
Q &lt;- matrix( 0, nrow=I, ncol=3 )
Q[ grep( "arith", iteminfo$domain ), 1 ] &lt;- 1
Q[ grep( "Meas", iteminfo$domain ), 2 ] &lt;- 1
Q[ grep( "geom", iteminfo$domain ), 3 ] &lt;- 1

# fit three-dimensional model in TAM
mod1 &lt;- TAM::tam.mml.2pl(  dat, Q=Q, control=list(maxiter=40, snodes=1000) )
summary(mod1)

# specify functional unidimensional model
intercept &lt;- mod1$xsi[, c("xsi") ]
names(intercept) &lt;- rownames(mod1$xsi)
fumod1 &lt;- sirt::f1d.irt( A=mod1$B[,2,], intercept=intercept, Sigma=mod1$variance)
fumod1$item

## End(Not run)
</code></pre>

<hr>
<h2 id='fit.isop'>
Fitting the ISOP and ADISOP Model for Frequency Tables
</h2><span id='topic+fit.isop'></span><span id='topic+fit.adisop'></span>

<h3>Description</h3>

<p>Fit the isotonic probabilistic model (ISOP;
Scheiblechner, 1995) and the additive isotonic probabilistic
model (ADISOP; Scheiblechner, 1999).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit.isop(freq.correct, wgt, conv=1e-04, maxit=100,
      progress=TRUE, calc.ll=TRUE)

fit.adisop(freq.correct, wgt, conv=1e-04, maxit=100,
      epsilon=0.01, progress=TRUE, calc.ll=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit.isop_+3A_freq.correct">freq.correct</code></td>
<td>

<p>Frequency table
</p>
</td></tr>
<tr><td><code id="fit.isop_+3A_wgt">wgt</code></td>
<td>

<p>Weights for frequency table (number of persons in
each cell)
</p>
</td></tr>
<tr><td><code id="fit.isop_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="fit.isop_+3A_maxit">maxit</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="fit.isop_+3A_epsilon">epsilon</code></td>
<td>
<p>Additive constant to handle cell frequencies
of 0 or 1 in <code>fit.adisop</code>
</p>
</td></tr>
<tr><td><code id="fit.isop_+3A_progress">progress</code></td>
<td>

<p>Display progress?
</p>
</td></tr>
<tr><td><code id="fit.isop_+3A_calc.ll">calc.ll</code></td>
<td>
<p>Calculate log-likelihood values?
The default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+isop.dich">isop.dich</a></code> for more details of the
ISOP and ADISOP model.
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>fX</code></td>
<td>
<p>Fitted frequency table</p>
</td></tr>
<tr><td><code>ResX</code></td>
<td>
<p>Residual frequency table</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>Fit statistic: weighted least squares
of deviations between observed and expected frequencies</p>
</td></tr>
<tr><td><code>item.sc</code></td>
<td>
<p>Estimated item parameters</p>
</td></tr>
<tr><td><code>person.sc</code></td>
<td>

<p>Estimated person parameters
</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>Log-likelihood of the model</p>
</td></tr>
<tr><td><code>freq.fitted</code></td>
<td>
<p>Fitted frequencies in a long data frame</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For fitting the ADISOP model it is recommended to first
fit the ISOP model and then proceed with the fitted frequency
table from ISOP (see Examples).
</p>


<h3>References</h3>

<p>Scheiblechner, H. (1995). Isotonic ordinal
probabilistic models (ISOP). <em>Psychometrika,
60</em>, 281-304.
</p>
<p>Scheiblechner, H. (1999). Additive conjoint isotonic
probabilistic models (ADISOP). <em>Psychometrika,
64</em>, 295-316.
</p>


<h3>See Also</h3>

<p>For fitting the ISOP model to dichotomous and
polytomous data see <code><a href="#topic+isop.dich">isop.dich</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset Reading
#############################################################################

data(data.read)
dat &lt;- as.matrix( data.read)
dat.resp &lt;- 1 - is.na(dat) # response indicator matrix
I &lt;- ncol(dat)

#***
# (1) Data preparation
#     actually only freq.correct and wgt are needed
#     but these matrices must be computed in advance.

# different scores of students
stud.p &lt;- rowMeans( dat, na.rm=TRUE )
# different item p values
item.p &lt;- colMeans( dat, na.rm=TRUE )
item.ps &lt;- sort( item.p, index.return=TRUE)
dat &lt;- dat[,  item.ps$ix ]
# define score groups students
scores &lt;- sort( unique( stud.p ) )
SC &lt;- length(scores)
# create table
freq.correct &lt;- matrix( NA, SC, I )
wgt &lt;- freq.correct
# percent correct
a1 &lt;- stats::aggregate( dat==1, list( stud.p ), mean, na.rm=TRUE )
freq.correct &lt;- a1[,-1]
# weights
a1 &lt;- stats::aggregate( dat.resp, list( stud.p ), sum, na.rm=TRUE )
wgt &lt;- a1[,-1]

#***
# (2) Fit ISOP model
res.isop &lt;- sirt::fit.isop( freq.correct, wgt )
# fitted frequency table
res.isop$fX

#***
# (3) Fit ADISOP model
# use monotonely smoothed frequency table from ISOP model
res.adisop &lt;- sirt::fit.adisop( freq.correct=res.isop$fX, wgt )
# fitted frequency table
res.adisop$fX
</code></pre>

<hr>
<h2 id='fuzcluster'>
Clustering for Continuous Fuzzy Data
</h2><span id='topic+fuzcluster'></span><span id='topic+summary.fuzcluster'></span>

<h3>Description</h3>

<p>This function performs clustering for continuous fuzzy data
for which membership functions are assumed to be Gaussian
(Denoeux, 2013). The mixture is also assumed to be Gaussian and
(conditionally cluster membership) independent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fuzcluster(dat_m, dat_s, K=2, nstarts=7, seed=NULL, maxiter=100,
     parmconv=0.001, fac.oldxsi=0.75, progress=TRUE)

## S3 method for class 'fuzcluster'
summary(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fuzcluster_+3A_dat_m">dat_m</code></td>
<td>

<p>Centers for individual item specific membership functions
</p>
</td></tr>
<tr><td><code id="fuzcluster_+3A_dat_s">dat_s</code></td>
<td>

<p>Standard deviations for individual item specific membership functions
</p>
</td></tr>
<tr><td><code id="fuzcluster_+3A_k">K</code></td>
<td>

<p>Number of latent classes
</p>
</td></tr>
<tr><td><code id="fuzcluster_+3A_nstarts">nstarts</code></td>
<td>

<p>Number of random starts. The default is 7 random starts.
</p>
</td></tr>
<tr><td><code id="fuzcluster_+3A_seed">seed</code></td>
<td>

<p>Simulation seed. If one value is provided, then only one
start is performed.
</p>
</td></tr>
<tr><td><code id="fuzcluster_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="fuzcluster_+3A_parmconv">parmconv</code></td>
<td>

<p>Maximum absolute change in parameters
</p>
</td></tr>
<tr><td><code id="fuzcluster_+3A_fac.oldxsi">fac.oldxsi</code></td>
<td>
<p>Convergence acceleration factor which
should take values between 0 and 1. The default is 0.75.</p>
</td></tr>
<tr><td><code id="fuzcluster_+3A_progress">progress</code></td>
<td>

<p>An optional logical indicating whether iteration progress
should be displayed.
</p>
</td></tr>
<tr><td><code id="fuzcluster_+3A_object">object</code></td>
<td>
<p>Object of class <code>fuzcluster</code></p>
</td></tr>
<tr><td><code id="fuzcluster_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>pi_est</code></td>
<td>
<p>Estimated class probabilities</p>
</td></tr>
<tr><td><code>mu_est</code></td>
<td>
<p>Cluster means</p>
</td></tr>
<tr><td><code>sd_est</code></td>
<td>
<p>Cluster standard deviations</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>Individual posterior distributions of cluster membership</p>
</td></tr>
<tr><td><code>seed</code></td>
<td>
<p>Simulation seed for cluster solution</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria</p>
</td></tr>
</table>


<h3>References</h3>

<p>Denoeux, T. (2013). Maximum likelihood estimation from uncertain data
in the belief function framework.
<em>IEEE Transactions on Knowledge and Data Engineering, 25</em>, 119-130.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+fuzdiscr">fuzdiscr</a></code> for estimating discrete distributions for
fuzzy data.
</p>
<p>See the <span class="pkg">fclust</span> package for fuzzy clustering.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: 2 classes and 3 items
#############################################################################

#*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-
# simulate data (2 classes and 3 items)
set.seed(876)
library(mvtnorm)
Ntot &lt;- 1000  # number of subjects
# define SDs for simulating uncertainty
sd_uncertain &lt;- c( .2, 1, 2 )

dat_m &lt;- NULL   # data frame containing mean of membership function
dat_s &lt;- NULL   # data frame containing SD of membership function

# *** Class 1
pi_class &lt;- .6
Nclass &lt;- Ntot * pi_class
mu &lt;- c(3,1,0)
Sigma &lt;- diag(3)
# simulate data
dat_m1 &lt;- mvtnorm::rmvnorm( Nclass, mean=mu, sigma=Sigma )
dat_s1 &lt;- matrix( stats::runif( Nclass * 3 ), nrow=Nclass )
for ( ii in 1:3){ dat_s1[,ii] &lt;- dat_s1[,ii] * sd_uncertain[ii] }
dat_m &lt;- rbind( dat_m, dat_m1 )
dat_s &lt;- rbind( dat_s, dat_s1 )

# *** Class 2
pi_class &lt;- .4
Nclass &lt;- Ntot * pi_class
mu &lt;- c(0,-2,0.4)
Sigma &lt;- diag(c(0.5, 2, 2 ) )
# simulate data
dat_m1 &lt;- mvtnorm::rmvnorm( Nclass, mean=mu, sigma=Sigma )
dat_s1 &lt;- matrix( stats::runif( Nclass * 3 ), nrow=Nclass )
for ( ii in 1:3){ dat_s1[,ii] &lt;- dat_s1[,ii] * sd_uncertain[ii] }
dat_m &lt;- rbind( dat_m, dat_m1 )
dat_s &lt;- rbind( dat_s, dat_s1 )
colnames(dat_s) &lt;- colnames(dat_m) &lt;- paste0("I", 1:3 )

#*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-
# estimation

#*** Model 1: Clustering with 8 random starts
res1 &lt;- sirt::fuzcluster(K=2,dat_m, dat_s, nstarts=8, maxiter=25)
summary(res1)
  ##  Number of iterations=22 (Seed=5090 )
  ##  ---------------------------------------------------
  ##  Class probabilities (2 Classes)
  ##  [1] 0.4083 0.5917
  ##
  ##  Means
  ##           I1      I2     I3
  ##  [1,] 0.0595 -1.9070 0.4011
  ##  [2,] 3.0682  1.0233 0.0359
  ##
  ##  Standard deviations
  ##         [,1]   [,2]   [,3]
  ##  [1,] 0.7238 1.3712 1.2647
  ##  [2,] 0.9740 0.8500 0.7523

#*** Model 2: Clustering with one start with seed 4550
res2 &lt;- sirt::fuzcluster(K=2,dat_m, dat_s, nstarts=1, seed=5090 )
summary(res2)

#*** Model 3: Clustering for crisp data
#             (assuming no uncertainty, i.e. dat_s=0)
res3 &lt;- sirt::fuzcluster(K=2,dat_m, dat_s=0*dat_s, nstarts=30, maxiter=25)
summary(res3)
  ##  Class probabilities (2 Classes)
  ##  [1] 0.3645 0.6355
  ##
  ##  Means
  ##           I1      I2      I3
  ##  [1,] 0.0463 -1.9221  0.4481
  ##  [2,] 3.0527  1.0241 -0.0008
  ##
  ##  Standard deviations
  ##         [,1]   [,2]   [,3]
  ##  [1,] 0.7261 1.4541 1.4586
  ##  [2,] 0.9933 0.9592 0.9535

#*** Model 4: kmeans cluster analysis
res4 &lt;- stats::kmeans( dat_m, centers=2 )
  ##   K-means clustering with 2 clusters of sizes 607, 393
  ##   Cluster means:
  ##             I1        I2          I3
  ##   1 3.01550780  1.035848 -0.01662275
  ##   2 0.03448309 -2.008209  0.48295067

## End(Not run)
</code></pre>

<hr>
<h2 id='fuzdiscr'>
Estimation of a Discrete Distribution for Fuzzy Data (Data in Belief Function
Framework)
</h2><span id='topic+fuzdiscr'></span>

<h3>Description</h3>

<p>This function estimates a discrete distribution for uncertain data
based on the belief function framework (Denoeux, 2013; see Details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fuzdiscr(X, theta0=NULL, maxiter=200, conv=1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fuzdiscr_+3A_x">X</code></td>
<td>

<p>Matrix with fuzzy data. Rows corresponds to subjects and columns to
values of the membership function
</p>
</td></tr>
<tr><td><code id="fuzdiscr_+3A_theta0">theta0</code></td>
<td>

<p>Initial vector of parameter estimates
</p>
</td></tr>
<tr><td><code id="fuzdiscr_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="fuzdiscr_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code class="reqn">n</code> subjects, membership functions <code class="reqn">m_n(k)</code> are observed which indicate
the belief in data <code class="reqn">X_n=k</code>. The membership function is interpreted as
<em>epistemic uncertainty</em> (Denoeux, 2011). However, associated parameters
in statistical models are crisp which means that models are formulated at the
basis of precise (crisp) data if they would be observed.
</p>
<p>In the present estimation problem of a discrete distribution,
the parameters of interest are category probabilities
<code class="reqn">\theta_k=P( X=k)</code>.
</p>
<p>The parameter estimation follows the evidential EM algorithm (Denoeux, 2013).
</p>


<h3>Value</h3>

<p>Vector of probabilities of the discrete distribution
</p>


<h3>References</h3>

<p>Denoeux, T. (2011). Maximum likelihood estimation from fuzzy data using the
EM algorithm. <em>Fuzzy Sets and Systems, 183</em>, 72-91.
</p>
<p>Denoeux, T. (2013). Maximum likelihood estimation from uncertain data
in the belief function framework.
<em>IEEE Transactions on Knowledge and Data Engineering, 25</em>, 119-130.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Binomial distribution Denoeux Example 4.3 (2013)
#############################################################################

#*** define uncertain data
X_alpha &lt;- function( alpha ){
    Q &lt;- matrix( 0, 6, 2 )
    Q[5:6,2] &lt;- Q[1:3,1] &lt;- 1
    Q[4,] &lt;- c( alpha, 1 - alpha )
    return(Q)
        }

# define data for alpha=0.5
X &lt;- X_alpha( alpha=.5 )
  ##   &gt; X
  ##        [,1] [,2]
  ##   [1,]  1.0  0.0
  ##   [2,]  1.0  0.0
  ##   [3,]  1.0  0.0
  ##   [4,]  0.5  0.5
  ##   [5,]  0.0  1.0
  ##   [6,]  0.0  1.0

  ## The fourth observation has equal plausibility for the first and the
  ## second category.

# parameter estimate uncertain data
fuzdiscr( X )
  ##   &gt; sirt::fuzdiscr( X )
  ##   [1] 0.5999871 0.4000129

# parameter estimate pseudo likelihood
colMeans( X )
  ##   &gt; colMeans( X )
  ##   [1] 0.5833333 0.4166667
##-&gt; Observations are weighted according to belief function values.

#*****
# plot parameter estimates as function of alpha
alpha &lt;- seq( 0, 1, len=100 )
res &lt;- sapply( alpha, FUN=function(aa){
             X &lt;- X_alpha( alpha=aa )
             c( sirt::fuzdiscr( X )[1], colMeans( X )[1] )
                    } )
# plot
plot( alpha, res[1,], xlab=expression(alpha), ylab=expression( theta[alpha] ), type="l",
        main="Comparison Belief Function and Pseudo-Likelihood (Example 1)")
lines( alpha, res[2,], lty=2, col=2)
legend( 0, .67, c("Belief Function", "Pseudo-Likelihood" ), col=c(1,2), lty=c(1,2) )

#############################################################################
# EXAMPLE 2: Binomial distribution (extends Example 1)
#############################################################################

X_alpha &lt;- function( alpha ){
    Q &lt;- matrix( 0, 6, 2 )
    Q[6,2] &lt;- Q[1:2,1] &lt;- 1
    Q[3:5,] &lt;- matrix( c( alpha, 1 - alpha ), 3, 2, byrow=TRUE)
    return(Q)
        }

X &lt;- X_alpha( alpha=.5 )
alpha &lt;- seq( 0, 1, len=100 )
res &lt;- sapply( alpha, FUN=function(aa){
           X &lt;- X_alpha( alpha=aa )
           c( sirt::fuzdiscr( X )[1], colMeans( X )[1] )
                    } )
# plot
plot( alpha, res[1,], xlab=expression(alpha), ylab=expression( theta[alpha] ), type="l",
        main="Comparison Belief Function and Pseudo-Likelihood (Example 2)")
lines( alpha, res[2,], lty=2, col=2)
legend( 0, .67, c("Belief Function", "Pseudo-Likelihood" ), col=c(1,2), lty=c(1,2) )

#############################################################################
# EXAMPLE 3: Multinomial distribution with three categories
#############################################################################

# define uncertain data
X &lt;- matrix( c( 1,0,0, 1,0,0,   0,1,0, 0,0,1, .7, .2, .1,
         .4, .6, 0 ), 6, 3, byrow=TRUE )
  ##   &gt; X
  ##        [,1] [,2] [,3]
  ##   [1,]  1.0  0.0  0.0
  ##   [2,]  1.0  0.0  0.0
  ##   [3,]  0.0  1.0  0.0
  ##   [4,]  0.0  0.0  1.0
  ##   [5,]  0.7  0.2  0.1
  ##   [6,]  0.4  0.6  0.0

##-&gt;  Only the first four observations are crisp.

#*** estimation for uncertain data
fuzdiscr( X )
  ##   &gt; sirt::fuzdiscr( X )
  ##   [1] 0.5772305 0.2499931 0.1727764

#*** estimation pseudo-likelihood
colMeans(X)
  ##   &gt; colMeans(X)
  ##   [1] 0.5166667 0.3000000 0.1833333

##-&gt; Obviously, the treatment uncertainty is different in belief function
##   and in pseudo-likelihood framework.
</code></pre>

<hr>
<h2 id='gom.em'>
Discrete (Rasch) Grade of Membership Model
</h2><span id='topic+gom.em'></span><span id='topic+summary.gom'></span><span id='topic+logLik.gom'></span><span id='topic+anova.gom'></span><span id='topic+IRT.irfprob.gom'></span><span id='topic+IRT.likelihood.gom'></span><span id='topic+IRT.posterior.gom'></span><span id='topic+IRT.modelfit.gom'></span><span id='topic+summary.IRT.modelfit.gom'></span>

<h3>Description</h3>

<p>This function estimates the grade of membership model (Erosheva, Fienberg
&amp; Joutard, 2007; also called mixed membership model) by the EM algorithm
assuming a discrete membership score distribution. The function is restricted
to dichotomous item responses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gom.em(dat, K=NULL, problevels=NULL, weights=NULL, model="GOM", theta0.k=seq(-5,5,len=15),
    xsi0.k=exp(seq(-6, 3, len=15)), max.increment=0.3, numdiff.parm=1e-4,
    maxdevchange=1e-6, globconv=1e-4, maxiter=1000, msteps=4, mstepconv=0.001,
    theta_adjust=FALSE, lambda.inits=NULL, lambda.index=NULL, pi.k.inits=NULL,
    newton_raphson=TRUE, optimizer="nlminb", progress=TRUE)

## S3 method for class 'gom'
summary(object, file=NULL, ...)

## S3 method for class 'gom'
anova(object,...)

## S3 method for class 'gom'
logLik(object,...)

## S3 method for class 'gom'
IRT.irfprob(object,...)

## S3 method for class 'gom'
IRT.likelihood(object,...)

## S3 method for class 'gom'
IRT.posterior(object,...)

## S3 method for class 'gom'
IRT.modelfit(object,...)

## S3 method for class 'IRT.modelfit.gom'
summary(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gom.em_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous responses
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_k">K</code></td>
<td>

<p>Number of classes (only applies for <code>model="GOM"</code>)
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_problevels">problevels</code></td>
<td>

<p>Vector containing probability levels for membership functions
(only applies for <code>model="GOM"</code>). If a specific space of probability
levels should be estimated, then a matrix can be supplied (see Example 1,
Model 2a).
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_weights">weights</code></td>
<td>
<p>Optional vector of sampling weights</p>
</td></tr>
<tr><td><code id="gom.em_+3A_model">model</code></td>
<td>

<p>The type of grade of membership model. The default <code>"GOM"</code>
is the nonparametric grade of membership model. A parametric multivariate normal
representation can be requested by <code>"GOMnormal"</code>.
The probabilities and membership
functions specifications described in Details are called via <code>"GOMRasch"</code>.
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_theta0.k">theta0.k</code></td>
<td>

<p>Vector of <code class="reqn">\tilde{\theta}_k</code> grid (applies only for <code>model="GOMRasch"</code>)
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_xsi0.k">xsi0.k</code></td>
<td>

<p>Vector of <code class="reqn">\xi_p</code> grid (applies only for <code>model="GOMRasch"</code>)
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_max.increment">max.increment</code></td>
<td>

<p>Maximum increment
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_numdiff.parm">numdiff.parm</code></td>
<td>

<p>Numerical differentiation parameter
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_maxdevchange">maxdevchange</code></td>
<td>

<p>Convergence criterion for change in relative deviance
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_globconv">globconv</code></td>
<td>

<p>Global convergence criterion for parameter change
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_msteps">msteps</code></td>
<td>

<p>Number of iterations within a M step
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_mstepconv">mstepconv</code></td>
<td>

<p>Convergence criterion within a M step
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_theta_adjust">theta_adjust</code></td>
<td>
<p>Logical indicating whether multivariate normal distribution
should be adaptively chosen during the EM algorithm.</p>
</td></tr>
<tr><td><code id="gom.em_+3A_lambda.inits">lambda.inits</code></td>
<td>
<p>Initial values for item parameters</p>
</td></tr>
<tr><td><code id="gom.em_+3A_lambda.index">lambda.index</code></td>
<td>
<p>Optional integer matrix with integers indicating
equality constraints among <code class="reqn">\lambda</code> item parameters</p>
</td></tr>
<tr><td><code id="gom.em_+3A_pi.k.inits">pi.k.inits</code></td>
<td>
<p>Initial values for distribution parameters</p>
</td></tr>
<tr><td><code id="gom.em_+3A_newton_raphson">newton_raphson</code></td>
<td>
<p>Logical indicating whether Newton-Raphson should be
used for final iterations</p>
</td></tr>
<tr><td><code id="gom.em_+3A_optimizer">optimizer</code></td>
<td>
<p>Type of optimizer. Can be <code>"optim"</code> or <code>"nlminb"</code>.</p>
</td></tr>
<tr><td><code id="gom.em_+3A_progress">progress</code></td>
<td>

<p>Display iteration progress? Default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_object">object</code></td>
<td>

<p>Object of class <code>gom</code>
</p>
</td></tr>
<tr><td><code id="gom.em_+3A_file">file</code></td>
<td>
<p>Optional file name for summary output</p>
</td></tr>
<tr><td><code id="gom.em_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The item response model of the grade of membership model
(Erosheva, Fienberg &amp; Junker, 2002;
Erosheva, Fienberg &amp; Joutard, 2007) with <code class="reqn">K</code> classes
for dichotomous correct responses <code class="reqn">X_{pi}</code>
of person <code class="reqn">p</code> on item <code class="reqn">i</code> is as follows (<code>model="GOM"</code>)
</p>
<p style="text-align: center;"><code class="reqn">
    P(X_{pi}=1 | g_{p1}, \ldots, g_{pK} )=\sum_k \lambda_{ik} g_{pk}
\quad, \quad \sum_{k=1}^K g_{pk}=1
\quad, \quad 0 \leq g_{pk} \leq 1
                </code>
</p>

<p>In most applications (e.g. Erosheva et al., 2007), the grade of
membership function <code class="reqn">\{g_{pk}\}</code> is assumed to follow a Dirichlet
distribution. In our <code>gom.em</code> implementation
the membership function is assumed to be discretely represented
by a grid <code class="reqn">u=(u_1, \ldots, u_L)</code> with entries between 0 and 1
(e.g. <code>seq(0,1,length=5)</code> with <code class="reqn">L=5</code>).
The values <code class="reqn">g_{pk}</code> of the membership function can then
only take values in <code class="reqn">\{ u_1, \ldots, u_L \}</code> with the restriction
<code class="reqn">\sum_k g_{pk} \sum_l \bold{1}(g_{pk}=u_l )=1</code>.
The grid <code class="reqn">u</code> is specified by using the argument <code>problevels</code>.
</p>
<p>The Rasch grade of membership model (<code>model="GOMRasch"</code>) poses constraints
on probabilities <code class="reqn">\lambda_{ik}</code> and membership functions <code class="reqn">g_{pk}</code>.
The membership
function of person <code class="reqn">p</code> is parameterized by a location parameter <code class="reqn">\theta_p</code>
and a variability parameter <code class="reqn">\xi_p</code>. Each class <code class="reqn">k</code> is represented by
a location parameter <code class="reqn">\tilde{\theta}_k</code>. The membership function is defined as
</p>
<p style="text-align: center;"><code class="reqn"> g_{pk} \propto
\exp \left[ - \frac{ (\theta_p - \tilde{\theta}_k)^2 }{2 \xi_p^2 } \right]
</code>
</p>

<p>The person parameter <code class="reqn">\theta_p</code> indicates the usual 'ability', while
<code class="reqn">\xi_p</code> describes the individual tendency to change between classes
<code class="reqn">1,\ldots,K</code> and their corresponding locations
<code class="reqn">\tilde{\theta}_1, \ldots,\tilde{\theta}_K</code>.
The extremal class probabilities <code class="reqn">\lambda_{ik}</code> follow the Rasch model
</p>
<p style="text-align: center;"><code class="reqn"> \lambda_{ik}=invlogit( \tilde{\theta}_k - b_i  )=
\frac{ \exp( \tilde{\theta}_k - b_i ) }{ 1 + \exp( \tilde{\theta}_k - b_i ) }</code>
</p>

<p>Putting these assumptions together leads to the model equation
</p>
<p style="text-align: center;"><code class="reqn">
    P(X_{pi}=1 | g_{p1}, \ldots, g_{pK} )=
    P(X_{pi}=1 | \theta_p, \xi_p  )=
        \sum_k \frac{ \exp( \tilde{\theta}_k - b_i ) }{ 1 + \exp(\tilde{\theta}_k - b_i ) }
        \cdot \exp \left[ - \frac{ (\theta_p - \tilde{\theta}_k)^2 }{2 \xi_p^2 } \right]
                </code>
</p>

<p>In the extreme case of a very small <code class="reqn">\xi_p=\varepsilon &gt; 0</code> and
<code class="reqn">\theta_p=\theta_0</code>, the Rasch model is obtained
</p>
<p style="text-align: center;"><code class="reqn">
    P(X_{pi}=1 | \theta_p, \xi_p  )=
    P(X_{pi}=1 | \theta_0, \varepsilon  )=
        \frac{ \exp( \theta_0 - b_i ) }{ 1 + \exp( \theta_0 - b_i ) }
                </code>
</p>

<p>See Erosheva et al. (2002), Erosheva (2005, 2006) or Galyart (2015)
for a comparison of grade of membership models with latent trait models
and latent class models.
</p>
<p>The grade of membership model is also published under the name
Bernoulli aspect model, see Bingham, Kaban and Fortelius (2009).
</p>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Data frame with item parameters</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with person parameters</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliability (only applies for <code>model="GOMRasch"</code>)</p>
</td></tr>
<tr><td><code>MAP</code></td>
<td>
<p>Maximum aposteriori estimate of the membership function
</p>
</td></tr>
<tr><td><code>EAP</code></td>
<td>
<p>EAP estimate for individual membership scores</p>
</td></tr>
<tr><td><code>classdesc</code></td>
<td>
<p>Descriptives for class membership</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Estimated response probabilities <code class="reqn">\lambda_{ik}</code></p>
</td></tr>
<tr><td><code>se.lambda</code></td>
<td>
<p>Standard error for estimated response probabilities
<code class="reqn">\lambda_{ik}</code></p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Mean of the distribution of <code class="reqn">(\theta_p, \xi_p)</code>
(only applies for <code>model="GOMRasch"</code>)</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>Covariance matrix of <code class="reqn">(\theta_p, \xi_p)</code>
(only applies for <code>model="GOMRasch"</code>)</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>Estimated item difficulties (only applies for <code>model="GOMRasch"</code>)</p>
</td></tr>
<tr><td><code>se.b</code></td>
<td>
<p>Standard error of estimated difficulties
(only applies for <code>model="GOMRasch"</code>)</p>
</td></tr>
<tr><td><code>f.yi.qk</code></td>
<td>
<p>Individual likelihood</p>
</td></tr>
<tr><td><code>f.qk.yi</code></td>
<td>
<p>Individual posterior</p>
</td></tr>
<tr><td><code>probs</code></td>
<td>
<p>Array with response probabilities</p>
</td></tr>
<tr><td><code>n.ik</code></td>
<td>
<p>Expected counts</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>I</code></td>
<td>
<p>Number of items</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>Number of classes</p>
</td></tr>
<tr><td><code>TP</code></td>
<td>
<p>Number of discrete integration points for <code class="reqn">(g_{p1},...,g_{pK})</code></p>
</td></tr>
<tr><td><code>theta.k</code></td>
<td>
<p>Used grid of membership functions</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bingham, E., Kaban, A., &amp; Fortelius, M. (2009).
The aspect Bernoulli model: multiple causes of presences and absences.
<em>Pattern Analysis and Applications, 12</em>(1), 55-78.
</p>
<p>Erosheva, E. A. (2005). Comparing latent structures of the grade of membership,
Rasch, and latent class models. <em>Psychometrika, 70</em>, 619-628.
</p>
<p>Erosheva, E. A. (2006). <em>Latent class representation of the grade of membership
model</em>. Seattle: University of Washington.
</p>
<p>Erosheva, E. A., Fienberg, S. E., &amp; Junker, B. W. (2002).
Alternative statistical models and representations for large sparse
multi-dimensional contingency tables.
<em>Annales-Faculte Des Sciences Toulouse Mathematiques, 11</em>,
485-505.
</p>
<p>Erosheva, E. A., Fienberg, S. E., &amp; Joutard, C. (2007).
Describing disability through individual-level mixture models
for multivariate binary data. <em>Annals of Applied Statistics,
1</em>, 502-537.
</p>
<p>Galyardt, A. (2015).
Interpreting mixed membership models: Implications of Erosheva's representation
theorem. In E. M. Airoldi, D. Blei, E. A. Erosheva, &amp; S. E. Fienberg (Eds.).
<em>Handbook of Mixed Membership Models</em> (pp. 39-65). Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p>For joint maximum likelihood estimation of the grade of membership model
see <code><a href="#topic+gom.jml">gom.jml</a></code>.
</p>
<p>See also the <span class="pkg">mixedMem</span> package for estimating mixed membership
models by a variational EM algorithm.
</p>
<p>The C code of Erosheva et al. (2007) can be downloaded from
<em>http://projecteuclid.org/euclid.aoas/1196438029#supplemental</em>.
</p>
<p>Code from Manrique-Vallier can be downloaded from
<em>http://pages.iu.edu/~dmanriqu/software.html</em>.
</p>
<p>See <em>http://users.ics.aalto.fi/ella/publications/aspect_bernoulli.m</em>
for a Matlab implementation of the algorithm in Bingham, Kaban and
Fortelius (2009).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: PISA data mathematics
#############################################################################

data(data.pisaMath)
dat &lt;- data.pisaMath$data
dat &lt;- dat[, grep("M", colnames(dat)) ]

#***
# Model 1: Discrete GOM with 3 classes and 5 probability levels
problevels &lt;- seq( 0, 1, len=5 )
mod1 &lt;- sirt::gom.em( dat, K=3, problevels, model="GOM")
summary(mod1)

## Not run: 
#-- some plots

#* multivariate scatterplot
car::scatterplotMatrix(mod1$EAP, regLine=FALSE, smooth=FALSE, pch=16, cex=.4)
#* ternary plot
vcd::ternaryplot(mod1$EAP, pch=16, col=1, cex=.3)

#***
# Model 1a: Multivariate normal distribution
problevels &lt;- seq( 0, 1, len=5 )
mod1a &lt;- sirt::gom.em( dat, K=3, theta0.k=seq(-15,15,len=21), model="GOMnormal" )
summary(mod1a)

#***
# Model 2: Discrete GOM with 4 classes and 5 probability levels
problevels &lt;- seq( 0, 1, len=5 )
mod2 &lt;- sirt::gom.em( dat, K=4, problevels,  model="GOM"  )
summary(mod2)

# model comparison
smod1 &lt;- IRT.modelfit(mod1)
smod2 &lt;- IRT.modelfit(mod2)
IRT.compareModels(smod1,smod2)

#***
# Model 2a: Estimate discrete GOM with 4 classes and restricted space of probability levels
#  the 2nd, 4th and 6th class correspond to "intermediate stages"
problevels &lt;- scan()
 1  0  0  0
.5 .5  0  0
 0  1  0  0
 0 .5 .5  0
 0  0  1  0
 0  0 .5 .5
 0  0  0  1

problevels &lt;- matrix( problevels, ncol=4, byrow=TRUE)
mod2a &lt;- sirt::gom.em( dat, K=4, problevels,  model="GOM" )
# probability distribution for latent classes
cbind( mod2a$theta.k, mod2a$pi.k )
  ##        [,1] [,2] [,3] [,4]       [,5]
  ##   [1,]  1.0  0.0  0.0  0.0 0.17214630
  ##   [2,]  0.5  0.5  0.0  0.0 0.04965676
  ##   [3,]  0.0  1.0  0.0  0.0 0.09336660
  ##   [4,]  0.0  0.5  0.5  0.0 0.06555719
  ##   [5,]  0.0  0.0  1.0  0.0 0.27523678
  ##   [6,]  0.0  0.0  0.5  0.5 0.08458620
  ##   [7,]  0.0  0.0  0.0  1.0 0.25945016

## End(Not run)

#***
# Model 3: Rasch GOM
mod3 &lt;- sirt::gom.em( dat, model="GOMRasch", maxiter=20 )
summary(mod3)

#***
# Model 4: 'Ordinary' Rasch model
mod4 &lt;- sirt::rasch.mml2( dat )
summary(mod4)

## Not run: 
#############################################################################
# EXAMPLE 2: Grade of membership model with 2 classes
#############################################################################

#********* DATASET 1 *************
# define an ordinary 2 latent class model
set.seed(8765)
I &lt;- 10
prob.class1 &lt;- stats::runif( I, 0, .35 )
prob.class2 &lt;- stats::runif( I, .70, .95 )
probs &lt;- cbind( prob.class1, prob.class2 )

# define classes
N &lt;- 1000
latent.class &lt;- c( rep( 1, 1/4*N ), rep( 2,3/4*N ) )

# simulate item responses
dat &lt;- matrix( NA, nrow=N, ncol=I )
for (ii in 1:I){
    dat[,ii] &lt;- probs[ ii, latent.class ]
    dat[,ii] &lt;- 1 * ( stats::runif(N) &lt; dat[,ii] )
}
colnames(dat) &lt;- paste0( "I", 1:I)

# Model 1: estimate latent class model
mod1 &lt;- sirt::gom.em(dat, K=2, problevels=c(0,1), model="GOM" )
summary(mod1)
# Model 2: estimate GOM
mod2 &lt;- sirt::gom.em(dat, K=2, problevels=seq(0,1,0.5), model="GOM" )
summary(mod2)
# estimated distribution
cbind( mod2$theta.k, mod2$pi.k )
  ##       [,1] [,2]        [,3]
  ##  [1,]  1.0  0.0 0.243925644
  ##  [2,]  0.5  0.5 0.006534278
  ##  [3,]  0.0  1.0 0.749540078

#********* DATASET 2 *************
# define a 2-class model with graded membership
set.seed(8765)
I &lt;- 10
prob.class1 &lt;- stats::runif( I, 0, .35 )
prob.class2 &lt;- stats::runif( I, .70, .95 )
prob.class3 &lt;- .5*prob.class1+.5*prob.class2  # probabilities for 'fuzzy class'
probs &lt;- cbind( prob.class1, prob.class2, prob.class3)
# define classes
N &lt;- 1000
latent.class &lt;- c( rep(1,round(1/3*N)),rep(2,round(1/2*N)),rep(3,round(1/6*N)))
# simulate item responses
dat &lt;- matrix( NA, nrow=N, ncol=I )
for (ii in 1:I){
    dat[,ii] &lt;- probs[ ii, latent.class ]
    dat[,ii] &lt;- 1 * ( stats::runif(N) &lt; dat[,ii] )
        }
colnames(dat) &lt;- paste0( "I", 1:I)

#** Model 1: estimate latent class model
mod1 &lt;- sirt::gom.em(dat, K=2, problevels=c(0,1), model="GOM" )
summary(mod1)

#** Model 2: estimate GOM
mod2 &lt;- sirt::gom.em(dat, K=2, problevels=seq(0,1,0.5), model="GOM" )
summary(mod2)
# inspect distribution
cbind( mod2$theta.k, mod2$pi.k )
  ##       [,1] [,2]      [,3]
  ##  [1,]  1.0  0.0 0.3335666
  ##  [2,]  0.5  0.5 0.1810114
  ##  [3,]  0.0  1.0 0.4854220

#***
# Model2m: estimate discrete GOM in mirt
# define latent classes
Theta &lt;- scan( nlines=1)
   1 0   .5 .5    0 1
Theta &lt;- matrix( Theta, nrow=3, ncol=2,byrow=TRUE)
# define mirt model
I &lt;- ncol(dat)
#*** create customized item response function for mirt model
name &lt;- 'gom'
par &lt;- c("a1"=-1, "a2"=1 )
est &lt;- c(TRUE, TRUE)
P.gom &lt;- function(par,Theta,ncat){
    # GOM for two extremal classes
    pext1 &lt;- stats::plogis(par[1])
    pext2 &lt;- stats::plogis(par[2])
    P1 &lt;- Theta[,1]*pext1 + Theta[,2]*pext2
    cbind(1-P1, P1)
}
# create item response function
icc_gom &lt;- mirt::createItem(name, par=par, est=est, P=P.gom)
#** define prior for latent class analysis
lca_prior &lt;- function(Theta,Etable){
  # number of latent Theta classes
  TP &lt;- nrow(Theta)
  # prior in initial iteration
  if ( is.null(Etable) ){ prior &lt;- rep( 1/TP, TP ) }
  # process Etable (this is correct for datasets without missing data)
  if ( ! is.null(Etable) ){
    # sum over correct and incorrect expected responses
    prior &lt;- ( rowSums(Etable[, seq(1,2*I,2)]) + rowSums(Etable[,seq(2,2*I,2)]) )/I
                 }
  prior &lt;- prior / sum(prior)
  return(prior)
}
#*** estimate discrete GOM in mirt package
mod2m &lt;- mirt::mirt(dat, 1, rep( "icc_gom",I), customItems=list("icc_gom"=icc_gom),
           technical=list( customTheta=Theta, customPriorFun=lca_prior)  )
# correct number of estimated parameters
mod2m@nest &lt;- as.integer(sum(mod.pars$est) + nrow(Theta)-1 )
# extract log-likelihood and compute AIC and BIC
mod2m@logLik
( AIC &lt;- -2*mod2m@logLik+2*mod2m@nest )
( BIC &lt;- -2*mod2m@logLik+log(mod2m@Data$N)*mod2m@nest )
# extract coefficients
( cmod2m &lt;- sirt::mirt.wrapper.coef(mod2m) )
# compare estimated distributions
round( cbind( "sirt"=mod2$pi.k, "mirt"=mod2m@Prior[[1]] ), 5 )
  ##           sirt    mirt
  ##   [1,] 0.33357 0.33627
  ##   [2,] 0.18101 0.17789
  ##   [3,] 0.48542 0.48584
# compare estimated item parameters
dfr &lt;- data.frame( "sirt"=mod2$item[,4:5] )
dfr$mirt &lt;- apply(cmod2m$coef[, c("a1", "a2") ], 2, stats::plogis )
round(dfr,4)
  ##      sirt.lam.Cl1 sirt.lam.Cl2 mirt.a1 mirt.a2
  ##   1        0.1157       0.8935  0.1177  0.8934
  ##   2        0.0790       0.8360  0.0804  0.8360
  ##   3        0.0743       0.8165  0.0760  0.8164
  ##   4        0.0398       0.8093  0.0414  0.8094
  ##   5        0.1273       0.7244  0.1289  0.7243
  ##   [...]

#############################################################################
# EXAMPLE 3: Lung cancer dataset; using sampling weights
#############################################################################

data(data.si08, package="sirt")
dat &lt;- data.si08

#- Latent class model with 3 classes
problevels &lt;- c(0,1)
mod1 &lt;- sirt::gom.em( dat[,1:5], weights=dat$wgt, K=3, problevels=problevels )
summary(mod1)

#- Grade of membership model with discrete distribution
problevels &lt;- seq(0,1,length=5)
mod2 &lt;- sirt::gom.em( dat[,1:5], weights=dat$wgt, K=3, problevels=problevels )
summary(mod2)

#- Grade of membership model with multivariate normal distribution
mod3 &lt;- sirt::gom.em( dat[,1:5], weights=dat$wgt, K=3, theta0.k=10*seq(-1,1,len=11),
            model="GOMnormal", optimizer="nlminb" )
summary(mod3)

## End(Not run)
</code></pre>

<hr>
<h2 id='gom.jml'>
Grade of Membership Model (Joint Maximum Likelihood Estimation)
</h2><span id='topic+gom.jml'></span>

<h3>Description</h3>

<p>This function estimates the grade of membership model
employing a joint maximum likelihood estimation
method (Erosheva, 2002; p. 23ff.).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gom.jml(dat, K=2, seed=NULL, globconv=0.001, maxdevchange=0.001,
        maxiter=600, min.lambda=0.001, min.g=0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gom.jml_+3A_dat">dat</code></td>
<td>

<p>Data frame of dichotomous item responses
</p>
</td></tr>
<tr><td><code id="gom.jml_+3A_k">K</code></td>
<td>

<p>Number of classes
</p>
</td></tr>
<tr><td><code id="gom.jml_+3A_seed">seed</code></td>
<td>

<p>Seed value of random number generator. Deterministic starting values
are used for the default value <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="gom.jml_+3A_globconv">globconv</code></td>
<td>

<p>Global parameter convergence criterion
</p>
</td></tr>
<tr><td><code id="gom.jml_+3A_maxdevchange">maxdevchange</code></td>
<td>

<p>Maximum change in relative deviance
</p>
</td></tr>
<tr><td><code id="gom.jml_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="gom.jml_+3A_min.lambda">min.lambda</code></td>
<td>

<p>Minimum <code class="reqn">\lambda_{ik}</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="gom.jml_+3A_min.g">min.g</code></td>
<td>

<p>Minimum <code class="reqn">g_{pk}</code> parameter to be estimated
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The item response model of the grade of membership model with <code class="reqn">K</code> classes
for dichotomous correct responses <code class="reqn">X_{pi}</code>
of person <code class="reqn">p</code> on item <code class="reqn">i</code> is
</p>
<p style="text-align: center;"><code class="reqn">
    P(X_{pi}=1 | g_{p1}, \ldots, g_{pK} )=\sum_k \lambda_{ik} g_{pk}
\quad, \quad \sum_k g_{pk}=1
                </code>
</p>



<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>lambda</code></td>
<td>
<p>Data frame of item parameters <code class="reqn">\lambda_{ik}</code></p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>Data frame of individual membership scores <code class="reqn">g_{pk}</code></p>
</td></tr>
<tr><td><code>g.mean</code></td>
<td>
<p>Mean membership scores</p>
</td></tr>
<tr><td><code>gcut</code></td>
<td>
<p>Discretized membership scores</p>
</td></tr>
<tr><td><code>gcut.distr</code></td>
<td>
<p>Distribution of discretized membership scores</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>Number of classes</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>Number of students</p>
</td></tr>
<tr><td><code>score</code></td>
<td>
<p>Person score</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>datproc</code></td>
<td>
<p>List with processed data (recoded data, starting values, ...)</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Erosheva, E. A. (2002). <em>Grade of membership
and latent structure models with application to
disability survey data</em>.
PhD thesis, Carnegie Mellon University, Department
of Statistics.
</p>


<h3>See Also</h3>

<p>S3 method <code><a href="#topic+summary.gom">summary.gom</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: TIMSS data
#############################################################################

data( data.timss)
dat &lt;- data.timss$data[, grep("M", colnames(data.timss$data) ) ]

# 2 Classes (deterministic starting values)
m2 &lt;- sirt::gom.jml(dat,K=2, maxiter=10 )
summary(m2)

## Not run: 
# 3 Classes with fixed seed and maximum number of iterations
m3 &lt;- sirt::gom.jml(dat,K=3, maxiter=50,seed=89)
summary(m3)

## End(Not run)
</code></pre>

<hr>
<h2 id='greenyang.reliability'>
Reliability for Dichotomous Item Response Data
Using the Method of Green and Yang (2009)
</h2><span id='topic+greenyang.reliability'></span>

<h3>Description</h3>

<p>This function estimates the model-based reliability
of dichotomous data using the Green &amp; Yang (2009) method.
The underlying factor model is <code class="reqn">D</code>-dimensional where
the dimension <code class="reqn">D</code> is specified by the argument
<code>nfactors</code>. The factor solution is subject to the
application of the Schmid-Leiman transformation (see Reise, 2012;
Reise, Bonifay, &amp; Haviland, 2013; Reise, Moore, &amp; Haviland, 2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>greenyang.reliability(object.tetra, nfactors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="greenyang.reliability_+3A_object.tetra">object.tetra</code></td>
<td>

<p>Object as the output of the function <code>tetrachoric</code>, the
<code>fa.parallel.poly</code> from the <span class="pkg">psych</span> package or the
<code>tetrachoric2</code> function (from <span class="pkg">sirt</span>).
This object can also be created
as a list by the user where the tetrachoric correlation
must must be in the list entry <code>rho</code> and the thresholds must
be in the list entry <code>thresh</code>.
</p>
</td></tr>
<tr><td><code id="greenyang.reliability_+3A_nfactors">nfactors</code></td>
<td>

<p>Number of factors (dimensions)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with columns:
</p>
<table>
<tr><td><code>coefficient</code></td>
<td>
<p>Name of the reliability measure. <code>omega_1</code> (Omega)
is the reliability estimate for the total score for dichotomous data
based on a one-factor model, <code>omega_t</code> (Omega Total) is the
estimate for a <code class="reqn">D</code>-dimensional model. For the nested factor model,
<code>omega_h</code> (Omega Asymptotic) is the reliability of the general factor model,
<code>omega_ha</code> (Omega Hierarchical Asymptotic) eliminates item-specific
variance. The explained common variance (<code>ECV</code>) explained by the
common factor is based on the <code class="reqn">D</code>-dimensional but does not take
item thresholds into account. The amount of explained
variance <code>ExplVar</code> is defined as the quotient of the first
eigenvalue of the tetrachoric correlation matrix to the
sum of all eigenvalues. The statistic <code>EigenvalRatio</code>
is the ratio of the first and second eigenvalue.
</p>
</td></tr>
<tr><td><code>dimensions</code></td>
<td>
<p>Number of dimensions</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>Reliability estimate</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function needs the <span class="pkg">psych</span> package.
</p>


<h3>References</h3>

<p>Green, S. B., &amp; Yang, Y. (2009). Reliability of summed item
scores using structural equation modeling: An alternative to
coefficient alpha. <em>Psychometrika, 74</em>, 155-167.
</p>
<p>Reise, S. P. (2012). The rediscovery of bifactor measurement models.
<em>Multivariate Behavioral Research, 47</em>, 667-696.
</p>
<p>Reise, S. P., Bonifay, W. E., &amp; Haviland, M. G. (2013).
Scoring and modeling psychological measures in the presence of
multidimensionality. <em>Journal of Personality Assessment,
95</em>, 129-140.
</p>
<p>Reise, S. P., Moore, T. M., &amp; Haviland, M. G.  (2010).
Bifactor models and rotations: Exploring the extent to which
multidimensional data yield univocal scale scores,
<em>Journal of Personality Assessment, 92</em>, 544-559.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+f1d.irt">f1d.irt</a></code> for estimating the functional unidimensional
item response model.
</p>
<p>This function uses <code><a href="#topic+reliability.nonlinearSEM">reliability.nonlinearSEM</a></code>.
</p>
<p>See also the <code>MBESS::ci.reliability</code> function for estimating
reliability for polytomous item responses.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Reliability estimation of Reading dataset data.read
#############################################################################
miceadds::library_install("psych")
set.seed(789)
data( data.read )
dat &lt;- data.read

# calculate matrix of tetrachoric correlations
dat.tetra &lt;- psych::tetrachoric(dat)      # using tetrachoric from psych package
dat.tetra2 &lt;- sirt::tetrachoric2(dat)       # using tetrachoric2 from sirt package

# perform parallel factor analysis
fap &lt;- psych::fa.parallel.poly(dat, n.iter=1 )
  ##   Parallel analysis suggests that the number of factors=3
  ##   and the number of components=2

# parallel factor analysis based on tetrachoric correlation matrix
##       (tetrachoric2)
fap2 &lt;- psych::fa.parallel(dat.tetra2$rho, n.obs=nrow(dat),  n.iter=1 )
  ## Parallel analysis suggests that the number of factors=6
  ## and the number of components=2
  ## Note that in this analysis, uncertainty with respect to thresholds is ignored.

# calculate reliability using a model with 4 factors
greenyang.reliability( object.tetra=dat.tetra, nfactors=4 )
  ##                                            coefficient dimensions estimate
  ## Omega Total (1D)                               omega_1          1    0.771
  ## Omega Total (4D)                               omega_t          4    0.844
  ## Omega Hierarchical (4D)                        omega_h          4    0.360
  ## Omega Hierarchical Asymptotic (4D)            omega_ha          4    0.427
  ## Explained Common Variance (4D)                     ECV          4    0.489
  ## Explained Variance (First Eigenvalue)          ExplVar         NA   35.145
  ## Eigenvalue Ratio (1st to 2nd Eigenvalue) EigenvalRatio         NA    2.121

# calculation of Green-Yang-Reliability based on tetrachoric correlations
#   obtained by tetrachoric2
greenyang.reliability( object.tetra=dat.tetra2, nfactors=4 )

# The same result will be obtained by using fap as the input
greenyang.reliability( object.tetra=fap, nfactors=4 ) 
## End(Not run)
</code></pre>

<hr>
<h2 id='invariance.alignment'>
Alignment Procedure for Linking under Approximate Invariance
</h2><span id='topic+invariance.alignment'></span><span id='topic+summary.invariance.alignment'></span><span id='topic+invariance_alignment_constraints'></span><span id='topic+summary.invariance_alignment_constraints'></span><span id='topic+invariance_alignment_simulate'></span><span id='topic+invariance_alignment_cfa_config'></span>

<h3>Description</h3>

<p>The function <code>invariance.alignment</code> performs alignment under approximate
invariance for <code class="reqn">G</code> groups and <code class="reqn">I</code> items
(Asparouhov &amp; Muthen, 2014; Byrne &amp; van de Vijver, 2017; DeMars, 2020; Finch, 2016;
Fischer &amp; Karl, 2019; Flake &amp; McCoach, 2018; Kim et al., 2017; Marsh et al., 2018;
Muthen &amp; Asparouhov, 2014, 2018; Pokropek, Davidov &amp; Schmidt, 2019).
It is assumed that item loadings and intercepts are
previously estimated as a unidimensional factor model under the assumption of a factor
with zero mean and a variance of one.
</p>
<p>The function <code>invariance_alignment_constraints</code> postprocesses the output of the
<code>invariance.alignment</code> function and estimates item parameters under equality
constraints for prespecified absolute values of parameter tolerance.
</p>
<p>The function <code>invariance_alignment_simulate</code> simulates a one-factor model
for multiple groups for given matrices of <code class="reqn">\nu</code> and <code class="reqn">\lambda</code> parameters of
item intercepts and item slopes (see Example 6).
</p>
<p>The function <code>invariance_alignment_cfa_config</code> estimates one-factor
models separately for each group as a preliminary step for invariance
alignment (see Example 6). Sampling weights are accommodated by the
argument <code>weights</code>.  The computed variance matrix <code>vcov</code> by this function
can be used to obtain standard errors in the <code>invariance.alignment</code> function
if it is supplied as the argument <code>vcov</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>invariance.alignment(lambda, nu, wgt=NULL, align.scale=c(1, 1),
    align.pow=c(.5, .5), eps=1e-3, psi0.init=NULL, alpha0.init=NULL, center=FALSE,
    optimizer="optim", fixed=NULL, meth=1, vcov=NULL, eps_grid=seq(0,-10, by=-.5),
    num_deriv=FALSE, ...)

## S3 method for class 'invariance.alignment'
summary(object, digits=3, file=NULL, ...)

invariance_alignment_constraints(model, lambda_parm_tol, nu_parm_tol )

## S3 method for class 'invariance_alignment_constraints'
summary(object, digits=3, file=NULL, ...)

invariance_alignment_simulate(nu, lambda, err_var, mu, sigma, N, output="data",
     groupwise=FALSE, exact=FALSE)

invariance_alignment_cfa_config(dat, group, weights=NULL, model="2PM", verbose=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="invariance.alignment_+3A_lambda">lambda</code></td>
<td>

<p>A <code class="reqn">G \times I</code> matrix with item loadings
</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_nu">nu</code></td>
<td>

<p>A <code class="reqn">G \times I</code> matrix with item intercepts
</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_wgt">wgt</code></td>
<td>

<p>A <code class="reqn">G \times I</code> matrix for weighing groups
for each item
</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_align.scale">align.scale</code></td>
<td>

<p>A vector of length two containing scale parameter
<code class="reqn">a_\lambda</code> and <code class="reqn">a_\nu</code> (see Details)
</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_align.pow">align.pow</code></td>
<td>

<p>A vector of length two containing power
<code class="reqn">p_\lambda</code> and <code class="reqn">p_\nu</code> (see Details)
</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_eps">eps</code></td>
<td>

<p>A parameter in the optimization function
</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_psi0.init">psi0.init</code></td>
<td>

<p>An optional vector of initial <code class="reqn">\psi_0</code> parameters
</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_alpha0.init">alpha0.init</code></td>
<td>

<p>An optional vector of initial <code class="reqn">\alpha_0</code> parameters
</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_center">center</code></td>
<td>

<p>Logical indicating whether estimated means and standard deviations should
be centered.
</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_optimizer">optimizer</code></td>
<td>
<p>Name of the optimizer chosen for alignment. Options are
<code>"optim"</code> (using <code><a href="stats.html#topic+optim">stats::optim</a></code>)
or <code>"nlminb"</code> (using <code><a href="stats.html#topic+nlminb">stats::nlminb</a></code>).</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_fixed">fixed</code></td>
<td>
<p>Logical indicating whether SD of first group should
be fixed to one. If <code>fixed=FALSE</code>, the product of all SDs is set to one.
If <code>NULL</code>, then <code>fixed</code> is automatically chosen by default. For many groups,
<code>fixed=FALSE</code> is chosen.</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_meth">meth</code></td>
<td>
<p>Type of method used for optimization function. <code>meth=1</code> is the default
and the optimization function used in Mplus. <code>meth=2</code> uses logarithmized
item loadings in alignment. The choice <code>meth=4</code> uses the constraint
<code class="reqn">\prod_g \psi_g=1</code> and adds the penalty <code class="reqn">\lambda \sum_g \alpha_g^2</code> for
a fixed value <code class="reqn">\lambda</code> that depends on the weights <code>wgt</code>
(similar to Mplus' free method).
The choice <code>meth=3</code> only uses the constraint
<code class="reqn">\prod_g \psi_g=1</code> (similar to Mplus' FIXED method).</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_vcov">vcov</code></td>
<td>
<p>Variance matrix produced by <code>invariance_alignment_cfa_config</code>
for standard error computation. If a matrix is provided, standard errors
are computed.</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_eps_grid">eps_grid</code></td>
<td>
<p>Grid of logarithmized epsilon values in optimization</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_num_deriv">num_deriv</code></td>
<td>
<p>Logical indicating whether numerical derivatives should be used</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_object">object</code></td>
<td>

<p>Object of class <code>invariance.alignment</code>
</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_digits">digits</code></td>
<td>
<p>Number of digits used for rounding</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_file">file</code></td>
<td>
<p>Optional file name in which summary should be sunk</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_...">...</code></td>
<td>

<p>Further optional arguments to be passed</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_model">model</code></td>
<td>
<p>Model of class <code>invariance.alignment</code>.
For <code>invariance_alignment_cfa_config</code>: Model type: <code>"2PM"</code> for two-parameter
model with unequal loadings
and <code>"1PM"</code> with equal loadings and equal residual variances</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_lambda_parm_tol">lambda_parm_tol</code></td>
<td>
<p>Parameter tolerance for <code class="reqn">\lambda</code> parameters</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_nu_parm_tol">nu_parm_tol</code></td>
<td>
<p>Parameter tolerance for <code class="reqn">\nu</code> parameters</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_err_var">err_var</code></td>
<td>
<p>Error variance</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_mu">mu</code></td>
<td>
<p>Vector of means</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_sigma">sigma</code></td>
<td>
<p>Vector of standard deviations</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_n">N</code></td>
<td>
<p>Vector of sample sizes per group</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_output">output</code></td>
<td>
<p>Specifies output type: <code>"data"</code> for dataset and <code>"suffstat"</code>
for sufficient statistics (i.e., means and covariance matrices)</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_groupwise">groupwise</code></td>
<td>
<p>Logical indicating whether group-wise output is requested</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_exact">exact</code></td>
<td>
<p>Logical indicating whether distributions should be exactly preserved in
simulated data</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_dat">dat</code></td>
<td>
<p>Dataset with items or a list containing sufficient statistics</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_group">group</code></td>
<td>
<p>Vector containing group indicators</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_weights">weights</code></td>
<td>
<p>Optional vector of sampling weights</p>
</td></tr>
<tr><td><code id="invariance.alignment_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating whether progress should be printed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code class="reqn">G</code> groups and <code class="reqn">I</code> items, item loadings <code class="reqn">\lambda_{ig0}</code>
and intercepts <code class="reqn">\nu_{ig0}</code> are available and have been estimated
in a 1-dimensional factor analysis assuming a standardized factor.
</p>
<p>The alignment procedure searches means <code class="reqn">\alpha_{g0}</code>
and standard deviations <code class="reqn">\psi_{g0}</code> using an alignment
optimization function <code class="reqn">F</code>. This function is defined as
</p>
<p style="text-align: center;"><code class="reqn">F=\sum_i \sum_{ g_1 &lt; g_2} w_{i,g1} w_{i,g2}
    f_\lambda( \lambda_{i g_1,1} - \lambda_{i g_2,1} )
    + \sum_i \sum_{ g_1 &lt; g_2} w_{i,g1} w_{i,g2}
f_\nu( \nu_{i g_1,1} - \nu_{i g_2,1} ) </code>
</p>

<p>where the aligned item parameters <code class="reqn">\lambda_{i g,1}</code>
and <code class="reqn">\nu_{i g,1}</code> are defined such that
</p>
<p style="text-align: center;"><code class="reqn"> \lambda_{i g,1}=\lambda_{i g 0} / \psi_{g0}
    \qquad \mbox{and} \qquad
    \nu_{i g,1}=\nu_{i g 0} -  \alpha_{g0} \lambda_{ig0} / \psi_{g0}
            </code>
</p>

<p>and the optimization functions are defined as
</p>
<p style="text-align: center;"><code class="reqn"> f_\lambda (x)=| x/ a_\lambda | ^{p_\lambda}
\approx [ ( x/ a_\lambda )^2 + \varepsilon ]^{p_\lambda / 2}
    \qquad \mbox{and} \qquad
    f_\nu (x)=|  x/ a_\nu ]^{p_\nu}
    \approx [ ( x/ a_\nu )^2 + \varepsilon ]^{p_\nu / 2}
            </code>
</p>

<p>using a small <code class="reqn"> \varepsilon &gt; 0</code> (e.g. .001) to obtain
a differentiable optimization function. For <code class="reqn">p_\nu=0</code> or <code class="reqn">p_\lambda=0</code>, the
optimization function essentially counts the number of different parameter
and mimicks a <code class="reqn">L_0</code> penalty which is zero iff the argument is zero
and one otherwise. It is approximated by
</p>
<p style="text-align: center;"><code class="reqn">f(x)=x^2 (x^2 + \varepsilon )^{-1} </code>
</p>

<p>(O'Neill &amp; Burke, 2023).
</p>
<p>For identification reasons, the product <code class="reqn">\Pi_g \psi_{g0}</code> (<code>meth</code>=0,0.5)
of all group standard deviations or <code class="reqn">\psi_1</code> (<code>meth</code>=1,2)
is set to one. The mean
<code class="reqn">\alpha_{g0}</code> of the first group is set to zero (<code>meth</code>=0.5,1,2) or
a penalty function is added to the linking function (<code>meth</code>=0).
</p>
<p>Note that Asparouhov and Muthen (2014) use <code class="reqn">a_\lambda=a_\nu=1</code>
(which can be modified in <code>align.scale</code>)
and <code class="reqn">p_\lambda=p_\nu=0.5</code> (which can be modified in <code>align.pow</code>).
In case of <code class="reqn">p_\lambda=2</code>, the penalty is approximately
<code class="reqn">f_\lambda(x)=x^2 </code>, in case of <code class="reqn">p_\lambda=0.5</code>
it is approximately <code class="reqn">f_\lambda(x)=\sqrt{|x|} </code>. Note that <span class="pkg">sirt</span> used a
different parametrization in versions up to 3.5. The <code class="reqn">p</code> parameters have to be halved
for consistency with previous versions (e.g., the Asparouhov &amp; Muthen parametrization
corresponds to <code class="reqn">p=.25</code>; see also Fischer &amp; Karl, 2019, for an application of
the previous parametrization).
</p>
<p>Effect sizes of approximate invariance based on <code class="reqn">R^2</code> have
been proposed by Asparouhov and Muthen (2014). These are
calculated separately for item loading and intercepts, resulting
in <code class="reqn">R^2_\lambda</code> and <code class="reqn">R^2_\nu</code> measures which are
included in the output <code>es.invariance</code>. In addition,
the average correlation of aligned item parameters among groups (<code>rbar</code>)
is reported.
</p>
<p><em>Metric invariance</em> means that all aligned item loadings <code class="reqn">\lambda_{ig,1}</code>
are equal across groups and therefore <code class="reqn">R^2_\lambda=1</code>.
<em>Scalar invariance</em> means that all aligned item loadings
<code class="reqn">\lambda_{ig,1}</code> and aligned item intercepts <code class="reqn">\nu_{ig,1}</code> are
equal across groups and therefore <code class="reqn">R^2_\lambda=1</code> and <code class="reqn">R^2_\nu=1</code>
(see Vandenberg &amp; Lance, 2000).
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>pars</code></td>
<td>
<p>Aligned distribution parameters</p>
</td></tr>
<tr><td><code>itempars.aligned</code></td>
<td>
<p>Aligned item parameters for all groups</p>
</td></tr>
<tr><td><code>es.invariance</code></td>
<td>
<p>Effect sizes of approximate invariance</p>
</td></tr>
<tr><td><code>lambda.aligned</code></td>
<td>
<p>Aligned <code class="reqn"> \lambda_{i g,1}</code> parameters</p>
</td></tr>
<tr><td><code>lambda.resid</code></td>
<td>
<p>Residuals of <code class="reqn"> \lambda_{i g,1}</code> parameters</p>
</td></tr>
<tr><td><code>nu.aligned</code></td>
<td>
<p>Aligned <code class="reqn"> \nu_{i g,1}</code> parameters</p>
</td></tr>
<tr><td><code>nu.resid</code></td>
<td>
<p>Residuals of <code class="reqn"> \nu_{i g,1}</code> parameters</p>
</td></tr>
<tr><td><code>Niter</code></td>
<td>
<p>Number of iterations for <code class="reqn">f_\lambda</code> and
<code class="reqn">f_\nu</code> optimization functions</p>
</td></tr>
<tr><td><code>fopt</code></td>
<td>
<p>Minimum optimization value</p>
</td></tr>
<tr><td><code>align.scale</code></td>
<td>
<p>Used alignment scale parameters</p>
</td></tr>
<tr><td><code>align.pow</code></td>
<td>
<p>Used alignment power parameters</p>
</td></tr>
<tr><td><code>vcov</code></td>
<td>
<p>Estimated variance matrix of aligned means and
standard deviations</p>
</td></tr>
</table>


<h3>References</h3>

<p>Asparouhov, T., &amp; Muthen, B. (2014). Multiple-group factor analysis alignment.
<em>Structural Equation Modeling, 21</em>(4), 1-14.
<a href="https://doi.org/10.1080/10705511.2014.919210">doi:10.1080/10705511.2014.919210</a>
</p>
<p>Byrne, B. M., &amp; van de Vijver, F. J. R. (2017). The maximum likelihood alignment
approach to testing for approximate measurement invariance:
A paradigmatic cross-cultural application. <em>Psicothema, 29</em>(4), 539-551.
<a href="https://doi.org/10.7334/psicothema2017.178">doi:10.7334/psicothema2017.178</a>
</p>
<p>DeMars, C. E. (2020). Alignment as an alternative to anchor purification in DIF analyses.
<em>Structural Equation Modeling, 27</em>(1), 56-72.
<a href="https://doi.org/10.1080/10705511.2019.1617151">doi:10.1080/10705511.2019.1617151</a>
</p>
<p>Finch, W. H. (2016). Detection of differential item functioning for more
than two groups: A Monte Carlo comparison of methods.
<em>Applied Measurement in Education, 29</em>,(1), 30-45,
<a href="https://doi.org/10.1080/08957347.2015.1102916">doi:10.1080/08957347.2015.1102916</a>
</p>
<p>Fischer, R., &amp; Karl, J. A. (2019). A primer to (cross-cultural) multi-group invariance
testing possibilities in R.
<em>Frontiers in Psychology | Cultural Psychology, 10</em>:1507.
<a href="https://doi.org/10.3389/fpsyg.2019.01507">doi:10.3389/fpsyg.2019.01507</a>
</p>
<p>Flake, J. K., &amp; McCoach, D. B. (2018). An investigation of the alignment method with
polytomous indicators under conditions of partial measurement invariance.
<em>Structural Equation Modeling, 25</em>(1), 56-70.
<a href="https://doi.org/10.1080/10705511.2017.1374187">doi:10.1080/10705511.2017.1374187</a>
</p>
<p>Kim, E. S., Cao, C., Wang, Y., &amp; Nguyen, D. T. (2017). Measurement invariance testing
with many groups: A comparison of five approaches.
<em>Structural Equation Modeling, 24</em>(4), 524-544.
<a href="https://doi.org/10.1080/10705511.2017.1304822">doi:10.1080/10705511.2017.1304822</a>
</p>
<p>Marsh, H. W., Guo, J., Parker, P. D., Nagengast, B., Asparouhov, T., Muthen, B.,
&amp; Dicke, T. (2018). What to do when scalar invariance fails: The extended alignment
method for multi-group factor analysis comparison of latent means across many groups.
<em>Psychological Methods, 23</em>(3), 524-545.
doi: 10.1037/met0000113
</p>
<p>Muthen, B., &amp; Asparouhov, T. (2014). IRT studies of many groups: The alignment method.
<em>Frontiers in Psychology | Quantitative Psychology and Measurement, 5</em>:978.
<a href="https://doi.org/10.3389/fpsyg.2014.00978">doi:10.3389/fpsyg.2014.00978</a>
</p>
<p>Muthen, B., &amp; Asparouhov, T. (2018). Recent methods for the study of measurement
invariance with many groups: Alignment and random effects.
<em>Sociological Methods &amp; Research, 47</em>(4), 637-664.
<a href="https://doi.org/10.1177/0049124117701488">doi:10.1177/0049124117701488</a>
</p>
<p>O'Neill, M., &amp; Burke, K. (2023). Variable selection using a smooth information criterion
for distributional regression models. <em>Statistics and Computing, 33</em>(3), 71.
<a href="https://doi.org/10.1007/s11222-023-10204-8">doi:10.1007/s11222-023-10204-8</a>
</p>
<p>Pokropek, A., Davidov, E., &amp; Schmidt, P. (2019). A Monte Carlo simulation study to
assess the appropriateness of traditional and newer approaches to test for
measurement invariance. <em>Structural Equation Modeling, 26</em>(5), 724-744.
<a href="https://doi.org/10.1080/10705511.2018.1561293">doi:10.1080/10705511.2018.1561293</a>
</p>
<p>Vandenberg, R. J., &amp; Lance, C. E. (2000). A review and synthesis of the
measurement invariance literature: Suggestions, practices, and
recommendations for organizational research. <em>Organizational Research
Methods, 3</em>, 4-70.
<a href="https://doi.org/10.1177/109442810031002">doi:10.1177/109442810031002</a>s
</p>


<h3>See Also</h3>

<p>For IRT linking see also <code><a href="#topic+linking.haberman">linking.haberman</a></code> or
<code><a href="TAM.html#topic+tam.linking">TAM::tam.linking</a></code>.
</p>
<p>For modeling random item effects for loadings and intercepts
see <code><a href="#topic+mcmc.2pno.ml">mcmc.2pno.ml</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Item parameters cultural activities
#############################################################################

data(data.activity.itempars, package="sirt")
lambda &lt;- data.activity.itempars$lambda
nu &lt;- data.activity.itempars$nu
Ng &lt;-  data.activity.itempars$N
wgt &lt;- matrix( sqrt(Ng), length(Ng), ncol(nu) )

#***
# Model 1: Alignment using a quadratic loss function
mod1 &lt;- sirt::invariance.alignment( lambda, nu, wgt, align.pow=c(2,2) )
summary(mod1)

#****
# Model 2: Different powers for alignment
mod2 &lt;- sirt::invariance.alignment( lambda, nu, wgt,  align.pow=c(.5,1),
              align.scale=c(.95,.95))
summary(mod2)

# compare means from Models 1 and 2
plot( mod1$pars$alpha0, mod2$pars$alpha0, pch=16,
    xlab="M (Model 1)", ylab="M (Model 2)", xlim=c(-.3,.3), ylim=c(-.3,.3) )
lines( c(-1,1), c(-1,1), col="gray")
round( cbind( mod1$pars$alpha0, mod2$pars$alpha0 ), 3 )
round( mod1$nu.resid, 3)
round( mod2$nu.resid,3 )

# L0 penalty
mod2b &lt;- sirt::invariance.alignment( lambda, nu, wgt,  align.pow=c(0,0),
              align.scale=c(.3,.3))
summary(mod2b)

#****
# Model 3: Low powers for alignment of scale and power
# Note that setting increment.factor larger than 1 seems necessary
mod3 &lt;- sirt::invariance.alignment( lambda, nu, wgt, align.pow=c(.5,.75),
            align.scale=c(.55,.55), psi0.init=mod1$psi0, alpha0.init=mod1$alpha0 )
summary(mod3)

# compare mean and SD estimates of Models 1 and 3
plot( mod1$pars$alpha0, mod3$pars$alpha0, pch=16)
plot( mod1$pars$psi0, mod3$pars$psi0, pch=16)

# compare residuals for Models 1 and 3
# plot lambda
plot( abs(as.vector(mod1$lambda.resid)), abs(as.vector(mod3$lambda.resid)),
      pch=16, xlab="Residuals lambda (Model 1)",
      ylab="Residuals lambda (Model 3)", xlim=c(0,.1), ylim=c(0,.1))
lines( c(-3,3),c(-3,3), col="gray")
# plot nu
plot( abs(as.vector(mod1$nu.resid)), abs(as.vector(mod3$nu.resid)),
      pch=16, xlab="Residuals nu (Model 1)", ylab="Residuals nu (Model 3)",
      xlim=c(0,.4),ylim=c(0,.4))
lines( c(-3,3),c(-3,3), col="gray")

## Not run: 
#############################################################################
# EXAMPLE 2: Comparison 4 groups | data.inv4gr
#############################################################################

data(data.inv4gr)
dat &lt;- data.inv4gr
miceadds::library_install("semTools")

model1 &lt;- "
    F=~ I01 + I02 + I03 + I04 + I05 + I06 + I07 + I08 + I09 + I10 + I11
    F ~~ 1*F
    "

res &lt;- semTools::measurementInvariance(model1, std.lv=TRUE, data=dat, group="group")
  ##   Measurement invariance tests:
  ##
  ##   Model 1: configural invariance:
  ##       chisq        df    pvalue       cfi     rmsea       bic
  ##     162.084   176.000     0.766     1.000     0.000 95428.025
  ##
  ##   Model 2: weak invariance (equal loadings):
  ##       chisq        df    pvalue       cfi     rmsea       bic
  ##     519.598   209.000     0.000     0.973     0.039 95511.835
  ##
  ##   [Model 1 versus model 2]
  ##     delta.chisq      delta.df delta.p.value     delta.cfi
  ##         357.514        33.000         0.000         0.027
  ##
  ##   Model 3: strong invariance (equal loadings + intercepts):
  ##       chisq        df    pvalue       cfi     rmsea       bic
  ##    2197.260   239.000     0.000     0.828     0.091 96940.676
  ##
  ##   [Model 1 versus model 3]
  ##     delta.chisq      delta.df delta.p.value     delta.cfi
  ##        2035.176        63.000         0.000         0.172
  ##
  ##   [Model 2 versus model 3]
  ##     delta.chisq      delta.df delta.p.value     delta.cfi
  ##        1677.662        30.000         0.000         0.144
  ##

# extract item parameters separate group analyses
ipars &lt;- lavaan::parameterEstimates(res$fit.configural)
# extract lambda's: groups are in rows, items in columns
lambda &lt;- matrix( ipars[ ipars$op=="=~", "est"], nrow=4,  byrow=TRUE)
colnames(lambda) &lt;- colnames(dat)[-1]
# extract nu's
nu &lt;- matrix( ipars[ ipars$op=="~1"  &amp; ipars$se !=0, "est" ], nrow=4,  byrow=TRUE)
colnames(nu) &lt;- colnames(dat)[-1]

# Model 1: least squares optimization
mod1 &lt;- sirt::invariance.alignment( lambda=lambda, nu=nu )
summary(mod1)
  ##   Effect Sizes of Approximate Invariance
  ##          loadings intercepts
  ##   R2       0.9826     0.9972
  ##   sqrtU2   0.1319     0.0526
  ##   rbar     0.6237     0.7821
  ##   -----------------------------------------------------------------
  ##   Group Means and Standard Deviations
  ##     alpha0  psi0
  ##   1  0.000 0.965
  ##   2 -0.105 1.098
  ##   3 -0.081 1.011
  ##   4  0.171 0.935

# Model 2: sparse target function
mod2 &lt;- sirt::invariance.alignment( lambda=lambda, nu=nu, align.pow=c(.5,.5) )
summary(mod2)
  ##   Effect Sizes of Approximate Invariance
  ##          loadings intercepts
  ##   R2       0.9824     0.9972
  ##   sqrtU2   0.1327     0.0529
  ##   rbar     0.6237     0.7856
  ##   -----------------------------------------------------------------
  ##   Group Means and Standard Deviations
  ##     alpha0  psi0
  ##   1 -0.002 0.965
  ##   2 -0.107 1.098
  ##   3 -0.083 1.011
  ##   4  0.170 0.935

#############################################################################
# EXAMPLE 3: European Social Survey data.ess2005
#############################################################################

data(data.ess2005)
lambda &lt;- data.ess2005$lambda
nu &lt;- data.ess2005$nu

# Model 1: least squares optimization
mod1 &lt;- sirt::invariance.alignment( lambda=lambda, nu=nu, align.pow=c(2,2) )
summary(mod1)

# Model 2: sparse target function and definition of scales
mod2 &lt;- sirt::invariance.alignment( lambda=lambda, nu=nu, control=list(trace=2) )
summary(mod2)

#############################################################################
# EXAMPLE 4: Linking with item parameters containing outliers
#############################################################################

# see Help file in linking.robust

# simulate some item difficulties in the Rasch model
I &lt;- 38
set.seed(18785)
itempars &lt;- data.frame("item"=paste0("I",1:I) )
itempars$study1 &lt;- stats::rnorm( I, mean=.3, sd=1.4 )
# simulate DIF effects plus some outliers
bdif &lt;- stats::rnorm(I, mean=.4, sd=.09) +
             (stats::runif(I)&gt;.9 )*rep( 1*c(-1,1)+.4, each=I/2 )
itempars$study2 &lt;- itempars$study1 + bdif
# create input for function invariance.alignment
nu &lt;- t( itempars[,2:3] )
colnames(nu) &lt;- itempars$item
lambda &lt;- 1+0*nu

# linking using least squares optimization
mod1 &lt;- sirt::invariance.alignment( lambda=lambda, nu=nu )
summary(mod1)
  ##   Group Means and Standard Deviations
  ##          alpha0 psi0
  ##   study1 -0.286    1
  ##   study2  0.286    1

# linking using powers of .5
mod2 &lt;- sirt::invariance.alignment( lambda=lambda, nu=nu, align.pow=c(1,1) )
summary(mod2)
  ##   Group Means and Standard Deviations
  ##          alpha0 psi0
  ##   study1 -0.213    1
  ##   study2  0.213    1

# linking using powers of .25
mod3 &lt;- sirt::invariance.alignment( lambda=lambda, nu=nu, align.pow=c(.5,.5) )
summary(mod3)
  ##   Group Means and Standard Deviations
  ##          alpha0 psi0
  ##   study1 -0.207    1
  ##   study2  0.207    1

#############################################################################
# EXAMPLE 5: Linking gender groups with data.math
#############################################################################

data(data.math)
dat &lt;- data.math$data
dat.male &lt;- dat[ dat$female==0, substring( colnames(dat),1,1)=="M"  ]
dat.female &lt;- dat[ dat$female==1, substring( colnames(dat),1,1)=="M"  ]

#*************************
# Model 1: Linking using the Rasch model
mod1m &lt;- sirt::rasch.mml2( dat.male )
mod1f &lt;- sirt::rasch.mml2( dat.female )

# create objects for invariance.alignment
nu &lt;- rbind( mod1m$item$thresh, mod1f$item$thresh )
colnames(nu) &lt;- mod1m$item$item
rownames(nu) &lt;- c("male", "female")
lambda &lt;- 1+0*nu

# mean of item difficulties
round( rowMeans(nu), 3 )

# Linking using least squares optimization
res1a &lt;- sirt::invariance.alignment( lambda, nu, align.scale=c( .3, .5 ) )
summary(res1a)

# Linking using optimization with absolute value function (pow=.5)
res1b &lt;- sirt::invariance.alignment( lambda, nu, align.scale=c( .3, .5 ),
                align.pow=c(1,1) )
summary(res1b)

#-- compare results with Haberman linking
I &lt;- ncol(dat.male)
itempartable &lt;- data.frame( "study"=rep( c("male", "female"), each=I ) )
itempartable$item &lt;- c( paste0(mod1m$item$item),  paste0(mod1f$item$item) )
itempartable$a &lt;- 1
itempartable$b &lt;- c( mod1m$item$b, mod1f$item$b )
# estimate linking parameters
res1c &lt;- sirt::linking.haberman( itempars=itempartable )

#-- results of sirt::equating.rasch
x &lt;- itempartable[ 1:I, c("item", "b") ]
y &lt;- itempartable[ I + 1:I, c("item", "b") ]
res1d &lt;- sirt::equating.rasch( x, y )
round( res1d$B.est, 3 )
  ##     Mean.Mean Haebara Stocking.Lord
  ##   1     0.032   0.032         0.029

#*************************
# Model 2: Linking using the 2PL model
I &lt;- ncol(dat.male)
mod2m &lt;- sirt::rasch.mml2( dat.male, est.a=1:I)
mod2f &lt;- sirt::rasch.mml2( dat.female, est.a=1:I)

# create objects for invariance.alignment
nu &lt;- rbind( mod2m$item$thresh, mod2f$item$thresh )
colnames(nu) &lt;- mod2m$item$item
rownames(nu) &lt;- c("male", "female")
lambda &lt;- rbind( mod2m$item$a, mod2f$item$a )
colnames(lambda) &lt;- mod2m$item$item
rownames(lambda) &lt;- c("male", "female")

res2a &lt;- sirt::invariance.alignment( lambda, nu, align.scale=c( .3, .5 ) )
summary(res2a)

res2b &lt;- sirt::invariance.alignment( lambda, nu, align.scale=c( .3, .5 ),
                align.pow=c(1,1) )
summary(res2b)

# compare results with Haberman linking
I &lt;- ncol(dat.male)
itempartable &lt;- data.frame( "study"=rep( c("male", "female"), each=I ) )
itempartable$item &lt;- c( paste0(mod2m$item$item),  paste0(mod2f$item$item ) )
itempartable$a &lt;- c( mod2m$item$a, mod2f$item$a )
itempartable$b &lt;- c( mod2m$item$b, mod2f$item$b )
# estimate linking parameters
res2c &lt;- sirt::linking.haberman( itempars=itempartable )

#############################################################################
# EXAMPLE 6: Data from Asparouhov &amp; Muthen (2014) simulation study
#############################################################################

G &lt;- 3  # number of groups
I &lt;- 5  # number of items
# define lambda and nu parameters
lambda &lt;- matrix(1, nrow=G, ncol=I)
nu &lt;- matrix(0, nrow=G, ncol=I)

# define size of noninvariance
dif &lt;- 1

#- 1st group: N(0,1)
lambda[1,3] &lt;- 1+dif*.4; nu[1,5] &lt;- dif*.5

#- 2nd group: N(0.3,1.5)
gg &lt;- 2 ; mu &lt;- .3; sigma &lt;- sqrt(1.5)
lambda[gg,5] &lt;- 1-.5*dif; nu[gg,1] &lt;- -.5*dif
nu[gg,] &lt;- nu[gg,] + mu*lambda[gg,]
lambda[gg,] &lt;- lambda[gg,] * sigma

#- 3nd group: N(.8,1.2)
gg &lt;- 3 ; mu &lt;- .8; sigma &lt;- sqrt(1.2)
lambda[gg,4] &lt;- 1-.7*dif; nu[gg,2] &lt;- -.5*dif
nu[gg,] &lt;- nu[gg,] + mu*lambda[gg,]
lambda[gg,] &lt;- lambda[gg,] * sigma

# define alignment scale
align.scale &lt;- c(.2,.4)   # Asparouhov and Muthen use c(1,1)
# define alignment powers
align.pow &lt;- c(.5,.5)   # as in Asparouhov and Muthen

#*** estimate alignment parameters
mod1 &lt;- sirt::invariance.alignment( lambda, nu, eps=.01, optimizer="optim",
            align.scale=align.scale, align.pow=align.pow, center=FALSE )
summary(mod1)

#--- find parameter constraints for prespecified tolerance
cmod1 &lt;- sirt::invariance_alignment_constraints(model=mod1, nu_parm_tol=.4,
            lambda_parm_tol=.2 )
summary(cmod1)

#############################################################################
# EXAMPLE 7: Similar to Example 6, but with data simulation and CFA estimation
#############################################################################

#--- data simulation

set.seed(65)
G &lt;- 3  # number of groups
I &lt;- 5  # number of items
# define lambda and nu parameters
lambda &lt;- matrix(1, nrow=G, ncol=I)
nu &lt;- matrix(0, nrow=G, ncol=I)
err_var &lt;- matrix(1, nrow=G, ncol=I)

# define size of noninvariance
dif &lt;- 1
#- 1st group: N(0,1)
lambda[1,3] &lt;- 1+dif*.4; nu[1,5] &lt;- dif*.5
#- 2nd group: N(0.3,1.5)
gg &lt;- 2 ;
lambda[gg,5] &lt;- 1-.5*dif; nu[gg,1] &lt;- -.5*dif
#- 3nd group: N(.8,1.2)
gg &lt;- 3
lambda[gg,4] &lt;- 1-.7*dif; nu[gg,2] &lt;- -.5*dif
#- define distributions of groups
mu &lt;- c(0,.3,.8)
sigma &lt;- sqrt(c(1,1.5,1.2))
N &lt;- rep(1000,3) # sample sizes per group

#* simulate data
dat &lt;- sirt::invariance_alignment_simulate(nu, lambda, err_var, mu, sigma, N)
head(dat)

#--- estimate CFA models
pars &lt;- sirt::invariance_alignment_cfa_config(dat[,-1], group=dat$group)
print(pars)

#--- invariance alignment
# define alignment scale
align.scale &lt;- c(.2,.4)
# define alignment powers
align.pow &lt;- c(.5,.5)
mod1 &lt;- sirt::invariance.alignment( lambda=pars$lambda, nu=pars$nu, eps=.01,
            optimizer="optim", align.scale=align.scale, align.pow=align.pow, center=FALSE)
#* find parameter constraints for prespecified tolerance
cmod1 &lt;- sirt::invariance_alignment_constraints(model=mod1, nu_parm_tol=.4,
            lambda_parm_tol=.2 )
summary(cmod1)

#--- estimate CFA models with sampling weights

#* simulate weights
weights &lt;- stats::runif(sum(N), 0, 2)
#* estimate models
pars2 &lt;- sirt::invariance_alignment_cfa_config(dat[,-1], group=dat$group, weights=weights)
print(pars2$nu)
print(pars$nu)

#--- estimate one-parameter model
pars &lt;- sirt::invariance_alignment_cfa_config(dat[,-1], group=dat$group, model="1PM")
print(pars)

#############################################################################
# EXAMPLE 8: Computation of standard errors
#############################################################################

G &lt;- 3  # number of groups
I &lt;- 5  # number of items
# define lambda and nu parameters
lambda &lt;- matrix(1, nrow=G, ncol=I)
nu &lt;- matrix(0, nrow=G, ncol=I)

# define size of noninvariance
dif &lt;- 1

mu1 &lt;- c(0,.3,.8)
sigma1 &lt;- c(1,1.25,1.1)

#- 1st group
lambda[1,3] &lt;- 1+dif*.4; nu[1,5] &lt;- dif*.5

#- 2nd group
gg &lt;- 2
lambda[gg,5] &lt;- 1-.5*dif; nu[gg,1] &lt;- -.5*dif

#- 3nd group
gg &lt;- 3
lambda[gg,4] &lt;- 1-.7*dif; nu[gg,2] &lt;- -.5*dif

dat &lt;- sirt::invariance_alignment_simulate(nu=nu, lambda=lambda, err_var=1+0*lambda,
                mu=mu1, sigma=sigma1, N=500, output="data", exact=TRUE)

#* estimate CFA
res &lt;- sirt::invariance_alignment_cfa_config(dat=dat[,-1], group=dat$group )

#- perform invariance alignment
eps &lt;- .001
align.pow &lt;- 0.5*rep(1,2)
lambda &lt;- res$lambda
nu &lt;- res$nu
mod1 &lt;- sirt::invariance.alignment( lambda=lambda, nu=nu, eps=eps, optimizer="optim",
             align.pow=align.pow, meth=meth, vcov=res$vcov)
# variance matrix and standard errors
mod1$vcov
sqrt(diag(mod1$vcov))

## End(Not run)
</code></pre>

<hr>
<h2 id='IRT.mle'>
Person Parameter Estimation
</h2><span id='topic+IRT.mle'></span>

<h3>Description</h3>

<p>Computes the maximum likelihood estimate (MLE),
weighted likelihood estimate (WLE) and maximum aposterior
estimate (MAP) of ability in unidimensional item response models
(Penfield &amp; Bergeron, 2005; Warm, 1989). Item response functions can be
defined by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IRT.mle(data, irffct, arg.list, theta=rep(0,nrow(data)), type="MLE",
     mu=0, sigma=1, maxiter=20, maxincr=3, h=0.001, convP=1e-04,
     maxval=9, progress=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IRT.mle_+3A_data">data</code></td>
<td>

<p>Data frame with item responses
</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_irffct">irffct</code></td>
<td>

<p>User defined item response (see Examples). Arguments must be
specified in <code>arg.list</code>. The function must contain <code>theta</code>
and <code>ii</code> (item index) as arguments.
</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_theta">theta</code></td>
<td>

<p>Initial ability estimate
</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_arg.list">arg.list</code></td>
<td>

<p>List of arguments for <code>irffct</code>.
</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_type">type</code></td>
<td>
<p>Type of ability estimate. It can be <code>"MLE"</code> (the default),
<code>"WLE"</code> or <code>"MAP"</code>.
</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_mu">mu</code></td>
<td>
<p>Mean of normal prior distribution (for <code>type="MAP"</code>)</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_sigma">sigma</code></td>
<td>
<p>Standard deviation of normal prior distribution (for <code>type="MAP"</code>)</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_maxincr">maxincr</code></td>
<td>

<p>Maximum increment
</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_h">h</code></td>
<td>

<p>Numerical differentiation parameter
</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_convp">convP</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_maxval">maxval</code></td>
<td>

<p>Maximum ability value to be estimated
</p>
</td></tr>
<tr><td><code id="IRT.mle_+3A_progress">progress</code></td>
<td>

<p>Logical indicating whether iteration progress should be displayed
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame with estimated abilities (<code>est</code>) and its standard error
(<code>se</code>).
</p>


<h3>References</h3>

<p>Penfield, R. D., &amp; Bergeron, J. M. (2005). Applying a weighted
maximum likelihood latent trait estimator to the generalized
partial credit model. <em>Applied Psychological Measurement,
29</em>, 218-233.
</p>
<p>Warm, T. A. (1989). Weighted likelihood estimation of ability in item response theory.
<em>Psychometrika, 54</em>, 427-450.
</p>


<h3>See Also</h3>

<p>See also the <span class="pkg">PP</span> package for further person parameter
estimation methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Generalized partial credit model
#############################################################################

data(data.ratings1)
dat &lt;- data.ratings1

# estimate model
mod1 &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], rater=dat$rater,
             pid=dat$idstud, maxiter=15)
# extract dataset and item parameters
data &lt;- mod1$procdata$dat2.NA
a &lt;- mod1$ipars.dat2$a
b &lt;- mod1$ipars.dat2$b
theta0 &lt;- mod1$person$EAP
# define item response function for item ii
calc.pcm &lt;- function( theta, a, b, ii ){
    K &lt;- ncol(b)
    N &lt;- length(theta)
    matrK &lt;- matrix( 0:K, nrow=N, ncol=K+1, byrow=TRUE)
    eta &lt;- a[ii] * theta * matrK - matrix( c(0,b[ii,]), nrow=N, ncol=K+1, byrow=TRUE)
    eta &lt;- exp(eta)
    probs &lt;- eta / rowSums(eta, na.rm=TRUE)
    return(probs)
}
arg.list &lt;- list("a"=a, "b"=b )

# MLE
abil1 &lt;- sirt::IRT.mle( data, irffct=calc.pcm, theta=theta0, arg.list=arg.list )
str(abil1)
# WLE
abil2 &lt;- sirt::IRT.mle( data, irffct=calc.pcm, theta=theta0, arg.list=arg.list, type="WLE")
str(abil2)
# MAP with prior distribution N(.2, 1.3)
abil3 &lt;- sirt::IRT.mle( data, irffct=calc.pcm, theta=theta0, arg.list=arg.list,
              type="MAP", mu=.2, sigma=1.3 )
str(abil3)

#############################################################################
# EXAMPLE 2: Rasch model
#############################################################################

data(data.read)
dat &lt;- data.read
I &lt;- ncol(dat)

# estimate Rasch model
mod1 &lt;- sirt::rasch.mml2( dat )
summary(mod1)

# define item response function
irffct &lt;- function( theta, b, ii){
    eta &lt;- exp( theta - b[ii] )
    probs &lt;- eta / ( 1 + eta )
    probs &lt;- cbind( 1 - probs, probs )
    return(probs)
}
# initial person parameters and item parameters
theta0 &lt;- mod1$person$EAP
arg.list &lt;- list( "b"=mod1$item$b  )

# estimate WLE
abil &lt;- sirt::IRT.mle( data=dat, irffct=irffct, arg.list=arg.list,
            theta=theta0, type="WLE")
# compare with wle.rasch function
theta &lt;- sirt::wle.rasch( dat, b=mod1$item$b )
cbind( abil[,1], theta$theta, abil[,2], theta$se.theta )

#############################################################################
# EXAMPLE 3: Ramsay quotient model
#############################################################################

data(data.read)
dat &lt;- data.read
I &lt;- ncol(dat)

# estimate Ramsay model
mod1 &lt;- sirt::rasch.mml2( dat, irtmodel="ramsay.qm" )
summary(mod1)
# define item response function
irffct &lt;- function( theta, b, K, ii){
    eta &lt;- exp( theta / b[ii] )
    probs &lt;- eta / ( K[ii] + eta )
    probs &lt;- cbind( 1 - probs, probs )
    return(probs)
}
# initial person parameters and item parameters
theta0 &lt;- exp( mod1$person$EAP )
arg.list &lt;- list( "b"=mod1$item2$b, "K"=mod1$item2$K )
# estimate MLE
res &lt;- sirt::IRT.mle( data=dat, irffct=irffct, arg.list=arg.list, theta=theta0,
            maxval=20, maxiter=50)

## End(Not run)
</code></pre>

<hr>
<h2 id='isop'>
Fit Unidimensional ISOP and ADISOP Model to Dichotomous
and Polytomous Item Responses
</h2><span id='topic+isop.dich'></span><span id='topic+isop.poly'></span><span id='topic+summary.isop'></span><span id='topic+plot.isop'></span>

<h3>Description</h3>

<p>Fit the unidimensional isotonic probabilistic model (ISOP;
Scheiblechner, 1995, 2007) and the additive istotonic
probabilistic model (ADISOP; Scheiblechner, 1999).
The <code>isop.dich</code> function can be used for dichotomous
data while the <code>isop.poly</code> function can be applied
to polytomous data. Note that for applying the ISOP model for
polytomous data it is necessary that all items do have the
same number of categories.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isop.dich(dat, score.breaks=NULL, merge.extreme=TRUE,
     conv=.0001, maxit=1000, epsilon=.025, progress=TRUE)

isop.poly( dat, score.breaks=seq(0,1,len=10 ),
     conv=.0001, maxit=1000, epsilon=.025, progress=TRUE )

## S3 method for class 'isop'
summary(object,...)

## S3 method for class 'isop'
plot(x,ask=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isop_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous or polytomous item responses
</p>
</td></tr>
<tr><td><code id="isop_+3A_score.breaks">score.breaks</code></td>
<td>

<p>Vector with breaks to define score groups. For dichotomous
data, the person score grouping is applied for the mean
person score, for polytomous data it is applied to
the modified percentile score.
</p>
</td></tr>
<tr><td><code id="isop_+3A_merge.extreme">merge.extreme</code></td>
<td>

<p>Merge extreme groups with zero and maximum score
with succeeding score categories? The default is
<code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="isop_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="isop_+3A_maxit">maxit</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="isop_+3A_epsilon">epsilon</code></td>
<td>
<p>Additive constant to handle cell frequencies
of 0 or 1 in <code><a href="#topic+fit.adisop">fit.adisop</a></code>
</p>
</td></tr>
<tr><td><code id="isop_+3A_progress">progress</code></td>
<td>

<p>Display progress?
</p>
</td></tr>
<tr><td><code id="isop_+3A_object">object</code></td>
<td>
<p>Object of class <code>isop</code> (generated by
<code>isop.dich</code> or <code>isop.poly</code>)</p>
</td></tr>
<tr><td><code id="isop_+3A_x">x</code></td>
<td>
<p>Object of class <code>isop</code> (generated by
<code>isop.dich</code> or <code>isop.poly</code>)</p>
</td></tr>
<tr><td><code id="isop_+3A_ask">ask</code></td>
<td>
<p>Ask for a new plot?</p>
</td></tr>
<tr><td><code id="isop_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ISOP model for dichotomous data was firstly proposed
by Irtel and Schmalhofer (1982). Consider person groups
<code class="reqn">p</code> (ordered from low to high scores)
and items <code class="reqn">i</code> (ordered from difficult to easy items).
Here, <code class="reqn">F(p,i)</code> denotes
the proportion correct for item <code class="reqn">i</code>  in score group
<code class="reqn">p</code>, while <code class="reqn">n_{pi}</code> denotes the number of persons
in group <code class="reqn">p</code> and on item <code class="reqn">i</code>. The isotonic
probabilistic model (Scheiblechner, 1995) monotonically
smooths this distribution function <code class="reqn">F</code> such that
</p>
<p style="text-align: center;"><code class="reqn"> P( X_{pi}=1 | p, i )=F^\ast( p, i ) </code>
</p>

<p>where the two-dimensional distribution function
<code class="reqn">F^\ast</code> is isotonic in <code class="reqn">p</code> and <code class="reqn">i</code>. Model fit is
assessed by the square root of weighted squares of deviations
</p>
<p style="text-align: center;"><code class="reqn">Fit=\sqrt{ \frac{1}{I} \sum_{p,i} w_{pi} \left(  F(p, i) -
    F^\ast(p,i ) \right )^2 }</code>
</p>

<p>with frequency weights <code class="reqn">w_{pi}</code> and
<code class="reqn">\sum_p w_{pi}=1</code> for every item <code class="reqn">i</code>.
The additive isotonic model (ADISOP; Scheiblechner, 1999)
assumes the existence of person parameters <code class="reqn">\theta_p</code>
and item parameters <code class="reqn">\delta_i</code> such that
</p>
<p style="text-align: center;"><code class="reqn"> P( X_{pi}=1 | p )=g( \theta_p + \delta_i )</code>
</p>

<p>and <code class="reqn">g</code> is a nonparametrically estimated isotonic
function. The functions <code>isop.dich</code> and <code>isop.poly</code> uses <code class="reqn">F^\ast</code>
from the ISOP models and estimates person and item parameters of the
ADISOP model. For comparison, <code>isop.dich</code> also fits a model with
the logistic function <code class="reqn">g</code> which results in the Rasch
model.
</p>
<p>For polytomous data, the starting point is the empirical
distribution function
</p>
<p style="text-align: center;"><code class="reqn"> P( X_i \le k | p  )=F( k ; p, i )  </code>
</p>
<p> which is increasing
in the argument <code class="reqn">k</code> (the item categories).
The ISOP model is defined to be antitonic in <code class="reqn">p</code> and <code class="reqn">i</code>
while items are ordered with respect to item P-scores and persons are ordered
according to modified percentile scores (Scheiblechner, 2007).
The estimated ISOP model results in a distribution
function <code class="reqn">F^\ast</code>. Using this function, the additive
isotonic probabilistic model (ADISOP) aims at estimating
a distribution function
</p>
<p style="text-align: center;"><code class="reqn">P( X_i \le k ; p  )=F^{\ast \ast} ( k ; p, i )=F^{ \ast \ast }
    ( k, \theta_p + \delta_i ) </code>
</p>

<p>which is antitonic in <code class="reqn">k</code> and in <code class="reqn">\theta_p + \delta_i</code>.
Due to this additive relation, the ADISOP scale values
are claimed to be measured at interval scale level (Scheiblechner, 1999).
</p>
<p>The ADISOP model is compared to the graded response model which is
defined by the response equation
</p>
<p style="text-align: center;"><code class="reqn">P( X_i \le k ; p  )=g( \theta_p + \delta_i + \gamma_k ) </code>
</p>

<p>where <code class="reqn">g</code> denotes the logistic function.
Estimated parameters are in the value <code>fit.grm</code>:
person parameters <code class="reqn">\theta_p</code> (<code>person.sc</code>),
item parameters <code class="reqn">\delta_i</code> (<code>item.sc</code>) and
category parameters <code class="reqn">\gamma_k</code> (<code>cat.sc</code>).
</p>
<p>The calculation of person and item scores is explained
in <code><a href="#topic+isop.scoring">isop.scoring</a></code>.
</p>
<p>For an application of the ISOP and ADISOP model
see Scheiblechner and Lutz (2009).
</p>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>freq.correct</code></td>
<td>

<p>Used frequency table (distribution function) for
dichotomous and polytomous data
</p>
</td></tr>
<tr><td><code>wgt</code></td>
<td>

<p>Used weights (frequencies)
</p>
</td></tr>
<tr><td><code>prob.saturated</code></td>
<td>

<p>Frequencies of the saturated model
</p>
</td></tr>
<tr><td><code>prob.isop</code></td>
<td>

<p>Fitted frequencies of the ISOP model
</p>
</td></tr>
<tr><td><code>prob.adisop</code></td>
<td>

<p>Fitted frequencies of the ADISOP model
</p>
</td></tr>
<tr><td><code>prob.logistic</code></td>
<td>

<p>Fitted frequencies of the logistic model
(only for <code>isop.dich</code>)
</p>
</td></tr>
<tr><td><code>prob.grm</code></td>
<td>

<p>Fitted frequencies of the graded response model
(only for <code>isop.poly</code>)
</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>List with log-likelihood values</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>Vector of fit statistics</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame of person parameters</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Data frame of item parameters</p>
</td></tr>
<tr><td><code>p.itemcat</code></td>
<td>
<p>Frequencies for every item category</p>
</td></tr>
<tr><td><code>score.itemcat</code></td>
<td>

<p>Scoring points for every item category
</p>
</td></tr>
<tr><td><code>fit.isop</code></td>
<td>
<p>Values of fitting the ISOP model
(see <code><a href="#topic+fit.isop">fit.isop</a></code>) </p>
</td></tr>
<tr><td><code>fit.isop</code></td>
<td>
<p>Values of fitting the ADISOP model
(see <code><a href="#topic+fit.adisop">fit.adisop</a></code>) </p>
</td></tr>
<tr><td><code>fit.logistic</code></td>
<td>
<p>Values of fitting the logistic model
(only for <code>isop.dich</code>) </p>
</td></tr>
<tr><td><code>fit.grm</code></td>
<td>
<p>Values of fitting the graded response model
(only for <code>isop.poly</code>) </p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Irtel, H., &amp; Schmalhofer, F. (1982).
Psychodiagnostik auf Ordinalskalenniveau:
Messtheoretische Grundlagen, Modelltest und
Parameterschaetzung. <em>Archiv fuer Psychologie,
134</em>, 197-218.
</p>
<p>Scheiblechner, H. (1995). Isotonic ordinal
probabilistic models (ISOP). <em>Psychometrika,
60</em>, 281-304.
</p>
<p>Scheiblechner, H. (1999). Additive conjoint isotonic
probabilistic models (ADISOP). <em>Psychometrika,
64</em>, 295-316.
</p>
<p>Scheiblechner, H. (2007). A unified nonparametric IRT model for
d-dimensional psychological test data (d-ISOP).
<em>Psychometrika, 72</em>, 43-67.
</p>
<p>Scheiblechner, H., &amp; Lutz, R. (2009).
Die Konstruktion eines optimalen eindimensionalen
Tests mittels nichtparametrischer Testtheorie (NIRT)
am Beispiel des MR SOC. <em>Diagnostica, 55</em>,
41-54.
</p>


<h3>See Also</h3>

<p>This function uses <code><a href="#topic+isop.scoring">isop.scoring</a></code>,
<code><a href="#topic+fit.isop">fit.isop</a></code> and <code><a href="#topic+fit.adisop">fit.adisop</a></code>.
</p>
<p>Tests of the W1 axiom of the ISOP model (Scheiblechner, 1995) can be performed with
<code><a href="#topic+isop.test">isop.test</a></code>.
</p>
<p>See also the <span class="pkg">ISOP</span> package at <em>Rforge</em>: <em>http://www.rforge.net/ISOP/</em>.
</p>
<p>Install this package using
</p>
<p><code>install.packages("ISOP",repos="http://www.rforge.net/")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset Reading (dichotomous items)
#############################################################################

data(data.read)
dat &lt;- as.matrix( data.read)
I &lt;- ncol(dat)

# Model 1: ISOP Model (11 score groups)
mod1 &lt;- sirt::isop.dich( dat )
summary(mod1)
plot(mod1)

## Not run: 
# Model 2: ISOP Model (5 score groups)
score.breaks &lt;- seq( -.005, 1.005, len=5+1 )
mod2 &lt;- sirt::isop.dich( dat, score.breaks=score.breaks)
summary(mod2)

#############################################################################
# EXAMPLE 2: Dataset PISA mathematics (dichotomous items)
#############################################################################

data(data.pisaMath)
dat &lt;- data.pisaMath$data
dat &lt;- dat[, grep("M", colnames(dat) ) ]

# fit ISOP model
# Note that for this model many iterations are needed
#   to reach convergence for ADISOP
mod1 &lt;- sirt::isop.dich( dat, maxit=4000)
summary(mod1)

## End(Not run)

#############################################################################
# EXAMPLE 3: Dataset Students (polytomous items)
#############################################################################

# Dataset students: scale cultural activities
library(CDM)
data(data.Students, package="CDM")
dat &lt;- stats::na.omit( data.Students[, paste0("act",1:4) ] )

# fit models
mod1 &lt;- sirt::isop.poly( dat )
summary(mod1)
plot(mod1)
</code></pre>

<hr>
<h2 id='isop.scoring'>
Scoring Persons and Items in the ISOP Model
</h2><span id='topic+isop.scoring'></span>

<h3>Description</h3>

<p>This function does the scoring in the isotonic probabilistic
model (Scheiblechner, 1995, 2003, 2007).
Person parameters are ordinally scaled but the ISOP
model also allows <em>specific objective</em> (ordinal) comparisons
for persons (Scheiblechner, 1995).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isop.scoring(dat,score.itemcat=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isop.scoring_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous or polytomous item responses
</p>
</td></tr>
<tr><td><code id="isop.scoring_+3A_score.itemcat">score.itemcat</code></td>
<td>

<p>Optional data frame with scoring points for every item and
every category (see Example 2).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function extracts the scoring rule of the ISOP model
(if <code>score.itemcat !=NULL</code>) and calculates the
modified percentile score for every person. The score
<code class="reqn">s_{ik}</code> for item <code class="reqn">i</code> and category <code class="reqn">k</code>
is calculated as
</p>
<p style="text-align: center;"><code class="reqn"> s_{ik}=\sum_{j=0}^{k-1} f_{ij} -
\sum_{j=k+1}^K f_{ij}=P( X_i &lt; k  )
- P( X_i &gt; k ) </code>
</p>

<p>where <code class="reqn">f_{ik}</code> is the relative frequency of item <code class="reqn">i</code>
in category <code class="reqn">k</code> and <code class="reqn">K</code> is the maximum category.
The modified percentile score <code class="reqn">\rho_p</code>
for subject <code class="reqn">p</code> (<code>mpsc</code> in <code>person</code>) is
defined by
</p>
<p style="text-align: center;"><code class="reqn"> \rho_p=\frac{1}{I} \sum_{i=1}^I
        \sum_{j=0}^K s_{ik} \mathbf{1}( X_{pi}=k ) </code>
</p>

<p>Note that for dichotomous items, the sum score is a
sufficient statistic for <code class="reqn">\rho_p</code> but this is
not the case for polytomous items.
The modified percentile score <code class="reqn">\rho_p</code>
ranges between -1 and 1.
</p>
<p>The modified item P-score <code class="reqn">\rho_i</code> (Scheiblechner, 2007, p. 52) is
defined by
</p>
<p style="text-align: center;"><code class="reqn"> \rho_i=\frac{1}{I-1} \cdot \sum_j \left[ P( X_j &lt; X_i )
        - P( X_j &gt; X_i ) \right ] </code>
</p>



<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>person</code></td>
<td>

<p>A data frame with person parameters. The modified
percentile score <code class="reqn">\rho_p</code> is denoted by <code>mpsc</code>.
</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Item statistics and scoring parameters.
The item P-scores <code class="reqn">\rho_i</code>
are labeled as <code>pscore</code>.</p>
</td></tr>
<tr><td><code>p.itemcat</code></td>
<td>
<p>Frequencies for every item category</p>
</td></tr>
<tr><td><code>score.itemcat</code></td>
<td>

<p>Scoring points for every item category
</p>
</td></tr>
<tr><td><code>distr.fct</code></td>
<td>
<p>Empirical distribution function</p>
</td></tr>
</table>


<h3>References</h3>

<p>Scheiblechner, H. (1995). Isotonic ordinal
probabilistic models (ISOP). <em>Psychometrika,
60</em>, 281-304.
</p>
<p>Scheiblechner, H. (2003). <em>Nonparametric IRT:
Scoring functions and ordinal parameter estimation
of isotonic probabilistic models (ISOP)</em>.
Technical Report, Philipps-Universitaet Marburg.
</p>
<p>Scheiblechner, H. (2007). A unified nonparametric IRT model for
d-dimensional psychological test data (d-ISOP).
<em>Psychometrika, 72</em>, 43-67.
</p>


<h3>See Also</h3>

<p>For fitting the ISOP and ADISOP model see
<code><a href="#topic+isop.dich">isop.dich</a></code> or <code><a href="#topic+fit.isop">fit.isop</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset Reading
#############################################################################

data( data.read )
dat &lt;- data.read

# Scoring according to the ISOP model
msc &lt;- sirt::isop.scoring( dat )
# plot student scores
boxplot( msc$person$mpsc ~ msc$person$score )

#############################################################################
# EXAMPLE 2: Dataset students from CDM package | polytomous items
#############################################################################

library("CDM")
data( data.Students, package="CDM")
dat &lt;- stats::na.omit(data.Students[, -c(1:2) ])

# Scoring according to the ISOP model
msc &lt;- sirt::isop.scoring( dat )
# plot student scores
boxplot( msc$person$mpsc ~ msc$person$score )

# scoring with known scoring rule for activity items
items &lt;- paste0( "act", 1:5 )
score.itemcat &lt;- msc$score.itemcat
score.itemcat &lt;- score.itemcat[ items, ]
msc2 &lt;- sirt::isop.scoring( dat[,items], score.itemcat=score.itemcat )
</code></pre>

<hr>
<h2 id='isop.test'>
Testing the ISOP Model
</h2><span id='topic+isop.test'></span><span id='topic+summary.isop.test'></span>

<h3>Description</h3>

<p>This function performs tests of the W1 axiom of the ISOP model
(Scheiblechner, 2003). Standard errors of the corresponding <code class="reqn">W1_i</code> statistics
are obtained by Jackknife.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isop.test(data, jackunits=20, weights=rep(1, nrow(data)))

## S3 method for class 'isop.test'
summary(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isop.test_+3A_data">data</code></td>
<td>

<p>Data frame with item responses
</p>
</td></tr>
<tr><td><code id="isop.test_+3A_jackunits">jackunits</code></td>
<td>

<p>A number of Jackknife units (if an integer is provided as the argument
value) or a vector in the Jackknife units are already
defined.
</p>
</td></tr>
<tr><td><code id="isop.test_+3A_weights">weights</code></td>
<td>

<p>Optional vector of sampling weights
</p>
</td></tr>
<tr><td><code id="isop.test_+3A_object">object</code></td>
<td>
<p>Object of class <code>isop.test</code></p>
</td></tr>
<tr><td><code id="isop.test_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>itemstat</code></td>
<td>
<p>Data frame with test and item statistics for the W1 axiom.
The <code class="reqn">W1_i</code> statistic is denoted as <code>est</code> while <code>se</code>
is the corresponding standard error of the statistic.
The sample size per item is <code>N</code> and <code>M</code> denotes the item mean.</p>
</td></tr>
<tr><td><code>Es</code></td>
<td>
<p>Number of concordances per item</p>
</td></tr>
<tr><td><code>Ed</code></td>
<td>
<p>Number of disconcordances per item</p>
</td></tr>
</table>
<p>The <code class="reqn">W1_i</code> statistics are printed by the <code>summary</code> method.
</p>


<h3>References</h3>

<p>Scheiblechner, H. (2003). Nonparametric IRT: Testing the bi-isotonicity of
isotonic probabilistic models (ISOP). <em>Psychometrika, 68</em>,
79-96.
</p>


<h3>See Also</h3>

<p>Fit the ISOP model with <code><a href="#topic+isop.dich">isop.dich</a></code> or <code><a href="#topic+isop.poly">isop.poly</a></code>.
</p>
<p>See also the <span class="pkg">ISOP</span> package at <em>Rforge</em>: <em>http://www.rforge.net/ISOP/</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: ISOP model data.Students
#############################################################################

data(data.Students, package="CDM")
dat &lt;- data.Students[, paste0("act",1:5) ]
dat &lt;- dat[1:300, ]    # select first 300 students

# perform the ISOP test
mod &lt;- sirt::isop.test(dat)
summary(mod)
  ## -&gt; W1i statistics
  ##     parm   N     M   est    se      t
  ##   1 test 300    NA 0.430 0.036 11.869
  ##   2 act1 278 0.601 0.451 0.048  9.384
  ##   3 act2 275 0.473 0.473 0.035 13.571
  ##   4 act3 274 0.277 0.352 0.098  3.596
  ##   5 act4 291 1.320 0.381 0.054  7.103
  ##   6 act5 276 0.460 0.475 0.042 11.184
</code></pre>

<hr>
<h2 id='latent.regression.em.raschtype'>
Latent Regression Model for the Generalized
Logistic Item Response Model and the Linear Model for Normal Responses
</h2><span id='topic+latent.regression.em.raschtype'></span><span id='topic+latent.regression.em.normal'></span><span id='topic+summary.latent.regression'></span>

<h3>Description</h3>

<p>This function estimates a unidimensional latent regression model if
a likelihood is specified, parameters from the generalized
item response model (Stukel, 1988) or a mean and a standard error
estimate for individual scores is provided as input.
Item parameters are treated as fixed in the estimation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>latent.regression.em.raschtype(data=NULL, f.yi.qk=NULL, X,
    weights=rep(1, nrow(X)), beta.init=rep(0,ncol(X)),
    sigma.init=1, b=rep(0,ncol(X)), a=rep(1,length(b)),
    c=rep(0, length(b)), d=rep(1, length(b)), alpha1=0, alpha2=0,
    max.parchange=1e-04, theta.list=seq(-5, 5, len=20),
    maxiter=300, progress=TRUE )

latent.regression.em.normal(y, X, sig.e, weights=rep(1, nrow(X)),
    beta.init=rep(0, ncol(X)), sigma.init=1, max.parchange=1e-04,
    maxiter=300, progress=TRUE)

## S3 method for class 'latent.regression'
summary(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="latent.regression.em.raschtype_+3A_data">data</code></td>
<td>
<p> An <code class="reqn">N \times I</code> data frame of dichotomous item responses.
If no data frame is supplied, then a user can input the individual
likelihood <code>f.yi.qk</code>.
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_f.yi.qk">f.yi.qk</code></td>
<td>
<p>An optional matrix which contains the individual likelihood.
This matrix is produced by <code><a href="#topic+rasch.mml2">rasch.mml2</a></code> or
<code><a href="#topic+rasch.copula2">rasch.copula2</a></code>. The use of this argument allows the
estimation of the latent regression model independent of the
parameters of the used item response model.
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_x">X</code></td>
<td>
<p>An <code class="reqn">N \times K</code> matrix of <code class="reqn">K</code> covariates in the latent
regression model. Note that the intercept (i.e. a vector of ones)
must be included in <code>X</code>.
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_weights">weights</code></td>
<td>

<p>Student weights (optional).
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_beta.init">beta.init</code></td>
<td>

<p>Initial regression coefficients (optional).
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_sigma.init">sigma.init</code></td>
<td>

<p>Initial residual standard deviation (optional).
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_b">b</code></td>
<td>

<p>Item difficulties (optional). They must only be provided
if the likelihood <code>f.yi.qk</code> is not given as an input.
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_a">a</code></td>
<td>

<p>Item discriminations (optional).
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_c">c</code></td>
<td>

<p>Guessing parameter (lower asymptotes) (optional).
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_d">d</code></td>
<td>

<p>One minus slipping parameter (upper asymptotes) (optional).
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_alpha1">alpha1</code></td>
<td>

<p>Upper tail parameter <code class="reqn">\alpha_1</code> in the generalized logistic item response model.
Default is 0.
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_alpha2">alpha2</code></td>
<td>

<p>Lower tail parameter <code class="reqn">\alpha_2</code> parameter in the generalized
logistic item response model. Default is 0.
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_max.parchange">max.parchange</code></td>
<td>

<p>Maximum change in regression parameters
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_theta.list">theta.list</code></td>
<td>

<p>Grid of person ability where theta is evaluated
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_progress">progress</code></td>
<td>
<p>An optional logical indicating whether computation
progress should be displayed.</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_y">y</code></td>
<td>
<p>Individual scores</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_sig.e">sig.e</code></td>
<td>
<p>Standard errors for individual scores</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_object">object</code></td>
<td>

<p>Object of class <code>latent.regression</code>
</p>
</td></tr>
<tr><td><code id="latent.regression.em.raschtype_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the output <em>Regression Parameters</em>
the fraction of missing information (<code>fmi</code>) is reported
which is the increase of variance in regression
parameter estimates because ability is defined as
a latent variable. The effective sample size <code>pseudoN.latent</code>
corresponds to a sample size when the ability would be
available with a reliability of one.
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>iterations</code></td>
<td>
<p>Number of iterations needed</p>
</td></tr>
<tr><td><code>maxiter</code></td>
<td>
<p>Maximal number of iterations</p>
</td></tr>
<tr><td><code>max.parchange</code></td>
<td>
<p>Maximum change in parameter estimates</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>Coefficients</p>
</td></tr>
<tr><td><code>summary.coef</code></td>
<td>
<p>Summary of regression coefficients</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimate of residual standard deviation</p>
</td></tr>
<tr><td><code>vcov.simple</code></td>
<td>
<p>Covariance parameters of estimated parameters
(simplified version)</p>
</td></tr>
<tr><td><code>vcov.latent</code></td>
<td>
<p>Covariance parameters of estimated parameters
which accounts for latent ability</p>
</td></tr>
<tr><td><code>post</code></td>
<td>
<p>Individual posterior distribution</p>
</td></tr>
<tr><td><code>EAP</code></td>
<td>
<p>Individual EAP estimates</p>
</td></tr>
<tr><td><code>SE.EAP</code></td>
<td>
<p>Standard error estimates of EAP</p>
</td></tr>
<tr><td><code>explvar</code></td>
<td>
<p>Explained variance in latent regression</p>
</td></tr>
<tr><td><code>totalvar</code></td>
<td>
<p>Total variance in latent regression</p>
</td></tr>
<tr><td><code>rsquared</code></td>
<td>
<p>Explained variance <code class="reqn">R^2</code> in latent regression</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Using the defaults in <code>a</code>, <code>c</code>, <code>d</code>,
<code>alpha1</code> and <code>alpha2</code> corresponds to the
Rasch model.
</p>


<h3>References</h3>

<p>Adams, R., &amp; Wu. M. (2007). The mixed-coefficients multinomial
logit model: A generalized form of the Rasch model.
In M. von Davier &amp; C. H. Carstensen (Eds.). <em>Multivariate and mixture
distribution Rasch models: Extensions and applications</em> (pp. 57-76).
New York: Springer.
<a href="https://doi.org/10.1007/978-0-387-49839-3_4">doi:10.1007/978-0-387-49839-3_4</a>
</p>
<p>Mislevy, R. J. (1991). Randomization-based inference about latent variables
from complex samples. <em>Psychometrika, 56</em>(2), 177-196.
<a href="https://doi.org/10.1007/BF02294457">doi:10.1007/BF02294457</a>
</p>
<p>Stukel, T. A. (1988). Generalized logistic models.
<em>Journal of the American Statistical Association, 83</em>(402), 426-431.
<a href="https://doi.org/10.1080/01621459.1988.10478613">doi:10.1080/01621459.1988.10478613</a>
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+plausible.value.imputation.raschtype">plausible.value.imputation.raschtype</a></code>
for plausible value imputation of generalized logistic
item type models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
#  EXAMPLE 1: PISA Reading | Rasch model for dichotomous data
#############################################################################

data(data.pisaRead, package="sirt")
dat &lt;- data.pisaRead$data
items &lt;- grep("R", colnames(dat))
# define matrix of covariates
X &lt;- cbind( 1, dat[, c("female","hisei","migra" ) ] )

#***
# Model 1: Latent regression model in the Rasch model
# estimate Rasch model
mod1 &lt;- sirt::rasch.mml2( dat[,items] )
# latent regression model
lm1 &lt;- sirt::latent.regression.em.raschtype( data=dat[,items ], X=X, b=mod1$item$b )

## Not run: 
#***
# Model 2: Latent regression with generalized link function
# estimate alpha parameters for link function
mod2 &lt;- sirt::rasch.mml2( dat[,items], est.alpha=TRUE)
# use model estimated likelihood for latent regression model
lm2 &lt;- sirt::latent.regression.em.raschtype( f.yi.qk=mod2$f.yi.qk,
            X=X, theta.list=mod2$theta.k)

#***
# Model 3: Latent regression model based on Rasch copula model
testlets &lt;- paste( data.pisaRead$item$testlet)
itemclusters &lt;- match( testlets, unique(testlets) )
# estimate Rasch copula model
mod3 &lt;- sirt::rasch.copula2( dat[,items], itemcluster=itemclusters )
# use model estimated likelihood for latent regression model
lm3 &lt;- sirt::latent.regression.em.raschtype( f.yi.qk=mod3$f.yi.qk,
                X=X, theta.list=mod3$theta.k)

#############################################################################
# EXAMPLE 2: Simulated data according to the Rasch model
#############################################################################

set.seed(899)
I &lt;- 21     # number of items
b &lt;- seq(-2,2, len=I)   # item difficulties
n &lt;- 2000       # number of students

# simulate theta and covariates
theta &lt;- stats::rnorm( n )
x &lt;- .7 * theta + stats::rnorm( n, .5 )
y &lt;- .2 * x+ .3*theta + stats::rnorm( n, .4 )
dfr &lt;- data.frame( theta, 1, x, y )

# simulate Rasch model
dat1 &lt;- sirt::sim.raschtype( theta=theta, b=b )

# estimate latent regression
mod &lt;- sirt::latent.regression.em.raschtype( data=dat1, X=dfr[,-1], b=b )
  ## Regression Parameters
  ##
  ##        est se.simple     se        t p   beta    fmi N.simple pseudoN.latent
  ## X1 -0.2554    0.0208 0.0248 -10.2853 0 0.0000 0.2972     2000       1411.322
  ## x   0.4113    0.0161 0.0193  21.3037 0 0.4956 0.3052     2000       1411.322
  ## y   0.1715    0.0179 0.0213   8.0438 0 0.1860 0.2972     2000       1411.322
  ##
  ## Residual Variance=0.685
  ## Explained Variance=0.3639
  ## Total Variance=1.049
  ##                 R2=0.3469

# compare with linear model (based on true scores)
summary( stats::lm( theta  ~ x + y, data=dfr ) )
  ## Coefficients:
  ##             Estimate Std. Error t value Pr(&gt;|t|)
  ## (Intercept) -0.27821    0.01984  -14.02   &lt;2e-16 ***
  ## x            0.40747    0.01534   26.56   &lt;2e-16 ***
  ## y            0.18189    0.01704   10.67   &lt;2e-16 ***
  ## ---
  ##
  ## Residual standard error: 0.789 on 1997 degrees of freedom
  ## Multiple R-squared: 0.3713,     Adjusted R-squared: 0.3707

#***********
# define guessing parameters (lower asymptotes) and
# upper asymptotes ( 1 minus slipping parameters)
cI &lt;- rep(.2, I)        # all items get a guessing parameter of .2
cI[ c(7,9) ] &lt;- .25     # 7th and 9th get a guessing parameter of .25
dI &lt;- rep( .95, I )    # upper asymptote of .95
dI[ c(7,11) ] &lt;- 1        # 7th and 9th item have an asymptote of 1

# latent regression model
mod1 &lt;- sirt::latent.regression.em.raschtype( data=dat1, X=dfr[,-1],
           b=b, c=cI, d=dI    )
  ## Regression Parameters
  ##
  ##        est se.simple     se        t p   beta    fmi N.simple pseudoN.latent
  ## X1 -0.7929    0.0243 0.0315 -25.1818 0 0.0000 0.4044     2000       1247.306
  ## x   0.5025    0.0188 0.0241  20.8273 0 0.5093 0.3936     2000       1247.306
  ## y   0.2149    0.0209 0.0266   8.0850 0 0.1960 0.3831     2000       1247.306
  ##
  ## Residual Variance=0.9338
  ## Explained Variance=0.5487
  ## Total Variance=1.4825
  ##                 R2=0.3701

#############################################################################
# EXAMPLE 3: Measurement error in dependent variable
#############################################################################

set.seed(8766)
N &lt;- 4000       # number of persons
X &lt;- stats::rnorm(N)           # independent variable
Z &lt;- stats::rnorm(N)           # independent variable
y &lt;- .45 * X + .25 * Z + stats::rnorm(N)   # dependent variable true score
sig.e &lt;- stats::runif( N, .5, .6 )       # measurement error standard deviation
yast &lt;- y + stats::rnorm( N, sd=sig.e ) # dependent variable measured with error

#****
# Model 1: Estimation with latent.regression.em.raschtype using
#          individual likelihood
# define theta grid for evaluation of density
theta.list &lt;- mean(yast) + stats::sd(yast) * seq( - 5, 5, length=21)
# compute individual likelihood
f.yi.qk &lt;- stats::dnorm( outer( yast, theta.list, "-" ) / sig.e )
f.yi.qk &lt;- f.yi.qk / rowSums(f.yi.qk)
# define predictor matrix
X1 &lt;- as.matrix(data.frame( "intercept"=1, "X"=X, "Z"=Z ))

# latent regression model
res &lt;- sirt::latent.regression.em.raschtype( f.yi.qk=f.yi.qk,
                    X=X1, theta.list=theta.list)
  ##   Regression Parameters
  ##
  ##                est se.simple     se       t      p   beta    fmi N.simple pseudoN.latent
  ##   intercept 0.0112    0.0157 0.0180  0.6225 0.5336 0.0000 0.2345     4000       3061.998
  ##   X         0.4275    0.0157 0.0180 23.7926 0.0000 0.3868 0.2350     4000       3061.998
  ##   Z         0.2314    0.0156 0.0178 12.9868 0.0000 0.2111 0.2349     4000       3061.998
  ##
  ##   Residual Variance=0.9877
  ##   Explained Variance=0.2343
  ##   Total Variance=1.222
  ##                   R2=0.1917

#****
# Model 2: Estimation with latent.regression.em.normal
res2 &lt;- sirt::latent.regression.em.normal( y=yast, sig.e=sig.e, X=X1)
  ##   Regression Parameters
  ##
  ##                est se.simple     se       t      p   beta    fmi N.simple pseudoN.latent
  ##   intercept 0.0112    0.0157 0.0180  0.6225 0.5336 0.0000 0.2345     4000       3062.041
  ##   X         0.4275    0.0157 0.0180 23.7927 0.0000 0.3868 0.2350     4000       3062.041
  ##   Z         0.2314    0.0156 0.0178 12.9870 0.0000 0.2111 0.2349     4000       3062.041
  ##
  ##   Residual Variance=0.9877
  ##   Explained Variance=0.2343
  ##   Total Variance=1.222
  ##                   R2=0.1917

  ## -&gt; Results between Model 1 and Model 2 are identical because they use
  ##    the same input.

#***
# Model 3: Regression model based on true scores y
mod3 &lt;- stats::lm( y ~ X + Z )
summary(mod3)
  ##   Coefficients:
  ##               Estimate Std. Error t value Pr(&gt;|t|)
  ##   (Intercept)  0.02364    0.01569   1.506    0.132
  ##   X            0.42401    0.01570  27.016   &lt;2e-16 ***
  ##   Z            0.23804    0.01556  15.294   &lt;2e-16 ***
  ##   Residual standard error: 0.9925 on 3997 degrees of freedom
  ##   Multiple R-squared:  0.1923,    Adjusted R-squared:  0.1919
  ##   F-statistic: 475.9 on 2 and 3997 DF,  p-value: &lt; 2.2e-16

#***
# Model 4: Regression model based on observed scores yast
mod4 &lt;- stats::lm( yast ~ X + Z )
summary(mod4)
  ##   Coefficients:
  ##               Estimate Std. Error t value Pr(&gt;|t|)
  ##   (Intercept)  0.01101    0.01797   0.613     0.54
  ##   X            0.42716    0.01797  23.764   &lt;2e-16 ***
  ##   Z            0.23174    0.01783  13.001   &lt;2e-16 ***
  ##   Residual standard error: 1.137 on 3997 degrees of freedom
  ##   Multiple R-squared:  0.1535,    Adjusted R-squared:  0.1531
  ##   F-statistic: 362.4 on 2 and 3997 DF,  p-value: &lt; 2.2e-16

## End(Not run)
</code></pre>

<hr>
<h2 id='lavaan2mirt'>
Converting a <code>lavaan</code> Model into a <code>mirt</code> Model
</h2><span id='topic+lavaan2mirt'></span>

<h3>Description</h3>

<p>Converts a <code>lavaan</code> model into a <code>mirt</code> model.
Optionally, the model can be estimated with the
<code><a href="mirt.html#topic+mirt">mirt::mirt</a></code> function
(<code>est.mirt=TRUE</code>) or just <code>mirt</code> syntax
is generated (<code>est.mirt=FALSE</code>).
</p>
<p>Extensions of the <code>lavaan</code> syntax include guessing and slipping
parameters (operators <code>?=g1</code> and <code>?=s1</code>)
and a shortage operator for item groups (see <code>__</code>).
See <code><a href="TAM.html#topic+lavaanify.IRT">TAM::lavaanify.IRT</a></code> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lavaan2mirt(dat, lavmodel, est.mirt=TRUE, poly.itemtype="gpcm", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lavaan2mirt_+3A_dat">dat</code></td>
<td>

<p>Dataset with item responses
</p>
</td></tr>
<tr><td><code id="lavaan2mirt_+3A_lavmodel">lavmodel</code></td>
<td>

<p>Model specified in <code>lavaan</code> syntax
(see <code><a href="lavaan.html#topic+lavaanify">lavaan::lavaanify</a></code>)
</p>
</td></tr>
<tr><td><code id="lavaan2mirt_+3A_est.mirt">est.mirt</code></td>
<td>

<p>An optional logical indicating whether the model
should be estimated with <code><a href="mirt.html#topic+mirt">mirt::mirt</a></code>
</p>
</td></tr>
<tr><td><code id="lavaan2mirt_+3A_poly.itemtype">poly.itemtype</code></td>
<td>
<p>Item type for polytomous data. This can
be <code>gpcm</code> for the generalized partial credit model
or <code>graded</code> for the graded response model.</p>
</td></tr>
<tr><td><code id="lavaan2mirt_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed for estimation in
<code><a href="mirt.html#topic+mirt">mirt</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses the <code><a href="lavaan.html#topic+lavaanify">lavaan::lavaanify</a></code>
(<span class="pkg">lavaan</span>) function.
</p>
<p>Only single group models are supported (for now).
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>mirt</code></td>
<td>
<p>Object generated by <code>mirt</code> function if
<code>est.mirt=TRUE</code></p>
</td></tr>
<tr><td><code>mirt.model</code></td>
<td>
<p>Generated <code>mirt</code> model</p>
</td></tr>
<tr><td><code>mirt.syntax</code></td>
<td>
<p>Generated <code>mirt</code> syntax</p>
</td></tr>
<tr><td><code>mirt.pars</code></td>
<td>
<p>Generated parameter specifications
in <code>mirt</code></p>
</td></tr>
<tr><td><code>lavaan.model</code></td>
<td>
<p>Used <code>lavaan</code> model transformed by
<code>lavaanify</code> function</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Used dataset. If necessary, only items used in the
model are included in the dataset.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>See <a href="https://lavaan.ugent.be/">https://lavaan.ugent.be/</a> for <span class="pkg">lavaan</span> resources.
</p>
<p>See <a href="https://groups.google.com/forum/#!forum/lavaan">https://groups.google.com/forum/#!forum/lavaan</a>
for discussion about the <span class="pkg">lavaan</span> package. <br />
</p>
<p>See <code><a href="#topic+mirt.wrapper">mirt.wrapper</a></code> for convenience wrapper functions
for <code><a href="mirt.html#topic+mirt">mirt::mirt</a></code> objects.
</p>
<p>See <code><a href="TAM.html#topic+lavaanify.IRT">TAM::lavaanify.IRT</a></code>
for extensions of <code>lavaanify</code>.
</p>
<p>See <code><a href="#topic+tam2mirt">tam2mirt</a></code> for converting fitted objects in the <span class="pkg">TAM</span>
package into fitted <code><a href="mirt.html#topic+mirt">mirt::mirt</a></code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Convert some lavaan syntax to mirt syntax for data.read
#############################################################################

library(mirt)
data(data.read)
dat &lt;- data.read

#******************
#*** Model 1: Single factor model
lavmodel &lt;- "
     # omit item C3
     F=~ A1+A2+A3+A4 + C1+C2+C4 + B1+B2+B3+B4
     F ~~ 1*F
            "

# convert syntax and estimate model
res &lt;- sirt::lavaan2mirt( dat,  lavmodel, verbose=TRUE, technical=list(NCYCLES=3) )
# inspect coefficients
coef(res$mirt)
mirt.wrapper.coef(res$mirt)
# converted mirt model and parameter table
cat(res$mirt.syntax)
res$mirt.pars

#******************
#*** Model 2: Rasch Model with first six items
lavmodel &lt;- "
     F=~ a*A1+a*A2+a*A3+a*A4+a*B1+a*B2
     F ~~ 1*F
            "
# convert syntax and estimate model
res &lt;- sirt::lavaan2mirt( dat,  lavmodel, est.mirt=FALSE)
# converted mirt model
cat(res$mirt.syntax)
# mirt parameter table
res$mirt.pars
# estimate model using generated objects
res2 &lt;- mirt::mirt( res$dat, res$mirt.model, pars=res$mirt.pars )
mirt.wrapper.coef(res2)     # parameter estimates

#******************
#*** Model 3: Bifactor model
lavmodel &lt;- "
     G=~ A1+A2+A3+A4 + B1+B2+B3+B4  + C1+C2+C3+C4
     A=~ A1+A2+A3+A4
     B=~ B1+B2+B3+B4
     C=~ C1+C2+C3+C4
     G ~~ 1*G
     A ~~ 1*A
     B ~~ 1*B
     C ~~ 1*C
            "
res &lt;- sirt::lavaan2mirt( dat,  lavmodel, est.mirt=FALSE )
# mirt syntax and mirt model
cat(res$mirt.syntax)
res$mirt.model
res$mirt.pars

#******************
#*** Model 4: 3-dimensional model with some parameter constraints
lavmodel &lt;- "
     # some equality constraints among loadings
     A=~ a*A1+a*A2+a2*A3+a2*A4
     B=~ B1+B2+b3*B3+B4
     C=~ c*C1+c*C2+c*C3+c*C4
     # some equality constraints among thresholds
     A1 | da*t1
     A3 | da*t1
     B3 | da*t1
     C3 | dg*t1
     C4 | dg*t1
     # standardized latent variables
     A ~~ 1*A
     B ~~ 1*B
     C ~~ 1*C
     # estimate Cov(A,B) and Cov(A,C)
     A ~~ B
     A ~~ C
     # estimate mean of B
     B ~ 1
            "
res &lt;- sirt::lavaan2mirt( dat,  lavmodel, verbose=TRUE, technical=list(NCYCLES=3) )
# estimated parameters
mirt.wrapper.coef(res$mirt)
# generated mirt syntax
cat(res$mirt.syntax)
# mirt parameter table
mirt::mod2values(res$mirt)

#******************
#*** Model 5: 3-dimensional model with some parameter constraints and
#             parameter fixings
lavmodel &lt;- "
     A=~ a*A1+a*A2+1.3*A3+A4  # set loading of A3 to 1.3
     B=~ B1+1*B2+b3*B3+B4
     C=~ c*C1+C2+c*C3+C4
     A1 | da*t1
     A3 | da*t1
     C4 | dg*t1
     B1 | 0*t1
     B3 | -1.4*t1   # fix item threshold of B3 to -1.4
     A ~~ 1*A
     B ~~ B         # estimate variance of B freely
     C ~~ 1*C
     A ~~ B         # estimate covariance between A and B
     A ~~ .6 * C    # fix covariance to .6
     A ~ .5*1       # set mean of A to .5
     B ~ 1          # estimate mean of B
            "
res &lt;- sirt::lavaan2mirt( dat,  lavmodel, verbose=TRUE, technical=list(NCYCLES=3) )
mirt.wrapper.coef(res$mirt)

#******************
#*** Model 6: 1-dimensional model with guessing and slipping parameters
#******************

lavmodel &lt;- "
     F=~ c*A1+c*A2+1*A3+1.3*A4 + C1__C4 + a*B1+b*B2+b*B3+B4
     # guessing parameters
     A1+A2 ?=guess1*g1
     A3 ?=.25*g1
     B1+C1 ?=g1
     B2__B4 ?=0.10*g1
     # slipping parameters
     A1+A2+C3 ?=slip1*s1
     A3 ?=.02*s1
     # fix item intercepts
     A1 | 0*t1
     A2 | -.4*t1
     F ~ 1    # estimate mean of F
     F ~~ 1*F   # fix variance of F
            "
# convert syntax and estimate model
res &lt;- sirt::lavaan2mirt( dat,  lavmodel, verbose=TRUE, technical=list(NCYCLES=3) )
# coefficients
mirt.wrapper.coef(res$mirt)
# converted mirt model
cat(res$mirt.syntax)

#############################################################################
# EXAMPLE 2: Convert some lavaan syntax to mirt syntax for
#            longitudinal data data.long
#############################################################################

data(data.long)
dat &lt;- data.long[,-1]

#******************
#*** Model 1: Rasch model for T1
lavmodel &lt;- "
     F=~ 1*I1T1 +1*I2T1+1*I3T1+1*I4T1+1*I5T1+1*I6T1
     F ~~ F
            "
# convert syntax and estimate model
res &lt;- sirt::lavaan2mirt( dat,  lavmodel, verbose=TRUE, technical=list(NCYCLES=20) )
# inspect coefficients
mirt.wrapper.coef(res$mirt)
# converted mirt model
cat(res$mirt.syntax)

#******************
#*** Model 2: Rasch model for two time points
lavmodel &lt;- "
     F1=~ 1*I1T1 +1*I2T1+1*I3T1+1*I4T1+1*I5T1+1*I6T1
     F2=~ 1*I3T2 +1*I4T2+1*I5T2+1*I6T2+1*I7T2+1*I8T2
     F1 ~~ F1
     F1 ~~ F2
     F2 ~~ F2
     # equal item difficulties of same items
     I3T1 | i3*t1
     I3T2 | i3*t1
     I4T1 | i4*t1
     I4T2 | i4*t1
     I5T1 | i5*t1
     I5T2 | i5*t1
     I6T1 | i6*t1
     I6T2 | i6*t1
     # estimate mean of F1, but fix mean of F2
     F1 ~ 1
     F2 ~ 0*1
            "
# convert syntax and estimate model
res &lt;- sirt::lavaan2mirt( dat,  lavmodel, verbose=TRUE, technical=list(NCYCLES=20) )
# inspect coefficients
mirt.wrapper.coef(res$mirt)
# converted mirt model
cat(res$mirt.syntax)

#-- compare estimation with smirt function
# define Q-matrix
I &lt;- ncol(dat)
Q &lt;- matrix(0,I,2)
Q[1:6,1] &lt;- 1
Q[7:12,2] &lt;- 1
rownames(Q) &lt;- colnames(dat)
colnames(Q) &lt;- c("T1","T2")
# vector with same items
itemnr &lt;- as.numeric( substring( colnames(dat),2,2) )
# fix mean at T2 to zero
mu.fixed &lt;- cbind( 2,0 )
# estimate model in smirt
mod1 &lt;- sirt::smirt(dat, Qmatrix=Q, irtmodel="comp", est.b=itemnr, mu.fixed=mu.fixed )
summary(mod1)

#############################################################################
# EXAMPLE 3: Converting lavaan syntax for polytomous data
#############################################################################

data(data.big5)
# select some items
items &lt;- c( grep( "O", colnames(data.big5), value=TRUE )[1:6],
            grep( "N", colnames(data.big5), value=TRUE )[1:4] )
#  O3  O8  O13 O18 O23 O28 N1  N6  N11 N16
dat &lt;- data.big5[, items ]
library(psych)
psych::describe(dat)

#******************
#*** Model 1: Partial credit model
lavmodel &lt;- "
      O=~ 1*O3+1*O8+1*O13+1*O18+1*O23+1*O28
      O ~~ O
         "
# estimate model in mirt
res &lt;- sirt::lavaan2mirt( dat, lavmodel, technical=list(NCYCLES=20), verbose=TRUE)
# estimated mirt model
mres &lt;- res$mirt
# mirt syntax
cat(res$mirt.syntax)
  ##   O=1,2,3,4,5,6
  ##   COV=O*O
# estimated parameters
mirt.wrapper.coef(mres)
# some plots
mirt::itemplot( mres, 3 )   # third item
plot(mres)   # item information
plot(mres,type="trace")  # item category functions

# graded response model with equal slopes
res1 &lt;- sirt::lavaan2mirt( dat, lavmodel, poly.itemtype="graded", technical=list(NCYCLES=20),
              verbose=TRUE )
mirt.wrapper.coef(res1$mirt)

#******************
#*** Model 2: Generalized partial credit model with some constraints
lavmodel &lt;- "
      O=~ O3+O8+O13+a*O18+a*O23+1.2*O28
      O ~ 1   # estimate mean
      O ~~ O  # estimate variance
      # some constraints among thresholds
      O3  | d1*t1
      O13 | d1*t1
      O3  | d2*t2
      O8  | d3*t2
      O28 | (-0.5)*t1
         "
# estimate model in mirt
res &lt;- sirt::lavaan2mirt( dat, lavmodel, technical=list(NCYCLES=5), verbose=TRUE)
# estimated mirt model
mres &lt;- res$mirt
# estimated parameters
mirt.wrapper.coef(mres)

#*** generate syntax for mirt for this model and estimate it in mirt package
# Items: O3  O8  O13 O18 O23 O28
mirtmodel &lt;- mirt::mirt.model( "
             O=1-6
             # a(O18)=a(O23), t1(O3)=t1(O18), t2(O3)=t2(O8)
             CONSTRAIN=(4,5,a1), (1,3,d1), (1,2,d2)
             MEAN=O
             COV=O*O
               ")
# initial table of parameters in mirt
mirt.pars &lt;- mirt::mirt( dat[,1:6], mirtmodel, itemtype="gpcm", pars="values")
# fix slope of item O28 to 1.2
ind &lt;- which( ( mirt.pars$item=="O28" ) &amp; ( mirt.pars$name=="a1") )
mirt.pars[ ind, "est"] &lt;- FALSE
mirt.pars[ ind, "value"] &lt;- 1.2
# fix d1 of item O28 to -0.5
ind &lt;- which( ( mirt.pars$item=="O28" ) &amp; ( mirt.pars$name=="d1") )
mirt.pars[ ind, "est"] &lt;- FALSE
mirt.pars[ ind, "value"] &lt;- -0.5
# estimate model
res2 &lt;- mirt::mirt( dat[,1:6], mirtmodel, pars=mirt.pars,
             verbose=TRUE, technical=list(NCYCLES=4) )
mirt.wrapper.coef(res2)
plot(res2, type="trace")

## End(Not run)
</code></pre>

<hr>
<h2 id='lc.2raters'>
Latent Class Model for Two Exchangeable Raters and One Item
</h2><span id='topic+lc.2raters'></span><span id='topic+summary.lc.2raters'></span>

<h3>Description</h3>

<p>This function computes a latent class model for ratings on an item
based on exchangeable raters (Uebersax &amp; Grove, 1990). Additionally,
several measures of rater agreement are computed (see e.g. Gwet, 2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lc.2raters(data, conv=0.001, maxiter=1000, progress=TRUE)

## S3 method for class 'lc.2raters'
summary(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lc.2raters_+3A_data">data</code></td>
<td>

<p>Data frame with item responses (must be ordered from 0 to <code class="reqn">K</code>) and two
columns which correspond to ratings of two (exchangeable) raters.
</p>
</td></tr>
<tr><td><code id="lc.2raters_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="lc.2raters_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="lc.2raters_+3A_progress">progress</code></td>
<td>

<p>An optional logical indicating whether iteration progress should be displayed.
</p>
</td></tr>
<tr><td><code id="lc.2raters_+3A_object">object</code></td>
<td>
<p>Object of class <code>lc.2raters</code></p>
</td></tr>
<tr><td><code id="lc.2raters_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For two exchangeable raters which provide ratings on an item, a latent
class model with <code class="reqn">K+1</code> classes (if there are <code class="reqn">K+1</code> item categories
<code class="reqn">0,...,K</code>) is defined. Where <code class="reqn">P(X=x, Y=y | c)</code> denotes
the probability that the first rating is <code class="reqn">x</code> and the second rating is
<code class="reqn">y</code> given the true but unknown item category (class) <code class="reqn">c</code>. Ratings are
assumed to be locally independent, i.e.
</p>
<p style="text-align: center;"><code class="reqn"> P(X=x, Y=y | c )=P( X=x | c) \cdot P(Y=y | c )=p_{x|c} \cdot p_{y|c}</code>
</p>

<p>Note that <code class="reqn">P(X=x|c)=P(Y=x|c)=p_{x|c}</code> holds due to the exchangeability of raters.
The latent class model estimates true class proportions <code class="reqn">\pi_c</code> and
conditional item probabilities <code class="reqn">p_{x|c}</code>.
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>classprob.1rater.like</code></td>
<td>
<p>Classification probability <code class="reqn">P(c|x)</code> of latent
category <code class="reqn">c</code> given a manifest rating <code class="reqn">x</code> (estimated by maximum likelihood)</p>
</td></tr>
<tr><td><code>classprob.1rater.post</code></td>
<td>
<p>Classification probability <code class="reqn">P(c|x)</code> of latent
category <code class="reqn">c</code> given a manifest rating <code class="reqn">x</code> (estimated by the posterior
distribution)</p>
</td></tr>
<tr><td><code>classprob.2rater.like</code></td>
<td>
<p>Classification probability <code class="reqn">P(c|(x,y))</code>
of latent category <code class="reqn">c</code> given two manifest ratings <code class="reqn">x</code> and <code class="reqn">y</code>
(estimated by maximum likelihood)</p>
</td></tr>
<tr><td><code>classprob.2rater.post</code></td>
<td>
<p>Classification probability <code class="reqn">P(c|(x,y))</code>
of latent category <code class="reqn">c</code> given two manifest ratings <code class="reqn">x</code> and <code class="reqn">y</code>
(estimated by posterior distribution)</p>
</td></tr>
<tr><td><code>f.yi.qk</code></td>
<td>
<p>Likelihood of each pair of ratings</p>
</td></tr>
<tr><td><code>f.qk.yi</code></td>
<td>
<p>Posterior of each pair of ratings</p>
</td></tr>
<tr><td><code>probs</code></td>
<td>
<p>Item response probabilities <code class="reqn">p_{x|c}</code></p>
</td></tr>
<tr><td><code>pi.k</code></td>
<td>
<p>Estimated class proportions <code class="reqn">\pi_c</code></p>
</td></tr>
<tr><td><code>pi.k.obs</code></td>
<td>
<p>Observed manifest class proportions</p>
</td></tr>
<tr><td><code>freq.long</code></td>
<td>
<p>Frequency table of ratings in long format</p>
</td></tr>
<tr><td><code>freq.table</code></td>
<td>
<p>Symmetrized frequency table of ratings</p>
</td></tr>
<tr><td><code>agree.stats</code></td>
<td>
<p>Measures of rater agreement. These measures include
percentage agreement (<code>agree0</code>, <code>agree1</code>), Cohen's kappa and weighted
Cohen's kappa (<code>kappa</code>, <code>wtd.kappa.linear</code>),
Gwet's AC1 agreement measures (<code>AC1</code>; Gwet, 2008, 2010) and
Aickin's alpha (<code>alpha.aickin</code>; Aickin, 1990).
</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>Used dataset</p>
</td></tr>
<tr><td><code>N.categ</code></td>
<td>
<p>Number of categories</p>
</td></tr>
</table>


<h3>References</h3>

<p>Aickin, M. (1990). Maximum likelihood estimation of agreement in the constant
predictive probability model, and its relation to Cohen's kappa.
<em>Biometrics, 46</em>, 293-302.
</p>
<p>Gwet, K. L. (2008). Computing inter-rater reliability and its variance
in the presence of high agreement.
<em>British Journal of Mathematical and Statistical Psychology,
61</em>, 29-48.
</p>
<p>Gwet, K. L. (2010). <em>Handbook of Inter-Rater Reliability</em>.
Advanced Analytics, Gaithersburg. <em>http://www.agreestat.com/</em>
</p>
<p>Uebersax, J. S., &amp; Grove, W. M. (1990). Latent class analysis of diagnostic
agreement. <em>Statistics in Medicine, 9</em>, 559-572.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+rm.facets">rm.facets</a></code> and <code><a href="#topic+rm.sdt">rm.sdt</a></code> for
specifying rater models.
</p>
<p>See also the <span class="pkg">irr</span> package for measures of rater agreement.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Latent class models for rating datasets data.si05
#############################################################################

data(data.si05)

#*** Model 1: one item with two categories
mod1 &lt;- sirt::lc.2raters( data.si05$Ex1)
summary(mod1)

#*** Model 2: one item with five categories
mod2 &lt;- sirt::lc.2raters( data.si05$Ex2)
summary(mod2)

#*** Model 3: one item with eight categories
mod3 &lt;- sirt::lc.2raters( data.si05$Ex3)
summary(mod3)
</code></pre>

<hr>
<h2 id='likelihood.adjustment'>
Adjustment and Approximation of Individual Likelihood Functions
</h2><span id='topic+likelihood.adjustment'></span>

<h3>Description</h3>

<p>Approximates individual likelihood functions <code class="reqn">L(\bold{X}_p | \theta)</code>
by normal distributions (see Mislevy, 1990). Extreme response patterns
are handled by adding pseudo-observations of items with extreme item
difficulties (see argument <code>extreme.item</code>. The individual standard
deviations of the likelihood, used in the normal approximation, can be
modified by individual adjustment factors which are specified in <code>adjfac</code>.
In addition, a reliability of the adjusted likelihood can be specified
in <code>target.EAP.rel</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>likelihood.adjustment(likelihood, theta=NULL, prob.theta=NULL,
     adjfac=rep(1, nrow(likelihood)), extreme.item=5, target.EAP.rel=NULL,
     min_tuning=0.2, max_tuning=3, maxiter=100, conv=1e-04,
     trait.normal=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="likelihood.adjustment_+3A_likelihood">likelihood</code></td>
<td>

<p>A matrix containing the individual likelihood <code class="reqn">L(\bold{X}_p | \theta)</code> or
an object of class <code>IRT.likelihood</code>.
</p>
</td></tr>
<tr><td><code id="likelihood.adjustment_+3A_theta">theta</code></td>
<td>

<p>Optional vector of (unidimensional) <code class="reqn">\theta</code> values
</p>
</td></tr>
<tr><td><code id="likelihood.adjustment_+3A_prob.theta">prob.theta</code></td>
<td>

<p>Optional vector of probabilities of <code class="reqn">\theta</code> trait distribution
</p>
</td></tr>
<tr><td><code id="likelihood.adjustment_+3A_adjfac">adjfac</code></td>
<td>

<p>Vector with individual adjustment factors of the standard deviations of the
likelihood
</p>
</td></tr>
<tr><td><code id="likelihood.adjustment_+3A_extreme.item">extreme.item</code></td>
<td>

<p>Item difficulties of two extreme pseudo items which are added as additional
observed data to the likelihood. A large number (e.g. <code>extreme.item=15</code>)
leaves the likelihood almost unaffected. See also Mislevy (1990).
</p>
</td></tr>
<tr><td><code id="likelihood.adjustment_+3A_target.eap.rel">target.EAP.rel</code></td>
<td>

<p>Target EAP reliability. An additional tuning parameter is estimated
which adjusts the likelihood to obtain a pre-specified reliability.
</p>
</td></tr>
<tr><td><code id="likelihood.adjustment_+3A_min_tuning">min_tuning</code></td>
<td>

<p>Minimum value of tuning parameter (if <code>! is.null(target.EAP.rel) </code>)
</p>
</td></tr>
<tr><td><code id="likelihood.adjustment_+3A_max_tuning">max_tuning</code></td>
<td>

<p>Maximum value of tuning parameter (if <code>! is.null(target.EAP.rel) </code>)
</p>
</td></tr>
<tr><td><code id="likelihood.adjustment_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations (if <code>! is.null(target.EAP.rel) </code>)
</p>
</td></tr>
<tr><td><code id="likelihood.adjustment_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion (if <code>! is.null(target.EAP.rel) </code>)
</p>
</td></tr>
<tr><td><code id="likelihood.adjustment_+3A_trait.normal">trait.normal</code></td>
<td>

<p>Optional logical indicating whether the trait distribution should be
normally distributed (if <code>! is.null(target.EAP.rel) </code>).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>IRT.likelihood</code>.
</p>


<h3>References</h3>

<p>Mislevy, R. (1990). Scaling procedures. In E. Johnson &amp; R. Zwick (Eds.),
<em>Focusing the new design: The NAEP 1988 technical report</em> (ETS RR 19-20).
Princeton, NJ: Educational Testing Service.
</p>


<h3>See Also</h3>

<p><code><a href="CDM.html#topic+IRT.likelihood">CDM::IRT.likelihood</a></code>,
<code><a href="TAM.html#topic+tam.latreg">TAM::tam.latreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Adjustment of the likelihood | data.read
#############################################################################

library(CDM)
library(TAM)
data(data.read)
dat &lt;- data.read

# define theta grid
theta.k &lt;- seq(-6,6,len=41)

#*** Model 1: fit Rasch model in TAM
mod1 &lt;- TAM::tam.mml( dat, control=list( nodes=theta.k) )
summary(mod1)

#*** Model 2: fit Rasch copula model
testlets &lt;- substring( colnames(dat), 1, 1 )
mod2 &lt;- sirt::rasch.copula2( dat, itemcluster=testlets, theta.k=theta.k)
summary(mod2)

# model comparison
IRT.compareModels( mod1, mod2 )

# extract EAP reliabilities
rel1 &lt;- mod1$EAP.rel
rel2 &lt;- mod2$EAP.Rel
# variance inflation factor
vif &lt;- (1-rel2) / (1-rel1)
  ##  &gt; vif
  ##  [1] 1.211644

# extract individual likelihood
like1 &lt;- IRT.likelihood( mod1 )
# adjust likelihood from Model 1 to obtain a target EAP reliability of .599
like1b &lt;- sirt::likelihood.adjustment( like1, target.EAP.rel=.599 )

# compare estimated latent regressions
lmod1a &lt;- TAM::tam.latreg( like1, Y=NULL )
lmod1b &lt;- TAM::tam.latreg( like1b, Y=NULL )
summary(lmod1a)
summary(lmod1b)

## End(Not run)
</code></pre>

<hr>
<h2 id='linking.haberman'>
Linking in the 2PL/Generalized Partial Credit Model
</h2><span id='topic+linking.haberman'></span><span id='topic+summary.linking.haberman'></span><span id='topic+linking.haberman.lq'></span><span id='topic+summary.linking.haberman.lq'></span><span id='topic+linking_haberman_itempars_prepare'></span><span id='topic+linking_haberman_itempars_convert'></span><span id='topic+L0_polish'></span>

<h3>Description</h3>

<p>This function does the linking of several studies which are calibrated
using the 2PL or the generalized item response model according to
Haberman (2009). This method is a generalization of log-mean-mean
linking from one study to several studies. The default <code>a_log=TRUE</code>
logarithmizes item slopes for linking while otherwise an additive regression
model is assumed for the original item loadings (see Details; Battauz, 2017)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linking.haberman(itempars, personpars, estimation="OLS", a_trim=Inf, b_trim=Inf,
    lts_prop=.5, a_log=TRUE, conv=1e-05, maxiter=1000, progress=TRUE,
    adjust_main_effects=TRUE, vcov=TRUE)

## S3 method for class 'linking.haberman'
summary(object, digits=3, file=NULL, ...)

linking.haberman.lq(itempars, pow=2, eps=1e-3, a_log=TRUE, use_nu=FALSE,
      est_pow=FALSE, lower_pow=.1, upper_pow=3)

## S3 method for class 'linking.haberman.lq'
summary(object, digits=3, file=NULL, ...)

## prepare 'itempars' argument for linking.haberman()
linking_haberman_itempars_prepare(b, a=NULL, wgt=NULL)

## conversion of different parameterizations of item parameters
linking_haberman_itempars_convert(itempars=NULL, lambda=NULL, nu=NULL, a=NULL, b=NULL)

## L0 polish precedure minimizing number of interactions in two-way table
L0_polish(x, tol, conv=0.01, maxiter=30, type=1, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linking.haberman_+3A_itempars">itempars</code></td>
<td>

<p>A data frame with four or five columns. The first four columns contain
in the order: study name, item name, <code class="reqn">a</code> parameter, <code class="reqn">b</code> parameter.
The fifth column is an optional weight for every item and every study.
</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_personpars">personpars</code></td>
<td>
<p>A list with vectors (e.g. EAPs or WLEs) or data frames
(e.g. plausible values) containing person parameters which
should be transformed.
If a data frame in each list entry has <code>se</code> or <code>SE</code>
(standard error) in a column name, then the corresponding
column is only multiplied by <code class="reqn">A_t</code>.
If a column is labeled as <code>pid</code> (person ID),
then it is left untransformed.
</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_estimation">estimation</code></td>
<td>
<p>Estimation method. Can be <code>"OLS"</code> (ordinary least squares),
<code>"BSQ"</code> (bisquare weighted regression), <code>"HUB"</code> (regression using Huber
weights), <code>"MED"</code> (median regression), <code>"LTS"</code> (trimmed least squares),
<code>"L1"</code> (median polish), <code>"L0"</code> (minimizing number of interactions)
</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_a_trim">a_trim</code></td>
<td>
<p>Trimming parameter for item slopes <code class="reqn">a_{it}</code> in
bisquare regression (see Details).</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_b_trim">b_trim</code></td>
<td>
<p>Trimming parameter for item slopes <code class="reqn">b_{it}</code> in
bisquare regression (see Details).</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_lts_prop">lts_prop</code></td>
<td>
<p>Proportion of retained observations in <code>"LTS"</code>
regression estimation</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_a_log">a_log</code></td>
<td>
<p>Logical indicating whether item slopes should be logarithmized
for linking.</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion.
</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations.
</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_progress">progress</code></td>
<td>

<p>An optional logical indicating whether computational progress
should be displayed.
</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_adjust_main_effects">adjust_main_effects</code></td>
<td>
<p>Logical indicating whether all elements in the vector
of main effects should be simultaneously adjusted</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_vcov">vcov</code></td>
<td>
<p>Optional indicating whether covariance matrix for linking errors
should be computed</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_pow">pow</code></td>
<td>
<p>Power <code class="reqn">q</code></p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_eps">eps</code></td>
<td>
<p>Epsilon value used in differentiable approximating function</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_use_nu">use_nu</code></td>
<td>
<p>Logical indicating whether item intercepts instead of
item difficulties are used in linking</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_est_pow">est_pow</code></td>
<td>
<p>Logical indicating whether power values should be estimated</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_lower_pow">lower_pow</code></td>
<td>
<p>Lower bound for estimated power</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_upper_pow">upper_pow</code></td>
<td>
<p>Upper bound for estimated power</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_lambda">lambda</code></td>
<td>
<p>Matrix containing item loadings</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_nu">nu</code></td>
<td>
<p>Matrix containing item intercepts</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_object">object</code></td>
<td>
<p>Object of class <code>linking.haberman</code>.</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_digits">digits</code></td>
<td>
<p>Number of digits after decimals for rounding in <code>summary</code>.</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_file">file</code></td>
<td>
<p>Optional file name if <code>summary</code> should be sunk into a file.</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_b">b</code></td>
<td>
<p>Matrix of item intercepts (items <code class="reqn">times</code> studies)</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_a">a</code></td>
<td>
<p>Matrix of item slopes</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_wgt">wgt</code></td>
<td>
<p>Matrix of weights</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_x">x</code></td>
<td>
<p>Matrix</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_tol">tol</code></td>
<td>
<p>Tolerance value</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_type">type</code></td>
<td>
<p>Can be <code>1</code> (using Tukey's median polish) or
<code>2</code> (alternating median regression).</p>
</td></tr>
<tr><td><code id="linking.haberman_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating whether iteration progress should be displayed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code class="reqn">t=1,\ldots,T</code> studies, item difficulties <code class="reqn">b_{it}</code> and
item slopes <code class="reqn">a_{it}</code> are available. For dichotomous responses, these
parameters are defined by the 2PL response equation
</p>
<p style="text-align: center;"><code class="reqn"> logit P(X_{pi}=1| \theta_p )=a_i ( \theta_p - b_i ) </code>
</p>

<p>while for polytomous responses the generalized partial credit model holds
</p>
<p style="text-align: center;"><code class="reqn"> log \frac{P(X_{pi}=k| \theta_p )}{P(X_{pi}=k-1| \theta_p )}
=a_i ( \theta_p - b_i + d_{ik} ) </code>
</p>

<p>The parameters <code class="reqn"> \{ a_{it}, b_{it} \}</code> of all items and studies are
linearly transformed using equations <code class="reqn">a_{it} \approx a_i / A_t</code>
(if <code>a_log=TRUE</code>) or <code class="reqn">a_{it} \approx a_i + A_t</code>
(if <code>a_log=FALSE</code>) and
<code class="reqn">b_{it} \cdot A_t \approx B_t + b_i</code>. For identification reasons,
we define <code class="reqn">A_1=1</code> and <code class="reqn">B_1</code>=0.
</p>
<p>The optimization function (which is a least squares criterion;
see Haberman, 2009) seeks the transformation parameters <code class="reqn">A_t</code> and
<code class="reqn">B_t</code> with an alternating least squares
method (<code>estimation="OLS"</code>). Note that every item <code class="reqn">i</code> and every study <code class="reqn">t</code> can
be weighted (specified in the fifth column of <code>itempars</code>).
Alternatively, a robust regression method based on bisquare weighting (Fox, 2015)
can be employed for linking using the argument <code>estimation="BSQ"</code>.
For example, in the case of item loadings, bisquare weighting is applied to
residuals <code class="reqn">e_{it}=a_{it} - a_i - A_t </code> (where logarithmized or non-logarithmized
item loadings are employed) forming weights
<code class="reqn">w_{it}=[ 1 - ( e_{it} / k )^2 ]^2</code> for <code class="reqn">e_{it} &lt;k</code> and 0 for <code class="reqn">e_{it} \ge k</code>
where <code class="reqn">k</code> is the trimming constant which can be estimated or fixed
during estimation using arguments <code>a_trim</code> or <code>b_trim</code>. Items in studies with
large residuals
(i.e., presence differential item functioning) are effectively set to zero in the
linking procedure. Alternatively, Huber weights (<code>estimation="HUB"</code>) downweight
large residuals by applying <code class="reqn">w_{it}=k / | e_{it} |</code> for residuals
<code class="reqn">|e_{it}|&gt;k</code>.  The method <code>estimation="LTS"</code> employs trimmed least squares
where the proportion
of data retained is specified in <code>lts_prop</code> with default set to .50.
</p>
<p>The method <code>estimation="MED"</code> estimates item parameters and linking constants
based on alternating median regression. A similar approach is the median polish
procedure of Tukey (Tukey, 1977, p. 362ff.; Maronna, Martin &amp; Yohai, 2006, p. 104;
see also <code><a href="stats.html#topic+medpolish">stats::medpolish</a></code>) implemented in
<code>estimation="L1"</code> which aims to minimize <code class="reqn">\sum_{i,t} | e_{it} |</code>.
For a pre-specified tolerance value <code class="reqn">t</code> (in <code>a_trim</code> or <code>b_trim</code>),
the approach <code>estimation="L0"</code> minimizes the number of interactions
(i.e., DIF effects) in the <code class="reqn">e_{it}</code> effects. In more detail, it minimizes
<code class="reqn">\sum_{i,t} \# \{ | e_{it} | &gt; t \} </code> which is computationally conducted
by repeatedly applying the median polish procedure in which one cell is
omitted (Davies, 2012; Terbeck &amp; Davies, 1998).
</p>
<p>Effect sizes of invariance are calculated as R-squared measures
of explained item slopes and intercepts after linking
in comparison to item parameters across groups
(Asparouhov &amp; Muthen, 2014).
</p>
<p>The function <code class="reqn">linking.haberman.lq</code> uses the loss function <code class="reqn">\rho(x)=|x|^q</code>.
The originally proposed Haberman linking can be obtained with <code>pow=2</code> (<code class="reqn">q=2</code>).
The powers can also be estimated (argument <code>est_pow=TRUE</code>).
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>transf.pars</code></td>
<td>
<p>Data frame with transformation parameters
<code class="reqn">A_t</code> and <code class="reqn">B_t</code>
</p>
</td></tr>
<tr><td><code>transf.personpars</code></td>
<td>
<p>Data frame with linear transformation functions
for person parameters</p>
</td></tr>
<tr><td><code>joint.itempars</code></td>
<td>
<p>Estimated joint item parameters <code class="reqn">a_i</code> and <code class="reqn">b_i</code>
</p>
</td></tr>
<tr><td><code>a.trans</code></td>
<td>
<p>Transformed <code class="reqn">a_{it}</code> parameters</p>
</td></tr>
<tr><td><code>b.trans</code></td>
<td>
<p>Transformed <code class="reqn">b_{it}</code> parameters</p>
</td></tr>
<tr><td><code>a.orig</code></td>
<td>
<p>Original <code class="reqn">a_{it}</code> parameters</p>
</td></tr>
<tr><td><code>b.orig</code></td>
<td>
<p>Original <code class="reqn">b_{it}</code> parameters</p>
</td></tr>
<tr><td><code>a.resid</code></td>
<td>
<p>Residual <code class="reqn">a_{it}</code> parameters (DIF parameters)</p>
</td></tr>
<tr><td><code>b.resid</code></td>
<td>
<p>Residual <code class="reqn">b_{it}</code> parameters (DIF parameters)</p>
</td></tr>
<tr><td><code>personpars</code></td>
<td>
<p>Transformed person parameters</p>
</td></tr>
<tr><td><code>es.invariance</code></td>
<td>
<p>Effect size measures of invariance,
separately for item slopes and intercepts.
In the rows, <code class="reqn">R^2</code> and <code class="reqn">\sqrt{1-R^2}</code> are reported.</p>
</td></tr>
<tr><td><code>es.robust</code></td>
<td>
<p>Effect size measures of invariance based on
robust estimation (if used).</p>
</td></tr>
<tr><td><code>selitems</code></td>
<td>
<p>Indices of items which are present in more than one
study.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Asparouhov, T., &amp; Muthen, B. (2014). Multiple-group factor analysis alignment.
<em>Structural Equation Modeling, 21</em>(4), 1-14.
<a href="https://doi.org/10.1080/10705511.2014.919210">doi:10.1080/10705511.2014.919210</a>
</p>
<p>Battauz, M. (2017). Multiple equating of separate IRT calibrations.
<em>Psychometrika, 82</em>(3), 610-636.
<a href="https://doi.org/10.1007/s11336-016-9517-x">doi:10.1007/s11336-016-9517-x</a>
</p>
<p>Davies, P. L. (2012). Interactions in the analysis of variance.
<em>Journal of the American Statistical Association, 107</em>(500), 1502-1509.
<a href="https://doi.org/10.1080/01621459.2012.726895">doi:10.1080/01621459.2012.726895</a>
</p>
<p>Fox, J. (2015). <em>Applied regression analysis and generalized linear models</em>.
Thousand Oaks: Sage.
</p>
<p>Haberman, S. J. (2009). <em>Linking parameter estimates derived
from an item response model through separate calibrations</em>.
ETS Research Report ETS RR-09-40. Princeton, ETS.
<a href="https://doi.org/10.1002/j.2333-8504.2009.tb02197.x">doi:10.1002/j.2333-8504.2009.tb02197.x</a>
</p>
<p>Kolen, M. J., &amp; Brennan, R. L. (2014). <em>Test equating, scaling, and linking:
Methods and practices</em>. New York: Springer.
<a href="https://doi.org/10.1007/978-1-4939-0317-7">doi:10.1007/978-1-4939-0317-7</a>
</p>
<p>Magis, D., &amp; De Boeck, P. (2012). A robust outlier approach to prevent type I error
inflation in differential item functioning.
<em>Educational and Psychological Measurement, 72</em>(2), 291-311.
<a href="https://doi.org/10.1177/0013164411416975">doi:10.1177/0013164411416975</a>
</p>
<p>Maronna, R. A., Martin, R. D., &amp; Yohai, V. J. (2006). <em>Robust statistics</em>.
West Sussex: Wiley. <a href="https://doi.org/10.1002/0470010940">doi:10.1002/0470010940</a>
</p>
<p>Terbeck, W., &amp; Davies, P. L. (1998). Interactions and outliers in the two-way
analysis of variance. <em>Annals of Statistics, 26</em>(4), 1279-1305.
doi: 10.1214/aos/1024691243
</p>
<p>Tukey, J. W. (1977). <em>Exploratory data analysis</em>. Addison-Wesley.
</p>
<p>Weeks, J. P. (2010). <span class="pkg">plink</span>: An <span class="rlang"><b>R</b></span> package for linking mixed-format tests
using IRT-based methods. <code>Journal of Statistical Software, 35</code>(12), 1-33.
<a href="https://doi.org/10.18637/jss.v035.i12">doi:10.18637/jss.v035.i12</a>
</p>


<h3>See Also</h3>

<p>See the <span class="pkg">plink</span> package (Weeks, 2010) for a diversity of linking methods.
</p>
<p>Mean-mean linking, Stocking-Lord and Haebara linking (see Kolen &amp; Brennan, 2014,
for an overview) in the generalized logistic item response model can be conducted with
<code><a href="#topic+equating.rasch">equating.rasch</a></code>. See also <code><a href="TAM.html#topic+tam.linking">TAM::tam.linking</a></code>
in the <span class="pkg">TAM</span> package. Haebara linking and a robustified version of it can be
found in <code><a href="#topic+linking.haebara">linking.haebara</a></code>.
</p>
<p>The invariance alignment method employs an optimization function based on
pairwise loss functions of item parameters (Asparouhov &amp; Muthen, 2014),
see <code><a href="#topic+invariance.alignment">invariance.alignment</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Item parameters data.pars1.rasch and data.pars1.2pl
#############################################################################

# Model 1: Linking three studies calibrated by the Rasch model
data(data.pars1.rasch)
mod1 &lt;- sirt::linking.haberman( itempars=data.pars1.rasch )
summary(mod1)

# Model 1b: Linking these studies but weigh these studies by
#     proportion weights 3 : 0.5 : 1 (see below).
#     All weights are the same for each item but they could also
#     be item specific.
itempars &lt;- data.pars1.rasch
itempars$wgt &lt;- 1
itempars[ itempars$study=="study1","wgt"] &lt;- 3
itempars[ itempars$study=="study2","wgt"] &lt;- .5
mod1b &lt;- sirt::linking.haberman( itempars=itempars )
summary(mod1b)

# Model 2: Linking three studies calibrated by the 2PL model
data(data.pars1.2pl)
mod2 &lt;- sirt::linking.haberman( itempars=data.pars1.2pl )
summary(mod2)

# additive model instead of logarithmic model for item slopes
mod2b &lt;- sirt::linking.haberman( itempars=data.pars1.2pl, a_log=FALSE )
summary(mod2b)

## Not run: 
#############################################################################
# EXAMPLE 2: Linking longitudinal data
#############################################################################
data(data.long)

#******
# Model 1: Scaling with the 1PL model

# scaling at T1
dat1 &lt;- data.long[, grep("T1", colnames(data.long) ) ]
resT1 &lt;- sirt::rasch.mml2( dat1 )
itempartable1 &lt;- data.frame( "study"="T1", resT1$item[, c("item", "a", "b" ) ] )
# scaling at T2
dat2 &lt;- data.long[, grep("T2", colnames(data.long) ) ]
resT2 &lt;- sirt::rasch.mml2( dat2 )
summary(resT2)
itempartable2 &lt;- data.frame( "study"="T2", resT2$item[, c("item", "a", "b" ) ] )
itempartable &lt;- rbind( itempartable1, itempartable2 )
itempartable[,2] &lt;- substring( itempartable[,2], 1, 2 )
# estimate linking parameters
mod1 &lt;- sirt::linking.haberman( itempars=itempartable )

#******
# Model 2: Scaling with the 2PL model

# scaling at T1
dat1 &lt;- data.long[, grep("T1", colnames(data.long) ) ]
resT1 &lt;- sirt::rasch.mml2( dat1, est.a=1:6)
itempartable1 &lt;- data.frame( "study"="T1", resT1$item[, c("item", "a", "b" ) ] )

# scaling at T2
dat2 &lt;- data.long[, grep("T2", colnames(data.long) ) ]
resT2 &lt;- sirt::rasch.mml2( dat2, est.a=1:6)
summary(resT2)
itempartable2 &lt;- data.frame( "study"="T2", resT2$item[, c("item", "a", "b" ) ] )
itempartable &lt;- rbind( itempartable1, itempartable2 )
itempartable[,2] &lt;- substring( itempartable[,2], 1, 2 )
# estimate linking parameters
mod2 &lt;- sirt::linking.haberman( itempars=itempartable )

#############################################################################
# EXAMPLE 3: 2 Studies - 1PL and 2PL linking
#############################################################################
set.seed(789)
I &lt;- 20        # number of items
N &lt;- 2000       # number of persons
# define item parameters
b &lt;- seq( -1.5, 1.5, length=I )
# simulate data
dat1 &lt;- sirt::sim.raschtype( stats::rnorm( N, mean=0,sd=1 ), b=b )
dat2 &lt;- sirt::sim.raschtype( stats::rnorm( N, mean=0.5,sd=1.50 ), b=b )

#*** Model 1: 1PL
# 1PL Study 1
mod1 &lt;- sirt::rasch.mml2( dat1, est.a=rep(1,I) )
summary(mod1)
# 1PL Study 2
mod2 &lt;- sirt::rasch.mml2( dat2, est.a=rep(1,I) )
summary(mod2)

# collect item parameters
dfr1 &lt;- data.frame( "study1", mod1$item$item, mod1$item$a, mod1$item$b )
dfr2 &lt;- data.frame( "study2", mod2$item$item, mod2$item$a, mod2$item$b )
colnames(dfr2) &lt;- colnames(dfr1) &lt;- c("study", "item", "a", "b" )
itempars &lt;- rbind( dfr1, dfr2 )

# Haberman linking
linkhab1 &lt;- sirt::linking.haberman(itempars=itempars)
  ## Transformation parameters (Haberman linking)
  ##    study    At     Bt
  ## 1 study1 1.000  0.000
  ## 2 study2 1.465 -0.512
  ##
  ## Linear transformation for item parameters a and b
  ##    study   A_a   A_b    B_b
  ## 1 study1 1.000 1.000  0.000
  ## 2 study2 0.682 1.465 -0.512
  ##
  ## Linear transformation for person parameters theta
  ##    study A_theta B_theta
  ## 1 study1   1.000   0.000
  ## 2 study2   1.465   0.512
  ##
  ## R-Squared Measures of Invariance
  ##        slopes intercepts
  ## R2          1     0.9979
  ## sqrtU2      0     0.0456

#*** Model 2: 2PL
# 2PL Study 1
mod1 &lt;- sirt::rasch.mml2( dat1, est.a=1:I )
summary(mod1)
# 2PL Study 2
mod2 &lt;- sirt::rasch.mml2( dat2, est.a=1:I )
summary(mod2)

# collect item parameters
dfr1 &lt;- data.frame( "study1", mod1$item$item, mod1$item$a, mod1$item$b )
dfr2 &lt;- data.frame( "study2", mod2$item$item, mod2$item$a, mod2$item$b )
colnames(dfr2) &lt;- colnames(dfr1) &lt;- c("study", "item", "a", "b" )
itempars &lt;- rbind( dfr1, dfr2 )

# Haberman linking
linkhab2 &lt;- sirt::linking.haberman(itempars=itempars)
  ## Transformation parameters (Haberman linking)
  ##    study    At     Bt
  ## 1 study1 1.000  0.000
  ## 2 study2 1.468 -0.515
  ##
  ## Linear transformation for item parameters a and b
  ##    study   A_a   A_b    B_b
  ## 1 study1 1.000 1.000  0.000
  ## 2 study2 0.681 1.468 -0.515
  ##
  ## Linear transformation for person parameters theta
  ##    study A_theta B_theta
  ## 1 study1   1.000   0.000
  ## 2 study2   1.468   0.515
  ##
  ## R-Squared Measures of Invariance
  ##        slopes intercepts
  ## R2     0.9984     0.9980
  ## sqrtU2 0.0397     0.0443

#############################################################################
# EXAMPLE 4: 3 Studies - 1PL and 2PL linking
#############################################################################
set.seed(789)
I &lt;- 20         # number of items
N &lt;- 1500       # number of persons
# define item parameters
b &lt;- seq( -1.5, 1.5, length=I )
# simulate data
dat1 &lt;- sirt::sim.raschtype( stats::rnorm( N, mean=0, sd=1), b=b )
dat2 &lt;- sirt::sim.raschtype( stats::rnorm( N, mean=0.5, sd=1.50), b=b )
dat3 &lt;- sirt::sim.raschtype( stats::rnorm( N, mean=-0.2, sd=0.8), b=b )
# set some items to non-administered
dat3 &lt;- dat3[, -c(1,4) ]
dat2 &lt;- dat2[, -c(1,2,3) ]

#*** Model 1: 1PL in sirt
# 1PL Study 1
mod1 &lt;- sirt::rasch.mml2( dat1, est.a=rep(1,ncol(dat1)) )
summary(mod1)
# 1PL Study 2
mod2 &lt;- sirt::rasch.mml2( dat2, est.a=rep(1,ncol(dat2)) )
summary(mod2)
# 1PL Study 3
mod3 &lt;- sirt::rasch.mml2( dat3, est.a=rep(1,ncol(dat3)) )
summary(mod3)

# collect item parameters
dfr1 &lt;- data.frame( "study1", mod1$item$item, mod1$item$a, mod1$item$b )
dfr2 &lt;- data.frame( "study2", mod2$item$item, mod2$item$a, mod2$item$b )
dfr3 &lt;- data.frame( "study3", mod3$item$item, mod3$item$a, mod3$item$b )
colnames(dfr3) &lt;- colnames(dfr2) &lt;- colnames(dfr1) &lt;- c("study", "item", "a", "b" )
itempars &lt;- rbind( dfr1, dfr2, dfr3 )

# use person parameters
personpars &lt;- list( mod1$person[, c("EAP","SE.EAP") ], mod2$person[, c("EAP","SE.EAP") ],
    mod3$person[, c("EAP","SE.EAP") ] )

# Haberman linking
linkhab1 &lt;- sirt::linking.haberman(itempars=itempars, personpars=personpars)
# compare item parameters
round( cbind( linkhab1$joint.itempars[,-1], linkhab1$b.trans )[1:5,], 3 )
  ##            aj     bj study1 study2 study3
  ##   I0001 0.998 -1.427 -1.427     NA     NA
  ##   I0002 0.998 -1.290 -1.324     NA -1.256
  ##   I0003 0.998 -1.140 -1.068     NA -1.212
  ##   I0004 0.998 -0.986 -1.003 -0.969     NA
  ##   I0005 0.998 -0.869 -0.809 -0.872 -0.926

# summary of person parameters of second study
round( psych::describe( linkhab1$personpars[[2]] ), 2 )
  ##   var    n mean   sd median trimmed  mad   min  max range  skew kurtosis
  ## EAP      1 1500 0.45 1.36   0.41    0.47 1.52 -2.61 3.25  5.86 -0.08    -0.62
  ## SE.EAP   2 1500 0.57 0.09   0.53    0.56 0.04  0.49 0.84  0.35  1.47     1.56
  ##          se
  ## EAP    0.04
  ## SE.EAP 0.00

#*** Model 2: 2PL in TAM
library(TAM)
# 2PL Study 1
mod1 &lt;- TAM::tam.mml.2pl( resp=dat1, irtmodel="2PL" )
pvmod1 &lt;- TAM::tam.pv(mod1, ntheta=300, normal.approx=TRUE) # draw plausible values
summary(mod1)
# 2PL Study 2
mod2 &lt;- TAM::tam.mml.2pl( resp=dat2, irtmodel="2PL" )
pvmod2 &lt;- TAM::tam.pv(mod2, ntheta=300, normal.approx=TRUE)
summary(mod2)
# 2PL Study 3
mod3 &lt;- TAM::tam.mml.2pl( resp=dat3, irtmodel="2PL" )
pvmod3 &lt;- TAM::tam.pv(mod3, ntheta=300, normal.approx=TRUE)
summary(mod3)

# collect item parameters
#!!  Note that in TAM the parametrization is a*theta - b while linking.haberman
#!!  needs the parametrization a*(theta-b)
dfr1 &lt;- data.frame( "study1", mod1$item$item, mod1$B[,2,1], mod1$xsi$xsi / mod1$B[,2,1] )
dfr2 &lt;- data.frame( "study2", mod2$item$item, mod2$B[,2,1], mod2$xsi$xsi / mod2$B[,2,1] )
dfr3 &lt;- data.frame( "study3", mod3$item$item, mod3$B[,2,1], mod3$xsi$xsi / mod3$B[,2,1] )
colnames(dfr3) &lt;- colnames(dfr2) &lt;- colnames(dfr1) &lt;- c("study", "item", "a", "b" )
itempars &lt;- rbind( dfr1, dfr2, dfr3 )

# define list containing person parameters
personpars &lt;- list(  pvmod1$pv[,-1], pvmod2$pv[,-1], pvmod3$pv[,-1] )

# Haberman linking
linkhab2 &lt;- sirt::linking.haberman(itempars=itempars,personpars=personpars)
  ##   Linear transformation for person parameters theta
  ##      study A_theta B_theta
  ##   1 study1   1.000   0.000
  ##   2 study2   1.485   0.465
  ##   3 study3   0.786  -0.192

# extract transformed person parameters
personpars.trans &lt;- linkhab2$personpars

#############################################################################
# EXAMPLE 5: Linking with simulated item parameters containing outliers
#############################################################################

# simulate some parameters
I &lt;- 38
set.seed(18785)
b &lt;- stats::rnorm( I, mean=.3, sd=1.4 )
# simulate DIF effects plus some outliers
bdif &lt;- stats::rnorm(I,mean=.4,sd=.09)+( stats::runif(I)&gt;.9 )* rep( 1*c(-1,1)+.4, each=I/2 )
# create item parameter table
itempars &lt;- data.frame( "study"=paste0("study",rep(1:2, each=I)),
                "item"=paste0( "I", 100 + rep(1:I,2) ), "a"=1,
                 "b"=c( b, b + bdif  )  )

#*** Model 1: Haberman linking with least squares regression
mod1 &lt;- sirt::linking.haberman( itempars=itempars )
summary(mod1)

#*** Model 2: Haberman linking with robust bisquare regression with fixed trimming value
mod2 &lt;- sirt::linking.haberman( itempars=itempars, estimation="BSQ", b_trim=.4)
summary(mod2)

#*** Model 2: Haberman linking with robust bisquare regression with estimated trimming value
mod3 &lt;- sirt::linking.haberman( itempars=itempars, estimation="BSQ")
summary(mod3)

## see also Example 3 of ?sirt::robust.linking

#############################################################################
# EXAMPLE 6: Toy example of Magis and De Boeck (2012)
#############################################################################

# define item parameters from Magis &amp; De Boeck (20212, p. 293)
b1 &lt;- c(1,1,1,1)
b2 &lt;- c(1,1,1,2)
itempars &lt;- data.frame(study=rep(1:2, each=4), item=rep(1:4,2), a=1, b=c(b1,b2) )

#- Least squares regression
mod1 &lt;- sirt::linking.haberman( itempars=itempars, estimation="OLS")
summary(mod1)

#- Bisquare regression with estimated and fixed trimming factors
mod2 &lt;- sirt::linking.haberman( itempars=itempars, estimation="BSQ")
mod2a &lt;- sirt::linking.haberman( itempars=itempars, estimation="BSQ", b_trim=.4)
mod2b &lt;- sirt::linking.haberman( itempars=itempars, estimation="BSQ", b_trim=1.2)
summary(mod2)
summary(mod2a)
summary(mod2b)

#- Least squares trimmed regression
mod3 &lt;- sirt::linking.haberman( itempars=itempars, estimation="LTS")
summary(mod3)

#- median regression
mod4 &lt;- sirt::linking.haberman( itempars=itempars, estimation="MED")
summary(mod4)

#############################################################################
# EXAMPLE 7: Simulated example with directional DIF
#############################################################################

set.seed(98)
I &lt;- 8
mu &lt;- c(-.5, 0, .5)
b &lt;- sample(seq(-1.5,1.5, len=I))
sd_dif &lt;- 0.001
pars &lt;- outer(b, mu, "+") + stats::rnorm(I*3, sd=sd_dif)
ind &lt;- c(1,2); pars[ind,1] &lt;- pars[ind,1] + c(.5,.5)
ind &lt;- c(3,4); pars[ind,2] &lt;- pars[ind,2] + (-1)*c(.6,.6)
ind &lt;- c(5,6); pars[ind,3] &lt;- pars[ind,3] + (-1)*c(1,1)

# median polish (=stats::medpolish())
tmod1 &lt;- sirt:::L1_polish(x=pars)
# L0 polish with tolerance criterion of .3
tmod2 &lt;- sirt::L0_polish(x=pars, tol=.3)

#- prepare itempars input
itempars &lt;- sirt::linking_haberman_itempars_prepare(b=pars)

#- compare different estimation functions for Haberman linking
mod01 &lt;- sirt::linking.haberman(itempars, estimation="L1")
mod02 &lt;- sirt::linking.haberman(itempars, estimation="L0", b_trim=.3)
mod1 &lt;- sirt::linking.haberman(itempars, estimation="OLS")
mod2 &lt;- sirt::linking.haberman(itempars, estimation="BSQ")
mod2a &lt;- sirt::linking.haberman(itempars, estimation="BSQ", b_trim=.4)
mod3 &lt;- sirt::linking.haberman(itempars, estimation="MED")
mod4 &lt;- sirt::linking.haberman(itempars, estimation="LTS")
mod5 &lt;- sirt::linking.haberman(itempars, estimation="HUB")
mod01$transf.pars
mod02$transf.pars
mod1$transf.pars
mod2$transf.pars
mod2a$transf.pars
mod3$transf.pars
mod4$transf.pars
mod5$transf.pars

#############################################################################
# EXAMPLE 8: Many studies and directional DIF
#############################################################################

## dataset 2
set.seed(98)
I &lt;- 10 # number of items
S &lt;- 7  # number of studies
mu &lt;- round( seq(0, 1, len=S))
b &lt;- sample(seq(-1.5,1.5, len=I))
sd_dif &lt;- 0.001
pars0 &lt;- pars &lt;- outer(b, mu, "+") + stats::rnorm(I*S, sd=sd_dif)

# select n_dif items at random per group and set it to dif or -dif
n_dif &lt;- 2
dif &lt;- .6
for (ss in 1:S){
    ind &lt;- sample( 1:I, n_dif )
    pars[ind,ss] &lt;- pars[ind,ss] + dif*sign( runif(1) - .5 )
}

# check DIF
pars - pars0

#* estimate models
itempars &lt;- sirt::linking_haberman_itempars_prepare(b=pars)
mod0 &lt;- sirt::linking.haberman(itempars, estimation="L0", b_trim=.2)
mod1 &lt;- sirt::linking.haberman(itempars, estimation="OLS")
mod2 &lt;- sirt::linking.haberman(itempars, estimation="BSQ")
mod2a &lt;- sirt::linking.haberman(itempars, estimation="BSQ", b_trim=.4)
mod3 &lt;- sirt::linking.haberman(itempars, estimation="MED")
mod3a &lt;- sirt::linking.haberman(itempars, estimation="L1")
mod4 &lt;- sirt::linking.haberman(itempars, estimation="LTS")
mod5 &lt;- sirt::linking.haberman(itempars, estimation="HUB")
mod0$transf.pars
mod1$transf.pars
mod2$transf.pars
mod2a$transf.pars
mod3$transf.pars
mod3a$transf.pars
mod4$transf.pars
mod5$transf.pars

#* compare results with Haebara linking
mod11 &lt;- sirt::linking.haebara(itempars, dist="L2")
mod12 &lt;- sirt::linking.haebara(itempars, dist="L1")
summary(mod11)
summary(mod12)

## End(Not run)
</code></pre>

<hr>
<h2 id='linking.haebara'>
Haebara Linking of the 2PL Model for Multiple Studies
</h2><span id='topic+linking.haebara'></span><span id='topic+summary.linking.haebara'></span>

<h3>Description</h3>

<p>The function <code>linking.haebara</code> is a generalization of Haebara linking
of the 2PL model to multiple groups (or multiple studies; see Battauz, 2017,
for a similar approach). The optimization estimates transformation parameters for
means and standard deviations of the groups and joint item parameters.
The function allows two different distance functions <code>dist="L2"</code> and
<code>dist="L1"</code> where the latter is a robustified version of
Haebara linking (see Details; He, Cui, &amp; Osterlind, 2015; He &amp; Cui, 2020;
Hu, Rogers, &amp; Vukmirovic, 2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linking.haebara(itempars, dist="L2", theta=seq(-4,4, length=61),
        optimizer="optim", center=FALSE, eps=1e-3, par_init=NULL, use_rcpp=TRUE,
        pow=2, use_der=TRUE, ...)

## S3 method for class 'linking.haebara'
summary(object, digits=3, file=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linking.haebara_+3A_itempars">itempars</code></td>
<td>

<p>A data frame with four or five columns. The first four columns contain
in the order: study name, item name, <code class="reqn">a</code> parameter, <code class="reqn">b</code> parameter.
The fifth column is an optional weight for every item and every study. See
<code><a href="#topic+linking.haberman">linking.haberman</a></code> for a function which uses the same argument.
</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_dist">dist</code></td>
<td>
<p>Distance function. Options are <code>"L2"</code> for squared loss and
<code>"L1"</code> for absolute value loss.</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_theta">theta</code></td>
<td>
<p>Grid of theta points for 2PL item response functions</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_optimizer">optimizer</code></td>
<td>
<p>Name of the optimizer chosen for alignment. Options are
<code>"optim"</code> (using <code><a href="stats.html#topic+optim">stats::optim</a></code>)
or <code>"nlminb"</code> (using <code><a href="stats.html#topic+nlminb">stats::nlminb</a></code>).
</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_center">center</code></td>
<td>
<p>Logical indicating whether means and standard deviations should
be centered after estimation</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_eps">eps</code></td>
<td>
<p>Small value for smooth approximation of the absolute value function</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_par_init">par_init</code></td>
<td>
<p>Optional vector of initial parameter estimates</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_use_rcpp">use_rcpp</code></td>
<td>
<p>Logical indicating whether <span class="pkg">Rcpp</span> is used for computation</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_pow">pow</code></td>
<td>
<p>Power for method <code>dist="Lq"</code></p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_use_der">use_der</code></td>
<td>
<p>Logical indicating whether analytical derivative should be used</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_object">object</code></td>
<td>
<p>Object of class <code>linking.haabara</code>.</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_digits">digits</code></td>
<td>
<p>Number of digits after decimals for rounding in <code>summary</code>.</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_file">file</code></td>
<td>
<p>Optional file name if <code>summary</code> should be sunk into a file.</p>
</td></tr>
<tr><td><code id="linking.haebara_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code class="reqn">t=1,\ldots,T</code> studies, item difficulties <code class="reqn">b_{it}</code> and
item slopes <code class="reqn">a_{it}</code> are available. The 2PL item response functions are given by
</p>
<p style="text-align: center;"><code class="reqn"> logit P(X_{pi}=1| \theta_p )=a_i ( \theta_p - b_i ) </code>
</p>

<p>Haebara linking compares the observed item response functions <code class="reqn">P_{it}</code>
based on the equation for the logits <code class="reqn">a_{it}(\theta - b_{it})</code> and the expected
item response functions <code class="reqn">P_{it}^\ast</code> based on the equation for the logits
<code class="reqn">a_i^\ast \sigma_t ( \theta - ( b_i - \mu_t)/\sigma_t )</code> where the joint
item parameters <code class="reqn">a_i</code> and <code class="reqn">b_i</code> and means <code class="reqn">\mu_t</code> and standard
deviations <code class="reqn">\sigma_t</code> are estimated.
</p>
<p>Two loss functions are implemented. The quadratic loss of Haebara linking
(<code>dist="L2"</code>) minimizes
</p>
<p style="text-align: center;"><code class="reqn">f_{opt, L2}=\sum_t \sum_i \int ( P_{it} (\theta ) - P_{it}^\ast (\theta ) )^2 w(\theta)</code>
</p>

<p>was originally proposed by Haebara. A robustified version (<code>dist="L1"</code>)
uses the optimization function (He et al., 2015)
</p>
<p style="text-align: center;"><code class="reqn">f_{opt, L1}=\sum_t \sum_i \int | P_{it} (\theta ) - P_{it}^\ast (\theta ) |  w(\theta)</code>
</p>

<p>As a further generalization, the follwing distance function (<code>dist="Lp"</code>)
can be minimized:
</p>
<p style="text-align: center;"><code class="reqn">f_{opt, Lp}=\sum_t \sum_i \int | P_{it} (\theta ) - P_{it}^\ast (\theta ) |^p  w(\theta)</code>
</p>



<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>pars</code></td>
<td>
<p>Estimated means and standard deviations (transformation parameters)</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Estimated joint item parameters</p>
</td></tr>
<tr><td><code>a.orig</code></td>
<td>
<p>Original <code class="reqn">a_{it}</code> parameters</p>
</td></tr>
<tr><td><code>b.orig</code></td>
<td>
<p>Original <code class="reqn">b_{it}</code> parameters</p>
</td></tr>
<tr><td><code>a.resid</code></td>
<td>
<p>Residual <code class="reqn">a_{it}</code> parameters (DIF parameters)</p>
</td></tr>
<tr><td><code>b.resid</code></td>
<td>
<p>Residual <code class="reqn">b_{it}</code> parameters (DIF parameters)</p>
</td></tr>
<tr><td><code>res_optim</code></td>
<td>
<p>Value of optimization routine</p>
</td></tr>
</table>


<h3>References</h3>

<p>Battauz, M. (2017). Multiple equating of separate IRT calibrations.
<em>Psychometrika, 82</em>, 610-636.
<a href="https://doi.org/10.1007/s11336-016-9517-x">doi:10.1007/s11336-016-9517-x</a>
</p>
<p>He, Y., Cui, Z., &amp; Osterlind, S. J. (2015). New robust scale transformation methods
in the presence of outlying common items. <em>Applied Psychological Measurement, 39</em>(8),
613-626.
<a href="https://doi.org/10.1177/0146621615587003">doi:10.1177/0146621615587003</a>
</p>
<p>He, Y., &amp; Cui, Z. (2020). Evaluating robust scale transformation methods with multiple
outlying common items under IRT true score equating.
<em>Applied Psychological Measurement, 44</em>(4), 296-310.
<a href="https://doi.org/10.1177/0146621619886050">doi:10.1177/0146621619886050</a>
</p>
<p>Hu, H., Rogers, W. T., &amp; Vukmirovic, Z. (2008). Investigation of IRT-based equating
methods in the presence of outlier common items. <em>Applied Psychological Measurement,
32</em>(4), 311-333.
<a href="https://doi.org/10.1177/0146621606292215">doi:10.1177/0146621606292215</a>
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+invariance.alignment">invariance.alignment</a></code> and <code><a href="#topic+linking.haberman">linking.haberman</a></code>
for alternative linking methods in the <span class="pkg">sirt</span> package. See also
<code>TAM::tam.linking</code> in the <span class="pkg">TAM</span> package for more linking functionality
and the <span class="rlang"><b>R</b></span> packages <span class="pkg">plink</span>, <span class="pkg">equateIRT</span>, <span class="pkg">equateMultiple</span> and
<span class="pkg">SNSequate</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Robust linking methods in the presence of outliers
#############################################################################

#** simulate data
I &lt;- 10
a &lt;- seq(.9, 1.1, len=I)
b &lt;- seq(-2, 2, len=I)

#- define item parameters
item_names &lt;- paste0("I",100+1:I)
# th=SIG*TH+MU=&gt; logit(p)=a*(SIG*TH+MU-b)=a*SIG*(TH-(-MU)/SIG-b/SIG)
d1 &lt;- data.frame( study="S1", item=item_names, a=a, b=b )
mu &lt;- .5; sigma &lt;- 1.3
d2 &lt;- data.frame( study="S2", item=item_names, a=a*sigma, b=(b-mu)/sigma )
mu &lt;- -.3; sigma &lt;- .7
d3 &lt;- data.frame( study="S3", item=item_names, a=a*sigma, b=(b-mu)/sigma )

#- define DIF effect
# dif &lt;- 0  # no DIF effects
dif &lt;- 1
d2[4,"a"] &lt;- d2[4,"a"] * (1-.8*dif)
d3[5,"b"] &lt;- d3[5,"b"] - 2*dif
itempars &lt;- rbind(d1, d2, d3)

#* Haebara linking non-robust
mod1 &lt;- sirt::linking.haebara( itempars, dist="L2", control=list(trace=2) )
summary(mod1)

#* Haebara linking robust
mod2 &lt;- sirt::linking.haebara( itempars, dist="L1", control=list(trace=2) )
summary(mod2)

#* using initial parameter estimates
par_init &lt;- mod1$res_optim$par
mod2b &lt;- sirt::linking.haebara( itempars, dist="L1", par_init=par_init)
summary(mod2b)

#* power p=.25
mod2c &lt;- sirt::linking.haebara( itempars, dist="Lp", pow=.25, par_init=par_init)
summary(mod2c)

#* Haberman linking non-robust
mod3 &lt;- sirt::linking.haberman(itempars)
summary(mod3)

#* Haberman linking robust
mod4 &lt;- sirt::linking.haberman(itempars, estimation="BSQ", a_trim=.25, b_trim=.5)
summary(mod4)

#* compare transformation parameters (means and standard deviations)
mod1$pars
mod2$pars
mod3$transf.personpars
mod4$transf.personpars

## End(Not run)
</code></pre>

<hr>
<h2 id='linking.robust'>
Robust Linking of Item Intercepts
</h2><span id='topic+linking.robust'></span><span id='topic+summary.linking.robust'></span><span id='topic+plot.linking.robust'></span>

<h3>Description</h3>

<p>This function implements a robust alternative of mean-mean linking which
employs trimmed means instead of means.
The linking constant is calculated for varying trimming parameters <code class="reqn">k</code>.
The treatment of differential item functioning as outliers and application of
robust statistics is discussed in Magis and De Boeck (2011, 2012).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linking.robust(itempars)

## S3 method for class 'linking.robust'
summary(object,...)

## S3 method for class 'linking.robust'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linking.robust_+3A_itempars">itempars</code></td>
<td>

<p>Data frame of item parameters (item intercepts). The first column
contains the item label, the 2nd and 3rd columns item parameters of
two studies.
</p>
</td></tr>
<tr><td><code id="linking.robust_+3A_object">object</code></td>
<td>
<p>Object of class <code>linking.robust</code></p>
</td></tr>
<tr><td><code id="linking.robust_+3A_x">x</code></td>
<td>
<p>Object of class <code>linking.robust</code></p>
</td></tr>
<tr><td><code id="linking.robust_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>ind.kopt</code></td>
<td>
<p>Index for optimal scale parameter</p>
</td></tr>
<tr><td><code>kopt</code></td>
<td>
<p>Optimal scale parameter</p>
</td></tr>
<tr><td><code>meanpars.kopt</code></td>
<td>
<p>Linking constant for optimal scale parameter</p>
</td></tr>
<tr><td><code>se.kopt</code></td>
<td>
<p>Standard error for linking constant obtained with optimal scale parameter</p>
</td></tr>
<tr><td><code>meanpars</code></td>
<td>
<p>Linking constant dependent on the scale parameter</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>Standard error of the linking constant dependent on the scale parameter</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>DIF standard deviation (non-robust estimate)</p>
</td></tr>
<tr><td><code>mad</code></td>
<td>
<p>DIF standard deviation (robust estimate using the MAD measure)</p>
</td></tr>
<tr><td><code>pars</code></td>
<td>
<p>Original item parameters</p>
</td></tr>
<tr><td><code>k.robust</code></td>
<td>
<p>Used vector of scale parameters</p>
</td></tr>
<tr><td><code>I</code></td>
<td>
<p>Number of items</p>
</td></tr>
<tr><td><code>itempars</code></td>
<td>
<p>Used data frame of item parameters</p>
</td></tr>
</table>


<h3>References</h3>

<p>Magis, D., &amp; De Boeck, P. (2011). Identification of differential item functioning in
multiple-group settings: A multivariate outlier detection approach.
<em>Multivariate Behavioral Research, 46</em>(5), 733-755.
<a href="https://doi.org/10.1080/00273171.2011.606757">doi:10.1080/00273171.2011.606757</a>
</p>
<p>Magis, D., &amp; De Boeck, P. (2012). A robust outlier approach to prevent type I error
inflation in differential item functioning.
<em>Educational and Psychological Measurement, 72</em>(2), 291-311.
<a href="https://doi.org/10.1177/0013164411416975">doi:10.1177/0013164411416975</a>
</p>


<h3>See Also</h3>

<p>Other functions for linking: <code><a href="#topic+linking.haberman">linking.haberman</a></code>,
<code><a href="#topic+equating.rasch">equating.rasch</a></code>
</p>
<p>See also the <span class="pkg">plink</span> package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Linking data.si03
#############################################################################

data(data.si03)
res1 &lt;- sirt::linking.robust( itempars=data.si03 )
summary(res1)
  ##   Number of items=27
  ##   Optimal trimming parameter k=8 |  non-robust parameter k=0
  ##   Linking constant=-0.0345 |  non-robust estimate=-0.056
  ##   Standard error=0.0186 |  non-robust estimate=0.027
  ##   DIF SD: MAD=0.0771 (robust) | SD=0.1405 (non-robust)
plot(res1)

## Not run: 
#############################################################################
# EXAMPLE 2: Linking PISA item parameters data.pisaPars
#############################################################################

data(data.pisaPars)

# Linking with items
res2 &lt;- sirt::linking.robust( data.pisaPars[, c(1,3,4)] )
summary(res2)
  ##   Optimal trimming parameter k=0 |  non-robust parameter k=0
  ##   Linking constant=-0.0883 |  non-robust estimate=-0.0883
  ##   Standard error=0.0297 |  non-robust estimate=0.0297
  ##   DIF SD: MAD=0.1824 (robust) | SD=0.1487 (non-robust)
##  -&gt; no trimming is necessary for reducing the standard error
plot(res2)

#############################################################################
# EXAMPLE 3: Linking with simulated item parameters containing outliers
#############################################################################

# simulate some parameters
I &lt;- 38
set.seed(18785)
itempars &lt;- data.frame("item"=paste0("I",1:I) )
itempars$study1 &lt;- stats::rnorm( I, mean=.3, sd=1.4 )
# simulate DIF effects plus some outliers
bdif &lt;- stats::rnorm(I,mean=.4,sd=.09)+( stats::runif(I)&gt;.9 )* rep( 1*c(-1,1)+.4, each=I/2 )
itempars$study2 &lt;- itempars$study1 + bdif

# robust linking
res &lt;- sirt::linking.robust( itempars )
summary(res)
  ##   Number of items=38
  ##   Optimal trimming parameter k=12 |  non-robust parameter k=0
  ##   Linking constant=-0.4285 |  non-robust estimate=-0.5727
  ##   Standard error=0.0218 |  non-robust estimate=0.0913
  ##   DIF SD: MAD=0.1186 (robust) | SD=0.5628 (non-robust)
## -&gt; substantial differences of estimated linking constants in this case of
##    deviations from normality of item parameters
plot(res)

## End(Not run)
</code></pre>

<hr>
<h2 id='lq_fit'>
Fit of a <code class="reqn">L_q</code> Regression Model
</h2><span id='topic+lq_fit'></span><span id='topic+lq_fit_estimate_power'></span><span id='topic+dexppow'></span><span id='topic+rexppow'></span>

<h3>Description</h3>

<p>Fits a regression model in the <code class="reqn">L_q</code> norm (also labeled as the <code class="reqn">L_p</code> norm).
In more detail,
the optimization function <code class="reqn"> \sum_i | y_i - x_i \beta | ^p</code> is optimized.
The nondifferentiable function is approximated by a differentiable approximation,
i.e., we use <code class="reqn">|x| \approx \sqrt{x^2 + \varepsilon } </code>. The power <code class="reqn">p</code>
can also be estimated by using <code>est_pow=TRUE</code>, see
Giacalone, Panarello and Mattera (2018). The algorithm iterates between estimating
regression coefficients and the estimation of power values. The estimation of the
power based on a vector of residuals <code>e</code> can be conducted using the
function <code>lq_fit_estimate_power</code>.
</p>
<p>Using the <code class="reqn">L_q</code> norm in the regression is equivalent to assuming an expontial
power function for residuals (Giacalone et al., 2018). The density function and
a simulation function is provided by <code>dexppow</code> and <code>rexppow</code>, respectively.
See also the <span class="pkg">normalp</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lq_fit(y, X, w=NULL, pow=2, eps=0.001, beta_init=NULL, est_pow=FALSE, optimizer="optim",
    eps_vec=10^seq(0,-10, by=-.5), conv=1e-4, miter=20, lower_pow=.1, upper_pow=5)

lq_fit_estimate_power(e, pow_init=2, lower_pow=.1, upper_pow=10)

dexppow(x, mu=0, sigmap=1, pow=2, log=FALSE)

rexppow(n, mu=0, sigmap=1, pow=2, xbound=100, xdiff=.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lq_fit_+3A_y">y</code></td>
<td>

<p>Dependent variable
</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_x">X</code></td>
<td>

<p>Design matrix
</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_w">w</code></td>
<td>

<p>Optional vector of weights
</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_pow">pow</code></td>
<td>

<p>Power <code class="reqn">p</code> in <code class="reqn">L_q</code> norm
</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_est_pow">est_pow</code></td>
<td>

<p>Logical indicating whether power should be estimated
</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_eps">eps</code></td>
<td>

<p>Parameter governing the differentiable approximation
</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_e">e</code></td>
<td>
<p>Vector of resiuals</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_pow_init">pow_init</code></td>
<td>
<p>Initial value of power</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_beta_init">beta_init</code></td>
<td>

<p>Initial vector
</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_optimizer">optimizer</code></td>
<td>

<p>Can be <code>"optim"</code> or <code>"nlminb"</code>.
</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_eps_vec">eps_vec</code></td>
<td>
<p>Vector with decreasing <code class="reqn">\varepsilon</code> values used in
optimization</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_conv">conv</code></td>
<td>
<p>Convergence criterion</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_miter">miter</code></td>
<td>
<p>Maximum number of iterations</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_lower_pow">lower_pow</code></td>
<td>
<p>Lower bound for estimated power</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_upper_pow">upper_pow</code></td>
<td>
<p>Upper bound for estimated power</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_x">x</code></td>
<td>
<p>Vector</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_mu">mu</code></td>
<td>
<p>Location parameter</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_sigmap">sigmap</code></td>
<td>
<p>Scale parameter</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_log">log</code></td>
<td>
<p>Logical indicating whether the logarithm should be provided</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_n">n</code></td>
<td>
<p>Sample size</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_xbound">xbound</code></td>
<td>
<p>Lower and upper bound for density approximation</p>
</td></tr>
<tr><td><code id="lq_fit_+3A_xdiff">xdiff</code></td>
<td>
<p>Grid width for density approximation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with following several entries
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>Vector of coefficients</p>
</td></tr>
<tr><td><code>res_optim</code></td>
<td>
<p>Results of optimization</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>More values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Giacalone, M., Panarello, D., &amp; Mattera, R. (2018).
Multicollinearity in regression: an efficiency comparison between $L_p$-norm and least
squares estimators. <em>Quality &amp; Quantity, 52</em>(4), 1831-1859.
<a href="https://doi.org/10.1007/s11135-017-0571-y">doi:10.1007/s11135-017-0571-y</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Small simulated example with fixed power
#############################################################################

set.seed(98)
N &lt;- 300
x1 &lt;- stats::rnorm(N)
x2 &lt;- stats::rnorm(N)
par1 &lt;- c(1,.5,-.7)
y &lt;- par1[1]+par1[2]*x1+par1[3]*x2 + stats::rnorm(N)
X &lt;- cbind(1,x1,x2)

#- lm function in stats
mod1 &lt;- stats::lm.fit(y=y, x=X)

#- use lq_fit function
mod2 &lt;- sirt::lq_fit( y=y, X=X, pow=2, eps=1e-4)
mod1$coefficients
mod2$coefficients

## Not run: 
#############################################################################
# EXAMPLE 2: Example with estimated power values
#############################################################################

#*** simulate regression model with residuals from the exponential power distribution
#*** using a power of .30
set.seed(918)
N &lt;- 2000
X &lt;- cbind( 1, c(rep(1,N), rep(0,N)) )
e &lt;- sirt::rexppow(n=2*N, pow=.3, xdiff=.01, xbound=200)
y &lt;- X %*% c(1,.5) + e

#*** estimate model
mod &lt;- sirt::lq_fit( y=y, X=X, est_pow=TRUE, lower_pow=.1)
mod1 &lt;- stats::lm( y ~ 0 + X )
mod$coefficients
mod$pow
mod1$coefficients

## End(Not run)
</code></pre>

<hr>
<h2 id='lsdm'>
Least Squares Distance Method of Cognitive Validation
</h2><span id='topic+lsdm'></span><span id='topic+summary.lsdm'></span><span id='topic+plot.lsdm'></span>

<h3>Description</h3>

<p>This function estimates the least squares distance method
of cognitive validation (Dimitrov, 2007; Dimitrov &amp; Atanasov, 2012)
which assumes a multiplicative relationship of attribute response
probabilities to explain item response probabilities. The argument <code>distance</code>
allows the estimation of a squared loss function (<code>distance="L2"</code>)
and an absolute value loss function (<code>distance="L1"</code>).
</p>
<p>The function also estimates the classical linear logistic test
model (LLTM; Fischer, 1973) which assumes a linear relationship
for item difficulties in the Rasch model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsdm(data, Qmatrix, theta=seq(-3,3,by=.5), wgt_theta=rep(1, length(theta)), distance="L2",
   quant.list=c(0.5,0.65,0.8), b=NULL, a=rep(1,nrow(Qmatrix)), c=rep(0,nrow(Qmatrix)) )

## S3 method for class 'lsdm'
summary(object, file=NULL, digits=3, ...)

## S3 method for class 'lsdm'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsdm_+3A_data">data</code></td>
<td>

<p>An <code class="reqn">I \times L</code> matrix of dichotomous item responses.
The <code>data</code> consists of <code class="reqn">I</code> item response functions
(parametrically or nonparametrically estimated) which are
evaluated at a discrete grid of <code class="reqn">L</code> <code>theta</code> values
(person parameters)
and are specified in the argument <code>theta</code>.
</p>
</td></tr>
<tr><td><code id="lsdm_+3A_qmatrix">Qmatrix</code></td>
<td>

<p>An <code class="reqn">I \times K</code> matrix where the allocation of items
to attributes is coded. Values of zero and one and all values between
zero and one are permitted. There must not be any items with only zero
Q-matrix entries in a row.
</p>
</td></tr>
<tr><td><code id="lsdm_+3A_theta">theta</code></td>
<td>

<p>The discrete grid points <code class="reqn">\theta</code> where item response functions
are evaluated for doing the LSDM method.
</p>
</td></tr>
<tr><td><code id="lsdm_+3A_wgt_theta">wgt_theta</code></td>
<td>
<p>Optional vector for weights of discrete <code class="reqn">\theta</code> points</p>
</td></tr>
<tr><td><code id="lsdm_+3A_quant.list">quant.list</code></td>
<td>

<p>A vector of quantiles where attribute response functions are
evaluated.
</p>
</td></tr>
<tr><td><code id="lsdm_+3A_distance">distance</code></td>
<td>
<p>Type of distance function for minimizing the discrepancy between
observed and expected item response functions. Options are <code>"L2"</code> which is the
squared distance (proposed in the original LSDM formulation in Dimitrov, 2007)
and the absolute value distance <code>"L1"</code> (see Details).
</p>
</td></tr>
<tr><td><code id="lsdm_+3A_b">b</code></td>
<td>

<p>An optional vector of item difficulties. If it is specified,
then no <code>data</code> input is necessary.
</p>
</td></tr>
<tr><td><code id="lsdm_+3A_a">a</code></td>
<td>

<p>An optional vector of item discriminations.
</p>
</td></tr>
<tr><td><code id="lsdm_+3A_c">c</code></td>
<td>

<p>An optional vector of guessing parameters.
</p>
</td></tr>
<tr><td><code id="lsdm_+3A_object">object</code></td>
<td>
<p>Object of class <code>lsdm</code></p>
</td></tr>
<tr><td><code id="lsdm_+3A_file">file</code></td>
<td>
<p>Optional file name for <code>summary</code> output</p>
</td></tr>
<tr><td><code id="lsdm_+3A_digits">digits</code></td>
<td>
<p>Number of digits aftert decimal in <code>summary</code></p>
</td></tr>
<tr><td><code id="lsdm_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
<tr><td><code id="lsdm_+3A_x">x</code></td>
<td>
<p>Object of class <code>lsdm</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The least squares distance method (LSDM; Dimitrov 2007) is based on the
assumption that estimated item response functions <code class="reqn">P(X_i=1 | \theta)</code>
can be decomposed in a multiplicative way (in the implemented
conjunctive model):
</p>
<p style="text-align: center;"><code class="reqn"> P( X_i=1  | \theta ) \approx \prod_{k=1}^K [ P( A_k=1 | \theta ) ]^{q_{ik}} </code>
</p>

<p>where <code class="reqn">P( A_k=1 | \theta )</code> are attribute response functions and
<code class="reqn">q_{ik}</code> are entries of the Q-matrix. Note that the multiplicative form
can be rewritten by taking the logarithm
</p>
<p style="text-align: center;"><code class="reqn"> \log P( X_i=1  | \theta ) \approx
    \sum_{k=1}^K q_{ik} \log [ P( A_k=1 | \theta ) ] </code>
</p>

<p>The item and attribute response functions are evaluated on a grid of <code class="reqn">\theta</code> values.
Using the definitions of matrices <code class="reqn">\bold{L}=\{ \log P( X_i=1 ) | \theta ) \} </code>,
<code class="reqn">\bold{Q}=\{ q_{ik} \} </code> and
<code class="reqn">\bold{X}=\{ \log P( A_k=1  | \theta ) \} </code>, the estimation problem can be formulated
as <code class="reqn"> \bold{L} \approx \bold{Q} \bold{X}</code>. Two different loss functions for minimizing
the discrepancy between <code class="reqn"> \bold{L}</code> and <code class="reqn">\bold{Q} \bold{X}</code> are implemented.
First, the squared loss function computes the weighted difference
<code class="reqn">|| \bold{L} - \bold{Q} \bold{X}||_2=\sum_i ( l_i - \sum_t q_{it} x_{it})^2</code>
(<code>distance="L2"</code>) and has
been originally proposed by Dimitrov (2007). Second, the
absolute value loss function
<code class="reqn">|| \bold{L} - \bold{Q} \bold{X}||_1=\sum_i | l_i - \sum_t q_{it} x_{it} |</code>
(<code>distance="L1"</code>) is more robust to outliers (i.e., items which
show misfit to the assumed multiplicative LSDM formulation).
</p>
<p>After fitting the attribute response functions, empirical item-attribute
discriminations <code class="reqn">w_{ik}</code> are calculated as the approximation of the following
equation
</p>
<p style="text-align: center;"><code class="reqn"> \log P( X_i=1  | \theta )=
\sum_{k=1}^K w_{ik} q_{ik} \log [ P( A_k=1 | \theta ) ] </code>
</p>



<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>mean.mad.lsdm0</code></td>
<td>
<p>Mean of <code class="reqn">MAD</code> statistics for LSDM</p>
</td></tr>
<tr><td><code>mean.mad.lltm</code></td>
<td>
<p>Mean of <code class="reqn">MAD</code> statistics for LLTM</p>
</td></tr>
<tr><td><code>attr.curves</code></td>
<td>
<p>Estimated attribute response curves evaluated at <code>theta</code></p>
</td></tr>
<tr><td><code>attr.pars</code></td>
<td>
<p>Estimated attribute parameters for LSDM and LLTM</p>
</td></tr>
<tr><td><code>data.fitted</code></td>
<td>
<p>LSDM-fitted item response functions evaluated at <code>theta</code></p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>Grid of ability distributions at which
functions are evaluated</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Item statistics (p value, <code class="reqn">MAD</code>, ...)</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>Estimated or fixed item response functions evaluated at <code>theta</code></p>
</td></tr>
<tr><td><code>Qmatrix</code></td>
<td>
<p>Used Q-matrix</p>
</td></tr>
<tr><td><code>lltm</code></td>
<td>
<p>Model output of LLTM (<code>lm</code> values)</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>Matrix with empirical item-attribute discriminations</p>
</td></tr>
</table>


<h3>References</h3>

<p>Al-Shamrani, A., &amp; Dimitrov, D. M. (2016). Cognitive diagnostic analysis of reading
comprehension items: The case of English proficiency assessment in Saudi Arabia.
<em>International Journal of School and Cognitive Psychology, 4</em>(3). 1000196.
http://dx.doi.org/10.4172/2469-9837.1000196
</p>
<p>DiBello, L. V., Roussos, L. A., &amp; Stout, W. F. (2007). Review of
cognitively diagnostic assessment and a summary of psychometric models.
In C. R. Rao and S. Sinharay (Eds.), <em>Handbook of Statistics</em>,
Vol. 26 (pp. 979-1030). Amsterdam: Elsevier.
</p>
<p>Dimitrov, D. M. (2007). Least squares distance method of cognitive validation
and analysis for binary items using their item response theory parameters.
<em>Applied Psychological Measurement, 31</em>, 367-387.
http://dx.doi.org/10.1177/0146621606295199
</p>
<p>Dimitrov, D. M., &amp; Atanasov, D. V. (2012). Conjunctive and disjunctive
extensions of the least squares distance model of cognitive diagnosis.
<em>Educational and Psychological Measurement, 72</em>, 120-138.
http://dx.doi.org/10.1177/0013164411402324
</p>
<p>Dimitrov, D. M., Gerganov, E. N., Greenberg, M., &amp; Atanasov, D. V. (2008).
<em>Analysis of cognitive attributes for mathematics items in the framework of
Rasch measurement</em>. AERA 2008, New York.
</p>
<p>Fischer, G. H. (1973). The linear logistic test model as an instrument
in educational research. <em>Acta Psychologica, 37</em>, 359-374.
http://dx.doi.org/10.1016/0001-6918(73)90003-6
</p>
<p>Sonnleitner, P. (2008). Using the LLTM to evaluate an item-generating system
for reading comprehension. <em>Psychology Science, 50</em>, 345-362.
</p>


<h3>See Also</h3>

<p>Get a summary of the LSDM analysis with <code><a href="#topic+summary.lsdm">summary.lsdm</a></code>.
</p>
<p>See the <span class="pkg">CDM</span> package for the estimation of related
cognitive diagnostic models (DiBello, Roussos &amp; Stout, 2007).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset Fischer (see Dimitrov, 2007)
#############################################################################

# item difficulties
b &lt;- c( 0.171,-1.626,-0.729,0.137,0.037,-0.787,-1.322,-0.216,1.802,
    0.476,1.19,-0.768,0.275,-0.846,0.213,0.306,0.796,0.089,
    0.398,-0.887,0.888,0.953,-1.496,0.905,-0.332,-0.435,0.346,
    -0.182,0.906)
# read Q-matrix
Qmatrix &lt;- c( 1,1,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,
    1,0,1,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,1,1,0,0,1,0,1,0,1,0,0,0,
    1,0,1,0,1,1,0,0,1,0,1,1,0,1,0,0,1,0,0,1,0,1,0,0,1,0,1,1,1,0,0,0,
    1,0,0,1,0,0,1,0,1,0,0,1,0,0,1,0,1,0,1,0,0,0,1,0,1,1,0,1,0,1,1,0,
    1,0,1,1,0,0,1,0,1,0,0,1,0,0,0,1,1,0,1,1,0,0,0,1,1,0,0,1,0,0,0,1,
    0,1,0,0,0,1,0,1,1,1,0,1,0,1,0,1,1,0,0,1,0,1,0,0,1,1,0,0,1,0,0,0,
    1,0,0,1,1,0,0,0,1,1,0,1,0,0,0,0,1,0,1,1,0,0,0,0,1,0,1,1,0,1,0,0,
    1,1,0,1,0,0,0,0,1,0,1,1,1,1,0,0 )
Qmatrix &lt;- matrix( Qmatrix, nrow=29, byrow=TRUE )
colnames(Qmatrix) &lt;- paste("A",1:8,sep="")
rownames(Qmatrix) &lt;- paste("Item",1:29,sep="")

#* Model 1: perform a LSDM analysis with defaults
mod1 &lt;- sirt::lsdm( b=b, Qmatrix=Qmatrix )
summary(mod1)
plot(mod1)

#* Model 2: different theta values and weights
theta &lt;- seq(-4,4,len=31)
wgt_theta &lt;- stats::dnorm(theta)
mod2 &lt;- sirt::lsdm( b=b, Qmatrix=Qmatrix, theta=theta, wgt_theta=wgt_theta )
summary(mod2)

#* Model 3: absolute value distance function
mod3 &lt;- sirt::lsdm( b=b, Qmatrix=Qmatrix, distance="L1" )
summary(mod3)

#############################################################################
# EXAMPLE 2: Dataset Henning (see Dimitrov, 2007)
#############################################################################

# item difficulties
b &lt;- c(-2.03,-1.29,-1.03,-1.58,0.59,-1.65,2.22,-1.46,2.58,-0.66)
# item slopes
a &lt;- c(0.6,0.81,0.75,0.81,0.62,0.75,0.54,0.65,0.75,0.54)
# define Q-matrix
Qmatrix &lt;- c(1,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,1,0,0,0,0,1,1,0,0,
    0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,0,0,1,1,1,0,1,0,0 )
Qmatrix &lt;- matrix( Qmatrix, nrow=10, byrow=TRUE )
colnames(Qmatrix) &lt;- paste("A",1:5,sep="")
rownames(Qmatrix) &lt;- paste("Item",1:10,sep="")

# LSDM analysis
mod &lt;- sirt::lsdm( b=b, a=a, Qmatrix=Qmatrix )
summary(mod)

## Not run: 
#############################################################################
# EXAMPLE 3: PISA reading (data.pisaRead)
#    using nonparametrically estimated item response functions
#############################################################################

data(data.pisaRead)
# response data
dat &lt;- data.pisaRead$data
dat &lt;- dat[, substring( colnames(dat),1,1)=="R" ]
# define Q-matrix
pars &lt;- data.pisaRead$item
Qmatrix &lt;- data.frame(  "A0"=1*(pars$ItemFormat=="MC" ),
                  "A1"=1*(pars$ItemFormat=="CR" ) )

# start with estimating the 1PL in order to get person parameters
mod &lt;- sirt::rasch.mml2( dat )
theta &lt;- sirt::wle.rasch( dat=dat,b=mod$item$b )$theta
# Nonparametric estimation of item response functions
mod2 &lt;- sirt::np.dich( dat=dat, theta=theta, thetagrid=seq(-3,3,len=100) )

# LSDM analysis
lmod &lt;- sirt::lsdm( data=mod2$estimate, Qmatrix=Qmatrix, theta=mod2$thetagrid)
summary(lmod)
plot(lmod)

#############################################################################
# EXAMPLE 4: Fraction subtraction dataset
#############################################################################

data( data.fraction1, package="CDM")
data &lt;- data.fraction1$data
q.matrix &lt;- data.fraction1$q.matrix

#****
# Model 1: 2PL estimation
mod1 &lt;- sirt::rasch.mml2( data, est.a=1:nrow(q.matrix) )

# LSDM analysis
lmod1 &lt;- sirt::lsdm( b=mod1$item$b, a=mod1$item$a, Qmatrix=q.matrix )
summary(lmod1)

#****
# Model 2: 1PL estimation
mod2 &lt;- sirt::rasch.mml2(data)

# LSDM analysis
lmod2 &lt;- sirt::lsdm( b=mod1$item$b, Qmatrix=q.matrix )
summary(lmod2)

#############################################################################
# EXAMPLE 5: Dataset LLTM Sonnleitner Reading Comprehension (Sonnleitner, 2008)
#############################################################################

# item difficulties Table 7, p. 355 (Sonnleitner, 2008)
b &lt;- c(-1.0189,1.6754,-1.0842,-.4457,-1.9419,-1.1513,2.0871,2.4874,-1.659,-1.197,-1.2437,
    2.1537,.3301,-.5181,-1.3024,-.8248,-.0278,1.3279,2.1454,-1.55,1.4277,.3301)
b &lt;- b[-21] # remove Item 21

# Q-matrix Table 9, p. 357 (Sonnleitner, 2008)
Qmatrix &lt;- scan()
   1 0 0 0 0 0 0 7 4 0 0 0   0 1 0 0 0 0 0 5 1 0 0 0   1 1 0 1 0 0 0 9 1 0 1 0
   1 1 1 0 0 0 0 5 2 0 1 0   1 1 0 0 1 0 0 7 5 1 1 0   1 1 0 0 0 0 0 7 3 0 0 0
   0 1 0 0 0 0 2 6 1 0 0 0   0 0 0 0 0 0 2 6 1 0 0 0   1 0 0 0 0 0 1 7 4 1 0 0
   0 1 0 0 0 0 0 6 2 1 1 0   0 1 0 0 0 1 0 7 3 1 0 0   0 1 0 0 0 0 0 5 1 0 0 0
   0 0 0 0 0 1 0 4 1 0 0 1   0 0 0 0 0 0 0 6 1 0 1 1   0 0 1 0 0 0 0 6 3 0 1 1
   0 0 0 1 0 0 1 7 5 0 0 1   0 1 0 0 0 0 1 2 2 0 0 1   0 1 1 0 0 0 1 4 1 0 0 1
   0 1 0 0 1 0 0 5 1 0 0 1   0 1 0 0 0 0 1 7 2 0 0 1   0 0 0 0 0 1 0 5 1 0 0 1

Qmatrix &lt;- matrix( as.numeric(Qmatrix), nrow=21, ncol=12, byrow=TRUE )
colnames(Qmatrix) &lt;- scan( what="character", nlines=1)
   pc ic ier inc iui igc ch nro ncro td a t

# divide Q-matrix entries by maximum in each column
Qmatrix &lt;- round(Qmatrix / matrix(apply(Qmatrix,2,max),21,12,byrow=TRUE),3)
# LSDM analysis
mod &lt;- sirt::lsdm( b=b, Qmatrix=Qmatrix )
summary(mod)

#############################################################################
# EXAMPLE 6: Dataset Dimitrov et al. (2008)
#############################################################################

Qmatrix &lt;- scan()
1 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0

Qmatrix &lt;- matrix(Qmatrix, ncol=4, byrow=TRUE)
colnames(Qmatrix) &lt;- paste0("A",1:4)
rownames(Qmatrix) &lt;- paste0("I",1:9)

b &lt;- scan()
0.068 1.095 -0.641 -1.129 -0.061 1.218 1.244 -0.648 -1.146

# estimate model
mod &lt;- sirt::lsdm( b=b, Qmatrix=Qmatrix )
summary(mod)
plot(mod)

#############################################################################
# EXAMPLE 7: Dataset Al-Shamrani &amp; Dimitrov et al. (2017)
#############################################################################

I &lt;- 39  # number of items

Qmatrix &lt;- scan()
0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0
0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0
0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0
0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0
0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0
0 0 1 0 0 0 0 0 0 0 0 0 0 1 0

Qmatrix &lt;- matrix(Qmatrix, nrow=I, byrow=TRUE)
colnames(Qmatrix) &lt;- paste0("A",1:7)
rownames(Qmatrix) &lt;- paste0("I",1:I)

pars &lt;- scan()
1.952 0.9833 0.1816 1.1053 0.9631 0.1653 1.3904 1.3208 0.2545 0.7391 1.9367 0.2083 2.0833
1.8627 0.1873 1.4139 1.0107 0.2454 0.8274 0.9913 0.2137 1.0338 -0.0068 0.2368 2.4803
0.7939 0.1997 1.4867 1.1705 0.2541 1.4482 1.4176 0.2889 1.0789 0.8062 0.269 1.6258 1.1739
0.1723 1.5995 1.0936 0.2054 1.1814 1.0909 0.2623 2.0389 1.5023 0.2466 1.3636 1.1485 0.2059
1.8468 1.2755 0.192 1.9461 1.4947 0.2001 1.194 0.0889 0.2275 1.2114 0.8925 0.2367 2.0912
0.5961 0.2036 2.5769 1.3014 0.186 1.4554 1.2529 0.2423 1.4919 0.4763 0.2482 2.6787 1.7069
0.1796 1.5611 1.3991 0.2312 1.4353 0.678 0.1851 0.9127 1.3523 0.2525 0.6886 -0.3652 0.207
0.7039 -0.2494 0.2315 1.3683 0.8953 0.2326 1.4992 0.1025 0.2403 1.0727 0.2591 0.2152
1.3854 1.3802 0.2448 0.7748 0.4304 0.184 1.0218 1.8964 0.1949 1.5773 1.8934 0.2231 0.8631
1.4145 0.2132

pars &lt;- matrix(pars, nrow=I, byrow=TRUE)
colnames(pars) &lt;- c("a","b","c")
rownames(pars) &lt;- paste0("I",1:I)
pars &lt;- as.data.frame(pars)

#* Model 1: fit LSDM to 3PL curves (as in Al-Shamrani)
mod1 &lt;- sirt::lsdm(b=pars$b, a=pars$a, c=pars$c, Qmatrix=Qmatrix)
summary(mod1)
plot(mod1)

#* Model 2: fit LSDM to 2PL curves
mod2 &lt;- sirt::lsdm(b=pars$b, a=pars$a, Qmatrix=Qmatrix)
summary(mod2)
plot(mod2)

## End(Not run)
</code></pre>

<hr>
<h2 id='lsem.estimate'>
Local Structural Equation Models (LSEM)
</h2><span id='topic+lsem.estimate'></span><span id='topic+summary.lsem'></span><span id='topic+plot.lsem'></span><span id='topic+lsem.MGM.stepfunctions'></span><span id='topic+lsem_local_weights'></span><span id='topic+lsem.bootstrap'></span>

<h3>Description</h3>

<p>Local structural equation models (LSEM) are structural equation models (SEM)
which are evaluated for each value of a pre-defined moderator variable
(Hildebrandt et al., 2009, 2016).
As in nonparametric regression models, observations near a focal point - at
which the model is evaluated - obtain higher weights, far distant observations
obtain lower weights. The LSEM can be specified by making use of <span class="pkg">lavaan</span> syntax.
It is also possible to specify a discretized version of LSEM in which
values of the moderator are grouped and a multiple group SEM is specified.
The LSEM can be tested by employing a permutation test, see
<code><a href="#topic+lsem.permutationTest">lsem.permutationTest</a></code>.
</p>
<p>The function <code>lsem.MGM.stepfunctions</code> outputs stepwise functions
for a multiple group model evaluated at a grid of focal points of the
moderator, specified in <code>moderator.grid</code>.
</p>
<p>The argument <code>pseudo_weights</code> provides an ad hoc solution to estimate
an LSEM for any model which can be fitted in <span class="pkg">lavaan</span>.
</p>
<p>It is also possible to constrain some of the parameters along the values
of the moderator in a joint estimation approach (<code>est_joint=TRUE</code>). Parameter
names can be specified which are assumed to be invariant (in <code>par_invariant</code>).
In addition, linear or quadratic constraints can be imposed on
parameters (<code>par_linear</code> or <code>par_quadratic</code>).
</p>
<p>Statistical inference in case of joint estimation (but also for separate estimation)
can be conducted via bootstrap using the function <code>lsem.bootstrap</code>.
Bootstrap at the level of a cluster identifier is allowed (argument <code>cluster</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsem.estimate(data, moderator, moderator.grid, lavmodel, type="LSEM", h=1.1, bw=NULL,
    residualize=TRUE, fit_measures=c("rmsea", "cfi", "tli", "gfi", "srmr"),
    standardized=FALSE, standardized_type="std.all", lavaan_fct="sem",
    sufficient_statistics=TRUE, pseudo_weights=0,
    sampling_weights=NULL, loc_linear_smooth=TRUE, est_joint=FALSE, par_invariant=NULL,
    par_linear=NULL, par_quadratic=NULL, partable_joint=NULL, pw_linear=1,
    pw_quadratic=1, pd=TRUE, est_DIF=FALSE, se=NULL, kernel="gaussian",
    eps=1e-08, verbose=TRUE, ...)

## S3 method for class 'lsem'
summary(object, file=NULL, digits=3, ...)

## S3 method for class 'lsem'
plot(x, parindex=NULL, ask=TRUE, ci=TRUE, lintrend=TRUE,
       parsummary=TRUE, ylim=NULL, xlab=NULL,  ylab=NULL, main=NULL,
       digits=3, ...)

lsem.MGM.stepfunctions( object, moderator.grid )

# compute local weights
lsem_local_weights(data.mod, moderator.grid, h, sampling_weights=NULL,  bw=NULL,
     kernel="gaussian")

lsem.bootstrap(object, R=100, verbose=TRUE, cluster=NULL,
     repl_design=NULL, repl_factor=NULL, use_starting_values=TRUE,
     n.core=1, cl.type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsem.estimate_+3A_data">data</code></td>
<td>

<p>Data frame
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_moderator">moderator</code></td>
<td>

<p>Variable name of the moderator
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_moderator.grid">moderator.grid</code></td>
<td>

<p>Focal points at which the LSEM should be evaluated. If <code>type="MGM"</code>,
breaks are defined in this vector.
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_lavmodel">lavmodel</code></td>
<td>

<p>Specified SEM in <span class="pkg">lavaan</span>.
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_type">type</code></td>
<td>
<p>Type of estimated model. The default is <code>type="LSEM"</code> which means
that a local structural equation model is estimated.
A multiple group model with a discretized moderator as the
grouping variable can be estimated with <code>type="MGM"</code>. In this
case, the breaks must be defined in <code>moderator.grid</code>.
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_h">h</code></td>
<td>

<p>Bandwidth factor
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_bw">bw</code></td>
<td>
<p>Optional bandwidth parameter if <code>h</code> should not be used</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_residualize">residualize</code></td>
<td>
<p>Logical indicating whether a residualization
should be applied. </p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_fit_measures">fit_measures</code></td>
<td>

<p>Vector with names of fit measures following the labels in <span class="pkg">lavaan</span>
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_standardized">standardized</code></td>
<td>
<p>Optional logical indicating whether
standardized solution should be included as parameters in
the output using the
<code><a href="lavaan.html#topic+standardizedSolution">lavaan::standardizedSolution</a></code>
function. Standardized parameters are labeled as <code>std__</code>.
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_standardized_type">standardized_type</code></td>
<td>
<p>Type of standardization if <code>standardized=TRUE</code>.
The types are described in
<code><a href="lavaan.html#topic+standardizedSolution">lavaan::standardizedSolution</a></code>.
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_lavaan_fct">lavaan_fct</code></td>
<td>
<p>String whether
<code><a href="lavaan.html#topic+lavaan">lavaan::lavaan</a></code> (<code>lavaan_fct="lavaan"</code>),
<code><a href="lavaan.html#topic+sem">lavaan::sem</a></code> (<code>lavaan_fct="sem"</code>),
<code><a href="lavaan.html#topic+cfa">lavaan::cfa</a></code> (<code>lavaan_fct="cfa"</code>) or
<code><a href="lavaan.html#topic+growth">lavaan::growth</a></code> (<code>lavaan_fct="growth"</code>)
should be used.
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_sufficient_statistics">sufficient_statistics</code></td>
<td>
<p>Logical whether sufficient statistics of weighted
means and covariances should be used for model fitting. This option
can be set to <code>sufficient_statistics=FALSE</code>
if the data contain missing values. Note that the option
<code>sufficient_statistics=TRUE</code> is
only valid for (approximate) missing completely at random (MCAR) data.
The option can only be used for continuous data.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_pseudo_weights">pseudo_weights</code></td>
<td>
<p>Integer defining a target sample size. Local weights
are multiplied by a factor which is rounded to integers.
This approach is referred as a pseudo weighting approach.
For example, using <code>pseudo_weights=30000</code> implies
that the sum of local weights at each focal point is <code>30000</code>.
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_sampling_weights">sampling_weights</code></td>
<td>
<p>Optional vector of sampling weights</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_loc_linear_smooth">loc_linear_smooth</code></td>
<td>
<p>Logical indicating whether local linear
smoothing should be used for computing sufficient statistics for
means and covariances. The default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_est_joint">est_joint</code></td>
<td>
<p>Logical indicating whether LSEM should be estimated in a
joint estimation approach. This options only works wih continuous data and
sufficient statistics.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_par_invariant">par_invariant</code></td>
<td>
<p>Vector of invariant parameters</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_par_linear">par_linear</code></td>
<td>
<p>Vector of parameters with linear function</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_par_quadratic">par_quadratic</code></td>
<td>
<p>Vector of parameters with quadratic function</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_partable_joint">partable_joint</code></td>
<td>
<p>User-defined parameter table if joint estimation is
used (<code>est_joint=TRUE</code>).</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_pw_linear">pw_linear</code></td>
<td>
<p>Number of segments if piecewise linear estimation of parameters is used</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_pw_quadratic">pw_quadratic</code></td>
<td>
<p>Number of segments if piecewise quadratic estimation of parameters
is used</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_pd">pd</code></td>
<td>
<p>Logical indicating whether nearest positive definite covariance matrix
should be computed if sufficient statistics are used</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_est_dif">est_DIF</code></td>
<td>
<p>Logical indicating whether parameters under differential
item functioning (DIF) should be additionally
computed for invariant item parameters</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_se">se</code></td>
<td>
<p>Type of standard error used in <code>lavaan::lavaan</code>. If <code>NULL</code>,
the <span class="pkg">lavaan</span> default is used.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_kernel">kernel</code></td>
<td>
<p>Type of kernel function. Can be <code>"gaussian"</code>,
<code>"uniform"</code> or <code>"epanechnikov"</code>.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_eps">eps</code></td>
<td>
<p>Minimum number for weights</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_verbose">verbose</code></td>
<td>
<p>Optional logical printing information about computation progress.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_object">object</code></td>
<td>
<p>Object of class <code>lsem</code></p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_file">file</code></td>
<td>
<p>A file name in which the summary output will be written.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_digits">digits</code></td>
<td>
<p>Number of digits.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_x">x</code></td>
<td>
<p>Object of class <code>lsem</code>.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_parindex">parindex</code></td>
<td>
<p>Vector of indices for parameters in plot function.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_ask">ask</code></td>
<td>
<p>A logical which asks for changing the graphic for each parameter.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_ci">ci</code></td>
<td>
<p>Logical indicating whether confidence intervals should be plotted.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_lintrend">lintrend</code></td>
<td>
<p>Logical indicating whether a linear trend should be plotted.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_parsummary">parsummary</code></td>
<td>
<p>Logical indicating whether a parameter summary
should be displayed.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_ylim">ylim</code></td>
<td>
<p>Plot parameter <code>ylim</code>. Can be a list, see Examples.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_xlab">xlab</code></td>
<td>
<p>Plot parameter <code>xlab</code>. Can be a vector.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_ylab">ylab</code></td>
<td>
<p>Plot parameter <code>ylab</code>. Can be a vector.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_main">main</code></td>
<td>
<p>Plot parameter <code>main</code>. Can be a vector.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed to <code><a href="lavaan.html#topic+sem">lavaan::sem</a></code> or
<code><a href="lavaan.html#topic+lavaan">lavaan::lavaan</a></code>.
</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_data.mod">data.mod</code></td>
<td>
<p>Observed values of the moderator</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_r">R</code></td>
<td>
<p>Number of bootstrap samples</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_cluster">cluster</code></td>
<td>
<p>Optional variable name for bootstrap at the level of a cluster identifier</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_repl_design">repl_design</code></td>
<td>
<p>Optional matrix containing replication weights for computation of
standard errors. Note that sampling weights have to be already included in
<code>repl_design</code>.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_repl_factor">repl_factor</code></td>
<td>
<p>Replication factor in variance formula for statistical
inference, e.g., 0.05 in PISA.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_use_starting_values">use_starting_values</code></td>
<td>
<p>Logical indicating whether starting values should be
used from the original sample</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_n.core">n.core</code></td>
<td>
<p>A scalar indicating the number of cores that should be used.</p>
</td></tr>
<tr><td><code id="lsem.estimate_+3A_cl.type">cl.type</code></td>
<td>
<p>The cluster type.
Default value is <code>"PSOCK"</code>. Posix machines (Linux, Mac) generally benefit
from much faster cluster computation if type is set to <code>type="FORK"</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with following entries
</p>
<table>
<tr><td><code>parameters</code></td>
<td>
<p>Data frame with all parameters estimated at focal points of
moderator. Bias-corrected estimates under boostrap can be found in
the column <code>est_bc</code>.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>Data frame with weights at each focal point</p>
</td></tr>
<tr><td><code>parameters_summary</code></td>
<td>
<p>Summary table for estimated parameters</p>
</td></tr>
<tr><td><code>parametersM</code></td>
<td>
<p>Estimated parameters in matrix form. Parameters are in
columns and values of the grid of the moderator are in rows.</p>
</td></tr>
<tr><td><code>bw</code></td>
<td>
<p>Used bandwidth</p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p>Used bandwidth factor</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>Sample size</p>
</td></tr>
<tr><td><code>moderator.density</code></td>
<td>
<p>Estimated frequencies and effective sample size for
moderator at focal points</p>
</td></tr>
<tr><td><code>moderator.stat</code></td>
<td>
<p>Descriptive statistics for moderator</p>
</td></tr>
<tr><td><code>moderator</code></td>
<td>
<p>Variable name of moderator</p>
</td></tr>
<tr><td><code>moderator.grid</code></td>
<td>
<p>Used grid of focal points for moderator</p>
</td></tr>
<tr><td><code>moderator.grouped</code></td>
<td>
<p>Data frame with informations about grouping of
moderator if <code>type="MGM"</code>.</p>
</td></tr>
<tr><td><code>residualized.intercepts</code></td>
<td>
<p>Estimated intercept functions used for
residualization.</p>
</td></tr>
<tr><td><code>lavmodel</code></td>
<td>
<p>Used lavaan model</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>Used data frame, possibly residualized if <code>residualize=TRUE</code></p>
</td></tr>
<tr><td><code>model_parameters</code></td>
<td>
<p>Model parameters in LSEM</p>
</td></tr>
<tr><td><code>parameters_boot</code></td>
<td>
<p>Parameter values in each bootstrap sample
(for <code>lsem.bootstrap</code>)</p>
</td></tr>
<tr><td><code>fitstats_joint_boot</code></td>
<td>
<p>Fit statistics in each bootstrap sample
(for <code>lsem.bootstrap</code>)</p>
</td></tr>
<tr><td><code>dif_effects</code></td>
<td>
<p>Estimated item parameters under DIF</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexander Robitzsch, Oliver Luedtke, Andrea Hildebrandt
</p>


<h3>References</h3>

<p>Hildebrandt, A., Luedtke, O., Robitzsch, A., Sommer, C., &amp;
Wilhelm, O. (2016). Exploring factor model parameters across continuous variables
with local structural equation models.
<em>Multivariate Behavioral Research, 51</em>(2-3), 257-278.
<a href="https://doi.org/10.1080/00273171.2016.1142856">doi:10.1080/00273171.2016.1142856</a>
</p>
<p>Hildebrandt, A., Wilhelm, O., &amp; Robitzsch, A. (2009). Complementary and
competing factor analytic approaches for the investigation of measurement invariance.
<em>Review of Psychology, 16</em>, 87-102.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+lsem.permutationTest">lsem.permutationTest</a></code> for conducting a permutation test
and <code><a href="#topic+lsem.test">lsem.test</a></code> for applying a Wald test to a bootstrapped LSEM model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: data.lsem01 | Age differentiation
#############################################################################

data(data.lsem01, package="sirt")
dat &lt;- data.lsem01

# specify lavaan model
lavmodel &lt;- "
        F=~ v1+v2+v3+v4+v5
        F ~~ 1*F"

# define grid of moderator variable age
moderator.grid &lt;- seq(4,23,1)

#********************************
#*** Model 1: estimate LSEM with bandwidth 2
mod1 &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
               lavmodel=lavmodel, h=2, std.lv=TRUE)
summary(mod1)
plot(mod1, parindex=1:5)

# perform permutation test for Model 1
pmod1 &lt;- sirt::lsem.permutationTest( mod1, B=10 )
          # only for illustrative purposes the number of permutations B is set
          # to a low number of 10
summary(pmod1)
plot(pmod1, type="global")

#* perform permutation test with parallel computation
pmod1a &lt;- sirt::lsem.permutationTest( mod1, B=10, n.core=3 )
summary(pmod1a)

#** estimate Model 1 based on pseudo weights
mod1b &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
               lavmodel=lavmodel, h=2, std.lv=TRUE, pseudo_weights=50 )
summary(mod1b)

#** estimation with sampling weights

# generate random sampling weights
set.seed(987)
weights &lt;- stats::runif(nrow(dat), min=.4, max=3 )
mod1c &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
               lavmodel=lavmodel, h=2, sampling_weights=weights)
summary(mod1c)

#********************************
#*** Model 2: estimate multiple group model with 4 age groups

# define breaks for age groups
moderator.grid &lt;- seq( 3.5, 23.5, len=5) # 4 groups
# estimate model
mod2 &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
           lavmodel=lavmodel, type="MGM", std.lv=TRUE)
summary(mod2)

# output step functions
smod2 &lt;- sirt::lsem.MGM.stepfunctions( object=mod2, moderator.grid=seq(4,23,1) )
str(smod2)

#********************************
#*** Model 3: define standardized loadings as derived variables

# specify lavaan model
lavmodel &lt;- "
        F=~ a1*v1+a2*v2+a3*v3+a4*v4
        v1 ~~ s1*v1
        v2 ~~ s2*v2
        v3 ~~ s3*v3
        v4 ~~ s4*v4
        F ~~ 1*F
        # standardized loadings
        l1 :=a1 / sqrt(a1^2 + s1 )
        l2 :=a2 / sqrt(a2^2 + s2 )
        l3 :=a3 / sqrt(a3^2 + s3 )
        l4 :=a4 / sqrt(a4^2 + s4 )
        "
# estimate model
mod3 &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
               lavmodel=lavmodel, h=2, std.lv=TRUE)
summary(mod3)
plot(mod3)

#********************************
#*** Model 4: estimate LSEM and automatically include standardized solutions

lavmodel &lt;- "
        F=~ 1*v1+v2+v3+v4
        F ~~ F"
mod4 &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
               lavmodel=lavmodel, h=2, standardized=TRUE)
summary(mod4)
# permutation test (use only few permutations for testing purposes)
pmod1 &lt;- sirt::lsem.permutationTest( mod4, B=3 )

#**** compute LSEM local weights
wgt &lt;- sirt::lsem_local_weights(data.mod=dat$age, moderator.grid=moderator.grid,
             h=2)$weights
print(str(weights))

#********************************
#*** Model 5: invariance parameter constraints and other constraints

lavmodel &lt;- "
        F=~ 1*v1+v2+v3+v4
        F ~~ F"
moderator.grid &lt;- seq(4,23,4)

#- estimate model without constraints
mod5a &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
               lavmodel=lavmodel, h=2, standardized=TRUE)
summary(mod5a)
# extract parameter names
mod5a$model_parameters

#- invariance constraints on residual variances
par_invariant &lt;- c("F=~v2","v2~~v2")
mod5b &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
               lavmodel=lavmodel, h=2, standardized=TRUE, par_invariant=par_invariant)
summary(mod5b)

#- bootstrap for statistical inference
bmod5b &lt;- sirt::lsem.bootstrap(mod5b, R=100)
# inspect parameter values and standard errors
bmod5b$parameters

#- bootstrap using parallel computing (i.e., multiple cores)
bmod5ba &lt;- sirt::lsem.bootstrap(mod5b, R=100, n.core=3)

#- user-defined replication design
R &lt;- 100    # bootstrap samples
N &lt;- nrow(dat)
repl_design &lt;- matrix(0, nrow=N, ncol=R)
for (rr in 1:R){
    indices &lt;- sort( sample(1:N, replace=TRUE) )
    repl_design[,rr] &lt;- sapply(1:N, FUN=function(ii){ sum(indices==ii) } )
}
head(repl_design)
bmod5b1 &lt;- sirt::lsem.bootstrap(mod5a, repl_design=repl_design, repl_factor=1/R)

#- compare model mod5b with joint estimation without constraints
mod5c &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
               lavmodel=lavmodel, h=2, standardized=TRUE, est_joint=TRUE)
summary(mod5c)

#- linear and quadratic functions
par_invariant &lt;- c("F=~v1","v2~~v2")
par_linear &lt;- c("v1~~v1")
par_quadratic &lt;- c("v4~~v4")

mod5d &lt;- sirt::lsem.estimate( dat1, moderator="age", moderator.grid=moderator.grid,
            lavmodel=lavmodel, h=2, par_invariant=par_invariant, par_linear=par_linear,
            par_quadratic=par_quadratic)
summary(mod5d)

#- user-defined constraints: step functions for parameters

# inspect parameter table (from lavaan) of fitted model
pj &lt;- mod5d$partable_joint
#* modify parameter table for user-defined constraints
# define step function for F=~v1 which is constant on intervals 1:4 and 5:7
pj2 &lt;- pj[ pj$con==1, ]
pj2[ c(5,6), "lhs" ] &lt;- "p1g5"
pj2 &lt;- pj2[ -4, ]
partable_joint &lt;- rbind(pj1, pj2)
# estimate model with constraints
mod5e &lt;- lsem::lsem.estimate( dat1, moderator="age", moderator.grid=moderator.grid,
             lavmodel=lavmodel, h=2, std.lv=TRUE, estimator="ML",
             partable_joint=partable_joint)
summary(mod5e)

#############################################################################
# EXAMPLE 2: data.lsem01 | FIML with missing data
#############################################################################

data(data.lsem01)
dat &lt;- data.lsem01
# induce artifical missing values
set.seed(98)
dat[ stats::runif(nrow(dat)) &lt; .5, c("v1")] &lt;- NA
dat[ stats::runif(nrow(dat)) &lt; .25, c("v2")] &lt;- NA

# specify lavaan model
lavmodel1 &lt;- "
        F=~ v1+v2+v3+v4+v5
        F ~~ 1*F"

# define grid of moderator variable age
moderator.grid &lt;- seq(4,23,2)

#*** estimate LSEM with FIML
mod1 &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
                lavmodel=lavmodel1, h=2, std.lv=TRUE, estimator="ML", missing="fiml")
summary(mod1)

#############################################################################
# EXAMPLE 3: data.lsem01 | WLSMV estimation
#############################################################################

data(data.lsem01)
dat &lt;- data.lsem01

# create artificial dichotomous data
for (vv in 2:6){
dat[,vv] &lt;- 1*(dat[,vv] &gt; mean(dat[,vv]))
}

# specify lavaan model
lavmodel1 &lt;- "
        F=~ v1+v2+v3+v4+v5
        F ~~ 1*F
        v1 | t1
        v2 | t1
        v3 | t1
        v4 | t1
        v5 | t1
        "

# define grid of moderator variable age
moderator.grid &lt;- seq(4,23,2)

#*** local WLSMV estimation
mod1 &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
          lavmodel=lavmodel1, h=2, std.lv=TRUE, estimator="DWLS", ordered=paste0("v",1:5),
          residualize=FALSE, pseudo_weights=10000, parameterization="THETA" )
summary(mod1)

## End(Not run)
</code></pre>

<hr>
<h2 id='lsem.permutationTest'>
Permutation Test for a Local Structural Equation Model
</h2><span id='topic+lsem.permutationTest'></span><span id='topic+summary.lsem.permutationTest'></span><span id='topic+plot.lsem.permutationTest'></span>

<h3>Description</h3>

<p>Performs a permutation test for testing the hypothesis that model
parameter are independent of a moderator variable (see Hildebrandt,
Wilhelm, &amp; Robitzsch, 2009; Hildebrandt, Luedtke, Robitzsch, Sommer, &amp; Wilhelm, 2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsem.permutationTest(lsem.object, B=1000, residualize=TRUE, verbose=TRUE,
     n.core=1, cl.type="PSOCK")

## S3 method for class 'lsem.permutationTest'
summary(object, file=NULL, digits=3, ...)

## S3 method for class 'lsem.permutationTest'
plot(x, type="global", stattype="SD",
    parindex=NULL, sig_add=TRUE, sig_level=0.05, sig_pch=17, nonsig_pch=2,
    sig_cex=1, sig_lab="p value",  stat_lab="Test statistic",
    moderator_lab=NULL, digits=3, title=NULL, parlabels=NULL,
    ask=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsem.permutationTest_+3A_lsem.object">lsem.object</code></td>
<td>

<p>Fitted object of class <code>lsem</code> with <code><a href="#topic+lsem.estimate">lsem.estimate</a></code>
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_b">B</code></td>
<td>

<p>Number of permutation samples
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_residualize">residualize</code></td>
<td>

<p>Optional logical indicating whether residualization of the moderator
should be performed for each permutation sample.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_verbose">verbose</code></td>
<td>

<p>Optional logical printing information about computation progress.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_n.core">n.core</code></td>
<td>
<p>A scalar indicating the number of cores that should be used.</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_cl.type">cl.type</code></td>
<td>
<p>The cluster type.
Default value is <code>"PSOCK"</code>. Posix machines (Linux, Mac) generally benefit
from much faster cluster computation if type is set to <code>type="FORK"</code>.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_object">object</code></td>
<td>
<p>Object of class <code>lsem</code></p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_file">file</code></td>
<td>
<p>A file name in which the summary output will be written.</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_digits">digits</code></td>
<td>
<p>Number of digits.</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed.</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_x">x</code></td>
<td>

<p>Object of class <code>lsem</code>
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_type">type</code></td>
<td>

<p>Type of the statistic to be plotted. If <code>type="global"</code>, a global
test will be displayed. If <code>type="pointwise"</code> for each value at the
focal point (defined in <code>moderator.grid</code>) are calculated.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_stattype">stattype</code></td>
<td>

<p>Type of test statistics. Can be <code>MAD</code> (mean absolute deviation),
<code>SD</code> (standard deviation) or <code>lin_slo</code> (linear slope).
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_parindex">parindex</code></td>
<td>

<p>Vector of indices of selected parameters.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_sig_add">sig_add</code></td>
<td>

<p>Logical indicating whether significance values (p values) should
be displayed.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_sig_level">sig_level</code></td>
<td>

<p>Significance level.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_sig_pch">sig_pch</code></td>
<td>

<p>Point symbol for significant values.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_nonsig_pch">nonsig_pch</code></td>
<td>

<p>Point symbol for non-significant values.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_sig_cex">sig_cex</code></td>
<td>
<p>Point size for graphic displaying p values</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_sig_lab">sig_lab</code></td>
<td>

<p>Label for significance value (p value).
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_stat_lab">stat_lab</code></td>
<td>
<p>Label of y axis for graphic with pointwise test statistic</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_moderator_lab">moderator_lab</code></td>
<td>

<p>Label of the moderator.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_title">title</code></td>
<td>

<p>Title of the plot. Can be a vector.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_parlabels">parlabels</code></td>
<td>

<p>Labels of the parameters. Can be a vector.
</p>
</td></tr>
<tr><td><code id="lsem.permutationTest_+3A_ask">ask</code></td>
<td>
<p>A logical which asks for changing the graphic for each parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with following entries
</p>
<table>
<tr><td><code>teststat</code></td>
<td>
<p>Data frame with global test statistics. The statistics
are <code>SD</code>, <code>MAD</code> and <code>lin_slo</code> with their corresponding
p values.</p>
</td></tr>
<tr><td><code>parameters_pointwise_test</code></td>
<td>
<p>Data frame with pointwise test statistics.</p>
</td></tr>
<tr><td><code>parameters</code></td>
<td>
<p>Original parameters.</p>
</td></tr>
<tr><td><code>parameters</code></td>
<td>
<p>Parameters in permutation samples.</p>
</td></tr>
<tr><td><code>parameters_summary</code></td>
<td>
<p>Original parameter summary.</p>
</td></tr>
<tr><td><code>parameters_summary_M</code></td>
<td>
<p>Mean of each parameter in permutation sample.</p>
</td></tr>
<tr><td><code>parameters_summary_SD</code></td>
<td>
<p>Standard deviation (SD) statistic in permutation slope.</p>
</td></tr>
<tr><td><code>parameters_summary_MAD</code></td>
<td>
<p>Mean absolute deviation (MAD)
statistic in permutation sample.</p>
</td></tr>
<tr><td><code>parameters_summary_MAD</code></td>
<td>
<p>Linear slope parameter in permutation sample.</p>
</td></tr>
<tr><td><code>nonconverged_rate</code></td>
<td>
<p>Percentage of permuted dataset in which a LSEM model
did not converge</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexander Robitzsch, Oliver Luedtke, Andrea Hildebrandt
</p>


<h3>References</h3>

<p>Hildebrandt, A., Luedtke, O., Robitzsch, A., Sommer, C., &amp;
Wilhelm, O. (2016). Exploring factor model parameters across continuous variables
with local structural equation models.
<em>Multivariate Behavioral Research, 51</em>(2-3), 257-278.
<a href="https://doi.org/10.1080/00273171.2016.1142856">doi:10.1080/00273171.2016.1142856</a>
</p>
<p>Hildebrandt, A., Wilhelm, O., &amp; Robitzsch, A. (2009). Complementary and
competing factor analytic approaches for the investigation of measurement invariance.
<em>Review of Psychology, 16</em>, 87-102.
</p>


<h3>See Also</h3>

<p>For Examples see <code><a href="#topic+lsem.estimate">lsem.estimate</a></code>.
</p>

<hr>
<h2 id='lsem.test'>
Test a Local Structural Equation Model Based on Bootstrap
</h2><span id='topic+lsem.test'></span>

<h3>Description</h3>

<p>Performs global and parameter tests for a fitted local structural equation model.
The LSEM must have been fitted and bootstrap estimates of the LSEM model must be
available for statistical inference. The hypothesis of a constant parameter is tested
by means of a Wald test. Moreover, regression functions can be specified and tested
if these are specified in the argument <code>models</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsem.test(mod, bmod, models=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsem.test_+3A_mod">mod</code></td>
<td>

<p>Fitted LSEM object
</p>
</td></tr>
<tr><td><code id="lsem.test_+3A_bmod">bmod</code></td>
<td>

<p>Fitted LSEM bootstrap object. The argument <code>bmod</code> can also be missing.
</p>
</td></tr>
<tr><td><code id="lsem.test_+3A_models">models</code></td>
<td>

<p>List of model formulas for named LSEM model parameters
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with following entries
</p>
<table>
<tr><td><code>wald_test_global</code></td>
<td>
<p>Global Wald test for model parameters</p>
</td></tr>
<tr><td><code>test_models</code></td>
<td>
<p>Output for fitted regression models</p>
</td></tr>
<tr><td><code>parameters</code></td>
<td>
<p>Original model parameters after fitting (i.e., smoothing) a particular
parameter using a regression model specified in <code>models</code>.</p>
</td></tr>
<tr><td><code>parameters_boot</code></td>
<td>
<p>Bootstrapped model parameters after fitting (i.e., smoothing)
a particular parameter using a regression model specified in <code>models</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>See also <code><a href="#topic+lsem.estimate">lsem.estimate</a></code> for estimating LSEM models and
<code><a href="#topic+lsem.bootstrap">lsem.bootstrap</a></code> for bootstrapping LSEM models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: data.lsem01 | Age differentiation and tested models
#############################################################################

data(data.lsem01, package="sirt")
dat &lt;- data.lsem01

# specify lavaan model
lavmodel &lt;- "
        F=~ v1+v2+v3+v4+v5
        F ~~ 1*F
    "

# define grid of moderator variable age
moderator.grid &lt;- seq(4,23,1)

#-- estimate LSEM with bandwidth 2
mod &lt;- sirt::lsem.estimate( dat, moderator="age", moderator.grid=moderator.grid,
               lavmodel=lavmodel, h=2, std.lv=TRUE)
summary(mod1)

#-- bootstrap model
bmod &lt;- sirt::lsem.bootstrap(mod, R=200)

#-- test models
models &lt;- list( "F=~v1"=y ~ m + I(m^2),
                "F=~v2"=y ~ I( splines::bs(m, df=4) ) )
tmod &lt;- sirt::lsem.test(mod=mod, bmod=bmod, models=models)
str(tmod)
sirt::print_digits(wald_test_global, 3)
sirt::print_digits(test_models, 3)

## End(Not run)
</code></pre>

<hr>
<h2 id='marginal.truescore.reliability'>
True-Score Reliability for Dichotomous Data
</h2><span id='topic+marginal.truescore.reliability'></span>

<h3>Description</h3>

<p>This function computes the marginal true-score reliability for
dichotomous data (Dimitrov, 2003; May &amp; Nicewander, 1994) for
the four-parameter logistic item response model
(see <code><a href="#topic+rasch.mml2">rasch.mml2</a></code> for details regarding this IRT model).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>marginal.truescore.reliability(b, a=1+0*b,c=0*b,d=1+0*b,
    mean.trait=0, sd.trait=1, theta.k=seq(-6,6,len=200) )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="marginal.truescore.reliability_+3A_b">b</code></td>
<td>

<p>Vector of item difficulties
</p>
</td></tr>
<tr><td><code id="marginal.truescore.reliability_+3A_a">a</code></td>
<td>

<p>Vector of item discriminations
</p>
</td></tr>
<tr><td><code id="marginal.truescore.reliability_+3A_c">c</code></td>
<td>

<p>Vector of guessing parameters
</p>
</td></tr>
<tr><td><code id="marginal.truescore.reliability_+3A_d">d</code></td>
<td>

<p>Vector of upper asymptotes
</p>
</td></tr>
<tr><td><code id="marginal.truescore.reliability_+3A_mean.trait">mean.trait</code></td>
<td>

<p>Mean of trait distribution
</p>
</td></tr>
<tr><td><code id="marginal.truescore.reliability_+3A_sd.trait">sd.trait</code></td>
<td>

<p>Standard deviation of trait distribution
</p>
</td></tr>
<tr><td><code id="marginal.truescore.reliability_+3A_theta.k">theta.k</code></td>
<td>

<p>Grid at which the trait distribution should be evaluated
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>rel.test</code></td>
<td>
<p>Reliability of the test</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>True score variance (<code>sig2.true</code>, error variance
(<code>sig2.error</code>) and item reliability (<code>rel.item</code>).
Expected proportions correct are in the column <code>pi</code>.
</p>
</td></tr>
<tr><td><code>pi</code></td>
<td>
<p>Average proportion correct for all items and persons</p>
</td></tr>
<tr><td><code>sig2.tau</code></td>
<td>
<p>True score variance <code class="reqn">\sigma^2_{\tau}</code>
(calculated by the formula in May &amp; Nicewander, 1994)</p>
</td></tr>
<tr><td><code>sig2.error</code></td>
<td>
<p>Error variance <code class="reqn">\sigma^2_{e}</code></p>
</td></tr>
</table>


<h3>References</h3>

<p>Dimitrov, D. (2003). Marginal true-score measures and reliability
for binary items as a function of their IRT parameters.
<em>Applied Psychological Measurement, 27</em>, 440-458.
</p>
<p>May, K., &amp; Nicewander, W. A. (1994). Reliability and information
functions for percentile ranks. <em>Journal of Educational
Measurement, 31</em>, 313-325.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+greenyang.reliability">greenyang.reliability</a></code> for calculating the reliability
for multidimensional measures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dimitrov (2003) Table 1 - 2PL model
#############################################################################

# item discriminations
a &lt;- 1.7*c(0.449,0.402,0.232,0.240,0.610,0.551,0.371,0.321,0.403,0.434,0.459,
    0.410,0.302,0.343,0.225,0.215,0.487,0.608,0.341,0.465)
# item difficulties
b &lt;- c( -2.554,-2.161,-1.551,-1.226,-0.127,-0.855,-0.568,-0.277,-0.017,
    0.294,0.532,0.773,1.004,1.250,1.562,1.385,2.312,2.650,2.712,3.000 )

marginal.truescore.reliability( b=b, a=a )
  ##   Reliability=0.606

#############################################################################
# EXAMPLE 2: Dimitrov (2003) Table 2
#  3PL model: Poetry items (4 items)
#############################################################################

# slopes, difficulties and guessing parameters
a &lt;- 1.7*c(1.169,0.724,0.554,0.706 )
b &lt;- c(0.468,-1.541,-0.042,0.698 )
c &lt;- c(0.159,0.211,0.197,0.177 )

res &lt;- sirt::marginal.truescore.reliability( b=b, a=a, c=c)
  ##   Reliability=0.403
  ##   &gt; round( res$item, 3 )
  ##     item    pi sig2.tau sig2.error rel.item
  ##   1    1 0.463    0.063      0.186    0.252
  ##   2    2 0.855    0.017      0.107    0.135
  ##   3    3 0.605    0.026      0.213    0.107
  ##   4    4 0.459    0.032      0.216    0.130

#############################################################################
# EXAMPLE 3: Reading Data
#############################################################################
data( data.read)

#***
# Model 1: 1PL
mod &lt;- sirt::rasch.mml2( data.read )
marginal.truescore.reliability( b=mod$item$b )
  ##   Reliability=0.653

#***
# Model 2: 2PL
mod &lt;- sirt::rasch.mml2( data.read, est.a=1:12 )
marginal.truescore.reliability( b=mod$item$b, a=mod$item$a)
  ##   Reliability=0.696

## Not run: 
# compare results with Cronbach's alpha and McDonald's omega
# posing a 'wrong model' for normally distributed data
library(psych)
psych::omega(dat, nfactors=1)     # 1 factor
  ##  Omega_h for 1 factor is not meaningful, just omega_t
  ##   Omega
  ##   Call: omega(m=dat, nfactors=1)
  ##   Alpha:                 0.69
  ##   G.6:                   0.7
  ##   Omega Hierarchical:    0.66
  ##   Omega H asymptotic:    0.95
  ##   Omega Total            0.69

##! Note that alpha in psych is the standardized one.

## End(Not run)
</code></pre>

<hr>
<h2 id='matrixfunctions.sirt'>
Some Matrix Functions
</h2><span id='topic+rowMaxs.sirt'></span><span id='topic+rowMins.sirt'></span><span id='topic+rowCumsums.sirt'></span><span id='topic+colCumsums.sirt'></span><span id='topic+rowIntervalIndex.sirt'></span><span id='topic+rowKSmallest.sirt'></span><span id='topic+rowKSmallest2.sirt'></span>

<h3>Description</h3>

<p>Some matrix functions which are written in <span class="pkg">Rcpp</span> for speed
reasons.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rowMaxs.sirt(matr)      # rowwise maximum

rowMins.sirt(matr)      # rowwise minimum

rowCumsums.sirt(matr)   # rowwise cumulative sum

colCumsums.sirt(matr)   # columnwise cumulative sum

rowIntervalIndex.sirt(matr,rn) # first index in row nn when matr(nn,zz) &gt; rn(nn)

rowKSmallest.sirt(matr, K, break.ties=TRUE) # k smallest elements in a row
rowKSmallest2.sirt(matr, K )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matrixfunctions.sirt_+3A_matr">matr</code></td>
<td>

<p>A numeric matrix
</p>
</td></tr>
<tr><td><code id="matrixfunctions.sirt_+3A_rn">rn</code></td>
<td>
<p>A vector, usually a random number in applications</p>
</td></tr>
<tr><td><code id="matrixfunctions.sirt_+3A_k">K</code></td>
<td>
<p>An integer indicating the number of smallest elements to be
extracted</p>
</td></tr>
<tr><td><code id="matrixfunctions.sirt_+3A_break.ties">break.ties</code></td>
<td>
<p>A logical which indicates if ties are randomly
broken. The default is <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>rowIntervalIndex.sirt</code> searches for all rows <code>n</code>
the first index <code>i</code> for which <code>matr(n,i) &gt; rn(n)</code> holds.
</p>
<p>The functions <code>rowKSmallest.sirt</code> and <code>rowKSmallest2.sirt</code>
extract the <code class="reqn">K</code> smallest entries in a matrix row. For small
numbers of <code class="reqn">K</code> the function <code>rowKSmallest2.sirt</code> is
the faster one.
</p>


<h3>Value</h3>

<p>The output of <code>rowMaxs.sirt</code> is a list with the elements
<code>maxval</code> (rowwise maximum values) and <code>maxind</code> (rowwise
maximum indices). The output of <code>rowMins.sirt</code> contains
corresponding minimum values with entries <code>minval</code> and
<code>minind</code>.
</p>
<p>The output of <code>rowKSmallest.sirt</code> are two matrices:
<code>smallval</code> contains the <code class="reqn">K</code> smallest values whereas
<code>smallind</code> contains the <code class="reqn">K</code> smallest indices.
</p>


<h3>Author(s)</h3>

<p>Alexander Robitzsch
</p>
<p>The <span class="pkg">Rcpp</span> code for <code>rowCumsums.sirt</code> is copied from code of
Romain Francois
(<a href="https://lists.r-forge.r-project.org/pipermail/rcpp-devel/2010-October/001198.html">https://lists.r-forge.r-project.org/pipermail/rcpp-devel/2010-October/001198.html</a>).
</p>


<h3>See Also</h3>

<p>For other matrix functions see the <span class="pkg">matrixStats</span> package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: a small toy example (I)
#############################################################################
set.seed(789)
N1 &lt;- 10 ; N2 &lt;- 4
M1 &lt;- round( matrix( runif(N1*N2), nrow=N1, ncol=N2), 1 )

rowMaxs.sirt(M1)      # rowwise maximum
rowMins.sirt(M1)      # rowwise minimum
rowCumsums.sirt(M1)   # rowwise cumulative sum

# row index for exceeding a certain threshold value
matr &lt;- M1
matr &lt;- matr / rowSums( matr )
matr &lt;- sirt::rowCumsums.sirt( matr )
rn &lt;- runif(N1)    # generate random numbers
rowIntervalIndex.sirt(matr,rn)

# select the two smallest values
rowKSmallest.sirt(matr=M1, K=2)
rowKSmallest2.sirt(matr=M1, K=2)
</code></pre>

<hr>
<h2 id='mcmc_coef'>
Some Methods for Objects of Class <code>mcmc.list</code>
</h2><span id='topic+mcmc_coef'></span><span id='topic+mcmc_vcov'></span><span id='topic+mcmc_confint'></span><span id='topic+mcmc_plot'></span><span id='topic+mcmc_summary'></span><span id='topic+mcmc_derivedPars'></span><span id='topic+mcmc_WaldTest'></span><span id='topic+summary.mcmc_WaldTest'></span>

<h3>Description</h3>

<p>Some methods for objects of class <code>mcmc.list</code> created
from the <span class="pkg">coda</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## coefficients
mcmc_coef(mcmcobj, exclude="deviance")

## covariance matrix
mcmc_vcov(mcmcobj, exclude="deviance")

## confidence interval
mcmc_confint( mcmcobj, parm, level=.95, exclude="deviance" )

## summary function
mcmc_summary( mcmcobj, quantiles=c(.025,.05,.50,.95,.975) )

## plot function
mcmc_plot(mcmcobj, ...)

## inclusion of derived parameters in mcmc object
mcmc_derivedPars( mcmcobj, derivedPars )

## Wald test for parameters
mcmc_WaldTest( mcmcobj, hypotheses )

## S3 method for class 'mcmc_WaldTest'
summary(object, digits=3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_coef_+3A_mcmcobj">mcmcobj</code></td>
<td>

<p>Objects of class <code>mcmc.list</code> as created by
<code><a href="coda.html#topic+mcmc">coda::mcmc</a></code>
</p>
</td></tr>
<tr><td><code id="mcmc_coef_+3A_exclude">exclude</code></td>
<td>

<p>Vector of parameters which should be excluded in calculations
</p>
</td></tr>
<tr><td><code id="mcmc_coef_+3A_parm">parm</code></td>
<td>
<p>Optional vector of parameters</p>
</td></tr>
<tr><td><code id="mcmc_coef_+3A_level">level</code></td>
<td>
<p>Confidence level</p>
</td></tr>
<tr><td><code id="mcmc_coef_+3A_quantiles">quantiles</code></td>
<td>
<p>Vector of quantiles to be computed.</p>
</td></tr>
<tr><td><code id="mcmc_coef_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to <code>mcmc_plot</code>.
See <code>LAM::plot.amh</code> for arguments.
</p>
</td></tr>
<tr><td><code id="mcmc_coef_+3A_derivedpars">derivedPars</code></td>
<td>
<p>List with derived parameters (see examples).</p>
</td></tr>
<tr><td><code id="mcmc_coef_+3A_hypotheses">hypotheses</code></td>
<td>
<p>List with hypotheses of the form
<code class="reqn">g_i( \bold{\theta})=0</code>.</p>
</td></tr>
<tr><td><code id="mcmc_coef_+3A_object">object</code></td>
<td>
<p>Object of class <code>mcmc_WaldTest</code>.</p>
</td></tr>
<tr><td><code id="mcmc_coef_+3A_digits">digits</code></td>
<td>
<p>Number of digits used for rounding.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="coda.html#topic+mcmc">coda::mcmc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Logistic regression in rcppbugs package
#############################################################################


#***************************************
# (1) simulate data
set.seed(8765)
N &lt;- 500
x1 &lt;- stats::rnorm(N)
x2 &lt;- stats::rnorm(N)
y &lt;- 1*( stats::plogis( -.6 + .7*x1 + 1.1 *x2 ) &gt; stats::runif(N) )

#***************************************
# (2) estimate logistic regression with glm
mod &lt;- stats::glm( y ~ x1 + x2, family="binomial" )
summary(mod)

#***************************************
# (3) estimate model with rcppbugs package
library(rcppbugs)
b &lt;- rcppbugs::mcmc.normal( stats::rnorm(3),mu=0,tau=0.0001)
y.hat &lt;- rcppbugs::deterministic( function(x1,x2,b){
                stats::plogis( b[1] + b[2]*x1 + b[3]*x2 ) },
                  x1, x2, b)
y.lik &lt;- rcppbugs::mcmc.bernoulli( y, p=y.hat, observed=TRUE)
model &lt;- rcppbugs::create.model(b, y.hat, y.lik)

#*** estimate model in rcppbugs; 5000 iterations, 1000 burnin iterations
n.burnin &lt;- 500 ; n.iter &lt;- 2000 ; thin &lt;- 2
ans &lt;- rcppbugs::run.model(model, iterations=n.iter, burn=n.burnin, adapt=200, thin=thin)
print(rcppbugs::get.ar(ans)) # get acceptance rate
print(apply(ans[["b"]],2,mean)) # get means of posterior

#*** convert rcppbugs into mcmclist object
mcmcobj &lt;- data.frame( ans$b )
colnames(mcmcobj) &lt;- paste0("b",1:3)
mcmcobj &lt;- as.matrix(mcmcobj)
class(mcmcobj) &lt;- "mcmc"
attr(mcmcobj, "mcpar") &lt;- c( n.burnin+1, n.iter, thin )
mcmcobj &lt;- coda::mcmc( mcmcobj )

# coefficients, variance covariance matrix and confidence interval
mcmc_coef(mcmcobj)
mcmc_vcov(mcmcobj)
mcmc_confint( mcmcobj, level=.90 )

# summary and plot
mcmc_summary(mcmcobj)
mcmc_plot(mcmcobj, ask=TRUE)

# include derived parameters in mcmc object
derivedPars &lt;- list( "diff12"=~ I(b2-b1), "diff13"=~ I(b3-b1) )
mcmcobj2 &lt;- sirt::mcmc_derivedPars(mcmcobj, derivedPars=derivedPars )
mcmc_summary(mcmcobj2)

#*** Wald test for parameters
 # hyp1: b2 - 0.5=0
 # hyp2: b2 * b3=0
hypotheses &lt;- list( "hyp1"=~ I( b2 - .5 ), "hyp2"=~ I( b2*b3 ) )
test1 &lt;- sirt::mcmc_WaldTest( mcmcobj, hypotheses=hypotheses )
summary(test1)

## End(Not run)
</code></pre>

<hr>
<h2 id='mcmc_Rhat'>
Computation of the Rhat Statistic from a Single MCMC Chain
</h2><span id='topic+mcmc_Rhat'></span>

<h3>Description</h3>

<p>Computes the Rhat statistic from a single MCMC chain.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_Rhat(mcmc_object, n_splits=3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_Rhat_+3A_mcmc_object">mcmc_object</code></td>
<td>

<p>Object of class <code>mcmc</code>
</p>
</td></tr>
<tr><td><code id="mcmc_Rhat_+3A_n_splits">n_splits</code></td>
<td>

<p>Number of splits for MCMC chain
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Computation Rhat statistic for 2PNO model fitting by MCMC
#############################################################################

data(data.read)

# estimate 2PNO with MCMC with 3000 iterations and 500 burn-in iterations
mod &lt;- sirt::mcmc.2pno( dat=data.read, iter=1000, burnin=100 )
# plot MCMC chains
plot( mod$mcmcobj, ask=TRUE )
# compute Rhat statistics
round( sirt::mcmc_Rhat( mod$mcmcobj[[1]] ), 3 )

## End(Not run)
</code></pre>

<hr>
<h2 id='mcmc.2pno'>
MCMC Estimation of the Two-Parameter Normal Ogive Item Response Model
</h2><span id='topic+mcmc.2pno'></span>

<h3>Description</h3>

<p>This function estimates the Two-Parameter normal ogive item response model
by MCMC sampling (Johnson &amp; Albert, 1999, p. 195ff.).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc.2pno(dat, weights=NULL, burnin=500, iter=1000, N.sampvalues=1000,
      progress.iter=50, save.theta=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc.2pno_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous item responses
</p>
</td></tr>
<tr><td><code id="mcmc.2pno_+3A_weights">weights</code></td>
<td>

<p>An optional vector with student sample weights
</p>
</td></tr>
<tr><td><code id="mcmc.2pno_+3A_burnin">burnin</code></td>
<td>

<p>Number of burnin iterations
</p>
</td></tr>
<tr><td><code id="mcmc.2pno_+3A_iter">iter</code></td>
<td>

<p>Total number of iterations
</p>
</td></tr>
<tr><td><code id="mcmc.2pno_+3A_n.sampvalues">N.sampvalues</code></td>
<td>

<p>Maximum number of sampled values to save
</p>
</td></tr>
<tr><td><code id="mcmc.2pno_+3A_progress.iter">progress.iter</code></td>
<td>

<p>Display progress every <code>progress.iter</code>-th iteration. If no progress
display is wanted, then choose <code>progress.iter</code> larger than <code>iter</code>.
</p>
</td></tr>
<tr><td><code id="mcmc.2pno_+3A_save.theta">save.theta</code></td>
<td>

<p>Should theta values be saved?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The two-parameter normal ogive item response model with a probit link
function is defined by
</p>
<p style="text-align: center;"><code class="reqn"> P(X_{pi}=1 | \theta_p )=\Phi ( a_i \theta_p - b_i )
        \quad,  \quad \theta_p \sim N(0,1) </code>
</p>

<p>Note that in this implementation non-informative priors for the item
parameters are chosen (Johnson &amp; Albert, 1999, p. 195ff.).
</p>


<h3>Value</h3>

<p>A list of class <code>mcmc.sirt</code> with following entries:
</p>
<table>
<tr><td><code>mcmcobj</code></td>
<td>
<p>Object of class <code>mcmc.list</code></p>
</td></tr>
<tr><td><code>summary.mcmcobj</code></td>
<td>
<p>Summary of the <code>mcmcobj</code> object. In this
summary the Rhat statistic and the mode estimate MAP is included.
The variable <code>PercSEratio</code> indicates the proportion of the Monte Carlo
standard error in relation to the total standard deviation of the
posterior distribution.</p>
</td></tr>
<tr><td><code>burnin</code></td>
<td>
<p>Number of burnin iterations</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Total number of iterations</p>
</td></tr>
<tr><td><code>a.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">a_i</code> parameters</p>
</td></tr>
<tr><td><code>b.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">b_i</code> parameters</p>
</td></tr>
<tr><td><code>theta.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">\theta_p</code> parameters</p>
</td></tr>
<tr><td><code>deviance.chain</code></td>
<td>
<p>Sampled values of Deviance values</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliability</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with EAP person parameter estimates for
<code class="reqn">\theta_p</code> and their corresponding posterior standard
deviations</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Used data frame</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>Used student weights</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Johnson, V. E., &amp; Albert, J. H. (1999). <em>Ordinal Data Modeling</em>.
New York: Springer.
</p>


<h3>See Also</h3>

<p>S3 methods: <code><a href="#topic+summary.mcmc.sirt">summary.mcmc.sirt</a></code>, <code><a href="#topic+plot.mcmc.sirt">plot.mcmc.sirt</a></code>
</p>
<p>For estimating the 2PL model with marginal maximum likelihood see
<code><a href="#topic+rasch.mml2">rasch.mml2</a></code> or <code><a href="#topic+smirt">smirt</a></code>.
</p>
<p>A hierarchical version of this model can be estimated with
<code><a href="#topic+mcmc.2pnoh">mcmc.2pnoh</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Dataset Reading
#############################################################################
data(data.read)
# estimate 2PNO with MCMC with 3000 iterations and 500 burn-in iterations
mod &lt;- sirt::mcmc.2pno( dat=data.read, iter=3000, burnin=500 )
# plot MCMC chains
plot( mod$mcmcobj, ask=TRUE )
# write sampled chains into codafile
mcmclist2coda( mod$mcmcobj, name="dataread_2pno" )
# summary
summary(mod)

#############################################################################
# EXAMPLE 2
#############################################################################
# simulate data
N &lt;- 1000
I &lt;- 10
b &lt;- seq( -1.5, 1.5, len=I )
a &lt;- rep( c(1,2), I/2 )
theta1 &lt;- stats::rnorm(N)
dat &lt;- sirt::sim.raschtype( theta=theta1, fixed.a=a, b=b )

#***
# Model 1: estimate model without weights
mod1 &lt;- sirt::mcmc.2pno( dat, iter=1500, burnin=500)
mod1$summary.mcmcobj
plot( mod1$mcmcobj, ask=TRUE )

#***
# Model 2: estimate model with weights
# define weights
weights &lt;- c( rep( 5, N/4 ), rep( .2, 3/4*N ) )
mod2 &lt;- sirt::mcmc.2pno( dat, weights=weights, iter=1500, burnin=500)
mod1$summary.mcmcobj

## End(Not run)
</code></pre>

<hr>
<h2 id='mcmc.2pno.ml'>
Random Item Response Model / Multilevel IRT Model
</h2><span id='topic+mcmc.2pno.ml'></span>

<h3>Description</h3>

<p>This function enables the estimation of random item models and multilevel
(or hierarchical) IRT models (Chaimongkol, Huffer &amp; Kamata, 2007;
Fox &amp; Verhagen, 2010; van den Noortgate, de Boeck &amp; Meulders, 2003;
Asparouhov &amp; Muthen, 2012; Muthen &amp; Asparouhov, 2013, 2014).
Dichotomous response data is supported using a probit link. Normally
distributed responses can also be analyzed.
See Details for a description of the implemented item
response models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc.2pno.ml(dat, group, link="logit", est.b.M="h", est.b.Var="n",
    est.a.M="f", est.a.Var="n", burnin=500, iter=1000,
    N.sampvalues=1000, progress.iter=50, prior.sigma2=c(1, 0.4),
    prior.sigma.b=c(1, 1), prior.sigma.a=c(1, 1), prior.omega.b=c(1, 1),
    prior.omega.a=c(1, 0.4), sigma.b.init=.3 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc.2pno.ml_+3A_dat">dat</code></td>
<td>

<p>Data frame with item responses.
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_group">group</code></td>
<td>

<p>Vector of group identifiers (e.g. classes, schools or countries)
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_link">link</code></td>
<td>
<p>Link function. Choices are <code>"logit"</code> for dichotomous data
and <code>"normal"</code> for data under normal distribution assumptions
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_est.b.m">est.b.M</code></td>
<td>

<p>Estimation type of <code class="reqn">b_i</code> parameters: <br /> <code>n</code> - non-hierarchical prior
distribution, i.e. <code class="reqn">\omega_b</code> is set to a very high value and is not
estimated <br /> <code>h</code> - hierarchical prior distribution with estimated
distribution parameters <code class="reqn">\mu_b</code> and <code class="reqn">\omega_b</code>
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_est.b.var">est.b.Var</code></td>
<td>

<p>Estimation type of standard deviations of item difficulties <code class="reqn">b_i</code>. <br />
<code>n</code> &ndash; no estimation of the item variance, i.e. <code class="reqn">\sigma_{b,i}</code> is
assumed to be zero <br />
<code>i</code> &ndash; item-specific standard deviation of item difficulties <br />
<code>j</code> &ndash; a joint standard deviation of all item difficulties is estimated,
i.e. <code class="reqn">\sigma_{b,1}=\ldots=\sigma_{b,I}=\sigma_b</code>
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_est.a.m">est.a.M</code></td>
<td>

<p>Estimation type of <code class="reqn">a_i</code> parameters: <br />
<code>f</code> - no estimation of item slopes, i.e all item slopes <code class="reqn">a_i</code> are
fixed at one <br />
<code>n</code> - non-hierarchical prior distribution, i.e. <code class="reqn">\omega_a=0</code> <br />
<code>h</code> - hierarchical prior distribution with estimated
distribution parameter <code class="reqn">\omega_a</code>
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_est.a.var">est.a.Var</code></td>
<td>

<p>Estimation type of standard deviations of item slopes <code class="reqn">a_i</code>. <br />
<code>n</code> &ndash; no estimation of the item variance <br />
<code>i</code> &ndash; item-specific standard deviation of item slopes <br />
<code>j</code> &ndash; a joint standard deviation of all item slopes is estimated,
i.e. <code class="reqn">\sigma_{a,1}=\ldots=\sigma_{a,I}=\sigma_a</code>
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_burnin">burnin</code></td>
<td>

<p>Number of burnin iterations
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_iter">iter</code></td>
<td>

<p>Total number of iterations
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_n.sampvalues">N.sampvalues</code></td>
<td>

<p>Maximum number of sampled values to save
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_progress.iter">progress.iter</code></td>
<td>

<p>Display progress every <code>progress.iter</code>-th iteration. If no progress
display is wanted, then choose <code>progress.iter</code> larger than <code>iter</code>.
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_prior.sigma2">prior.sigma2</code></td>
<td>

<p>Prior for Level 2 standard deviation <code class="reqn">\sigma_{L2}</code>
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_prior.sigma.b">prior.sigma.b</code></td>
<td>

<p>Priors for item difficulty standard deviations <code class="reqn">\sigma_{b,i}</code>
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_prior.sigma.a">prior.sigma.a</code></td>
<td>

<p>Priors for item difficulty standard deviations <code class="reqn">\sigma_{a,i}</code>
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_prior.omega.b">prior.omega.b</code></td>
<td>

<p>Prior for <code class="reqn">\omega_b</code>
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_prior.omega.a">prior.omega.a</code></td>
<td>

<p>Prior for <code class="reqn">\omega_a</code>
</p>
</td></tr>
<tr><td><code id="mcmc.2pno.ml_+3A_sigma.b.init">sigma.b.init</code></td>
<td>

<p>Initial standard deviation for <code class="reqn">\sigma_{b,i}</code> parameters
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For dichotomous item responses (<code>link="logit"</code>) of persons <code class="reqn">p</code> in
group <code class="reqn">j</code> on
item <code class="reqn">i</code>, the probability of a correct response is defined as
</p>
<p style="text-align: center;"><code class="reqn">P( X_{pji}=1 | \theta_{pj} )=\Phi ( a_{ij} \theta_{pj} - b_{ij} )</code>
</p>

<p>The ability <code class="reqn">\theta_{pj}</code> is decomposed into a Level 1 and a Level 2
effect </p>
<p style="text-align: center;"><code class="reqn">\theta_{pj}=u_j + e_{pj} \quad, \quad
    u_j \sim N ( 0, \sigma_{L2}^2 ) \quad, \quad
    e_{pj} \sim N ( 0, \sigma_{L1}^2 ) </code>
</p>

<p>In a multilevel IRT model (or a random item model), item parameters are
allowed to vary across groups:
</p>
<p style="text-align: center;"><code class="reqn"> b_{ij} \sim N( b_i, \sigma^2_{b,i} ) \quad, \quad
    a_{ij} \sim N( a_i, \sigma^2_{a,i} ) </code>
</p>

<p>In a hierarchical IRT model, a hierarchical distribution of the (main)
item parameters is assumed
</p>
<p style="text-align: center;"><code class="reqn"> b_{i} \sim N( \mu_b, \omega^2_{b} ) \quad, \quad
    a_{i} \sim N( 1, \omega^2_{a} ) </code>
</p>

<p>Note that for identification purposes, the mean of all item slopes <code class="reqn">a_i</code>
is set to one. Using the arguments <code>est.b.M</code>, <code>est.b.Var</code>,
<code>est.a.M</code> and <code>est.a.Var</code> defines which variance components
should be estimated.
</p>
<p>For normally distributed item responses (<code>link="normal"</code>), the model
equations remain the same except the item response model which is now written as
</p>
<p style="text-align: center;"><code class="reqn"> X_{pji}=a_{ij} \theta_{pj} - b_{ij} + \varepsilon_{pji} \quad,
\quad \varepsilon_{pji} \sim N( 0, \sigma^2_{res,i} ) </code>
</p>



<h3>Value</h3>

<p>A list of class <code>mcmc.sirt</code> with following entries:
</p>
<table>
<tr><td><code>mcmcobj</code></td>
<td>
<p>Object of class <code>mcmc.list</code></p>
</td></tr>
<tr><td><code>summary.mcmcobj</code></td>
<td>
<p>Summary of the <code>mcmcobj</code> object. In this
summary the Rhat statistic and the mode estimate MAP is included.
The variable <code>PercSEratio</code> indicates the proportion of the Monte Carlo
standard error in relation to the total standard deviation of the
posterior distribution.</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria (DIC)</p>
</td></tr>
<tr><td><code>burnin</code></td>
<td>
<p>Number of burnin iterations</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Total number of iterations</p>
</td></tr>
<tr><td><code>theta.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">\theta_{pj}</code> parameters</p>
</td></tr>
<tr><td><code>theta.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">u_{j}</code> parameters</p>
</td></tr>
<tr><td><code>deviance.chain</code></td>
<td>
<p>Sampled values of Deviance values</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliability</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with EAP person parameter estimates for
<code class="reqn">\theta_pj</code> and their corresponding posterior standard
deviations</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Used data frame</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Asparouhov, T. &amp; Muthen, B. (2012). General random effect latent variable
modeling: Random subjects, items, contexts, and parameters.
<em>http://www.statmodel.com/papers_date.shtml</em>.
</p>
<p>Chaimongkol, S., Huffer, F. W., &amp; Kamata, A. (2007).
An explanatory differential item functioning (DIF)
model by the WinBUGS 1.4.
<em>Songklanakarin Journal of Science and Technology, 29</em>, 449-458.
</p>
<p>Fox, J.-P., &amp; Verhagen, A.-J. (2010). Random item effects modeling for
cross-national survey data.
In E. Davidov, P. Schmidt, &amp; J. Billiet (Eds.),
<em>Cross-cultural Analysis: Methods and Applications</em>
(pp. 467-488), London: Routledge Academic.
</p>
<p>Muthen, B. &amp; Asparouhov, T. (2013). New methods for the study of measurement
invariance with many groups. <em>http://www.statmodel.com/papers_date.shtml</em>
</p>
<p>Muthen, B. &amp; Asparouhov, T. (2014).
Item response modeling in Mplus: A multi-dimensional, multi-level,
and multi-timepoint example. In W. Linden &amp; R. Hambleton (2014).
<em>Handbook of item response theory: Models, statistical tools, and
applications</em>. <em>http://www.statmodel.com/papers_date.shtml</em>
</p>
<p>van den Noortgate, W., De Boeck, P., &amp; Meulders, M. (2003).
Cross-classification multilevel logistic models in psychometrics.
<em>Journal of Educational and Behavioral Statistics, 28</em>, 369-386.
</p>


<h3>See Also</h3>

<p>S3 methods: <code><a href="#topic+summary.mcmc.sirt">summary.mcmc.sirt</a></code>, <code><a href="#topic+plot.mcmc.sirt">plot.mcmc.sirt</a></code>
</p>
<p>For MCMC estimation of three-parameter (testlet) models see
<code><a href="#topic+mcmc.3pno.testlet">mcmc.3pno.testlet</a></code>.
</p>
<p>See also the <span class="pkg">MLIRT</span> package (http://www.jean-paulfox.com).
</p>
<p>For more flexible estimation of multilevel IRT models see the
<span class="pkg">MCMCglmm</span> and <span class="pkg">lme4</span> packages.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Dataset Multilevel data.ml1 - dichotomous items
#############################################################################
data(data.ml1)
dat &lt;- data.ml1[,-1]
group &lt;- data.ml1$group
# just for a try use a very small number of iterations
burnin &lt;- 50 ; iter &lt;- 100

#***
# Model 1: 1PNO with no cluster item effects
mod1 &lt;- sirt::mcmc.2pno.ml( dat, group, est.b.Var="n", burnin=burnin, iter=iter )
summary(mod1)    # summary
plot(mod1,layout=2,ask=TRUE) # plot results
# write results to coda file
mcmclist2coda( mod1$mcmcobj, name="data.ml1_mod1" )

#***
# Model 2: 1PNO with cluster item effects of item difficulties
mod2 &lt;- sirt::mcmc.2pno.ml( dat, group, est.b.Var="i", burnin=burnin, iter=iter )
summary(mod2)
plot(mod2, ask=TRUE, layout=2 )

#***
# Model 3: 2PNO with cluster item effects of item difficulties but
#          joint item slopes
mod3 &lt;- sirt::mcmc.2pno.ml( dat, group, est.b.Var="i", est.a.M="h",
              burnin=burnin, iter=iter )
summary(mod3)

#***
# Model 4: 2PNO with cluster item effects of item difficulties and
#          cluster item effects with a jointly estimated SD
mod4 &lt;- sirt::mcmc.2pno.ml( dat, group, est.b.Var="i", est.a.M="h",
                est.a.Var="j", burnin=burnin, iter=iter )
summary(mod4)

#############################################################################
# EXAMPLE 2: Dataset Multilevel data.ml2 - polytomous items
#            assuming a normal distribution for polytomous items
#############################################################################
data(data.ml2)
dat &lt;- data.ml2[,-1]
group &lt;- data.ml2$group
# set iterations for all examples (too few!!)
burnin &lt;- 100 ; iter &lt;- 500

#***
# Model 1: no intercept variance, no slopes
mod1 &lt;- sirt::mcmc.2pno.ml( dat=dat, group=group, est.b.Var="n",
             burnin=burnin, iter=iter, link="normal",  progress.iter=20  )
summary(mod1)

#***
# Model 2a: itemwise intercept variance, no slopes
mod2a &lt;- sirt::mcmc.2pno.ml( dat=dat, group=group, est.b.Var="i",
            burnin=burnin, iter=iter,link="normal",  progress.iter=20  )
summary(mod2a)

#***
# Model 2b: homogeneous intercept variance, no slopes
mod2b &lt;- sirt::mcmc.2pno.ml( dat=dat, group=group, est.b.Var="j",
              burnin=burnin, iter=iter,link="normal",  progress.iter=20  )
summary(mod2b)

#***
# Model 3: intercept variance and slope variances
#          hierarchical item and slope parameters
mod3 &lt;- sirt::mcmc.2pno.ml( dat=dat, group=group,
               est.b.M="h", est.b.Var="i", est.a.M="h", est.a.Var="i",
               burnin=burnin, iter=iter,link="normal",  progress.iter=20  )
summary(mod3)

#############################################################################
# EXAMPLE 3: Simulated random effects model | dichotomous items
#############################################################################
set.seed(7698)

#*** model parameters
sig2.lev2 &lt;- .3^2   # theta level 2 variance
sig2.lev1 &lt;- .8^2   # theta level 1 variance
G &lt;- 100            # number of groups
n &lt;- 20             # number of persons within a group
I &lt;- 12             # number of items
#*** simuate theta
theta2 &lt;- stats::rnorm( G, sd=sqrt(sig2.lev2) )
theta1 &lt;- stats::rnorm( n*G, sd=sqrt(sig2.lev1) )
theta  &lt;- theta1 + rep( theta2, each=n )
#*** item difficulties
b &lt;- seq( -2, 2, len=I )
#*** define group identifier
group &lt;- 1000 + rep(1:G, each=n )
#*** SD of group specific difficulties for items 3 and 5
sigma.item &lt;- rep(0,I)
sigma.item[c(3,5)] &lt;- 1
#*** simulate group specific item difficulties
b.class &lt;- sapply( sigma.item, FUN=function(sii){ stats::rnorm( G, sd=sii ) } )
b.class &lt;- b.class[ rep( 1:G,each=n ), ]
b &lt;- matrix( b, n*G, I, byrow=TRUE ) + b.class
#*** simulate item responses
m1 &lt;- stats::pnorm( theta - b )
dat &lt;- 1 * ( m1 &gt; matrix( stats::runif( n*G*I ), n*G, I ) )

#*** estimate model
mod &lt;- sirt::mcmc.2pno.ml( dat, group=group, burnin=burnin, iter=iter,
            est.b.M="n", est.b.Var="i", progress.iter=20)
summary(mod)
plot(mod, layout=2, ask=TRUE )

## End(Not run)
</code></pre>

<hr>
<h2 id='mcmc.2pnoh'>
MCMC Estimation of the Hierarchical IRT Model for Criterion-Referenced
Measurement
</h2><span id='topic+mcmc.2pnoh'></span>

<h3>Description</h3>

<p>This function estimates the hierarchical IRT model for criterion-referenced
measurement which is based on a two-parameter normal ogive response
function (Janssen, Tuerlinckx, Meulders &amp; de Boeck, 2000).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc.2pnoh(dat, itemgroups, prob.mastery=c(.5,.8), weights=NULL,
      burnin=500, iter=1000, N.sampvalues=1000,
      progress.iter=50, prior.variance=c(1,1), save.theta=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc.2pnoh_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous item responses
</p>
</td></tr>
<tr><td><code id="mcmc.2pnoh_+3A_itemgroups">itemgroups</code></td>
<td>
<p>Vector with characters or integers which define the
criterion to which an item is associated.</p>
</td></tr>
<tr><td><code id="mcmc.2pnoh_+3A_prob.mastery">prob.mastery</code></td>
<td>
<p>Probability levels which define nonmastery, transition
and mastery stage (see Details) </p>
</td></tr>
<tr><td><code id="mcmc.2pnoh_+3A_weights">weights</code></td>
<td>

<p>An optional vector with student sample weights
</p>
</td></tr>
<tr><td><code id="mcmc.2pnoh_+3A_burnin">burnin</code></td>
<td>

<p>Number of burnin iterations
</p>
</td></tr>
<tr><td><code id="mcmc.2pnoh_+3A_iter">iter</code></td>
<td>

<p>Total number of iterations
</p>
</td></tr>
<tr><td><code id="mcmc.2pnoh_+3A_n.sampvalues">N.sampvalues</code></td>
<td>

<p>Maximum number of sampled values to save
</p>
</td></tr>
<tr><td><code id="mcmc.2pnoh_+3A_progress.iter">progress.iter</code></td>
<td>

<p>Display progress every <code>progress.iter</code>-th iteration. If no progress
display is wanted, then choose <code>progress.iter</code> larger than <code>iter</code>.
</p>
</td></tr>
<tr><td><code id="mcmc.2pnoh_+3A_prior.variance">prior.variance</code></td>
<td>
<p>Scale parameter of the inverse gamma distribution
for the <code class="reqn">\sigma^2</code> and <code class="reqn">\nu^2</code> item variance parameters</p>
</td></tr>
<tr><td><code id="mcmc.2pnoh_+3A_save.theta">save.theta</code></td>
<td>

<p>Should theta values be saved?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The hierarchical IRT model for criterion-referenced measurement
(Janssen et al., 2000) assumes that every item <code class="reqn">i</code> intends
to measure a criterion <code class="reqn">k</code>. The item response function is defined as
</p>
<p style="text-align: center;"><code class="reqn"> P(X_{pik}=1 | \theta_p )=
        \Phi [ \alpha_{ik} ( \theta_p - \beta_{ik} ) ]
        \quad,  \quad \theta_p \sim N(0,1) </code>
</p>

<p>Item parameters <code class="reqn">(\alpha_{ik},\beta_{ik})</code> are hierarchically modeled, i.e.
</p>
<p style="text-align: center;"><code class="reqn"> \beta_{ik} \sim N( \xi_k, \sigma^2 ) \quad \mbox{and} \quad
    \alpha_{ik} \sim N( \omega_k, \nu^2 ) </code>
</p>

<p>In the <code>mcmc.list</code> output object, also the derived parameters
<code class="reqn">d_{ik}=\alpha_{ik} \beta_{ik}</code> and <code class="reqn">\tau_k=\xi_k \omega_k</code> are
calculated.
Mastery and nonmastery probabilities are based on a reference item <code class="reqn">Y_{k}</code>
of criterion <code class="reqn">k</code> and a response function
</p>
<p style="text-align: center;"><code class="reqn"> P(Y_{pk}=1 | \theta_p )=
        \Phi [ \omega_{k} ( \theta_p - \xi_{k} ) ]
        \quad,  \quad \theta_p \sim N(0,1) </code>
</p>

<p>With known item parameters and person parameters, response probabilities of
criterion <code class="reqn">k</code> are calculated. If a response probability of criterion <code class="reqn">k</code>
is larger than <code>prob.mastery[2]</code>, then a student is defined as a
master. If this probability is smaller than <code>prob.mastery[1]</code>, then
a student is a nonmaster. In all other cases, students are in a transition
stage.
</p>
<p>In the <code>mcmcobj</code> output object, the parameters <code>d[i]</code> are defined by
<code class="reqn">d_{ik}=\alpha_{ik} \cdot \beta_{ik}</code> while <code>tau[k]</code> are defined by
<code class="reqn"> \tau_k=\xi_k \cdot \omega_k </code>.
</p>


<h3>Value</h3>

<p>A list of class <code>mcmc.sirt</code> with following entries:
</p>
<table>
<tr><td><code>mcmcobj</code></td>
<td>
<p>Object of class <code>mcmc.list</code></p>
</td></tr>
<tr><td><code>summary.mcmcobj</code></td>
<td>
<p>Summary of the <code>mcmcobj</code> object. In this
summary the Rhat statistic and the mode estimate MAP is included.
The variable <code>PercSEratio</code> indicates the proportion of the Monte Carlo
standard error in relation to the total standard deviation of the
posterior distribution.</p>
</td></tr>
<tr><td><code>burnin</code></td>
<td>
<p>Number of burnin iterations</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Total number of iterations</p>
</td></tr>
<tr><td><code>alpha.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">\alpha_{ik}</code> parameters</p>
</td></tr>
<tr><td><code>beta.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">\beta_{ik}</code> parameters</p>
</td></tr>
<tr><td><code>xi.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">\xi_{k}</code> parameters</p>
</td></tr>
<tr><td><code>omega.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">\omega_{k}</code> parameters</p>
</td></tr>
<tr><td><code>sigma.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">\sigma</code> parameter</p>
</td></tr>
<tr><td><code>nu.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">\nu</code> parameter</p>
</td></tr>
<tr><td><code>theta.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">\theta_p</code> parameters</p>
</td></tr>
<tr><td><code>deviance.chain</code></td>
<td>
<p>Sampled values of Deviance values</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliability</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with EAP person parameter estimates for
<code class="reqn">\theta_p</code> and their corresponding posterior standard
deviations</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Used data frame</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>Used student weights</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Janssen, R., Tuerlinckx, F., Meulders, M., &amp; De Boeck, P. (2000).
A hierarchical IRT model for criterion-referenced measurement.
<em>Journal of Educational and Behavioral Statistics, 25</em>, 285-306.
</p>


<h3>See Also</h3>

<p>S3 methods: <code><a href="#topic+summary.mcmc.sirt">summary.mcmc.sirt</a></code>, <code><a href="#topic+plot.mcmc.sirt">plot.mcmc.sirt</a></code>
</p>
<p>The two-parameter normal ogive model can be estimated with
<code><a href="#topic+mcmc.2pno">mcmc.2pno</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Simulated data according to Janssen et al. (2000, Table 2)
#############################################################################

N &lt;- 1000
Ik &lt;- c(4,6,8,5,9,6,8,6,5)
xi.k &lt;- c( -.89, -1.13, -1.23, .06, -1.41, -.66, -1.09, .57, -2.44)
omega.k &lt;- c(.98, .91, .76, .74, .71, .80, .79, .82, .54)

# select 4 attributes
K &lt;- 4
Ik &lt;- Ik[1:K] ; xi.k &lt;- xi.k[1:K] ; omega.k &lt;- omega.k[1:K]
sig2 &lt;- 3.02
nu2 &lt;- .09
I &lt;- sum(Ik)
b &lt;- rep( xi.k, Ik ) + stats::rnorm(I, sd=sqrt(sig2) )
a &lt;- rep( omega.k, Ik ) + stats::rnorm(I, sd=sqrt(nu2) )
theta1 &lt;- stats::rnorm(N)
t1 &lt;- rep(1,N)
p1 &lt;- stats::pnorm( outer(t1,a) * ( theta1 - outer(t1,b) ) )
dat &lt;- 1  * ( p1 &gt; stats::runif(N*I)  )
itemgroups &lt;- rep( paste0("A", 1:K ), Ik )

# estimate model
mod &lt;- sirt::mcmc.2pnoh(dat, itemgroups, burnin=200, iter=1000 )
# summary
summary(mod)
# plot
plot(mod$mcmcobj, ask=TRUE)
# write coda files
mcmclist2coda( mod$mcmcobj, name="simul_2pnoh" )

## End(Not run)
</code></pre>

<hr>
<h2 id='mcmc.3pno.testlet'>
3PNO Testlet Model
</h2><span id='topic+mcmc.3pno.testlet'></span>

<h3>Description</h3>

<p>This function estimates the 3PNO testlet model (Wang, Bradlow &amp; Wainer, 2002, 2007)
by Markov Chain Monte Carlo methods (Glas, 2012).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc.3pno.testlet(dat, testlets=rep(NA, ncol(dat)),
   weights=NULL, est.slope=TRUE, est.guess=TRUE, guess.prior=NULL,
   testlet.variance.prior=c(1, 0.2), burnin=500, iter=1000,
   N.sampvalues=1000, progress.iter=50, save.theta=FALSE, save.gamma.testlet=FALSE )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc.3pno.testlet_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous item responses for <code class="reqn">N</code> persons and <code class="reqn">I</code> items
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_testlets">testlets</code></td>
<td>

<p>An integer or character vector which indicates the allocation of items to
testlets. Same entries corresponds to same testlets.
If an entry is <code>NA</code>, then this item does not belong to any testlet.
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_weights">weights</code></td>
<td>

<p>An optional vector with student sample weights
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_est.slope">est.slope</code></td>
<td>

<p>Should item slopes be estimated? The default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_est.guess">est.guess</code></td>
<td>

<p>Should guessing parameters be estimated? The default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_guess.prior">guess.prior</code></td>
<td>

<p>A vector of length two or a matrix with <code class="reqn">I</code> items and two columns
which defines the beta prior distribution of guessing
parameters. The default is a non-informative prior, i.e. the Beta(1,1)
distribution.
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_testlet.variance.prior">testlet.variance.prior</code></td>
<td>

<p>A vector of length two which defines the (joint) prior for testlet variances
assuming an inverse chi-squared distribution.
The first entry is the effective sample size of the prior while the second
entry defines the prior variance of the testlet. The default of <code>c(1,.2)</code>
means that the prior sample size is 1 and the prior testlet variance is .2.
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_burnin">burnin</code></td>
<td>

<p>Number of burnin iterations
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_iter">iter</code></td>
<td>

<p>Number of iterations
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_n.sampvalues">N.sampvalues</code></td>
<td>

<p>Maximum number of sampled values to save
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_progress.iter">progress.iter</code></td>
<td>

<p>Display progress every <code>progress.iter</code>-th iteration. If no progress
display is wanted, then choose <code>progress.iter</code> larger than <code>iter</code>.
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_save.theta">save.theta</code></td>
<td>

<p>Logical indicating whether theta values should be saved
</p>
</td></tr>
<tr><td><code id="mcmc.3pno.testlet_+3A_save.gamma.testlet">save.gamma.testlet</code></td>
<td>

<p>Logical indicating whether gamma values should be saved
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The testlet response model for person <code class="reqn">p</code> at item <code class="reqn">i</code>
is defined as
</p>
<p style="text-align: center;"><code class="reqn"> P(X_{pi}=1 )=c_i + ( 1 - c_i )
        \Phi ( a_i \theta_p + \gamma_{p,t(i)} + b_i    ) \quad, \quad
    \theta_p \sim N ( 0,1 ), \gamma_{p,t(i)} \sim N( 0, \sigma^2_t ) </code>
</p>

<p>In case of <code>est.slope=FALSE</code>, all item slopes <code class="reqn">a_i</code> are set to 1. Then
a variance <code class="reqn">\sigma^2</code> of the <code class="reqn">\theta_p</code> distribution is estimated
which is called the Rasch testlet model in the literature (Wang &amp; Wilson, 2005).
</p>
<p>In case of <code>est.guess=FALSE</code>, all guessing parameters <code class="reqn">c_i</code> are
set to 0.
</p>
<p>After fitting the testlet model, marginal item parameters are calculated (integrating
out testlet effects <code class="reqn">\gamma_{p,t(i)}</code>) according the defining response equation
</p>
<p style="text-align: center;"><code class="reqn"> P(X_{pi}=1 )=c_i + ( 1 - c_i )
        \Phi ( a_i^\ast \theta_p + b_i^\ast    ) </code>
</p>



<h3>Value</h3>

<p>A list of class <code>mcmc.sirt</code> with following entries:
</p>
<table>
<tr><td><code>mcmcobj</code></td>
<td>
<p>Object of class <code>mcmc.list</code> containing item parameters
(<code>b_marg</code> and <code>a_marg</code> denote marginal item parameters)
and person parameters (if requested)</p>
</td></tr>
<tr><td><code>summary.mcmcobj</code></td>
<td>
<p>Summary of the <code>mcmcobj</code> object. In this
summary the Rhat statistic and the mode estimate MAP is included.
The variable <code>PercSEratio</code> indicates the proportion of the Monte Carlo
standard error in relation to the total standard deviation of the
posterior distribution.</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria (DIC)</p>
</td></tr>
<tr><td><code>burnin</code></td>
<td>
<p>Number of burnin iterations</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Total number of iterations</p>
</td></tr>
<tr><td><code>theta.chain</code></td>
<td>
<p>Sampled values of <code class="reqn">\theta_p</code> parameters</p>
</td></tr>
<tr><td><code>deviance.chain</code></td>
<td>
<p>Sampled values of deviance values</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliability</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with EAP person parameter estimates for
<code class="reqn">\theta_p</code> and their corresponding posterior standard
deviations and for all testlet effects</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Used data frame</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>Used student weights</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Glas, C. A. W. (2012). <em>Estimating and testing the extended testlet model.</em>
LSAC Research Report Series, RR 12-03.
</p>
<p>Wainer, H., Bradlow, E. T., &amp; Wang, X. (2007).
<em>Testlet response theory and its applications</em>.
Cambridge: Cambridge University Press.
</p>
<p>Wang, W.-C., &amp; Wilson, M. (2005). The Rasch testlet model.
<em>Applied Psychological Measurement, 29</em>, 126-149.
</p>
<p>Wang, X., Bradlow, E. T., &amp; Wainer, H. (2002). A general Bayesian model
for testlets: Theory and applications.
<em>Applied Psychological Measurement, 26</em>, 109-128.
</p>


<h3>See Also</h3>

<p>S3 methods: <code><a href="#topic+summary.mcmc.sirt">summary.mcmc.sirt</a></code>, <code><a href="#topic+plot.mcmc.sirt">plot.mcmc.sirt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Dataset Reading
#############################################################################
data(data.read)
dat &lt;- data.read
I &lt;- ncol(dat)

# set burnin and total number of iterations here (CHANGE THIS!)
burnin &lt;- 200
iter &lt;- 500

#***
# Model 1: 1PNO model
mod1 &lt;- sirt::mcmc.3pno.testlet( dat,  est.slope=FALSE, est.guess=FALSE,
            burnin=burnin, iter=iter )
summary(mod1)
plot(mod1,ask=TRUE) # plot MCMC chains in coda style
plot(mod1,ask=TRUE, layout=2) # plot MCMC output in different layout

#***
# Model 2: 3PNO model with Beta(5,17) prior for guessing parameters
mod2 &lt;- sirt::mcmc.3pno.testlet( dat,  guess.prior=c(5,17),
               burnin=burnin, iter=iter )
summary(mod2)

#***
# Model 3: Rasch (1PNO) testlet model
testlets &lt;- substring( colnames(dat), 1, 1 )
mod3 &lt;- sirt::mcmc.3pno.testlet( dat,  testlets=testlets,  est.slope=FALSE,
           est.guess=FALSE, burnin=burnin, iter=iter )
summary(mod3)

#***
# Model 4: 3PNO testlet model with (almost) fixed guessing parameters .25
mod4 &lt;- sirt::mcmc.3pno.testlet( dat,  guess.prior=1000*c(25,75), testlets=testlets,
              burnin=burnin, iter=iter )
summary(mod4)
plot(mod4, ask=TRUE, layout=2)

#############################################################################
# EXAMPLE 2: Simulated data according to the Rasch testlet model
#############################################################################
set.seed(678)

N &lt;- 3000   # number of persons
I &lt;- 4      # number of items per testlet
TT &lt;- 3     # number of testlets

ITT &lt;- I*TT
b &lt;- round( stats::rnorm( ITT, mean=0, sd=1 ), 2 )
sd0 &lt;- 1 # sd trait
sdt &lt;- seq( 0, 2, len=TT ) # sd testlets

# simulate theta
theta &lt;- stats::rnorm( N, sd=sd0 )
# simulate testlets
ut &lt;- matrix(0,nrow=N, ncol=TT )
for (tt in 1:TT){
    ut[,tt] &lt;- stats::rnorm( N, sd=sdt[tt] )
}
ut &lt;- ut[, rep(1:TT,each=I) ]
# calculate response probability
prob &lt;- matrix( stats::pnorm( theta + ut + matrix( b, nrow=N, ncol=ITT,
            byrow=TRUE ) ), N, ITT)
Y &lt;- (matrix( stats::runif(N*ITT), N, ITT) &lt; prob )*1
colMeans(Y)

# define testlets
testlets &lt;- rep(1:TT, each=I )

burnin &lt;- 300
iter &lt;- 1000

#***
# Model 1: 1PNO model (without testlet structure)
mod1 &lt;- sirt::mcmc.3pno.testlet( dat=Y,  est.slope=FALSE, est.guess=FALSE,
            burnin=burnin, iter=iter, testlets=testlets )
summary(mod1)

summ1 &lt;- mod1$summary.mcmcobj
# compare item parameters
cbind( b, summ1[ grep("b", summ1$parameter ), "Mean" ] )
# Testlet standard deviations
cbind( sdt, summ1[ grep("sigma\.testlet", summ1$parameter ), "Mean" ] )

#***
# Model 2: 1PNO model (without testlet structure)
mod2 &lt;- sirt::mcmc.3pno.testlet( dat=Y,  est.slope=TRUE, est.guess=FALSE,
           burnin=burnin, iter=iter, testlets=testlets )
summary(mod2)

summ2 &lt;- mod2$summary.mcmcobj
# compare item parameters
cbind( b, summ2[ grep("b\[", summ2$parameter ), "Mean" ] )
# item discriminations
cbind( sd0, summ2[ grep("a\[", summ2$parameter ), "Mean" ] )
# Testlet standard deviations
cbind( sdt, summ2[ grep("sigma\.testlet", summ2$parameter ), "Mean" ] )

#############################################################################
# EXAMPLE 3: Simulated data according to the 2PNO testlet model
#############################################################################
set.seed(678)

N &lt;- 3000    # number of persons
I &lt;- 3      # number of items per testlet
TT &lt;- 5    # number of testlets

ITT &lt;- I*TT
b &lt;- round( stats::rnorm( ITT, mean=0, sd=1 ), 2 )
a &lt;- round( stats::runif( ITT, 0.5, 2 ),2)
sdt &lt;- seq( 0, 2, len=TT ) # sd testlets
sd0 &lt;- 1

# simulate theta
theta &lt;- stats::rnorm( N, sd=sd0 )
# simulate testlets
ut &lt;- matrix(0,nrow=N, ncol=TT )
for (tt in 1:TT){
   ut[,tt] &lt;- stats::rnorm( N, sd=sdt[tt] )
}
ut &lt;- ut[, rep(1:TT,each=I) ]
# calculate response probability
bM &lt;- matrix( b, nrow=N, ncol=ITT, byrow=TRUE )
aM &lt;- matrix( a, nrow=N, ncol=ITT, byrow=TRUE )
prob &lt;- matrix( stats::pnorm( aM*theta + ut + bM ), N, ITT)
Y &lt;- (matrix( stats::runif(N*ITT), N, ITT) &lt; prob )*1
colMeans(Y)

# define testlets
testlets &lt;- rep(1:TT, each=I )

burnin &lt;- 500
iter &lt;- 1500

#***
# Model 1: 2PNO model
mod1 &lt;- sirt::mcmc.3pno.testlet( dat=Y,  est.slope=TRUE, est.guess=FALSE,
             burnin=burnin, iter=iter, testlets=testlets )
summary(mod1)

summ1 &lt;- mod1$summary.mcmcobj
# compare item parameters
cbind( b, summ1[ grep("b\[", summ1$parameter ), "Mean" ] )
# item discriminations
cbind( a, summ1[ grep("a\[", summ1$parameter ), "Mean" ] )
# Testlet standard deviations
cbind( sdt, summ1[ grep("sigma\.testlet", summ1$parameter ), "Mean" ] )

## End(Not run)
</code></pre>

<hr>
<h2 id='mcmc.list.descriptives'>
Computation of Descriptive Statistics for a <code>mcmc.list</code> Object
</h2><span id='topic+mcmc.list.descriptives'></span>

<h3>Description</h3>

<p>Computation of descriptive statistics, Rhat convergence statistic
and MAP for a <code>mcmc.list</code> object. The Rhat statistic
is computed by splitting one Monte Carlo chain into three segments of equal
length. The MAP is the mode estimate of the posterior distribution which is
approximated by the mode of the kernel density estimate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc.list.descriptives( mcmcobj, quantiles=c(.025,.05,.1,.5,.9,.95,.975) )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc.list.descriptives_+3A_mcmcobj">mcmcobj</code></td>
<td>

<p>Object of class <code>mcmc.list</code>
</p>
</td></tr>
<tr><td><code id="mcmc.list.descriptives_+3A_quantiles">quantiles</code></td>
<td>
<p>Quantiles to be calculated for all parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with descriptive statistics for all parameters in
the <code>mcmc.list</code> object.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+mcmclist2coda">mcmclist2coda</a></code> for writing an object of class <code>mcmc.list</code>
into a coda file (see also the <span class="pkg">coda</span> package).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
miceadds::library_install("coda")
miceadds::library_install("R2WinBUGS")

#############################################################################
# EXAMPLE 1: Logistic regression
#############################################################################

#***************************************
# (1) simulate data
set.seed(8765)
N &lt;- 500
x1 &lt;- stats::rnorm(N)
x2 &lt;- stats::rnorm(N)
y &lt;- 1*( stats::plogis( -.6 + .7*x1 + 1.1 *x2 ) &gt; stats::runif(N) )

#***************************************
# (2) estimate logistic regression with glm
mod &lt;- stats::glm( y ~ x1 + x2, family="binomial" )
summary(mod)

#***************************************
# (3) estimate model with rcppbugs package
b &lt;- rcppbugs::mcmc.normal( stats::rnorm(3),mu=0,tau=0.0001)
y.hat &lt;- rcppbugs::deterministic(function(x1,x2,b) {
             stats::plogis( b[1] + b[2]*x1 + b[3]*x2 ) }, x1, x2, b)
y.lik &lt;- rcppbugs::mcmc.bernoulli( y, p=y.hat, observed=TRUE)
m &lt;- rcppbugs::create.model(b, y.hat, y.lik)

#*** estimate model in rcppbugs; 5000 iterations, 1000 burnin iterations
ans &lt;- rcppbugs::run.model(m, iterations=5000, burn=1000, adapt=1000, thin=5)
print(rcppbugs::get.ar(ans))     # get acceptance rate
print(apply(ans[["b"]],2,mean))  # get means of posterior

#*** convert rcppbugs into mcmclist object
mcmcobj &lt;- data.frame( ans$b  )
colnames(mcmcobj) &lt;- paste0("b",1:3)
mcmcobj &lt;- as.matrix(mcmcobj)
class(mcmcobj) &lt;- "mcmc"
attr(mcmcobj, "mcpar") &lt;- c( 1, nrow(mcmcobj), 1 )
mcmcobj &lt;- coda::as.mcmc.list( mcmcobj )

# plot results
plot(mcmcobj)

# summary
summ1 &lt;-  sirt::mcmc.list.descriptives( mcmcobj )
summ1

## End(Not run)
</code></pre>

<hr>
<h2 id='mcmclist2coda'>
Write Coda File from an Object of Class <code>mcmc.list</code>
</h2><span id='topic+mcmclist2coda'></span>

<h3>Description</h3>

<p>This function  writes a coda file from an object of class <code>mcmc.list</code>.
Note that only first entry (i.e. one chain) will be processed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmclist2coda(mcmclist, name, coda.digits=5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmclist2coda_+3A_mcmclist">mcmclist</code></td>
<td>

<p>An object of class <code>mcmc.list</code>.
</p>
</td></tr>
<tr><td><code id="mcmclist2coda_+3A_name">name</code></td>
<td>

<p>Name of the coda file to be written
</p>
</td></tr>
<tr><td><code id="mcmclist2coda_+3A_coda.digits">coda.digits</code></td>
<td>

<p>Number of digits after decimal in the coda file
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The coda file and a corresponding index file are
written into the working directory.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: MCMC estimation 2PNO dataset Reading
#############################################################################

data(data.read)
# estimate 2PNO with MCMC with 3000 iterations and 500 burn-in iterations
mod &lt;- sirt::mcmc.2pno( dat=data.read, iter=3000, burnin=500 )
# plot MCMC chains
plot( mod$mcmcobj, ask=TRUE )
# write sampled chains into codafile
mcmclist2coda( mod$mcmcobj, name="dataread_2pl" )

## End(Not run)
</code></pre>

<hr>
<h2 id='md.pattern.sirt'>
Response Pattern in a Binary Matrix
</h2><span id='topic+md.pattern.sirt'></span>

<h3>Description</h3>

<p>Computes different statistics of the response pattern in a binary
matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>md.pattern.sirt(dat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="md.pattern.sirt_+3A_dat">dat</code></td>
<td>

<p>A binary data matrix
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>dat</code></td>
<td>
<p>Original dataset</p>
</td></tr>
<tr><td><code>dat.resp1</code></td>
<td>
<p>Indices for responses of 1's</p>
</td></tr>
<tr><td><code>dat.resp0</code></td>
<td>
<p>Indices for responses of 0's</p>
</td></tr>
<tr><td><code>resp_patt</code></td>
<td>
<p>Vector of response patterns</p>
</td></tr>
<tr><td><code>unique_resp_patt</code></td>
<td>
<p>Unique response patterns</p>
</td></tr>
<tr><td><code>unique_resp_patt_freq</code></td>
<td>
<p>Frequencies of unique response patterns</p>
</td></tr>
<tr><td><code>unique_resp_patt_firstobs</code></td>
<td>
<p>First observation in original dataset
<code>dat</code> of a unique response pattern</p>
</td></tr>
<tr><td><code>freq1</code></td>
<td>
<p>Frequencies of 1's</p>
</td></tr>
<tr><td><code>freq0</code></td>
<td>
<p>Frequencies of 0's</p>
</td></tr>
<tr><td><code>dat.ordered</code></td>
<td>
<p>Dataset according to response patterns</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>See also the <code>md.pattern</code> function in the <span class="pkg">mice</span> package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Response patterns
#############################################################################
set.seed(7654)
N &lt;- 21         # number of rows
I &lt;- 4          # number of columns
dat &lt;- matrix( 1*( stats::runif(N*I) &gt; .3 ), N, I )
res &lt;- sirt::md.pattern.sirt(dat)
# plot of response patterns
res$dat.ordered
image( z=t(res$dat.ordered), y=1:N, x=1:I, xlab="Items", ylab="Persons")
# 0's are yellow and 1's are red

#############################################################################
# EXAMPLE 2: Item response patterns for dataset data.read
#############################################################################

data(data.read)
dat &lt;- data.read  ; N &lt;- nrow(dat) ; I &lt;- ncol(dat)
# order items according to p values
dat &lt;- dat[, order(colMeans(dat, na.rm=TRUE )) ]
# analyzing response pattern
res &lt;- sirt::md.pattern.sirt(dat)
res$dat.ordered
image( z=t(res$dat.ordered), y=1:N, x=1:I, xlab="Items", ylab="Persons")
</code></pre>

<hr>
<h2 id='mgsem'>
Estimation of Multiple-Group Structural Equation Models
</h2><span id='topic+mgsem'></span>

<h3>Description</h3>

<p>Estimates a multiple-group structural equation model. The function allows arbitrary
prior distributions on model parameters and regularized estimation with the SCAD and
the LASSO penalty. Moreover, it can also conduct robust moment estimation using
the <code class="reqn">L_p</code> loss function <code class="reqn">\rho(x)=|x|^p</code> for <code class="reqn">p \ge 0</code>.
See Robitzsch (2023) for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mgsem(suffstat, model, data=NULL, group=NULL, weights=NULL, estimator="ML",
     p_me=2, p_pen=1, pen_type="scad", diffpar_pen=NULL, pen_sample_size=TRUE,
     a_scad=3.7, eps_approx=0.001, comp_se=TRUE, se_delta_formula=FALSE,
     prior_list=NULL, hessian=TRUE, fixed_parms=FALSE, cd=FALSE,
     cd_control=list(maxiter=20, tol=5*1e-04, interval_length=0.05, method="exact"),
     partable_start=NULL, num_approx=FALSE, technical=NULL, control=list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mgsem_+3A_suffstat">suffstat</code></td>
<td>

<p>List containing sufficient statistics
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_model">model</code></td>
<td>

<p>Model specification, see examples. Can have entries <code>est</code>, <code>index</code>,
<code>lower</code>, <code>upper</code>, <code>prior</code>, <code>pen_l2</code>, <code>pen_lp</code>,
<code>pen_difflp</code>. Each entry can be defined for model matrices <code>ALPHA</code>,
<code>NU</code>, <code>LAM</code>, <code>PHI</code>, and <code>PSI</code>.
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_data">data</code></td>
<td>

<p>Optional data frame
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_group">group</code></td>
<td>

<p>Optional vector of group identifiers
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_weights">weights</code></td>
<td>

<p>Optional vector of sampling weights
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_estimator">estimator</code></td>
<td>

<p>Character. Can be either <code>"ML"</code> for maximum likelihood fitting function or
<code>"ME"</code> for robust moment estimation.
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_p_me">p_me</code></td>
<td>

<p>Power in $L_p$ loss function for robust moment estimation
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_p_pen">p_pen</code></td>
<td>

<p>Power for penalty in regularized estimation. For regular LASSO and SCAD penalty
functions, it is $p=1$.
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_pen_type">pen_type</code></td>
<td>

<p>Penalty type. Can be either <code>"scad"</code> or <code>"lasso"</code>.
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_diffpar_pen">diffpar_pen</code></td>
<td>

<p>List containing values of regularization parameters in fused lasso estimation
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_pen_sample_size">pen_sample_size</code></td>
<td>

<p>List containing values for sample sizes for regularized estimation
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_a_scad">a_scad</code></td>
<td>

<p>Parameter $a$ used in SCAD penalty
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_eps_approx">eps_approx</code></td>
<td>

<p>Approximation value for nondifferentiable robust moment fitting function or
penalty function
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_comp_se">comp_se</code></td>
<td>

<p>Logical indicating whether standard errors should be computed
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_se_delta_formula">se_delta_formula</code></td>
<td>

<p>Logical indicating whether standard errors should be computed according to the
delta formula
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_prior_list">prior_list</code></td>
<td>

<p>List containing specifications of the prior distributions
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_hessian">hessian</code></td>
<td>

<p>Logical indicating whether the Hessian matrix should be computed
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_fixed_parms">fixed_parms</code></td>
<td>

<p>Logical indicating whether all model parameters should be fixed
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_cd">cd</code></td>
<td>

<p>Logical indicating whether coordinate descent should be used for estimation
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_cd_control">cd_control</code></td>
<td>

<p>Control parameters for coordinate descent estimation
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_partable_start">partable_start</code></td>
<td>

<p>Starting values for parameter estimation
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_num_approx">num_approx</code></td>
<td>

<p>Logical indicating whether derivatives should be computed based on numerical
differentiation
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_technical">technical</code></td>
<td>

<p>Parameters used for optimization in <code>sirt_optimizer</code>
</p>
</td></tr>
<tr><td><code id="mgsem_+3A_control">control</code></td>
<td>

<p>Control paramaters for optimization
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>[MORE INFORMATION TO BE ADDED]
</p>


<h3>Value</h3>

<p>A list with following values
</p>
<table>
<tr><td><code>coef</code></td>
<td>
<p>Coeffients</p>
</td></tr>
<tr><td><code>vcov</code></td>
<td>
<p>Variance matrix</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>Vector of standard errors</p>
</td></tr>
<tr><td><code>partable</code></td>
<td>
<p>Parameter table</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>Specified model</p>
</td></tr>
<tr><td><code>opt_res</code></td>
<td>
<p>Result from optimization</p>
</td></tr>
<tr><td><code>opt_value</code></td>
<td>
<p>Value of fitting function</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Residuals of sufficient statistics</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria</p>
</td></tr>
<tr><td><code>technical</code></td>
<td>
<p>Specifications of optimizer</p>
</td></tr>
<tr><td><code>suffstat_vcov</code></td>
<td>
<p>Variance matrix of sufficient statistics</p>
</td></tr>
<tr><td><code>me_delta_method</code></td>
<td>
<p>Input and output matrices for delta method if
<code>estimator="ME"</code></p>
</td></tr>
<tr><td><code>data_proc</code></td>
<td>
<p>Processed data</p>
</td></tr>
<tr><td><code>case_ll</code></td>
<td>
<p>Casewise log-likelihood function</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Robitzsch, A. (2023). Model-robust estimation of multiple-group structural
equation models. <em>Algorithms, 16</em>(4), 210. <a href="https://doi.org/10.3390/a16040210">doi:10.3390/a16040210</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Noninvariant item intercepts in a multiple-group SEM
#############################################################################

#---- simulate data
set.seed(65)
G &lt;- 3  # number of groups
I &lt;- 5  # number of items
# define lambda and nu parameters
lambda &lt;- matrix(1, nrow=G, ncol=I)
nu &lt;- matrix(0, nrow=G, ncol=I)
err_var &lt;- matrix(1, nrow=G, ncol=I)

# define extent of noninvariance
dif_int &lt;- .5

#- 1st group: N(0,1)
nu[1,4] &lt;- dif_int
#- 2nd group: N(0.3,1.5)
gg &lt;- 2 ;
nu[gg,1] &lt;- -dif_int
#- 3nd group: N(.8,1.2)
gg &lt;- 3
nu[gg,2] &lt;- -dif_int
#- define distributions of groups
mu &lt;- c(0,.3,.8)
sigma &lt;- sqrt(c(1,1.5,1.2))
N &lt;- rep(1000,3) # sample sizes per group

exact &lt;- FALSE
suffstat &lt;- sirt::invariance_alignment_simulate(nu, lambda, err_var, mu, sigma, N,
                output="suffstat", groupwise=TRUE, exact=exact)

#---- model specification

# model specifications joint group
est &lt;- list(
        ALPHA=matrix( c(0), ncol=1),
        NU=matrix( 0, nrow=I, ncol=1),
        LAM=matrix(1, nrow=I, ncol=1),
        PHI=matrix(0,nrow=1,ncol=1),
        PSI=diag(rep(1,I))
        )

# parameter index
index &lt;- list(
        ALPHA=0*est$ALPHA,
        NU=1+0*est$NU,
        LAM=1+0*est$LAM,
        PHI=0*est$PHI,
        PSI=diag(1,I)
        )

# lower bounds
lower &lt;- list(
        PSI=diag(rep(0.01,I)), PHI=matrix(0.01,1,1)
        )

#*** joint parameters
group0 &lt;- list(est=est, index=index, lower=lower)

#*** group1
est &lt;- list(
        ALPHA=matrix( c(0), ncol=1),
        NU=matrix( 0, nrow=I, ncol=1),
        LAM=matrix(0, nrow=I, ncol=1),
        PHI=matrix(1,nrow=1,ncol=1)
            )

# parameter index
index &lt;- list(
        ALPHA=0*est$ALPHA,
        NU=0*est$NU,
        LAM=1*est$LAM,
        PHI=0*est$PHI
        )

group1 &lt;- list(est=est, index=index, lower=lower)

#*** group 2 and group 3

# modify parameter index
index$ALPHA &lt;- 1+0*est$ALPHA
index$PHI &lt;- 1+0*est$PHI
group3 &lt;- group2 &lt;- list(est=est, index=index, lower=lower)

#*** define model
model &lt;- list(group0=group0, group1=group1, group2=group2, group3=group3)

#-- estimate model with ML
res1 &lt;- sirt::mgsem( suffstat=suffstat, model=model2, eps_approx=1e-4, estimator="ML",
                    technical=list(maxiter=500, optimizer="optim"),
                    hessian=FALSE, comp_se=FALSE, control=list(trace=1) )
str(res1)

#-- robust moment estimation with p=0.5

optimizer &lt;- "optim"
technical &lt;- list(maxiter=500, optimizer=optimizer)
eps_approx &lt;- 1e-3

res2 &lt;- sirt::mgsem( suffstat=suffstat, model=res1$model, p_me=0.5,
                    eps_approx=eps_approx, estimator="ME", technical=technical,
                    hessian=FALSE, comp_se=FALSE, control=list(trace=1) )

#---- regularized estimation

nu_lam &lt;- 0.1    # regularization parameter

# redefine model
define_model &lt;- function(model, nu_lam)
{
    pen_lp &lt;- list( NU=nu_lam+0*model$group1$est$NU)
    ee &lt;- "group1"
    for (ee in c("group1","group2","group3"))
    {
        model[[ee]]$index$NU &lt;- 1+0*index$NU
        model[[ee]]$pen_lp &lt;- pen_lp
    }
    return(model)
}

model3 &lt;- define_model(model=model, nu_lam=nu_lam)
pen_type &lt;- "scad"

res3 &lt;- sirt::mgsem( suffstat=suffstat, model=model3, p_pen=1, pen_type=pen_type,
                    eps_approx=eps_approx, estimator="ML",
                    technical=list(maxiter=500, optimizer="optim"),
                    hessian=FALSE, comp_se=FALSE, control=list(trace=1) )
str(res3)

## End(Not run)
</code></pre>

<hr>
<h2 id='mirt.specify.partable'>
Specify or modify a Parameter Table in <span class="pkg">mirt</span>
</h2><span id='topic+mirt.specify.partable'></span>

<h3>Description</h3>

<p>Specify or modify a parameter table in <span class="pkg">mirt</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mirt.specify.partable(mirt.partable, parlist, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mirt.specify.partable_+3A_mirt.partable">mirt.partable</code></td>
<td>

<p>Parameter table in <span class="pkg">mirt</span> package
</p>
</td></tr>
<tr><td><code id="mirt.specify.partable_+3A_parlist">parlist</code></td>
<td>

<p>List of parameters which are used for specification in the parameter
table. See Examples.
</p>
</td></tr>
<tr><td><code id="mirt.specify.partable_+3A_verbose">verbose</code></td>
<td>

<p>An optional logical indicating whether the some warnings
should be printed.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A modified parameter table
</p>


<h3>Author(s)</h3>

<p>Alexander Robitzsch, Phil Chalmers
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Modifying a parameter table for single group
#############################################################################

library(mirt)
data(LSAT7,package="mirt")
data &lt;- mirt::expand.table(LSAT7)

mirt.partable &lt;- mirt::mirt(data, 1, pars="values")
colnames(mirt.partable)
## &gt; colnames(mirt.partable) [1] 'group' 'item' 'class' 'name' 'parnum' 'value'
##   'lbound' 'ubound' 'est' 'prior.type' 'prior_1' 'prior_2'

# specify some values of item parameters
value &lt;- data.frame(d=c(0.7, -1, NA), a1=c(1, 1.2, 1.3), g=c(NA, 0.25, 0.25))
rownames(value) &lt;- c("Item.1", "Item.4", "Item.3")

# fix some item paramters
est1 &lt;- data.frame(d=c(TRUE, NA), a1=c(FALSE, TRUE))
rownames(est1) &lt;- c("Item.4", "Item.3")

# estimate all guessing parameters
est2 &lt;- data.frame(g=rep(TRUE, 5))
rownames(est2) &lt;- colnames(data)

# prior distributions
prior.type &lt;- data.frame(g=rep("norm", 4))
rownames(prior.type) &lt;- c("Item.1", "Item.2", "Item.4", "Item.5")
prior_1 &lt;- data.frame(g=rep(-1.38, 4))
rownames(prior_1) &lt;- c("Item.1", "Item.2", "Item.4", "Item.5")
prior_2 &lt;- data.frame(g=rep(0.5, 4))
rownames(prior_2) &lt;- c("Item.1", "Item.2", "Item.4", "Item.5")

# misspecify some entries
rownames(prior_2)[c(3,2)] &lt;- c("A", "B")
rownames(est1)[2] &lt;- c("B")

# define complete list with parameter specification
parlist &lt;- list(value=value, est=est1, est=est2, prior.type=prior.type,
      prior_1=prior_1, prior_2=prior_2)

# modify parameter table
mirt.specify.partable(mirt.partable, parlist)
</code></pre>

<hr>
<h2 id='mirt.wrapper'>
Some Functions for Wrapping with the <span class="pkg">mirt</span> Package
</h2><span id='topic+mirt.wrapper'></span><span id='topic+mirt.wrapper.coef'></span><span id='topic+mirt.wrapper.posterior'></span><span id='topic+mirt.wrapper.fscores'></span><span id='topic+mirt.wrapper.itemplot'></span><span id='topic+IRT.irfprob.SingleGroupClass'></span><span id='topic+IRT.irfprob.MultipleGroupClass'></span><span id='topic+IRT.likelihood.SingleGroupClass'></span><span id='topic+IRT.posterior.SingleGroupClass'></span><span id='topic+IRT.likelihood.MultipleGroupClass'></span><span id='topic+IRT.posterior.MultipleGroupClass'></span><span id='topic+IRT.expectedCounts.SingleGroupClass'></span><span id='topic+IRT.expectedCounts.MultipleGroupClass'></span><span id='topic+mirt_summary'></span>

<h3>Description</h3>

<p>Some functions for wrapping with the <span class="pkg">mirt</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># extract coefficients
mirt.wrapper.coef(mirt.obj)

# summary output
mirt_summary(object, digits=4, file=NULL, ...)

# extract posterior, likelihood, ...
mirt.wrapper.posterior(mirt.obj, weights=NULL, group=NULL)
## S3 method for class 'SingleGroupClass'
IRT.likelihood(object, ...)
## S3 method for class 'MultipleGroupClass'
IRT.likelihood(object, ...)
## S3 method for class 'SingleGroupClass'
IRT.posterior(object, ...)
## S3 method for class 'MultipleGroupClass'
IRT.posterior(object, ...)
## S3 method for class 'SingleGroupClass'
IRT.expectedCounts(object, ...)
## S3 method for class 'MultipleGroupClass'
IRT.expectedCounts(object, ...)

# S3 method for extracting item response functions
## S3 method for class 'SingleGroupClass'
IRT.irfprob(object, ...)
## S3 method for class 'MultipleGroupClass'
IRT.irfprob(object, group=1, ...)

# compute factor scores
mirt.wrapper.fscores(mirt.obj, weights=NULL)

# convenience function for itemplot
mirt.wrapper.itemplot( mirt.obj, ask=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mirt.wrapper_+3A_mirt.obj">mirt.obj</code></td>
<td>

<p>A fitted model in <span class="pkg">mirt</span> package
</p>
</td></tr>
<tr><td><code id="mirt.wrapper_+3A_object">object</code></td>
<td>
<p>A fitted object in <span class="pkg">mirt</span> package of class
<code>SingleGroupClass</code> or <code>MultipleGroupClass</code>.
</p>
</td></tr>
<tr><td><code id="mirt.wrapper_+3A_group">group</code></td>
<td>
<p>Group index for <code>IRT.irfprob</code> (only
applicable for object of class <code>MultipleGroupClass</code>)</p>
</td></tr>
<tr><td><code id="mirt.wrapper_+3A_digits">digits</code></td>
<td>
<p>Number of digits after decimal used for rounding</p>
</td></tr>
<tr><td><code id="mirt.wrapper_+3A_file">file</code></td>
<td>
<p>File name for sinking summary output</p>
</td></tr>
<tr><td><code id="mirt.wrapper_+3A_weights">weights</code></td>
<td>
<p>Optional vector of student weights</p>
</td></tr>
<tr><td><code id="mirt.wrapper_+3A_ask">ask</code></td>
<td>
<p>Optional logical indicating whether each new plot should be
confirmed.</p>
</td></tr>
<tr><td><code id="mirt.wrapper_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>mirt.wrapper.coef</code> collects all item parameters
in a data frame.
</p>
<p>The function <code>mirt.wrapper.posterior</code> extracts the individual
likelihood, individual likelihood and expected counts. This function does not
yet cover the case of multiple groups.
</p>
<p>The function <code>mirt.wrapper.fscores</code> computes factor scores
EAP, MAP and MLE. The factor scores are computed on the
discrete grid of latent traits (contrary to the computation in <code>mirt</code>) as
specified in <code>mirt.obj@Theta</code>. This function does also not work
for multiple groups.
</p>
<p>The function <code>mirt.wrapper.itemplot</code> displays all item plots
after each other.
</p>


<h3>Value</h3>

<p>Function <code>mirt.wrapper.coef</code> &ndash; List with entries
</p>
<table>
<tr><td><code>coef</code></td>
<td>
<p>Data frame with item parameters</p>
</td></tr>
<tr><td><code>GroupPars</code></td>
<td>
<p>Data frame or list with distribution parameters</p>
</td></tr>
</table>
<p>Function <code>mirt.wrapper.posterior</code> &ndash; List with entries
</p>
<table>
<tr><td><code>theta.k</code></td>
<td>
<p>Grid of theta points</p>
</td></tr>
<tr><td><code>pi.k</code></td>
<td>
<p>Trait distribution on <code>theta.k</code></p>
</td></tr>
<tr><td><code>f.yi.qk</code></td>
<td>
<p>Individual likelihood</p>
</td></tr>
<tr><td><code>f.qk.yi</code></td>
<td>
<p>Individual posterior</p>
</td></tr>
<tr><td><code>n.ik</code></td>
<td>
<p>Expected counts</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>Used dataset</p>
</td></tr>
</table>
<p>Function <code>mirt.wrapper.fscores</code> &ndash; List with entries
</p>
<table>
<tr><td><code>person</code></td>
<td>
<p>Data frame with person parameter estimates (factor scores)
EAP, MAP and MLE for all dimensions.</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliabilities</p>
</td></tr>
</table>


<h3>Examples for the <span class="pkg">mirt</span> Package</h3>


<ol>
<li><p> Latent class analysis (<code><a href="#topic+data.read">data.read</a></code>, Model 7)
</p>
</li>
<li><p> Mixed Rasch model  (<code><a href="#topic+data.read">data.read</a></code>, Model 8)
</p>
</li>
<li><p> Located unidimensional and multidimensional
latent class models / Multidimensional latent class IRT models
(<code><a href="#topic+data.read">data.read</a></code>, Model 12;
<code><a href="#topic+rasch.mirtlc">rasch.mirtlc</a></code>, Example 4)
</p>
</li>
<li><p> Multidimensional IRT model with discrete latent traits
(<code><a href="#topic+data.read">data.read</a></code>, Model 13)
</p>
</li>
<li><p> DINA model (<code><a href="#topic+data.read">data.read</a></code>, Model 14;
<code><a href="CDM.html#topic+data.dcm">data.dcm</a></code>, <span class="pkg">CDM</span>, Model 1m)
</p>
</li>
<li><p> Unidimensional IRT model with non-normal distribution
(<code><a href="#topic+data.read">data.read</a></code>, Model 15)
</p>
</li>
<li><p> Grade of membership model  (<code><a href="#topic+gom.em">gom.em</a></code>, Example 2)
</p>
</li>
<li><p> Rasch copula model (<code><a href="#topic+rasch.copula2">rasch.copula2</a></code>, Example 5)
</p>
</li>
<li><p> Additive GDINA model
(<code><a href="CDM.html#topic+data.dcm">data.dcm</a></code>, <span class="pkg">CDM</span>, Model 6m)
</p>
</li>
<li><p> Longitudinal Rasch model (<code><a href="#topic+data.long">data.long</a></code>, Model 3)
</p>
</li>
<li><p> Normally distributed residuals (<code><a href="#topic+data.big5">data.big5</a></code>, Example 1, Model 5)
</p>
</li>
<li><p> Nedelsky model (<code><a href="#topic+nedelsky.irf">nedelsky.irf</a></code>,
Examples 1, 2)
</p>
</li>
<li><p> Beta item response model (<code><a href="#topic+brm.irf">brm.irf</a></code>, Example 1)
</p>
</li></ol>



<h3>See Also</h3>

<p>See the <span class="pkg">mirt</span> package manual for more information.
</p>
<p>See for the main estimation functions in <span class="pkg">mirt</span>:
<code><a href="mirt.html#topic+mirt">mirt::mirt</a></code>,
<code><a href="mirt.html#topic+multipleGroup">mirt::multipleGroup</a></code>
and <code><a href="mirt.html#topic+bfactor">mirt::bfactor</a></code>.
</p>
<p>See <code><a href="mirt.html#topic+coef-method">mirt::coef-method</a></code> for extracting
coefficients.
</p>
<p>See <code><a href="mirt.html#topic+mod2values">mirt::mod2values</a></code> for collecting
parameter values in a mirt parameter table.
</p>
<p>See <code><a href="#topic+lavaan2mirt">lavaan2mirt</a></code> for converting <code>lavaan</code> syntax
to <code>mirt</code> syntax.
</p>
<p>See <code><a href="#topic+tam2mirt">tam2mirt</a></code> for converting fitted <code>tam</code> models
into <code>mirt</code> objects.
</p>
<p>See also <code><a href="CDM.html#topic+IRT.likelihood">CDM::IRT.likelihood</a></code>,
<code><a href="CDM.html#topic+IRT.posterior">CDM::IRT.posterior</a></code> and
<code><a href="CDM.html#topic+IRT.irfprob">CDM::IRT.irfprob</a></code> for general
extractor functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# A development version can be installed from GitHub
if (FALSE){ # default is set to FALSE, use the installed version
   library(devtools)
   devtools::install_github("philchalmers/mirt")
          }
# now, load mirt
library(mirt)

#############################################################################
# EXAMPLE 1: Extracting item parameters and posterior LSAT data
#############################################################################

data(LSAT7, package="mirt")
data &lt;- mirt::expand.table(LSAT7)

#*** Model 1: 3PL model for item 5 only, other items 2PL
mod1 &lt;- mirt::mirt(data, 1, itemtype=c("2PL","2PL","2PL","2PL","3PL"), verbose=TRUE)
print(mod1)
summary(mod1)
# extracting coefficients
coef(mod1)
mirt.wrapper.coef(mod1)$coef
# summary output
mirt_summary(mod1)
# extract parameter values in mirt
mirt::mod2values(mod1)
# extract posterior
post1 &lt;- sirt::mirt.wrapper.posterior(mod1)
# extract item response functions
probs1 &lt;- IRT.irfprob(mod1)
str(probs1)
# extract individual likelihood
likemod1 &lt;- IRT.likelihood(mod1)
str(likemod1)
# extract individual posterior
postmod1 &lt;- IRT.posterior(mod1)
str(postmod1)

#*** Model 2: Confirmatory model with two factors
cmodel &lt;- mirt::mirt.model("
        F1=1,4,5
        F2=2,3
        ")
mod2 &lt;- mirt::mirt(data, cmodel, verbose=TRUE)
print(mod2)
summary(mod2)
# extract coefficients
coef(mod2)
mirt.wrapper.coef(mod2)$coef
# extract posterior
post2 &lt;- sirt::mirt.wrapper.posterior(mod2)

#############################################################################
# EXAMPLE 2: Extracting item parameters and posterior for differering
#            number of response catagories | Dataset Science
#############################################################################

data(Science,package="mirt")
library(psych)
psych::describe(Science)

# modify dataset
dat &lt;- Science
dat[ dat[,1] &gt; 3,1] &lt;- 3
psych::describe(dat)

# estimate generalized partial credit model
mod1 &lt;- mirt::mirt(dat, 1, itemtype="gpcm")
print(mod1)
# extract coefficients
coef(mod1)
mirt.wrapper.coef(mod1)$coef
# extract posterior
post1 &lt;- sirt::mirt.wrapper.posterior(mod1)

#############################################################################
# EXAMPLE 3: Multiple group model; simulated dataset from mirt package
#############################################################################

#*** simulate data (copy from the multipleGroup manual site in mirt package)
set.seed(1234)
a &lt;- matrix(c(abs( stats::rnorm(5,1,.3)), rep(0,15),abs( stats::rnorm(5,1,.3)),
          rep(0,15),abs( stats::rnorm(5,1,.3))), 15, 3)
d &lt;- matrix( stats::rnorm(15,0,.7),ncol=1)
mu &lt;- c(-.4, -.7, .1)
sigma &lt;- matrix(c(1.21,.297,1.232,.297,.81,.252,1.232,.252,1.96),3,3)
itemtype &lt;- rep("dich", nrow(a))
N &lt;- 1000
dataset1 &lt;- mirt::simdata(a, d, N, itemtype)
dataset2 &lt;- mirt::simdata(a, d, N, itemtype, mu=mu, sigma=sigma)
dat &lt;- rbind(dataset1, dataset2)
group &lt;- c(rep("D1", N), rep("D2", N))

#group models
model &lt;- mirt::mirt.model("
   F1=1-5
   F2=6-10
   F3=11-15
      ")

# separate analysis
mod_configural &lt;- mirt::multipleGroup(dat, model, group=group, verbose=TRUE)
mirt.wrapper.coef(mod_configural)

# equal slopes (metric invariance)
mod_metric &lt;- mirt::multipleGroup(dat, model, group=group, invariance=c("slopes"),
                verbose=TRUE)
mirt.wrapper.coef(mod_metric)

# equal slopes and intercepts (scalar invariance)
mod_scalar &lt;- mirt::multipleGroup(dat, model, group=group,
          invariance=c("slopes","intercepts","free_means","free_varcov"), verbose=TRUE)
mirt.wrapper.coef(mod_scalar)

# full constraint
mod_fullconstrain &lt;- mirt::multipleGroup(dat, model, group=group,
             invariance=c("slopes", "intercepts", "free_means", "free_var"), verbose=TRUE )
mirt.wrapper.coef(mod_fullconstrain)

#############################################################################
# EXAMPLE 4: Nonlinear item response model
#############################################################################

data(data.read)
dat &lt;- data.read
# specify mirt model with some interactions
mirtmodel &lt;- mirt.model("
   A=1-4
   B=5-8
   C=9-12
   (A*B)=4,8
   (C*C)=9
   (A*B*C)=12
   " )
# estimate model
res &lt;- mirt::mirt( dat, mirtmodel, verbose=TRUE, technical=list(NCYCLES=3) )
# look at estimated parameters
mirt.wrapper.coef(res)
coef(res)
mirt::mod2values(res)
# model specification
res@model

#############################################################################
# EXAMPLE 5: Extracting factor scores
#############################################################################

data(data.read)
dat &lt;- data.read
# define lavaan model and convert syntax to mirt
lavmodel &lt;- "
    A=~ a*A1+a*A2+1.3*A3+A4       # set loading of A3 to 1.3
    B=~ B1+1*B2+b3*B3+B4
    C=~ c*C1+C2+c*C3+C4
    A1 | da*t1
    A3 | da*t1
    C4 | dg*t1
    B1 | 0*t1
    B3 | -1.4*t1                  # fix item threshold of B3 to -1.4
    A ~~ B                        # estimate covariance between A and B
    A ~~ .6 * C                   # fix covariance to .6
    B ~~ B                        # estimate variance of B
    A ~ .5*1                      # set mean of A to .5
    B ~ 1                         # estimate mean of B
    "
res &lt;- sirt::lavaan2mirt( dat, lavmodel, verbose=TRUE, technical=list(NCYCLES=3) )
# estimated coefficients
mirt.wrapper.coef(res$mirt)
# extract factor scores
fres &lt;- sirt::mirt.wrapper.fscores(res$mirt)
# look at factor scores
head( round(fres$person,2))
  ##     case    M EAP.Var1 SE.EAP.Var1 EAP.Var2 SE.EAP.Var2 EAP.Var3 SE.EAP.Var3 MLE.Var1
  ##   1    1 0.92     1.26        0.67     1.61        0.60     0.05        0.69     2.65
  ##   2    2 0.58     0.06        0.59     1.14        0.55    -0.80        0.56     0.00
  ##   3    3 0.83     0.86        0.66     1.15        0.55     0.48        0.74     0.53
  ##   4    4 1.00     1.52        0.67     1.57        0.60     0.73        0.76     2.65
  ##   5    5 0.50    -0.13        0.58     0.85        0.48    -0.82        0.55    -0.53
  ##   6    6 0.75     0.41        0.63     1.09        0.54     0.27        0.71     0.00
  ##     MLE.Var2 MLE.Var3 MAP.Var1 MAP.Var2 MAP.Var3
  ##   1     2.65    -0.53     1.06     1.59     0.00
  ##   2     1.06    -1.06     0.00     1.06    -1.06
  ##   3     1.06     2.65     1.06     1.06     0.53
  ##   4     2.65     2.65     1.59     1.59     0.53
  ##   5     0.53    -1.06    -0.53     0.53    -1.06
  ##   6     1.06     2.65     0.53     1.06     0.00
# EAP reliabilities
round(fres$EAP.rel,3)
  ##    Var1  Var2  Var3
  ##   0.574 0.452 0.541

## End(Not run)
</code></pre>

<hr>
<h2 id='mle.pcm.group'>
Maximum Likelihood Estimation of Person or Group Parameters
in the Generalized Partial Credit Model
</h2><span id='topic+mle.pcm.group'></span>

<h3>Description</h3>

<p>This function estimates person or group parameters in the
partial credit model (see Details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mle.pcm.group(dat, b, a=rep(1, ncol(dat)), group=NULL,
    pid=NULL, adj_eps=0.3, conv=1e-04, maxiter=30)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mle.pcm.group_+3A_dat">dat</code></td>
<td>

<p>A numeric <code class="reqn">N \times I</code> matrix
</p>
</td></tr>
<tr><td><code id="mle.pcm.group_+3A_b">b</code></td>
<td>

<p>Matrix with item thresholds
</p>
</td></tr>
<tr><td><code id="mle.pcm.group_+3A_a">a</code></td>
<td>

<p>Vector of item slopes
</p>
</td></tr>
<tr><td><code id="mle.pcm.group_+3A_group">group</code></td>
<td>

<p>Vector of group identifiers
</p>
</td></tr>
<tr><td><code id="mle.pcm.group_+3A_pid">pid</code></td>
<td>

<p>Vector of person identifiers
</p>
</td></tr>
<tr><td><code id="mle.pcm.group_+3A_adj_eps">adj_eps</code></td>
<td>

<p>Numeric value which is used in <code class="reqn">\varepsilon</code> adjustment
of the likelihood. A value of zero (or a very small
<code class="reqn">\varepsilon&gt;0</code>) corresponds to the usual maximum
likelihood estimate.
</p>
</td></tr>
<tr><td><code id="mle.pcm.group_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="mle.pcm.group_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that the generalized partial credit model holds.
In case one estimates a person parameter <code class="reqn">\theta_p</code>,
the log-likelihood is maximized and the following
estimating equation results: (see Penfield &amp; Bergeron, 2005):
</p>
<p style="text-align: center;"><code class="reqn"> 0=( \log L )'=\sum_i a_i \cdot [ \tilde{x}_{pi} -
    E(X_{pi} | \theta_p ) ] </code>
</p>

<p>where <code class="reqn">E(X_{pi} | \theta_p )</code> denotes the expected item response
conditionally on <code class="reqn">\theta_p</code>.
</p>
<p>With the method of <code class="reqn">\varepsilon</code>-adjustment (Bertoli-Barsotti &amp; Punzo, 2012;
Bertoli-Barsotti, Lando &amp; Punzo, 2014),
the observed item responses <code class="reqn">x_{pi}</code> are transformed such that
no perfect scores arise and bias is reduced. If <code class="reqn">S_p</code>
is the sum score of person <code class="reqn">p</code> and <code class="reqn">M_p</code> the maximum
score of this person, then the transformed sum scores <code class="reqn">\tilde{S}_p</code>
are
</p>
<p style="text-align: center;"><code class="reqn"> \tilde{S}_p=\varepsilon + \frac{M_p - 2 \varepsilon}{M_p} S_p</code>
</p>

<p>However, the adjustment is directly conducted on item responses
to also handle the case of the generalized partial credit
model with item slope parameters different from 1.
</p>
<p>In case one estimates a group parameter <code class="reqn">\theta_g</code>,
the following estimating equation is used:
</p>
<p style="text-align: center;"><code class="reqn"> 0=(\log L )'=\sum_p \sum_i a_i \cdot [ \tilde{x}_{pgi} -
    E(X_{pgi} | \theta_g ) ] </code>
</p>

<p>where persons <code class="reqn">p</code> are nested within a group <code class="reqn">g</code>.
The <code class="reqn">\varepsilon</code>-adjustment is then performed at the
group level, not at the individual level.
</p>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>person</code></td>
<td>
<p>Data frame with person or group parameters</p>
</td></tr>
<tr><td><code>data_adjeps</code></td>
<td>
<p>Modified dataset according to the
<code class="reqn">\varepsilon</code> adjustment.
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bertoli-Barsotti, L., &amp; Punzo, A. (2012).
Comparison of two bias reduction techniques for the Rasch model.
<em>Electronic Journal of Applied Statistical Analysis, 5</em>, 360-366.
</p>
<p>Bertoli-Barsotti, L., Lando, T., &amp; Punzo, A. (2014).
Estimating a Rasch Model via fuzzy empirical probability functions.
In D. Vicari, A. Okada, G. Ragozini &amp;  C. Weihs (Eds.).
<em>Analysis and Modeling of Complex Data in Behavioral and Social Sciences</em>,
Springer.
</p>
<p>Penfield, R. D., &amp; Bergeron, J. M. (2005). Applying a weighted
maximum likelihood latent trait estimator to the generalized
partial credit model. <em>Applied Psychological Measurement,
29</em>, 218-233.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Estimation of a group parameter for only one item per group
#############################################################################

data(data.si01)
dat &lt;- data.si01
# item parameter estimation (partial credit model) in TAM
library(TAM)
mod &lt;- TAM::tam.mml( dat[,2:3], irtmodel="PCM")
# extract item difficulties
b &lt;- matrix( mod$xsi$xsi, nrow=2, byrow=TRUE )
# groupwise estimation
res1 &lt;- sirt::mle.pcm.group( dat[,2:3], b=b, group=dat$idgroup )
# individual estimation
res2 &lt;- sirt::mle.pcm.group( dat[,2:3], b=b  )

#############################################################################
# EXAMPLE 2: Data Reading data.read
#############################################################################

data(data.read)
# estimate Rasch model
mod &lt;- sirt::rasch.mml2( data.read )
score &lt;- rowSums( data.read )
data.read &lt;- data.read[ order(score), ]
score &lt;- score[ order(score) ]
# compare different epsilon-adjustments
res30 &lt;- sirt::mle.pcm.group( data.read, b=matrix( mod$item$b, 12, 1 ),
               adj_eps=.3 )$person
res10 &lt;- sirt::mle.pcm.group( data.read, b=matrix( mod$item$b, 12, 1 ),
             adj_eps=.1 )$person
res05 &lt;- sirt::mle.pcm.group( data.read, b=matrix( mod$item$b, 12, 1 ),
              adj_eps=.05 )$person
# plot different scorings
plot( score, res05$theta, type="l", xlab="Raw score", ylab=expression(theta[epsilon]),
         main="Scoring with different epsilon-adjustments")
lines( score, res10$theta, col=2, lty=2 )
lines( score, res30$theta, col=4, lty=3 )

## End(Not run)
</code></pre>

<hr>
<h2 id='modelfit.sirt'>
Assessing Model Fit and Local Dependence by Comparing Observed and Expected
Item Pair Correlations
</h2><span id='topic+modelfit.sirt'></span><span id='topic+modelfit.cor.poly'></span><span id='topic+IRT.modelfit.sirt'></span>

<h3>Description</h3>

<p>This function computes several measures of absolute model fit and local
dependence indices for dichotomous item responses which are
based on comparing observed and expected frequencies of item pairs
(Chen, de la Torre &amp; Zhang, 2013; see <code><a href="CDM.html#topic+modelfit.cor">modelfit.cor</a></code>
for more details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelfit.sirt(object)

modelfit.cor.poly( data, probs, theta.k, f.qk.yi)

## S3 method for class 'sirt'
IRT.modelfit(object, mod, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modelfit.sirt_+3A_object">object</code></td>
<td>

<p>An object generated by <code><a href="#topic+rasch.mml2">rasch.mml2</a></code>,
<code><a href="#topic+rasch.mirtlc">rasch.mirtlc</a></code>, <code><a href="#topic+rasch.pml3">rasch.pml3</a></code> (<code><a href="#topic+rasch.pml2">rasch.pml2</a></code>),
<code><a href="#topic+smirt">smirt</a></code>, <code><a href="#topic+R2noharm">R2noharm</a></code>, <code><a href="#topic+noharm.sirt">noharm.sirt</a></code>,
<code><a href="#topic+gom.em">gom.em</a></code>, <code><a href="TAM.html#topic+tam.mml">TAM::tam.mml</a></code>,
<code><a href="TAM.html#topic+tam.mml.2pl">TAM::tam.mml.2pl</a></code>,
<code><a href="TAM.html#topic+tam.fa">TAM::tam.fa</a></code>,
<code><a href="mirt.html#topic+mirt">mirt::mirt</a></code>
</p>
</td></tr>
<tr><td><code id="modelfit.sirt_+3A_data">data</code></td>
<td>
<p>Dataset with polytomous item responses</p>
</td></tr>
<tr><td><code id="modelfit.sirt_+3A_probs">probs</code></td>
<td>
<p>Item response probabilities at grid <code>theta.k</code></p>
</td></tr>
<tr><td><code id="modelfit.sirt_+3A_theta.k">theta.k</code></td>
<td>
<p>Grid of theta vector</p>
</td></tr>
<tr><td><code id="modelfit.sirt_+3A_f.qk.yi">f.qk.yi</code></td>
<td>
<p>Individual posterior</p>
</td></tr>
<tr><td><code id="modelfit.sirt_+3A_mod">mod</code></td>
<td>
<p>Model name</p>
</td></tr>
<tr><td><code id="modelfit.sirt_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>modelfit</code></td>
<td>
<p>Model fit statistics:
</p>
<p><code>MADcor</code>: mean of absolute deviations in observed and expected correlations
(DiBello et al., 2007)
</p>
<p><code>SRMSR</code>: standardized mean square root of squared residuals
(Maydeu-Olivares, 2013; Maydeu-Olivares &amp; Joe, 2014)
</p>
<p><code>MX2</code>: Mean of <code class="reqn">\chi^2</code> statistics of all item pairs
(Chen &amp; Thissen, 1997)
</p>
<p><code>MADRESIDCOV</code>: Mean of absolute deviations of residual
covariances (McDonald &amp; Mok, 1995)
</p>
<p><code>MADQ3</code>: Mean of absolute values of <code class="reqn">Q_3</code> statistic (Yen, 1984)
</p>
<p><code>MADaQ3</code>: Mean of absolute values of centered <code class="reqn">Q_3</code> statistic
</p>
</td></tr>
<tr><td><code>itempairs</code></td>
<td>
<p>Fit of every item pair</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The function <code>modelfit.cor.poly</code> is just a wrapper to
<code><a href="TAM.html#topic+tam.modelfit">TAM::tam.modelfit</a></code> in the <span class="pkg">TAM</span> package.
</p>


<h3>References</h3>

<p>Chen, W., &amp; Thissen, D. (1997). Local dependence indexes for item pairs
using item response theory. <em>Journal of Educational and Behavioral Statistics,
22</em>, 265-289.
</p>
<p>DiBello, L. V., Roussos, L. A., &amp; Stout, W. F. (2007) Review of
cognitively diagnostic assessment and a summary of psychometric models.
In C. R. Rao and S. Sinharay (Eds.), <em>Handbook of Statistics</em>,
Vol. 26 (pp. 979&ndash;1030). Amsterdam: Elsevier.
</p>
<p>Maydeu-Olivares, A. (2013). Goodness-of-fit assessment of item response
theory models (with discussion).
<em>Measurement: Interdisciplinary Research and Perspectives,
11</em>, 71-137.
</p>
<p>Maydeu-Olivares, A., &amp; Joe, H. (2014). Assessing approximate fit in categorical
data analysis. <em>Multivariate Behavioral Research, 49</em>, 305-328.
</p>
<p>McDonald, R. P., &amp; Mok, M. M.-C. (1995). Goodness of fit in item response models.
<em>Multivariate Behavioral Research, 30</em>, 23-40.
</p>
<p>Yen, W. M. (1984). Effects of local item dependence on the fit and equating
performance of the three-parameter logistic model.
<em>Applied Psychological Measurement, 8</em>, 125-145.
</p>


<h3>See Also</h3>

<p>Supported classes: <code><a href="#topic+rasch.mml2">rasch.mml2</a></code>,
<code><a href="#topic+rasch.mirtlc">rasch.mirtlc</a></code>, <code><a href="#topic+rasch.pml3">rasch.pml3</a></code> (<code><a href="#topic+rasch.pml2">rasch.pml2</a></code>),
<code><a href="#topic+smirt">smirt</a></code>, <code><a href="#topic+R2noharm">R2noharm</a></code>, <code><a href="#topic+noharm.sirt">noharm.sirt</a></code>,
<code><a href="#topic+gom.em">gom.em</a></code>,
<code><a href="TAM.html#topic+tam.mml">TAM::tam.mml</a></code>,
<code><a href="TAM.html#topic+tam.mml.2pl">TAM::tam.mml.2pl</a></code>,
<code><a href="TAM.html#topic+tam.fa">TAM::tam.fa</a></code>,
<code><a href="mirt.html#topic+mirt">mirt::mirt</a></code>
</p>
<p>For more details on fit statistics of this function
see <code><a href="CDM.html#topic+modelfit.cor">CDM::modelfit.cor</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Reading data
#############################################################################
data(data.read)
dat &lt;- data.read
I &lt;- ncol(dat)

#*** Model 1: Rasch model
mod1 &lt;- sirt::rasch.mml2(dat)
fmod1 &lt;- sirt::modelfit.sirt( mod1 )
summary(fmod1)

#*** Model 1b: Rasch model in TAM package
library(TAM)
mod1b &lt;- TAM::tam.mml(dat)
fmod1b &lt;- sirt::modelfit.sirt( mod1b )
summary(fmod1b)

#*** Model 2: Rasch model with smoothed distribution
mod2 &lt;- sirt::rasch.mml2( dat, distribution.trait="smooth3" )
fmod2 &lt;- sirt::modelfit.sirt( mod2 )
summary(fmod2 )

#*** Model 3: 2PL model
mod3 &lt;- sirt::rasch.mml2( dat, distribution.trait="normal", est.a=1:I )
fmod3 &lt;- sirt::modelfit.sirt( mod3 )
summary(fmod3 )

#*** Model 3: 2PL model in TAM package
mod3b &lt;- TAM::tam.mml.2pl( dat )
fmod3b &lt;- sirt::modelfit.sirt(mod3b)
summary(fmod3b)
# model fit in TAM package
tmod3b &lt;- TAM::tam.modelfit(mod3b)
summary(tmod3b)
# model fit in mirt package
library(mirt)
mmod3b &lt;- sirt::tam2mirt(mod3b)   # convert to mirt object
mirt::M2(mmod3b$mirt)         # global fit statistic
mirt::residuals( mmod3b$mirt, type="LD")  # local dependence statistics

#*** Model 4: 3PL model with equal guessing parameter
mod4 &lt;- TAM::rasch.mml2( dat, distribution.trait="smooth3", est.a=1:I, est.c=rep(1,I) )
fmod4 &lt;- sirt::modelfit.sirt( mod4 )
summary(fmod4 )

#*** Model 5: Latent class model with 2 classes
mod5 &lt;- sirt::rasch.mirtlc( dat, Nclasses=2 )
fmod5 &lt;- sirt::modelfit.sirt( mod5 )
summary(fmod5 )

#*** Model 6: Rasch latent class model with 3 classes
mod6 &lt;- sirt::rasch.mirtlc( dat, Nclasses=3, modeltype="MLC1", mmliter=100)
fmod6 &lt;- sirt::modelfit.sirt( mod6 )
summary(fmod6 )

#*** Model 7: PML estimation
mod7 &lt;- sirt::rasch.pml3( dat )
fmod7 &lt;- sirt::modelfit.sirt( mod7 )
summary(fmod7 )

#*** Model 8: PML estimation
#      Modelling error correlations:
#          joint residual correlations for each item cluster
error.corr &lt;- diag(1,ncol(dat))
itemcluster &lt;- rep( 1:4,each=3 )
for ( ii in 1:3){
    ind.ii &lt;- which( itemcluster==ii )
    error.corr[ ind.ii, ind.ii ] &lt;- ii
        }
mod8 &lt;- sirt::rasch.pml3( dat, error.corr=error.corr )
fmod8 &lt;- sirt::modelfit.sirt( mod8 )
summary(fmod8 )

#*** Model 9: 1PL in smirt
Qmatrix &lt;- matrix( 1, nrow=I, ncol=1 )
mod9 &lt;- sirt::smirt( dat, Qmatrix=Qmatrix )
fmod9 &lt;- sirt::modelfit.sirt( mod9 )
summary(fmod9 )

#*** Model 10: 3-dimensional Rasch model in NOHARM
noharm.path &lt;- "c:/NOHARM"
Q &lt;- matrix( 0, nrow=12, ncol=3 )
Q[ cbind(1:12, rep(1:3,each=4) ) ] &lt;- 1
rownames(Q) &lt;- colnames(dat)
colnames(Q) &lt;- c("A","B","C")
# covariance matrix
P.pattern &lt;- matrix( 1, ncol=3, nrow=3 )
P.init &lt;- 0.8+0*P.pattern
diag(P.init) &lt;- 1
# loading matrix
F.pattern &lt;- 0*Q
F.init &lt;- Q
# estimate model
mod10 &lt;- sirt::R2noharm( dat=dat, model.type="CFA", F.pattern=F.pattern,
            F.init=F.init, P.pattern=P.pattern, P.init=P.init,
            writename="ex4e", noharm.path=noharm.path, dec="," )
fmod10 &lt;- sirt::modelfit.sirt( mod10 )
summary(fmod10)

#*** Model 11: Rasch model in mirt package
library(mirt)
mod11 &lt;- mirt::mirt(dat, 1, itemtype="Rasch",verbose=TRUE)
fmod11 &lt;- sirt::modelfit.sirt( mod11 )
summary(fmod11)
# model fit in mirt package
mirt::M2(mod11)
mirt::residuals(mod11)

## End(Not run)
</code></pre>

<hr>
<h2 id='monoreg.rowwise'>
Monotone Regression for Rows or Columns in a Matrix
</h2><span id='topic+monoreg.rowwise'></span><span id='topic+monoreg.colwise'></span>

<h3>Description</h3>

<p>Monotone (isotone) regression for rows (<code>monoreg.rowwise</code>) or
columns (<code>monoreg.colwise</code>) in a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>monoreg.rowwise(yM, wM)

monoreg.colwise(yM, wM)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="monoreg.rowwise_+3A_ym">yM</code></td>
<td>

<p>Matrix with dependent variable for the regression.
Values are assumed to be sorted.
</p>
</td></tr>
<tr><td><code id="monoreg.rowwise_+3A_wm">wM</code></td>
<td>

<p>Matrix with weights for every entry in the <code>yM</code>
matrix.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix with fitted values
</p>


<h3>Note</h3>

<p>This function is used for fitting the ISOP model
(see <code><a href="#topic+isop.dich">isop.dich</a></code>).
</p>


<h3>Author(s)</h3>

<p>Alexander Robitzsch
</p>
<p>The <code>monoreg</code> function from the <span class="pkg">fdrtool</span>
package is simply extended to handle matrix input.
</p>


<h3>See Also</h3>

<p>See also the <code>monoreg</code> function from the <span class="pkg">fdrtool</span>
package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(22.5, 23.33, 20.83, 24.25 )
w &lt;- c( 3,3,3,2)
# define matrix input
yM &lt;- matrix( 0, nrow=2, ncol=4 )
wM &lt;- yM
yM[1,] &lt;- yM[2,] &lt;- y
wM[1,] &lt;- w
wM[2,] &lt;- c(1,3,4, 3 )

# fit rowwise monotone regression
monoreg.rowwise( yM, wM )
# compare results with monoreg function from fdrtool package
## Not run: 
miceadds::library_install("fdrtool")
fdrtool::monoreg(x=yM[1,], w=wM[1,])$yf
fdrtool::monoreg(x=yM[2,], w=wM[2,])$yf

## End(Not run)
</code></pre>

<hr>
<h2 id='nedelsky-methods'>
Functions for the Nedelsky Model
</h2><span id='topic+nedelsky.sim'></span><span id='topic+nedelsky.latresp'></span><span id='topic+nedelsky.irf'></span>

<h3>Description</h3>

<p>Functions for simulating and estimating the Nedelsky model
(Bechger et al., 2003, 2005). <code>nedelsky.sim</code> can be used for
simulating the model, <code>nedelsky.irf</code> computes the item response
function and can be used for example when estimating the
Nedelsky model in the <span class="pkg">mirt</span> package or using the
<code><a href="#topic+xxirt">xxirt</a></code> function in the <span class="pkg">sirt</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># simulating the Nedelsky model
nedelsky.sim(theta, b, a=NULL, tau=NULL)

# creating latent responses of the Nedelsky model
nedelsky.latresp(K)

# computing the item response function of the Nedelsky model
nedelsky.irf(Theta, K, b, a, tau, combis, thdim=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nedelsky-methods_+3A_theta">theta</code></td>
<td>

<p>Unidimensional ability (theta)
</p>
</td></tr>
<tr><td><code id="nedelsky-methods_+3A_b">b</code></td>
<td>

<p>Matrix of category difficulties
</p>
</td></tr>
<tr><td><code id="nedelsky-methods_+3A_a">a</code></td>
<td>

<p>Vector of item discriminations
</p>
</td></tr>
<tr><td><code id="nedelsky-methods_+3A_tau">tau</code></td>
<td>

<p>Category attractivity parameters <code class="reqn">\tau</code> (see Bechger et al., 2005)
</p>
</td></tr>
<tr><td><code id="nedelsky-methods_+3A_k">K</code></td>
<td>
<p>(Maximum) Number of distractors of the used multiple choice items</p>
</td></tr>
<tr><td><code id="nedelsky-methods_+3A_theta">Theta</code></td>
<td>

<p>Theta vector. Note that the Nedelsky model can be only specified
as models with between item dimensionality (defined in <code>thdim</code>).
</p>
</td></tr>
<tr><td><code id="nedelsky-methods_+3A_combis">combis</code></td>
<td>

<p>Latent response classes as produced by <code>nedelsky.latresp</code>.
</p>
</td></tr>
<tr><td><code id="nedelsky-methods_+3A_thdim">thdim</code></td>
<td>

<p>Theta dimension at which the item loads
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that for item <code class="reqn">i</code> there exists <code class="reqn">K+1</code> categories <code class="reqn">0,1,...,K</code>.
The category 0 denotes the correct alternative. The Nedelsky model assumes
that a respondent eliminates all distractors which are thought to be
incorrect and guesses the solution from the remaining alternatives.
This means, that for item <code class="reqn">i</code>, <code class="reqn">K</code> latent variables <code class="reqn">S_{ik}</code>
are defined which indicate whether alternative <code class="reqn">k</code> has been correctly
identified as a distractor. By definition, the correct alternative is never
been judged as wrong by the respondent.
</p>
<p>Formally, the Nedelsky model assumes a 2PL model for eliminating each of the
distractors
</p>
<p style="text-align: center;"><code class="reqn">P(S_{ik}=1 | \theta )=invlogit[ a_i ( \theta - b_{ik} ) ] </code>
</p>

<p>where <code class="reqn">\theta</code> is the person ability and <code class="reqn">b_{ik}</code> are
distractor difficulties.
</p>
<p>The guessing process of the Nedelsky model is defined as
</p>
<p style="text-align: center;"><code class="reqn">P(X_i=j | \theta, S_{i1}, ..., S_{iK} )=
\frac{ ( 1- S_{ij} ) \tau_{ij} }{ \sum_{k=0}^K [ ( 1- S_{ik} ) \tau_{ik} ] }</code>
</p>

<p>where <code class="reqn">\tau_{ij}</code> are attractivity parameters of alternative <code class="reqn">j</code>.
By definition <code class="reqn">\tau_{i0}</code> is set to 1. By default, all attractivity parameters
are set to 1.
</p>


<h3>References</h3>

<p>Bechger, T. M., Maris, G., Verstralen, H. H. F. M., &amp; Verhelst, N. D. (2003).
<em>The Nedelsky model for multiple-choice items</em>.
CITO Research Report, 2003-5.
</p>
<p>Bechger, T. M., Maris, G., Verstralen, H. H. F. M., &amp; Verhelst, N. D. (2005).
The Nedelsky model for multiple-choice items.
In L. van der Ark, M. Croon, &amp; Sijtsma, K. (Eds.).
<em>New developments in categorical data analysis for the social and behavioral
sciences</em>, pp. 187-206. Mahwah, Lawrence Erlbaum.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Simulated data according to the Nedelsky model
#############################################################################

#*** simulate data
set.seed(123)
I &lt;- 20          # number of items
b &lt;- matrix(NA,I,ncol=3)
b[,1] &lt;- -0.5 + stats::runif( I, -.75, .75 )
b[,2] &lt;- -1.5 + stats::runif( I, -.75, .75 )
b[,3] &lt;- -2.5 + stats::runif( I, -.75, .75 )
K &lt;- 3           # number of distractors
N &lt;- 2000        # number of persons
# apply simulation function
dat &lt;- sirt::nedelsky.sim( theta=stats::rnorm(N,sd=1.2), b=b )

#*** latent response patterns
K &lt;- 3
combis &lt;- sirt::nedelsky.latresp(K=3)

#*** defining the Nedelsky item response function for estimation in mirt
par &lt;- c( 3, rep(-1,K), 1, rep(1,K+1),1)
names(par) &lt;- c("K", paste0("b",1:K), "a", paste0("tau", 0:K),"thdim")
est &lt;- c( FALSE, rep(TRUE,K), rep(FALSE, K+1 + 2 ) )
names(est) &lt;- names(par)
nedelsky.icc &lt;- function( par, Theta, ncat ){
     K &lt;- par[1]
     b &lt;- par[ 1:K + 1]
     a &lt;- par[ K+2]
     tau &lt;- par[1:(K+1) + (K+2) ]
     thdim &lt;- par[ K+2+K+1 +1 ]
     probs &lt;- sirt::nedelsky.irf( Theta, K=K, b=b, a=a, tau=tau, combis,
                    thdim=thdim  )$probs
     return(probs)
}
name &lt;- "nedelsky"
# create item response function
nedelsky.itemfct &lt;- mirt::createItem(name, par=par, est=est, P=nedelsky.icc)

#*** define model in mirt
mirtmodel &lt;- mirt::mirt.model("
           F1=1-20
           COV=F1*F1
           # define some prior distributions
           PRIOR=(1-20,b1,norm,-1,2),(1-20,b2,norm,-1,2),
                   (1-20,b3,norm,-1,2)
        " )

itemtype &lt;- rep("nedelsky", I )
customItems &lt;- list("nedelsky"=nedelsky.itemfct)
# define parameters to be estimated
mod1.pars &lt;- mirt::mirt(dat, mirtmodel, itemtype=itemtype,
                   customItems=customItems, pars="values")
# estimate model
mod1 &lt;- mirt::mirt(dat,mirtmodel, itemtype=itemtype, customItems=customItems,
               pars=mod1.pars, verbose=TRUE  )
# model summaries
print(mod1)
summary(mod1)
mirt.wrapper.coef( mod1 )$coef
mirt.wrapper.itemplot(mod1,ask=TRUE)

#******************************************************
# fit Nedelsky model with xxirt function in sirt

# define item class for xxirt
item_nedelsky &lt;- sirt::xxirt_createDiscItem( name="nedelsky", par=par,
                est=est, P=nedelsky.icc,
                prior=c( b1="dnorm", b2="dnorm", b3="dnorm" ),
                prior_par1=c( b1=-1, b2=-1, b3=-1),
                prior_par2=c(b1=2, b2=2, b3=2) )
customItems &lt;- list( item_nedelsky )

#---- definition theta distribution
#** theta grid
Theta &lt;- matrix( seq(-6,6,length=21), ncol=1 )
#** theta distribution
P_Theta1 &lt;- function( par, Theta, G){
    mu &lt;- par[1]
    sigma &lt;- max( par[2], .01 )
    TP &lt;- nrow(Theta)
    pi_Theta &lt;- matrix( 0, nrow=TP, ncol=G)
    pi1 &lt;- dnorm( Theta[,1], mean=mu, sd=sigma )
    pi1 &lt;- pi1 / sum(pi1)
    pi_Theta[,1] &lt;- pi1
    return(pi_Theta)
}
#** create distribution class
par_Theta &lt;- c( "mu"=0, "sigma"=1 )
customTheta &lt;- sirt::xxirt_createThetaDistribution( par=par_Theta, est=c(FALSE,TRUE),
                   P=P_Theta1 )

#-- create parameter table
itemtype &lt;- rep( "nedelsky", I )
partable &lt;- sirt::xxirt_createParTable( dat, itemtype=itemtype, customItems=customItems)

# estimate model
mod2 &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable, customItems=customItems,
                    customTheta=customTheta)
summary(mod2)
# compare sirt::xxirt and mirt::mirt
logLik(mod2)
mod1@Fit$logLik

#############################################################################
# EXAMPLE 2: Multiple choice dataset data.si06
#############################################################################

data(data.si06)
dat &lt;- data.si06

#*** create latent responses
combis &lt;- sirt::nedelsky.latresp(K)
I &lt;- ncol(dat)
#*** define item response function
K &lt;- 3
par &lt;- c( 3, rep(-1,K), 1, rep(1,K+1),1)
names(par) &lt;- c("K", paste0("b",1:K), "a", paste0("tau", 0:K),"thdim")
est &lt;- c( FALSE, rep(TRUE,K), rep(FALSE, K+1 + 2 ) )
names(est) &lt;- names(par)
nedelsky.icc &lt;- function( par, Theta, ncat ){
     K &lt;- par[1]
     b &lt;- par[ 1:K + 1]
     a &lt;- par[ K+2]
     tau &lt;- par[1:(K+1) + (K+2) ]
     thdim &lt;- par[ K+2+K+1 +1 ]
     probs &lt;- sirt::nedelsky.irf( Theta, K=K, b=b, a=a, tau=tau, combis,
                    thdim=thdim  )$probs
     return(probs)
}
name &lt;- "nedelsky"
# create item response function
nedelsky.itemfct &lt;- mirt::createItem(name, par=par, est=est, P=nedelsky.icc)

#*** define model in mirt
mirtmodel &lt;- mirt::mirt.model("
           F1=1-14
           COV=F1*F1
           PRIOR=(1-14,b1,norm,-1,2),(1-14,b2,norm,-1,2),
                   (1-14,b3,norm,-1,2)
        " )

itemtype &lt;- rep("nedelsky", I )
customItems &lt;- list("nedelsky"=nedelsky.itemfct)
# define parameters to be estimated
mod1.pars &lt;- mirt::mirt(dat, mirtmodel, itemtype=itemtype,
                   customItems=customItems, pars="values")

#*** estimate model
mod1 &lt;- mirt::mirt(dat,mirtmodel, itemtype=itemtype, customItems=customItems,
               pars=mod1.pars, verbose=TRUE )
#*** summaries
print(mod1)
summary(mod1)
mirt.wrapper.coef( mod1 )$coef
mirt.wrapper.itemplot(mod1,ask=TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='noharm.sirt'>
NOHARM Model in <span class="rlang"><b>R</b></span>
</h2><span id='topic+noharm.sirt'></span><span id='topic+summary.noharm.sirt'></span>

<h3>Description</h3>

<p>The function is an <span class="rlang"><b>R</b></span> implementation of the normal ogive harmonic analysis
robust method (the NOHARM model; McDonald, 1997). Exploratory and confirmatory
multidimensional item response models for dichotomous data using the
probit link function can be estimated. Lower asymptotes (guessing parameters)
and upper asymptotes (one minus slipping parameters) can be provided as
fixed values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>noharm.sirt(dat, pm=NULL, N=NULL, weights=NULL, Fval=NULL, Fpatt=NULL, Pval=NULL,
   Ppatt=NULL, Psival=NULL, Psipatt=NULL, dimensions=NULL, lower=0, upper=1, wgtm=NULL,
   pos.loading=FALSE, pos.variance=FALSE, pos.residcorr=FALSE, maxiter=1000, conv=1e-6,
   optimizer="nlminb", par_lower=NULL, reliability=FALSE, ...)

## S3 method for class 'noharm.sirt'
summary(object, file=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="noharm.sirt_+3A_dat">dat</code></td>
<td>

<p>Matrix of dichotomous item responses. This matrix may contain missing data (indicated
by <code>NA</code>) but missingness is assumed to be missing completely at random (MCAR).
Alternatively, a product-moment matrix <code>pm</code> can be used as input.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_pm">pm</code></td>
<td>
<p>Optional product-moment matrix</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_n">N</code></td>
<td>
<p>Sample size if <code>pm</code> is provided</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_weights">weights</code></td>
<td>

<p>Optional vector of student weights.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_fval">Fval</code></td>
<td>

<p>Initial or fixed values of the loading matrix <code class="reqn">\bold{F}</code>.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_fpatt">Fpatt</code></td>
<td>

<p>Pattern matrix of the loading matrix <code class="reqn">\bold{F}</code>. If elements should be
estimated, then an entry of 1 must be included in the pattern matrix. Parameters
which should be estimated with equality constraints must be indicated by same
integers but values largers than 1.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_pval">Pval</code></td>
<td>

<p>Initial or fixed values for the covariance matrix <code class="reqn">\bold{P}</code>.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_ppatt">Ppatt</code></td>
<td>

<p>Pattern matrix for the covariance matrix <code class="reqn">\bold{P}</code>.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_psival">Psival</code></td>
<td>

<p>Initial or fixed values for the matrix of residual correlations <code class="reqn">\bold{\Psi}</code>.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_psipatt">Psipatt</code></td>
<td>

<p>Pattern matrix for the matrix of residual correlations <code class="reqn">\bold{\Psi}</code>.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_dimensions">dimensions</code></td>
<td>

<p>Number of dimensions if an exploratory factor analysis should be estimated.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_lower">lower</code></td>
<td>

<p>Fixed vector (or numeric) of lower asymptotes <code class="reqn">c_i</code>.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_upper">upper</code></td>
<td>

<p>Fixed vector (or numeric) of upper asymptotes <code class="reqn">d_i</code>.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_wgtm">wgtm</code></td>
<td>

<p>Matrix with positive entries which indicates by a positive entry which
item pairs should be used for estimation.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_pos.loading">pos.loading</code></td>
<td>
<p>An optional logical indicating whether all entries in the
loading matrix <code class="reqn">\bold{F}</code> should be positive</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_pos.variance">pos.variance</code></td>
<td>
<p>An optional logical indicating whether all variances (i.e.
diagonal entries in <code class="reqn">\bold{P}</code>) should be positive</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_pos.residcorr">pos.residcorr</code></td>
<td>
<p>An optional logical indicating whether all
entries in the matrix of residual correlations <code class="reqn">\bold{\Psi}</code> should
be positive</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_par_lower">par_lower</code></td>
<td>
<p>Optional vector of lower parameter bounds</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion for parameters
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_optimizer">optimizer</code></td>
<td>

<p>Optimization function to be used. Can be <code>"nlminb"</code> for
<code><a href="stats.html#topic+nlminb">stats::nlminb</a></code>
or <code>"optim"</code> for <code><a href="stats.html#topic+optim">stats::optim</a></code>.
</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_reliability">reliability</code></td>
<td>
<p>Logical indicating whether reliability should be computed.</p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed. </p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_object">object</code></td>
<td>
<p>Object of class <code>noharm.sirt</code></p>
</td></tr>
<tr><td><code id="noharm.sirt_+3A_file">file</code></td>
<td>
<p>String indicating a file name for summary.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The NOHARM item response model follows the response equation
</p>
<p style="text-align: center;"><code class="reqn">P( X_{pi}=1 | \bold{\theta}_p )=c_i + ( d_i - c_i )
\Phi( f_{i0} + f_{i1} \theta_{p1} + ... +
f_{iD} \theta_{pD} ) </code>
</p>

<p>for item responses <code class="reqn">X_{pi}</code> of person <code class="reqn">p</code> on
item <code class="reqn">i</code>, <code class="reqn">\bold{F}=(f_{id})</code> is a loading matrix and <code class="reqn">\bold{P}</code>
the covariance matrix of <code class="reqn">\bold{\theta}_p</code>. The lower
asymptotes <code class="reqn">c_i</code> and upper asymptotes <code class="reqn">d_i</code> must be
provided as fixed values.
The response equation can be equivalently written by introducing a latent
continuous item response <code class="reqn">X_{pi}^\ast</code>
</p>
<p style="text-align: center;"><code class="reqn"> X_{pi}^\ast=f_{i0} + f_{i1} \theta_{p1} + ... +  f_{iD} \theta_{pD} + e_{pi} </code>
</p>

<p>with a standard normally distributed residual <code class="reqn">e_{pi}</code>. These residuals
have a correlation matrix <code class="reqn">\bold{\Psi}</code> with ones in the diagonal.
In this <span class="rlang"><b>R</b></span> implementation of the NOHARM model, correlations between residuals
are allowed.
</p>
<p>The estimation relies on a Hermite series approximation of the normal ogive
item response functions. In more detail, a series expansion
</p>
<p style="text-align: center;"><code class="reqn">\Phi(x)=b_0 + b_1 H_1(x) + b_2 H_2(x) + b_3 H_3(x)</code>
</p>
<p> is used
(McDonald, 1982a).
This enables to express cross products <code class="reqn">p_{ij}=P(X_i=1, X_j=1)</code> as a function of
unknown model parameters
</p>
<p style="text-align: center;"><code class="reqn">\hat{p}_{ij}=b_{0i} b_{0j} + \sum_{m=1}^3 b_{mi} b_{mj}
\left( \frac{\bold{f}_i \bold{P} \bold{f}_j }{\sqrt{ (1+\bold{f}_i \bold{P} \bold{f}_i)
(1+\bold{f}_j \bold{P} \bold{f}_j)}} \right) ^m </code>
</p>

<p>where <code class="reqn">b_{0i}=p_{i}=P(X_i=1)=c_i + (d_i - c_i) \Phi(\tau_i)</code>,
<code class="reqn">b_{1i}=(d_i-c_i)\phi(\tau_i)</code>, <code class="reqn">b_{2i}=(d_i-c_i)\tau_i \phi(\tau_i) / \sqrt{2}</code>,
and <code class="reqn">b_{3i}=(d_i-c_i)(\tau_i^2 - 1)\phi(\tau_i) / \sqrt{6}</code>.
</p>
<p>The least squares criterion <code class="reqn">\sum_{i&lt;j} ( p_{ij} - \hat{p}_{ij})^2</code> is used
for estimating unknown model parameters (McDonald, 1982a, 1982b, 1997).
</p>
<p>For derivations of standard errors and fit statistics see
Maydeu-Olivares (2001) and Swaminathan and Rogers (2016).
</p>
<p>For the statistical properties of the NOHARM approach see
Knol and Berger (1991), Finch (2011) or Svetina and Levy (2016).
</p>


<h3>Value</h3>

<p>A list. The most important entries are
</p>
<table>
<tr><td><code>tanaka</code></td>
<td>
<p>Tanaka fit statistic</p>
</td></tr>
<tr><td><code>rmsr</code></td>
<td>
<p>RMSR fit statistic</p>
</td></tr>
<tr><td><code>N.itempair</code></td>
<td>
<p>Sample size per item pair</p>
</td></tr>
<tr><td><code>pm</code></td>
<td>
<p>Product moment matrix</p>
</td></tr>
<tr><td><code>wgtm</code></td>
<td>
<p>Matrix of weights for each item pair</p>
</td></tr>
<tr><td><code>sumwgtm</code></td>
<td>
<p>Sum of lower triangle matrix <code>wgtm</code></p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p>Lower asymptotes</p>
</td></tr>
<tr><td><code>upper</code></td>
<td>
<p>Upper asymptotes</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Residual matrix from approximation of the <code>pm</code> matrix</p>
</td></tr>
<tr><td><code>final.constants</code></td>
<td>
<p>Final constants</p>
</td></tr>
<tr><td><code>factor.cor</code></td>
<td>
<p>Covariance matrix</p>
</td></tr>
<tr><td><code>thresholds</code></td>
<td>
<p>Threshold parameters</p>
</td></tr>
<tr><td><code>uniquenesses</code></td>
<td>
<p>Uniquenesses</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>Matrix of standardized factor loadings (delta parametrization)</p>
</td></tr>
<tr><td><code>loadings.theta</code></td>
<td>
<p>Matrix of factor loadings <code class="reqn">\bold{F}</code> (theta parametrization)</p>
</td></tr>
<tr><td><code>residcorr</code></td>
<td>
<p>Matrix of residual correlations</p>
</td></tr>
<tr><td><code>Nobs</code></td>
<td>
<p>Number of observations</p>
</td></tr>
<tr><td><code>Nitems</code></td>
<td>
<p>Number of items</p>
</td></tr>
<tr><td><code>Fpatt</code></td>
<td>
<p>Pattern loading matrix for <code class="reqn">\bold{F}</code></p>
</td></tr>
<tr><td><code>Ppatt</code></td>
<td>
<p>Pattern loading matrix for <code class="reqn">\bold{P}</code></p>
</td></tr>
<tr><td><code>Psipatt</code></td>
<td>
<p>Pattern loading matrix for <code class="reqn">\bold{\Psi}</code></p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Used dataset</p>
</td></tr>
<tr><td><code>dimensions</code></td>
<td>
<p>Number of dimensions</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>Nestpars</code></td>
<td>
<p>Number of estimated parameters</p>
</td></tr>
<tr><td><code>chisquare</code></td>
<td>
<p>Statistic <code class="reqn">\chi^2</code></p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Degrees of freedom</p>
</td></tr>
<tr><td><code>chisquare_df</code></td>
<td>
<p>Ratio <code class="reqn">\chi^2 / df</code></p>
</td></tr>
<tr><td><code>rmsea</code></td>
<td>
<p>RMSEA statistic</p>
</td></tr>
<tr><td><code>p.chisquare</code></td>
<td>
<p>Significance for <code class="reqn">\chi^2</code> statistic</p>
</td></tr>
<tr><td><code>omega.rel</code></td>
<td>
<p>Reliability of the sum score according to Green and Yang (2009)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Finch, H. (2011). Multidimensional item response theory parameter estimation with
nonsimple structure items. <em>Applied Psychological Measurement, 35</em>(1), 67-82.
<a href="https://doi.org/10.1177/0146621610367787">doi:10.1177/0146621610367787</a>
</p>
<p>Fraser, C., &amp; McDonald, R. P. (1988). NOHARM: Least squares item factor analysis.
<em>Multivariate Behavioral Research, 23</em>, 267-269.
<a href="https://doi.org/10.1207/s15327906mbr2302_9">doi:10.1207/s15327906mbr2302_9</a>
</p>
<p>Fraser, C., &amp; McDonald, R. P. (2012). <em>NOHARM 4 Manual</em>. <br />
http://noharm.niagararesearch.ca/nh4man/nhman.html.
</p>
<p>Knol, D. L., &amp; Berger, M. P. (1991). Empirical comparison between factor analysis
and multidimensional item response models. <em>Multivariate Behavioral Research, 26</em>(3),
457-477. <a href="https://doi.org/10.1207/s15327906mbr2603_5">doi:10.1207/s15327906mbr2603_5</a>
</p>
<p>Maydeu-Olivares, A. (2001). Multidimensional item response theory modeling of binary data:
Large sample properties of NOHARM estimates.
<em>Journal of Educational and Behavioral Statistics, 26</em>(1), 51-71.
<a href="https://doi.org/10.3102/10769986026001051">doi:10.3102/10769986026001051</a>
</p>
<p>McDonald, R. P. (1982a). Linear versus nonlinear models in item response theory.
<em>Applied Psychological Measurement, 6</em>(4), 379-396.
<a href="https://doi.org/10.1177/014662168200600402">doi:10.1177/014662168200600402</a>
</p>
<p>McDonald, R. P. (1982b). <em>Unidimensional and multidimensional models for
item response theory</em>. I.R.T., C.A.T. conference, Minneapolis, 1982, Proceedings.
</p>
<p>McDonald, R. P. (1997). Normal-ogive multidimensional model.
In W. van der Linden &amp; R. K. Hambleton (1997):
<em>Handbook of modern item response theory</em> (pp. 257-269).
New York: Springer. <a href="https://doi.org/10.1007/978-1-4757-2691-6">doi:10.1007/978-1-4757-2691-6</a>
</p>
<p>Svetina, D., &amp; Levy, R. (2016). Dimensionality in compensatory MIRT when complex structure
exists: Evaluation of DETECT and NOHARM. <em>The Journal of Experimental Education, 84</em>(2),
398-420. <a href="https://doi.org/10.1080/00220973.2015.1048845">doi:10.1080/00220973.2015.1048845</a>
</p>
<p>Swaminathan, H., &amp; Rogers, H. J. (2016). Normal-ogive multidimensional models.
In W. J. van der Linden (Ed.). <em>Handbook of item response theory.
Volume One: Models</em> (pp. 167-187). Boca Raton: CRC Press.
<a href="https://doi.org/10.1201/9781315374512">doi:10.1201/9781315374512</a>
</p>


<h3>See Also</h3>

<p>EAP person parameter estimates can be obtained by <code><a href="#topic+R2noharm.EAP">R2noharm.EAP</a></code>.
</p>
<p>Model fit can be assessed by <code><a href="#topic+modelfit.sirt">modelfit.sirt</a></code>.
</p>
<p>See <code><a href="#topic+R2noharm">R2noharm</a></code> for running the NOHARM software from within <span class="rlang"><b>R</b></span>.
</p>
<p>See Fraser and McDonald (1988, 2012) for an implementation of the NOHARM model which
is available as freeware (http://noharm.niagararesearch.ca/;
the link seems to be broken in the meanwhile).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Two-dimensional IRT model with 10 items
#############################################################################

#**** data simulation
set.seed(9776)
N &lt;- 3400 # sample size
# define difficulties
f0 &lt;- c( .5, .25, -.25, -.5, 0, -.5, -.25, .25, .5, 0 )
I &lt;- length(f0)
# define loadings
f1 &lt;- matrix( 0, I, 2 )
f1[ 1:5,1] &lt;- c(.8,.7,.6,.5, .5)
f1[ 6:10,2] &lt;- c(.8,.7,.6,.5, .5 )
# covariance matrix
Pval &lt;- matrix( c(1,.5,.5,1), 2, 2 )
# simulate theta
library(mvtnorm)
theta &lt;- mvtnorm::rmvnorm(N, mean=c(0,0), sigma=Pval )
# simulate item responses
dat &lt;- matrix( NA, N, I )
for (ii in 1:I){ # ii &lt;- 1
    dat[,ii] &lt;- 1*( stats::pnorm(f0[ii]+theta[,1]*f1[ii,1]+theta[,2]*f1[ii,2])&gt;
                     stats::runif(N) )
        }
colnames(dat) &lt;- paste0("I", 1:I)

#**** Model 1: Two-dimensional CFA with estimated item loadings
# define pattern matrices
Pval &lt;- .3+0*Pval
Ppatt &lt;- 1*(Pval&gt;0)
diag(Ppatt) &lt;- 0
diag(Pval) &lt;- 1
Fval &lt;- .7 * ( f1&gt;0)
Fpatt &lt;- 1 * ( Fval &gt; 0 )
# estimate model
mod1 &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt, Fpatt=Fpatt, Fval=Fval, Pval=Pval )
summary(mod1)
# EAP ability estimates
pmod1 &lt;- sirt::R2noharm.EAP(mod1, theta.k=seq(-4,4,len=10) )
# model fit
summary( sirt::modelfit.sirt(mod1) )

## Not run: 
#*** compare results with NOHARM software
noharm.path &lt;- "c:/NOHARM"   # specify path for noharm software
mod1a &lt;- sirt::R2noharm( dat=dat, model.type="CFA",  F.pattern=Fpatt, F.init=Fval,
             P.pattern=Ppatt, P.init=Pval, writename="r2noharm_example",
             noharm.path=noharm.path, dec="," )
summary(mod1a)

#**** Model 1c: put some equality constraints
Fpatt[ c(1,4),1] &lt;- 3
Fpatt[ cbind( c(3,7), c(1,2)) ] &lt;- 4
mod1c &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt, Fpatt=Fpatt, Fval=Fval, Pval=Pval)
summary(mod1c)

#**** Model 2: Two-dimensional CFA with correlated residuals
# define pattern matrix for residual correlation
Psipatt &lt;- 0*diag(I)
Psipatt[1,2] &lt;- 1
Psival &lt;- 0*Psipatt
# estimate model
mod2 &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval,
            Psival=Psival, Psipatt=Psipatt )
summary(mod2)

#**** Model 3: Two-dimensional Rasch model
# pattern matrices
Fval &lt;- matrix(0,10,2)
Fval[1:5,1] &lt;- Fval[6:10,2] &lt;- 1
Fpatt &lt;- 0*Fval
Ppatt &lt;- Pval &lt;- matrix(1,2,2)
Pval[1,2] &lt;- Pval[2,1] &lt;- 0
# estimate model
mod3 &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval )
summary(mod3)
# model fit
summary( sirt::modelfit.sirt( mod3 ))

#** compare fit with NOHARM
noharm.path &lt;- "c:/NOHARM"
P.pattern &lt;- Ppatt ; P.init &lt;- Pval
F.pattern &lt;- Fpatt ; F.init &lt;- Fval
mod3b &lt;- sirt::R2noharm( dat=dat, model.type="CFA",
             F.pattern=F.pattern, F.init=F.init, P.pattern=P.pattern,
             P.init=P.init, writename="example_sim_2dim_rasch",
             noharm.path=noharm.path, dec="," )
summary(mod3b)

#############################################################################
# EXAMPLE 2: data.read
#############################################################################

data(data.read)
dat &lt;- data.read
I &lt;- ncol(dat)

#**** Model 1: Unidimensional Rasch model
Fpatt &lt;- matrix( 0, I, 1 )
Fval &lt;- 1 + 0*Fpatt
Ppatt &lt;- Pval &lt;- matrix(1,1,1)
# estimate model
mod1 &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval )
summary(mod1)
plot(mod1)    # semPaths plot

#**** Model 2: Rasch model in which item pairs within a testlet are excluded
wgtm &lt;- matrix( 1, I, I )
wgtm[1:4,1:4] &lt;- wgtm[5:8,5:8] &lt;- wgtm[ 9:12, 9:12] &lt;- 0
# estimation
mod2 &lt;- sirt::noharm.sirt(dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval, wgtm=wgtm)
summary(mod2)

#**** Model 3: Rasch model with correlated residuals
Psipatt &lt;- Psival &lt;- 0*diag(I)
Psipatt[1:4,1:4] &lt;- Psipatt[5:8,5:8] &lt;- Psipatt[ 9:12, 9:12] &lt;- 1
diag(Psipatt) &lt;- 0
Psival &lt;- .6*(Psipatt&gt;0)
# estimation
mod3 &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval,
            Psival=Psival, Psipatt=Psipatt )
summary(mod3)
# allow only positive residual correlations
mod3b &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt, Fpatt=Fpatt, Fval=Fval, Pval=Pval,
            Psival=Psival, Psipatt=Psipatt, pos.residcorr=TRUE)
summary(mod3b)
#* constrain residual correlations
Psipatt[1:4,1:4] &lt;- 2
Psipatt[5:8,5:8] &lt;- 3
Psipatt[ 9:12, 9:12] &lt;- 4
mod3c &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt, Fpatt=Fpatt, Fval=Fval, Pval=Pval,
            Psival=Psival, Psipatt=Psipatt, pos.residcorr=TRUE)
summary(mod3c)

#**** Model 4: Rasch testlet model
Fval &lt;- Fpatt &lt;- matrix( 0, I, 4 )
Fval[,1] &lt;- Fval[1:4,2] &lt;- Fval[5:8,3] &lt;- Fval[9:12,4 ] &lt;- 1
Ppatt &lt;- Pval &lt;- diag(4)
colnames(Ppatt) &lt;- c("g", "A", "B","C")
Pval &lt;- .5*Pval
# estimation
mod4 &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval  )
summary(mod4)
# allow only positive variance entries
mod4b &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval,
               pos.variance=TRUE )
summary(mod4b)

#**** Model 5: Bifactor model
Fval &lt;- matrix( 0, I, 4 )
Fval[,1] &lt;- Fval[1:4,2] &lt;- Fval[5:8,3] &lt;- Fval[9:12,4 ] &lt;- .6
Fpatt &lt;- 1 * ( Fval &gt; 0 )
Pval &lt;- diag(4)
Ppatt &lt;- 0*Pval
colnames(Ppatt) &lt;- c("g", "A", "B","C")
# estimation
mod5 &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval  )
summary(mod5)
# allow only positive loadings
mod5b &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval,
              pos.loading=TRUE )
summary(mod5b)
summary( sirt::modelfit.sirt(mod5b))

#**** Model 6: 3-dimensional Rasch model
Fval &lt;- matrix( 0, I, 3 )
Fval[1:4,1] &lt;- Fval[5:8,2] &lt;- Fval[9:12,3 ] &lt;- 1
Fpatt &lt;- 0*Fval
Pval &lt;- .6*diag(3)
diag(Pval) &lt;- 1
Ppatt &lt;- 1+0*Pval
colnames(Ppatt) &lt;- c("A", "B","C")
# estimation
mod6 &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval  )
summary(mod6)
summary( sirt::modelfit.sirt(mod6) )  # model fit

#**** Model 7: 3-dimensional 2PL model
Fval &lt;- matrix( 0, I, 3 )
Fval[1:4,1] &lt;- Fval[5:8,2] &lt;- Fval[9:12,3 ] &lt;- 1
Fpatt &lt;- Fval
Pval &lt;- .6*diag(3)
diag(Pval) &lt;- 1
Ppatt &lt;- 1+0*Pval
diag(Ppatt) &lt;- 0
colnames(Ppatt) &lt;- c("A", "B","C")
# estimation
mod7 &lt;- sirt::noharm.sirt( dat=dat, Ppatt=Ppatt,Fpatt=Fpatt, Fval=Fval, Pval=Pval  )
summary(mod7)
summary( sirt::modelfit.sirt(mod7) )

#**** Model 8: Exploratory factor analysis with 3 dimensions
# estimation
mod8 &lt;- sirt::noharm.sirt( dat=dat, dimensions=3  )
summary(mod8)

#############################################################################
# EXAMPLE 3: Product-moment matrix input, McDonald (1997)
#############################################################################

# data from Table 1 of McDonald (1997, p. 266)
pm0 &lt;- "
0.828
0.567 0.658
0.664 0.560 0.772
0.532 0.428 0.501 0.606
0.718 0.567 0.672 0.526 0.843
"
pm &lt;- miceadds::string_to_matrix(x=pm0, as_numeric=TRUE, extend=TRUE)
I &lt;- nrow(pm)
rownames(pm) &lt;- colnames(pm) &lt;- paste0("I", 1:I)

#- Model 1: Unidimensional model
Fval &lt;- matrix(.7, nrow=I, ncol=1)
Fpatt &lt;- 1+0*Fval
Pval &lt;- matrix(1, nrow=1,ncol=1)
Ppatt &lt;- 0*Pval

mod1 &lt;- sirt::noharm.sirt(pm=pm, N=1000, Fval=Fval, Fpatt=Fpatt, Pval=Pval, Ppatt=Ppatt)
summary(mod1)

#- Model 2: Twodimensional exploratory model
mod2 &lt;- sirt::noharm.sirt(pm=pm, N=1000, dimensions=2)
summary(mod2)

#- Model 3: Unidimensional model with correlated residuals
Psival &lt;- matrix(0, nrow=I, ncol=I)
Psipatt &lt;- 0*Psival
Psipatt[5,1] &lt;- 1

mod3 &lt;- sirt::noharm.sirt(pm=pm, N=1000, Fval=Fval, Fpatt=Fpatt, Pval=Pval, Ppatt=Ppatt,
            Psival=Psival, Psipatt=Psipatt)
summary(mod3)

## End(Not run)
</code></pre>

<hr>
<h2 id='np.dich'>
Nonparametric Estimation of Item Response Functions
</h2><span id='topic+np.dich'></span>

<h3>Description</h3>

<p>This function does nonparametric item response function
estimation (Ramsay, 1991).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>np.dich(dat, theta, thetagrid, progress=FALSE, bwscale=1.1,
       method="normal")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="np.dich_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous
item responses
</p>
</td></tr>
<tr><td><code id="np.dich_+3A_theta">theta</code></td>
<td>

<p>Estimated theta values, for example weighted likelihood
estimates from <code><a href="#topic+wle.rasch">wle.rasch</a></code>
</p>
</td></tr>
<tr><td><code id="np.dich_+3A_thetagrid">thetagrid</code></td>
<td>

<p>A vector of theta values where the nonparametric item
response functions shall be evaluated.
</p>
</td></tr>
<tr><td><code id="np.dich_+3A_progress">progress</code></td>
<td>

<p>Display progress?
</p>
</td></tr>
<tr><td><code id="np.dich_+3A_bwscale">bwscale</code></td>
<td>

<p>The bandwidth parameter <code class="reqn">h</code> is calculated by
the formula <code class="reqn">h=</code><code>bwscale</code><code class="reqn">\cdot N^{-1/5}</code>
</p>
</td></tr>
<tr><td><code id="np.dich_+3A_method">method</code></td>
<td>

<p>The default <code>normal</code> performs kernel regression
with untransformed item responses. The method <code>binomial</code>
uses nonparametric logistic regression implemented
in the <span class="pkg">sm</span> library.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>dat</code></td>
<td>
<p>Original data frame</p>
</td></tr>
<tr><td><code>thetagrid</code></td>
<td>
<p>Vector of theta values at which the item response
functions are evaluated</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>Used theta values as person parameter estimates</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>Estimated item response functions</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>References</h3>

<p>Ramsay, J. O. (1991). Kernel smoothing approaches to
nonparametric item characteristic curve estimation.
<em>Psychometrika, 56</em>, 611-630.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Reading dataset
#############################################################################
data( data.read )
dat &lt;- data.read

# estimate Rasch model
mod &lt;- sirt::rasch.mml2( dat )
# WLE estimation
wle1 &lt;- sirt::wle.rasch( dat=dat, b=mod$item$b )$theta
# nonparametric function estimation
np1 &lt;- sirt::np.dich( dat=dat, theta=wle1, thetagrid=seq(-2.5, 2.5, len=100 ) )
print( str(np1))
# plot nonparametric item response curves
plot( np1, b=mod$item$b )
</code></pre>

<hr>
<h2 id='parmsummary_extend'>
Includes Confidence Interval in Parameter Summary Table
</h2><span id='topic+parmsummary_extend'></span>

<h3>Description</h3>

<p>Includes confidence interval in parameter summary table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parmsummary_extend(dfr, level=.95, est_label="est", se_label="se",
      df_label="df")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parmsummary_extend_+3A_dfr">dfr</code></td>
<td>

<p>Data frame containing parameter summary
</p>
</td></tr>
<tr><td><code id="parmsummary_extend_+3A_level">level</code></td>
<td>

<p>Significance level
</p>
</td></tr>
<tr><td><code id="parmsummary_extend_+3A_est_label">est_label</code></td>
<td>

<p>Label for parameter estimate
</p>
</td></tr>
<tr><td><code id="parmsummary_extend_+3A_se_label">se_label</code></td>
<td>

<p>Label for standard error
</p>
</td></tr>
<tr><td><code id="parmsummary_extend_+3A_df_label">df_label</code></td>
<td>

<p>Label for degrees of freedom
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Extended parameter summary table
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+confint">stats::confint</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
## EXAMPLE 1: Toy example parameter summary table
#############################################################################

dfr &lt;- data.frame( "parm"=c("b0", "b1" ), "est"=c(0.1, 1.3 ),
                "se"=c(.21, .32) )
print( sirt::parmsummary_extend(dfr), digits=4 )
  ##    parm est   se      t         p lower95 upper95
  ##  1   b0 0.1 0.21 0.4762 6.339e-01 -0.3116  0.5116
  ##  2   b1 1.3 0.32 4.0625 4.855e-05  0.6728  1.9272
</code></pre>

<hr>
<h2 id='pbivnorm2'>
Cumulative Function for the Bivariate Normal Distribution
</h2><span id='topic+pbivnorm2'></span>

<h3>Description</h3>

<p>This function evaluates the bivariate normal distribution
<code class="reqn">\Phi_2 ( x, y ; \rho )</code>
assuming zero means and unit variances. It uses a simple approximation
by Cox and Wermuth (1991) with corrected formulas in Hong (1999).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pbivnorm2(x, y, rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pbivnorm2_+3A_x">x</code></td>
<td>

<p>Vector of <code class="reqn">x</code> coordinates
</p>
</td></tr>
<tr><td><code id="pbivnorm2_+3A_y">y</code></td>
<td>

<p>Vector of <code class="reqn">y</code> coordinates
</p>
</td></tr>
<tr><td><code id="pbivnorm2_+3A_rho">rho</code></td>
<td>

<p>Vector of correlations between random normal variates
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of probabilities
</p>


<h3>Note</h3>

<p>The function is less precise for correlations near 1 or -1.
</p>


<h3>References</h3>

<p>Cox, D. R., &amp; Wermuth, N. (1991). A simple approximation for bivariate and
trivariate normal integrals.
<em>International Statistical Review, 59</em>(2), 263-269.
</p>
<p>Hong, H. P. (1999). An approximation to bivariate and trivariate normal
integrals. <em>Engineering and Environmental Systems, 16</em>(2), 115-127.
<a href="https://doi.org/10.1080/02630259908970256">doi:10.1080/02630259908970256</a>
</p>


<h3>See Also</h3>

<p>See also the
<code><a href="pbivnorm.html#topic+pbivnorm">pbivnorm::pbivnorm</a></code>
function in the <span class="pkg">pbivnorm</span> package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(pbivnorm)
# define input
x &lt;- c(0, 0,  .5, 1, 1  )
y &lt;- c( 0, -.5,  1, 3, .5 )
rho &lt;- c( .2, .8, -.4, .6, .5 )
# compare pbivnorm2 and pbivnorm functions
pbiv2 &lt;- sirt::pbivnorm2( x=x, y=y, rho=rho )
pbiv &lt;- pbivnorm::pbivnorm(  x,  y, rho=rho )
max( abs(pbiv-pbiv2))
  ## [1] 0.0030626
round( cbind( x, y, rho,pbiv, pbiv2 ), 4 )
  ##          x    y  rho   pbiv  pbiv2
  ##   [1,] 0.0  0.0  0.2 0.2820 0.2821
  ##   [2,] 0.0 -0.5  0.8 0.2778 0.2747
  ##   [3,] 0.5  1.0 -0.4 0.5514 0.5514
  ##   [4,] 1.0  3.0  0.6 0.8412 0.8412
  ##   [5,] 1.0  0.5  0.5 0.6303 0.6304
</code></pre>

<hr>
<h2 id='pcm.conversion'>
Conversion of the Parameterization of the Partial Credit Model
</h2><span id='topic+pcm.conversion'></span>

<h3>Description</h3>

<p>Converts a parameterization of the partial credit model (see Details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcm.conversion(b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcm.conversion_+3A_b">b</code></td>
<td>

<p>Matrix of item-category-wise intercepts <code class="reqn">b_{ik}</code> (see Details).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that the input matrix <code>b</code> containing parameters <code class="reqn">b_{ik}</code>
is defined according to the following parametrization of the partial credit
model
</p>
<p style="text-align: center;"><code class="reqn"> P( X_{pi}=k | \theta_p ) \propto exp ( k \theta_p - b_{ik} ) </code>
</p>

<p>if item <code class="reqn">i</code> possesses <code class="reqn">K_i</code> categories.
The transformed parameterization is defined as
</p>
<p style="text-align: center;"><code class="reqn">b_{ik}=k \delta_i + \sum_{v=1}^{k} \tau_{iv} \quad
\mbox{with} \quad \sum_{k=1}^{K_i} \tau_{ik}=0 </code>
</p>

<p>The function <code>pcm.conversion</code> has the <code class="reqn">\delta</code> and <code class="reqn">\tau</code>
parameters as values. The <code class="reqn">\delta</code> parameter is simply
<code class="reqn">\delta_i=b_{iK_i} / K_i</code>.
</p>


<h3>Value</h3>

<p>List with the following entries
</p>
<table>
<tr><td><code>delta</code></td>
<td>
<p>Vector of <code class="reqn">\delta</code> parameters</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>Matrix of <code class="reqn">\tau</code> parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Transformation PCM for data.mg
#############################################################################

library(CDM)
data(data.mg,package="CDM")
dat &lt;- data.mg[ 1:1000, paste0("I",1:11) ]

#*** Model 1: estimate partial credit model in parameterization "PCM"
mod1a &lt;- TAM::tam.mml( dat, irtmodel="PCM")
# use parameterization "PCM2"
mod1b &lt;- TAM::tam.mml( dat, irtmodel="PCM2")
summary(mod1a)
summary(mod1b)

# convert parameterization of Model 1a into parameterization of Model 1b
b &lt;- mod1a$item[, c("AXsi_.Cat1","AXsi_.Cat2","AXsi_.Cat3") ]
# compare results
pcm.conversion(b)
mod1b$xsi

## End(Not run)
</code></pre>

<hr>
<h2 id='pcm.fit'>
Item and Person Fit Statistics for the Partial Credit Model
</h2><span id='topic+pcm.fit'></span>

<h3>Description</h3>

<p>Computes item and person fit statistics in the
partial credit model (Wright &amp; Masters, 1990).
The rating scale model is accommodated as a particular partial
credit model (see Example 3).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcm.fit(b, theta, dat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcm.fit_+3A_b">b</code></td>
<td>

<p>Matrix with item category parameters (see Examples)
</p>
</td></tr>
<tr><td><code id="pcm.fit_+3A_theta">theta</code></td>
<td>

<p>Vector with estimated person parameters
</p>
</td></tr>
<tr><td><code id="pcm.fit_+3A_dat">dat</code></td>
<td>

<p>Dataset with item responses
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with entries
</p>
<table>
<tr><td><code>itemfit</code></td>
<td>
<p>Item fit statistics</p>
</td></tr>
<tr><td><code>personfit</code></td>
<td>
<p>Person fit statistics</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wright, B. D., &amp; Masters, G. N. (1990). Computation of outfit and
infit statistics. <em>Rasch Measurement Transactions, 3</em>:4, 84-85.
</p>


<h3>See Also</h3>

<p>See also <code>personfit.stat</code> for person fit statistics for dichotomous
item responses. See also the <span class="pkg">PerFit</span> package for further person
fit statistics.
</p>
<p>Item fit in other <span class="rlang"><b>R</b></span> packages:
<code>eRm::itemfit</code>,
<code><a href="TAM.html#topic+tam.fit">TAM::tam.fit</a></code>,
<code><a href="mirt.html#topic+itemfit">mirt::itemfit</a></code>,
<code>ltm::item.fit</code>,
</p>
<p>Person fit in other <span class="rlang"><b>R</b></span> packages:
<code>eRm::itemfit</code>,
<code><a href="mirt.html#topic+itemfit">mirt::itemfit</a></code>,
<code>ltm::person.fit</code>,
</p>
<p>See <code><a href="#topic+pcm.conversion">pcm.conversion</a></code> for conversions of different
parametrizations of the partial credit model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Partial credit model
#############################################################################

data(data.Students,package="CDM")
dat &lt;- data.Students
# select items
items &lt;- c(paste0("sc", 1:4 ), paste0("mj", 1:4 ) )
dat &lt;- dat[,items]
dat &lt;- dat[ rowSums( 1 - is.na(dat) ) &gt; 0, ]

#*** Model 1a: Partial credit model in TAM
# estimate model
mod1a &lt;- TAM::tam.mml( resp=dat )
summary(mod1a)
# estimate person parameters
wle1a &lt;- TAM::tam.wle(mod1a)
# extract item parameters
b1 &lt;- - mod1a$AXsi[, -1 ]
# parametrization in xsi parameters
b2 &lt;- matrix( mod1a$xsi$xsi, ncol=3, byrow=TRUE )
# convert b2 to b1
b1b &lt;- 0*b1
b1b[,1] &lt;- b2[,1]
b1b[,2] &lt;- rowSums( b2[,1:2] )
b1b[,3] &lt;- rowSums( b2[,1:3] )
# assess fit
fit1a &lt;- sirt::pcm.fit(b=b1, theta=wle1a$theta, dat)
fit1a$item

#############################################################################
# EXAMPLE 2: Rasch model
#############################################################################

data(data.read)
dat &lt;- data.read

#*** Rasch model in TAM
# estimate model
mod &lt;- TAM::tam.mml( resp=dat )
summary(mod)
# estimate person parameters
wle &lt;- TAM::tam.wle(mod)
# extract item parameters
b1 &lt;- - mod$AXsi[, -1 ]
# assess fit
fit1a &lt;- sirt::pcm.fit(b=b1, theta=wle$theta, dat)
fit1a$item

#############################################################################
# EXAMPLE 3: Rating scale model
#############################################################################

data(data.Students,package="CDM")
dat &lt;- data.Students
items &lt;- paste0("sc", 1:4 )
dat &lt;- dat[,items]
dat &lt;- dat[ rowSums( 1 - is.na(dat) ) &gt; 0, ]

#*** Model 1: Rating scale model in TAM
# estimate model
mod1 &lt;- tam.mml( resp=dat, irtmodel="RSM")
summary(mod1)
# estimate person parameters
wle1 &lt;- tam.wle(mod1)
# extract item parameters
b1 &lt;- - mod1a$AXsi[, -1 ]
# fit statistic
pcm.fit(b=b1, theta=wle1$theta, dat)

## End(Not run)
</code></pre>

<hr>
<h2 id='person.parameter.rasch.copula'>
Person Parameter Estimation of the Rasch Copula Model (Braeken, 2011)
</h2><span id='topic+person.parameter.rasch.copula'></span>

<h3>Description</h3>

<p>Ability estimates as maximum likelihood estimates (MLE)
are provided by the Rasch copula model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>person.parameter.rasch.copula(raschcopula.object, numdiff.parm=0.001,
    conv.parm=0.001, maxiter=20, stepwidth=1,
    print.summary=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="person.parameter.rasch.copula_+3A_raschcopula.object">raschcopula.object</code></td>
<td>

<p>Object which is generated by the <code>rasch.copula2</code> function.
</p>
</td></tr>
<tr><td><code id="person.parameter.rasch.copula_+3A_numdiff.parm">numdiff.parm</code></td>
<td>

<p>Parameter <code class="reqn">h</code> for numerical differentiation
</p>
</td></tr>
<tr><td><code id="person.parameter.rasch.copula_+3A_conv.parm">conv.parm</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="person.parameter.rasch.copula_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="person.parameter.rasch.copula_+3A_stepwidth">stepwidth</code></td>
<td>

<p>Maximal increment in iterations
</p>
</td></tr>
<tr><td><code id="person.parameter.rasch.copula_+3A_print.summary">print.summary</code></td>
<td>

<p>Print summary?
</p>
</td></tr>
<tr><td><code id="person.parameter.rasch.copula_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>person</code></td>
<td>
<p>Estimated person parameters</p>
</td></tr>
<tr><td><code>se.inflat</code></td>
<td>
<p>Inflation of individual standard errors due
to local dependence</p>
</td></tr>
<tr><td><code>theta.table</code></td>
<td>
<p>Ability estimates for each unique response pattern</p>
</td></tr>
<tr><td><code>pattern.in.data</code></td>
<td>
<p>Item response pattern</p>
</td></tr>
<tr><td><code>summary.theta.table</code></td>
<td>
<p>Summary statistics of person parameter estimates</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>See <code><a href="#topic+rasch.copula2">rasch.copula2</a></code> for estimating Rasch copula models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Reading Data
#############################################################################

data(data.read)
dat &lt;- data.read

# define item cluster
itemcluster &lt;- rep( 1:3, each=4 )
mod1 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster )
summary(mod1)

# person parameter estimation under the Rasch copula model
pmod1 &lt;- sirt::person.parameter.rasch.copula(raschcopula.object=mod1 )
## Mean percentage standard error inflation
##   missing.pattern Mperc.seinflat
## 1               1           6.35

## Not run: 
#############################################################################
# EXAMPLE 2: 12 items nested within 3 item clusters (testlets)
#   Cluster 1 -&gt; Items 1-4; Cluster 2 -&gt; Items 6-9;  Cluster 3 -&gt; Items 10-12
#############################################################################

set.seed(967)
I &lt;- 12                             # number of items
n &lt;- 450                            # number of persons
b &lt;- seq(-2,2, len=I)               # item difficulties
b &lt;- sample(b)                      # sample item difficulties
theta &lt;- stats::rnorm( n, sd=1 ) # person abilities
# itemcluster
itemcluster &lt;- rep(0,I)
itemcluster[ 1:4 ] &lt;- 1
itemcluster[ 6:9 ] &lt;- 2
itemcluster[ 10:12 ] &lt;- 3
# residual correlations
rho &lt;- c( .35, .25, .30 )

# simulate data
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("I", seq(1,ncol(dat)), sep="")

# estimate Rasch copula model
mod1 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster )
summary(mod1)

# person parameter estimation under the Rasch copula model
pmod1 &lt;- sirt::person.parameter.rasch.copula(raschcopula.object=mod1 )
  ## Mean percentage standard error inflation
  ##   missing.pattern Mperc.seinflat
  ## 1               1          10.48

## End(Not run)
</code></pre>

<hr>
<h2 id='personfit.stat'>
Person Fit Statistics for the Rasch Model
</h2><span id='topic+personfit.stat'></span>

<h3>Description</h3>

<p>This function collects some person fit statistics
for the Rasch model (Karabatsos, 2003; Meijer &amp; Sijtsma, 2001).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>personfit.stat(dat, abil, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="personfit.stat_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous item
responses
</p>
</td></tr>
<tr><td><code id="personfit.stat_+3A_abil">abil</code></td>
<td>

<p>An ability estimate, e.g. the WLE
</p>
</td></tr>
<tr><td><code id="personfit.stat_+3A_b">b</code></td>
<td>

<p>Estimated item difficulty
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with following columns (see Meijer &amp; Sijtsma 2001
for a review of different person fit statistics):
</p>
<table>
<tr><td><code>case</code></td>
<td>
<p>Case index</p>
</td></tr>
<tr><td><code>abil</code></td>
<td>
<p>Ability estimate <code>abil</code></p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p>Person mean of correctly solved items</p>
</td></tr>
<tr><td><code>caution</code></td>
<td>
<p>Caution index</p>
</td></tr>
<tr><td><code>depend</code></td>
<td>
<p>Dependability index</p>
</td></tr>
<tr><td><code>ECI1</code></td>
<td>
<p><code class="reqn">ECI1</code></p>
</td></tr>
<tr><td><code>ECI2</code></td>
<td>
<p><code class="reqn">ECI2</code></p>
</td></tr>
<tr><td><code>ECI3</code></td>
<td>
<p><code class="reqn">ECI3</code></p>
</td></tr>
<tr><td><code>ECI4</code></td>
<td>
<p><code class="reqn">ECI4</code></p>
</td></tr>
<tr><td><code>ECI5</code></td>
<td>
<p><code class="reqn">ECI5</code></p>
</td></tr>
<tr><td><code>ECI6</code></td>
<td>
<p><code class="reqn">ECI6</code></p>
</td></tr>
<tr><td><code>l0</code></td>
<td>
<p>Fit statistic <code class="reqn">l_0</code></p>
</td></tr>
<tr><td><code>lz</code></td>
<td>
<p>Fit statistic <code class="reqn">l_z</code></p>
</td></tr>
<tr><td><code>outfit</code></td>
<td>
<p>Person outfit statistic</p>
</td></tr>
<tr><td><code>infit</code></td>
<td>
<p>Person infit statistic</p>
</td></tr>
<tr><td><code>rpbis</code></td>
<td>
<p>Point biserial correlation of item responses
and item <code class="reqn">p</code> values</p>
</td></tr>
<tr><td><code>rpbis.itemdiff</code></td>
<td>
<p>Point biserial correlation of item responses
and item difficulties <code>b</code></p>
</td></tr>
<tr><td><code>U3</code></td>
<td>
<p>Fit statistic <code class="reqn">U_3</code></p>
</td></tr>
</table>


<h3>References</h3>

<p>Karabatsos, G. (2003). Comparing the aberrant response detection performance
of thirty-six person-fit statistics. <em>Applied Measurement in Education,
16</em>, 277-298.
</p>
<p>Meijer, R. R., &amp; Sijtsma, K. (2001). Methodology
review: Evaluating person fit. <em>Applied Psychological
Measurement, 25</em>, 107-135.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+pcm.fit">pcm.fit</a></code> for person fit in the partial credit model.
</p>
<p>See the <span class="pkg">irtProb</span> and <span class="pkg">PerFit</span> packages for person fit statistics
and person response curves and functions included in other packages:
<code><a href="mirt.html#topic+personfit">mirt::personfit</a></code>,
<code>eRm::personfit</code> and
<code>ltm::person.fit</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Person fit Reading Data
#############################################################################

data(data.read)
dat &lt;- data.read

# estimate Rasch model
mod &lt;- sirt::rasch.mml2( dat )
# WLE
wle1 &lt;- sirt::wle.rasch( dat,b=mod$item$b )$theta
b &lt;- mod$item$b # item difficulty

# evaluate person fit
pf1 &lt;- sirt::personfit.stat( dat=dat, abil=wle1, b=b)

## Not run: 
# dimensional analysis of person fit statistics
x0 &lt;- stats::na.omit(pf1[, -c(1:3) ] )
stats::factanal( x=x0, factors=2, rotation="promax" )
  ## Loadings:
  ##                Factor1 Factor2
  ## caution         0.914
  ## depend          0.293   0.750
  ## ECI1            0.869   0.160
  ## ECI2            0.869   0.162
  ## ECI3            1.011
  ## ECI4            1.159  -0.269
  ## ECI5            1.012
  ## ECI6            0.879   0.130
  ## l0              0.409  -1.255
  ## lz             -0.504  -0.529
  ## outfit          0.297   0.702
  ## infit           0.362   0.695
  ## rpbis          -1.014
  ## rpbis.itemdiff  1.032
  ## U3              0.735   0.309
  ##
  ## Factor Correlations:
  ##         Factor1 Factor2
  ## Factor1   1.000  -0.727
  ## Factor2  -0.727   1.000
  ##

## End(Not run)
</code></pre>

<hr>
<h2 id='pgenlogis'>
Calculation of Probabilities and Moments for the
Generalized Logistic Item Response Model
</h2><span id='topic+pgenlogis'></span><span id='topic+genlogis.moments'></span>

<h3>Description</h3>

<p>Calculation of probabilities and moments for the generalized logistic
item response model (Stukel, 1988).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pgenlogis(x, alpha1=0, alpha2=0)

genlogis.moments(alpha1, alpha2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pgenlogis_+3A_x">x</code></td>
<td>

<p>Vector
</p>
</td></tr>
<tr><td><code id="pgenlogis_+3A_alpha1">alpha1</code></td>
<td>

<p>Upper tail parameter <code class="reqn">\alpha_1</code> in the generalized
logistic item response model. The default is 0.
</p>
</td></tr>
<tr><td><code id="pgenlogis_+3A_alpha2">alpha2</code></td>
<td>

<p>Lower tail parameter <code class="reqn">\alpha_2</code> parameter in the generalized
logistic item response model. The default is 0.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The class of generalized logistic link functions contain
the most important link functions using the specifications (Stukel, 1988):
</p>

<ul>
<li><p> logistic link function <code class="reqn">L</code>:
</p>
<p style="text-align: center;"><code class="reqn"> L(x) \approx G_{ ( \alpha_1=0, \alpha_2=0)}[ x ] </code>
</p>

</li>
<li><p> probit link function <code class="reqn">\Phi</code>:
</p>
<p style="text-align: center;"><code class="reqn"> \Phi(x) \approx G_{ ( \alpha_1=0.165, \alpha_2=0.165)}[ 1.47 x ] </code>
</p>

</li>
<li><p> loglog link function <code class="reqn">H</code>:
</p>
<p style="text-align: center;"><code class="reqn"> H(x) \approx G_{ (\alpha_1=-0.037, \alpha_2=0.62)}[
-0.39+1.20x-0.007x^2] </code>
</p>

</li>
<li><p> cloglog link function <code class="reqn">H</code>:
</p>
<p style="text-align: center;"><code class="reqn"> H(x) \approx G_{ ( \alpha_1=0.62, \alpha_2=-0.037)}[
0.54+1.64x+0.28x^2+0.046x^3] </code>
</p>

</li></ul>



<h3>Value</h3>

<p>Vector of probabilities or moments
</p>


<h3>References</h3>

<p>Stukel, T. A. (1988). Generalized logistic models.
<em>Journal of the American Statistical Association, 83</em>(402), 426-431.
<a href="https://doi.org/10.1080/01621459.1988.10478613">doi:10.1080/01621459.1988.10478613</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sirt::pgenlogis( x=c(-.3, 0, .25, 1 ), alpha1=0, alpha2=.6 )
  ##   [1] 0.4185580 0.5000000 0.5621765 0.7310586

####################################################################
# compare link functions
x &lt;- seq( -3,3, .1 )

#***
# logistic link
y &lt;- sirt::pgenlogis( x, alpha1=0, alpha2=0 )
plot( x, stats::plogis(x), type="l", main="Logistic Link", lwd=2)
points( x, y, pch=1, col=2 )

#***
# probit link
round( sirt::genlogis.moments( alpha1=.165, alpha2=.165 ), 3 )
  ##       M    SD   Var
  ##   0.000 1.472 2.167
# SD of generalized logistic link function is 1.472
y &lt;- sirt::pgenlogis( x * 1.47, alpha1=.165, alpha2=.165 )
plot( x, stats::pnorm(x), type="l", main="Probit Link", lwd=2)
points( x, y, pch=1, col=2 )

#***
# loglog link
y &lt;- sirt::pgenlogis( -.39 + 1.20*x -.007*x^2, alpha1=-.037, alpha2=.62 )
plot( x, exp( - exp( -x ) ), type="l", main="Loglog Link", lwd=2,
    ylab="loglog(x)=exp(-exp(-x))" )
points( x, y, pch=17, col=2 )

#***
# cloglog link
y &lt;- sirt::pgenlogis( .54+1.64*x +.28*x^2 + .046*x^3, alpha1=.062, alpha2=-.037 )
plot( x, 1-exp( - exp(x) ), type="l", main="Cloglog Link", lwd=2,
    ylab="loglog(x)=1-exp(-exp(x))" )
points( x, y, pch=17, col=2 )
</code></pre>

<hr>
<h2 id='plausible.value.imputation.raschtype'>
Plausible Value Imputation in Generalized Logistic Item
Response Model
</h2><span id='topic+plausible.value.imputation.raschtype'></span>

<h3>Description</h3>

<p>This function performs unidimensional plausible value imputation
(Adams &amp; Wu, 2007; Mislevy, 1991).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plausible.value.imputation.raschtype(data=NULL, f.yi.qk=NULL, X,
   Z=NULL, beta0=rep(0, ncol(X)), sig0=1, b=rep(1, ncol(X)),
   a=rep(1, length(b)), c=rep(0, length(b)), d=1+0*b,
   alpha1=0, alpha2=0, theta.list=seq(-5, 5, len=50),
   cluster=NULL, iter, burnin, nplausible=1, printprogress=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_data">data</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous responses
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_f.yi.qk">f.yi.qk</code></td>
<td>

<p>An optional matrix which contains the individual likelihood.
This matrix is produced by <code><a href="#topic+rasch.mml2">rasch.mml2</a></code> or
<code><a href="#topic+rasch.copula2">rasch.copula2</a></code>. The use of this argument allows the
estimation of the latent regression model independent of the
parameters of the used item response model.
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_x">X</code></td>
<td>

<p>A matrix of individual covariates for the latent
regression of <code class="reqn">\theta</code> on <code class="reqn">X</code>
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_z">Z</code></td>
<td>

<p>A matrix of individual covariates for the regression
of individual residual variances on <code class="reqn">Z</code>
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_beta0">beta0</code></td>
<td>

<p>Initial vector of regression coefficients
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_sig0">sig0</code></td>
<td>

<p>Initial vector of coefficients for the
variance heterogeneity model
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_b">b</code></td>
<td>

<p>Vector of item difficulties. It must not be provided
if the individual likelihood <code>f.yi.qk</code> is
specified.
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_a">a</code></td>
<td>

<p>Optional vector of item slopes
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_c">c</code></td>
<td>

<p>Optional vector of lower item asymptotes
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_d">d</code></td>
<td>

<p>Optional vector of upper item asymptotes
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_alpha1">alpha1</code></td>
<td>

<p>Parameter <code class="reqn">\alpha_1</code> in generalized
item response model
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_alpha2">alpha2</code></td>
<td>

<p>Parameter <code class="reqn">\alpha_2</code> in generalized
item response model
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_theta.list">theta.list</code></td>
<td>

<p>Vector of theta values at which the ability
distribution should be evaluated
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_cluster">cluster</code></td>
<td>

<p>Cluster identifier (e.g. schools or classes)
for including theta means in the plausible imputation.
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_iter">iter</code></td>
<td>

<p>Number of iterations
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_burnin">burnin</code></td>
<td>

<p>Number of burn-in iterations for plausible value imputation
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_nplausible">nplausible</code></td>
<td>

<p>Number of plausible values
</p>
</td></tr>
<tr><td><code id="plausible.value.imputation.raschtype_+3A_printprogress">printprogress</code></td>
<td>

<p>A logical indicated whether iteration progress should be displayed
at the console.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plausible values are drawn from the latent regression model with
heterogeneous variances:
</p>
<p style="text-align: center;"><code class="reqn">\theta_p=X_p \beta + \epsilon_p  \quad, \quad
\epsilon_p \sim N( 0, \sigma_p^2 ) \quad, \quad
\log( \sigma_p )=Z_p \gamma + \nu_p </code>
</p>



<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>coefs.X</code></td>
<td>
<p>Sampled regression coefficients for covariates <code class="reqn">X</code></p>
</td></tr>
<tr><td><code>coefs.Z</code></td>
<td>
<p>Sampled coefficients for modeling variance heterogeneity
for covariates <code class="reqn">Z</code></p>
</td></tr>
<tr><td><code>pvdraws</code></td>
<td>
<p>Matrix with drawn plausible values</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>Posterior distribution from last iteration</p>
</td></tr>
<tr><td><code>EAP</code></td>
<td>
<p>Individual EAP estimate</p>
</td></tr>
<tr><td><code>SE.EAP</code></td>
<td>
<p>Standard error of the EAP estimate</p>
</td></tr>
<tr><td><code>pv.indexes</code></td>
<td>
<p>Index of iterations for which plausible
values were drawn</p>
</td></tr>
</table>


<h3>References</h3>

<p>Adams, R., &amp; Wu. M. (2007). The mixed-coefficients multinomial
logit model: A generalized form of the Rasch model.
In M. von Davier &amp; C. H. Carstensen: <em>Multivariate and Mixture
Distribution Rasch Models: Extensions and Applications</em> (pp. 57-76).
New York: Springer.
</p>
<p>Mislevy, R. J. (1991). Randomization-based inference about latent
variables from complex samples. <em>Psychometrika, 56</em>, 177-196.
</p>


<h3>See Also</h3>

<p>For estimating the latent regression model see
<code><a href="#topic+latent.regression.em.raschtype">latent.regression.em.raschtype</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Rasch model with covariates
#############################################################################

set.seed(899)
I &lt;- 21     # number of items
b &lt;- seq(-2,2, len=I)   # item difficulties
n &lt;- 2000       # number of students

# simulate theta and covariates
theta &lt;- stats::rnorm( n )
x &lt;- .7 * theta + stats::rnorm( n, .5 )
y &lt;- .2 * x+ .3*theta + stats::rnorm( n, .4 )
dfr &lt;- data.frame( theta, 1, x, y )

# simulate Rasch model
dat1 &lt;- sirt::sim.raschtype( theta=theta, b=b )

# Plausible value draws
pv1 &lt;- sirt::plausible.value.imputation.raschtype(data=dat1, X=dfr[,-1], b=b,
            nplausible=3, iter=10, burnin=5)
# estimate linear regression based on first plausible value
mod1 &lt;- stats::lm( pv1$pvdraws[,1] ~ x+y )
summary(mod1)
  ##               Estimate Std. Error t value Pr(&gt;|t|)
  ##   (Intercept) -0.27755    0.02121  -13.09   &lt;2e-16 ***
  ##   x            0.40483    0.01640   24.69   &lt;2e-16 ***
  ##   y            0.20307    0.01822   11.15   &lt;2e-16 ***

# true regression estimate
summary( stats::lm( theta ~ x + y ) )
  ## Coefficients:
  ##             Estimate Std. Error t value Pr(&gt;|t|)
  ## (Intercept) -0.27821    0.01984  -14.02   &lt;2e-16 ***
  ## x            0.40747    0.01534   26.56   &lt;2e-16 ***
  ## y            0.18189    0.01704   10.67   &lt;2e-16 ***

## Not run: 
#############################################################################
# EXAMPLE 2: Classical test theory, homogeneous regression variance
#############################################################################

set.seed(899)
n &lt;- 3000       # number of students
x &lt;- round( stats::runif( n, 0,1 ) )
y &lt;- stats::rnorm(n)
# simulate true score theta
theta &lt;- .4*x + .5 * y + stats::rnorm(n)
# simulate observed score by adding measurement error
sig.e &lt;- rep( sqrt(.40), n )
theta_obs &lt;- theta + stats::rnorm( n, sd=sig.e)

# define theta grid for evaluation of density
theta.list &lt;- mean(theta_obs) + stats::sd(theta_obs) * seq( - 5, 5, length=21)
# compute individual likelihood
f.yi.qk &lt;- stats::dnorm( outer( theta_obs, theta.list, "-" ) / sig.e )
f.yi.qk &lt;- f.yi.qk / rowSums(f.yi.qk)
# define covariates
X &lt;- cbind( 1, x, y )
# draw plausible values
mod2 &lt;- sirt::plausible.value.imputation.raschtype( f.yi.qk=f.yi.qk,
                  theta.list=theta.list, X=X, iter=10, burnin=5)

# linear regression
mod1 &lt;- stats::lm( mod2$pvdraws[,1] ~ x+y )
summary(mod1)
  ##             Estimate Std. Error t value Pr(&gt;|t|)
  ## (Intercept) -0.01393    0.02655  -0.525      0.6
  ## x            0.35686    0.03739   9.544   &lt;2e-16 ***
  ## y            0.53759    0.01872  28.718   &lt;2e-16 ***

# true regression model
summary( stats::lm( theta ~ x + y ) )
  ##             Estimate Std. Error t value Pr(&gt;|t|)
  ## (Intercept) 0.002931   0.026171   0.112    0.911
  ## x           0.359954   0.036864   9.764   &lt;2e-16 ***
  ## y           0.509073   0.018456  27.584   &lt;2e-16 ***

#############################################################################
# EXAMPLE 3: Classical test theory, heterogeneous regression variance
#############################################################################

set.seed(899)
n &lt;- 5000       # number of students
x &lt;- round( stats::runif( n, 0,1 ) )
y &lt;- stats::rnorm(n)
# simulate true score theta
theta &lt;- .4*x + .5 * y + stats::rnorm(n) * ( 1 - .4 * x )
# simulate observed score by adding measurement error
sig.e &lt;- rep( sqrt(.40), n )
theta_obs &lt;- theta + stats::rnorm( n, sd=sig.e)

# define theta grid for evaluation of density
theta.list &lt;- mean(theta_obs) + stats::sd(theta_obs) * seq( - 5, 5, length=21)
# compute individual likelihood
f.yi.qk &lt;- stats::dnorm( outer( theta_obs, theta.list, "-" ) / sig.e )
f.yi.qk &lt;- f.yi.qk / rowSums(f.yi.qk)
# define covariates
X &lt;- cbind( 1, x, y )
# draw plausible values (assuming variance homogeneity)
mod3a &lt;- sirt::plausible.value.imputation.raschtype( f.yi.qk=f.yi.qk,
                  theta.list=theta.list, X=X, iter=10, burnin=5)
# draw plausible values (assuming variance heterogeneity)
#  -&gt; include predictor Z
mod3b &lt;- sirt::plausible.value.imputation.raschtype( f.yi.qk=f.yi.qk,
                  theta.list=theta.list, X=X, Z=X, iter=10, burnin=5)

# investigate variance of theta conditional on x
res3 &lt;- sapply( 0:1, FUN=function(vv){
        c( stats::var(theta[x==vv]), stats::var(mod3b$pvdraw[x==vv,1]),
              stats::var(mod3a$pvdraw[x==vv,1]))})
rownames(res3) &lt;- c("true", "pv(hetero)", "pv(homog)" )
colnames(res3) &lt;- c("x=0","x=1")
  ## &gt; round( res3, 2 )
  ##             x=0  x=1
  ## true       1.30 0.58
  ## pv(hetero) 1.29 0.55
  ## pv(homog)  1.06 0.77
## -&gt; assuming heteroscedastic variances recovers true conditional variance

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.mcmc.sirt'>
Plot Function for Objects of Class <code>mcmc.sirt</code>
</h2><span id='topic+plot.mcmc.sirt'></span>

<h3>Description</h3>

<p>Plot function for objects of class <code>mcmc.sirt</code>. These objects are generated
by: <code><a href="#topic+mcmc.2pno">mcmc.2pno</a></code>, <code><a href="#topic+mcmc.2pnoh">mcmc.2pnoh</a></code>,
<code><a href="#topic+mcmc.3pno.testlet">mcmc.3pno.testlet</a></code>, <code><a href="#topic+mcmc.2pno.ml">mcmc.2pno.ml</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcmc.sirt'
plot( x, layout=1, conflevel=0.9, round.summ=3,
   lag.max=.1, col.smooth="red", lwd.smooth=2, col.ci="orange",
   cex.summ=1, ask=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.mcmc.sirt_+3A_x">x</code></td>
<td>

<p>Object of class <code>mcmc.sirt</code>
</p>
</td></tr>
<tr><td><code id="plot.mcmc.sirt_+3A_layout">layout</code></td>
<td>

<p>Layout type. <code>layout=1</code> is the standard coda plot output,
<code>layout=2</code> gives a slightly different display.
</p>
</td></tr>
<tr><td><code id="plot.mcmc.sirt_+3A_conflevel">conflevel</code></td>
<td>

<p>Confidence level (only applies to <code>layout=2</code>)
</p>
</td></tr>
<tr><td><code id="plot.mcmc.sirt_+3A_round.summ">round.summ</code></td>
<td>

<p>Number of digits to be rounded in summary (only applies to <code>layout=2</code>)
</p>
</td></tr>
<tr><td><code id="plot.mcmc.sirt_+3A_lag.max">lag.max</code></td>
<td>

<p>Maximum lag for autocorrelation plot (only applies to <code>layout=2</code>).
The default of .1 means that it is set to 1/10 of the number of iterations.
</p>
</td></tr>
<tr><td><code id="plot.mcmc.sirt_+3A_col.smooth">col.smooth</code></td>
<td>

<p>Color of smooth trend in traceplot (only applies to <code>layout=2</code>)
</p>
</td></tr>
<tr><td><code id="plot.mcmc.sirt_+3A_lwd.smooth">lwd.smooth</code></td>
<td>

<p>Line type of smooth trend in traceplot (only applies to <code>layout=2</code>)
</p>
</td></tr>
<tr><td><code id="plot.mcmc.sirt_+3A_col.ci">col.ci</code></td>
<td>

<p>Color for displaying confidence interval (only applies to <code>layout=2</code>)
</p>
</td></tr>
<tr><td><code id="plot.mcmc.sirt_+3A_cex.summ">cex.summ</code></td>
<td>

<p>Cex size for descriptive summary (only applies to <code>layout=2</code>)
</p>
</td></tr>
<tr><td><code id="plot.mcmc.sirt_+3A_ask">ask</code></td>
<td>

<p>Ask for a new plot (only applies to <code>layout=2</code>)
</p>
</td></tr>
<tr><td><code id="plot.mcmc.sirt_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+mcmc.2pno">mcmc.2pno</a></code>, <code><a href="#topic+mcmc.2pnoh">mcmc.2pnoh</a></code>,
<code><a href="#topic+mcmc.3pno.testlet">mcmc.3pno.testlet</a></code>, <code><a href="#topic+mcmc.2pno.ml">mcmc.2pno.ml</a></code>
</p>

<hr>
<h2 id='plot.np.dich'>
Plot Method for Object of Class <code>np.dich</code>
</h2><span id='topic+plot.np.dich'></span>

<h3>Description</h3>

<p>This function plots nonparametric item response
functions estimated with <code>dich.np</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'np.dich'
plot(x, b, infit=NULL, outfit=NULL,
    nsize=100, askplot=TRUE, progress=TRUE, bands=FALSE,
    plot.b=FALSE, shade=FALSE, shadecol="burlywood1", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.np.dich_+3A_x">x</code></td>
<td>

<p>Object of class <code><a href="#topic+np.dich">np.dich</a></code>
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_b">b</code></td>
<td>

<p>Estimated item difficulty (threshold)
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_infit">infit</code></td>
<td>

<p>Infit (optional)
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_outfit">outfit</code></td>
<td>

<p>Outfit (optional)
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_nsize">nsize</code></td>
<td>

<p>XXX
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_askplot">askplot</code></td>
<td>

<p>Ask for new plot?
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_progress">progress</code></td>
<td>

<p>Display progress?
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_bands">bands</code></td>
<td>

<p>Draw confidence bands?
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_plot.b">plot.b</code></td>
<td>

<p>Plot difficulty parameter?
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_shade">shade</code></td>
<td>

<p>Shade curves?
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_shadecol">shadecol</code></td>
<td>

<p>Shade color
</p>
</td></tr>
<tr><td><code id="plot.np.dich_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>For examples see <code><a href="#topic+np.dich">np.dich</a></code>.
</p>

<hr>
<h2 id='polychoric2'>
Polychoric Correlation
</h2><span id='topic+polychoric2'></span><span id='topic+sirt_rcpp_polychoric2'></span>

<h3>Description</h3>

<p>This function estimates the polychoric correlation coefficient
using maximum likelihood estimation (Olsson, 1979).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polychoric2(dat, maxiter=100, cor.smooth=TRUE, use_pbv=1, conv=1e-10,
      rho_init=NULL, weights=NULL)

## exported Rcpp function
sirt_rcpp_polychoric2( dat, maxK, maxiter, use_pbv, conv, rho_init, weights)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="polychoric2_+3A_dat">dat</code></td>
<td>

<p>A dataset with integer values <code class="reqn">0,1,\ldots,K</code>
</p>
</td></tr>
<tr><td><code id="polychoric2_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="polychoric2_+3A_cor.smooth">cor.smooth</code></td>
<td>

<p>An optional logical indicating whether the polychoric correlation
matrix should be smooth to ensure positive definiteness.
</p>
</td></tr>
<tr><td><code id="polychoric2_+3A_use_pbv">use_pbv</code></td>
<td>
<p>Integer indicating whether the <span class="pkg">pbv</span> package is used
for computation of bivariate normal distribution. <code>0</code> stands for
the simplest approximation in <span class="pkg">sirt</span> (Cox &amp; Wermuth, 1991, as
implemented in <code><a href="#topic+polychoric2">polychoric2</a></code>)
while versions <code>1</code> and
<code>2</code> uses the algorithm of <span class="pkg">pbv</span> (the first one copied into
the <span class="pkg">sirt</span> package, the second one linking Rcpp code to <span class="pkg">pbv</span>.)</p>
</td></tr>
<tr><td><code id="polychoric2_+3A_conv">conv</code></td>
<td>
<p>Convergence criterion</p>
</td></tr>
<tr><td><code id="polychoric2_+3A_rho_init">rho_init</code></td>
<td>
<p>Optional matrix of initial values for polychoric correlations</p>
</td></tr>
<tr><td><code id="polychoric2_+3A_weights">weights</code></td>
<td>
<p>Optional vector of sampling weights</p>
</td></tr>
<tr><td><code id="polychoric2_+3A_maxk">maxK</code></td>
<td>
<p>Maximum number of categories</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>tau</code></td>
<td>
<p>Matrix of thresholds</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>Polychoric correlation matrix</p>
</td></tr>
<tr><td><code>Nobs</code></td>
<td>
<p>Sample size for every item pair</p>
</td></tr>
<tr><td><code>maxcat</code></td>
<td>
<p>Maximum number of categories per item</p>
</td></tr>
</table>


<h3>References</h3>

<p>Cox, D. R., &amp; Wermuth, N. (1991). A simple approximation for bivariate and
trivariate normal integrals.
<em>International Statistical Review, 59</em>(2), 263-269.
</p>
<p>Olsson, U. (1979). Maximum likelihood estimation of the polychoric correlation
coefficient. <em>Psychometrika, 44</em>(4), 443-460.
<a href="https://doi.org/10.1007/BF02296207">doi:10.1007/BF02296207</a>
</p>


<h3>See Also</h3>

<p>See the <code><a href="psych.html#topic+polychoric">psych::polychoric</a></code>
function in the <span class="pkg">psych</span> package.
</p>
<p>For estimating tetrachoric correlations see <code><a href="#topic+tetrachoric2">tetrachoric2</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: data.Students | activity scale
#############################################################################

data(data.Students, package="CDM")
dat &lt;- data.Students[, paste0("act", 1:5 ) ]

# tetrachoric correlation from psych package
library(psych)
t0 &lt;- psych::polychoric(dat)$rho
# Olsson method (maximum likelihood estimation)
t1 &lt;- sirt::polychoric2(dat)$rho
# maximum absolute difference
max( abs( t0 - t1 ) )
  ##   [1] 0.004102429
</code></pre>

<hr>
<h2 id='prior_model_parse'>
Parsing a Prior Model
</h2><span id='topic+prior_model_parse'></span>

<h3>Description</h3>

<p>Parses a string specifying a prior model which is needed
for the <code>prior</code> argument in <code>LAM::amh</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prior_model_parse(prior_model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prior_model_parse_+3A_prior_model">prior_model</code></td>
<td>

<p>String specifying the prior conforming to <span class="rlang"><b>R</b></span> syntax.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with specified prior distributions for parameters
as needed for the <code>prior</code> argument in <code>LAM::amh</code>
</p>


<h3>See Also</h3>

<p><code>LAM::amh</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Toy example prior distributions
#############################################################################

#*** define prior model as a string
prior_model &lt;- "
  # prior distributions means
  mu1 ~ dnorm( NA, mean=0, sd=1 )
  mu2 ~ dnorm(NA)       # mean T2 and T3
  # prior distribution standard deviation
  sig1 ~ dunif(NA,0, max=10)
      "

#*** convert priors into a list
res &lt;- sirt::prior_model_parse( prior_model )
str(res)
  ##  List of 3
  ##   $ mu1 :List of 2
  ##    ..$ : chr "dnorm"
  ##    ..$ :List of 3
  ##    .. ..$ NA  : num NA
  ##    .. ..$ mean: num 0
  ##    .. ..$ sd  : num 1
  ##   $ mu2 :List of 2
  ##    ..$ : chr "dnorm"
  ##    ..$ :List of 1
  ##    .. ..$ : num NA
  ##   $ sig1:List of 2
  ##    ..$ : chr "dunif"
  ##    ..$ :List of 3
  ##    .. ..$ NA : num NA
  ##    .. ..$ NA : num 0
  ##    .. ..$ max: num 10
</code></pre>

<hr>
<h2 id='prmse.subscores.scales'>
Proportional Reduction of Mean Squared
Error (PRMSE) for Subscale Scores
</h2><span id='topic+prmse.subscores.scales'></span>

<h3>Description</h3>

<p>This function estimates the proportional reduction of mean squared
error (PRMSE) according to
Haberman (Haberman 2008; Haberman, Sinharay &amp; Puhan, 2008; see
Meijer et al. 2017 for an overview).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prmse.subscores.scales(data, subscale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prmse.subscores.scales_+3A_data">data</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of item responses
</p>
</td></tr>
<tr><td><code id="prmse.subscores.scales_+3A_subscale">subscale</code></td>
<td>

<p>Vector of labels corresponding to subscales
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix with columns corresponding to subscales <br />
The symbol <code>X</code> denotes the subscale and <code>Z</code>
the whole scale (see also in the Examples section for the structure of
this matrix).
</p>


<h3>References</h3>

<p>Haberman, S. J. (2008). When can subscores have value?
<em>Journal of Educational and Behavioral Statistics,
33</em>, 204-229.
</p>
<p>Haberman, S., Sinharay, S., &amp; Puhan, G. (2008).
Reporting subscores for institutions.
<em>British Journal of Mathematical and Statistical Psychology,
62</em>, 79-95.
</p>
<p>Meijer, R. R., Boeve, A. J., Tendeiro, J. N., Bosker, R. J., &amp; Albers, C. J. (2017).
The use of subscores in higher education: When is this useful?.
<em>Frontiers in Psychology | Educational Psychology, 8</em>.
</p>


<h3>See Also</h3>

<p>See the <span class="pkg">subscore</span> package for computing subscores and the PRMSE measures,
especially <code>subscore::CTTsub</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: PRMSE Reading data data.read
#############################################################################

data( data.read )
p1 &lt;- sirt::prmse.subscores.scales(data=data.read,
         subscale=substring( colnames(data.read), 1,1 ) )
print( p1, digits=3 )
  ##                 A       B       C
  ## N         328.000 328.000 328.000
  ## nX          4.000   4.000   4.000
  ## M.X         2.616   2.811   3.253
  ## Var.X       1.381   1.059   1.107
  ## SD.X        1.175   1.029   1.052
  ## alpha.X     0.545   0.381   0.640
  ## [...]
  ## nZ         12.000  12.000  12.000
  ## M.Z         8.680   8.680   8.680
  ## Var.Z       5.668   5.668   5.668
  ## SD.Z        2.381   2.381   2.381
  ## alpha.Z     0.677   0.677   0.677
  ## [...]
  ## cor.TX_Z    0.799   0.835   0.684
  ## rmse.X      0.585   0.500   0.505
  ## rmse.Z      0.522   0.350   0.614
  ## rmse.XZ     0.495   0.350   0.478
  ## prmse.X     0.545   0.381   0.640
  ## prmse.Z     0.638   0.697   0.468
  ## prmse.XZ    0.674   0.697   0.677
#-&gt; Scales A and B do not have lower RMSE,
#   but for scale C the RMSE is smaller than the RMSE of a
#   prediction based on a whole scale.
</code></pre>

<hr>
<h2 id='prob.guttman'>
Probabilistic Guttman Model
</h2><span id='topic+prob.guttman'></span><span id='topic+summary.prob.guttman'></span><span id='topic+logLik.prob.guttman'></span><span id='topic+anova.prob.guttman'></span><span id='topic+IRT.likelihood.prob.guttman'></span><span id='topic+IRT.posterior.prob.guttman'></span><span id='topic+IRT.irfprob.prob.guttman'></span>

<h3>Description</h3>

<p>This function estimates the probabilistic Guttman model which
is a special case of an ordered latent trait model (Hanson, 2000;
Proctor, 1970).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prob.guttman(dat, pid=NULL, guess.equal=FALSE,  slip.equal=FALSE,
    itemlevel=NULL, conv1=0.001, glob.conv=0.001, mmliter=500)

## S3 method for class 'prob.guttman'
summary(object,...)

## S3 method for class 'prob.guttman'
anova(object,...)

## S3 method for class 'prob.guttman'
logLik(object,...)

## S3 method for class 'prob.guttman'
IRT.irfprob(object,...)

## S3 method for class 'prob.guttman'
IRT.likelihood(object,...)

## S3 method for class 'prob.guttman'
IRT.posterior(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prob.guttman_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous item responses
</p>
</td></tr>
<tr><td><code id="prob.guttman_+3A_pid">pid</code></td>
<td>

<p>Optional vector of person identifiers
</p>
</td></tr>
<tr><td><code id="prob.guttman_+3A_guess.equal">guess.equal</code></td>
<td>

<p>Should the same guessing parameters for all the items estimated?
</p>
</td></tr>
<tr><td><code id="prob.guttman_+3A_slip.equal">slip.equal</code></td>
<td>

<p>Should the same slipping parameters for all the items estimated?
</p>
</td></tr>
<tr><td><code id="prob.guttman_+3A_itemlevel">itemlevel</code></td>
<td>

<p>A vector of item levels of the Guttman scale for each item. If there
are <code class="reqn">K</code> different item levels, then the Guttman scale possesses
<code class="reqn">K</code> ordered trait values.
</p>
</td></tr>
<tr><td><code id="prob.guttman_+3A_conv1">conv1</code></td>
<td>

<p>Convergence criterion for item parameters
</p>
</td></tr>
<tr><td><code id="prob.guttman_+3A_glob.conv">glob.conv</code></td>
<td>

<p>Global convergence criterion for the deviance
</p>
</td></tr>
<tr><td><code id="prob.guttman_+3A_mmliter">mmliter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="prob.guttman_+3A_object">object</code></td>
<td>

<p>Object of class <code>prob.guttman</code>
</p>
</td></tr>
<tr><td><code id="prob.guttman_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>prob.guttman</code>
</p>
<table>
<tr><td><code>person</code></td>
<td>
<p>Estimated person parameters</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Estimated item parameters</p>
</td></tr>
<tr><td><code>theta.k</code></td>
<td>
<p>Ability levels</p>
</td></tr>
<tr><td><code>trait</code></td>
<td>
<p>Estimated trait distribution</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>itemdesign</code></td>
<td>
<p>Specified allocation of items to trait levels</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hanson, B. (2000). <em>IRT parameter estimation using the EM algorithm</em>.
Technical Report.
</p>
<p>Proctor, C. H. (1970). A probabilistic formulation and statistical analysis
for Guttman scaling. <em>Psychometrika, 35</em>, 73-78.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset Reading
#############################################################################
data(data.read)
dat &lt;- data.read

#***
# Model 1: estimate probabilistic Guttman model
mod1 &lt;- sirt::prob.guttman( dat )
summary(mod1)

#***
# Model 2: probabilistic Guttman model with equal guessing and slipping parameters
mod2 &lt;- sirt::prob.guttman( dat, guess.equal=TRUE, slip.equal=TRUE)
summary(mod2)

#***
# Model 3: Guttman model with three a priori specified item levels
itemlevel &lt;- rep(1,12)
itemlevel[ c(2,5,8,10,12) ] &lt;- 2
itemlevel[ c(3,4,6) ] &lt;- 3
mod3 &lt;- sirt::prob.guttman( dat, itemlevel=itemlevel )
summary(mod3)

## Not run: 
#***
# Model3m: estimate Model 3 in mirt

library(mirt)
# define four ordered latent classes
Theta &lt;- scan(nlines=1)
   0 0 0    1 0 0   1 1 0   1 1 1
Theta &lt;- matrix( Theta, nrow=4, ncol=3,byrow=TRUE)

# define mirt model
I &lt;- ncol(dat)  # I=12
mirtmodel &lt;- mirt::mirt.model("
        # specify factors for each item level
        C1=1,7,9,11
        C2=2,5,8,10,12
        C3=3,4,6
        ")
# get initial parameter values
mod.pars &lt;- mirt::mirt(dat, model=mirtmodel,  pars="values")
# redefine initial parameter values
mod.pars[ mod.pars$name=="d","value" ]  &lt;- -1
mod.pars[ mod.pars$name %in% paste0("a",1:3) &amp; mod.pars$est,"value" ]  &lt;- 2
mod.pars
# define prior for latent class analysis
lca_prior &lt;- function(Theta,Etable){
  # number of latent Theta classes
  TP &lt;- nrow(Theta)
  # prior in initial iteration
  if ( is.null(Etable) ){ prior &lt;- rep( 1/TP, TP ) }
  # process Etable (this is correct for datasets without missing data)
  if ( ! is.null(Etable) ){
    # sum over correct and incorrect expected responses
    prior &lt;- ( rowSums(Etable[, seq(1,2*I,2)]) + rowSums(Etable[,seq(2,2*I,2)]) )/I
                 }
  prior &lt;- prior / sum(prior)
  return(prior)
}
# estimate model in mirt
mod3m &lt;- mirt::mirt(dat, mirtmodel, pars=mod.pars, verbose=TRUE,
            technical=list( customTheta=Theta, customPriorFun=lca_prior) )
# correct number of estimated parameters
mod3m@nest &lt;- as.integer(sum(mod.pars$est) + nrow(Theta)-1 )
# extract log-likelihood and compute AIC and BIC
mod3m@logLik
( AIC &lt;- -2*mod3m@logLik+2*mod3m@nest )
( BIC &lt;- -2*mod3m@logLik+log(mod3m@Data$N)*mod3m@nest )
# compare with information criteria from prob.guttman
mod3$ic
# model fit in mirt
mirt::M2(mod3m)
# extract coefficients
( cmod3m &lt;- sirt::mirt.wrapper.coef(mod3m) )
# compare estimated distributions
round( cbind( "sirt"=mod3$trait$prob, "mirt"=mod3m@Prior[[1]] ), 5 )
  ##           sirt    mirt
  ##   [1,] 0.13709 0.13765
  ##   [2,] 0.30266 0.30303
  ##   [3,] 0.15239 0.15085
  ##   [4,] 0.40786 0.40846
# compare estimated item parameters
ipars &lt;- data.frame( "guess.sirt"=mod3$item$guess,
                     "guess.mirt"=plogis( cmod3m$coef$d ) )
ipars$slip.sirt &lt;- mod3$item$slip
ipars$slip.mirt &lt;- 1-plogis( rowSums(cmod3m$coef[, c("a1","a2","a3","d") ] ) )
round( ipars, 4 )
  ##      guess.sirt guess.mirt slip.sirt slip.mirt
  ##   1      0.7810     0.7804    0.1383    0.1382
  ##   2      0.4513     0.4517    0.0373    0.0368
  ##   3      0.3203     0.3200    0.0747    0.0751
  ##   4      0.3009     0.3007    0.3082    0.3087
  ##   5      0.5776     0.5779    0.1800    0.1798
  ##   6      0.3758     0.3759    0.3047    0.3051
  ##   7      0.7262     0.7259    0.0625    0.0623
  ##   [...]

#***
# Model 4: Monotone item response function estimated in mirt

# define four ordered latent classes
Theta &lt;- scan(nlines=1)
   0 0 0    1 0 0   1 1 0   1 1 1
Theta &lt;- matrix( Theta, nrow=4, ncol=3,byrow=TRUE)

# define mirt model
I &lt;- ncol(dat)  # I=12
mirtmodel &lt;- mirt::mirt.model("
        # specify factors for each item level
        C1=1-12
        C2=1-12
        C3=1-12
        ")
# get initial parameter values
mod.pars &lt;- mirt::mirt(dat, model=mirtmodel,  pars="values")
# redefine initial parameter values
mod.pars[ mod.pars$name=="d","value" ]  &lt;- -1
mod.pars[ mod.pars$name %in% paste0("a",1:3) &amp; mod.pars$est,"value" ]  &lt;- .6
# set lower bound to zero ton ensure monotonicity
mod.pars[ mod.pars$name %in% paste0("a",1:3),"lbound" ]  &lt;- 0
mod.pars
# estimate model in mirt
mod4 &lt;- mirt::mirt(dat, mirtmodel, pars=mod.pars, verbose=TRUE,
            technical=list( customTheta=Theta, customPriorFun=lca_prior) )
# correct number of estimated parameters
mod4@nest &lt;- as.integer(sum(mod.pars$est) + nrow(Theta)-1 )
# extract coefficients
cmod4 &lt;- sirt::mirt.wrapper.coef(mod4)
cmod4
# compute item response functions
cmod4c &lt;- cmod4$coef[, c("d", "a1", "a2", "a3" ) ]
probs4 &lt;- t( apply( cmod4c, 1, FUN=function(ll){
                 plogis(cumsum(as.numeric(ll))) } ) )
matplot( 1:4,  t(probs4), type="b", pch=1:I)

## End(Not run)
</code></pre>

<hr>
<h2 id='Q3'>
Estimation of the <code class="reqn">Q_3</code> Statistic (Yen, 1984)
</h2><span id='topic+Q3'></span>

<h3>Description</h3>

<p>This function estimates the <code class="reqn">Q_3</code> statistic according to Yen (1984).
The statistic <code class="reqn">Q_3</code> is calculated for every item pair <code class="reqn">(i,j)</code>
which is the correlation between item residuals after fitting the Rasch model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Q3(dat, theta, b, progress=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Q3_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous item responses
</p>
</td></tr>
<tr><td><code id="Q3_+3A_theta">theta</code></td>
<td>

<p>Vector of length <code class="reqn">N</code> of person parameter estimates (e.g. obtained from
<code><a href="#topic+wle.rasch">wle.rasch</a></code>)
</p>
</td></tr>
<tr><td><code id="Q3_+3A_b">b</code></td>
<td>

<p>Vector of length <code class="reqn">I</code> (e.g. obtained from <code><a href="#topic+rasch.mml2">rasch.mml2</a></code>)
</p>
</td></tr>
<tr><td><code id="Q3_+3A_progress">progress</code></td>
<td>

<p>Should iteration progress be displayed?
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>q3.matrix</code></td>
<td>
<p>An <code class="reqn">I \times I</code> matrix of <code class="reqn">Q_3</code> statistics</p>
</td></tr>
<tr><td><code>q3.long</code></td>
<td>
<p>Just the <code>q3.matrix</code> in long matrix format where every row
corresponds to an item pair</p>
</td></tr>
<tr><td><code>expected</code></td>
<td>
<p>An <code class="reqn">N \times I</code> matrix of expected probabilities
by the Rasch model</p>
</td></tr>
<tr><td><code>residual</code></td>
<td>
<p>An <code class="reqn">N \times I</code> matrix of residuals obtained after
fitting the Rasch model</p>
</td></tr>
<tr><td><code>Q3.stat</code></td>
<td>
<p>Vector with descriptive statistics of <code class="reqn">Q_3</code></p>
</td></tr>
</table>


<h3>References</h3>

<p>Yen, W. M. (1984). Effects of local item dependence on the fit and equating
performance of the three-parameter logistic model.
<em>Applied Psychological Measurement, 8</em>, 125-145.
</p>


<h3>See Also</h3>

<p>For the estimation of the average <code class="reqn">Q_3</code> statistic within testlets see
<code><a href="#topic+Q3.testlet">Q3.testlet</a></code>.
</p>
<p>For modeling testlet effects see <code><a href="#topic+mcmc.3pno.testlet">mcmc.3pno.testlet</a></code>.
</p>
<p>For handling local dependencies in IRT models see
<code><a href="#topic+rasch.copula2">rasch.copula2</a></code>, <code><a href="#topic+rasch.pml3">rasch.pml3</a></code> or <br />
<code><a href="#topic+rasch.pairwise.itemcluster">rasch.pairwise.itemcluster</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: data.read. The 12 items are arranged in 4 testlets
#############################################################################
data(data.read)

# estimate the Rasch model
mod &lt;- sirt::rasch.mml2( data.read)
# estmate WLEs
mod.wle &lt;- sirt::wle.rasch( dat=data.read, b=mod$item$b )
# calculate Yen's Q3 statistic
mod.q3 &lt;- sirt::Q3( dat=data.read, theta=mod.wle$theta, b=mod$item$b )
  ##   Yen's Q3 Statistic based on an estimated theta score
  ##   *** 12 Items | 66 item pairs
  ##   *** Q3 Descriptives
  ##        M     SD    Min    10%    25%    50%    75%    90%    Max
  ##   -0.085  0.110 -0.261 -0.194 -0.152 -0.107 -0.051  0.041  0.412

# plot Q3 statistics
I &lt;- ncol(data.read)
image( 1:I, 1:I, mod.q3$q3.matrix, col=gray( 1 - (0:32)/32),
        xlab="Item", ylab="Item")
abline(v=c(5,9)) # borders for testlets
abline(h=c(5,9))

## Not run: 
# obtain Q3 statistic from modelfit.sirt function which is based on the
# posterior distribution of theta and not on observed values
fitmod &lt;- sirt::modelfit.sirt( mod )
# extract Q3 statistic
q3stat &lt;- fitmod$itempairs$Q3
  ##  &gt; summary(q3stat)
  ##      Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
  ##  -0.21760 -0.11590 -0.07280 -0.05545 -0.01220  0.44710
  ##  &gt; sd(q3stat)
  ##  [1] 0.1101451

## End(Not run)
</code></pre>

<hr>
<h2 id='Q3.testlet'>
<code class="reqn">Q_3</code> Statistic of Yen (1984) for Testlets
</h2><span id='topic+Q3.testlet'></span>

<h3>Description</h3>

<p>This function calculates the average <code class="reqn">Q_3</code> statistic (Yen, 1984) within and
between testlets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Q3.testlet(q3.res, testlet.matrix, progress=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Q3.testlet_+3A_q3.res">q3.res</code></td>
<td>

<p>An object generated by <code><a href="#topic+Q3">Q3</a></code>
</p>
</td></tr>
<tr><td><code id="Q3.testlet_+3A_testlet.matrix">testlet.matrix</code></td>
<td>

<p>A matrix with two columns. The first column contains names of the testlets and
the second names of the items. See the examples for the definition of such
matrices.
</p>
</td></tr>
<tr><td><code id="Q3.testlet_+3A_progress">progress</code></td>
<td>
<p>Logical indicating whether computation
progress should be displayed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>testlet.q3</code></td>
<td>
<p>Data frame with average <code class="reqn">Q_3</code> statistics within testlets</p>
</td></tr>
<tr><td><code>testlet.q3.korr</code></td>
<td>
<p>Matrix of average <code class="reqn">Q_3</code> statistics within and
between testlets</p>
</td></tr>
</table>


<h3>References</h3>

<p>Yen, W. M. (1984). Effects of local item dependence on the fit and equating
performance of the three-parameter logistic model.
<em>Applied Psychological Measurement, 8</em>, 125-145.
</p>


<h3>See Also</h3>

<p>For estimating all <code class="reqn">Q_3</code> statistics between item pairs use
<code><a href="#topic+Q3">Q3</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: data.read. The 12 items are arranged in 4 testlets
#############################################################################
data(data.read)

# estimate the Rasch model
mod &lt;- sirt::rasch.mml2( data.read)
mod$item

# estmate WLEs
mod.wle &lt;- sirt::wle.rasch( dat=data.read, b=mod$item$b )

# Yen's Q3 statistic
mod.q3 &lt;- sirt::Q3( dat=data.read, theta=mod.wle$theta, b=mod$item$b )

# Yen's Q3 statistic with testlets
items &lt;- colnames(data.read)
testlet.matrix &lt;- cbind( substring(  items,1,1), items )
mod.testletq3 &lt;- sirt::Q3.testlet( q3.res=mod.q3,testlet.matrix=testlet.matrix)
mod.testletq3
</code></pre>

<hr>
<h2 id='qmc.nodes'>
Calculation of Quasi Monte Carlo Integration Points
</h2><span id='topic+qmc.nodes'></span>

<h3>Description</h3>

<p>This function calculates integration nodes based on the multivariate
normal distribution with zero mean vector and identity covariance
matrix. See Pan and Thompson (2007) and Gonzales et al. (2006)
for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qmc.nodes(snodes, ndim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qmc.nodes_+3A_snodes">snodes</code></td>
<td>

<p>Number of integration nodes
</p>
</td></tr>
<tr><td><code id="qmc.nodes_+3A_ndim">ndim</code></td>
<td>

<p>Number of dimensions
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>theta</code></td>
<td>
<p>A matrix of integration points</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function uses the
<code><a href="sfsmisc.html#topic+QUnif">sfsmisc::QUnif</a></code> function from
the <span class="pkg">sfsmisc</span> package.
</p>


<h3>References</h3>

<p>Gonzalez, J., Tuerlinckx, F., De Boeck, P., &amp; Cools, R. (2006).
Numerical integration in logistic-normal models.
<em>Computational Statistics &amp; Data Analysis, 51</em>, 1535-1548.
</p>
<p>Pan, J., &amp; Thompson, R. (2007). Quasi-Monte Carlo estimation in
generalized linear mixed models. <em>Computational Statistics &amp;
Data Analysis, 51</em>, 5765-5775.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## some toy examples

# 5 nodes on one dimension
qmc.nodes( snodes=5, ndim=1 )
  ##            [,1]
  ## [1,]  0.0000000
  ## [2,] -0.3863753
  ## [3,]  0.8409238
  ## [4,] -0.8426682
  ## [5,]  0.3850568

# 7 nodes on two dimensions
qmc.nodes( snodes=7, ndim=2 )
  ##             [,1]        [,2]
  ## [1,]  0.00000000 -0.43072730
  ## [2,] -0.38637529  0.79736332
  ## [3,]  0.84092380 -1.73230641
  ## [4,] -0.84266815 -0.03840544
  ## [5,]  0.38505683  1.51466109
  ## [6,] -0.00122394 -0.86704605
  ## [7,]  1.35539115  0.33491073
</code></pre>

<hr>
<h2 id='R2conquest'>
Running ConQuest From Within <span class="rlang"><b>R</b></span>
</h2><span id='topic+R2conquest'></span><span id='topic+summary.R2conquest'></span><span id='topic+read.show'></span><span id='topic+read.show.term'></span><span id='topic+read.show.regression'></span><span id='topic+read.pv'></span><span id='topic+read.multidimpv'></span><span id='topic+read.pimap'></span>

<h3>Description</h3>

<p>The function <code>R2conquest</code> runs the IRT software ConQuest
(Wu, Adams, Wilson &amp; Haldane, 2007) from within <span class="rlang"><b>R</b></span>.
</p>
<p>Other functions are utility functions for reading item parameters, plausible
values or person-item maps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>R2conquest(dat, path.conquest, conquest.name="console", converge=0.001,
    deviancechange=1e-04, iter=800, nodes=20, minnode=-6, maxnode=6,
    show.conquestoutput=FALSE, name="rasch", pid=1:(nrow(dat)), wgt=NULL, X=NULL,
    set.constraints=NULL, model="item", regression=NULL,
    itemcodes=seq(0,max(dat,na.rm=TRUE)), constraints=NULL, digits=5, onlysyntax=FALSE,
    qmatrix=NULL, import.regression=NULL, anchor.regression=NULL,
    anchor.covariance=NULL, pv=TRUE, designmatrix=NULL, only.calibration=FALSE,
    init_parameters=NULL, n_plausible=10,  persons.elim=TRUE, est.wle=TRUE,
    save.bat=TRUE, use.bat=FALSE, read.output=TRUE, ignore.pid=FALSE)

## S3 method for class 'R2conquest'
summary(object, ...)

# read all terms in a show file or only some terms
read.show(showfile)
read.show.term(showfile, term)

# read regression parameters in a show file
read.show.regression(showfile)

# read unidimensional plausible values form a pv file
read.pv(pvfile, npv=5)
# read multidimensional plausible values
read.multidimpv(pvfile, ndim, npv=5)

# read person-item map
read.pimap(showfile)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="R2conquest_+3A_dat">dat</code></td>
<td>

<p>Data frame of item responses
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_path.conquest">path.conquest</code></td>
<td>

<p>Directory where the ConQuest executable file is located
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_conquest.name">conquest.name</code></td>
<td>

<p>Name of the ConQuest executable.
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_converge">converge</code></td>
<td>

<p>Maximal change in parameters
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_deviancechange">deviancechange</code></td>
<td>

<p>Maximal change in deviance
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_iter">iter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_nodes">nodes</code></td>
<td>

<p>Number of nodes for integration
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_minnode">minnode</code></td>
<td>
<p>Minimum value of discrete grid of <code class="reqn">\theta</code> nodes</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_maxnode">maxnode</code></td>
<td>
<p>Maximum value of discrete grid of <code class="reqn">\theta</code> nodes</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_show.conquestoutput">show.conquestoutput</code></td>
<td>

<p>Show ConQuest run log file on console?
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_name">name</code></td>
<td>

<p>Name of the output files. The default is <code>'rasch'</code>.
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_pid">pid</code></td>
<td>

<p>Person identifier
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_wgt">wgt</code></td>
<td>

<p>Vector of person weights
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_x">X</code></td>
<td>

<p>Matrix of covariates for the latent regression model
(e.g. gender, socioeconomic status, ..) or
for the item design (e.g. raters, booklets, ...)
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_set.constraints">set.constraints</code></td>
<td>

<p>This is the set.constraints in ConQuest. It can be
<code>"cases"</code> (constraint for persons), <code>"items"</code>
or <code>"none"</code>
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_model">model</code></td>
<td>

<p>Definition model statement. It can be for example
<code>"item+item*step"</code> or <code>"item+booklet+rater"</code>
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_regression">regression</code></td>
<td>

<p>The ConQuest regression statement (for example <code>"gender+status"</code>)
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_itemcodes">itemcodes</code></td>
<td>

<p>Vector of valid codes for item responses. E.g. for partial credit
data with at most 3 points it must be <code>c(0,1,2,3)</code>.
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_constraints">constraints</code></td>
<td>

<p>Matrix of item parameter constraints. 1st column: Item names,
2nd column: Item parameters. It only works correctly
for dichotomous data.
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_digits">digits</code></td>
<td>

<p>Number of digits for covariates in the latent regression model
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_onlysyntax">onlysyntax</code></td>
<td>

<p>Should only be ConQuest syntax generated?
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_qmatrix">qmatrix</code></td>
<td>

<p>Matrix of item loadings on dimensions in a multidimensional
IRT model
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_import.regression">import.regression</code></td>
<td>

<p>Name of an file with initial covariance parameters
(follow the ConQuest specification rules!)
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_anchor.regression">anchor.regression</code></td>
<td>

<p>Name of an file with anchored regression parameters
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_anchor.covariance">anchor.covariance</code></td>
<td>

<p>Name of an file with anchored covariance parameters
(follow the ConQuest specification rules!)
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_pv">pv</code></td>
<td>

<p>Draw plausible values?
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_designmatrix">designmatrix</code></td>
<td>

<p>Design matrix for item parameters (see the ConQuest manual)
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_only.calibration">only.calibration</code></td>
<td>

<p>Estimate only item parameters and not person parameters
(no WLEs or plausible values are estimated)?
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_init_parameters">init_parameters</code></td>
<td>

<p>Name of an file with initial item parameters (follow the
ConQuest specification rules!)
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_n_plausible">n_plausible</code></td>
<td>

<p>Number of plausible values
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_persons.elim">persons.elim</code></td>
<td>

<p>Eliminate persons with only missing item responses?
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_est.wle">est.wle</code></td>
<td>

<p>Estimate weighted likelihood estimate?
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_save.bat">save.bat</code></td>
<td>

<p>Save bat file?
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_use.bat">use.bat</code></td>
<td>

<p>Run ConQuest from within <span class="rlang"><b>R</b></span> due a direct call via the <code>system</code>
command (<code>use.bat=FALSE</code>) or via a <code>system</code> call
of a bat file in the working directory (<code>use.bat=TRUE</code>)
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_read.output">read.output</code></td>
<td>

<p>Should ConQuest output files be processed? Default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_ignore.pid">ignore.pid</code></td>
<td>
<p>Logical indicating whether person identifiers (<code>pid</code>)
should be processed in ConQuest input syntax.</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_object">object</code></td>
<td>

<p>Object of class <code>R2conquest</code>
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_showfile">showfile</code></td>
<td>

<p>A ConQuest show file (<code>shw</code> file)
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_term">term</code></td>
<td>

<p>Name of the term to be extracted in the show file
</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_pvfile">pvfile</code></td>
<td>
<p>File with plausible values</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_ndim">ndim</code></td>
<td>
<p>Number of dimensions</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_npv">npv</code></td>
<td>
<p>Number of plausible values</p>
</td></tr>
<tr><td><code id="R2conquest_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Consult the ConQuest manual (Wu et al., 2007) for specification
details.
</p>


<h3>Value</h3>

<p>A list with several entries
</p>
<table>
<tr><td><code>item</code></td>
<td>
<p>Data frame with item parameters and item statistics</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with person parameters</p>
</td></tr>
<tr><td><code>shw.itemparameter</code></td>
<td>
<p>ConQuest output table for item parameters</p>
</td></tr>
<tr><td><code>shw.regrparameter</code></td>
<td>
<p>ConQuest output table for regression parameters</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>More values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wu, M. L., Adams, R. J., Wilson, M. R. &amp; Haldane, S. (2007).
<em>ACER ConQuest Version 2.0</em>. Mulgrave.
https://shop.acer.edu.au/acer-shop/group/CON3.
</p>


<h3>See Also</h3>

<p>See also the <span class="pkg">eat</span> package (<a href="https://r-forge.r-project.org/projects/eat/">https://r-forge.r-project.org/projects/eat/</a>)
for elaborate functionality of using ConQuest from within <span class="rlang"><b>R</b></span>. See also
the <span class="pkg">conquestr</span> package for another <span class="rlang"><b>R</b></span> wrapper to the ConQuest software
(at least version 4 of ConQuest has to be installed).
</p>
<p>See also the <span class="pkg">TAM</span> package for similar (and even extended)
functionality for specifying item response models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# define ConQuest path
path.conquest &lt;- "C:/Conquest/"

#############################################################################
# EXAMPLE 1: Dichotomous data (data.pisaMath)
#############################################################################
library(sirt)
data(data.pisaMath)
dat &lt;- data.pisaMath$data

# select items
items &lt;- colnames(dat)[ which( substring( colnames(dat), 1, 1)=="M" ) ]

#***
# Model 11: Rasch model
mod11 &lt;- sirt::R2conquest(dat=dat[,items], path.conquest=path.conquest,
             pid=dat$idstud, name="mod11")
summary(mod11)
# read show file
shw11 &lt;- sirt::read.show( "mod11.shw" )
# read person-item map
pi11 &lt;- sirt::read.pimap(showfile="mod11.shw")

#***
# Model 12: Rasch model with fixed item difficulties (from Model 1)
mod12 &lt;- sirt::R2conquest(dat=dat[,items], path.conquest=path.conquest,
             pid=dat$idstud, constraints=mod11$item[, c("item","itemdiff")],
             name="mod12")
summary(mod12)

#***
# Model 13: Latent regression model with predictors female, hisei and migra
mod13a &lt;- sirt::R2conquest(dat=dat[,items], path.conquest=path.conquest,
             pid=dat$idstud, X=dat[, c("female", "hisei", "migra") ],
             name="mod13a")
summary(mod13a)

# latent regression with a subset of predictors
mod13b &lt;- sirt::R2conquest(dat=dat[,items], path.conquest=path.conquest,
             pid=dat$idstud, X=dat[, c("female", "hisei", "migra") ],
             regression="hisei migra", name="mod13b")

#***
# Model 14: Differential item functioning (female)
mod14 &lt;- sirt::R2conquest(dat=dat[,items], path.conquest=path.conquest,
             pid=dat$idstud, X=dat[, c("female"), drop=FALSE],
             model="item+female+item*female",  regression="",  name="mod14")

#############################################################################
# EXAMPLE 2: Polytomous data (data.Students)
#############################################################################
library(CDM)
data(data.Students)
dat &lt;- data.Students

# select items
items &lt;- grep.vec( "act", colnames(dat) )$x

#***
# Model 21: Partial credit model
mod21 &lt;- sirt::R2conquest(dat=dat[,items], path.conquest=path.conquest,
              model="item+item*step",  name="mod21")

#***
# Model 22: Rating scale model
mod22 &lt;- sirt::R2conquest(dat=dat[,items], path.conquest=path.conquest,
              model="item+step", name="mod22")

#***
# Model 23: Multidimensional model
items &lt;- grep.vec( c("act", "sc" ), colnames(dat),  "OR" )$x
qmatrix &lt;- matrix( 0, nrow=length(items), 2 )
qmatrix[1:5,1] &lt;- 1
qmatrix[6:9,2] &lt;- 1
mod23 &lt;- sirt::R2conquest(dat=dat[,items], path.conquest=path.conquest,
            model="item+item*step", qmatrix=qmatrix, name="mod23")

#############################################################################
# EXAMPLE 3: Multi facet models (data.ratings1)
#############################################################################
library(sirt)
data(data.ratings1)
dat &lt;- data.ratings1

items &lt;- paste0("k",1:5)

# use numeric rater ID's
raters &lt;- as.numeric( substring( paste( dat$rater ), 3 ) )

#***
# Model 31: Rater model 'item+item*step+rater'
mod31 &lt;- sirt::R2conquest(dat=dat[,items], path.conquest=path.conquest,
              itemcodes=0:3, model="item+item*step+rater",
              pid=dat$idstud, X=data.frame("rater"=raters),
              regression="", name="mod31")

#***
# Model 32: Rater model 'item+item*step+rater+item*rater'
mod32 &lt;- sirt::R2conquest(dat=dat[,items], path.conquest=path.conquest,
              model="item+item*step+rater+item*rater",
              pid=dat$idstud, X=data.frame("rater"=raters),
              regression="", name="mod32")

## End(Not run)
</code></pre>

<hr>
<h2 id='R2noharm'>
Estimation of a NOHARM Analysis from within <span class="rlang"><b>R</b></span>
</h2><span id='topic+R2noharm'></span><span id='topic+summary.R2noharm'></span>

<h3>Description</h3>

<p>This function enables the estimation of a NOHARM analysis
(Fraser &amp; McDonald, 1988; McDonald, 1982a, 1982b, 1997) from within <span class="rlang"><b>R</b></span>.
NOHARM estimates a compensatory multidimensional factor analysis for dichotomous
response data. Arguments of this function strictly follow
the rules of the NOHARM manual (see Fraser &amp; McDonald, 2012; Lee &amp; Lee, 2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>R2noharm(dat=NULL,pm=NULL, n=NULL, model.type, weights=NULL, dimensions=NULL,
      guesses=NULL, noharm.path, F.pattern=NULL, F.init=NULL,
      P.pattern=NULL, P.init=NULL, digits.pm=4, writename=NULL,
      display.fit=5,  dec=".", display=TRUE)

## S3 method for class 'R2noharm'
summary(object, logfile=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="R2noharm_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of item responses for <code class="reqn">N</code> subjects
and <code class="reqn">I</code> items
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_pm">pm</code></td>
<td>
<p>A matrix or a vector containing product-moment correlations</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_n">n</code></td>
<td>
<p>Sample size. This value must only be included if <code>pm</code> is
provided.</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_model.type">model.type</code></td>
<td>

<p>Can be <code>"EFA"</code> (exploratory factor analysis) or <code>"CFA"</code>
(confirmatory factor analysis).
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_weights">weights</code></td>
<td>
<p>Optional vector of student weights</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_dimensions">dimensions</code></td>
<td>

<p>Number of dimensions in exploratory factor analysis
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_guesses">guesses</code></td>
<td>

<p>An optional vector of fixed guessing parameters of length <code class="reqn">I</code>.
In case of the default <code>NULL</code>, all guessing parameters are set
to zero.
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_noharm.path">noharm.path</code></td>
<td>

<p>Local path where the NOHARM 4 command line 64-bit version is located.
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_f.pattern">F.pattern</code></td>
<td>

<p>Pattern matrix for <code class="reqn">F</code> (<code class="reqn">I \times D</code>)
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_f.init">F.init</code></td>
<td>

<p>Initial matrix for <code class="reqn">F</code> (<code class="reqn">I \times D</code>)
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_p.pattern">P.pattern</code></td>
<td>

<p>Pattern matrix for <code class="reqn">P</code> (<code class="reqn">D \times D</code>)
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_p.init">P.init</code></td>
<td>

<p>Initial matrix for <code class="reqn">P</code> (<code class="reqn">D \times D</code>)
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_digits.pm">digits.pm</code></td>
<td>

<p>Number of digits after decimal separator which are used for
estimation
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_writename">writename</code></td>
<td>

<p>Name for NOHARM input and output files
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_display.fit">display.fit</code></td>
<td>

<p>How many digits (after decimal separator) should be used
for printing results on the <span class="rlang"><b>R</b></span> console?
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_dec">dec</code></td>
<td>

<p>Decimal separator (<code>"."</code> or <code>","</code>)
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_display">display</code></td>
<td>

<p>Display output?
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_object">object</code></td>
<td>

<p>Object of class <code>R2noharm</code>
</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_logfile">logfile</code></td>
<td>
<p>File name if the summary should be sunk into a file</p>
</td></tr>
<tr><td><code id="R2noharm_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NOHARM estimates a multidimensional compensatory
item response model with the probit link function <code class="reqn">\Phi</code>.
For item responses <code class="reqn">X_{pi}</code> of person <code class="reqn">p</code> on
item <code class="reqn">i</code> the model equation is defined as
</p>
<p style="text-align: center;"><code class="reqn">P( X_{pi}=1 | \bold{\theta}_p )=c_i + ( 1 - c_i )
\Phi( f_{i0} + f_{i1} \theta_{p1} + ... +
f_{iD} \theta_{pD} ) </code>
</p>

<p>where <code class="reqn">F=(f_{id})</code> is a loading matrix and <code class="reqn">P</code>
the covariance matrix of <code class="reqn">\bold{\theta}_p</code>. The guessing
parameters <code class="reqn">c_i</code> must be provided as fixed values.
</p>
<p>For the definition of <code class="reqn">F</code> and <code class="reqn">P</code> matrices, please
consult the NOHARM manual.
</p>
<p>This function needs the 64-bit command line version which can be downloaded
from (some links may be broken in the meantime) <br /> <br />
http://noharm.niagararesearch.ca/nh4cldl.html <br />
https://noharm.software.informer.com/4.0/   <br />
https://cehs.unl.edu/edpsych/software-urls-and-other-interesting-sites/ <br />
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>tanaka</code></td>
<td>
<p>Tanaka index</p>
</td></tr>
<tr><td><code>rmsr</code></td>
<td>
<p>RMSR statistic</p>
</td></tr>
<tr><td><code>N.itempair</code></td>
<td>
<p>Sample sizes of pairwise item observations</p>
</td></tr>
<tr><td><code>pm</code></td>
<td>
<p>Product moment matrix</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>Used student weights</p>
</td></tr>
<tr><td><code>guesses</code></td>
<td>
<p>Fixed guessing parameters</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Residual covariance matrix</p>
</td></tr>
<tr><td><code>final.constants</code></td>
<td>
<p>Vector of final constants</p>
</td></tr>
<tr><td><code>thresholds</code></td>
<td>
<p>Threshold parameters</p>
</td></tr>
<tr><td><code>uniquenesses</code></td>
<td>
<p>Item uniquenesses</p>
</td></tr>
<tr><td><code>loadings.theta</code></td>
<td>
<p>Matrix of loadings in theta parametrization
(common factor parametrization)</p>
</td></tr>
<tr><td><code>factor.cor</code></td>
<td>
<p>Covariance matrix of factors</p>
</td></tr>
<tr><td><code>difficulties</code></td>
<td>
<p>Item difficulties (for unidimensional models)</p>
</td></tr>
<tr><td><code>discriminations</code></td>
<td>
<p>Item discriminations (for unidimensional models)</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>Loading matrix (latent trait parametrization)</p>
</td></tr>
<tr><td><code>model.type</code></td>
<td>
<p>Used model type</p>
</td></tr>
<tr><td><code>Nobs</code></td>
<td>
<p>Number of observations</p>
</td></tr>
<tr><td><code>Nitems</code></td>
<td>
<p>Number of items</p>
</td></tr>
<tr><td><code>modtype</code></td>
<td>
<p>Model type according to the NOHARM specification (see NOHARM manual)</p>
</td></tr>
<tr><td><code>F.init</code></td>
<td>
<p>Initial loading matrix for <code class="reqn">F</code></p>
</td></tr>
<tr><td><code>F.pattern</code></td>
<td>
<p>Pattern loading matrix for <code class="reqn">F</code></p>
</td></tr>
<tr><td><code>P.init</code></td>
<td>
<p>Initial covariance matrix for <code class="reqn">P</code></p>
</td></tr>
<tr><td><code>P.pattern</code></td>
<td>
<p>Pattern covariance matrix for <code class="reqn">P</code></p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Original data frame</p>
</td></tr>
<tr><td><code>systime</code></td>
<td>
<p>System time</p>
</td></tr>
<tr><td><code>noharm.path</code></td>
<td>
<p>Used NOHARM directory</p>
</td></tr>
<tr><td><code>digits.pm</code></td>
<td>
<p>Number of digits in product moment matrix</p>
</td></tr>
<tr><td><code>dec</code></td>
<td>
<p>Used decimal symbol</p>
</td></tr>
<tr><td><code>display.fit</code></td>
<td>
<p>Number of digits for fit display</p>
</td></tr>
<tr><td><code>dimensions</code></td>
<td>
<p>Number of dimensions</p>
</td></tr>
<tr><td><code>chisquare</code></td>
<td>
<p>Statistic <code class="reqn">\chi^2</code></p>
</td></tr>
<tr><td><code>Nestpars</code></td>
<td>
<p>Number of estimated parameters</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Degrees of freedom</p>
</td></tr>
<tr><td><code>chisquare_df</code></td>
<td>
<p>Ratio <code class="reqn">\chi^2 / df</code></p>
</td></tr>
<tr><td><code>rmsea</code></td>
<td>
<p>RMSEA statistic</p>
</td></tr>
<tr><td><code>p.chisquare</code></td>
<td>
<p>Significance for <code class="reqn">\chi^2</code> statistic</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Possible errors often occur due to wrong <code>dec</code>
specification.
</p>


<h3>References</h3>

<p>Fraser, C., &amp; McDonald, R. P. (1988). NOHARM: Least squares item factor analysis.
<em>Multivariate Behavioral Research, 23</em>, 267-269.
https://doi.org/10.1207/s15327906mbr2302_9
</p>
<p>Fraser, C., &amp; McDonald, R. P. (2012). <em>NOHARM 4 Manual</em>. <br />
http://noharm.niagararesearch.ca/nh4man/nhman.html.
</p>
<p>Lee, J. J., &amp; Lee, M. K. (2016). An overview of the normal ogive harmonic analysis
robust method (NOHARM) approach to item response theory.
<em>Tutorials in Quantitative Methods for Psychology, 12</em>(1), 1-8.
https://doi.org/10.20982/tqmp.12.1.p001
</p>
<p>McDonald, R. P. (1982a). Linear versus nonlinear models in item response theory.
<em>Applied Psychological Measurement, 6</em>(4), 379-396.
<a href="https://doi.org/10.1177/014662168200600402">doi:10.1177/014662168200600402</a>
</p>
<p>McDonald, R. P. (1982b). <em>Unidimensional and multidimensional models for
item response theory</em>. I.R.T., C.A.T. conference, Minneapolis, 1982, Proceedings.
</p>
<p>McDonald, R. P. (1997). Normal-ogive multidimensional model.
In W. van der Linden &amp; R. K. Hambleton (1997):
<em>Handbook of modern item response theory</em> (pp. 257-269).
New York: Springer. http://dx.doi.org/10.1007/978-1-4757-2691-6
</p>


<h3>See Also</h3>

<p>For estimating standard errors see <code><a href="#topic+R2noharm.jackknife">R2noharm.jackknife</a></code>.
</p>
<p>For EAP person parameter estimates see <code><a href="#topic+R2noharm.EAP">R2noharm.EAP</a></code>.
</p>
<p>For an <span class="rlang"><b>R</b></span> implementation of the NOHARM model see <code><a href="#topic+noharm.sirt">noharm.sirt</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Data data.noharm18 with 18 items
#############################################################################

# load data
data(data.noharm18)
dat &lt;- data.noharm18
I &lt;- ncol(dat) # number of items

# locate noharm.path
noharm.path &lt;- "c:/NOHARM"

#****************************************
# Model 1: 1-dimensional Rasch model (1-PL model)

# estimate one factor variance
P.pattern &lt;- matrix( 1, ncol=1, nrow=1 )
P.init &lt;- P.pattern
# fix all entries in the loading matrix to 1
F.pattern &lt;- matrix( 0, nrow=I, ncol=1 )
F.init &lt;- 1 + 0*F.pattern       #
# estimate model
mod &lt;- sirt::R2noharm( dat=dat, model.type="CFA",
           F.pattern=F.pattern, F.init=F.init, P.pattern=P.pattern,
           P.init=P.init, writename="ex1__1dim_1pl",
       noharm.path=noharm.path, dec="," )
# summary
summary(mod, logfile="ex1__1dim_1pl__SUMMARY")
# jackknife
jmod &lt;- sirt::R2noharm.jackknife( mod, jackunits=20 )
summary(jmod, logfile="ex1__1dim_1pl__JACKKNIFE")
# compute factor scores (EAPs)
emod &lt;- sirt::R2noharm.EAP(mod)

#*****-----
# Model 1b: Include student weights in estimation
N &lt;- nrow(dat)
weights &lt;- stats::runif( N, 1, 5 )
mod1b &lt;- sirt::R2noharm( dat=dat, model.type="CFA",  weights=weights,
            F.pattern=F.pattern, F.init=F.init, P.pattern=P.pattern,
            P.init=P.init, writename="ex1__1dim_1pl_w",
            noharm.path=noharm.path, dec="," )
summary(mod1b)

#****************************************
# Model 2: 1-dimensional 2PL Model

# set trait variance equal to 1
P.pattern &lt;- matrix( 0, ncol=1, nrow=1 )
P.init &lt;- 1+0*P.pattern
# loading matrix
F.pattern &lt;- matrix( 1, nrow=I, ncol=1 )
F.init &lt;- 1 + 0*F.pattern

mod &lt;- sirt::R2noharm( dat=dat, model.type="CFA",
            F.pattern=F.pattern, F.init=F.init, P.pattern=P.pattern,
            P.init=P.init, writename="ex2__1dim_2pl",
            noharm.path=noharm.path, dec="," )

summary(mod)
jmod &lt;- sirt::R2noharm.jackknife( mod, jackunits=20 )
summary(jmod)

#****************************************
# Model 3: 1-dimensional 3PL Model with fixed guessing parameters

# set trait variance equal to 1
P.pattern &lt;- matrix( 0, ncol=1, nrow=1 )
P.init &lt;- 1+0*P.pattern
# loading matrix
F.pattern &lt;- matrix( 1, nrow=I, ncol=1 )
F.init &lt;- 1 + 0*F.pattern       #
# fix guessing parameters equal to .2 (for all items)
guesses &lt;- rep( .1, I )

mod &lt;- sirt::R2noharm( dat=dat, model.type="CFA",
          F.pattern=F.pattern, F.init=F.init, P.pattern=P.pattern,
          P.init=P.init, guesses=guesses,
          writename="ex3__1dim_3pl", noharm.path=noharm.path, dec=","  )
summary(mod)
jmod &lt;- sirt::R2noharm.jackknife( mod, jackunits=20 )
summary(jmod)

#****************************************
# Model 4: 3-dimensional Rasch model

# estimate one factor variance
P.pattern &lt;- matrix( 1, ncol=3, nrow=3 )
P.init &lt;- .8*P.pattern
diag(P.init) &lt;- 1
# fix all entries in the loading matrix to 1
F.init &lt;- F.pattern &lt;- matrix( 0, nrow=I, ncol=3 )
F.init[1:6,1] &lt;- 1
F.init[7:12,2] &lt;- 1
F.init[13:18,3] &lt;- 1

mod &lt;- sirt::R2noharm( dat=dat, model.type="CFA",
          F.pattern=F.pattern, F.init=F.init, P.pattern=P.pattern,
          P.init=P.init, writename="ex4__3dim_1pl",
          noharm.path=noharm.path, dec="," )
# write output from R console in a file
summary(mod, logfile="ex4__3dim_1pl__SUMMARY.Rout")

jmod &lt;- sirt::R2noharm.jackknife( mod, jackunits=20 )
summary(jmod)

# extract factor scores
emod &lt;- sirt::R2noharm.EAP(mod)

#****************************************
# Model 5: 3-dimensional 2PL model

# estimate one factor variance
P.pattern &lt;- matrix( 1, ncol=3, nrow=3 )
P.init &lt;- .8*P.pattern
diag(P.init) &lt;- 0
# fix all entries in the loading matrix to 1
F.pattern &lt;- matrix( 0, nrow=I, ncol=3 )
F.pattern[1:6,1] &lt;- 1
F.pattern[7:12,2] &lt;- 1
F.pattern[13:18,3] &lt;- 1
F.init &lt;- F.pattern

mod &lt;- sirt::R2noharm( dat=dat, model.type="CFA",
          F.pattern=F.pattern, F.init=F.init, P.pattern=P.pattern,
          P.init=P.init, writename="ex5__3dim_2pl",
          noharm.path=noharm.path, dec="," )
summary(mod)
# use 50 jackknife units with 4 persons within a unit
jmod &lt;- sirt::R2noharm.jackknife( mod, jackunits=seq( 1:50, each=4 ) )
summary(jmod)

#****************************************
# Model 6: Exploratory Factor Analysis with 3 factors

mod &lt;- sirt::R2noharm( dat=dat, model.type="EFA",  dimensions=3,
           writename="ex6__3dim_efa", noharm.path=noharm.path,dec=",")
summary(mod)

jmod &lt;- sirt::R2noharm.jackknife( mod, jackunits=20 )

#############################################################################
# EXAMPLE 2: NOHARM manual Example A
#############################################################################

# See NOHARM manual: http://noharm.niagararesearch.ca/nh4man/nhman.html
# The following text and data is copied from this manual.
#
# In the first example, we demonstrate how to prepare the input for a 2-dimensional
# model using exploratory analysis. Data from a 9 item test were collected from
# 200 students and the 9x9 product-moment matrix of the responses was computed.
#
# Our hypothesis is for a 2-dimensional model with no guessing,
# i.e., all guesses are equal to zero. However, because we are unsure of any
# particular pattern for matrix F, we wish to prescribe an exploratory analysis, i.e.,
# set EX=1. Also, we will content ourselves with letting the program supply all
# initial values.
#
# We would like both the sample product-moment matrix and the residual matrix to
# be included in the output.

# scan product-moment matrix copied from the NOHARM manual
pm &lt;- scan()
     0.8967
     0.2278 0.2356
     0.6857 0.2061 0.7459
     0.8146 0.2310 0.6873 0.8905
     0.4505 0.1147 0.3729 0.4443 0.5000
     0.7860 0.2080 0.6542 0.7791 0.4624 0.8723
     0.2614 0.0612 0.2140 0.2554 0.1914 0.2800 0.2907
     0.7549 0.1878 0.6236 0.7465 0.4505 0.7590 0.2756 0.8442
     0.6191 0.1588 0.5131 0.6116 0.3845 0.6302 0.2454 0.6129 0.6879

ex2 &lt;- sirt::R2noharm( pm=pm, n=200, model.type="EFA", dimensions=2,
         noharm.path=noharm.path, writename="ex2_noharmExA", dec=",")
summary(ex2)

#############################################################################
# EXAMPLE 3: NOHARM manual Example B
#############################################################################

# See NOHARM manual: http://noharm.niagararesearch.ca/nh4man/nhman.html
# The following text and data is copied from this manual.

# Suppose we have the product-moment matrix of data from 125 students on 9 items.
# Our hypothesis is for 2 dimensions with simple structure. In this case,
# items 1 to 5 have coefficients of theta which are to be estimated for one
# latent trait but are to be fixed at zero for the other one.
# For the latent trait for which items 1 to 5 have zero coefficients,
# items 6 to 9 have coefficients which are to be estimated. For the other
# latent trait, items 6 to 9 will have zero coefficients.
# We also wish to estimate the correlation between the latent traits,
# so we prescribe P as a 2x2 correlation matrix.
#
# Our hypothesis prescribes that there was no guessing involved, i.e.,
# all guesses are equal to zero. For demonstration purposes,
# let us not have the program print out the sample product-moment matrix.
# Also let us not supply any starting values but, rather, use the defaults
# supplied by the program.

pm &lt;- scan()
    0.930
    0.762 0.797
    0.541 0.496 0.560
    0.352 0.321 0.261 0.366
    0.205 0.181 0.149 0.110 0.214
    0.858 0.747 0.521 0.336 0.203 0.918
    0.773 0.667 0.465 0.308 0.184 0.775 0.820
    0.547 0.474 0.347 0.233 0.132 0.563 0.524 0.579
    0.329 0.290 0.190 0.140 0.087 0.333 0.308 0.252 0.348

I &lt;- 9    # number of items
# define loading matrix
F.pattern &lt;- matrix(0,I,2)
F.pattern[1:5,1] &lt;- 1
F.pattern[6:9,2] &lt;- 1
F.init &lt;- F.pattern
# define covariance matrix
P.pattern &lt;- matrix(1,2,2)
diag(P.pattern) &lt;- 0
P.init &lt;- 1+P.pattern

ex3 &lt;- sirt::R2noharm( pm=pm, n=125,, model.type="CFA",
           F.pattern=F.pattern, F.init=F.init, P.pattern=P.pattern,
           P.init=P.init, writename="ex3_noharmExB",
           noharm.path=noharm.path, dec="," )
summary(ex3)

#############################################################################
# EXAMPLE 4: NOHARM manual Example C
#############################################################################

data(data.noharmExC)
# See NOHARM manual: http://noharm.niagararesearch.ca/nh4man/nhman.html
# The following text and data is copied from this manual.

# In this example, suppose that from 300 respondents we have item
# responses scored dichotomously, 1 or 0, for 8 items.
#
# Our hypothesis is for a unidimensional model where all eight items
# have coefficients of theta which are to be estimated.
# Suppose that since the items were multiple choice with 5 options each,
# we set the fixed guesses all to 0.2 (not necessarily good reasoning!)
#
# Let's supply initial values for the coefficients of theta (F matrix)
# as .75 for items 1 to 4 and .6 for items 5 to 8.

I &lt;- 8
guesses &lt;- rep(.2,I)
F.pattern &lt;- matrix(1,I,1)
F.init &lt;- F.pattern
F.init[1:4,1] &lt;- .75
F.init[5:8,1] &lt;- .6
P.pattern &lt;- matrix(0,1,1)
P.init &lt;- 1 + 0 * P.pattern

ex4 &lt;- sirt::R2noharm( dat=data.noharmExC,, model.type="CFA",
           guesses=guesses, F.pattern=F.pattern, F.init=F.init,
           P.pattern=P.pattern, P.init=P.init, writename="ex3_noharmExC",
           noharm.path=noharm.path, dec="," )
summary(ex4)

# modify F pattern matrix
# f11=f51 (since both have equal pattern values of 2),
# f21=f61 (since both have equal pattern values of 3),
# f31=f71 (since both have equal pattern values of 4),
# f41=f81 (since both have equal pattern values of 5).
F.pattern[ c(1,5) ] &lt;- 2
F.pattern[ c(2,6) ] &lt;- 3
F.pattern[ c(3,7) ] &lt;- 4
F.pattern[ c(4,8) ] &lt;- 5
F.init &lt;- .5+0*F.init

ex4a &lt;- sirt::R2noharm( dat=data.noharmExC,, model.type="CFA",
           guesses=guesses, F.pattern=F.pattern, F.init=F.init,
           P.pattern=P.pattern, P.init=P.init, writename="ex3_noharmExC1",
           noharm.path=noharm.path, dec="," )
summary(ex4a)

## End(Not run)
</code></pre>

<hr>
<h2 id='R2noharm.EAP'>
EAP Factor Score Estimation
</h2><span id='topic+R2noharm.EAP'></span>

<h3>Description</h3>

<p>This function performs EAP factor score estimation of
an item response model estimated with NOHARM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>R2noharm.EAP(noharmobj, theta.k=seq(-6, 6, len=21), print.output=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="R2noharm.EAP_+3A_noharmobj">noharmobj</code></td>
<td>

<p>Object of class <code>R2noharm</code> or <code>noharm.sirt</code>
</p>
</td></tr>
<tr><td><code id="R2noharm.EAP_+3A_theta.k">theta.k</code></td>
<td>

<p>Vector of discretized theta values on which the posterior
is evaluated. This vector applies to all dimensions.
</p>
</td></tr>
<tr><td><code id="R2noharm.EAP_+3A_print.output">print.output</code></td>
<td>
<p>An optional logical indicating whether output should
be displayed at the console</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>person</code></td>
<td>
<p>Data frame of person parameter EAP estimates and
their corresponding standard errors
</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>Grid of multidimensional theta values where the posterior is
evaluated.</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>Individual posterior distribution evaluated at <code>theta</code></p>
</td></tr>
<tr><td><code>like</code></td>
<td>
<p>Individual likelihood</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliabilities of all dimensions</p>
</td></tr>
<tr><td><code>probs</code></td>
<td>
<p>Item response probabilities evaluated at <code>theta</code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p>For examples see <code><a href="#topic+R2noharm">R2noharm</a></code> and <code><a href="#topic+noharm.sirt">noharm.sirt</a></code>.
</p>

<hr>
<h2 id='R2noharm.jackknife'>
Jackknife Estimation of NOHARM Analysis
</h2><span id='topic+R2noharm.jackknife'></span><span id='topic+summary.R2noharm.jackknife'></span>

<h3>Description</h3>

<p>This function performs a jackknife estimation of NOHARM analysis
to get standard errors based on a replication method (see Christoffersson, 1977).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>R2noharm.jackknife(object, jackunits=NULL)

## S3 method for class 'R2noharm.jackknife'
summary(object, logfile=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="R2noharm.jackknife_+3A_object">object</code></td>
<td>

<p>Object of class <code>R2noharm</code>
</p>
</td></tr>
<tr><td><code id="R2noharm.jackknife_+3A_jackunits">jackunits</code></td>
<td>

<p>A vector of integers or a number. If it is a number, then it refers
to the number of jackknife units. If it is a vector of integers, then this vector
defines the allocation of persons jackknife units. Integers corresponds to
row indexes in the data set.
</p>
</td></tr>
<tr><td><code id="R2noharm.jackknife_+3A_logfile">logfile</code></td>
<td>
<p>File name if the summary should be sunk into a file</p>
</td></tr>
<tr><td><code id="R2noharm.jackknife_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of lists with following entries:
</p>
<table>
<tr><td><code>partable</code></td>
<td>

<p>Data frame with parameters
</p>
</td></tr>
<tr><td><code>se.pars</code></td>
<td>
<p>List of estimated standard errors for all parameter estimates:
<code>tanaka.stat</code>, <code>rmsr.stat</code>, <code>rmsea.stat</code>,
<code>chisquare_df.stat</code>, <code>thresholds.stat</code>, <code>final.constants.stat</code>,
<code>uniquenesses.stat</code>, <code>factor.cor.stat</code>, <code>loadings.stat</code>,
<code>loadings.theta.stat</code>
</p>
</td></tr>
<tr><td><code>jackknife.pars</code></td>
<td>
<p>List with obtained results by jackknifing for all parameters:
<code>j.tanaka</code>, <code>j.rmsr</code>, <code>rmsea</code>, <code>chisquare_df</code>,
<code>j.pm</code>, <code>j.thresholds</code>, <code>j.factor.cor</code>,
<code>j.loadings</code>, <code>j.loadings.theta</code>
</p>
</td></tr>
<tr><td><code>u.jacknunits</code></td>
<td>
<p>Unique jackknife elements</p>
</td></tr>
</table>


<h3>References</h3>

<p>Christoffersson, A. (1977). Two-step weighted least squares factor analysis of
dichotomized variables. <em>Psychometrika, 42</em>, 433-438.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+R2noharm">R2noharm</a></code>
</p>

<hr>
<h2 id='rasch.copula2'>
Multidimensional IRT Copula Model
</h2><span id='topic+rasch.copula2'></span><span id='topic+rasch.copula3'></span><span id='topic+summary.rasch.copula2'></span><span id='topic+summary.rasch.copula3'></span><span id='topic+anova.rasch.copula2'></span><span id='topic+anova.rasch.copula3'></span><span id='topic+logLik.rasch.copula2'></span><span id='topic+logLik.rasch.copula3'></span><span id='topic+IRT.likelihood.rasch.copula2'></span><span id='topic+IRT.likelihood.rasch.copula3'></span><span id='topic+IRT.posterior.rasch.copula2'></span><span id='topic+IRT.posterior.rasch.copula3'></span>

<h3>Description</h3>

<p>This function handles local dependence by specifying
copulas for residuals in multidimensional item response
models for dichotomous item responses
(Braeken, 2011; Braeken, Tuerlinckx &amp; de Boeck, 2007;
Schroeders, Robitzsch &amp; Schipolowski, 2014).
Estimation is allowed for item difficulties, item slopes and
a generalized logistic link function (Stukel, 1988).
</p>
<p>The function <code>rasch.copula3</code> allows the estimation of multidimensional
models while <code>rasch.copula2</code> only handles unidimensional models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.copula2(dat, itemcluster, weights=NULL, copula.type="bound.mixt",
    progress=TRUE, mmliter=1000, delta=NULL,
    theta.k=seq(-4, 4, len=21), alpha1=0, alpha2=0,
    numdiff.parm=1e-06,  est.b=seq(1, ncol(dat)),
    est.a=rep(1, ncol(dat)), est.delta=NULL, b.init=NULL, a.init=NULL,
    est.alpha=FALSE, glob.conv=0.0001, alpha.conv=1e-04, conv1=0.001,
    dev.crit=.2, increment.factor=1.01)

rasch.copula3(dat, itemcluster, dims=NULL, copula.type="bound.mixt",
    progress=TRUE, mmliter=1000, delta=NULL,
    theta.k=seq(-4, 4, len=21), alpha1=0, alpha2=0,
    numdiff.parm=1e-06,  est.b=seq(1, ncol(dat)),
    est.a=rep(1, ncol(dat)), est.delta=NULL, b.init=NULL, a.init=NULL,
    est.alpha=FALSE, glob.conv=0.0001, alpha.conv=1e-04, conv1=0.001,
    dev.crit=.2, rho.init=.5, increment.factor=1.01)

## S3 method for class 'rasch.copula2'
summary(object, file=NULL, digits=3, ...)
## S3 method for class 'rasch.copula3'
summary(object, file=NULL, digits=3, ...)

## S3 method for class 'rasch.copula2'
anova(object,...)
## S3 method for class 'rasch.copula3'
anova(object,...)

## S3 method for class 'rasch.copula2'
logLik(object,...)
## S3 method for class 'rasch.copula3'
logLik(object,...)

## S3 method for class 'rasch.copula2'
IRT.likelihood(object,...)
## S3 method for class 'rasch.copula3'
IRT.likelihood(object,...)

## S3 method for class 'rasch.copula2'
IRT.posterior(object,...)
## S3 method for class 'rasch.copula3'
IRT.posterior(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.copula2_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame. Cases with only missing responses
are removed from the analysis.
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_itemcluster">itemcluster</code></td>
<td>

<p>An integer vector of length <code class="reqn">I</code> (number of items). Items with
the same integers define a joint item cluster of (positively) locally
dependent items. Values of zero indicate that the corresponding item
is not included in any item cluster of dependent responses.
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_weights">weights</code></td>
<td>
<p>Optional vector of sampling weights</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_dims">dims</code></td>
<td>

<p>A vector indicating to which dimension an item is allocated.
The default is that all items load on the first dimension.
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_copula.type">copula.type</code></td>
<td>

<p>A character or a vector containing one of the following copula
types: <code>bound.mixt</code> (boundary mixture copula),
<code>cook.johnson</code> (Cook-Johnson copula) or <code>frank</code> (Frank copula)
(see Braeken, 2011).
The vector <code>copula.type</code> must match the number of different
itemclusters. For every itemcluster, a different copula type
may be specified (see Examples).
</p>
</td></tr>  
<tr><td><code id="rasch.copula2_+3A_progress">progress</code></td>
<td>

<p>Print progress? Default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_mmliter">mmliter</code></td>
<td>

<p>Maximum number of iterations.
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_delta">delta</code></td>
<td>

<p>An optional vector of starting values for the dependency parameter <code>delta</code>.
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_theta.k">theta.k</code></td>
<td>

<p>Discretized trait distribution
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_alpha1">alpha1</code></td>
<td>

<p><code>alpha1</code> parameter in the generalized logistic item response model
(Stukel, 1988). The default is 0 which leads together with <code>alpha2=0</code>
to the logistic link function.
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_alpha2">alpha2</code></td>
<td>

<p><code>alpha2</code> parameter in the generalized logistic item response model
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_numdiff.parm">numdiff.parm</code></td>
<td>

<p>Parameter for numerical differentiation
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_est.b">est.b</code></td>
<td>

<p>Integer vector of item difficulties to be estimated
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_est.a">est.a</code></td>
<td>

<p>Integer vector of item discriminations to be estimated
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_est.delta">est.delta</code></td>
<td>

<p>Integer vector of length <code>length(itemcluster)</code>. Nonzero integers
correspond to <code>delta</code> parameters which are estimated.
Equal integers indicate parameter equality constraints.
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_b.init">b.init</code></td>
<td>

<p>Initial <code class="reqn">b</code> parameters
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_a.init">a.init</code></td>
<td>

<p>Initial <code class="reqn">a</code> parameters
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_est.alpha">est.alpha</code></td>
<td>

<p>Should both alpha parameters be estimated? Default is <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_glob.conv">glob.conv</code></td>
<td>

<p>Convergence criterion for all parameters
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_alpha.conv">alpha.conv</code></td>
<td>

<p>Maximal change in alpha parameters for convergence
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_conv1">conv1</code></td>
<td>

<p>Maximal change in item parameters for convergence
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_dev.crit">dev.crit</code></td>
<td>

<p>Maximal change in the deviance. Default is <code>.2</code>.
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_rho.init">rho.init</code></td>
<td>
<p>Initial value for off-diagonal elements in correlation matrix</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_increment.factor">increment.factor</code></td>
<td>
<p>A numeric value larger than one which controls the
size of increments in iterations. To stabilize convergence,
choose values 1.05 or 1.1 in some situations.</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_object">object</code></td>
<td>

<p>Object of class <code>rasch.copula2</code> or <code>rasch.copula3</code>
</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_file">file</code></td>
<td>
<p>Optional file name for <code>summary</code> output</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_digits">digits</code></td>
<td>
<p>Number of digits after decimal in <code>summary</code> output</p>
</td></tr>
<tr><td><code id="rasch.copula2_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>N.itemclusters</code></td>
<td>
<p>Number of item clusters</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Estimated item parameters</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>dev</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>Estimated dependency parameters <code class="reqn">\delta</code></p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>Estimated item difficulties</p>
</td></tr>
<tr><td><code>a</code></td>
<td>
<p>Estimated item slopes</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Mean</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Standard deviation</p>
</td></tr>
<tr><td><code>alpha1</code></td>
<td>
<p>Parameter <code class="reqn">\alpha_1</code> in the generalized item response model</p>
</td></tr>
<tr><td><code>alpha2</code></td>
<td>
<p>Parameter <code class="reqn">\alpha_2</code> in the generalized item response model</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria</p>
</td></tr>
<tr><td><code>theta.k</code></td>
<td>
<p>Discretized ability distribution</p>
</td></tr>
<tr><td><code>pi.k</code></td>
<td>
<p>Fixed <code class="reqn">\theta</code> distribution</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>pattern</code></td>
<td>
<p>Item response patterns with frequencies and posterior
distribution</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with person parameters</p>
</td></tr>
<tr><td><code>datalist</code></td>
<td>
<p>List of generated data frames during estimation</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>Reliability of the EAP</p>
</td></tr>
<tr><td><code>copula.type</code></td>
<td>
<p>Type of copula</p>
</td></tr>
<tr><td><code>summary.delta</code></td>
<td>
<p>Summary for estimated <code class="reqn">\delta</code> parameters</p>
</td></tr>
<tr><td><code>f.qk.yi</code></td>
<td>
<p>Individual posterior</p>
</td></tr>
<tr><td><code>f.yi.qk</code></td>
<td>
<p>Individual likelihood</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Braeken, J. (2011). A boundary mixture approach to violations of conditional
independence. <em>Psychometrika, 76</em>(1), 57-76.
<a href="https://doi.org/10.1007/s11336-010-9190-4">doi:10.1007/s11336-010-9190-4</a>
</p>
<p>Braeken, J., Kuppens, P., De Boeck, P., &amp; Tuerlinckx, F. (2013).
Contextualized personality questionnaires: A case for copulas in structural
equation models for categorical data. <em>Multivariate Behavioral Research, 48</em>(6),
845-870.
<a href="https://doi.org/10.1080/00273171.2013.827965">doi:10.1080/00273171.2013.827965</a>
</p>
<p>Braeken, J., &amp; Tuerlinckx, F. (2009). Investigating latent constructs with item
response models: A MATLAB IRTm toolbox.
<em>Behavior Research Methods, 41</em>(4), 1127-1137.
</p>
<p>Braeken, J., Tuerlinckx, F., &amp; De Boeck, P. (2007).
Copula functions for residual dependency. <em>Psychometrika, 72</em>(3), 393-411.
<a href="https://doi.org/10.1007/s11336-007-9005-4">doi:10.1007/s11336-007-9005-4</a>
</p>
<p>Schroeders, U., Robitzsch, A., &amp; Schipolowski, S. (2014). A comparison of different
psychometric approaches to modeling testlet structures: An example with C-tests.
<em>Journal of Educational Measurement, 51</em>(4), 400-418.
<a href="https://doi.org/10.1111/jedm.12054">doi:10.1111/jedm.12054</a>
</p>
<p>Stukel, T. A. (1988). Generalized logistic models.
<em>Journal of the American Statistical Association, 83</em>(402), 426-431.
<a href="https://doi.org/10.1080/01621459.1988.10478613">doi:10.1080/01621459.1988.10478613</a>
</p>


<h3>See Also</h3>

<p>For a summary see <code><a href="#topic+summary.rasch.copula2">summary.rasch.copula2</a></code>.
</p>
<p>For simulating locally dependent item responses see <code><a href="#topic+sim.rasch.dep">sim.rasch.dep</a></code>.
</p>
<p>Person parameters estimates are obtained by <code><a href="#topic+person.parameter.rasch.copula">person.parameter.rasch.copula</a></code>.
</p>
<p>See <code><a href="#topic+rasch.mml2">rasch.mml2</a></code> for the generalized logistic link function.
</p>
<p>See also Braeken and Tuerlinckx (2009) for alternative (and more expanded)
copula models implemented in the MATLAB software. See
<a href="https://ppw.kuleuven.be/okp/software/irtm/">https://ppw.kuleuven.be/okp/software/irtm/</a>.
</p>
<p>See Braeken, Kuppens, De Boeck and Tuerlinckx (2013) for an extension of the
copula modeling approach to polytomous data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Reading Data
#############################################################################

data(data.read)
dat &lt;- data.read

# define item clusters
itemcluster &lt;- rep( 1:3, each=4 )

# estimate Copula model
mod1 &lt;- sirt::rasch.copula2( dat=dat, itemcluster=itemcluster)

## Not run: 
# estimate Rasch model
mod2 &lt;- sirt::rasch.copula2( dat=dat, itemcluster=itemcluster,
        delta=rep(0,3), est.delta=rep(0,3) )
summary(mod1)
summary(mod2)

# estimate copula 2PL model
I &lt;- ncol(dat)
mod3 &lt;- sirt::rasch.copula2( dat=dat, itemcluster=itemcluster, est.a=1:I,
                increment.factor=1.05)
summary(mod3)

#############################################################################
# EXAMPLE 2: 11 items nested within 2 item clusters (testlets)
#    with 2 resp. 3 dependent and 6 independent items
#############################################################################

set.seed(5698)
I &lt;- 11                             # number of items
n &lt;- 3000                           # number of persons
b &lt;- seq(-2,2, len=I)               # item difficulties
theta &lt;- stats::rnorm( n, sd=1 ) # person abilities
# define item clusters
itemcluster &lt;- rep(0,I)
itemcluster[ c(3,5 )] &lt;- 1
itemcluster[c(2,4,9)] &lt;- 2
# residual correlations
rho &lt;- c( .7, .5 )

# simulate data
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("I", seq(1,ncol(dat)), sep="")

# estimate Rasch copula model
mod1 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster )
summary(mod1)

# both item clusters have Cook-Johnson copula as dependency
mod1c &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster,
            copula.type="cook.johnson")
summary(mod1c)

# first item boundary mixture and second item Cook-Johnson copula
mod1d &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster,
            copula.type=c( "bound.mixt", "cook.johnson" ) )
summary(mod1d)

# compare result with Rasch model estimation in rasch.copula2
# delta must be set to zero
mod2 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster, delta=c(0,0),
            est.delta=c(0,0) )
summary(mod2)

#############################################################################
# EXAMPLE 3: 12 items nested within 3 item clusters (testlets)
#   Cluster 1 -&gt; Items 1-4; Cluster 2 -&gt; Items 6-9;  Cluster 3 -&gt; Items 10-12
#############################################################################

set.seed(967)
I &lt;- 12                             # number of items
n &lt;- 450                            # number of persons
b &lt;- seq(-2,2, len=I)               # item difficulties
b &lt;- sample(b)                      # sample item difficulties
theta &lt;- stats::rnorm( n, sd=1 ) # person abilities
# itemcluster
itemcluster &lt;- rep(0,I)
itemcluster[ 1:4 ] &lt;- 1
itemcluster[ 6:9 ] &lt;- 2
itemcluster[ 10:12 ] &lt;- 3
# residual correlations
rho &lt;- c( .35, .25, .30 )

# simulate data
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("I", seq(1,ncol(dat)), sep="")

# estimate Rasch copula model
mod1 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster )
summary(mod1)

# person parameter estimation assuming the Rasch copula model
pmod1 &lt;- sirt::person.parameter.rasch.copula(raschcopula.object=mod1 )

# Rasch model estimation
mod2 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster,
             delta=rep(0,3), est.delta=rep(0,3) )
summary(mod1)
summary(mod2)

#############################################################################
# EXAMPLE 4: Two-dimensional copula model
#############################################################################

set.seed(5698)
I &lt;- 9
n &lt;- 1500                           # number of persons
b &lt;- seq(-2,2, len=I)               # item difficulties
theta0 &lt;- stats::rnorm( n, sd=sqrt( .6 ) )

#*** Dimension 1
theta &lt;- theta0 + stats::rnorm( n, sd=sqrt( .4 ) )   # person abilities
# itemcluster
itemcluster &lt;- rep(0,I)
itemcluster[ c(3,5 )] &lt;- 1
itemcluster[c(2,4,9)] &lt;- 2
itemcluster1 &lt;- itemcluster
# residual correlations
rho &lt;- c( .7, .5 )
# simulate data
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("A", seq(1,ncol(dat)), sep="")
dat1 &lt;- dat
# estimate model of dimension 1
mod0a &lt;- sirt::rasch.copula2( dat1, itemcluster=itemcluster1)
summary(mod0a)

#*** Dimension 2
theta &lt;- theta0 + stats::rnorm( n, sd=sqrt( .8 ) )        # person abilities
# itemcluster
itemcluster &lt;- rep(0,I)
itemcluster[ c(3,7,8 )] &lt;- 1
itemcluster[c(4,6)] &lt;- 2
itemcluster2 &lt;- itemcluster
# residual correlations
rho &lt;- c( .2, .4 )
# simulate data
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("B", seq(1,ncol(dat)), sep="")
dat2 &lt;- dat
# estimate model of dimension 2
mod0b &lt;- sirt::rasch.copula2( dat2, itemcluster=itemcluster2)
summary(mod0b)

# both dimensions
dat &lt;- cbind( dat1, dat2 )
itemcluster2 &lt;- ifelse( itemcluster2 &gt; 0, itemcluster2 + 2, 0 )
itemcluster &lt;- c( itemcluster1, itemcluster2 )
dims &lt;- rep( 1:2, each=I)

# estimate two-dimensional copula model
mod1 &lt;- sirt::rasch.copula3( dat, itemcluster=itemcluster, dims=dims, est.a=dims,
            theta.k=seq(-5,5,len=15) )
summary(mod1)

#############################################################################
# EXAMPLE 5: Subset of data Example 2
#############################################################################

set.seed(5698)
I &lt;- 11                             # number of items
n &lt;- 3000                           # number of persons
b &lt;- seq(-2,2, len=I)               # item difficulties
theta &lt;- stats::rnorm( n, sd=1.3 )  # person abilities
# define item clusters
itemcluster &lt;- rep(0,I)
itemcluster[ c(3,5)] &lt;- 1
itemcluster[c(2,4,9)] &lt;- 2
# residual correlations
rho &lt;- c( .7, .5 )
# simulate data
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("I", seq(1,ncol(dat)), sep="")

# select subdataset with only one dependent item cluster
item.sel &lt;- scan( what="character", nlines=1 )
    I1 I6 I7 I8 I10 I11 I3 I5
dat1 &lt;- dat[,item.sel]

#******************
#*** Model 1a: estimate Copula model in sirt
itemcluster &lt;- rep(0,8)
itemcluster[c(7,8)] &lt;- 1
mod1a &lt;- sirt::rasch.copula2( dat3, itemcluster=itemcluster )
summary(mod1a)

#******************
#*** Model 1b: estimate Copula model in mirt
library(mirt)
#*** redefine dataset for estimation in mirt
dat2 &lt;- dat1[, itemcluster==0 ]
dat2 &lt;- as.data.frame(dat2)
# combine items 3 and 5
dat2$C35 &lt;- dat1[,"I3"] + 2*dat1[,"I5"]
table( dat2$C35, paste0( dat1[,"I3"],dat1[,"I5"]) )
#* define mirt model
mirtmodel &lt;- mirt::mirt.model("
      F=1-7
      CONSTRAIN=(1-7,a1)
      " )
#-- Copula function with two dependent items
# define item category function for pseudo-items like C35
P.copula2 &lt;- function(par,Theta, ncat){
     b1 &lt;- par[1]
     b2 &lt;- par[2]
     a1 &lt;- par[3]
     ldelta &lt;- par[4]
     P1 &lt;- stats::plogis( a1*(Theta - b1 ) )
     P2 &lt;- stats::plogis( a1*(Theta - b2 ) )
     Q1 &lt;- 1-P1
     Q2 &lt;- 1-P2
     # define vector-wise minimum function
     minf2 &lt;- function( x1, x2 ){
         ifelse( x1 &lt; x2, x1, x2 )
                                }
     # distribution under independence
     F00 &lt;- Q1*Q2
     F10 &lt;- Q1*Q2 + P1*Q2
     F01 &lt;- Q1*Q2 + Q1*P2
     F11 &lt;- 1+0*Q1
     F_ind &lt;- c(F00,F10,F01,F11)
     # distribution under maximal dependence
     F00 &lt;- minf2(Q1,Q2)
     F10 &lt;- Q2              #=minf2(1,Q2)
     F01 &lt;- Q1              #=minf2(Q1,1)
     F11 &lt;- 1+0*Q1          #=minf2(1,1)
     F_dep &lt;- c(F00,F10,F01,F11)
     # compute mixture distribution
     delta &lt;- stats::plogis(ldelta)
     F_tot &lt;- (1-delta)*F_ind + delta * F_dep
     # recalculate probabilities of mixture distribution
     L1 &lt;- length(Q1)
     v1 &lt;- 1:L1
     F00 &lt;- F_tot[v1]
     F10 &lt;- F_tot[v1+L1]
     F01 &lt;- F_tot[v1+2*L1]
     F11 &lt;- F_tot[v1+3*L1]
     P00 &lt;- F00
     P10 &lt;- F10 - F00
     P01 &lt;- F01 - F00
     P11 &lt;- 1 - F10 - F01 + F00
     prob_tot &lt;- c( P00, P10, P01, P11 )
     return(prob_tot)
        }
# create item
copula2 &lt;- mirt::createItem(name="copula2", par=c(b1=0, b2=0.2, a1=1, ldelta=0),
                est=c(TRUE,TRUE,TRUE,TRUE), P=P.copula2,
                lbound=c(-Inf,-Inf,0,-Inf), ubound=c(Inf,Inf,Inf,Inf) )
# define item types
itemtype &lt;- c( rep("2PL",6), "copula2" )
customItems &lt;- list("copula2"=copula2)
# parameter table
mod.pars &lt;- mirt::mirt(dat2, 1, itemtype=itemtype,
                customItems=customItems, pars='values')
# estimate model
mod1b &lt;- mirt::mirt(dat2, mirtmodel, itemtype=itemtype, customItems=customItems,
                verbose=TRUE, pars=mod.pars,
                technical=list(customTheta=as.matrix(seq(-4,4,len=21)) ) )
# estimated coefficients
cmod &lt;- sirt::mirt.wrapper.coef(mod)$coef

# compare common item discrimination
round( c("sirt"=mod1a$item$a[1], "mirt"=cmod$a1[1] ), 4 )
  ##     sirt   mirt
  ##   1.2845 1.2862
# compare delta parameter
round( c("sirt"=mod1a$item$delta[7], "mirt"=stats::plogis( cmod$ldelta[7] ) ), 4 )
  ##     sirt   mirt
  ##   0.6298 0.6297
# compare thresholds a*b
dfr &lt;- cbind( "sirt"=mod1a$item$thresh,
               "mirt"=c(- cmod$d[-7],cmod$b1[7]*cmod$a1[1], cmod$b2[7]*cmod$a1[1]))
round(dfr,4)
  ##           sirt    mirt
  ##   [1,] -1.9236 -1.9231
  ##   [2,] -0.0565 -0.0562
  ##   [3,]  0.3993  0.3996
  ##   [4,]  0.8058  0.8061
  ##   [5,]  1.5293  1.5295
  ##   [6,]  1.9569  1.9572
  ##   [7,] -1.1414 -1.1404
  ##   [8,] -0.4005 -0.3996

## End(Not run)
</code></pre>

<hr>
<h2 id='rasch.evm.pcm'>
Estimation of the Partial Credit Model using the Eigenvector Method
</h2><span id='topic+rasch.evm.pcm'></span><span id='topic+summary.rasch.evm.pcm'></span><span id='topic+coef.rasch.evm.pcm'></span><span id='topic+vcov.rasch.evm.pcm'></span>

<h3>Description</h3>

<p>This function performs the eigenvector approach to estimate item
parameters which is based on a pairwise estimation approach
(Garner &amp; Engelhard, 2002). No assumption about person parameters
is required for item parameter estimation. Statistical inference is performed by
Jackknifing. If a group identifier is provided, tests for differential item
functioning are performed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.evm.pcm(dat, jackunits=20, weights=NULL, pid=NULL,
    group=NULL, powB=2, adj_eps=0.3, progress=TRUE )

## S3 method for class 'rasch.evm.pcm'
summary(object, digits=3, file=NULL, ...)

## S3 method for class 'rasch.evm.pcm'
coef(object,...)

## S3 method for class 'rasch.evm.pcm'
vcov(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.evm.pcm_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous or polytomous item responses
</p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_jackunits">jackunits</code></td>
<td>

<p>A number of Jackknife units (if an integer is provided as the argument
value) or a vector in which the Jackknife units are already
defined.
</p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_weights">weights</code></td>
<td>

<p>Optional vector of sample weights
</p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_pid">pid</code></td>
<td>

<p>Optional vector of person identifiers
</p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_group">group</code></td>
<td>

<p>Optional vector of group identifiers. In this case, item parameters are group wise
estimated and tests for differential item functioning are performed.
</p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_powb">powB</code></td>
<td>

<p>Power created in <code class="reqn">B</code> matrix which is the basis of parameter estimation
</p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_adj_eps">adj_eps</code></td>
<td>

<p>Adjustment parameter for person parameter estimation
(see <code><a href="#topic+mle.pcm.group">mle.pcm.group</a></code>)
</p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_progress">progress</code></td>
<td>

<p>An optional logical indicating whether progress should be displayed
</p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_object">object</code></td>
<td>
<p>Object of class <code>rasch.evm.pcm</code></p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_digits">digits</code></td>
<td>
<p>Number of digits after decimals for rounding in <code>summary</code>.</p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_file">file</code></td>
<td>
<p>Optional file name if <code>summary</code> should be sunk into a file.</p>
</td></tr>
<tr><td><code id="rasch.evm.pcm_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>item</code></td>
<td>
<p>Data frame with item parameters. The item parameter estimate
is denoted by <code>est</code> while a Jackknife bias-corrected estimate
is <code>est_jack</code>. The Jackknife standard error is <code>se</code>.
</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>Item threshold parameters</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with person parameters obtained (MLE)</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>Paired comparison matrix</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>Transformed paired comparison matrix</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>Vector of estimated coefficients</p>
</td></tr>
<tr><td><code>vcov</code></td>
<td>
<p>Covariance matrix of estimated item parameters</p>
</td></tr>
<tr><td><code>JJ</code></td>
<td>
<p>Number of jackknife units</p>
</td></tr>
<tr><td><code>JJadj</code></td>
<td>
<p>Reduced number of jackknife units</p>
</td></tr>
<tr><td><code>powB</code></td>
<td>
<p>Used power of comparison matrix <code class="reqn">B</code></p>
</td></tr>
<tr><td><code>maxK</code></td>
<td>
<p>Maximum number of categories per item</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>Number of groups</p>
</td></tr>
<tr><td><code>desc</code></td>
<td>
<p>Some descriptives</p>
</td></tr>
<tr><td><code>difstats</code></td>
<td>
<p>Statistics for differential item functioning if <code>group</code>
is provided as an argument</p>
</td></tr>
</table>


<h3>References</h3>

<p>Choppin, B. (1985). A fully conditional estimation procedure for Rasch Model
parameters. <em>Evaluation in Education, 9</em>, 29-42.
</p>
<p>Garner, M., &amp; Engelhard, G. J. (2002). An eigenvector method for
estimating item parameters of the dichotomous and polytomous Rasch models.
<em>Journal of Applied Measurement, 3</em>, 107-128.
</p>
<p>Wang, J., &amp; Engelhard, G. (2014). A pairwise algorithm in <span class="rlang"><b>R</b></span> for rater-mediated
assessments. <em>Rasch Measurement Transactions, 28</em>(1), 1457-1459.
</p>


<h3>See Also</h3>

<p>See the <span class="pkg">pairwise</span> package for the alternative row averaging
approach of Choppin (1985) and Wang and Engelhard (2014) for an
alternative <span class="rlang"><b>R</b></span> implementation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset Liking for Science
#############################################################################

data(data.liking.science)
dat &lt;- data.liking.science

# estimate partial credit model using 10 Jackknife units
mod1 &lt;- sirt::rasch.evm.pcm( dat, jackunits=10 )
summary(mod1)

## Not run: 
# compare results with TAM
library(TAM)
mod2 &lt;- TAM::tam.mml( dat )
r1 &lt;- mod2$xsi$xsi
r1 &lt;- r1 - mean(r1)
# item parameters are similar
dfr &lt;- data.frame( "b_TAM"=r1, mod1$item[,c( "est","est_jack") ] )
round( dfr, 3 )
  ##      b_TAM    est est_jack
  ##  1  -2.496 -2.599   -2.511
  ##  2   0.687  0.824    1.030
  ##  3  -0.871 -0.975   -0.943
  ##  4  -0.360 -0.320   -0.131
  ##  5  -0.833 -0.970   -0.856
  ##  6   1.298  1.617    1.444
  ##  7   0.476  0.465    0.646
  ##  8   2.808  3.194    3.439
  ##  9   1.611  1.460    1.433
  ##  10  2.396  1.230    1.095
  ##  [...]

# partial credit model in eRm package
miceadds::library_install("eRm")
mod3 &lt;- eRm::PCM(X=dat)
summary(mod3)
eRm::plotINFO(mod3)      # plot item and test information
eRm::plotICC(mod3)       # plot ICCs
eRm::plotPImap(mod3)     # plot person-item maps

#############################################################################
# EXAMPLE 2: Garner and Engelhard (2002) toy example dichotomous data
#############################################################################

dat &lt;- scan()
   1 0 1 1   1 1 0 0   1 0 0 0   0 1 1 1   1 1 1 0
   1 1 0 1   1 1 1 1   1 0 1 0   1 1 1 1   1 1 0 0

dat &lt;- matrix( dat, 10, 4, byrow=TRUE)
colnames(dat) &lt;- paste0("I", 1:4 )

# estimate Rasch model with no jackknifing
mod1 &lt;- sirt::rasch.evm.pcm( dat, jackunits=0 )

# paired comparison matrix
mod1$B
  ##          I1_Cat1 I2_Cat1 I3_Cat1 I4_Cat1
  ##  I1_Cat1       0       3       4       5
  ##  I2_Cat1       1       0       3       3
  ##  I3_Cat1       1       2       0       2
  ##  I4_Cat1       1       1       1       0

#############################################################################
# EXAMPLE 3: Garner and Engelhard (2002) toy example polytomous data
#############################################################################

dat &lt;- scan()
   2 2 1 1 1   2 1 2 0 0   1 0 0 0 0   0 1 1 2 0   1 2 2 1 1
   2 2 0 2 1   2 2 1 1 0   1 0 1 0 0   2 1 2 2 2   2 1 0 0 1

dat &lt;- matrix( dat, 10, 5, byrow=TRUE)
colnames(dat) &lt;- paste0("I", 1:5 )

# estimate partial credit model with no jackknifing
mod1 &lt;- sirt::rasch.evm.pcm( dat, jackunits=0, powB=3 )

# paired comparison matrix
mod1$B
  ##          I1_Cat1 I1_Cat2 I2_Cat1 I2_Cat2 I3_Cat1 I3_Cat2 I4_Cat1 I4_Cat2 I5_Cat1 I5_Cat2
  ##  I1_Cat1       0       0       2       0       1       1       2       1       2       1
  ##  I1_Cat2       0       0       0       3       2       2       2       2       2       3
  ##  I2_Cat1       1       0       0       0       1       1       2       0       2       1
  ##  I2_Cat2       0       1       0       0       1       2       0       3       1       3
  ##  I3_Cat1       1       1       1       1       0       0       1       2       3       1
  ##  I3_Cat2       0       1       0       2       0       0       1       1       1       1
  ##  I4_Cat1       0       1       0       0       0       2       0       0       1       2
  ##  I4_Cat2       1       0       0       2       1       1       0       0       1       1
  ##  I5_Cat1       0       1       0       1       2       1       1       2       0       0
  ##  I5_Cat2       0       0       0       1       0       0       0       0       0       0

#############################################################################
# EXAMPLE 4: Partial credit model for dataset data.mg from CDM package
#############################################################################

library(CDM)
data(data.mg,package="CDM")
dat &lt;- data.mg[, paste0("I",1:11) ]

#*** Model 1: estimate partial credit model
mod1 &lt;- sirt::rasch.evm.pcm( dat )
# item parameters
round( mod1$b, 3 )
  ##        Cat1   Cat2  Cat3
  ##  I1  -1.537     NA    NA
  ##  I2  -2.360     NA    NA
  ##  I3  -0.574     NA    NA
  ##  I4  -0.971 -2.086    NA
  ##  I5  -0.104  0.201    NA
  ##  I6   0.470  0.806    NA
  ##  I7  -1.027  0.756 1.969
  ##  I8   0.897     NA    NA
  ##  I9   0.766     NA    NA
  ##  I10  0.069     NA    NA
  ##  I11 -1.122  1.159 2.689

#*** Model 2: estimate PCM with pairwise package
miceadds::library_install("pairwise")
mod2 &lt;- pairwise::pair(daten=dat)
summary(mod2)
plot(mod2)
# compute standard errors
semod2 &lt;- pairwise::pairSE(daten=dat,  nsample=20)
semod2

#############################################################################
# EXAMPLE 5: Differential item functioning for dataset data.mg
#############################################################################

library(CDM)
data(data.mg,package="CDM")
dat &lt;- data.mg[ data.mg$group %in% c(2,3,11), ]
# define items
items &lt;- paste0("I",1:11)
# estimate model
mod1 &lt;- sirt::rasch.evm.pcm( dat[,items], weights=dat$weight, group=dat$group )
summary(mod1)

#############################################################################
# EXAMPLE 6: Differential item functioning for Rasch model
#############################################################################

# simulate some data
set.seed(9776)
N &lt;- 1000    # number of persons
I &lt;- 10        # number of items
# simulate data for first group
b &lt;- seq(-1.5,1.5,len=I)
dat1 &lt;- sirt::sim.raschtype( stats::rnorm(N), b )
# simulate data for second group
b1 &lt;- b
b1[4] &lt;- b1[4] + .5 # introduce DIF for fourth item
dat2 &lt;- sirt::sim.raschtype( stats::rnorm(N,mean=.3), b1 )
dat &lt;- rbind(dat1, dat2 )
group &lt;- rep( 1:2, each=N )
# estimate model
mod1 &lt;- sirt::rasch.evm.pcm( dat, group=group )
summary(mod1)

## End(Not run)
</code></pre>

<hr>
<h2 id='rasch.jml'>
Joint Maximum Likelihood (JML) Estimation of the Rasch Model
</h2><span id='topic+rasch.jml'></span><span id='topic+summary.rasch.jml'></span>

<h3>Description</h3>

<p>This function estimates the Rasch model using joint maximum likelihood
estimation (Lincare, 1994). The PROX algorithm (Lincare, 1994) is used
for the generation of starting values of item parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.jml(dat, method="MLE", b.init=NULL, constraints=NULL, weights=NULL,
    center="persons", glob.conv=10^(-6), conv1=1e-05, conv2=0.001, progress=TRUE,
    bsteps=4, thetasteps=2, wle.adj=0, jmliter=100, prox=TRUE,
    proxiter=30, proxconv=0.01, dp=NULL, theta.init=NULL, calc.fit=TRUE,
    prior_sd=NULL)

## S3 method for class 'rasch.jml'
summary(object, digits=3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.jml_+3A_dat">dat</code></td>
<td>
<p>An <code class="reqn">N \times I</code> data frame of dichotomous item responses
where <code class="reqn">N</code> indicates the number of persons
and <code class="reqn">I</code> the number of items
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_method">method</code></td>
<td>

<p>Method for estimating person parameters during JML iterations.
<code>MLE</code> is maximum likelihood estimation (where person with
perfect scores are deleted from analysis). <code>WLE</code> uses weighted
likelihood estimation (Warm, 1989) for person parameter estimation.
Default is <code>MLE</code>.
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_b.init">b.init</code></td>
<td>

<p>Initial values of item difficulties
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_constraints">constraints</code></td>
<td>

<p>Optional matrix or data.frame with two columns. First column is an integer of
item indexes or item names (<code>colnames(dat)</code>) which shall be fixed
during estimation. The second column is the corresponding
item difficulty.
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_weights">weights</code></td>
<td>

<p>Person sample weights. Default is <code>NULL</code>, i.e. all persons in the sample
are equally weighted.
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_center">center</code></td>
<td>
<p>Character indicator whether persons (<code>"persons"</code>),
items (<code>"items"</code>) should be centered or (<code>"none"</code>)
should be conducted.
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_glob.conv">glob.conv</code></td>
<td>

<p>Global convergence criterion with respect to the log-likelihood function
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_conv1">conv1</code></td>
<td>

<p>Convergence criterion for estimation of item parameters
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_conv2">conv2</code></td>
<td>

<p>Convergence criterion for estimation of person parameters
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_progress">progress</code></td>
<td>

<p>Display progress? Default is <code>TRUE</code>
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_bsteps">bsteps</code></td>
<td>

<p>Number of steps for b parameter estimation
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_thetasteps">thetasteps</code></td>
<td>

<p>Number of steps for theta parameter estimation
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_wle.adj">wle.adj</code></td>
<td>

<p>Score adjustment for WLE estimation
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_jmliter">jmliter</code></td>
<td>

<p>Number of maximal iterations during JML estimation
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_prox">prox</code></td>
<td>

<p>Should the PROX algorithm (see <code><a href="#topic+rasch.prox">rasch.prox</a></code>) be used as initial estimations?
Default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_proxiter">proxiter</code></td>
<td>

<p>Number of maximal PROX iterations
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_proxconv">proxconv</code></td>
<td>

<p>Convergence criterion for PROX iterations
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_dp">dp</code></td>
<td>

<p>Object created from data preparation function (<code>.data.prep</code>)
which could be created in earlier JML runs. Default is <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_theta.init">theta.init</code></td>
<td>

<p>Initial person parameter estimate
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_calc.fit">calc.fit</code></td>
<td>

<p>Should itemfit being calculated?
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_prior_sd">prior_sd</code></td>
<td>

<p>Optional value for standard deviation of prior distribution for
ability values if penalized JML should be utilized
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_object">object</code></td>
<td>

<p>Object of class <code>rasch.jml</code>
</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_digits">digits</code></td>
<td>
<p>Number of digits used for rounding</p>
</td></tr>
<tr><td><code id="rasch.jml_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimation is known to have a bias in item parameters for
a fixed (finite) number of items. In literature (Lincare, 1994), a simple
bias correction formula is proposed and included in the value
<code>item$itemdiff.correction</code> in this function. If <code class="reqn">I</code> denotes the number
of items, then the correction factor is <code class="reqn">\frac{I-1}{I}</code>.
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>item</code></td>
<td>
<p>Estimated item parameters</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Estimated person parameters</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Person parameter estimation method</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Original data frame</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>data.proc</code></td>
<td>
<p>Processed data frames excluding persons with extreme scores</p>
</td></tr>
<tr><td><code>dp</code></td>
<td>
<p>Value of data preparation (it is used in the function
<code><a href="#topic+rasch.jml.jackknife1">rasch.jml.jackknife1</a></code>)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Linacre, J. M. (1994). <em>Many-Facet Rasch Measurement</em>. Chicago: MESA Press.
</p>
<p>Warm, T. A. (1989). Weighted likelihood estimation of ability in the item
response theory. <em>Psychometrika, 54</em>, 427-450.
</p>


<h3>See Also</h3>

<p>Get a summary with <code><a href="#topic+summary.rasch.jml">summary.rasch.jml</a></code>.
</p>
<p>See <code><a href="#topic+rasch.prox">rasch.prox</a></code> for the PROX algorithm as initial iterations.
</p>
<p>For a bias correction of the JML method try <code><a href="#topic+rasch.jml.jackknife1">rasch.jml.jackknife1</a></code>.
</p>
<p>JML estimation can also be conducted with the <span class="pkg">TAM</span>
(<code><a href="TAM.html#topic+tam.jml">TAM::tam.jml</a></code>)
and <span class="pkg">immer</span> (<code>immer::immer_jml</code>)
packages.
</p>
<p>See also marginal maximum likelihood estimation with <code><a href="#topic+rasch.mml2">rasch.mml2</a></code>
or the <span class="rlang"><b>R</b></span> package <span class="pkg">ltm</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Simulated data from the Rasch model
#############################################################################

set.seed(789)
N &lt;- 500    # number of persons
I &lt;- 11     # number of items
b &lt;- seq( -2, 2, length=I )
dat &lt;- sirt::sim.raschtype( stats::rnorm( N, mean=.5 ), b )
colnames(dat) &lt;- paste( "I", 1:I, sep="")

# JML estimation of the Rasch model (centering persons)
mod1 &lt;- sirt::rasch.jml( dat )
summary(mod1)

# JML estimation of the Rasch model (centering items)
mod1b &lt;- sirt::rasch.jml( dat, center="items" )
summary(mod1b)

# MML estimation with rasch.mml2 function
mod2 &lt;- sirt::rasch.mml2( dat )
summary(mod2)

# Pairwise method of Fischer
mod3 &lt;- sirt::rasch.pairwise( dat )
summary(mod3)

# JML estimation in TAM
## Not run: 
library(TAM)
mod4 &lt;- TAM::tam.jml( resp=dat )

#******
# item parameter constraints in JML estimation
# fix item difficulties: b[4]=-.76 and b[6]=.10
constraints &lt;- matrix( cbind( 4, -.76,
                              6, .10 ),
                  ncol=2, byrow=TRUE )
mod6 &lt;- sirt::rasch.jml( dat, constraints=constraints )
summary(mod6)
  # For constrained item parameters, it this not obvious
  # how to calculate a 'right correction' of item parameter bias

## End(Not run)
</code></pre>

<hr>
<h2 id='rasch.jml.biascorr'>
Bias Correction of Item Parameters for Joint Maximum Likelihood Estimation
in the Rasch model
</h2><span id='topic+rasch.jml.biascorr'></span>

<h3>Description</h3>

<p>This function computes an analytical bias correction for the Rasch
model according to the method of Arellano and Hahn (2007).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.jml.biascorr(jmlobj,itemfac=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.jml.biascorr_+3A_jmlobj">jmlobj</code></td>
<td>

<p>An object which is the output of the <code><a href="#topic+rasch.jml">rasch.jml</a></code> function
</p>
</td></tr>
<tr><td><code id="rasch.jml.biascorr_+3A_itemfac">itemfac</code></td>
<td>
<p>Number of items which are used for bias correction.
By default it is the average number of item responses per person.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>b.biascorr</code></td>
<td>
<p>Matrix of item difficulty estimates. The column
<code>b.analytcorr1</code> contains item difficulties by analytical bias
correction of Method 1 in Arellano and Hahn (2007) whereas <code>b.analytcorr2</code>
corresponds to Method 2.</p>
</td></tr>
<tr><td><code>b.bias1</code></td>
<td>
<p>Estimated bias by Method 1</p>
</td></tr>
<tr><td><code>b.bias2</code></td>
<td>
<p>Estimated bias by Method 2</p>
</td></tr>
<tr><td><code>itemfac</code></td>
<td>
<p>Number of items which are used as the factor for bias
correction</p>
</td></tr>
</table>


<h3>References</h3>

<p>Arellano, M., &amp; Hahn, J. (2007). Understanding bias in nonlinear panel
models: Some recent developments. In R. Blundell, W. Newey &amp;
T. Persson (Eds.): <em>Advances in Economics and
Econometrics, Ninth World Congress</em>, Cambridge University Press.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+rasch.jml.jackknife1">rasch.jml.jackknife1</a></code> for bias correction based on
Jackknife.
</p>
<p>See also the <span class="pkg">bife</span> <span class="rlang"><b>R</b></span> package for analytical bias corrections.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset Reading
#############################################################################
data(data.read)
dat &lt;- data( data.read )

# estimate Rasch model
mod &lt;- sirt::rasch.jml( data.read  )

# JML with analytical bias correction
res1 &lt;- sirt::rasch.jml.biascorr( jmlobj=mod  )
print( res1$b.biascorr, digits=3 )
  ##        b.JML b.JMLcorr b.analytcorr1 b.analytcorr2
  ##   1  -2.0086   -1.8412        -1.908        -1.922
  ##   2  -1.1121   -1.0194        -1.078        -1.088
  ##   3  -0.0718   -0.0658        -0.150        -0.127
  ##   4   0.5457    0.5002         0.393         0.431
  ##   5  -0.9504   -0.8712        -0.937        -0.936
  ##  [...]
</code></pre>

<hr>
<h2 id='rasch.jml.jackknife1'>
Jackknifing the IRT Model Estimated by Joint Maximum Likelihood (JML)
</h2><span id='topic+rasch.jml.jackknife1'></span>

<h3>Description</h3>

<p>Jackknife estimation is an alternative to other ad hoc proposed
methods for bias correction (Hahn &amp; Newey, 2004).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.jml.jackknife1(jmlobj)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.jml.jackknife1_+3A_jmlobj">jmlobj</code></td>
<td>

<p>Output of <code>rasch.jml</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that items are used for jackknifing (Hahn &amp; Newey, 2004).
By default, all <code class="reqn">I</code> items in the data frame are used as
jackknife units.
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>item</code></td>
<td>

<p>A data frame with item parameters
</p>

<ul>
<li> <p><code>b.JML</code>: Item difficulty from JML estimation
</p>
</li>
<li> <p><code>b.JMLcorr</code>: Item difficulty from JML estimation by
applying the correction factor <code class="reqn">(I-1)/I</code>
</p>
</li>
<li> <p><code>b.jack</code>: Item difficulty from Jackknife estimation
</p>
</li>
<li> <p><code>b.jackse</code>: Standard error of Jackknife estimation
for item difficulties.
Note that this parameter refer to the standard error
with respect to item sampling
</p>
</li>
<li> <p><code>b.JMLse</code>: Standard error for item difficulties
obtained from JML estimation
</p>
</li></ul>

</td></tr>
<tr><td><code>jack.itemdiff</code></td>
<td>
<p>A matrix containing all item difficulties obtained by Jackknife</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hahn, J., &amp; Newey, W. (2004). Jackknife and analytical bias reduction for
nonlinear panel models. <em>Econometrica, 72</em>, 1295-1319.
</p>


<h3>See Also</h3>

<p>For JML estimation <code><a href="#topic+rasch.jml">rasch.jml</a></code>.
</p>
<p>For analytical bias correction methods see <code><a href="#topic+rasch.jml.biascorr">rasch.jml.biascorr</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Simulated data from the Rasch model
#############################################################################
set.seed(7655)
N &lt;- 5000    # number of persons
I &lt;- 11      # number of items
b &lt;- seq( -2, 2, length=I )
dat &lt;- sirt::sim.raschtype( rnorm( N ), b )
colnames(dat) &lt;- paste( "I", 1:I, sep="")

# estimate the Rasch model with JML
mod &lt;- sirt::rasch.jml(dat)
summary(mod)

# re-estimate the Rasch model using Jackknife
mod2 &lt;- sirt::rasch.jml.jackknife1( mod )
  ##
  ##   Joint Maximum Likelihood Estimation
  ##   Jackknife Estimation
  ##   11 Jackknife Units are used
  ##   |--------------------PROGRESS--------------------|
  ##   |------------------------------------------------|
  ##
  ##          N     p  b.JML b.JMLcorr b.jack b.jackse b.JMLse
  ##   I1  4929 0.853 -2.345    -2.131 -2.078    0.079   0.045
  ##   I2  4929 0.786 -1.749    -1.590 -1.541    0.075   0.039
  ##   I3  4929 0.723 -1.298    -1.180 -1.144    0.065   0.036
  ##   I4  4929 0.657 -0.887    -0.806 -0.782    0.059   0.035
  ##   I5  4929 0.576 -0.420    -0.382 -0.367    0.055   0.033
  ##   I6  4929 0.492  0.041     0.038  0.043    0.054   0.033
  ##   I7  4929 0.409  0.502     0.457  0.447    0.056   0.034
  ##   I8  4929 0.333  0.939     0.854  0.842    0.058   0.035
  ##   I9  4929 0.264  1.383     1.257  1.229    0.065   0.037
  ##   I10 4929 0.210  1.778     1.617  1.578    0.071   0.040
  ##   I11 4929 0.154  2.266     2.060  2.011    0.077   0.044
#-&gt; Item parameters obtained by jackknife seem to be acceptable.

## End(Not run)
</code></pre>

<hr>
<h2 id='rasch.mirtlc'>
Multidimensional Latent Class 1PL and 2PL Model
</h2><span id='topic+rasch.mirtlc'></span><span id='topic+summary.rasch.mirtlc'></span><span id='topic+logLik.rasch.mirtlc'></span><span id='topic+anova.rasch.mirtlc'></span><span id='topic+IRT.irfprob.rasch.mirtlc'></span><span id='topic+IRT.likelihood.rasch.mirtlc'></span><span id='topic+IRT.posterior.rasch.mirtlc'></span><span id='topic+IRT.modelfit.rasch.mirtlc'></span><span id='topic+summary.IRT.modelfit.rasch.mirtlc'></span>

<h3>Description</h3>

<p>This function estimates the multidimensional latent class
Rasch (1PL) and 2PL model (Bartolucci, 2007; Bartolucci, Montanari &amp; Pandolfi,
2012) for dichotomous data which emerges from the original latent class model
(Goodman, 1974) and a multidimensional IRT model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.mirtlc(dat, Nclasses=NULL, modeltype="LC", dimensions=NULL,
    group=NULL, weights=rep(1,nrow(dat)), theta.k=NULL, ref.item=NULL,
    distribution.trait=FALSE,  range.b=c(-8,8), range.a=c(.2, 6 ),
    progress=TRUE, glob.conv=10^(-5), conv1=10^(-5), mmliter=1000,
    mstep.maxit=3, seed=0, nstarts=1, fac.iter=.35)

## S3 method for class 'rasch.mirtlc'
summary(object,...)

## S3 method for class 'rasch.mirtlc'
anova(object,...)

## S3 method for class 'rasch.mirtlc'
logLik(object,...)

## S3 method for class 'rasch.mirtlc'
IRT.irfprob(object,...)

## S3 method for class 'rasch.mirtlc'
IRT.likelihood(object,...)

## S3 method for class 'rasch.mirtlc'
IRT.posterior(object,...)

## S3 method for class 'rasch.mirtlc'
IRT.modelfit(object,...)

## S3 method for class 'IRT.modelfit.rasch.mirtlc'
summary(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.mirtlc_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_nclasses">Nclasses</code></td>
<td>

<p>Number of latent classes. If the trait vector (or matrix)
<code>theta.k</code> is specified, then <code>Nclasses</code> is set
to the dimension of <code>theta.k</code>.
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_modeltype">modeltype</code></td>
<td>

<p>Modeltype. <code>LC</code> is the latent class model of Goodman (1974).
<code>MLC1</code> is the multidimensional latent class Rasch model with
item discrimination parameter of 1. <code>MLC2</code> allows for the
estimation of item discriminations.
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_dimensions">dimensions</code></td>
<td>

<p>Vector of dimension integers which allocate items to dimensions.
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_group">group</code></td>
<td>

<p>A group identifier for multiple group estimation
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_weights">weights</code></td>
<td>

<p>Vector of sample weights
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_theta.k">theta.k</code></td>
<td>

<p>A grid of theta values can be specified if theta
should not be estimated. In the one-dimensional case, it
must be a vector, in the <code class="reqn">D</code>-dimensional case it must
be a matrix of dimension <code class="reqn">D</code>.
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_ref.item">ref.item</code></td>
<td>
<p>An optional vector of integers which indicate the items
whose intercept and slope are fixed at 0 and 1, respectively.
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_distribution.trait">distribution.trait</code></td>
<td>
<p>A type of the assumed theta distribution can
be specified. One alternative is <code>normal</code>
for the normal distribution assumption.
The options <code>smooth2</code>, <code>smooth3</code> and
<code>smooth4</code> use the log-linear smoothing of
Xu and von Davier (2008) to smooth the distribution
up to two, three or four moments, respectively.
This function only works in unidimensional models. <br />
If a different string is provided as an input (e.g. <code>no</code>),
then no smoothing is conducted.
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_range.b">range.b</code></td>
<td>

<p>Range of item difficulties which are allowed for estimation
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_range.a">range.a</code></td>
<td>

<p>Range of item slopes which are allowed for estimation
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_progress">progress</code></td>
<td>

<p>Display progress? Default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_glob.conv">glob.conv</code></td>
<td>

<p>Global relative deviance convergence criterion
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_conv1">conv1</code></td>
<td>

<p>Item parameter convergence criterion
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_mmliter">mmliter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_mstep.maxit">mstep.maxit</code></td>
<td>

<p>Maximum number of iterations within an M step
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_seed">seed</code></td>
<td>

<p>Set random seed for latent class estimation. A seed
can be specified. If the seed is negative, then
the function will generate a random seed.
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_nstarts">nstarts</code></td>
<td>

<p>If a positive integer is provided, then a <code>nstarts</code>
starts with different starting values are conducted.
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_fac.iter">fac.iter</code></td>
<td>
<p>A parameter between 0 and 1 to control the maximum
increment in each iteration. The larger the parameter the more increments
will become smaller from iteration to iteration.
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_object">object</code></td>
<td>

<p>Object of class <code>rasch.mirtlc</code>
</p>
</td></tr>
<tr><td><code id="rasch.mirtlc_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multidimensional latent class Rasch model (Bartolucci, 2007)
is an item response model which combines ideas from
latent class analysis and item response models with continuous variables.
With <code>modeltype="MLC2"</code> the following <code class="reqn">D</code>-dimensional
item response model is estimated
</p>
<p style="text-align: center;"><code class="reqn">logit P(X_{pi}=1 | \theta_p )=a_i \theta_{pcd}- b_i</code>
</p>

<p>Besides the item thresholds <code class="reqn">b_i</code> and item slopes <code class="reqn">a_i</code>,
for a prespecified number of latent classes <code class="reqn">c=1,\ldots,C</code>
a set of <code class="reqn">C</code> <code class="reqn">D</code>-dimensional <code class="reqn">\{\theta_{cd} \}_{cd}</code>
vectors are estimated.
These vectors represent the locations of latent classes. If the user
provides a grid of theta distribution <code>theta.k</code> as an argument in
<code>rasch.mirtlc</code>, then the ability distribution is fixed.
</p>
<p>In the unidimensional Rasch model with <code class="reqn">I</code> items, <code class="reqn">(I+1)/2</code>
(if <code class="reqn">I</code> odd) or <code class="reqn">I/2 + 1</code> (if <code class="reqn">I</code> even) trait location
parameters are identified (see De Leeuw &amp; Verhelst, 1986;
Lindsay et al., 1991; for a review see Formann, 2007).
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>pjk</code></td>
<td>
<p>Item probabilities evaluated at discretized ability distribution</p>
</td></tr>
<tr><td><code>rprobs</code></td>
<td>
<p>Item response probabilities like in <code>pjk</code>,
but for each item category</p>
</td></tr>
<tr><td><code>pi.k</code></td>
<td>
<p>Estimated trait distribution</p>
</td></tr>
<tr><td><code>theta.k</code></td>
<td>
<p>Discretized ability distribution</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Estimated item parameters</p>
</td></tr>
<tr><td><code>trait</code></td>
<td>
<p>Estimated ability distribution (<code>theta.k</code> and <code>pi.k</code>)</p>
</td></tr>
<tr><td><code>mean.trait</code></td>
<td>
<p>Estimated mean of ability distribution</p>
</td></tr>
<tr><td><code>sd.trait</code></td>
<td>
<p>Estimated standard deviation of ability distribution</p>
</td></tr>
<tr><td><code>skewness.trait</code></td>
<td>
<p>Estimated skewness of ability distribution</p>
</td></tr>
<tr><td><code>cor.trait</code></td>
<td>
<p>Estimated correlation between abilities (only applies for
multidimensional models)</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>Number of dimensions</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>Number of groups</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>Log-likelihood</p>
</td></tr>
<tr><td><code>Nclasses</code></td>
<td>
<p>Number of classes</p>
</td></tr>
<tr><td><code>modeltype</code></td>
<td>
<p>Used model type</p>
</td></tr>
<tr><td><code>estep.res</code></td>
<td>
<p>Result from E step: <code>f.qk.yi</code> is the individual posterior,
<code>f.yi.qk</code> is the individual likelihood</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Original data frame</p>
</td></tr>
<tr><td><code>devL</code></td>
<td>
<p>Vector of deviances if multiple random starts were conducted</p>
</td></tr>
<tr><td><code>seedL</code></td>
<td>
<p>Vector of seed if multiple random starts were conducted</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For the estimation of latent class models,
rerunning the model with different starting values
(different random seeds) is recommended.
</p>


<h3>References</h3>

<p>Bartolucci, F. (2007). A class of multidimensional
IRT models for testing unidimensionality and clustering
items. <em>Psychometrika, 72</em>(2), 141-157.
<a href="https://doi.org/10.1007/s11336-005-1376-9">doi:10.1007/s11336-005-1376-9</a>
</p>
<p>Bartolucci, F., Montanari, G. E., &amp; Pandolfi, S. (2012).
Dimensionality of the latent structure and item selection via latent
class multidimensional IRT models.
<em>Psychometrika, 77</em>(4), 782-802.
<a href="https://doi.org/10.1007/s11336-012-9278-0">doi:10.1007/s11336-012-9278-0</a>
</p>
<p>De Leeuw, J., &amp; Verhelst, N. (1986). Maximum likelihood estimation in
generalized Rasch models. <em>Journal of Educational and Behavioral Statistics, 11</em>(3),
183-196. <a href="https://doi.org/10.3102/10769986011003183">doi:10.3102/10769986011003183</a>
</p>
<p>Formann, A. K. (2007). (Almost) Equivalence between conditional and mixture
maximum likelihood estimates for some models of the Rasch type.
In M. von Davier &amp; C. H. Carstensen:
<em>Multivariate and Mixture Distribution Rasch Models</em> (pp. 177-189).
Springer: New York.
<a href="https://doi.org/10.1007/978-0-387-49839-3_11">doi:10.1007/978-0-387-49839-3_11</a>
</p>
<p>Goodman, L. A. (1974). Exploratory latent structure
analysis using both identifiable and unidentifiable
models. <em>Biometrika, 61</em>(2), 215-231.
<a href="https://doi.org/10.1093/biomet/61.2.215">doi:10.1093/biomet/61.2.215</a>
</p>
<p>Lindsay, B., Clogg, C. C., &amp; Grego, J. (1991).
Semiparametric estimation in the Rasch model and related exponential response
models, including a simple latent class model for item analysis.
<em>Journal of the American Statistical Association, 86</em>(413), 96-107.
<a href="https://doi.org/10.1080/01621459.1991.10475008">doi:10.1080/01621459.1991.10475008</a>
</p>
<p>Xu, X., &amp; von Davier, M. (2008). <em>Fitting the structured general
diagnostic model to NAEP data</em>. ETS Research Report ETS RR-08-27.
Princeton, ETS.
<a href="https://doi.org/10.1002/j.2333-8504.2008.tb02113.x">doi:10.1002/j.2333-8504.2008.tb02113.x</a>
</p>


<h3>See Also</h3>

<p>See also the <code><a href="CDM.html#topic+gdm">CDM::gdm</a></code>
function in the <span class="pkg">CDM</span> package.
</p>
<p>For an assessment of global model fit see <code><a href="#topic+modelfit.sirt">modelfit.sirt</a></code>.
</p>
<p>The estimation of the multidimensional latent class item response
model for polytomous data can be conducted in the <span class="pkg">MultiLCIRT</span>
package. Latent class analysis can be carried out with
<span class="pkg">poLCA</span> and <span class="pkg">randomLCA</span> packages.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Reading data
#############################################################################
data( data.read )
dat &lt;- data.read

#***************
# latent class models

# latent class model with 1 class
mod1 &lt;- sirt::rasch.mirtlc( dat, Nclasses=1 )
summary(mod1)

# latent class model with 2 classes
mod2 &lt;- sirt::rasch.mirtlc( dat, Nclasses=2 )
summary(mod2)

## Not run: 
# latent class model with 3 classes
mod3 &lt;- sirt::rasch.mirtlc( dat, Nclasses=3, seed=- 30)
summary(mod3)

# extract individual likelihood
lmod3 &lt;- IRT.likelihood(mod3)
str(lmod3)
# extract likelihood value
logLik(mod3)
# extract item response functions
IRT.irfprob(mod3)

# compare models 1, 2 and 3
anova(mod2,mod3)
IRT.compareModels(mod1,mod2,mod3)
# avsolute and relative model fit
smod2 &lt;- IRT.modelfit(mod2)
smod3 &lt;- IRT.modelfit(mod3)
summary(smod2)
IRT.compareModels(smod2,smod3)

# latent class model with 4 classes and 3 starts with different seeds
mod4 &lt;- sirt::rasch.mirtlc( dat, Nclasses=4,seed=-30,  nstarts=3 )
# display different solutions
sort(mod4$devL)
summary(mod4)

# latent class multiple group model
# define group identifier
group &lt;- rep( 1, nrow(dat))
group[ 1:150 ] &lt;- 2
mod5 &lt;- sirt::rasch.mirtlc( dat, Nclasses=3, group=group )
summary(mod5)

#*************
# Unidimensional IRT models with ordered trait

# 1PL model with 3 classes
mod11 &lt;- sirt::rasch.mirtlc( dat, Nclasses=3, modeltype="MLC1", mmliter=30)
summary(mod11)

# 1PL model with 11 classes
mod12 &lt;- sirt::rasch.mirtlc( dat, Nclasses=11,modeltype="MLC1", mmliter=30)
summary(mod12)

# 1PL model with 11 classes and fixed specified theta values
mod13 &lt;- sirt::rasch.mirtlc( dat,  modeltype="MLC1",
             theta.k=seq( -4, 4, len=11 ), mmliter=100)
summary(mod13)

# 1PL model with fixed theta values and normal distribution
mod14 &lt;- sirt::rasch.mirtlc( dat,  modeltype="MLC1", mmliter=30,
             theta.k=seq( -4, 4, len=11 ), distribution.trait="normal")
summary(mod14)

# 1PL model with a smoothed trait distribution (up to 3 moments)
mod15 &lt;- sirt::rasch.mirtlc( dat,  modeltype="MLC1", mmliter=30,
             theta.k=seq( -4, 4, len=11 ),  distribution.trait="smooth3")
summary(mod15)

# 2PL with 3 classes
mod16 &lt;- sirt::rasch.mirtlc( dat, Nclasses=3, modeltype="MLC2", mmliter=30 )
summary(mod16)

# 2PL with fixed theta and smoothed distribution
mod17 &lt;- sirt::rasch.mirtlc( dat, theta.k=seq(-4,4,len=12), mmliter=30,
             modeltype="MLC2", distribution.trait="smooth4"  )
summary(mod17)

# 1PL multiple group model with 8 classes
# define group identifier
group &lt;- rep( 1, nrow(dat))
group[ 1:150 ] &lt;- 2
mod21 &lt;- sirt::rasch.mirtlc( dat, Nclasses=8, modeltype="MLC1", group=group )
summary(mod21)

#***************
# multidimensional latent class IRT models

# define vector of dimensions
dimensions &lt;- rep( 1:3, each=4 )

# 3-dimensional model with 8 classes and seed 145
mod31 &lt;- sirt::rasch.mirtlc( dat, Nclasses=8, mmliter=30,
             modeltype="MLC1", seed=145, dimensions=dimensions )
summary(mod31)

# try the model above with different starting values
mod31s &lt;- sirt::rasch.mirtlc( dat, Nclasses=8,
             modeltype="MLC1", seed=-30, nstarts=30, dimensions=dimensions )
summary(mod31s)

# estimation with fixed theta vectors
#=&gt; 4^3=216 classes
theta.k &lt;- seq(-4, 4, len=6 )
theta.k &lt;- as.matrix( expand.grid( theta.k, theta.k, theta.k ) )
mod32 &lt;- sirt::rasch.mirtlc( dat,  dimensions=dimensions,
              theta.k=theta.k, modeltype="MLC1"  )
summary(mod32)

# 3-dimensional 2PL model
mod33 &lt;- sirt::rasch.mirtlc( dat, dimensions=dimensions, theta.k=theta.k, modeltype="MLC2")
summary(mod33)

#############################################################################
# EXAMPLE 2: Skew trait distribution
#############################################################################
set.seed(789)
N &lt;- 1000   # number of persons
I &lt;- 20     # number of items
theta &lt;- sqrt( exp( stats::rnorm( N ) ) )
theta &lt;- theta - mean(theta )
# calculate skewness of theta distribution
mean( theta^3 ) / stats::sd(theta)^3
# simulate item responses
dat &lt;- sirt::sim.raschtype( theta, b=seq(-2,2,len=I ) )

# normal distribution
mod1 &lt;- sirt::rasch.mirtlc( dat, theta.k=seq(-4,4,len=15), modeltype="MLC1",
               distribution.trait="normal", mmliter=30)

# allow for skew distribution with smoothed distribution
mod2 &lt;- sirt::rasch.mirtlc( dat, theta.k=seq(-4,4,len=15), modeltype="MLC1",
               distribution.trait="smooth3", mmliter=30)

# nonparametric distribution
mod3 &lt;- sirt::rasch.mirtlc( dat, theta.k=seq(-4,4,len=15), modeltype="MLC1", mmliter=30)

summary(mod1)
summary(mod2)
summary(mod3)

#############################################################################
# EXAMPLE 3: Stouffer-Toby dataset data.si02 with 5 items
#############################################################################

data(dat.si02)
dat &lt;- data.si02$data
weights &lt;- data.si02$weights   # extract weights

# Model 1: 2 classes Rasch model
mod1 &lt;- sirt::rasch.mirtlc( dat, Nclasses=2, modeltype="MLC1", weights=weights,
                 ref.item=4, nstarts=5)
summary(mod1)

# Model 2: 3 classes Rasch model: not all parameters are identified
mod2 &lt;- sirt::rasch.mirtlc( dat, Nclasses=3, modeltype="MLC1", weights=weights,
                ref.item=4, nstarts=5)
summary(mod2)

# Model 3: Latent class model with 2 classes
mod3 &lt;- sirt::rasch.mirtlc( dat, Nclasses=2, modeltype="LC", weights=weights, nstarts=5)
summary(mod3)

# Model 4: Rasch model with normal distribution
mod4 &lt;- sirt::rasch.mirtlc( dat,  modeltype="MLC1", weights=weights,
            theta.k=seq( -6, 6, len=21 ), distribution.trait="normal", ref.item=4)
summary(mod4)

## End(Not run)

#############################################################################
# EXAMPLE 4: 5 classes, 3 dimensions and 27 items
#############################################################################

set.seed(979)
I &lt;- 9
N &lt;- 5000
b &lt;- seq( - 1.5, 1.5, len=I)
b &lt;- rep(b,3)
# define class locations
theta.k &lt;- c(-3.0, -4.1, -2.8, 1.7, 2.3, 1.8,
   0.2, 0.4, -0.1,   2.6, 0.1, -0.9, -1.1,-0.7, 0.9 )

Nclasses &lt;- 5
theta.k0 &lt;- theta.k &lt;- matrix( theta.k, Nclasses, 3, byrow=TRUE )
pi.k &lt;- c(.20,.25,.25,.10,.15)
theta &lt;- theta.k[ rep( 1:Nclasses, round(N*pi.k) ), ]
dimensions &lt;- rep( 1:3, each=I)
# simulate item responses
dat &lt;- matrix( NA, nrow=N, ncol=I*3)
for (ii in 1:(3*I) ){
    dat[,ii] &lt;- 1 * ( stats::runif(N) &lt; stats::plogis( theta[,dimensions[ii]] - b[ii]))
}
colnames(dat) &lt;- paste0( rep( LETTERS[1:3], each=I ), 1:(3*I) )

# estimate model
mod1 &lt;- sirt::rasch.mirtlc( dat, Nclasses=Nclasses, dimensions=dimensions,
             modeltype="MLC1", ref.item=c(5,14,23), glob.conv=.0005, conv1=.0005)

round( cbind( mod1$theta.k, mod1$pi.k ), 3 )
  ##          [,1]   [,2]   [,3]  [,4]
  ##   [1,] -2.776 -3.791 -2.667 0.250
  ##   [2,] -0.989 -0.605  0.957 0.151
  ##   [3,]  0.332  0.418 -0.046 0.246
  ##   [4,]  2.601  0.171 -0.854 0.101
  ##   [5,]  1.791  2.330  1.844 0.252
cbind( theta.k, pi.k )
  ##                       pi.k
  ##   [1,] -3.0 -4.1 -2.8 0.20
  ##   [2,]  1.7  2.3  1.8 0.25
  ##   [3,]  0.2  0.4 -0.1 0.25
  ##   [4,]  2.6  0.1 -0.9 0.10
  ##   [5,] -1.1 -0.7  0.9 0.15

# plot class locations
plot( 1:3, mod1$theta.k[1,], xlim=c(1,3), ylim=c(-5,3), col=1, pch=1, type="n",
    axes=FALSE, xlab="Dimension", ylab="Location")
axis(1, 1:3 ) ;  axis(2) ; axis(4)
for (cc in 1:Nclasses){ # cc &lt;- 1
    lines(1:3, mod1$theta.k[cc,], col=cc, lty=cc )
    points(1:3, mod1$theta.k[cc,], col=cc,  pch=cc )
}

## Not run: 
#------
# estimate model with gdm function in CDM package
library(CDM)
# define Q-matrix
Qmatrix &lt;- matrix(0,3*I,3)
Qmatrix[ cbind( 1:(3*I), rep(1:3, each=I) ) ] &lt;- 1

set.seed(9176)
# random starting values for theta locations
theta.k &lt;- matrix( 2*stats::rnorm(5*3), 5, 3 )
colnames(theta.k) &lt;- c("Dim1","Dim2","Dim3")
# try possibly different starting values

# estimate model in CDM
b.constraint  &lt;- cbind( c(5,14,23), 1, 0 )
mod2 &lt;- CDM::gdm( dat, theta.k=theta.k, b.constraint=b.constraint, skillspace="est",
               irtmodel="1PL",  Qmatrix=Qmatrix)
summary(mod2)

#------
# estimate model with MultiLCIRT package
miceadds::library_install("MultiLCIRT")

# define matrix to allocate each item to one dimension
multi1 &lt;- matrix( 1:(3*I), nrow=3, byrow=TRUE )
# define reference items in item-dimension allocation matrix
multi1[ 1, c(1,5)  ] &lt;- c(5,1)
multi1[ 2, c(10,14) - 9  ] &lt;- c(14,9)
multi1[ 3, c(19,23) - 18 ] &lt;- c(23,19)

# Rasch model with 5 latent classes (random start: start=1)
mod3 &lt;- MultiLCIRT::est_multi_poly(S=dat,k=5,       # k=5 ability levels
                start=1,link=1,multi=multi1,tol=10^-5,
                output=TRUE, disp=TRUE, fort=TRUE)
# estimated location points and class probabilities in MultiLCIRT
cbind( t( mod3$Th ), mod3$piv )
# compare results with rasch.mirtlc
cbind( mod1$theta.k, mod1$pi.k )
# simulated data parameters
cbind( theta.k, pi.k )

#----
# estimate model with cutomized input in mirt
library(mirt)
#-- define Theta design matrix for 5 classes
Theta &lt;- diag(5)
Theta &lt;- cbind( Theta, Theta, Theta )
r1 &lt;- rownames(Theta) &lt;- paste0("C",1:5)
colnames(Theta) &lt;- c( paste0(r1, "D1"), paste0(r1, "D2"), paste0(r1, "D3") )
  ##      C1D1 C2D1 C3D1 C4D1 C5D1 C1D2 C2D2 C3D2 C4D2 C5D2 C1D3 C2D3 C3D3 C4D3 C5D3
  ##   C1    1    0    0    0    0    1    0    0    0    0    1    0    0    0    0
  ##   C2    0    1    0    0    0    0    1    0    0    0    0    1    0    0    0
  ##   C3    0    0    1    0    0    0    0    1    0    0    0    0    1    0    0
  ##   C4    0    0    0    1    0    0    0    0    1    0    0    0    0    1    0
  ##   C5    0    0    0    0    1    0    0    0    0    1    0    0    0    0    1
#-- define mirt model
I &lt;- ncol(dat)  # I=27
mirtmodel &lt;- mirt::mirt.model("
        C1D1=1-9 \n C2D1=1-9 \n  C3D1=1-9 \n  C4D1=1-9  \n  C5D1=1-9
        C1D2=10-18 \n C2D2=10-18 \n  C3D2=10-18 \n  C4D2=10-18  \n  C5D2=10-18
        C1D3=19-27 \n C2D3=19-27 \n  C3D3=19-27 \n  C4D3=19-27  \n  C5D3=19-27
        CONSTRAIN=(1-9,a1),(1-9,a2),(1-9,a3),(1-9,a4),(1-9,a5),
                    (10-18,a6),(10-18,a7),(10-18,a8),(10-18,a9),(10-18,a10),
                    (19-27,a11),(19-27,a12),(19-27,a13),(19-27,a14),(19-27,a15)
                ")
#-- get initial parameter values
mod.pars &lt;- mirt::mirt(dat, model=mirtmodel,  pars="values")
#-- redefine initial parameter values
# set all d parameters initially to zero
ind &lt;- which( ( mod.pars$name=="d" ) )
mod.pars[ ind,"value" ]  &lt;- 0
# fix item difficulties of reference items to zero
mod.pars[ ind[ c(5,14,23) ], "est"] &lt;- FALSE
mod.pars[ind,]
# initial item parameters of cluster locations (a1,...,a15)
ind &lt;- which( ( mod.pars$name %in% paste0("a", c(1,6,11) ) ) &amp; ( mod.pars$est ) )
mod.pars[ind,"value"] &lt;- -2
ind &lt;- which( ( mod.pars$name %in% paste0("a", c(1,6,11)+1 ) ) &amp; ( mod.pars$est ) )
mod.pars[ind,"value"] &lt;- -1
ind &lt;- which( ( mod.pars$name %in% paste0("a", c(1,6,11)+2 ) ) &amp; ( mod.pars$est ) )
mod.pars[ind,"value"] &lt;- 0
ind &lt;- which( ( mod.pars$name %in% paste0("a", c(1,6,11)+3 ) ) &amp; ( mod.pars$est ) )
mod.pars[ind,"value"] &lt;- 1
ind &lt;- which( ( mod.pars$name %in% paste0("a", c(1,6,11)+4 ) ) &amp; ( mod.pars$est ) )
mod.pars[ind,"value"] &lt;- 0
#-- define prior for latent class analysis
lca_prior &lt;- function(Theta,Etable){
  TP &lt;- nrow(Theta)
  if ( is.null(Etable) ){ prior &lt;- rep( 1/TP, TP ) }
  if ( ! is.null(Etable) ){
    prior &lt;- ( rowSums(Etable[, seq(1,2*I,2)]) + rowSums(Etable[,seq(2,2*I,2)]) )/I
  }
  prior &lt;- prior / sum(prior)
  return(prior)
}

#-- estimate model in mirt
mod4 &lt;- mirt::mirt(dat, mirtmodel, pars=mod.pars, verbose=TRUE,
              technical=list( customTheta=Theta, customPriorFun=lca_prior,
                    MAXQUAD=1E20) )
# correct number of estimated parameters
mod4@nest &lt;- as.integer(sum(mod.pars$est) + nrow(Theta)-1 )
# extract coefficients
# source.all(pfsirt)
cmod4 &lt;- sirt::mirt.wrapper.coef(mod4)

# estimated item difficulties
dfr &lt;- data.frame( "sim"=b, "mirt"=-cmod4$coef$d, "sirt"=mod1$item$thresh )
round( dfr, 4 )
  ##         sim    mirt    sirt
  ##   1  -1.500 -1.3782 -1.3382
  ##   2  -1.125 -1.0059 -0.9774
  ##   3  -0.750 -0.6157 -0.6016
  ##   4  -0.375 -0.2099 -0.2060
  ##   5   0.000  0.0000  0.0000
  ##   6   0.375  0.5085  0.4984
  ##   7   0.750  0.8661  0.8504
  ##   8   1.125  1.3079  1.2847
  ##   9   1.500  1.5891  1.5620
  ##   [...]

#-- reordering estimated latent clusters to make solutions comparable
#* extract estimated cluster locations from sirt
order.sirt &lt;- c(1,5,3,4,2)  # sort(order.sirt)
round(mod1$trait[,1:3],3)
dfr &lt;- data.frame( "sim"=theta.k, mod1$trait[order.sirt,1:3] )
colnames(dfr)[4:6] &lt;- paste0("sirt",1:3)
#* extract estimated cluster locations from mirt
c4 &lt;- cmod4$coef[, paste0("a",1:15) ]
c4 &lt;- apply( c4,2, FUN=function(ll){ ll[ ll!=0 ][1] } )
trait.loc &lt;- matrix(c4,5,3)
order.mirt &lt;- c(1,4,3,5,2)  # sort(order.mirt)
dfr &lt;- cbind( dfr, trait.loc[ order.mirt, ] )
colnames(dfr)[7:9] &lt;- paste0("mirt",1:3)
# compare estimated cluster locations
round(dfr,3)
  ##     sim.1 sim.2 sim.3  sirt1  sirt2  sirt3  mirt1  mirt2  mirt3
  ##   1  -3.0  -4.1  -2.8 -2.776 -3.791 -2.667 -2.856 -4.023 -2.741
  ##   5   1.7   2.3   1.8  1.791  2.330  1.844  1.817  2.373  1.869
  ##   3   0.2   0.4  -0.1  0.332  0.418 -0.046  0.349  0.421 -0.051
  ##   4   2.6   0.1  -0.9  2.601  0.171 -0.854  2.695  0.166 -0.876
  ##   2  -1.1  -0.7   0.9 -0.989 -0.605  0.957 -1.009 -0.618  0.962
#* compare estimated cluster sizes
dfr &lt;- data.frame( "sim"=pi.k, "sirt"=mod1$pi.k[order.sirt,1],
            "mirt"=mod4@Prior[[1]][ order.mirt] )
round(dfr,4)
  ##      sim   sirt   mirt
  ##   1 0.20 0.2502 0.2500
  ##   2 0.25 0.2522 0.2511
  ##   3 0.25 0.2458 0.2494
  ##   4 0.10 0.1011 0.0986
  ##   5 0.15 0.1507 0.1509

#############################################################################
# EXAMPLE 5: Dataset data.si04 from Bartolucci et al. (2012)
#############################################################################

data(data.si04)

# define reference items
ref.item &lt;- c(7,17,25,44,64)
dimensions &lt;- data.si04$itempars$dim

# estimate a Rasch latent class with 9 classes
mod1 &lt;- sirt::rasch.mirtlc( data.si04$data, Nclasses=9, dimensions=dimensions,
             modeltype="MLC1", ref.item=ref.item, glob.conv=.005, conv1=.005,
             nstarts=1, mmliter=200 )

# compare estimated distribution with simulated distribution
round( cbind( mod1$theta.k, mod1$pi.k ), 4 ) # estimated
  ##            [,1]    [,2]    [,3]    [,4]    [,5]   [,6]
  ##    [1,] -3.6043 -5.1323 -5.3022 -6.8255 -4.3611 0.1341
  ##    [2,]  0.2083 -2.7422 -2.8754 -5.3416 -2.5085 0.1573
  ##    [3,] -2.8641 -4.0272 -5.0580 -0.0340 -0.9113 0.1163
  ##    [4,] -0.3575 -2.0081 -1.7431  1.2992 -0.1616 0.0751
  ##    [5,]  2.9329  0.3662 -1.6516 -3.0284  0.1844 0.1285
  ##    [6,]  1.5092 -2.0461 -4.3093  1.0481  1.0806 0.1094
  ##    [7,]  3.9899  3.1955 -4.0010  1.8879  2.2988 0.1460
  ##    [8,]  4.3062  0.7080 -1.2324  1.4351  2.0893 0.1332
  ##    [9,]  5.0855  4.1214 -0.9141  2.2744  1.5314 0.0000

round(d2,4) # simulated
  ##         class      A      B      C      D      E     pi
  ##    [1,]     1 -3.832 -5.399 -5.793 -7.042 -4.511 0.1323
  ##    [2,]     2 -2.899 -4.217 -5.310 -0.055 -0.915 0.1162
  ##    [3,]     3 -0.376 -2.137 -1.847  1.273 -0.078 0.0752
  ##    [4,]     4  0.208 -2.934 -3.011 -5.526 -2.511 0.1583
  ##    [5,]     5  1.536 -2.137 -4.606  1.045  1.143 0.1092
  ##    [6,]     6  2.042 -0.573 -0.404 -4.331 -1.044 0.0471
  ##    [7,]     7  3.853  0.841 -2.993 -2.746  0.803 0.0822
  ##    [8,]     8  4.204  3.296 -4.328  1.892  2.419 0.1453
  ##    [9,]     9  4.466  0.700 -1.334  1.439  2.161 0.1343

## End(Not run)
</code></pre>

<hr>
<h2 id='rasch.mml2'>
Estimation of the Generalized Logistic Item Response Model,
Ramsay's Quotient Model, Nonparametric Item Response Model,
Pseudo-Likelihood Estimation and a Missing Data Item Response Model
</h2><span id='topic+rasch.mml2'></span><span id='topic+summary.rasch.mml'></span><span id='topic+plot.rasch.mml'></span><span id='topic+logLik.rasch.mml'></span><span id='topic+anova.rasch.mml'></span><span id='topic+IRT.irfprob.rasch.mml'></span><span id='topic+IRT.likelihood.rasch.mml'></span><span id='topic+IRT.posterior.rasch.mml'></span><span id='topic+IRT.modelfit.rasch.mml'></span><span id='topic+IRT.expectedCounts.rasch.mml'></span><span id='topic+summary.IRT.modelfit.rasch.mml'></span>

<h3>Description</h3>

<p>This function employs marginal maximum likelihood estimation
of item response models for dichotomous data.
First, the Rasch type model (generalized
item response model) can be estimated. The generalized logistic
link function (Stukel, 1988) can be estimated or fixed for conducting
IRT with different link functions than the logistic one. The Four-Parameter
logistic item response model is a special case of this model
(Loken &amp; Rulison, 2010). Second, Ramsay's quotient model (Ramsay, 1989)
can be estimated by specifying <code>irtmodel="ramsay.qm"</code>.
Third, quite general item response functions can be estimated
in a nonparametric framework (Rossi, Wang &amp; Ramsay, 2002).
Fourth, pseudo-likelihood estimation for fractional item responses can be
conducted for Rasch type models. Fifth, a simple two-dimensional
missing data item response model (<code>irtmodel='missing1'</code>;
Mislevy &amp; Wu, 1996) can be estimated.
</p>
<p>See Details for more explanations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.mml2( dat, theta.k=seq(-6,6,len=21), group=NULL, weights=NULL,
   constraints=NULL, glob.conv=10^(-5), parm.conv=10^(-4), mitermax=4,
   mmliter=1000, progress=TRUE,  fixed.a=rep(1,ncol(dat)),
   fixed.c=rep(0,ncol(dat)), fixed.d=rep(1,ncol(dat)),
   fixed.K=rep(3,ncol(dat)), b.init=NULL, est.a=NULL, est.b=NULL,
   est.c=NULL, est.d=NULL, min.b=-99, max.b=99, min.a=-99, max.a=99,
   min.c=0, max.c=1, min.d=0, max.d=1, prior.b=NULL, prior.a=NULL, prior.c=NULL,
   prior.d=NULL, est.K=NULL, min.K=1, max.K=20, min.delta=-20, max.delta=20,
   beta.init=NULL, min.beta=-8, pid=1:(nrow(dat)), trait.weights=NULL,  center.trait=TRUE,
   center.b=FALSE, alpha1=0, alpha2=0,est.alpha=FALSE, equal.alpha=FALSE,
   designmatrix=NULL, alpha.conv=parm.conv, numdiff.parm=0.00001,
   numdiff.alpha.parm=numdiff.parm, distribution.trait="normal", Qmatrix=NULL,
   variance.fixed=NULL, variance.init=NULL,
   mu.fixed=cbind(seq(1,ncol(Qmatrix)),rep(0,ncol(Qmatrix))),
   irtmodel="raschtype", npformula=NULL, npirt.monotone=TRUE,
   use.freqpatt=is.null(group), delta.miss=0, est.delta=rep(NA,ncol(dat)),
   nimps=0, ... )

## S3 method for class 'rasch.mml'
summary(object, file=NULL, ...)

## S3 method for class 'rasch.mml'
plot(x, items=NULL, xlim=NULL, main=NULL, ...)

## S3 method for class 'rasch.mml'
anova(object,...)

## S3 method for class 'rasch.mml'
logLik(object,...)

## S3 method for class 'rasch.mml'
IRT.irfprob(object,...)

## S3 method for class 'rasch.mml'
IRT.likelihood(object,...)

## S3 method for class 'rasch.mml'
IRT.posterior(object,...)

## S3 method for class 'rasch.mml'
IRT.modelfit(object,...)

## S3 method for class 'rasch.mml'
IRT.expectedCounts(object,...)

## S3 method for class 'IRT.modelfit.rasch.mml'
summary(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.mml2_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous item responses. <br />
For the missing data item response model (<code>irtmodel='missing1'</code>),
code item responses by <code>9</code> which should be treated by the missing
data model. Other missing responses can be coded by <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_theta.k">theta.k</code></td>
<td>

<p>Optional vector of discretized theta values. For multidimensional
IRT models with <code class="reqn">D</code> dimensions, it is a matrix with <code class="reqn">D</code> columns.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_group">group</code></td>
<td>

<p>Vector of integers with group identifiers in multiple group estimation.
The multiple group does not work for <code>irtmodel="missing1"</code>.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_weights">weights</code></td>
<td>

<p>Optional vector of person weights (sample weights).
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_constraints">constraints</code></td>
<td>

<p>Constraints on <code>b</code> parameters (item difficulties). It must be
a matrix with two columns: the first column contains item names, the
second column fixed parameter values.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_glob.conv">glob.conv</code></td>
<td>

<p>Convergence criterion for deviance
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_parm.conv">parm.conv</code></td>
<td>

<p>Convergence criterion for item parameters
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_mitermax">mitermax</code></td>
<td>

<p>Maximum number of iterations in M step. This argument does only
apply for the estimation of the <code class="reqn">b</code> parameters.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_mmliter">mmliter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_progress">progress</code></td>
<td>

<p>Should progress be displayed at the console?
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_fixed.a">fixed.a</code></td>
<td>

<p>Fixed or initial <code class="reqn">a</code> parameters
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_fixed.c">fixed.c</code></td>
<td>

<p>Fixed or initial <code class="reqn">c</code> parameters
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_fixed.d">fixed.d</code></td>
<td>

<p>Fixed or initial <code class="reqn">d</code> parameters
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_fixed.k">fixed.K</code></td>
<td>

<p>Fixed or initial <code class="reqn">K</code> parameters in Ramsay's
quotient model.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_b.init">b.init</code></td>
<td>

<p>Initial <code class="reqn">b</code> parameters
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_est.a">est.a</code></td>
<td>

<p>Vector of integers which indicate which <code class="reqn">a</code>
parameters should be estimated. Equal integers correspond
to the same estimated parameters.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_est.b">est.b</code></td>
<td>

<p>Vector of integers which indicate which <code class="reqn">b</code>
parameters should be estimated. Equal integers correspond
to the same estimated parameters.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_est.c">est.c</code></td>
<td>

<p>Vector of integers which indicate which <code class="reqn">c</code>
parameters should be estimated. Equal integers correspond
to the same estimated parameters.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_est.d">est.d</code></td>
<td>

<p>Vector of integers which indicate which <code class="reqn">d</code>
parameters should be estimated. Equal integers correspond
to the same estimated parameters.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_min.b">min.b</code></td>
<td>

<p>Minimal <code class="reqn">b</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_max.b">max.b</code></td>
<td>

<p>Maximal <code class="reqn">b</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_min.a">min.a</code></td>
<td>

<p>Minimal <code class="reqn">a</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_max.a">max.a</code></td>
<td>

<p>Maximal <code class="reqn">a</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_min.c">min.c</code></td>
<td>

<p>Minimal <code class="reqn">c</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_max.c">max.c</code></td>
<td>

<p>Maximal <code class="reqn">c</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_min.d">min.d</code></td>
<td>

<p>Minimal <code class="reqn">d</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_max.d">max.d</code></td>
<td>

<p>Maximal <code class="reqn">d</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_prior.b">prior.b</code></td>
<td>
<p>Optional prior distribution for <code class="reqn">b</code> parameters:
<code class="reqn">N(\mu, \sigma)</code>. Input is a vector of length two with parameters
<code class="reqn">\mu</code> and <code class="reqn">\sigma</code>.</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_prior.a">prior.a</code></td>
<td>
<p>Optional prior distribution for <code class="reqn">a</code> parameters:
<code class="reqn">N(\mu, \sigma)</code>. Input is a vector of length two with parameters
<code class="reqn">\mu</code> and <code class="reqn">\sigma</code>.</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_prior.c">prior.c</code></td>
<td>
<p>Optional prior distribution for <code class="reqn">c</code> parameters:
<code class="reqn">Beta(a, b)</code>. Input is a vector of length two with parameters
<code class="reqn">a</code> and <code class="reqn">b</code>.</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_prior.d">prior.d</code></td>
<td>
<p>Optional prior distribution for <code class="reqn">d</code> parameters:
<code class="reqn">Beta(a, b)</code>. Input is a vector of length two with parameters
<code class="reqn">a</code> and <code class="reqn">b</code>.</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_est.k">est.K</code></td>
<td>

<p>Vector of integers which indicate which <code class="reqn">K</code>
parameters should be estimated. Equal integers correspond
to the same estimated parameters.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_min.k">min.K</code></td>
<td>
<p> Minimal <code class="reqn">K</code> parameter to be estimated</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_max.k">max.K</code></td>
<td>
<p> Maximal <code class="reqn">K</code> parameter to be estimated</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_min.delta">min.delta</code></td>
<td>
<p> Minimal <code class="reqn">delta.miss</code> parameter to be estimated</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_max.delta">max.delta</code></td>
<td>
<p> Maximal <code class="reqn">delta.miss</code> parameter to be estimated</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_beta.init">beta.init</code></td>
<td>
<p>Optional vector of initial <code class="reqn">\beta</code> parameters</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_min.beta">min.beta</code></td>
<td>
<p>Minimum <code class="reqn">\beta</code> parameter to be estimated.</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_pid">pid</code></td>
<td>

<p>Optional vector of person identifiers
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_trait.weights">trait.weights</code></td>
<td>

<p>Optional vector of trait weights for a fixing
the trait distribution.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_center.trait">center.trait</code></td>
<td>

<p>Should the trait distribution be centered
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_center.b">center.b</code></td>
<td>
<p>An optional logical indicating whether <code class="reqn">b</code> parameters
should be centered at each dimension</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_alpha1">alpha1</code></td>
<td>

<p>Fixed or initial <code class="reqn">\alpha_1</code> parameter
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_alpha2">alpha2</code></td>
<td>

<p>Fixed or initial <code class="reqn">\alpha_2</code> parameter
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_est.alpha">est.alpha</code></td>
<td>

<p>Should <code class="reqn">\alpha</code> parameters be estimated?
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_equal.alpha">equal.alpha</code></td>
<td>

<p>Estimate <code class="reqn">\alpha</code> parameters under the
assumption <code class="reqn">\alpha_1=\alpha_2</code>?
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_designmatrix">designmatrix</code></td>
<td>

<p>Design matrix for item difficulties <code class="reqn">b</code> to estimate
linear logistic test models
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_alpha.conv">alpha.conv</code></td>
<td>

<p>Convergence criterion for <code class="reqn">\alpha</code> parameter
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_numdiff.parm">numdiff.parm</code></td>
<td>

<p>Parameter for numerical differentiation
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_numdiff.alpha.parm">numdiff.alpha.parm</code></td>
<td>

<p>Parameter for numerical differentiation for <code class="reqn">\alpha</code>
parameter
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_distribution.trait">distribution.trait</code></td>
<td>

<p>Assumed trait distribution. The default is the normal
distribution (<code>"normal"</code>). Log-linear smoothing of the
trait distribution is also possible (<code>"smooth2"</code>,
<code>"smooth3"</code> or <code>"smooth4"</code> for smoothing up to
2, 3 or 4 moments, respectively).
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_qmatrix">Qmatrix</code></td>
<td>

<p>The Q-matrix
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_variance.fixed">variance.fixed</code></td>
<td>

<p>Matrix for fixing covariance matrix (See Examples)
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_variance.init">variance.init</code></td>
<td>
<p>Optional initial covariance matrix</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_mu.fixed">mu.fixed</code></td>
<td>

<p>Matrix for fixing mean vector (See Examples)
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_irtmodel">irtmodel</code></td>
<td>

<p>Specify estimable IRT models: <code>raschtype</code> (Rasch type model),
<code>ramsay.qm</code> (Ramsay's quotient model), <code>npirt</code> (Nonparametric
item response model). If <code>npirt</code> is used as the argument
for <code>irtmodel</code>, the argument <code>npformula</code>
specifies different item response functions in the
<span class="rlang"><b>R</b></span> formula framework (like <code>"y~I(theta^2)"</code>; see Examples).
For estimating the missing data item response model, use
<code>irtmodel='missing1'</code>.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_npformula">npformula</code></td>
<td>

<p>A string or a vector which contains <span class="rlang"><b>R</b></span> formula objects for specifying
the item response function. For example, <code>"y~theta"</code> is the specification
of the 2PL model (see Details). If <code>irtmodel="npirt"</code> and <code>npformula</code>
is not specified, then an unrestricted item response functions on the
grid of <code class="reqn">\theta</code> values is estimated.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_npirt.monotone">npirt.monotone</code></td>
<td>
<p>Should nonparametrically estimated item response functions
be monotone? The default is <code>TRUE</code>. This function applies only
to <code>irtmodel='npirt'</code> and <code>npformula=NULL</code>.
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_use.freqpatt">use.freqpatt</code></td>
<td>
<p>A logical if frequencies of pattern should be used or not.
The default is <code>is.null(group)</code>. This means that for single
group analyses, frequency patterns are used but not for multiple
groups. If data processing times are large, then <code>use.freqpatt=FALSE</code>
is recommended.</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_delta.miss">delta.miss</code></td>
<td>
<p>Missingness parameter <code class="reqn">\delta</code> quantifying the meaning
of responding to an item between the two extremes of ignoring
missing responses and setting all missing responses to incorrect</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_est.delta">est.delta</code></td>
<td>
<p>Vector with indices indicating the <code class="reqn">\delta</code> parameters
to be estimated if <code>irtmodel="missing1"</code>.</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_nimps">nimps</code></td>
<td>
<p>Number of imputed datasets of item responses</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_object">object</code></td>
<td>

<p>Object of class <code>rasch.mml</code>
</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_x">x</code></td>
<td>
<p>Object of class <code>rasch.mml</code></p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_items">items</code></td>
<td>
<p>Vector of integer or item names which should be plotted</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_xlim">xlim</code></td>
<td>
<p>Specification for <code>xlim</code> in plot</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_main">main</code></td>
<td>
<p>Title of the plot</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_file">file</code></td>
<td>
<p>Optional file name for summary output</p>
</td></tr>
<tr><td><code id="rasch.mml2_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The item response function of the generalized item response model
(<code>irtmodel="raschtype"</code>; Stukel, 1988) can be written as
</p>
<p style="text-align: center;"><code class="reqn">P( X_{pi}=1 | \theta_{pd} )=c_i + (d_i - c_i ) g_{\alpha_1, \alpha_2}
[ a_i ( \theta_{pd} - b_i ) ] </code>
</p>

<p>where <code class="reqn">g</code> is the generalized logistic link function depending
on parameters <code class="reqn">\alpha_1</code> and <code class="reqn">\alpha_2</code>.
</p>
<p>For the most important link functions the specifications are (Stukel, 1988):
</p>
<p>logistic link function: <code class="reqn">\alpha_1=0</code> and <code class="reqn">\alpha_2=0</code> <br />
probit link function: <code class="reqn">\alpha_1=0.165</code> and <code class="reqn">\alpha_2=0.165</code> <br />
loglog link function: <code class="reqn">\alpha_1=-0.037</code> and <code class="reqn">\alpha_2=0.62</code> <br />
cloglog link function: <code class="reqn">\alpha_1=0.62</code> and <code class="reqn">\alpha_2=-0.037</code>
</p>
<p>See <code><a href="#topic+pgenlogis">pgenlogis</a></code> for exact transformation formulas of
the mentioned link functions. <br />
</p>
<p>A <code class="reqn">D</code>-dimensional model can also be specified
but only allows for between item dimensionality
(one item loads on one and only dimension).
Setting <code class="reqn">c_i=0</code>, <code class="reqn">d_i=1</code> and  <code class="reqn">a_i=1</code> for all items <code class="reqn">i</code>,
an additive item response model
</p>
<p style="text-align: center;"><code class="reqn">P( X_{pi}=1 | \theta_p )=g_{\alpha_1, \alpha_2} ( \theta_p - b_i  ) </code>
</p>

<p>is estimated.
</p>
<p>Ramsay's quotient model (<code>irtmodel="qm.ramsay"</code>) uses
the item response function
</p>
<p style="text-align: center;"><code class="reqn">P( X_{pi}=1 | \theta_p )=\frac{ \exp(\theta_p / b_i)}
    { K_i + \exp (\theta_p / b_i )} </code>
</p>

<p>Quite general unidimensional item response models can be estimated
in a nonparametric framework (<code>irtmodel="npirt"</code>). The response
functions are a linear combination of transformed <code class="reqn">\theta</code>
values
</p>
<p style="text-align: center;"><code class="reqn">logit[ P( X_{pi}=1 | \theta_p ) ]=Y_\theta \beta </code>
</p>

<p>Where <code class="reqn">Y_\theta</code> is a design matrix of <code class="reqn">\theta</code> and
<code class="reqn">\beta</code> are item parameters to be estimated.
The formula <code class="reqn">Y_\theta \beta</code> can be specified in the <span class="rlang"><b>R</b></span> formula
framework (see Example 3, Model 3c).
</p>
<p>Pseudo-likelihood estimation can be conducted for fractional item response data
as input (i.e. some item response <code class="reqn">x_{pi}</code> do have values
between 0 and 1). Then the pseudo-likelihood <code class="reqn">L_p</code> for person <code class="reqn">p</code> is defined as
</p>
<p style="text-align: center;"><code class="reqn"> L_p=\prod_i P_i ( \theta_p )^{x_{pi}} [1-P_i ( \theta_p )]^{(1-x_{pi})}</code>
</p>

<p>Note that for dichotomous responses this term corresponds to the ordinary
likelihood. See Example 7.
</p>
<p>A special two-dimensional missing data item response model (<code>irtmodel="missing1"</code>)
is implemented according to Mislevy and Wu (1996).
Besides an unidimensional ability <code class="reqn">\theta_p</code>,
an individual response propensity <code class="reqn">\xi_p</code> is proposed. We define
item responses <code class="reqn">X_{pi}</code> and response indicators <code class="reqn">R_{pi}</code> indicating whether
item responses <code class="reqn">X_{pi}</code> are observed or not. Denoting the logistic function
by <code class="reqn">\Psi</code>, the item response model for ability is defined as
</p>
<p style="text-align: center;"><code class="reqn"> P( X_{pi}=1  | \theta_p, \xi_p )=P( X_{pi}=1 | \theta_p )
=\Psi( a_i (\theta_p - b_i ))</code>
</p>

<p>We also define a measurement model for response indicators <code class="reqn">R_{pi}</code> which depends
on the item response <code class="reqn">X_{pi}</code> itself:
</p>
<p style="text-align: center;"><code class="reqn">P( R_{pi}=1 | X_{pi}=k, \theta_p, \xi_p )=
    P( R_{pi}=1 | X_{pi}=k, \xi_p )=
\Psi \left[  \xi_p -  \beta_i - k \delta _i  \right] \quad \mbox{ for }
\quad k=0,1</code>
</p>

<p>If <code class="reqn">\delta _i=0</code>, then the probability of responding to an item is independent
of the incompletely observed item <code class="reqn">X_{pi}</code> which is an
item response model with nonignorable missings (Holman &amp; Glas, 2005;
see also Pohl, Graefe &amp; Rose, 2014).
If <code class="reqn">\delta _i</code> is a large negative number (e.g. <code class="reqn">\delta=-100</code>), then
it follows <code class="reqn">P( R_{pi}=1 | X_{pi}=1, \theta_p, \xi_p )=1</code>
and as a consequence it holds that <code class="reqn">P(X_{pi}=1 | R_{pi}=0, \theta_p, \xi_p)=0</code>,
which is equivalent to treating
all missing item responses as incorrect. The missingness parameter
<code class="reqn">\delta</code> can be specified
by the user and studied as a sensitivity analysis under different
missing not at random assumptions or can be estimated by choosing
<code>est.delta=TRUE</code>.
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>dat</code></td>
<td>
<p>Original data frame</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Estimated item parameters in the generalized
item response model</p>
</td></tr>
<tr><td><code>item2</code></td>
<td>
<p>Estimated item parameters for Ramsay's quotient model</p>
</td></tr>
<tr><td><code>trait.distr</code></td>
<td>
<p>Discretized ability distribution points and probabilities</p>
</td></tr>
<tr><td><code>mean.trait</code></td>
<td>
<p>Estimated mean vector</p>
</td></tr>
<tr><td><code>sd.trait</code></td>
<td>
<p>Estimated standard deviations</p>
</td></tr>
<tr><td><code>skewness.trait</code></td>
<td>
<p>Estimated skewnesses</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>pjk</code></td>
<td>
<p>Estimated probabilities of item correct evaluated at <code>theta.k</code></p>
</td></tr>
<tr><td><code>rprobs</code></td>
<td>
<p>Item response probabilities like in <code>pjk</code>, but slightly
extended to accommodate all categories</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Person parameter estimates: mode (<code>MAP</code>) and
mean (<code>EAP</code>) of the posterior distribution</p>
</td></tr>
<tr><td><code>pid</code></td>
<td>
<p>Person identifier</p>
</td></tr>
<tr><td><code>ability.est.pattern</code></td>
<td>
<p>Response pattern estimates</p>
</td></tr>
<tr><td><code>f.qk.yi</code></td>
<td>
<p>Individual posterior distribution</p>
</td></tr>
<tr><td><code>f.yi.qk</code></td>
<td>
<p>Individual likelihood</p>
</td></tr>
<tr><td><code>fixed.a</code></td>
<td>
<p>Estimated <code class="reqn">a</code> parameters</p>
</td></tr>
<tr><td><code>fixed.c</code></td>
<td>
<p>Estimated <code class="reqn">c</code> parameters</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>Number of groups</p>
</td></tr>
<tr><td><code>alpha1</code></td>
<td>
<p>Estimated <code class="reqn">\alpha_1</code> parameter in generalized logistic
item response model</p>
</td></tr>
<tr><td><code>alpha2</code></td>
<td>
<p>Estimated <code class="reqn">\alpha_2</code> parameter in generalized logistic
item response model</p>
</td></tr>
<tr><td><code>se.b</code></td>
<td>
<p>Standard error of <code class="reqn">b</code> parameter in generalized logistic model
or Ramsay's quotient model</p>
</td></tr>
<tr><td><code>se.a</code></td>
<td>
<p>Standard error of <code class="reqn">a</code> parameter in generalized logistic model</p>
</td></tr>
<tr><td><code>se.c</code></td>
<td>
<p>Standard error of <code class="reqn">c</code> parameter in generalized logistic model</p>
</td></tr>
<tr><td><code>se.d</code></td>
<td>
<p>Standard error of <code class="reqn">d</code> parameter in generalized logistic model</p>
</td></tr>
<tr><td><code>se.alpha</code></td>
<td>
<p>Standard error of <code class="reqn">\alpha</code> parameter in generalized
logistic model</p>
</td></tr>
<tr><td><code>se.K</code></td>
<td>
<p>Standard error of <code class="reqn">K</code> parameter in Ramsay's quotient model</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>reliability</code></td>
<td>
<p>EAP reliability</p>
</td></tr>
<tr><td><code>irtmodel</code></td>
<td>
<p>Type of estimated item response model</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>Number of dimensions</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Mean vector (for multidimensional models)</p>
</td></tr>
<tr><td><code>Sigma.cov</code></td>
<td>
<p>Covariance matrix (for multdimensional models)</p>
</td></tr>
<tr><td><code>theta.k</code></td>
<td>
<p>Grid of discretized ability distributions</p>
</td></tr>
<tr><td><code>trait.weights</code></td>
<td>
<p>Fixed vector of probabilities for the ability distribution</p>
</td></tr>
<tr><td><code>pi.k</code></td>
<td>
<p>Trait distribution</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria</p>
</td></tr>
<tr><td><code>esttype</code></td>
<td>
<p>Estimation type: <code>ll</code> (Log-Likelihood),
<code>pseudoll</code> (Pseudo-Log-Likelihood)
</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Note</h3>

<p>Multiple group estimation is not possible for
Ramsay's quotient model and multdimensional models.
</p>


<h3>References</h3>

<p>Holman, R., &amp; Glas, C. A. (2005). Modelling non-ignorable missing-data mechanisms
with item response theory models.
<em>British Journal of Mathematical and Statistical Psychology, 58</em>(1), 1-17.
<a href="https://doi.org/10.1348/000711005X47168">doi:10.1348/000711005X47168</a>
</p>
<p>Loken, E., &amp; Rulison, K. L. (2010). Estimation of a four-parameter
item response theory model. <em>British Journal of Mathematical
and Statistical Psychology, 63</em>(3), 509-525.
<a href="https://doi.org/10.1348/000711009X474502">doi:10.1348/000711009X474502</a>
</p>
<p>Mislevy, R. J., &amp; Wu, P. K. (1996). <em>Missing responses and IRT ability
estimation: Omits, choice, time Limits, and adaptive testing</em>.
ETS Research Report ETS RR-96-30. Princeton, ETS.
<a href="https://doi.org/10.1002/j.2333-8504.1996.tb01708.x">doi:10.1002/j.2333-8504.1996.tb01708.x</a>
</p>
<p>Pohl, S., Graefe, L., &amp; Rose, N. (2014). Dealing with omitted and
not-reached items in competence tests evaluating approaches accounting for
missing responses in item response theory models.
<em>Educational and Psychological Measurement, 74</em>(3), 423-452.
<a href="https://doi.org/10.1177/0013164413504926">doi:10.1177/0013164413504926</a>
</p>
<p>Ramsay, J. O. (1989). A comparison of three simple test theory models.
<em>Psychometrika, 54</em>, 487-499.
<a href="https://doi.org/10.1007/BF02294631">doi:10.1007/BF02294631</a>
</p>
<p>Rossi, N., Wang, X., &amp; Ramsay, J. O. (2002). Nonparametric item response
function estimates with the EM algorithm.
<em>Journal of Educational and Behavioral Statistics, 27</em>(3), 291-317.
<a href="https://doi.org/10.3102/10769986027003291">doi:10.3102/10769986027003291</a>
</p>
<p>Stukel, T. A. (1988). Generalized logistic models.
<em>Journal of the American Statistical Association, 83</em>(402), 426-431.
<a href="https://doi.org/10.1080/01621459.1988.10478613">doi:10.1080/01621459.1988.10478613</a>
</p>
<p>van der Maas, H. J. L., Molenaar, D., Maris, G., Kievit, R. A., &amp;
Borsboom, D. (2011).
Cognitive psychology meets psychometric theory: On the relation between
process models for decision making and latent variable models for
individual differences.
<em>Psychological Review, 118</em>(2), 339-356.
doi: 10.1037/a0022749
</p>


<h3>See Also</h3>

<p>Simulate the generalized logistic Rasch model with <code><a href="#topic+sim.raschtype">sim.raschtype</a></code>.
</p>
<p>Simulate Ramsay's quotient model with <code><a href="#topic+sim.qm.ramsay">sim.qm.ramsay</a></code>.
</p>
<p>Simulate locally dependent item response data using <code><a href="#topic+sim.rasch.dep">sim.rasch.dep</a></code>.
</p>
<p>For an assessment of global model fit see <code><a href="#topic+modelfit.sirt">modelfit.sirt</a></code>.
</p>
<p>See <code><a href="CDM.html#topic+itemfit.sx2">CDM::itemfit.sx2</a></code> for item fit
statistics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Reading dataset
#############################################################################

library(CDM)
data(data.read)
dat &lt;- data.read
I &lt;- ncol(dat) # number of items

# Rasch model
mod1 &lt;- sirt::rasch.mml2( dat )
summary(mod1)
plot( mod1 )    # plot all items
# title 'Rasch model', display curves from -3 to 3 only for items 1, 5 and 8
plot(mod1, main="Rasch model Items 1, 5 and 8", xlim=c(-3,3), items=c(1,5,8) )

# Rasch model with constraints on item difficulties
# set item parameters of A1 and C3 equal to -2
constraints &lt;- data.frame( c("A1","C3"), c(-2,-2) )
mod1a &lt;- sirt::rasch.mml2( dat, constraints=constraints)
summary(mod1a)

# estimate equal item parameters for 1st and 11th item
est.b &lt;- 1:I
est.b[11] &lt;- 1
mod1b &lt;- sirt::rasch.mml2( dat, est.b=est.b )
summary(mod1b)

# estimate Rasch model with skew trait distribution
mod1c &lt;- sirt::rasch.mml2( dat, distribution.trait="smooth3")
summary(mod1c)

# 2PL model
mod2 &lt;- sirt::rasch.mml2( dat, est.a=1:I )
summary(mod2)
plot(mod2)    # plot 2PL item response curves

# extract individual likelihood
llmod2 &lt;- IRT.likelihood(mod2)
str(llmod2)

## Not run: 
library(CDM)
# model comparisons
CDM::IRT.compareModels(mod1, mod1c, mod2 )
anova(mod1,mod2)

# assess model fit
smod1 &lt;- IRT.modelfit(mod1)
smod2 &lt;- IRT.modelfit(mod2)
IRT.compareModels(smod1, smod2)

# set some bounds for a and b parameters
mod2a &lt;- sirt::rasch.mml2( dat, est.a=1:I, min.a=.7, max.a=2, min.b=-2 )
summary(mod2a)

# 3PL model
mod3 &lt;- sirt::rasch.mml2( dat, est.a=1:I, est.c=1:I,
              mmliter=400 # maximal 400 iterations
                 )
summary(mod3)

# 3PL model with fixed guessing paramters of .25 and equal slopes
mod4 &lt;- sirt::rasch.mml2( dat, fixed.c=rep( .25, I )   )
summary(mod4)

# 3PL model with equal guessing paramters for all items
mod5 &lt;- sirt::rasch.mml2( dat, est.c=rep(1, I )   )
summary(mod5)

# difficulty + guessing model
mod6 &lt;- sirt::rasch.mml2( dat, est.c=1:I   )
summary(mod6)

# 4PL model
mod7 &lt;- sirt::rasch.mml2( dat, est.a=1:I, est.c=1:I, est.d=1:I,
            min.d=.95, max.c=.25)
        # set minimal d and maximal c parameter to .95 and .25
summary(mod7)

# 4PL model with prior distributions
mod7b &lt;- sirt::rasch.mml2( dat, est.a=1:I, est.c=1:I, est.d=1:I, prior.a=c(1,2),
            prior.c=c(5,17), prior.d=c(20,2) )
summary(mod7b)

# constrained 4PL model
# equal slope, guessing and slipping parameters
mod8 &lt;- sirt::rasch.mml2( dat,est.c=rep(1,I), est.d=rep(1,I) )
summary(mod8)

# estimation of an item response model with an
# uniform theta distribution
theta.k &lt;- seq( 0.01, .99, len=20 )
trait.weights &lt;- rep( 1/length(theta.k), length(theta.k) )
mod9 &lt;- sirt::rasch.mml2( dat, theta.k=theta.k, trait.weights=trait.weights,
              normal.trait=FALSE, est.a=1:12  )
summary(mod9)

#############################################################################
# EXAMPLE 2: Longitudinal data
#############################################################################

data(data.long)
dat &lt;- data.long[,-1]

# define Q loading matrix
Qmatrix &lt;- matrix( 0, 12, 2 )
Qmatrix[1:6,1] &lt;- 1 # T1 items
Qmatrix[7:12,2] &lt;- 1    # T2 items

# define restrictions on item difficulties
est.b &lt;- c(1,2,3,4,5,6,   3,4,5,6,7,8)
mu.fixed &lt;- cbind(1,0)
    # set first mean to 0 for identification reasons

# Model 1: 2-dimensional Rasch model
mod1 &lt;- sirt::rasch.mml2( dat, Qmatrix=Qmatrix, miterstep=4,
            est.b=est.b,  mu.fixed=mu.fixed, mmliter=30 )
summary(mod1)
plot(mod1)
##     Plot function is only applicable for unidimensional models

## End(Not run)

#############################################################################
# EXAMPLE 3: One group, estimation of alpha parameter in the generalized logistic model
#############################################################################

# simulate theta values
set.seed(786)
N &lt;- 1000                  # number of persons
theta &lt;- stats::rnorm( N, sd=1.5 ) # N persons with SD 1.5
b &lt;- seq( -2, 2, len=15)

# simulate data
dat &lt;- sirt::sim.raschtype( theta=theta, b=b, alpha1=0, alpha2=-0.3 )

#  estimating alpha parameters
mod1 &lt;- sirt::rasch.mml2( dat, est.alpha=TRUE, mmliter=30 )
summary(mod1)
plot(mod1)

## Not run: 
# fixed alpha parameters
mod1b &lt;- sirt::rasch.mml2( dat, est.alpha=FALSE, alpha1=0, alpha2=-.3 )
summary(mod1b)

# estimation with equal alpha parameters
mod1c &lt;- sirt::rasch.mml2( dat, est.alpha=TRUE, equal.alpha=TRUE )
summary(mod1c)

# Ramsay QM
mod2a &lt;- sirt::rasch.mml2( dat, irtmodel="ramsay.qm" )
summary(mod2a)

## End(Not run)

# Ramsay QM with estimated K parameters
mod2b &lt;- sirt::rasch.mml2( dat, irtmodel="ramsay.qm", est.K=1:15, mmliter=30)
summary(mod2b)
plot(mod2b)

## Not run: 
# nonparametric estimation of monotone item response curves
mod3a &lt;- sirt::rasch.mml2( dat, irtmodel="npirt", mmliter=100,
            theta.k=seq( -3, 3, len=10) ) # evaluations at 10 theta grid points
# nonparametric ICC of first 4 items
round( t(mod3a$pjk)[1:4,], 3 )
summary(mod3a)
plot(mod3a)

# nonparametric IRT estimation without monotonicity assumption
mod3b &lt;- sirt::rasch.mml2( dat, irtmodel="npirt", mmliter=10,
            theta.k=seq( -3, 3, len=10), npirt.monotone=FALSE)
plot(mod3b)

# B-Spline estimation of ICCs
library(splines)
mod3c &lt;- sirt::rasch.mml2( dat, irtmodel="npirt",
             npformula="y~bs(theta,df=3)", theta.k=seq(-3,3,len=15) )
summary(mod3c)
round( t(mod3c$pjk)[1:6,], 3 )
plot(mod3c)

# estimation of quadratic item response functions: ~ theta + I( theta^2)
mod3d &lt;- sirt::rasch.mml2( dat, irtmodel="npirt",
             npformula="y~theta + I(theta^2)" )
summary(mod3d)
plot(mod3d)

# estimation of a stepwise ICC function
# ICCs are constant on the theta domains: [-Inf,-1], [-1,1], [1,Inf]
mod3e &lt;- sirt::rasch.mml2( dat, irtmodel="npirt",
             npformula="y~I(theta&gt;-1 )+I(theta&gt;1)" )
summary(mod3e)
plot(mod3e, xlim=c(-2.5,2.5) )

# 2PL model
mod4 &lt;- sirt::rasch.mml2( dat,  est.a=1:15)
summary(mod4)

#############################################################################
# EXAMPLE 4: Two groups, estimation of generalized logistic model
#############################################################################

# simulate generalized logistic Rasch model in two groups
set.seed(8765)
N1 &lt;- 1000     # N1=1000 persons in group 1
N2 &lt;- 500      # N2=500 persons in group 2
dat1 &lt;- sirt::sim.raschtype( theta=stats::rnorm( N1, sd=1.5 ), b=b,
            alpha1=-0.3, alpha2=0)
dat2 &lt;- sirt::sim.raschtype( theta=stats::rnorm( N2, mean=-.5, sd=.75),
            b=b, alpha1=-0.3, alpha2=0)
dat1 &lt;- rbind( dat1, dat2 )
group &lt;- c( rep(1,N1), rep(2,N2))

mod1 &lt;-  sirt::rasch.mml2( dat1, parm.conv=.0001, group=group, est.alpha=TRUE )
summary(mod1)

#############################################################################
# EXAMPLE 5: Multidimensional model
#############################################################################

#***
# (1) simulate data
set.seed(785)
library(mvtnorm)
N &lt;- 500
theta &lt;- mvtnorm::rmvnorm( N,mean=c(0,0), sigma=matrix( c(1.45,.5,.5,1.7), 2, 2 ))
I &lt;- 10
# 10 items load on the first dimension
p1 &lt;- stats::plogis( outer( theta[,1], seq( -2, 2, len=I ), "-" ) )
resp1 &lt;- 1 * ( p1 &gt; matrix( stats::runif( N*I ), nrow=N, ncol=I ) )
# 10 items load on the second dimension
p1 &lt;- stats::plogis( outer( theta[,2], seq( -2, 2, len=I ), "-" ) )
resp2 &lt;- 1 * ( p1 &gt; matrix( stats::runif( N*I ), nrow=N, ncol=I ) )
#Combine the two sets of items into one response matrix
resp &lt;- cbind(resp1,resp2)
colnames(resp) &lt;- paste("I", 1:(2*I), sep="")
dat &lt;- resp

# define Q-matrix
Qmatrix &lt;- matrix( 0, 2*I, 2 )
Qmatrix[1:I,1] &lt;- 1
Qmatrix[1:I+I,2] &lt;- 1

#***
# (2) estimation of models
# 2-dimensional Rasch model
mod1 &lt;- sirt::rasch.mml2( dat, Qmatrix=Qmatrix )
summary(mod1)

# 2-dimensional 2PL model
mod2 &lt;- sirt::rasch.mml2( dat, Qmatrix=Qmatrix, est.a=1:(2*I) )
summary(mod2)

# estimation with some fixed variances and covariances
# set variance of 1st dimension to 1 and
#  covariance to zero
variance.fixed &lt;- matrix( cbind(c(1,1), c(1,2), c(1,0)),
             byrow=FALSE, ncol=3 )
mod3 &lt;- sirt::rasch.mml2( dat, Qmatrix=Qmatrix, variance.fixed=variance.fixed )
summary(mod3)

# constraints on item difficulties
#  useful for example in longitudinal linking
est.b &lt;- c( 1:I, 1:I )
    # equal indices correspond to equally estimated item parameters
mu.fixed &lt;- cbind( 1, 0 )
mod4 &lt;- sirt::rasch.mml2( dat, Qmatrix=Qmatrix, est.b=est.b, mu.fixed=mu.fixed )
summary(mod4)

#############################################################################
# EXAMPLE 6: Two booklets with same items but with item context effects.
# Therefore, item slopes and item difficulties are assumed to be shifted in the
# second design group.
#############################################################################

#***
# simulate data
set.seed(987)
I &lt;- 10     # number of items
# define person design groups 1 and 2
n1 &lt;- 700
n2 &lt;- 1500
# item difficulties group 1
b1 &lt;- seq(-1.5,1.5,length=I)
# item slopes group 1
a1 &lt;- rep(1, I)
# simulate data group 1
dat1 &lt;- sirt::sim.raschtype( stats::rnorm(n1), b=b1, fixed.a=a1 )
colnames(dat1) &lt;- paste0("I", 1:I, "des1" )
# group 2
b2 &lt;- b1 - .15
a2 &lt;- 1.1*a1
# Item parameters are slightly transformed in the second group
# compared to the first group. This indicates possible item context effects.

# simulate data group 2
dat2 &lt;- sirt::sim.raschtype( stats::rnorm(n2), b=b2, fixed.a=a2 )
colnames(dat2) &lt;- paste0("I", 1:I, "des2" )
# define joint dataset
dat &lt;- matrix( NA, nrow=n1+n2, ncol=2*I)
colnames(dat) &lt;- c( colnames(dat1), colnames(dat2) )
dat[ 1:n1, 1:I ] &lt;- dat1
dat[ n1 + 1:n2, I + 1:I ] &lt;- dat2
# define group identifier
group &lt;- c( rep(1,n1), rep(2,n2) )

#***
# Model 1: Rasch model two groups
itemindex &lt;- rep( 1:I, 2 )
mod1 &lt;- sirt::rasch.mml2( dat, group=group, est.b=itemindex )
summary(mod1)

#***
# Model 2: two item slope groups and designmatrix for intercepts
designmatrix &lt;- matrix( 0, 2*I, I+1)
designmatrix[ ( 1:I )+ I,1:I] &lt;- designmatrix[1:I,1:I] &lt;- diag(I)
designmatrix[ ( 1:I )+ I,I+1] &lt;- 1
mod2 &lt;- sirt::rasch.mml2( dat, est.a=rep(1:2,each=I), designmatrix=designmatrix )
summary(mod2)

#############################################################################
# EXAMPLE 7: PIRLS dataset with missing responses
#############################################################################

data(data.pirlsmissing)
items &lt;- grep( "R31", colnames(data.pirlsmissing), value=TRUE )
I &lt;- length(items)
dat &lt;- data.pirlsmissing

#****
# Model 1: recode missing responses as missing (missing are ignorable)

# data recoding
dat1 &lt;- dat
dat1[ dat1==9 ] &lt;- NA
# estimate Rasch model
mod1 &lt;- sirt::rasch.mml2( dat1[,items], weights=dat$studwgt, group=dat$country )
summary(mod1)
##   Mean=0 0.341 -0.134 0.219
##   SD=1.142 1.166 1.197 0.959

#****
# Model 2: recode missing responses as wrong

# data recoding
dat2 &lt;- dat
dat2[ dat2==9 ] &lt;- 0
# estimate Rasch model
mod2 &lt;- sirt::rasch.mml2( dat2[,items], weights=dat$studwgt, group=dat$country )
summary(mod2)
  ##   Mean=0 0.413 -0.172 0.446
  ##   SD=1.199 1.263 1.32 0.996

#****
# Model 3: recode missing responses as rho * P_i( theta ) and
#          apply pseudo-log-likelihood estimation
# Missing item responses are predicted by the model implied probability
# P_i( theta ) where theta is the ability estimate when ignoring missings (Model 1)
# and rho is an adjustment parameter. rho=0 is equivalent to Model 2 (treating
# missing as wrong) and rho=1 is equivalent to Model 1 (treating missing as ignorable).

# data recoding
dat3 &lt;- dat
# simulate theta estimate from posterior distribution
theta &lt;- stats::rnorm( nrow(dat3), mean=mod1$person$EAP, sd=mod1$person$SE.EAP )
rho &lt;- .3   # define a rho parameter value of .3
for (ii in items){
    ind &lt;- which( dat[,ii]==9 )
    dat3[ind,ii] &lt;- rho*stats::plogis( theta[ind] - mod1$item$b[ which( items==ii ) ] )
                }

# estimate Rasch model
mod3 &lt;- sirt::rasch.mml2( dat3[,items], weights=dat$studwgt, group=dat$country )
summary(mod3)
  ##   Mean=0 0.392 -0.153 0.38
  ##   SD=1.154 1.209 1.246 0.973

#****
# Model 4: simulate missing responses as rho * P_i( theta )
# The definition is the same as in Model 3. But it is now assumed
# that the missing responses are 'latent responses'.
set.seed(789)

# data recoding
dat4 &lt;- dat
# simulate theta estimate from posterior distribution
theta &lt;- stats::rnorm( nrow(dat4), mean=mod1$person$EAP, sd=mod1$person$SE.EAP )
rho &lt;- .3   # define a rho parameter value of .3
for (ii in items){
    ind &lt;- which( dat[,ii]==9 )
    p3 &lt;- rho*stats::plogis( theta[ind] - mod1$item$b[ which( items==ii ) ] )
    dat4[ ind, ii ] &lt;- 1*( stats::runif( length(ind), 0, 1 ) &lt; p3)
                }

# estimate Rasch model
mod4 &lt;- sirt::rasch.mml2( dat4[,items], weights=dat$studwgt, group=dat$country )
summary(mod4)
  ##   Mean=0 0.396 -0.156 0.382
  ##   SD=1.16 1.216 1.253 0.979

#****
# Model 5: recode missing responses for multiple choice items with four alternatives
#          to 1/4 and apply pseudo-log-likelihood estimation.
#          Missings for constructed response items are treated as incorrect.

# data recoding
dat5 &lt;- dat
items_mc &lt;- items[ substring( items, 7,7)=="M" ]
items_cr &lt;- items[ substring( items, 7,7)=="C" ]
for (ii in items_mc){
    ind &lt;- which( dat[,ii]==9 )
    dat5[ind,ii] &lt;- 1/4
                }
for (ii in items_cr){
    ind &lt;- which( dat[,ii]==9 )
    dat5[ind,ii] &lt;- 0
                }

# estimate Rasch model
mod5 &lt;- sirt::rasch.mml2( dat5[,items], weights=dat$studwgt, group=dat$country )
summary(mod5)
  ##   Mean=0 0.411 -0.165 0.435
  ##   SD=1.19 1.245 1.293 0.995

#*** For the following analyses, we ignore sample weights and the
#    country grouping.
data(data.pirlsmissing)
items &lt;- grep( "R31", colnames(data.pirlsmissing), value=TRUE )
dat &lt;- data.pirlsmissing
dat1 &lt;- dat
dat1[ dat1==9 ] &lt;- 0

#*** Model 6: estimate item difficulties assuming incorrect missing data treatment
mod6 &lt;- sirt::rasch.mml2( dat1[,items], mmliter=50 )
summary(mod6)

#*** Model 7: reestimate model with constrained item difficulties
I &lt;- length(items)
constraints &lt;- cbind( 1:I, mod6$item$b )
mod7 &lt;- sirt::rasch.mml2( dat1[,items], constraints=constraints)
summary(mod7)

#*** Model 8: score all missings responses as missing items
dat2 &lt;- dat[,items]
dat2[ dat2==9 ] &lt;- NA
mod8 &lt;- sirt::rasch.mml2( dat2, constraints=constraints, mu.fixed=NULL )
summary(mod8)

#*** Model 9: estimate missing data model 'missing1' assuming a missingness
#       parameter delta.miss of zero
dat2 &lt;-  dat[,items]    # note that missing item responses must be defined by 9
mod9 &lt;- sirt::rasch.mml2( dat2, constraints=constraints, irtmodel="missing1",
            theta.k=seq(-5,5,len=10), delta.miss=0, mitermax=4, mu.fixed=NULL )
summary(mod9)

#*** Model 10: estimate missing data model with a large negative missing delta parameter
#=&gt; This model is equivalent to treating missing responses as wrong
mod10 &lt;- sirt::rasch.mml2( dat2, constraints=constraints, irtmodel="missing1",
             theta.k=seq(-5, 5, len=10), delta.miss=-10, mitermax=4, mmliter=200,
             mu.fixed=NULL )
summary(mod10)

#*** Model 11: choose a missingness delta parameter of -1
mod11 &lt;- sirt::rasch.mml2( dat2, constraints=constraints, irtmodel="missing1",
             theta.k=seq(-5, 5, len=10), delta.miss=-1, mitermax=4,
             mmliter=200, mu.fixed=NULL )
summary(mod11)

#*** Model 12: estimate joint delta parameter
mod12 &lt;- sirt::rasch.mml2( dat2, irtmodel="missing1", mu.fixed=cbind( c(1,2), 0 ),
             theta.k=seq(-8, 8, len=10), delta.miss=0, mitermax=4,
             mmliter=30, est.delta=rep(1,I)  )
summary(mod12)

#*** Model 13: estimate delta parameter in item groups defined by item format
est.delta &lt;- 1 + 1 * ( substring( colnames(dat2),7,7 )=="M" )
mod13 &lt;- sirt::rasch.mml2( dat2, irtmodel="missing1", mu.fixed=cbind( c(1,2), 0 ),
             theta.k=seq(-8, 8, len=10), delta.miss=0, mitermax=4,
             mmliter=30, est.delta=est.delta  )
summary(mod13)

#*** Model 14: estimate item specific delta parameter
mod14 &lt;- sirt::rasch.mml2( dat2, irtmodel="missing1", mu.fixed=cbind( c(1,2), 0 ),
             theta.k=seq(-8, 8, len=10), delta.miss=0, mitermax=4,
             mmliter=30, est.delta=1:I  )
summary(mod14)

#############################################################################
# EXAMPLE 8: Comparison of different models for polytomous data
#############################################################################

data(data.Students, package="CDM")
head(data.Students)
dat &lt;- data.Students[, paste0("act",1:5) ]
I &lt;- ncol(dat)

#**************************************************
#*** Model 1: Partial Credit Model (PCM)

#*** Model 1a: PCM in TAM
mod1a &lt;- TAM::tam.mml( dat )
summary(mod1a)

#*** Model 1b: PCM in sirt
mod1b &lt;- sirt::rm.facets( dat )
summary(mod1b)

#*** Model 1c: PCM in mirt
mod1c &lt;- mirt::mirt( dat, 1, itemtype=rep("Rasch",I), verbose=TRUE )
print(mod1c)

#**************************************************
#*** Model 2: Sequential Model (SM): Equal Loadings

#*** Model 2a: SM in sirt
dat1 &lt;- CDM::sequential.items(dat)
resp &lt;- dat1$dat.expand
iteminfo &lt;- dat1$iteminfo
# fit model
mod2a &lt;- sirt::rasch.mml2( resp )
summary(mod2a)

#**************************************************
#*** Model 3: Sequential Model (SM): Different Loadings

#*** Model 3a: SM in sirt
mod3a &lt;- sirt::rasch.mml2( resp, est.a=iteminfo$itemindex )
summary(mod3a)

#**************************************************
#*** Model 4: Generalized partial credit model (GPCM)

#*** Model 4a: GPCM in TAM
mod4a &lt;- TAM::tam.mml.2pl( dat, irtmodel="GPCM")
summary(mod4a)

#**************************************************
#*** Model 5: Graded response model (GRM)

#*** Model 5a: GRM in mirt
mod5a &lt;- mirt::mirt( dat, 1, itemtype=rep("graded",I), verbose=TRUE)
print(mod5a)

# model comparison
logLik(mod1a);logLik(mod1b);mod1c@logLik  # PCM
logLik(mod2a)   # SM (Rasch)
logLik(mod3a)   # SM (GPCM)
logLik(mod4a)   # GPCM
mod5a@logLik    # GRM

## End(Not run)
</code></pre>

<hr>
<h2 id='rasch.pairwise'>
Pairwise Estimation Method of the Rasch Model
</h2><span id='topic+rasch.pairwise'></span><span id='topic+summary.rasch.pairwise'></span>

<h3>Description</h3>

<p>This function estimates the Rasch model with a minimum chi
square estimation method (cited in Fischer, 2007, p. 544)
which is a pairwise conditional likelihood estimation approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.pairwise(dat, weights=NULL, conv=1e-04, maxiter=3000, progress=TRUE,
        b.init=NULL, zerosum=FALSE, power=1, direct_optim=TRUE)

## S3 method for class 'rasch.pairwise'
summary(object, digits=3, file=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.pairwise_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous item responses
</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_weights">weights</code></td>
<td>
<p>Optional vector of sampling weights</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_progress">progress</code></td>
<td>

<p>Display iteration progress?
</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_b.init">b.init</code></td>
<td>

<p>An optional vector of length <code class="reqn">I</code> of item difficulties
</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_zerosum">zerosum</code></td>
<td>
<p>Optional logical indicating whether item difficulties
should be centered in each iteration. The default is that
no centering is conducted.</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_power">power</code></td>
<td>
<p>Power used for computing pairwise response probabilities like
in row averaging approach</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_direct_optim">direct_optim</code></td>
<td>
<p>Logical indicating whether least squares criterion
funcion should be minimized with <code><a href="stats.html#topic+nlminb">stats::nlminb</a></code>
</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_object">object</code></td>
<td>

<p>Object of class <code>rasch.pairwise</code>
</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_digits">digits</code></td>
<td>
<p>Number of digits after decimal for rounding</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_file">file</code></td>
<td>
<p>Optional file name for summary output</p>
</td></tr>
<tr><td><code id="rasch.pairwise_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>rasch.pairwise</code> with following entries
</p>
<table>
<tr><td><code>b</code></td>
<td>
<p>Item difficulties</p>
</td></tr>
<tr><td><code>eps</code></td>
<td>
<p>Exponentiated item difficulties, i.e. <code>eps=exp(-b)</code></p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>conv</code></td>
<td>
<p>Convergence criterion</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Original data frame</p>
</td></tr>
<tr><td><code>freq.ij</code></td>
<td>
<p>Frequency table of all item pairs</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Summary table of item parameters</p>
</td></tr>
</table>


<h3>References</h3>

<p>Fischer, G. H. (2007). Rasch models.
In C. R. Rao and S. Sinharay (Eds.), <em>Handbook of Statistics</em>,
Vol. 26 (pp. 515-585). Amsterdam: Elsevier.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+summary.rasch.pairwise">summary.rasch.pairwise</a></code> for a summary.
</p>
<p>A slightly different implementation of this conditional pairwise method
is implemented in <br /> <code><a href="#topic+rasch.pairwise.itemcluster">rasch.pairwise.itemcluster</a></code>.
</p>
<p>Pairwise marginal likelihood estimation (also labeled as pseudolikelihood
estimation) can be conducted with <code><a href="#topic+rasch.pml3">rasch.pml3</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Reading data set | pairwise estimation Rasch model
#############################################################################

data(data.read)
dat &lt;- data.read

#*** Model 1: no constraint on item difficulties
mod1 &lt;- sirt::rasch.pairwise(dat)
summary(mod1)

#*** Model 2: sum constraint on item difficulties
mod2 &lt;- sirt::rasch.pairwise(dat, zerosum=TRUE)
summary(mod2)

## Not run: 
#** obtain standard errors by bootstrap
mod2$item$b   # extract item difficulties

# Bootstrap of item difficulties
boot_pw &lt;- function(data, indices ){
    dd &lt;- data[ indices, ] # bootstrap of indices
    mod &lt;- sirt::rasch.pairwise( dat=dd, zerosum=TRUE, progress=FALSE)
    return(mod$item$b)
}
set.seed(986)
library(boot)
bmod2 &lt;- boot::boot(data=dat, statistic=boot_pw, R=999 )
print(bmod2)
summary(bmod2)
# quantiles for bootstrap sample (and confidence interval)
apply(bmod2$t, 2, stats::quantile, probs=c(.025, .5, .975) )

## End(Not run)
</code></pre>

<hr>
<h2 id='rasch.pairwise.itemcluster'>
Pairwise Estimation of the Rasch Model for Locally Dependent Items
</h2><span id='topic+rasch.pairwise.itemcluster'></span>

<h3>Description</h3>

<p>This function uses pairwise conditional likelihood estimation
for estimating item parameters in the Rasch model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.pairwise.itemcluster(dat, itemcluster=NULL, b.fixed=NULL, weights=NULL,
    conv=1e-05, maxiter=3000, progress=TRUE, b.init=NULL, zerosum=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.pairwise.itemcluster_+3A_dat">dat</code></td>
<td>
<p>An <code class="reqn">N \times I</code> data frame.
Missing responses are allowed and must be recoded as <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="rasch.pairwise.itemcluster_+3A_itemcluster">itemcluster</code></td>
<td>

<p>Optional integer vector of itemcluster (see Examples). Different
integers correspond to different item clusters. No item cluster
is set as default.
</p>
</td></tr>
<tr><td><code id="rasch.pairwise.itemcluster_+3A_b.fixed">b.fixed</code></td>
<td>

<p>Matrix for fixing item parameters. The first columns contains the item
(number or name), the second column the parameter to be fixed.
</p>
</td></tr>
<tr><td><code id="rasch.pairwise.itemcluster_+3A_weights">weights</code></td>
<td>
<p>Optional Vector of sampling weights</p>
</td></tr>
<tr><td><code id="rasch.pairwise.itemcluster_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion in maximal absolute parameter change
</p>
</td></tr>
<tr><td><code id="rasch.pairwise.itemcluster_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximal number of iterations
</p>
</td></tr>
<tr><td><code id="rasch.pairwise.itemcluster_+3A_progress">progress</code></td>
<td>

<p>A logical which displays progress. Default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="rasch.pairwise.itemcluster_+3A_b.init">b.init</code></td>
<td>

<p>Vector of initial item difficulty estimates. Default is <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="rasch.pairwise.itemcluster_+3A_zerosum">zerosum</code></td>
<td>
<p>Optional logical indicating whether item difficulties
should be centered in each iteration. The default is that
no centering is conducted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is an adaptation of the algorithm of van der Linden and Eggen (1986). Only item pairs
of different item clusters are taken into account for item difficulty estimation.
Therefore, the problem of locally dependent items within each itemcluster is (almost)
eliminated (see Examples below) because contributions of local dependencies
do not appear in the pairwise likelihood terms. In detail, the estimation rests
on observed frequency tables of items <code class="reqn">i</code> and <code class="reqn">j</code> and therefore on conditional
probabilities
</p>
<p style="text-align: center;"><code class="reqn"> \frac{P(X_i=x, X_j=y)}{P(X_i + X_j=1 )} \quad \mbox{with}
\quad x,y=0,1 \quad \mbox{and} \quad x+y=1 </code>
</p>

<p>If for some item pair <code class="reqn">(i,j)</code> a higher positive (or negative) correlation
is expected (i.e. deviation from local dependence), then this pair is
removed from estimation. Clearly, there is a loss in precision but item
parameters can be less biased.
</p>


<h3>Value</h3>

<p>Object of class <code>rasch.pairwise</code> with elements
</p>
<table>
<tr><td><code>b</code></td>
<td>

<p>Vector of item difficulties
</p>
</td></tr>
<tr><td><code>item</code></td>
<td>

<p>Data frame of item parameters (<code class="reqn">N</code>, <code class="reqn">p</code> and item difficulty)
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>No standard errors are provided by this function. Use resampling
methods for conducting statistical inference.
</p>
<p>Formulas for asymptotic standard errors of this pairwise estimation method are
described in Zwinderman (1995).
</p>


<h3>References</h3>

<p>van der Linden, W. J., &amp; Eggen, T. J. H. M. (1986). <em>An empirical
Bayes approach to item banking</em>. Research Report 86-6, University of Twente.
</p>
<p>Zwinderman, A. H. (1995). Pairwise parameter estimation in Rasch models.
<em>Applied Psychological Measurement, 19</em>, 369-375.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rasch.pairwise">rasch.pairwise</a></code>, <code><a href="#topic+summary.rasch.pairwise">summary.rasch.pairwise</a></code>,
</p>
<p>Pairwise marginal likelihood estimation (also labeled as pseudolikelihood
estimation) can be conducted with <code><a href="#topic+rasch.pml3">rasch.pml3</a></code>.
</p>
<p>Other estimation methods are implemented in <code><a href="#topic+rasch.copula2">rasch.copula2</a></code> or
<code><a href="#topic+rasch.mml2">rasch.mml2</a></code>.
</p>
<p>For simulation of locally dependent data see <code><a href="#topic+sim.rasch.dep">sim.rasch.dep</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Example with locally dependent items
#      12 Items: Cluster 1 -&gt; Items 1,...,4
#                Cluster 2 -&gt; Items 6,...,9
#                Cluster 3 -&gt; Items 10,11,12
#############################################################################

set.seed(7896)
I &lt;- 12                             # number of items
n &lt;- 5000                           # number of persons
b &lt;- seq(-2,2, len=I)               # item difficulties
bsamp &lt;- b &lt;- sample(b)             # sample item difficulties
theta &lt;- stats::rnorm( n, sd=1 ) # person abilities
# itemcluster
itemcluster &lt;- rep(0,I)
itemcluster[ 1:4 ] &lt;- 1
itemcluster[ 6:9 ] &lt;- 2
itemcluster[ 10:12 ] &lt;- 3
# residual correlations
rho &lt;- c( .55, .25, .45 )

# simulate data
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("I", seq(1,ncol(dat)), sep="")

# estimation with pairwise Rasch model
mod3 &lt;- sirt::rasch.pairwise( dat )
summary(mod3)

# use item cluster in rasch pairwise estimation
mod &lt;- sirt::rasch.pairwise.itemcluster( dat=dat, itemcluster=itemcluster )
summary(mod)

## Not run: 
# Rasch MML estimation
mod4 &lt;- sirt::rasch.mml2( dat )
summary(mod4)

# Rasch Copula estimation
mod5 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster )
summary(mod5)

# compare different item parameter estimates
M1 &lt;- cbind( "true.b"=bsamp, "b.rasch"=mod4$item$b, "b.rasch.copula"=mod5$item$thresh,
         "b.rasch.pairwise"=mod3$b, "b.rasch.pairwise.cluster"=mod$b )
# center item difficulties
M1 &lt;- scale( M1, scale=FALSE )
round( M1, 3 )
round( apply( M1, 2, stats::sd ), 3 )

#  Below the output of the example is presented.
#  The rasch.pairwise.itemcluster is pretty close to the estimate in the Rasch copula model.

  ##   &gt; round( M1, 3 )
  ##       true.b b.rasch b.rasch.copula b.rasch.pairwise b.rasch.pairwise.cluster
  ##   I1   0.545   0.561          0.526            0.628                    0.524
  ##   I2  -0.182  -0.168         -0.174           -0.121                   -0.156
  ##   I3  -0.909  -0.957         -0.867           -0.971                   -0.899
  ##   I4  -1.636  -1.726         -1.625           -1.765                   -1.611
  ##   I5   1.636   1.751          1.648            1.694                    1.649
  ##   I6   0.909   0.892          0.836            0.898                    0.827
  ##   I7  -2.000  -2.134         -2.020           -2.051                   -2.000
  ##   I8  -1.273  -1.355         -1.252           -1.303                   -1.271
  ##   I9  -0.545  -0.637         -0.589           -0.581                   -0.598
  ##   I10  1.273   1.378          1.252            1.308                    1.276
  ##   I11  0.182   0.241          0.226            0.109                    0.232
  ##   I12  2.000   2.155          2.039            2.154                    2.026
  ##   &gt; round( apply( M1, 2, sd ), 3 )
  ##                     true.b                  b.rasch           b.rasch.copula
  ##                      1.311                    1.398                    1.310
  ##      b.rasch.pairwise    b.rasch.pairwise.cluster
  ##                 1.373                       1.310

# set item parameters of first item to 0 and of second item to -0.7
b.fixed &lt;- cbind( c(1,2), c(0,-.7) )
mod5 &lt;- sirt::rasch.pairwise.itemcluster( dat=dat, b.fixed=b.fixed,
             itemcluster=itemcluster )
# difference between estimations 'mod' and 'mod5'
dfr &lt;- cbind( mod5$item$b, mod$item$b )
plot( mod5$item$b, mod$item$b, pch=16)
apply( dfr, 1, diff)

## End(Not run)
</code></pre>

<hr>
<h2 id='rasch.pml3'>
Pairwise Marginal Likelihood Estimation for the Probit Rasch Model
</h2><span id='topic+rasch.pml3'></span><span id='topic+summary.rasch.pml'></span>

<h3>Description</h3>

<p>This function estimates unidimensional 1PL and 2PL models with
the probit link using pairwise marginal maximum likelihood
estimation (PMML; Renard, Molenberghs &amp; Geys, 2004).
Item pairs within an itemcluster can be excluded from the
pairwise likelihood (argument <code>itemcluster</code>).
The other alternative is to model a residual
error structure with itemclusters (argument <code>error.corr</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.pml3(dat, est.b=seq(1, ncol(dat)), est.a=rep(0,ncol(dat)),
    est.sigma=TRUE, itemcluster=NULL, weight=rep(1, nrow(dat)), numdiff.parm=0.001,
    b.init=NULL, a.init=NULL,  sigma.init=NULL, error.corr=0*diag( 1, ncol(dat) ),
    err.constraintM=NULL, err.constraintV=NULL, glob.conv=10^(-6), conv1=10^(-4),
    pmliter=300, progress=TRUE, use.maxincrement=TRUE )

## S3 method for class 'rasch.pml'
summary(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.pml3_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous item responses
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_est.b">est.b</code></td>
<td>

<p>Vector of integers of length <code class="reqn">I</code>. Same integers mean that the
corresponding items do have the same item difficulty <code>b</code>.
Entries of <code>0</code> mean fixing item parameters to values
specified in <code>b.init</code>.
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_est.a">est.a</code></td>
<td>

<p>Vector of integers of length <code class="reqn">I</code>. Same integers mean that the
corresponding items do have the same item slope <code>a</code>.
Entries of <code>0</code> mean fixing item parameters to values
specified in <code>a.init</code>.
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_est.sigma">est.sigma</code></td>
<td>

<p>Should sigma (the trait standard deviation) be estimated?
The default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_itemcluster">itemcluster</code></td>
<td>

<p>Optional vector of length <code class="reqn">I</code> of integers which indicates itemclusters.
Same integers correspond to the same itemcluster. An entry of <code>0</code>
correspond to an item which is not included in any itemcluster.
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_weight">weight</code></td>
<td>

<p>Optional vector of person weights
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_numdiff.parm">numdiff.parm</code></td>
<td>

<p>Step parameter for numerical differentiation
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_b.init">b.init</code></td>
<td>

<p>Initial or fixed item difficulty
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_a.init">a.init</code></td>
<td>

<p>Initial or fixed item slopes
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_sigma.init">sigma.init</code></td>
<td>

<p>Initial or fixed trait standard deviation
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_error.corr">error.corr</code></td>
<td>

<p>An optional <code class="reqn">I \times I</code> integer matrix
which defines the estimation of residual correlations.
Entries of zero indicate that the corresponding
residual correlation should not be estimated.
Integers which differ from zero indicate correlations to be estimated.
All entries with an equal integer are estimated by the same residual
correlation. The default of <code>error.corr</code> is a diagonal matrix
which means that no residual correlation is estimated. If <code>error.corr</code>
deviates from this default, then the argument <code>itemcluster</code> is set
to <code>NULL</code>.<br />
If some error correlations are estimated, then no itempairs in
<code>itemcluster</code> can be excluded from the pairwise modeling.
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_err.constraintm">err.constraintM</code></td>
<td>

<p>An optional <code class="reqn">P \times L</code> matrix where <code class="reqn">P</code> denotes
the number of item pairs in pseudolikelihood estimation
and <code class="reqn">L</code> is the number of linear constraints for residual
correlations (see Details).
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_err.constraintv">err.constraintV</code></td>
<td>

<p>An optional <code class="reqn">L \times 1</code> matrix with specified values
for linear constraints on residual correlations (see Details).
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_glob.conv">glob.conv</code></td>
<td>

<p>Global convergence criterion
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_conv1">conv1</code></td>
<td>

<p>Convergence criterion for model parameters
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_pmliter">pmliter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_progress">progress</code></td>
<td>

<p>Display progress?
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_use.maxincrement">use.maxincrement</code></td>
<td>
<p>Optional logical whether increments in
slope parameters should be controlled in size in iterations.
The default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_object">object</code></td>
<td>

<p>Object of class <code>rasch.pml</code>
</p>
</td></tr>
<tr><td><code id="rasch.pml3_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probit item response model can be estimated with this function:
</p>
<p style="text-align: center;"><code class="reqn">P(X_{pi}=1|\theta_p )=\Phi( a_i  \theta_p - b_i ) \quad, \quad
    \theta_p \sim N ( 0, \sigma^2 )</code>
</p>

<p>where <code class="reqn">\Phi</code> denotes the normal distribution function. This model can
also be expressed as a latent variable model which assumes
a latent response tendency <code class="reqn">X_{pi}^\ast</code> which is equal to
1 if <code class="reqn">X_{pi}&gt; - b_i</code> and otherwise zero. If <code class="reqn">\epsilon_{pi}</code> is
standard normally distributed, then
</p>
<p style="text-align: center;"><code class="reqn">X_{pi}^{\ast}=a_i \theta_p - b_i + \epsilon_{pi} </code>
</p>

<p>An arbitrary pattern of residual correlations between
<code class="reqn">\epsilon_{pi}</code> and <code class="reqn">\epsilon_{pj}</code> for item pairs <code class="reqn">i</code>
and <code class="reqn">j</code> can be imposed using the <code>error.corr</code> argument.
</p>
<p>Linear constraints <code class="reqn">Me=v</code> on residual correlations
<code class="reqn">e=Cov( \epsilon_{pi}, \epsilon_{pj})_{ij}</code> (in a vectorized form) can be
specified using the arguments <code>err.constraintM</code> (matrix <code class="reqn">M</code>)
and <code>err.constraintV</code> (vector <code class="reqn">v</code>). The estimation
is described in Neuhaus (1996).
</p>
<p>For the pseudo likelihood information criterion (PLIC)
see Stanford and Raftery (2002).
</p>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>item</code></td>
<td>
<p>Data frame with estimated item parameters</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>Pseudolikelihood multiplied by minus 2</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>Estimated item difficulties</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Estimated standard deviation</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Original dataset</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Data frame with information criteria (sample size,
number of estimated parameters, pseudolikelihood
information criterion <code>PLIC</code>)</p>
</td></tr>
<tr><td><code>link</code></td>
<td>
<p>Used link function (only probit is permitted)</p>
</td></tr>
<tr><td><code>itempairs</code></td>
<td>
<p>Estimated statistics of item pairs</p>
</td></tr>
<tr><td><code>error.corr</code></td>
<td>
<p>Estimated error correlation matrix</p>
</td></tr>
<tr><td><code>eps.corr</code></td>
<td>

<p>Vectorized error correlation matrix
</p>
</td></tr>
<tr><td><code>omega.rel</code></td>
<td>

<p>Reliability of the sum score according to Green and Yang (2009).
If some item pairs are excluded in the estimation, the residual
correlation for these item pairs is assumed to be zero.
</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Note</h3>

<p>This function needs the <span class="pkg">combinat</span> library.
</p>


<h3>References</h3>

<p>Green, S. B., &amp; Yang, Y. (2009). Reliability of summed item
scores using structural equation modeling: An alternative to
coefficient alpha. <em>Psychometrika, 74</em>, 155-167.
</p>
<p>Neuhaus, W. (1996). Optimal estimation under
linear constraints. <em>Astin Bulletin, 26</em>, 233-245.
</p>
<p>Renard, D., Molenberghs, G., &amp; Geys, H. (2004). A pairwise likelihood
approach to estimation in multilevel probit models. <em>Computational Statistics
&amp; Data Analysis, 44</em>, 649-667.
</p>
<p>Stanford, D. C., &amp; Raftery, A. E. (2002).
Approximate Bayes factors for image segmentation:
The pseudolikelihood information criterion (PLIC).
<em>IEEE Transactions on Pattern Analysis and
Machine Intelligence, 24</em>, 1517-1520.
</p>


<h3>See Also</h3>

<p>Get a summary of <code>rasch.pml3</code> with <code><a href="#topic+summary.rasch.pml">summary.rasch.pml</a></code>.
</p>
<p>For simulation of locally dependent items see <code><a href="#topic+sim.rasch.dep">sim.rasch.dep</a></code>.
</p>
<p>For pairwise conditional likelihood estimation see <code><a href="#topic+rasch.pairwise">rasch.pairwise</a></code>
or <code><a href="#topic+rasch.pairwise.itemcluster">rasch.pairwise.itemcluster</a></code>.
</p>
<p>For an assessment of global model fit see <code><a href="#topic+modelfit.sirt">modelfit.sirt</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Reading data set
#############################################################################

data(data.read)
dat &lt;- data.read

#******
# Model 1: Rasch model with PML estimation
mod1 &lt;- sirt::rasch.pml3( dat )
summary(mod1)

#******
# Model 2: Excluding item pairs with local dependence
#          from bivariate composite likelihood
itemcluster &lt;- rep( 1:3, each=4)
mod2 &lt;- sirt::rasch.pml3( dat, itemcluster=itemcluster )
summary(mod2)

## Not run: 
#*****
# Model 3: Modelling error correlations:
#          joint residual correlations for each itemcluster
error.corr &lt;- diag(1,ncol(dat))
for ( ii in 1:3){
    ind.ii &lt;- which( itemcluster==ii )
    error.corr[ ind.ii, ind.ii ] &lt;- ii
        }
# estimate the model with error correlations
mod3 &lt;- sirt::rasch.pml3( dat, error.corr=error.corr )
summary(mod3)

#****
# Model 4: model separate residual correlations
I &lt;- ncol(error.corr)
error.corr1 &lt;- matrix( 1:(I*I), ncol=I )
error.corr &lt;- error.corr1 * ( error.corr &gt; 0 )
# estimate the model with error correlations
mod4 &lt;- sirt::rasch.pml3( dat, error.corr=error.corr )
summary(mod4)

#****
# Model 5:  assume equal item difficulties:
# b_1=b_7 and b_2=b_12
# fix item difficulty of the 6th item to .1
est.b &lt;- 1:I
est.b[7] &lt;- 1; est.b[12] &lt;- 2 ; est.b[6] &lt;- 0
b.init &lt;- rep( 0, I ) ; b.init[6] &lt;- .1
mod5 &lt;- sirt::rasch.pml3( dat, est.b=est.b, b.init=b.init)
summary(mod5)

#****
# Model 6: estimate three item slope groups
est.a &lt;- rep(1:3, each=4 )
mod6 &lt;- sirt::rasch.pml3( dat, est.a=est.a, est.sigma=0)
summary(mod6)

#############################################################################
# EXAMPLE 2: PISA reading
#############################################################################

data(data.pisaRead)
dat &lt;- data.pisaRead$data

# select items
dat &lt;- dat[, substring(colnames(dat),1,1)=="R" ]

#******
# Model 1: Rasch model with PML estimation
mod1 &lt;- sirt::rasch.pml3( as.matrix(dat) )
  ## Trait SD (Logit Link) : 1.419

#******
# Model 2: Model correlations within testlets
error.corr &lt;- diag(1,ncol(dat))
testlets &lt;- paste( data.pisaRead$item$testlet )
itemcluster &lt;- match( testlets, unique(testlets ) )
for ( ii in 1:(length(unique(testlets))) ){
    ind.ii &lt;- which( itemcluster==ii )
    error.corr[ ind.ii, ind.ii ] &lt;- ii
        }
# estimate the model with error correlations
mod2 &lt;- sirt::rasch.pml3( dat, error.corr=error.corr )
  ## Trait SD (Logit Link) : 1.384

#****
# Model 3: model separate residual correlations
I &lt;- ncol(error.corr)
error.corr1 &lt;- matrix( 1:(I*I), ncol=I )
error.corr &lt;- error.corr1 * ( error.corr &gt; 0 )
# estimate the model with error correlations
mod3 &lt;- sirt::rasch.pml3( dat, error.corr=error.corr )
  ## Trait SD (Logit Link) : 1.384

#############################################################################
# EXAMPLE 3: 10 locally independent items
#############################################################################

#**********
# simulate some data
set.seed(554)
N &lt;- 500    # persons
I &lt;- 10        # items
theta &lt;- stats::rnorm(N,sd=1.3 )    # trait SD of 1.3
b &lt;- seq(-2, 2, length=I) # item difficulties

# simulate data from the Rasch model
dat &lt;- sirt::sim.raschtype( theta=theta, b=b )

# estimation with rasch.pml and probit link
mod1 &lt;- sirt::rasch.pml3( dat )
summary(mod1)

# estimation with rasch.mml2 function
mod2 &lt;- sirt::rasch.mml2( dat )

# estimate item parameters for groups with five item parameters each
est.b &lt;- rep( 1:(I/2), each=2 )
mod3 &lt;- sirt::rasch.pml3( dat, est.b=est.b )
summary(mod3)

# compare parameter estimates
summary(mod1)
summary(mod2)
summary(mod3)

#############################################################################
# EXAMPLE 4: 11 items and 2 item clusters with 2 and 3 items
#############################################################################

set.seed(5698)
I &lt;- 11                             # number of items
n &lt;- 5000                           # number of persons
b &lt;- seq(-2,2, len=I)               # item difficulties
theta &lt;- stats::rnorm( n, sd=1 ) # person abilities
# itemcluster
itemcluster &lt;- rep(0,I)
itemcluster[c(3,5)] &lt;- 1
itemcluster[c(2,4,9)] &lt;- 2
# residual correlations
rho &lt;- c( .7, .5 )

# simulate data (under the logit link)
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("I", seq(1,ncol(dat)), sep="")

#***
# Model 1: estimation using the Rasch model (with probit link)
mod1 &lt;- sirt::rasch.pml3( dat )
#***
# Model 2: estimation when pairs of locally dependent items are eliminated
mod2 &lt;- sirt::rasch.pml3( dat, itemcluster=itemcluster)

#***
# Model 3: Positive correlations within testlets
est.corrs &lt;- diag( 1, I )
est.corrs[ c(3,5), c(3,5) ] &lt;- 2
est.corrs[ c(2,4,9), c(2,4,9) ] &lt;- 3
mod3 &lt;- sirt::rasch.pml3( dat, error.corr=est.corrs )

#***
# Model 4: Negative correlations between testlets
est.corrs &lt;- diag( 1, I )
est.corrs[ c(3,5), c(2,4,9) ] &lt;- 2
est.corrs[ c(2,4,9), c(3,5) ] &lt;- 2
mod4 &lt;- sirt::rasch.pml3( dat, error.corr=est.corrs )

#***
# Model 5: sum constraint of zero within and between testlets
est.corrs &lt;- matrix( 1:(I*I),  I, I )
cluster2 &lt;- c(2,4,9)
est.corrs[ setdiff( 1:I, c(cluster2)),  ] &lt;- 0
est.corrs[, setdiff( 1:I, c(cluster2))  ] &lt;- 0
# define an error constraint matrix
itempairs0 &lt;- mod4$itempairs
IP &lt;- nrow(itempairs0)
err.constraint &lt;- matrix( 0, IP, 1 )
err.constraint[ ( itempairs0$item1 %in% cluster2 )
       &amp; ( itempairs0$item2 %in% cluster2 ), 1 ] &lt;- 1
# set sum of error covariances to 1.2
err.constraintV &lt;- matrix(3*.4,1,1)

mod5 &lt;- sirt::rasch.pml3( dat, error.corr=est.corrs,
         err.constraintM=err.constraint, err.constraintV=err.constraintV)

#****
# Model 6: Constraint on sum of all correlations
est.corrs &lt;- matrix( 1:(I*I),  I, I )
# define an error constraint matrix
itempairs0 &lt;- mod4$itempairs
IP &lt;- nrow(itempairs0)
# define two side conditions
err.constraint &lt;- matrix( 0, IP, 2 )
err.constraintV &lt;- matrix( 0, 2, 1)
# sum of all correlations is zero
err.constraint[, 1 ] &lt;- 1
err.constraintV[1,1] &lt;- 0
# sum of items cluster c(1,2,3) is 0
cluster2 &lt;- c(1,2,3)
err.constraint[ ( itempairs0$item1 %in%  cluster2 )
       &amp; ( itempairs0$item2 %in% cluster2 ), 2 ] &lt;- 1
err.constraintV[2,1] &lt;- 0

mod6 &lt;- sirt::rasch.pml3( dat, error.corr=est.corrs,
    err.constraintM=err.constraint,  err.constraintV=err.constraintV)
summary(mod6)

#############################################################################
# EXAMPLE 5: 10 Items: Cluster 1 -&gt; Items 1,2
#         Cluster 2 -&gt; Items 3,4,5;   Cluster 3 -&gt; Items 7,8,9
#############################################################################

set.seed(7650)
I &lt;- 10                             # number of items
n &lt;- 5000                           # number of persons
b &lt;- seq(-2,2, len=I)               # item difficulties
bsamp &lt;- b &lt;- sample(b)             # sample item difficulties
theta &lt;- stats::rnorm( n, sd=1 ) # person abilities
# define itemcluster
itemcluster &lt;- rep(0,I)
itemcluster[ 1:2 ] &lt;- 1
itemcluster[ 3:5 ] &lt;- 2
itemcluster[ 7:9 ] &lt;- 3
# define residual correlations
rho &lt;- c( .55, .35, .45)

# simulate data
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("I", seq(1,ncol(dat)), sep="")

#***
# Model 1: residual correlation (equal within item clusters)
# define a matrix of integers for estimating error correlations
error.corr &lt;- diag(1,ncol(dat))
for ( ii in 1:3){
    ind.ii &lt;- which( itemcluster==ii )
    error.corr[ ind.ii, ind.ii ] &lt;- ii
        }
# estimate the model
mod1 &lt;- sirt::rasch.pml3( dat, error.corr=error.corr )

#***
# Model 2: residual correlation (different within item clusters)
# define again a matrix of integers for estimating error correlations
error.corr &lt;- diag(1,ncol(dat))
for ( ii in 1:3){
    ind.ii &lt;- which( itemcluster==ii )
    error.corr[ ind.ii, ind.ii ] &lt;- ii
        }
I &lt;- ncol(error.corr)
error.corr1 &lt;- matrix( 1:(I*I), ncol=I )
error.corr &lt;- error.corr1 * ( error.corr &gt; 0 )
# estimate the model
mod2 &lt;- sirt::rasch.pml3( dat, error.corr=error.corr )

#***
# Model 3: eliminate item pairs within itemclusters for PML estimation
mod3 &lt;- sirt::rasch.pml3( dat, itemcluster=itemcluster )

#***
# Model 4: Rasch model ignoring dependency
mod4 &lt;- sirt::rasch.pml3( dat )

# compare different models
summary(mod1)
summary(mod2)
summary(mod3)
summary(mod4)

## End(Not run)
</code></pre>

<hr>
<h2 id='rasch.prox'>
PROX Estimation Method for the Rasch Model
</h2><span id='topic+rasch.prox'></span>

<h3>Description</h3>

<p>This function estimates the Rasch model using the PROX algorithm
(cited in Wright &amp; Stone, 1999).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.prox(dat, dat.resp=1 - is.na(dat), freq=rep(1,nrow(dat)),
    conv=0.001, maxiter=30, progress=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.prox_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous response data. <code>NA</code>s
are not allowed and must be indicated by zero entries in the
response indicator matrix <code>dat.resp</code>.
</p>
</td></tr>
<tr><td><code id="rasch.prox_+3A_dat.resp">dat.resp</code></td>
<td>

<p>An <code class="reqn">N \times I</code> indicator data frame of nonmissing item responses.
</p>
</td></tr>
<tr><td><code id="rasch.prox_+3A_freq">freq</code></td>
<td>

<p>A vector of frequencies (or weights) of all rows in data frame <code>dat</code>.
</p>
</td></tr>
<tr><td><code id="rasch.prox_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion for item parameters
</p>
</td></tr>
<tr><td><code id="rasch.prox_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="rasch.prox_+3A_progress">progress</code></td>
<td>

<p>Display progress?
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>b</code></td>
<td>
<p>Estimated item difficulties</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>Estimated person abilities</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>sigma.i</code></td>
<td>
<p>Item standard deviations</p>
</td></tr>
<tr><td><code>sigma.n</code></td>
<td>
<p>Person standard deviations</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wright, B., &amp; Stone, W. (1999). <em>Measurement Essentials</em>.
Wilmington: Wide Range.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: PROX data.read
#############################################################################

data(data.read)
mod &lt;- sirt::rasch.prox( data.read )
mod$b       # item difficulties
</code></pre>

<hr>
<h2 id='rasch.va'>
Estimation of the Rasch Model with Variational Approximation
</h2><span id='topic+rasch.va'></span>

<h3>Description</h3>

<p>This function estimates the Rasch model by the estimation method
of variational approximation (Rijmen &amp; Vomlel, 2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.va(dat, globconv=0.001, maxiter=1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch.va_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous item responses
</p>
</td></tr>
<tr><td><code id="rasch.va_+3A_globconv">globconv</code></td>
<td>

<p>Convergence criterion for item parameters
</p>
</td></tr>
<tr><td><code id="rasch.va_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximal number of iterations
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>sig</code></td>
<td>
<p>Standard deviation of the trait</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Data frame with item parameters</p>
</td></tr>
<tr><td><code>xsi.ij</code></td>
<td>
<p>Data frame with variational parameters <code class="reqn">\xi_{ij}</code></p>
</td></tr>
<tr><td><code>mu.i</code></td>
<td>
<p>Vector with individual means <code class="reqn">\mu_i</code></p>
</td></tr>
<tr><td><code>sigma2.i</code></td>
<td>
<p>Vector with individual variances <code class="reqn">\sigma_i^2</code></p>
</td></tr>
</table>


<h3>References</h3>

<p>Rijmen, F., &amp; Vomlel, J. (2008). Assessing the performance of
variational methods for mixed logistic regression models.
<em>Journal of Statistical Computation and Simulation,
78</em>, 765-779.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Rasch model
#############################################################################
set.seed(8706)
N &lt;- 5000
I &lt;- 20
dat &lt;- sirt::sim.raschtype( stats::rnorm(N,sd=1.3), b=seq(-2,2,len=I) )

# estimation via variational approximation
mod1 &lt;- sirt::rasch.va(dat)

# estimation via marginal maximum likelihood
mod2 &lt;- sirt::rasch.mml2(dat)

# estmation via joint maximum likelihood
mod3 &lt;- sirt::rasch.jml(dat)

# compare sigma
round( c( mod1$sig, mod2$sd.trait ), 3 )
## [1] 1.222 1.314

# compare b
round( cbind( mod1$item$b, mod2$item$b, mod3$item$itemdiff), 3 )
##         [,1]   [,2]   [,3]
##  [1,] -1.898 -1.967 -2.090
##  [2,] -1.776 -1.841 -1.954
##  [3,] -1.561 -1.618 -1.715
##  [4,] -1.326 -1.375 -1.455
##  [5,] -1.121 -1.163 -1.228
</code></pre>

<hr>
<h2 id='reliability.nonlinearSEM'>
Estimation of Reliability for Confirmatory Factor Analyses
Based on Dichotomous Data
</h2><span id='topic+reliability.nonlinearSEM'></span>

<h3>Description</h3>

<p>This function estimates a model based reliability using confirmatory factor
analysis (Green &amp; Yang, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reliability.nonlinearSEM(facloadings, thresh, resid.cov=NULL, cor.factors=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reliability.nonlinearSEM_+3A_facloadings">facloadings</code></td>
<td>

<p>Matrix of factor loadings
</p>
</td></tr>
<tr><td><code id="reliability.nonlinearSEM_+3A_thresh">thresh</code></td>
<td>

<p>Vector of thresholds
</p>
</td></tr>
<tr><td><code id="reliability.nonlinearSEM_+3A_resid.cov">resid.cov</code></td>
<td>
<p>Matrix of residual covariances</p>
</td></tr>
<tr><td><code id="reliability.nonlinearSEM_+3A_cor.factors">cor.factors</code></td>
<td>

<p>Optional matrix of covariances (correlations) between factors. The
default is a diagonal matrix with variances of 1.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list. The reliability is the list element <code>omega.rel</code>
</p>


<h3>Note</h3>

<p>This function needs the <span class="pkg">mvtnorm</span> package.
</p>


<h3>References</h3>

<p>Green, S. B., &amp; Yang, Y. (2009). Reliability of summed item
scores using structural equation modeling: An alternative to
coefficient alpha. <em>Psychometrika, 74</em>, 155-167.
</p>


<h3>See Also</h3>

<p>This function is used in <code><a href="#topic+greenyang.reliability">greenyang.reliability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Reading data set
#############################################################################
data(data.read)
dat &lt;- data.read
I &lt;- ncol(dat)

# define item clusters
itemcluster &lt;- rep( 1:3, each=4)
error.corr &lt;- diag(1,ncol(dat))
for ( ii in 1:3){
    ind.ii &lt;- which( itemcluster==ii )
    error.corr[ ind.ii, ind.ii ] &lt;- ii
        }
# estimate the model with error correlations
mod1 &lt;- sirt::rasch.pml3( dat, error.corr=error.corr)
summary(mod1)

# extract item parameters
thresh &lt;- - matrix( mod1$item$a * mod1$item$b, I, 1 )
A &lt;- matrix( mod1$item$a * mod1$item$sigma, I, 1 )
# extract estimated correlation matrix
corM &lt;- mod1$eps.corrM
# compute standardized factor loadings
facA &lt;- 1 / sqrt( A^2 + 1 )
resvar &lt;- 1 - facA^2
covM &lt;- outer( sqrt(resvar[,1]), sqrt(resvar[,1] ) ) * corM
facloadings &lt;- A *facA

# estimate reliability
rel1 &lt;- sirt::reliability.nonlinearSEM( facloadings=facloadings, thresh=thresh,
           resid.cov=covM)
rel1$omega.rel
</code></pre>

<hr>
<h2 id='resp_groupwise'>
Creates Group-Wise Item Response Dataset
</h2><span id='topic+resp_groupwise'></span>

<h3>Description</h3>

<p>Creates group-wise item response dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resp_groupwise(resp, group, items_group)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="resp_groupwise_+3A_resp">resp</code></td>
<td>

<p>Dataset with item responses
</p>
</td></tr>
<tr><td><code id="resp_groupwise_+3A_group">group</code></td>
<td>

<p>Vector of group identifiers
</p>
</td></tr>
<tr><td><code id="resp_groupwise_+3A_items_group">items_group</code></td>
<td>

<p>List containing vectors of groups for each item which should be made group-specific
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Dataset
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
# EXAMPLE 1: Toy dataset
#############################################################################

library(CDM)
library(TAM)

data(data.ex11, package="TAM")
dat &lt;- data.ex11
dat[ dat==9 ] &lt;- 0
resp &lt;- dat[,-1]

# group labels
booklets &lt;- sort( unique(paste(dat$booklet)))

#- fit initial model
mod0 &lt;- TAM::tam.mml( resp, group=dat$booklet)
summary(mod0)

# fit statistics
fmod &lt;- IRT.RMSD(mod)
stat &lt;- abs(fmod$MD[,-1])
stat[ is.na( fmod$RMSD[,2:4] ) ] &lt;- NA
thresh &lt;- .01
round(stat,3)
# define list define groups for group-specific items
items_group &lt;- apply( stat, 1, FUN=function(ll){
                v1 &lt;- booklets[ which( ll &gt; thresh ) ]
                v1[ ! is.na(v1) ]  } )

#- create extended response dataset
dat2 &lt;- sirt::resp_groupwise(resp=resp, group=paste(dat$booklet), items_group=items_group)
colSums( ! is.na(dat2) )

#- fit model for extended response dataset
mod2 &lt;- TAM::tam.mml( dat2, group=dat$booklet)
summary(mod2)

## End(Not run)
</code></pre>

<hr>
<h2 id='rinvgamma2'>
Inverse Gamma Distribution in Prior Sample Size Parameterization
</h2><span id='topic+rinvgamma2'></span><span id='topic+dinvgamma2'></span>

<h3>Description</h3>

<p>Random draws and density of inverse gamma distribution parameterized
in prior sample size <code>n0</code> and prior variance <code>var0</code>
(see Gelman et al., 2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rinvgamma2(n, n0, var0)

dinvgamma2(x, n0, var0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rinvgamma2_+3A_n">n</code></td>
<td>

<p>Number of draws for inverse gamma distribution
</p>
</td></tr>
<tr><td><code id="rinvgamma2_+3A_n0">n0</code></td>
<td>

<p>Prior sample size
</p>
</td></tr>
<tr><td><code id="rinvgamma2_+3A_var0">var0</code></td>
<td>

<p>Prior variance
</p>
</td></tr>
<tr><td><code id="rinvgamma2_+3A_x">x</code></td>
<td>
<p>Vector with numeric values for density evaluation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector containing random draws or density values
</p>


<h3>References</h3>

<p>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A.,
&amp; Rubin, D. B. (2014).
<em>Bayesian data analysis</em> (Vol. 3). Boca Raton, FL, USA: Chapman &amp; Hall/CRC.
</p>


<h3>See Also</h3>

<p><code>MCMCpack::rinvgamma</code>,
<code><a href="stats.html#topic+GammaDist">stats::rgamma</a></code>,
<code>MCMCpack::dinvgamma</code>,
<code><a href="stats.html#topic+GammaDist">stats::dgamma</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Inverse gamma distribution
#############################################################################

# prior sample size of 100 and prior variance of 1.5
n0 &lt;- 100
var0 &lt;- 1.5

# 100 random draws
y1 &lt;- sirt::rinvgamma2( n=100, n0, var0 )
summary(y1)
graphics::hist(y1)

# density y at grid x
x &lt;- seq( 0, 2, len=100 )
y &lt;- sirt::dinvgamma2( x, n0, var0 )
graphics::plot( x, y, type="l")
</code></pre>

<hr>
<h2 id='rm.facets'>
Rater Facets Models with Item/Rater Intercepts and Slopes
</h2><span id='topic+rm.facets'></span><span id='topic+summary.rm.facets'></span><span id='topic+logLik.rm.facets'></span><span id='topic+anova.rm.facets'></span><span id='topic+IRT.irfprob.rm.facets'></span><span id='topic+IRT.likelihood.rm.facets'></span><span id='topic+IRT.posterior.rm.facets'></span><span id='topic+IRT.modelfit.rm.facets'></span><span id='topic+IRT.factor.scores.rm.facets'></span><span id='topic+summary.IRT.modelfit.rm.facets'></span><span id='topic+rm_proc_data'></span>

<h3>Description</h3>

<p>This function estimates the unidimensional rater facets model (Lincare, 1994)
and an extension to slopes (see Details; Robitzsch &amp; Steinfeld, 2018). The estimation
is conducted by an EM algorithm employing marginal
maximum likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rm.facets(dat, pid=NULL, rater=NULL, Qmatrix=NULL, theta.k=seq(-9, 9, len=30),
    est.b.rater=TRUE, est.a.item=FALSE, est.a.rater=FALSE, rater_item_int=FALSE,
    est.mean=FALSE, tau.item.fixed=NULL, a.item.fixed=NULL, b.rater.fixed=NULL,
    a.rater.fixed=NULL, b.rater.center=2, a.rater.center=2, a.item.center=2, a_lower=.05,
    a_upper=10, reference_rater=NULL, max.b.increment=1, numdiff.parm=0.00001,
    maxdevchange=0.1, globconv=0.001, maxiter=1000, msteps=4, mstepconv=0.001,
    PEM=FALSE, PEM_itermax=maxiter)

## S3 method for class 'rm.facets'
summary(object, file=NULL, ...)

## S3 method for class 'rm.facets'
anova(object,...)

## S3 method for class 'rm.facets'
logLik(object,...)

## S3 method for class 'rm.facets'
IRT.irfprob(object,...)

## S3 method for class 'rm.facets'
IRT.factor.scores(object, type="EAP", ...)

## S3 method for class 'rm.facets'
IRT.likelihood(object,...)

## S3 method for class 'rm.facets'
IRT.posterior(object,...)

## S3 method for class 'rm.facets'
IRT.modelfit(object,...)

## S3 method for class 'IRT.modelfit.rm.facets'
summary(object, ...)

## function for processing data
rm_proc_data( dat, pid, rater, rater_item_int=FALSE, reference_rater=NULL )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rm.facets_+3A_dat">dat</code></td>
<td>

<p>Original data frame. Ratings on variables must be in rows,
i.e. every row corresponds to a person-rater combination.
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_pid">pid</code></td>
<td>

<p>Person identifier.
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_rater">rater</code></td>
<td>

<p>Rater identifier
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_qmatrix">Qmatrix</code></td>
<td>

<p>An optional Q-matrix. If this matrix is not provided,
then by default the ordinary scoring of categories
(from 0 to the maximum score of <code class="reqn">K</code>) is used.
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_theta.k">theta.k</code></td>
<td>

<p>A grid of theta values for the ability distribution.
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_est.b.rater">est.b.rater</code></td>
<td>

<p>Should the rater severities <code class="reqn">b_r</code> be estimated?
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_est.a.item">est.a.item</code></td>
<td>

<p>Should the item slopes <code class="reqn">a_i</code> be estimated?
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_est.a.rater">est.a.rater</code></td>
<td>

<p>Should the rater slopes <code class="reqn">a_r</code> be estimated?
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_rater_item_int">rater_item_int</code></td>
<td>
<p>Logical indicating whether rater-item-interactions
should be modeled.</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_est.mean">est.mean</code></td>
<td>
<p>Optional logical indicating whether the mean of the
trait distribution should be estimated.</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_tau.item.fixed">tau.item.fixed</code></td>
<td>
<p>Matrix with fixed <code class="reqn">\tau</code> parameters. Non-fixed
parameters must be declared by <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_a.item.fixed">a.item.fixed</code></td>
<td>
<p>Vector with fixed item discriminations</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_b.rater.fixed">b.rater.fixed</code></td>
<td>
<p>Vector with fixed rater intercept parameters</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_a.rater.fixed">a.rater.fixed</code></td>
<td>
<p>Vector with fixed rater discrimination parameters</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_b.rater.center">b.rater.center</code></td>
<td>
<p>Centering method for rater intercept parameters. The
value <code>0</code> corresponds to no centering, the values <code>1</code> and
<code>2</code> to different methods to ensure that they sum to zero.</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_a.rater.center">a.rater.center</code></td>
<td>
<p>Centering method for rater discrimination parameters. The
value <code>0</code> corresponds to no centering, the values <code>1</code> and
<code>2</code> to different methods to ensure that their product equals one.</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_a.item.center">a.item.center</code></td>
<td>
<p>Centering method for item discrimination parameters. The
value <code>0</code> corresponds to no centering, the values <code>1</code> and
<code>2</code> to different methods to ensure that their product equals one.</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_a_lower">a_lower</code></td>
<td>
<p>Lower bound for <code class="reqn">a</code> parameters</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_a_upper">a_upper</code></td>
<td>
<p>Upper bound for <code class="reqn">a</code> parameters</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_reference_rater">reference_rater</code></td>
<td>
<p>Identifier for rater as a reference rater for which
a fixed rater mean of 0 and a fixed rater slope of 1 is assumed.</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_max.b.increment">max.b.increment</code></td>
<td>

<p>Maximum increment of item parameters during estimation
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_numdiff.parm">numdiff.parm</code></td>
<td>

<p>Numerical differentiation step width
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_maxdevchange">maxdevchange</code></td>
<td>

<p>Maximum relative deviance change as a convergence criterion
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_globconv">globconv</code></td>
<td>

<p>Maximum parameter change
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_msteps">msteps</code></td>
<td>

<p>Maximum number of iterations during an M step
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_mstepconv">mstepconv</code></td>
<td>

<p>Convergence criterion in an M step
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_pem">PEM</code></td>
<td>
<p>Logical indicating whether the P-EM acceleration should be
applied (Berlinet &amp; Roland, 2012).</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_pem_itermax">PEM_itermax</code></td>
<td>
<p>Number of iterations in which the P-EM method should be
applied.</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_object">object</code></td>
<td>

<p>Object of class <code>rm.facets</code>
</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_file">file</code></td>
<td>
<p>Optional file name in which summary should be written.</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_type">type</code></td>
<td>
<p>Factor score estimation method. Factor score types
<code>"EAP"</code>, <code>"MLE"</code> and <code>"WLE"</code> are supported.</p>
</td></tr>
<tr><td><code id="rm.facets_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function models ratings <code class="reqn">X_{pri}</code>
for person <code class="reqn">p</code>, rater <code class="reqn">r</code> and item <code class="reqn">i</code>
and category <code class="reqn">k</code> (see also Robitzsch &amp; Steinfeld, 2018; Uto &amp; Ueno, 2010; Wu, 2017)
</p>
<p style="text-align: center;"><code class="reqn">P( X_{pri}=k | \theta_p ) \propto
    \exp( a_i a_r q_{ik} \theta_p - q_{ik} b_r -   \tau_{ik} ) \quad,
    \quad \theta_p \sim N( 0, \sigma^2 )</code>
</p>

<p>By default, the scores in the <code class="reqn">Q</code> matrix are
<code class="reqn">q_{ik}=k</code>. Item slopes <code class="reqn">a_i</code> and rater slopes
<code class="reqn">a_r</code> are standardized such that their product equals
one, i.e. <code class="reqn"> \prod_i a_i=\prod_r a_r=1</code>.
</p>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria and number of parameters</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Data frame with item parameters</p>
</td></tr>
<tr><td><code>rater</code></td>
<td>
<p>Data frame with rater parameters</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with person parameters: EAP and corresponding
standard errors</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliability</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Mean of the trait distribution</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Standard deviation of the trait distribution</p>
</td></tr>
<tr><td><code>theta.k</code></td>
<td>
<p>Grid of theta values</p>
</td></tr>
<tr><td><code>pi.k</code></td>
<td>
<p>Fitted distribution at <code>theta.k</code> values</p>
</td></tr>
<tr><td><code>tau.item</code></td>
<td>
<p>Item parameters <code class="reqn">\tau_{ik}</code></p>
</td></tr>
<tr><td><code>se.tau.item</code></td>
<td>
<p>Standard error of item parameters <code class="reqn">\tau_{ik}</code></p>
</td></tr>
<tr><td><code>a.item</code></td>
<td>
<p>Item slopes <code class="reqn">a_i</code></p>
</td></tr>
<tr><td><code>se.a.item</code></td>
<td>
<p>Standard error of item slopes <code class="reqn">a_i</code></p>
</td></tr>
<tr><td><code>delta.item</code></td>
<td>
<p>Delta item parameter. See
<code><a href="#topic+pcm.conversion">pcm.conversion</a></code>.
</p>
</td></tr>
<tr><td><code>b.rater</code></td>
<td>
<p>Rater severity parameter <code class="reqn">b_r</code></p>
</td></tr>
<tr><td><code>se.b.rater</code></td>
<td>
<p>Standard error of rater severity parameter <code class="reqn">b_r</code></p>
</td></tr>
<tr><td><code>a.rater</code></td>
<td>
<p>Rater slope parameter <code class="reqn">a_r</code></p>
</td></tr>
<tr><td><code>se.a.rater</code></td>
<td>
<p>Standard error of rater slope parameter <code class="reqn">a_r</code></p>
</td></tr>
<tr><td><code>f.yi.qk</code></td>
<td>
<p>Individual likelihood</p>
</td></tr>
<tr><td><code>f.qk.yi</code></td>
<td>
<p>Individual posterior distribution</p>
</td></tr>
<tr><td><code>probs</code></td>
<td>
<p>Item probabilities at grid <code>theta.k</code></p>
</td></tr>
<tr><td><code>n.ik</code></td>
<td>
<p>Expected counts</p>
</td></tr>
<tr><td><code>maxK</code></td>
<td>
<p>Maximum number of categories</p>
</td></tr>
<tr><td><code>procdata</code></td>
<td>
<p>Processed data</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>ipars.dat2</code></td>
<td>
<p>Item parameters for expanded dataset <code>dat2</code></p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If the trait standard deviation <code>sigma</code> strongly
differs from 1, then a user should investigate the sensitivity
of results using different theta integration points <code>theta.k</code>.
</p>


<h3>References</h3>

<p>Berlinet, A. F., &amp; Roland, C. (2012).
Acceleration of the EM algorithm: P-EM versus epsilon algorithm.
<em>Computational Statistics &amp; Data Analysis, 56</em>(12), 4122-4137.
</p>
<p>Linacre, J. M. (1994). <em>Many-Facet Rasch Measurement</em>.
Chicago: MESA Press.
</p>
<p>Robitzsch, A., &amp; Steinfeld, J. (2018). Item response models for human ratings: Overview,
estimation methods, and implementation in R.
<em>Psychological Test and Assessment Modeling, 60</em>(1), 101-139.
</p>
<p>Uto, M., &amp; Ueno, M. (2016). Item response theory for peer assessment.
<em>IEEE Transactions on Learning Technologies, 9</em>(2), 157-170.
</p>
<p>Wu, M. (2017). Some IRT-based analyses for interpreting rater effects.
<em>Psychological Test and Assessment Modeling, 59</em>(4), 453-470.
</p>


<h3>See Also</h3>

<p>See also the <span class="pkg">TAM</span> package for the estimation
of more complicated facet models.
</p>
<p>See <code><a href="#topic+rm.sdt">rm.sdt</a></code> for estimating a hierarchical rater model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Partial Credit Model and Generalized partial credit model
#                   5 items and 1 rater
#############################################################################
data(data.ratings1)
dat &lt;- data.ratings1

# select rater db01
dat &lt;- dat[ paste(dat$rater)=="db01", ]

#****  Model 1: Partial Credit Model
mod1 &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], pid=dat$idstud )

#****  Model 2: Generalized Partial Credit Model
mod2 &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ],  pid=dat$idstud, est.a.item=TRUE)

summary(mod1)
summary(mod2)

## Not run: 
#############################################################################
# EXAMPLE 2: Facets Model: 5 items, 7 raters
#############################################################################

data(data.ratings1)
dat &lt;- data.ratings1

#****  Model 1: Partial Credit Model: no rater effects
mod1 &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], rater=dat$rater,
             pid=dat$idstud, est.b.rater=FALSE )

#****  Model 2: Partial Credit Model: intercept rater effects
mod2 &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], rater=dat$rater, pid=dat$idstud)

# extract individual likelihood
lmod1 &lt;- IRT.likelihood(mod1)
str(lmod1)
# likelihood value
logLik(mod1)
# extract item response functions
pmod1 &lt;- IRT.irfprob(mod1)
str(pmod1)
# model comparison
anova(mod1,mod2)
# absolute and relative model fit
smod1 &lt;- IRT.modelfit(mod1)
summary(smod1)
smod2 &lt;- IRT.modelfit(mod2)
summary(smod2)
IRT.compareModels( smod1, smod2 )
# extract factor scores (EAP is the default)
IRT.factor.scores(mod2)
# extract WLEs
IRT.factor.scores(mod2, type="WLE")

#****  Model 2a: compare results with TAM package
#   Results should be similar to Model 2
library(TAM)
mod2a &lt;- TAM::tam.mml.mfr( resp=dat[, paste0( "k",1:5) ],
             facets=dat[, "rater", drop=FALSE],
             pid=dat$pid, formulaA=~ item*step + rater )

#****  Model 2b: Partial Credit Model: some fixed parameters
# fix rater parameters for raters 1, 4 and 5
b.rater.fixed &lt;- rep(NA,7)
b.rater.fixed[ c(1,4,5) ] &lt;- c(1,-.8,0)  # fixed parameters
# fix item parameters of first and second item
tau.item.fixed &lt;- round( mod2$tau.item, 1 )    # use parameters from mod2
tau.item.fixed[ 3:5, ] &lt;- NA    # free item parameters of items 3, 4 and 5
mod2b &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], rater=dat$rater,
             b.rater.fixed=b.rater.fixed, tau.item.fixed=tau.item.fixed,
             est.mean=TRUE, pid=dat$idstud)
summary(mod2b)

#****  Model 3: estimated rater slopes
mod3 &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], rater=dat$rater,
            est.a.rater=TRUE)

#****  Model 4: estimated item slopes
mod4 &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], rater=dat$rater,
             pid=dat$idstud, est.a.item=TRUE)

#****  Model 5: estimated rater and item slopes
mod5 &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], rater=dat$rater,
             pid=dat$idstud, est.a.rater=TRUE, est.a.item=TRUE)
summary(mod1)
summary(mod2)
summary(mod2a)
summary(mod3)
summary(mod4)
summary(mod5)

#****  Model 5a: Some fixed parameters in Model 5
# fix rater b parameters for raters 1, 4 and 5
b.rater.fixed &lt;- rep(NA,7)
b.rater.fixed[ c(1,4,5) ] &lt;- c(1,-.8,0)
# fix rater a parameters for first four raters
a.rater.fixed &lt;- rep(NA,7)
a.rater.fixed[ c(1,2,3,4) ] &lt;- c(1.1,0.9,.85,1)
# fix item b parameters of first item
tau.item.fixed &lt;- matrix( NA, nrow=5, ncol=3 )
tau.item.fixed[ 1, ] &lt;- c(-2,-1.5, 1 )
# fix item a parameters
a.item.fixed &lt;- rep(NA,5)
a.item.fixed[ 1:4 ] &lt;- 1
# estimate model
mod5a &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], rater=dat$rater,
             pid=dat$idstud, est.a.rater=TRUE, est.a.item=TRUE,
             tau.item.fixed=tau.item.fixed, b.rater.fixed=b.rater.fixed,
             a.rater.fixed=a.rater.fixed, a.item.fixed=a.item.fixed,
             est.mean=TRUE)
summary(mod5a)

#****  Model 6: Estimate rater model with reference rater 'db03'
mod6 &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], rater=dat$rater, est.a.item=TRUE,
             est.a.rater=TRUE, pid=dat$idstud, reference_rater="db03" )
summary(mod6)

#**** Model 7: Modelling rater-item-interactions
mod7 &lt;- sirt::rm.facets( dat[, paste0( "k",1:5) ], rater=dat$rater, est.a.item=FALSE,
             est.a.rater=TRUE, pid=dat$idstud, reference_rater="db03",
             rater_item_int=TRUE)
summary(mod7)

## End(Not run)
</code></pre>

<hr>
<h2 id='rm.sdt'>
Hierarchical Rater Model Based on Signal Detection Theory (HRM-SDT)
</h2><span id='topic+rm.sdt'></span><span id='topic+summary.rm.sdt'></span><span id='topic+logLik.rm.sdt'></span><span id='topic+anova.rm.sdt'></span><span id='topic+IRT.irfprob.rm.sdt'></span><span id='topic+IRT.likelihood.rm.sdt'></span><span id='topic+IRT.posterior.rm.sdt'></span><span id='topic+IRT.modelfit.rm.sdt'></span><span id='topic+summary.IRT.modelfit.rm.sdt'></span><span id='topic+IRT.factor.scores.rm.sdt'></span><span id='topic+plot.rm.sdt'></span>

<h3>Description</h3>

<p>This function estimates a version of the hierarchical rater
model (HRM) based on signal detection theory (HRM-SDT; DeCarlo, 2005;
DeCarlo, Kim &amp; Johnson, 2011; Robitzsch &amp; Steinfeld, 2018).
The model is estimated by means of an EM algorithm adapted from
multilevel latent class analysis (Vermunt, 2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rm.sdt(dat, pid, rater, Qmatrix=NULL, theta.k=seq(-9, 9, len=30),
    est.a.item=FALSE, est.c.rater="n", est.d.rater="n", est.mean=FALSE, est.sigma=TRUE,
    skillspace="normal", tau.item.fixed=NULL, a.item.fixed=NULL,
    d.min=0.5, d.max=100, d.start=3, c.start=NULL, tau.start=NULL, sd.start=1,
    d.prior=c(3,100), c.prior=c(3,100), tau.prior=c(0,1000), a.prior=c(1,100),
    link_item="GPCM", max.increment=1, numdiff.parm=0.00001, maxdevchange=0.1,
    globconv=.001, maxiter=1000, msteps=4, mstepconv=0.001, optimizer="nlminb" )

## S3 method for class 'rm.sdt'
summary(object, file=NULL, ...)

## S3 method for class 'rm.sdt'
plot(x, ask=TRUE, ...)

## S3 method for class 'rm.sdt'
anova(object,...)

## S3 method for class 'rm.sdt'
logLik(object,...)

## S3 method for class 'rm.sdt'
IRT.factor.scores(object, type="EAP", ...)

## S3 method for class 'rm.sdt'
IRT.irfprob(object,...)

## S3 method for class 'rm.sdt'
IRT.likelihood(object,...)

## S3 method for class 'rm.sdt'
IRT.posterior(object,...)

## S3 method for class 'rm.sdt'
IRT.modelfit(object,...)

## S3 method for class 'IRT.modelfit.rm.sdt'
summary(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rm.sdt_+3A_dat">dat</code></td>
<td>

<p>Original data frame. Ratings on variables must be in rows,
i.e. every row corresponds to a person-rater combination.
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_pid">pid</code></td>
<td>

<p>Person identifier.
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_rater">rater</code></td>
<td>

<p>Rater identifier.
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_qmatrix">Qmatrix</code></td>
<td>

<p>An optional Q-matrix. If this matrix is not provided,
then by default the ordinary scoring of categories
(from 0 to the maximum score of <code class="reqn">K</code>) is used.
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_theta.k">theta.k</code></td>
<td>

<p>A grid of theta values for the ability distribution.
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_est.a.item">est.a.item</code></td>
<td>

<p>Should item parameters <code class="reqn">a_i</code> be estimated?
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_est.c.rater">est.c.rater</code></td>
<td>

<p>Type of estimation for item-rater parameters <code class="reqn">c_{ir}</code>
in the signal detection model. Options are <code>'n'</code> (no estimation),
<code>'e'</code> (set all parameters equal to each other),
<code>'i'</code> (itemwise estimation), <code>'r'</code> (rater wise estimation)
and <code>'a'</code> (all parameters are estimated independently
from each other).
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_est.d.rater">est.d.rater</code></td>
<td>

<p>Type of estimation of <code class="reqn">d</code> parameters. Options are
the same as in <code>est.c.rater</code>.
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_est.mean">est.mean</code></td>
<td>
<p>Optional logical indicating whether the mean of the
trait distribution should be estimated.</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_est.sigma">est.sigma</code></td>
<td>
<p>Optional logical indicating whether the standard deviation of the
trait distribution should be estimated.</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_skillspace">skillspace</code></td>
<td>
<p>Specified <code class="reqn">\theta</code> distribution type. It can be
<code>"normal"</code> or <code>"discrete"</code>. In the latter case,
all probabilities of the distribution are separately
estimated.</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_tau.item.fixed">tau.item.fixed</code></td>
<td>
<p>Optional matrix with three columns specifying
fixed <code class="reqn">\tau</code> parameters. The first two columns denote item and
category indices, the third the fixed value. See Example 3.</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_a.item.fixed">a.item.fixed</code></td>
<td>
<p>Optional matrix with two columns specifying fixed
<code class="reqn">a</code> parameters. First column: Item index. Second column:
Fixed <code class="reqn">a</code> parameter.</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_d.min">d.min</code></td>
<td>

<p>Minimal <code class="reqn">d</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_d.max">d.max</code></td>
<td>

<p>Maximal <code class="reqn">d</code> parameter to be estimated
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_d.start">d.start</code></td>
<td>

<p>Starting value(s) of <code class="reqn">d</code> parameters
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_c.start">c.start</code></td>
<td>

<p>Starting values of <code class="reqn">c</code> parameters
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_tau.start">tau.start</code></td>
<td>

<p>Starting values of <code class="reqn">\tau</code> parameters
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_sd.start">sd.start</code></td>
<td>

<p>Starting value for trait standard deviation
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_d.prior">d.prior</code></td>
<td>
<p>Normal prior <code class="reqn">N(M,S^2)</code> for <code class="reqn">d</code> parameters</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_c.prior">c.prior</code></td>
<td>
<p>Normal prior for <code class="reqn">c</code> parameters. The prior for
parameter <code class="reqn">c_{irk}</code> is defined as <code class="reqn">M \cdot ( k - 0.5) </code>
where <code class="reqn">M</code> is <code>c.prior[1]</code>. </p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_tau.prior">tau.prior</code></td>
<td>
<p>Normal prior for <code class="reqn">\tau</code> parameters</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_a.prior">a.prior</code></td>
<td>
<p>Normal prior for <code class="reqn">a</code> parameters</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_link_item">link_item</code></td>
<td>
<p>Type of item response function for latent responses.
Can be <code>"GPCM"</code> for the generalized partial credit model or
<code>"GRM"</code> for the graded response model.
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_max.increment">max.increment</code></td>
<td>

<p>Maximum increment of item parameters during estimation
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_numdiff.parm">numdiff.parm</code></td>
<td>

<p>Numerical differentiation step width
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_maxdevchange">maxdevchange</code></td>
<td>

<p>Maximum relative deviance change as a convergence criterion
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_globconv">globconv</code></td>
<td>

<p>Maximum parameter change
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_msteps">msteps</code></td>
<td>

<p>Maximum number of iterations during an M step
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_mstepconv">mstepconv</code></td>
<td>

<p>Convergence criterion in an M step
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_optimizer">optimizer</code></td>
<td>
<p>Choice of optimization function in M-step for
item parameters. Options are <code>"nlminb"</code> for <code><a href="stats.html#topic+nlminb">stats::nlminb</a></code>
and <code>"optim"</code> for <code><a href="stats.html#topic+optim">stats::optim</a></code>.
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_object">object</code></td>
<td>

<p>Object of class <code>rm.sdt</code>
</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_file">file</code></td>
<td>
<p>Optional file name in which summary should be written.</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_x">x</code></td>
<td>
<p>Object of class <code>rm.sdt</code></p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_ask">ask</code></td>
<td>
<p>Optional logical indicating whether a new plot should be asked for.</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_type">type</code></td>
<td>
<p>Factor score estimation method. Up to now,
only <code>type="EAP"</code> is supported.</p>
</td></tr>
<tr><td><code id="rm.sdt_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The specification of the model follows DeCarlo et al. (2011).
The second level models the ideal rating (latent response) <code class="reqn">\eta=0, ...,K</code>
of person <code class="reqn">p</code> on item <code class="reqn">i</code>. The option <code>link_item='GPCM'</code> follows the
generalized partial credit model
</p>
<p style="text-align: center;"><code class="reqn"> P( \eta_{pi}=\eta | \theta_p ) \propto
exp( a_{i} q_{i \eta } \theta_p - \tau_{i \eta } ) </code>
</p>
<p>. The option <code>link_item='GRM'</code> employs the
graded response model   </p>
<p style="text-align: center;"><code class="reqn"> P( \eta_{pi}=\eta | \theta_p )=
\Psi( \tau_{i,\eta + 1} - a_i \theta_p ) - \Psi( \tau_{i,\eta} - a_i \theta_p ) </code>
</p>

<p>At the first level, the ratings <code class="reqn">X_{pir}</code> for
person <code class="reqn">p</code> on item <code class="reqn">i</code> and rater <code class="reqn">r</code>
are modeled as a signal detection model
</p>
<p style="text-align: center;"><code class="reqn"> P( X_{pir} \le k | \eta_{pi} )=
G( c_{irk} - d_{ir} \eta_{pi} )</code>
</p>

<p>where <code class="reqn">G</code> is the logistic distribution function
and the categories are <code class="reqn">k=1,\ldots, K+1</code>.
Note that the item response model can be equivalently written
as </p>
<p style="text-align: center;"><code class="reqn"> P( X_{pir} \ge k | \eta_{pi} )=
G(   d_{ir} \eta_{pi} - c_{irk})</code>
</p>

<p>The thresholds <code class="reqn">c_{irk}</code> can be further restricted to
<code class="reqn">c_{irk}=c_{k}</code> (<code>est.c.rater='e'</code>),
<code class="reqn">c_{irk}=c_{ik}</code> (<code>est.c.rater='i'</code>) or
<code class="reqn">c_{irk}=c_{ir}</code> (<code>est.c.rater='r'</code>). The same
holds for rater precision parameters <code class="reqn">d_{ir}</code>.
</p>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria and number of parameters</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Data frame with item parameters. The columns
<code>N</code> and <code>M</code> denote the number of observed ratings and the
observed mean of all ratings, respectively. <br />
In addition to item parameters <code class="reqn">\tau_{ik}</code> and <code class="reqn">a_i</code>, the mean
for the latent response (<code>latM</code>) is computed as
<code class="reqn">E( \eta_i )=\sum_p P( \theta_p ) q_{ik} P( \eta_i=k | \theta_p ) </code>
which provides an item parameter at the original metric of ratings. The latent standard
deviation (<code>latSD</code>) is computed in the same manner.
</p>
</td></tr>
<tr><td><code>rater</code></td>
<td>
<p>Data frame with rater parameters.
Transformed <code class="reqn">c</code> parameters
(<code>c_x.trans</code>) are computed as <code class="reqn">c_{irk} / ( d_{ir}  )</code>.
</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with person parameters: EAP and corresponding standard
errors</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliability</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliability</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Mean of the trait distribution</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Standard deviation of the trait distribution</p>
</td></tr>
<tr><td><code>tau.item</code></td>
<td>
<p>Item parameters <code class="reqn">\tau_{ik}</code></p>
</td></tr>
<tr><td><code>se.tau.item</code></td>
<td>
<p>Standard error of item parameters <code class="reqn">\tau_{ik}</code></p>
</td></tr>
<tr><td><code>a.item</code></td>
<td>
<p>Item slopes <code class="reqn">a_i</code></p>
</td></tr>
<tr><td><code>se.a.item</code></td>
<td>
<p>Standard error of item slopes <code class="reqn">a_i</code></p>
</td></tr>
<tr><td><code>c.rater</code></td>
<td>
<p>Rater parameters <code class="reqn">c_{irk}</code></p>
</td></tr>
<tr><td><code>se.c.rater</code></td>
<td>
<p>Standard error of rater severity parameter <code class="reqn">c_{irk}</code></p>
</td></tr>
<tr><td><code>d.rater</code></td>
<td>
<p>Rater slope parameter <code class="reqn">d_{ir}</code></p>
</td></tr>
<tr><td><code>se.d.rater</code></td>
<td>
<p>Standard error of rater slope parameter <code class="reqn">d_{ir}</code></p>
</td></tr>
<tr><td><code>f.yi.qk</code></td>
<td>
<p>Individual likelihood</p>
</td></tr>
<tr><td><code>f.qk.yi</code></td>
<td>
<p>Individual posterior distribution</p>
</td></tr>
<tr><td><code>probs</code></td>
<td>
<p>Item probabilities at grid <code>theta.k</code>. Note that these
probabilities are calculated on the pseudo items <code class="reqn">i \times r</code>,
i.e. the interaction of item and rater.</p>
</td></tr>
<tr><td><code>prob.item</code></td>
<td>
<p>Probabilities <code class="reqn">P( \eta_i=\eta | \theta )</code> of
latent item responses evaluated at theta grid <code class="reqn">\theta_p</code>.
</p>
</td></tr>
<tr><td><code>n.ik</code></td>
<td>
<p>Expected counts</p>
</td></tr>
<tr><td><code>pi.k</code></td>
<td>
<p>Estimated trait distribution <code class="reqn">P(\theta_p)</code>.
</p>
</td></tr>
<tr><td><code>maxK</code></td>
<td>
<p>Maximum number of categories</p>
</td></tr>
<tr><td><code>procdata</code></td>
<td>
<p>Processed data</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further values</p>
</td></tr>
</table>


<h3>References</h3>

<p>DeCarlo, L. T. (2005). A model of rater behavior in essay grading
based on signal detection theory.
<em>Journal of Educational Measurement, 42</em>, 53-76.
</p>
<p>DeCarlo, L. T. (2010). <em>Studies of a latent-class signal-detection model
for constructed response scoring II: Incomplete and hierarchical designs</em>.
ETS Research Report ETS RR-10-08. Princeton NJ: ETS.
</p>
<p>DeCarlo, T., Kim, Y., &amp; Johnson, M. S. (2011).
A hierarchical rater model for constructed responses,
with a signal detection rater model.
<em>Journal of Educational Measurement, 48</em>, 333-356.
</p>
<p>Robitzsch, A., &amp; Steinfeld, J. (2018). Item response models for human ratings: Overview,
estimation methods, and implementation in R.
<em>Psychological Test and Assessment Modeling, 60</em>(1), 101-139.
</p>
<p>Vermunt, J. K. (2008). Latent class and finite mixture models for
multilevel data sets. <em>Statistical Methods in Medical Research, 17</em>, 33-51.
</p>


<h3>See Also</h3>

<p>The facets rater model can be estimated with <code><a href="#topic+rm.facets">rm.facets</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Hierarchical rater model (HRM-SDT) data.ratings1
#############################################################################
data(data.ratings1)
dat &lt;- data.ratings1

## Not run: 
# Model 1: Partial Credit Model: no rater effects
mod1 &lt;- sirt::rm.sdt( dat[, paste0( "k",1:5) ], rater=dat$rater,
            pid=dat$idstud, est.c.rater="n", d.start=100,  est.d.rater="n" )
summary(mod1)

# Model 2: Generalized Partial Credit Model: no rater effects
mod2 &lt;- sirt::rm.sdt( dat[, paste0( "k",1:5) ], rater=dat$rater,
            pid=dat$idstud, est.c.rater="n", est.d.rater="n",
            est.a.item=TRUE, d.start=100)
summary(mod2)

# Model 3: Equal effects in SDT
mod3 &lt;- sirt::rm.sdt( dat[, paste0( "k",1:5) ], rater=dat$rater,
            pid=dat$idstud, est.c.rater="e", est.d.rater="e")
summary(mod3)

# Model 4: Rater effects in SDT
mod4 &lt;- sirt::rm.sdt( dat[, paste0( "k",1:5) ], rater=dat$rater,
            pid=dat$idstud, est.c.rater="r", est.d.rater="r")
summary(mod4)

#############################################################################
# EXAMPLE 2: HRM-SDT data.ratings3
#############################################################################

data(data.ratings3)
dat &lt;- data.ratings3
dat &lt;- dat[ dat$rater &lt; 814, ]
psych::describe(dat)

# Model 1: item- and rater-specific effects
mod1 &lt;- sirt::rm.sdt( dat[, paste0( "crit",c(2:4)) ], rater=dat$rater,
            pid=dat$idstud, est.c.rater="a", est.d.rater="a" )
summary(mod1)
plot(mod1)

# Model 2: Differing number of categories per variable
mod2 &lt;- sirt::rm.sdt( dat[, paste0( "crit",c(2:4,6)) ], rater=dat$rater,
            pid=dat$idstud, est.c.rater="a", est.d.rater="a")
summary(mod2)
plot(mod2)

#############################################################################
# EXAMPLE 3: Hierarchical rater model with discrete skill spaces
#############################################################################

data(data.ratings3)
dat &lt;- data.ratings3
dat &lt;- dat[ dat$rater &lt; 814, ]
psych::describe(dat)

# Model 1: Discrete theta skill space with values of 0,1,2 and 3
mod1 &lt;- sirt::rm.sdt( dat[, paste0( "crit",c(2:4)) ], theta.k=0:3, rater=dat$rater,
            pid=dat$idstud, est.c.rater="a", est.d.rater="a", skillspace="discrete" )
summary(mod1)
plot(mod1)

# Model 2: Modelling of one item by using a discrete skill space and
#          fixed item parameters

# fixed tau and a parameters
tau.item.fixed &lt;- cbind( 1, 1:3,  100*cumsum( c( 0.5, 1.5, 2.5)) )
a.item.fixed &lt;- cbind( 1, 100 )
# fit HRM-SDT
mod2 &lt;- sirt::rm.sdt( dat[, "crit2", drop=FALSE], theta.k=0:3, rater=dat$rater,
            tau.item.fixed=tau.item.fixed,a.item.fixed=a.item.fixed, pid=dat$idstud,
            est.c.rater="a", est.d.rater="a", skillspace="discrete" )
summary(mod2)
plot(mod2)

## End(Not run)
</code></pre>

<hr>
<h2 id='rmvn'>
Simulation of a Multivariate Normal Distribution with Exact Moments
</h2><span id='topic+rmvn'></span><span id='topic+ruvn'></span>

<h3>Description</h3>

<p>Simulates a dataset from a multivariate or univariate normal distribution that
exactly fulfils the specified mean vector and the covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># multivariate normal distribution
rmvn(N, mu, Sigma, exact=TRUE)

# univariate normal distribution
ruvn(N, mean=0, sd=1, exact=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmvn_+3A_n">N</code></td>
<td>

<p>Sample size
</p>
</td></tr>
<tr><td><code id="rmvn_+3A_mu">mu</code></td>
<td>

<p>Mean vector
</p>
</td></tr>
<tr><td><code id="rmvn_+3A_sigma">Sigma</code></td>
<td>

<p>Covariance matrix
</p>
</td></tr>
<tr><td><code id="rmvn_+3A_exact">exact</code></td>
<td>

<p>Logical indicating whether <code>mu</code> and <code>Sigma</code> should be exactly reproduced.
</p>
</td></tr>
<tr><td><code id="rmvn_+3A_mean">mean</code></td>
<td>
<p>Numeric value for mean</p>
</td></tr>
<tr><td><code id="rmvn_+3A_sd">sd</code></td>
<td>
<p>Numeric value for standard deviation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe or a vector
</p>


<h3>See Also</h3>

<p><code>mvtnorm::rmvnorm</code>, <code>mgcv::rmvn</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Simulate multivariate normal data
#############################################################################

# define covariance matrix and mean vector
rho &lt;- .8
Sigma &lt;- matrix(rho,3,3)
diag(Sigma) &lt;- 1
mu &lt;- c(0,.5,1)

#* simulate data
set.seed(87)
dat &lt;- sirt::rmvn(N=200, mu=mu, Sigma=Sigma)
#* check means and covariances
stats::cov.wt(dat, method="ML")

## Not run: 
#############################################################################
# EXAMPLE 2: Simulate univariate normal data
#############################################################################

#* simulate data
x &lt;- sirt::ruvn(N=20, mean=.5, sd=1.2, exact=TRUE)
# check results
stats::var(x)
sirt:::sirt_var(x)

## End(Not run)
</code></pre>

<hr>
<h2 id='scale_group_means'>
Scaling of Group Means and Standard Deviations
</h2><span id='topic+scale_group_means'></span><span id='topic+predict_scale_group_means'></span>

<h3>Description</h3>

<p>Scales a vector of means and standard deviations containing group values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scale_group_means(M, SD, probs=NULL, M_target=0, SD_target=1)

## predict method
predict_scale_group_means(object, M, SD)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scale_group_means_+3A_m">M</code></td>
<td>
<p>Vector of means</p>
</td></tr>
<tr><td><code id="scale_group_means_+3A_sd">SD</code></td>
<td>
<p>Vector of standard deviations</p>
</td></tr>
<tr><td><code id="scale_group_means_+3A_probs">probs</code></td>
<td>
<p>Optional vector containing probabilities</p>
</td></tr>
<tr><td><code id="scale_group_means_+3A_m_target">M_target</code></td>
<td>
<p>Target value for mean</p>
</td></tr>
<tr><td><code id="scale_group_means_+3A_sd_target">SD_target</code></td>
<td>
<p>Target value for standard deviation</p>
</td></tr>
<tr><td><code id="scale_group_means_+3A_object">object</code></td>
<td>
<p>Fitted object from <code>scale_group_means</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with entries
</p>
<table>
<tr><td><code>M1</code></td>
<td>
<p>total mean</p>
</td></tr>
<tr><td><code>SD1</code></td>
<td>
<p>total standard deviation</p>
</td></tr>
<tr><td><code>M_z</code></td>
<td>
<p>standardized means</p>
</td></tr>
<tr><td><code>SD_z</code></td>
<td>
<p>standardized standard deviations</p>
</td></tr>
<tr><td><code>M_trafo</code></td>
<td>
<p>transformed means</p>
</td></tr>
<tr><td><code>SD_trafo</code></td>
<td>
<p>transformed standard deviations</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Toy example
#############################################################################

M &lt;- c(-.03, .18, -.23, -.15, .29)
SD &lt;- c(.97, 1.13, .77, 1.05, 1.17)
sirt::scale_group_means(M=M, SD=SD)
</code></pre>

<hr>
<h2 id='sia.sirt'>
Statistical Implicative Analysis (SIA)
</h2><span id='topic+sia.sirt'></span>

<h3>Description</h3>

<p>This function is a simplified implementation of statistical implicative
analysis (Gras &amp; Kuntz, 2008) which aims at deriving implications
<code class="reqn">X_i \rightarrow X_j</code>. This means that solving item <code class="reqn">i</code> implies
solving item <code class="reqn">j</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sia.sirt(dat, significance=0.85)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sia.sirt_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous item responses
</p>
</td></tr>
<tr><td><code id="sia.sirt_+3A_significance">significance</code></td>
<td>

<p>Minimum implicative probability for inclusion of an arrow in the graph.
The probability can be interpreted as a kind of significance level, i.e.
higher probabilities indicate more probable implications.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The test statistic for selection an implicative relation follows
Gras and Kuntz (2008).
Transitive arrows (implications) are removed from the graph.
If some implications are symmetric, then only the more probable
implication will be retained.
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>adj.matrix</code></td>
<td>
<p>Adjacency matrix of the graph. Transitive and
symmetric implications (arrows) have been removed.</p>
</td></tr>
<tr><td><code>adj.pot</code></td>
<td>
<p>Adjacency matrix including all powers, i.e. all direct and
indirect paths from item <code class="reqn">i</code> to item <code class="reqn">j</code>.</p>
</td></tr>
<tr><td><code>adj.matrix.trans</code></td>
<td>
<p>Adjacency matrix including transitive arrows.</p>
</td></tr>
<tr><td><code>desc</code></td>
<td>
<p>List with descriptive statistics of the graph.</p>
</td></tr>
<tr><td><code>desc.item</code></td>
<td>
<p>Descriptive statistics for each item.</p>
</td></tr>
<tr><td><code>impl.int</code></td>
<td>
<p>Implication intensity (probability) as the basis
for deciding the significance of an arrow</p>
</td></tr>
<tr><td><code>impl.t</code></td>
<td>
<p>Corresponding <code class="reqn">t</code> values of <code>impl.int</code></p>
</td></tr>
<tr><td><code>impl.significance</code></td>
<td>
<p>Corresponding <code class="reqn">p</code> values (significancies)
of <code>impl.int</code></p>
</td></tr>
<tr><td><code>conf.loev</code></td>
<td>
<p>Confidence according to Loevinger (see Gras &amp; Kuntz, 2008).
This values are just conditional probabilities <code class="reqn">P( X_j=1|X_i=1)</code>.</p>
</td></tr>
<tr><td><code>graph.matr</code></td>
<td>
<p>Matrix containing all arrows. Can be used
for example for the <span class="pkg">Rgraphviz</span> package.</p>
</td></tr>
<tr><td><code>graph.edges</code></td>
<td>
<p>Vector containing all edges of the graph, e.g. for
the <span class="pkg">Rgraphviz</span> package.</p>
</td></tr>
<tr><td><code>igraph.matr</code></td>
<td>
<p>Matrix containing all arrows for the <span class="pkg">igraph</span>
package.</p>
</td></tr>
<tr><td><code>igraph.obj</code></td>
<td>
<p>An object of the graph for the <span class="pkg">igraph</span> package.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For an implementation of statistical implicative analysis in the
C.H.I.C. (Classification Hierarchique, Implicative et Cohesitive)
software.
</p>
<p>See <a href="https://ardm.eu/partenaires/logiciel-danalyse-de-donnees-c-h-i-c/">https://ardm.eu/partenaires/logiciel-danalyse-de-donnees-c-h-i-c/</a>.
</p>


<h3>References</h3>

<p>Gras, R., &amp; Kuntz, P. (2008). An overview of the statistical implicative analysis
(SIA) development. In R. Gras, E. Suzuki, F. Guillet, &amp; F. Spagnolo (Eds.).
<em>Statistical Implicative Analysis</em> (pp. 11-40).
Springer, Berlin Heidelberg.
</p>


<h3>See Also</h3>

<p>See also the <span class="pkg">IsingFit</span> package for calculating a graph for
dichotomous item responses using the Ising model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: SIA for data.read
#############################################################################

data(data.read)
dat &lt;- data.read

res &lt;- sirt::sia.sirt(dat, significance=.85 )

#*** plot results with igraph package
library(igraph)
plot( res$igraph.obj ) #, vertex.shape="rectangle", vertex.size=30 )

## Not run: 
#*** plot results with qgraph package
miceadds::library_install(qgraph)
qgraph::qgraph( res$adj.matrix )

#*** plot results with Rgraphviz package
# Rgraphviz can only be obtained from Bioconductor
# If it should be downloaded, select TRUE for the following lines
if (FALSE){
     source("http://bioconductor.org/biocLite.R")
     biocLite("Rgraphviz")
            }
# define graph
grmatrix &lt;- res$graph.matr
res.graph &lt;- new("graphNEL", nodes=res$graph.edges, edgemode="directed")
# add edges
RR &lt;- nrow(grmatrix)
for (rr in 1:RR){
    res.graph &lt;- Rgraphviz::addEdge(grmatrix[rr,1], grmatrix[rr,2], res.graph, 1)
                    }
# define cex sizes and shapes
V &lt;- length(res$graph.edges)
size2 &lt;- rep(16,V)
shape2 &lt;- rep("rectangle", V )
names(shape2) &lt;- names(size2) &lt;- res$graph.edges
# plot graph
Rgraphviz::plot( res.graph, nodeAttrs=list("fontsize"=size2, "shape"=shape2) )

## End(Not run)
</code></pre>

<hr>
<h2 id='sim.qm.ramsay'>
Simulate from Ramsay's Quotient Model
</h2><span id='topic+sim.qm.ramsay'></span>

<h3>Description</h3>

<p>This function simulates dichotomous item response data
according to Ramsay's quotient model (Ramsay, 1989).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.qm.ramsay(theta, b, K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.qm.ramsay_+3A_theta">theta</code></td>
<td>

<p>Vector of of length <code class="reqn">N</code> person parameters (must be positive!)
</p>
</td></tr>
<tr><td><code id="sim.qm.ramsay_+3A_b">b</code></td>
<td>

<p>Vector of length <code class="reqn">I</code> of item difficulties (must be positive)
</p>
</td></tr>
<tr><td><code id="sim.qm.ramsay_+3A_k">K</code></td>
<td>

<p>Vector of length <code class="reqn">I</code> of guessing parameters (must be positive)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ramsay's quotient model (Ramsay, 1989) is defined by the equation
</p>
<p style="text-align: center;"><code class="reqn">P(X_{pi}=1 | \theta_p )=\frac{ \exp { ( \theta_p / b_i ) } }
    { K_i + \exp { ( \theta_p / b_i ) } }</code>
</p>



<h3>Value</h3>

<p>An <code class="reqn">N \times I</code>  data frame with dichotomous item responses.
</p>


<h3>References</h3>

<p>Ramsay, J. O. (1989). A comparison of three simple test theory models.
<em>Psychometrika, 54</em>, 487-499.
</p>
<p>van der Maas, H. J. L., Molenaar, D., Maris, G., Kievit, R. A., &amp;
Borsboom, D. (2011).
Cognitive psychology meets psychometric theory: On the relation between
process models for decision making and latent variable models for
individual differences.
<em>Psychological Review, 318</em>, 339-356.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+rasch.mml2">rasch.mml2</a></code> for estimating Ramsay's quotient model.
</p>
<p>See <code><a href="#topic+sim.raschtype">sim.raschtype</a></code> for simulating response data from
the generalized logistic item response model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Estimate Ramsay Quotient Model with rasch.mml2
#############################################################################

set.seed(657)
# simulate data according to the Ramsay model
N &lt;- 1000       # persons
I &lt;- 11         # items
theta &lt;- exp( stats::rnorm( N ) )  # person ability
b &lt;- exp( seq(-2,2,len=I))  # item difficulty
K &lt;- rep( 3, I )           # K parameter (=&gt; guessing)

# apply simulation function
dat &lt;- sirt::sim.qm.ramsay( theta, b, K )

#***
# analysis
mmliter &lt;- 50       # maximum number of iterations
I &lt;- ncol(dat)
fixed.K &lt;- rep( 3, I )

# Ramsay QM with fixed K parameter (K=3 in fixed.K specification)
mod1 &lt;- sirt::rasch.mml2( dat, mmliter=mmliter, irtmodel="ramsay.qm",
              fixed.K=fixed.K )
summary(mod1)

# Ramsay QM with joint estimated K parameters
mod2 &lt;- sirt::rasch.mml2( dat, mmliter=mmliter, irtmodel="ramsay.qm",
             est.K=rep(1,I)  )
summary(mod2)

## Not run: 
# Ramsay QM with itemwise estimated K parameters
mod3 &lt;- sirt::rasch.mml2( dat, mmliter=mmliter, irtmodel="ramsay.qm",
              est.K=1:I  )
summary(mod3)

# Rasch model
mod4 &lt;- sirt::rasch.mml2( dat )
summary(mod4)

# generalized logistic model
mod5 &lt;- sirt::rasch.mml2( dat, est.alpha=TRUE, mmliter=mmliter)
summary(mod5)

# 2PL model
mod6 &lt;- sirt::rasch.mml2( dat, est.a=rep(1,I) )
summary(mod6)

# Difficulty + Guessing (b+c) Model
mod7 &lt;- sirt::rasch.mml2( dat, est.c=rep(1,I) )
summary(mod7)

# estimate separate guessing (c) parameters
mod8 &lt;- sirt::rasch.mml2( dat, est.c=1:I  )
summary(mod8)

#*** estimate Model 1 with user defined function in mirt package

# create user defined function for Ramsay's quotient model
name &lt;- 'ramsayqm'
par &lt;- c("K"=3, "b"=1 )
est &lt;- c(TRUE, TRUE)
P.ramsay &lt;- function(par,Theta){
     eps &lt;- .01
     K &lt;- par[1]
     b &lt;- par[2]
     num &lt;- exp( exp( Theta[,1] ) / b )
     denom &lt;- K + num
     P1 &lt;- num / denom
     P1 &lt;- eps + ( 1 - 2*eps ) * P1
     cbind(1-P1, P1)
}

# create item response function
ramsayqm &lt;- mirt::createItem(name, par=par, est=est, P=P.ramsay)
# define parameters to be estimated
mod1m.pars &lt;- mirt::mirt(dat, 1, rep( "ramsayqm",I),
                   customItems=list("ramsayqm"=ramsayqm), pars="values")
mod1m.pars[ mod1m.pars$name=="K", "est" ] &lt;- FALSE
# define Theta design matrix
Theta &lt;- matrix( seq(-3,3,len=10), ncol=1)
# estimate model
mod1m &lt;- mirt::mirt(dat, 1, rep( "ramsayqm",I), customItems=list("ramsayqm"=ramsayqm),
               pars=mod1m.pars, verbose=TRUE,
               technical=list( customTheta=Theta, NCYCLES=50)
                )
print(mod1m)
summary(mod1m)
cmod1m &lt;- sirt::mirt.wrapper.coef( mod1m )$coef
# compare simulated and estimated values
dfr &lt;- cbind( b, cmod1m$b, exp(mod1$item$b ) )
colnames(dfr) &lt;- c("simulated", "mirt", "sirt_rasch.mml2")
round( dfr, 2 )
  ##      simulated mirt sirt_rasch.mml2
  ## [1,]      0.14 0.11            0.11
  ## [2,]      0.20 0.17            0.18
  ## [3,]      0.30 0.27            0.29
  ## [4,]      0.45 0.42            0.43
  ## [5,]      0.67 0.65            0.67
  ## [6,]      1.00 1.00            1.01
  ## [7,]      1.49 1.53            1.54
  ## [8,]      2.23 2.21            2.21
  ## [9,]      3.32 3.00            2.98
  ##[10,]      4.95 5.22            5.09
  ##[11,]      7.39 5.62            5.51

## End(Not run)
</code></pre>

<hr>
<h2 id='sim.rasch.dep'>
Simulation of the Rasch Model with Locally Dependent Responses
</h2><span id='topic+sim.rasch.dep'></span>

<h3>Description</h3>

<p>This function simulates dichotomous item responses where for some
itemclusters residual correlations can be defined.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.rasch.dep(theta, b, itemcluster, rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.rasch.dep_+3A_theta">theta</code></td>
<td>

<p>Vector of person abilities of length <code class="reqn">N</code>
</p>
</td></tr>
<tr><td><code id="sim.rasch.dep_+3A_b">b</code></td>
<td>

<p>Vector of item difficulties of length <code class="reqn">I</code>
</p>
</td></tr>
<tr><td><code id="sim.rasch.dep_+3A_itemcluster">itemcluster</code></td>
<td>

<p>Vector of integers (including 0) of length <code class="reqn">I</code>. Different integers
correspond to different itemclusters.
</p>
</td></tr>
<tr><td><code id="sim.rasch.dep_+3A_rho">rho</code></td>
<td>

<p>Vector of residual correlations. The length of vector must be equal
to the number of itemclusters.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code class="reqn">N \times I</code> data frame of dichotomous item responses.
</p>


<h3>Note</h3>

<p>The specification of the simulation models follows a marginal interpretation
of the latent trait. Local dependencies are only interpreted as nuisance
and not of substantive interest. If local dependencies should be substantively
interpreted, a testlet model seems preferable
(see <code><a href="#topic+mcmc.3pno.testlet">mcmc.3pno.testlet</a></code>).
</p>


<h3>See Also</h3>

<p>To simulate the generalized logistic item response model see
<code><a href="#topic+sim.raschtype">sim.raschtype</a></code>. Ramsay's quotient model can be simulated
using <code><a href="#topic+sim.qm.ramsay">sim.qm.ramsay</a></code>.
</p>
<p>Marginal item reponse models for locally dependent item responses can be
estimated with <code><a href="#topic+rasch.copula2">rasch.copula2</a></code>, <code><a href="#topic+rasch.pairwise">rasch.pairwise</a></code> or
<code><a href="#topic+rasch.pairwise.itemcluster">rasch.pairwise.itemcluster</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: 11 Items: 2 itemclusters with 2 resp. 3 dependent items
#             and 6 independent items
#############################################################################

set.seed(7654)
I &lt;- 11                             # number of items
n &lt;- 1500                           # number of persons
b &lt;- seq(-2,2, len=I)               # item difficulties
theta &lt;- stats::rnorm( n, sd=1 )        # person abilities
# itemcluster
itemcluster &lt;- rep(0,I)
itemcluster[ c(3,5)] &lt;- 1
itemcluster[c(2,4,9)] &lt;- 2
# residual correlations
rho &lt;- c( .7, .5 )

# simulate data
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("I", seq(1,ncol(dat)), sep="")

# estimate Rasch copula model
mod1 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster )
summary(mod1)

# compare result with Rasch model estimation in rasch.copula
# delta must be set to zero
mod2 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster, delta=c(0,0),
            est.delta=c(0,0)  )
summary(mod2)

# estimate Rasch model with rasch.mml2 function
mod3 &lt;- sirt::rasch.mml2( dat )
summary(mod3)

## Not run: 
#############################################################################
# EXAMPLE 2: 12 Items: Cluster 1 -&gt; Items 1,...,4;
#       Cluster 2 -&gt; Items 6,...,9; Cluster 3 -&gt; Items 10,11,12
#############################################################################

set.seed(7896)
I &lt;- 12                             # number of items
n &lt;- 450                            # number of persons
b &lt;- seq(-2,2, len=I)               # item difficulties
b &lt;- sample(b)                      # sample item difficulties
theta &lt;- stats::rnorm( n, sd=1 )        # person abilities
# itemcluster
itemcluster &lt;- rep(0,I)
itemcluster[ 1:4 ] &lt;- 1
itemcluster[ 6:9 ] &lt;- 2
itemcluster[ 10:12 ] &lt;- 3
# residual correlations
rho &lt;- c( .55, .25, .45 )

# simulate data
dat &lt;- sirt::sim.rasch.dep( theta, b, itemcluster, rho )
colnames(dat) &lt;- paste("I", seq(1,ncol(dat)), sep="")

# estimate Rasch copula model
mod1 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster, numdiff.parm=.001 )
summary(mod1)

# Rasch model estimation
mod2 &lt;- sirt::rasch.copula2( dat, itemcluster=itemcluster,
            delta=rep(0,3), est.delta=rep(0,3) )
summary(mod2)

# estimation with pairwise Rasch model
mod3 &lt;- sirt::rasch.pairwise( dat )
summary(mod3)

## End(Not run)
</code></pre>

<hr>
<h2 id='sim.raschtype'>
Simulate from Generalized Logistic Item Response Model
</h2><span id='topic+sim.raschtype'></span>

<h3>Description</h3>

<p>This function simulates dichotomous item responses from a
generalized logistic item response model (Stukel, 1988).
The four-parameter logistic item response model
(Loken &amp; Rulison, 2010) is a special case. See <code><a href="#topic+rasch.mml2">rasch.mml2</a></code>
for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.raschtype(theta, b, alpha1=0, alpha2=0, fixed.a=NULL,
    fixed.c=NULL, fixed.d=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.raschtype_+3A_theta">theta</code></td>
<td>

<p>Unidimensional ability vector <code class="reqn">\theta</code>
</p>
</td></tr>
<tr><td><code id="sim.raschtype_+3A_b">b</code></td>
<td>

<p>Vector of item difficulties <code class="reqn">b</code>
</p>
</td></tr>
<tr><td><code id="sim.raschtype_+3A_alpha1">alpha1</code></td>
<td>

<p>Parameter <code class="reqn">\alpha_1</code> in generalized logistic link function
</p>
</td></tr>
<tr><td><code id="sim.raschtype_+3A_alpha2">alpha2</code></td>
<td>

<p>Parameter <code class="reqn">\alpha_2</code> in generalized logistic link function
</p>
</td></tr>
<tr><td><code id="sim.raschtype_+3A_fixed.a">fixed.a</code></td>
<td>

<p>Vector of item slopes <code class="reqn">a</code></p>
</td></tr>
<tr><td><code id="sim.raschtype_+3A_fixed.c">fixed.c</code></td>
<td>

<p>Vector of lower item asymptotes <code class="reqn">c</code>
</p>
</td></tr>
<tr><td><code id="sim.raschtype_+3A_fixed.d">fixed.d</code></td>
<td>

<p>Vector of lower item asymptotes <code class="reqn">d</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The class of generalized logistic link functions contain
the most important link functions using the specifications (Stukel, 1988):
</p>
<p>logistic link function: <code class="reqn">\alpha_1=0</code> and <code class="reqn">\alpha_2=0</code> <br />
probit link function: <code class="reqn">\alpha_1=0.165</code> and <code class="reqn">\alpha_2=0.165</code> <br />
loglog link function: <code class="reqn">\alpha_1=-0.037</code> and <code class="reqn">\alpha_2=0.62</code> <br />
cloglog link function: <code class="reqn">\alpha_1=0.62</code> and <code class="reqn">\alpha_2=-0.037</code> <br />
</p>
<p>See <code><a href="#topic+pgenlogis">pgenlogis</a></code> for exact transformation formulas of
the mentioned link functions.
</p>


<h3>Value</h3>

<p>Data frame with simulated item responses
</p>


<h3>References</h3>

<p>Loken, E., &amp; Rulison, K. L. (2010). Estimation of a four-parameter
item response theory model. <em>British Journal of Mathematical
and Statistical Psychology, 63</em>, 509-525.
</p>
<p>Stukel, T. A. (1988). Generalized logistic models.
<em>Journal of the American Statistical Association,
83</em>, 426-431.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rasch.mml2">rasch.mml2</a></code>, <code><a href="#topic+pgenlogis">pgenlogis</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
## EXAMPLE 1: Simulation of data from a Rasch model (alpha_1=alpha_2=0)
#############################################################################

set.seed(9765)
N &lt;- 500    # number of persons
I &lt;- 11     # number of items
b &lt;- seq( -2, 2, length=I )
dat &lt;- sirt::sim.raschtype( stats::rnorm( N ), b )
colnames(dat) &lt;- paste0( "I", 1:I )
</code></pre>

<hr>
<h2 id='sirt_eigenvalues'>
First Eigenvalues of a Symmetric Matrix
</h2><span id='topic+sirt_eigenvalues'></span>

<h3>Description</h3>

<p>This function computes the first <code class="reqn">D</code> eigenvalues and eigenvectors of a
symmetric positive definite matrices. The eigenvalues are computed
by the Rayleigh quotient method (Lange, 2010, p. 120).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sirt_eigenvalues( X, D, maxit=200, conv=10^(-6) )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sirt_eigenvalues_+3A_x">X</code></td>
<td>

<p>Symmetric matrix
</p>
</td></tr>
<tr><td><code id="sirt_eigenvalues_+3A_d">D</code></td>
<td>

<p>Number of eigenvalues to be estimated
</p>
</td></tr>
<tr><td><code id="sirt_eigenvalues_+3A_maxit">maxit</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="sirt_eigenvalues_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>d</code></td>
<td>
<p>Vector of eigenvalues
</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>Matrix with eigenvectors in columns</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lange, K. (2010). <em>Numerical Analysis for Statisticians</em>.
New York: Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Sigma &lt;- diag(1,3)
Sigma[ lower.tri(Sigma) ] &lt;- Sigma[ upper.tri(Sigma) ] &lt;- c(.4,.6,.8 )
sirt::sirt_eigenvalues(X=Sigma, D=2 )
# compare with svd function
svd(Sigma)
</code></pre>

<hr>
<h2 id='sirt-defunct'>Defunct <span class="pkg">sirt</span> Functions</h2><span id='topic+sirt-defunct'></span><span id='topic+rasch.conquest'></span><span id='topic+rasch.pml2'></span><span id='topic+yen.q3'></span><span id='topic+testlet.yen.q3'></span>

<h3>Description</h3>

<p>These functions have been removed or replaced in the <span class="pkg">sirt</span>
package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch.conquest(...)
rasch.pml2(...)
testlet.yen.q3(...)
yen.q3(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sirt-defunct_+3A_...">...</code></td>
<td>
<p>Arguments to be passed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>rasch.conquest</code> function has been replaced by
<code><a href="#topic+R2conquest">R2conquest</a></code>.
</p>
<p>The <code>rasch.pml2</code> function has been superseded by
<code><a href="#topic+rasch.pml3">rasch.pml3</a></code>.
</p>
<p>The <code>testlet.yen.q3</code> function has been replaced by
<code><a href="#topic+Q3.testlet">Q3.testlet</a></code>.
</p>
<p>The <code>yen.q3</code> function has been replaced by
<code><a href="#topic+Q3">Q3</a></code>.
</p>

<hr>
<h2 id='sirt-utilities'>Utility Functions in <span class="pkg">sirt</span></h2><span id='topic+sirt-utilities'></span><span id='topic+tracemat'></span><span id='topic+pow'></span><span id='topic+soft_thresholding'></span><span id='topic+hard_thresholding'></span><span id='topic+bounds_parameters'></span><span id='topic+dimproper'></span><span id='topic+ginverse_sym'></span><span id='topic+sirt_rbind_fill'></span><span id='topic+sirt_fisherz'></span><span id='topic+sirt_antifisherz'></span><span id='topic+sirt_attach_list_elements'></span><span id='topic+sirt_summary_print_objects'></span><span id='topic+sirt_summary_print_package_rsession'></span><span id='topic+sirt_summary_print_package'></span><span id='topic+sirt_summary_print_rsession'></span><span id='topic+sirt_summary_print_call'></span><span id='topic+sirt_optimizer'></span><span id='topic+sirt_matrix2'></span><span id='topic+sirt_colMeans'></span><span id='topic+sirt_colSDs'></span><span id='topic+sirt_colMins'></span><span id='topic+sirt_colMaxs'></span><span id='topic+sirt_colMedians'></span><span id='topic+sirt_sum_norm'></span><span id='topic+sirt_dnorm_discrete'></span><span id='topic+sirt_abs_smooth'></span><span id='topic+sirt_permutations'></span><span id='topic+sirt_rcpp_discrete_inverse'></span><span id='topic+move_variables_df'></span><span id='topic+print_digits'></span>

<h3>Description</h3>

<p>Utility functions in <span class="pkg">sirt</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># bounds entries in a vector
bounds_parameters( pars, lower=NULL, upper=NULL)

# improper density function which always returns a value of 1
dimproper(x)

# generalized inverse of a symmetric function
ginverse_sym(A, eps=1E-8)
# hard thresholding function
hard_thresholding(x, lambda)
# soft thresholding function
soft_thresholding(x, lambda)

# power function x^a, like in Cpp
pow(x, a)
# trace of a matrix
tracemat(A)

#** matrix functions
sirt_matrix2(x, nrow)   # matrix() function with byrow=TRUE
sirt_colMeans(x, na.rm=TRUE)
sirt_colSDs(x, na.rm=TRUE)
sirt_colMins(x, na.rm=TRUE)
sirt_colMaxs(x, na.rm=TRUE)
sirt_colMedians(x, na.rm=TRUE)

#* normalize vector to have sum of one
sirt_sum_norm(x, na.rm=TRUE)
#* discrete normal distribution
sirt_dnorm_discrete(x, mean=0, sd=1, ...)

# plyr::rbind.fill implementation in sirt
sirt_rbind_fill(x, y)

# Fisher-z transformation, see psych::fisherz
sirt_fisherz(rho)
# inverse Fisher-z transformation, see psych::fisherz2r
sirt_antifisherz(z)

# smooth approximation of the absolute value function
sirt_abs_smooth(x, deriv=0, eps=1e-4)

# permutations with replacement
sirt_permutations(r,v)
  #-&gt; is equivalent to gtools::permutations(n=length(v), r=D, v=v, repeats.allowed=TRUE)

# attach all elements in a list in a specified environment
sirt_attach_list_elements(x, envir)

# switch between stats::optim and stats::nlminb
sirt_optimizer(optimizer, par, fn, grad=NULL, method="L-BFGS-B", hessian=TRUE,
                   control=list(), ...)

# print objects in a summary
sirt_summary_print_objects(obji, from=NULL, to=NULL, digits=3, rownames_null=TRUE,
      grep_string=NULL)
# print package version and R session
sirt_summary_print_package_rsession(pack)
# print package version
sirt_summary_print_package(pack)
# print R session
sirt_summary_print_rsession()
# print call
sirt_summary_print_call(CALL)

# print a data frame x with fixed numbers of digits after the decimal
print_digits(x, digits=NULL)

# discrete inverse function
sirt_rcpp_discrete_inverse(x0, y0, y)

# move variables in a data frame
move_variables_df(x, after_var, move_vars)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sirt-utilities_+3A_pars">pars</code></td>
<td>
<p>Numeric vector</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_lower">lower</code></td>
<td>
<p>Numeric vector</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_upper">upper</code></td>
<td>
<p>Numeric vector</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_x">x</code></td>
<td>
<p>Numeric vector or a matrix or a list</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_eps">eps</code></td>
<td>
<p>Numerical. Shrinkage parameter of eigenvalue in <code>ginverse_sym</code></p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_a">a</code></td>
<td>
<p>Numeric vector</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_lambda">lambda</code></td>
<td>
<p>Numeric value</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_a">A</code></td>
<td>
<p>Matrix</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_nrow">nrow</code></td>
<td>
<p>Integer</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_mean">mean</code></td>
<td>
<p>Numeric</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_sd">sd</code></td>
<td>
<p>Numeric</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_y">y</code></td>
<td>
<p>Matrix</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_rho">rho</code></td>
<td>
<p>Numeric</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_deriv">deriv</code></td>
<td>
<p>Integer indicating the order of derivative</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_z">z</code></td>
<td>
<p>Numeric</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_r">r</code></td>
<td>
<p>Integer</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_v">v</code></td>
<td>
<p>Vector</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_envir">envir</code></td>
<td>
<p>Environment</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_optimizer">optimizer</code></td>
<td>
<p>Can be one of the following optimizers: <code>optim</code>, <code>nlminb</code>,
<code>bobyqa</code> (from the <span class="pkg">minqa</span> packlage), <code>Rvmmin</code> (from the
<span class="pkg">optimx</span> package) or <code>nloptr</code> (from the <span class="pkg">nloptr</span> package using
the argument <code>opts$algorithm="NLOPT_LD_MMA"</code>).
</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_par">par</code></td>
<td>
<p>Initial parameter</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_fn">fn</code></td>
<td>
<p>Function</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_grad">grad</code></td>
<td>
<p>Gradient function</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_method">method</code></td>
<td>
<p>Optimization method</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_hessian">hessian</code></td>
<td>
<p>Logical</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_control">control</code></td>
<td>
<p>Control list for R optimizers</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_obji">obji</code></td>
<td>
<p>Data frame</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_from">from</code></td>
<td>
<p>Integer</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_to">to</code></td>
<td>
<p>Integer</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_digits">digits</code></td>
<td>
<p>Integer</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_rownames_null">rownames_null</code></td>
<td>
<p>Logical</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_grep_string">grep_string</code></td>
<td>
<p>String</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_pack">pack</code></td>
<td>
<p>Package name</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_call">CALL</code></td>
<td>
<p>Call statement</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_x0">x0</code></td>
<td>
<p>Vector</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_y0">y0</code></td>
<td>
<p>Vector</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_after_var">after_var</code></td>
<td>
<p>String indicating variable name after which variable specified
variables in <code>move_vars</code> should be moved</p>
</td></tr>
<tr><td><code id="sirt-utilities_+3A_move_vars">move_vars</code></td>
<td>
<p>Variables which should be moved after <code>after_var</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
## EXAMPLE 1: Trace of a matrix
#############################################################################

set.seed(86)
A &lt;- matrix( stats::runif(4), 2,2 )
tracemat(A)
sum(diag(A))    #=sirt::tracemat(A)

#############################################################################
## EXAMPLE 2: Power function
#############################################################################

x &lt;- 2.3
a &lt;- 1.7
pow(x=x,a=a)
x^a            #=sirt::pow(x,a)

#############################################################################
## EXAMPLE 3: Soft and hard thresholding function (e.g. in LASSO estimation)
#############################################################################

x &lt;- seq(-2, 2, length=100)
y &lt;- sirt::soft_thresholding( x, lambda=.5)
graphics::plot( x, y, type="l")

z &lt;- sirt::hard_thresholding( x, lambda=.5)
graphics::lines( x, z, lty=2, col=2)

#############################################################################
## EXAMPLE 4: Bounds on parameters
#############################################################################

pars &lt;- c(.721, .346)
bounds_parameters( pars=pars, lower=c(-Inf, .5), upper=c(Inf,1) )

#############################################################################
## EXAMPLE 5: Smooth approximation of absolute value function
#############################################################################

x &lt;- seq(-1,1,len=100)
graphics::plot(x, abs(x), lwd=2, col=1, lty=1, type="l", ylim=c(-1,1) )
# smooth approximation
tt &lt;- 2
graphics::lines(x, sirt::sirt_abs_smooth(x), lty=tt, col=tt, lwd=2)
# first derivative
tt &lt;- 3
graphics::lines(x, sirt::sirt_abs_smooth(x, deriv=1), lty=tt, col=tt, lwd=2)
# second derivative
tt &lt;- 4
graphics::lines(x, sirt::sirt_abs_smooth(x, deriv=2), lty=tt, col=tt, lwd=2)

# analytic computation of first and second derivative
stats::deriv( ~ sqrt(x^2 + eps), namevec="x", hessian=TRUE )

## Not run: 
#############################################################################
## EXAMPLE 6: Permutations with replacement
#############################################################################

D &lt;- 4
v &lt;- 0:1
sirt::sirt_permutations(r=D, v=v)
gtools::permutations(n=length(v), r=D, v=v, repeats.allowed=TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='smirt'>
Multidimensional Noncompensatory, Compensatory and Partially
Compensatory Item Response Model
</h2><span id='topic+smirt'></span><span id='topic+summary.smirt'></span><span id='topic+logLik.smirt'></span><span id='topic+anova.smirt'></span><span id='topic+IRT.irfprob.smirt'></span><span id='topic+IRT.likelihood.smirt'></span><span id='topic+IRT.posterior.smirt'></span><span id='topic+IRT.modelfit.smirt'></span><span id='topic+summary.IRT.modelfit.smirt'></span>

<h3>Description</h3>

<p>This function estimates the noncompensatory and compensatory multidimensional
item response model (Bolt &amp; Lall, 2003; Reckase, 2009) as well as
the partially compensatory item response model (Spray et al., 1990)
for dichotomous data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smirt(dat, Qmatrix, irtmodel="noncomp", est.b=NULL, est.a=NULL,
     est.c=NULL, est.d=NULL, est.mu.i=NULL, b.init=NULL, a.init=NULL,
     c.init=NULL, d.init=NULL, mu.i.init=NULL, Sigma.init=NULL,
     b.lower=-Inf, b.upper=Inf, a.lower=-Inf, a.upper=Inf,
     c.lower=-Inf, c.upper=Inf, d.lower=-Inf, d.upper=Inf,
     theta.k=seq(-6,6,len=20), theta.kDES=NULL,
     qmcnodes=0, mu.fixed=NULL, variance.fixed=NULL,  est.corr=FALSE,
     max.increment=1, increment.factor=1, numdiff.parm=0.0001,
     maxdevchange=0.1, globconv=0.001, maxiter=1000, msteps=4,
     mstepconv=0.001)

## S3 method for class 'smirt'
summary(object,...)

## S3 method for class 'smirt'
anova(object,...)

## S3 method for class 'smirt'
logLik(object,...)

## S3 method for class 'smirt'
IRT.irfprob(object,...)

## S3 method for class 'smirt'
IRT.likelihood(object,...)

## S3 method for class 'smirt'
IRT.posterior(object,...)

## S3 method for class 'smirt'
IRT.modelfit(object,...)

## S3 method for class 'IRT.modelfit.smirt'
summary(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smirt_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous item responses
</p>
</td></tr>
<tr><td><code id="smirt_+3A_qmatrix">Qmatrix</code></td>
<td>

<p>The Q-matrix which specifies the loadings to be estimated
</p>
</td></tr>
<tr><td><code id="smirt_+3A_irtmodel">irtmodel</code></td>
<td>

<p>The item response model. Options are the noncompensatory model (<code>"noncomp"</code>),
the compensatory model (<code>"comp"</code>) and
the partially compensatory model (<code>"partcomp"</code>).
See Details for more explanations.
</p>
</td></tr>
<tr><td><code id="smirt_+3A_est.b">est.b</code></td>
<td>

<p>An integer matrix (if <code>irtmodel="noncomp"</code>) or integer vector
(if <code>irtmodel="comp"</code>) for <code class="reqn">b</code> parameters to be estimated
</p>
</td></tr>
<tr><td><code id="smirt_+3A_est.a">est.a</code></td>
<td>

<p>An integer matrix for <code class="reqn">a</code> parameters to be estimated.
If <code>est.a="2PL"</code>, then all item loadings will be estimated and the
variances are set to one (and therefore <code>est.corr=TRUE</code>).
</p>
</td></tr>
<tr><td><code id="smirt_+3A_est.c">est.c</code></td>
<td>

<p>An integer vector for <code class="reqn">c</code> parameters to be estimated
</p>
</td></tr>
<tr><td><code id="smirt_+3A_est.d">est.d</code></td>
<td>

<p>An integer vector for <code class="reqn">d</code> parameters to be estimated
</p>
</td></tr>
<tr><td><code id="smirt_+3A_est.mu.i">est.mu.i</code></td>
<td>

<p>An integer vector for <code class="reqn">\mu_i</code> parameters to be estimated
</p>
</td></tr>
<tr><td><code id="smirt_+3A_b.init">b.init</code></td>
<td>

<p>Initial <code class="reqn">b</code> coefficients. For <code>irtmodel="noncomp"</code> it must be a
matrix, for <code>irtmodel="comp"</code> it is a vector.
</p>
</td></tr>
<tr><td><code id="smirt_+3A_a.init">a.init</code></td>
<td>

<p>Initial <code class="reqn">a</code> coefficients arranged in a matrix
</p>
</td></tr>
<tr><td><code id="smirt_+3A_c.init">c.init</code></td>
<td>

<p>Initial <code class="reqn">c</code> coefficients
</p>
</td></tr>
<tr><td><code id="smirt_+3A_d.init">d.init</code></td>
<td>

<p>Initial <code class="reqn">d</code> coefficients
</p>
</td></tr>
<tr><td><code id="smirt_+3A_mu.i.init">mu.i.init</code></td>
<td>

<p>Initial <code class="reqn">d</code> coefficients
</p>
</td></tr>
<tr><td><code id="smirt_+3A_sigma.init">Sigma.init</code></td>
<td>

<p>Initial covariance matrix <code class="reqn">\Sigma</code>
</p>
</td></tr>
<tr><td><code id="smirt_+3A_b.lower">b.lower</code></td>
<td>
<p>Lower bound for <code class="reqn">b</code> parameter</p>
</td></tr>
<tr><td><code id="smirt_+3A_b.upper">b.upper</code></td>
<td>
<p>Upper bound for <code class="reqn">b</code> parameter</p>
</td></tr>
<tr><td><code id="smirt_+3A_a.lower">a.lower</code></td>
<td>
<p>Lower bound for <code class="reqn">a</code> parameter</p>
</td></tr>
<tr><td><code id="smirt_+3A_a.upper">a.upper</code></td>
<td>
<p>Upper bound for <code class="reqn">a</code> parameter</p>
</td></tr>
<tr><td><code id="smirt_+3A_c.lower">c.lower</code></td>
<td>
<p>Lower bound for <code class="reqn">c</code> parameter</p>
</td></tr>
<tr><td><code id="smirt_+3A_c.upper">c.upper</code></td>
<td>
<p>Upper bound for <code class="reqn">c</code> parameter</p>
</td></tr>
<tr><td><code id="smirt_+3A_d.lower">d.lower</code></td>
<td>
<p>Lower bound for <code class="reqn">d</code> parameter</p>
</td></tr>
<tr><td><code id="smirt_+3A_d.upper">d.upper</code></td>
<td>
<p>Upper bound for <code class="reqn">d</code> parameter</p>
</td></tr>
<tr><td><code id="smirt_+3A_theta.k">theta.k</code></td>
<td>

<p>Vector of discretized trait distribution. This vector is expanded in all
dimensions by using the <code><a href="base.html#topic+expand.grid">base::expand.grid</a></code>
function. If a user specifies a design matrix <code>theta.kDES</code> of transformed
<code class="reqn">\bold{\theta}_p</code>
values (see Details and Examples), then <code>theta.k</code> must be a matrix, too.
</p>
</td></tr>
<tr><td><code id="smirt_+3A_theta.kdes">theta.kDES</code></td>
<td>

<p>An optional design matrix. This matrix will differ from the ordinary
theta grid in case of nonlinear item response models.
</p>
</td></tr>
<tr><td><code id="smirt_+3A_qmcnodes">qmcnodes</code></td>
<td>

<p>Number of integration nodes for quasi Monte Carlo integration (see Pan &amp;
Thompson, 2007; Gonzales et al., 2006). Integration points are obtained by using
the function <code><a href="#topic+qmc.nodes">qmc.nodes</a></code>. Note that when using quasi Monte Carlo nodes,
no theta design matrix <code>theta.kDES</code> can be specified.
See Example 1, Model 11.
</p>
</td></tr>
<tr><td><code id="smirt_+3A_mu.fixed">mu.fixed</code></td>
<td>

<p>Matrix with fixed entries in the mean vector. By default, all means
are set to zero.
</p>
</td></tr>
<tr><td><code id="smirt_+3A_variance.fixed">variance.fixed</code></td>
<td>

<p>Matrix (with rows and three columns) with fixed entries in the covariance matrix
(see Examples). The entry <code class="reqn">c_{kd}</code> of the covariance between dimensions
<code class="reqn">k</code> and <code class="reqn">d</code> is set to <code class="reqn">c_0</code> iff <code>variance.fixed</code> has a row with
a <code class="reqn">k</code> in the first column, a <code class="reqn">d</code> in the second column and the value
<code class="reqn">c_0</code> in the third column.
</p>
</td></tr>
<tr><td><code id="smirt_+3A_est.corr">est.corr</code></td>
<td>

<p>Should only a correlation matrix instead of a covariance matrix
be estimated?
</p>
</td></tr>
<tr><td><code id="smirt_+3A_max.increment">max.increment</code></td>
<td>

<p>Maximum increment
</p>
</td></tr>
<tr><td><code id="smirt_+3A_increment.factor">increment.factor</code></td>
<td>

<p>A value (larger than one) which defines the extent of the decrease of the maximum
increment of item parameters in every iteration. The maximum increment in iteration
<code>iter</code> is defined as <code>max.increment*increment.factor^(-iter)</code>
where <code>max.increment=1</code>. Using a value larger than 1 helps
to reach convergence in some non-converging analyses (use values of 1.01, 1.02
or even 1.05). See also Example 1 Model 2a.
</p>
</td></tr>
<tr><td><code id="smirt_+3A_numdiff.parm">numdiff.parm</code></td>
<td>

<p>Numerical differentiation parameter
</p>
</td></tr>
<tr><td><code id="smirt_+3A_maxdevchange">maxdevchange</code></td>
<td>

<p>Convergence criterion for change in relative deviance
</p>
</td></tr>
<tr><td><code id="smirt_+3A_globconv">globconv</code></td>
<td>

<p>Global convergence criterion for parameter change
</p>
</td></tr>
<tr><td><code id="smirt_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td></tr>
<tr><td><code id="smirt_+3A_msteps">msteps</code></td>
<td>

<p>Number of iterations within a M step
</p>
</td></tr>
<tr><td><code id="smirt_+3A_mstepconv">mstepconv</code></td>
<td>

<p>Convergence criterion within a M step
</p>
</td></tr>
<tr><td><code id="smirt_+3A_object">object</code></td>
<td>

<p>Object of class <code>smirt</code>
</p>
</td></tr>
<tr><td><code id="smirt_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The noncompensatory item response model (<code>irtmodel="noncomp"</code>;
e.g. Bolt &amp; Lall, 2003) is defined as
</p>
<p style="text-align: center;"><code class="reqn">P(X_{pi}=1 | \bold{\theta}_p )=c_i + (d_i - c_i )
\prod_l invlogit( a_{il} q_{il}  \theta_{pl} - b_{il} ) </code>
</p>

<p>where <code class="reqn">i</code>, <code class="reqn">p</code>, <code class="reqn">l</code> denote items, persons and dimensions
respectively.
</p>
<p>The compensatory item response model (<code>irtmodel="comp"</code>) is defined by
</p>
<p style="text-align: center;"><code class="reqn">P(X_{pi}=1 | \bold{\theta}_p )=c_i + (d_i - c_i )
invlogit( \sum_l  a_{il}  q_{il} \theta_{pl} - b_{i} ) </code>
</p>

<p>Using a design matrix <code>theta.kDES</code> the model can be made even more general
in a model which is linear in item parameters
</p>
<p style="text-align: center;"><code class="reqn">P(X_{pi}=1 | \bold{\theta}_p )=c_i + (d_i - c_i )
invlogit( \sum_l  a_{il}  q_{il} t_l ( \bold{ \theta_{p} } )   - b_{i} ) </code>
</p>

<p>with known functions <code class="reqn">t_l</code> of the trait vector <code class="reqn">\bold{\theta}_p</code>.
Fixed values of the functions <code class="reqn">t_l</code> are specified in the
<code class="reqn">\bold{\theta}_p</code> design matrix <code>theta.kDES</code>.
</p>
<p>The partially compensatory item response model (<code>irtmodel="partcomp"</code>)
is defined by
</p>
<p style="text-align: center;"><code class="reqn">P(X_{pi}=1 | \bold{\theta}_p )=c_i + (d_i - c_i )
\frac{ \exp \left( \sum_l ( a_{il}  q_{il} \theta_{pl} - b_{il} ) \right) }
{  \mu_i \prod_l ( 1 + \exp ( a_{il}  q_{il} \theta_{pl} - b_{il} ) ) +
    ( 1- \mu_i) ( 1 + \exp \left( \sum_l  ( a_{il}  q_{il} \theta_{pl} - b_{il} ) \right) )
        } </code>
</p>

<p>with item parameters <code class="reqn">\mu_i</code> indicating the degree of compensatory.
<code class="reqn">\mu_i=1</code> indicates a noncompensatory model while <code class="reqn">\mu_i=0</code>
indicates a (fully) compensatory model.
</p>
<p>The models are estimated by an EM algorithm employing marginal maximum
likelihood.
</p>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria</p>
</td></tr>
<tr><td><code>item</code></td>
<td>
<p>Data frame with item parameters</p>
</td></tr>
<tr><td><code>person</code></td>
<td>
<p>Data frame with person parameters. It includes
the person mean of all item responses (<code>M</code>; percentage correct
of all non-missing items), the
EAP estimate and its corresponding standard error for all dimensions
(<code>EAP</code> and <code>SE.EAP</code>) and the maximum likelihood estimate
as well as the mode of the posterior distribution (<code>MLE</code> and <code>MAP</code>).</p>
</td></tr>
<tr><td><code>EAP.rel</code></td>
<td>
<p>EAP reliability</p>
</td></tr>
<tr><td><code>mean.trait</code></td>
<td>
<p>Means of trait</p>
</td></tr>
<tr><td><code>sd.trait</code></td>
<td>
<p>Standard deviations of trait</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>Trait covariance matrix</p>
</td></tr>
<tr><td><code>cor.trait</code></td>
<td>
<p>Trait correlation matrix</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>Matrix (vector) of <code class="reqn">b</code> parameters</p>
</td></tr>
<tr><td><code>se.b</code></td>
<td>
<p>Matrix (vector) of standard errors <code class="reqn">b</code> parameters</p>
</td></tr>
<tr><td><code>a</code></td>
<td>
<p>Matrix of <code class="reqn">a</code> parameters</p>
</td></tr>
<tr><td><code>se.a</code></td>
<td>
<p>Matrix of standard errors of <code class="reqn">a</code> parameters</p>
</td></tr>
<tr><td><code>c</code></td>
<td>
<p>Vector of <code class="reqn">c</code> parameters</p>
</td></tr>
<tr><td><code>se.c</code></td>
<td>
<p>Vector of standard errors of <code class="reqn">c</code> parameters</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>Vector of <code class="reqn">d</code> parameters</p>
</td></tr>
<tr><td><code>se.d</code></td>
<td>
<p>Vector of standard errors of <code class="reqn">d</code> parameters</p>
</td></tr>
<tr><td><code>mu.i</code></td>
<td>
<p>Vector of <code class="reqn">\mu_i</code> parameters</p>
</td></tr>
<tr><td><code>se.mu.i</code></td>
<td>
<p>Vector of standard errors of <code class="reqn">\mu_i</code> parameters</p>
</td></tr>
<tr><td><code>f.yi.qk</code></td>
<td>
<p>Individual likelihood</p>
</td></tr>
<tr><td><code>f.qk.yi</code></td>
<td>
<p>Individual posterior</p>
</td></tr>
<tr><td><code>probs</code></td>
<td>
<p>Probabilities of item response functions evaluated at <code>theta.k</code></p>
</td></tr>
<tr><td><code>n.ik</code></td>
<td>
<p>Expected counts</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>dat2</code></td>
<td>
<p>Processed data set</p>
</td></tr>
<tr><td><code>dat2.resp</code></td>
<td>
<p>Data set of response indicators</p>
</td></tr>
<tr><td><code>I</code></td>
<td>
<p>Number of items</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>Number of dimensions</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>Maximum item response score</p>
</td></tr>
<tr><td><code>theta.k</code></td>
<td>
<p>Used theta integration grid</p>
</td></tr>
<tr><td><code>pi.k</code></td>
<td>
<p>Distribution function evaluated at <code>theta.k</code></p>
</td></tr>
<tr><td><code>irtmodel</code></td>
<td>
<p>Used IRT model</p>
</td></tr>
<tr><td><code>Qmatrix</code></td>
<td>
<p>Used Q-matrix</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bolt, D. M., &amp; Lall, V. F. (2003). Estimation of compensatory and
noncompensatory multidimensional item response models using Markov chain
Monte Carlo. <em>Applied Psychological Measurement, 27</em>, 395-414.
</p>
<p>Gonzalez, J., Tuerlinckx, F., De Boeck, P., &amp; Cools, R. (2006).
Numerical integration in logistic-normal models.
<em>Computational Statistics &amp; Data Analysis, 51</em>, 1535-1548.
</p>
<p>Pan, J., &amp; Thompson, R. (2007). Quasi-Monte Carlo estimation in
generalized linear mixed models. <em>Computational Statistics &amp;
Data Analysis, 51</em>, 5765-5775.
</p>
<p>Reckase, M. (2009). <em>Multidimensional item response theory</em>.
New York: Springer. <a href="https://doi.org/10.1007/978-0-387-89976-3">doi:10.1007/978-0-387-89976-3</a>
</p>
<p>Spray, J. A., Davey, T. C., Reckase, M. D., Ackerman, T. A., &amp; Carlson, J. E. (1990).
<em>Comparison of two logistic multidimensional item response theory models</em>.
ACT Research Report No. ACT-RR-ONR-90-8.
</p>


<h3>See Also</h3>

<p>See the <code><a href="mirt.html#topic+mirt">mirt::mirt</a></code> and
<code>itemtype="partcomp"</code> for estimating noncompensatory item response models
using the <span class="pkg">mirt</span> package. See also <code><a href="mirt.html#topic+mixedmirt">mirt::mixedmirt</a></code>.
</p>
<p>Other multidimensional IRT models can also be estimated
with <code><a href="#topic+rasch.mml2">rasch.mml2</a></code> and <code><a href="#topic+rasch.mirtlc">rasch.mirtlc</a></code>.
</p>
<p>See <code><a href="CDM.html#topic+itemfit.sx2">itemfit.sx2</a></code> (<span class="pkg">CDM</span>) for item fit
statistics.
</p>
<p>See also the <span class="pkg">mirt</span> and <span class="pkg">TAM</span> packages for estimation of
compensatory multidimensional item response models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
## EXAMPLE 1: Noncompensatory and compensatory IRT models
#############################################################################
set.seed(997)

# (1) simulate data from a two-dimensional noncompensatory
#     item response model
#   -&gt; increase number of iterations in all models!

N &lt;- 1000    # number of persons
I &lt;- 10        # number of items
theta0 &lt;- rnorm( N, sd=1 )
theta1 &lt;- theta0 + rnorm(N, sd=.7 )
theta2 &lt;- theta0 + rnorm(N, sd=.7 )
Q &lt;- matrix( 1, nrow=I,ncol=2 )
Q[ 1:(I/2), 2 ] &lt;- 0
Q[ I,1] &lt;- 0
b &lt;- matrix( rnorm( I*2 ), I, 2 )
a &lt;- matrix( 1, I, 2 )

# simulate data
prob &lt;- dat &lt;- matrix(0, nrow=N, ncol=I )
for (ii in 1:I){
prob[,ii] &lt;- ( stats::plogis( theta1 - b[ii,1]  ) )^Q[ii,1]
prob[,ii] &lt;- prob[,ii] * ( stats::plogis( theta2 - b[ii,2]  ) )^Q[ii,2]
            }
dat[ prob &gt; matrix( stats::runif( N*I),N,I) ] &lt;- 1
colnames(dat) &lt;- paste0("I",1:I)

#***
# Model 1: Noncompensatory 1PL model
mod1 &lt;- sirt::smirt(dat, Qmatrix=Q, maxiter=10 ) # change number of iterations
summary(mod1)

## Not run: 
#***
# Model 2: Noncompensatory 2PL model
mod2 &lt;- sirt::smirt(dat,Qmatrix=Q, est.a="2PL", maxiter=15 )
summary(mod2)

# Model 2a: avoid convergence problems with increment.factor
mod2a &lt;- sirt::smirt(dat,Qmatrix=Q, est.a="2PL", maxiter=30, increment.factor=1.03)
summary(mod2a)

#***
# Model 3: some fixed c and d parameters different from zero or one
c.init &lt;- rep(0,I)
c.init[ c(3,7)] &lt;- .2
d.init &lt;- rep(1,I)
d.init[c(4,8)] &lt;- .95
mod3 &lt;- sirt::smirt( dat, Qmatrix=Q, c.init=c.init, d.init=d.init )
summary(mod3)

#***
# Model 4: some estimated c and d parameters (in parameter groups)
est.c &lt;- c.init &lt;- rep(0,I)
c.estpars &lt;- c(3,6,7)
c.init[ c.estpars ] &lt;- .2
est.c[c.estpars] &lt;- 1
est.d &lt;- rep(0,I)
d.init &lt;- rep(1,I)
d.estpars &lt;- c(6,9)
d.init[ d.estpars ] &lt;- .95
est.d[ d.estpars ] &lt;- d.estpars   # different d parameters
mod4 &lt;- sirt::smirt(dat,Qmatrix=Q, est.c=est.c, c.init=c.init,
            est.d=est.d, d.init=d.init  )
summary(mod4)

#***
# Model 5: Unidimensional 1PL model
Qmatrix &lt;- matrix( 1, nrow=I, ncol=1 )
mod5 &lt;- sirt::smirt( dat, Qmatrix=Qmatrix )
summary(mod5)

#***
# Model 6: Unidimensional 2PL model
mod6 &lt;- sirt::smirt( dat, Qmatrix=Qmatrix, est.a="2PL" )
summary(mod6)

#***
# Model 7: Compensatory model with between item dimensionality
# Note that the data is simulated under the noncompensatory condition
# Therefore Model 7 should have a worse model fit than Model 1
Q1 &lt;- Q
Q1[ 6:10, 1] &lt;- 0
mod7 &lt;- sirt::smirt(dat,Qmatrix=Q1, irtmodel="comp", maxiter=30)
summary(mod7)

#***
# Model 8: Compensatory model with within item dimensionality
#         assuming zero correlation between dimensions
variance.fixed &lt;- as.matrix( cbind( 1,2,0) )
# set the covariance between the first and second dimension to zero
mod8 &lt;- sirt::smirt(dat,Qmatrix=Q, irtmodel="comp", variance.fixed=variance.fixed,
            maxiter=30)
summary(mod8)

#***
# Model 8b: 2PL model with starting values for a and b parameters
b.init &lt;- rep(0,10)  # set all item difficulties initially to zero
# b.init &lt;- NULL
a.init &lt;- Q       # initialize a.init with Q-matrix
# provide starting values for slopes of first three items on Dimension 1
a.init[1:3,1] &lt;- c( .55, .32, 1.3)

mod8b &lt;- sirt::smirt(dat,Qmatrix=Q, irtmodel="comp", variance.fixed=variance.fixed,
              b.init=b.init, a.init=a.init, maxiter=20, est.a="2PL" )
summary(mod8b)

#***
# Model 9: Unidimensional model with quadratic item response functions
# define theta
theta.k &lt;- seq( - 6, 6, len=15 )
theta.k &lt;- as.matrix( theta.k, ncol=1 )
# define design matrix
theta.kDES &lt;- cbind( theta.k[,1], theta.k[,1]^2 )
# define Q-matrix
Qmatrix &lt;- matrix( 0, I, 2 )
Qmatrix[,1] &lt;- 1
Qmatrix[ c(3,6,7), 2 ] &lt;- 1
colnames(Qmatrix) &lt;- c("F1", "F1sq" )
# estimate model
mod9 &lt;- sirt::smirt(dat,Qmatrix=Qmatrix, maxiter=50, irtmodel="comp",
           theta.k=theta.k, theta.kDES=theta.kDES, est.a="2PL" )
summary(mod9)

#***
# Model 10: Two-dimensional item response model with latent interaction
#           between dimensions
theta.k &lt;- seq( - 6, 6, len=15 )
theta.k &lt;- expand.grid( theta.k, theta.k )    # expand theta to 2 dimensions
# define design matrix
theta.kDES &lt;- cbind( theta.k, theta.k[,1]*theta.k[,2] )
# define Q-matrix
Qmatrix &lt;- matrix( 0, I, 3 )
Qmatrix[,1] &lt;- 1
Qmatrix[ 6:10, c(2,3) ] &lt;- 1
colnames(Qmatrix) &lt;- c("F1", "F2", "F1iF2" )
# estimate model
mod10 &lt;- sirt::smirt(dat,Qmatrix=Qmatrix,irtmodel="comp", theta.k=theta.k,
            theta.kDES=theta.kDES, est.a="2PL" )
summary(mod10)

#****
# Model 11: Example Quasi Monte Carlo integration
Qmatrix &lt;- matrix( 1, I, 1 )
mod11 &lt;- sirt::smirt( dat, irtmodel="comp", Qmatrix=Qmatrix, qmcnodes=1000 )
summary(mod11)

#############################################################################
## EXAMPLE 2: Dataset Reading data.read
##            Multidimensional models for dichotomous data
#############################################################################

data(data.read)
dat &lt;- data.read
I &lt;- ncol(dat)    # number of items

#***
# Model 1: 3-dimensional 2PL model

# define Q-matrix
Qmatrix &lt;- matrix(0,nrow=I,ncol=3)
Qmatrix[1:4,1] &lt;- 1
Qmatrix[5:8,2] &lt;- 1
Qmatrix[9:12,3] &lt;- 1

# estimate model
mod1 &lt;- sirt::smirt( dat, Qmatrix=Qmatrix, irtmodel="comp", est.a="2PL",
            qmcnodes=1000, maxiter=20)
summary(mod1)

#***
# Model 2: 3-dimensional Rasch model
mod2 &lt;- sirt::smirt( dat, Qmatrix=Qmatrix, irtmodel="comp",
              qmcnodes=1000, maxiter=20)
summary(mod2)

#***
# Model 3: 3-dimensional 2PL model with uncorrelated dimensions
# fix entries in variance matrix
variance.fixed &lt;- cbind( c(1,1,2), c(2,3,3), 0 )
# set the following covariances to zero: cov[1,2]=cov[1,3]=cov[2,3]=0

# estimate model
mod3 &lt;- sirt::smirt( dat, Qmatrix=Qmatrix, irtmodel="comp", est.a="2PL",
             variance.fixed=variance.fixed, qmcnodes=1000, maxiter=20)
summary(mod3)

#***
# Model 4: Bifactor model with one general factor (g) and
#          uncorrelated specific factors

# define a new Q-matrix
Qmatrix1 &lt;- cbind( 1, Qmatrix )
# uncorrelated factors
variance.fixed &lt;- cbind( c(1,1,1,2,2,3), c(2,3,4,3,4,4), 0 )
# The first dimension refers to the general factors while the other
# dimensions refer to the specific factors.
# The specification means that:
# Cov[1,2]=Cov[1,3]=Cov[1,4]=Cov[2,3]=Cov[2,4]=Cov[3,4]=0

# estimate model
mod4 &lt;- sirt::smirt( dat, Qmatrix=Qmatrix1, irtmodel="comp", est.a="2PL",
             variance.fixed=variance.fixed, qmcnodes=1000, maxiter=20)
summary(mod4)

#############################################################################
## EXAMPLE 3: Partially compensatory model
#############################################################################

#**** simulate data
set.seed(7656)
I &lt;- 10         # number of items
N &lt;- 2000        # number of subjects
Q &lt;- matrix( 0, 3*I,2)  # Q-matrix
Q[1:I,1] &lt;- 1
Q[1:I + I,2] &lt;- 1
Q[1:I + 2*I,1:2] &lt;- 1
b &lt;- matrix( stats::runif( 3*I *2, -2, 2 ), nrow=3*I, 2 )
b &lt;- b*Q
b &lt;- round( b, 2 )
mui &lt;- rep(0,3*I)
mui[ seq(2*I+1, 3*I) ] &lt;- 0.65
# generate data
dat &lt;- matrix( NA, N, 3*I )
colnames(dat) &lt;- paste0("It", 1:(3*I) )
# simulate item responses
library(mvtnorm)
theta &lt;- mvtnorm::rmvnorm(N, mean=c(0,0), sigma=matrix( c( 1.2, .6,.6,1.6),2, 2 ) )
for (ii in 1:(3*I)){
    # define probability
    tmp1 &lt;- exp( theta[,1] * Q[ii,1] - b[ii,1] +  theta[,2] * Q[ii,2] - b[ii,2] )
    # non-compensatory model
    nco1 &lt;- ( 1 + exp( theta[,1] * Q[ii,1] - b[ii,1] ) ) *
                  ( 1 + exp( theta[,2] * Q[ii,2] - b[ii,2] ) )
    co1 &lt;- ( 1 + tmp1 )
    p1 &lt;- tmp1 / ( mui[ii] * nco1 + ( 1 - mui[ii] )*co1 )
    dat[,ii] &lt;- 1 * ( stats::runif(N) &lt; p1 )
}

#*** Model 1: Joint mu.i parameter for all items
est.mu.i &lt;- rep(0,3*I)
est.mu.i[ seq(2*I+1,3*I)] &lt;- 1
mod1 &lt;- sirt::smirt( dat, Qmatrix=Q, irtmodel="partcomp", est.mu.i=est.mu.i)
summary(mod1)

#*** Model 2: Separate mu.i parameter for all items
est.mu.i[ seq(2*I+1,3*I)] &lt;- 1:I
mod2 &lt;- sirt::smirt( dat, Qmatrix=Q, irtmodel="partcomp", est.mu.i=est.mu.i)
summary(mod2)

## End(Not run)
</code></pre>

<hr>
<h2 id='stratified.cronbach.alpha'>
Stratified Cronbach's Alpha
</h2><span id='topic+stratified.cronbach.alpha'></span>

<h3>Description</h3>

<p>This function computes the stratified Cronbach's Alpha
for composite scales (Cronbach, Schoenemann &amp; McKie, 1965;
He, 2010; Meyer, 2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stratified.cronbach.alpha(data, itemstrata=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stratified.cronbach.alpha_+3A_data">data</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame
</p>
</td></tr>
<tr><td><code id="stratified.cronbach.alpha_+3A_itemstrata">itemstrata</code></td>
<td>

<p>A matrix with two columns defining the item stratification.
The first column contains the item names, the second column
the item stratification label (these can be integers).
The default <code>NULL</code> does only compute Cronbach's Alpha
for the whole scale.
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Cronbach, L. J., Schoenemann, P., &amp; McKie, D. (1965).
Alpha coefficient for stratified-parallel tests.
<em>Educational and Psychological Measurement, 25</em>, 291-312.
<a href="https://doi.org/10.1177/001316446502500201">doi:10.1177/001316446502500201</a>
</p>
<p>He, Q. (2010). <em>Estimating the reliability of composite scores</em>.
Ofqual/10/4703. Coventry: The Office of Qualifications and Examinations Regulation.
</p>
<p>Meyer, P. (2010). <em>Reliability</em>. Cambridge: Oxford University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: data.read
#############################################################################

data(data.read, package="sirt")
dat &lt;- data.read
I &lt;- ncol(dat)

# apply function without defining item strata
sirt::stratified.cronbach.alpha( data.read  )

# define item strata
itemstrata &lt;- cbind( colnames(dat), substring( colnames(dat), 1,1 ) )
sirt::stratified.cronbach.alpha( dat, itemstrata=itemstrata )
  ##   scale  I alpha mean.tot var.tot alpha.stratified
  ## 1 total 12 0.677    8.680   5.668            0.703
  ## 2     A  4 0.545    2.616   1.381               NA
  ## 3     B  4 0.381    2.811   1.059               NA
  ## 4     C  4 0.640    3.253   1.107               NA

## Not run: 
#**************************
# reliability analysis in psych package
library(psych)
# Cronbach's alpha and item discriminations
psych::alpha(dat)
# McDonald's omega
psych::omega(dat, nfactors=1)     # 1 factor
  ##   Alpha:                 0.69
  ##   Omega Total            0.69
##=&gt; Note that alpha in this function is the standardized Cronbach's
##     alpha, i.e. alpha computed for standardized variables.
psych::omega(dat, nfactors=2)     # 2 factors
  ##   Omega Total            0.72
psych::omega(dat, nfactors=3)     # 3 factors
  ##   Omega Total            0.74

## End(Not run)
</code></pre>

<hr>
<h2 id='summary.mcmc.sirt'>
Summary Method for Objects of Class <code>mcmc.sirt</code>
</h2><span id='topic+summary.mcmc.sirt'></span>

<h3>Description</h3>

<p>S3 method to summarize objects of class <code>mcmc.sirt</code>.
This object is generated by following functions: <code><a href="#topic+mcmc.2pno">mcmc.2pno</a></code>,
<code><a href="#topic+mcmc.2pnoh">mcmc.2pnoh</a></code>, <code><a href="#topic+mcmc.3pno.testlet">mcmc.3pno.testlet</a></code>,
<code><a href="#topic+mcmc.2pno.ml">mcmc.2pno.ml</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcmc.sirt'
summary(object,digits=3, file=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.mcmc.sirt_+3A_object">object</code></td>
<td>

<p>Object of class <code>mcmc.sirt</code>
</p>
</td></tr>
<tr><td><code id="summary.mcmc.sirt_+3A_digits">digits</code></td>
<td>
<p>Number of digits after decimal</p>
</td></tr>
<tr><td><code id="summary.mcmc.sirt_+3A_file">file</code></td>
<td>
<p>Optional file name to which <code>summary</code> output is written</p>
</td></tr>
<tr><td><code id="summary.mcmc.sirt_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+mcmc.2pno">mcmc.2pno</a></code>, <code><a href="#topic+mcmc.2pnoh">mcmc.2pnoh</a></code>,
<code><a href="#topic+mcmc.3pno.testlet">mcmc.3pno.testlet</a></code>, <code><a href="#topic+mcmc.2pno.ml">mcmc.2pno.ml</a></code>
</p>

<hr>
<h2 id='tam2mirt'>
Converting a fitted <code>TAM</code> Object into a <code>mirt</code> Object
</h2><span id='topic+tam2mirt'></span>

<h3>Description</h3>

<p>Converts a fitted <code>TAM</code> object into a <code>mirt</code> object.
As a by-product, <code>lavaan</code> syntax is generated which can
be used with <code><a href="#topic+lavaan2mirt">lavaan2mirt</a></code> for re-estimating
the model in the <span class="pkg">mirt</span> package.
Up to now, only single group models are supported.
There must not exist background covariates (no latent regression
models!).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tam2mirt(tamobj)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tam2mirt_+3A_tamobj">tamobj</code></td>
<td>

<p>Object of class  <code><a href="TAM.html#topic+tam.mml">TAM::tam.mml</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>mirt</code></td>
<td>
<p>Object generated by <code>mirt</code> function if
<code>est.mirt=TRUE</code></p>
</td></tr>
<tr><td><code>mirt.model</code></td>
<td>
<p>Generated <code>mirt</code> model</p>
</td></tr>
<tr><td><code>mirt.syntax</code></td>
<td>
<p>Generated <code>mirt</code> syntax</p>
</td></tr>
<tr><td><code>mirt.pars</code></td>
<td>
<p>Generated parameter specifications
in <code>mirt</code></p>
</td></tr>
<tr><td><code>lavaan.model</code></td>
<td>
<p>Used <code>lavaan</code> model transformed by
<code>lavaanify</code> function</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Used dataset. If necessary, only items used in the
model are included in the dataset.</p>
</td></tr>
<tr><td><code>lavaan.syntax.fixed</code></td>
<td>
<p>Generated <code>lavaan</code> syntax
with fixed parameter estimates.</p>
</td></tr>
<tr><td><code>lavaan.syntax.freed</code></td>
<td>
<p>Generated <code>lavaan</code> syntax
with freed parameters for estimation.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>See <code><a href="#topic+mirt.wrapper">mirt.wrapper</a></code> for convenience wrapper functions
for <code><a href="mirt.html#topic+mirt">mirt</a></code> objects.
</p>
<p>See <code><a href="#topic+lavaan2mirt">lavaan2mirt</a></code> for converting <code>lavaan</code>
syntax to <code>mirt</code> syntax.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(TAM)
library(mirt)

#############################################################################
# EXAMPLE 1: Estimations in TAM for data.read dataset
#############################################################################

data(data.read)
dat &lt;- data.read

#**************************************
#*** Model 1: Rasch model
#**************************************

# estimation in TAM package
mod &lt;- TAM::tam.mml( dat )
summary(mod)
# conversion to mirt
res &lt;- sirt::tam2mirt(mod)
# generated lavaan syntax
cat(res$lavaan.syntax.fixed)
cat(res$lavaan.syntax.freed)
# extract object of class mirt
mres &lt;- res$mirt
# print and parameter values
print(mres)
mirt::mod2values(mres)
# model fit
mirt::M2(mres)
# residual statistics
mirt::residuals(mres, type="Q3")
mirt::residuals(mres, type="LD")
# item fit
mirt::itemfit(mres)
# person fit
mirt::personfit(mres)
# compute several types of factor scores (quite slow)
f1 &lt;- mirt::fscores(mres, method='WLE',response.pattern=dat[1:10,])
     # method=MAP and EAP also possible
# item plot
mirt::itemplot(mres,"A3")    # item A3
mirt::itemplot(mres,4)       # fourth item
# some more plots
plot(mres,type="info")
plot(mres,type="score")
plot(mres,type="trace")
# compare estimates with estimated Rasch model in mirt
mres1 &lt;- mirt::mirt(dat,1,"Rasch" )
print(mres1)
mirt.wrapper.coef(mres1)

#**************************************
#*** Model 2: 2PL model
#**************************************

# estimation in TAM
mod &lt;- TAM::tam.mml.2pl( dat )
summary(mod)
# conversion to mirt
res &lt;- sirt::tam2mirt(mod)
mres &lt;- res$mirt
# lavaan syntax
cat(res$lavaan.syntax.fixed)
cat(res$lavaan.syntax.freed)
# parameter estimates
print(mres)
mod2values(mres)
mres@nest   # number of estimated parameters
# some plots
plot(mres,type="info")
plot(mres,type="score")
plot(mres,type="trace")
# model fit
mirt::M2(mres)
# residual statistics
mirt::residuals(mres, type="Q3")
mirt::residuals(mres, type="LD")
# item fit
mirt::itemfit(mres)

#**************************************
#*** Model 3: 3-dimensional Rasch model
#**************************************

# define Q-matrix
Q &lt;- matrix( 0, nrow=12, ncol=3 )
Q[ cbind(1:12, rep(1:3,each=4) ) ] &lt;- 1
rownames(Q) &lt;- colnames(dat)
colnames(Q) &lt;- c("A","B","C")
# estimation in TAM
mod &lt;- TAM::tam.mml( resp=dat, Q=Q, control=list(snodes=1000,maxiter=30) )
summary(mod)
# mirt conversion
res &lt;- sirt::tam2mirt(mod)
mres &lt;- res$mirt
# mirt syntax
cat(res$mirt.syntax)
  ##   Dim01=1,2,3,4
  ##   Dim02=5,6,7,8
  ##   Dim03=9,10,11,12
  ##   COV=Dim01*Dim01,Dim02*Dim02,Dim03*Dim03,Dim01*Dim02,Dim01*Dim03,Dim02*Dim03
  ##   MEAN=Dim01,Dim02,Dim03
# lavaan syntax
cat(res$lavaan.syntax.freed)
  ##   Dim01=~ 1*A1+1*A2+1*A3+1*A4
  ##   Dim02=~ 1*B1+1*B2+1*B3+1*B4
  ##   Dim03=~ 1*C1+1*C2+1*C3+1*C4
  ##   A1 | t1_1*t1
  ##   A2 | t1_2*t1
  ##   A3 | t1_3*t1
  ##   A4 | t1_4*t1
  ##   B1 | t1_5*t1
  ##   B2 | t1_6*t1
  ##   B3 | t1_7*t1
  ##   B4 | t1_8*t1
  ##   C1 | t1_9*t1
  ##   C2 | t1_10*t1
  ##   C3 | t1_11*t1
  ##   C4 | t1_12*t1
  ##   Dim01 ~ 0*1
  ##   Dim02 ~ 0*1
  ##   Dim03 ~ 0*1
  ##   Dim01 ~~ Cov_11*Dim01
  ##   Dim02 ~~ Cov_22*Dim02
  ##   Dim03 ~~ Cov_33*Dim03
  ##   Dim01 ~~ Cov_12*Dim02
  ##   Dim01 ~~ Cov_13*Dim03
  ##   Dim02 ~~ Cov_23*Dim03
# model fit
mirt::M2(mres)
# residual statistics
residuals(mres,type="LD")
# item fit
mirt::itemfit(mres)

#**************************************
#*** Model 4: 3-dimensional 2PL model
#**************************************

# estimation in TAM
mod &lt;- TAM::tam.mml.2pl( resp=dat, Q=Q, control=list(snodes=1000,maxiter=30) )
summary(mod)
# mirt conversion
res &lt;- sirt::tam2mirt(mod)
mres &lt;- res$mirt
# generated lavaan syntax
cat(res$lavaan.syntax.fixed)
cat(res$lavaan.syntax.freed)
# write lavaan syntax on disk
  sink( "mod4_lav_freed.txt", split=TRUE )
cat(res$lavaan.syntax.freed)
  sink()
# some statistics from mirt
print(mres)
summary(mres)
mirt::M2(mres)
mirt::residuals(mres)
mirt::itemfit(mres)

# estimate mirt model by using the generated lavaan syntax with freed parameters
res2 &lt;- sirt::lavaan2mirt( dat, res$lavaan.syntax.freed,
            technical=list(NCYCLES=3), verbose=TRUE)
                 # use only few cycles for illustrational purposes
mirt.wrapper.coef(res2$mirt)
summary(res2$mirt)
print(res2$mirt)

#############################################################################
# EXAMPLE 4: mirt conversions for polytomous dataset data.big5
#############################################################################

data(data.big5)
# select some items
items &lt;- c( grep( "O", colnames(data.big5), value=TRUE )[1:6],
     grep( "N", colnames(data.big5), value=TRUE )[1:4] )
# O3 O8 O13 O18 O23 O28 N1 N6 N11 N16
dat &lt;- data.big5[, items ]
library(psych)
psych::describe(dat)

library(TAM)
#******************
#*** Model 1: Partial credit model in TAM
mod1 &lt;- TAM::tam.mml( dat[,1:6] )
summary(mod1)
# convert to mirt object
mmod1 &lt;- sirt::tam2mirt( mod1 )
rmod1 &lt;- mmod1$mirt
# coefficients in mirt
coef(rmod1)
mirt.wrapper.coef(rmod1)
# model fit
mirt::M2(rmod1)
# item fit
mirt::itemfit(rmod1)
# plots
plot(rmod1,type="trace")
plot(rmod1, type="trace", which.items=1:4 )
mirt::itemplot(rmod1,"O3")

#******************
#*** Model 2: Generalized partial credit model in TAM
mod2 &lt;- TAM::tam.mml.2pl( dat[,1:6], irtmodel="GPCM" )
summary(mod2)
# convert to mirt object
mmod2 &lt;- sirt::tam2mirt( mod2 )
rmod2 &lt;- mmod2$mirt
# coefficients in mirt
mirt.wrapper.coef(rmod2)
# model fit
mirt::M2(rmod2)
# item fit
mirt::itemfit(rmod2)

## End(Not run)
</code></pre>

<hr>
<h2 id='testlet.marginalized'>
Marginal Item Parameters from a Testlet (Bifactor) Model
</h2><span id='topic+testlet.marginalized'></span>

<h3>Description</h3>

<p>This function computes marginal item parameters of a general
factor if item parameters from a testlet (bifactor) model are
provided as an input (see Details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testlet.marginalized(tam.fa.obj=NULL,a1=NULL, d1=NULL, testlet=NULL,
      a.testlet=NULL, var.testlet=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="testlet.marginalized_+3A_tam.fa.obj">tam.fa.obj</code></td>
<td>
<p>Optional object of class <code>tam.fa</code>
generated by <code><a href="TAM.html#topic+tam.fa">TAM::tam.fa</a></code> from
the <span class="pkg">TAM</span> package. </p>
</td></tr>
<tr><td><code id="testlet.marginalized_+3A_a1">a1</code></td>
<td>

<p>Vector of item discriminations of general factor
</p>
</td></tr>
<tr><td><code id="testlet.marginalized_+3A_d1">d1</code></td>
<td>

<p>Vector of item intercepts of general factor
</p>
</td></tr>
<tr><td><code id="testlet.marginalized_+3A_testlet">testlet</code></td>
<td>

<p>Integer vector of testlet (bifactor) identifiers (must be integers
between 1 to <code class="reqn">T</code>).
</p>
</td></tr>
<tr><td><code id="testlet.marginalized_+3A_a.testlet">a.testlet</code></td>
<td>

<p>Vector of testlet (bifactor) item discriminations
</p>
</td></tr>
<tr><td><code id="testlet.marginalized_+3A_var.testlet">var.testlet</code></td>
<td>

<p>Vector of testlet (bifactor) variances
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A testlet (bifactor) model is assumed to be estimated:
</p>
<p style="text-align: center;"><code class="reqn">P(X_{pit}=1 | \theta_{p}, u_{pt} )=
invlogit( a_{i1} \theta_p + a_t u_{pt} - d_{i} ) </code>
</p>

<p>with <code class="reqn">Var( u_{pt} )=\sigma_t^2 </code>. This multidimensional
item response model with locally independent items is
equivalent to a unidimensional IRT model with locally
dependent items (Ip, 2010). Marginal item parameters <code class="reqn">a_i^\ast</code>
and <code class="reqn">d_i^\ast</code> are obtained according to the response
equation </p>
<p style="text-align: center;"><code class="reqn">P(X_{pit}=1 | \theta_{p}^\ast  )=
invlogit( a_{i}^\ast \theta_p^\ast - d_{i}^\ast ) </code>
</p>

<p>Calculation details can be found in Ip (2010).
</p>


<h3>Value</h3>

<p>A data frame containing all input item parameters and
marginal item intercept <code class="reqn">d_i^\ast</code> (<code>d1_marg</code>) and
marginal item slope <code class="reqn">a_i^\ast</code> (<code>a1_marg</code>).
</p>


<h3>References</h3>

<p>Ip, E. H. (2010). Empirically indistinguishable multidimensional
IRT and locally dependent unidimensional item response models.
<em>British Journal of Mathematical and Statistical Psychology,
63</em>, 395-416.
</p>


<h3>See Also</h3>

<p>For estimating a testlet (bifactor) model see
<code><a href="TAM.html#topic+tam.fa">TAM::tam.fa</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Small numeric example for Rasch testlet model
#############################################################################

# Rasch testlet model with 9 items contained into 3 testlets
# the third testlet has essentially no dependence and therefore
# no testlet variance
testlet &lt;- rep( 1:3, each=3 )
a1 &lt;- rep(1, 9 )   # item slopes first dimension
d1 &lt;- rep( c(-1.25,0,1.5), 3 ) # item intercepts
a.testlet &lt;- rep( 1, 9 )  # item slopes testlets
var.testlet &lt;- c( .8, .2, 0 )  # testlet variances

# apply function
res &lt;- sirt::testlet.marginalized( a1=a1, d1=d1, testlet=testlet,
            a.testlet=a.testlet, var.testlet=var.testlet )
round( res, 2 )
  ##    item testlet a1    d1 a.testlet var.testlet a1_marg d1_marg
  ##  1    1       1  1 -1.25         1         0.8    0.89   -1.11
  ##  2    2       1  1  0.00         1         0.8    0.89    0.00
  ##  3    3       1  1  1.50         1         0.8    0.89    1.33
  ##  4    4       2  1 -1.25         1         0.2    0.97   -1.21
  ##  5    5       2  1  0.00         1         0.2    0.97    0.00
  ##  6    6       2  1  1.50         1         0.2    0.97    1.45
  ##  7    7       3  1 -1.25         1         0.0    1.00   -1.25
  ##  8    8       3  1  0.00         1         0.0    1.00    0.00
  ##  9    9       3  1  1.50         1         0.0    1.00    1.50

## Not run: 
#############################################################################
# EXAMPLE 2: Dataset reading
#############################################################################

library(TAM)
data(data.read)
resp &lt;- data.read
maxiter &lt;-  100

# Model 1: Rasch testlet model with 3 testlets
dims &lt;- substring( colnames(resp),1,1 )  # define dimensions
mod1 &lt;- TAM::tam.fa( resp=resp, irtmodel="bifactor1", dims=dims,
               control=list(maxiter=maxiter) )
# marginal item parameters
res1 &lt;- sirt::testlet.marginalized( mod1 )

#***
# Model 2: estimate bifactor model but assume that items 3 and 5 do not load on
#           specific factors
dims1 &lt;- dims
dims1[c(3,5)] &lt;- NA
mod2 &lt;- TAM::tam.fa( resp=resp, irtmodel="bifactor2", dims=dims1,
              control=list(maxiter=maxiter) )
res2 &lt;- sirt::testlet.marginalized( mod2 )
res2

## End(Not run)
</code></pre>

<hr>
<h2 id='tetrachoric2'>
Tetrachoric Correlation Matrix
</h2><span id='topic+tetrachoric2'></span>

<h3>Description</h3>

<p>This function estimates a tetrachoric correlation matrix according to
the maximum likelihood estimation of Olsson (Olsson, 1979; <code>method="Ol"</code>),
the Tucker method (Method 2 of Froemel, 1971; <code>method="Tu"</code>) and
Divgi (1979, <code>method="Di"</code>).
In addition, an alternative non-iterative approximation of
Bonett and Price (2005; <code>method="Bo"</code>) is provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tetrachoric2(dat, method="Ol", delta=0.007, maxit=1000000, cor.smooth=TRUE,
   progress=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tetrachoric2_+3A_dat">dat</code></td>
<td>

<p>A data frame of dichotomous response
</p>
</td></tr>
<tr><td><code id="tetrachoric2_+3A_method">method</code></td>
<td>
<p>Computation method for calculating the tetrachoric correlation.
The ML method is <code>method="Ol"</code> (which is the default),
the Tucker method is <code>method="Tu"</code>,
the Divgi method is <code>method="Di"</code>
the method of Bonett and
Price (2005) is <code>method="Bo"</code>.</p>
</td></tr>
<tr><td><code id="tetrachoric2_+3A_delta">delta</code></td>
<td>

<p>The step parameter. It is set by default to <code class="reqn">2^{-7}</code>
which is approximately .007.
</p>
</td></tr>
<tr><td><code id="tetrachoric2_+3A_maxit">maxit</code></td>
<td>

<p>Maximum number of iterations.
</p>
</td></tr>
<tr><td><code id="tetrachoric2_+3A_cor.smooth">cor.smooth</code></td>
<td>

<p>Should smoothing of the tetrachoric correlation matrix
be performed to ensure positive definiteness? Choosing
<code>cor.smooth=TRUE</code>, the function <code>cor.smooth</code>
from the <span class="pkg">psych</span> package is used for obtaining a positive definite
tetrachoric correlation matrix.
</p>
</td></tr>
<tr><td><code id="tetrachoric2_+3A_progress">progress</code></td>
<td>
<p>Display progress? Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>tau</code></td>
<td>
<p>Item thresholds</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>Tetrachoric correlation matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexander Robitzsch
</p>
<p>The code is adapted from an <span class="rlang"><b>R</b></span> script of Cengiz Zopluoglu.
See <em>http://sites.education.miami.edu/zopluoglu/software-programs/</em>.
</p>


<h3>References</h3>

<p>Bonett, D. G., &amp; Price, R. M. (2005). Inferential methods for the tetrachoric
correlation coefficient. <em>Journal of Educational and Behavioral Statistics,
30</em>(2), 213-225. <a href="https://doi.org/10.3102/10769986030002213">doi:10.3102/10769986030002213</a>
</p>
<p>Divgi, D. R. (1979). Calculation of the tetrachoric correlation coefficient.
<em>Psychometrika, 44</em>(2), 169-172.
<a href="https://doi.org/10.1007/BF02293968">doi:10.1007/BF02293968</a>
</p>
<p>Froemel, E. C. (1971). A comparison of computer routines for the
calculation of the tetrachoric correlation coefficient.
<em>Psychometrika, 36</em>(2), 165-174.
<a href="https://doi.org/10.1007/BF02291396">doi:10.1007/BF02291396</a>
</p>
<p>Olsson, U. (1979). Maximum likelihood estimation of the polychoric correlation
coefficient. <em>Psychometrika, 44</em>(4), 443-460.
<a href="https://doi.org/10.1007/BF02296207">doi:10.1007/BF02296207</a>
</p>


<h3>See Also</h3>

<p>See also the <code>psych::tetrachoric</code>
function in the <span class="pkg">psych</span> package
and the function <code>irtoys::tet</code> in the <span class="pkg">irtoys</span> package.
</p>
<p>See <code><a href="#topic+polychoric2">polychoric2</a></code> for estimating polychoric correlations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: data.read
#############################################################################

data(data.read)

# tetrachoric correlation from psych package
library(psych)
t0 &lt;- psych::tetrachoric( data.read )$rho
# Olsson method (maximum likelihood estimation)
t1 &lt;- sirt::tetrachoric2( data.read )$rho
# Divgi method
t2 &lt;- sirt::tetrachoric2( data.read, method="Di"  )$rho
# Tucker method
t3 &lt;- sirt::tetrachoric2( data.read, method="Tu" )$rho
# Bonett method
t4 &lt;- sirt::tetrachoric2( data.read, method="Bo" )$rho

# maximum absolute deviation ML method
max( abs( t0 - t1 ) )
  ##   [1] 0.008224986
# mean absolute deviation Divgi method
max( abs( t0 - t2 ) )
  ##   [1] 0.1766688
# mean absolute deviation Tucker method
max( abs( t0 - t3 ) )
  ##   [1] 0.1766292
# mean absolute deviation Bonett method
max( abs( t0 - t4 ) )
  ##   [1] 0.05695522
</code></pre>

<hr>
<h2 id='truescore.irt'>
Conversion of Trait Scores <code class="reqn">\theta</code> into
True Scores <code class="reqn">\tau ( \theta )</code>
</h2><span id='topic+truescore.irt'></span>

<h3>Description</h3>

<p>This function computes the true score
<code class="reqn">\tau=\tau(\theta)=\sum_{i=1}^I P_i(\theta)</code>
in a unidimensional item response model with <code class="reqn">I</code> items. In addition, it also
transforms conditional standard errors if they are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>truescore.irt(A, B, c=NULL, d=NULL, theta=seq(-3, 3, len=21),
    error=NULL, pid=NULL, h=0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="truescore.irt_+3A_a">A</code></td>
<td>

<p>Matrix or vector of item slopes. See Examples for polytomous responses.
</p>
</td></tr>
<tr><td><code id="truescore.irt_+3A_b">B</code></td>
<td>

<p>Matrix or vector of item intercepts. Note that the entries in
<code>B</code> refer to item intercepts and not to item difficulties.
</p>
</td></tr>
<tr><td><code id="truescore.irt_+3A_c">c</code></td>
<td>

<p>Optional vector of guessing parameters
</p>
</td></tr>
<tr><td><code id="truescore.irt_+3A_d">d</code></td>
<td>

<p>Optional vector of slipping parameters
</p>
</td></tr>
<tr><td><code id="truescore.irt_+3A_theta">theta</code></td>
<td>

<p>Vector of trait values
</p>
</td></tr>
<tr><td><code id="truescore.irt_+3A_error">error</code></td>
<td>

<p>Optional vector of standard errors of trait
</p>
</td></tr>
<tr><td><code id="truescore.irt_+3A_pid">pid</code></td>
<td>

<p>Optional vector of person identifiers
</p>
</td></tr>
<tr><td><code id="truescore.irt_+3A_h">h</code></td>
<td>

<p>Numerical differentiation parameter
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In addition, the function <code class="reqn">\pi(\theta)=\frac{1}{I} \cdot \tau( \theta)</code>
of the expected percent score is approximated by a logistic function
</p>
<p style="text-align: center;"><code class="reqn"> \pi ( \theta ) \approx l + ( u - l ) \cdot invlogit ( a \theta + b )
</code>
</p>



<h3>Value</h3>

<p>A data frame with following columns:
</p>
<table>
<tr><td><code>truescore</code></td>
<td>
<p>True scores <code class="reqn">\tau=\tau ( \theta )</code></p>
</td></tr>
<tr><td><code>truescore.error</code></td>
<td>
<p>Standard errors of true scores</p>
</td></tr>
<tr><td><code>percscore</code></td>
<td>
<p>Expected correct scores which is <code class="reqn">\tau</code>
divided by the maximum true score</p>
</td></tr>
<tr><td><code>percscore.error</code></td>
<td>
<p>Standard errors of expected correct scores</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p>The <code class="reqn">l</code> parameter</p>
</td></tr>
<tr><td><code>upper</code></td>
<td>
<p>The <code class="reqn">u</code> parameter</p>
</td></tr>
<tr><td><code>a</code></td>
<td>
<p>The <code class="reqn">a</code> parameter</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>The <code class="reqn">b</code> parameter</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset with mixed dichotomous and polytomous responses
#############################################################################

data(data.mixed1)
dat &lt;- data.mixed1

#****
# Model 1: Partial credit model
# estimate model with TAM package
library(TAM)
mod1 &lt;- TAM::tam.mml( dat )
# estimate person parameter estimates
wmod1 &lt;- TAM::tam.wle( mod1 )
wmod1 &lt;- wmod1[ order(wmod1$theta), ]
# extract item parameters
A &lt;- mod1$B[,-1,1]
B &lt;- mod1$AXsi[,-1]
# person parameters and standard errors
theta &lt;- wmod1$theta
error &lt;- wmod1$error

# estimate true score transformation
dfr &lt;- sirt::truescore.irt( A=A, B=B, theta=theta, error=error )

# plot different person parameter estimates and standard errors
par(mfrow=c(2,2))
plot( theta, dfr$truescore, pch=16, cex=.6, xlab=expression(theta), type="l",
    ylab=expression(paste( tau, "(",theta, ")" )), main="True Score Transformation" )
plot( theta, dfr$percscore, pch=16, cex=.6, xlab=expression(theta), type="l",
    ylab=expression(paste( pi, "(",theta, ")" )), main="Percent Score Transformation" )
points( theta, dfr$lower + (dfr$upper-dfr$lower)*
                stats::plogis(dfr$a*theta+dfr$b), col=2, lty=2)
plot( theta, error, pch=16, cex=.6, xlab=expression(theta), type="l",
    ylab=expression(paste("SE(",theta, ")" )), main="Standard Error Theta" )
plot( dfr$truescore, dfr$truescore.error, pch=16, cex=.6, xlab=expression(tau),
    ylab=expression(paste("SE(",tau, ")" ) ), main="Standard Error True Score Tau",
    type="l")
par(mfrow=c(1,1))

## Not run: 
#****
# Model 2: Generalized partial credit model
mod2 &lt;- TAM::tam.mml.2pl( dat, irtmodel="GPCM")
# estimate person parameter estimates
wmod2 &lt;- TAM::tam.wle( mod2 )
# extract item parameters
A &lt;- mod2$B[,-1,1]
B &lt;- mod2$AXsi[,-1]
# person parameters and standard errors
theta &lt;- wmod2$theta
error &lt;- wmod2$error
# estimate true score transformation
dfr &lt;- sirt::truescore.irt( A=A, B=B, theta=theta, error=error )

#############################################################################
# EXAMPLE 2: Dataset Reading data.read
#############################################################################
data(data.read)

#****
# Model 1: estimate difficulty + guessing model
mod1 &lt;- sirt::rasch.mml2( data.read, fixed.c=rep(.25,12) )
mod1$person &lt;- mod1$person[ order( mod1$person$EAP), ]
# person parameters and standard errors
theta &lt;- mod1$person$EAP
error &lt;- mod1$person$SE.EAP
A &lt;- rep(1,12)
B &lt;- - mod1$item$b
c &lt;- rep(.25,12)
# estimate true score transformation
dfr &lt;- sirt::truescore.irt( A=A, B=B, theta=theta, error=error,c=c)

plot( theta, dfr$percscore, pch=16, cex=.6, xlab=expression(theta), type="l",
    ylab=expression(paste( pi, "(",theta, ")" )), main="Percent Score Transformation" )
points( theta, dfr$lower + (dfr$upper-dfr$lower)*
             stats::plogis(dfr$a*theta+dfr$b), col=2, lty=2)

#****
# Model 2: Rasch model
mod2 &lt;- sirt::rasch.mml2( data.read  )
# person parameters and standard errors
theta &lt;- mod2$person$EAP
error &lt;- mod2$person$SE.EAP
A &lt;- rep(1,12)
B &lt;- - mod2$item$b
# estimate true score transformation
dfr &lt;- sirt::truescore.irt( A=A, B=B, theta=theta, error=error )

## End(Not run)
</code></pre>

<hr>
<h2 id='unidim.test.csn'>
Test for Unidimensionality of CSN
</h2><span id='topic+unidim.test.csn'></span>

<h3>Description</h3>

<p>This function tests whether item covariances given the sum
score are non-positive (CSN; see Junker 1993),
i.e. for items <code class="reqn">i</code> and <code class="reqn">j</code> it holds that
</p>
<p style="text-align: center;"><code class="reqn"> Cov( X_i, X_j | X^+ ) \le 0 </code>
</p>

<p>Note that this function only works for dichotomous data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unidim.test.csn(dat, RR=400, prop.perm=0.75, progress=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unidim.test.csn_+3A_dat">dat</code></td>
<td>

<p>Data frame with dichotomous item responses. All persons with (some)
missing responses are removed.
</p>
</td></tr>
<tr><td><code id="unidim.test.csn_+3A_rr">RR</code></td>
<td>

<p>Number of permutations used for statistical testing
</p>
</td></tr>
<tr><td><code id="unidim.test.csn_+3A_prop.perm">prop.perm</code></td>
<td>

<p>A positive value indicating the amount of permutation in an
existing permuted data set
</p>
</td></tr>
<tr><td><code id="unidim.test.csn_+3A_progress">progress</code></td>
<td>

<p>An optional logical indicating whether computation progress
should be displayed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each item pair <code class="reqn">(i,j)</code> and a each sum score group <code class="reqn">k</code>
a conditional covariance <code class="reqn">r(i,j|k)</code> is calculated. Then,
the test statistic for CSN is
</p>
<p style="text-align: center;"><code class="reqn"> h=\sum_{k=1}^{I-1} \frac{n_k}{n} \max_{i,j} r(i,j|k) </code>
</p>

<p>where <code class="reqn">n_k</code> is the number of persons in score group <code class="reqn">k</code>.
&quot;'Large values&quot;' of <code class="reqn">h</code> are not in agreement with the null
hypothesis of non-positivity of conditional covariances.
</p>
<p>The distribution of the test statistic <code class="reqn">h</code> under the null
hypothesis is empirically obtained by column wise permutation
of items within all score groups. In the population, this procedure
corresponds to conditional covariances of zero. See de Gooijer and Yuan (2011)
for more details.
</p>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>stat</code></td>
<td>
<p>Value of the statistic</p>
</td></tr>
<tr><td><code>stat_perm</code></td>
<td>
<p>Distribution of statistic under <code class="reqn">H_0</code> of
permuted dataset</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>The corresponding p value of the statistic</p>
</td></tr>
<tr><td><code>H0_quantiles</code></td>
<td>
<p>Quantiles of the statistic under permutation
(the null hypothesis <code class="reqn">H_0</code>)</p>
</td></tr>
</table>


<h3>References</h3>

<p>De Gooijer, J. G., &amp; Yuan, A. (2011). Some exact tests for
manifest properties of latent trait models.
<em>Computational Statistics and Data Analysis, 55</em>,
34-44.
</p>
<p>Junker, B.W. (1993). Conditional association, essential independence, and
monotone unidimensional item response models.
<em>Annals of Statistics, 21</em>, 1359-1378.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset data.read
#############################################################################

data(data.read)
dat &lt;- data.read
set.seed(778)
res &lt;- sirt::unidim.test.csn( dat )
  ##  CSN Statistic=0.04737, p=0.02

## Not run: 
#############################################################################
# EXAMPLE 2: CSN statistic for two-dimensional simulated data
#############################################################################

set.seed(775)
N &lt;- 2000
I &lt;- 30   # number of items
rho &lt;- .60   # correlation between 2 dimensions
t0 &lt;- stats::rnorm(N)
t1 &lt;- sqrt(rho)*t0 + sqrt(1-rho)*stats::rnorm(N)
t2 &lt;- sqrt(rho)*t0 + sqrt(1-rho)*stats::rnorm(N)
dat1 &lt;- sirt::sim.raschtype(t1, b=seq(-1.5,1.5,length=I/2) )
dat2 &lt;- sirt::sim.raschtype(t2, b=seq(-1.5,1.5,length=I/2) )
dat &lt;- as.matrix(cbind( dat1, dat2) )
res &lt;- sirt::unidim.test.csn( dat )
  ##  CSN Statistic=0.06056, p=0.02

## End(Not run)
</code></pre>

<hr>
<h2 id='wle.rasch'>
Weighted Likelihood Estimation of Person Abilities
</h2><span id='topic+wle.rasch'></span>

<h3>Description</h3>

<p>This function computes weighted likelihood estimates for dichotomous responses based
on the Rasch model (Warm, 1989).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wle.rasch(dat, dat.resp=NULL, b, itemweights=1 + 0 * b,
    theta=rep(0, nrow(dat)), conv=0.001, maxit=200,
    wle.adj=0, progress=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wle.rasch_+3A_dat">dat</code></td>
<td>
<p> An <code class="reqn">N \times I</code> data frame of
dichotomous item responses
</p>
</td></tr>
<tr><td><code id="wle.rasch_+3A_dat.resp">dat.resp</code></td>
<td>

<p>Optional data frame with dichotomous response indicators
</p>
</td></tr>
<tr><td><code id="wle.rasch_+3A_b">b</code></td>
<td>

<p>Vector of length <code class="reqn">I</code> with fixed item difficulties
</p>
</td></tr>
<tr><td><code id="wle.rasch_+3A_itemweights">itemweights</code></td>
<td>

<p>Optional vector of fixed item discriminations
</p>
</td></tr>
<tr><td><code id="wle.rasch_+3A_theta">theta</code></td>
<td>

<p>Optional vector of initial person parameter estimates
</p>
</td></tr>
<tr><td><code id="wle.rasch_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion
</p>
</td></tr>
<tr><td><code id="wle.rasch_+3A_maxit">maxit</code></td>
<td>

<p>Maximal number of iterations
</p>
</td></tr>
<tr><td><code id="wle.rasch_+3A_wle.adj">wle.adj</code></td>
<td>

<p>Constant for WLE adjustment
</p>
</td></tr>
<tr><td><code id="wle.rasch_+3A_progress">progress</code></td>
<td>

<p>Display progress?
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with following entries
</p>
<table>
<tr><td><code>theta</code></td>
<td>

<p>Estimated weighted likelihood estimate
</p>
</td></tr>
<tr><td><code>dat.resp</code></td>
<td>

<p>Data frame with dichotomous response indicators. A one indicates
an observed response, a zero a missing response. See also <code>dat.resp</code>
in the list of arguments of this function.
</p>
</td></tr>
<tr><td><code>p.ia</code></td>
<td>
<p>Matrix with expected item response, i.e.
the probabilities <code class="reqn">P(X_{pi}=1|\theta_p )=invlogit( \theta_p - b_i )</code>.
</p>
</td></tr>
<tr><td><code>wle</code></td>
<td>
<p>WLE reliability (Adams, 2005)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Adams, R. J. (2005). Reliability as a measurement design effect.
<em>Studies in Educational Evaluation, 31</em>, 162-172.
</p>
<p>Warm, T. A. (1989). Weighted likelihood estimation of ability in item response theory.
<em>Psychometrika, 54</em>, 427-450.
</p>


<h3>See Also</h3>

<p>For standard errors of weighted likelihood estimates estimated via jackknife
see <code><a href="#topic+wle.rasch.jackknife">wle.rasch.jackknife</a></code>.
</p>
<p>For a joint estimation of item and person parameters see the joint maximum
likelihood estimation method in <code><a href="#topic+rasch.jml">rasch.jml</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset Reading
#############################################################################
data(data.read)

# estimate the Rasch model
mod &lt;- sirt::rasch.mml2(data.read)
mod$item

# estmate WLEs
mod.wle &lt;- sirt::wle.rasch( dat=data.read, b=mod$item$b )
</code></pre>

<hr>
<h2 id='wle.rasch.jackknife'>
Standard Error Estimation of WLE by Jackknifing
</h2><span id='topic+wle.rasch.jackknife'></span>

<h3>Description</h3>

<p>This function calculates standard errors of WLEs (Warm, 1989) for
stratified item designs and item designs with testlets for the
Rasch model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wle.rasch.jackknife(dat, b, itemweights=1 + 0 * b, pid=NULL,
    testlet=NULL, stratum=NULL, size.itempop=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wle.rasch.jackknife_+3A_dat">dat</code></td>
<td>

<p>An <code class="reqn">N \times I</code> data frame of item responses
</p>
</td></tr>
<tr><td><code id="wle.rasch.jackknife_+3A_b">b</code></td>
<td>

<p>Vector of item difficulties
</p>
</td></tr>
<tr><td><code id="wle.rasch.jackknife_+3A_itemweights">itemweights</code></td>
<td>

<p>Weights for items, i.e. fixed item discriminations
</p>
</td></tr>
<tr><td><code id="wle.rasch.jackknife_+3A_pid">pid</code></td>
<td>

<p>Person identifier
</p>
</td></tr>
<tr><td><code id="wle.rasch.jackknife_+3A_testlet">testlet</code></td>
<td>

<p>A vector of length <code class="reqn">I</code> which defines which item
belongs to which testlet. If some items
does not belong to any testlet, then define separate
testlet labels for these single items.
</p>
</td></tr>
<tr><td><code id="wle.rasch.jackknife_+3A_stratum">stratum</code></td>
<td>

<p>Item stratum
</p>
</td></tr>
<tr><td><code id="wle.rasch.jackknife_+3A_size.itempop">size.itempop</code></td>
<td>

<p>Number of items in an item stratum of the finite item population.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea of Jackknife in item response models can be found in
Wainer and Wright (1980).
</p>


<h3>Value</h3>

<p>A list with following entries:
</p>
<table>
<tr><td><code>wle</code></td>
<td>
<p>Data frame with some estimated statistics. The column
<code>wle</code> is the WLE and <code>wle.jackse</code> its corresponding
standard error estimated by jackknife.
</p>
</td></tr>
<tr><td><code>wle.rel</code></td>
<td>
<p>WLE reliability (Adams, 2005)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Adams, R. J. (2005). Reliability as a measurement design effect.
<em>Studies in Educational Evaluation, 31</em>(2-3), 162-172.
<a href="https://doi.org/10.1016/j.stueduc.2005.05.008">doi:10.1016/j.stueduc.2005.05.008</a>
</p>
<p>Gershunskaya, J., Jiang, J., &amp; Lahiri, P. (2009). Resampling methods in surveys.
In D. Pfeffermann and C.R. Rao (Eds.). <em>Handbook of Statistics 29B; Sample
Surveys: Inference and Analysis</em> (pp. 121-151). Amsterdam: North Holland.
<a href="https://doi.org/10.1016/S0169-7161%2809%2900228-4">doi:10.1016/S0169-7161(09)00228-4</a>
</p>
<p>Wainer, H., &amp; Wright, B. D. (1980). Robust estimation of ability in the Rasch
model. <em>Psychometrika, 45</em>(3), 373-391.
<a href="https://doi.org/10.1007/BF02293910">doi:10.1007/BF02293910</a>
</p>
<p>Warm, T. A. (1989). Weighted likelihood estimation of ability in item response theory.
<em>Psychometrika, 54</em>(3), 427-450.
<a href="https://doi.org/10.1007/BF02294627">doi:10.1007/BF02294627</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wle.rasch">wle.rasch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
# EXAMPLE 1: Dataset Reading
#############################################################################
data(data.read)
dat &lt;- data.read

# estimation of the Rasch model
res &lt;- sirt::rasch.mml2( dat, parm.conv=.001)

# WLE estimation
wle1 &lt;- sirt::wle.rasch(dat, b=res$item$thresh )

# simple jackknife WLE estimation
wle2 &lt;- sirt::wle.rasch.jackknife(dat, b=res$item$thresh )
  ## WLE Reliability=0.651

# SE(WLE) for testlets A, B and C
wle3 &lt;- sirt::wle.rasch.jackknife(dat, b=res$item$thresh,
           testlet=substring( colnames(dat),1,1) )
  ## WLE Reliability=0.572

# SE(WLE) for item strata A,B, C
wle4 &lt;- sirt::wle.rasch.jackknife(dat, b=res$item$thresh,
             stratum=substring( colnames(dat),1,1) )
  ## WLE Reliability=0.683

# SE (WLE) for finite item strata
# A (10 items), B (7 items), C (4 items -&gt; no sampling error)
# in every stratum 4 items were sampled
size.itempop &lt;- c(10,7,4)
names(size.itempop) &lt;- c("A","B","C")
wle5 &lt;- sirt::wle.rasch.jackknife(dat, b=res$item$thresh,
             stratum=substring( colnames(dat),1,1),
             size.itempop=size.itempop )
  ## Stratum  A (Mean) Correction Factor 0.6
  ## Stratum  B (Mean) Correction Factor 0.42857
  ## Stratum  C (Mean) Correction Factor 0
  ## WLE Reliability=0.876

# compare different estimated standard errors
a2 &lt;- stats::aggregate( wle2$wle$wle.jackse, list( wle2$wle$wle), mean )
colnames(a2) &lt;- c("wle", "se.simple")
a2$se.testlet &lt;- stats::aggregate( wle3$wle$wle.jackse, list( wle3$wle$wle), mean )[,2]
a2$se.strata &lt;- stats::aggregate( wle4$wle$wle.jackse, list( wle4$wle$wle), mean )[,2]
a2$se.finitepop.strata &lt;- stats::aggregate( wle5$wle$wle.jackse,
    list( wle5$wle$wle), mean )[,2]
round( a2, 3 )
  ## &gt; round( a2, 3 )
  ##       wle se.simple se.testlet se.strata se.finitepop.strata
  ## 1  -5.085     0.440      0.649     0.331               0.138
  ## 2  -3.114     0.865      1.519     0.632               0.379
  ## 3  -2.585     0.790      0.849     0.751               0.495
  ## 4  -2.133     0.715      1.177     0.546               0.319
  ## 5  -1.721     0.597      0.767     0.527               0.317
  ## 6  -1.330     0.633      0.623     0.617               0.377
  ## 7  -0.942     0.631      0.643     0.604               0.365
  ## 8  -0.541     0.655      0.678     0.617               0.384
  ## 9  -0.104     0.671      0.646     0.659               0.434
  ## 10  0.406     0.771      0.706     0.751               0.461
  ## 11  1.080     1.118      0.893     1.076               0.630
  ## 12  2.332     0.400      0.631     0.272               0.195
</code></pre>

<hr>
<h2 id='xxirt'>
User Defined Item Response Model
</h2><span id='topic+xxirt'></span><span id='topic+summary.xxirt'></span><span id='topic+print.xxirt'></span><span id='topic+logLik.xxirt'></span><span id='topic+anova.xxirt'></span><span id='topic+coef.xxirt'></span><span id='topic+vcov.xxirt'></span><span id='topic+confint.xxirt'></span><span id='topic+IRT.se.xxirt'></span><span id='topic+IRT.expectedCounts.xxirt'></span><span id='topic+IRT.irfprob.xxirt'></span><span id='topic+IRT.likelihood.xxirt'></span><span id='topic+IRT.posterior.xxirt'></span><span id='topic+IRT.modelfit.xxirt'></span><span id='topic+summary.IRT.modelfit.xxirt'></span><span id='topic+IRT.factor.scores.xxirt'></span><span id='topic+xxirt_hessian'></span>

<h3>Description</h3>

<p>Estimates a user defined item response model. Both, item response functions
and latent trait distributions can be specified by the user (see Details).
By default, the EM algorithm is used for estimation. The number of maximum
EM iterations can be defined with the argument <code>maxit</code>. The <code>xxirt</code>
function also allows Newton-Raphson optimization by specifying values of maximum
number of iterations in <code>maxit_nr</code> larger than zero. Typically, a small initial
number of EM iterations should be chosen to obtain reasonable starting values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xxirt(dat, Theta=NULL, itemtype=NULL, customItems=NULL, partable=NULL,
       customTheta=NULL, group=NULL, weights=NULL, globconv=1e-06, conv=1e-04,
       maxit=1000, mstep_iter=4, mstep_reltol=1e-06, maxit_nr=0, optimizer_nr="nlminb",
       control_nr=list(trace=1), h=1E-4, use_grad=TRUE, verbose=TRUE,
       penalty_fun_item=NULL, np_fun_item=NULL, verbose_index=NULL,
       cv_kfold=0, cv_maxit=10)

## S3 method for class 'xxirt'
summary(object, digits=3, file=NULL, ...)

## S3 method for class 'xxirt'
print(x, ...)

## S3 method for class 'xxirt'
anova(object,...)

## S3 method for class 'xxirt'
coef(object,...)

## S3 method for class 'xxirt'
logLik(object,...)

## S3 method for class 'xxirt'
vcov(object,...)

## S3 method for class 'xxirt'
confint(object, parm, level=.95, ... )

## S3 method for class 'xxirt'
IRT.expectedCounts(object,...)

## S3 method for class 'xxirt'
IRT.factor.scores(object, type="EAP", ...)

## S3 method for class 'xxirt'
IRT.irfprob(object,...)

## S3 method for class 'xxirt'
IRT.likelihood(object,...)

## S3 method for class 'xxirt'
IRT.posterior(object,...)

## S3 method for class 'xxirt'
IRT.modelfit(object,...)

## S3 method for class 'IRT.modelfit.xxirt'
summary(object,...)

## S3 method for class 'xxirt'
IRT.se(object,...)

# computes Hessian matrix
xxirt_hessian(object, h=1e-4, use_shortcut=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xxirt_+3A_dat">dat</code></td>
<td>

<p>Data frame with item responses
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_theta">Theta</code></td>
<td>

<p>Matrix with <code class="reqn">\bold{\theta}</code> grid vector of latent trait
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_itemtype">itemtype</code></td>
<td>

<p>Vector of item types
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_customitems">customItems</code></td>
<td>

<p>List containing types of item response functions created by
<code><a href="#topic+xxirt_createDiscItem">xxirt_createDiscItem</a></code>.
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_partable">partable</code></td>
<td>

<p>Item parameter table which is initially created by
<code><a href="#topic+xxirt_createParTable">xxirt_createParTable</a></code> and which can be modified by
<code><a href="#topic+xxirt_modifyParTable">xxirt_modifyParTable</a></code>.
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_customtheta">customTheta</code></td>
<td>

<p>User defined <code class="reqn">\bold{\theta}</code> distribution created by
<code><a href="#topic+xxirt_createThetaDistribution">xxirt_createThetaDistribution</a></code>.
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_group">group</code></td>
<td>

<p>Optional vector of group indicators
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_weights">weights</code></td>
<td>

<p>Optional vector of person weights
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_globconv">globconv</code></td>
<td>

<p>Convergence criterion for relative change in deviance
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_conv">conv</code></td>
<td>

<p>Convergence criterion for absolute change in parameters
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_maxit">maxit</code></td>
<td>

<p>Maximum number of iterations in the EM algorithm
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_mstep_iter">mstep_iter</code></td>
<td>

<p>Maximum number of iterations in M-step
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_mstep_reltol">mstep_reltol</code></td>
<td>

<p>Convergence criterion in M-step
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_maxit_nr">maxit_nr</code></td>
<td>
<p>Number of Newton-Raphson iterations after EM algorithm</p>
</td></tr>
<tr><td><code id="xxirt_+3A_optimizer_nr">optimizer_nr</code></td>
<td>
<p>Type of optimizer for Newton-Raphson optimization.
Alternatives are <code>"optim"</code> or <code>"nlminb"</code> or other options of
<code><a href="#topic+sirt_optimizer">sirt_optimizer</a></code>.</p>
</td></tr>
<tr><td><code id="xxirt_+3A_control_nr">control_nr</code></td>
<td>
<p>Argument <code>control</code> for optimizer.</p>
</td></tr>
<tr><td><code id="xxirt_+3A_h">h</code></td>
<td>
<p>Numerical differentiation parameter</p>
</td></tr>
<tr><td><code id="xxirt_+3A_use_grad">use_grad</code></td>
<td>
<p>Logical indicating whether the gradient should be supplied
to <code><a href="stats.html#topic+optim">stats::optim</a></code></p>
</td></tr>
<tr><td><code id="xxirt_+3A_verbose">verbose</code></td>
<td>

<p>Logical indicating whether iteration progress should be displayed
</p>
</td></tr>
<tr><td><code id="xxirt_+3A_penalty_fun_item">penalty_fun_item</code></td>
<td>
<p>Optional penalty function used in regularized
estimation. Used as a function of <code>x</code> (vector of item parameters)</p>
</td></tr>
<tr><td><code id="xxirt_+3A_np_fun_item">np_fun_item</code></td>
<td>
<p>Function that counts the number of item parameters in regularized
estimation. Used as a function of <code>x</code> (vector of item parameters)</p>
</td></tr>
<tr><td><code id="xxirt_+3A_object">object</code></td>
<td>
<p>Object of class <code>xxirt</code></p>
</td></tr>
<tr><td><code id="xxirt_+3A_digits">digits</code></td>
<td>
<p>Number of digits to be rounded</p>
</td></tr>
<tr><td><code id="xxirt_+3A_file">file</code></td>
<td>
<p>Optional file name to which <code>summary</code> output is written</p>
</td></tr>
<tr><td><code id="xxirt_+3A_parm">parm</code></td>
<td>
<p>Optional vector of parameters</p>
</td></tr>
<tr><td><code id="xxirt_+3A_level">level</code></td>
<td>
<p>Confidence level</p>
</td></tr>
<tr><td><code id="xxirt_+3A_verbose_index">verbose_index</code></td>
<td>
<p>Logical indicating whether item index should be
printed in estimation output</p>
</td></tr>
<tr><td><code id="xxirt_+3A_cv_kfold">cv_kfold</code></td>
<td>
<p>Number of k folds in cross validation. The default is 0 (no
cross-validation)</p>
</td></tr>
<tr><td><code id="xxirt_+3A_cv_maxit">cv_maxit</code></td>
<td>
<p>Maximum number of iterations for each cross-validation sample</p>
</td></tr>
<tr><td><code id="xxirt_+3A_x">x</code></td>
<td>
<p>Object of class <code>xxirt</code></p>
</td></tr>
<tr><td><code id="xxirt_+3A_type">type</code></td>
<td>
<p>Type of person parameter estimate. Currently, only
<code>EAP</code> is implemented.</p>
</td></tr>
<tr><td><code id="xxirt_+3A_use_shortcut">use_shortcut</code></td>
<td>
<p>Logical indicating whether a shortcut in the computation
should be utilized</p>
</td></tr>
<tr><td><code id="xxirt_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Item response functions can be specified as functions of unknown parameters
<code class="reqn">\bold{\delta}_i</code> such that
<code class="reqn">P(X_{i}=x | \bold{\theta})=f_i( x | \bold{\theta} ; \bold{\delta}_i  )</code>
The item response model is estimated under the assumption of
local stochastic independence of items. Equality constraints of
item parameters <code class="reqn">\bold{\delta}_i</code> among items are allowed.
</p>
<p>The probability distribution <code class="reqn">P(\bold{\theta})</code> are specified as functions
of an unknown parameter vector <code class="reqn">\bold{\gamma}</code>.
</p>
<p>A penalty function for item parameters can be specified in
<code>penalty_fun_item</code>. The penalty function should be differentiable and
a non-differentiable function (e.g., the absolute value function) should
be approximated by a differentiable function.
</p>


<h3>Value</h3>

<p>List with following entries
</p>
<table>
<tr><td><code>partable</code></td>
<td>
<p>Item parameter table</p>
</td></tr>
<tr><td><code>par_items</code></td>
<td>
<p>Vector with estimated item parameters</p>
</td></tr>
<tr><td><code>par_items_summary</code></td>
<td>
<p>Data frame with item parameters</p>
</td></tr>
<tr><td><code>par_items_bounds</code></td>
<td>
<p>Data frame with summary on bounds of estimated
item parameters</p>
</td></tr>
<tr><td><code>par_Theta</code></td>
<td>
<p>Vector with estimated parameters of theta distribution</p>
</td></tr>
<tr><td><code>Theta</code></td>
<td>
<p>Matrix with <code class="reqn">\bold{\theta}</code> grid</p>
</td></tr>
<tr><td><code>probs_items</code></td>
<td>
<p>Item response functions</p>
</td></tr>
<tr><td><code>probs_Theta</code></td>
<td>
<p>Theta distribution</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>Deviance</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>Log likelihood value</p>
</td></tr>
<tr><td><code>ic</code></td>
<td>
<p>Information criteria</p>
</td></tr>
<tr><td><code>item_list</code></td>
<td>
<p>List with item functions</p>
</td></tr>
<tr><td><code>customItems</code></td>
<td>
<p>Used customized item response functions</p>
</td></tr>
<tr><td><code>customTheta</code></td>
<td>
<p>Used customized theta distribution</p>
</td></tr>
<tr><td><code>cv_loglike</code></td>
<td>
<p>Cross-validated log-likelihood value (if <code>cv_kfold&gt;0</code>)</p>
</td></tr>
<tr><td><code>p.xi.aj</code></td>
<td>
<p>Individual likelihood</p>
</td></tr>
<tr><td><code>p.aj.xi</code></td>
<td>
<p>Individual posterior</p>
</td></tr>
<tr><td><code>ll_case</code></td>
<td>
<p>Case-wise log-likelihood values</p>
</td></tr>
<tr><td><code>n.ik</code></td>
<td>
<p>Array of expected counts</p>
</td></tr>
<tr><td><code>EAP</code></td>
<td>
<p>EAP person parameter estimates</p>
</td></tr>
<tr><td><code>dat</code></td>
<td>
<p>Used dataset with item responses</p>
</td></tr>
<tr><td><code>dat_resp</code></td>
<td>
<p>Dataset with response indicators</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>Vector of person weights</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>Number of groups</p>
</td></tr>
<tr><td><code>group</code></td>
<td>
<p>Integer vector of group indicators</p>
</td></tr>
<tr><td><code>group_orig</code></td>
<td>
<p>Vector of original group_identifiers</p>
</td></tr>
<tr><td><code>ncat</code></td>
<td>
<p>Number of categories per item</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>Logical whether model has converged</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations needed</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>See the <code><a href="mirt.html#topic+createItem">mirt::createItem</a></code> and
<code><a href="mirt.html#topic+mirt">mirt::mirt</a></code> functions in the <span class="pkg">mirt</span>
package for similar functionality.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#############################################################################
## EXAMPLE 1: Unidimensional item response functions
#############################################################################

data(data.read)
dat &lt;- data.read

#------ Definition of item response functions

#*** IRF 2PL
P_2PL &lt;- function( par, Theta, ncat){
    a &lt;- par[1]
    b &lt;- par[2]
    TP &lt;- nrow(Theta)
    P &lt;- matrix( NA, nrow=TP, ncol=ncat)
    P[,1] &lt;- 1
    for (cc in 2:ncat){
        P[,cc] &lt;- exp( (cc-1) * a * Theta[,1] - b )
    }
    P &lt;- P / rowSums(P)
    return(P)
}

#*** IRF 1PL
P_1PL &lt;- function( par, Theta, ncat){
    b &lt;- par[1]
    TP &lt;- nrow(Theta)
    P &lt;- matrix( NA, nrow=TP, ncol=ncat)
    P[,1] &lt;- 1
    for (cc in 2:ncat){
        P[,cc] &lt;- exp( (cc-1) * Theta[,1] - b )
    }
    P &lt;- P / rowSums(P)
    return(P)
}

#** created item classes of 1PL and 2PL models
par &lt;- c( "a"=1, "b"=0 )
# define some slightly informative prior of 2PL
item_2PL &lt;- sirt::xxirt_createDiscItem( name="2PL", par=par, est=c(TRUE,TRUE),
               P=P_2PL, prior=c(a="dlnorm"), prior_par1=c( a=0 ),
               prior_par2=c(a=5) )
item_1PL &lt;- sirt::xxirt_createDiscItem( name="1PL", par=par[2], est=c(TRUE),
               P=P_1PL )
customItems &lt;- list( item_1PL,  item_2PL )

#---- definition theta distribution

#** theta grid
Theta &lt;- matrix( seq(-6,6,length=21), ncol=1 )

#** theta distribution
P_Theta1 &lt;- function( par, Theta, G){
    mu &lt;- par[1]
    sigma &lt;- max( par[2], .01 )
    TP &lt;- nrow(Theta)
    pi_Theta &lt;- matrix( 0, nrow=TP, ncol=G)
    pi1 &lt;- dnorm( Theta[,1], mean=mu, sd=sigma )
    pi1 &lt;- pi1 / sum(pi1)
    pi_Theta[,1] &lt;- pi1
    return(pi_Theta)
}
#** create distribution class
par_Theta &lt;- c( "mu"=0, "sigma"=1 )
customTheta  &lt;- sirt::xxirt_createThetaDistribution( par=par_Theta, est=c(FALSE,TRUE),
                       P=P_Theta1 )

#****************************************************************************
#******* Model 1: Rasch model

#-- create parameter table
itemtype &lt;- rep( "1PL", 12 )
partable &lt;- sirt::xxirt_createParTable( dat, itemtype=itemtype,
                        customItems=customItems )

# estimate model
mod1 &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable,
                   customItems=customItems, customTheta=customTheta)
summary(mod1)

# estimate Rasch model by providing starting values
partable1 &lt;- sirt::xxirt_modifyParTable( partable, parname="b",
                   value=- stats::qlogis( colMeans(dat) ) )
# estimate model again
mod1b &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable1,
                   customItems=customItems, customTheta=customTheta )
summary(mod1b)

# extract coefficients, covariance matrix and standard errors
coef(mod1b)
vcov(mod1b)
IRT.se(mod1b)

#** start with EM and finalize with Newton-Raphson algorithm
mod1c &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable,
                   customItems=customItems, customTheta=customTheta,
                   maxit=20, maxit_nr=300)
summary(mod1c)

#****************************************************************************
#******* Model 2: 2PL Model with three groups of item discriminations

#-- create parameter table
itemtype &lt;- rep( "2PL", 12 )
partable &lt;- sirt::xxirt_createParTable( dat, itemtype=itemtype, customItems=customItems)
# modify parameter table: set constraints for item groups A, B and C
partable1 &lt;- sirt::xxirt_modifyParTable(partable, item=paste0("A",1:4),
                         parname="a", parindex=111)
partable1 &lt;- sirt::xxirt_modifyParTable(partable1, item=paste0("B",1:4),
                         parname="a", parindex=112)
partable1 &lt;- sirt::xxirt_modifyParTable(partable1, item=paste0("C",1:4),
                         parname="a", parindex=113)
# delete prior distributions
partable1 &lt;- sirt::xxirt_modifyParTable(partable1, parname="a", prior=NA)

#-- fix sigma to 1
customTheta1 &lt;- customTheta
customTheta1$est &lt;- c("mu"=FALSE,"sigma"=FALSE )

# estimate model
mod2 &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable1,
                  customItems=customItems, customTheta=customTheta1 )
summary(mod2)

#****************************************************************************
#******* Model 3: Cloglog link function

#*** IRF cloglog
P_1N &lt;- function( par, Theta, ncat){
    b &lt;- par
    TP &lt;- nrow(Theta)
    P &lt;- matrix( NA, nrow=TP, ncol=ncat)
    P[,2] &lt;- 1 - exp( - exp( Theta - b ) )
    P[,1] &lt;- 1 - P[,2]
    return(P)
}
par &lt;- c("b"=0)
item_1N &lt;- sirt::xxirt_createDiscItem( name="1N", par=par, est=c(TRUE),
                    P=P_1N )
customItems &lt;- list( item_1N )
itemtype &lt;- rep( "1N", I )
partable &lt;- sirt::xxirt_createParTable( dat[,items], itemtype=itemtype,
                      customItems=customItems )
partable &lt;- sirt::xxirt_modifyParTable( partable=partable, parname="b",
                 value=- stats::qnorm( colMeans(dat[,items] )) )

#*** estimate model
mod3 &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable, customItems=customItems,
                customTheta=customTheta )
summary(mod3)
IRT.compareModels(mod1,mod3)

#****************************************************************************
#******* Model 4: Latent class model

K &lt;- 3 # number of classes
Theta &lt;- diag(K)

#*** Theta distribution
P_Theta1 &lt;- function( par, Theta, G  ){
    logitprobs &lt;- par[1:(K-1)]
    l1 &lt;- exp( c( logitprobs, 0 ) )
    probs &lt;- matrix( l1/sum(l1), ncol=1)
    return(probs)
}

par_Theta &lt;- stats::qlogis( rep( 1/K, K-1 ) )
names(par_Theta) &lt;- paste0("pi",1:(K-1) )
customTheta  &lt;- sirt::xxirt_createThetaDistribution( par=par_Theta,
                     est=rep(TRUE,K-1), P=P_Theta1)

#*** IRF latent class
P_lc &lt;- function( par, Theta, ncat){
    b &lt;- par
    TP &lt;- nrow(Theta)
    P &lt;- matrix( NA, nrow=TP, ncol=ncat)
    P[,1] &lt;- 1
    for (cc in 2:ncat){
        P[,cc] &lt;- exp( Theta %*% b )
    }
    P &lt;- P / rowSums(P)
    return(P)
}
par &lt;- seq( -1.5, 1.5, length=K )
names(par) &lt;- paste0("b",1:K)
item_lc &lt;- sirt::xxirt_createDiscItem( name="LC", par=par,
                 est=rep(TRUE,K), P=P_lc )
customItems &lt;- list( item_lc )

# create parameter table
itemtype &lt;- rep( "LC", 12 )
partable &lt;- sirt::xxirt_createParTable( dat, itemtype=itemtype, customItems=customItems)
partable

#*** estimate model
mod4 &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable, customItems=customItems,
                customTheta=customTheta)
summary(mod4)
# class probabilities
mod4$probs_Theta
# item response functions
imod4 &lt;- IRT.irfprob( mod5 )
round( imod4[,2,], 3 )

#****************************************************************************
#******* Model 5: Ordered latent class model

K &lt;- 3 # number of classes
Theta &lt;- diag(K)
Theta &lt;- apply( Theta, 1, cumsum )

#*** Theta distribution
P_Theta1 &lt;- function( par, Theta, G  ){
    logitprobs &lt;- par[1:(K-1)]
    l1 &lt;- exp( c( logitprobs, 0 ) )
    probs &lt;- matrix( l1/sum(l1), ncol=1)
    return(probs)
}
par_Theta &lt;- stats::qlogis( rep( 1/K, K-1 ) )
names(par_Theta) &lt;- paste0("pi",1:(K-1) )
customTheta  &lt;- sirt::xxirt_createThetaDistribution( par=par_Theta,
                est=rep(TRUE,K-1), P=P_Theta1  )

#*** IRF ordered latent class
P_olc &lt;- function( par, Theta, ncat){
    b &lt;- par
    TP &lt;- nrow(Theta)
    P &lt;- matrix( NA, nrow=TP, ncol=ncat)
    P[,1] &lt;- 1
    for (cc in 2:ncat){
        P[,cc] &lt;- exp( Theta %*% b )
    }
    P &lt;- P / rowSums(P)
    return(P)
}

par &lt;- c( -1, rep( .5,, length=K-1 ) )
names(par) &lt;- paste0("b",1:K)
item_olc &lt;- sirt::xxirt_createDiscItem( name="OLC", par=par, est=rep(TRUE,K),
                    P=P_olc, lower=c( -Inf, 0, 0 ) )
customItems &lt;- list( item_olc )
itemtype &lt;- rep( "OLC", 12 )
partable &lt;- sirt::xxirt_createParTable( dat, itemtype=itemtype, customItems=customItems)
partable

#*** estimate model
mod5 &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable, customItems=customItems,
                customTheta=customTheta )
summary(mod5)
# estimated item response functions
imod5 &lt;- IRT.irfprob( mod5 )
round( imod5[,2,], 3 )

#############################################################################
## EXAMPLE 2: Multiple group models with xxirt
#############################################################################

data(data.math)
dat &lt;- data.math$data
items &lt;- grep( "M[A-Z]", colnames(dat), value=TRUE )
I &lt;- length(items)

Theta &lt;- matrix( seq(-8,8,len=31), ncol=1 )

#****************************************************************************
#******* Model 1: Rasch model, single group

#*** Theta distribution
P_Theta1 &lt;- function( par, Theta, G  ){
    mu &lt;- par[1]
    sigma &lt;- max( par[2], .01 )
    p1 &lt;- stats::dnorm( Theta[,1], mean=mu, sd=sigma)
    p1 &lt;- p1 / sum(p1)
    probs &lt;- matrix( p1, ncol=1)
    return(probs)
}

par_Theta &lt;- c(0,1)
names(par_Theta) &lt;- c("mu","sigma")
customTheta  &lt;- sirt::xxirt_createThetaDistribution( par=par_Theta,
                   est=c(FALSE,TRUE), P=P_Theta1  )
customTheta

#*** IRF 1PL logit
P_1PL &lt;- function( par, Theta, ncat){
    b &lt;- par
    TP &lt;- nrow(Theta)
    P &lt;- matrix( NA, nrow=TP, ncol=ncat)
    P[,2] &lt;- plogis( Theta - b )
    P[,1] &lt;- 1 - P[,2]
    return(P)
}
par &lt;- c("b"=0)
item_1PL &lt;- sirt::xxirt_createDiscItem( name="1PL", par=par, est=c(TRUE), P=P_1PL)
customItems &lt;- list( item_1PL )

itemtype &lt;- rep( "1PL", I )
partable &lt;- sirt::xxirt_createParTable( dat[,items], itemtype=itemtype,
                       customItems=customItems )
partable &lt;- sirt::xxirt_modifyParTable( partable=partable, parname="b",
                  value=- stats::qlogis( colMeans(dat[,items] )) )

#*** estimate model
mod1 &lt;- sirt::xxirt( dat=dat[,items], Theta=Theta, partable=partable,
                customItems=customItems, customTheta=customTheta )
summary(mod1)

#****************************************************************************
#******* Model 2: Rasch model, multiple groups

#*** Theta distribution
P_Theta2 &lt;- function( par, Theta, G  ){
    mu1 &lt;- par[1]
    mu2 &lt;- par[2]
    sigma1 &lt;- max( par[3], .01 )
    sigma2 &lt;- max( par[4], .01 )
    TP &lt;- nrow(Theta)
    probs &lt;- matrix( NA, nrow=TP, ncol=G)
    p1 &lt;- stats::dnorm( Theta[,1], mean=mu1, sd=sigma1)
    probs[,1] &lt;- p1 / sum(p1)
    p1 &lt;- stats::dnorm( Theta[,1], mean=mu2, sd=sigma2)
    probs[,2] &lt;- p1 / sum(p1)
    return(probs)
}
par_Theta &lt;- c(0,0,1,1)
names(par_Theta) &lt;- c("mu1","mu2","sigma1","sigma2")
customTheta2  &lt;- sirt::xxirt_createThetaDistribution( par=par_Theta,
                    est=c(FALSE,TRUE,TRUE,TRUE), P=P_Theta2  )
print(customTheta2)

#*** estimate model
mod2 &lt;- sirt::xxirt( dat=dat[,items], group=dat$female, Theta=Theta, partable=partable,
           customItems=customItems, customTheta=customTheta2, maxit=40)
summary(mod2)
IRT.compareModels(mod1, mod2)

#*** compare results with TAM package
library(TAM)
mod2b &lt;- TAM::tam.mml( resp=dat[,items], group=dat$female )
summary(mod2b)
IRT.compareModels(mod1, mod2, mod2b)

#############################################################################
## EXAMPLE 3: Regularized 2PL model
#############################################################################

data(data.read, package="sirt")
dat &lt;- data.read

#------ Definition of item response functions

#*** IRF 2PL
P_2PL &lt;- function( par, Theta, ncat){
    a &lt;- par[1]
    b &lt;- par[2]
    TP &lt;- nrow(Theta)
    P &lt;- matrix( NA, nrow=TP, ncol=ncat)
    P[,1] &lt;- 1
    for (cc in 2:ncat){
        P[,cc] &lt;- exp( (cc-1) * a * Theta[,1] - b )
    }
    P &lt;- P / rowSums(P)
    return(P)
}

#** created item classes of 1PL and 2PL models
par &lt;- c( "a"=1, "b"=0 )
# define some slightly informative prior of 2PL
item_2PL &lt;- sirt::xxirt_createDiscItem( name="2PL", par=par, est=c(TRUE,TRUE),
               P=P_2PL, prior=c(a="dlnorm"), prior_par1=c( a=0 ),
               prior_par2=c(a=5) )
customItems &lt;- list( item_2PL )

#---- definition theta distribution

#** theta grid
Theta &lt;- matrix( seq(-6,6,length=21), ncol=1 )

#** theta distribution
P_Theta1 &lt;- function( par, Theta, G){
    mu &lt;- par[1]
    sigma &lt;- max( par[2], .01 )
    TP &lt;- nrow(Theta)
    pi_Theta &lt;- matrix( 0, nrow=TP, ncol=G)
    pi1 &lt;- dnorm( Theta[,1], mean=mu, sd=sigma )
    pi1 &lt;- pi1 / sum(pi1)
    pi_Theta[,1] &lt;- pi1
    return(pi_Theta)
}
#** create distribution class
par_Theta &lt;- c( "mu"=0, "sigma"=1 )
customTheta  &lt;- sirt::xxirt_createThetaDistribution( par=par_Theta, est=c(FALSE,FALSE),
                       P=P_Theta1 )

#****************************************************************************
#******* Model 1: 2PL model

itemtype &lt;- rep( "2PL", 12 )
partable &lt;- sirt::xxirt_createParTable( dat, itemtype=itemtype,
                        customItems=customItems )

mod1 &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable,
                   customItems=customItems, customTheta=customTheta)
summary(mod1)

#****************************************************************************
#******* Model 2: Regularized 2PL model with regularization on item loadings

# define regularized estimation of item loadings
parindex &lt;- partable[ partable$parname=="a","parindex"]

#** penalty is defined by -N*lambda*sum_i (a_i-1)^2
N &lt;- nrow(dat)
lambda &lt;- .02
penalty_fun_item &lt;- function(x)
{
    val &lt;- N*lambda*sum( ( x[parindex]-1)^2)
    return(val)
}
# estimate standard deviation
customTheta1  &lt;- sirt::xxirt_createThetaDistribution( par=par_Theta, est=c(FALSE,TRUE),
                       P=P_Theta1 )
mod2 &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable,
                   customItems=customItems, customTheta=customTheta1,
                   penalty_fun_item=penalty_fun_item)
summary(mod2)

#############################################################################
## EXAMPLE 4: 2PL mixture model
#############################################################################

#*** simulate data
set.seed(123)
N &lt;- 4000   # number of persons
I &lt;- 15     # number of items
prop &lt;- .25 # mixture proportion for second class

# discriminations and difficulties in first class
a1 &lt;- rep(1,I)
b1 &lt;- seq(-2,2,len=I)
# distribution in second class
mu2 &lt;- 1
sigma2 &lt;- 1.2
# compute parameters with constraint N(0,1) in second class
# a*(sigma*theta+mu-b)=a*sigma*(theta-(b-mu)/sigma)
#=&gt; a2=a*sigma and b2=(b-mu)/sigma
a2 &lt;- a1
a2[c(2,4,6,8)] &lt;- 0.2  # some items with different discriminations
a2 &lt;- a2*sigma2
b2 &lt;- b1
b2[1:5] &lt;- 1   # first 5 item with different difficulties
b2 &lt;- (b2-mu2)/sigma2
dat1 &lt;- sirt::sim.raschtype(theta=stats::rnorm(N*(1-prop)), b=b1, fixed.a=a1)
dat2 &lt;- sirt::sim.raschtype(theta=stats::rnorm(N*prop), b=b2, fixed.a=a2)
dat &lt;- rbind(dat1, dat2)

#**** model specification

#*** define theta distribution
TP &lt;- 21
theta &lt;- seq(-6,6,length=TP)
# stack theta vectors below each others=&gt; 2 latent classes
Theta &lt;- matrix( c(theta, theta ), ncol=1 )
# distribution of theta (i.e., N(0,1))
w_theta &lt;- dnorm(theta)
w_theta &lt;- w_theta / sum(w_theta)

P_Theta1 &lt;- function( par, Theta, G){
    p2_logis &lt;- par[1]
    p2 &lt;- stats::plogis( p2_logis )
    p1 &lt;- 1-p2
    pi_Theta &lt;- c( p1*w_theta, p2*w_theta)
    pi_Theta &lt;- matrix(pi_Theta, ncol=1)
    return(pi_Theta)
}

par_Theta &lt;- c( p2_logis=qlogis(.25))
customTheta  &lt;- sirt::xxirt_createThetaDistribution( par=par_Theta, est=c(TRUE),
                       P=P_Theta1)

# IRF for 2-class mixture 2PL model
par &lt;- c(a1=1, a2=1, b1=0, b2=.5)

P_2PLmix &lt;- function( par, Theta, ncat)
{
    a1 &lt;- par[1]
    a2 &lt;- par[2]
    b1 &lt;- par[3]
    b2 &lt;- par[4]
    P &lt;- matrix( NA, nrow=2*TP, ncol=ncat)
    TP &lt;- nrow(Theta)/2
    P1 &lt;- stats::plogis( a1*(Theta[1:TP,1]-b1) )
    P2 &lt;- stats::plogis( a2*(Theta[TP+1:(2*TP),1]-b2) )
    P[,2] &lt;- c(P1, P2)
    P[,1] &lt;- 1-P[,2]
    return(P)
}

# define some slightly informative prior of 2PL
item_2PLmix &lt;- sirt::xxirt_createDiscItem( name="2PLmix", par=par,
               est=c(TRUE,TRUE,TRUE,TRUE), P=P_2PLmix )
customItems &lt;- list( item_2PLmix )

#****************************************************************************
#******* Model 1: 2PL mixture model

itemtype &lt;- rep( "2PLmix", I )
partable &lt;- sirt::xxirt_createParTable( dat, itemtype=itemtype,
                        customItems=customItems )
mod1 &lt;- sirt::xxirt( dat=dat, Theta=Theta, partable=partable,
                   customItems=customItems, customTheta=customTheta)
summary(mod1)

## End(Not run)
</code></pre>

<hr>
<h2 id='xxirt_createParTable'>
Create Item Response Functions and Item Parameter Table
</h2><span id='topic+xxirt_createParTable'></span><span id='topic+xxirt_createDiscItem'></span><span id='topic+xxirt_modifyParTable'></span>

<h3>Description</h3>

<p>Create item response functions and item parameter table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xxirt_createDiscItem( name, par, est, P, lower=-Inf, upper=Inf,
     prior=NULL, prior_par1=NULL, prior_par2=NULL)

xxirt_createParTable(dat, itemtype, customItems=NULL)

xxirt_modifyParTable( partable, parname, item=NULL, value=NULL,
     est=NULL, parlabel=NULL, parindex=NULL, lower=NULL,
     upper=NULL, prior=NULL, prior_par1=NULL, prior_par2=NULL )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xxirt_createParTable_+3A_name">name</code></td>
<td>
<p>Type of item response function</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_par">par</code></td>
<td>
<p>Named vector of starting values of item parameters</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_est">est</code></td>
<td>
<p>Logical vector indicating which parameters should be estimated</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_p">P</code></td>
<td>
<p>Item response function</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_lower">lower</code></td>
<td>
<p>Lower bounds</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_upper">upper</code></td>
<td>
<p>Upper bounds</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_prior">prior</code></td>
<td>
<p>Prior distribution</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_prior_par1">prior_par1</code></td>
<td>
<p>First parameter prior distribution</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_prior_par2">prior_par2</code></td>
<td>
<p>Second parameter prior distribution</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_dat">dat</code></td>
<td>

<p>Data frame with item responses
</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_itemtype">itemtype</code></td>
<td>

<p>Vector of item types
</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_customitems">customItems</code></td>
<td>

<p>List with item objects created by <code>xxirt_createDiscItem</code>
</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_partable">partable</code></td>
<td>
<p>Item parameter table</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_parname">parname</code></td>
<td>
<p>Parameter name</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_item">item</code></td>
<td>
<p>Item</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_value">value</code></td>
<td>
<p>Value of item parameter</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_parindex">parindex</code></td>
<td>
<p>Parameter index</p>
</td></tr>
<tr><td><code id="xxirt_createParTable_+3A_parlabel">parlabel</code></td>
<td>
<p>Item parameter label</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+xxirt">xxirt</a></code>
</p>
<p>See <code><a href="mirt.html#topic+createItem">mirt::createItem</a></code> for similar
functionality.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
## EXAMPLE 1: Definition of item response functions
#############################################################################

data(data.read)
dat &lt;- data.read

#------ Definition of item response functions
#*** IRF 2PL
P_2PL &lt;- function( par, Theta, ncat){
    a &lt;- par[1]
    b &lt;- par[2]
    TP &lt;- nrow(Theta)
    P &lt;- matrix( NA, nrow=TP, ncol=ncat)
    P[,1] &lt;- 1
    for (cc in 2:ncat){
        P[,cc] &lt;- exp( (cc-1) * a * Theta[,1] - b )
    }
    P &lt;- P / rowSums(P)
    return(P)
}

#*** IRF 1PL
P_1PL &lt;- function( par, Theta, ncat){
    b &lt;- par[1]
    TP &lt;- nrow(Theta)
    par0 &lt;- c(1,b)
    P &lt;- P_2PL( par=par0, Theta=Theta, ncat=ncat)
    return(P)
}

#** created item classes of 1PL and 2PL models
par &lt;- c( "a"=1, "b"=0 )
# define some slightly informative prior of 2PL
item_2PL &lt;- sirt::xxirt_createDiscItem( name="2PL", par=par, est=c(TRUE,TRUE),
                P=P_2PL, prior=c( a="dlnorm"), prior_par1=c(a=0),
                prior_par2=c(a=5) )
item_1PL &lt;- sirt::xxirt_createDiscItem( name="1PL", par=par[2], est=c(TRUE),
                P=P_1PL )
# list of item classes in customItems
customItems &lt;- list( item_1PL,  item_2PL )

#-- create parameter table
itemtype &lt;- rep( "1PL", 12 )
partable &lt;- sirt::xxirt_createParTable(dat, itemtype=itemtype, customItems=customItems)
# privide starting values
partable1 &lt;- sirt::xxirt_modifyParTable( partable, parname="b",
                   value=- stats::qlogis( colMeans(dat) ) )
# equality constraint of parameters and definition of lower bounds
partable1 &lt;- sirt::xxirt_modifyParTable( partable1, item=c("A1","A2"),
                parname="b", parindex=110, lower=-1, value=0)
print(partable1)
</code></pre>

<hr>
<h2 id='xxirt_createThetaDistribution'>
Creates a User Defined Theta Distribution
</h2><span id='topic+xxirt_createThetaDistribution'></span>

<h3>Description</h3>

<p>Creates a user defined theta distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xxirt_createThetaDistribution(par, est, P, prior=NULL, prior_par1=NULL,
       prior_par2=NULL, lower=NULL, upper=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xxirt_createThetaDistribution_+3A_par">par</code></td>
<td>

<p>Parameter vector with starting values
</p>
</td></tr>
<tr><td><code id="xxirt_createThetaDistribution_+3A_est">est</code></td>
<td>

<p>Vector of logicals indicating which parameters should be estimated
</p>
</td></tr>
<tr><td><code id="xxirt_createThetaDistribution_+3A_p">P</code></td>
<td>

<p>Distribution function for <code class="reqn">\bold{\theta}</code>
</p>
</td></tr>
<tr><td><code id="xxirt_createThetaDistribution_+3A_prior">prior</code></td>
<td>

<p>Prior distribution
</p>
</td></tr>
<tr><td><code id="xxirt_createThetaDistribution_+3A_prior_par1">prior_par1</code></td>
<td>

<p>First parameter of prior distribution
</p>
</td></tr>
<tr><td><code id="xxirt_createThetaDistribution_+3A_prior_par2">prior_par2</code></td>
<td>

<p>Second parameter of prior distribution
</p>
</td></tr>
<tr><td><code id="xxirt_createThetaDistribution_+3A_lower">lower</code></td>
<td>
<p>Lower bounds for parameters</p>
</td></tr>
<tr><td><code id="xxirt_createThetaDistribution_+3A_upper">upper</code></td>
<td>
<p>Upper bounds for parameters</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+xxirt">xxirt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################################################
## EXAMPLE 1: Definition of theta distribution
#############################################################################

#** theta grid
Theta &lt;- matrix( seq(-10,10,length=31), ncol=1 )

#** theta distribution
P_Theta1 &lt;- function( par, Theta, G){
    mu &lt;- par[1]
    sigma &lt;- max( par[2], .01 )
    TP &lt;- nrow(Theta)
    pi_Theta &lt;- matrix( 0, nrow=TP, ncol=G)
    pi1 &lt;- stats::dnorm( Theta[,1], mean=mu, sd=sigma )
    pi1 &lt;- pi1 / sum(pi1)
    pi_Theta[,1] &lt;- pi1
    return(pi_Theta)
                }
#** create distribution class
par_Theta &lt;- c( "mu"=0, "sigma"=1 )
customTheta  &lt;- sirt::xxirt_createThetaDistribution( par=par_Theta,
                       est=c(FALSE,TRUE), P=P_Theta1 )
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
