<!DOCTYPE html><html><head><title>Help for package FSelectorRcpp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FSelectorRcpp}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.information_gain'><p>Direct Interface to Information Gain.</p></a></li>
<li><a href='#cut_attrs'><p>Select Attributes by Score Depending on the Cutoff</p></a></li>
<li><a href='#discretize'><p>Discretization</p></a></li>
<li><a href='#discretize_transform'><p>Transform a data.frame using split points returned by discretize function.</p></a></li>
<li><a href='#feature_search'><p>General Feature Searching Engine</p></a></li>
<li><a href='#information_gain'><p>Entropy-based Filters</p></a></li>
<li><a href='#relief'><p>RReliefF filter</p></a></li>
<li><a href='#to_formula'><p>Create a formula Object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>'Rcpp' Implementation of 'FSelector' Entropy-Based Feature
Selection Algorithms with a Sparse Matrix Support</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.11</td>
</tr>
<tr>
<td>Description:</td>
<td>'Rcpp' (free of 'Java'/'Weka') implementation of 'FSelector' entropy-based feature selection 
 algorithms based on an MDL discretization (Fayyad U. M., Irani K. B.: Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning.
 In 13'th International Joint Conference on Uncertainly in Artificial Intelligence (IJCAI93), pages 1022-1029, Chambery, France, 1993.) <a href="https://www.ijcai.org/Proceedings/93-2/Papers/022.pdf">https://www.ijcai.org/Proceedings/93-2/Papers/022.pdf</a>
 with a sparse matrix support.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.12), foreach, iterators</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, BH, RcppArmadillo, testthat</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, Matrix, RcppArmadillo, dplyr, RWeka, entropy,
FSelector, randomForest, doParallel, rpart, MASS, covr,
parallel, htmltools, magrittr, knitr, RTCGA.rnaseq, ggplot2,
microbenchmark, pbapply, tibble, rmarkdown, lintr, pkgdown,
withr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mi2-warsaw/FSelectorRcpp/issues">https://github.com/mi2-warsaw/FSelectorRcpp/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mi2-warsaw/FSelectorRcpp">https://github.com/mi2-warsaw/FSelectorRcpp</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-25 13:25:47 UTC; zzawwadz</td>
</tr>
<tr>
<td>Author:</td>
<td>Zygmunt Zawadzki [aut, cre],
  Marcin Kosinski [aut],
  Krzysztof Slomczynski [ctb],
  Damian Skrzypiec [ctb],
  Patrick Schratz <a href="https://orcid.org/0000-0003-0748-6624"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Zygmunt Zawadzki &lt;zygmunt@zstat.pl&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-28 16:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.information_gain'>Direct Interface to Information Gain.</h2><span id='topic+.information_gain'></span>

<h3>Description</h3>

<p>Direct Interface to Information Gain.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.information_gain(
  x,
  y,
  type = c("infogain", "gainratio", "symuncert"),
  equal = FALSE,
  discIntegers = TRUE,
  nbins = 5,
  threads = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".information_gain_+3A_x">x</code></td>
<td>
<p>A <a href="base.html#topic+data.frame">data.frame</a>, sparse matrix or formula with attributes.</p>
</td></tr>
<tr><td><code id=".information_gain_+3A_y">y</code></td>
<td>
<p>A vector with response variable or data.frame if formula is used.</p>
</td></tr>
<tr><td><code id=".information_gain_+3A_type">type</code></td>
<td>
<p>Method name.</p>
</td></tr>
<tr><td><code id=".information_gain_+3A_equal">equal</code></td>
<td>
<p>A logical. Whether to discretize dependent variable with the
<code>equal frequency binning discretization</code> or not.</p>
</td></tr>
<tr><td><code id=".information_gain_+3A_discintegers">discIntegers</code></td>
<td>
<p>logical value.
If true (default), then integers are treated as numeric vectors and they are discretized.
If false  integers are treated as factors and they are left as is.</p>
</td></tr>
<tr><td><code id=".information_gain_+3A_nbins">nbins</code></td>
<td>
<p>Number of bins used for discretization. Only used if 'equal = TRUE' and the response is numeric.</p>
</td></tr>
<tr><td><code id=".information_gain_+3A_threads">threads</code></td>
<td>
<p>defunct. Number of threads for parallel backend - now turned off because of safety reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In principle using <code><a href="#topic+information_gain">information_gain</a></code> is safer.
</p>
<p>data.frame with the following columns:
</p>

<ul>
<li><p>attributes - variables names.
</p>
</li>
<li><p>importance - worth of the attributes.
</p>
</li></ul>


<hr>
<h2 id='cut_attrs'>Select Attributes by Score Depending on the Cutoff</h2><span id='topic+cut_attrs'></span>

<h3>Description</h3>

<p>Select attributes by their score/rank/weights, depending on the cutoff that may be specified
by the percentage of the highest ranked attributes or by the number of the highest ranked attributes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cut_attrs(attrs, k = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cut_attrs_+3A_attrs">attrs</code></td>
<td>
<p>A <a href="base.html#topic+data.frame">data.frame</a> with attributes' importance.</p>
</td></tr>
<tr><td><code id="cut_attrs_+3A_k">k</code></td>
<td>
<p>A numeric. For <code>k &gt;= 1</code> it takes <code>floor(k)</code> and then it indicates how many attributes to
take with the highest attribute rank (chooses k best attributes).
For <code>0 &lt; k &lt; 1</code> it stands for the percent of top attributes to take
(chooses best k * 100% of attributes).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Damian Skrzypiec <a href="mailto:damian.j.skrzypiec@gmail.com">damian.j.skrzypiec@gmail.com</a> and
Zygmunt Zawadzki <a href="mailto:zygmunt@zstat.pl">zygmunt@zstat.pl</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- information_gain(Species ~ ., iris)
cut_attrs(attrs = x)
to_formula(cut_attrs(attrs = x), "Species")
cut_attrs(attrs = x, k = 1)
</code></pre>

<hr>
<h2 id='discretize'>Discretization</h2><span id='topic+discretize'></span><span id='topic+mdlControl'></span><span id='topic+equalsizeControl'></span><span id='topic+customBreaksControl'></span>

<h3>Description</h3>

<p>Discretize a range of numeric attributes in the dataset into nominal
attributes. <code>Minimum Description Length</code> (MDL) method is set as the default
control. There is also available <code>equalsizeControl</code> method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discretize(
  x,
  y,
  control = list(mdlControl(), equalsizeControl()),
  all = TRUE,
  discIntegers = TRUE,
  call = NULL
)

mdlControl()

equalsizeControl(k = 10)

customBreaksControl(breaks)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discretize_+3A_x">x</code></td>
<td>
<p>Explanatory continuous variables to be discretized or a <a href="stats.html#topic+formula">formula</a>.</p>
</td></tr>
<tr><td><code id="discretize_+3A_y">y</code></td>
<td>
<p>Dependent variable for supervised discretization or a <a href="base.html#topic+data.frame">data.frame</a> when <code>x</code> ia a <a href="stats.html#topic+formula">formula</a>.</p>
</td></tr>
<tr><td><code id="discretize_+3A_control">control</code></td>
<td>
<p><code>discretizationControl</code> object containing the parameters for
discretization algorithm. Possible inputs are <code>mdlControl</code> or <code>equalsizeControl</code>, so far. If passed as a list, the first element is used.</p>
</td></tr>
<tr><td><code id="discretize_+3A_all">all</code></td>
<td>
<p>Logical indicating if a returned <a href="base.html#topic+data.frame">data.frame</a> should contain other features that were not discretized.
(Example: should <code>Sepal.Width</code> be returned, when you pass <code>iris</code> and discretize <code>Sepal.Length, Petal.Length, Petal.Width</code>.)</p>
</td></tr>
<tr><td><code id="discretize_+3A_discintegers">discIntegers</code></td>
<td>
<p>logical value.
If true (default), then integers are treated as numeric vectors and they are discretized.
If false integers are treated as factors and they are left as is.</p>
</td></tr>
<tr><td><code id="discretize_+3A_call">call</code></td>
<td>
<p>Keep as <code>NULL</code>. Inner method parameter for consistency.</p>
</td></tr>
<tr><td><code id="discretize_+3A_k">k</code></td>
<td>
<p>Number of partitions.</p>
</td></tr>
<tr><td><code id="discretize_+3A_breaks">breaks</code></td>
<td>
<p>custom breaks used for partitioning.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zygmunt Zawadzki <a href="mailto:zygmunt@zstat.pl">zygmunt@zstat.pl</a>
</p>


<h3>References</h3>

<p>U. M. Fayyad and K. B. Irani. Multi-Interval Discretization of
Continuous-Valued Attributes for Classification Learning. In 13th
International Joint Conference on Uncertainly in Artificial
Intelligence(IJCAI93), pages 1022-1029, 1993.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# vectors
discretize(x = iris[[1]], y = iris[[5]])

# list and vector
head(discretize(x = list(iris[[1]], iris$Sepal.Width), y = iris$Species))

# formula input
head(discretize(x = Species ~ ., y = iris))
head(discretize(Species ~ ., iris))

# use different methods for specific columns
ir1 &lt;- discretize(Species ~ Sepal.Length, iris)
ir2 &lt;- discretize(Species ~ Sepal.Width, ir1, control = equalsizeControl(3))
ir3 &lt;- discretize(Species ~ Petal.Length, ir2, control = equalsizeControl(5))
head(ir3)

# custom breaks
ir &lt;- discretize(Species ~ Sepal.Length, iris,
  control = customBreaksControl(breaks = c(0, 2, 5, 7.5, 10)))
head(ir)

## Not run: 
# Same results
library(RWeka)
Rweka_disc_out &lt;- RWeka::Discretize(Species ~ Sepal.Length, iris)[, 1]
FSelectorRcpp_disc_out &lt;- FSelectorRcpp::discretize(Species ~ Sepal.Length,
                                                    iris)[, 1]
table(Rweka_disc_out, FSelectorRcpp_disc_out)
# But faster method
library(microbenchmark)
microbenchmark(FSelectorRcpp::discretize(Species ~ Sepal.Length, iris),
               RWeka::Discretize(Species ~ Sepal.Length, iris))


## End(Not run)

</code></pre>

<hr>
<h2 id='discretize_transform'>Transform a data.frame using split points returned by discretize function.</h2><span id='topic+discretize_transform'></span><span id='topic+extract_discretize_transformer'></span>

<h3>Description</h3>

<p>Transform a data.frame using split points returned by discretize function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discretize_transform(disc, data, dropColumns = NA)

extract_discretize_transformer(disc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discretize_transform_+3A_disc">disc</code></td>
<td>
<p>a result of the <code><a href="#topic+discretize">discretize</a></code> function.</p>
</td></tr>
<tr><td><code id="discretize_transform_+3A_data">data</code></td>
<td>
<p>a data.frame to transform using cutpoints from disc.</p>
</td></tr>
<tr><td><code id="discretize_transform_+3A_dropcolumns">dropColumns</code></td>
<td>
<p>determine</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new data.frame with discretized columns using cutpoints
from the result of discretize function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(123)
idx &lt;- sort(sample.int(150, 100))
iris1 &lt;- iris[idx, ]
iris2 &lt;- iris[-idx, ]
disc &lt;- discretize(Species ~ ., iris)
head(discretize_transform(disc, iris2))

# Chain discretization:
ir1 &lt;- discretize(Species ~ Sepal.Length, iris1)
ir2 &lt;- discretize(Species ~ Sepal.Width, ir1, control = equalsizeControl(3))
ir3 &lt;- discretize(Species ~ Petal.Length, ir2, control = equalsizeControl(5))

## note that Petal.Width is untouched:
head(discretize_transform(ir3, iris2))

## extract_discretize_transformer
discObj &lt;- extract_discretize_transformer(ir3)
head(discretize_transform(discObj, iris2))

</code></pre>

<hr>
<h2 id='feature_search'>General Feature Searching Engine</h2><span id='topic+feature_search'></span>

<h3>Description</h3>

<p>A convenience wrapper for <code>greedy</code> and <code>exhaustive</code> feature selection algorithms that
extract valuable attributes depending on the evaluation method (called evaluator). This function
is a reimplementation of <span class="pkg">FSelector</span>'s <a href="FSelector.html#topic+exhaustive.search">exhaustive.search</a> and <a href="FSelector.html#topic+greedy.search">greedy.search</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>feature_search(
  attributes,
  fun,
  data,
  mode = c("greedy", "exhaustive"),
  type = c("forward", "backward"),
  sizes = 1:length(attributes),
  parallel = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feature_search_+3A_attributes">attributes</code></td>
<td>
<p>A character vector with attributes' names to be used to extract the most valuable features.</p>
</td></tr>
<tr><td><code id="feature_search_+3A_fun">fun</code></td>
<td>
<p>A function (evaluator) to be used to score features' sets at each iteration of the algorithm passed via <code>mode</code>.
See Examples.</p>
</td></tr>
<tr><td><code id="feature_search_+3A_data">data</code></td>
<td>
<p>A data set for <code>fun</code> function (evaluator).</p>
</td></tr>
<tr><td><code id="feature_search_+3A_mode">mode</code></td>
<td>
<p>A character that determines which search algorithm to perform. Defualt is <code>"greedy"</code>.</p>
</td></tr>
<tr><td><code id="feature_search_+3A_type">type</code></td>
<td>
<p>Used when <code>mode = "greedy"</code> - whether to use the
<code>backward</code> or the <code>forward</code> multiple-way search. Default is <code>"forward"</code>.</p>
</td></tr>
<tr><td><code id="feature_search_+3A_sizes">sizes</code></td>
<td>
<p>Used when <code>mode = "exhaustive"</code> - a vector of sizes
of attributes subsets.</p>
</td></tr>
<tr><td><code id="feature_search_+3A_parallel">parallel</code></td>
<td>
<p>Allow parallelization.</p>
</td></tr>
<tr><td><code id="feature_search_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <a href="foreach.html#topic+foreach">foreach</a> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The evaluator function passed with <code>fun</code> is used to determine
the importance score of current features' subset.
The score is used in a multiple-way (backward or forward) <code>greedy</code>
algorithm as a stopping moment or as a selection criterion
in the <code>exhaustive</code> search that checks all possible
attributes' subset combinations (of sizes passed in <code>sizes</code>).
</p>


<h3>Value</h3>

<p>A list with following components
</p>

<ul>
<li><p> best - a <a href="base.html#topic+data.frame">data.frame</a> with the best subset and it's score (1 - feature used, 0 - feature not used),
</p>
</li>
<li><p> all - a <a href="base.html#topic+data.frame">data.frame</a> with all checked features' subsets and their score (1 - feature used, 0 - feature not used),
</p>
</li>
<li><p> data - the data used in the feature selection,
</p>
</li>
<li><p> fun - the evaluator used to compute the score of importance for features' subsets,
</p>
</li>
<li><p> call - an origin call of the <code>feature_search</code>,
</p>
</li>
<li><p> mode - the mode used in the call.
</p>
</li></ul>



<h3>Note</h3>

<p>Note that score depends on the evaluator you provide in the <code>fun</code> parameter.
</p>


<h3>Author(s)</h3>

<p>Zygmunt Zawadzki <a href="mailto:zygmunt@zstat.pl">zygmunt@zstat.pl</a>
</p>
<p>Krzysztof Slomczynski <a href="mailto:krzysztofslomczynski@gmail.com">krzysztofslomczynski@gmail.com</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Enable parallelization in examples
## Not run: 
 library(doParallel)
 cl &lt;- makeCluster(2)
 registerDoParallel(cl)

## End(Not run)
# Close at the end
# stopCluster(cl) #nolint
# registerDoSEQ() #nolint

# 1) Evaluator from FSelector package.
evaluator &lt;- function(subset, data, dependent = names(iris)[5]) {
  library(rpart)
  k &lt;- 5
  splits &lt;- runif(nrow(data))
  results &lt;- sapply(1:k, function(i) {
    test.idx &lt;- (splits &gt;= (i - 1) / k) &amp; (splits &lt; i / k)
    train.idx &lt;- !test.idx
    test &lt;- data[test.idx, , drop = FALSE]
    train &lt;- data[train.idx, , drop = FALSE]
    tree &lt;- rpart(to_formula(subset, dependent), train)
    error.rate &lt;- sum(test[[dependent]] != predict(tree, test, type = "c")) /
    nrow(test)
    return(1 - error.rate)
  })
  return(mean(results))
}

set.seed(123)
# Default greedy search.
system.time(
  feature_search(attributes = names(iris)[-5],
                 fun = evaluator,
                 data = iris)
)
system.time(
  feature_search(attributes = names(iris)[-5],
                 fun = evaluator,
                 data = iris,
                 parallel = FALSE)
)

# Optional exhaustive search.
system.time(
  feature_search(attributes = names(iris)[-5],
                 fun = evaluator,
                 data = iris,
                 mode = "exhaustive")
)
system.time(
  feature_search(attributes = names(iris)[-5],
                 fun = evaluator,
                 data = iris,
                 mode = "exhaustive",
                 parallel = FALSE)
)

# 2) Maximize R^2 statistics in the linear regression model/problem.

evaluator_R2_lm &lt;- function(attributes, data, dependent = names(iris)[1]) {
  summary(
    lm(to_formula(attributes, dependent), data = data)
  )$r.squared
}

feature_search(attributes = names(iris)[-1],
               fun = evaluator_R2_lm, data = iris,
               mode = "exhaustive")

# 3) Optimize BIC crietion in generalized linear model.
# Aim of Bayesian approach it to identify the model with the highest
# probability of being the true model. - Kuha 2004

utils::data(anorexia, package = "MASS")

evaluator_BIC_glm &lt;- function(attributes, data, dependent = "Postwt") {
  extractAIC(
    fit = glm(to_formula(attributes, dependent), family = gaussian,
              data = data),
    k = log(nrow(data))
  )[2]
}

feature_search(attributes = c("Prewt", "Treat", "offset(Prewt)"),
               fun = evaluator_BIC_glm,
               data = anorexia,
               mode = "exhaustive")

# Close parallelization
## Not run: 
stopCluster(cl)
registerDoSEQ()

## End(Not run)
</code></pre>

<hr>
<h2 id='information_gain'>Entropy-based Filters</h2><span id='topic+information_gain'></span>

<h3>Description</h3>

<p>Algorithms that find ranks of importance of discrete attributes, basing on their entropy with a continous class attribute. This function
is a reimplementation of <span class="pkg">FSelector</span>'s <a href="FSelector.html#topic+information.gain">information.gain</a>,
<a href="FSelector.html#topic+gain.ratio">gain.ratio</a> and <a href="FSelector.html#topic+symmetrical.uncertainty">symmetrical.uncertainty</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>information_gain(
  formula,
  data,
  x,
  y,
  type = c("infogain", "gainratio", "symuncert"),
  equal = FALSE,
  discIntegers = TRUE,
  nbins = 5,
  threads = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="information_gain_+3A_formula">formula</code></td>
<td>
<p>An object of class <a href="stats.html#topic+formula">formula</a> with model description.</p>
</td></tr>
<tr><td><code id="information_gain_+3A_data">data</code></td>
<td>
<p>A <a href="base.html#topic+data.frame">data.frame</a> accompanying formula.</p>
</td></tr>
<tr><td><code id="information_gain_+3A_x">x</code></td>
<td>
<p>A <a href="base.html#topic+data.frame">data.frame</a> or sparse matrix with attributes.</p>
</td></tr>
<tr><td><code id="information_gain_+3A_y">y</code></td>
<td>
<p>A vector with response variable.</p>
</td></tr>
<tr><td><code id="information_gain_+3A_type">type</code></td>
<td>
<p>Method name.</p>
</td></tr>
<tr><td><code id="information_gain_+3A_equal">equal</code></td>
<td>
<p>A logical. Whether to discretize dependent variable with the
<code>equal frequency binning discretization</code> or not.</p>
</td></tr>
<tr><td><code id="information_gain_+3A_discintegers">discIntegers</code></td>
<td>
<p>logical value.
If true (default), then integers are treated as numeric vectors and they are discretized.
If false  integers are treated as factors and they are left as is.</p>
</td></tr>
<tr><td><code id="information_gain_+3A_nbins">nbins</code></td>
<td>
<p>Number of bins used for discretization. Only used if 'equal = TRUE' and the response is numeric.</p>
</td></tr>
<tr><td><code id="information_gain_+3A_threads">threads</code></td>
<td>
<p>defunct. Number of threads for parallel backend - now turned off because of safety reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>type = "infogain"</code> is </p>
<p style="text-align: center;"><code class="reqn">H(Class) + H(Attribute) - H(Class,
Attribute)</code>
</p>

<p><code>type = "gainratio"</code> is </p>
<p style="text-align: center;"><code class="reqn">\frac{H(Class) + H(Attribute) - H(Class,
Attribute)}{H(Attribute)}</code>
</p>

<p><code>type = "symuncert"</code> is </p>
<p style="text-align: center;"><code class="reqn">2\frac{H(Class) + H(Attribute) - H(Class,
Attribute)}{H(Attribute) + H(Class)}</code>
</p>

<p>where H(X) is Shannon's Entropy for a variable X and H(X, Y) is a joint
Shannon's Entropy for a variable X with a condition to Y.
</p>


<h3>Value</h3>

<p>data.frame with the following columns:
</p>

<ul>
<li><p>attributes - variables names.
</p>
</li>
<li><p>importance - worth of the attributes.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zygmunt Zawadzki <a href="mailto:zygmunt@zstat.pl">zygmunt@zstat.pl</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
irisX &lt;- iris[-5]
y &lt;- iris$Species

## data.frame interface
information_gain(x = irisX, y = y)

# formula interface
information_gain(formula = Species ~ ., data = iris)
information_gain(formula = Species ~ ., data = iris, type = "gainratio")
information_gain(formula = Species ~ ., data = iris, type = "symuncert")

# sparse matrix interface
library(Matrix)
i &lt;- c(1, 3:8); j &lt;- c(2, 9, 6:10); x &lt;- 7 * (1:7)
x &lt;- sparseMatrix(i, j, x = x)
y &lt;- c(1, 1, 1, 1, 2, 2, 2, 2)

information_gain(x = x, y = y)
information_gain(x = x, y = y, type = "gainratio")
information_gain(x = x, y = y, type = "symuncert")

</code></pre>

<hr>
<h2 id='relief'>RReliefF filter</h2><span id='topic+relief'></span>

<h3>Description</h3>

<p>The algorithm finds weights of continuous and discrete attributes basing on a distance between instances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relief(formula, data, x, y, neighboursCount = 5, sampleSize = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relief_+3A_formula">formula</code></td>
<td>
<p>An object of class <a href="stats.html#topic+formula">formula</a> with model description.</p>
</td></tr>
<tr><td><code id="relief_+3A_data">data</code></td>
<td>
<p>A <a href="base.html#topic+data.frame">data.frame</a> accompanying formula.</p>
</td></tr>
<tr><td><code id="relief_+3A_x">x</code></td>
<td>
<p>A <a href="base.html#topic+data.frame">data.frame</a> with attributes.</p>
</td></tr>
<tr><td><code id="relief_+3A_y">y</code></td>
<td>
<p>A vector with response variable.</p>
</td></tr>
<tr><td><code id="relief_+3A_neighbourscount">neighboursCount</code></td>
<td>
<p>number of neighbours to find for every sampled instance</p>
</td></tr>
<tr><td><code id="relief_+3A_samplesize">sampleSize</code></td>
<td>
<p>number of instances to sample</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function and it's manual page taken directly from <span class="pkg">FSelector</span>:
Piotr Romanski and Lars Kotthoff (2018). FSelector: Selecting Attributes.
R package version 0.31. https://CRAN.R-project.org/package=FSelector
</p>


<h3>Value</h3>

<p>a data.frame containing the worth of attributes in the first column and their names as row names
</p>


<h3>References</h3>

<p>Igor Kononenko: Estimating Attributes: Analysis and Extensions of RELIEF. In: European Conference on Machine Learning, 171-182, 1994.
</p>
<p>Marko Robnik-Sikonja, Igor Kononenko: An adaptation of Relief for attribute estimation in regression. In: Fourteenth International Conference on Machine Learning, 296-304, 1997.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)

weights &lt;- relief(Species~., iris, neighboursCount = 5, sampleSize = 20)
print(weights)
subset &lt;- cut_attrs(weights, 2)
f &lt;- to_formula(subset, "Species")
print(f)
</code></pre>

<hr>
<h2 id='to_formula'>Create a formula Object</h2><span id='topic+to_formula'></span>

<h3>Description</h3>

<p>Utility function to create a <a href="stats.html#topic+formula">formula</a> object. Note that it may be very useful when you use pipes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to_formula(attrs, class)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="to_formula_+3A_attrs">attrs</code></td>
<td>
<p>Character vector with names of independent variables.</p>
</td></tr>
<tr><td><code id="to_formula_+3A_class">class</code></td>
<td>
<p>Single string with a dependent variable's name.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
# evaluator from FSelector package
evaluator &lt;- function(subset, data, dependent = names(iris)[5]) {
  library(rpart)
  k &lt;- 5
  splits &lt;- runif(nrow(data))
  results &lt;- sapply(1:k, function(i) {
    test.idx &lt;- (splits &gt;= (i - 1) / k) &amp; (splits &lt; i / k)
    train.idx &lt;- !test.idx
    test &lt;- data[test.idx, , drop = FALSE]
    train &lt;- data[train.idx, , drop = FALSE]
    tree &lt;- rpart(to_formula(subset, dependent), train)
    error.rate &lt;- sum(test[[dependent]] != predict(tree, test, type = "c")) /
    nrow(test)
    return(1 - error.rate)
  })
  return(mean(results))
}

set.seed(123)
fit &lt;- feature_search(attributes = names(iris)[-5], fun = evaluator, data = iris,
                mode = "exhaustive", parallel = FALSE)
fit$best
names(fit$best)[fit$best == 1]
# with to_formula
to_formula(names(fit$best)[fit$best == 1], "Species")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
