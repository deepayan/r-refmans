<!DOCTYPE html><html><head><title>Help for package BUCSS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BUCSS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BUCSS-package'><p>Bias and Uncertainty Corrected Sample Size (BUCSS)</p></a></li>
<li><a href='#ss.power.ba'><p>Necessary sample size to reach desired power for a one or two-way</p>
between-subjects ANOVA using an uncertainty and publication bias correction
procedure</a></li>
<li><a href='#ss.power.ba.general'><p>Necessary sample size to reach desired power for a between-subjects ANOVA</p>
with any number of factors using an uncertainty and publication bias
correction procedure</a></li>
<li><a href='#ss.power.dt'><p>Necessary sample size to reach desired power for a dependent t-test using an</p>
uncertainty and publication bias correction procedure</a></li>
<li><a href='#ss.power.it'><p>Necessary sample size to reach desired power for an independent t-test using</p>
an uncertainty and publication bias correction procedure</a></li>
<li><a href='#ss.power.reg.all'><p>Necessary sample size to reach desired power for a test of model R2 in a</p>
multiple regression using an uncertainty and publication bias correction
procedure</a></li>
<li><a href='#ss.power.reg.joint'><p>Necessary sample size to reach desired power for a test of multiple predictors</p>
in a multiple regression using an uncertainty and publication bias correction
procedure</a></li>
<li><a href='#ss.power.reg1'><p>Necessary sample size to reach desired power for a single coefficient in a</p>
multiple regression using an uncertainty and publication bias correction
procedure</a></li>
<li><a href='#ss.power.spa'><p>Necessary sample size to reach desired power for two-factor split-plot</p>
(mixed) ANOVA using an uncertainty and publication bias correction procedure</a></li>
<li><a href='#ss.power.spa.general'><p>Necessary sample size to reach desired power for a split-plot (mixed) ANOVA</p>
with any number of factors using an uncertainty and publication bias
correction procedure</a></li>
<li><a href='#ss.power.wa'><p>Necessary sample size to reach desired power for a one or two-way</p>
within-subjects ANOVA using an uncertainty and publication bias correction
procedure</a></li>
<li><a href='#ss.power.wa.general'><p>Necessary sample size to reach desired power for a within-subjects ANOVA with</p>
any number of factors using an uncertainty and publication bias correction
procedure</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bias and Uncertainty Corrected Sample Size</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Samantha F. Anderson &lt;samantha.f.anderson@asu.edu&gt;, Ken Kelley &lt;kkelley@nd.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ken Kelley &lt;kkelley@nd.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Bias- and Uncertainty-Corrected Sample Size. BUCSS implements a method of correcting for publication bias and
    uncertainty when planning sample sizes in a future study from an original study. See Anderson, Kelley, &amp; Maxwell (2017; Psychological Science, 28, 1547-1562). </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-08-21 20:16:17 UTC; kkelley</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-08-25 16:50:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='BUCSS-package'>Bias and Uncertainty Corrected Sample Size (BUCSS)</h2><span id='topic+BUCSS'></span><span id='topic+BUCSS-package'></span>

<h3>Description</h3>

<p>Bias- and Uncertainty-Corrected Sample Size. BUCSS implements a method of correcting for publication bias and uncertainty when planning sample sizes in a future study from an original study
</p>


<h3>Details</h3>

<p>Note that <a href="https://designingexperiments.com">https://designingexperiments.com</a> uses Shiny R apps that implement, via a web interface, the functions contained in BUCSS.
</p>


<h3>Author(s)</h3>

<p>Samantha Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a> and Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. &amp; Kelley, K., Maxwell, S. E. (2017). Sample size planning for more accurate statistical power: A method correcting sample effect sizes for uncertainty and publication bias. <em>Psychological Science</em>, <em>28</em>, 1547&ndash;1562.
</p>
<p>See <a href="https://designingexperiments.com/">https://designingexperiments.com/</a> for Shiny R implementation of the functions.
</p>
<p>For suggested updates, please email Samantha Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a> or Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>.
</p>

<hr>
<h2 id='ss.power.ba'>Necessary sample size to reach desired power for a one or two-way
between-subjects ANOVA using an uncertainty and publication bias correction
procedure</h2><span id='topic+ss.power.ba'></span>

<h3>Description</h3>

<p><code>ss.power.ba</code> returns the necessary per-group sample size
to achieve a desired level of statistical power for a planned study testing
an omnibus effect using a one or two-way fully between-subjects ANOVA,
based on information obtained from a previous study. The effect from the
previous study can be corrected for publication bias and/or uncertainty to
provide a sample size that will achieve more accurate statistical power for
a planned study, when compared to approaches that use a sample effect size
at face value or rely on sample size only. The bias and uncertainty
adjusted previous study noncentrality parameter is also returned, which can
be transformed to various effect size metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.ba(F.observed, N, levels.A, levels.B = NULL,
  effect = c("factor.A", "factor.B", "interaction"),
  alpha.prior = 0.05, alpha.planned = 0.05, assurance = 0.8,
  power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.ba_+3A_f.observed">F.observed</code></td>
<td>
<p>Observed F-value from a previous study used to plan sample
size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.ba_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study</p>
</td></tr>
<tr><td><code id="ss.power.ba_+3A_levels.a">levels.A</code></td>
<td>
<p>Number of levels for factor A</p>
</td></tr>
<tr><td><code id="ss.power.ba_+3A_levels.b">levels.B</code></td>
<td>
<p>Number of levels for factor B, which is NULL if a single
factor design</p>
</td></tr>
<tr><td><code id="ss.power.ba_+3A_effect">effect</code></td>
<td>
<p>Effect most of interest to the planned study: main effect of A
(<code>factor.A</code>), main effect of B (<code>factor.B</code>), interaction
(<code>interaction</code>)</p>
</td></tr>
<tr><td><code id="ss.power.ba_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.ba_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha-level (<code class="reqn">\alpha</code>) assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.ba_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.ba_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.ba_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentrality
parameter necessary for sample size planning (0 &lt; step &lt; .01) (users should
not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.ba</code> uses the observed
<code class="reqn">F</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary per-group sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. Thus, the ratio of the numerator and the denominator is a
truncated noncentral F distribution. (See Taylor &amp; Muller, 1996, Equation
2.1. and Anderson &amp; Maxwell,  2017, for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not correct for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>
<p><code>ss.power.ba</code> assumes that the planned study will have equal n.
Unequal n in the previous study is handled in the following way for
between-subjects anova designs. If the user enters an N not equally
divisible by the number of cells, the function calculates n by dividing N
by the number of cells and both rounding up and rounding down the result,
effectively assuming equal n. The suggested sample size for the planned
study is calculated using both of these values of n, and the function
returns the larger of these two suggestions, to be conservative. The
adjusted noncentrality parameter that is output is the lower of the two
possibilities, again, to be conservative. Although equal-n previous studies
are preferable, this approach will work well as long as the cell sizes are
only slightly discrepant.
</p>


<h3>Value</h3>

<p>Suggested per-group sample size for planned study
Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.ba(F.observed=5, N=120, levels.A=2, levels.B=3,  effect="factor.B",
alpha.prior=.05, alpha.planned=.05, assurance=.80, power=.80, step=.001)

</code></pre>

<hr>
<h2 id='ss.power.ba.general'>Necessary sample size to reach desired power for a between-subjects ANOVA
with any number of factors using an uncertainty and publication bias
correction procedure</h2><span id='topic+ss.power.ba.general'></span>

<h3>Description</h3>

<p><code>ss.power.ba.general</code> returns the necessary per-group
sample size to achieve a desired level of statistical power for a planned
study testing any type of effect (omnibus, contrast) using a fully
between-subjects ANOVA with any number of factors, based on information
obtained from a previous study. The effect from the previous study can be
corrected for publication bias and/or uncertainty to provide a sample size
that will achieve more accurate statistical power for a planned study, when
compared to approaches that use a sample effect size at face value or rely
on sample size only. The bias and uncertainty adjusted previous study
noncentrality parameter is also returned, which can be transformed to
various effect size metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.ba.general(F.observed, N, cells, df.numerator, df.denominator,
  alpha.prior = 0.05, alpha.planned = 0.05, assurance = 0.8,
  power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.ba.general_+3A_f.observed">F.observed</code></td>
<td>
<p>Observed F-value from a previous study used to plan sample
size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.ba.general_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study</p>
</td></tr>
<tr><td><code id="ss.power.ba.general_+3A_cells">cells</code></td>
<td>
<p>Number of cells for the design (the product of the number of
levels of each factor)</p>
</td></tr>
<tr><td><code id="ss.power.ba.general_+3A_df.numerator">df.numerator</code></td>
<td>
<p>Numerator degrees of freedom for the effect of interest</p>
</td></tr>
<tr><td><code id="ss.power.ba.general_+3A_df.denominator">df.denominator</code></td>
<td>
<p>Denominator degrees of freedom for the effect of
interest</p>
</td></tr>
<tr><td><code id="ss.power.ba.general_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.ba.general_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha-level (<code class="reqn">\alpha</code>) assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.ba.general_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.ba.general_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.ba.general_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentrality
parameter necessary for sample size planning (0 &lt; step &lt; .01) (users should
not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.ba.general</code> uses the observed
<code class="reqn">F</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary per-group sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. Thus, the ratio of the numerator and the denominator is a
truncated noncentral F distribution. (See Taylor &amp; Muller, 1996, Equation
2.1. and Anderson &amp; Maxwell,  2017, for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not correct for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>
<p><code>ss.power.ba.general</code> assumes that the planned study will have equal
n. Unequal n in the previous study is handled in the following way for
between-subjects anova designs. If the user enters an N not equally
divisible by the number of cells, the function calculates n by dividing N
by the number of cells and both rounding up and rounding down the result,
effectively assuming equal n. The suggested sample size for the planned
study is calculated using both of these values of n, and the function
returns the larger of these two suggestions, to be conservative. The
adjusted noncentrality parameter that is output is the lower of the two
possibilities, again, to be conservative. Although equal-n previous studies
are preferable, this approach will work well as  long as the cell sizes are
only slightly discrepant.
</p>


<h3>Value</h3>

<p>Suggested per-group sample size for planned study
Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.ba.general(F.observed=5, N=120, cells=6,  df.numerator=2,
df.denominator=117, alpha.prior=.05, alpha.planned=.05, assurance=.80,
power=.80, step=.001)

</code></pre>

<hr>
<h2 id='ss.power.dt'>Necessary sample size to reach desired power for a dependent t-test using an
uncertainty and publication bias correction procedure</h2><span id='topic+ss.power.dt'></span>

<h3>Description</h3>

<p><code>ss.power.dt</code> returns the necessary per-group sample size
to achieve a desired level of statistical power for a planned study using
a dependent t-test, based on information obtained from a previous study.
The effect from the previous study can be corrected for publication bias
and/or uncertainty to provide a sample size that will achieve more accurate
statistical power for a planned study, when compared to approaches that use
a sample effect size at face value or rely on sample size only. The bias
and uncertainty adjusted previous study noncentrality parameter is also
returned, which can be transformed to various effect size metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.dt(t.observed, N, alpha.prior = 0.05, alpha.planned = 0.05,
  assurance = 0.8, power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.dt_+3A_t.observed">t.observed</code></td>
<td>
<p>Observed <code class="reqn">t</code>-value from a previous study used to plan
sample size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.dt_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study</p>
</td></tr>
<tr><td><code id="ss.power.dt_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.dt_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha-level (<code class="reqn">\alpha</code>) assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.dt_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.dt_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.dt_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentrality
parameter necessary for sample size planning (0 &lt; step &lt; .01) (users should
not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.dt</code> uses the observed
<code class="reqn">t</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary per-group sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. In the two-group case, this formula reduces to the density of
a truncated noncentral <code class="reqn">t</code>-distribution.(See Taylor &amp; Muller, 1996,
Equation 2.1. and Anderson &amp; Maxwell, 2017, for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not correct for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>


<h3>Value</h3>

<p>Suggested per-group sample size for planned study
Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.dt(t.observed=3, N=40, alpha.prior=.05, alpha.planned=.05,
assurance=.80, power=.80, step=.001)

</code></pre>

<hr>
<h2 id='ss.power.it'>Necessary sample size to reach desired power for an independent t-test using
an uncertainty and publication bias correction procedure</h2><span id='topic+ss.power.it'></span>

<h3>Description</h3>

<p><code>ss.power.it</code> returns the necessary per-group sample size
to achieve a desired level of statistical power for a planned study using
an independent t-test, based on information obtained from a previous study.
The effect from the previous study can be corrected for publication bias
and/or uncertainty to provide a sample size that will achieve more accurate
statistical power for a planned study, when compared to approaches that use
a sample effect size at face value or rely on sample size only. The bias
and uncertainty adjusted previous study noncentrality parameter is also
returned, which can be transformed to various effect size metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.it(t.observed, n, N, alpha.prior = 0.05, alpha.planned = 0.05,
  assurance = 0.8, power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.it_+3A_t.observed">t.observed</code></td>
<td>
<p>Observed <code class="reqn">t</code>-value from a previous study used to plan
sample size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.it_+3A_n">n</code></td>
<td>
<p>Per group sample size (if equal) or the two group sample sizes of
the previous study (enter either a single number or a vector of length 2)</p>
</td></tr>
<tr><td><code id="ss.power.it_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study, assumed equal across groups
if specified</p>
</td></tr>
<tr><td><code id="ss.power.it_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.it_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha-level (<code class="reqn">\alpha</code>) assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.it_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.it_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.it_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentrality
parameter necessary for sample size planning (0 &lt; step &lt; .01) (users
should not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.it</code> uses the observed
<code class="reqn">t</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary per-group sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. In the two-group case, this formula reduces to the density of
a truncated noncentral <code class="reqn">t</code>-distribution.(See Taylor &amp; Muller, 1996,
Equation 2.1. and Anderson &amp; Maxwell, 2017, for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not corrects for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>
<p><code>ss.power.it</code> assumes that the planned study will have equal n.
Unequal n in the previous study is handled in the following way for the
independent-t. If the user enters an odd value for N, no information is
available on the exact group sizes. The function calculates n by dividing N
by 2 and both rounding up and rounding down the result, thus assuming equal
n. The suggested sample size for the planned study is calculated using both
of these values of n, and the function returns the larger of these two
suggestions, to be conservative. If the user enters a vector for n with two
different values, specific information is available on the exact group
sizes. n is calcualted as the harmonic mean of these two values (a measure
of effective sample size). Again, this is rounded both up and down. The
suggested sample size for the planned study is calculated using both of
these values of n, and the function returns the larger of these two
suggestions, to be conservative. The adjusted noncentrality parameter
that is output is the lower of the two possibilities, again, to be
conservative. When the individual group sizes of an unequal-n previous study
are known, we highly encourage entering these explicitly, especially if the
sample sizes are quite discrepant, as this allows for the greatest precision
in estimating an appropriate planned study n.
</p>


<h3>Value</h3>

<p>Suggested per-group sample size for planned study
Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.it(t.observed=3, n=20, alpha.prior=.05, alpha.planned=.05,
assurance=.80, power=.80, step=.001)

</code></pre>

<hr>
<h2 id='ss.power.reg.all'>Necessary sample size to reach desired power for a test of model R2 in a
multiple regression using an uncertainty and publication bias correction
procedure</h2><span id='topic+ss.power.reg.all'></span>

<h3>Description</h3>

<p><code>ss.power.reg.all</code> returns the necessary total sample size
to achieve a desired level of statistical power for a test of model R2
in a planned study using multiple regression, based on information
obtained from a previous study.The effect from the previous study
can be corrected for publication bias and/or uncertainty to provide
a sample size that will achieve more accurate statistical power for a
planned study, when compared to approaches that use a sample effect size at
face value or rely on sample size only. The bias and uncertainty adjusted
previous study noncentrality parameter is also returned, which can be
transformed to various effect size metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.reg.all(F.observed, N, p, alpha.prior = 0.05,
  alpha.planned = 0.05, assurance = 0.8, power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.reg.all_+3A_f.observed">F.observed</code></td>
<td>
<p>Observed <code class="reqn">F</code>-value from a previous study used to plan
sample size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.reg.all_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study</p>
</td></tr>
<tr><td><code id="ss.power.reg.all_+3A_p">p</code></td>
<td>
<p>Number of predictors; be sure to include any product terms or
polynomials that are in the model</p>
</td></tr>
<tr><td><code id="ss.power.reg.all_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.reg.all_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha-level (<code class="reqn">\alpha</code>) assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.reg.all_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.reg.all_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.reg.all_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentrality
parameter necessary for sample size planning (0 &lt; step &lt; .01) (users should
not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.reg.all</code> uses the observed
<code class="reqn">F</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary total sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. In the single predictor case, this formula reduces to the density
of a truncated noncentral <code class="reqn">t</code>-distribution.(See Taylor &amp; Muller, 1996,
Equation 2.1. and Anderson &amp; Maxwell, 2017, for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not correct for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>


<h3>Value</h3>

<p>Suggested total sample size for planned study
</p>
<p>Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.reg.all(F.observed=5, N=150, p=4, alpha.prior=.05, alpha.planned=.05,
assurance=.80, power=.80, step=.001)

</code></pre>

<hr>
<h2 id='ss.power.reg.joint'>Necessary sample size to reach desired power for a test of multiple predictors
in a multiple regression using an uncertainty and publication bias correction
procedure</h2><span id='topic+ss.power.reg.joint'></span>

<h3>Description</h3>

<p><code>ss.power.reg.joint</code> returns the necessary total sample size
to achieve a desired level of statistical power for a test of multiple
predictors in a planned study using multiple regression, based on information
obtained from a previous study.The effect from the previous study
can be corrected for publication bias and/or uncertainty to provide
a sample size that will achieve more accurate statistical power for a
planned study, when compared to approaches that use a sample effect size at
face value or rely on sample size only. The bias and uncertainty adjusted
previous study noncentrality parameter is also returned, which can be
transformed to various effect size metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.reg.joint(F.observed, N, p, p.joint, alpha.prior = 0.05,
  alpha.planned = 0.05, assurance = 0.8, power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.reg.joint_+3A_f.observed">F.observed</code></td>
<td>
<p>Observed <code class="reqn">F</code>-value from a previous study used to plan
sample size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.reg.joint_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study</p>
</td></tr>
<tr><td><code id="ss.power.reg.joint_+3A_p">p</code></td>
<td>
<p>Number of predictors; be sure to include any product terms or
polynomials that are in the model</p>
</td></tr>
<tr><td><code id="ss.power.reg.joint_+3A_p.joint">p.joint</code></td>
<td>
<p>Number of predictors tested jointly for significance</p>
</td></tr>
<tr><td><code id="ss.power.reg.joint_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.reg.joint_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha-level (<code class="reqn">\alpha</code>) assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.reg.joint_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.reg.joint_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.reg.joint_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentrality
parameter necessary for sample size planning (0 &lt; step &lt; .01) (users should
not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.reg.joint</code> uses the observed
<code class="reqn">F</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary total sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. In the single predictor case, this formula reduces to the density
of a truncated noncentral <code class="reqn">t</code>-distribution.(See Taylor &amp; Muller, 1996,
Equation 2.1. and Anderson &amp; Maxwell, 2017, for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not correct for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>


<h3>Value</h3>

<p>Suggested total sample size for planned study
</p>
<p>Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.reg.joint(F.observed=5, N=150, p=4, p.joint=2, alpha.prior=.05,
alpha.planned=.05, assurance=.80, power=.80, step=.001)

</code></pre>

<hr>
<h2 id='ss.power.reg1'>Necessary sample size to reach desired power for a single coefficient in a
multiple regression using an uncertainty and publication bias correction
procedure</h2><span id='topic+ss.power.reg1'></span>

<h3>Description</h3>

<p><code>ss.power.reg1</code> returns the necessary total sample size
to achieve a desired level of statistical power for a single regression
coefficient in a planned study using multiple regression, based on
information obtained from a previous study.The effect from the previous
study can be corrected for publication bias and/or uncertainty to provide
a sample size that will achieve more accurate statistical power for a
planned study, when compared to approaches that use a sample effect size at
face value or rely on sample size only. The bias and uncertainty adjusted
previous study noncentrality parameter is also returned, which can be
transformed to various effect size metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.reg1(t.observed, N, p, alpha.prior = 0.05,
  alpha.planned = 0.05, assurance = 0.8, power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.reg1_+3A_t.observed">t.observed</code></td>
<td>
<p>Observed <code class="reqn">t</code>-value from a previous study used to plan
sample size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.reg1_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study</p>
</td></tr>
<tr><td><code id="ss.power.reg1_+3A_p">p</code></td>
<td>
<p>Number of predictors; be sure to include any product terms or
polynomials that are in the model</p>
</td></tr>
<tr><td><code id="ss.power.reg1_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.reg1_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha-level (<code class="reqn">\alpha</code>) assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.reg1_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.reg1_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.reg1_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentrality
parameter necessary for sample size planning (0 &lt; step &lt; .01) (users should
not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.reg1</code> uses the observed
<code class="reqn">t</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary total sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. In the single predictor case, this formula reduces to the density
of a truncated noncentral <code class="reqn">t</code>-distribution.(See Taylor &amp; Muller, 1996,
Equation 2.1. and Anderson &amp; Maxwell, 2017, for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not correct for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>


<h3>Value</h3>

<p>Suggested total sample size for planned study
</p>
<p>Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.reg1(t.observed=3, N=150, p=3, alpha.prior=.05, alpha.planned=.05,
assurance=.80, power=.80, step=.001)

</code></pre>

<hr>
<h2 id='ss.power.spa'>Necessary sample size to reach desired power for two-factor split-plot
(mixed) ANOVA using an uncertainty and publication bias correction procedure</h2><span id='topic+ss.power.spa'></span>

<h3>Description</h3>

<p><code>ss.power.spa</code> returns the necessary per-group sample size
to achieve a desired level of statistical power for a planned study testing
an omnibus effect using a two-factor split-plot (mixed) ANOVA, based on
information obtained from a previous study. The effect from the previous
study can be corrected for publication bias and/or uncertainty to provide a
sample size that will achieve more accurate statistical power for a planned
study, when compared to approaches that use a sample effect size at face
value or rely on sample size only. The bias and uncertainty adjusted previous
study noncentrality parameter is also returned, which can be transformed to
various effect size metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.spa(F.observed, N, levels.between, levels.within,
  effect = c("between", "within", "interaction"), alpha.prior = 0.05,
  alpha.planned = 0.05, assurance = 0.8, power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.spa_+3A_f.observed">F.observed</code></td>
<td>
<p>Observed F-value from a previous study used to plan sample
size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.spa_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study</p>
</td></tr>
<tr><td><code id="ss.power.spa_+3A_levels.between">levels.between</code></td>
<td>
<p>Number of levels for the between-subjects factor</p>
</td></tr>
<tr><td><code id="ss.power.spa_+3A_levels.within">levels.within</code></td>
<td>
<p>Number of levels for the within-subjects factor</p>
</td></tr>
<tr><td><code id="ss.power.spa_+3A_effect">effect</code></td>
<td>
<p>Effect most of interest to the planned study: between main
effect (between), within main effect (within), interaction</p>
</td></tr>
<tr><td><code id="ss.power.spa_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.spa_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha level assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.spa_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.spa_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.spa_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentral
parameters necessary for sample size planning (0 &lt; step &lt; .01) (users
should not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.spa</code> uses the observed
<code class="reqn">F</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary per-group sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. Thus, the ratio of the numerator and the denominator is a
truncated noncentral F distribution. (See Taylor &amp; Muller, 1996, Equation
2.1. and Anderson &amp; Maxwell, 2017, for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not correct for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>
<p><code>ss.power.spa</code> assumes that the planned study will have equal n.
Unequal n in the previous study is handled in the following way for split
plot designs. If the user enters an N not equally divisible by the number
of between-subjects cells, the function calculates n by dividing N by the
number of cells and both rounding up and rounding down the result,
effectively assuming equal n. The suggested sample size for the planned
study is calculated using both of these values of n, and the function
returns the larger of these two suggestions, to be conservative. The
adjusted noncentrality parameter that is output is the lower of the two
possibilities, again, to be conservative. Although equal-n previous
studies are preferable, this approach will work well as long as the cell
sizes are only slightly discrepant.
</p>
<p><code>ss.power.spa</code> assumes sphericity for the within-subjects effects.
</p>


<h3>Value</h3>

<p>Suggested per-group sample size for planned study
Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.spa(F.observed=5, N=60, levels.between=2, levels.within=3,
effect="within", alpha.prior=.05, alpha.planned=.05, assurance=.80,
power=.80, step=.001)

</code></pre>

<hr>
<h2 id='ss.power.spa.general'>Necessary sample size to reach desired power for a split-plot (mixed) ANOVA
with any number of factors using an uncertainty and publication bias
correction procedure</h2><span id='topic+ss.power.spa.general'></span>

<h3>Description</h3>

<p><code>ss.power.spa.general</code> returns the necessary per-group
sample size to achieve a desired level of statistical power for a planned
study testing any type of effect (omnibus, contrast) using a split-plot
(mixed) ANOVA with any number of factors, based on information obtained
from a previous study. The effect from the previous study can be corrected
for publication bias and/or uncertainty to provide a sample size that will
achieve more accurate statistical power for a planned study, when compared
to approaches that use a sample effect size at face value or rely on sample
size only. The bias and uncertainty adjusted previous study noncentrality
parameter is also returned, which can be transformed to various effect
size metrics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.spa.general(F.observed, N, df.numerator, num.groups,
  effect = c("between.only", "within.only", "between.within"),
  df.num.within, alpha.prior = 0.05, alpha.planned = 0.05,
  assurance = 0.8, power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.spa.general_+3A_f.observed">F.observed</code></td>
<td>
<p>Observed F-value from a previous study used to plan sample
size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.spa.general_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study</p>
</td></tr>
<tr><td><code id="ss.power.spa.general_+3A_df.numerator">df.numerator</code></td>
<td>
<p>Numerator degrees of freedom for the effect of interest</p>
</td></tr>
<tr><td><code id="ss.power.spa.general_+3A_num.groups">num.groups</code></td>
<td>
<p>Number of distinct groups (product of the number of levels
of between-subjects factors)</p>
</td></tr>
<tr><td><code id="ss.power.spa.general_+3A_effect">effect</code></td>
<td>
<p>Effect of interest: involves only between-subjects effects
(<code>between.only</code>), involves only within-subjects effects
(<code>within.only</code>), involves both between and within effects
(<code>between.within</code>)</p>
</td></tr>
<tr><td><code id="ss.power.spa.general_+3A_df.num.within">df.num.within</code></td>
<td>
<p>Numerator degrees of freedom only for the within
subjects components of the effect of interest. Only needed when effect =
<code>between.within</code>.</p>
</td></tr>
<tr><td><code id="ss.power.spa.general_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.spa.general_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha-level (<code class="reqn">\alpha</code>) assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.spa.general_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.spa.general_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.spa.general_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentrality
parameter necessary for sample size planning (0 &lt; step &lt; .01) (users should
not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.spa.general</code> uses the observed
<code class="reqn">F</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary per-group sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. Thus, the ratio of the numerator and the denominator is a
truncated noncentral F distribution. (See Taylor &amp; Muller, 1996, Equation
2.1. and Anderson &amp; Maxwell, 2017, for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not correct for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>
<p><code>ss.power.spa.general</code> assumes that the planned study will have equal
n. Unequal n in the previous study is handled in the following way for
split plot designs. If the user enters an N not equally divisible by the
number of between-subjects cells, the function calculates n by dividing N
by the number of cells and both rounding up and rounding down the result,
effectively assuming equal n. The suggested sample size for the planned
study is calculated using both of these values of n, and the function
returns the larger of these two suggestions, to be conservative. The
adjusted noncentrality parameter that is output is the lower of the two
possibilities, again, to be conservative. Although equal-n previous studies
are preferable, this approach will work well as long as the cell sizes are
only slightly discrepant.
</p>
<p><code>ss.power.spa.general</code> assumes sphericity for the within-subjects
effects.
</p>


<h3>Value</h3>

<p>Suggested per-group sample size for planned study
Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.spa.general(F.observed=5, N=90,  df.numerator=2, num.groups=3,
effect="between.only", df.num.within=3, alpha.prior=.05, alpha.planned=.05,
assurance=.80, power=.80, step=.001)

</code></pre>

<hr>
<h2 id='ss.power.wa'>Necessary sample size to reach desired power for a one or two-way
within-subjects ANOVA using an uncertainty and publication bias correction
procedure</h2><span id='topic+ss.power.wa'></span>

<h3>Description</h3>

<p><code>ss.power.wa</code> returns the necessary per-group sample size
to achieve a desired level of statistical power for a planned study testing
an omnibus effect using a one or two-way fully within-subjects ANOVA, based
on information obtained from a previous study. The effect from the previous
study can be corrected for publication bias and/or uncertainty to provide a
sample size that will achieve more accurate statistical power for a planned
study, when compared to approaches that use a sample effect size at face
value or rely on sample size only. The bias and uncertainty adjusted previous
study noncentrality parameter is also returned, which can be transformed to
various effect size metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.wa(F.observed, N, levels.A, levels.B = NULL,
  effect = c("factor.A", "factor.B", "interaction"),
  alpha.prior = 0.05, alpha.planned = 0.05, assurance = 0.8,
  power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.wa_+3A_f.observed">F.observed</code></td>
<td>
<p>Observed F-value from a previous study used to plan sample
size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.wa_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study</p>
</td></tr>
<tr><td><code id="ss.power.wa_+3A_levels.a">levels.A</code></td>
<td>
<p>Number of levels for factor A</p>
</td></tr>
<tr><td><code id="ss.power.wa_+3A_levels.b">levels.B</code></td>
<td>
<p>Number of levels for factor B, which is NULL if a single
factor design</p>
</td></tr>
<tr><td><code id="ss.power.wa_+3A_effect">effect</code></td>
<td>
<p>Effect most of interest to the planned study: main effect of A
(<code>factor.A</code>), main effect of B (<code>factor.B</code>), interaction
(<code>interaction</code>)</p>
</td></tr>
<tr><td><code id="ss.power.wa_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.wa_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha-level (<code class="reqn">\alpha</code>) assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.wa_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.wa_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.wa_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentrality
parameter necessary for sample size planning (0 &lt; step &lt; .01) (users should
not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.wa</code> uses the observed
<code class="reqn">F</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary per-group sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. Thus, the ratio of the numerator and the denominator is a
truncated noncentral F distribution. (See Taylor &amp; Muller, 1996, Equation
2.1. and Anderson &amp; Maxwell, 2017, for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not correct for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>
<p><code>ss.power.wa</code> assumes sphericity for the within-subjects effects.
</p>


<h3>Value</h3>

<p>Suggested per-group sample size for planned study
Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.wa(F.observed=5, N=60, levels.A=2, levels.B=3,  effect="factor.B",
alpha.prior=.05, alpha.planned=.05, assurance=.80, power=.80, step=.001)

</code></pre>

<hr>
<h2 id='ss.power.wa.general'>Necessary sample size to reach desired power for a within-subjects ANOVA with
any number of factors using an uncertainty and publication bias correction
procedure</h2><span id='topic+ss.power.wa.general'></span>

<h3>Description</h3>

<p><code>ss.power.wa.general</code> returns the necessary per-group
sample size to achieve a desired level of statistical power for a planned
study testing any type of effect (omnibus, contrast) using a fully
within-subjects ANOVA with any number of factors, based on information
obtained from a previous study. The effect from the previous study can be
corrected for publication bias and/or uncertainty to provide a sample size
that will achieve more accurate statistical power for a planned study, when
compared to approaches that use a sample effect size at face value or rely
on sample size only. The bias and uncertainty adjusted previous study
noncentrality parameter is also returned, which can be transformed to
various effect size metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss.power.wa.general(F.observed, N, df.numerator, alpha.prior = 0.05,
  alpha.planned = 0.05, assurance = 0.8, power = 0.8, step = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ss.power.wa.general_+3A_f.observed">F.observed</code></td>
<td>
<p>Observed F-value from a previous study used to plan sample
size for a planned study</p>
</td></tr>
<tr><td><code id="ss.power.wa.general_+3A_n">N</code></td>
<td>
<p>Total sample size of the previous study</p>
</td></tr>
<tr><td><code id="ss.power.wa.general_+3A_df.numerator">df.numerator</code></td>
<td>
<p>Numerator degrees of freedom for the effect of interest</p>
</td></tr>
<tr><td><code id="ss.power.wa.general_+3A_alpha.prior">alpha.prior</code></td>
<td>
<p>Alpha-level <code class="reqn">\alpha</code> for the previous study or the
assumed statistical significance necessary for publishing in the field; to
assume no publication bias, a value of 1 can be entered</p>
</td></tr>
<tr><td><code id="ss.power.wa.general_+3A_alpha.planned">alpha.planned</code></td>
<td>
<p>Alpha-level (<code class="reqn">\alpha</code>) assumed for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.wa.general_+3A_assurance">assurance</code></td>
<td>
<p>Desired level of assurance, or the long run proportion of
times that the planned study power will reach or surpass desired level
(assurance &gt; .5 corrects for uncertainty; assurance &lt; .5 not recommended)</p>
</td></tr>
<tr><td><code id="ss.power.wa.general_+3A_power">power</code></td>
<td>
<p>Desired level of statistical power for the planned study</p>
</td></tr>
<tr><td><code id="ss.power.wa.general_+3A_step">step</code></td>
<td>
<p>Value used in the iterative scheme to determine the noncentrality
parameter necessary for sample size planning (0 &lt; step &lt; .01) (users should
not generally need to change this value; smaller values lead to more
accurate sample size planning results, but unnecessarily small values will
add unnecessary computational time)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Researchers often use the sample effect size from a prior study as
an estimate of the likely size of an expected future effect in sample size
planning. However, sample effect size estimates should not usually be used
at face value to plan sample size, due to both publication bias and
uncertainty.
</p>
<p>The approach implemented in <code>ss.power.wa.general</code> uses the observed
<code class="reqn">F</code>-value and sample size from a previous study to correct the
noncentrality parameter associated with the effect of interest for
publication bias and/or uncertainty. This new estimated noncentrality
parameter is then used to calculate the necessary per-group sample size to
achieve the desired level of power in the planned study.
</p>
<p>The approach uses a likelihood function of a truncated non-central F
distribution, where the truncation occurs due to small effect sizes being
unobserved due to publication bias. The numerator of the likelihood
function is simply the density of a noncentral F distribution. The
denominator is the power of the test, which serves to truncate the
distribution. Thus, the ratio of the numerator and the denominator is a
truncated noncentral F distribution. (See Taylor &amp; Muller, 1996, Equation
2.1. and Anderson &amp; Maxwell, 2017 for more details.)
</p>
<p>Assurance is the proportion of times that power will be at or above the
desired level, if the experiment were to be reproduced many times. For
example, assurance = .5 means that power will be above the desired level
half of the time, but below the desired level the other half of the time.
Selecting assurance = .5 (selecting the noncentrality parameter at the 50th
percentile of the likelihood distribution) results in a median-unbiased
estimate of the population noncentrality parameter and does not correct for
uncertainty. In order to correct for uncertainty, assurance &gt; .5
can be selected, which corresponds to selecting the noncentrality parameter
associated with the (1 - assurance) quantile of the likelihood
distribution.
</p>
<p>If the previous study of interest has not been subjected to publication
bias (e.g., a pilot study), <code>alpha.prior</code> can be set to 1 to indicate
no publication bias. Alternative <code class="reqn">\alpha</code>-levels can also be
accommodated to represent differing amounts of publication bias. For
example, setting <code>alpha.prior</code>=.20 would reflect less severe
publication bias than the default of .05. In essence, setting
<code>alpha.prior</code> at .20 assumes that studies with <code class="reqn">p</code>-values less
than .20 are published, whereas those with larger <code class="reqn">p</code>-values are not.
</p>
<p>In some cases, the corrected noncentrality parameter for a given level of
assurance will be estimated to be zero. This is an indication that, at the
desired level of assurance, the previous study's effect cannot be
accurately estimated due to high levels of uncertainty and bias. When this
happens, subsequent sample size planning is not possible with the chosen
specifications. Two alternatives are recommended. First, users can select a
lower value of assurance (e.g. .8 instead of .95). Second, users can reduce
the influence of publciation bias by setting <code>alpha.prior</code> at a value
greater than .05. It is possible to correct for uncertainty only by setting
<code>alpha.prior</code>=1 and choosing the desired level of assurance. We
encourage users to make the adjustments as minimal as possible.
</p>
<p><code>ss.power.wa.general</code> assumes sphericity for the within-subjects
effects.
</p>


<h3>Value</h3>

<p>Suggested per-group sample size for planned study
Publication bias and uncertainty- adjusted prior study noncentrality parameter
</p>


<h3>Author(s)</h3>

<p>Samantha F. Anderson <a href="mailto:samantha.f.anderson@asu.edu">samantha.f.anderson@asu.edu</a>,
Ken Kelley <a href="mailto:kkelley@nd.edu">kkelley@nd.edu</a>
</p>


<h3>References</h3>

<p>Anderson, S. F., &amp; Maxwell, S. E. (2017).
Addressing the 'replication crisis': Using original studies to design
replication studies with appropriate statistical power. <em>Multivariate
Behavioral Research, 52,</em> 305-322.
</p>
<p>Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample size
planning for more accurate statistical power: A method correcting sample
effect sizes for uncertainty and publication bias. <em>Psychological
Science, 28,</em> 1547-1562.
</p>
<p>Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and
sample size calculation due to estimating noncentrality.
<em>Communications in Statistics: Theory and Methods, 25,</em> 1595-1610.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ss.power.wa.general(F.observed=6.5, N=80,  df.numerator=1,
alpha.prior=.05, alpha.planned=.05, assurance=.50, power=.80, step=.001)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
