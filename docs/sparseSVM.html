<!DOCTYPE html><html><head><title>Help for package sparseSVM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparseSVM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#sparseSVM-package'><p>Solution Paths for Sparse High-Dimensional Support Vector Machine with Lasso or Elastic-Net Regularization</p></a></li>
<li><a href='#cv.sparseSVM'>
<p>Cross validation for sparseSVM</p></a></li>
<li><a href='#plot.cv.sparseSVM'><p>Plot the cross-validation curve for a &quot;cv.sparseSVM&quot; object</p></a></li>
<li><a href='#plot.sparseSVM'><p>Plot coefficients from a &quot;sparseSVM&quot; object</p></a></li>
<li><a href='#predict.cv.sparseSVM'><p>Model predictions based on &quot;cv.sparseSVM&quot; object.</p></a></li>
<li><a href='#predict.sparseSVM'><p>Model predictions based on &quot;sparseSVM&quot; object.</p></a></li>
<li><a href='#sparseSVM'><p>Fit sparse linear SVM with lasso or elasti-net regularization</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Solution Paths of Sparse High-Dimensional Support Vector Machine
with Lasso or Elastic-Net Regularization</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1-6</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-06-01</td>
</tr>
<tr>
<td>Author:</td>
<td>Congrui Yi and Yaohui Zeng</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Congrui Yi &lt;eric.ycr@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Fast algorithm for fitting solution paths of sparse SVM models with lasso or elastic-net regularization.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Imports:</td>
<td>parallel</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-06-02 06:32:28 UTC; cyi</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-06-02 12:27:22 UTC</td>
</tr>
</table>
<hr>
<h2 id='sparseSVM-package'>Solution Paths for Sparse High-Dimensional Support Vector Machine with Lasso or Elastic-Net Regularization</h2><span id='topic+sparseSVM-package'></span>

<h3>Description</h3>

<p>Fast algorithm for fitting solution paths for sparse SVM regularized by lasso or elastic-net that generate sparse solutions.</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> sparseSVM</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.1-6</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2018-06-01</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-3</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Accepts <code>X,y</code> data for binary classification and
produces the solution path over a grid of values of the regularization parameter <code>lambda</code>. Also provides functions for plotting, prediction and parallelized cross-validation.
</p>


<h3>Author(s)</h3>

<p>Congrui Yi and Yaohui Zeng <br />
Maintainer: Congrui Yi &lt;eric.ycr@gmail.com&gt;
</p>


<h3>References</h3>

<p>Yi, C. and Huang, J. (2017) 
<em>Semismooth Newton Coordinate Descent Algorithm for 
Elastic-Net Penalized Huber Loss Regression and Quantile Regression</em>,
<a href="https://www.tandfonline.com/doi/abs/10.1080/10618600.2016.1256816?journalCode=ucgs20">https://www.tandfonline.com/doi/abs/10.1080/10618600.2016.1256816?journalCode=ucgs20</a> <br />
<em>Journal of Computational and Graphical Statistics</em> <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(rnorm(1000*100), 1000, 100)
b = 3
w = 5*rnorm(10)
eps = rnorm(1000)
y = sign(b + drop(X[,1:10] %*% w + eps))

fit = sparseSVM(X, y)
coef(fit, 0.05)
predict(fit, X[1:5,], lambda = c(0.2, 0.1))
plot(fit)

cv.fit &lt;- cv.sparseSVM(X, y, ncores = 2, seed = 1234)
predict(cv.fit, X)
coef(cv.fit)
plot(cv.fit)
</code></pre>

<hr>
<h2 id='cv.sparseSVM'>
Cross validation for sparseSVM
</h2><span id='topic+cv.sparseSVM'></span>

<h3>Description</h3>

<p>Perform k-fold cross validation for sparse linear SVM regularized by lasso or elastic-net over a sequence of lambda values and find an optimal lambda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.sparseSVM(X, y, ..., ncores = 1, eval.metric = c("me"),
             nfolds = 10, fold.id, seed, trace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.sparseSVM_+3A_x">X</code></td>
<td>
<p>Input matrix.</p>
</td></tr>
<tr><td><code id="cv.sparseSVM_+3A_y">y</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code id="cv.sparseSVM_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>sparseSVM</code>.</p>
</td></tr>
<tr><td><code id="cv.sparseSVM_+3A_ncores">ncores</code></td>
<td>
<p><code>cv.sparseSVM</code> can be run in parallel across a
cluster using the <code>parallel</code> package. If <code>ncores &gt; 1</code>,a cluster is 
created to run <code>cv.sparseSVM</code> in parallel. The code is run in series if 
<code>ncores = 1</code> (the default). An error occurs if <code>ncores</code> is larger than the
total number of available cores.</p>
</td></tr>
<tr><td><code id="cv.sparseSVM_+3A_eval.metric">eval.metric</code></td>
<td>
<p>The metric used to choose optimial <code>lambda</code>. Current version only
supports &quot;me&quot;: misclassification error.</p>
</td></tr>
<tr><td><code id="cv.sparseSVM_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of cross-validation folds.  Default is 10.</p>
</td></tr>
<tr><td><code id="cv.sparseSVM_+3A_seed">seed</code></td>
<td>
<p>The seed of the random number generator in order to obtain 
reproducible results.</p>
</td></tr>
<tr><td><code id="cv.sparseSVM_+3A_fold.id">fold.id</code></td>
<td>
<p>Which fold each observation belongs to.  By default the
observations are randomly assigned by <code>cv.sparseSVM</code>.</p>
</td></tr>
<tr><td><code id="cv.sparseSVM_+3A_trace">trace</code></td>
<td>
<p>If set to TRUE, cv.sparseSVM will inform the user of its
progress by announcing the beginning of each CV fold.  Default is
FALSE. (No trace output when running in parallel even if <code>trace=TRUE</code>.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function randomly partitions the data in nfolds. It calls <code>sparseSVM</code> nfolds+1 times, the first
to obtain the lambda sequence, and the remainder to fit with each of the folds left out once for
validation. The cross-validation error is the average of validation errors for the nfolds fits.
</p>
<p>Note by default, the cross-validation fold assignments are balanced across the two classes, so that each fold has the same class proportion (or as close to the same proportion as it is possible to achieve if cases do not divide evenly).
</p>


<h3>Value</h3>

<p>The function returns an object of S3 class &quot;cv.sparseSVM&quot;, which is a list containing:
</p>
<table>
<tr><td><code>cve</code></td>
<td>
<p>The validation error for each value of <code>lambda</code>, averaged across the cross-validation folds.</p>
</td></tr>
<tr><td><code>cvse</code></td>
<td>
<p>The estimated standard error associated with each value of <code>cve</code>.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The values of lambda used in the cross-validation fits.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>The fitted <code>sparseSVM</code> object for the whole data.</p>
</td></tr>
<tr><td><code>min</code></td>
<td>
<p>The index of <code>lambda</code> corresponding to <code>lambda.min</code>.</p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>
<p>The value of <code>lambda</code> with the minimum cross-validation error in terms of <code>eval.metric</code>.</p>
</td></tr>
<tr><td><code>eval.metric</code></td>
<td>
<p>The metric used in selecting optimal <code>lambda</code>.</p>
</td></tr>
<tr><td><code>fold.id</code></td>
<td>
<p>The same as above.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Congrui Yi and Yaohui Zeng <br />
Maintainer: Congrui Yi &lt;eric.ycr@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sparseSVM">sparseSVM</a></code>, <code><a href="#topic+predict.cv.sparseSVM">predict.cv.sparseSVM</a></code>, <code><a href="#topic+plot.cv.sparseSVM">plot.cv.sparseSVM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(rnorm(1000*100), 1000, 100)
b = 3
w = 5*rnorm(10)
eps = rnorm(1000)
y = sign(b + drop(X[,1:10] %*% w + eps))

cv.fit1 &lt;- cv.sparseSVM(X, y, nfolds = 5, ncores = 2, seed = 1234)
cv.fit2 &lt;- cv.sparseSVM(X, y, nfolds = 5, seed = 1234)
stopifnot(all.equal(cv.fit1, cv.fit2))
</code></pre>

<hr>
<h2 id='plot.cv.sparseSVM'>Plot the cross-validation curve for a &quot;cv.sparseSVM&quot; object</h2><span id='topic+plot.cv.sparseSVM'></span>

<h3>Description</h3>

<p>Plot the cross-validation curve for a &quot;cv.sparseSVM&quot; object against the 
<code>lambda</code> values used, along with standard error bars.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.sparseSVM'
plot(x, log.l = TRUE, nvars = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cv.sparseSVM_+3A_x">x</code></td>
<td>
<p>A <code>"cv.sparseSVM"</code> object.</p>
</td></tr>
<tr><td><code id="plot.cv.sparseSVM_+3A_log.l">log.l</code></td>
<td>
<p>Should <code>log(lambda)</code> be used instead of <code>lambda</code> for the X-axis?  Default is TRUE.</p>
</td></tr>
<tr><td><code id="plot.cv.sparseSVM_+3A_nvars">nvars</code></td>
<td>
<p>If <code>TRUE</code> (the default), places an axis on top of the plot denoting 
the number of variables with nonzero coefficients at each <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="plot.cv.sparseSVM_+3A_...">...</code></td>
<td>
<p>Other graphical parameters to <code>plot</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Produces a plot of mean cv errors at each <code>lambda</code> along with upper and lower standard error bars.</p>


<h3>Author(s)</h3>

<p>Congrui Yi and Yaohui Zeng <br />
Maintainer: Congrui Yi &lt;eric.ycr@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sparseSVM">sparseSVM</a></code>, <code><a href="#topic+cv.sparseSVM">cv.sparseSVM</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(rnorm(1000*100), 1000, 100)
b = 3
w = 5*rnorm(10)
eps = rnorm(1000)
y = sign(b + drop(X[,1:10] %*% w + eps))

cv.fit &lt;- cv.sparseSVM(X, y, ncores = 2, seed = 1234)
plot(cv.fit)
plot(cv.fit, log.l = FALSE)
</code></pre>

<hr>
<h2 id='plot.sparseSVM'>Plot coefficients from a &quot;sparseSVM&quot; object</h2><span id='topic+plot.sparseSVM'></span>

<h3>Description</h3>

<p>Produce a plot of the coefficient paths for a fitted
<code>"sparseSVM"</code> object.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sparseSVM'
plot(x, xvar = c("lambda", "norm"), log.l = TRUE, nvars = TRUE, 
    alpha = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.sparseSVM_+3A_x">x</code></td>
<td>
<p>A <code>sparseSVM</code> object.</p>
</td></tr>
<tr><td><code id="plot.sparseSVM_+3A_xvar">xvar</code></td>
<td>
<p>What is on the X-axis. <code>"lambda"</code> plots against the lambda sequence, 
<code>"norm"</code> against the L1-norm of the coefficients. Default is <code>"lambda"</code>.</p>
</td></tr>
<tr><td><code id="plot.sparseSVM_+3A_log.l">log.l</code></td>
<td>
<p>Should <code>log(lambda)</code> be used instead of <code>lambda</code> when <code>xvar = "lambda"</code>?  Default is TRUE. It has no effect on <code>"norm"</code>.</p>
</td></tr>
<tr><td><code id="plot.sparseSVM_+3A_nvars">nvars</code></td>
<td>
<p>If <code>TRUE</code> (the default), places an axis on top of the plot denoting the 
number of variables with nonzero coefficients at each <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="plot.sparseSVM_+3A_alpha">alpha</code></td>
<td>
<p>A value between 0 and 1 for alpha transparency channel(0 means transparent 
and 1 means opaque), helpful when the number of variables is large.</p>
</td></tr>
<tr><td><code id="plot.sparseSVM_+3A_...">...</code></td>
<td>
<p>Other graphical parameters to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Congrui Yi and Yaohui Zeng <br />
Maintainer: Congrui Yi &lt;eric.ycr@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sparseSVM">sparseSVM</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(rnorm(1000*100), 1000, 100)
b = 3
w = 5*rnorm(10)
eps = rnorm(1000)
y = sign(b + drop(X[,1:10] %*% w + eps))

fit = sparseSVM(X, y)
par(mfrow = c(2,2))
plot(fit)
plot(fit, nvars = FALSE, alpha = 0.5)
plot(fit, log.l = FALSE)
plot(fit, xvar = "norm")
</code></pre>

<hr>
<h2 id='predict.cv.sparseSVM'>Model predictions based on &quot;cv.sparseSVM&quot; object.</h2><span id='topic+predict.cv.sparseSVM'></span><span id='topic+coef.cv.sparseSVM'></span>

<h3>Description</h3>

<p>This function returns fitted values, coefficients and more from a fitted <code>"cv.sparseSVM"</code> object.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.sparseSVM'
predict(object, X, lambda = object$lambda.min, 
        type = c("class","coefficients","nvars"), exact = FALSE, ...)
## S3 method for class 'cv.sparseSVM'
coef(object, lambda = object$lambda.min, exact = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cv.sparseSVM_+3A_object">object</code></td>
<td>
<p>Fitted <code>"cv.sparseSVM"</code> model object.</p>
</td></tr>
<tr><td><code id="predict.cv.sparseSVM_+3A_x">X</code></td>
<td>
<p>Matrix of values at which predictions are to be made. Used only for <code>type = "class"</code>.</p>
</td></tr>
<tr><td><code id="predict.cv.sparseSVM_+3A_lambda">lambda</code></td>
<td>
<p>Values of the regularization parameter <code>lambda</code> at which predictions 
are requested. Default is the one corresponding to the minimum cross-validation error.</p>
</td></tr>
<tr><td><code id="predict.cv.sparseSVM_+3A_type">type</code></td>
<td>
<p>Type of prediction. <code>"class"</code> returns the class labels; 
<code>"coefficients"</code> returns the coefficients; <code>"nvars"</code> returns the number of 
nonzero coefficients at each value of <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="predict.cv.sparseSVM_+3A_exact">exact</code></td>
<td>
<p>If <code>exact=FALSE</code> (default), then the function uses linear interpolation 
to make predictions for values of <code>lambda</code> that do not coincide with those used to 
fit the model. If <code>exact=TRUE</code>, and predictions are requested at values of <code>lambda</code> 
not included in the original fit, the model is refit on a lambda sequence consisting 
<code>object$lambda</code> and the new ones before predictions are made. </p>
</td></tr>
<tr><td><code id="predict.cv.sparseSVM_+3A_...">...</code></td>
<td>
<p>Not used. Other arguments to predict.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on type.</p>


<h3>Author(s)</h3>

<p>Congrui Yi and Yaohui Zeng <br />
Maintainer: Congrui Yi &lt;eric.ycr@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sparseSVM">sparseSVM</a></code>, <code><a href="#topic+cv.sparseSVM">cv.sparseSVM</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(rnorm(1000*100), 1000, 100)
b = 3
w = 5*rnorm(10)
eps = rnorm(1000)
y = sign(b + drop(X[,1:10] %*% w + eps))

cv.fit &lt;- cv.sparseSVM(X, y, ncores = 2, seed = 1234)
predict(cv.fit, X)
predict(cv.fit, type = 'nvars')
predict(cv.fit, type = 'coef')
coef(cv.fit)

</code></pre>

<hr>
<h2 id='predict.sparseSVM'>Model predictions based on &quot;sparseSVM&quot; object.</h2><span id='topic+predict.sparseSVM'></span><span id='topic+coef.sparseSVM'></span>

<h3>Description</h3>

<p>This function returns fitted values, coefficients and more from a fitted <code>"sparseSVM"</code> object.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sparseSVM'
predict(object, X, lambda, type = c("class","coefficients","nvars"), 
    exact = FALSE, ...)
## S3 method for class 'sparseSVM'
coef(object, lambda, exact = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.sparseSVM_+3A_object">object</code></td>
<td>
<p>Fitted <code>"sparseSVM"</code> model object.</p>
</td></tr>
<tr><td><code id="predict.sparseSVM_+3A_x">X</code></td>
<td>
<p>Matrix of values at which predictions are to be made. Used only for <code>type = "class"</code>.</p>
</td></tr>
<tr><td><code id="predict.sparseSVM_+3A_lambda">lambda</code></td>
<td>
<p>Values of the regularization parameter <code>lambda</code> at which predictions 
are requested. Default is the entire sequence used to create the model.</p>
</td></tr>
<tr><td><code id="predict.sparseSVM_+3A_type">type</code></td>
<td>
<p>Type of prediction. <code>"class"</code> returns the class labels; 
<code>"coefficients"</code> returns the coefficients; <code>"nvars"</code> returns the number of 
nonzero coefficients at each value of <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="predict.sparseSVM_+3A_exact">exact</code></td>
<td>
<p>If <code>exact=FALSE</code> (default), then the function uses linear interpolation 
to make predictions for values of <code>lambda</code> that do not coincide with those used to 
fit the model. If <code>exact=TRUE</code>, and predictions are requested at values of <code>lambda</code> 
not included in the original fit, the model is refit on a lambda sequence consisting 
<code>object$lambda</code> and the new ones before predictions are made. </p>
</td></tr>
<tr><td><code id="predict.sparseSVM_+3A_...">...</code></td>
<td>
<p>Not used. Other arguments to predict.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned depends on type.</p>


<h3>Author(s)</h3>

<p>Congrui Yi and Yaohui Zeng <br />
Maintainer: Congrui Yi &lt;eric.ycr@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sparseSVM">sparseSVM</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(rnorm(1000*100), 1000, 100)
b = 3
w = 5*rnorm(10)
eps = rnorm(1000)
y = sign(b + drop(X[,1:10] %*% w + eps))

fit = sparseSVM(X, y)
predict(fit, X[1:5,], lambda = c(0.05, 0.03))
predict(fit, X[1:5,], lambda = 0.05, exact = TRUE)
predict(fit, type = "nvars")
coef(fit, lambda = 0.05)
</code></pre>

<hr>
<h2 id='sparseSVM'>Fit sparse linear SVM with lasso or elasti-net regularization</h2><span id='topic+sparseSVM'></span>

<h3>Description</h3>

<p>Fit solution paths for sparse linear SVM regularized by lasso or elastic-net 
over a grid of values for the regularization parameter lambda.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparseSVM(X, y, alpha = 1, gamma = 0.1, nlambda=100, 
	  lambda.min = ifelse(nrow(X)&gt;ncol(X), 0.01, 0.05), 
          lambda, preprocess = c("standardize", "rescale", "none"),  
          screen = c("ASR", "SR", "none"), max.iter = 1000, eps = 1e-5, 
          dfmax = ncol(X)+1, penalty.factor=rep(1, ncol(X)), message = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparseSVM_+3A_x">X</code></td>
<td>
<p>Input matrix.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_y">y</code></td>
<td>
<p>Output vector. Currently the function only supports binary output and converts 
the output into +1/-1 coding internally.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_alpha">alpha</code></td>
<td>
<p>The elastic-net mixing parameter that controls the relative contribution 
from the lasso and the ridge penalty. It must be a number between 0 and 1. <code>alpha=1</code> 
is the lasso penalty and <code>alpha=0</code> the ridge penalty.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_gamma">gamma</code></td>
<td>
<p>The tuning parameter for huberization smoothing of hinge loss. Default is 0.1.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of lambda values.  Default is 100.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_lambda.min">lambda.min</code></td>
<td>
<p>The smallest value for lambda, as a fraction of lambda.max, the data 
derived entry value. Default is 0.01 if the number of observations is larger than the 
number of variables and 0.05 otherwise.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_lambda">lambda</code></td>
<td>
<p>A user-specified sequence of lambda values. Typical usage is to leave 
blank and have the program automatically compute a <code>lambda</code> sequence based on 
<code>nlambda</code> and <code>lambda.min</code>. Specifying <code>lambda</code> overrides this. This 
argument should be used with care and supplied with a decreasing sequence instead of 
a single value. To get coefficients for a single <code>lambda</code>, use <code>coef</code> or 
<code>predict</code> instead after fitting the solution path with <code>sparseSVM</code>.

</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_preprocess">preprocess</code></td>
<td>
<p>Preprocessing technique to be applied to the input. Either 
&quot;standardize&quot; (default), &quot;rescale&quot; or &quot;none&quot; (see <code>Details</code>). The coefficients 
are always returned on the original scale.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_screen">screen</code></td>
<td>
<p>Screening rule to be applied at each <code>lambda</code> that discards variables 
for speed. Either &quot;ASR&quot; (default), &quot;SR&quot; or &quot;none&quot;. &quot;SR&quot; stands for the strong rule, 
and &quot;ASR&quot; for the adaptive strong rule. Using &quot;ASR&quot; typically requires fewer iterations 
to converge than &quot;SR&quot;, but the computing time are generally close. Note that the option 
&quot;none&quot; is used mainly for debugging, which may lead to much longer computing time.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations. Default is 1000.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_eps">eps</code></td>
<td>
<p>Convergence threshold. The algorithms continue until the maximum change in the
objective after any coefficient update is less than <code>eps</code> times the null deviance. 
Default is <code>1E-7</code>.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_dfmax">dfmax</code></td>
<td>
<p>Upper bound for the number of nonzero coefficients. The algorithm exits and 
returns a partial path if <code>dfmax</code> is reached. Useful for very large dimensions.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_penalty.factor">penalty.factor</code></td>
<td>
<p>A numeric vector of length equal to the number of variables. Each 
component multiplies <code>lambda</code> to allow differential penalization. Can be 0 for 
some variables, in which case the variable is always in the model without penalization. 
Default is 1 for all variables.</p>
</td></tr>
<tr><td><code id="sparseSVM_+3A_message">message</code></td>
<td>
<p>If set to TRUE,  sparseSVM will inform the user of its progress. This argument 
is kept for debugging. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sequence of models indexed by the regularization parameter <code>lambda</code> is fitted
using a semismooth Newton coordinate descent algorithm. The objective function is defined 
to be </p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n} \sum hingeLoss(y_i (x_i' w + b)) + \lambda\textrm{penalty}(w).</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">hingeLoss(t) = max(0, 1-t)</code>
</p>
<p> and the intercept <code>b</code> is unpenalized. 
</p>
<p>The program supports different types of preprocessing techniques. They are applied to 
each column of the input matrix <code>X</code>. Let x be a column of <code>X</code>. For 
<code>preprocess = "standardize"</code>, the formula is 
</p>
<p style="text-align: center;"><code class="reqn">x' = \frac{x-mean(x)}{sd(x)};</code>
</p>

<p>for <code>preprocess = "rescale"</code>, 
</p>
<p style="text-align: center;"><code class="reqn">x' = \frac{x-min(x)}{max(x)-min(x)}.</code>
</p>

<p>The models are fit with preprocessed input, then the coefficients are transformed back
to the original scale via some algebra.
</p>


<h3>Value</h3>

<p>The function returns an object of S3 class <code>"sparseSVM"</code>, which is a list containing:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>The call that produced this object.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>The fitted matrix of coefficients.  The number of rows is equal to the number 
of coefficients, and the number of columns is equal to <code>nlambda</code>. An intercept is included.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>A vector of length <code>nlambda</code> containing the number of iterations until 
convergence at each value of <code>lambda</code>.</p>
</td></tr>
<tr><td><code>saturated</code></td>
<td>
<p>A logical flag for whether the number of nonzero coefficients has reached <code>dfmax</code>.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The sequence of regularization parameter values in the path.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Same as above.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>Same as above.</p>
</td></tr>
<tr><td><code>penalty.factor</code></td>
<td>
<p>Same as above.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>Levels of the output class labels.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Congrui Yi and Yaohui Zeng <br />
Maintainer: Congrui Yi &lt;eric.ycr@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.sparseSVM">plot.sparseSVM</a></code>, <code><a href="#topic+cv.sparseSVM">cv.sparseSVM</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(rnorm(1000*100), 1000, 100)
b = 3
w = 5*rnorm(10)
eps = rnorm(1000)
y = sign(b + drop(X[,1:10] %*% w + eps))

fit = sparseSVM(X, y)
coef(fit, 0.05)
predict(fit, X[1:5,], lambda = c(0.2, 0.1))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
