<!DOCTYPE html><html><head><title>Help for package chatAI4R</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {chatAI4R}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#addCommentCode'><p>Add Comments to R Code</p></a></li>
<li><a href='#addRoxygenDescription'><p>Add Roxygen Description to R Function</p></a></li>
<li><a href='#autocreateFunction4R'><p>autocreateFunction4R (development / experimental)</p></a></li>
<li><a href='#chat4R'><p>Chat4R: Interact with GPT-3.5 (default) using OpenAI API</p></a></li>
<li><a href='#chat4R_history'><p>chat4R_history: Use chat history for interacting with GPT.</p></a></li>
<li><a href='#chatAI4pdf'><p>chatAI4pdf</p></a></li>
<li><a href='#checkErrorDet'><p>Check Error Details</p></a></li>
<li><a href='#checkErrorDet_JP'><p>Check Error Details in Japanese</p></a></li>
<li><a href='#completions4R'><p>completions4R: Generate text using OpenAI completions API (Legacy)</p></a></li>
<li><a href='#conversation4R'><p>Conversation Interface for R</p></a></li>
<li><a href='#convertBullet2Sentence'><p>convertBullet2Sentence</p></a></li>
<li><a href='#convertRscript2Function'><p>convertRscript2Function</p></a></li>
<li><a href='#convertScientificLiterature'><p>convertScientificLiterature</p></a></li>
<li><a href='#createImagePrompt_v1'><p>Create Image Prompt version 1</p></a></li>
<li><a href='#createImagePrompt_v2'><p>Generate Image Prompts version 2</p></a></li>
<li><a href='#createRcode'><p>Create R Code from Clipboard Content and Output into the R Console</p></a></li>
<li><a href='#createRfunction'><p>Create R Function from Selected Text or Clipboard Content and Output into the R Console</p></a></li>
<li><a href='#createSpecifications4R'><p>Create Specifications for R Function</p></a></li>
<li><a href='#designPackage'><p>designPackage</p></a></li>
<li><a href='#discussion_flow_v1'><p>discussion_flow_v1: Interactions and Flow Control Between LLM-based Bots (LLBs)</p></a></li>
<li><a href='#enrichTextContent'><p>Enrich Text Content</p></a></li>
<li><a href='#extractKeywords'><p>extractKeywords</p></a></li>
<li><a href='#ngsub'><p>ngsub</p></a></li>
<li><a href='#OptimizeRcode'><p>Optimize and Complete R Code</p></a></li>
<li><a href='#proofreadEnglishText'><p>Proofread English Text</p></a></li>
<li><a href='#proofreadText'><p>proofreadText</p></a></li>
<li><a href='#RcodeImprovements'><p>Suggest Improvements to the R Code on Your Clipboard</p></a></li>
<li><a href='#removeQuotations'><p>Remove All Types of Quotations from Text</p></a></li>
<li><a href='#revisedText'><p>Revised Scientific Text</p></a></li>
<li><a href='#searchFunction'><p>Search R Functions based on Text</p></a></li>
<li><a href='#slow_print_v2'><p>Slowly Print Text</p></a></li>
<li><a href='#speakInJA'><p>Speak Selected Text in Japanese</p></a></li>
<li><a href='#speakInJA_v2'><p>Speak Clipboard in Japanese</p></a></li>
<li><a href='#summaryWebScrapingText'><p>Text Summary via Web Scraping</p></a></li>
<li><a href='#supportIdeaGeneration'><p>supportIdeaGeneration: Support Idea Generation from Selected Text or Clipboard.</p></a></li>
<li><a href='#textEmbedding'><p>Text Embedding from OpenAI Embeddings API</p></a></li>
<li><a href='#TextSummary'><p>Summarize Long Text</p></a></li>
<li><a href='#TextSummaryAsBullet'><p>Summarize Text into Bullet Points</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Chat-Based Interactive Artificial Intelligence for R</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.10</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-09-12</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Satoshi Kume &lt;satoshi.kume.1984@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The Large Language Model (LLM) represents a groundbreaking advancement 
    in data science and programming, and also allows us to extend the world of R. 
    A seamless interface for integrating the 'OpenAI' Web APIs into R is provided in this package. 
    This package leverages LLM-based AI techniques, enabling efficient knowledge discovery and data analysis 
    (see 'OpenAI' Web APIs details <a href="https://openai.com/blog/openai-api">https://openai.com/blog/openai-api</a>).
    The previous functions such as seamless translation and image generation have been moved 
    to other packages 'deepRstudio' and 'stableDiffusion4R'.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>httr, jsonlite, assertthat, clipr, crayon, rstudioapi, future,
igraph, deepRstudio, pdftools, xml2, rvest</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/Artistic-2.0">Artistic-2.0</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://kumes.github.io/chatAI4R/">https://kumes.github.io/chatAI4R/</a>,
<a href="https://github.com/kumeS/chatAI4R">https://github.com/kumeS/chatAI4R</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/kumeS/chatAI4R/issues">https://github.com/kumeS/chatAI4R/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-12 14:25:27 UTC; sas</td>
</tr>
<tr>
<td>Author:</td>
<td>Satoshi Kume <a href="https://orcid.org/0000-0001-7481-2843"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-12 21:00:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='addCommentCode'>Add Comments to R Code</h2><span id='topic+addCommentCode'></span>

<h3>Description</h3>

<p>This function adds comments to R code without modifying the input R code.
It can either take the selected code from RStudio or read from the clipboard.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addCommentCode(Model = "gpt-4-0613", language = "English", SelectedCode = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addCommentCode_+3A_model">Model</code></td>
<td>
<p>A character string specifying the GPT model to be used. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="addCommentCode_+3A_language">language</code></td>
<td>
<p>A character string specifying the language for the comments. Default is &quot;English&quot;.</p>
</td></tr>
<tr><td><code id="addCommentCode_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>A logical value indicating whether to use the selected code in RStudio. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Add Comments to R Code
</p>


<h3>Value</h3>

<p>A message indicating completion if 'SelectedCode' is TRUE, otherwise the commented code is copied to the clipboard.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
addCommentCode(Model = "gpt-4-0613", language = "English", SelectedCode = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='addRoxygenDescription'>Add Roxygen Description to R Function</h2><span id='topic+addRoxygenDescription'></span>

<h3>Description</h3>

<p>This function adds a Roxygen description to an R function using the GPT-4 model.
It can either take the selected code from RStudio or read from the clipboard.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addRoxygenDescription(
  Model = "gpt-4-0613",
  SelectedCode = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addRoxygenDescription_+3A_model">Model</code></td>
<td>
<p>A character string specifying the GPT model to be used. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="addRoxygenDescription_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>A logical value indicating whether to use the selected code in RStudio. Default is TRUE.</p>
</td></tr>
<tr><td><code id="addRoxygenDescription_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag to indicate whether to display the generated text. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Add Roxygen Description to R Function
</p>


<h3>Value</h3>

<p>A message indicating completion if 'SelectedCode' is TRUE, otherwise the Roxygen-annotated code is copied to the clipboard.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
addRoxygenDescription(Model = "gpt-4-0613", SelectedCode = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='autocreateFunction4R'>autocreateFunction4R (development / experimental)</h2><span id='topic+autocreateFunction4R'></span>

<h3>Description</h3>

<p>This function generates an R function based on a given description,
proposes improvements, and then generates an improved version of the function.
It is expected to use an AI model (possibly GPT-3 or similar) to perform these tasks.
This is an experimental function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autocreateFunction4R(
  Func_description,
  packages = "base",
  max_tokens = 250,
  View = TRUE,
  roxygen = TRUE,
  api_key = Sys.getenv("OPENAI_API_KEY"),
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autocreateFunction4R_+3A_func_description">Func_description</code></td>
<td>
<p>A character string that describes the function to be generated.</p>
</td></tr>
<tr><td><code id="autocreateFunction4R_+3A_packages">packages</code></td>
<td>
<p>A character string that specifies the packages to be used in the function. Default is &quot;base&quot;.</p>
</td></tr>
<tr><td><code id="autocreateFunction4R_+3A_max_tokens">max_tokens</code></td>
<td>
<p>An integer that specifies the maximum number of tokens to be returned by the AI model. Default is 250.</p>
</td></tr>
<tr><td><code id="autocreateFunction4R_+3A_view">View</code></td>
<td>
<p>A logical that indicates whether to view the intermediate steps. Default is TRUE.</p>
</td></tr>
<tr><td><code id="autocreateFunction4R_+3A_roxygen">roxygen</code></td>
<td>
<p>A logical that indicates whether to include roxygen comments in the generated function. Default is TRUE.</p>
</td></tr>
<tr><td><code id="autocreateFunction4R_+3A_api_key">api_key</code></td>
<td>
<p>A character string that represents the API key for the AI model being used. Default is the &quot;OPENAI_API_KEY&quot; environment variable.</p>
</td></tr>
<tr><td><code id="autocreateFunction4R_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag to print the message Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generate and Improve R Functions
</p>


<h3>Value</h3>

<p>The function returns a character string that represents the generated and improved R function.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  Sys.setenv(OPENAI_API_KEY = "&lt;APIKEY&gt;")
  autocreateFunction4R(Func_description = "2*n+3 sequence")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat4R'>Chat4R: Interact with GPT-3.5 (default) using OpenAI API</h2><span id='topic+chat4R'></span>

<h3>Description</h3>

<p>This function uses the OpenAI API to interact with the
GPT-3.5 model (default) and generates responses based on user input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat4R(
  content,
  Model = "gpt-3.5-turbo-16k",
  temperature = 1,
  simple = TRUE,
  fromJSON_parsed = FALSE,
  api_key = Sys.getenv("OPENAI_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat4R_+3A_content">content</code></td>
<td>
<p>A string containing the user's input message.</p>
</td></tr>
<tr><td><code id="chat4R_+3A_model">Model</code></td>
<td>
<p>A string specifying the GPT model to use (default: &quot;gpt-3.5-turbo-16k&quot;).</p>
</td></tr>
<tr><td><code id="chat4R_+3A_temperature">temperature</code></td>
<td>
<p>A numeric value controlling the randomness of the model's output (default: 1).</p>
</td></tr>
<tr><td><code id="chat4R_+3A_simple">simple</code></td>
<td>
<p>Logical, if TRUE, only the content of the model's message will be returned.</p>
</td></tr>
<tr><td><code id="chat4R_+3A_fromjson_parsed">fromJSON_parsed</code></td>
<td>
<p>Logical, if TRUE, content will be parsed from JSON.</p>
</td></tr>
<tr><td><code id="chat4R_+3A_api_key">api_key</code></td>
<td>
<p>A string containing the user's OpenAI API key. Defaults to the value of the environment variable &quot;OPENAI_API_KEY&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Chat4R Function
</p>


<h3>Value</h3>

<p>A data frame containing the response from the GPT model.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
Sys.setenv(OPENAI_API_KEY = "Your API key")
response &lt;- chat4R("What is the capital of France?")
response

## End(Not run)
</code></pre>

<hr>
<h2 id='chat4R_history'>chat4R_history: Use chat history for interacting with GPT.</h2><span id='topic+chat4R_history'></span>

<h3>Description</h3>

<p>This function use the chat history with the the specified GPT model, and chat the AI model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat4R_history(
  history,
  api_key = Sys.getenv("OPENAI_API_KEY"),
  Model = "gpt-3.5-turbo-16k",
  temperature = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat4R_history_+3A_history">history</code></td>
<td>
<p>A list of message objects.
Each object should have a 'role' that can be 'system', 'user', or 'assistant',
and 'content' which is the content of the message from that role.</p>
</td></tr>
<tr><td><code id="chat4R_history_+3A_api_key">api_key</code></td>
<td>
<p>A string. Input your OpenAI API key. Defaults to the value of the environment variable &quot;OPENAI_API_KEY&quot;.</p>
</td></tr>
<tr><td><code id="chat4R_history_+3A_model">Model</code></td>
<td>
<p>A string. The model to use for the chat completion. Default is &quot;gpt-3.5-turbo-16k&quot;.</p>
</td></tr>
<tr><td><code id="chat4R_history_+3A_temperature">temperature</code></td>
<td>
<p>The temperature to use for the chat completion. Default is 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Chat History for R
</p>


<h3>Value</h3>

<p>A data frame containing the parsed response from the Web API server.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
Sys.setenv(OPENAI_API_KEY = "Your API key")

history &lt;- list(list('role' = 'system', 'content' = 'You are a helpful assistant.'),
                list('role' = 'user', 'content' = 'Who won the world series in 2020?'))

chat4R_history(history)

## End(Not run)

</code></pre>

<hr>
<h2 id='chatAI4pdf'>chatAI4pdf</h2><span id='topic+chatAI4pdf'></span>

<h3>Description</h3>

<p>Reads a PDF file and summarizes its content using a specified Large Language Model (LLM).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chatAI4pdf(pdf_file_path, nch = 2000, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chatAI4pdf_+3A_pdf_file_path">pdf_file_path</code></td>
<td>
<p>The path to the PDF file to be summarized. Must be a string.</p>
</td></tr>
<tr><td><code id="chatAI4pdf_+3A_nch">nch</code></td>
<td>
<p>The maximum number of characters for the summary. Default is 2000.</p>
</td></tr>
<tr><td><code id="chatAI4pdf_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag to print the summary. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Summarize PDF Text via LLM
</p>
<p>This function reads a PDF file and summarizes its content using a specified Large Language Model (LLM).
</p>


<h3>Value</h3>

<p>A string containing the summarized text.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

#Baktash et al: GPT-4: A REVIEW ON ADVANCEMENTS AND OPPORTUNITIES IN NATURAL LANGUAGE PROCESSING
pdf_file_path &lt;- "https://arxiv.org/pdf/2305.03195.pdf"

#Execute
summary_text &lt;- chatAI4pdf(pdf_file_path)

## End(Not run)
</code></pre>

<hr>
<h2 id='checkErrorDet'>Check Error Details</h2><span id='topic+checkErrorDet'></span>

<h3>Description</h3>

<p>A function to analyze and provide guidance on how to fix an error message copied from the R console.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkErrorDet(
  Summary_nch = 100,
  Model = "gpt-4-0613",
  language = "English",
  verbose = TRUE,
  SlowTone = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkErrorDet_+3A_summary_nch">Summary_nch</code></td>
<td>
<p>An integer specifying the maximum number of characters for the summary.</p>
</td></tr>
<tr><td><code id="checkErrorDet_+3A_model">Model</code></td>
<td>
<p>A string specifying the model to be used, default is &quot;gpt-4-0314&quot;.
Currently, &quot;gpt-4&quot;, &quot;gpt-4-0314&quot; and &quot;gpt-4-0613&quot; can be selected as gpt-4 models.
Execution with GPT-4 is recommended.</p>
</td></tr>
<tr><td><code id="checkErrorDet_+3A_language">language</code></td>
<td>
<p>A string specifying the output language, default is &quot;English&quot;.</p>
</td></tr>
<tr><td><code id="checkErrorDet_+3A_verbose">verbose</code></td>
<td>
<p>A logical value to control the verbosity of the output, default is TRUE.</p>
</td></tr>
<tr><td><code id="checkErrorDet_+3A_slowtone">SlowTone</code></td>
<td>
<p>A logical value to control the printing speed of the output, default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Check Error Details
</p>
<p>This function provides a way to check error details in R. It takes an error message from the R console,
executes the function, and shows how to fix the error in the specified language.
</p>


<h3>Value</h3>

<p>The function prints the guidance on how to fix the error message.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Copy the error message that you want to fix.
  checkErrorDet()
  checkErrorDet(language = "Japanese")

## End(Not run)
</code></pre>

<hr>
<h2 id='checkErrorDet_JP'>Check Error Details in Japanese</h2><span id='topic+checkErrorDet_JP'></span>

<h3>Description</h3>

<p>A function to analyze and provide guidance on how to fix an error message copied from the R console.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkErrorDet_JP(
  Summary_nch = 100,
  Model = "gpt-4-0613",
  verbose = TRUE,
  SlowTone = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkErrorDet_JP_+3A_summary_nch">Summary_nch</code></td>
<td>
<p>An integer specifying the maximum number of characters for the summary.</p>
</td></tr>
<tr><td><code id="checkErrorDet_JP_+3A_model">Model</code></td>
<td>
<p>A string specifying the model to be used, default is &quot;gpt-4-0314&quot;.
Currently, &quot;gpt-4&quot;, &quot;gpt-4-0314&quot; and &quot;gpt-4-0613&quot; can be selected as gpt-4 models.
Execution with GPT-4 is recommended.</p>
</td></tr>
<tr><td><code id="checkErrorDet_JP_+3A_verbose">verbose</code></td>
<td>
<p>A logical value to control the verbosity of the output, default is TRUE.</p>
</td></tr>
<tr><td><code id="checkErrorDet_JP_+3A_slowtone">SlowTone</code></td>
<td>
<p>A logical value to control the printing speed of the output, default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Check Error Details in Japanese via RStudio API
</p>
<p>This function provides a way to check error details in R. It reads the error message from the clipboard,
executes the function, and shows how to fix the error in Japanese.
</p>


<h3>Value</h3>

<p>The function prints the guidance on how to fix the error message in Japanese. If verbose is FALSE, it returns the guidance as a string.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Analyzing error message from the clipboard
  checkErrorDet_JP(Summary_nch = 100, Model = "gpt-4-0613", verbose = TRUE, SlowTone = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='completions4R'>completions4R: Generate text using OpenAI completions API (Legacy)</h2><span id='topic+completions4R'></span>

<h3>Description</h3>

<p>This function sends a request to the OpenAI completions API (Legacy)
to generate text based on the provided prompt and parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>completions4R(
  prompt,
  api_key = Sys.getenv("OPENAI_API_KEY"),
  Model = "text-davinci-003",
  max_tokens = 50,
  temperature = 1,
  simple = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="completions4R_+3A_prompt">prompt</code></td>
<td>
<p>A string. The initial text that the model responds to.</p>
</td></tr>
<tr><td><code id="completions4R_+3A_api_key">api_key</code></td>
<td>
<p>A string. The API key for OpenAI. Defaults to the value of the environment variable &quot;OPENAI_API_KEY&quot;.</p>
</td></tr>
<tr><td><code id="completions4R_+3A_model">Model</code></td>
<td>
<p>A string. The model ID to use, such as &quot;text-davinci-003&quot;.</p>
</td></tr>
<tr><td><code id="completions4R_+3A_max_tokens">max_tokens</code></td>
<td>
<p>Integer. The maximum number of tokens to generate.</p>
</td></tr>
<tr><td><code id="completions4R_+3A_temperature">temperature</code></td>
<td>
<p>A numeric value to control the randomness of the generated text.
A value close to 0 produces more deterministic output, while a higher value (up to 2) produces more random output.</p>
</td></tr>
<tr><td><code id="completions4R_+3A_simple">simple</code></td>
<td>
<p>If TRUE, returns the generated text without newline characters. If FALSE, returns the full response from the API.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The generated text or the full response from the API, depending on the value of 'simple'.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
Sys.setenv(OPENAI_API_KEY = "Your API key")

prompt &lt;- "Translate the following English text to French: 'Hello, world!'"

completions4R(prompt)

## End(Not run)
</code></pre>

<hr>
<h2 id='conversation4R'>Conversation Interface for R</h2><span id='topic+conversation4R'></span>

<h3>Description</h3>

<p>Interface to communicate with OpenAI's models using R, maintaining a conversation history and allowing for initialization of a new conversation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conversation4R(
  message,
  api_key = Sys.getenv("OPENAI_API_KEY"),
  template = "",
  ConversationBufferWindowMemory_k = 2,
  Model = "gpt-3.5-turbo-16k",
  language = "English",
  initialization = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conversation4R_+3A_message">message</code></td>
<td>
<p>A string containing the message to be sent to the model.</p>
</td></tr>
<tr><td><code id="conversation4R_+3A_api_key">api_key</code></td>
<td>
<p>A string containing the OpenAI API key. Default is retrieved from the system environment variable &quot;OPENAI_API_KEY&quot;.</p>
</td></tr>
<tr><td><code id="conversation4R_+3A_template">template</code></td>
<td>
<p>A string containing the template for the conversation. Default is an empty string.</p>
</td></tr>
<tr><td><code id="conversation4R_+3A_conversationbufferwindowmemory_k">ConversationBufferWindowMemory_k</code></td>
<td>
<p>An integer representing the conversation buffer window memory. Default is 2.</p>
</td></tr>
<tr><td><code id="conversation4R_+3A_model">Model</code></td>
<td>
<p>A string representing the model to be used. Default is &quot;gpt-3.5-turbo-16k&quot;.</p>
</td></tr>
<tr><td><code id="conversation4R_+3A_language">language</code></td>
<td>
<p>A string representing the language to be used in the conversation. Default is &quot;English&quot;.</p>
</td></tr>
<tr><td><code id="conversation4R_+3A_initialization">initialization</code></td>
<td>
<p>A logical flag to initialize a new conversation. Default is FALSE.</p>
</td></tr>
<tr><td><code id="conversation4R_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag to print the conversation. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Conversation Interface for R with OpenAI
</p>
<p>This function provides an interface to communicate with OpenAI's models using R. It maintains a conversation history and allows for initialization of a new conversation.
</p>


<h3>Value</h3>

<p>Prints the conversation if verbose is TRUE. No return value.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
conversation4R(message = "Hello, OpenAI!",
               api_key = "your_api_key_here",
               language = "English",
               initialization = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='convertBullet2Sentence'>convertBullet2Sentence</h2><span id='topic+convertBullet2Sentence'></span>

<h3>Description</h3>

<p>Convert bullet points to sentences using OpenAI GPT model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convertBullet2Sentence(
  Model = "gpt-4-0613",
  temperature = 1,
  verbose = TRUE,
  SpeakJA = FALSE,
  SelectedCode = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convertBullet2Sentence_+3A_model">Model</code></td>
<td>
<p>The OpenAI GPT model to use for text generation. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="convertBullet2Sentence_+3A_temperature">temperature</code></td>
<td>
<p>The temperature parameter for text generation. Default is 1.</p>
</td></tr>
<tr><td><code id="convertBullet2Sentence_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag to indicate whether to display progress. Default is TRUE.</p>
</td></tr>
<tr><td><code id="convertBullet2Sentence_+3A_speakja">SpeakJA</code></td>
<td>
<p>Logical flag to indicate whether to use Japanese speech output. Default is FALSE.</p>
</td></tr>
<tr><td><code id="convertBullet2Sentence_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>Logical flag to indicate whether to use selected text in RStudio. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Convert Bullet Points to Sentences
</p>
<p>This function takes bullet points as input and converts them into sentences.
The function uses the OpenAI GPT model for text generation to assist in the conversion.
The function can either take the selected text from the RStudio environment or from the clipboard.
</p>


<h3>Value</h3>

<p>Inserts the converted sentences into the RStudio editor if 'SelectedCode' is TRUE, otherwise writes to clipboard.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
convertBullet2Sentence(Model = "gpt-4-0613", SelectedCode = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='convertRscript2Function'>convertRscript2Function</h2><span id='topic+convertRscript2Function'></span>

<h3>Description</h3>

<p>Convert selected R script to an R function using LLM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convertRscript2Function(
  Model = "gpt-4-0613",
  SelectedCode = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convertRscript2Function_+3A_model">Model</code></td>
<td>
<p>The OpenAI GPT model to use for text generation. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="convertRscript2Function_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>Logical flag to indicate whether to use selected code in RStudio. Default is TRUE.</p>
</td></tr>
<tr><td><code id="convertRscript2Function_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether to print the result to the console, default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Convert Selected R Script to R Function
</p>
<p>This function takes a selected portion of an R script and converts it into an R function.
The function uses the OpenAI GPT model for text generation to assist in the conversion.
The function can either take the selected code from the RStudio environment or from the clipboard.
</p>


<h3>Value</h3>

<p>Inserts the converted function into the RStudio editor if 'SelectedCode' is TRUE, otherwise writes to clipboard.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
convertRscript2Function(Model = "gpt-4-0613", SelectedCode = F)

## End(Not run)
</code></pre>

<hr>
<h2 id='convertScientificLiterature'>convertScientificLiterature</h2><span id='topic+convertScientificLiterature'></span>

<h3>Description</h3>

<p>Convert input text into scientific literature.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convertScientificLiterature(Model = "gpt-4-0613", SelectedCode = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convertScientificLiterature_+3A_model">Model</code></td>
<td>
<p>The OpenAI GPT model to use for text generation. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="convertScientificLiterature_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>Logical flag to indicate whether to read the input from RStudio's active document. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Convert to Scientific Literature
</p>
<p>This function assists in converting the input text into scientific literature.
It uses the OpenAI GPT model for text generation to assist in the conversion process.
The function reads the input either from the RStudio active document or the clipboard.
</p>


<h3>Value</h3>

<p>Inserts the converted text into the RStudio active document if SelectedCode is TRUE, otherwise writes to the clipboard.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
convertScientificLiterature(SelectedCode = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='createImagePrompt_v1'>Create Image Prompt version 1</h2><span id='topic+createImagePrompt_v1'></span>

<h3>Description</h3>

<p>This function creates a prompt for generating an image from text using an AI model.
This is an experimental function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createImagePrompt_v1(content, Model = "gpt-3.5-turbo-16k", len = 200)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createImagePrompt_v1_+3A_content">content</code></td>
<td>
<p>A character string describing the image to be generated. If not provided, the function will throw a warning and stop.</p>
</td></tr>
<tr><td><code id="createImagePrompt_v1_+3A_model">Model</code></td>
<td>
<p>A character string specifying the AI model to be used for text generation.</p>
</td></tr>
<tr><td><code id="createImagePrompt_v1_+3A_len">len</code></td>
<td>
<p>Integer specifying the maximum length of the text input.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create Image Prompt version 1
</p>


<h3>Value</h3>

<p>A character string that serves as the prompt for generating an image.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
createImagePrompt_v1(content = "A Japanese girl animation with blonde hair.")

## End(Not run)

</code></pre>

<hr>
<h2 id='createImagePrompt_v2'>Generate Image Prompts version 2</h2><span id='topic+createImagePrompt_v2'></span>

<h3>Description</h3>

<p>Generates optimal prompts for creating images using the OpenAI API.
Given a base prompt, image attributes, and model parameters, it generates the optimal prompts for image creation.
This is an experimental function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createImagePrompt_v2(
  Base_prompt = "",
  removed_from_image = "",
  stable_diffusion = "N/A",
  Model = "gpt-3.5-turbo-16k",
  len = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createImagePrompt_v2_+3A_base_prompt">Base_prompt</code></td>
<td>
<p>A string. This is the base prompt that forms the basis for the generated prompts.</p>
</td></tr>
<tr><td><code id="createImagePrompt_v2_+3A_removed_from_image">removed_from_image</code></td>
<td>
<p>A string. This is an attribute that should be removed from the image.</p>
</td></tr>
<tr><td><code id="createImagePrompt_v2_+3A_stable_diffusion">stable_diffusion</code></td>
<td>
<p>A string. This parameter is used to control the stability of the diffusion process.</p>
</td></tr>
<tr><td><code id="createImagePrompt_v2_+3A_model">Model</code></td>
<td>
<p>A string. This is the model used for generating the prompts. Default is &quot;gpt-3.5-turbo-16k&quot;.</p>
</td></tr>
<tr><td><code id="createImagePrompt_v2_+3A_len">len</code></td>
<td>
<p>An integer. This is the maximum length of the generated prompts. Must be between 1 and 1000. Default is 1000.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generate Image Prompts version 2
</p>


<h3>Value</h3>

<p>A vector of strings. Each string in the vector is a generated prompt.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Define the base prompt and image attributes
base_prompt = "A sunset over a serene lake"
removed_from_image = "The sun"
stable_diffusion = "Moderate"

# Generate image prompts
res &lt;- createImagePrompt_v2(Base_prompt = base_prompt, removed_from_image = removed_from_image,
stable_diffusion = stable_diffusion, len = 100)

# Print the generated prompts
print(res)

## End(Not run)
</code></pre>

<hr>
<h2 id='createRcode'>Create R Code from Clipboard Content and Output into the R Console</h2><span id='topic+createRcode'></span>

<h3>Description</h3>

<p>Reads text from the clipboard and generates R code based on the given input, printing the result to the R console.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createRcode(
  Summary_nch = 100,
  Model = "gpt-4-0613",
  SelectedCode = TRUE,
  verbose = TRUE,
  SlowTone = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createRcode_+3A_summary_nch">Summary_nch</code></td>
<td>
<p>The maximum number of characters for the summary.</p>
</td></tr>
<tr><td><code id="createRcode_+3A_model">Model</code></td>
<td>
<p>The model to be used for code generation, default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="createRcode_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>A logical flag to indicate whether to read from RStudio's selected text. Default is TRUE.</p>
</td></tr>
<tr><td><code id="createRcode_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether to print the result to the console, default is TRUE.</p>
</td></tr>
<tr><td><code id="createRcode_+3A_slowtone">SlowTone</code></td>
<td>
<p>A logical value indicating whether to print the result slowly, default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create R Code from Selected Text or Clipboard Content
</p>
<p>This function reads text from your selected text or clipboard, interprets it as a prompt, and generates R code based on the given input.
The generated R code is then printed to the R console with optional slow printing.
This function can be executed from RStudio's Addins menu.
</p>


<h3>Value</h3>

<p>Prints the generated R code to the R console.
</p>


<h3>RStudio Addins</h3>

<p>This function can be added to RStudio's Addins menu for easy access.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# Copy the origin of R code to your clipboard then execute from RStudio's Addins.


## End(Not run)
</code></pre>

<hr>
<h2 id='createRfunction'>Create R Function from Selected Text or Clipboard Content and Output into the R Console</h2><span id='topic+createRfunction'></span>

<h3>Description</h3>

<p>This function reads text either from your selected text in RStudio or from the clipboard, interprets it as a prompt, and generates an R function based on the given input. The generated R code is then printed into the source file or the R console with optional slow printing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createRfunction(
  Model = "gpt-4-0613",
  SelectedCode = TRUE,
  verbose = TRUE,
  SlowTone = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createRfunction_+3A_model">Model</code></td>
<td>
<p>A character string representing the model to be used. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="createRfunction_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>A logical value indicating if the selected text should be used as input. Default is TRUE.</p>
</td></tr>
<tr><td><code id="createRfunction_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating if progress should be printed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="createRfunction_+3A_slowtone">SlowTone</code></td>
<td>
<p>A logical value indicating if slow printing should be used. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create R Function from Selected Text or Clipboard Content
</p>


<h3>Value</h3>

<p>This function returns the generated R code as a clipboard content if SelectedCode is FALSE.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

#Copy the idea text of the R function to your clipboard and run this function.
createRfunction(SelectedCode = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='createSpecifications4R'>Create Specifications for R Function</h2><span id='topic+createSpecifications4R'></span>

<h3>Description</h3>

<p>This function generates specifications for an R function from your selected text or clipboard.
It takes in a text input, model name, verbosity, and tone speed to generate the specifications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createSpecifications4R(
  Model = "gpt-4-0613",
  SelectedCode = TRUE,
  verbose = TRUE,
  SlowTone = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createSpecifications4R_+3A_model">Model</code></td>
<td>
<p>A character string specifying the GPT model to be used. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="createSpecifications4R_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>A logical flag to indicate whether to read from RStudio's selected text. Default is TRUE.</p>
</td></tr>
<tr><td><code id="createSpecifications4R_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether to print the output. Default is TRUE.</p>
</td></tr>
<tr><td><code id="createSpecifications4R_+3A_slowtone">SlowTone</code></td>
<td>
<p>A logical value indicating whether to print the output slowly. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create Specifications for R Function
</p>


<h3>Value</h3>

<p>The function prints the generated specifications to the console.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
createSpecifications4R(input = "Your R function specification")

## End(Not run)
</code></pre>

<hr>
<h2 id='designPackage'>designPackage</h2><span id='topic+designPackage'></span>

<h3>Description</h3>

<p>Assist in proposing the overall design and architecture of an R package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>designPackage(Model = "gpt-4-0613", verbose = TRUE, SlowTone = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="designPackage_+3A_model">Model</code></td>
<td>
<p>The OpenAI GPT model to use for text generation. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="designPackage_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag to indicate whether to display the generated text. Default is TRUE.</p>
</td></tr>
<tr><td><code id="designPackage_+3A_slowtone">SlowTone</code></td>
<td>
<p>Logical flag to indicate whether to print the text slowly. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Design Package for R
</p>
<p>This function assists in proposing the overall design and architecture of an R package.
It uses the OpenAI GPT model for text generation to assist in the design process.
The function reads the input from the clipboard.
</p>


<h3>Value</h3>

<p>Prints the proposed design and architecture based on the verbosity and tone speed settings.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
designPackage(Model = "gpt-4-0613", verbose = TRUE, SlowTone = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='discussion_flow_v1'>discussion_flow_v1: Interactions and Flow Control Between LLM-based Bots (LLBs)</h2><span id='topic+discussion_flow_v1'></span>

<h3>Description</h3>

<p>Simulates interactions and flow control between three different roles of LLM-based bots (LLBs).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discussion_flow_v1(
  issue,
  Domain = "bioinformatics",
  Model = "gpt-4-0613",
  api_key = Sys.getenv("OPENAI_API_KEY"),
  language = "English",
  Summary_nch = 50,
  verbose = TRUE,
  Nonfuture = TRUE,
  sayENorJA = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discussion_flow_v1_+3A_issue">issue</code></td>
<td>
<p>The issue to be discussed. Example: &quot;I want to solve linear programming and create a timetable.&quot;</p>
</td></tr>
<tr><td><code id="discussion_flow_v1_+3A_domain">Domain</code></td>
<td>
<p>The domain of the discussion, default is &quot;bioinformatics&quot;.</p>
</td></tr>
<tr><td><code id="discussion_flow_v1_+3A_model">Model</code></td>
<td>
<p>The model to be used, default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="discussion_flow_v1_+3A_api_key">api_key</code></td>
<td>
<p>The API key for OpenAI, default is retrieved from the system environment variable &quot;OPENAI_API_KEY&quot;.</p>
</td></tr>
<tr><td><code id="discussion_flow_v1_+3A_language">language</code></td>
<td>
<p>The language for the discussion, default is &quot;English&quot;.</p>
</td></tr>
<tr><td><code id="discussion_flow_v1_+3A_summary_nch">Summary_nch</code></td>
<td>
<p>The number of characters for the summary, default is 50.</p>
</td></tr>
<tr><td><code id="discussion_flow_v1_+3A_verbose">verbose</code></td>
<td>
<p>Logical, whether to print verbose output, default is TRUE.</p>
</td></tr>
<tr><td><code id="discussion_flow_v1_+3A_nonfuture">Nonfuture</code></td>
<td>
<p>Logical, whether to use an asynchronous processing or not, default is not to use (TRUE).</p>
</td></tr>
<tr><td><code id="discussion_flow_v1_+3A_sayenorja">sayENorJA</code></td>
<td>
<p>Logical, whether to say in English or Japanese, default is TRUE. This feature is available on  macOS system only.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Interactions and Flow Control Between LLM-based Bots (LLBs)
</p>
<p>This function is described to simulate the interactions and flow control between
three different roles of LLM-based bots, abbreviated as LLBs,
and to reproduce more realistic dialogues and discussions.
Here is a brief description of the roles:
A (Beginner): This bot generates questions and summaries based on the content of the discussion provided by the user.
B (Expert): This bot provides professional answers to questions posed by LLB A.
C (Peer Reviewer):  This bot reviews the dialog between LLB A and LLB B and suggests improvements or refinements.
The three parties independently call the OpenAI API according to their roles.
In addition, it keeps track of the conversation history between the bots and performs
processes such as questioning, answering, and peer review.
The function is designed to work in a &quot;domain,&quot; which is essentially a specific area
or topic around which conversations revolve.
It is recommended to use GPT-4 or a model with higher accuracy than GPT-4.
English is recommended as the input language, but the review will also be conducted in Japanese, the native language of the author.
</p>


<h3>Value</h3>

<p>A summary of the conversation between the bots.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
issue &lt;-  "I want to solve linear programming and create a timetable."

#Run Discussion with the domain of bioinformatics
discussion_flow_v1(issue)

## End(Not run)
</code></pre>

<hr>
<h2 id='enrichTextContent'>Enrich Text Content</h2><span id='topic+enrichTextContent'></span>

<h3>Description</h3>

<p>This function doubles the amount of text without changing its meaning.
The GPT-4 model is currently recommended for text generation. It can either read from the RStudio selection or the clipboard.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enrichTextContent(Model = "gpt-4-0613", SelectedCode = TRUE, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="enrichTextContent_+3A_model">Model</code></td>
<td>
<p>A character string specifying the AI model to be used for text enrichment. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="enrichTextContent_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>A logical flag to indicate whether to read from RStudio's selected text. Default is TRUE.</p>
</td></tr>
<tr><td><code id="enrichTextContent_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag to indicate whether to display the generated text. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Enrich Text Content
</p>


<h3>Value</h3>

<p>If SelectedCode is TRUE, the enriched text is inserted into the RStudio editor and a message &quot;Finished!!&quot; is returned.
Otherwise, the enriched text is placed into the clipboard.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  enrichTextContent(Model = "gpt-4-0613", SelectedCode = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='extractKeywords'>extractKeywords</h2><span id='topic+extractKeywords'></span>

<h3>Description</h3>

<p>Extract keywords from input text and output them in a comma-separated format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractKeywords(Model = "gpt-4-0613", verbose = TRUE, SlowTone = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extractKeywords_+3A_model">Model</code></td>
<td>
<p>The OpenAI GPT model to use for text generation. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="extractKeywords_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag to indicate whether to print the result. Default is TRUE.</p>
</td></tr>
<tr><td><code id="extractKeywords_+3A_slowtone">SlowTone</code></td>
<td>
<p>Logical flag to indicate the speed of the output. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extract Keywords from Text
</p>
<p>This function extracts keywords from the input text. It uses the OpenAI GPT model
for text generation to assist in the extraction process. The function reads the input
from the clipboard and outputs the extracted keywords in a comma-separated format.
</p>


<h3>Value</h3>

<p>Prints the extracted keywords based on verbosity and tone speed settings.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
extractKeywords(Model = "gpt-4-0613", verbose = TRUE, SlowTone = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='ngsub'>ngsub</h2><span id='topic+ngsub'></span>

<h3>Description</h3>

<p>Remove extra spaces and newline characters from text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ngsub(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ngsub_+3A_text">text</code></td>
<td>
<p>The input text from which extra spaces and newline characters need to be removed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Remove Extra Spaces and Newline Characters
</p>
<p>This function removes extra spaces and newline characters from the given text.
It replaces sequences of multiple spaces with a single space and removes newline characters followed by a space.
</p>


<h3>Value</h3>

<p>Returns the modified text as a character string.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ngsub("This is  a text \n with  extra   spaces.")

## End(Not run)
</code></pre>

<hr>
<h2 id='OptimizeRcode'>Optimize and Complete R Code</h2><span id='topic+OptimizeRcode'></span>

<h3>Description</h3>

<p>Optimizes and completes the R code from the selected code or clipboard.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>OptimizeRcode(
  Model = "gpt-4-0613",
  SelectedCode = TRUE,
  verbose = TRUE,
  verbose_Reasons4change = FALSE,
  SlowTone = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="OptimizeRcode_+3A_model">Model</code></td>
<td>
<p>A string specifying the machine learning model to use for code optimization. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="OptimizeRcode_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>A boolean indicating whether to get the code from RStudio or the clipboard. Default is TRUE.</p>
</td></tr>
<tr><td><code id="OptimizeRcode_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether to print the result to the console, default is TRUE.</p>
</td></tr>
<tr><td><code id="OptimizeRcode_+3A_verbose_reasons4change">verbose_Reasons4change</code></td>
<td>
<p>A boolean indicating whether to provide detailed reasons for the changes made. Default is FALSE</p>
</td></tr>
<tr><td><code id="OptimizeRcode_+3A_slowtone">SlowTone</code></td>
<td>
<p>A boolean indicating whether to print the output slowly. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Optimize and Complete R Code
</p>
<p>This function takes a snippet of R code and optimizes it for performance and readability.
It uses a machine learning model to generate the optimized code.
</p>


<h3>Value</h3>

<p>A message indicating the completion of the optimization process.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#Copy your R code then run the following function.
OptimizeRcode(SelectedCode = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='proofreadEnglishText'>Proofread English Text</h2><span id='topic+proofreadEnglishText'></span>

<h3>Description</h3>

<p>A function to proofread English text or text in different languages during R package development.
It translates the input into English if necessary and returns meticulously checked English text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proofreadEnglishText(Model = "gpt-4-0613", SelectedCode = TRUE, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proofreadEnglishText_+3A_model">Model</code></td>
<td>
<p>A string specifying the model to be used for proofreading, defaulting to &quot;gpt-4-0314&quot;.
Currently, &quot;gpt-4&quot;, &quot;gpt-4-0314&quot; and &quot;gpt-4-0613&quot; can be selected as gpt-4 models.
Execution with GPT-4 is recommended.</p>
</td></tr>
<tr><td><code id="proofreadEnglishText_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>A logical value indicating whether to read the selected text from the RStudio editor (TRUE) or from the clipboard (FALSE). Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="proofreadEnglishText_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag to print the progress. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Proofread English Text During R Package Development via RStudio API
</p>
<p>This function provides a feature to proofread English text during the development of an R package.
It can either take the selected text from the RStudio editor or read from the clipboard, executes the proofreading, and returns the result to the user's clipboard or replaces the selected text.
The user can then paste and check the result if read from the clipboard. The function adheres to R package policies and carefully proofreads the English text.
Execution with GPT-4 is recommended.
</p>


<h3>Value</h3>

<p>The proofread text, which is also written to the clipboard if SelectedCode is FALSE, or replaces the selected text if SelectedCode is TRUE.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Proofreading selected text in RStudio
  proofreadEnglishText(Model = "gpt-4-0613", SelectedCode = TRUE)
  # Proofreading text from the clipboard
  proofreadEnglishText(Model = "gpt-4-0613", SelectedCode = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='proofreadText'>proofreadText</h2><span id='topic+proofreadText'></span>

<h3>Description</h3>

<p>Proofreads text during the development of an R package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proofreadText(Model = "gpt-4-0613", SelectedCode = TRUE, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proofreadText_+3A_model">Model</code></td>
<td>
<p>The Large Language Model to be used for proofreading. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="proofreadText_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>Logical flag to indicate whether to use the selected text in RStudio editor. Default is TRUE.</p>
</td></tr>
<tr><td><code id="proofreadText_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag to print the progress. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Proofread Text During R Package Development Through the RStudio API
</p>
<p>This function offers a feature for proofreading text while developing an R package.
It can either use the text selected in the RStudio editor or read from the clipboard, perform the proofreading, and then either replace the selected text or return the result to the user's clipboard.
The language of the output will match the language of the input text. Using GPT-4 for execution is recommended.
</p>


<h3>Value</h3>

<p>NULL if 'SelectedCode' is TRUE, otherwise returns the proofread text to the clipboard.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Proofread text from clipboard
proofreadText(SelectedCode = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='RcodeImprovements'>Suggest Improvements to the R Code on Your Clipboard</h2><span id='topic+RcodeImprovements'></span>

<h3>Description</h3>

<p>This function uses LLM to analyze the R code from the clipboard and suggests improvements.
The function can also control the verbosity and speed of the output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RcodeImprovements(
  Summary_nch = 100,
  Model = "gpt-4-0613",
  verbose = TRUE,
  SlowTone = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RcodeImprovements_+3A_summary_nch">Summary_nch</code></td>
<td>
<p>An integer specifying the maximum number of characters for the summary. Default is 100.</p>
</td></tr>
<tr><td><code id="RcodeImprovements_+3A_model">Model</code></td>
<td>
<p>A character string specifying the GPT model to be used. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="RcodeImprovements_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether to print the result. Default is TRUE.</p>
</td></tr>
<tr><td><code id="RcodeImprovements_+3A_slowtone">SlowTone</code></td>
<td>
<p>A logical value indicating whether to print the result slowly. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suggest Improvements for R Code
</p>


<h3>Value</h3>

<p>No return value; the function prints the suggestions for code improvement.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#Copy your function to your clipboard
RcodeImprovements(Summary_nch = 100, Model = "gpt-4-0613")

## End(Not run)
</code></pre>

<hr>
<h2 id='removeQuotations'>Remove All Types of Quotations from Text</h2><span id='topic+removeQuotations'></span>

<h3>Description</h3>

<p>This function takes a text string as input and removes all occurrences
of single, double, and back quotations marks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>removeQuotations(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="removeQuotations_+3A_text">text</code></td>
<td>
<p>A character string from which quotations will be removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string with all types of quotations removed.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
removeQuotations("\"XXX'`\"YYY'`") # Returns "XXXYYY"

## End(Not run)
</code></pre>

<hr>
<h2 id='revisedText'>Revised Scientific Text</h2><span id='topic+revisedText'></span>

<h3>Description</h3>

<p>This function prompts the user to input text, revision comments, and additional background information.
It then revises the text according to the comments and outputs the revised text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>revisedText(verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="revisedText_+3A_verbose">verbose</code></td>
<td>
<p>Logical, whether to output verbose messages, default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Revise Scientific Text
</p>


<h3>Value</h3>

<p>Revised text or relevant message.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
 revisedText()

## End(Not run)
</code></pre>

<hr>
<h2 id='searchFunction'>Search R Functions based on Text</h2><span id='topic+searchFunction'></span>

<h3>Description</h3>

<p>Searches for an R function related to the provided text
either through the RStudio editor selection or clipboard.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>searchFunction(
  Summary_nch = 100,
  Model = "gpt-4-0613",
  SelectedCode = TRUE,
  verbose = TRUE,
  SlowTone = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="searchFunction_+3A_summary_nch">Summary_nch</code></td>
<td>
<p>Numeric, number of characters to limit the function description (default = 100).</p>
</td></tr>
<tr><td><code id="searchFunction_+3A_model">Model</code></td>
<td>
<p>String, the model used for the search, default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="searchFunction_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>Logical, whether to get text from RStudio selection (default = TRUE).</p>
</td></tr>
<tr><td><code id="searchFunction_+3A_verbose">verbose</code></td>
<td>
<p>Logical, whether to print the results verbosely (default = TRUE).</p>
</td></tr>
<tr><td><code id="searchFunction_+3A_slowtone">SlowTone</code></td>
<td>
<p>Logical, whether to slow down the print speed for readability (default = FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Search the R function based on the provided text
</p>
<p>This function searches for an R function that corresponds to the text
provided either through the RStudio editor selection or the clipboard.
It fetches the related R function and outputs its name, package, and
a brief description. The function uses GPT-4 for its underlying search.
</p>


<h3>Value</h3>

<p>Console output of the identified R function, its package, and a brief description.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# To search for an R function related to "linear regression"
searchFunction(Summary_nch = 50, SelectedCode = FALSE)

## End(Not run)

</code></pre>

<hr>
<h2 id='slow_print_v2'>Slowly Print Text</h2><span id='topic+slow_print_v2'></span>

<h3>Description</h3>

<p>Prints the characters of the input text string one by one,
with a specified delay between each character. If the random parameter
is set to TRUE, the delay will be a random value between 0.0001 and 0.3 seconds.
Otherwise, the delay will be the value specified by the delay parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slow_print_v2(text, random = FALSE, delay = 0.125)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slow_print_v2_+3A_text">text</code></td>
<td>
<p>A string representing the text to be printed. Must be a non-NA string.</p>
</td></tr>
<tr><td><code id="slow_print_v2_+3A_random">random</code></td>
<td>
<p>A logical value indicating whether the delay between characters should be random.
Default is FALSE.</p>
</td></tr>
<tr><td><code id="slow_print_v2_+3A_delay">delay</code></td>
<td>
<p>A numeric value representing the fixed delay between characters in seconds.
Default is 0.125. Must be a non-negative number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Slowly Print Text
</p>
<p>This function prints the characters of a given text string one by one,
with a specified delay between each character. The delay can be either fixed or random.
</p>


<h3>Value</h3>

<p>Invisible NULL. The function prints the text to the console.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
slow_print_v2("Hello, World!")
slow_print_v2("Hello, World!", random = TRUE)
slow_print_v2("Hello, World!", delay = 0.1)

## End(Not run)
</code></pre>

<hr>
<h2 id='speakInJA'>Speak Selected Text in Japanese</h2><span id='topic+speakInJA'></span>

<h3>Description</h3>

<p>This function reads aloud the selected text in Japanese using the MacOS system's 'say' command.
The function specifically uses the 'Kyoko' voice for the speech. It only works on MacOS systems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>speakInJA()
</code></pre>


<h3>Details</h3>

<p>Speak Selected Text in Japanese on MacOS System
</p>


<h3>Value</h3>

<p>A message indicating the completion of the speech is returned.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Select some text in RStudio and then run the rstudio addins

## End(Not run)
</code></pre>

<hr>
<h2 id='speakInJA_v2'>Speak Clipboard in Japanese</h2><span id='topic+speakInJA_v2'></span>

<h3>Description</h3>

<p>This function reads aloud the clipboard content in Japanese using the MacOS system's 'say' command.
The function specifically uses the 'Kyoko' voice for the speech. It only works on MacOS systems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>speakInJA_v2()
</code></pre>


<h3>Details</h3>

<p>Speak Clipboard in Japanese on MacOS System
</p>


<h3>Value</h3>

<p>A message indicating the completion of the speech is returned.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Copy some text into your clipboard in RStudio and then run the function
  speakInJA_v2()

## End(Not run)
</code></pre>

<hr>
<h2 id='summaryWebScrapingText'>Text Summary via Web Scraping</h2><span id='topic+summaryWebScrapingText'></span>

<h3>Description</h3>

<p>Scrapes Google Search results for the provided query and summarizes the content.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summaryWebScrapingText(
  query = "LLM",
  t = "w",
  gl = "us",
  hl = "en",
  URL_num = 5,
  verbose = TRUE,
  translateJA = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summaryWebScrapingText_+3A_query">query</code></td>
<td>
<p>The search query. Default is &quot;LLM&quot;.</p>
</td></tr>
<tr><td><code id="summaryWebScrapingText_+3A_t">t</code></td>
<td>
<p>Time period for search. 'w' for last week, 'm' for last month, 'y' for last year.
Default is 'w'.</p>
</td></tr>
<tr><td><code id="summaryWebScrapingText_+3A_gl">gl</code></td>
<td>
<p>Geographical location based on ISO 3166-1 alpha-2 country code. Default is 'us'.</p>
</td></tr>
<tr><td><code id="summaryWebScrapingText_+3A_hl">hl</code></td>
<td>
<p>Language for search results based on ISO 639-1 language code. Default is 'en'.</p>
</td></tr>
<tr><td><code id="summaryWebScrapingText_+3A_url_num">URL_num</code></td>
<td>
<p>Number of URLs to scrape. Default is 5.</p>
</td></tr>
<tr><td><code id="summaryWebScrapingText_+3A_verbose">verbose</code></td>
<td>
<p>A boolean value indicating if details should be printed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="summaryWebScrapingText_+3A_translateja">translateJA</code></td>
<td>
<p>A boolean value indicating if results should be translated to Japanese.
Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Summarize Text via Web Scraping of Google Search
</p>
<p>Scrape text information from Google Search and summarize it using LLM.
Uses OpenAI API key for execution. Translation to Japanese requires a Deepl API key.
</p>


<h3>Value</h3>

<p>Returns a list of summaries.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
summaryWebScrapingText(query = "LLM", t = "w", gl = "us", hl = "en", URL_num = 5)

## End(Not run)
</code></pre>

<hr>
<h2 id='supportIdeaGeneration'>supportIdeaGeneration: Support Idea Generation from Selected Text or Clipboard.</h2><span id='topic+supportIdeaGeneration'></span>

<h3>Description</h3>

<p>Assist in generating ideas or concepts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>supportIdeaGeneration(
  Model = "gpt-4-0613",
  SelectedCode = TRUE,
  verbose = TRUE,
  SlowTone = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="supportIdeaGeneration_+3A_model">Model</code></td>
<td>
<p>The OpenAI GPT model to use for text generation. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="supportIdeaGeneration_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>Logical flag to indicate whether to use the selected text in RStudio editor. Default is TRUE.</p>
</td></tr>
<tr><td><code id="supportIdeaGeneration_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag to indicate whether to display the generated text. Default is TRUE.</p>
</td></tr>
<tr><td><code id="supportIdeaGeneration_+3A_slowtone">SlowTone</code></td>
<td>
<p>Logical flag to indicate whether to print the text slowly. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Support Idea Generation from Selected Text or Clipboard Input
</p>
<p>This feature helps you generate ideas or concepts based on input from your selected text or clipboard.
It uses the OpenAI GPT model for text generation to assist in the idea generation process.
The function reads the input from the clipboard.
</p>


<h3>Value</h3>

<p>Prints the generated ideas or concepts based on the verbosity and tone speed settings.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
supportIdeaGeneration()

## End(Not run)
</code></pre>

<hr>
<h2 id='textEmbedding'>Text Embedding from OpenAI Embeddings API</h2><span id='topic+textEmbedding'></span>

<h3>Description</h3>

<p>This function calls the OpenAI Embeddings API to get the multidimensional vector
via text embedding of the input text. This function uses the 'text-embedding-ada-002' model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textEmbedding(text, api_key = Sys.getenv("OPENAI_API_KEY"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textEmbedding_+3A_text">text</code></td>
<td>
<p>A string. The input text to get the embedding for. This should be a character string.</p>
</td></tr>
<tr><td><code id="textEmbedding_+3A_api_key">api_key</code></td>
<td>
<p>A string. The API key for the OpenAI API. Defaults to the value of the environment variable &quot;OPENAI_API_KEY&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector representing the text embeddings.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
Sys.setenv(OPENAI_API_KEY = "Your API key")
textEmbedding("Hello, world!")

## End(Not run)
</code></pre>

<hr>
<h2 id='TextSummary'>Summarize Long Text</h2><span id='topic+TextSummary'></span>

<h3>Description</h3>

<p>This function summarizes a long text using LLM.
The development of this function started with the idea that it might be interesting
to perform a copy-and-paste, sentence summarization and aims to be an evangelist for
copy-and-paste LLM execution. It is recommended to run this function with GPT-4, but it is not cost effective and slow.
This is still an experimental feature.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TextSummary(
  text = clipr::read_clip(),
  nch = 2000,
  verbose = TRUE,
  returnText = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TextSummary_+3A_text">text</code></td>
<td>
<p>A character vector containing the text to be summarized.
If not provided, the function will attempt to read from the clipboard.</p>
</td></tr>
<tr><td><code id="TextSummary_+3A_nch">nch</code></td>
<td>
<p>Integer specifying the number of characters at which to split the input text for processing.</p>
</td></tr>
<tr><td><code id="TextSummary_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag to print the message. Default is TRUE.</p>
</td></tr>
<tr><td><code id="TextSummary_+3A_returntext">returnText</code></td>
<td>
<p>A logical flag to return summarized text results. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Summarize Long Text
</p>


<h3>Value</h3>

<p>The summarized text is placed into the clipboard and the function returns the result of <code>clipr::write_clip</code>.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
TextSummary(text = c("This is a long text to be summarized.",
                     "It spans multiple sentences and goes into much detail."),
            nch = 10)

## End(Not run)
</code></pre>

<hr>
<h2 id='TextSummaryAsBullet'>Summarize Text into Bullet Points</h2><span id='topic+TextSummaryAsBullet'></span>

<h3>Description</h3>

<p>This function takes a text input and summarizes it into a specified number of bullet points.
It can either take the selected code from RStudio or read from the clipboard.
The results are output to your clipboard.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TextSummaryAsBullet(
  Model = "gpt-4-0613",
  temperature = 1,
  verbose = TRUE,
  SelectedCode = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TextSummaryAsBullet_+3A_model">Model</code></td>
<td>
<p>A string specifying the machine learning model to use for text summarization. Default is &quot;gpt-4-0613&quot;.</p>
</td></tr>
<tr><td><code id="TextSummaryAsBullet_+3A_temperature">temperature</code></td>
<td>
<p>A numeric value between 0 and 1 indicating the randomness of the text generation. Default is 1.</p>
</td></tr>
<tr><td><code id="TextSummaryAsBullet_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether to print the summary. Default is FALSE.</p>
</td></tr>
<tr><td><code id="TextSummaryAsBullet_+3A_selectedcode">SelectedCode</code></td>
<td>
<p>A logical value indicating whether to use the selected code in RStudio. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Summarize Selected Text into Bullet Points
</p>


<h3>Value</h3>

<p>The summarized text in bullet points is returned.
</p>


<h3>Author(s)</h3>

<p>Satoshi Kume
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
TextSummaryAsBullet(text = "This is a sample text.")

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
