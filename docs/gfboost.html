<!DOCTYPE html><html lang="en"><head><title>Help for package gfboost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gfboost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CMB'><p>CMB aggregation function</p></a></li>
<li><a href='#CMB.stabpath'><p>CMB stability paths</p></a></li>
<li><a href='#CMB.Stabsel'><p>Loss-adapted Stability Selection</p></a></li>
<li><a href='#CMB3S'><p>Column Measure Boosting with SingBoost and Stability Selection (CMB-3S)</p></a></li>
<li><a href='#CV.CMB3S'><p>Cross-validated version of CMB-3S</p></a></li>
<li><a href='#genDataFromExamples'><p>Data generation</p></a></li>
<li><a href='#LocRank'><p>Localized ranking familty</p></a></li>
<li><a href='#path.singboost'><p>Coefficient paths for SingBoost</p></a></li>
<li><a href='#random.CVind'><p>Cross validation index generator</p></a></li>
<li><a href='#Rank'><p>Hard ranking family</p></a></li>
<li><a href='#RejStep'><p>CMB validation step</p></a></li>
<li><a href='#singboost'><p>SingBoost Boosting method</p></a></li>
<li><a href='#singboost.plot'><p>Plot function for the SingBoost coefficient paths</p></a></li>
<li><a href='#WeakRank'><p>Weak ranking family</p></a></li>
<li><a href='#WeakRankNorm'><p>Weak ranking family (normalized)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Gradient-Free Gradient Boosting</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tino Werner &lt;tino.werner1@uni-oldenburg.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of routines of the author's PhD thesis on gradient-free Gradient Boosting (Werner, Tino (2020) "Gradient-Free Gradient Boosting", URL '<a href="https://oops.uni-oldenburg.de/id/eprint/4290">https://oops.uni-oldenburg.de/id/eprint/4290</a>').</td>
</tr>
<tr>
<td>Depends:</td>
<td>mvtnorm, pcaPP, mboost</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-01-07 08:45:28 UTC; werner</td>
</tr>
<tr>
<td>Author:</td>
<td>Tino Werner [aut, cre, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-01-07 09:10:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='CMB'>CMB aggregation function</h2><span id='topic+CMB'></span>

<h3>Description</h3>

<p>Aggregates the selection frequencies of multiple SingBoost models. May be used with caution since
there are not yet recommendations about good hyperparameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CMB(
  D,
  nsing,
  Bsing = 1,
  alpha = 1,
  singfam = Gaussian(),
  evalfam = Gaussian(),
  sing = FALSE,
  M = 10,
  m_iter = 100,
  kap = 0.1,
  LS = FALSE,
  best = 1,
  wagg,
  robagg = FALSE,
  lower = 0,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CMB_+3A_d">D</code></td>
<td>
<p>Data matrix. Has to be an <code class="reqn">n \times (p+1)-</code>dimensional data frame in the format <code class="reqn">(X,Y)</code>. The <code class="reqn">X-</code>part must not
contain an intercept column containing only ones since this column will be added automatically.</p>
</td></tr>
<tr><td><code id="CMB_+3A_nsing">nsing</code></td>
<td>
<p>Number of observations (rows) used for the SingBoost submodels.</p>
</td></tr>
<tr><td><code id="CMB_+3A_bsing">Bsing</code></td>
<td>
<p>Number of subsamples based on which the SingBoost models are validated. Default is 1. Not to confuse with parameter <code>B</code> for the Stability Selection.</p>
</td></tr>
<tr><td><code id="CMB_+3A_alpha">alpha</code></td>
<td>
<p>Optional real number in <code class="reqn">]0,1]</code>. Defines the fraction of best SingBoost models used in the aggregation step. Default is 1 (use all models).</p>
</td></tr>
<tr><td><code id="CMB_+3A_singfam">singfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are trained based on the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CMB_+3A_evalfam">evalfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are validated according to the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CMB_+3A_sing">sing</code></td>
<td>
<p>If <code>sing=FALSE</code> and the <code>singfam</code> family is a standard Boosting family that is contained in the package
<code>mboost</code>, the CMB aggregation procedure is executed for the corresponding standard Boosting models.</p>
</td></tr>
<tr><td><code id="CMB_+3A_m">M</code></td>
<td>
<p>An integer between 2 and <code>m_iter</code>. Indicates that in every <code class="reqn">M-</code>th iteration, a singular iteration will be
performed. Default is 10.</p>
</td></tr>
<tr><td><code id="CMB_+3A_m_iter">m_iter</code></td>
<td>
<p>Number of SingBoost iterations. Default is 100.</p>
</td></tr>
<tr><td><code id="CMB_+3A_kap">kap</code></td>
<td>
<p>Learning rate (step size). Must be a real number in <code class="reqn">]0,1]</code>. Default is 0.1 It is recommended to use
a value smaller than 0.5.</p>
</td></tr>
<tr><td><code id="CMB_+3A_ls">LS</code></td>
<td>
<p>If a <code>singfamily</code> object that is already provided by <code>mboost</code> is used, the respective Boosting algorithm
will be performed in the singular iterations if <code>Ls</code> is set to <code>TRUE</code>. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="CMB_+3A_best">best</code></td>
<td>
<p>Needed in the case of localized ranking. The parameter <code>K</code> of the localized ranking loss will be
computed by <code class="reqn">best \cdot n</code> (rounded to the next larger integer). Warning: If a parameter <code>K</code> is inserted into the
<code>LocRank</code> family, it will be ignored when executing SingBoost.</p>
</td></tr>
<tr><td><code id="CMB_+3A_wagg">wagg</code></td>
<td>
<p>Type of row weight aggregation. <code>'weights1'</code> indicates that the selection frequencies of the (best)
SingBoost models are averaged. <code>'weights2'</code> respects the validation losses for each model and downweights the ones
with higher validation losses.</p>
</td></tr>
<tr><td><code id="CMB_+3A_robagg">robagg</code></td>
<td>
<p>Optional. If setting <code>robagg=TRUE</code>, the best SingBoost models are ignored when executing the
aggregation to avoid inlier effects. Only reasonable in combination with <code>lower</code>.</p>
</td></tr>
<tr><td><code id="CMB_+3A_lower">lower</code></td>
<td>
<p>Optional argument. Only reasonable when setting <code>robagg=TRUE</code>. <code>lower</code> is a real number in <code class="reqn">[0,1[</code> (a rather
small number is recommended) and indicates that the aggregation ignores the SingBoost models with the best
performances to avoid possible inlier effects.</p>
</td></tr>
<tr><td><code id="CMB_+3A_...">...</code></td>
<td>
<p>Optional further arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SingBoost is designed to detect variables that standard Boosting procedures may not but which may be
relevant w.r.t. the target loss function. However, one may try to stabilize this &rdquo;singular part&rdquo; of the
column measure by aggregating several SingBoost models in the sense that they are evaluated on a validation set
and that the selection frequencies are averaged, maybe in a weighted manner according to the validation losses.
Warning: This procedure does not replace a Stability Selection!
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Column measure</code></td>
<td>
<p>Aggregated column measure as <code class="reqn">(p+1)-</code>dimensional vector.</p>
</td></tr>
<tr><td><code>Selected variables</code></td>
<td>
<p>Names of the variables with positive aggregated column measure.</p>
</td></tr>
<tr><td><code>Variables names</code></td>
<td>
<p>Names of all variables including the intercept.</p>
</td></tr>
<tr><td><code>Row measure</code></td>
<td>
<p>Aggregated row measure as <code class="reqn">n-</code>dimensional vector.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020
</p>


<h3>Examples</h3>

<pre><code class='language-R'>firis&lt;-as.formula(Sepal.Length~.)
Xiris&lt;-model.matrix(firis,iris)
Diris&lt;-data.frame(Xiris[,-1],iris$Sepal.Length)
colnames(Diris)[6]&lt;-"Y"
set.seed(19931023)
cmb1&lt;-CMB(Diris,nsing=100,Bsing=50,alpha=0.8,singfam=Rank(),
evalfam=Rank(),sing=TRUE,M=10,m_iter=100,
kap=0.1,LS=TRUE,wagg='weights1',robagg=FALSE,lower=0)
cmb1
set.seed(19931023)
cmb2&lt;-CMB(Diris,nsing=100,Bsing=50,alpha=0.8,singfam=Rank(),
evalfam=Rank(),sing=TRUE,M=2,m_iter=100,
kap=0.1,LS=TRUE,wagg='weights1',robagg=FALSE,lower=0)
cmb2[[1]]
set.seed(19931023)
cmb3&lt;-CMB(Diris,nsing=100,Bsing=50,alpha=0.8,singfam=Rank(),
evalfam=Rank(),sing=TRUE,M=10,m_iter=100,
kap=0.1,LS=TRUE,wagg='weights2',robagg=FALSE,lower=0)
cmb3[[1]]
</code></pre>

<hr>
<h2 id='CMB.stabpath'>CMB stability paths</h2><span id='topic+CMB.stabpath'></span>

<h3>Description</h3>

<p>Draws a Stability plot for CMB.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CMB.stabpath(
  D,
  nsing,
  Bsing = 1,
  alpha = 1,
  singfam = Gaussian(),
  evalfam = Gaussian(),
  sing = FALSE,
  Mseq,
  m_iter = 100,
  kap = 0.1,
  LS = FALSE,
  best = 1,
  wagg,
  robagg = FALSE,
  lower = 0,
  B,
  ncmb,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CMB.stabpath_+3A_d">D</code></td>
<td>
<p>Data matrix. Has to be an <code class="reqn">n \times (p+1)-</code>dimensional data frame in the format <code class="reqn">(X,Y)</code>. The <code class="reqn">X-</code>part must not
contain an intercept column containing only ones since this column will be added automatically.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_nsing">nsing</code></td>
<td>
<p>Number of observations (rows) used for the SingBoost submodels.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_bsing">Bsing</code></td>
<td>
<p>Number of subsamples based on which the SingBoost models are validated. Default is 1. Not to confuse with parameter <code>B</code> for the Stability Selection.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_alpha">alpha</code></td>
<td>
<p>Optional real number in <code class="reqn">]0,1]</code>. Defines the fraction of best SingBoost models used in the aggregation step. Default is 1 (use all models).</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_singfam">singfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are trained based on the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_evalfam">evalfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are validated according to the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_sing">sing</code></td>
<td>
<p>If <code>sing=FALSE</code> and the <code>singfam</code> family is a standard Boosting family that is contained in the package
<code>mboost</code>, the CMB aggregation procedure is executed for the corresponding standard Boosting models.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_mseq">Mseq</code></td>
<td>
<p>A vector of different values for <code class="reqn">M</code>.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_m_iter">m_iter</code></td>
<td>
<p>Number of SingBoost iterations. Default is 100.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_kap">kap</code></td>
<td>
<p>Learning rate (step size). Must be a real number in <code class="reqn">]0,1]</code>. Default is 0.1 It is recommended to use
a value smaller than 0.5.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_ls">LS</code></td>
<td>
<p>If a <code>singfamily</code> object that is already provided by <code>mboost</code> is used, the respective Boosting algorithm
will be performed in the singular iterations if <code>Ls</code> is set to <code>TRUE</code>. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_best">best</code></td>
<td>
<p>Needed in the case of localized ranking. The parameter <code>K</code> of the localized ranking loss will be
computed by <code class="reqn">best \cdot n</code> (rounded to the next larger integer). Warning: If a parameter <code>K</code> is inserted into the
<code>LocRank</code> family, it will be ignored when executing SingBoost.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_wagg">wagg</code></td>
<td>
<p>Type of row weight aggregation. <code>'weights1'</code> indicates that the selection frequencies of the (best)
SingBoost models are averaged. <code>'weights2'</code> respects the validation losses for each model and downweights the ones
with higher validation losses.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_robagg">robagg</code></td>
<td>
<p>Optional. If setting <code>robagg=TRUE</code>, the best SingBoost models are ignored when executing the
aggregation to avoid inlier effects. Only reasonable in combination with <code>lower</code>.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_lower">lower</code></td>
<td>
<p>Optional argument. Only reasonable when setting <code>robagg=TRUE</code>. <code>lower</code> is a real number in <code class="reqn">[0,1[</code> (a rather
small number is recommended) and indicates that the aggregation ignores the SingBoost models with the best
performances to avoid possible inlier effects.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_b">B</code></td>
<td>
<p>Number of subsamples of size <code class="reqn">n_{cmb}</code> of the training data for CMB aggregation.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_ncmb">ncmb</code></td>
<td>
<p>Number of samples used for <code>CMB</code>. Integer that must be smaller than the number of samples in <code>Dtrain</code>.</p>
</td></tr>
<tr><td><code id="CMB.stabpath_+3A_...">...</code></td>
<td>
<p>Optional further arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>relev</code></td>
<td>
<p>List of relevant variables (represented as their column number).</p>
</td></tr>
<tr><td><code>ind</code></td>
<td>
<p>Vector of relevant variables (represented as their column number).</p>
</td></tr>
</table>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020
</p>

<hr>
<h2 id='CMB.Stabsel'>Loss-adapted Stability Selection</h2><span id='topic+CMB.Stabsel'></span>

<h3>Description</h3>

<p>Workhorse function for the Stability Selection variant where either a grid of thresholds or a grid of
cardinalities is given so that the Boosting models are evaluated on a validation set according to all elements of
the respective grid. The model which performs best is finally selected as stable model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CMB.Stabsel(
  Dtrain,
  nsing,
  Bsing = 1,
  B = 100,
  alpha = 1,
  singfam = Gaussian(),
  evalfam = Gaussian(),
  sing = FALSE,
  M = 10,
  m_iter = 100,
  kap = 0.1,
  LS = FALSE,
  best = 1,
  wagg,
  gridtype,
  grid,
  Dvalid,
  ncmb,
  robagg = FALSE,
  lower = 0,
  singcoef = FALSE,
  Mfinal,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CMB.Stabsel_+3A_dtrain">Dtrain</code></td>
<td>
<p>Data matrix. Has to be an <code class="reqn">n \times (p+1)-</code>dimensional data frame in the format <code class="reqn">(X,Y)</code>. The <code class="reqn">X-</code>part must not
contain an intercept column containing only ones since this column will be added automatically.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_nsing">nsing</code></td>
<td>
<p>Number of observations (rows) used for the SingBoost submodels.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_bsing">Bsing</code></td>
<td>
<p>Number of subsamples based on which the SingBoost models are validated. Default is 1. Not to confuse with parameter <code>B</code> for the Stability Selection.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_b">B</code></td>
<td>
<p>Number of subsamples based on which the CMB models are validated. Default is 100. Not to confuse with <code>Bsing</code> for CMB.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_alpha">alpha</code></td>
<td>
<p>Optional real number in <code class="reqn">]0,1]</code>. Defines the fraction of best SingBoost models used in the aggregation step. Default is 1 (use all models).</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_singfam">singfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are trained based on the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_evalfam">evalfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are validated according to the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_sing">sing</code></td>
<td>
<p>If <code>sing=FALSE</code> and the <code>singfam</code> family is a standard Boosting family that is contained in the package
<code>mboost</code>, the CMB aggregation procedure is executed for the corresponding standard Boosting models.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_m">M</code></td>
<td>
<p>An integer between 2 and <code>m_iter</code>. Indicates that in every <code class="reqn">M-</code>th iteration, a singular iteration will be
performed. Default is 10.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_m_iter">m_iter</code></td>
<td>
<p>Number of SingBoost iterations. Default is 100.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_kap">kap</code></td>
<td>
<p>Learning rate (step size). Must be a real number in <code class="reqn">]0,1]</code>. Default is 0.1 It is recommended to use
a value smaller than 0.5.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_ls">LS</code></td>
<td>
<p>If a <code>singfamily</code> object that is already provided by <code>mboost</code> is used, the respective Boosting algorithm
will be performed in the singular iterations if <code>Ls</code> is set to <code>TRUE</code>. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_best">best</code></td>
<td>
<p>Needed in the case of localized ranking. The parameter <code>K</code> of the localized ranking loss will be
computed by <code class="reqn">best \cdot n</code> (rounded to the next larger integer). Warning: If a parameter <code>K</code> is inserted into the
<code>LocRank</code> family, it will be ignored when executing SingBoost.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_wagg">wagg</code></td>
<td>
<p>Type of row weight aggregation. <code>'weights1'</code> indicates that the selection frequencies of the (best)
SingBoost models are averaged. <code>'weights2'</code> respects the validation losses for each model and downweights the ones
with higher validation losses.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_gridtype">gridtype</code></td>
<td>
<p>Choose between <code>'pigrid'</code> and <code>'qgrid'</code>.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_grid">grid</code></td>
<td>
<p>The grid for the thresholds (in <code class="reqn">]0,1]</code>) or the numbers of final variables (positive integers).</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_dvalid">Dvalid</code></td>
<td>
<p>Validation data for selecting the optimal element of the grid and with it the best corresponding model.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_ncmb">ncmb</code></td>
<td>
<p>Number of samples used for <code>CMB</code>. Integer that must be smaller than the number of samples in <code>Dtrain</code> and higher than <code>nsing</code>.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_robagg">robagg</code></td>
<td>
<p>Optional. If setting <code>robagg=TRUE</code>, the best SingBoost models are ignored when executing the
aggregation to avoid inlier effects. Only reasonable in combination with <code>lower</code>.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_lower">lower</code></td>
<td>
<p>Optional argument. Only reasonable when setting <code>robagg=TRUE</code>. <code>lower</code> is a real number in <code class="reqn">[0,1[</code> (a rather
small number is recommended) and indicates that the aggregation ignores the SingBoost models with the best
performances to avoid possible inlier effects.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_singcoef">singcoef</code></td>
<td>
<p>Default is <code>FALSE</code>. Then the coefficients for the candidate stable models are computed by standard
linear regression (provided that the number of columns is smaller than the number of samples in the training set
for each grid element). If set to <code>TRUE</code>, the coefficients are computed by SingBoost.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_mfinal">Mfinal</code></td>
<td>
<p>Optional. Necessary if <code>singcoef=TRUE</code> to determine the frequency of singular iterations in the
SingBoost models.</p>
</td></tr>
<tr><td><code id="CMB.Stabsel_+3A_...">...</code></td>
<td>
<p>Optional further arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Stability Selection in the packages <code>stabs</code> and <code>mboost</code> requires to fix two of three parameters which are
the per-family error rate, the threshold and the number of variables which have to be selected in each model. Our
Stability Selection is based on another idea. We also train Boosting models on subsamples but we use a validation
step to determine the size of the optimal model. More precisely, if  <code>'pigrid'</code> is used as <code>gridtype</code>, the corresponding
stable models for each threshold are computed by selecting all variables whose aggregated selection frequency exceeds
the threshold. Then, these candidate stable models are validated according to the target loss function (inserted
through <code>evalfam</code>) and the optimal one is finally selected. If <code>'qgrid'</code> is used as <code>gridtype</code>, a vector of positive
integers has to be entered instead of a vector of thresholds. The candidate stable models then consist of the best
variables ordered by their aggregated selection frequencies, respectively. The validation step is the same.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>colind.opt</code></td>
<td>
<p>The column numbers of the variables that form the best stable model as a vector.</p>
</td></tr>
<tr><td><code>coeff.opt</code></td>
<td>
<p>The coefficients corresponding to the optimal stable model as a vector.</p>
</td></tr>
<tr><td><code>aggnu</code></td>
<td>
<p>Aggregated empirical column measure (i.e., selection frequencies) as a vector.</p>
</td></tr>
<tr><td><code>aggzeta</code></td>
<td>
<p>Aggregated empirical row measure (i.e., row weights) as a vector.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020
</p>
<p>T. Hothorn, P. Bühlmann, T. Kneib, M. Schmid, and B. Hofner. mboost: Model-Based
Boosting, 2017
</p>
<p>B. Hofner and T. Hothorn. stabs: Stability Selection with Error Control, 2017.
</p>
<p>B. Hofner, L. Boccuto, and M. Göker. Controlling false discoveries in high-dimensional
situations: Boosting with stability selection. BMC Bioinformatics, 16(1):144, 2015.
</p>
<p>N. Meinshausen and P. Bühlmann. Stability selection. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(4):417–473, 2010.
</p>

<hr>
<h2 id='CMB3S'>Column Measure Boosting with SingBoost and Stability Selection (CMB-3S)</h2><span id='topic+CMB3S'></span>

<h3>Description</h3>

<p>Executes CMB and the loss-based Stability Selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CMB3S(
  Dtrain,
  nsing,
  Bsing = 1,
  B = 100,
  alpha = 1,
  singfam = Gaussian(),
  evalfam = Gaussian(),
  sing = FALSE,
  M = 10,
  m_iter = 100,
  kap = 0.1,
  LS = FALSE,
  best = 1,
  wagg,
  gridtype,
  grid,
  Dvalid,
  ncmb,
  robagg = FALSE,
  lower = 0,
  singcoef = FALSE,
  Mfinal = 10,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CMB3S_+3A_dtrain">Dtrain</code></td>
<td>
<p>Data matrix. Has to be an <code class="reqn">n \times (p+1)-</code>dimensional data frame in the format <code class="reqn">(X,Y)</code>. The <code class="reqn">X-</code>part must not
contain an intercept column containing only ones since this column will be added automatically.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_nsing">nsing</code></td>
<td>
<p>Number of observations (rows) used for the SingBoost submodels.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_bsing">Bsing</code></td>
<td>
<p>Number of subsamples based on which the SingBoost models are validated. Default is 1. Not to confuse with parameter <code>B</code> for the Stability Selection.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_b">B</code></td>
<td>
<p>Number of subsamples based on which the CMB models are validated. Default is 100. Not to confuse with <code>Bsing</code> for CMB.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_alpha">alpha</code></td>
<td>
<p>Optional real number in <code class="reqn">]0,1]</code>. Defines the fraction of best SingBoost models used in the aggregation step. Default is 1 (use all models).</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_singfam">singfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are trained based on the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_evalfam">evalfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are validated according to the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_sing">sing</code></td>
<td>
<p>If <code>sing=FALSE</code> and the <code>singfam</code> family is a standard Boosting family that is contained in the package
<code>mboost</code>, the CMB aggregation procedure is executed for the corresponding standard Boosting models.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_m">M</code></td>
<td>
<p>An integer between 2 and <code>m_iter</code>. Indicates that in every <code class="reqn">M-</code>th iteration, a singular iteration will be
performed. Default is 10.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_m_iter">m_iter</code></td>
<td>
<p>Number of SingBoost iterations. Default is 100.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_kap">kap</code></td>
<td>
<p>Learning rate (step size). Must be a real number in <code class="reqn">]0,1]</code>. Default is 0.1 It is recommended to use
a value smaller than 0.5.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_ls">LS</code></td>
<td>
<p>If a <code>singfamily</code> object that is already provided by <code>mboost</code> is used, the respective Boosting algorithm
will be performed in the singular iterations if <code>Ls</code> is set to <code>TRUE</code>. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_best">best</code></td>
<td>
<p>Needed in the case of localized ranking. The parameter <code>K</code> of the localized ranking loss will be
computed by <code class="reqn">best \cdot n</code> (rounded to the next larger integer). Warning: If a parameter <code>K</code> is inserted into the
<code>LocRank</code> family, it will be ignored when executing SingBoost.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_wagg">wagg</code></td>
<td>
<p>Type of row weight aggregation. <code>'weights1'</code> indicates that the selection frequencies of the (best)
SingBoost models are averaged. <code>'weights2'</code> respects the validation losses for each model and downweights the ones
with higher validation losses.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_gridtype">gridtype</code></td>
<td>
<p>Choose between <code>'pigrid'</code> and <code>'qgrid'</code>.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_grid">grid</code></td>
<td>
<p>The grid for the thresholds (in <code class="reqn">]0,1]</code>) or the numbers of final variables (positive integers).</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_dvalid">Dvalid</code></td>
<td>
<p>Validation data for selecting the optimal element of the grid and with it the best corresponding model.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_ncmb">ncmb</code></td>
<td>
<p>Number of samples used for <code>CMB</code>. Integer that must be smaller than the number of samples in <code>Dtrain</code>.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_robagg">robagg</code></td>
<td>
<p>Optional. If setting <code>robagg=TRUE</code>, the best SingBoost models are ignored when executing the
aggregation to avoid inlier effects. Only reasonable in combination with <code>lower</code>.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_lower">lower</code></td>
<td>
<p>Optional argument. Only reasonable when setting <code>robagg=TRUE</code>. <code>lower</code> is a real number in <code class="reqn">[0,1[</code> (a rather
small number is recommended) and indicates that the aggregation ignores the SingBoost models with the best
performances to avoid possible inlier effects.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_singcoef">singcoef</code></td>
<td>
<p>Default is <code>FALSE</code>. Then the coefficients for the candidate stable models are computed by standard
linear regression (provided that the number of columns is smaller than the number of samples in the training set
for each grid element). If set to <code>TRUE</code>, the coefficients are computed by SingBoost.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_mfinal">Mfinal</code></td>
<td>
<p>Optional. Necessary if <code>singcoef=TRUE</code> to determine the frequency of singular iterations in the
SingBoost models.</p>
</td></tr>
<tr><td><code id="CMB3S_+3A_...">...</code></td>
<td>
<p>Optional further arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code>CMB</code> and <code>CMB.Stabsel</code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Final coefficients</code></td>
<td>
<p>The coefficients corresponding to the optimal stable model as a vector.</p>
</td></tr>
<tr><td><code>Stable column measure</code></td>
<td>
<p>Aggregated empirical column measure (i.e., selection frequencies) as a vector.</p>
</td></tr>
<tr><td><code>Selected columns</code></td>
<td>
<p>The column numbers of the variables that form the best stable model as a vector.</p>
</td></tr>
<tr><td><code>Used row measure</code></td>
<td>
<p>Aggregated empirical row measure (i.e., row weights) as a vector.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020
</p>
<p>T. Hothorn, P. Bühlmann, T. Kneib, M. Schmid, and B. Hofner. mboost: Model-Based
Boosting, 2017
</p>
<p>B. Hofner and T. Hothorn. stabs: Stability Selection with Error Control, 2017.
</p>
<p>B. Hofner, L. Boccuto, and M. Göker. Controlling false discoveries in high-dimensional
situations: Boosting with stability selection. BMC Bioinformatics, 16(1):144, 2015.
</p>
<p>N. Meinshausen and P. Bühlmann. Stability selection. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(4):417–473, 2010.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>firis&lt;-as.formula(Sepal.Length~.)
Xiris&lt;-model.matrix(firis,iris)
Diris&lt;-data.frame(Xiris[,-1],iris$Sepal.Length)
colnames(Diris)[6]&lt;-"Y"
set.seed(19931023)
ind&lt;-sample(1:150,120,replace=FALSE)
Dtrain&lt;-Diris[ind,]
Dvalid&lt;-Diris[-ind,]
set.seed(19931023)
cmb3s&lt;-CMB3S(Dtrain,nsing=120,Dvalid=Dvalid,ncmb=120,Bsing=1,B=1,alpha=1,singfam=Gaussian()
,evalfam=Gaussian(),sing=FALSE,M=10,m_iter=100,kap=0.1,LS=FALSE,wagg='weights1',
gridtype='pigrid',grid=seq(0.8,0.9,1),robagg=FALSE,lower=0,singcoef=TRUE,Mfinal=10)
cmb3s$Fin
cmb3s$Stab
cmb3s$Sel
glmres4&lt;-glmboost(Sepal.Length~.,iris[ind,])
coef(glmres4)
set.seed(19931023)
cmb3s1&lt;-CMB3S(Dtrain,nsing=80,Dvalid=Dvalid,ncmb=100,Bsing=10,B=100,alpha=0.5,singfam=Gaussian(),
evalfam=Gaussian(),sing=FALSE,M=10,m_iter=100,kap=0.1,LS=FALSE,wagg='weights1',gridtype='pigrid',
grid=seq(0.8,0.9,1),robagg=FALSE,lower=0,singcoef=TRUE,Mfinal=10)
cmb3s1$Fin
cmb3s1$Stab
## This will may take around a minute
set.seed(19931023)
cmb3s2&lt;-CMB3S(Dtrain,nsing=80,Dvalid=Dvalid,ncmb=100,Bsing=10,B=100,alpha=0.5,singfam=Rank(),
evalfam=Rank(),sing=TRUE,M=10,m_iter=100,kap=0.1,LS=TRUE,wagg='weights2',gridtype='pigrid',
grid=seq(0.8,0.9,1),robagg=FALSE,lower=0,singcoef=TRUE,Mfinal=10)
cmb3s2$Fin
cmb3s2$Stab
set.seed(19931023)
cmb3s3&lt;-CMB3S(Dtrain,nsing=80,Dvalid=Dvalid,ncmb=100,Bsing=10,B=100,alpha=0.5,singfam=Huber(),
evalfam=Huber(),sing=FALSE,M=10,m_iter=100,kap=0.1,LS=FALSE,wagg='weights2',gridtype='pigrid',
grid=seq(0.8,0.9,1),robagg=FALSE,lower=0,singcoef=FALSE,Mfinal=10)
cmb3s3$Fin
cmb3s3$Stab

</code></pre>

<hr>
<h2 id='CV.CMB3S'>Cross-validated version of CMB-3S</h2><span id='topic+CV.CMB3S'></span>

<h3>Description</h3>

<p>Cross-validates the whole loss-based Stability Selection by aggregating several stable models according
to their performance on validation sets. Also computes a cross-validated test loss on a disjoint test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CV.CMB3S(
  D,
  nsing,
  Bsing = 1,
  B = 100,
  alpha = 1,
  singfam = Gaussian(),
  evalfam = Gaussian(),
  sing = FALSE,
  M = 10,
  m_iter = 100,
  kap = 0.1,
  LS = FALSE,
  best = 1,
  wagg,
  gridtype,
  grid,
  ncmb,
  CVind,
  targetfam = Gaussian(),
  print = TRUE,
  robagg = FALSE,
  lower = 0,
  singcoef = FALSE,
  Mfinal = 10,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CV.CMB3S_+3A_d">D</code></td>
<td>
<p>Data matrix. Has to be an <code class="reqn">n \times (p+1)-</code>dimensional data frame in the format <code class="reqn">(X,Y)</code>. The <code class="reqn">X-</code>part must not
contain an intercept column containing only ones since this column will be added automatically.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_nsing">nsing</code></td>
<td>
<p>Number of observations (rows) used for the SingBoost submodels.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_bsing">Bsing</code></td>
<td>
<p>Number of subsamples based on which the SingBoost models are validated. Default is 1. Not to confuse with parameter <code>B</code> for the Stability Selection.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_b">B</code></td>
<td>
<p>Number of subsamples based on which the CMB models are validated. Default is 100. Not to confuse with <code>Bsing</code> for CMB.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_alpha">alpha</code></td>
<td>
<p>Optional real number in <code class="reqn">]0,1]</code>. Defines the fraction of best SingBoost models used in the aggregation step. Default is 1 (use all models).</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_singfam">singfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are trained based on the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_evalfam">evalfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are validated according to the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_sing">sing</code></td>
<td>
<p>If <code>sing=FALSE</code> and the <code>singfam</code> family is a standard Boosting family that is contained in the package
<code>mboost</code>, the CMB aggregation procedure is executed for the corresponding standard Boosting models.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_m">M</code></td>
<td>
<p>An integer between 2 and <code>m_iter</code>. Indicates that in every <code class="reqn">M-</code>th iteration, a singular iteration will be
performed. Default is 10.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_m_iter">m_iter</code></td>
<td>
<p>Number of SingBoost iterations. Default is 100.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_kap">kap</code></td>
<td>
<p>Learning rate (step size). Must be a real number in <code class="reqn">]0,1]</code>. Default is 0.1 It is recommended to use
a value smaller than 0.5.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_ls">LS</code></td>
<td>
<p>If a <code>singfamily</code> object that is already provided by <code>mboost</code> is used, the respective Boosting algorithm
will be performed in the singular iterations if <code>Ls</code> is set to <code>TRUE</code>. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_best">best</code></td>
<td>
<p>Needed in the case of localized ranking. The parameter <code>K</code> of the localized ranking loss will be
computed by <code class="reqn">best \cdot n</code> (rounded to the next larger integer). Warning: If a parameter <code>K</code> is inserted into the
<code>LocRank</code> family, it will be ignored when executing SingBoost.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_wagg">wagg</code></td>
<td>
<p>Type of row weight aggregation. <code>'weights1'</code> indicates that the selection frequencies of the (best)
SingBoost models are averaged. <code>'weights2'</code> respects the validation losses for each model and downweights the ones
with higher validation losses.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_gridtype">gridtype</code></td>
<td>
<p>Choose between <code>'pigrid'</code> and <code>'qgrid'</code>.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_grid">grid</code></td>
<td>
<p>The grid for the thresholds (in <code class="reqn">]0,1]</code>) or the numbers of final variables (positive integers).</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_ncmb">ncmb</code></td>
<td>
<p>Number of samples used for <code>CMB</code>. Integer that must be smaller than the number of samples in <code>D</code>.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_cvind">CVind</code></td>
<td>
<p>A list where each element contains a vector on length <code class="reqn">n</code> (number of samples in the data matrix <code>D</code>) which
contains the strings <code>'tr'</code> (training set), <code>'v'</code> (validation set) and <code>'te'</code> (test set). This list can be easily
generated using the function <code>random_CVind</code>.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_targetfam">targetfam</code></td>
<td>
<p>Target loss. Should be the same family as <code>evalfam</code>. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_print">print</code></td>
<td>
<p>If set to <code>TRUE</code> (default), the number of the currently finished outer cross-validation loop is printed.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_robagg">robagg</code></td>
<td>
<p>Optional. If setting <code>robagg=TRUE</code>, the best SingBoost models are ignored when executing the
aggregation to avoid inlier effects. Only reasonable in combination with <code>lower</code>.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_lower">lower</code></td>
<td>
<p>Optional argument. Only reasonable when setting <code>robagg=TRUE</code>. <code>lower</code> is a real number in <code class="reqn">[0,1[</code> (a rather
small number is recommended) and indicates that the aggregation ignores the SingBoost models with the best
performances to avoid possible inlier effects.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_singcoef">singcoef</code></td>
<td>
<p>Default is <code>FALSE</code>. Then the coefficients for the candidate stable models are computed by standard
linear regression (provided that the number of columns is smaller than the number of samples in the training set
for each grid element). If set to <code>TRUE</code>, the coefficients are computed by SingBoost.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_mfinal">Mfinal</code></td>
<td>
<p>Optional. Necessary if <code>singcoef=TRUE</code> to determine the frequency of singular iterations in the
SingBoost models.</p>
</td></tr>
<tr><td><code id="CV.CMB3S_+3A_...">...</code></td>
<td>
<p>Optional further arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In <code>CMB3S</code>, a validation set is given based on which the optimal stable model is chosen. The <code>CV.CMB3S</code>
function adds an outer cross-validation step such that both the training and the validation data sets (and
optionally the test data sets) are chosen randomly by disjointly dividing the initial data set. The aggregated
stable models form an &rdquo;ultra-stable&rdquo; model. It is strongly recommended to use this function is a parallelized
manner due to huge computation time.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Cross-validated loss</code></td>
<td>
<p>A vector containing the cross-validated test losses.</p>
</td></tr>
<tr><td><code>Ultra-stable column measure</code></td>
<td>
<p>A vector containing the aggregated selection frequencies of the stable models.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020
</p>

<hr>
<h2 id='genDataFromExamples'>Data generation</h2><span id='topic+genDataFromExamples'></span>

<h3>Description</h3>

<p>Auxiliary function for generating simple artificial data sets with normally distributed coefficients
and regressors. Note that we only report this function for reproducibility of the simulations from the PhD thesis of the author.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genDataFromExamples(
  p,
  n,
  s = 1,
  xmean = 0,
  betamean = 0,
  betasd = 1,
  snr = 2,
  rho = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="genDataFromExamples_+3A_p">p</code></td>
<td>
<p>Number of variables (columns).</p>
</td></tr>
<tr><td><code id="genDataFromExamples_+3A_n">n</code></td>
<td>
<p>Number of observations (rows).</p>
</td></tr>
<tr><td><code id="genDataFromExamples_+3A_s">s</code></td>
<td>
<p>Sparsity. Real number between 0 and 1. <code>s=1</code> (default) leads to a coefficient vector without zero entries.</p>
</td></tr>
<tr><td><code id="genDataFromExamples_+3A_xmean">xmean</code></td>
<td>
<p>Mean of each of the normally distributed columns. Default is 0.</p>
</td></tr>
<tr><td><code id="genDataFromExamples_+3A_betamean">betamean</code></td>
<td>
<p>Mean of each of the normally distributed coefficients. Default is 0.</p>
</td></tr>
<tr><td><code id="genDataFromExamples_+3A_betasd">betasd</code></td>
<td>
<p>Standard deviation of the normally distributed coefficients. Default is 1.</p>
</td></tr>
<tr><td><code id="genDataFromExamples_+3A_snr">snr</code></td>
<td>
<p>Signal to noise ratio. Real number greater than zero. Default is 2.</p>
</td></tr>
<tr><td><code id="genDataFromExamples_+3A_rho">rho</code></td>
<td>
<p>Parameter for a Toeplitz covariance structure of the regressors. Real number between -1 and 1. Default is
0 which corresponds to uncorrelated columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>D</code></td>
<td>
<p>Data matrix <code class="reqn">(X,Y)</code>.</p>
</td></tr>
<tr><td><code>vars</code></td>
<td>
<p>A list of the relevant variables.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>genDataFromExamples(10,25,0.3)
</code></pre>

<hr>
<h2 id='LocRank'>Localized ranking familty</h2><span id='topic+LocRank'></span>

<h3>Description</h3>

<p>Gradient-free Gradient Boosting family for the localized ranking loss function including its fast
computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LocRank(K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="LocRank_+3A_k">K</code></td>
<td>
<p>Indicates that we are interesting in the top <code class="reqn">K</code> instances and their correct ordering. Must be an integer
between 1 and the number <code class="reqn">n</code> of observations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The localized ranking loss combines the hard and the weak ranking loss, i.e., it penalizes misrankings at
the top of the list (the best <code class="reqn">K</code> instances according to the response value) and &rdquo;misclassification&rdquo; in the sense
that instances belonging to the top of the list are ranked lower and vice versa. The localized ranking loss already
returns a normalized loss that can take values between 0 and 1. <code>LocRank</code> returns a family object
as in the package <code>mboost</code>.
</p>


<h3>Value</h3>

<p>A Boosting family object
</p>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020,
Equation (5.2.5)
</p>
<p>T. Hothorn, P. Bühlmann, T. Kneib, M. Schmid, and B. Hofner. mboost: Model-Based
Boosting, 2017
</p>


<h3>Examples</h3>

<pre><code class='language-R'>{y&lt;-c(-3, 10.3,-8, 12, 14,-0.5, 29,-1.1,-5.7, 119)
 yhat&lt;-c(0.02, 0.6, 0.1, 0.47, 0.82, 0.04, 0.77, 0.09, 0.01, 0.79)
 LocRank(4)@risk(y,yhat)}
{y&lt;-c(-3, 10.3,-8, 12, 14,-0.5, 29,-1.1,-5.7, 119)
 yhat&lt;-c(0.02, 0.6, 0.1, 0.47, 0.82, 0.04, 0.77, 0.09, 0.01, 0.79)
 LocRank(5)@risk(y,yhat)}
</code></pre>

<hr>
<h2 id='path.singboost'>Coefficient paths for SingBoost</h2><span id='topic+path.singboost'></span>

<h3>Description</h3>

<p>Runs SingBoost but saves the coefficients paths. If no coefficient path plot is needed, just use
<code>singboost</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>path.singboost(
  D,
  M = 10,
  m_iter = 100,
  kap = 0.1,
  singfamily = Gaussian(),
  best = 1,
  LS = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="path.singboost_+3A_d">D</code></td>
<td>
<p>Data matrix. Has to be an <code class="reqn">n \times (p+1)-</code>dimensional data frame in the format <code class="reqn">(X,Y)</code>. The <code class="reqn">X-</code>part must not
contain an intercept column containing only ones since this column will be added automatically.</p>
</td></tr>
<tr><td><code id="path.singboost_+3A_m">M</code></td>
<td>
<p>An integer between 2 and <code>m_iter</code>. Indicates that in every <code class="reqn">M-</code>th iteration, a singular iteration will be
performed. Default is 10.</p>
</td></tr>
<tr><td><code id="path.singboost_+3A_m_iter">m_iter</code></td>
<td>
<p>Number of SingBoost iterations. Default is 100.</p>
</td></tr>
<tr><td><code id="path.singboost_+3A_kap">kap</code></td>
<td>
<p>Learning rate (step size). Must be a real number in <code class="reqn">]0,1]</code>. Default is 0.1 It is recommended to use
a value smaller than 0.5.</p>
</td></tr>
<tr><td><code id="path.singboost_+3A_singfamily">singfamily</code></td>
<td>
<p>A Boosting family corresponding to the target loss function. See .<code>mboost</code> for families
corresponding to standard loss functions. May also use the loss functions for ranking losses provided in this
package. Default is <code>Gaussian()</code> for which SingBoost is just standard <code class="reqn">L_2-</code>Boosting.</p>
</td></tr>
<tr><td><code id="path.singboost_+3A_best">best</code></td>
<td>
<p>Needed in the case of localized ranking. The parameter <code>K</code> of the localized ranking loss will be
computed by <code class="reqn">best \cdot n</code> (rounded to the next larger integer). Warning: If a parameter <code>K</code> is inserted into the
<code>LocRank</code> family, it will be ignored when executing SingBoost.</p>
</td></tr>
<tr><td><code id="path.singboost_+3A_ls">LS</code></td>
<td>
<p>If a <code>singfamily</code> object that is already provided by <code>mboost</code> is used, the respective Boosting algorithm
will be performed in the singular iterations if <code>Ls</code> is set to <code>TRUE</code>. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Selected variables</code></td>
<td>
<p>Names of the selected variables.</p>
</td></tr>
<tr><td><code>Coefficients</code></td>
<td>
<p>The selected coefficients as an <code class="reqn">(p+1)-</code>dimensional vector (i.e., including the zeroes).</p>
</td></tr>
<tr><td><code>Freqs</code></td>
<td>
<p>Selection frequencies and a matrix for intercept and coefficient paths, respectively.</p>
</td></tr>
<tr><td><code>Intercept path</code></td>
<td>
<p>The intercept path as an <code class="reqn">m_{iter}-</code>dimensional vector.</p>
</td></tr>
<tr><td><code>Coefficient path</code></td>
<td>
<p>The coefficient paths as a <code class="reqn">2 \cdot m_{iter} \times 2-</code>dimensional matrix.</p>
</td></tr>
</table>

<hr>
<h2 id='random.CVind'>Cross validation index generator</h2><span id='topic+random.CVind'></span>

<h3>Description</h3>

<p>Simple auxiliary function for randomly generating the indices for training, validation and test data
for cross validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random.CVind(n, ncmb, nval, CV)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="random.CVind_+3A_n">n</code></td>
<td>
<p>Number of observations (rows).</p>
</td></tr>
<tr><td><code id="random.CVind_+3A_ncmb">ncmb</code></td>
<td>
<p>Number of training samples for the SingBoost models in CMB. Must be an integer between 1 and <code class="reqn">n</code>.</p>
</td></tr>
<tr><td><code id="random.CVind_+3A_nval">nval</code></td>
<td>
<p>Number of validation samples in the CMB aggregation procedure. Must be an integer between 1 and <code class="reqn">n-n_{cmb}-1</code>.</p>
</td></tr>
<tr><td><code id="random.CVind_+3A_cv">CV</code></td>
<td>
<p>Number of cross validation steps. Must be a positive integer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data set consists of $n$ observations. <code class="reqn">n_{cmb}</code> of them are used for the CMB aggregation procedure.
Note that within CMB itself, only a subset of these observations may be used for SingBoost training. The Stability
Selection is based on the validation set consisting of <code class="reqn">n_{val}</code> observations. The cross-validated loss of the
final model is evaluated on the test data set with <code class="reqn">n-n_{cmb}-n_{val}</code> observations. Clearly, all data sets need to
be disjoint.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>CVind</code></td>
<td>
<p>List of row indices for training, validation and test data for each cross validation loop.</p>
</td></tr>
</table>

<hr>
<h2 id='Rank'>Hard ranking family</h2><span id='topic+Rank'></span>

<h3>Description</h3>

<p>Gradient-free Gradient Boosting family for the hard ranking loss function including its fast computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Rank()
</code></pre>


<h3>Details</h3>

<p>The hard ranking loss is used to compare different orderings, usually the true ordering of instances of a data
set according to their responses with the predicted counterparts. The usage of the <code>pcaPP</code> package avoids the
cumbersome computation that would require </p>
<p style="text-align: center;"><code class="reqn">frac{n}{2(n-1)}</code>
</p>
<p> comparisons. <code>Rank</code> returns a family object
as in the package <code>mboost</code>.
</p>


<h3>Value</h3>

<p>A Boosting family object
</p>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020,
Equations (5.2.2) and (5.2.3)
</p>
<p>T. Hothorn, P. Bühlmann, T. Kneib, M. Schmid, and B. Hofner. mboost: Model-Based
Boosting, 2017
</p>


<h3>Examples</h3>

<pre><code class='language-R'>{y&lt;-c(-3, 10.3,-8, 12, 14,-0.5, 29,-1.1,-5.7, 119)
 yhat&lt;-c(0.02, 0.6, 0.1, 0.47, 0.82, 0.04, 0.77, 0.09, 0.01, 0.79)
 Rank()@risk(y,yhat)}
{x&lt;-1:6
z&lt;-6:1
Rank()@risk(x,z)}
{x&lt;-1:6
z&lt;-1:6
Rank()@risk(x,z)}
</code></pre>

<hr>
<h2 id='RejStep'>CMB validation step</h2><span id='topic+RejStep'></span>

<h3>Description</h3>

<p>Validation step to combine different SingBoost models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RejStep(
  D,
  nsing,
  Bsing = 1,
  ind,
  sing = FALSE,
  singfam = Gaussian(),
  evalfam = Gaussian(),
  M = 10,
  m_iter = 100,
  kap = 0.1,
  LS = FALSE,
  best = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RejStep_+3A_d">D</code></td>
<td>
<p>Data matrix. Has to be an <code class="reqn">n \times (p+1)-</code>dimensional data frame in the format <code class="reqn">(X,Y)</code>. The <code class="reqn">X-</code>part must not
contain an intercept column containing only ones since this column will be added automatically.</p>
</td></tr>
<tr><td><code id="RejStep_+3A_nsing">nsing</code></td>
<td>
<p>Number of observations (rows) used for the SingBoost submodels.</p>
</td></tr>
<tr><td><code id="RejStep_+3A_bsing">Bsing</code></td>
<td>
<p>Number of subsamples based on which the SingBoost models are validated. Default is 1. Not to confuse with parameter <code>B</code> for the Stability Selection.</p>
</td></tr>
<tr><td><code id="RejStep_+3A_ind">ind</code></td>
<td>
<p>Vector with indices for dividing the data set into training and validation data.</p>
</td></tr>
<tr><td><code id="RejStep_+3A_sing">sing</code></td>
<td>
<p>If <code>sing=FALSE</code> and the <code>singfam</code> family is a standard Boosting family that is contained in the package
<code>mboost</code>, the CMB aggregation procedure is executed for the corresponding standard Boosting models.</p>
</td></tr>
<tr><td><code id="RejStep_+3A_singfam">singfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are trained based on the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="RejStep_+3A_evalfam">evalfam</code></td>
<td>
<p>A SingBoost family. The SingBoost models are validated according to the corresponding loss function. Default is <code>Gaussian()</code> (squared loss).</p>
</td></tr>
<tr><td><code id="RejStep_+3A_m">M</code></td>
<td>
<p>An integer between 2 and <code>m_iter</code>. Indicates that in every <code class="reqn">M-</code>th iteration, a singular iteration will be
performed. Default is 10.</p>
</td></tr>
<tr><td><code id="RejStep_+3A_m_iter">m_iter</code></td>
<td>
<p>Number of SingBoost iterations. Default is 100.</p>
</td></tr>
<tr><td><code id="RejStep_+3A_kap">kap</code></td>
<td>
<p>Learning rate (step size). Must be a real number in <code class="reqn">]0,1]</code>. Default is 0.1 It is recommended to use
a value smaller than 0.5.</p>
</td></tr>
<tr><td><code id="RejStep_+3A_ls">LS</code></td>
<td>
<p>If a <code>singfamily</code> object that is already provided by <code>mboost</code> is used, the respective Boosting algorithm
will be performed in the singular iterations if <code>Ls</code> is set to <code>TRUE</code>. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="RejStep_+3A_best">best</code></td>
<td>
<p>Needed in the case of localized ranking. The parameter <code>K</code> of the localized ranking loss will be
computed by <code class="reqn">best \cdot n</code> (rounded to the next larger integer). Warning: If a parameter <code>K</code> is inserted into the
<code>LocRank</code> family, it will be ignored when executing SingBoost.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Divides the data set into a training and a validation set. The SingBoost models are computed on the training
set and evaluated on the validation set based on the loss function corresponding to the selected Boosting family.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>loss</code></td>
<td>
<p>Vector of validation losses.</p>
</td></tr>
<tr><td><code>occ</code></td>
<td>
<p>Selection frequencies for each Boosting model.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020
</p>

<hr>
<h2 id='singboost'>SingBoost Boosting method</h2><span id='topic+singboost'></span>

<h3>Description</h3>

<p>SingBoost is a Boosting method that can deal with complicated loss functions that do not allow for
a gradient. SingBoost is based on L2-Boosting in its current implementation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>singboost(
  D,
  M = 10,
  m_iter = 100,
  kap = 0.1,
  singfamily = Gaussian(),
  best = 1,
  LS = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="singboost_+3A_d">D</code></td>
<td>
<p>Data matrix. Has to be an <code class="reqn">n \times (p+1)-</code>dimensional data frame in the format <code class="reqn">(X,Y)</code>. The <code class="reqn">X-</code>part must not
contain an intercept column containing only ones since this column will be added automatically.</p>
</td></tr>
<tr><td><code id="singboost_+3A_m">M</code></td>
<td>
<p>An integer between 2 and <code>m_iter</code>. Indicates that in every <code class="reqn">M-</code>th iteration, a singular iteration will be
performed. Default is 10.</p>
</td></tr>
<tr><td><code id="singboost_+3A_m_iter">m_iter</code></td>
<td>
<p>Number of SingBoost iterations. Default is 100.</p>
</td></tr>
<tr><td><code id="singboost_+3A_kap">kap</code></td>
<td>
<p>Learning rate (step size). Must be a real number in <code class="reqn">]0,1]</code>. Default is 0.1 It is recommended to use
a value smaller than 0.5.</p>
</td></tr>
<tr><td><code id="singboost_+3A_singfamily">singfamily</code></td>
<td>
<p>A Boosting family corresponding to the target loss function. See .<code>mboost</code> for families
corresponding to standard loss functions. May also use the loss functions for ranking losses provided in this
package. Default is <code>Gaussian()</code> for which SingBoost is just standard <code class="reqn">L_2-</code>Boosting.</p>
</td></tr>
<tr><td><code id="singboost_+3A_best">best</code></td>
<td>
<p>Needed in the case of localized ranking. The parameter <code>K</code> of the localized ranking loss will be
computed by <code class="reqn">best \cdot n</code> (rounded to the next larger integer). Warning: If a parameter <code>K</code> is inserted into the
<code>LocRank</code> family, it will be ignored when executing SingBoost.</p>
</td></tr>
<tr><td><code id="singboost_+3A_ls">LS</code></td>
<td>
<p>If a <code>singfamily</code> object that is already provided by <code>mboost</code> is used, the respective Boosting algorithm
will be performed in the singular iterations if <code>Ls</code> is set to <code>TRUE</code>. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gradient Boosting algorithms require convexity and differentiability of the underlying loss function.
SingBoost is a Boosting algorithm based on <code class="reqn">L_2-</code>Boosting that allows for complicated loss functions that do not
need to satisfy these requirements. In fact, SingBoost alternates between standard <code class="reqn">L_2-</code>Boosting iterations and
singular iterations where essentially an empirical gradient step is executed in the sense that the baselearner
that performs best, evaluated in the complicated loss, is selected in the respective iteration. The implementation
is based on <code>glmboost</code> from the package <code>mboost</code> and using the <code class="reqn">L_2-</code>loss in the singular iterations returns exactly the
same coefficients as <code class="reqn">L_2-</code>Boosting.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Selected variables</code></td>
<td>
<p>Names of the selected variables.</p>
</td></tr>
<tr><td><code>Coefficients</code></td>
<td>
<p>The selected coefficients as an <code class="reqn">(p+1)-</code>dimensional vector (i.e., including the zeroes).</p>
</td></tr>
<tr><td><code>Freqs</code></td>
<td>
<p>Selection frequencies and a matrix for intercept and coefficient paths, respectively.</p>
</td></tr>
<tr><td><code>VarCoef</code></td>
<td>
<p>Vector of the non-zero coefficients.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020
</p>
<p>P. Bühlmann and B. Yu. Boosting with the l2 loss: Regression and Classification. Journal
of the American Statistical Association, 98(462):324–339, 2003
</p>
<p>T. Hothorn, P. Bühlmann, T. Kneib, M. Schmid, and B. Hofner. mboost: Model-Based
Boosting, 2017
</p>


<h3>Examples</h3>

<pre><code class='language-R'>{glmres&lt;-glmboost(Sepal.Length~.,iris)
glmres
attributes(varimp(glmres))$self
attributes(varimp(glmres))$var
firis&lt;-as.formula(Sepal.Length~.)
Xiris&lt;-model.matrix(firis,iris)
Diris&lt;-data.frame(Xiris[,-1],iris$Sepal.Length)
colnames(Diris)[6]&lt;-"Y"
coef(glmboost(Xiris,iris$Sepal.Length))
singboost(Diris)
singboost(Diris,LS=TRUE)}
{glmres2&lt;-glmboost(Sepal.Length~Petal.Length+Sepal.Width:Species,iris)
finter&lt;-as.formula(Sepal.Length~Petal.Length+Sepal.Width:Species-1)
Xinter&lt;-model.matrix(finter,iris)
Dinter&lt;-data.frame(Xinter,iris$Sepal.Length)
singboost(Dinter)
coef(glmres2)}
{glmres3&lt;-glmboost(Xiris,iris$Sepal.Length,control=boost_control(mstop=250,nu=0.05))
coef(glmres3)
attributes(varimp(glmres3))$self
singboost(Diris,m_iter=250,kap=0.05)
singboost(Diris,LS=TRUE,m_iter=250,kap=0.05)}
{glmquant&lt;-glmboost(Sepal.Length~.,iris,family=QuantReg(tau=0.75))
coef(glmquant)
attributes(varimp(glmquant))$self
singboost(Diris,singfamily=QuantReg(tau=0.75),LS=TRUE)
singboost(Diris,singfamily=QuantReg(tau=0.75),LS=TRUE,M=2)}
{singboost(Diris,singfamily=Rank(),LS=TRUE)
singboost(Diris,singfamily=Rank(),LS=TRUE,M=2)}
</code></pre>

<hr>
<h2 id='singboost.plot'>Plot function for the SingBoost coefficient paths</h2><span id='topic+singboost.plot'></span>

<h3>Description</h3>

<p>Plot function for the SingBoost coefficient paths
</p>


<h3>Usage</h3>

<pre><code class='language-R'>singboost.plot(mod, M, m_iter, subnames = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="singboost.plot_+3A_mod">mod</code></td>
<td>
<p>singboost object.</p>
</td></tr>
<tr><td><code id="singboost.plot_+3A_m">M</code></td>
<td>
<p>An integer between 2 and <code>m_iter</code>. Indicates that in every M-th iteration, a singular iteration will be
performed. Default is 10.</p>
</td></tr>
<tr><td><code id="singboost.plot_+3A_m_iter">m_iter</code></td>
<td>
<p>Number of SingBoost iterations. Default is 100.</p>
</td></tr>
<tr><td><code id="singboost.plot_+3A_subnames">subnames</code></td>
<td>
<p>Use it only if the variable names are of the form &rdquo;letter plus number&rdquo;. Better just ignore it.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing. Plots SingBoost coefficient paths
</p>


<h3>Examples</h3>

<pre><code class='language-R'>{glmres&lt;-glmboost(Sepal.Length~.,iris)
glmres
attributes(varimp(glmres))$self
attributes(varimp(glmres))$var
firis&lt;-as.formula(Sepal.Length~.)
Xiris&lt;-model.matrix(firis,iris)
Diris&lt;-data.frame(Xiris[,-1],iris$Sepal.Length)
plot(glmres)
singpath&lt;-path.singboost(Diris)
singboost.plot(singpath,10,100,subnames=FALSE)}

</code></pre>

<hr>
<h2 id='WeakRank'>Weak ranking family</h2><span id='topic+WeakRank'></span>

<h3>Description</h3>

<p>Gradient-free Gradient Boosting family for the weak ranking loss function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WeakRank(K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="WeakRank_+3A_k">K</code></td>
<td>
<p>Indicates that we are only interesting in the top <code class="reqn">K</code> instances. Must be an integer between 1
and the number <code class="reqn">n</code> of observations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The weak ranking loss may be regarded as a classification loss. The parameter <code>K</code> defines the top of the list,
consisting of the best <code class="reqn">K</code> instances according to their response values. Then the weak ranking loss penalizes
&rdquo;misclassification&rdquo; in the sense that instances belonging to the top of the list are ranked lower and vice versa.
<code>WeakRank</code> returns a family object as in the package <code>mboost</code>.
</p>


<h3>Value</h3>

<p>A Boosting family object
</p>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020,
Remark (5.2.1)
</p>
<p>T. Hothorn, P. Bühlmann, T. Kneib, M. Schmid, and B. Hofner. mboost: Model-Based
Boosting, 2017
</p>


<h3>Examples</h3>

<pre><code class='language-R'>{y&lt;-c(-3, 10.3,-8, 12, 14,-0.5, 29,-1.1,-5.7, 119)
yhat&lt;-c(0.02, 0.6, 0.1, 0.47, 0.82, 0.04, 0.77, 0.09, 0.01, 0.79)
WeakRank(4)@risk(y,yhat)}
{y&lt;-c(-3, 10.3,-8, 12, 14,-0.5, 29,-1.1,-5.7, 119)
yhat&lt;-c(0.02, 0.6, 0.1, 0.47, 0.82, 0.04, 0.77, 0.09, 0.01, 0.79)
WeakRank(5)@risk(y,yhat)}
</code></pre>

<hr>
<h2 id='WeakRankNorm'>Weak ranking family (normalized)</h2><span id='topic+WeakRankNorm'></span>

<h3>Description</h3>

<p>Gradient-free Gradient Boosting family for the normalized weak ranking loss function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WeakRankNorm(K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="WeakRankNorm_+3A_k">K</code></td>
<td>
<p>Indicates that we are only interesting in the top <code class="reqn">K</code> instances. Must be an integer between 1 and the number
<code class="reqn">n</code> of observations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A more intuitive loss function than the weak ranking loss thanks to its normalization to a maximum value
of 1. For example, if a number <code class="reqn">c</code> of the top <code class="reqn">K</code> instances has not been ranked at the top of the list, the
normalized weak ranking loss is <code class="reqn">C/K</code>. <code>WeakRankNorm</code> returns a family object as in the package <code>mboost</code>.
</p>


<h3>Value</h3>

<p>A Boosting family object
</p>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020,
Remark (5.2.4)
</p>
<p>T. Hothorn, P. Bühlmann, T. Kneib, M. Schmid, and B. Hofner. mboost: Model-Based
Boosting, 2017
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
