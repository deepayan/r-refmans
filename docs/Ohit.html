<!DOCTYPE html><html lang="en"><head><title>Help for package Ohit</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Ohit}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#OGA'><p>Orthogonal greedy algorithm</p></a></li>
<li><a href='#Ohit'><p>Fit a high-dimensional linear regression model via OGA+HDIC+Trim</p></a></li>
<li><a href='#predict_Ohit'><p>Make predictions based on a fitted &quot;Ohit&quot; object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>OGA+HDIC+Trim and High-Dimensional Linear Regression Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2017-09-06</td>
</tr>
<tr>
<td>Author:</td>
<td>Hai-Tang Chiou, Ching-Kang Ing, Tze Leung Lai</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hai-Tang Chiou &lt;htchiou1@gmail.com&gt;</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats</td>
</tr>
<tr>
<td>Description:</td>
<td>Ing and Lai (2011) &lt;<a href="https://doi.org/10.5705%2Fss.2010.081">doi:10.5705/ss.2010.081</a>&gt; proposed a high-dimensional model selection procedure that comprises three steps: orthogonal greedy algorithm (OGA), high-dimensional information criterion (HDIC), and Trim. The first two steps, OGA and HDIC, are used to sequentially select input variables and determine stopping rules, respectively. The third step, Trim, is used to delete irrelevant variables remaining in the second step. This package aims at fitting a high-dimensional linear regression model via OGA+HDIC+Trim.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://mx.nthu.edu.tw/~cking/pdf/IngLai2011.pdf">http://mx.nthu.edu.tw/~cking/pdf/IngLai2011.pdf</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-09-06 06:00:36 UTC; stat_pc</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-09-06 12:01:26 UTC</td>
</tr>
</table>
<hr>
<h2 id='OGA'>Orthogonal greedy algorithm</h2><span id='topic+OGA'></span>

<h3>Description</h3>

<p>Select valuables via orthogonal greedy algorithm (OGA).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>OGA(X, y, Kn = NULL, c1 = 5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="OGA_+3A_x">X</code></td>
<td>
<p>Input matrix of <code>n</code> rows and <code>p</code> columns.</p>
</td></tr>
<tr><td><code id="OGA_+3A_y">y</code></td>
<td>
<p>Response vector of length <code>n</code>.</p>
</td></tr>
<tr><td><code id="OGA_+3A_kn">Kn</code></td>
<td>
<p>The number of OGA iterations. <code>Kn</code> must be a positive integer between <code>1</code> and <code>p</code>. Default is <code>Kn=max(1, min(floor(c1</code>*<code>sqrt(n/log(p))), p))</code>, where <code>c1</code> is a tuning parameter.</p>
</td></tr>
<tr><td><code id="OGA_+3A_c1">c1</code></td>
<td>
<p>The tuning parameter for the number of OGA iterations. Default is <code>c1=5</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>n</code></td>
<td>
<p>The number of observations.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>The number of input variables.</p>
</td></tr>
<tr><td><code>Kn</code></td>
<td>
<p>The number of OGA iterations.</p>
</td></tr>
<tr><td><code>J_OGA</code></td>
<td>
<p>The index set of <code>Kn</code> variables sequencially selected by OGA.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hai-Tang Chiou, Ching-Kang Ing and Tze Leung Lai.
</p>


<h3>References</h3>

<p>Ing, C.-K. and Lai, T. L. (2011). A stepwise regression method and consistent model selection for high-dimensional sparse linear models. <em>Statistica Sinica</em>, <strong>21</strong>, 1473&ndash;1513.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example setup (Example 3 in Section 5 of Ing and Lai (2011))
n = 400
p = 4000
q = 10
beta_1q = c(3, 3.75, 4.5, 5.25, 6, 6.75, 7.5, 8.25, 9, 9.75)
b = sqrt(3/(4 * q))

x_relevant = matrix(rnorm(n * q), n, q)
d = matrix(rnorm(n * (p - q), 0, 0.5), n, p - q)
x_relevant_sum = apply(x_relevant, 1, sum)
x_irrelevant = apply(d, 2, function(a) a + b * x_relevant_sum)
X = cbind(x_relevant, x_irrelevant)
epsilon = rnorm(n)
y = as.vector((x_relevant %*% beta_1q) + epsilon)

# Select valuables via OGA
OGA(X, y)
</code></pre>

<hr>
<h2 id='Ohit'>Fit a high-dimensional linear regression model via OGA+HDIC+Trim</h2><span id='topic+Ohit'></span>

<h3>Description</h3>

<p>The first step is to sequentially select input variables via orthogonal greedy algorithm (OGA). The second step is to determine the number of OGA iterations using high-dimensional information criterion (HDIC). The third step is to remove irrelevant variables remaining in the second step using HDIC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Ohit(X, y, Kn = NULL, c1 = 5, HDIC_Type = "HDBIC", c2 = 2, c3 = 2.01,
  intercept = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Ohit_+3A_x">X</code></td>
<td>
<p>Input matrix of <code>n</code> rows and <code>p</code> columns.</p>
</td></tr>
<tr><td><code id="Ohit_+3A_y">y</code></td>
<td>
<p>Response vector of length <code>n</code>.</p>
</td></tr>
<tr><td><code id="Ohit_+3A_kn">Kn</code></td>
<td>
<p>The number of OGA iterations. <code>Kn</code> must be a positive integer between <code>1</code> and <code>p</code>. Default is <code>Kn=max(1, min(floor(c1</code>*<code>sqrt(n/log(p))), p))</code>, where <code>c1</code> is a tuning parameter.</p>
</td></tr>
<tr><td><code id="Ohit_+3A_c1">c1</code></td>
<td>
<p>The tuning parameter for the number of OGA iterations. Default is <code>c1=5</code>.</p>
</td></tr>
<tr><td><code id="Ohit_+3A_hdic_type">HDIC_Type</code></td>
<td>
<p>High-dimensional information criterion. The value must be <code>"HDAIC"</code>, <code>"HDBIC"</code> or <code>"HDHQ"</code>. The formula is <code>n</code>*<code>log(rmse)+k_use</code>*<code>omega_n</code>*<code>log(p)</code> where <code>rmse</code> is the residual mean squared error and <code>k_use</code> is the number of variables used to fit the model. For <code>HDIC_Type="HDAIC"</code>, it is HDIC with <code>omega_n=c2</code>. For <code>HDIC_Type="HDBIC"</code>, it is HDIC with <code>omega_n=log(n)</code>. For <code>HDIC_Type="HDHQ"</code>, it is HDIC with <code>omega_n=c3</code>*<code>log(log(n))</code>. Default is <code>HDIC_Type="HDBIC"</code>.</p>
</td></tr>
<tr><td><code id="Ohit_+3A_c2">c2</code></td>
<td>
<p>The tuning parameter for <code>HDIC_Type="HDAIC"</code>. Default is <code>c2=2</code>.</p>
</td></tr>
<tr><td><code id="Ohit_+3A_c3">c3</code></td>
<td>
<p>The tuning parameter for <code>HDIC_Type="HDHQ"</code>. Default is <code>c3=2.01</code>.</p>
</td></tr>
<tr><td><code id="Ohit_+3A_intercept">intercept</code></td>
<td>
<p>Should an intercept be fitted? Default is <code>intercept=TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>n</code></td>
<td>
<p>The number of observations.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>The number of input variables.</p>
</td></tr>
<tr><td><code>Kn</code></td>
<td>
<p>The number of OGA iterations.</p>
</td></tr>
<tr><td><code>J_OGA</code></td>
<td>
<p>The index set of Kn variables sequencially selected by OGA.</p>
</td></tr>
<tr><td><code>HDIC</code></td>
<td>
<p>The HDIC values along the OGA path.</p>
</td></tr>
<tr><td><code>J_HDIC</code></td>
<td>
<p>The index set of valuables determined by OGA+HDIC.</p>
</td></tr>
<tr><td><code>J_Trim</code></td>
<td>
<p>The index set of valuables determined by OGA+HDIC+Trim.</p>
</td></tr>
<tr><td><code>betahat_HDIC</code></td>
<td>
<p>The estimated regression coefficients of the model determined by OGA+HDIC.</p>
</td></tr>
<tr><td><code>betahat_Trim</code></td>
<td>
<p>The estimated regression coefficients of the model determined by OGA+HDIC+Trim.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hai-Tang Chiou, Ching-Kang Ing and Tze Leung Lai.
</p>


<h3>References</h3>

<p>Ing, C.-K. and Lai, T. L. (2011). A stepwise regression method and consistent model selection for high-dimensional sparse linear models. <em>Statistica Sinica</em>, <strong>21</strong>, 1473&ndash;1513.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example setup (Example 3 in Section 5 of Ing and Lai (2011))
n = 400
p = 4000
q = 10
beta_1q = c(3, 3.75, 4.5, 5.25, 6, 6.75, 7.5, 8.25, 9, 9.75)
b = sqrt(3/(4 * q))

x_relevant = matrix(rnorm(n * q), n, q)
d = matrix(rnorm(n * (p - q), 0, 0.5), n, p - q)
x_relevant_sum = apply(x_relevant, 1, sum)
x_irrelevant = apply(d, 2, function(a) a + b * x_relevant_sum)
X = cbind(x_relevant, x_irrelevant)
epsilon = rnorm(n)
y = as.vector((x_relevant %*% beta_1q) + epsilon)

# Fit a high-dimensional linear regression model via OGA+HDIC+Trim
Ohit(X, y, intercept = FALSE)
</code></pre>

<hr>
<h2 id='predict_Ohit'>Make predictions based on a fitted &quot;Ohit&quot; object</h2><span id='topic+predict_Ohit'></span>

<h3>Description</h3>

<p>This function returns predictions from a fitted <code>"Ohit"</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_Ohit(object, newX)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_Ohit_+3A_object">object</code></td>
<td>
<p>Fitted &quot;Ohit&quot; model object.</p>
</td></tr>
<tr><td><code id="predict_Ohit_+3A_newx">newX</code></td>
<td>
<p>Matrix of new values for <code>X</code> at which predictions are to be made.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>pred_HDIC</code></td>
<td>
<p>The predicted value based on the model determined by OGA+HDIC.</p>
</td></tr>
<tr><td><code>pred_Trim</code></td>
<td>
<p>The predicted value based on the model determined by OGA+HDIC+Trim.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hai-Tang Chiou, Ching-Kang Ing and Tze Leung Lai.
</p>


<h3>References</h3>

<p>Ing, C.-K. and Lai, T. L. (2011). A stepwise regression method and consistent model selection for high-dimensional sparse linear models. <em>Statistica Sinica</em>, <strong>21</strong>, 1473&ndash;1513.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example setup (Example 3 in Section 5 of Ing and Lai (2011))
n = 410
p = 4000
q = 10
beta_1q = c(3, 3.75, 4.5, 5.25, 6, 6.75, 7.5, 8.25, 9, 9.75)
b = sqrt(3/(4 * q))

x_relevant = matrix(rnorm(n * q), n, q)
d = matrix(rnorm(n * (p - q), 0, 0.5), n, p - q)
x_relevant_sum = apply(x_relevant, 1, sum)
x_irrelevant = apply(d, 2, function(a) a + b * x_relevant_sum)
X = cbind(x_relevant, x_irrelevant)
epsilon = rnorm(n)
y = as.vector((x_relevant %*% beta_1q) + epsilon)

# with intercept
fit1 = Ohit(X[1:400, ], y[1:400])
predict_Ohit(fit1, rbind(X[401:401, ]))
predict_Ohit(fit1, X[401:410, ])
# without intercept
fit2 = Ohit(X[1:400, ], y[1:400], intercept = FALSE)
predict_Ohit(fit2, rbind(X[401:401, ]))
predict_Ohit(fit2, X[401:410, ])
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
