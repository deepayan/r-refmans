<!DOCTYPE html><html><head><title>Help for package superml</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {superml}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bm_25'><p>BM25 Matching</p></a></li>
<li><a href='#bm25'><p>Best Matching(BM25) - Deprecated</p></a></li>
<li><a href='#check_package'><p>Internal function</p></a></li>
<li><a href='#cla_train'><p>cla_train</p></a></li>
<li><a href='#Counter'><p>Calculate count of values in a list or vector</p></a></li>
<li><a href='#CountVectorizer'><p>Count Vectorizer</p></a></li>
<li><a href='#createFolds'><p>Internal function</p></a></li>
<li><a href='#dot'><p>Dot product similarity in vectors</p></a></li>
<li><a href='#dotmat'><p>Dot product similarity between a vector and matrix</p></a></li>
<li><a href='#GridSearchCV'><p>Grid Search CV</p></a></li>
<li><a href='#kFoldMean'><p>kFoldMean Calculator</p></a></li>
<li><a href='#KMeansTrainer'><p>K-Means Trainer</p></a></li>
<li><a href='#KNNTrainer'><p>K Nearest Neighbours Trainer</p></a></li>
<li><a href='#LabelEncoder'><p>Label Encoder</p></a></li>
<li><a href='#LMTrainer'><p>Linear Models Trainer</p></a></li>
<li><a href='#NBTrainer'><p>Naive Bayes Trainer</p></a></li>
<li><a href='#normalise1d'><p>normalise1d</p></a></li>
<li><a href='#normalise2d'><p>normalise2d</p></a></li>
<li><a href='#RandomSearchCV'><p>Random Search CV</p></a></li>
<li><a href='#reg_train'><p>reg_train</p></a></li>
<li><a href='#RFTrainer'><p>Random Forest Trainer</p></a></li>
<li><a href='#smoothMean'><p>smoothMean Calculator</p></a></li>
<li><a href='#sort_index'><p>sort_index</p></a></li>
<li><a href='#testdata'><p>Internal function</p></a></li>
<li><a href='#TfIdfVectorizer'><p>TfIDF(Term Frequency Inverse Document Frequency) Vectorizer</p></a></li>
<li><a href='#XGBTrainer'><p>Extreme Gradient Boosting Trainer</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Build Machine Learning Models Like Using Python's Scikit-Learn
Library in R</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.7</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Manish Saraswat &lt;manish06saraswat@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The idea is to provide a standard interface 
             to users who use both R and Python for building machine learning models. 
             This package provides a scikit-learn's fit, predict interface to 
             train machine learning models in R.    </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> | file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/saraswatmks/superml">https://github.com/saraswatmks/superml</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/saraswatmks/superml/issues">https://github.com/saraswatmks/superml/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 3.6), R6(&ge; 2.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table (&ge; 1.10), Rcpp (&ge; 1.0), assertthat (&ge; 0.2),
Metrics (&ge; 0.1)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, BH, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rlang, testthat, rmarkdown, naivebayes(&ge; 0.9),
ClusterR(&ge; 1.1), FNN(&ge; 1.1), ranger(&ge; 0.10), caret(&ge; 6.0),
xgboost(&ge; 0.6), glmnet(&ge; 2.0), e1071(&ge; 1.7)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-18 14:11:27 UTC; manish.saraswat</td>
</tr>
<tr>
<td>Author:</td>
<td>Manish Saraswat [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-18 15:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bm_25'>BM25 Matching</h2><span id='topic+bm_25'></span>

<h3>Description</h3>

<p>BM25 stands for Best Matching 25. It is widely using for ranking documents and a preferred method than TF*IDF scores.
It is used to find the similar documents from a corpus, given a new document. It is popularly used in information retrieval systems.
This implementation is based on c++ functions hence quite optimised as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bm_25(document, corpus, top_n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bm_25_+3A_document">document</code></td>
<td>
<p>a string for which to find similar documents</p>
</td></tr>
<tr><td><code id="bm_25_+3A_corpus">corpus</code></td>
<td>
<p>a vector of strings against which document is to be matched</p>
</td></tr>
<tr><td><code id="bm_25_+3A_top_n">top_n</code></td>
<td>
<p>top n similar documents to find</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector containing similar documents and their scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>docs &lt;- c("chimpanzees are found in jungle",
          "chimps are jungle animals",
          "Mercedes automobiles are best",
          "merc is made in germany",
          "chimps are intelligent animals")

sentence &lt;- "automobiles are"
s &lt;- bm_25(document=sentence, corpus=docs, top_n=2)

</code></pre>

<hr>
<h2 id='bm25'>Best Matching(BM25) - Deprecated</h2><span id='topic+bm25'></span>

<h3>Description</h3>

<p>Computer BM25 distance between sentences/documents.
</p>


<h3>Details</h3>

<p>BM25 stands for Best Matching 25. It is widely using for ranking documents and a preferred method than TF*IDF scores.
It is used to find the similar documents from a corpus, given a new document. It is popularly used in information retrieval systems.
This implementation uses multiple cores for faster and parallel computation.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>corpus</code></dt><dd><p>a list containing sentences</p>
</dd>
<dt><code>use_parallel</code></dt><dd><p>enables parallel computation, defaults to FALSE</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-bm25-new"><code>bm25$new()</code></a>
</p>
</li>
<li> <p><a href="#method-bm25-most_similar"><code>bm25$most_similar()</code></a>
</p>
</li>
<li> <p><a href="#method-bm25-clone"><code>bm25$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-bm25-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>bm25$new(corpus, use_parallel)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>corpus</code></dt><dd><p>list, a list containing sentences</p>
</dd>
<dt><code>use_parallel</code></dt><dd><p>logical, enables parallel computation, defaults to FALSE. if TRUE uses n - 1 cores.</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'bm25' object.
</p>



<h5>Returns</h5>

<p>A 'bm25' object.
</p>
<p>example &lt;- c('white audi 2.5 car','black shoes from office',
'new mobile iphone 7','audi tyres audi a3',
'nice audi bmw toyota corolla')
obj &lt;- bm25$new(example, use_parallel=FALSE)
</p>


<hr>
<a id="method-bm25-most_similar"></a>



<h4>Method <code>most_similar()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>bm25$most_similar(document, topn = 1)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>document</code></dt><dd><p>character, for this value we find most similar sentences.</p>
</dd>
<dt><code>topn</code></dt><dd><p>integer, top n sentences to retrieve</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Returns a list of the most similar sentence
</p>



<h5>Returns</h5>

<p>a vector of most similar documents
</p>
<p>example &lt;- c('white audi 2.5 car','black shoes from office',
'new mobile iphone 7','audi tyres audi a3',
'nice audi bmw toyota corolla')
get_bm &lt;- bm25$new(example, use_parallel=FALSE)
input_document &lt;- c('white toyota corolla')
get_bm$most_similar(document = input_document, topn = 2)
</p>


<hr>
<a id="method-bm25-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>bm25$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='check_package'>Internal function</h2><span id='topic+check_package'></span>

<h3>Description</h3>

<p>Used to check the package is installed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_package(pkg)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_package_+3A_pkg">pkg</code></td>
<td>
<p>should be a string containing package name</p>
</td></tr>
</table>


<h3>Value</h3>

<p>null
</p>

<hr>
<h2 id='cla_train'>cla_train</h2><span id='topic+cla_train'></span>

<h3>Description</h3>

<p>Training Dataset used for classification examples.
This is classic titanic dataset used to predict if a passenger will survive or not in titanic ship disaster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cla_train
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.table</code> (inherits from <code>data.frame</code>) with 891 rows and 12 columns.
</p>


<h3>Source</h3>

<p><a href="https://www.kaggle.com/c/titanic/data">https://www.kaggle.com/c/titanic/data</a>
</p>

<hr>
<h2 id='Counter'>Calculate count of values in a list or vector</h2><span id='topic+Counter'></span>

<h3>Description</h3>

<p>Handy function to calculate count of values given in a list or vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Counter(data, sort = TRUE, decreasing = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Counter_+3A_data">data</code></td>
<td>
<p>should be a vector or list of input values</p>
</td></tr>
<tr><td><code id="Counter_+3A_sort">sort</code></td>
<td>
<p>a logical value, to sort the result or not</p>
</td></tr>
<tr><td><code id="Counter_+3A_decreasing">decreasing</code></td>
<td>
<p>a logical value, the order of sorting to be followed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>count of values in a list
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- list(c('i','am','bad'),c('you','are','also','bad'))
counts &lt;- Counter(d, sort=TRUE, decreasing=TRUE)
</code></pre>

<hr>
<h2 id='CountVectorizer'>Count Vectorizer</h2><span id='topic+CountVectorizer'></span>

<h3>Description</h3>

<p>Creates CountVectorizer Model.
</p>


<h3>Details</h3>

<p>Given a list of text, it generates a bag of words model and returns a sparse matrix consisting of token counts.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>sentences</code></dt><dd><p>a list containing sentences</p>
</dd>
<dt><code>max_df</code></dt><dd><p>When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold, value lies between 0 and 1.</p>
</dd>
<dt><code>min_df</code></dt><dd><p>When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold, value lies between 0 and 1.</p>
</dd>
<dt><code>max_features</code></dt><dd><p>Build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</p>
</dd>
<dt><code>ngram_range</code></dt><dd><p>The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n &lt;= n &lt;= max_n will be used. For example an ngram_range of c(1, 1) means only unigrams, c(1, 2) means unigrams and bigrams, and c(2, 2) means only bigrams.</p>
</dd>
<dt><code>split</code></dt><dd><p>splitting criteria for strings, default: &quot; &quot;</p>
</dd>
<dt><code>lowercase</code></dt><dd><p>convert all characters to lowercase before tokenizing</p>
</dd>
<dt><code>regex</code></dt><dd><p>regex expression to use for text cleaning.</p>
</dd>
<dt><code>remove_stopwords</code></dt><dd><p>a list of stopwords to use, by default it uses its inbuilt list of standard stopwords</p>
</dd>
<dt><code>model</code></dt><dd><p>internal attribute which stores the count model</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-CountVectorizer-new"><code>CountVectorizer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-CountVectorizer-fit"><code>CountVectorizer$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-CountVectorizer-fit_transform"><code>CountVectorizer$fit_transform()</code></a>
</p>
</li>
<li> <p><a href="#method-CountVectorizer-transform"><code>CountVectorizer$transform()</code></a>
</p>
</li>
<li> <p><a href="#method-CountVectorizer-clone"><code>CountVectorizer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-CountVectorizer-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>CountVectorizer$new(
  min_df,
  max_df,
  max_features,
  ngram_range,
  regex,
  remove_stopwords,
  split,
  lowercase
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>min_df</code></dt><dd><p>numeric, When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold, value lies between 0 and 1.</p>
</dd>
<dt><code>max_df</code></dt><dd><p>numeric, When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold, value lies between 0 and 1.</p>
</dd>
<dt><code>max_features</code></dt><dd><p>integer, Build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</p>
</dd>
<dt><code>ngram_range</code></dt><dd><p>vector, The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n &lt;= n &lt;= max_n will be used. For example an ngram_range of c(1, 1) means only unigrams, c(1, 2) means unigrams and bigrams, and c(2, 2) means only bigrams.</p>
</dd>
<dt><code>regex</code></dt><dd><p>character, regex expression to use for text cleaning.</p>
</dd>
<dt><code>remove_stopwords</code></dt><dd><p>list, a list of stopwords to use, by default it uses its inbuilt list of standard english stopwords</p>
</dd>
<dt><code>split</code></dt><dd><p>character, splitting criteria for strings, default: &quot; &quot;</p>
</dd>
<dt><code>lowercase</code></dt><dd><p>logical, convert all characters to lowercase before tokenizing, default: TRUE</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'CountVectorizer' object.
</p>



<h5>Returns</h5>

<p>A 'CountVectorizer' object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>cv = CountVectorizer$new(min_df=0.1)
</pre>
</div>


<hr>
<a id="method-CountVectorizer-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>CountVectorizer$fit(sentences)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sentences</code></dt><dd><p>a list of text sentences</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Fits the countvectorizer model on sentences
</p>



<h5>Returns</h5>

<p>NULL
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>sents = c('i am alone in dark.','mother_mary a lot',
          'alone in the dark?', 'many mothers in the lot....')
cv = CountVectorizer$new(min_df=0.1)
cv$fit(sents)
</pre>
</div>


<hr>
<a id="method-CountVectorizer-fit_transform"></a>



<h4>Method <code>fit_transform()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>CountVectorizer$fit_transform(sentences)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sentences</code></dt><dd><p>a list of text sentences</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Fits the countvectorizer model and returns a sparse matrix of count of tokens
</p>



<h5>Returns</h5>

<p>a sparse matrix containing count of tokens in each given sentence
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>sents = c('i am alone in dark.','mother_mary a lot',
         'alone in the dark?', 'many mothers in the lot....')
cv &lt;- CountVectorizer$new(min_df=0.1)
cv_count_matrix &lt;- cv$fit_transform(sents)
</pre>
</div>


<hr>
<a id="method-CountVectorizer-transform"></a>



<h4>Method <code>transform()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>CountVectorizer$transform(sentences)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sentences</code></dt><dd><p>a list of new text sentences</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Returns a matrix of count of tokens
</p>



<h5>Returns</h5>

<p>a sparse matrix containing count of tokens in each given sentence
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>sents = c('i am alone in dark.','mother_mary a lot',
          'alone in the dark?', 'many mothers in the lot....')
new_sents &lt;- c("dark at night",'mothers day')
cv = CountVectorizer$new(min_df=0.1)
cv$fit(sents)
cv_count_matrix &lt;- cv$transform(new_sents)
</pre>
</div>


<hr>
<a id="method-CountVectorizer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>CountVectorizer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `CountVectorizer$new`
## ------------------------------------------------

cv = CountVectorizer$new(min_df=0.1)

## ------------------------------------------------
## Method `CountVectorizer$fit`
## ------------------------------------------------

sents = c('i am alone in dark.','mother_mary a lot',
          'alone in the dark?', 'many mothers in the lot....')
cv = CountVectorizer$new(min_df=0.1)
cv$fit(sents)

## ------------------------------------------------
## Method `CountVectorizer$fit_transform`
## ------------------------------------------------

sents = c('i am alone in dark.','mother_mary a lot',
         'alone in the dark?', 'many mothers in the lot....')
cv &lt;- CountVectorizer$new(min_df=0.1)
cv_count_matrix &lt;- cv$fit_transform(sents)

## ------------------------------------------------
## Method `CountVectorizer$transform`
## ------------------------------------------------

sents = c('i am alone in dark.','mother_mary a lot',
          'alone in the dark?', 'many mothers in the lot....')
new_sents &lt;- c("dark at night",'mothers day')
cv = CountVectorizer$new(min_df=0.1)
cv$fit(sents)
cv_count_matrix &lt;- cv$transform(new_sents)
</code></pre>

<hr>
<h2 id='createFolds'>Internal function</h2><span id='topic+createFolds'></span>

<h3>Description</h3>

<p>Taken from caret package, used to create folds
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createFolds(y, k = 10, list = TRUE, returnTrain = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createFolds_+3A_y">y</code></td>
<td>
<p>target variable</p>
</td></tr>
<tr><td><code id="createFolds_+3A_k">k</code></td>
<td>
<p>number of folds</p>
</td></tr>
</table>


<h3>Value</h3>

<p>null
</p>

<hr>
<h2 id='dot'>Dot product similarity in vectors</h2><span id='topic+dot'></span>

<h3>Description</h3>

<p>Computes the dot product between two given vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dot(a, b, norm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dot_+3A_a">a</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="dot_+3A_b">b</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="dot_+3A_norm">norm</code></td>
<td>
<p>logical, compute normalised dot product, default=True</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector containing sdot product score
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- runif(5)
b &lt;- runif(5)
s &lt;- dot(a, b)

</code></pre>

<hr>
<h2 id='dotmat'>Dot product similarity between a vector and matrix</h2><span id='topic+dotmat'></span>

<h3>Description</h3>

<p>Computes the dot product between a vector and a given matrix.
The vector returned has a dot product similarity value for each row in the matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dotmat(a, b, norm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dotmat_+3A_a">a</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="dotmat_+3A_b">b</code></td>
<td>
<p>numeric matrix</p>
</td></tr>
<tr><td><code id="dotmat_+3A_norm">norm</code></td>
<td>
<p>logical, compute normalised dot product, default=True</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector containing dot product scores
</p>

<hr>
<h2 id='GridSearchCV'>Grid Search CV</h2><span id='topic+GridSearchCV'></span>

<h3>Description</h3>

<p>Runs grid search cross validation scheme to find best model training parameters.
</p>


<h3>Details</h3>

<p>Grid search CV is used to train a machine learning model with multiple combinations
of training hyper parameters and finds the best combination of parameters which optimizes the evaluation metric.
It creates an exhaustive set of hyperparameter combinations and train model on each combination.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>trainer</code></dt><dd><p>superml trainer object, could be either XGBTrainer, RFTrainer, NBTrainer etc.</p>
</dd>
<dt><code>parameters</code></dt><dd><p>a list of parameters to tune</p>
</dd>
<dt><code>n_folds</code></dt><dd><p>number of folds to use to split the train data</p>
</dd>
<dt><code>scoring</code></dt><dd><p>scoring metric used to evaluate the best model, multiple values can be provided. currently supports: auc, accuracy, mse, rmse, logloss, mae, f1, precision, recall</p>
</dd>
<dt><code>evaluation_scores</code></dt><dd><p>parameter for internal use</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-GridSearchCV-new"><code>GridSearchCV$new()</code></a>
</p>
</li>
<li> <p><a href="#method-GridSearchCV-fit"><code>GridSearchCV$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-GridSearchCV-best_iteration"><code>GridSearchCV$best_iteration()</code></a>
</p>
</li>
<li> <p><a href="#method-GridSearchCV-clone"><code>GridSearchCV$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-GridSearchCV-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>GridSearchCV$new(trainer = NA, parameters = NA, n_folds = NA, scoring = NA)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>trainer</code></dt><dd><p>superml trainer object, could be either XGBTrainer, RFTrainer, NBTrainer etc.</p>
</dd>
<dt><code>parameters</code></dt><dd><p>list, a list of parameters to tune</p>
</dd>
<dt><code>n_folds</code></dt><dd><p>integer, number of folds to use to split the train data</p>
</dd>
<dt><code>scoring</code></dt><dd><p>character, scoring metric used to evaluate the best model, multiple values can be provided.
currently supports: auc, accuracy, mse, rmse, logloss, mae, f1, precision, recall</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'GridSearchCV' object.
</p>



<h5>Returns</h5>

<p>A 'GridSearchCV' object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>rf &lt;- RFTrainer$new()
gst &lt;-GridSearchCV$new(trainer = rf,
                      parameters = list(n_estimators = c(100),
                                        max_depth = c(5,2,10)),
                                        n_folds = 3,
                                        scoring = c('accuracy','auc'))
</pre>
</div>


<hr>
<a id="method-GridSearchCV-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>GridSearchCV$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame or data.table</p>
</dd>
<dt><code>y</code></dt><dd><p>character, name of target variable</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Trains the model using grid search
</p>



<h5>Returns</h5>

<p>NULL
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>rf &lt;- RFTrainer$new()
gst &lt;-GridSearchCV$new(trainer = rf,
                      parameters = list(n_estimators = c(100),
                                        max_depth = c(5,2,10)),
                                        n_folds = 3,
                                        scoring = c('accuracy','auc'))
data("iris")
gst$fit(iris, "Species")
</pre>
</div>


<hr>
<a id="method-GridSearchCV-best_iteration"></a>



<h4>Method <code>best_iteration()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>GridSearchCV$best_iteration(metric = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>metric</code></dt><dd><p>character, which metric to use for evaluation</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Returns the best parameters
</p>



<h5>Returns</h5>

<p>a list of best parameters
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>rf &lt;- RFTrainer$new()
gst &lt;-GridSearchCV$new(trainer = rf,
                      parameters = list(n_estimators = c(100),
                                        max_depth = c(5,2,10)),
                                        n_folds = 3,
                                        scoring = c('accuracy','auc'))
data("iris")
gst$fit(iris, "Species")
gst$best_iteration()
</pre>
</div>


<hr>
<a id="method-GridSearchCV-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>GridSearchCV$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `GridSearchCV$new`
## ------------------------------------------------

rf &lt;- RFTrainer$new()
gst &lt;-GridSearchCV$new(trainer = rf,
                      parameters = list(n_estimators = c(100),
                                        max_depth = c(5,2,10)),
                                        n_folds = 3,
                                        scoring = c('accuracy','auc'))

## ------------------------------------------------
## Method `GridSearchCV$fit`
## ------------------------------------------------

rf &lt;- RFTrainer$new()
gst &lt;-GridSearchCV$new(trainer = rf,
                      parameters = list(n_estimators = c(100),
                                        max_depth = c(5,2,10)),
                                        n_folds = 3,
                                        scoring = c('accuracy','auc'))
data("iris")
gst$fit(iris, "Species")

## ------------------------------------------------
## Method `GridSearchCV$best_iteration`
## ------------------------------------------------

rf &lt;- RFTrainer$new()
gst &lt;-GridSearchCV$new(trainer = rf,
                      parameters = list(n_estimators = c(100),
                                        max_depth = c(5,2,10)),
                                        n_folds = 3,
                                        scoring = c('accuracy','auc'))
data("iris")
gst$fit(iris, "Species")
gst$best_iteration()
</code></pre>

<hr>
<h2 id='kFoldMean'>kFoldMean Calculator</h2><span id='topic+kFoldMean'></span>

<h3>Description</h3>

<p>Calculates out-of-fold mean features (also known as target encoding) for train and test data.
This strategy is widely used to avoid overfitting or causing leakage while creating features
using the target variable. This method is experimental. If the results you get are unexpected,
please report them in github issues.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kFoldMean(train_df, test_df, colname, target, n_fold = 5, seed = 42)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kFoldMean_+3A_train_df">train_df</code></td>
<td>
<p>train dataset</p>
</td></tr>
<tr><td><code id="kFoldMean_+3A_test_df">test_df</code></td>
<td>
<p>test dataset</p>
</td></tr>
<tr><td><code id="kFoldMean_+3A_colname">colname</code></td>
<td>
<p>name of categorical column</p>
</td></tr>
<tr><td><code id="kFoldMean_+3A_target">target</code></td>
<td>
<p>the target or dependent variable, should be a string.</p>
</td></tr>
<tr><td><code id="kFoldMean_+3A_n_fold">n_fold</code></td>
<td>
<p>the number of folds to use for doing kfold computation, default=5</p>
</td></tr>
<tr><td><code id="kFoldMean_+3A_seed">seed</code></td>
<td>
<p>the seed value, to ensure reproducibility,
it could be any positive value, default=42</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a train and test data table with out-of-fold mean value
of the target for the given categorical variable
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- data.frame(region=c('del','csk','rcb','del','csk','pune','guj','del'),
                    win = c(0,1,1,0,0,0,0,1))
test &lt;- data.frame(region=c('rcb','csk','rcb','del','guj','pune','csk','kol'))
train_result &lt;- kFoldMean(train_df = train,
                          test_df = test,
                          colname = 'region',
                          target = 'win',
                          seed = 1220)$train

test_result &lt;- kFoldMean(train_df = train,
                         test_df = test,
                         colname = 'region',
                         target = 'win',
                         seed = 1220)$test
</code></pre>

<hr>
<h2 id='KMeansTrainer'>K-Means Trainer</h2><span id='topic+KMeansTrainer'></span>

<h3>Description</h3>

<p>Trains a k-means machine learning model in R
</p>


<h3>Details</h3>

<p>Trains a unsupervised K-Means clustering algorithm. It borrows mini-batch k-means function from
ClusterR package written in c++, hence it is quite fast.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>clusters</code></dt><dd><p>the number of clusters</p>
</dd>
<dt><code>batch_size</code></dt><dd><p>the size of the mini batches</p>
</dd>
<dt><code>num_init</code></dt><dd><p>number of times the algorithm will be run with different centroid seeds</p>
</dd>
<dt><code>max_iters</code></dt><dd><p>the maximum number of clustering iterations</p>
</dd>
<dt><code>init_fraction</code></dt><dd><p>percentage of data to use for the initialization centroids (applies if initializer is kmeans++ or optimal_init). Should be a float number between 0.0 and 1.0.</p>
</dd>
<dt><code>initializer</code></dt><dd><p>the method of initialization. One of, optimal_init, quantile_init, kmeans++ and random.</p>
</dd>
<dt><code>early_stop_iter</code></dt><dd><p>continue that many iterations after calculation of the best within-cluster-sum-ofsquared-error</p>
</dd>
<dt><code>verbose</code></dt><dd><p>either TRUE or FALSE, indicating whether progress is printed during clustering</p>
</dd>
<dt><code>centroids</code></dt><dd><p>a matrix of initial cluster centroids. The rows of the CENTROIDS matrix should be equal to the number of clusters and the columns should be equal to the columns of the data</p>
</dd>
<dt><code>tol</code></dt><dd><p>a float number. If, in case of an iteration (iteration &gt; 1 and iteration &lt; max_iters) &quot;tol&quot; is greater than the squared norm of the centroids, then kmeans has converged</p>
</dd>
<dt><code>tol_optimal_init</code></dt><dd><p>tolerance value for the ’optimal_init’ initializer. The higher this value is, the far appart from each other the centroids are.</p>
</dd>
<dt><code>seed</code></dt><dd><p>integer value for random number generator (RNG)</p>
</dd>
<dt><code>model</code></dt><dd><p>use for internal purpose</p>
</dd>
<dt><code>max_clusters</code></dt><dd><p>either a numeric value, a contiguous or non-continguous numeric vector specifying the cluster search space</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-KMeansTrainer-new"><code>KMeansTrainer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-KMeansTrainer-fit"><code>KMeansTrainer$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-KMeansTrainer-predict"><code>KMeansTrainer$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-KMeansTrainer-clone"><code>KMeansTrainer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-KMeansTrainer-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>KMeansTrainer$new(
  clusters,
  batch_size = 10,
  num_init = 1,
  max_iters = 100,
  init_fraction = 1,
  initializer = "kmeans++",
  early_stop_iter = 10,
  verbose = FALSE,
  centroids = NULL,
  tol = 1e-04,
  tol_optimal_init = 0.3,
  seed = 1,
  max_clusters = NA
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>clusters</code></dt><dd><p>numeric, When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold, value lies between 0 and 1.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p>nuemric, When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold, value lies between 0 and 1.</p>
</dd>
<dt><code>num_init</code></dt><dd><p>integer, use top features sorted by count to be used in bag of words matrix.</p>
</dd>
<dt><code>max_iters</code></dt><dd><p>character, regex expression to use for text cleaning.</p>
</dd>
<dt><code>init_fraction</code></dt><dd><p>list, a list of stopwords to use, by default it uses its inbuilt list of standard stopwords</p>
</dd>
<dt><code>initializer</code></dt><dd><p>character, splitting criteria for strings, default: &quot; &quot;</p>
</dd>
<dt><code>early_stop_iter</code></dt><dd><p>continue that many iterations after calculation of the best within-cluster-sum-ofsquared-error</p>
</dd>
<dt><code>verbose</code></dt><dd><p>either TRUE or FALSE, indicating whether progress is printed during clustering</p>
</dd>
<dt><code>centroids</code></dt><dd><p>a matrix of initial cluster centroids. The rows of the CENTROIDS matrix should be equal to the number of clusters and the columns should be equal to the columns of the data</p>
</dd>
<dt><code>tol</code></dt><dd><p>a float number. If, in case of an iteration (iteration &gt; 1 and iteration &lt; max_iters) &quot;tol&quot; is greater than the squared norm of the centroids, then kmeans has converged</p>
</dd>
<dt><code>tol_optimal_init</code></dt><dd><p>tolerance value for the ’optimal_init’ initializer. The higher this value is, the far appart from each other the centroids are.</p>
</dd>
<dt><code>seed</code></dt><dd><p>integer value for random number generator (RNG)</p>
</dd>
<dt><code>max_clusters</code></dt><dd><p>either a numeric value, a contiguous or non-continguous numeric vector specifying the cluster search space</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'KMeansTrainer' object.
</p>



<h5>Returns</h5>

<p>A 'KMeansTrainer' object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data &lt;- rbind(replicate(20, rnorm(1e4, 2)),
             replicate(20, rnorm(1e4, -1)),
             replicate(20, rnorm(1e4, 5)))
km_model &lt;- KMeansTrainer$new(clusters=2, batch_size=30, max_clusters=6)
</pre>
</div>


<hr>
<a id="method-KMeansTrainer-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>KMeansTrainer$fit(X, y = NULL, find_optimal = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame or matrix containing features</p>
</dd>
<dt><code>y</code></dt><dd><p>NULL only kept here for superml's standard way</p>
</dd>
<dt><code>find_optimal</code></dt><dd><p>logical, to find the optimal clusters automatically</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Trains the KMeansTrainer model
</p>



<h5>Returns</h5>

<p>NULL
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data &lt;- rbind(replicate(20, rnorm(1e4, 2)),
             replicate(20, rnorm(1e4, -1)),
             replicate(20, rnorm(1e4, 5)))
km_model &lt;- KMeansTrainer$new(clusters=2, batch_size=30, max_clusters=6)
km_model$fit(data, find_optimal = FALSE)
</pre>
</div>


<hr>
<a id="method-KMeansTrainer-predict"></a>



<h4>Method <code>predict()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>KMeansTrainer$predict(X)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame or matrix</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Returns the prediction on test data
</p>



<h5>Returns</h5>

<p>a vector of predictions
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data &lt;- rbind(replicate(20, rnorm(1e4, 2)),
             replicate(20, rnorm(1e4, -1)),
             replicate(20, rnorm(1e4, 5)))
km_model &lt;- KMeansTrainer$new(clusters=2, batch_size=30, max_clusters=6)
km_model$fit(data, find_optimal = FALSE)
predictions &lt;- km_model$predict(data)
</pre>
</div>


<hr>
<a id="method-KMeansTrainer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>KMeansTrainer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `KMeansTrainer$new`
## ------------------------------------------------

data &lt;- rbind(replicate(20, rnorm(1e4, 2)),
             replicate(20, rnorm(1e4, -1)),
             replicate(20, rnorm(1e4, 5)))
km_model &lt;- KMeansTrainer$new(clusters=2, batch_size=30, max_clusters=6)

## ------------------------------------------------
## Method `KMeansTrainer$fit`
## ------------------------------------------------

data &lt;- rbind(replicate(20, rnorm(1e4, 2)),
             replicate(20, rnorm(1e4, -1)),
             replicate(20, rnorm(1e4, 5)))
km_model &lt;- KMeansTrainer$new(clusters=2, batch_size=30, max_clusters=6)
km_model$fit(data, find_optimal = FALSE)

## ------------------------------------------------
## Method `KMeansTrainer$predict`
## ------------------------------------------------

data &lt;- rbind(replicate(20, rnorm(1e4, 2)),
             replicate(20, rnorm(1e4, -1)),
             replicate(20, rnorm(1e4, 5)))
km_model &lt;- KMeansTrainer$new(clusters=2, batch_size=30, max_clusters=6)
km_model$fit(data, find_optimal = FALSE)
predictions &lt;- km_model$predict(data)
</code></pre>

<hr>
<h2 id='KNNTrainer'>K Nearest Neighbours Trainer</h2><span id='topic+KNNTrainer'></span>

<h3>Description</h3>

<p>Trains a k nearest neighbour model using fast search algorithms. KNN is a supervised learning
algorithm which is used for both regression and classification problems.
</p>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object.
</p>


<h3>Usage</h3>

<p>For usage details see <b>Methods, Arguments and Examples</b> sections.
</p>
<pre>
bst = KNNTrainer$new(k=1, prob=FALSE, algorithm=NULL, type="class")
bst$fit(X_train, X_test, "target")
bst$predict(type)
</pre>


<h3>Methods</h3>


<dl>
<dt><code>$new()</code></dt><dd><p>Initialise the instance of the trainer</p>
</dd>
<dt><code>$fit()</code></dt><dd><p>trains the knn model and stores the test prediction</p>
</dd>
<dt><code>$predict()</code></dt><dd><p>returns predictions</p>
</dd>
</dl>



<h3>Arguments</h3>


<dl>
<dt>k</dt><dd><p>number of neighbours to predict</p>
</dd>
<dt>prob</dt><dd><p>if probability should be computed, default=FALSE</p>
</dd>
<dt>algorithm</dt><dd><p>algorithm used to train the model, possible values are 'kd_tree','cover_tree','brute'</p>
</dd>
<dt>type</dt><dd><p>type of problem to solve i.e. regression or classification, possible values are 'reg' or 'class'</p>
</dd>
</dl>



<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>k</code></dt><dd><p>number of neighbours to predict</p>
</dd>
<dt><code>prob</code></dt><dd><p>if probability should be computed, default=FALSE</p>
</dd>
<dt><code>algorithm</code></dt><dd><p>algorithm used to train the model, possible values are 'kd_tree','cover_tree','brute'</p>
</dd>
<dt><code>type</code></dt><dd><p>type of problem to solve i.e. regression or classification, possible values are 'reg' or 'class'</p>
</dd>
<dt><code>model</code></dt><dd><p>for internal use</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-KNNTrainer-new"><code>KNNTrainer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-KNNTrainer-fit"><code>KNNTrainer$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-KNNTrainer-predict"><code>KNNTrainer$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-KNNTrainer-clone"><code>KNNTrainer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-KNNTrainer-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>KNNTrainer$new(k, prob, algorithm, type)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>k</code></dt><dd><p>k number of neighbours to predict</p>
</dd>
<dt><code>prob</code></dt><dd><p>if probability should be computed, default=FALSE</p>
</dd>
<dt><code>algorithm</code></dt><dd><p>algorithm used to train the model, possible values are 'kd_tree','cover_tree','brute'</p>
</dd>
<dt><code>type</code></dt><dd><p>type of problem to solve i.e. regression or classification, possible values are 'reg' or 'class'</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'KNNTrainer' object.
</p>



<h5>Returns</h5>

<p>A 'KNNTrainer' object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data("iris")

iris$Species &lt;- as.integer(as.factor(iris$Species))

xtrain &lt;- iris[1:100,]
xtest &lt;- iris[101:150,]

bst &lt;- KNNTrainer$new(k=3, prob=TRUE, type="class")
bst$fit(xtrain, xtest, 'Species')
pred &lt;- bst$predict(type="raw")
</pre>
</div>


<hr>
<a id="method-KNNTrainer-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>KNNTrainer$fit(train, test, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>train</code></dt><dd><p>data.frame or matrix</p>
</dd>
<dt><code>test</code></dt><dd><p>data.frame or matrix</p>
</dd>
<dt><code>y</code></dt><dd><p>character, name of target variable</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Trains the KNNTrainer model
</p>



<h5>Returns</h5>

<p>NULL
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data("iris")

iris$Species &lt;- as.integer(as.factor(iris$Species))

xtrain &lt;- iris[1:100,]
xtest &lt;- iris[101:150,]

bst &lt;- KNNTrainer$new(k=3, prob=TRUE, type="class")
bst$fit(xtrain, xtest, 'Species')
</pre>
</div>


<hr>
<a id="method-KNNTrainer-predict"></a>



<h4>Method <code>predict()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>KNNTrainer$predict(type = "raw")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>type</code></dt><dd><p>character, 'raw' for labels else 'prob'</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Predits the nearest neigbours for test data
</p>



<h5>Returns</h5>

<p>a list of predicted neighbours
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data("iris")

iris$Species &lt;- as.integer(as.factor(iris$Species))

xtrain &lt;- iris[1:100,]
xtest &lt;- iris[101:150,]

bst &lt;- KNNTrainer$new(k=3, prob=TRUE, type="class")
bst$fit(xtrain, xtest, 'Species')
pred &lt;- bst$predict(type="raw")
</pre>
</div>


<hr>
<a id="method-KNNTrainer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>KNNTrainer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>data("iris")

iris$Species &lt;- as.integer(as.factor(iris$Species))

xtrain &lt;- iris[1:100,]
xtest &lt;- iris[101:150,]

bst &lt;- KNNTrainer$new(k=3, prob=TRUE, type="class")
bst$fit(xtrain, xtest, 'Species')
pred &lt;- bst$predict(type="raw")

## ------------------------------------------------
## Method `KNNTrainer$new`
## ------------------------------------------------

data("iris")

iris$Species &lt;- as.integer(as.factor(iris$Species))

xtrain &lt;- iris[1:100,]
xtest &lt;- iris[101:150,]

bst &lt;- KNNTrainer$new(k=3, prob=TRUE, type="class")
bst$fit(xtrain, xtest, 'Species')
pred &lt;- bst$predict(type="raw")

## ------------------------------------------------
## Method `KNNTrainer$fit`
## ------------------------------------------------

data("iris")

iris$Species &lt;- as.integer(as.factor(iris$Species))

xtrain &lt;- iris[1:100,]
xtest &lt;- iris[101:150,]

bst &lt;- KNNTrainer$new(k=3, prob=TRUE, type="class")
bst$fit(xtrain, xtest, 'Species')

## ------------------------------------------------
## Method `KNNTrainer$predict`
## ------------------------------------------------

data("iris")

iris$Species &lt;- as.integer(as.factor(iris$Species))

xtrain &lt;- iris[1:100,]
xtest &lt;- iris[101:150,]

bst &lt;- KNNTrainer$new(k=3, prob=TRUE, type="class")
bst$fit(xtrain, xtest, 'Species')
pred &lt;- bst$predict(type="raw")
</code></pre>

<hr>
<h2 id='LabelEncoder'>Label Encoder</h2><span id='topic+LabelEncoder'></span>

<h3>Description</h3>

<p>Encodes and decodes categorical variables into integer values and vice versa.
This is a commonly performed task in data preparation during model training, because all machine learning models require
the data to be encoded into numerical format. It takes a vector of character or factor values and encodes them into numeric.
</p>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object.
</p>


<h3>Usage</h3>

<p>For usage details see <b>Methods, Arguments and Examples</b> sections.
</p>
<pre>
lbl = LabelEncoder$new()
lbl$fit(x)
lbl$fit_transform(x)
lbl$transform(x)
</pre>


<h3>Methods</h3>


<dl>
<dt><code>$new()</code></dt><dd><p>Initialise the instance of the encoder</p>
</dd>
<dt><code>$fit()</code></dt><dd><p>creates a memory of encodings but doesn't return anything</p>
</dd>
<dt><code>$transform()</code></dt><dd><p>based on encodings learned in <code>fit</code> method is applies the transformation</p>
</dd>
<dt><code>$fit_transform()</code></dt><dd><p>encodes the data and keep a memory of encodings simultaneously</p>
</dd>
<dt><code>$inverse_transform()</code></dt><dd><p>encodes the data and keep a memory of encodings simultaneously</p>
</dd>
</dl>



<h3>Arguments</h3>


<dl>
<dt>data</dt><dd><p>a vector or list containing the character / factor values</p>
</dd>
</dl>



<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>input_data</code></dt><dd><p>internal use</p>
</dd>
<dt><code>encodings</code></dt><dd><p>internal use</p>
</dd>
<dt><code>decodings</code></dt><dd><p>internal use</p>
</dd>
<dt><code>fit_model</code></dt><dd><p>internal use</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LabelEncoder-fit"><code>LabelEncoder$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-LabelEncoder-fit_transform"><code>LabelEncoder$fit_transform()</code></a>
</p>
</li>
<li> <p><a href="#method-LabelEncoder-transform"><code>LabelEncoder$transform()</code></a>
</p>
</li>
<li> <p><a href="#method-LabelEncoder-inverse_transform"><code>LabelEncoder$inverse_transform()</code></a>
</p>
</li>
<li> <p><a href="#method-LabelEncoder-clone"><code>LabelEncoder$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-LabelEncoder-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>LabelEncoder$fit(data_col)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_col</code></dt><dd><p>a vector containing non-null values</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Fits the labelencoder model on given data
</p>



<h5>Returns</h5>

<p>NULL, calculates the encoding and save in memory
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data_ex &lt;- data.frame(Score = c(10,20,30,4), Name=c('Ao','Bo','Bo','Co'))
lbl &lt;- LabelEncoder$new()
lbl$fit(data_ex$Name)
data_ex$Name &lt;- lbl$fit_transform(data_ex$Name)
decode_names &lt;- lbl$inverse_transform(data_ex$Name)
</pre>
</div>


<hr>
<a id="method-LabelEncoder-fit_transform"></a>



<h4>Method <code>fit_transform()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>LabelEncoder$fit_transform(data_col)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_col</code></dt><dd><p>a vector containing non-null values</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Fits and returns the encoding
</p>



<h5>Returns</h5>

<p>encoding values for the given input data
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data_ex &lt;- data.frame(Score = c(10,20,30,4), Name=c('Ao','Bo','Bo','Co'))
lbl &lt;- LabelEncoder$new()
lbl$fit(data_ex$Name)
data_ex$Name &lt;- lbl$fit_transform(data_ex$Name)
</pre>
</div>


<hr>
<a id="method-LabelEncoder-transform"></a>



<h4>Method <code>transform()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>LabelEncoder$transform(data_col)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_col</code></dt><dd><p>a vector containing non-null values</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Returns the encodings from the fitted model
</p>



<h5>Returns</h5>

<p>encoding values for the given input data
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data_ex &lt;- data.frame(Score = c(10,20,30,4), Name=c('Ao','Bo','Bo','Co'))
lbl &lt;- LabelEncoder$new()
lbl$fit(data_ex$Name)
data_ex$Name &lt;- lbl$transform(data_ex$Name)
</pre>
</div>


<hr>
<a id="method-LabelEncoder-inverse_transform"></a>



<h4>Method <code>inverse_transform()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>LabelEncoder$inverse_transform(coded_col)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>coded_col</code></dt><dd><p>a vector containing label encoded values</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Gives back the original values from a encoded values
</p>



<h5>Returns</h5>

<p>original values from the label encoded data
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data_ex &lt;- data.frame(Score = c(10,20,30,4), Name=c('Ao','Bo','Bo','Co'))
lbl &lt;- LabelEncoder$new()
lbl$fit(data_ex$Name)
data_ex$Name &lt;- lbl$fit_transform(data_ex$Name)
decode_names &lt;- lbl$inverse_transform(data_ex$Name)
</pre>
</div>


<hr>
<a id="method-LabelEncoder-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LabelEncoder$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>data_ex &lt;- data.frame(Score = c(10,20,30,4), Name=c('Ao','Bo','Bo','Co'))
lbl &lt;- LabelEncoder$new()
data_ex$Name &lt;- lbl$fit_transform(data_ex$Name)
decode_names &lt;- lbl$inverse_transform(data_ex$Name)

## ------------------------------------------------
## Method `LabelEncoder$fit`
## ------------------------------------------------

data_ex &lt;- data.frame(Score = c(10,20,30,4), Name=c('Ao','Bo','Bo','Co'))
lbl &lt;- LabelEncoder$new()
lbl$fit(data_ex$Name)
data_ex$Name &lt;- lbl$fit_transform(data_ex$Name)
decode_names &lt;- lbl$inverse_transform(data_ex$Name)

## ------------------------------------------------
## Method `LabelEncoder$fit_transform`
## ------------------------------------------------

data_ex &lt;- data.frame(Score = c(10,20,30,4), Name=c('Ao','Bo','Bo','Co'))
lbl &lt;- LabelEncoder$new()
lbl$fit(data_ex$Name)
data_ex$Name &lt;- lbl$fit_transform(data_ex$Name)

## ------------------------------------------------
## Method `LabelEncoder$transform`
## ------------------------------------------------

data_ex &lt;- data.frame(Score = c(10,20,30,4), Name=c('Ao','Bo','Bo','Co'))
lbl &lt;- LabelEncoder$new()
lbl$fit(data_ex$Name)
data_ex$Name &lt;- lbl$transform(data_ex$Name)

## ------------------------------------------------
## Method `LabelEncoder$inverse_transform`
## ------------------------------------------------

data_ex &lt;- data.frame(Score = c(10,20,30,4), Name=c('Ao','Bo','Bo','Co'))
lbl &lt;- LabelEncoder$new()
lbl$fit(data_ex$Name)
data_ex$Name &lt;- lbl$fit_transform(data_ex$Name)
decode_names &lt;- lbl$inverse_transform(data_ex$Name)
</code></pre>

<hr>
<h2 id='LMTrainer'>Linear Models Trainer</h2><span id='topic+LMTrainer'></span>

<h3>Description</h3>

<p>Trains regression, lasso, ridge model in R
</p>


<h3>Details</h3>

<p>Trains linear models such as Logistic, Lasso or Ridge regression model. It is built on glmnet R package.
This class provides fit, predict, cross valdidation functions.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>family</code></dt><dd><p>type of regression to perform, values can be &quot;gaussian&quot; ,&quot;binomial&quot;, &quot;multinomial&quot;,&quot;mgaussian&quot;</p>
</dd>
<dt><code>weights</code></dt><dd><p>observation weights. Can be total counts if responses are proportion matrices. Default is 1 for each observation</p>
</dd>
<dt><code>alpha</code></dt><dd><p>The elasticnet mixing parameter, alpha=1 is the lasso penalty, alpha=0 the ridge penalty, alpha=NULL is simple regression</p>
</dd>
<dt><code>lambda</code></dt><dd><p>the number of lambda values - default is 100</p>
</dd>
<dt><code>standardize</code></dt><dd><p>normalise the features in the given data</p>
</dd>
<dt><code>standardize.response</code></dt><dd><p>normalise the dependent variable between 0 and 1, default = FALSE</p>
</dd>
<dt><code>model</code></dt><dd><p>internal use</p>
</dd>
<dt><code>cvmodel</code></dt><dd><p>internal use</p>
</dd>
<dt><code>Flag</code></dt><dd><p>internal use</p>
</dd>
<dt><code>is_lasso</code></dt><dd><p>internal use</p>
</dd>
<dt><code>iid_names</code></dt><dd><p>internal use</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LMTrainer-new"><code>LMTrainer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LMTrainer-fit"><code>LMTrainer$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-LMTrainer-predict"><code>LMTrainer$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-LMTrainer-cv_model"><code>LMTrainer$cv_model()</code></a>
</p>
</li>
<li> <p><a href="#method-LMTrainer-cv_predict"><code>LMTrainer$cv_predict()</code></a>
</p>
</li>
<li> <p><a href="#method-LMTrainer-get_importance"><code>LMTrainer$get_importance()</code></a>
</p>
</li>
<li> <p><a href="#method-LMTrainer-clone"><code>LMTrainer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-LMTrainer-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>LMTrainer$new(family, weights, alpha, lambda, standardize.response)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>family</code></dt><dd><p>character, type of regression to perform, values can be &quot;gaussian&quot; ,&quot;binomial&quot;, &quot;multinomial&quot;,&quot;mgaussian&quot;</p>
</dd>
<dt><code>weights</code></dt><dd><p>numeric, observation weights. Can be total counts if responses are proportion matrices. Default is 1 for each observation</p>
</dd>
<dt><code>alpha</code></dt><dd><p>integer, The elasticnet mixing parameter, alpha=1 is the lasso penalty, alpha=0 the ridge penalty, alpha=NULL is simple regression</p>
</dd>
<dt><code>lambda</code></dt><dd><p>integer, the number of lambda values - default is 100</p>
</dd>
<dt><code>standardize.response</code></dt><dd><p>logical, normalise the dependent variable between 0 and 1, default = FALSE</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'LMTrainer' object.
</p>



<h5>Returns</h5>

<p>A 'LMTrainer' object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
}
</pre>
</div>


<hr>
<a id="method-LMTrainer-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>LMTrainer$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame containing train featuers</p>
</dd>
<dt><code>y</code></dt><dd><p>character, name of target variable</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Fits the LMTrainer model on given data
</p>



<h5>Returns</h5>

<p>NULL, train the model and saves internally
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
lf$fit(X = housing, y = 'MEDV')
}
</pre>
</div>


<hr>
<a id="method-LMTrainer-predict"></a>



<h4>Method <code>predict()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>LMTrainer$predict(df, lambda = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>df</code></dt><dd><p>data.frame containing test features</p>
</dd>
<dt><code>lambda</code></dt><dd><p>integer, the number of lambda values - default is 100. By default it picks the best value from the model.</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Returns predictions for test data
</p>



<h5>Returns</h5>

<p>vector, a vector containing predictions
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
lf$fit(X = housing, y = 'MEDV')
predictions &lt;- lf$cv_predict(df = housing)
}
</pre>
</div>


<hr>
<a id="method-LMTrainer-cv_model"></a>



<h4>Method <code>cv_model()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>LMTrainer$cv_model(X, y, nfolds, parallel, type.measure = "deviance")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame containing test features</p>
</dd>
<dt><code>y</code></dt><dd><p>character, name of target variable</p>
</dd>
<dt><code>nfolds</code></dt><dd><p>integer, number of folds</p>
</dd>
<dt><code>parallel</code></dt><dd><p>logical, if do parallel computation. Default=FALSE</p>
</dd>
<dt><code>type.measure</code></dt><dd><p>character, evaluation metric type. Default = deviance</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Train regression model using cross validation
</p>



<h5>Returns</h5>

<p>NULL, trains the model and saves it in memory
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
lf$cv_model(X = housing, y = 'MEDV', nfolds = 5, parallel = FALSE)
}
</pre>
</div>


<hr>
<a id="method-LMTrainer-cv_predict"></a>



<h4>Method <code>cv_predict()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>LMTrainer$cv_predict(df, lambda = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>df</code></dt><dd><p>data.frame containing test features</p>
</dd>
<dt><code>lambda</code></dt><dd><p>integer, the number of lambda values - default is 100. By default it picks the best value from the model.</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Get predictions from the cross validated regression model
</p>



<h5>Returns</h5>

<p>vector a vector containing predicted values
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
lf$cv_model(X = housing, y = 'MEDV', nfolds = 5, parallel = FALSE)
predictions &lt;- lf$cv_predict(df = housing)
}
</pre>
</div>


<hr>
<a id="method-LMTrainer-get_importance"></a>



<h4>Method <code>get_importance()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>LMTrainer$get_importance()</pre></div>



<h5>Details</h5>

<p>Get feature importance using model coefficients
</p>



<h5>Returns</h5>

<p>a matrix containing feature coefficients
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
lf$cv_model(X = housing, y = 'MEDV', nfolds = 5, parallel = FALSE)
predictions &lt;- lf$cv_predict(df = housing)
coefs &lt;- lf$get_importance()
}
</pre>
</div>


<hr>
<a id="method-LMTrainer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LMTrainer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `LMTrainer$new`
## ------------------------------------------------

## Not run: 
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)

## End(Not run)

## ------------------------------------------------
## Method `LMTrainer$fit`
## ------------------------------------------------

## Not run: 
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
lf$fit(X = housing, y = 'MEDV')

## End(Not run)

## ------------------------------------------------
## Method `LMTrainer$predict`
## ------------------------------------------------

## Not run: 
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
lf$fit(X = housing, y = 'MEDV')
predictions &lt;- lf$cv_predict(df = housing)

## End(Not run)

## ------------------------------------------------
## Method `LMTrainer$cv_model`
## ------------------------------------------------

## Not run: 
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
lf$cv_model(X = housing, y = 'MEDV', nfolds = 5, parallel = FALSE)

## End(Not run)

## ------------------------------------------------
## Method `LMTrainer$cv_predict`
## ------------------------------------------------

## Not run: 
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
lf$cv_model(X = housing, y = 'MEDV', nfolds = 5, parallel = FALSE)
predictions &lt;- lf$cv_predict(df = housing)

## End(Not run)

## ------------------------------------------------
## Method `LMTrainer$get_importance`
## ------------------------------------------------

## Not run: 
LINK &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
housing &lt;- read.table(LINK)
names &lt;- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
           "RAD","TAX","PTRATIO","B","LSTAT","MEDV")
names(housing)  &lt;-  names
lf &lt;- LMTrainer$new(family = 'gaussian', alpha=1)
lf$cv_model(X = housing, y = 'MEDV', nfolds = 5, parallel = FALSE)
predictions &lt;- lf$cv_predict(df = housing)
coefs &lt;- lf$get_importance()

## End(Not run)
</code></pre>

<hr>
<h2 id='NBTrainer'>Naive Bayes Trainer</h2><span id='topic+NBTrainer'></span>

<h3>Description</h3>

<p>Trains a probabilistic naive bayes model
</p>


<h3>Details</h3>

<p>Trains a naive bayes model. It is built on top high performance naivebayes R package.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>prior</code></dt><dd><p>numeric vector with prior probabilities. vector with prior probabilities of the classes.
If unspecified, the class proportions for the training set are used.
If present, the probabilities should be specified in the order of the factor levels.</p>
</dd>
<dt><code>laplace</code></dt><dd><p>value used for Laplace smoothing. Defaults to 0 (no Laplace smoothing)</p>
</dd>
<dt><code>usekernel</code></dt><dd><p>if TRUE, density is used to estimate the densities of metric predictors</p>
</dd>
<dt><code>model</code></dt><dd><p>for internal use</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-NBTrainer-new"><code>NBTrainer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-NBTrainer-fit"><code>NBTrainer$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-NBTrainer-predict"><code>NBTrainer$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-NBTrainer-clone"><code>NBTrainer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-NBTrainer-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>NBTrainer$new(prior, laplace, usekernel)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>prior</code></dt><dd><p>numeric, prior numeric vector with prior probabilities. vector with prior probabilities of the classes.
If unspecified, the class proportions for the training set are used.
If present, the probabilities should be specified in the order of the factor levels.</p>
</dd>
<dt><code>laplace</code></dt><dd><p>nuemric, value used for Laplace smoothing. Defaults to 0 (no Laplace smoothing)</p>
</dd>
<dt><code>usekernel</code></dt><dd><p>logical, if TRUE, density is used to estimate the densities of metric predictors</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'NBTrainer' object.
</p>



<h5>Returns</h5>

<p>A 'NBTrainer' object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(iris)
nb &lt;- NBTrainer$new()
</pre>
</div>


<hr>
<a id="method-NBTrainer-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>NBTrainer$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame containing train features</p>
</dd>
<dt><code>y</code></dt><dd><p>character, name of target variable</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Fits the naive bayes model
</p>



<h5>Returns</h5>

<p>NULL, trains and saves the model in memory
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(iris)
nb &lt;- NBTrainer$new()
nb$fit(iris, 'Species')
</pre>
</div>


<hr>
<a id="method-NBTrainer-predict"></a>



<h4>Method <code>predict()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>NBTrainer$predict(X, type = "class")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame containing test features</p>
</dd>
<dt><code>type</code></dt><dd><p>character, if the predictions should be labels or probability</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Returns predictions from the model
</p>



<h5>Returns</h5>

<p>NULL, trains and saves the model in memory
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data(iris)
nb &lt;- NBTrainer$new()
nb$fit(iris, 'Species')
y &lt;- nb$predict(iris)
</pre>
</div>


<hr>
<a id="method-NBTrainer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>NBTrainer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `NBTrainer$new`
## ------------------------------------------------

data(iris)
nb &lt;- NBTrainer$new()

## ------------------------------------------------
## Method `NBTrainer$fit`
## ------------------------------------------------

data(iris)
nb &lt;- NBTrainer$new()
nb$fit(iris, 'Species')

## ------------------------------------------------
## Method `NBTrainer$predict`
## ------------------------------------------------

data(iris)
nb &lt;- NBTrainer$new()
nb$fit(iris, 'Species')
y &lt;- nb$predict(iris)
</code></pre>

<hr>
<h2 id='normalise1d'>normalise1d</h2><span id='topic+normalise1d'></span>

<h3>Description</h3>

<p>Normalises a 1 dimensional vector towards unit p norm. By default, p = 2 is used.
For a given vector, eg: c(1,2,3), norm value is calculated as 'x / |x|' where '|x|' is calculated
as the square root of sum of square of values in the given vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalise1d(vec, pnorm = 2L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalise1d_+3A_vec">vec</code></td>
<td>
<p>vector containing integers or numeric values.</p>
</td></tr>
<tr><td><code id="normalise1d_+3A_pnorm">pnorm</code></td>
<td>
<p>integer, default: 2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector containing normalised values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>val &lt;- c(1,10,5,3,8)
norm_val &lt;- normalise1d(val)

</code></pre>

<hr>
<h2 id='normalise2d'>normalise2d</h2><span id='topic+normalise2d'></span>

<h3>Description</h3>

<p>Normalises a matrix towards unit p norm row wise or column wise. By default, p = 2 is used.
To normalise row wise, use axis=0. To normalise column wise, use axis=1.
as the square root of sum of square of values in the given vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalise2d(mat, pnorm = 2L, axis = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalise2d_+3A_mat">mat</code></td>
<td>
<p>numeric matrix</p>
</td></tr>
<tr><td><code id="normalise2d_+3A_pnorm">pnorm</code></td>
<td>
<p>integer value, default value=2</p>
</td></tr>
<tr><td><code id="normalise2d_+3A_axis">axis</code></td>
<td>
<p>integer (0 or 1), row wise = 0,  column wise = 1</p>
</td></tr>
</table>


<h3>Value</h3>

<p>normalised numeric matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mat &lt;- matrix(runif(12), 3, 4)

## normalise matrix row wise
r &lt;- normalise2d(mat, axis=0)

## normalise matrix column wise
r &lt;- normalise2d(mat, axis=1)

</code></pre>

<hr>
<h2 id='RandomSearchCV'>Random Search CV</h2><span id='topic+RandomSearchCV'></span>

<h3>Description</h3>

<p>Hyperparameter tuning using random search scheme.
</p>


<h3>Details</h3>

<p>Given a set of hyper parameters, random search trainer provides a faster way of hyper parameter tuning.
Here, the number of models to be trained can be defined by the user.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+GridSearchCV">superml::GridSearchCV</a></code> -&gt; <code>RandomSearchTrainer</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>n_iter</code></dt><dd><p>number of models to be trained</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-RandomSearchTrainer-new"><code>RandomSearchCV$new()</code></a>
</p>
</li>
<li> <p><a href="#method-RandomSearchTrainer-fit"><code>RandomSearchCV$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-RandomSearchTrainer-clone"><code>RandomSearchCV$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="superml" data-topic="GridSearchCV" data-id="best_iteration"><a href='../../superml/html/GridSearchCV.html#method-GridSearchCV-best_iteration'><code>superml::GridSearchCV$best_iteration()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-RandomSearchTrainer-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>RandomSearchCV$new(
  trainer = NA,
  parameters = NA,
  n_folds = NA,
  scoring = NA,
  n_iter = NA
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>trainer</code></dt><dd><p>superml trainer object, must be either XGBTrainer, LMTrainer, RFTrainer, NBTrainer</p>
</dd>
<dt><code>parameters</code></dt><dd><p>list, list containing parameters</p>
</dd>
<dt><code>n_folds</code></dt><dd><p>integer, number of folds to use to split the train data</p>
</dd>
<dt><code>scoring</code></dt><dd><p>character, scoring metric used to evaluate the best model, multiple values can be provided.
currently supports: auc, accuracy, mse, rmse, logloss, mae, f1, precision, recall</p>
</dd>
<dt><code>n_iter</code></dt><dd><p>integer, number of models to be trained</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'RandomSearchTrainer' object.
</p>



<h5>Returns</h5>

<p>A 'RandomSearchTrainer' object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>rf &lt;- RFTrainer$new()
rst &lt;-RandomSearchCV$new(trainer = rf,
                            parameters = list(n_estimators = c(100,500),
                            max_depth = c(5,2,10,14)),
                            n_folds = 3,
                            scoring = c('accuracy','auc'),
                            n_iter = 4)
</pre>
</div>


<hr>
<a id="method-RandomSearchTrainer-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>RandomSearchCV$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame containing features</p>
</dd>
<dt><code>y</code></dt><dd><p>character, name of target variable</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Train the model on given hyperparameters
</p>



<h5>Returns</h5>

<p>NULL, tunes hyperparameters and stores the result in memory
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>rf &lt;- RFTrainer$new()
rst &lt;-RandomSearchCV$new(trainer = rf,
                            parameters = list(n_estimators = c(100,500),
                            max_depth = c(5,2,10,14)),
                            n_folds = 3,
                            scoring = c('accuracy','auc'),
                            n_iter = 4)
data("iris")
rst$fit(iris, "Species")
rst$best_iteration()
</pre>
</div>


<hr>
<a id="method-RandomSearchTrainer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>RandomSearchCV$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `RandomSearchCV$new`
## ------------------------------------------------

rf &lt;- RFTrainer$new()
rst &lt;-RandomSearchCV$new(trainer = rf,
                            parameters = list(n_estimators = c(100,500),
                            max_depth = c(5,2,10,14)),
                            n_folds = 3,
                            scoring = c('accuracy','auc'),
                            n_iter = 4)

## ------------------------------------------------
## Method `RandomSearchCV$fit`
## ------------------------------------------------

rf &lt;- RFTrainer$new()
rst &lt;-RandomSearchCV$new(trainer = rf,
                            parameters = list(n_estimators = c(100,500),
                            max_depth = c(5,2,10,14)),
                            n_folds = 3,
                            scoring = c('accuracy','auc'),
                            n_iter = 4)
data("iris")
rst$fit(iris, "Species")
rst$best_iteration()
</code></pre>

<hr>
<h2 id='reg_train'>reg_train</h2><span id='topic+reg_train'></span>

<h3>Description</h3>

<p>Training Dataset used for regression examples. In this data set, we have to predict
the sale price of the houses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_train
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.table</code> (inherits from <code>data.frame</code>) with 1460 rows and 81 columns.
</p>


<h3>Source</h3>

<p><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a>
</p>

<hr>
<h2 id='RFTrainer'>Random Forest Trainer</h2><span id='topic+RFTrainer'></span>

<h3>Description</h3>

<p>Trains a random forest model.
</p>


<h3>Details</h3>

<p>Trains a Random Forest model. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.
This implementation uses ranger R package which provides faster model training.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>n_estimators</code></dt><dd><p>the number of trees in the forest, default= 100</p>
</dd>
<dt><code>max_features</code></dt><dd><p>the number of features to consider when looking for the best split.
Possible values are <code>auto(default)</code> takes sqrt(num_of_features),
<code>sqrt</code> same as auto,
<code>log</code> takes log(num_of_features),
<code>none</code> takes all features</p>
</dd>
<dt><code>max_depth</code></dt><dd><p>the maximum depth of each tree</p>
</dd>
<dt><code>min_node_size</code></dt><dd><p>the minumum number of samples required to split an internal node</p>
</dd>
<dt><code>criterion</code></dt><dd><p>the function to measure the quality of split. For classification, <code>gini</code> is used which
is a measure of gini index. For regression, the <code>variance</code> of responses is used.</p>
</dd>
<dt><code>classification</code></dt><dd><p>whether to train for classification (1) or regression (0)</p>
</dd>
<dt><code>verbose</code></dt><dd><p>show computation status and estimated runtime</p>
</dd>
<dt><code>seed</code></dt><dd><p>seed value</p>
</dd>
<dt><code>class_weights</code></dt><dd><p>weights associated with the classes for sampling of training observation</p>
</dd>
<dt><code>always_split</code></dt><dd><p>vector of feature names to be always used for splitting</p>
</dd>
<dt><code>importance</code></dt><dd><p>Variable importance mode, one of 'none', 'impurity', 'impurity_corrected', 'permutation'. The 'impurity' measure is the Gini index for classification, the variance of the responses for regression. Defaults to &quot;impurity&quot;</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-RFTrainer-new"><code>RFTrainer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-RFTrainer-fit"><code>RFTrainer$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-RFTrainer-predict"><code>RFTrainer$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-RFTrainer-get_importance"><code>RFTrainer$get_importance()</code></a>
</p>
</li>
<li> <p><a href="#method-RFTrainer-clone"><code>RFTrainer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-RFTrainer-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>RFTrainer$new(
  n_estimators,
  max_depth,
  max_features,
  min_node_size,
  classification,
  class_weights,
  always_split,
  verbose,
  save_model,
  seed,
  importance
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>n_estimators</code></dt><dd><p>integer, the number of trees in the forest, default= 100</p>
</dd>
<dt><code>max_depth</code></dt><dd><p>integer, the maximum depth of each tree</p>
</dd>
<dt><code>max_features</code></dt><dd><p>integer, the number of features to consider when looking for the best split.
Possible values are <code>auto(default)</code> takes sqrt(num_of_features),
<code>sqrt</code> same as auto,
<code>log</code> takes log(num_of_features),
<code>none</code> takes all features</p>
</dd>
<dt><code>min_node_size</code></dt><dd><p>integer, the minumum number of samples required to split an internal node</p>
</dd>
<dt><code>classification</code></dt><dd><p>integer, whether to train for classification (1) or regression (0)</p>
</dd>
<dt><code>class_weights</code></dt><dd><p>weights associated with the classes for sampling of training observation</p>
</dd>
<dt><code>always_split</code></dt><dd><p>vector of feature names to be always used for splitting</p>
</dd>
<dt><code>verbose</code></dt><dd><p>logical, show computation status and estimated runtime</p>
</dd>
<dt><code>save_model</code></dt><dd><p>logical, whether to save model</p>
</dd>
<dt><code>seed</code></dt><dd><p>integer, seed value</p>
</dd>
<dt><code>importance</code></dt><dd><p>Variable importance mode, one of 'none', 'impurity', 'impurity_corrected', 'permutation'. The 'impurity' measure is the Gini index for classification, the variance of the responses for regression. Defaults to &quot;impurity&quot;</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'RFTrainer' object.
</p>



<h5>Returns</h5>

<p>A 'RFTrainer' object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data("iris")
bst &lt;- RFTrainer$new(n_estimators=10,
                     max_depth=4,
                     classification=1,
                     seed=42,
                     verbose=TRUE)
</pre>
</div>


<hr>
<a id="method-RFTrainer-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>RFTrainer$fit(X, y)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame containing train features</p>
</dd>
<dt><code>y</code></dt><dd><p>character, name of the target variable</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Trains the random forest model
</p>



<h5>Returns</h5>

<p>NULL, trains and saves the model in memory
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data("iris")
bst &lt;- RFTrainer$new(n_estimators=10,
                     max_depth=4,
                     classification=1,
                     seed=42,
                     verbose=TRUE)
bst$fit(iris, 'Species')
</pre>
</div>


<hr>
<a id="method-RFTrainer-predict"></a>



<h4>Method <code>predict()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>RFTrainer$predict(df)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>df</code></dt><dd><p>data.frame containing test features</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Return predictions from random forest model
</p>



<h5>Returns</h5>

<p>a vector containing predictions
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data("iris")
bst &lt;- RFTrainer$new(n_estimators=10,
                     max_depth=4,
                     classification=1,
                     seed=42,
                     verbose=TRUE)
bst$fit(iris, 'Species')
predictions &lt;- bst$predict(iris)
</pre>
</div>


<hr>
<a id="method-RFTrainer-get_importance"></a>



<h4>Method <code>get_importance()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>RFTrainer$get_importance()</pre></div>



<h5>Details</h5>

<p>Returns feature importance from the model
</p>



<h5>Returns</h5>

<p>a data frame containing feature predictions
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>data("iris")
bst &lt;- RFTrainer$new(n_estimators=50,
                     max_depth=4,
                     classification=1,
                     seed=42,
                     verbose=TRUE)
bst$fit(iris, 'Species')
predictions &lt;- bst$predict(iris)
bst$get_importance()
</pre>
</div>


<hr>
<a id="method-RFTrainer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>RFTrainer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `RFTrainer$new`
## ------------------------------------------------

data("iris")
bst &lt;- RFTrainer$new(n_estimators=10,
                     max_depth=4,
                     classification=1,
                     seed=42,
                     verbose=TRUE)

## ------------------------------------------------
## Method `RFTrainer$fit`
## ------------------------------------------------

data("iris")
bst &lt;- RFTrainer$new(n_estimators=10,
                     max_depth=4,
                     classification=1,
                     seed=42,
                     verbose=TRUE)
bst$fit(iris, 'Species')

## ------------------------------------------------
## Method `RFTrainer$predict`
## ------------------------------------------------

data("iris")
bst &lt;- RFTrainer$new(n_estimators=10,
                     max_depth=4,
                     classification=1,
                     seed=42,
                     verbose=TRUE)
bst$fit(iris, 'Species')
predictions &lt;- bst$predict(iris)

## ------------------------------------------------
## Method `RFTrainer$get_importance`
## ------------------------------------------------

data("iris")
bst &lt;- RFTrainer$new(n_estimators=50,
                     max_depth=4,
                     classification=1,
                     seed=42,
                     verbose=TRUE)
bst$fit(iris, 'Species')
predictions &lt;- bst$predict(iris)
bst$get_importance()
</code></pre>

<hr>
<h2 id='smoothMean'>smoothMean Calculator</h2><span id='topic+smoothMean'></span>

<h3>Description</h3>

<p>Calculates target encodings using a smoothing parameter and count of categorical variables.
This approach is more robust to possibility of leakage and avoid overfitting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothMean(
  train_df,
  test_df,
  colname,
  target,
  min_samples_leaf = 1,
  smoothing = 1,
  noise_level = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothMean_+3A_train_df">train_df</code></td>
<td>
<p>train dataset</p>
</td></tr>
<tr><td><code id="smoothMean_+3A_test_df">test_df</code></td>
<td>
<p>test dataset</p>
</td></tr>
<tr><td><code id="smoothMean_+3A_colname">colname</code></td>
<td>
<p>name of categorical column</p>
</td></tr>
<tr><td><code id="smoothMean_+3A_target">target</code></td>
<td>
<p>name of target column</p>
</td></tr>
<tr><td><code id="smoothMean_+3A_min_samples_leaf">min_samples_leaf</code></td>
<td>
<p>minimum samples to take category average into account</p>
</td></tr>
<tr><td><code id="smoothMean_+3A_smoothing">smoothing</code></td>
<td>
<p>smoothing effect to balance categorical average vs prior</p>
</td></tr>
<tr><td><code id="smoothMean_+3A_noise_level">noise_level</code></td>
<td>
<p>random noise to add, optional</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a train and test data table with mean encodings
of the target for the given categorical variable
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- data.frame(region=c('del','csk','rcb','del','csk','pune','guj','del'),
                    win = c(0,1,1,0,0,1,0,1))
test &lt;- data.frame(region=c('rcb','csk','rcb','del','guj','pune','csk','kol'))

# calculate encodings
all_means &lt;- smoothMean(train_df = train,
                         test_df = test,
                         colname = 'region',
                         target = 'win')
train_mean &lt;- all_means$train
test_mean &lt;- all_means$test
</code></pre>

<hr>
<h2 id='sort_index'>sort_index</h2><span id='topic+sort_index'></span>

<h3>Description</h3>

<p>For a given vector, return the indexes of the sorted array and
not the sorted array itself.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sort_index(vec, ascending = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sort_index_+3A_vec">vec</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="sort_index_+3A_ascending">ascending</code></td>
<td>
<p>logical, order to return (ascending or descending), default = True</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector containing sorted indexes
</p>


<h3>Examples</h3>

<pre><code class='language-R'>v &lt;- c(10,3,1,4)
j &lt;- sort_index(v)

</code></pre>

<hr>
<h2 id='testdata'>Internal function</h2><span id='topic+testdata'></span>

<h3>Description</h3>

<p>Used to check the input data format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testdata(X, y, model = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="testdata_+3A_x">X</code></td>
<td>
<p>should be a data frame or data.table</p>
</td></tr>
<tr><td><code id="testdata_+3A_y">y</code></td>
<td>
<p>should be a string specifying the dependent variable</p>
</td></tr>
</table>


<h3>Value</h3>

<p>null
</p>

<hr>
<h2 id='TfIdfVectorizer'>TfIDF(Term Frequency Inverse Document Frequency) Vectorizer</h2><span id='topic+TfIdfVectorizer'></span>

<h3>Description</h3>

<p>Creates a tf-idf matrix
</p>


<h3>Details</h3>

<p>Given a list of text, it creates a sparse matrix consisting of tf-idf score for tokens from the text.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+CountVectorizer">superml::CountVectorizer</a></code> -&gt; <code>TfIdfVectorizer</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>sentences</code></dt><dd><p>a list containing sentences</p>
</dd>
<dt><code>max_df</code></dt><dd><p>When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold, value lies between 0 and 1.</p>
</dd>
<dt><code>min_df</code></dt><dd><p>When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold, value lies between 0 and 1.</p>
</dd>
<dt><code>max_features</code></dt><dd><p>use top features sorted by count to be used in bag of words matrix.</p>
</dd>
<dt><code>ngram_range</code></dt><dd><p>The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n &lt;= n &lt;= max_n will be used. For example an ngram_range of c(1, 1) means only unigrams, c(1, 2) means unigrams and bigrams, and c(2, 2) means only bigrams.</p>
</dd>
<dt><code>split</code></dt><dd><p>splitting criteria for strings, default: &quot; &quot;</p>
</dd>
<dt><code>lowercase</code></dt><dd><p>convert all characters to lowercase before tokenizing</p>
</dd>
<dt><code>regex</code></dt><dd><p>regex expression to use for text cleaning.</p>
</dd>
<dt><code>remove_stopwords</code></dt><dd><p>a list of stopwords to use, by default it uses its inbuilt list of standard stopwords</p>
</dd>
<dt><code>smooth_idf</code></dt><dd><p>logical, to prevent zero division, adds one to document frequencies, as if an extra document was seen containing every term in the collection exactly once</p>
</dd>
<dt><code>norm</code></dt><dd><p>logical, if TRUE, each output row will have unit norm ‘l2’: Sum of squares of vector elements is 1. if FALSE returns non-normalized vectors, default: TRUE</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TfIdfVectorizer-new"><code>TfIdfVectorizer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TfIdfVectorizer-fit"><code>TfIdfVectorizer$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-TfIdfVectorizer-fit_transform"><code>TfIdfVectorizer$fit_transform()</code></a>
</p>
</li>
<li> <p><a href="#method-TfIdfVectorizer-transform"><code>TfIdfVectorizer$transform()</code></a>
</p>
</li>
<li> <p><a href="#method-TfIdfVectorizer-clone"><code>TfIdfVectorizer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-TfIdfVectorizer-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>TfIdfVectorizer$new(
  min_df,
  max_df,
  max_features,
  ngram_range,
  regex,
  remove_stopwords,
  split,
  lowercase,
  smooth_idf,
  norm
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>min_df</code></dt><dd><p>numeric, When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold, value lies between 0 and 1.</p>
</dd>
<dt><code>max_df</code></dt><dd><p>numeric, When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold, value lies between 0 and 1.</p>
</dd>
<dt><code>max_features</code></dt><dd><p>integer, Build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</p>
</dd>
<dt><code>ngram_range</code></dt><dd><p>vector, The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n &lt;= n &lt;= max_n will be used. For example an ngram_range of c(1, 1) means only unigrams, c(1, 2) means unigrams and bigrams, and c(2, 2) means only bigrams.</p>
</dd>
<dt><code>regex</code></dt><dd><p>character, regex expression to use for text cleaning.</p>
</dd>
<dt><code>remove_stopwords</code></dt><dd><p>list, a list of stopwords to use, by default it uses its inbuilt list of standard english stopwords</p>
</dd>
<dt><code>split</code></dt><dd><p>character, splitting criteria for strings, default: &quot; &quot;</p>
</dd>
<dt><code>lowercase</code></dt><dd><p>logical, convert all characters to lowercase before tokenizing, default: TRUE</p>
</dd>
<dt><code>smooth_idf</code></dt><dd><p>logical, to prevent zero division, adds one to document frequencies, as if an extra document was seen containing every term in the collection exactly once</p>
</dd>
<dt><code>norm</code></dt><dd><p>logical, if TRUE, each output row will have unit norm ‘l2’: Sum of squares of vector elements is 1. if FALSE returns non-normalized vectors, default: TRUE</p>
</dd>
<dt><code>parallel</code></dt><dd><p>logical,  speeds up ngrams computation using n-1 cores, defaults: TRUE</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'TfIdfVectorizer' object.
</p>



<h5>Returns</h5>

<p>A 'TfIdfVectorizer' object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>TfIdfVectorizer$new()
</pre>
</div>


<hr>
<a id="method-TfIdfVectorizer-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>TfIdfVectorizer$fit(sentences)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sentences</code></dt><dd><p>a list of text sentences</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Fits the TfIdfVectorizer model on sentences
</p>



<h5>Returns</h5>

<p>NULL
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>sents = c('i am alone in dark.','mother_mary a lot',
          'alone in the dark?', 'many mothers in the lot....')
tf = TfIdfVectorizer$new(smooth_idf = TRUE, min_df = 0.3)
tf$fit(sents)
</pre>
</div>


<hr>
<a id="method-TfIdfVectorizer-fit_transform"></a>



<h4>Method <code>fit_transform()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>TfIdfVectorizer$fit_transform(sentences)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sentences</code></dt><dd><p>a list of text sentences</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Fits the TfIdfVectorizer model and returns a sparse matrix of count of tokens
</p>



<h5>Returns</h5>

<p>a sparse matrix containing tf-idf score for tokens in each given sentence
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
sents &lt;- c('i am alone in dark.','mother_mary a lot',
         'alone in the dark?', 'many mothers in the lot....')
tf &lt;- TfIdfVectorizer$new(smooth_idf = TRUE, min_df = 0.1)
tf_matrix &lt;- tf$fit_transform(sents)
}
</pre>
</div>


<hr>
<a id="method-TfIdfVectorizer-transform"></a>



<h4>Method <code>transform()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>TfIdfVectorizer$transform(sentences)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sentences</code></dt><dd><p>a list of new text sentences</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Returns a matrix of tf-idf score of tokens
</p>



<h5>Returns</h5>

<p>a sparse matrix containing tf-idf score for tokens in each given sentence
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
sents = c('i am alone in dark.','mother_mary a lot',
          'alone in the dark?', 'many mothers in the lot....')
new_sents &lt;- c("dark at night",'mothers day')
tf = TfIdfVectorizer$new(min_df=0.1)
tf$fit(sents)
tf_matrix &lt;- tf$transform(new_sents)
}
</pre>
</div>


<hr>
<a id="method-TfIdfVectorizer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TfIdfVectorizer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `TfIdfVectorizer$new`
## ------------------------------------------------

TfIdfVectorizer$new()

## ------------------------------------------------
## Method `TfIdfVectorizer$fit`
## ------------------------------------------------

sents = c('i am alone in dark.','mother_mary a lot',
          'alone in the dark?', 'many mothers in the lot....')
tf = TfIdfVectorizer$new(smooth_idf = TRUE, min_df = 0.3)
tf$fit(sents)

## ------------------------------------------------
## Method `TfIdfVectorizer$fit_transform`
## ------------------------------------------------

## Not run: 
sents &lt;- c('i am alone in dark.','mother_mary a lot',
         'alone in the dark?', 'many mothers in the lot....')
tf &lt;- TfIdfVectorizer$new(smooth_idf = TRUE, min_df = 0.1)
tf_matrix &lt;- tf$fit_transform(sents)

## End(Not run)

## ------------------------------------------------
## Method `TfIdfVectorizer$transform`
## ------------------------------------------------

## Not run: 
sents = c('i am alone in dark.','mother_mary a lot',
          'alone in the dark?', 'many mothers in the lot....')
new_sents &lt;- c("dark at night",'mothers day')
tf = TfIdfVectorizer$new(min_df=0.1)
tf$fit(sents)
tf_matrix &lt;- tf$transform(new_sents)

## End(Not run)
</code></pre>

<hr>
<h2 id='XGBTrainer'>Extreme Gradient Boosting Trainer</h2><span id='topic+XGBTrainer'></span>

<h3>Description</h3>

<p>Trains a XGBoost model in R
</p>


<h3>Details</h3>

<p>Trains a Extreme Gradient Boosting Model. XGBoost belongs to a family of boosting algorithms
that creates an ensemble of weak learner to learn about data. It is a wrapper for original xgboost
R package, you can find the documentation here: <a href="http://xgboost.readthedocs.io/en/latest/parameter.html">http://xgboost.readthedocs.io/en/latest/parameter.html</a>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>booster</code></dt><dd><p>the trainer type, the values are <code>gbtree(default)</code>, <code>gblinear</code>, <code>dart:gbtree</code></p>
</dd>
<dt><code>objective</code></dt><dd><p>specify the learning task. Check the link above for all possible values.</p>
</dd>
<dt><code>nthread</code></dt><dd><p>number of parallel threads used to run, default is to run using all threads available</p>
</dd>
<dt><code>silent</code></dt><dd><p>0 means printing running messages, 1 means silent mode</p>
</dd>
<dt><code>n_estimators</code></dt><dd><p>number of trees to grow, default = 100</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p>Step size shrinkage used in update to prevents overfitting. Lower the learning rate, more time it takes in training, value lies between between 0 and 1. Default = 0.3</p>
</dd>
<dt><code>gamma</code></dt><dd><p>Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. Value lies between 0 and infinity, Default = 0</p>
</dd>
<dt><code>max_depth</code></dt><dd><p>the maximum depth of each tree, default = 6</p>
</dd>
<dt><code>min_child_weight</code></dt><dd><p>Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be. Value lies between 0 and infinity. Default = 1</p>
</dd>
<dt><code>subsample</code></dt><dd><p>Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration. Value lies between 0 and 1. Default = 1</p>
</dd>
<dt><code>colsample_bytree</code></dt><dd><p>Subsample ratio of columns when constructing each tree. Subsampling will occur once in every boosting iteration. Value lies between 0 and 1. Default = 1</p>
</dd>
<dt><code>lambda</code></dt><dd><p>L2 regularization term on weights. Increasing this value will make model more conservative. Default = 1</p>
</dd>
<dt><code>alpha</code></dt><dd><p>L1 regularization term on weights. Increasing this value will make model more conservative. Default = 0</p>
</dd>
<dt><code>eval_metric</code></dt><dd><p>Evaluation metrics for validation data, a default metric will be assigned according to objective</p>
</dd>
<dt><code>print_every</code></dt><dd><p>print training log after n iterations. Default = 50</p>
</dd>
<dt><code>feval</code></dt><dd><p>custom evaluation function</p>
</dd>
<dt><code>early_stopping</code></dt><dd><p>Used to prevent overfitting, stops model training after this number of iterations if there is no improvement seen</p>
</dd>
<dt><code>maximize</code></dt><dd><p>If feval and early_stopping_rounds are set, then this parameter must be set as well. When it is TRUE, it means the larger the evaluation score the better.</p>
</dd>
<dt><code>custom_objective</code></dt><dd><p>custom objective function</p>
</dd>
<dt><code>save_period</code></dt><dd><p>when it is non-NULL, model is saved to disk after every save_period rounds, 0 means save at the end.</p>
</dd>
<dt><code>save_name</code></dt><dd><p>the name or path for periodically saved model file.</p>
</dd>
<dt><code>xgb_model</code></dt><dd><p>a previously built model to continue the training from. Could be either an object of class xgb.Booster, or its raw data, or the name of a file with a previously saved model.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>a list of callback functions to perform various task during boosting. See callbacks. Some of the callbacks are automatically created depending on the parameters' values. User can provide either existing or their own callback methods in order to customize the training process.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>If 0, xgboost will stay silent. If 1, xgboost will print information of performance. If 2, xgboost will print some additional information. Setting verbose &gt; 0 automatically engages the cb.evaluation.log and cb.print.evaluation callback functions.</p>
</dd>
<dt><code>watchlist</code></dt><dd><p>what information should be printed when verbose=1 or verbose=2. Watchlist is used to specify validation set monitoring during training. For example user can specify watchlist=list(validation1=mat1, validation2=mat2) to watch the performance of each round's model on mat1 and mat2</p>
</dd>
<dt><code>num_class</code></dt><dd><p>set number of classes in case of multiclassification problem</p>
</dd>
<dt><code>weight</code></dt><dd><p>a vector indicating the weight for each row of the input.</p>
</dd>
<dt><code>na_missing</code></dt><dd><p>by default is set to NA, which means that NA values should be considered as 'missing' by the algorithm. Sometimes, 0 or other extreme value might be used to represent missing values. This parameter is only used when input is a dense matrix.</p>
</dd>
<dt><code>feature_names</code></dt><dd><p>internal use, stores the feature names for model importance</p>
</dd>
<dt><code>cv_model</code></dt><dd><p>internal use</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-XGBTrainer-new"><code>XGBTrainer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-XGBTrainer-cross_val"><code>XGBTrainer$cross_val()</code></a>
</p>
</li>
<li> <p><a href="#method-XGBTrainer-fit"><code>XGBTrainer$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-XGBTrainer-predict"><code>XGBTrainer$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-XGBTrainer-show_importance"><code>XGBTrainer$show_importance()</code></a>
</p>
</li>
<li> <p><a href="#method-XGBTrainer-clone"><code>XGBTrainer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-XGBTrainer-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>XGBTrainer$new(
  booster,
  objective,
  nthread,
  silent,
  n_estimators,
  learning_rate,
  gamma,
  max_depth,
  min_child_weight,
  subsample,
  colsample_bytree,
  lambda,
  alpha,
  eval_metric,
  print_every,
  feval,
  early_stopping,
  maximize,
  custom_objective,
  save_period,
  save_name,
  xgb_model,
  callbacks,
  verbose,
  num_class,
  weight,
  na_missing
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>booster</code></dt><dd><p>the trainer type, the values are <code>gbtree(default)</code>, <code>gblinear</code>, <code>dart:gbtree</code></p>
</dd>
<dt><code>objective</code></dt><dd><p>specify the learning task. Check the link above for all possible values.</p>
</dd>
<dt><code>nthread</code></dt><dd><p>number of parallel threads used to run, default is to run using all threads available</p>
</dd>
<dt><code>silent</code></dt><dd><p>0 means printing running messages, 1 means silent mode</p>
</dd>
<dt><code>n_estimators</code></dt><dd><p>number of trees to grow, default = 100</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p>Step size shrinkage used in update to prevents overfitting. Lower the learning rate, more time it takes in training, value lies between between 0 and 1. Default = 0.3</p>
</dd>
<dt><code>gamma</code></dt><dd><p>Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. Value lies between 0 and infinity, Default = 0</p>
</dd>
<dt><code>max_depth</code></dt><dd><p>the maximum depth of each tree, default = 6</p>
</dd>
<dt><code>min_child_weight</code></dt><dd><p>Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be. Value lies between 0 and infinity. Default = 1</p>
</dd>
<dt><code>subsample</code></dt><dd><p>Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration. Value lies between 0 and 1. Default = 1</p>
</dd>
<dt><code>colsample_bytree</code></dt><dd><p>Subsample ratio of columns when constructing each tree. Subsampling will occur once in every boosting iteration. Value lies between 0 and 1. Default = 1</p>
</dd>
<dt><code>lambda</code></dt><dd><p>L2 regularization term on weights. Increasing this value will make model more conservative. Default = 1</p>
</dd>
<dt><code>alpha</code></dt><dd><p>L1 regularization term on weights. Increasing this value will make model more conservative. Default = 0</p>
</dd>
<dt><code>eval_metric</code></dt><dd><p>Evaluation metrics for validation data, a default metric will be assigned according to objective</p>
</dd>
<dt><code>print_every</code></dt><dd><p>print training log after n iterations. Default = 50</p>
</dd>
<dt><code>feval</code></dt><dd><p>custom evaluation function</p>
</dd>
<dt><code>early_stopping</code></dt><dd><p>Used to prevent overfitting, stops model training after this number of iterations if there is no improvement seen</p>
</dd>
<dt><code>maximize</code></dt><dd><p>If feval and early_stopping_rounds are set, then this parameter must be set as well. When it is TRUE, it means the larger the evaluation score the better.</p>
</dd>
<dt><code>custom_objective</code></dt><dd><p>custom objective function</p>
</dd>
<dt><code>save_period</code></dt><dd><p>when it is non-NULL, model is saved to disk after every save_period rounds, 0 means save at the end.</p>
</dd>
<dt><code>save_name</code></dt><dd><p>the name or path for periodically saved model file.</p>
</dd>
<dt><code>xgb_model</code></dt><dd><p>a previously built model to continue the training from. Could be either an object of class xgb.Booster, or its raw data, or the name of a file with a previously saved model.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>a list of callback functions to perform various task during boosting. See callbacks. Some of the callbacks are automatically created depending on the parameters' values. User can provide either existing or their own callback methods in order to customize the training process.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>If 0, xgboost will stay silent. If 1, xgboost will print information of performance. If 2, xgboost will print some additional information. Setting verbose &gt; 0 automatically engages the cb.evaluation.log and cb.print.evaluation callback functions.</p>
</dd>
<dt><code>num_class</code></dt><dd><p>set number of classes in case of multiclassification problem</p>
</dd>
<dt><code>weight</code></dt><dd><p>a vector indicating the weight for each row of the input.</p>
</dd>
<dt><code>na_missing</code></dt><dd><p>by default is set to NA, which means that NA values should be considered as 'missing' by the algorithm. Sometimes, 0 or other extreme value might be used to represent missing values. This parameter is only used when input is a dense matrix.</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Create a new 'XGBTrainer' object.
</p>



<h5>Returns</h5>

<p>A 'XGBTrainer' object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>library(data.table)
df &lt;- copy(iris)

# convert characters/factors to numeric
df$Species &lt;- as.numeric(as.factor(df$Species))-1

# initialise model
xgb &lt;- XGBTrainer$new(objective = 'multi:softmax',
                      maximize = FALSE,
                      eval_metric = 'merror',
                      num_class=3,
                      n_estimators = 2)
</pre>
</div>


<hr>
<a id="method-XGBTrainer-cross_val"></a>



<h4>Method <code>cross_val()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>XGBTrainer$cross_val(X, y, nfolds = 5, stratified = TRUE, folds = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame</p>
</dd>
<dt><code>y</code></dt><dd><p>character, name of target variable</p>
</dd>
<dt><code>nfolds</code></dt><dd><p>integer, number of folds</p>
</dd>
<dt><code>stratified</code></dt><dd><p>logical, whether to use stratified sampling</p>
</dd>
<dt><code>folds</code></dt><dd><p>the list of CV folds' indices - either those passed through the folds parameter or randomly generated.</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Trains the xgboost model using cross validation scheme
</p>



<h5>Returns</h5>

<p>NULL, trains a model and saves it in memory
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
library(data.table)
df &lt;- copy(iris)

# convert characters/factors to numeric
df$Species &lt;- as.numeric(as.factor(df$Species))-1

# initialise model
xgb &lt;- XGBTrainer$new(objective = 'multi:softmax',
                      maximize = FALSE,
                      eval_metric = 'merror',
                      num_class=3,
                      n_estimators = 2)

# do cross validation to find optimal value for n_estimators
xgb$cross_val(X = df, y = 'Species',nfolds = 3, stratified = TRUE)
}
</pre>
</div>


<hr>
<a id="method-XGBTrainer-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>XGBTrainer$fit(X, y, valid = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt><dd><p>data.frame, training data</p>
</dd>
<dt><code>y</code></dt><dd><p>character, name of target variable</p>
</dd>
<dt><code>valid</code></dt><dd><p>data.frame, validation data</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Fits the xgboost model on given data
</p>



<h5>Returns</h5>

<p>NULL, trains a model and keeps it in memory
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>library(data.table)
df &lt;- copy(iris)

# convert characters/factors to numeric
df$Species &lt;- as.numeric(as.factor(df$Species))-1

# initialise model
xgb &lt;- XGBTrainer$new(objective = 'multi:softmax',
                      maximize = FALSE,
                      eval_metric = 'merror',
                      num_class=3,
                      n_estimators = 2)
xgb$fit(df, 'Species')
</pre>
</div>


<hr>
<a id="method-XGBTrainer-predict"></a>



<h4>Method <code>predict()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>XGBTrainer$predict(df)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>df</code></dt><dd><p>data.frame, test data set</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Returns predicted values for a given test data
</p>



<h5>Returns</h5>

<p>xgboost predictions
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>#' library(data.table)
df &lt;- copy(iris)

# convert characters/factors to numeric
df$Species &lt;- as.numeric(as.factor(df$Species))-1

# initialise model
xgb &lt;- XGBTrainer$new(objective = 'multi:softmax',
                      maximize = FALSE,
                      eval_metric = 'merror',
                      num_class=3,
                      n_estimators = 2)
xgb$fit(df, 'Species')

# make predictions
preds &lt;- xgb$predict(as.matrix(iris[,1:4]))
</pre>
</div>


<hr>
<a id="method-XGBTrainer-show_importance"></a>



<h4>Method <code>show_importance()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>XGBTrainer$show_importance(type = "plot", topn = 10)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>type</code></dt><dd><p>character, could be 'plot' or 'table'</p>
</dd>
<dt><code>topn</code></dt><dd><p>integer, top n features to display</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Shows feature importance plot
</p>



<h5>Returns</h5>

<p>a table or a plot of feature importance
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
library(data.table)
df &lt;- copy(iris)

# convert characters/factors to numeric
df$Species &lt;- as.numeric(as.factor(df$Species))-1

# initialise model
xgb &lt;- XGBTrainer$new(objective = 'multi:softmax',
                      maximize = FALSE,
                      eval_metric = 'merror',
                      num_class=3,
                      n_estimators = 2)
xgb$fit(df, 'Species')
xgb$show_importance()
}
</pre>
</div>


<hr>
<a id="method-XGBTrainer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>XGBTrainer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `XGBTrainer$new`
## ------------------------------------------------

library(data.table)
df &lt;- copy(iris)

# convert characters/factors to numeric
df$Species &lt;- as.numeric(as.factor(df$Species))-1

# initialise model
xgb &lt;- XGBTrainer$new(objective = 'multi:softmax',
                      maximize = FALSE,
                      eval_metric = 'merror',
                      num_class=3,
                      n_estimators = 2)

## ------------------------------------------------
## Method `XGBTrainer$cross_val`
## ------------------------------------------------

## Not run: 
library(data.table)
df &lt;- copy(iris)

# convert characters/factors to numeric
df$Species &lt;- as.numeric(as.factor(df$Species))-1

# initialise model
xgb &lt;- XGBTrainer$new(objective = 'multi:softmax',
                      maximize = FALSE,
                      eval_metric = 'merror',
                      num_class=3,
                      n_estimators = 2)

# do cross validation to find optimal value for n_estimators
xgb$cross_val(X = df, y = 'Species',nfolds = 3, stratified = TRUE)

## End(Not run)

## ------------------------------------------------
## Method `XGBTrainer$fit`
## ------------------------------------------------

library(data.table)
df &lt;- copy(iris)

# convert characters/factors to numeric
df$Species &lt;- as.numeric(as.factor(df$Species))-1

# initialise model
xgb &lt;- XGBTrainer$new(objective = 'multi:softmax',
                      maximize = FALSE,
                      eval_metric = 'merror',
                      num_class=3,
                      n_estimators = 2)
xgb$fit(df, 'Species')

## ------------------------------------------------
## Method `XGBTrainer$predict`
## ------------------------------------------------

#' library(data.table)
df &lt;- copy(iris)

# convert characters/factors to numeric
df$Species &lt;- as.numeric(as.factor(df$Species))-1

# initialise model
xgb &lt;- XGBTrainer$new(objective = 'multi:softmax',
                      maximize = FALSE,
                      eval_metric = 'merror',
                      num_class=3,
                      n_estimators = 2)
xgb$fit(df, 'Species')

# make predictions
preds &lt;- xgb$predict(as.matrix(iris[,1:4]))

## ------------------------------------------------
## Method `XGBTrainer$show_importance`
## ------------------------------------------------

## Not run: 
library(data.table)
df &lt;- copy(iris)

# convert characters/factors to numeric
df$Species &lt;- as.numeric(as.factor(df$Species))-1

# initialise model
xgb &lt;- XGBTrainer$new(objective = 'multi:softmax',
                      maximize = FALSE,
                      eval_metric = 'merror',
                      num_class=3,
                      n_estimators = 2)
xgb$fit(df, 'Species')
xgb$show_importance()

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
