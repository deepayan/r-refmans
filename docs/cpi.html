<!DOCTYPE html><html><head><title>Help for package cpi</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cpi}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cpi'><p>Conditional Predictive Impact (CPI).</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Conditional Predictive Impact</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-03-02</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marvin N. Wright &lt;cran@wrig.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A general test for conditional independence in supervised learning 
  algorithms as proposed by Watson &amp; Wright (2021) &lt;<a href="https://doi.org/10.1007%2Fs10994-021-06030-6">doi:10.1007/s10994-021-06030-6</a>&gt;. 
  Implements a conditional variable importance measure which can be applied to any supervised 
  learning algorithm and loss function. Provides statistical inference procedures without 
  parametric assumptions and applies equally well to continuous and categorical predictors 
  and outcomes.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bips-hb/cpi">https://github.com/bips-hb/cpi</a>, <a href="https://bips-hb.github.io/cpi/">https://bips-hb.github.io/cpi/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bips-hb/cpi/issues">https://github.com/bips-hb/cpi/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>foreach, mlr3, lgr, knockoff</td>
</tr>
<tr>
<td>Suggests:</td>
<td>BEST, mlr3learners, ranger, glmnet, testthat (&ge; 3.0.0),
knitr, rmarkdown, doParallel</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-03-02 07:54:32 UTC; wright</td>
</tr>
<tr>
<td>Author:</td>
<td>Marvin N. Wright <a href="https://orcid.org/0000-0002-8542-6291"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  David S. Watson [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-03-03 09:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cpi'>Conditional Predictive Impact (CPI).</h2><span id='topic+cpi'></span>

<h3>Description</h3>

<p>A general test for conditional 
independence in supervised learning algorithms. Implements a conditional 
variable importance measure which can be applied to any supervised learning 
algorithm and loss function. Provides statistical inference procedures 
without parametric assumptions and applies equally well to continuous and 
categorical predictors and outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpi(
  task,
  learner,
  resampling = NULL,
  test_data = NULL,
  measure = NULL,
  test = "t",
  log = FALSE,
  B = 1999,
  alpha = 0.05,
  x_tilde = NULL,
  knockoff_fun = function(x) knockoff::create.second_order(as.matrix(x)),
  groups = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpi_+3A_task">task</code></td>
<td>
<p>The prediction <code>mlr3</code> task, see examples.</p>
</td></tr>
<tr><td><code id="cpi_+3A_learner">learner</code></td>
<td>
<p>The <code>mlr3</code> learner used in CPI. If you pass a string, the 
learner will be created via <code>mlr3::lrn</code>.</p>
</td></tr>
<tr><td><code id="cpi_+3A_resampling">resampling</code></td>
<td>
<p>Resampling strategy, <code>mlr3</code> resampling object 
(e.g. <code>rsmp("holdout")</code>), &quot;oob&quot; (out-of-bag) or &quot;none&quot; 
(in-sample loss).</p>
</td></tr>
<tr><td><code id="cpi_+3A_test_data">test_data</code></td>
<td>
<p>External validation data, use instead of resampling.</p>
</td></tr>
<tr><td><code id="cpi_+3A_measure">measure</code></td>
<td>
<p>Performance measure (loss). Per default, use MSE 
(<code>"regr.mse"</code>) for regression and logloss (<code>"classif.logloss"</code>) 
for classification.</p>
</td></tr>
<tr><td><code id="cpi_+3A_test">test</code></td>
<td>
<p>Statistical test to perform, one of <code>"t"</code> (t-test, default), 
<code>"wilcox"</code> (Wilcoxon signed-rank test), <code>"binom"</code> (binomial 
test), <code>"fisher"</code> (Fisher permutation test) or &quot;bayes&quot; 
(Bayesian testing, computationally intensive!). See Details.</p>
</td></tr>
<tr><td><code id="cpi_+3A_log">log</code></td>
<td>
<p>Set to <code>TRUE</code> for multiplicative CPI (<code class="reqn">\lambda</code>), to 
<code>FALSE</code> (default) for additive CPI (<code class="reqn">\Delta</code>).</p>
</td></tr>
<tr><td><code id="cpi_+3A_b">B</code></td>
<td>
<p>Number of permutations for Fisher permutation test.</p>
</td></tr>
<tr><td><code id="cpi_+3A_alpha">alpha</code></td>
<td>
<p>Significance level for confidence intervals.</p>
</td></tr>
<tr><td><code id="cpi_+3A_x_tilde">x_tilde</code></td>
<td>
<p>Knockoff matrix or data.frame. If not given (the default), it will be 
created with the function given in <code>knockoff_fun</code>.</p>
</td></tr>
<tr><td><code id="cpi_+3A_knockoff_fun">knockoff_fun</code></td>
<td>
<p>Function to generate knockoffs. Default: 
<code>knockoff::create.second_order</code> with matrix argument.</p>
</td></tr>
<tr><td><code id="cpi_+3A_groups">groups</code></td>
<td>
<p>(Named) list with groups. Set to <code>NULL</code> (default) for no
groups, i.e. compute CPI for each feature. See examples.</p>
</td></tr>
<tr><td><code id="cpi_+3A_verbose">verbose</code></td>
<td>
<p>Verbose output of resampling procedure.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the conditional predictive impact (CPI) of one or
several features on a given supervised learning task. This represents the 
mean error inflation when replacing a true variable with its knockoff. Large
CPI values are evidence that the feature(s) in question have high 
<em>conditional variable importance</em> &ndash; i.e., the fitted model relies on 
the feature(s) to predict the outcome, even after accounting for the signal
from all remaining covariates. 
</p>
<p>We build on the <code>mlr3</code> framework, which provides a unified interface for 
training models, specifying loss functions, and estimating generalization 
error. See the package documentation for more info.
</p>
<p>Methods are implemented for frequentist and Bayesian inference. The default
is <code>test = "t"</code>, which is fast and powerful for most sample sizes. The
Wilcoxon signed-rank test (<code>test = "wilcox"</code>) may be more appropriate if 
the CPI distribution is skewed, while the binomial test (<code>test = "binom"</code>) 
requires basically no assumptions but may have less power. For small sample 
sizes, we recommend permutation tests (<code>test = "fisher"</code>) or Bayesian 
methods (<code>test = "bayes"</code>). In the latter case, default priors are 
assumed. See the <code>BEST</code> package for more info.
</p>
<p>For parallel execution, register a backend, e.g. with
<code>doParallel::registerDoParallel()</code>.
</p>


<h3>Value</h3>

<p>For <code>test = "bayes"</code> a list of <code>BEST</code> objects. In any other 
case, a <code>data.frame</code> with a row for each feature and columns:
</p>
<table>
<tr><td><code>Variable/Group</code></td>
<td>
<p>Variable/group name</p>
</td></tr>
<tr><td><code>CPI</code></td>
<td>
<p>CPI value</p>
</td></tr>
<tr><td><code>SE</code></td>
<td>
<p>Standard error</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>Testing method</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>Test statistic (only for t-test, Wilcoxon and binomial test)</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>Estimated mean (for t-test), median (for Wilcoxon test),
or proportion of <code class="reqn">\Delta</code>-values greater than 0 (for binomial test).</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>p-value</p>
</td></tr>
<tr><td><code>ci.lo</code></td>
<td>
<p>Lower limit of (1 - <code>alpha</code>) * 100% confidence interval</p>
</td></tr>
</table>
<p>Note that NA values are no error but a result of a CPI value of 0, i.e. no 
difference in model performance after replacing a feature with its knockoff.
</p>


<h3>References</h3>

<p>Watson, D. &amp; Wright, M. (2020). Testing conditional independence in 
supervised learning algorithms. <em>Machine Learning</em>, <em>110</em>(8): 
2107-2129. doi: <a href="https://doi.org/10.1007/s10994-021-06030-6">10.1007/s10994-021-06030-6</a>
</p>
<p>Cand√®s, E., Fan, Y., Janson, L, &amp; Lv, J. (2018). Panning for gold: 'model-X'
knockoffs for high dimensional controlled variable selection. <em>J. R. 
Statistc. Soc. B</em>, <em>80</em>(3): 551-577. doi: <a href="https://doi.org/10.1111/rssb.12265">10.1111/rssb.12265</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mlr3)
library(mlr3learners)

# Regression with linear model and holdout validation
cpi(task = tsk("mtcars"), learner = lrn("regr.lm"), 
    resampling = rsmp("holdout"))


# Classification with logistic regression, log-loss and t-test
cpi(task = tsk("wine"), 
    learner = lrn("classif.glmnet", predict_type = "prob", lambda = 0.1), 
    resampling = rsmp("holdout"), 
    measure = "classif.logloss", test = "t")
 
# Use your own data (and out-of-bag loss with random forest)
mytask &lt;- as_task_classif(iris, target = "Species")
mylearner &lt;- lrn("classif.ranger", predict_type = "prob", keep.inbag = TRUE)
cpi(task = mytask, learner = mylearner, 
    resampling = "oob", measure = "classif.logloss")
    
# Group CPI
cpi(task = tsk("iris"), 
    learner = lrn("classif.ranger", predict_type = "prob", num.trees = 10), 
    resampling = rsmp("cv", folds = 3), 
    groups = list(Sepal = 1:2, Petal = 3:4))
     
## Not run:       
# Bayesian testing
res &lt;- cpi(task = tsk("iris"), 
           learner = lrn("classif.glmnet", predict_type = "prob", lambda = 0.1), 
           resampling = rsmp("holdout"), 
           measure = "classif.logloss", test = "bayes")
plot(res$Petal.Length)

# Parallel execution
doParallel::registerDoParallel()
cpi(task = tsk("wine"), 
    learner = lrn("classif.glmnet", predict_type = "prob", lambda = 0.1), 
    resampling = rsmp("cv", folds = 5))
    
# Use sequential knockoffs for categorical features
# package available here: https://github.com/kormama1/seqknockoff
mytask &lt;- as_task_regr(iris, target = "Petal.Length")
cpi(task = mytask, learner = lrn("regr.ranger"), 
    resampling = rsmp("holdout"), 
    knockoff_fun = seqknockoff::knockoffs_seq)

## End(Not run)   

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
