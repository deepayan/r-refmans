<!DOCTYPE html><html><head><title>Help for package forestRK</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {forestRK}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bstrap'>
<p>Performs bootstrap sampling of the (training) dataset</p></a></li>
<li><a href='#construct.treeRK'>
<p>Constructs a classification tree on the (training) dataset, by implementing</p>
the RK (Random 'K') algorithm</a></li>
<li><a href='#criteria.after.split.calculator'>
<p>Calculates Entropy or Gini Index of a node after a given split</p></a></li>
<li><a href='#criteria.calculator'>
<p>Calculates Entropy or Gini Index of a particular node before (or without) a</p>
split</a></li>
<li><a href='#cutoff.node.and.covariate.index.finder'>
<p>Identifies optimal cutoff point of an impure node for splitting after applying</p>
the <code>rk</code> (Random K) algorithm.</a></li>
<li><a href='#draw.treeRK'>
<p>Creates a <code>igraph</code> plot of a <code>rktree</code></p></a></li>
<li><a href='#ends.index.finder'>
<p>Identifies numerical indices of the end nodes of a <code>rktree</code> from the</p>
matrix of hierarchical flags.</a></li>
<li><a href='#forestRK'>
<p>Builds up a random forest RK model based on the given (training) dataset</p></a></li>
<li><a href='#get.tree.forestRK'>
<p>Extracts the structure of one or more trees in a forestRK object</p></a></li>
<li><a href='#importance.forestRK'>
<p>Calculates Gini Importance or Mean Decrease Impurity (same algorithm is used in</p>
'scikit-learn') of each covariate that we consider in the <code>forestRK</code> model</a></li>
<li><a href='#importance.plot.forestRK'>
<p>Generates importance <code>ggplot</code> of the covariates considered in the</p>
<code>forestRK</code> model</a></li>
<li><a href='#mds.plot.forestRK'>
<p>Makes 2D MDS (multidimensional scaling) <code>ggplot</code> of the test observations</p>
based on the predictions from a <code>forestRK</code> model.</a></li>
<li><a href='#pred.forestRK'>
<p>Make predictions on the test data based on the forestRK model</p>
constructed from the training data</a></li>
<li><a href='#pred.treeRK'>
<p>Make predictions on the test observations based on a rktree model</p></a></li>
<li><a href='#var.used.forestRK'>
<p>Extract the list of covariates used to perform the splits to</p>
generate a particular tree(s) in a <code>forestRK</code> object</a></li>
<li><a href='#x.organizer'>
<p>Numericizing a data frame of covariates from the original dataset</p>
via Binary or Numeric Encoding</a></li>
<li><a href='#y.organizer'>
<p>Numericize the vector containing categorical class type(<code>y</code>)</p>
of the original data</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.0-5</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Title:</td>
<td>Implements the Forest-R.K. Algorithm for Classification Problems</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions that calculates common types of splitting
    criteria used in random forests for classification problems, as well as 
    functions that make predictions based on a single tree or a Forest-R.K. model; 
    the package also provides functions to generate importance plot for a 
    Forest-R.K. model, as well as the 2D multidimensional-scaling plot of 
    data points that are colour coded by their predicted class types by the 
    Forest-R.K. model. This package is based on: 
    Bernard, S., Heutte, L., Adam, S., (2008, ISBN:978-3-540-85983-3) 
    "Forest-R.K.: A New Random Forest Induction Method", 
    Fourth International Conference on Intelligent Computing,
    September 2008, Shanghai, China, pp.430-437.</td>
</tr>
<tr>
<td>Author:</td>
<td>Hyunjin Cho [aut, cre],
  Rebecca Su [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hyunjin Cho &lt;h56cho@uwaterloo.ca&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>igraph, ggplot2, rapportools, partykit, stats, graphics,
pkgKitten, knitr, mlbench</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a> | file LICENSE</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Note:</td>
<td>The package is also based on the discussion
https://stats.stackexchange.com/questions/168964/building-a-regression-tree-with-r-from-scratch/168967#168967</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-07-18 17:33:49 UTC; jin-dominique</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-07-19 10:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bstrap'>
Performs bootstrap sampling of the (training) dataset
</h2><span id='topic+bstrap'></span>

<h3>Description</h3>

<p>Performs bootstrap sampling of our (training) dataset; this function is used
inside of the <code>forestRK</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> bstrap(dat = data.frame(), nbags, samp.size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bstrap_+3A_dat">dat</code></td>
<td>

<p>a numericized data frame that stores both the covariates of the observations
and their numericized class types <code>y</code>;
<code>dat</code> should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="bstrap_+3A_nbags">nbags</code></td>
<td>

<p>the number of bags or the number of bootstrap samples that we want to
generate.
</p>
</td></tr>
<tr><td><code id="bstrap_+3A_samp.size">samp.size</code></td>
<td>

<p>the number of samples that each bag (individual bootstrap sample) should
contain.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing a data frames of bootstrap samples generated from <code>dat</code>.
</p>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forestRK">forestRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  # covariates of training data set
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new
  # combine the covariates x with class types y
  b &lt;- data.frame(cbind(x.train, y.train))

  ## bstrp function example
  bootstrap.sample &lt;- bstrap(dat = b, nbags = 20, samp.size = 30)
</code></pre>

<hr>
<h2 id='construct.treeRK'>
Constructs a classification tree on the (training) dataset, by implementing
the RK (Random 'K') algorithm
</h2><span id='topic+construct.treeRK'></span>

<h3>Description</h3>

<p>Constructs a classification tree based on the dataset of interest by
implementing the RK (Random 'K') algorithm.
</p>
<p>The package <code>rapportools</code> is loaded internally when this function is
called; this is to use the method <code>is.boolean</code> to check one of the
stopping criteria in the beginning of the function. The functions specifically
from the <code>forestRK</code> package that are being used inside
<code>construct.treeRK</code> are <code>criteria.calculator</code> and
<code>cutoff.node.and.covariate.index.finder</code>.
</p>
<p>The <code>construct.treeRK</code> output is one of the arguments that is used to call
the <code>pred.treeRK</code> function.
</p>
<p>DESCRIPTIONS OF THE RETURNED VALUES:
</p>
<p>The hirarchical flag of a rktree (<code>construct.treeRK()$flag</code>) is
constructed in the following way:
</p>
<p>(1) the first entry of the flag, &quot;r&quot; denotes for &quot;root&quot;;
(2) the subsequent strings of the flag is constructed in the way that last &quot;x&quot;
denotes for the left child node of the node represented by the series of
characters that are before the last &quot;x&quot;, and the last &quot;y&quot; denotes for the
right child node of the node represented by the series of characters that are
before the last &quot;y&quot;.
</p>
<p>For example, the flag &quot;rxyx&quot; is the left child node of the node represented
by &quot;rxy&quot;.
</p>
<p><code>x.node.list</code> and <code>y.node.list</code> are the lists of children nodes
(for <code>x</code> and <code>y</code>, respectively) of the <code>rktree</code>,
listed in the order consistent to the order of the nodes represented in the
<code>rktree</code>'s hirarchical flag.
</p>
<p><code>covariate.split</code> is a matrix that lists the numericized covariate names
that were used for the splits to construct the rktree. The first entry of
<code>covariate.split</code> is <code>NA</code>, which stands for the condition at the
root. The number immediately underneath <code>NA</code> is the numericized covariate
name that was used for the first split in the <code>rktree</code>, and the number
below that is the numericized covariate name that was used for the
second split, etc. If the numericized covariate name listed under
<code>covariate.split</code> is the number &quot;n&quot;, this corresponds to the &quot;n&quot;-th
covariate or the name of the &quot;n&quot;-th column of the data frame <code>x.train</code>.
</p>
<p><code>value.at.split</code> is a vector that lists the actual values of the
covariates at which the split had occured while constructing the rktree.
The first entry of <code>value.at.split</code> is <code>NA</code>, which denotes for the
root prior to any splits. To give an example of how to interpret the
<code>value.at.split</code>, if the second entry appear in the <code>covariate.split</code>
is 4, and the second entry appear under <code>value.at.split</code> is 0.5, this
indicates that the first split of the rktree had occured on the covariate
corresponds to the 4th column of the data frame <code>x.train</code>, and the exact
criteria for that first split was (4th covariate value) &lt;= 0.5 vs.
(4th covariate value) &gt; 0.5.
</p>
<p><code>amount.decrease.criteria</code> is a matrix that lists the amount of decrease
in splitting criteria (Entropy or Gini Index) after each split had occurred.
The first entry of <code>amount.decrease.criteria</code> is <code>NA</code>,
which denotes for the condition at the root (no split). To give an example,
if the second entry appear in the <code>amount.decrease.criteria</code> is 0.91,
and if <code>entropy</code> was set to <code>TRUE</code>, this means that after the first
split, the Entropy of the original node had decreased by 0.91.
</p>
<p><code>num.obs</code> is a matrix that stores the number of observations contained
within a parent node prior to the split; the matrix starts with the entry &quot;NA&quot;,
in order to reflect the condition at &quot;root&quot;. The 2nd entry of <code>num.obs</code>
would inform us on the number of observations contained within the parent node
on which the 1st split had took place while the <code>rktree</code> was built; the
3rd entry of the <code>num.obs</code> would inform us on the number of observations
contained within the parent node on which the 2nd split had took place,
and so on.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> construct.treeRK(x.train = data.frame(), y.new.train = c(),
                  min.num.obs.end.node.tree = 5, entropy = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="construct.treeRK_+3A_x.train">x.train</code></td>
<td>

<p>a numericized data frame of covariates of the data on which we want to
build our rktree models (typically the training data); this data frame can be
obtained by applying the <code>x.organizer</code> function. <code>x.train</code> should
contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="construct.treeRK_+3A_y.new.train">y.new.train</code></td>
<td>

<p>a numericized class types of the observations from the dataset on which we
want to build our rktree models (typically the training data).
<code>y.new.train</code> should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="construct.treeRK_+3A_min.num.obs.end.node.tree">min.num.obs.end.node.tree</code></td>
<td>

<p>the minimum number of observations that we want each end node of our
rktree to contain. Default is set to '5'.
</p>
</td></tr>
<tr><td><code id="construct.treeRK_+3A_entropy">entropy</code></td>
<td>

<p><code>TRUE</code> if Entropy is used as the splitting criteria;
<code>FALSE</code> if Gini Index is used as the splitting criteria.
Default is set to <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following items:
</p>
<table>
<tr><td><code>covariate.names</code></td>
<td>

<p>a vector of the names of all covariates that we consider in our model.
</p>
</td></tr>
<tr><td><code>l</code></td>
<td>

<p>length of the hierarchical flag.
</p>
</td></tr>
<tr><td><code>x.node.list</code></td>
<td>

<p>a list containing a series of children nodes produced from the numericized
data frame <code>x.train</code> as the <code>rktree</code> model was building up.
</p>
</td></tr>
<tr><td><code>y.new.node.list</code></td>
<td>

<p>a list containing a series of children nodes produced from the numericized
vector of class type <code>y.new.train</code> as the <code>rktree</code> model was
building up.
</p>
</td></tr>
<tr><td><code>flag</code></td>
<td>

<p>hierchical flag that characterizes each split in the <code>rktree</code>.
</p>
</td></tr>
<tr><td><code>covariate.split</code></td>
<td>

<p>a matrix that lists numericized covariates used for each split as the
<code>rktree</code> was built.
</p>
</td></tr>
<tr><td><code>value.at.split</code></td>
<td>

<p>a vector that lists the values at which each node of the <code>rktree</code>
was split.
</p>
</td></tr>
<tr><td><code>amt.decrease.criteria</code></td>
<td>

<p>a matrix that lists the amount of decrease in splitting criteria after
each split as the <code>rktree</code> was built.
</p>
</td></tr>
<tr><td><code>num.obs</code></td>
<td>

<p>a matrix that stores the number of observations contained in each parent node
right before each split.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pred.treeRK">pred.treeRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  ## numericize the data
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

  # Construct a tree
  # min.num.obs.end.node.tree is set to 5 by default;
  # entropy is set to TRUE by default
  tree.entropy &lt;- construct.treeRK(x.train, y.train)
  tree.gini &lt;- construct.treeRK(x.train, y.train,
                                min.num.obs.end.node.tree = 6, entropy = FALSE)
  tree.entropy$covariate.names
  tree.gini$flag # ...etc...
</code></pre>

<hr>
<h2 id='criteria.after.split.calculator'>
Calculates Entropy or Gini Index of a node after a given split
</h2><span id='topic+criteria.after.split.calculator'></span>

<h3>Description</h3>

<p>Calculates Entropy or Gini Index of a particular node after a particular split;
this function is called within <code>construct.treeRK</code> function.
</p>
<p>The argument <code>split.record</code> is a <code>kidids_split</code> object
from the package <code>partykit</code>; the method <code>kidids_split</code> splits the
data according to the criteria specified by an user ahead of time, and returns
a vector storing the index of the split group (group &quot;1&quot; or &quot;2&quot;) that each
observation from the original data in question belongs to after the split has
occurred.
</p>
<p>For more information about the function, please see the <code>partykit</code>
documentation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> criteria.after.split.calculator(x.node = data.frame(), y.new.node = c(),
                                 split.record = kidids_split(),
                                 entropy = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="criteria.after.split.calculator_+3A_x.node">x.node</code></td>
<td>

<p>numericized data frame of covariates (obtained via <code>x.organizer()</code>)
from a particular node that is to be split; <code>x.node</code> should contain no
<code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="criteria.after.split.calculator_+3A_y.new.node">y.new.node</code></td>
<td>

<p>numericized class type of each observation from a particular node that is to
be split; <code>y.new.node</code> should contain no <code>NA</code> or<code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="criteria.after.split.calculator_+3A_split.record">split.record</code></td>
<td>

<p>output of the <code>kidids_split</code> function from the <code>partykit</code> package
that describes a particular split.
</p>
</td></tr>
<tr><td><code id="criteria.after.split.calculator_+3A_entropy">entropy</code></td>
<td>

<p><code>TRUE</code> if Entropy is used as the splitting criteria;
<code>FALSE</code> if Gini Index is used instead.
Default is set to <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value of Entropy or Gini Index of a particular node after a particular
split.
</p>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+criteria.calculator">criteria.calculator</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  library(forestRK) # load the package forestRK
  library(partykit)

  # covariates of training data set
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  # numericized class types of observations of training dataset
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new
  ## criteria.after.split.calculator() example in the implementation
  ## of the forestRK algorithm

  ent.status &lt;- TRUE

  # number.of.columns.of.x.node
  # = total number of covariates that we consider
  number.of.columns.of.x.node &lt;- dim(x.train)[2]
  # m.try = the randomly chosen number of covariates that we consider
  # at the time of split
  m.try &lt;- sample(1:(number.of.columns.of.x.node),1)
  ## sample m.try number of covariates from the list of all covariates
  K &lt;- sample(1:(number.of.columns.of.x.node), m.try)

  # split the data
  # (the choice of the type of split used here is only arbitrary)
  # for more information about kidids_split,
  # please refer to the documentation for the package 'partykit'
  sp &lt;- partysplit(varid=K[1], breaks = x.train[1,K[1]], index = NULL,
                   right = TRUE, prob = NULL, info = NULL)
  split.record &lt;- kidids_split(sp, data=x.train)

  # implement critera.after.split function based on kidids_split object
  criteria.after.split &lt;- criteria.after.split.calculator(x.train,
                                    y.train, split.record, ent.status)
  criteria.after.split
</code></pre>

<hr>
<h2 id='criteria.calculator'>
Calculates Entropy or Gini Index of a particular node before (or without) a
split
</h2><span id='topic+criteria.calculator'></span>

<h3>Description</h3>

<p>Calculates the Entropy or Gini Index of a particular node before (or without)
a split. This function is used inside the
<code>criteria.after.split.calculator</code> method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> criteria.calculator(x.node = data.frame(), y.new.node = c(),
                     entropy = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="criteria.calculator_+3A_x.node">x.node</code></td>
<td>

<p>numericized data frame of covariates of a particular node
(can be obtained by applying <code>x.organizer</code>) before or without a split;
<code>x.node</code> should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="criteria.calculator_+3A_y.new.node">y.new.node</code></td>
<td>

<p>numericized vector of class type (<code>y</code>) of a particular node
(can be obtained by applying <code>y.organizer</code>) before or without split;
<code>y.new.node</code> should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="criteria.calculator_+3A_entropy">entropy</code></td>
<td>

<p><code>TRUE</code> if Entropy is used as the splitting criteria;
<code>FALSE</code> if Gini Index is used as the splitting criteria.
Default is set to <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following items:
</p>
<table>
<tr><td><code>criteria</code></td>
<td>

<p>the value of the Entropy or the Gini Index of a particular node.
</p>
</td></tr>
<tr><td><code>ent.status</code></td>
<td>

<p>logical value (<code>TRUE</code> or <code>FALSE</code>) of the parameter <code>entropy</code>.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+criteria.after.split.calculator">criteria.after.split.calculator</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## example: iris dataset
 library(forestRK) # load the package forestRK

 # covariates of training data set
 x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
 # numericized class types of observations of training dataset
 y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

 ## criteria.calculator() example
 ## calculate the Entropy of the original training dataset
 criteria.calculator(x.node = x.train, y.new.node = y.train)
 ## calculate the Gini Index of the original training dataset
 criteria.calculator(x.node = x.train, y.new.node = y.train, entropy = FALSE)
</code></pre>

<hr>
<h2 id='cutoff.node.and.covariate.index.finder'>
Identifies optimal cutoff point of an impure node for splitting after applying
the <code>rk</code> (Random K) algorithm.
</h2><span id='topic+cutoff.node.and.covariate.index.finder'></span>

<h3>Description</h3>

<p>Identifies optimal cutoff point of an impure dataset for splitting after
applying the <code>rk</code> (Random K) algoritm, in terms of Entropy or Gini Index.
</p>
<p>To give an example, if the function gives <code>cutoff.value</code> of 2.5,
<code>covariate.ind</code> of 4, and <code>cutoff.node</code> of 23, this would inform the
user that if a split is to be performed on the particular node that the user is
considering, the split should occur on the 4th covariate (the actual name of
this covariate would be the name of the 4th column from the original dataset),
at the value of 2.5 (left child node in this case would be the group of
observations that have their 4th covariate value less than or equal to 2.5, and
for the right child node would be the group of observations that have their 4th
covariate value greater than 2.5), and that this splitting point corresponds to
the 23rd observation point of the node.
</p>
<p>This function internally loads the packages <code>partykit</code> and
<code>rapportools</code>; the package <code>partykit</code> is internally loaded to
generate the object <code>split.record.optimal</code>, and the package
<code>rapportools</code> is loaded to allow the validation of one of the
stopping criteria that uses <code>is.boolean</code> method.
</p>
<p>This function is ran internally in the <code>construct.treeRK</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> cutoff.node.and.covariate.index.finder(x.node = data.frame(),
                                        y.new.node = c(), entropy = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cutoff.node.and.covariate.index.finder_+3A_x.node">x.node</code></td>
<td>

<p>a numericized data frame of covariates of the observations from a particular
node prior to the split (can be obtained after applying <code>x.organizer()</code>);
<code>x.node</code> should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="cutoff.node.and.covariate.index.finder_+3A_y.new.node">y.new.node</code></td>
<td>

<p>a vector storing numericized class type of the observations from a particular
node before the split (can be obtained after applying <code>y.organizer()</code>);
<code>y.new.node</code> should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="cutoff.node.and.covariate.index.finder_+3A_entropy">entropy</code></td>
<td>

<p><code>TRUE</code> if Entropy is used as the splitting criteria;
<code>FALSE</code> if Gini Index is used as the splitting criteria.
Default is set to <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following items:
</p>
<table>
<tr><td><code>cutoff.value</code></td>
<td>

<p>the value at which the optimal split should take place.
</p>
</td></tr>
<tr><td><code>cutoff.node</code></td>
<td>

<p>the index of the observation (observation number) at which optimal split
should occur.
</p>
</td></tr>
<tr><td><code>covariate.ind</code></td>
<td>

<p>numeric index of the covariate at which the optimal split should occur.
</p>
</td></tr>
<tr><td><code>split.record.optimal</code></td>
<td>

<p>the <code>kidid_split</code> output of the optimal split.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+construct.treeRK">construct.treeRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  ## numericize the data
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

  # implementation of cutoff.node.and.covariate.index.finder()
  res &lt;- cutoff.node.and.covariate.index.finder(x.train, y.train,
                                               entropy=FALSE)
  res$cutoff.value
  res$cutoff.node
  res$covariate.ind
  res$split.record.optimal
</code></pre>

<hr>
<h2 id='draw.treeRK'>
Creates a <code>igraph</code> plot of a <code>rktree</code>
</h2><span id='topic+draw.treeRK'></span>

<h3>Description</h3>

<p>Creates a plot of a <code>rktree</code> that was built from the (training) dataset.
</p>
<p>The package <code>igraph</code> is loaded internally when this function is called,
to aid in generating the plot of a <code>rktree</code>.
</p>
<p>DESCRIPTIONS OF THE <code>rktree</code> PLOT:
</p>
<p>The resulting plot is a classical decision tree.
</p>
<p>The rectangular nodes (or vertices) that contain &quot;=&lt;&quot; symbol are used to
describe the splitting criteria applied to that very node while constructing
the <code>rktree</code>; for example, in the <code>rktree</code> plot generated by the code
shown in the &quot;examples&quot; section below, the node with the label
&quot;Petal.Width =&lt; 1.6&quot; indicates that this node was split into a chunk that
contains observations with Petal.Width &lt;= 1.6 and a chunk that contains
observations with Petal.Width greater than 1.6, in order to construct the
<code>rktree</code>.
</p>
<p>Any other rectangular nodes (or vertices) that do not contain the &quot;=&lt;&quot; symbol
indicate that we have reached an end node, and the text displayed in such node
is the actual name of the class type that the rktree model assigns to the
observations belonging to that node; for example, in the <code>rktree</code> plot
generated by the code shown in the &quot;examples&quot; section below, the vertex with
the label &quot;setosa&quot; indicates that the <code>rktree</code> assigns the class type
&quot;setosa&quot; to all observations that belong to that particular node.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> draw.treeRK(tr = construct.treeRK(), y.factor.levels,
 font = "Times", node.colour = "white", text.colour = "dark blue",
 text.size = 0.67, tree.vertex.size = 75, tree.title = "Diagram of a Tree",
 title.colour = "dark blue")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="draw.treeRK_+3A_tr">tr</code></td>
<td>

<p>a <code>construct.treeRK()</code> object (a tree).
</p>
</td></tr>
<tr><td><code id="draw.treeRK_+3A_y.factor.levels">y.factor.levels</code></td>
<td>

<p>a <code>y.organizer()$y.factor.levels</code> output.
</p>
</td></tr>
<tr><td><code id="draw.treeRK_+3A_font">font</code></td>
<td>

<p>font type used in the <code>rktree</code> plot; default is &quot;Times&quot;.
</p>
</td></tr>
<tr><td><code id="draw.treeRK_+3A_node.colour">node.colour</code></td>
<td>

<p>colour of the node used in the <code>rktree</code> plot; default is &quot;White&quot;.
</p>
</td></tr>
<tr><td><code id="draw.treeRK_+3A_text.colour">text.colour</code></td>
<td>

<p>colour of the text used in the <code>rktree</code> plot; default is &quot;Dark Blue&quot;.
</p>
</td></tr>
<tr><td><code id="draw.treeRK_+3A_text.size">text.size</code></td>
<td>

<p>size of the text in the <code>rktree</code> plot; default is 0.67.
</p>
</td></tr>
<tr><td><code id="draw.treeRK_+3A_tree.vertex.size">tree.vertex.size</code></td>
<td>

<p>size of the <code>rktree</code> plot vertices; default is 75.
</p>
</td></tr>
<tr><td><code id="draw.treeRK_+3A_tree.title">tree.title</code></td>
<td>

<p>title of the <code>rktree</code> plot; default title is &quot;Diagram of a Tree&quot;.
</p>
</td></tr>
<tr><td><code id="draw.treeRK_+3A_title.colour">title.colour</code></td>
<td>

<p>colour for the title of the <code>rktree</code> plot;
default title colour is &quot;Dark Blue&quot;.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>igraph</code> plot of a <code>rktree</code>.
</p>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mds.plot.forestRK">mds.plot.forestRK</a></code>
<code><a href="#topic+importance.plot.forestRK">importance.plot.forestRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  ## Numericize the data
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new
  y.factor.levels &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.factor.levels

  ## Construct a tree
  # min.num.obs.end.node.tree is set to 5 by default;
  # entropy is set to TRUE by default
  tree.entropy &lt;- construct.treeRK(x.train, y.train)

  # Plot the tree
  draw.treeRK(tree.entropy, y.factor.levels, font="Times",
              node.colour = "black", text.colour = "white", text.size = 0.7,
              tree.vertex.size = 100, tree.title = "Decision Tree",
              title.colour = "dark green")
</code></pre>

<hr>
<h2 id='ends.index.finder'>
Identifies numerical indices of the end nodes of a <code>rktree</code> from the
matrix of hierarchical flags.
</h2><span id='topic+ends.index.finder'></span>

<h3>Description</h3>

<p>Identifies numerical indices of the end nodes of a <code>rktree</code> by
closely examining the structure of the <code>rktree</code> flag (obtained via
<code>construct.treeRK()$flag</code>); the precise algorithm used is the following:
</p>
<p>if m-th string in the list of <code>rktree</code> flag is a substring of one or
more of (m + 1),...,n-th strings in the list of flag, then the node
represented by the m-th string of the flag is not an end node; otherwise,
the node represented by the m-th string of the flag is the end node.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> ends.index.finder(tr.flag = matrix())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ends.index.finder_+3A_tr.flag">tr.flag</code></td>
<td>

<p>a <code>construct.treeRK()$flag</code> object or a similar flag matrix.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector that lists the indices of the end nodes of a given <code>rktree</code>
(indices that are consistent to the indices in <code>x.node.list</code>,
<code>y.new.node.list</code>, and <code>flag</code> that are returned by the
<code>construct.treeRK</code> function).
</p>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+construct.treeRK">construct.treeRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  # covariates of training data set
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

  # Construct a tree
  # min.num.obs.end.node.tree is set to 5 by default;
  # entropy is set to TRUE by default
  tree.entropy &lt;- construct.treeRK(x.train, y.train)

  # Find indices of end nodes of tree.entropy
  end.node.index &lt;- ends.index.finder(tree.entropy$flag)
</code></pre>

<hr>
<h2 id='forestRK'>
Builds up a random forest RK model based on the given (training) dataset
</h2><span id='topic+forestRK'></span>

<h3>Description</h3>

<p>Builds up a random forest RK model onto the given (training) dataset.
</p>
<p>The functions <code>bstrap</code> and <code>construct.treeRK</code> are used inside
this function. Once the call for <code>bstrap</code> generates bootstrap samples of
the training dataset, then the function <code>construct.treeRK</code> is called
in order to build a tree on each of those bootstrap dataset, to form a bigger
forest.
</p>
<p>Calling of this function internally loads the package <code>rapportools</code>; this
is to allow the use of <code>is.boolean</code> method to check one of the stopping
criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> forestRK(X = data.frame(), Y.new = c(),
          min.num.obs.end.node.tree = 5, nbags, samp.size, entropy = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forestRK_+3A_x">X</code></td>
<td>

<p>a numericized data frame storing covariates of each observation
contained in the given (training) dataset (obtained via <code>x.organizer()</code>);
<code>X</code> should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="forestRK_+3A_y.new">Y.new</code></td>
<td>

<p>a vector storing the numericized class types of each observation contained in
the given (training) dataset <code>X</code>; <code>Y.new</code> should contain no
<code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="forestRK_+3A_min.num.obs.end.node.tree">min.num.obs.end.node.tree</code></td>
<td>

<p>the minimum number of observations that we want each end node of our
<code>rktree</code> to contain. Default is set to 5.
</p>
</td></tr>
<tr><td><code id="forestRK_+3A_nbags">nbags</code></td>
<td>

<p>number of bootstrap samples that we want to generate to generate a forest.
</p>
</td></tr>
<tr><td><code id="forestRK_+3A_samp.size">samp.size</code></td>
<td>

<p>number of observations that we want each of our bootstrap samples to contain.
</p>
</td></tr>
<tr><td><code id="forestRK_+3A_entropy">entropy</code></td>
<td>

<p><code>TRUE</code> if we use Entropy as the splitting criteria;
<code>FALSE</code> if we use the Gini Index for the splitting criteria.
Default is set to <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following items:
</p>
<table>
<tr><td><code>X</code></td>
<td>

<p>The original (training) dataset that was used to construct the
random forest RK model.
</p>
</td></tr>
<tr><td><code>forest.rk.tree.list</code></td>
<td>

<p>A list of trees (<code>construct.treeRK</code> objects) contained in the
<code>forestRK</code> model.
</p>
</td></tr>
<tr><td><code>bootsamp.list</code></td>
<td>

<p>A list containing data frames of bootstrap samples that were generated from
the given (training) dataset <code>X</code>.
</p>
</td></tr>
<tr><td><code>ent.status</code></td>
<td>

<p>The value of the parameter <code>entropy</code>.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bstrap">bstrap</a></code>
<code><a href="#topic+construct.treeRK">construct.treeRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  # covariates of training data set
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

  # Implement forestRK function
  # min.num.obs.end.node.tree is set to 5 by default;
  # entropy is set to TRUE by default
  # normally nbags and samp.size has to be much larger than 30 and 50
  forestRK.1 &lt;- forestRK(x.train, y.train, nbags = 30, samp.size = 50)

  # extract the first tree in the forestRK.1 model
  forestRK.1$forest.rk.tree.list[[1]]
</code></pre>

<hr>
<h2 id='get.tree.forestRK'>
Extracts the structure of one or more trees in a forestRK object
</h2><span id='topic+get.tree.forestRK'></span>

<h3>Description</h3>

<p>Extracts structure of one or more trees from a <code>forestRK</code> object.
</p>
<p>Each tree in the list are named by the exact indices of the tree;
for example, if the code
<code>obj &lt;- get.tree.forestRK(forestRK.1, tree.index=c(4,5,6))</code> was used to
extract the structure of the 4th, 5th, and 6th trees in the forest, the user
can retrieve the information pertains explicitly to the 4th tree in the forest
by doing <code>obj["4"]]</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> get.tree.forestRK(forestRK.object = forestRK(), tree.index=c())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get.tree.forestRK_+3A_forestrk.object">forestRK.object</code></td>
<td>

<p>a <code>forestRK</code> object.
</p>
</td></tr>
<tr><td><code id="get.tree.forestRK_+3A_tree.index">tree.index</code></td>
<td>

<p>a vector of indices of the trees that we want to extract from
the <code>forestRK</code> object.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing <code>forestRK</code> trees that have their indices
specified in the function argument <code>tree.index</code>.
</p>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forestRK">forestRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

  # random forest
  # min.num.obs.end.node.tree is set to 5 by default;
  # entropy is set to TRUE by default
  # normally nbags and samp.size have to be much larger than 30 and 50
  forestRK.1 &lt;- forestRK(x.train, y.train, nbags = 30, samp.size = 50)

  # get tree
  tree.index.ex &lt;- c(1,3,8)
  get.tree &lt;- get.tree.forestRK(forestRK.1, tree.index = tree.index.ex)
  get.tree[["8"]] # display the 8th tree of the random forest
</code></pre>

<hr>
<h2 id='importance.forestRK'>
Calculates Gini Importance or Mean Decrease Impurity (same algorithm is used in
'scikit-learn') of each covariate that we consider in the <code>forestRK</code> model
</h2><span id='topic+importance.forestRK'></span>

<h3>Description</h3>

<p>Calculates Gini Importance of each covariate considered in the forestRK model,
and list them in the order of most important to the least important.
</p>
<p>The Gini Importance or Mean Decrease in Impurity algorithm is also used in
'scikit-learn'. Gini Importance is defined as the total decrease in node
impurity averaged over all trees of the ensemble, where the decrease in node
impurity is obtained after weighting by the probability for an observation to
reach that node (which is approximated by the proportion of samples reaching
that node).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> importance.forestRK(forestRK.object = forestRK())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="importance.forestRK_+3A_forestrk.object">forestRK.object</code></td>
<td>
<p>a <code>forestRK</code> object.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following items:
</p>
<table>
<tr><td><code>importance.covariate.names</code></td>
<td>

<p>a vector of names of the covariates of our dataset ordered from the most
important to the least important.
</p>
</td></tr>
<tr><td><code>average.decrease.in.criteria.vec</code></td>
<td>

<p>a vector storing the average decrease in the weighted splitting criteria
by each covariate that was calculated across all trees in the forestRK
object; the numbers are ordered from the highest average decrease in
criteria (importance) to the lowest, so the i-th importance number from
this vector pertains to the i-th covariate listed in the vector output
<code>importance.covariate.names</code>.
</p>
</td></tr>
<tr><td><code>ent.status</code></td>
<td>

<p>status of the parameter <code>entropy</code>; <code>TRUE</code> if Entropy is used for
splitting critera, <code>FALSE</code> if Gini Index is used instead.
</p>
</td></tr>
<tr><td><code>x.original</code></td>
<td>

<p>a numericized data frame storing covariates of each observation from the given
(training) dataset that was used to construct the <code>forestRK</code> object in
the beginning of the <code>forestRK</code> function call.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forestRK">forestRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  ## numericize the data
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

  # random forest
  # min.num.obs.end.node.tree is set to 5 by default;
  # entropy is set to TRUE by default
  # typically the nbags and samp.size has to be much larger than 30 and 50
  forestRK.1 &lt;- forestRK(x.train, y.train, nbags=30, samp.size=50)
  # execute importance.forestRK function
  imp &lt;- importance.forestRK(forestRK.1)
</code></pre>

<hr>
<h2 id='importance.plot.forestRK'>
Generates importance <code>ggplot</code> of the covariates considered in the
<code>forestRK</code> model
</h2><span id='topic+importance.plot.forestRK'></span>

<h3>Description</h3>

<p>Generates importance <code>ggplot</code> of the covariates considered in the
<code>forestRK</code> model.
</p>
<p>When the number of covariates under consideration is huge, it can be pretty
difficult to read the covariate name from this plot. In this case, user can
identify the name of the covariate that he or she is interested in by
extracting <code>importance.covariate.names</code> from the
<code>importance.forestRK.object</code> that was used in the function call.
<code>importance.covariate.names</code> lists the original names of the covariates
after ordering them from the most important to the least important. So for
example, the exact name of the covariate that has the second highest importance
would be the second element of the vector <code>importance.covariate.names</code>,
and so on.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> importance.plot.forestRK(importance.forestRK.object = importance.forestRK(),
                          colour.used = "dark green", fill.colour = "dark green",
                          label.size = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="importance.plot.forestRK_+3A_importance.forestrk.object">importance.forestRK.object</code></td>
<td>

<p>an <code>importance.forestRK</code> object.
</p>
</td></tr>
<tr><td><code id="importance.plot.forestRK_+3A_colour.used">colour.used</code></td>
<td>

<p>colour used for the border of the importance plot; default is &quot;dark green&quot;.
</p>
</td></tr>
<tr><td><code id="importance.plot.forestRK_+3A_fill.colour">fill.colour</code></td>
<td>

<p>colour used to fill the bars of the importance plot; default is &quot;dark green&quot;
(yes, I like dark green).
</p>
</td></tr>
<tr><td><code id="importance.plot.forestRK_+3A_label.size">label.size</code></td>
<td>

<p>size of the labels; default is set to 10.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An importance plot of the covariates considered in the <code>forestRK</code> model,
ordered from the most important covariate to the least important covariate.
</p>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forestRK">forestRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  ## numericize the data
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

  # random forest
  # min.num.obs.end.node.tree is set to 5 by default;
  # entropy is set to TRUE by default
  # typically the nbags and samp.size has to be much larger than 30 and 50
  forestRK.1 &lt;- forestRK(x.train, y.train, nbags = 30, samp.size = 50)
  # execute forestRK.importance function
  imp &lt;- importance.forestRK(forestRK.1)

  # generate importance plot
  importance.plot.forestRK(imp)
</code></pre>

<hr>
<h2 id='mds.plot.forestRK'>
Makes 2D MDS (multidimensional scaling) <code>ggplot</code> of the test observations
based on the predictions from a <code>forestRK</code> model.
</h2><span id='topic+mds.plot.forestRK'></span>

<h3>Description</h3>

<p>Plots 2D MDS (Multi-Dimensional Scaling) <code>ggplot</code> of the test observations
based on the provided <code>forestRK</code> model, and each test observation is
colour coded by their predicted class types.
</p>
<p>The plot also has legends that tells user which colour pertains to which
predicted class type.
</p>
<p>The existing R functions <code>dist</code> and <code>cmdscale</code> were used in this
function to compute the Multi-Dimensional Scales of the test data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mds.plot.forestRK(pred.forestRK.object = pred.forestRK(),
 plot.title ="MDS Plot of Test Data Colour Coded by Forest RK Model Predictions",
 xlab ="First Coordinate", ylab = "Second Coordinate",
 colour.lab = "Predictions By The Random Forest RK Model")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mds.plot.forestRK_+3A_pred.forestrk.object">pred.forestRK.object</code></td>
<td>

<p>a <code>pred.forestRK()</code> object.
</p>
</td></tr>
<tr><td><code id="mds.plot.forestRK_+3A_plot.title">plot.title</code></td>
<td>

<p>an user specified title for the mds plot; the default is
&quot;MDS Plot of Test Data Colour Coded by Forest RK Model Predictions&quot;.
</p>
</td></tr>
<tr><td><code id="mds.plot.forestRK_+3A_xlab">xlab</code></td>
<td>

<p>label for the x-axis of the plot; the default is &quot;First Coordinate&quot;.
</p>
</td></tr>
<tr><td><code id="mds.plot.forestRK_+3A_ylab">ylab</code></td>
<td>

<p>label for the y-axis of the plot; the default is &quot;Second Coordinate&quot;.
</p>
</td></tr>
<tr><td><code id="mds.plot.forestRK_+3A_colour.lab">colour.lab</code></td>
<td>

<p>label title for the legend that specifies categories for each colour;
the default is &quot;Predictions By The Random Forest RK Model&quot;.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A multidimensional scaling ggplot (2D) of the test observations, colour coded
by their predicted class types.
</p>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forestRK">forestRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  x.test &lt;- x.organizer(iris[,1:4], encoding = "num")[c(26:50,76:100,126:150),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new
  y.factor.levels &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.factor.levels

  # min.num.obs.end.node.tree is set to 5 by default;
  # entropy is set to TRUE by default
  # typically the nbags and samp.size has to be much larger than 30 and 50
  pred.forest.rk &lt;- pred.forestRK(x.test = x.test,
                                  x.training = x.train, y.training = y.train,
                                  nbags = 30, samp.size = 50,
                                  y.factor.levels = y.factor.levels)

  # generate a classical mds plot of test observations
  # and colour code them by the predicted class
  mds.plot.forestRK(pred.forest.rk)
</code></pre>

<hr>
<h2 id='pred.forestRK'>
Make predictions on the test data based on the forestRK model
constructed from the training data
</h2><span id='topic+pred.forestRK'></span>

<h3>Description</h3>

<p>Makes predictions on the test dataset based on the <code>forestRK</code> model
constructed from the training dataset.
</p>
<p>Please be aware that, the test data points in <code>test.prediction.df.list</code>
, <code>pred.for.obs.forest.rk</code>, and <code>num.pred.for.obs.forest.rk</code> are
re-ordered by the increasing original index number (the original rownames) of
those test observations. So if you shuffled the data before seperating them
into a training and a test set, the order of the data points in which they are
presented under the attribute <code>test.prediction.df.list</code>,
<code>pred.for.obs.forest.rk</code>, and <code>num.pred.for.obs.forest.rk</code> may not be
same as the shuffled order of your original test set.
</p>
<p>Calling of this function internally loads the package <code>rapportools</code>; this
is to allow the use of <code>is.boolean</code> method to check one of the stopping
criteria in the beginning.
</p>
<p>The basic mechanism behind <code>pred.forestRK</code> function is the following:
</p>
<p>When the function is called, it calls <code>forestRK</code> function after passing
the user-specified training data as an argument, in order to first generate the
<code>forestRK</code> object. After that, the function uses <code>pred.treeRK</code>
function to make predictions on the test observations based on each individual
tree in the <code>forestRK</code> object. Once the individual prediction from each
tree are obtained for all of the test observations, the function stores those
individual predictions under a big dataframe. Once that data frame is complete,
then the function collapses the results by the rule of the majority votes.
For example, for the m-th observation from the test set, if the most frequently
predicted class type for that m-th test observation by all of the rkTrees in
the forest is class type 'A', then by the rule of the majority votes, the
<code>pred.forestRK</code> function will assign class 'A' as the predicted class type
for that m-th test observation based on the <code>forestRK</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  pred.forestRK(x.test = data.frame(), x.training = data.frame(),
                y.training = c(), y.factor.levels,
                min.num.obs.end.node.tree = 5,
                nbags, samp.size, entropy = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pred.forestRK_+3A_x.test">x.test</code></td>
<td>

<p>a numericized data frame of covariates of the data points on which we want
to make our predictions (typically the test observations); <code>x.test</code> can
be obtained by applying the <code>x.organizer()</code> function. <code>x.test</code>
should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="pred.forestRK_+3A_x.training">x.training</code></td>
<td>

<p>a numericized data frame of covariates of data points from which we build our
<code>forestRK</code> model (typically the training observations); <code>x.training</code>
can be obtained by applying the <code>x.organizer()</code> function.
<code>x.training</code>should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="pred.forestRK_+3A_y.training">y.training</code></td>
<td>

<p>a vector that stores numericized class types of the training
data points; <code>y.training</code> should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="pred.forestRK_+3A_min.num.obs.end.node.tree">min.num.obs.end.node.tree</code></td>
<td>

<p>the minimum number of observations that we want each end node of
our <code>rktree</code> to contain. Default is set to 5.
</p>
</td></tr>
<tr><td><code id="pred.forestRK_+3A_nbags">nbags</code></td>
<td>

<p>the number of bootstrap samples that we want to generate to form a
<code>forest-RK</code>.
</p>
</td></tr>
<tr><td><code id="pred.forestRK_+3A_samp.size">samp.size</code></td>
<td>

<p>the number of data points that we want each of our bootstrap sample to
contain.
</p>
</td></tr>
<tr><td><code id="pred.forestRK_+3A_y.factor.levels">y.factor.levels</code></td>
<td>

<p>a vector of original names of all class types that the user considers in his
or her study (can be obtained via <code>y.organizer()$y.factor.levels</code>)
</p>
</td></tr>
<tr><td><code id="pred.forestRK_+3A_entropy">entropy</code></td>
<td>

<p><code>TRUE</code> if we use Entropy as the splitting criteria;
<code>FALSE</code> if we use the Gini Index for the splitting criteria.
Default is set to <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following items:
</p>
<table>
<tr><td><code>x.test</code></td>
<td>

<p>the original test dataset that we used to make predictions.
</p>
</td></tr>
<tr><td><code>df.of.predictions.for.all.observations</code></td>
<td>

<p>a data frame storing predicted class types for all test observations from
each tree in the forest; each row of this data frame pertains to individual
test observation, and each column pertain to a specific tree from the
<code>forestRK</code> model.
This data frame stores predicted (numericized) class type of each test
observation from each tree in the <code>forestRK</code> model.
</p>
</td></tr>
<tr><td><code>forest.rk</code></td>
<td>

<p>a <code>forestRK</code> object that was generated in the beginning of the function
call.
</p>
</td></tr>
<tr><td><code>test.prediction.df.list</code></td>
<td>

<p>a list of data frames storing the <code>prediction.df</code>'s
(the data frame that can be obtained via <code>pred.treeRK()$prediction.df</code>)
of the test observations that were generated from each tree in the
<code>forestRK</code> model. Note that the test data points in
<code>test.prediction.df.list</code> are re-ordered by the increasing original
observation index number.
</p>
</td></tr>
<tr><td><code>pred.for.obs.forest.rk</code></td>
<td>

<p>a vector that stores the actual predicted class labels of the
test observations instead of their numericized (integer) class types.
Note that the test data points in <code>pred.for.obs.forest.rk</code> are
re-ordered by the increasing original observation index number.
</p>
</td></tr>
<tr><td><code>num.pred.for.obs.forest.rk</code></td>
<td>

<p>the numericized version of <code>pred.for.obs.forest.rk</code>.
Note that the test data points in <code>num.pred.for.obs.forest.rk</code> are
re-ordered by the increasing original observation index number.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pred.treeRK">pred.treeRK</a></code>
<code><a href="#topic+forestRK">forestRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  ## numericize the data
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  x.test &lt;- x.organizer(iris[,1:4], encoding = "num")[c(26:50,76:100,126:150),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

  y.factor.levels &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.factor.levels

  ## make prediction from a random forest RK model
  ## typically the nbags and samp.size has to be much larger than 30 and 50
  pred.forest.rk &lt;- pred.forestRK(x.test = x.test, x.training = x.train,
                                  y.training = y.train,
                                  y.factor.levels,
                                  min.num.obs.end.node.tree = 6,
                                  nbags = 30, samp.size = 50, entropy = FALSE)
  pred.forest.rk$test.prediction.df.list[[10]]
  pred.forest.rk$pred.for.obs.forest.rk # etc....
</code></pre>

<hr>
<h2 id='pred.treeRK'>
Make predictions on the test observations based on a rktree model
</h2><span id='topic+pred.treeRK'></span>

<h3>Description</h3>

<p>Makes predictions on the observations in the test dataset based on the
<code>rktree</code> model constructed from the training dataset.
</p>
<p>Please be aware that, at the end of the <code>pred.treeRK</code> function, the test
data points in <code>prediction.df</code> are re-ordered by the increasing original
index number (the original rownames) of those test observations. So if you
shuffled the data before seperating them into a training and a test set,
the order of the data points in which they are presented under the data frame
<code>prediction.df</code> may not be same as the shuffled order in your original
test set.
</p>
<p>Users of this function may be interested in identifying the original name of
the numericized predicted class type shown in the last column of data frame
<code>prediction.df</code>. This can easily be done by extracting the attribute
<code>y.factor.levels</code> from the <code>y.organizer</code> object. For example, if the
data frame <code>prediction.df</code> indicates that the predicted class type of the
1st test observation is &quot;2&quot;, that means the actual name of the predicted
class type for that 1st test observation is indicated as the 2nd element of the
vector  <code>y.organizer.object$y.factor.levels</code> that we can obtain during
the data cleaning phase.
</p>
<p>The <code>pred.treeRK</code> function makes a use of the list of hierarchical flags
generated by the <code>construct.treeRK</code> function; the function uses the list
of hierarchical flag as a guide to how it should split the test set to make
predictions. The function <code>pred.treeRK</code> itself actually generates a list
of hierarchical flag of its own as it splits the test set, and at the end of
the function <code>pred.treeRK</code> tries to match the list of hierarchical flag it
generated with the list of hierarchical flag from the <code>construct.treeRK</code>
function. If the two flags match exactly, then it is a good sign since this
would imply that the splitting on the test set was done in the manner consistent
with how the training set was split when the rkTree in question was built.
If there is any difference in the two flags, however, this is not a good sign
since it would signal that the splitting on the test set has done in a different
manner than how the splitting was done on the training set; if the mismatch
occurs, the <code>pred.treeRK</code> function will stop and throw an error. For more
information about the hierarchical flags of a <code>rkTree</code>, please see the
<code>construct.treeRK</code> section of this documentation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> pred.treeRK(X = data.frame(), rktree = construct.treeRK())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pred.treeRK_+3A_x">X</code></td>
<td>

<p>a numericized data frame of covariates of the test observations
or the observations that we want to make predictions for (obtained via
<code>x.organizer()</code>). <code>X</code> should contain no <code>NA</code> or <code>NaN</code>'s.
</p>
</td></tr>
<tr><td><code id="pred.treeRK_+3A_rktree">rktree</code></td>
<td>

<p>a <code>construct.treeRK</code> object.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following items:
</p>
<table>
<tr><td><code>prediction.df</code></td>
<td>

<p>a data frame of test observations. If <code>prediction.df</code> has <code>n</code>
columns, the first <code>n-1</code> columns will contain the numericized covariates
of the test observations, and the very last <code>n</code>-th column will contain
the predicted numericized class type for each of those test observations.
Note that, at the end of the <code>pred.treeRK</code> function, the test data points
in <code>prediction.df</code> are re-ordered by theincreasing original observation
index number.
</p>
</td></tr>
<tr><td><code>flag.pred</code></td>
<td>

<p>the hierarchical flag of splits performed on the test set by applying the
<code>rktree</code> model in question.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pred.forestRK">pred.forestRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the forestRK package
  library(forestRK)

  ## numericize the data
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  x.test &lt;- x.organizer(iris[,1:4], encoding = "num")[c(26:50,76:100,126:150),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

  ## Construct a tree
  # min.num.obs.end.node.tree is set to 5 by default;
  # entropy is set to TRUE by default
  tree.entropy &lt;- construct.treeRK(x.train, y.train)
  tree.gini &lt;- construct.treeRK(x.train, y.train,
                                min.num.obs.end.node.tree = 6, entropy = FALSE)

  ## Make predictions on the test set based on the constructed rktree model
  # last column of prediction.df stores predicted class on the test observations
  # based on a given rktree
  prediction.df &lt;- pred.treeRK(X = x.test, tree.entropy)$prediction.df
  flag.pred &lt;- pred.treeRK(X = x.test, tree.entropy)$flag.pred
</code></pre>

<hr>
<h2 id='var.used.forestRK'>
Extract the list of covariates used to perform the splits to
generate a particular tree(s) in a <code>forestRK</code> object
</h2><span id='topic+var.used.forestRK'></span>

<h3>Description</h3>

<p>Spits out the list of covariates used to perform the splits to generate
a particular tree(s) in a <code>forestRK</code> object that the user provided.
</p>
<p>The function extracts the list of names of covariates used in splits to
construct a single or a multiple numbers of trees from a <code>forestRK</code>
object. The <code>var.used.forestRK</code> displays the actual name of the covariate
used for each split (not their numericized ones), consistent to the exact
order of the split; for instance, the 1st element of the vector
<code>covariate.used.for.split.tree[["6"]]</code> from the example below is the
covariate on which the 1st split had occured while the 6th tree in the
<code>forestRK.1</code> object was built.
</p>
<p>Each vector in the list are named by the exact indices of the tree;
for example, if the code
<code>obj &lt;- var.used.forestRK(forestRK.1, tree.index=c(4,5,6))</code> is used to
extract the list of covariates used for splitting to construct 4th, 5th, and
6th trees in the forest, and the user can retrieve the information pertains
explicitly to the 6th tree in the forest by doing <code>obj[["6"]]</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> var.used.forestRK(forestRK.object = forestRK(), tree.index = c())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var.used.forestRK_+3A_forestrk.object">forestRK.object</code></td>
<td>

<p>a <code>forestRK</code> object.
</p>
</td></tr>
<tr><td><code id="var.used.forestRK_+3A_tree.index">tree.index</code></td>
<td>

<p>a vector storing the indices of the trees that we are interested to examine.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of vectors that stores the names of covariates on which each split was
performed to construct the specific tree(s) in a <code>forestRK</code> model that the
user provided.
</p>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forestRK">forestRK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(forestRK)

  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
  y.train &lt;- y.organizer(iris[c(1:25,51:75,101:125),5])$y.new

  # random forest
  # min.num.obs.end.node.tree is set to 5 by default;
  # entropy is set to TRUE by default
  # normally nbags and samp.size have to be much larger than 30 and 50
  forestRK.1 &lt;- forestRK(x.train, y.train, nbags = 30, samp.size = 50)

  # prediction from a random forest RK
  covariate.used.for.split.tree &lt;- var.used.forestRK(forestRK.1,
                                                     tree.index=c(4,5,6))

  # retrieve the list of covariates used for splitting for the 'tree #6'
  covariate.used.for.split.tree[["6"]]
</code></pre>

<hr>
<h2 id='x.organizer'>
Numericizing a data frame of covariates from the original dataset
via Binary or Numeric Encoding
</h2><span id='topic+x.organizer'></span>

<h3>Description</h3>

<p>Takes the original data frame of covariates as an input (which may or
may not be numeric), and converts it into a numericized data frame by
applying either Binary or Numeric Encoding.
</p>
<p>Binary Encoding for categorical features are recommended for
tree ensembles when the cardinality of categorical feature is &gt;= 1000;
Numeric Encoding for categorical features are recommended for
tree ensembles when the cardinality of categorical features is &lt; 1000.
</p>
<p>For more information about the Binary and Numeric Encoding and their
effectiveness under different cardinality, please visit:
https://medium.com/data-design/
visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931
</p>
<p>NOTE: In order to use other functions within the forestRK package,
you must ensure that the numericized data frame of covariates (the
<code>x.organizer</code> object) contains no missing record,
that is, you have to remove any record containing <code>NA</code> or <code>NaN</code>
prior to applying the <code>x.organizer</code> function.
</p>
<p>Following is the summary of the data cleaning process with
<code>x.organizer()</code>:
</p>
<p>1. remove all <code>NA</code> or <code>NaN</code>'s from the data in hand.
2. split the data into a data frame that contains covariates of
ALL data points, (BOTH training and test observations), and a
vector that contains class types of the training observations;
3. apply the <code>x.organizer</code> to the big data frame of covariates
of all observations.
4. split the <code>x.organizer</code> output into a training and
a test set, as needed.
</p>
<p>PROPER DATA CLEANING IS ABSOLUTELY NECESSARY FOR forestRK FUNCTIONS TO WORK!
</p>


<h3>Usage</h3>

<pre><code class='language-R'> x.organizer(x.dat = data.frame(), encoding = c("num","bin"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="x.organizer_+3A_x.dat">x.dat</code></td>
<td>

<p>a data frame storing covariates of each observation
(can be either numeric or non-numeric) from the original data;
<code>x.dat</code> should contain no <code>NA</code> or <code>NaN</code>.
The rownames of <code>x.dat</code> should be numerical index for each observations.
</p>
</td></tr>
<tr><td><code id="x.organizer_+3A_encoding">encoding</code></td>
<td>

<p>type of encoding done for the categorical features;
&quot;<code>num</code>&quot; stands for Numeric Encoding, and
&quot;<code>bin</code>&quot; stands for Binary Encoding.
When the data in question only has numeric features, then
the user can select either one of &quot;<code>num</code>&quot; or &quot;<code>bin</code>&quot;,
and the <code>x.organizer</code> function will just return the original
numeric dataset.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numericized data frame of the covariates from the original data
obtained via either Numeric or Binary Encoding.
</p>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+y.organizer">y.organizer</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  library(forestRK) # load the package forestRK

  ## Basic Procedures
  ## 1. Apply x.organizer to a data frame that stores covariates of
  ## ALL observations (BOTH training and test observations)
  ## 2. Split the output from 1 into a training and a test set, as needed

  # note: iris[,1:4] are the columns of the iris dataset that stores
  # covariate values

  # covariates of training data set
  x.train &lt;- x.organizer(iris[,1:4], encoding = "num")[c(1:25,51:75,101:125),]
</code></pre>

<hr>
<h2 id='y.organizer'>
Numericize the vector containing categorical class type(<code>y</code>)
of the original data
</h2><span id='topic+y.organizer'></span>

<h3>Description</h3>

<p>Numericizes a vector of categorical class type of each (training)
data point.
</p>
<p>NOTE: In order to use other functions within the forestRK package,
you must ensure that the original vector of class type <code>y</code>
contains no missing record (<code>NA</code>, <code>NaN</code>), that is,
you have to remove any record containing <code>NA</code> or <code>NaN</code>
prior to applying the <code>y.organizer</code> function.
</p>
<p>Following is the summary of the data cleaning process with
<code>y.organizer()</code>:
1. remove all <code>NA</code> or <code>NaN</code>'s from the dataset
in hand.
2. split the training dataset into a data frame that contains
covariates of ALL observations (BOTH training and test observations),
and a vector that contains class types of the training observations;
3. apply the <code>y.organizer</code> to the vector that contains class type
of each training observation.
</p>
<p>PROPER DATA CLEANING IS NECESSARY FOR THE forestRK FUNCTIONS TO WORK!
</p>


<h3>Usage</h3>

<pre><code class='language-R'> y.organizer(y = c())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="y.organizer_+3A_y">y</code></td>
<td>

<p>a vector containing the class type of each observation
from the dataset on which we want to build our
rktree models (the training dataset); <code>y</code> should contain
no <code>NA</code> or <code>NaN</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following items:
</p>
<table>
<tr><td><code>y.new</code></td>
<td>

<p>a vector containing numericized class type of each observation from
the dataset from which our rktree models are generated from.
(these are typically the observations from the training set)
</p>
</td></tr>
<tr><td><code>y.factor.levels</code></td>
<td>

<p>a vector storing original names of the numericized class types.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hyunjin Cho, <a href="mailto:h56cho@uwaterloo.ca">h56cho@uwaterloo.ca</a>
Rebecca Su, <a href="mailto:y57su@uwaterloo.ca">y57su@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+x.organizer">x.organizer</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## example: iris dataset
  ## load the package forestRK
  library(forestRK)

  ## Basic Procedures:
  ## 1. Extract the portion of the data that stores class type of each
  ##    TRAINING observation, and make it as a vector
  ## 2. apply y.organizer function to the vector obtained from 1

  y.train &lt;- y.organizer(as.vector(iris[c(1:25,51:75,101:125),5]))
  ## retrieves the original names of each class type, if the class names
  ## were originally non-numeric
  y.train$y.factor.levels
  ## retrieves the numericized vector that stores classification category
  y.train$y.new
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
