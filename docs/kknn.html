<!DOCTYPE html><html lang="en"><head><title>Help for package kknn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kknn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#kknn-package'>
<p>Weighted k-Nearest Neighbors Classification and Clustering</p></a></li>
<li><a href='#contr.dummy'><p>Contrast Matrices</p></a></li>
<li><a href='#glass'><p> Glass Identification Database</p></a></li>
<li><a href='#ionosphere'><p>Johns Hopkins University Ionosphere Database</p></a></li>
<li><a href='#kknn'><p>Weighted k-Nearest Neighbor Classifier</p></a></li>
<li><a href='#kknn-deprecated'>
<p>Deprecated Functions in Package kknn</p></a></li>
<li><a href='#miete'><p>Munich Rent Standard Database (1994)</p></a></li>
<li><a href='#specClust'>
<p>Spectral Clustering</p></a></li>
<li><a href='#train.kknn'><p>Training kknn</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Weighted k-Nearest Neighbors</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-03-26</td>
</tr>
<tr>
<td>Description:</td>
<td>Weighted k-Nearest Neighbors for Classification, Regression and Clustering.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>igraph (&ge; 1.0), Matrix, stats, graphics</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>TRUE</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/KlausVigo/kknn">https://github.com/KlausVigo/kknn</a></td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-03-26 18:35:26 UTC; klaus</td>
</tr>
<tr>
<td>Author:</td>
<td>Klaus Schliep [aut, cre],
  Klaus Hechenbichler [aut],
  Antoine Lizee [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Klaus Schliep &lt;klaus.schliep@gmail.com&gt;</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-03-26 22:02:21</td>
</tr>
</table>
<hr>
<h2 id='kknn-package'>
Weighted k-Nearest Neighbors Classification and Clustering 
</h2><span id='topic+kknn-package'></span>

<h3>Description</h3>

<p>Weighted k-Nearest Neighbors Classification, Regression and spectral Clustering
</p>
<p>The complete list of functions can be displayed with <code>library(help = kknn)</code>. 
</p>


<h3>Author(s)</h3>

<p>Klaus Schliep  <br />
Klaus Hechenbichler
</p>
<p>Maintainer: Klaus Schliep &lt;klaus.schliep@gmail.com&gt;
</p>


<h3>References</h3>

<p>Hechenbichler K. and Schliep K.P. (2004)  <em>Weighted k-Nearest-Neighbor Techniques and Ordinal Classification</em>, Discussion Paper 399, SFB 386, Ludwig-Maximilians University Munich
(<a href="http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps">http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps</a>)
</p>
<p>Hechenbichler K. (2005)  <em>Ensemble-Techniken und ordinale Klassifikation</em>, PhD-thesis
</p>

<hr>
<h2 id='contr.dummy'>Contrast Matrices</h2><span id='topic+contr.dummy'></span><span id='topic+contr.metric'></span><span id='topic+contr.ordinal'></span>

<h3>Description</h3>

<p>Returns a matrix of contrasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contr.dummy(n, contrasts = TRUE)
contr.ordinal(n, contrasts = TRUE)
contr.metric(n, contrasts = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="contr.dummy_+3A_n">n</code></td>
<td>
<p>A vector containing levels of a factor, or the number of levels.</p>
</td></tr>
<tr><td><code id="contr.dummy_+3A_contrasts">contrasts</code></td>
<td>
<p>A logical value indicating whether contrasts should be computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>contr.dummy</code> is standard dummy-coding, <code>contr.metric</code> has the same 
effect like <code>as.numeric</code> (makes sense of course only for ordered variables). 
<code>contr.ordinal</code> computes contrasts for ordinal variables.
</p>


<h3>Value</h3>

<p>A matrix with <em>n</em> rows and <em>n-1</em> columns for <code>contr.ordinal</code>, 
a matrix with <em>n</em> rows and <em>n</em> columns for <code>contr.dummy</code> 
and a vector of length <em>n</em> for <code>contr.metric</code>.
</p>


<h3>Author(s)</h3>

<p>Klaus P. Schliep <a href="mailto:klaus.schliep@gmail.com">klaus.schliep@gmail.com</a> </p>


<h3>References</h3>

<p>Hechenbichler K. and Schliep K.P. (2004)  <em>Weighted k-Nearest-Neighbor Techniques and Ordinal Classification</em>, Discussion Paper 399, SFB 386, Ludwig-Maximilians University Munich
(<a href="http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps">http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps</a>)</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+contrasts">contrasts</a></code>, <code><a href="stats.html#topic+contrast">contr.poly</a></code> and <code><a href="MASS.html#topic+contr.sdif">contr.sdif</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>contr.metric(5)
contr.ordinal(5)
contr.dummy(5)
</code></pre>

<hr>
<h2 id='glass'> Glass Identification Database</h2><span id='topic+glass'></span>

<h3>Description</h3>

<p>A data frame with 214 observations, where the problem is to 
predict the type of glass in terms of their oxide content 
(i.e. Na, Fe, K, etc). The study of classification of types of 
glass was motivated by criminological investigation.  At the 
scene of the crime, the glass left can be used as evidence...
if it is correctly identified!
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(glass)</code></pre>


<h3>Format</h3>

<p>A data frame with 214 observations on the following 11 variables.
</p>

<dl>
<dt>Id</dt><dd><p>Id number.</p>
</dd>
<dt>RI</dt><dd><p>Refractive index.</p>
</dd>
<dt>Na</dt><dd><p>Sodium (unit measurement: weight percent in corresponding
oxide, as are attributes 4-10).</p>
</dd>
<dt>Mg</dt><dd><p>Magnesium.</p>
</dd>
<dt>Al</dt><dd><p>Aluminum.</p>
</dd>
<dt>Si</dt><dd><p>Silicon.</p>
</dd>
<dt>K</dt><dd><p>Potassium.</p>
</dd>
<dt>Ca</dt><dd><p>Calcium.</p>
</dd>
<dt>Ba</dt><dd><p>Barium.</p>
</dd>
<dt>Fe</dt><dd><p>Iron.</p>
</dd>
<dt>Type</dt><dd><p>Type of glass: (class attribute) <br />
<code>1</code> building windows float processed <br /> 
<code>2</code> building windows non float processed <br />
<code>3</code> vehicle windows float processed <br />
<code>4</code> vehicle windows non float processed (none in this database) <br />
<code>5</code> containers <br />
<code>6</code> tableware <br />
<code>7</code> headlamps 
</p>
</dd>
</dl>



<h3>Source</h3>

 
<ul>
<li><p> Creator: B. German, Central Research Establishment,
Home Office Forensic Science Service,
Aldermaston, Reading, Berkshire RG7 4PN
</p>
</li>
<li><p> Donor: Vina Spiehler, Ph.D., DABFT, Diagnostic Products Corporation
</p>
</li></ul>
    
<p>The data have been taken from the UCI Machine Learning Database Repository <br /> 
<a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">http://www.ics.uci.edu/~mlearn/MLRepository.html</a> <br />   
and were converted to R format by <a href="mailto:klaus.schliep@gmail.com">klaus.schliep@gmail.com</a>.    
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(glass)
str(glass)
</code></pre>

<hr>
<h2 id='ionosphere'>Johns Hopkins University Ionosphere Database</h2><span id='topic+ionosphere'></span>

<h3>Description</h3>

<p>This radar data was collected by a system in Goose Bay, Labrador. 
This system consists of a phased array of 16 high-frequency antennas with a
total transmitted power on the order of 6.4 kilowatts.  See the paper
for more details.  The targets were free electrons in the ionosphere.
&quot;Good&quot; radar returns are those showing evidence of some type of structure 
in the ionosphere.  &quot;Bad&quot; returns are those that do not; their signals pass
through the ionosphere.  
</p>
<p>Received signals were processed using an autocorrelation function whose
arguments are the time of a pulse and the pulse number.  There were 17
pulse numbers for the Goose Bay system.  Instances in this database are
described by 2 attributes per pulse number, corresponding to the complex
values returned by the function resulting from the complex electromagnetic
signal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ionosphere)</code></pre>


<h3>Format</h3>

<p>A data frame with 351 observations on the following 35 variables.
The first 34 continuous covariables are used for the prediction.
The 35th attribute is either <code>g</code> (&quot;good&quot;) or <code>b</code> (&quot;bad&quot;) 
according to the definition summarized above. This is a binary 
classification task.
</p>


<h3>Source</h3>


<p>Vince Sigillito (vgs@aplcen.apl.jhu.edu), Space Physics Group, Applied Physics Laboratory,
Johns Hopkins University, Johns Hopkins Road, Laurel, MD 20723 
</p>
<p>The data have been taken from the UCI Machine Learning Database Repository <br />
<a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">http://www.ics.uci.edu/~mlearn/MLRepository.html</a><br />   
and were converted to R format by <a href="mailto:klaus.schliep@gmail.com">klaus.schliep@gmail.com</a>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ionosphere)
</code></pre>

<hr>
<h2 id='kknn'>Weighted k-Nearest Neighbor Classifier </h2><span id='topic+kknn'></span><span id='topic+print.kknn'></span><span id='topic+summary.kknn'></span><span id='topic+predict.kknn'></span><span id='topic+kknn.dist'></span>

<h3>Description</h3>

<p>Performs k-nearest neighbor classification of a test set using a
training set. For each row of the test set, the k nearest training
set vectors (according to Minkowski distance) are found, and the 
classification is done via the maximum of summed kernel densities. 
In addition even ordinal and continuous variables can be predicted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kknn(formula = formula(train), train, test, na.action = na.omit(), 
	k = 7, distance = 2, kernel = "optimal", ykernel = NULL, scale=TRUE,
	contrasts = c('unordered' = "contr.dummy", ordered = "contr.ordinal"))
kknn.dist(learn, valid, k = 10, distance = 2)     
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kknn_+3A_formula">formula</code></td>
<td>
<p>A formula object.</p>
</td></tr>
<tr><td><code id="kknn_+3A_train">train</code></td>
<td>
<p>Matrix or data frame of training set cases.</p>
</td></tr>
<tr><td><code id="kknn_+3A_test">test</code></td>
<td>
<p>Matrix or data frame of test set cases.</p>
</td></tr>
<tr><td><code id="kknn_+3A_learn">learn</code></td>
<td>
<p>Matrix or data frame of training set cases.</p>
</td></tr>
<tr><td><code id="kknn_+3A_valid">valid</code></td>
<td>
<p>Matrix or data frame of test set cases.</p>
</td></tr>  
<tr><td><code id="kknn_+3A_na.action">na.action</code></td>
<td>
<p>A function which indicates what should happen when the data contain 'NA's.</p>
</td></tr>
<tr><td><code id="kknn_+3A_k">k</code></td>
<td>
<p>Number of neighbors considered.</p>
</td></tr>
<tr><td><code id="kknn_+3A_distance">distance</code></td>
<td>
<p>Parameter of Minkowski distance.</p>
</td></tr>
<tr><td><code id="kknn_+3A_kernel">kernel</code></td>
<td>
<p>Kernel to use. Possible choices are &quot;rectangular&quot; (which is standard unweighted knn), &quot;triangular&quot;, &quot;epanechnikov&quot; (or beta(2,2)), 
&quot;biweight&quot; (or beta(3,3)), &quot;triweight&quot; (or beta(4,4)), &quot;cos&quot;, &quot;inv&quot;, &quot;gaussian&quot;, &quot;rank&quot; and &quot;optimal&quot;.</p>
</td></tr> 
<tr><td><code id="kknn_+3A_ykernel">ykernel</code></td>
<td>
<p>Window width of an y-kernel, especially for prediction of ordinal classes.</p>
</td></tr>
<tr><td><code id="kknn_+3A_scale">scale</code></td>
<td>
<p>logical, scale variable to have equal sd.</p>
</td></tr>   	
<tr><td><code id="kknn_+3A_contrasts">contrasts</code></td>
<td>
<p>A vector containing the 'unordered' and 'ordered' contrasts to use.</p>
</td></tr>	
</table>


<h3>Details</h3>

<p>This nearest neighbor method expands knn in several directions. First it can be used not only for classification, but also for regression and ordinal classification.
Second it uses kernel functions to weight the neighbors according to their distances.
In fact, not only kernel functions but every monotonic decreasing function 
<code class="reqn">f(x) \forall x&gt;0</code> will work fine. 
</p>
<p>The number of neighbours used for the &quot;optimal&quot; kernel should be <code class="reqn"> [ (2(d+4)/(d+2))^(d/(d+4)) k ]</code>, where k is the number that would be used for unweighted knn classification, i.e. kernel=&quot;rectangular&quot;. This factor <code class="reqn">(2(d+4)/(d+2))^(d/(d+4))</code> is between 1.2 and 2 (see Samworth (2012) for more details).
</p>


<h3>Value</h3>

<p><code>kknn</code> returns a list-object of class <code>kknn</code> including the components 
</p>
<table role = "presentation">
<tr><td><code>fitted.values</code></td>
<td>
<p>Vector of predictions.</p>
</td></tr>
<tr><td><code>CL</code></td>
<td>
<p>Matrix of classes of the k nearest neighbors.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>Matrix of weights of the k nearest neighbors.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>Matrix of distances of the k nearest neighbors.</p>
</td></tr>
<tr><td><code>C</code></td>
<td>
<p>Matrix of indices of the k nearest neighbors.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>Matrix of predicted class probabilities.</p>
</td></tr>
<tr><td><code>response</code></td>
<td>
<p>Type of response variable, one of <em>continuous</em>, <em>nominal</em> or <em>ordinal</em>.</p>
</td></tr>
<tr><td><code>distance</code></td>
<td>
<p>Parameter of Minkowski distance.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p>The 'terms' object used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Klaus P. Schliep <a href="mailto:klaus.schliep@gmail.com">klaus.schliep@gmail.com</a> <br /> Klaus Hechenbichler</p>


<h3>References</h3>

<p>Hechenbichler K. and Schliep K.P. (2004)  <em>Weighted k-Nearest-Neighbor Techniques and Ordinal Classification</em>, Discussion Paper 399, SFB 386, Ludwig-Maximilians University Munich
(<a href="http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps">http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps</a>)
</p>
<p>Hechenbichler K. (2005)  <em>Ensemble-Techniken und ordinale Klassifikation</em>, PhD-thesis
</p>
<p>Samworth, R.J. (2012) <em>Optimal weighted nearest neighbour classifiers.</em> Annals of Statistics, 40, 2733-2763.
(avaialble from <a href="http://www.statslab.cam.ac.uk/~rjs57/Research.html">http://www.statslab.cam.ac.uk/~rjs57/Research.html</a>) 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+train.kknn">train.kknn</a></code>, <code><a href="#topic+simulation">simulation</a></code>, <code><a href="class.html#topic+knn">knn</a></code> and <code><a href="class.html#topic+knn1">knn1</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(kknn)

data(iris)
m &lt;- dim(iris)[1]
val &lt;- sample(1:m, size = round(m/3), replace = FALSE, 
	prob = rep(1/m, m)) 
iris.learn &lt;- iris[-val,]
iris.valid &lt;- iris[val,]
iris.kknn &lt;- kknn(Species~., iris.learn, iris.valid, distance = 1,
	kernel = "triangular")
summary(iris.kknn)
fit &lt;- fitted(iris.kknn)
table(iris.valid$Species, fit)
pcol &lt;- as.character(as.numeric(iris.valid$Species))
pairs(iris.valid[1:4], pch = pcol, col = c("green3", "red")
	[(iris.valid$Species != fit)+1])

data(ionosphere)
ionosphere.learn &lt;- ionosphere[1:200,]
ionosphere.valid &lt;- ionosphere[-c(1:200),]
fit.kknn &lt;- kknn(class ~ ., ionosphere.learn, ionosphere.valid)
table(ionosphere.valid$class, fit.kknn$fit)
(fit.train1 &lt;- train.kknn(class ~ ., ionosphere.learn, kmax = 15, 
	kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 1))
table(predict(fit.train1, ionosphere.valid), ionosphere.valid$class)
(fit.train2 &lt;- train.kknn(class ~ ., ionosphere.learn, kmax = 15, 
	kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 2))
table(predict(fit.train2, ionosphere.valid), ionosphere.valid$class)
</code></pre>

<hr>
<h2 id='kknn-deprecated'>
Deprecated Functions in Package kknn
</h2><span id='topic+simulation'></span>

<h3>Description</h3>

<p>These functions are provided for compatibility with older versions of R only, and may be defunct as soon as of the next release. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulation(formula, data, runs = 10, train = TRUE, k = 11, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kknn-deprecated_+3A_formula">formula</code></td>
<td>
<p>A formula object.</p>
</td></tr>
<tr><td><code id="kknn-deprecated_+3A_data">data</code></td>
<td>
<p>Matrix or data frame. </p>
</td></tr>
<tr><td><code id="kknn-deprecated_+3A_runs">runs</code></td>
<td>
<p>Number of crossvalidation runs. </p>
</td></tr>
<tr><td><code id="kknn-deprecated_+3A_train">train</code></td>
<td>
<p>A logical value. If TRUE the training procedure for selecting optimal values of k and kernel is performed.</p>
</td></tr>
<tr><td><code id="kknn-deprecated_+3A_k">k</code></td>
<td>
<p>Number or maximal number of neighbors considered, dependent of choice for train.</p>
</td></tr>	
<tr><td><code id="kknn-deprecated_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix, containing the mean and variance of the misclassification
error, the absolute and the squared distances.
</p>


<h3>References</h3>

<p>Hechenbichler K. and Schliep K.P. (2004)  <em>Weighted k-Nearest-Neighbor Techniques and Ordinal Classification</em>, Discussion Paper 399, SFB 386, Ludwig-Maximilians University Munich
(<a href="http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps">http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps</a>) 
</p>


<h3>See Also</h3>

  <p><code><a href="base.html#topic+Defunct">Defunct</a></code> and <code><a href="base.html#topic+Deprecated">Deprecated</a></code> </p>

<hr>
<h2 id='miete'>Munich Rent Standard Database (1994)</h2><span id='topic+miete'></span>

<h3>Description</h3>

<p>Many german cities compose so-called rent standards to make a decision making instrument 
available to tenants, landlords, renting advisory boards and experts. The rent standards 
are used in particular for the determination of the local comparative rent (i.e. net rent 
as a function of household size, equipment, year of construction, etc.). For the composition
of the rent standards, a representative random sample is drawn from all relevant households, 
and the interesting data are determined by interviewers by means of questionnaires. The 
dataset contains the data of 1082 households interviewed for the munich rent standard 1994.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(miete)</code></pre>


<h3>Format</h3>

<p>A data frame with 1082 observations on the following 18 variables.
</p>

<dl>
<dt>nm</dt><dd><p>Net rent in DM.</p>
</dd>
<dt>wfl</dt><dd><p>Floor space in sqm.</p>
</dd>
<dt>bj</dt><dd><p>Year of construction.</p>
</dd>
<dt>bad0</dt><dd><p>Bathroom in apartment?<br />
1 : no<br />
0 : yes </p>
</dd>
<dt>zh</dt><dd><p>Central heating?<br />
1 : yes<br />
0 : no 
</p>
</dd>
<dt>ww0</dt><dd><p>Hot water supply?<br />
1 : no<br />
0 : yes  
</p>
</dd>
<dt>badkach</dt><dd><p>Tiled bathroom?<br />
1 : yes<br />
0 : no  
</p>
</dd>
<dt>fenster</dt><dd><p>Window type:<br />
1 : plain windows<br />
0 : state-of-the-art windows  
</p>
</dd>
<dt>kueche</dt><dd><p>Kitchen type<br />
1 : well equipped kitchen<br />
0 : plain kitchen  
</p>
</dd>
<dt>mvdauer</dt><dd><p>Lease duration in years.</p>
</dd>
<dt>bjkat</dt><dd><p>Age category of the building (bj categorized)<br />
1 : built before 1919<br />
2 : built between 1919 and 1948<br />
3 : built between 1949 and 1965<br />
4 : built between 1966 and 1977<br />
5 : built between 1978 and 1983<br />
6 : built after 1983  
</p>
</dd>
<dt>wflkat</dt><dd><p>Floor space category (wfl categorized):<br /> 
1 : less than 50 sqm<br /> 
2 : between 51 sqm and 80 sqm<br />
3 : at least 81 sqm  
</p>
</dd>
<dt>nmqm</dt><dd><p>Net rent per sqm.</p>
</dd>
<dt>rooms</dt><dd><p>Number of rooms in household.</p>
</dd>
<dt>nmkat</dt><dd><p>Net rent category (nm categorized):<br />      
1 : less than 500 DM<br /> 
2 : between 500 DM and 675 DM<br />
3 : between 675 DM and 850 DM<br />
4 : between 850 DM and 1150 DM<br />
5 : at least 1150 DM   
</p>
</dd>    
<dt>adr</dt><dd><p>Address type:<br />
1 : bad<br />
2 : average<br />
3 : good
</p>
</dd>
<dt>wohn</dt><dd><p>Residential type:<br />
1 : bad<br />
2 : average<br />
3 : good
</p>
</dd>
</dl>



<h3>Source</h3>

<p>Fahrmeir, L., Kuenstler, R., Pigeot, I. und Tutz, G. (1997): <em>Statistik: der Weg zur Datenanalyse</em>, Springer, Berlin. 
<a href="http://www.stat.uni-muenchen.de/service/datenarchiv">http://www.stat.uni-muenchen.de/service/datenarchiv</a>
</p>
<p>The data were converted to R format by <a href="mailto:klaus.schliep@gmail.com">klaus.schliep@gmail.com</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(miete)
str(miete)
</code></pre>

<hr>
<h2 id='specClust'>
Spectral Clustering
</h2><span id='topic+specClust'></span><span id='topic+plot.specClust'></span>

<h3>Description</h3>

<p>Spectral clustering based on k-nearest neighbor graph. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>specClust(data, centers=NULL, nn = 7, method = "symmetric", gmax=NULL, ...)
## S3 method for class 'specClust'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="specClust_+3A_data">data</code></td>
<td>
<p>Matrix or data frame.</p>
</td></tr>
<tr><td><code id="specClust_+3A_centers">centers</code></td>
<td>
<p>number of clusters to estimate, if NULL the number is chosen automatical.</p>
</td></tr>
<tr><td><code id="specClust_+3A_nn">nn</code></td>
<td>
<p>Number of neighbors considered.</p>
</td></tr>
<tr><td><code id="specClust_+3A_method">method</code></td>
<td>
<p>Normalisation of the Laplacian (&quot;none&quot;, &quot;symmetric&quot; or &quot;random-walk&quot;).</p>
</td></tr>
<tr><td><code id="specClust_+3A_gmax">gmax</code></td>
<td>
<p>maximal number of connected components.</p>
</td></tr>
<tr><td><code id="specClust_+3A_x">x</code></td>
<td>
<p>an object of class <code>specClust</code></p>
</td></tr>
<tr><td><code id="specClust_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>specClust</code> alllows to estimate several popular spectral clustering algorithms, for an overview see von Luxburg (2007).
</p>
<p>The Laplacian is constructed from a from nearest neighbors and there are several kernels available. 
The eigenvalues and eigenvectors are computed using the binding in igraph to arpack.  
This should ensure that this algorithm is also feasable for larger datasets as the the the distances used have dimension n*m, where n is the number of observations and m the number of nearest neighbors. The Laplacian is sparse and has roughly n*m elements and only k eigenvectors are computed, where k is the number of centers.   
</p>


<h3>Value</h3>

<p><code>specClust</code> returns a kmeans object or in case of k being a vector a list of kmeans objects.
</p>


<h3>Author(s)</h3>

<p> Klaus P. Schliep <a href="mailto:klaus.schliep@gmail.com">klaus.schliep@gmail.com</a></p>


<h3>References</h3>

<p>U. von Luxburg (2007) A tutorial on spectral clustering, <em>Stat Comput</em>, <b>17</b>, 395&ndash;416
</p>
<p>Ng, A., Jordan, M., Weiss, Y. (2002) On spectral clustering: analysis and an algorithm. In: Dietterich, T., Becker, S., Ghahramani, Z. (eds.)
<em>Advances in Neural Information Processing Systems</em>, <b>14</b>, 849&ndash;856. MIT Press, Cambridge 
</p>
<p>Lihi Zelnik-Manor and P. Perona (2004) Self-Tuning Spectral Clustering, <em>Eighteenth Annual Conference on Neural Information Processing Systems, (NIPS)</em>
</p>
<p>Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation. <em>IEEE Transactions on Pattern
Analysis and Machine Intelligence</em>, <b>22 (8)</b>, 888&ndash;905
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kknn">kknn</a></code>, <code>arpack</code>, <code><a href="stats.html#topic+kmeans">kmeans</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
cl &lt;- specClust(iris[,1:4], 3, nn=5)
pcol &lt;- as.character(as.numeric(iris$Species))
pairs(iris[1:4], pch = pcol, col = c("green", "red", "blue")[cl$cluster])
table(iris[,5], cl$cluster)
</code></pre>

<hr>
<h2 id='train.kknn'>Training kknn</h2><span id='topic+train.kknn'></span><span id='topic+plot.train.kknn'></span><span id='topic+print.train.kknn'></span><span id='topic+predict.train.kknn'></span><span id='topic+summary.train.kknn'></span><span id='topic+cv.kknn'></span>

<h3>Description</h3>

<p>Training of kknn method via leave-one-out (<code>train.kknn</code>) or k-fold (<code>cv.kknn</code>) crossvalidation. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train.kknn(formula, data, kmax = 11, ks = NULL, distance = 2, kernel = "optimal",
	ykernel = NULL, scale = TRUE, contrasts = c('unordered' = "contr.dummy",
	ordered = "contr.ordinal"), ...)
cv.kknn(formula, data, kcv = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="train.kknn_+3A_formula">formula</code></td>
<td>
<p>A formula object.</p>
</td></tr>
<tr><td><code id="train.kknn_+3A_data">data</code></td>
<td>
<p>Matrix or data frame. </p>
</td></tr>
<tr><td><code id="train.kknn_+3A_kmax">kmax</code></td>
<td>
<p>Maximum number of k, if <code>ks</code> is not specified.</p>
</td></tr>
<tr><td><code id="train.kknn_+3A_ks">ks</code></td>
<td>
<p>A vector specifying values of k. If not null, this takes precedence over <code>kmax</code>.</p>
</td></tr>
<tr><td><code id="train.kknn_+3A_distance">distance</code></td>
<td>
<p>Parameter of Minkowski distance.</p>
</td></tr>
<tr><td><code id="train.kknn_+3A_kernel">kernel</code></td>
<td>
<p>Kernel to use. Possible choices are &quot;rectangular&quot; 
(which is standard unweighted knn), &quot;triangular&quot;, &quot;epanechnikov&quot; 
(or beta(2,2)), &quot;biweight&quot; (or beta(3,3)), &quot;triweight&quot; (or beta(4,4)), 
&quot;cos&quot;, &quot;inv&quot;, &quot;gaussian&quot; and &quot;optimal&quot;.</p>
</td></tr>
<tr><td><code id="train.kknn_+3A_ykernel">ykernel</code></td>
<td>
<p>Window width of an y-kernel, especially for prediction 
of ordinal classes.</p>
</td></tr>
<tr><td><code id="train.kknn_+3A_scale">scale</code></td>
<td>
<p>logical, scale variable to have equal sd.</p>
</td></tr>     	  
<tr><td><code id="train.kknn_+3A_contrasts">contrasts</code></td>
<td>
<p>A vector containing the 'unordered' and 'ordered' contrasts to use.</p>
</td></tr>	
<tr><td><code id="train.kknn_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="train.kknn_+3A_kcv">kcv</code></td>
<td>
<p>Number of partitions for k-fold cross validation. </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>train.kknn</code> performs leave-one-out crossvalidation 
and is computatioanlly very efficient. <code>cv.kknn</code> performs k-fold crossvalidation and is generally slower and does not yet contain the test of different models yet.</p>


<h3>Value</h3>

<p><code>train.kknn</code> returns a list-object of class <code>train.kknn</code> including
the components. 
</p>
<table role = "presentation">
<tr><td><code>MISCLASS</code></td>
<td>
<p>Matrix of misclassification errors.</p>
</td></tr>
<tr><td><code>MEAN.ABS</code></td>
<td>
<p>Matrix of mean absolute errors.</p>
</td></tr>
<tr><td><code>MEAN.SQU</code></td>
<td>
<p>Matrix of mean squared errors.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>List of predictions for all combinations of kernel and k.</p>
</td></tr>
<tr><td><code>best.parameters</code></td>
<td>
<p>List containing the best parameter value for kernel and k.</p>
</td></tr>
<tr><td><code>response</code></td>
<td>
<p>Type of response variable, one of <em>continuous</em>, <em>nominal</em> or <em>ordinal</em>.</p>
</td></tr>
<tr><td><code>distance</code></td>
<td>
<p>Parameter of Minkowski distance.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p>The 'terms' object used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Klaus P. Schliep <a href="mailto:klaus.schliep@gmail.com">klaus.schliep@gmail.com</a> </p>


<h3>References</h3>

<p>Hechenbichler K. and Schliep K.P. (2004)  <em>Weighted k-Nearest-Neighbor Techniques
and Ordinal Classification</em>, Discussion Paper 399, SFB 386, Ludwig-Maximilians University Munich
(<a href="http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps">http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps</a>)
</p>
<p>Hechenbichler K. (2005)  <em>Ensemble-Techniken und ordinale Klassifikation</em>, PhD-thesis
</p>
<p>Samworth, R.J. (2012) <em>Optimal weighted nearest neighbour classifiers.</em> Annals of Statistics, 40, 2733-2763.
(avaialble from <a href="http://www.statslab.cam.ac.uk/~rjs57/Research.html">http://www.statslab.cam.ac.uk/~rjs57/Research.html</a>) 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+kknn">kknn</a></code> and <code><a href="#topic+simulation">simulation</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(kknn)
## Not run: 
data(miete)
(train.con &lt;- train.kknn(nmqm ~ wfl + bjkat + zh, data = miete, 
	kmax = 25, kernel = c("rectangular", "triangular", "epanechnikov",
	"gaussian", "rank", "optimal")))
plot(train.con)
(train.ord &lt;- train.kknn(wflkat ~ nm + bjkat + zh, miete, kmax = 25,
 	kernel = c("rectangular", "triangular", "epanechnikov", "gaussian", 
 	"rank", "optimal")))
plot(train.ord)
(train.nom &lt;- train.kknn(zh ~ wfl + bjkat + nmqm, miete, kmax = 25, 
	kernel = c("rectangular", "triangular", "epanechnikov", "gaussian", 
	"rank", "optimal")))
plot(train.nom)

## End(Not run)
data(glass)
glass &lt;- glass[,-1]
(fit.glass1 &lt;- train.kknn(Type ~ ., glass, kmax = 15, kernel = 
	c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 1))
(fit.glass2 &lt;- train.kknn(Type ~ ., glass, kmax = 15, kernel = 
	c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 2))
plot(fit.glass1)
plot(fit.glass2)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
