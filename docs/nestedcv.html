<!DOCTYPE html><html><head><title>Help for package nestedcv</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nestedcv}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#barplot_var_stability'><p>Barplot variable stability</p></a></li>
<li><a href='#boot_filter'><p>Bootstrap for filter functions</p></a></li>
<li><a href='#boot_ttest'><p>Bootstrap univariate filters</p></a></li>
<li><a href='#boruta_filter'><p>Boruta filter</p></a></li>
<li><a href='#boxplot_expression'><p>Boxplot expression levels of model predictors</p></a></li>
<li><a href='#class_balance'><p>Check class balance in training folds</p></a></li>
<li><a href='#coef.cva.glmnet'><p>Extract coefficients from a cva.glmnet object</p></a></li>
<li><a href='#coef.nestcv.glmnet'><p>Extract coefficients from nestcv.glmnet object</p></a></li>
<li><a href='#collinear'><p>Filter to reduce collinearity in predictors</p></a></li>
<li><a href='#combo_filter'><p>Combo filter</p></a></li>
<li><a href='#correls2'><p>Correlation between a vector and a matrix</p></a></li>
<li><a href='#cv_coef'><p>Coefficients from outer CV glmnet models</p></a></li>
<li><a href='#cv_varImp'><p>Extract variable importance from outer CV caret models</p></a></li>
<li><a href='#cva.glmnet'><p>Cross-validation of alpha for glmnet</p></a></li>
<li><a href='#glmnet_coefs'><p>glmnet coefficients</p></a></li>
<li><a href='#glmnet_filter'><p>glmnet filter</p></a></li>
<li><a href='#innercv_preds'><p>Inner CV predictions</p></a></li>
<li><a href='#innercv_roc'><p>Build ROC curve from left-out folds from inner CV</p></a></li>
<li><a href='#innercv_summary'><p>Summarise performance on inner CV test folds</p></a></li>
<li><a href='#lines.prc'><p>Add precision-recall curve to a plot</p></a></li>
<li><a href='#lm_filter'><p>Linear model filter</p></a></li>
<li><a href='#metrics'><p>Model performance metrics</p></a></li>
<li><a href='#model.hsstan'><p>hsstan model for cross-validation</p></a></li>
<li><a href='#nestcv.glmnet'><p>Nested cross-validation with glmnet</p></a></li>
<li><a href='#nestcv.SuperLearner'><p>Outer cross-validation of SuperLearner model</p></a></li>
<li><a href='#nestcv.train'><p>Nested cross-validation for caret</p></a></li>
<li><a href='#one_hot'><p>One-hot encode</p></a></li>
<li><a href='#outercv'><p>Outer cross-validation of selected models</p></a></li>
<li><a href='#plot_alphas'><p>Plot cross-validated glmnet alpha</p></a></li>
<li><a href='#plot_caret'><p>Plot caret tuning</p></a></li>
<li><a href='#plot_lambdas'><p>Plot cross-validated glmnet lambdas across outer folds</p></a></li>
<li><a href='#plot_shap_bar'><p>SHAP importance bar plot</p></a></li>
<li><a href='#plot_shap_beeswarm'><p>SHAP importance beeswarm plot</p></a></li>
<li><a href='#plot_var_stability'><p>Plot variable stability</p></a></li>
<li><a href='#plot_varImp'><p>Variable importance plot</p></a></li>
<li><a href='#plot.cva.glmnet'><p>Plot lambda across range of alphas</p></a></li>
<li><a href='#plot.prc'><p>Plot precision-recall curve</p></a></li>
<li><a href='#pls_filter'><p>Partial Least Squares filter</p></a></li>
<li><a href='#prc'><p>Build precision-recall curve</p></a></li>
<li><a href='#pred_nestcv_glmnet'><p>Prediction wrappers to use fastshap with nestedcv</p></a></li>
<li><a href='#predict.cva.glmnet'><p>Predict method for cva.glmnet models</p></a></li>
<li><a href='#predict.hsstan'><p>Predict from hsstan model fitted within cross-validation</p></a></li>
<li><a href='#predict.nestcv.glmnet'><p>Predict method for nestcv.glmnet fits</p></a></li>
<li><a href='#predSummary'><p>Summarise prediction performance metrics</p></a></li>
<li><a href='#randomsample'><p>Oversampling and undersampling</p></a></li>
<li><a href='#ranger_filter'><p>Random forest ranger filter</p></a></li>
<li><a href='#relieff_filter'><p>ReliefF filter</p></a></li>
<li><a href='#repeatcv'><p>Repeated nested CV</p></a></li>
<li><a href='#repeatfolds'><p>Create folds for repeated nested CV</p></a></li>
<li><a href='#rf_filter'><p>Random forest filter</p></a></li>
<li><a href='#smote'><p>SMOTE</p></a></li>
<li><a href='#stat_filter'><p>Univariate filter for binary classification with mixed predictor datatypes</p></a></li>
<li><a href='#summary_vars'><p>Summarise variables</p></a></li>
<li><a href='#supervisedPCA'><p>Supervised PCA plot</p></a></li>
<li><a href='#train_preds'><p>Outer training fold predictions</p></a></li>
<li><a href='#train_roc'><p>Build ROC curve from outer CV training folds</p></a></li>
<li><a href='#train_summary'><p>Summarise performance on outer training folds</p></a></li>
<li><a href='#ttest_filter'><p>Univariate filters</p></a></li>
<li><a href='#txtProgressBar2'><p>Text Progress Bar 2</p></a></li>
<li><a href='#var_direction'><p>Variable directionality</p></a></li>
<li><a href='#var_stability'><p>Variable stability</p></a></li>
<li><a href='#weight'><p>Calculate weights for class imbalance</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Nested Cross-Validation with 'glmnet' and 'caret'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.7.8</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Myles Lewis &lt;myles.lewis@qmul.ac.uk&gt;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/myles-lewis/nestedcv/issues">https://github.com/myles-lewis/nestedcv/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/myles-lewis/nestedcv">https://github.com/myles-lewis/nestedcv</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Implements nested k*l-fold cross-validation for lasso and elastic-net regularised linear models via the 'glmnet' package and other machine learning models via the 'caret' package. Cross-validation of 'glmnet' alpha mixing parameter and embedded fast filter functions for feature selection are provided. Described as double cross-validation by Stone (1977) &lt;<a href="https://doi.org/10.1111%2Fj.2517-6161.1977.tb01603.x">doi:10.1111/j.2517-6161.1977.tb01603.x</a>&gt;. Also implemented is a method using outer CV to measure unbiased model performance metrics when fitting Bayesian linear and logistic regression shrinkage models using the horseshoe prior over parameters to encourage a sparse model as described by Piironen &amp; Vehtari (2017) &lt;<a href="https://doi.org/10.1214%2F17-EJS1337SI">doi:10.1214/17-EJS1337SI</a>&gt;.</td>
</tr>
<tr>
<td>Language:</td>
<td>en-gb</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>caret, data.table, doParallel, foreach, ggplot2, glmnet,
matrixStats, matrixTests, methods, parallel, pROC, Rfast,
RhpcBLASctl, rlang, ROCR</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>Boruta, CORElearn, fastshap (&ge; 0.1.0), gbm, ggbeeswarm,
ggpubr, hsstan, mda, mlbench, pbapply, pls, randomForest,
ranger, RcppEigen, rmarkdown, knitr, SuperLearner</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-13 21:03:18 UTC; myles</td>
</tr>
<tr>
<td>Author:</td>
<td>Myles Lewis <a href="https://orcid.org/0000-0001-9365-5345"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Athina Spiliopoulou
    <a href="https://orcid.org/0000-0002-5929-6585"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Cankut Cubuk <a href="https://orcid.org/0000-0003-4646-0849"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Katriona Goldmann <a href="https://orcid.org/0000-0002-9073-6323"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-13 21:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='barplot_var_stability'>Barplot variable stability</h2><span id='topic+barplot_var_stability'></span>

<h3>Description</h3>

<p>Produces a ggplot2 plot of stability (as SEM) of variable importance across
models trained and tested across outer CV folds. Optionally overlays
directionality for binary response or regression outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>barplot_var_stability(
  x,
  final = TRUE,
  top = NULL,
  direction = 0,
  dir_labels = NULL,
  scheme = c("royalblue", "red"),
  breaks = NULL,
  percent = TRUE,
  level = 1,
  sort = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="barplot_var_stability_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code> or <code>nestcv.train</code> fitted object</p>
</td></tr>
<tr><td><code id="barplot_var_stability_+3A_final">final</code></td>
<td>
<p>Logical whether to restrict variables to only those which ended
up in the final fitted model or to include all variables selected across
all outer folds.</p>
</td></tr>
<tr><td><code id="barplot_var_stability_+3A_top">top</code></td>
<td>
<p>Limits number of variables plotted. Set to <code>NULL</code> to plot all
variables.</p>
</td></tr>
<tr><td><code id="barplot_var_stability_+3A_direction">direction</code></td>
<td>
<p>Integer controlling plotting of directionality for binary or
regression models. <code>0</code> means no directionality is shown, <code>1</code> means
directionality is overlaid as a colour, <code>2</code> means directionality is
reflected in the sign of variable importance. Not available for multiclass
caret models.</p>
</td></tr>
<tr><td><code id="barplot_var_stability_+3A_dir_labels">dir_labels</code></td>
<td>
<p>Character vector for controlling the legend when
<code>direction = 1</code></p>
</td></tr>
<tr><td><code id="barplot_var_stability_+3A_scheme">scheme</code></td>
<td>
<p>Vector of 2 colours for directionality when <code>direction = 1</code></p>
</td></tr>
<tr><td><code id="barplot_var_stability_+3A_breaks">breaks</code></td>
<td>
<p>Vector of continuous breaks for legend colour/size</p>
</td></tr>
<tr><td><code id="barplot_var_stability_+3A_percent">percent</code></td>
<td>
<p>Logical for <code>nestcv.glmnet</code> objects only, whether to scale
coefficients to percentage of the largest coefficient in each model. If set
to <code>FALSE</code>, model coefficients are shown and <code>direction</code> is ignored.</p>
</td></tr>
<tr><td><code id="barplot_var_stability_+3A_level">level</code></td>
<td>
<p>For multinomial <code>nestcv.glmnet</code> models only, either an integer
specifying which level of outcome is being examined, or the level can be
specified as a character value.</p>
</td></tr>
<tr><td><code id="barplot_var_stability_+3A_sort">sort</code></td>
<td>
<p>Logical whether to sort by mean variable importance. Passed to
<code><a href="#topic+var_stability">var_stability()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 plot
</p>


<h3>See Also</h3>

<p><code><a href="#topic+var_stability">var_stability()</a></code>
</p>

<hr>
<h2 id='boot_filter'>Bootstrap for filter functions</h2><span id='topic+boot_filter'></span>

<h3>Description</h3>

<p>Randomly samples predictors and averages the ranking to give an ensemble
measure of predictor variable importance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot_filter(y, x, filterFUN, B = 50, nfilter = NULL, type = "index", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boot_filter_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="boot_filter_+3A_x">x</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="boot_filter_+3A_filterfun">filterFUN</code></td>
<td>
<p>Filter function, e.g. <code><a href="#topic+ttest_filter">ttest_filter()</a></code>.</p>
</td></tr>
<tr><td><code id="boot_filter_+3A_b">B</code></td>
<td>
<p>Number of times to bootstrap</p>
</td></tr>
<tr><td><code id="boot_filter_+3A_nfilter">nfilter</code></td>
<td>
<p>Number of predictors to return</p>
</td></tr>
<tr><td><code id="boot_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices, &quot;full&quot;
returns full output.</p>
</td></tr>
<tr><td><code id="boot_filter_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to the function specified by <code>filterFUN</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (<code>type = "index"</code>)
or if <code>type = "full"</code> a matrix of rankings from each bootstrap is returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+boot_ttest">boot_ttest()</a></code>
</p>

<hr>
<h2 id='boot_ttest'>Bootstrap univariate filters</h2><span id='topic+boot_ttest'></span><span id='topic+boot_wilcoxon'></span><span id='topic+boot_anova'></span><span id='topic+boot_correl'></span><span id='topic+boot_lm'></span>

<h3>Description</h3>

<p>Randomly samples predictors and averages the ranking from filtering functions
including <code><a href="#topic+ttest_filter">ttest_filter()</a></code>, <code><a href="#topic+wilcoxon_filter">wilcoxon_filter()</a></code>, <code><a href="#topic+anova_filter">anova_filter()</a></code>,
<code><a href="#topic+correl_filter">correl_filter()</a></code> and <code><a href="#topic+lm_filter">lm_filter()</a></code> to give an ensemble measure of best
predictors by repeated random sampling subjected to a statistical test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot_ttest(y, x, B = 50, ...)

boot_wilcoxon(y, x, B = 50, ...)

boot_anova(y, x, B = 50, ...)

boot_correl(y, x, B = 50, ...)

boot_lm(y, x, B = 50, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boot_ttest_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="boot_ttest_+3A_x">x</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="boot_ttest_+3A_b">B</code></td>
<td>
<p>Number of times to bootstrap</p>
</td></tr>
<tr><td><code id="boot_ttest_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to the filter function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (<code>type = "index"</code>),
or if <code>type = "full"</code>, a matrix of rankings from each bootstrap is
returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ttest_filter">ttest_filter()</a></code>, <code><a href="#topic+wilcoxon_filter">wilcoxon_filter()</a></code>, <code><a href="#topic+anova_filter">anova_filter()</a></code>,
<code><a href="#topic+correl_filter">correl_filter()</a></code>, <code><a href="#topic+lm_filter">lm_filter()</a></code> and <code><a href="#topic+boot_filter">boot_filter()</a></code>
</p>

<hr>
<h2 id='boruta_filter'>Boruta filter</h2><span id='topic+boruta_filter'></span>

<h3>Description</h3>

<p>Filter using Boruta algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boruta_filter(
  y,
  x,
  select = c("Confirmed", "Tentative"),
  type = c("index", "names", "full"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boruta_filter_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="boruta_filter_+3A_x">x</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="boruta_filter_+3A_select">select</code></td>
<td>
<p>Which type of features to retain. Options include &quot;Confirmed&quot;
and/or &quot;Tentative&quot;.</p>
</td></tr>
<tr><td><code id="boruta_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices,
&quot;names&quot; returns predictor names, &quot;full&quot; returns a named vector of variable
importance.</p>
</td></tr>
<tr><td><code id="boruta_filter_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="Boruta.html#topic+Boruta">Boruta::Boruta()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Boruta works differently from other filters in that it does not rank
variables by variable importance, but tries to determine relevant features
and divides features into Rejected, Tentative or Confirmed.
</p>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (type = &quot;index&quot;) or
character vector of names (type = &quot;names&quot;) of filtered parameters. If
<code>type</code> is <code>"full"</code> full output from <code>Boruta</code> is returned.
</p>

<hr>
<h2 id='boxplot_expression'>Boxplot expression levels of model predictors</h2><span id='topic+boxplot_expression'></span>

<h3>Description</h3>

<p>Boxplots to show range of model predictors to identify exceptional predictors
with excessively low or high values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boxplot_expression(x, scheme = NULL, palette = "Dark 3", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boxplot_expression_+3A_x">x</code></td>
<td>
<p>a &quot;nestedcv&quot; object</p>
</td></tr>
<tr><td><code id="boxplot_expression_+3A_scheme">scheme</code></td>
<td>
<p>colour scheme</p>
</td></tr>
<tr><td><code id="boxplot_expression_+3A_palette">palette</code></td>
<td>
<p>palette name (one of <code>hcl.pals()</code>) which is passed to
<a href="grDevices.html#topic+hcl.colors">hcl.colors</a></p>
</td></tr>
<tr><td><code id="boxplot_expression_+3A_...">...</code></td>
<td>
<p>other arguments passed to <a href="graphics.html#topic+boxplot">boxplot</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value
</p>


<h3>Author(s)</h3>

<p>Myles Lewis
</p>


<h3>See Also</h3>

<p><a href="#topic+nestcv.glmnet">nestcv.glmnet</a>
</p>

<hr>
<h2 id='class_balance'>Check class balance in training folds</h2><span id='topic+class_balance'></span><span id='topic+class_balance.default'></span><span id='topic+class_balance.nestcv.train'></span>

<h3>Description</h3>

<p>Check class balance in training folds
</p>


<h3>Usage</h3>

<pre><code class='language-R'>class_balance(object)

## Default S3 method:
class_balance(object)

## S3 method for class 'nestcv.train'
class_balance(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="class_balance_+3A_object">object</code></td>
<td>
<p>Object of class <code>nestedcv.glmnet</code>, <code>nestcv.train</code> or <code>outercv</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly a table of the response classes in the training folds
</p>

<hr>
<h2 id='coef.cva.glmnet'>Extract coefficients from a cva.glmnet object</h2><span id='topic+coef.cva.glmnet'></span>

<h3>Description</h3>

<p>Extracts model coefficients from a fitted <code><a href="#topic+cva.glmnet">cva.glmnet()</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cva.glmnet'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.cva.glmnet_+3A_object">object</code></td>
<td>
<p>Fitted <code>cva.glmnet</code> object.</p>
</td></tr>
<tr><td><code id="coef.cva.glmnet_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>coef.glmnet()</code> e.g. <code>s</code> the value of
lambda at which coefficients are required.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Sparse matrix containing coefficients from a <code>cv.glmnet</code> model
</p>

<hr>
<h2 id='coef.nestcv.glmnet'>Extract coefficients from nestcv.glmnet object</h2><span id='topic+coef.nestcv.glmnet'></span>

<h3>Description</h3>

<p>Extracts coefficients from the final fit of a <code>"nestcv.glmnet"</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nestcv.glmnet'
coef(object, s = object$final_param["lambda"], ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.nestcv.glmnet_+3A_object">object</code></td>
<td>
<p>Object of class <code>"nestcv.glmnet"</code></p>
</td></tr>
<tr><td><code id="coef.nestcv.glmnet_+3A_s">s</code></td>
<td>
<p>Value of penalty parameter lambda. Default is the mean of lambda
values selected across each outer fold.</p>
</td></tr>
<tr><td><code id="coef.nestcv.glmnet_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <a href="glmnet.html#topic+coef.glmnet">coef.glmnet</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector or list of coefficients ordered with the intercept first,
followed by highest absolute value to lowest.
</p>

<hr>
<h2 id='collinear'>Filter to reduce collinearity in predictors</h2><span id='topic+collinear'></span>

<h3>Description</h3>

<p>This function identifies predictors with r^2 above a given cut-off and
produces an index of predictors to be removed. The function takes a matrix or
data.frame of predictors, and the columns need to be ordered in terms of
importance - first column of any pair that are correlated is retained and
subsequent columns which correlate above the cut-off are flagged for removal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>collinear(x, rsq_cutoff = 0.9, rsq_method = "pearson", verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collinear_+3A_x">x</code></td>
<td>
<p>A matrix or data.frame of values. The order of columns is used to
determine which columns to retain, so the columns in <code>x</code> should be sorted
with the most important columns first.</p>
</td></tr>
<tr><td><code id="collinear_+3A_rsq_cutoff">rsq_cutoff</code></td>
<td>
<p>Value of cut-off for r-squared</p>
</td></tr>
<tr><td><code id="collinear_+3A_rsq_method">rsq_method</code></td>
<td>
<p>character string indicating which correlation coefficient
is to be computed. One of &quot;pearson&quot; (default), &quot;kendall&quot;, or &quot;spearman&quot;.
See <code><a href="stats.html#topic+cor">cor()</a></code>.</p>
</td></tr>
<tr><td><code id="collinear_+3A_verbose">verbose</code></td>
<td>
<p>Boolean whether to print details</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer vector of the indices of columns in <code>x</code> to remove due to
collinearity
</p>

<hr>
<h2 id='combo_filter'>Combo filter</h2><span id='topic+combo_filter'></span>

<h3>Description</h3>

<p>Filter combining univariate (t-test or anova) filtering and reliefF filtering
in equal measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combo_filter(y, x, nfilter, type = c("index", "names", "full"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combo_filter_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="combo_filter_+3A_x">x</code></td>
<td>
<p>Matrix or dataframe of predictors</p>
</td></tr>
<tr><td><code id="combo_filter_+3A_nfilter">nfilter</code></td>
<td>
<p>Number of predictors to return, using 1/2 from <code>ttest_filter</code>
or <code>anova_filter</code> and 1/2 from <code>relieff_filter</code>. Since <code>unique</code> is applied,
the final number returned may be less than <code>nfilter</code>.</p>
</td></tr>
<tr><td><code id="combo_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices,
&quot;names&quot; returns predictor names, &quot;full&quot; returns full output.</p>
</td></tr>
<tr><td><code id="combo_filter_+3A_...">...</code></td>
<td>
<p>Optional arguments passed via <a href="#topic+relieff_filter">relieff_filter</a> to
<a href="CORElearn.html#topic+attrEval">CORElearn::attrEval</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (type = &quot;index&quot;) or
character vector of names (type = &quot;names&quot;) of filtered parameters. If <code>type</code>
is <code>"full"</code> a list containing full outputs from either <a href="#topic+ttest_filter">ttest_filter</a> or
<a href="#topic+anova_filter">anova_filter</a> and <a href="#topic+relieff_filter">relieff_filter</a> is returned.
</p>

<hr>
<h2 id='correls2'>Correlation between a vector and a matrix</h2><span id='topic+correls2'></span>

<h3>Description</h3>

<p>Fast Pearson/Spearman correlation where <code>y</code> is vector, <code>x</code> is matrix, adapted
from <a href="stats.html#topic+cor.test">stats::cor.test</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correls2(y, x, method = "pearson", use = "complete.obs")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correls2_+3A_y">y</code></td>
<td>
<p>Numerical vector</p>
</td></tr>
<tr><td><code id="correls2_+3A_x">x</code></td>
<td>
<p>Matrix</p>
</td></tr>
<tr><td><code id="correls2_+3A_method">method</code></td>
<td>
<p>Type of correlation, either &quot;pearson&quot; or &quot;spearman&quot;.</p>
</td></tr>
<tr><td><code id="correls2_+3A_use">use</code></td>
<td>
<p>Optional character string giving a method for computing
covariances in the presence of missing values. See <a href="stats.html#topic+cor">cor</a></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For speed, p-values for Spearman's test are computed by
asymptotic t approximation, equivalent to <a href="stats.html#topic+cor.test">cor.test</a> with <code>exact = FALSE</code>.
</p>


<h3>Value</h3>

<p>Matrix with columns containing the correlation statistic, either
Pearson r or Spearman rho, and p-values for each column of <code>x</code> correlated
against vector <code>y</code>
</p>

<hr>
<h2 id='cv_coef'>Coefficients from outer CV glmnet models</h2><span id='topic+cv_coef'></span>

<h3>Description</h3>

<p>Extracts coefficients from outer CV glmnet models from a <code>nestcv.glmnet</code>
fitted object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_coef(x, level = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_coef_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code> fitted object</p>
</td></tr>
<tr><td><code id="cv_coef_+3A_level">level</code></td>
<td>
<p>For multinomial models only, either an integer specifying which
level of outcome is being examined, or the level can be specified as a
character value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix of coefficients from outer CV glmnet models plus the final
glmnet model. Coefficients for variables which are not present in a
particular outer CV fold model are set to 0.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv_varImp">cv_varImp()</a></code>
</p>

<hr>
<h2 id='cv_varImp'>Extract variable importance from outer CV caret models</h2><span id='topic+cv_varImp'></span>

<h3>Description</h3>

<p>Extracts variable importance or coefficients from outer CV glmnet models from
a <code>nestcv.train</code> fitted object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_varImp(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_varImp_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.train</code> fitted object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that <code><a href="caret.html#topic+varImp">caret::varImp()</a></code> may require the model package to be fully loaded
in order to function. During the fitting process <code>caret</code> often only loads the
package by namespace.
</p>


<h3>Value</h3>

<p>matrix of variable importance from outer CV fold caret models as well
as the final model. Variable importance for variables which are not present
in a particular outer CV fold model is set to 0.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv_coef">cv_coef()</a></code>
</p>

<hr>
<h2 id='cva.glmnet'>Cross-validation of alpha for glmnet</h2><span id='topic+cva.glmnet'></span>

<h3>Description</h3>

<p>Performs k-fold cross-validation for glmnet, including alpha mixing parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cva.glmnet(x, y, nfolds = 10, alphaSet = seq(0.1, 1, 0.1), foldid = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cva.glmnet_+3A_x">x</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="cva.glmnet_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="cva.glmnet_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds (default 10)</p>
</td></tr>
<tr><td><code id="cva.glmnet_+3A_alphaset">alphaSet</code></td>
<td>
<p>Sequence of alpha values to cross-validate</p>
</td></tr>
<tr><td><code id="cva.glmnet_+3A_foldid">foldid</code></td>
<td>
<p>Optional vector of values between 1 and <code>nfolds</code> identifying
what fold each observation is in.</p>
</td></tr>
<tr><td><code id="cva.glmnet_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of S3 class &quot;cva.glmnet&quot;, which is a list of the cv.glmnet
objects for each value of alpha and <code>alphaSet</code>.
</p>
<table>
<tr><td><code>fits</code></td>
<td>
<p>List of fitted <a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a> objects</p>
</td></tr>
<tr><td><code>alphaSet</code></td>
<td>
<p>Sequence of alpha values used</p>
</td></tr>
<tr><td><code>alpha_cvm</code></td>
<td>
<p>The mean cross-validated error - a vector of length
<code>length(alphaSet)</code>.</p>
</td></tr>
<tr><td><code>best_alpha</code></td>
<td>
<p>Value of alpha giving lowest <code>alpha_cvm</code>.</p>
</td></tr>
<tr><td><code>which_alpha</code></td>
<td>
<p>Index of <code>alphaSet</code> with lowest <code>alpha_cvm</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Myles Lewis
</p>


<h3>See Also</h3>

<p><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a>, <a href="glmnet.html#topic+glmnet">glmnet</a>
</p>

<hr>
<h2 id='glmnet_coefs'>glmnet coefficients</h2><span id='topic+glmnet_coefs'></span>

<h3>Description</h3>

<p>Convenience function for retrieving coefficients from a <a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a> model at
a specified lambda. Sparsity is removed and non-intercept coefficients are
ranked by absolute value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glmnet_coefs(fit, s, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glmnet_coefs_+3A_fit">fit</code></td>
<td>
<p>A <a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a> fitted model object.</p>
</td></tr>
<tr><td><code id="glmnet_coefs_+3A_s">s</code></td>
<td>
<p>Value of lambda. See <a href="glmnet.html#topic+coef.glmnet">coef.glmnet</a> and <a href="glmnet.html#topic+predict.cv.glmnet">predict.cv.glmnet</a></p>
</td></tr>
<tr><td><code id="glmnet_coefs_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <a href="glmnet.html#topic+coef.glmnet">coef.glmnet</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector or list of coefficients ordered with the intercept first,
followed by highest absolute value to lowest.
</p>

<hr>
<h2 id='glmnet_filter'>glmnet filter</h2><span id='topic+glmnet_filter'></span>

<h3>Description</h3>

<p>Filter using sparsity of elastic net regression using glmnet to calculate
variable importance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glmnet_filter(
  y,
  x,
  family = NULL,
  force_vars = NULL,
  nfilter = NULL,
  method = c("mean", "nonzero"),
  type = c("index", "names", "full"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glmnet_filter_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="glmnet_filter_+3A_x">x</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="glmnet_filter_+3A_family">family</code></td>
<td>
<p>Either a character string representing one of the built-in
families, or else a <code>glm()</code> family object. See <code><a href="glmnet.html#topic+glmnet">glmnet()</a></code>. If not
specified, the function tries to set this automatically to one of either
&quot;gaussian&quot;, &quot;binomial&quot; or &quot;multinomial&quot;.</p>
</td></tr>
<tr><td><code id="glmnet_filter_+3A_force_vars">force_vars</code></td>
<td>
<p>Vector of column names <code>x</code> which have no shrinkage and are
always included in the model.</p>
</td></tr>
<tr><td><code id="glmnet_filter_+3A_nfilter">nfilter</code></td>
<td>
<p>Number of predictors to return</p>
</td></tr>
<tr><td><code id="glmnet_filter_+3A_method">method</code></td>
<td>
<p>String indicating method of determining variable importance.
&quot;mean&quot; (the default) uses the mean absolute coefficients across the range
of lambdas; &quot;nonzero&quot; counts the number of times variables are retained in
the model across all values of lambda.</p>
</td></tr>
<tr><td><code id="glmnet_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices, &quot;names&quot;
returns predictor names, &quot;full&quot; returns full output.</p>
</td></tr>
<tr><td><code id="glmnet_filter_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <a href="glmnet.html#topic+glmnet">glmnet</a></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The glmnet elastic net mixing parameter alpha can be varied to
include a larger number of predictors. Default alpha = 1 is pure LASSO,
resulting in greatest sparsity, while alpha = 0 is pure ridge regression,
retaining all predictors in the regression model. Note, the <code>family</code>
argument is commonly needed, see <a href="glmnet.html#topic+glmnet">glmnet</a>.
</p>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (type = &quot;index&quot;) or
character vector of names (type = &quot;names&quot;) of filtered parameters. If
<code>type</code> is <code>"full"</code> a named vector of variable importance is returned.
</p>


<h3>See Also</h3>

<p><a href="glmnet.html#topic+glmnet">glmnet</a>
</p>

<hr>
<h2 id='innercv_preds'>Inner CV predictions</h2><span id='topic+innercv_preds'></span><span id='topic+innercv_preds.nestcv.glmnet'></span><span id='topic+innercv_preds.nestcv.train'></span>

<h3>Description</h3>

<p>Obtain predictions on held-out test inner CV folds
</p>


<h3>Usage</h3>

<pre><code class='language-R'>innercv_preds(x)

## S3 method for class 'nestcv.glmnet'
innercv_preds(x)

## S3 method for class 'nestcv.train'
innercv_preds(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="innercv_preds_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code> or <code>nestcv.train</code> fitted object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Dataframe with columns <code>testy</code> and <code>predy</code>, and for binomial and
multinomial models additional columns containing probabilities or log
likelihood values.
</p>

<hr>
<h2 id='innercv_roc'>Build ROC curve from left-out folds from inner CV</h2><span id='topic+innercv_roc'></span>

<h3>Description</h3>

<p>Build ROC (receiver operating characteristic) curve from left-out folds
from inner CV. Object can be plotted using <code>plot()</code> or passed to functions
<code><a href="glmnet.html#topic+auc">auc()</a></code> etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>innercv_roc(x, direction = "&lt;", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="innercv_roc_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code> or <code>nestcv.train</code> fitted object</p>
</td></tr>
<tr><td><code id="innercv_roc_+3A_direction">direction</code></td>
<td>
<p>Set ROC directionality <a href="pROC.html#topic+roc">pROC::roc</a></p>
</td></tr>
<tr><td><code id="innercv_roc_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <a href="pROC.html#topic+roc">pROC::roc</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>"roc"</code> object, see <a href="pROC.html#topic+roc">pROC::roc</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Example binary classification problem with P &gt;&gt; n
x &lt;- matrix(rnorm(150 * 2e+04), 150, 2e+04)  # predictors
y &lt;- factor(rbinom(150, 1, 0.5))  # binary response

## Partition data into 2/3 training set, 1/3 test set
trainSet &lt;- caret::createDataPartition(y, p = 0.66, list = FALSE)

## t-test filter using whole dataset
filt &lt;- ttest_filter(y, x, nfilter = 100)
filx &lt;- x[, filt]

## Train glmnet on training set only using filtered predictor matrix
library(glmnet)
fit &lt;- cv.glmnet(filx[trainSet, ], y[trainSet], family = "binomial")
plot(fit)

## Predict response on test partition
predy &lt;- predict(fit, newx = filx[-trainSet, ], s = "lambda.min", type = "class")
predy &lt;- as.vector(predy)
predyp &lt;- predict(fit, newx = filx[-trainSet, ], s = "lambda.min", type = "response")
predyp &lt;- as.vector(predyp)
output &lt;- data.frame(testy = y[-trainSet], predy = predy, predyp = predyp)

## Results on test partition
## shows bias since univariate filtering was applied to whole dataset
predSummary(output)

## Nested CV
fit2 &lt;- nestcv.glmnet(y, x, family = "binomial", alphaSet = 1,
                      filterFUN = ttest_filter,
                      filter_options = list(nfilter = 100),
                      n_outer_folds = 3)
summary(fit2)

## ROC plots
library(pROC)
testroc &lt;- roc(output$testy, output$predyp, direction = "&lt;")
inroc &lt;- innercv_roc(fit2)
plot(fit2$roc)
lines(inroc, col = 'blue')
lines(testroc, col = 'red')
legend('bottomright', legend = c("Nested CV", "Left-out inner CV folds", 
                                 "Test partition, non-nested filtering"), 
       col = c("black", "blue", "red"), lty = 1, lwd = 2, bty = "n")

</code></pre>

<hr>
<h2 id='innercv_summary'>Summarise performance on inner CV test folds</h2><span id='topic+innercv_summary'></span>

<h3>Description</h3>

<p>Calculates performance metrics on inner CV held-out test folds: confusion
matrix, accuracy and balanced accuracy for classification; ROC AUC for binary
classification; RMSE, R^2 and mean absolute error (MAE) for regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>innercv_summary(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="innercv_summary_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code> or <code>nestcv.train</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns performance metrics from outer training folds, see
<a href="#topic+predSummary">predSummary</a>.
</p>


<h3>See Also</h3>

<p><a href="#topic+predSummary">predSummary</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- iris[, 1:4]
y &lt;- iris[, 5]

fit &lt;- nestcv.glmnet(y, x,
                     family = "multinomial",
                     alpha = 1,
                     n_outer_folds = 3)
summary(fit)
innercv_summary(fit)

</code></pre>

<hr>
<h2 id='lines.prc'>Add precision-recall curve to a plot</h2><span id='topic+lines.prc'></span>

<h3>Description</h3>

<p>Adds a precision-recall curve to a base graphics plot. It accepts an S3
object of class 'prc', see <code><a href="#topic+prc">prc()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'prc'
lines(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lines.prc_+3A_x">x</code></td>
<td>
<p>An object of class 'prc'</p>
</td></tr>
<tr><td><code id="lines.prc_+3A_...">...</code></td>
<td>
<p>Optional graphical arguments passed to <code><a href="graphics.html#topic+lines">lines()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prc">prc()</a></code> <code><a href="#topic+plot.prc">plot.prc()</a></code>
</p>

<hr>
<h2 id='lm_filter'>Linear model filter</h2><span id='topic+lm_filter'></span>

<h3>Description</h3>

<p>Linear models are fitted on each predictor, with inclusion of variable names
listed in <code>force_vars</code> in the model. Predictors are ranked by Akaike
information criteria (AIC) value, or can be filtered by the p-value on the
estimate of the coefficient for that predictor in its model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm_filter(
  y,
  x,
  force_vars = NULL,
  nfilter = NULL,
  p_cutoff = 0.05,
  rsq_cutoff = NULL,
  rsq_method = "pearson",
  type = c("index", "names", "full"),
  keep_factors = TRUE,
  method = 0L,
  mc.cores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lm_filter_+3A_y">y</code></td>
<td>
<p>Numeric or integer response vector</p>
</td></tr>
<tr><td><code id="lm_filter_+3A_x">x</code></td>
<td>
<p>Matrix of predictors. If <code>x</code> is a data.frame it will be turned into
a matrix. But note that factors will be reduced to numeric values, but a
full design matrix is not generated, so if factors have 3 or more levels,
it is recommended to convert <code>x</code> into a design (model) matrix first.</p>
</td></tr>
<tr><td><code id="lm_filter_+3A_force_vars">force_vars</code></td>
<td>
<p>Vector of column names <code>x</code> which are incorporated into the
linear model.</p>
</td></tr>
<tr><td><code id="lm_filter_+3A_nfilter">nfilter</code></td>
<td>
<p>Number of predictors to return. If <code>NULL</code> all predictors with
p-values &lt; <code>p_cutoff</code> are returned.</p>
</td></tr>
<tr><td><code id="lm_filter_+3A_p_cutoff">p_cutoff</code></td>
<td>
<p>p-value cut-off. P-values are calculated by t-statistic on
the estimated coefficient for the predictor being tested.</p>
</td></tr>
<tr><td><code id="lm_filter_+3A_rsq_cutoff">rsq_cutoff</code></td>
<td>
<p>r^2 cutoff for removing predictors due to collinearity.
Default <code>NULL</code> means no collinearity filtering. Predictors are ranked based
on AIC from a linear model. If 2 or more predictors are collinear, the
first ranked predictor by AIC is retained, while the other collinear
predictors are removed. See <code><a href="#topic+collinear">collinear()</a></code>.</p>
</td></tr>
<tr><td><code id="lm_filter_+3A_rsq_method">rsq_method</code></td>
<td>
<p>character string indicating which correlation coefficient
is to be computed. One of &quot;pearson&quot; (default), &quot;kendall&quot;, or &quot;spearman&quot;.
See <code><a href="#topic+collinear">collinear()</a></code>.</p>
</td></tr>
<tr><td><code id="lm_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices, &quot;names&quot;
returns predictor names, &quot;full&quot; returns a matrix of p values.</p>
</td></tr>
<tr><td><code id="lm_filter_+3A_keep_factors">keep_factors</code></td>
<td>
<p>Logical affecting factors with 3 or more levels.
Dataframes are coerced to a matrix using <a href="base.html#topic+data.matrix">data.matrix</a>. Binary
factors are converted to numeric values 0/1 and analysed as such. If
<code>keep_factors</code> is <code>TRUE</code> (the default), factors with 3 or more levels are
not filtered and are retained. If <code>keep_factors</code> is <code>FALSE</code>, they are
removed.</p>
</td></tr>
<tr><td><code id="lm_filter_+3A_method">method</code></td>
<td>
<p>Integer determining linear model method. See
<code><a href="RcppEigen.html#topic+fastLm">RcppEigen::fastLmPure()</a></code></p>
</td></tr>
<tr><td><code id="lm_filter_+3A_mc.cores">mc.cores</code></td>
<td>
<p>Number of cores for parallelisation using
<code><a href="parallel.html#topic+mclapply">parallel::mclapply()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This filter is based on the model <code>y ~ xvar + force_vars</code> where <code>y</code> is the
response vector, <code>xvar</code> are variables in columns taken sequentially from <code>x</code>
and <code>force_vars</code> are optional covariates extracted from <code>x</code>. It uses
<code><a href="RcppEigen.html#topic+fastLm">RcppEigen::fastLmPure()</a></code> with <code>method = 0</code> as default since it is
rank-revealing. <code>method = 3</code> is significantly faster but can give errors in
estimation of p-value with variables of zero variance. The algorithm attempts
to detect these and set their stats to <code>NA</code>. <code>NA</code> in <code>x</code> are not tolerated.
</p>
<p>Parallelisation is available via <code><a href="parallel.html#topic+mclapply">mclapply()</a></code>. This is provided mainly for
the use case of the filter being used as standalone. Nesting parallelisation
inside of parallelised <code><a href="#topic+nestcv.glmnet">nestcv.glmnet()</a></code> or <code><a href="#topic+nestcv.train">nestcv.train()</a></code> loops is not
recommended.
</p>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (<code>type = "index"</code>)
or character vector of names (<code>type = "names"</code>) of filtered parameters in
order of linear model AIC. Any variables in <code>force_vars</code> which are
incorporated into all models are listed first. If <code>type = "full"</code> a matrix
of AIC value, sigma (residual standard error, see <a href="stats.html#topic+summary.lm">summary.lm</a>),
coefficient, t-statistic and p-value for each tested predictor is returned.
</p>

<hr>
<h2 id='metrics'>Model performance metrics</h2><span id='topic+metrics'></span>

<h3>Description</h3>

<p>Returns model metrics from nestedcv models. Extended metrics including
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metrics(object, extra = FALSE, innerCV = FALSE, positive = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metrics_+3A_object">object</code></td>
<td>
<p>A 'nestcv.glmnet', 'nestcv.train', 'nestcv.SuperLearner' or
'outercv' object.</p>
</td></tr>
<tr><td><code id="metrics_+3A_extra">extra</code></td>
<td>
<p>Logical whether additional performance metrics are gathered for
binary classification models: area under precision recall curve (PR.AUC),
Cohen's kappa, F1 score, Matthew's correlation coefficient (MCC).</p>
</td></tr>
<tr><td><code id="metrics_+3A_innercv">innerCV</code></td>
<td>
<p>Whether to calculate metrics for inner CV folds. Only
available for 'nestcv.glmnet' and 'nestcv.train' objects.</p>
</td></tr>
<tr><td><code id="metrics_+3A_positive">positive</code></td>
<td>
<p>For binary classification, either an integer 1 or 2 for the
level of response factor considered to be 'positive' or 'relevant', or a
character value for that factor. This affects the F1 score. See
<code><a href="caret.html#topic+confusionMatrix">caret::confusionMatrix()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Area under precision recall curve is estimated by trapezoidal estimation
using <code>MLmetrics::PRAUC()</code>.
</p>


<h3>Value</h3>

<p>A named numeric vector of performance metrics.
</p>

<hr>
<h2 id='model.hsstan'>hsstan model for cross-validation</h2><span id='topic+model.hsstan'></span>

<h3>Description</h3>

<p>This function applies a cross-validation (CV) procedure for training Bayesian
models with hierarchical shrinkage priors using the <code>hsstan</code> package. The
function allows the option of embedded filtering of predictors for feature
selection within the CV loop. Within each training fold, an optional
filtering of predictors is performed, followed by fitting of an <code>hsstsan</code>
model. Predictions on the testing folds are brought back together and error
estimation/ accuracy determined. The default is 10-fold CV. The function is
implemented within the <code>nestedcv</code> package. The <code>hsstan</code> models do not require
tuning of meta-parameters and therefore only a single CV procedure is needed
to evaluate performance. This is implemented using the <code>outer</code> CV procedure
in the <code>nestedcv</code> package. Supports binary outcome (logistic regression) or
continuous outcome. Multinomial models are currently not supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model.hsstan(y, x, unpenalized = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model.hsstan_+3A_y">y</code></td>
<td>
<p>Response vector. For classification this should be a factor.</p>
</td></tr>
<tr><td><code id="model.hsstan_+3A_x">x</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="model.hsstan_+3A_unpenalized">unpenalized</code></td>
<td>
<p>Vector of column names <code>x</code> which are always retained into
the model (i.e. not penalized). Default <code>NULL</code> means the parameters for all
predictors will be drawn from a hierarchical prior distribution, i.e. will
be penalized. Note: if filtering of predictors is specified, then the
vector of <code>unpenalized</code> predictors should also be passed to the filter
function using the <code>filter_options$force_vars</code> argument. Filters currently
implementing this option are the <code>partial_ttest_filter</code> for binary outcomes
and the <code>lm_filter</code> for continuous outcomes.</p>
</td></tr>
<tr><td><code id="model.hsstan_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to <code>hsstan</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Caution should be used when setting the number of cores available for
parallelisation. The default setting in <code>hsstan</code> is to use 4 cores to
parallelise the Markov chains of the Bayesian inference procedure. This can
be switched off either by adding argument <code>cores = 1</code> (passed on to <code>rstan</code>)
or setting <code>options(mc.cores = 1)</code>.
</p>
<p>Argument <code>cv.cores</code> in <code>outercv()</code> controls parallelisation over the outer CV
folds. On unix/mac setting <code>cv.cores</code> to &gt;1 will induce nested
parallelisation which will generate an error, unless parallelisation of the
chains is disabled using <code>cores = 1</code> or setting <code>options(mc.cores = 1)</code>.
</p>
<p>Nested parallelisation is feasible if <code>cv.cores</code> is &gt;1 and
<code>multicore_fork = FALSE</code> is set as this uses cluster based parallelisation
instead. Beware that large numbers of processes will be spawned. If we are
performing 10-fold cross-validation with 4 chains and set <code>cv.cores = 10</code>
then 40 processes will be invoked simultaneously.
</p>


<h3>Value</h3>

<p>An object of class <code>hsstan</code>
</p>


<h3>Author(s)</h3>

<p>Athina Spiliopoulou
</p>


<h3>See Also</h3>

<p><code><a href="#topic+outercv">outercv()</a></code> <code><a href="hsstan.html#topic+hsstan">hsstan::hsstan()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Cross-validation is used to apply univariate filtering of predictors.
# only one CV split is needed (outercv) as the Bayesian model does not
# require learning of meta-parameters.

# control number of cores used for parallelisation over chains
oldopt &lt;- options(mc.cores = 2)

# load iris dataset and simulate a continuous outcome
data(iris)
dt &lt;- iris[, 1:4]
colnames(dt) &lt;- c("marker1", "marker2", "marker3", "marker4")
dt &lt;- as.data.frame(apply(dt, 2, scale))
dt$outcome.cont &lt;- -3 + 0.5 * dt$marker1 + 2 * dt$marker2 + rnorm(nrow(dt), 0, 2)

library(hsstan)
# unpenalised covariates: always retain in the prediction model
uvars &lt;- "marker1"
# penalised covariates: coefficients are drawn from hierarchical shrinkage
# prior
pvars &lt;- c("marker2", "marker3", "marker4") # penalised covariates
# run cross-validation with univariate filter and hsstan
# dummy sampling for fast execution of example
# recommend 4 chains, warmup 1000, iter 2000 in practice
res.cv.hsstan &lt;- outercv(y = dt$outcome.cont, x = dt[, c(uvars, pvars)],
                         model = "model.hsstan",
                         filterFUN = lm_filter,
                         filter_options = list(force_vars = uvars,
                                               nfilter = 2,
                                               p_cutoff = NULL,
                                               rsq_cutoff = 0.9),
                         n_outer_folds = 3,
                         chains = 2,
                         cv.cores = 1,
                         unpenalized = uvars, warmup = 100, iter = 200)
# view prediction performance based on testing folds
res.cv.hsstan$summary
# view coefficients for the final model
res.cv.hsstan$final_fit
# view covariates selected by the univariate filter
res.cv.hsstan$final_vars

# use hsstan package to examine the Bayesian model
sampler.stats(res.cv.hsstan$final_fit)
print(projsel(res.cv.hsstan$final_fit), digits = 4)  # adding marker2
options(oldopt)  # reset configuation

# Here adding `marker2` improves the model fit: substantial decrease of
# KL-divergence from the full model to the submodel. Adding `marker3` does
# not improve the model fit: no decrease of KL-divergence from the full model
# to the submodel.

</code></pre>

<hr>
<h2 id='nestcv.glmnet'>Nested cross-validation with glmnet</h2><span id='topic+nestcv.glmnet'></span>

<h3>Description</h3>

<p>This function enables nested cross-validation (CV) with glmnet including
tuning of elastic net alpha parameter. The function also allows the option of
embedded filtering of predictors for feature selection nested within the
outer loop of CV. Predictions on the outer test folds are brought back
together and error estimation/ accuracy determined. The default is 10x10
nested CV.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nestcv.glmnet(
  y,
  x,
  family = c("gaussian", "binomial", "poisson", "multinomial", "cox", "mgaussian"),
  filterFUN = NULL,
  filter_options = NULL,
  balance = NULL,
  balance_options = NULL,
  modifyX = NULL,
  modifyX_useY = FALSE,
  modifyX_options = NULL,
  outer_method = c("cv", "LOOCV"),
  n_outer_folds = 10,
  n_inner_folds = 10,
  outer_folds = NULL,
  pass_outer_folds = FALSE,
  alphaSet = seq(0.1, 1, 0.1),
  min_1se = 0,
  keep = TRUE,
  outer_train_predict = FALSE,
  weights = NULL,
  penalty.factor = rep(1, ncol(x)),
  cv.cores = 1,
  finalCV = TRUE,
  na.option = "omit",
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nestcv.glmnet_+3A_y">y</code></td>
<td>
<p>Response vector or matrix. Matrix is only used for
<code>family = 'mgaussian'</code> or <code>'cox'</code>.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_x">x</code></td>
<td>
<p>Matrix of predictors. Dataframes will be coerced to a matrix as
is necessary for glmnet.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_family">family</code></td>
<td>
<p>Either a character string representing one of the built-in
families, or else a <code>glm()</code> family object. Passed to <a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a> and
<a href="glmnet.html#topic+glmnet">glmnet</a></p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_filterfun">filterFUN</code></td>
<td>
<p>Filter function, e.g. <a href="#topic+ttest_filter">ttest_filter</a> or <a href="#topic+relieff_filter">relieff_filter</a>.
Any function can be provided and is passed <code>y</code> and <code>x</code>. Must return a
character vector with names of filtered predictors.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_filter_options">filter_options</code></td>
<td>
<p>List of additional arguments passed to the filter
function specified by <code>filterFUN</code>.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_balance">balance</code></td>
<td>
<p>Specifies method for dealing with imbalanced class data.
Current options are <code>"randomsample"</code> or <code>"smote"</code>. See <code><a href="#topic+randomsample">randomsample()</a></code> and
<code><a href="#topic+smote">smote()</a></code></p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_balance_options">balance_options</code></td>
<td>
<p>List of additional arguments passed to the balancing
function</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_modifyx">modifyX</code></td>
<td>
<p>Character string specifying the name of a function to modify
<code>x</code>. This can be an imputation function for replacing missing values, or a
more complex function which alters or even adds columns to <code>x</code>. The
required return value of this function depends on the <code>modifyX_useY</code>
setting.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_modifyx_usey">modifyX_useY</code></td>
<td>
<p>Logical value whether the <code>x</code> modifying function makes
use of response training data from <code>y</code>. If <code>FALSE</code> then the <code>modifyX</code>
function simply needs to return a modified <code>x</code> object, which will be
coerced to a matrix as required by <code>glmnet</code>. If <code>TRUE</code> then the <code>modifyX</code>
function must return a model type object on which <code>predict()</code> can be
called, so that train and test partitions of <code>x</code> can be modified
independently.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_modifyx_options">modifyX_options</code></td>
<td>
<p>List of additional arguments passed to the <code>x</code>
modifying function</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_outer_method">outer_method</code></td>
<td>
<p>String of either <code>"cv"</code> or <code>"LOOCV"</code> specifying whether
to do k-fold CV or leave one out CV (LOOCV) for the outer folds</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_n_outer_folds">n_outer_folds</code></td>
<td>
<p>Number of outer CV folds</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_n_inner_folds">n_inner_folds</code></td>
<td>
<p>Number of inner CV folds</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_outer_folds">outer_folds</code></td>
<td>
<p>Optional list containing indices of test folds for outer
CV. If supplied, <code>n_outer_folds</code> is ignored.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_pass_outer_folds">pass_outer_folds</code></td>
<td>
<p>Logical indicating whether the same outer folds are
used for fitting of the final model when final CV is applied. Note this can
only be applied when <code>n_outer_folds</code> and <code>n_inner_folds</code> are the same and
no balancing is applied.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_alphaset">alphaSet</code></td>
<td>
<p>Vector of alphas to be tuned</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_min_1se">min_1se</code></td>
<td>
<p>Value from 0 to 1 specifying choice of optimal lambda from
0=lambda.min to 1=lambda.1se</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_keep">keep</code></td>
<td>
<p>Logical indicating whether inner CV predictions are retained for
calculating left-out inner CV fold accuracy etc. See argument <code>keep</code> in
<a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a>.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_outer_train_predict">outer_train_predict</code></td>
<td>
<p>Logical whether to save predictions on outer
training folds to calculate performance on outer training folds.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_weights">weights</code></td>
<td>
<p>Weights applied to each sample. Note <code>weights</code> and <code>balance</code>
cannot be used at the same time. Weights are only applied in glmnet and not
in filters.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_penalty.factor">penalty.factor</code></td>
<td>
<p>Separate penalty factors can be applied to each
coefficient. Can be 0 for some variables, which implies no shrinkage, and
that variable is always included in the model. Default is 1 for all
variables. See <a href="glmnet.html#topic+glmnet">glmnet</a>. Note this works separately from filtering. For
some <code>nestedcv</code> filter functions you might need to set <code>force_vars</code> to
avoid filtering out features.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_cv.cores">cv.cores</code></td>
<td>
<p>Number of cores for parallel processing of the outer loops.
NOTE: this uses <code>parallel::mclapply</code> on unix/mac and <code>parallel::parLapply</code>
on windows.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_finalcv">finalCV</code></td>
<td>
<p>Logical whether to perform one last round of CV on the whole
dataset to determine the final model parameters. If set to <code>FALSE</code>, the
median of hyperparameters from outer CV folds are used for the final model.
Performance metrics are independent of this last step. If set to <code>NA</code>,
final model fitting is skipped altogether, which gives a useful speed boost
if performance metrics are all that is needed.</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_na.option">na.option</code></td>
<td>
<p>Character value specifying how <code>NA</code>s are dealt with.
<code>"omit"</code> (the default) is equivalent to <code>na.action = na.omit</code>. <code>"omitcol"</code>
removes cases if there are <code>NA</code> in 'y', but columns (predictors) containing
<code>NA</code> are removed from 'x' to preserve cases. Any other value means that
<code>NA</code> are ignored (a message is given).</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_verbose">verbose</code></td>
<td>
<p>Logical whether to print messages and show progress</p>
</td></tr>
<tr><td><code id="nestcv.glmnet_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to <a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></p>
</td></tr>
</table>


<h3>Details</h3>

<p>glmnet does not tolerate missing values, so <code>na.option = "omit"</code> is the
default.
</p>


<h3>Value</h3>

<p>An object with S3 class &quot;nestcv.glmnet&quot;
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the matched call</p>
</td></tr>
<tr><td><code>output</code></td>
<td>
<p>Predictions on the left-out outer folds</p>
</td></tr>
<tr><td><code>outer_result</code></td>
<td>
<p>List object of results from each outer fold containing
predictions on left-out outer folds, best lambda, best alpha, fitted glmnet
coefficients, list object of inner fitted cv.glmnet and number of filtered
predictors at each fold.</p>
</td></tr>
<tr><td><code>outer_method</code></td>
<td>
<p>the <code>outer_method</code> argument</p>
</td></tr>
<tr><td><code>n_inner_folds</code></td>
<td>
<p>number of inner folds</p>
</td></tr>
<tr><td><code>outer_folds</code></td>
<td>
<p>List of indices of outer test folds</p>
</td></tr>
<tr><td><code>dimx</code></td>
<td>
<p>dimensions of <code>x</code></p>
</td></tr>
<tr><td><code>xsub</code></td>
<td>
<p>subset of <code>x</code> containing all predictors used in both outer CV
folds and the final model</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>original response vector</p>
</td></tr>
<tr><td><code>yfinal</code></td>
<td>
<p>final response vector (post-balancing)</p>
</td></tr>
<tr><td><code>final_param</code></td>
<td>
<p>Final mean best lambda
and alpha from each fold</p>
</td></tr>
<tr><td><code>final_fit</code></td>
<td>
<p>Final fitted glmnet model</p>
</td></tr>
<tr><td><code>final_coef</code></td>
<td>
<p>Final model coefficients and mean expression. Variables
with coefficients shrunk to 0 are removed.</p>
</td></tr>
<tr><td><code>final_vars</code></td>
<td>
<p>Column names of filtered predictors entering final model.
This is useful for subsetting new data for predictions.</p>
</td></tr>
<tr><td><code>roc</code></td>
<td>
<p>ROC AUC for binary classification where available.</p>
</td></tr>
<tr><td><code>summary</code></td>
<td>
<p>Overall performance summary. Accuracy and balanced accuracy
for classification. ROC AUC for binary classification. RMSE for
regression.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Myles Lewis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Example binary classification problem with P &gt;&gt; n
x &lt;- matrix(rnorm(150 * 2e+04), 150, 2e+04)  # predictors
y &lt;- factor(rbinom(150, 1, 0.5))  # binary response

## Partition data into 2/3 training set, 1/3 test set
trainSet &lt;- caret::createDataPartition(y, p = 0.66, list = FALSE)

## t-test filter using whole dataset
filt &lt;- ttest_filter(y, x, nfilter = 100)
filx &lt;- x[, filt]

## Train glmnet on training set only using filtered predictor matrix
library(glmnet)
fit &lt;- cv.glmnet(filx[trainSet, ], y[trainSet], family = "binomial")
plot(fit)

## Predict response on test partition
predy &lt;- predict(fit, newx = filx[-trainSet, ], s = "lambda.min", type = "class")
predy &lt;- as.vector(predy)
predyp &lt;- predict(fit, newx = filx[-trainSet, ], s = "lambda.min", type = "response")
predyp &lt;- as.vector(predyp)
output &lt;- data.frame(testy = y[-trainSet], predy = predy, predyp = predyp)

## Results on test partition
## shows bias since univariate filtering was applied to whole dataset
predSummary(output)

## Nested CV
## n_outer_folds reduced to speed up example
fit2 &lt;- nestcv.glmnet(y, x, family = "binomial", alphaSet = 1,
                      n_outer_folds = 3,
                      filterFUN = ttest_filter,
                      filter_options = list(nfilter = 100),
                      cv.cores = 2)
summary(fit2)
plot_lambdas(fit2, showLegend = "bottomright")

## ROC plots
library(pROC)
testroc &lt;- roc(output$testy, output$predyp, direction = "&lt;")
inroc &lt;- innercv_roc(fit2)
plot(fit2$roc)
lines(inroc, col = 'blue')
lines(testroc, col = 'red')
legend('bottomright', legend = c("Nested CV", "Left-out inner CV folds", 
                                 "Test partition, non-nested filtering"), 
       col = c("black", "blue", "red"), lty = 1, lwd = 2, bty = "n")

</code></pre>

<hr>
<h2 id='nestcv.SuperLearner'>Outer cross-validation of SuperLearner model</h2><span id='topic+nestcv.SuperLearner'></span>

<h3>Description</h3>

<p>Provides a single loop of outer cross-validation to evaluate performance of
ensemble models from <code>SuperLearner</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nestcv.SuperLearner(
  y,
  x,
  filterFUN = NULL,
  filter_options = NULL,
  weights = NULL,
  balance = NULL,
  balance_options = NULL,
  modifyX = NULL,
  modifyX_useY = FALSE,
  modifyX_options = NULL,
  outer_method = c("cv", "LOOCV"),
  n_outer_folds = 10,
  outer_folds = NULL,
  cv.cores = 1,
  final = TRUE,
  na.option = "pass",
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nestcv.SuperLearner_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_x">x</code></td>
<td>
<p>Dataframe or matrix of predictors. Matrix will be coerced to
dataframe as this is the default for SuperLearner.</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_filterfun">filterFUN</code></td>
<td>
<p>Filter function, e.g. <a href="#topic+ttest_filter">ttest_filter</a> or <a href="#topic+relieff_filter">relieff_filter</a>.
Any function can be provided and is passed <code>y</code> and <code>x</code>. Must return a
character vector with names of filtered predictors. Not available if
<code>outercv</code> is called with a formula.</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_filter_options">filter_options</code></td>
<td>
<p>List of additional arguments passed to the filter
function specified by <code>filterFUN</code>.</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_weights">weights</code></td>
<td>
<p>Weights applied to each sample for models which can use
weights. Note <code>weights</code> and <code>balance</code> cannot be used at the same time.
Weights are not applied in filters.</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_balance">balance</code></td>
<td>
<p>Specifies method for dealing with imbalanced class data.
Current options are <code>"randomsample"</code> or <code>"smote"</code>. Not available if
<code>outercv</code> is called with a formula. See <code><a href="#topic+randomsample">randomsample()</a></code> and <code><a href="#topic+smote">smote()</a></code></p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_balance_options">balance_options</code></td>
<td>
<p>List of additional arguments passed to the balancing
function</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_modifyx">modifyX</code></td>
<td>
<p>Character string specifying the name of a function to modify
<code>x</code>. This can be an imputation function for replacing missing values, or a
more complex function which alters or even adds columns to <code>x</code>. The
required return value of this function depends on the <code>modifyX_useY</code>
setting.</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_modifyx_usey">modifyX_useY</code></td>
<td>
<p>Logical value whether the <code>x</code> modifying function makes
use of response training data from <code>y</code>. If <code>FALSE</code> then the <code>modifyX</code>
function simply needs to return a modified <code>x</code> object, which will be
coerced to a dataframe as required by <code>SuperLearner</code>. If <code>TRUE</code> then the
<code>modifyX</code> function must return a model type object on which <code>predict()</code> can
be called, so that train and test partitions of <code>x</code> can be modified
independently.</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_modifyx_options">modifyX_options</code></td>
<td>
<p>List of additional arguments passed to the <code>x</code>
modifying function</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_outer_method">outer_method</code></td>
<td>
<p>String of either <code>"cv"</code> or <code>"LOOCV"</code> specifying whether
to do k-fold CV or leave one out CV (LOOCV) for the outer folds</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_n_outer_folds">n_outer_folds</code></td>
<td>
<p>Number of outer CV folds</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_outer_folds">outer_folds</code></td>
<td>
<p>Optional list containing indices of test folds for outer
CV. If supplied, <code>n_outer_folds</code> is ignored.</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_cv.cores">cv.cores</code></td>
<td>
<p>Number of cores for parallel processing of the outer loops.
NOTE: this uses <code>parallel::mclapply</code> on unix/mac and <code>parallel::parLapply</code>
on windows.</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_final">final</code></td>
<td>
<p>Logical whether to fit final model.</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_na.option">na.option</code></td>
<td>
<p>Character value specifying how <code>NA</code>s are dealt with.
<code>"omit"</code> is equivalent to <code>na.action = na.omit</code>. <code>"omitcol"</code> removes cases
if there are <code>NA</code> in 'y', but columns (predictors) containing <code>NA</code> are
removed from 'x' to preserve cases. Any other value means that <code>NA</code> are
ignored (a message is given).</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_verbose">verbose</code></td>
<td>
<p>Logical whether to print messages and show progress</p>
</td></tr>
<tr><td><code id="nestcv.SuperLearner_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This performs an outer CV on SuperLearner package ensemble models to measure
performance, allowing balancing of imbalanced datasets as well as filtering
of predictors. SuperLearner prefers dataframes as inputs for the predictors.
If <code>x</code> is a matrix it will be coerced to a dataframe and variable names
adjusted by <code><a href="base.html#topic+make.names">make.names()</a></code>.
</p>
<p>Parallelisation of the outer CV folds is available on linux/mac, but not
available on windows. On windows, <code>snowSuperLearner()</code> is called instead, so
that parallelisation is performed across each call to SuperLearner.
</p>


<h3>Value</h3>

<p>An object with S3 class &quot;nestcv.SuperLearner&quot;
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the matched call</p>
</td></tr>
<tr><td><code>output</code></td>
<td>
<p>Predictions on the left-out outer folds</p>
</td></tr>
<tr><td><code>outer_result</code></td>
<td>
<p>List object of results from each outer fold containing
predictions on left-out outer folds, model result and number of filtered
predictors at each fold.</p>
</td></tr>
<tr><td><code>dimx</code></td>
<td>
<p>vector of number of observations and number of predictors</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>original response vector</p>
</td></tr>
<tr><td><code>yfinal</code></td>
<td>
<p>final response vector (post-balancing)</p>
</td></tr>
<tr><td><code>outer_folds</code></td>
<td>
<p>List of indices of outer test folds</p>
</td></tr>
<tr><td><code>final_fit</code></td>
<td>
<p>Final fitted model on whole data</p>
</td></tr>
<tr><td><code>final_vars</code></td>
<td>
<p>Column names of filtered predictors entering final model</p>
</td></tr>
<tr><td><code>summary_vars</code></td>
<td>
<p>Summary statistics of filtered predictors</p>
</td></tr>
<tr><td><code>roc</code></td>
<td>
<p>ROC AUC for binary classification where available.</p>
</td></tr>
<tr><td><code>summary</code></td>
<td>
<p>Overall performance summary. Accuracy and balanced accuracy
for classification. ROC AUC for binary classification. RMSE for
regression.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Care should be taken with some <code>SuperLearner</code> models e.g. <code>SL.gbm</code> as some
models have multicore enabled by default, which can lead to huge numbers of
processes being spawned.
</p>


<h3>See Also</h3>

<p><code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code>
</p>

<hr>
<h2 id='nestcv.train'>Nested cross-validation for caret</h2><span id='topic+nestcv.train'></span>

<h3>Description</h3>

<p>This function applies nested cross-validation (CV) to training of models
using the <code>caret</code> package. The function also allows the option of embedded
filtering of predictors for feature selection nested within the outer loop of
CV. Predictions on the outer test folds are brought back together and error
estimation/ accuracy determined. The default is 10x10 nested CV.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nestcv.train(
  y,
  x,
  method = "rf",
  filterFUN = NULL,
  filter_options = NULL,
  weights = NULL,
  balance = NULL,
  balance_options = NULL,
  modifyX = NULL,
  modifyX_useY = FALSE,
  modifyX_options = NULL,
  outer_method = c("cv", "LOOCV"),
  n_outer_folds = 10,
  n_inner_folds = 10,
  outer_folds = NULL,
  inner_folds = NULL,
  pass_outer_folds = FALSE,
  cv.cores = 1,
  multicore_fork = (Sys.info()["sysname"] != "Windows"),
  metric = ifelse(is.factor(y), "logLoss", "RMSE"),
  trControl = NULL,
  tuneGrid = NULL,
  savePredictions = "final",
  outer_train_predict = FALSE,
  finalCV = TRUE,
  na.option = "pass",
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nestcv.train_+3A_y">y</code></td>
<td>
<p>Response vector. For classification this should be a factor.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_x">x</code></td>
<td>
<p>Matrix or dataframe of predictors</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_method">method</code></td>
<td>
<p>String specifying which model to use. See <code><a href="caret.html#topic+train">caret::train()</a></code> for
details.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_filterfun">filterFUN</code></td>
<td>
<p>Filter function, e.g. <code><a href="#topic+ttest_filter">ttest_filter()</a></code> or <code><a href="#topic+relieff_filter">relieff_filter()</a></code>.
Any function can be provided and is passed <code>y</code> and <code>x</code>. Must return a
character vector with names of filtered predictors.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_filter_options">filter_options</code></td>
<td>
<p>List of additional arguments passed to the filter
function specified by <code>filterFUN</code>.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_weights">weights</code></td>
<td>
<p>Weights applied to each sample for models which can use
weights. Note <code>weights</code> and <code>balance</code> cannot be used at the same time.
Weights are not applied in filters.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_balance">balance</code></td>
<td>
<p>Specifies method for dealing with imbalanced class data.
Current options are <code>"randomsample"</code> or <code>"smote"</code>. See <code><a href="#topic+randomsample">randomsample()</a></code> and
<code><a href="#topic+smote">smote()</a></code></p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_balance_options">balance_options</code></td>
<td>
<p>List of additional arguments passed to the balancing
function</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_modifyx">modifyX</code></td>
<td>
<p>Character string specifying the name of a function to modify
<code>x</code>. This can be an imputation function for replacing missing values, or a
more complex function which alters or even adds columns to <code>x</code>. The
required return value of this function depends on the <code>modifyX_useY</code>
setting.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_modifyx_usey">modifyX_useY</code></td>
<td>
<p>Logical value whether the <code>x</code> modifying function makes
use of response training data from <code>y</code>. If <code>FALSE</code> then the <code>modifyX</code>
function simply needs to return a modified <code>x</code> object. If <code>TRUE</code> then the
<code>modifyX</code> function must return a model type object on which <code>predict()</code> can
be called, so that train and test partitions of <code>x</code> can be modified
independently.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_modifyx_options">modifyX_options</code></td>
<td>
<p>List of additional arguments passed to the <code>x</code>
modifying function</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_outer_method">outer_method</code></td>
<td>
<p>String of either <code>"cv"</code> or <code>"LOOCV"</code> specifying whether
to do k-fold CV or leave one out CV (LOOCV) for the outer folds</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_n_outer_folds">n_outer_folds</code></td>
<td>
<p>Number of outer CV folds</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_n_inner_folds">n_inner_folds</code></td>
<td>
<p>Sets number of inner CV folds. Note if <code>trControl</code> or
<code>inner_folds</code> is specified then these supersede <code>n_inner_folds</code>.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_outer_folds">outer_folds</code></td>
<td>
<p>Optional list containing indices of test folds for outer
CV. If supplied, <code>n_outer_folds</code> is ignored.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_inner_folds">inner_folds</code></td>
<td>
<p>Optional list of test fold indices for inner CV. This must
be structured as a list of the outer folds each containing a list of inner
folds. Can only be supplied if balancing is not applied. If supplied,
<code>n_inner_folds</code> is ignored.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_pass_outer_folds">pass_outer_folds</code></td>
<td>
<p>Logical indicating whether the same outer folds are
used for fitting of the final model when final CV is applied. Note this can
only be applied when <code>n_outer_folds</code> and the number of inner CV folds
specified in <code>n_inner_folds</code> or <code>trControl</code> are the same and that no
balancing is applied.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_cv.cores">cv.cores</code></td>
<td>
<p>Number of cores for parallel processing of the outer loops.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_multicore_fork">multicore_fork</code></td>
<td>
<p>Logical whether to use forked multicore parallel
processing. Forked multicore processing uses <code>parallel::mclapply</code>. It is
only available on unix/mac as windows does not allow forking. It is set to
<code>FALSE</code> by default in windows and <code>TRUE</code> in unix/mac. Non-forked parallel
processing is executed using <code>parallel::parLapply</code> or <code>pbapply::pblapply</code>
if <code>verbose</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_metric">metric</code></td>
<td>
<p>A string that specifies what summary metric will be used to
select the optimal model. By default, &quot;logLoss&quot; is used for classification
and &quot;RMSE&quot; is used for regression. Note this differs from the default
setting in caret which uses &quot;Accuracy&quot; for classification. See details.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_trcontrol">trControl</code></td>
<td>
<p>A list of values generated by the <code>caret</code> function
<code><a href="caret.html#topic+trainControl">caret::trainControl()</a></code>. This defines how inner CV training through <code>caret</code>
is performed. Default for the inner loop is 10-fold CV. Setting this
argument overrules <code>n_inner_folds</code>. See
http://topepo.github.io/caret/using-your-own-model-in-train.html.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_tunegrid">tuneGrid</code></td>
<td>
<p>Data frame of tuning values, see <code><a href="caret.html#topic+train">caret::train()</a></code>.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_savepredictions">savePredictions</code></td>
<td>
<p>Indicates whether hold-out predictions for each inner
CV fold should be saved for ROC curves, accuracy etc see
<a href="caret.html#topic+trainControl">caret::trainControl</a>. Default is <code>"final"</code> to capture predictions for
inner CV ROC.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_outer_train_predict">outer_train_predict</code></td>
<td>
<p>Logical whether to save predictions on outer
training folds to calculate performance on outer training folds.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_finalcv">finalCV</code></td>
<td>
<p>Logical whether to perform one last round of CV on the whole
dataset to determine the final model parameters. If set to <code>FALSE</code>, the
median of the best hyperparameters from outer CV folds for continuous/
ordinal hyperparameters, or highest voted for categorical hyperparameters,
are used to fit the final model. Performance metrics are independent of
this last step. If set to <code>NA</code>, final model fitting is skipped altogether,
which gives a useful speed boost if performance metrics are all that is
needed.</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_na.option">na.option</code></td>
<td>
<p>Character value specifying how <code>NA</code>s are dealt with.
<code>"omit"</code> is equivalent to <code>na.action = na.omit</code>. <code>"omitcol"</code> removes cases
if there are <code>NA</code> in 'y', but columns (predictors) containing <code>NA</code> are
removed from 'x' to preserve cases. Any other value means that <code>NA</code> are
ignored (a message is given).</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_verbose">verbose</code></td>
<td>
<p>Logical whether to print messages and show progress</p>
</td></tr>
<tr><td><code id="nestcv.train_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="caret.html#topic+train">caret::train()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>finalCV = TRUE</code>, the final fit on the whole data using is performed
first. This helps flag errors generated by <code>caret</code> such as missing packages.
Parallelisation of the final fit when <code>finalCV = TRUE</code> is performed in
<code>caret</code> using <code>registerDoParallel</code>. <code>caret</code> itself uses <code>foreach</code>.
</p>
<p>Parallelisation is performed on the outer CV folds using <code>parallel::mclapply</code>
by default on unix/mac and <code>parallel::parLapply</code> on windows. <code>mclapply</code> uses
forking which is faster. But some models use multi-threading which may cause
issues in some circumstances with forked multicore processing. Setting
<code>multicore_fork</code> to <code>FALSE</code> is slower but can alleviate some caret errors.
</p>
<p>If the outer folds are run using parallelisation, then parallelisation in
caret must be off, otherwise an error will be generated. Alternatively if you
wish to use parallelisation in caret, then parallelisation in <code>nestcv.train</code>
can be fully disabled by leaving <code>cv.cores = 1</code>.
</p>
<p>xgboost models fitted via caret using <code>method = "xgbTree"</code> or <code>"xgbLinear"</code>
invoke openMP multithreading on linux/windows by default which causes
<code>nestcv.train</code> to fail when <code>cv.cores</code> &gt;1 (nested parallelisation). Mac OS is
unaffected. In order to prevent this, <code>nestcv.train()</code> sets openMP threads to
1 if <code>cv.cores</code> &gt;1.
</p>
<p>For classification, <code>metric</code> defaults to using 'logLoss' with the <code>trControl</code>
arguments <code style="white-space: pre;">&#8288;classProbs = TRUE, summaryFunction = mnLogLoss&#8288;</code>, rather than
'Accuracy' which is the default classification metric in <code>caret</code>. See
<code><a href="caret.html#topic+trainControl">caret::trainControl()</a></code>. LogLoss is arguably more consistent than Accuracy
for tuning parameters in datasets with small sample size.
</p>
<p>Models can be fitted with a single set of fixed parameters, in which case
<code>trControl</code> defaults to <code>trainControl(method = "none")</code> which disables inner
CV as it is unnecessary. See
https://topepo.github.io/caret/model-training-and-tuning.html#fitting-models-without-parameter-tuning
</p>


<h3>Value</h3>

<p>An object with S3 class &quot;nestcv.train&quot;
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the matched call</p>
</td></tr>
<tr><td><code>output</code></td>
<td>
<p>Predictions on the left-out outer folds</p>
</td></tr>
<tr><td><code>outer_result</code></td>
<td>
<p>List object of results from each outer fold containing
predictions on left-out outer folds, caret result and number of filtered
predictors at each fold.</p>
</td></tr>
<tr><td><code>outer_folds</code></td>
<td>
<p>List of indices of outer test folds</p>
</td></tr>
<tr><td><code>dimx</code></td>
<td>
<p>dimensions of <code>x</code></p>
</td></tr>
<tr><td><code>xsub</code></td>
<td>
<p>subset of <code>x</code> containing all predictors used in both outer CV
folds and the final model</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>original response vector</p>
</td></tr>
<tr><td><code>yfinal</code></td>
<td>
<p>final response vector (post-balancing)</p>
</td></tr>
<tr><td><code>final_fit</code></td>
<td>
<p>Final fitted caret model using best tune parameters</p>
</td></tr>
<tr><td><code>final_vars</code></td>
<td>
<p>Column names of filtered predictors entering final model</p>
</td></tr>
<tr><td><code>summary_vars</code></td>
<td>
<p>Summary statistics of filtered predictors</p>
</td></tr>
<tr><td><code>roc</code></td>
<td>
<p>ROC AUC for binary classification where available.</p>
</td></tr>
<tr><td><code>trControl</code></td>
<td>
<p><code>caret::trainControl</code> object used for inner CV</p>
</td></tr>
<tr><td><code>bestTunes</code></td>
<td>
<p>best tuned parameters from each outer fold</p>
</td></tr>
<tr><td><code>finalTune</code></td>
<td>
<p>final parameters used for final model</p>
</td></tr>
<tr><td><code>summary</code></td>
<td>
<p>Overall performance summary. Accuracy and balanced accuracy
for classification. ROC AUC for binary classification. RMSE for
regression.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Myles Lewis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## sigmoid function
sigmoid &lt;- function(x) {1 / (1 + exp(-x))}

## load iris dataset and simulate a binary outcome
data(iris)
x &lt;- iris[, 1:4]
colnames(x) &lt;- c("marker1", "marker2", "marker3", "marker4")
x &lt;- as.data.frame(apply(x, 2, scale))
y2 &lt;- sigmoid(0.5 * x$marker1 + 2 * x$marker2) &gt; runif(nrow(x))
y2 &lt;- factor(y2, labels = c("class1", "class2"))

## Example using random forest with caret
cvrf &lt;- nestcv.train(y2, x, method = "rf",
                     n_outer_folds = 3,
                     cv.cores = 2)
summary(cvrf)

## Example of glmnet tuned using caret
## set up small tuning grid for quick execution
## length.out of 20-100 is usually recommended for lambda
## and more alpha values ranging from 0-1
tg &lt;- expand.grid(lambda = exp(seq(log(2e-3), log(1e0), length.out = 5)),
                  alpha = 1)

ncv &lt;- nestcv.train(y = y2, x = x,
                    method = "glmnet",
                    n_outer_folds = 3,
                    tuneGrid = tg, cv.cores = 2)
summary(ncv)

## plot tuning for outer fold #1
plot(ncv$outer_result[[1]]$fit, xTrans = log)

## plot final ROC curve
plot(ncv$roc)

## plot ROC for left-out inner folds
inroc &lt;- innercv_roc(ncv)
plot(inroc)

## example to show use of custom fold indices for 5 x 5-fold nested CV
library(caret)
y &lt;- iris$Species
out_folds &lt;- createFolds(y, k = 5)
in_folds &lt;- lapply(out_folds, function(i) {
  ytrain &lt;- y[-i]
  createFolds(ytrain, k = 5)
})

res &lt;- nestcv.train(y, x, method="rf", cv.cores = 2,
                    pass_outer_folds = TRUE,
                    inner_folds = in_folds,
                    outer_folds = out_folds)
summary(res)
res$outer_folds
res$final_fit$control$indexOut  # same as outer_folds

</code></pre>

<hr>
<h2 id='one_hot'>One-hot encode</h2><span id='topic+one_hot'></span>

<h3>Description</h3>

<p>Fast one-hot encoding of all factor and character columns in a dataframe to
convert it into a numeric matrix by creating dummy (binary) columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>one_hot(x, all_levels = FALSE, rename_binary = TRUE, sep = ".")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="one_hot_+3A_x">x</code></td>
<td>
<p>A dataframe, matrix or tibble. Matrices are returned untouched.</p>
</td></tr>
<tr><td><code id="one_hot_+3A_all_levels">all_levels</code></td>
<td>
<p>Logical, whether to create dummy variables for all levels
of each factor. Default is <code>FALSE</code> to avoid issues with regression models.</p>
</td></tr>
<tr><td><code id="one_hot_+3A_rename_binary">rename_binary</code></td>
<td>
<p>Logical, whether to rename binary factors by appending
the 2nd level of the factor to aid interpretation of encoded factor levels
and to allow consistency with naming.</p>
</td></tr>
<tr><td><code id="one_hot_+3A_sep">sep</code></td>
<td>
<p>Character for separating factor variable names and levels for
encoded columns.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Binary factor columns and logical columns are converted to integers (0 or
1). Multi-level unordered factors are converted to multiple columns of 0/1
(dummy variables): if <code>all_levels</code> is set to <code>FALSE</code> (the default), then the
first level is assumed to be a reference level and additional columns are
created for each additional level; if <code>all_levels</code> is set to <code>TRUE</code> one
column is used for each level. Unused levels are dropped. Character columns
are first converted to factors and then encoded. Ordered factors are
replaced by their internal codes. Numeric or integer columns are left
untouched.
</p>
<p>Having dummy variables for all levels of a factor can cause problems with
multicollinearity in regression (the dummy variable trap), so <code>all_levels</code>
is set to <code>FALSE</code> by default which is necessary for regression models such
as <code>glmnet</code> (equivalent to full rank parameterisation). However, setting
<code>all_levels</code> to <code>TRUE</code> can aid with interpretability (e.g. with SHAP
values), and in some cases filtering might result in some dummy variables
being excluded. Note this function is designed to quickly generate dummy
variables for more general machine learning purposes. To create a proper
design matrix object for regression models, use <code><a href="stats.html#topic+model.matrix">model.matrix()</a></code>.
</p>


<h3>Value</h3>

<p>A numeric matrix with the same number of rows as the input data.
Dummy variable columns replace the input factor or character columns.
Numeric columns are left intact.
</p>


<h3>See Also</h3>

<p><code><a href="caret.html#topic+dummyVars">caret::dummyVars()</a></code>, <code><a href="stats.html#topic+model.matrix">model.matrix()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- iris
x2 &lt;- one_hot(x)
head(x2)  # 3 columns for Species

x2 &lt;- one_hot(x, all_levels = FALSE)
head(x2)  # 2 columns for Species

</code></pre>

<hr>
<h2 id='outercv'>Outer cross-validation of selected models</h2><span id='topic+outercv'></span><span id='topic+outercv.default'></span><span id='topic+outercv.formula'></span>

<h3>Description</h3>

<p>This is a convenience function designed to use a single loop of
cross-validation to quickly evaluate performance of specific models (random
forest, naive Bayes, lm, glm) with fixed hyperparameters and no tuning. If
tuning of parameters on data is required, full nested CV with inner CV is
needed to tune model hyperparameters (see <a href="#topic+nestcv.train">nestcv.train</a>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>outercv(y, ...)

## Default S3 method:
outercv(
  y,
  x,
  model,
  filterFUN = NULL,
  filter_options = NULL,
  weights = NULL,
  balance = NULL,
  balance_options = NULL,
  modifyX = NULL,
  modifyX_useY = FALSE,
  modifyX_options = NULL,
  outer_method = c("cv", "LOOCV"),
  n_outer_folds = 10,
  outer_folds = NULL,
  cv.cores = 1,
  multicore_fork = (Sys.info()["sysname"] != "Windows"),
  predict_type = "prob",
  outer_train_predict = FALSE,
  returnList = FALSE,
  final = TRUE,
  na.option = "pass",
  verbose = FALSE,
  suppressMsg = verbose,
  ...
)

## S3 method for class 'formula'
outercv(
  formula,
  data,
  model,
  outer_method = c("cv", "LOOCV"),
  n_outer_folds = 10,
  outer_folds = NULL,
  cv.cores = 1,
  multicore_fork = (Sys.info()["sysname"] != "Windows"),
  predict_type = "prob",
  outer_train_predict = FALSE,
  verbose = FALSE,
  suppressMsg = verbose,
  ...,
  na.action = na.fail
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="outercv_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="outercv_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to the function specified by <code>model</code>.</p>
</td></tr>
<tr><td><code id="outercv_+3A_x">x</code></td>
<td>
<p>Matrix or dataframe of predictors</p>
</td></tr>
<tr><td><code id="outercv_+3A_model">model</code></td>
<td>
<p>Character value or function of the model to be fitted.</p>
</td></tr>
<tr><td><code id="outercv_+3A_filterfun">filterFUN</code></td>
<td>
<p>Filter function, e.g. <a href="#topic+ttest_filter">ttest_filter</a> or <a href="#topic+relieff_filter">relieff_filter</a>.
Any function can be provided and is passed <code>y</code> and <code>x</code>. Must return a
character vector with names of filtered predictors. Not available if
<code>outercv</code> is called with a formula.</p>
</td></tr>
<tr><td><code id="outercv_+3A_filter_options">filter_options</code></td>
<td>
<p>List of additional arguments passed to the filter
function specified by <code>filterFUN</code>.</p>
</td></tr>
<tr><td><code id="outercv_+3A_weights">weights</code></td>
<td>
<p>Weights applied to each sample for models which can use
weights. Note <code>weights</code> and <code>balance</code> cannot be used at the same time.
Weights are not applied in filters.</p>
</td></tr>
<tr><td><code id="outercv_+3A_balance">balance</code></td>
<td>
<p>Specifies method for dealing with imbalanced class data.
Current options are <code>"randomsample"</code> or <code>"smote"</code>. Not available if
<code>outercv</code> is called with a formula. See <code><a href="#topic+randomsample">randomsample()</a></code> and <code><a href="#topic+smote">smote()</a></code></p>
</td></tr>
<tr><td><code id="outercv_+3A_balance_options">balance_options</code></td>
<td>
<p>List of additional arguments passed to the balancing
function</p>
</td></tr>
<tr><td><code id="outercv_+3A_modifyx">modifyX</code></td>
<td>
<p>Character string specifying the name of a function to modify
<code>x</code>. This can be an imputation function for replacing missing values, or a
more complex function which alters or even adds columns to <code>x</code>. The
required return value of this function depends on the <code>modifyX_useY</code>
setting.</p>
</td></tr>
<tr><td><code id="outercv_+3A_modifyx_usey">modifyX_useY</code></td>
<td>
<p>Logical value whether the <code>x</code> modifying function makes
use of response training data from <code>y</code>. If <code>FALSE</code> then the <code>modifyX</code>
function simply needs to return a modified <code>x</code> object. If <code>TRUE</code> then the
<code>modifyX</code> function must return a model type object on which <code>predict()</code> can
be called, so that train and test partitions of <code>x</code> can be modified
independently.</p>
</td></tr>
<tr><td><code id="outercv_+3A_modifyx_options">modifyX_options</code></td>
<td>
<p>List of additional arguments passed to the <code>x</code>
modifying function</p>
</td></tr>
<tr><td><code id="outercv_+3A_outer_method">outer_method</code></td>
<td>
<p>String of either <code>"cv"</code> or <code>"LOOCV"</code> specifying whether
to do k-fold CV or leave one out CV (LOOCV) for the outer folds</p>
</td></tr>
<tr><td><code id="outercv_+3A_n_outer_folds">n_outer_folds</code></td>
<td>
<p>Number of outer CV folds</p>
</td></tr>
<tr><td><code id="outercv_+3A_outer_folds">outer_folds</code></td>
<td>
<p>Optional list containing indices of test folds for outer
CV. If supplied, <code>n_outer_folds</code> is ignored.</p>
</td></tr>
<tr><td><code id="outercv_+3A_cv.cores">cv.cores</code></td>
<td>
<p>Number of cores for parallel processing of the outer loops.</p>
</td></tr>
<tr><td><code id="outercv_+3A_multicore_fork">multicore_fork</code></td>
<td>
<p>Logical whether to use forked multicore parallel
processing. Forked multicore processing uses <code>parallel::mclapply</code>. It is
only available on unix/mac as windows does not allow forking. It is set to
<code>FALSE</code> by default in windows and <code>TRUE</code> in unix/mac. Non-forked parallel
processing is executed using <code>parallel::parLapply</code> or <code>pbapply::pblapply</code>
if <code>verbose</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="outercv_+3A_predict_type">predict_type</code></td>
<td>
<p>Only used with binary classification. Calculation of ROC
AUC requires predicted class probabilities from fitted models. Most model
functions use syntax of the form <code>predict(..., type = "prob")</code>. However,
some models require a different <code>type</code> to be specified, which can be passed
to <code>predict()</code> via <code>predict_type</code>.</p>
</td></tr>
<tr><td><code id="outercv_+3A_outer_train_predict">outer_train_predict</code></td>
<td>
<p>Logical whether to save predictions on outer
training folds to calculate performance on outer training folds.</p>
</td></tr>
<tr><td><code id="outercv_+3A_returnlist">returnList</code></td>
<td>
<p>Logical whether to return list of results after main outer
CV loop without concatenating results. Useful for debugging.</p>
</td></tr>
<tr><td><code id="outercv_+3A_final">final</code></td>
<td>
<p>Logical whether to fit final model.</p>
</td></tr>
<tr><td><code id="outercv_+3A_na.option">na.option</code></td>
<td>
<p>Character value specifying how <code>NA</code>s are dealt with.
<code>"omit"</code> is equivalent to <code>na.action = na.omit</code>. <code>"omitcol"</code> removes cases
if there are <code>NA</code> in 'y', but columns (predictors) containing <code>NA</code> are
removed from 'x' to preserve cases. Any other value means that <code>NA</code> are
ignored (a message is given).</p>
</td></tr>
<tr><td><code id="outercv_+3A_verbose">verbose</code></td>
<td>
<p>Logical whether to print messages and show progress</p>
</td></tr>
<tr><td><code id="outercv_+3A_suppressmsg">suppressMsg</code></td>
<td>
<p>Logical whether to suppress messages and printed output
from model functions. This is necessary when using forked multicore
parallelisation.</p>
</td></tr>
<tr><td><code id="outercv_+3A_formula">formula</code></td>
<td>
<p>A formula describing the model to be fitted</p>
</td></tr>
<tr><td><code id="outercv_+3A_data">data</code></td>
<td>
<p>A matrix or data frame containing variables in the model.</p>
</td></tr>
<tr><td><code id="outercv_+3A_na.action">na.action</code></td>
<td>
<p>Formula S3 method only: a function to specify the action to
be taken if NAs are found. The default action is for the procedure to fail.
An alternative is <code>na.omit</code>, which leads to rejection of cases with missing
values on any required variable. (NOTE: If given, this argument must be
named.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some predictive model functions do not have an x &amp; y interface. If the
function specified by <code>model</code> requires a formula, <code>x</code> &amp; <code>y</code> will be merged
into a dataframe with <code>model()</code> called with a formula equivalent to
<code>y ~ .</code>.
</p>
<p>The S3 formula method for <code>outercv</code> is not really recommended with large
data sets - it is envisaged to be primarily used to compare
performance of more basic models e.g. <code>lm()</code> specified by formulae for
example incorporating interactions. NOTE: filtering is not available if
<code>outercv</code> is called with a formula - use the <code>x-y</code> interface instead.
</p>
<p>An alternative method of tuning a single model with fixed parameters
is to use <a href="#topic+nestcv.train">nestcv.train</a> with <code>tuneGrid</code> set as a single row of a
data.frame. The parameters which are needed for a specific model can be
identified using <code><a href="caret.html#topic+modelLookup">caret::modelLookup()</a></code>.
</p>
<p>Case weights can be passed to model function which accept these, however
<code>outercv</code> assumes that these are passed to the model via an argument named
<code>weights</code>.
</p>
<p>Note that in the case of <code>model = "lm"</code>, although additional arguments e.g.
<code>subset</code>, <code>weights</code>, <code>offset</code> are passed into the model function via
<code>"..."</code> the scoping is known to go awry. Avoid using these arguments with
<code>model = "lm"</code>.
</p>
<p><code>NA</code> handling differs between the default S3 method and the formula S3
method. The <code>na.option</code> argument takes a character string, while the more
typical <code>na.action</code> argument takes a function.
</p>


<h3>Value</h3>

<p>An object with S3 class &quot;outercv&quot;
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the matched call</p>
</td></tr>
<tr><td><code>output</code></td>
<td>
<p>Predictions on the left-out outer folds</p>
</td></tr>
<tr><td><code>outer_result</code></td>
<td>
<p>List object of results from each outer fold containing
predictions on left-out outer folds, model result and number of filtered
predictors at each fold.</p>
</td></tr>
<tr><td><code>dimx</code></td>
<td>
<p>vector of number of observations and number of predictors</p>
</td></tr>
<tr><td><code>outer_folds</code></td>
<td>
<p>List of indices of outer test folds</p>
</td></tr>
<tr><td><code>final_fit</code></td>
<td>
<p>Final fitted model on whole data</p>
</td></tr>
<tr><td><code>final_vars</code></td>
<td>
<p>Column names of filtered predictors entering final model</p>
</td></tr>
<tr><td><code>roc</code></td>
<td>
<p>ROC AUC for binary classification where available.</p>
</td></tr>
<tr><td><code>summary</code></td>
<td>
<p>Overall performance summary. Accuracy and balanced accuracy
for classification. ROC AUC for binary classification. RMSE for
regression.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
## Classification example

## sigmoid function
sigmoid &lt;- function(x) {1 / (1 + exp(-x))}

# load iris dataset and simulate a binary outcome
data(iris)
dt &lt;- iris[, 1:4]
colnames(dt) &lt;- c("marker1", "marker2", "marker3", "marker4")
dt &lt;- as.data.frame(apply(dt, 2, scale))
x &lt;- dt
y2 &lt;- sigmoid(0.5 * dt$marker1 + 2 * dt$marker2) &gt; runif(nrow(dt))
y2 &lt;- factor(y2)

## Random forest
library(randomForest)
cvfit &lt;- outercv(y2, x, "randomForest")
summary(cvfit)
plot(cvfit$roc)

## Mixture discriminant analysis (MDA)
if (requireNamespace("mda", quietly = TRUE)) {
  library(mda)
  cvfit &lt;- outercv(y2, x, "mda", predict_type = "posterior")
  summary(cvfit)
}


## Example with continuous outcome
y &lt;- -3 + 0.5 * dt$marker1 + 2 * dt$marker2 + rnorm(nrow(dt), 0, 2)
dt$outcome &lt;- y

## simple linear model - formula interface
cvfit &lt;- outercv(outcome ~ ., data = dt, model = "lm")
summary(cvfit)

## random forest for regression
cvfit &lt;- outercv(y, x, "randomForest")
summary(cvfit)

## example with lm_filter() to reduce input predictors
cvfit &lt;- outercv(y, x, "randomForest", filterFUN = lm_filter,
                 filter_options = list(nfilter = 2, p_cutoff = NULL))
summary(cvfit)

</code></pre>

<hr>
<h2 id='plot_alphas'>Plot cross-validated glmnet alpha</h2><span id='topic+plot_alphas'></span>

<h3>Description</h3>

<p>Plot of cross-validated glmnet alpha parameter against deviance for each
outer CV fold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_alphas(x, col = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_alphas_+3A_x">x</code></td>
<td>
<p>Fitted &quot;nestcv.glmnet&quot; object</p>
</td></tr>
<tr><td><code id="plot_alphas_+3A_col">col</code></td>
<td>
<p>Optional vector of line colours for each fold</p>
</td></tr>
<tr><td><code id="plot_alphas_+3A_...">...</code></td>
<td>
<p>other arguments passed to plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value
</p>


<h3>Author(s)</h3>

<p>Myles Lewis
</p>


<h3>See Also</h3>

<p><a href="#topic+nestcv.glmnet">nestcv.glmnet</a>
</p>

<hr>
<h2 id='plot_caret'>Plot caret tuning</h2><span id='topic+plot_caret'></span>

<h3>Description</h3>

<p>Plots the main tuning parameter in models built using <a href="caret.html#topic+train">caret::train</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_caret(x, error.col = "darkgrey", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_caret_+3A_x">x</code></td>
<td>
<p>Object of class 'train' generated by <code>caret</code> function <a href="caret.html#topic+train">train</a></p>
</td></tr>
<tr><td><code id="plot_caret_+3A_error.col">error.col</code></td>
<td>
<p>Colour of error bars</p>
</td></tr>
<tr><td><code id="plot_caret_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="base.html#topic+plot">plot()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value
</p>

<hr>
<h2 id='plot_lambdas'>Plot cross-validated glmnet lambdas across outer folds</h2><span id='topic+plot_lambdas'></span>

<h3>Description</h3>

<p>Plot of cross-validated glmnet lambda parameter against deviance for each
outer CV fold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_lambdas(
  x,
  scheme = NULL,
  palette = "Dark 3",
  showLegend = if (x$outer_method == "cv") "topright" else NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_lambdas_+3A_x">x</code></td>
<td>
<p>Fitted &quot;nestcv.glmnet&quot; object</p>
</td></tr>
<tr><td><code id="plot_lambdas_+3A_scheme">scheme</code></td>
<td>
<p>colour scheme</p>
</td></tr>
<tr><td><code id="plot_lambdas_+3A_palette">palette</code></td>
<td>
<p>palette name (one of <code>hcl.pals()</code>) which is passed to
<a href="grDevices.html#topic+hcl.colors">hcl.colors</a></p>
</td></tr>
<tr><td><code id="plot_lambdas_+3A_showlegend">showLegend</code></td>
<td>
<p>Either a keyword to position the legend or <code>NULL</code> to hide
the legend.</p>
</td></tr>
<tr><td><code id="plot_lambdas_+3A_...">...</code></td>
<td>
<p>other arguments passed to plot. Use <code>type = 'p'</code> to plot a
scatter plot instead of a line plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value
</p>


<h3>Author(s)</h3>

<p>Myles Lewis
</p>


<h3>See Also</h3>

<p><a href="#topic+nestcv.glmnet">nestcv.glmnet</a>
</p>

<hr>
<h2 id='plot_shap_bar'>SHAP importance bar plot</h2><span id='topic+plot_shap_bar'></span>

<h3>Description</h3>

<p>SHAP importance bar plot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_shap_bar(
  shap,
  x,
  sort = TRUE,
  labels = c("Negative", "Positive"),
  top = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_shap_bar_+3A_shap">shap</code></td>
<td>
<p>a matrix of SHAP values</p>
</td></tr>
<tr><td><code id="plot_shap_bar_+3A_x">x</code></td>
<td>
<p>a matrix or dataframe of feature values containing only features
values from the training data. The rows must match rows in <code>shap</code>. If a
dataframe is supplied it is converted to a numeric matrix using
<code><a href="base.html#topic+data.matrix">data.matrix()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_shap_bar_+3A_sort">sort</code></td>
<td>
<p>Logical whether to sort predictors by mean absolute SHAP value</p>
</td></tr>
<tr><td><code id="plot_shap_bar_+3A_labels">labels</code></td>
<td>
<p>Character vector of labels for directionality</p>
</td></tr>
<tr><td><code id="plot_shap_bar_+3A_top">top</code></td>
<td>
<p>Sets a limit on the number of variables plotted or <code>NULL</code> to plot
all variables. If <code>top</code> is set then variables are sorted and <code>sort</code> is
overrode.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 plot
</p>

<hr>
<h2 id='plot_shap_beeswarm'>SHAP importance beeswarm plot</h2><span id='topic+plot_shap_beeswarm'></span>

<h3>Description</h3>

<p>SHAP importance beeswarm plot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_shap_beeswarm(
  shap,
  x,
  cex = 0.25,
  corral = "random",
  corral.width = 0.7,
  scheme = c("deepskyblue2", "purple3", "red"),
  sort = TRUE,
  top = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_shap_beeswarm_+3A_shap">shap</code></td>
<td>
<p>a matrix of SHAP values</p>
</td></tr>
<tr><td><code id="plot_shap_beeswarm_+3A_x">x</code></td>
<td>
<p>a matrix or dataframe of feature values containing only features
values from the training data. The rows must match rows in <code>shap</code>. If a
dataframe is supplied it is converted to a numeric matrix using
<code><a href="base.html#topic+data.matrix">data.matrix()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_shap_beeswarm_+3A_cex">cex</code></td>
<td>
<p>Scaling for adjusting point spacing. See
<code>ggbeeswarm::geom_beeswarm()</code>.</p>
</td></tr>
<tr><td><code id="plot_shap_beeswarm_+3A_corral">corral</code></td>
<td>
<p>String specifying method used to corral points. See
<code>ggbeeswarm::geom_beeswarm()</code>.</p>
</td></tr>
<tr><td><code id="plot_shap_beeswarm_+3A_corral.width">corral.width</code></td>
<td>
<p>Numeric specifying width of corral, passed to
<code>geom_beeswarm</code></p>
</td></tr>
<tr><td><code id="plot_shap_beeswarm_+3A_scheme">scheme</code></td>
<td>
<p>Colour scheme as a vector of 3 colours</p>
</td></tr>
<tr><td><code id="plot_shap_beeswarm_+3A_sort">sort</code></td>
<td>
<p>Logical whether to sort predictors by mean absolute SHAP value.</p>
</td></tr>
<tr><td><code id="plot_shap_beeswarm_+3A_top">top</code></td>
<td>
<p>Sets a limit on the number of variables plotted or <code>NULL</code> to plot
all variables. If <code>top</code> is set then variables are sorted and <code>sort</code> is
overrode.</p>
</td></tr>
<tr><td><code id="plot_shap_beeswarm_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>ggbeeswarm::geom_beeswarm()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 plot
</p>

<hr>
<h2 id='plot_var_stability'>Plot variable stability</h2><span id='topic+plot_var_stability'></span>

<h3>Description</h3>

<p>Produces a ggplot2 plot of stability (as SEM) of variable importance across
models trained and tested across outer CV folds. Overlays frequency with
which variables are selected across the outer folds and optionally overlays
directionality for binary response outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_var_stability(
  x,
  final = TRUE,
  top = NULL,
  direction = 0,
  dir_labels = NULL,
  scheme = c("royalblue", "red"),
  breaks = NULL,
  percent = TRUE,
  level = 1,
  sort = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_var_stability_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code> or <code>nestcv.train</code> fitted object</p>
</td></tr>
<tr><td><code id="plot_var_stability_+3A_final">final</code></td>
<td>
<p>Logical whether to restrict variables to only those which ended
up in the final fitted model or to include all variables selected across
all outer folds.</p>
</td></tr>
<tr><td><code id="plot_var_stability_+3A_top">top</code></td>
<td>
<p>Limits number of variables plotted. Set to <code>NULL</code> to plot all
variables.</p>
</td></tr>
<tr><td><code id="plot_var_stability_+3A_direction">direction</code></td>
<td>
<p>Integer controlling plotting of directionality for binary or
regression models. <code>0</code> means no directionality is shown, <code>1</code> means
directionality is overlaid as a colour, <code>2</code> means directionality is
reflected in the sign of variable importance. Not available for multiclass
caret models.</p>
</td></tr>
<tr><td><code id="plot_var_stability_+3A_dir_labels">dir_labels</code></td>
<td>
<p>Character vector for controlling the legend when
<code>direction = 1</code></p>
</td></tr>
<tr><td><code id="plot_var_stability_+3A_scheme">scheme</code></td>
<td>
<p>Vector of 2 colours for directionality when <code>direction = 1</code></p>
</td></tr>
<tr><td><code id="plot_var_stability_+3A_breaks">breaks</code></td>
<td>
<p>Vector of continuous breaks for legend colour/size</p>
</td></tr>
<tr><td><code id="plot_var_stability_+3A_percent">percent</code></td>
<td>
<p>Logical for <code>nestcv.glmnet</code> objects only, whether to scale
coefficients to percentage of the largest coefficient in each model. If set
to <code>FALSE</code>, model coefficients are shown and <code>direction</code> is ignored.</p>
</td></tr>
<tr><td><code id="plot_var_stability_+3A_level">level</code></td>
<td>
<p>For multinomial <code>nestcv.glmnet</code> models only, either an integer
specifying which level of outcome is being examined, or the level can be
specified as a character value.</p>
</td></tr>
<tr><td><code id="plot_var_stability_+3A_sort">sort</code></td>
<td>
<p>Logical whether to sort by mean variable importance. Passed to
<code><a href="#topic+var_stability">var_stability()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 plot
</p>


<h3>See Also</h3>

<p><code><a href="#topic+var_stability">var_stability()</a></code>
</p>

<hr>
<h2 id='plot_varImp'>Variable importance plot</h2><span id='topic+plot_varImp'></span>

<h3>Description</h3>

<p>Plot of variable importance of coefficients of a final fitted
'nestedcv.glmnet' model using ggplot2. Mean expression can be overlaid as the
size of points as this can be informative in models of biological attributes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_varImp(x, abs = TRUE, size = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_varImp_+3A_x">x</code></td>
<td>
<p>a 'nestcv.glmnet' class object</p>
</td></tr>
<tr><td><code id="plot_varImp_+3A_abs">abs</code></td>
<td>
<p>Logical whether to show absolute value of glmnet coefficients</p>
</td></tr>
<tr><td><code id="plot_varImp_+3A_size">size</code></td>
<td>
<p>Logical whether to show mean expression by size of points</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a ggplot2 plot
</p>

<hr>
<h2 id='plot.cva.glmnet'>Plot lambda across range of alphas</h2><span id='topic+plot.cva.glmnet'></span>

<h3>Description</h3>

<p>Different types of plot showing cross-validated tuning of alpha and lambda
from elastic net regression via <a href="glmnet.html#topic+glmnet">glmnet</a>. If <code>xaxis</code> is set to <code>"lambda"</code>,
log lambda is on the x axis while the tuning metric (log loss, deviance,
accuracy, AUC etc) is on the y axis. Multiple alpha values are shown by
different colours. If <code>xaxis</code> is set to <code>"alpha"</code>, alpha is on the x axis
with the tuning metric on y, with error bars showing metric SD. if <code>xaxis</code> is
set to <code>"nvar"</code> the number of non-zero coefficients is shown on x and how
this relates to model deviance/ accuracy on y.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cva.glmnet'
plot(
  x,
  xaxis = c("lambda", "alpha", "nvar"),
  errorBar = (xaxis == "alpha"),
  errorWidth = 0.015,
  min.pch = NULL,
  scheme = NULL,
  palette = "zissou",
  showLegend = "bottomright",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cva.glmnet_+3A_x">x</code></td>
<td>
<p>Object of class 'cva.glmnet'</p>
</td></tr>
<tr><td><code id="plot.cva.glmnet_+3A_xaxis">xaxis</code></td>
<td>
<p>String specifying what is plotted on the x axis, either log
lambda, alpha or the number of non-zero coefficients.</p>
</td></tr>
<tr><td><code id="plot.cva.glmnet_+3A_errorbar">errorBar</code></td>
<td>
<p>Logical whether to control error bars for the standard
deviation of model deviance when <code>xaxis = 'lambda'</code>. Because of overlapping
lines, only the deviance of the top and bottom points at a given lambda are
shown.</p>
</td></tr>
<tr><td><code id="plot.cva.glmnet_+3A_errorwidth">errorWidth</code></td>
<td>
<p>Width of error bars.</p>
</td></tr>
<tr><td><code id="plot.cva.glmnet_+3A_min.pch">min.pch</code></td>
<td>
<p>Plotting 'character' for the minimum point of each curve. Not
shown if set to <code>NULL</code>. See <a href="graphics.html#topic+points">points</a></p>
</td></tr>
<tr><td><code id="plot.cva.glmnet_+3A_scheme">scheme</code></td>
<td>
<p>Colour scheme. Overrides the <code>palette</code> argument.</p>
</td></tr>
<tr><td><code id="plot.cva.glmnet_+3A_palette">palette</code></td>
<td>
<p>Palette name (one of <code>hcl.pals()</code>) which is passed to
<a href="grDevices.html#topic+hcl.colors">hcl.colors</a></p>
</td></tr>
<tr><td><code id="plot.cva.glmnet_+3A_showlegend">showLegend</code></td>
<td>
<p>Either a keyword to position the legend or <code>NULL</code> to hide
the legend.</p>
</td></tr>
<tr><td><code id="plot.cva.glmnet_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <a href="base.html#topic+plot">plot</a>. Use <code>type = 'p'</code> to plot a
scatter plot instead of a line plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value
</p>


<h3>Author(s)</h3>

<p>Myles Lewis
</p>


<h3>See Also</h3>

<p><a href="#topic+nestcv.glmnet">nestcv.glmnet</a>
</p>

<hr>
<h2 id='plot.prc'>Plot precision-recall curve</h2><span id='topic+plot.prc'></span>

<h3>Description</h3>

<p>Plots a precision-recall curve using base graphics. It accepts an S3 object
of class 'prc', see <code><a href="#topic+prc">prc()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'prc'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.prc_+3A_x">x</code></td>
<td>
<p>An object of class 'prc'</p>
</td></tr>
<tr><td><code id="plot.prc_+3A_...">...</code></td>
<td>
<p>Optional graphical arguments passed to <code><a href="base.html#topic+plot">plot()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prc">prc()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mlbench)
data(Sonar)
y &lt;- Sonar$Class
x &lt;- Sonar[, -61]

fit1 &lt;- nestcv.glmnet(y, x, family = "binomial", alphaSet = 1, cv.cores = 2)
fit1$prc &lt;- prc(fit1)  # calculate precision-recall curve

fit2 &lt;- nestcv.train(y, x, method = "gbm", cv.cores = 2)
fit2$prc &lt;- prc(fit2)

plot(fit1$prc)
lines(fit2$prc, col = "red")

</code></pre>

<hr>
<h2 id='pls_filter'>Partial Least Squares filter</h2><span id='topic+pls_filter'></span>

<h3>Description</h3>

<p>Filter using coefficients from partial least squares (PLS) regression to
select optimal predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pls_filter(
  y,
  x,
  force_vars = NULL,
  nfilter,
  ncomp = 5,
  scale_x = TRUE,
  type = c("index", "names", "full"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pls_filter_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="pls_filter_+3A_x">x</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="pls_filter_+3A_force_vars">force_vars</code></td>
<td>
<p>Vector of column names within <code>x</code> which are always retained
in the model (i.e. not filtered). Default <code>NULL</code> means all predictors will
be filtered.</p>
</td></tr>
<tr><td><code id="pls_filter_+3A_nfilter">nfilter</code></td>
<td>
<p>Either a single value for the total number of predictors to
return. Or a vector of length <code>ncomp</code> to manually return predictors from
each PLS component.</p>
</td></tr>
<tr><td><code id="pls_filter_+3A_ncomp">ncomp</code></td>
<td>
<p>the number of components to include in the PLS model.</p>
</td></tr>
<tr><td><code id="pls_filter_+3A_scale_x">scale_x</code></td>
<td>
<p>Logical whether to scale predictors before fitting the PLS
model. This is recommended.</p>
</td></tr>
<tr><td><code id="pls_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices,
&quot;names&quot; returns predictor names, &quot;full&quot; returns a named vector of variable
importance.</p>
</td></tr>
<tr><td><code id="pls_filter_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="pls.html#topic+mvr">pls::plsr()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The best predictors may overlap between components, so if <code>nfilter</code> is
specified as a vector, the total number of unique predictors returned may be
variable.
</p>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (type = &quot;index&quot;) or
character vector of names (type = &quot;names&quot;) of filtered parameters. If
<code>type</code> is <code>"full"</code> full output of coefficients from <code>plsr</code> is returned as a
list for each model component ordered by highest absolute coefficient.
</p>

<hr>
<h2 id='prc'>Build precision-recall curve</h2><span id='topic+prc'></span><span id='topic+prc.default'></span><span id='topic+prc.data.frame'></span><span id='topic+prc.nestcv.glmnet'></span><span id='topic+prc.nestcv.train'></span><span id='topic+prc.nestcv.SuperLearner'></span><span id='topic+prc.outercv'></span><span id='topic+prc.repeatcv'></span>

<h3>Description</h3>

<p>Builds a precision-recall curve for a 'nestedcv' model using <code>prediction()</code>
and <code>performance()</code> functions from the ROCR package and returns an object of
class 'prc' for plotting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prc(...)

## Default S3 method:
prc(response, predictor, positive = 2, ...)

## S3 method for class 'data.frame'
prc(output, ...)

## S3 method for class 'nestcv.glmnet'
prc(object, ...)

## S3 method for class 'nestcv.train'
prc(object, ...)

## S3 method for class 'nestcv.SuperLearner'
prc(object, ...)

## S3 method for class 'outercv'
prc(object, ...)

## S3 method for class 'repeatcv'
prc(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prc_+3A_...">...</code></td>
<td>
<p>other arguments</p>
</td></tr>
<tr><td><code id="prc_+3A_response">response</code></td>
<td>
<p>binary factor vector of response of default order controls,
cases.</p>
</td></tr>
<tr><td><code id="prc_+3A_predictor">predictor</code></td>
<td>
<p>numeric vector of probabilities</p>
</td></tr>
<tr><td><code id="prc_+3A_positive">positive</code></td>
<td>
<p>Either an integer 1 or 2 for the level of response factor
considered to be 'positive' or 'relevant', or a character value for that
factor.</p>
</td></tr>
<tr><td><code id="prc_+3A_output">output</code></td>
<td>
<p>data.frame with columns <code>testy</code> containing observed response
from test folds, and <code>predyp</code> predicted probabilities for classification</p>
</td></tr>
<tr><td><code id="prc_+3A_object">object</code></td>
<td>
<p>a 'nestcv.glmnet', 'nestcv.train', 'nestcv.SuperLearn',
'outercv' or 'repeatcv' S3 class results object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of S3 class 'prc' containing the following fields:
</p>
<table>
<tr><td><code>recall</code></td>
<td>
<p>vector of recall values</p>
</td></tr>
<tr><td><code>precision</code></td>
<td>
<p>vector of precision values</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>
<p>area under precision-recall curve value using trapezoid method</p>
</td></tr>
<tr><td><code>baseline</code></td>
<td>
<p>baseline precision value</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mlbench)
data(Sonar)
y &lt;- Sonar$Class
x &lt;- Sonar[, -61]

fit1 &lt;- nestcv.glmnet(y, x, family = "binomial", alphaSet = 1, cv.cores = 2)

fit1$prc &lt;- prc(fit1)  # calculate precision-recall curve
fit1$prc$auc  # precision-recall AUC value

fit2 &lt;- nestcv.train(y, x, method = "gbm", cv.cores = 2)
fit2$prc &lt;- prc(fit2)
fit2$prc$auc

plot(fit1$prc, ylim = c(0, 1))
lines(fit2$prc, col = "red")

res &lt;- nestcv.glmnet(y, x, family = "binomial", alphaSet = 1) |&gt;
  repeatcv(n = 4, rep.cores = 2)

res$prc &lt;- prc(res)  # precision-recall curve on repeated predictions
plot(res$prc)

</code></pre>

<hr>
<h2 id='pred_nestcv_glmnet'>Prediction wrappers to use fastshap with nestedcv</h2><span id='topic+pred_nestcv_glmnet'></span><span id='topic+pred_nestcv_glmnet_class1'></span><span id='topic+pred_nestcv_glmnet_class2'></span><span id='topic+pred_nestcv_glmnet_class3'></span><span id='topic+pred_train'></span><span id='topic+pred_train_class1'></span><span id='topic+pred_train_class2'></span><span id='topic+pred_train_class3'></span><span id='topic+pred_SuperLearner'></span>

<h3>Description</h3>

<p>Prediction wrapper functions to enable the use of the <code>fastshap</code> package for
generating SHAP values from <code>nestedcv</code> trained models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pred_nestcv_glmnet(x, newdata)

pred_nestcv_glmnet_class1(x, newdata)

pred_nestcv_glmnet_class2(x, newdata)

pred_nestcv_glmnet_class3(x, newdata)

pred_train(x, newdata)

pred_train_class1(x, newdata)

pred_train_class2(x, newdata)

pred_train_class3(x, newdata)

pred_SuperLearner(x, newdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pred_nestcv_glmnet_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code> or <code>nestcv.train</code> object</p>
</td></tr>
<tr><td><code id="pred_nestcv_glmnet_+3A_newdata">newdata</code></td>
<td>
<p>a matrix of new data</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These prediction wrapper functions are designed to be used with the
<code>fastshap</code> package. The functions <code>pred_nestcv_glmnet</code> and <code>pred_train</code> work
for <code>nestcv.glmnet</code> and <code>nestcv.train</code> models respectively for either binary
classification or regression.
</p>
<p>For multiclass classification use <code>pred_nestcv_glmnet_class1</code>, <code>2</code> and <code>3</code>
for the first 3 classes. Similarly <code>pred_train_class1</code> etc for <a href="#topic+nestcv.train">nestcv.train</a>
objects. These functions can be inspected and easily modified to analyse
further classes.
</p>


<h3>Value</h3>

<p>prediction wrapper function designed for use with
<code><a href="fastshap.html#topic+explain">fastshap::explain()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(fastshap)

# Boston housing dataset
library(mlbench)
data(BostonHousing2)
dat &lt;- BostonHousing2
y &lt;- dat$cmedv
x &lt;- subset(dat, select = -c(cmedv, medv, town, chas))

# Fit a glmnet model using nested CV
# Only 3 outer CV folds and 1 alpha value for speed
fit &lt;- nestcv.glmnet(y, x, family = "gaussian", n_outer_folds = 3, alphaSet = 1)

# Generate SHAP values using fastshap::explain
# Only using 5 repeats here for speed, but recommend higher values of nsim
sh &lt;- explain(fit, X=x, pred_wrapper = pred_nestcv_glmnet, nsim = 1)

# Plot overall variable importance
plot_shap_bar(sh, x)

# Plot beeswarm plot
plot_shap_beeswarm(sh, x, size = 1)

</code></pre>

<hr>
<h2 id='predict.cva.glmnet'>Predict method for cva.glmnet models</h2><span id='topic+predict.cva.glmnet'></span>

<h3>Description</h3>

<p>Makes predictions from a cross-validated glmnet model with optimal value of
lambda and alpha.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cva.glmnet'
predict(object, newx, s = "lambda.1se", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cva.glmnet_+3A_object">object</code></td>
<td>
<p>Fitted <code>cva.glmnet</code> object.</p>
</td></tr>
<tr><td><code id="predict.cva.glmnet_+3A_newx">newx</code></td>
<td>
<p>Matrix of new values for <code>x</code> at which predictions are to be made.</p>
</td></tr>
<tr><td><code id="predict.cva.glmnet_+3A_s">s</code></td>
<td>
<p>Value of penalty parameter lambda. Default value is <code>s="lambda.1se"</code>
for consistency with glmnet. Alternatively <code>s="lambda.min"</code> can be used.</p>
</td></tr>
<tr><td><code id="predict.cva.glmnet_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>predict.cv.glmnet()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object returned depends on arguments in <code>...</code> such as <code>type</code>.
</p>

<hr>
<h2 id='predict.hsstan'>Predict from hsstan model fitted within cross-validation</h2><span id='topic+predict.hsstan'></span>

<h3>Description</h3>

<p>Draws from the posterior predictive distribution of the outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hsstan'
predict(object, newdata = NULL, type = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.hsstan_+3A_object">object</code></td>
<td>
<p>An object of class <code>hsstan</code>.</p>
</td></tr>
<tr><td><code id="predict.hsstan_+3A_newdata">newdata</code></td>
<td>
<p>Optional data frame containing the variables to use to
predict. If <code>NULL</code> (default), the model matrix is used. If specified, its
continuous variables should be standardized, since the model coefficients
are learnt on standardized data.</p>
</td></tr>
<tr><td><code id="predict.hsstan_+3A_type">type</code></td>
<td>
<p>Option for binary outcomes only. Default <code>NULL</code> will return a
class with the highest probability for each sample. If set to <code>probs</code>, it
will return the probabilities for outcome = 0 and for outcome = 1 for each
sample.</p>
</td></tr>
<tr><td><code id="predict.hsstan_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to <code>hsstan::posterior_predict</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>For a binary outcome and type = <code>NULL</code>, a character vector with the
name of the class that has the highest probability for each sample.
For a binary outcome and type = <code>prob</code>, a 2-dimensional matrix with the
probability of class 0 and of class 1 for each sample.
For a continuous outcome a numeric vector with the predicted value for
each sample.
</p>


<h3>Author(s)</h3>

<p>Athina Spiliopoulou
</p>

<hr>
<h2 id='predict.nestcv.glmnet'>Predict method for nestcv.glmnet fits</h2><span id='topic+predict.nestcv.glmnet'></span>

<h3>Description</h3>

<p>Obtains predictions from the final fitted model from a <a href="#topic+nestcv.glmnet">nestcv.glmnet</a>
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nestcv.glmnet'
predict(object, newdata, s = object$final_param["lambda"], modify = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.nestcv.glmnet_+3A_object">object</code></td>
<td>
<p>Fitted <code>nestcv.glmnet</code> object</p>
</td></tr>
<tr><td><code id="predict.nestcv.glmnet_+3A_newdata">newdata</code></td>
<td>
<p>New data to predict outcome on</p>
</td></tr>
<tr><td><code id="predict.nestcv.glmnet_+3A_s">s</code></td>
<td>
<p>Value of lambda for glmnet prediction</p>
</td></tr>
<tr><td><code id="predict.nestcv.glmnet_+3A_modify">modify</code></td>
<td>
<p>Logical whether to modify <code>newdata</code> based on <code>modifyX</code>
function. See <code>modifyX</code> and <code>modifyX_useY</code> arguments in <code><a href="#topic+nestcv.glmnet">nestcv.glmnet()</a></code>.</p>
</td></tr>
<tr><td><code id="predict.nestcv.glmnet_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>predict.glmnet</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Checks for missing predictors and if these are sparse (i.e. have
zero coefficients) columns of 0 are automatically added to enable
prediction to proceed.
</p>


<h3>Value</h3>

<p>Object returned depends on the <code>...</code> argument passed to predict
method for <code>glmnet</code> objects.
</p>


<h3>See Also</h3>

<p><a href="glmnet.html#topic+glmnet">glmnet::glmnet</a>
</p>

<hr>
<h2 id='predSummary'>Summarise prediction performance metrics</h2><span id='topic+predSummary'></span>

<h3>Description</h3>

<p>Quick function to calculate performance metrics: confusion matrix, accuracy
and balanced accuracy for classification; ROC AUC for binary classification;
RMSE and R^2 for regression. Multi-class AUC is returned for multinomial
classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predSummary(output, family = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predSummary_+3A_output">output</code></td>
<td>
<p>data.frame with columns <code>testy</code> containing observed response
from test folds; <code>predy</code> predicted response; <code>predyp</code> (optional) predicted
probabilities for classification to calculate ROC AUC</p>
</td></tr>
<tr><td><code id="predSummary_+3A_family">family</code></td>
<td>
<p>Optional character value to support specific glmnet models e.g.
'mgaussian', 'cox'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For multinomial classification, multi-class AUC as defined by Hand and Till
is calculated using <code><a href="pROC.html#topic+multiclass">pROC::multiclass.roc()</a></code>.
</p>


<h3>Value</h3>

<p>An object of class 'predSummary'. For classification a list is
returned containing the confusion matrix table and a vector containing
accuracy and balanced accuracy for classification, ROC AUC for
classification. For regression a vector containing RMSE and R^2 is
returned.
</p>

<hr>
<h2 id='randomsample'>Oversampling and undersampling</h2><span id='topic+randomsample'></span>

<h3>Description</h3>

<p>Random oversampling of the minority group(s) or undersampling of the majority
group to compensate for class imbalance in datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randomsample(y, x, minor = NULL, major = 1, yminor = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randomsample_+3A_y">y</code></td>
<td>
<p>Vector of response outcome as a factor</p>
</td></tr>
<tr><td><code id="randomsample_+3A_x">x</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="randomsample_+3A_minor">minor</code></td>
<td>
<p>Amount of oversampling of the minority class. If set to <code>NULL</code>
then all classes will be oversampled up to the number of samples in the
majority class. To turn off oversampling set <code>minor = 1</code>.</p>
</td></tr>
<tr><td><code id="randomsample_+3A_major">major</code></td>
<td>
<p>Amount of undersampling of the majority class</p>
</td></tr>
<tr><td><code id="randomsample_+3A_yminor">yminor</code></td>
<td>
<p>Optional character value specifying the level in <code>y</code> which is
to be oversampled. If <code>NULL</code>, this is set automatically to the class with
the smallest sample size.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>minor</code> &lt; 1 and <code>major</code> &gt; 1 are ignored.
</p>


<h3>Value</h3>

<p>List containing extended matrix <code>x</code> of synthesised data and extended
response vector <code>y</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Imbalanced dataset
set.seed(1, "L'Ecuyer-CMRG")
x &lt;- matrix(rnorm(150 * 2e+04), 150, 2e+04)  #' predictors
y &lt;- factor(rbinom(150, 1, 0.2))  #' imbalanced binary response
table(y)

## first 30 parameters are weak predictors
x[, 1:30] &lt;- rnorm(150 * 30, 0, 1) + as.numeric(y)*0.5

## Balance x &amp; y outside of CV loop by random oversampling minority group
out &lt;- randomsample(y, x)
y2 &lt;- out$y
x2 &lt;- out$x
table(y2)

## Nested CV glmnet with unnested balancing by random oversampling on
## whole dataset
fit1 &lt;- nestcv.glmnet(y2, x2, family = "binomial", alphaSet = 1,
                      cv.cores=2,
                      filterFUN = ttest_filter)
fit1$summary

## Balance x &amp; y outside of CV loop by random oversampling minority group
out &lt;- randomsample(y, x, minor=1, major=0.4)
y2 &lt;- out$y
x2 &lt;- out$x
table(y2)

## Nested CV glmnet with unnested balancing by random undersampling on
## whole dataset
fit1b &lt;- nestcv.glmnet(y2, x2, family = "binomial", alphaSet = 1,
                       cv.cores=2,
                       filterFUN = ttest_filter)
fit1b$summary

## Balance x &amp; y outside of CV loop by SMOTE
out &lt;- smote(y, x)
y2 &lt;- out$y
x2 &lt;- out$x
table(y2)

## Nested CV glmnet with unnested balancing by SMOTE on whole dataset
fit2 &lt;- nestcv.glmnet(y2, x2, family = "binomial", alphaSet = 1,
                      cv.cores=2,
                      filterFUN = ttest_filter)
fit2$summary

## Nested CV glmnet with nested balancing by random oversampling
fit3 &lt;- nestcv.glmnet(y, x, family = "binomial", alphaSet = 1,
                      cv.cores=2,
                      balance = "randomsample",
                      filterFUN = ttest_filter)
fit3$summary
class_balance(fit3)

## Plot ROC curves
plot(fit1$roc, col='green')
lines(fit1b$roc, col='red')
lines(fit2$roc, col='blue')
lines(fit3$roc)
legend('bottomright', legend = c("Unnested random oversampling", 
                                 "Unnested SMOTE",
                                 "Unnested random undersampling",
                                 "Nested balancing"), 
       col = c("green", "blue", "red", "black"), lty=1, lwd=2)


</code></pre>

<hr>
<h2 id='ranger_filter'>Random forest ranger filter</h2><span id='topic+ranger_filter'></span>

<h3>Description</h3>

<p>Fits a random forest model via the <code>ranger</code> package and ranks variables by
variable importance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ranger_filter(
  y,
  x,
  nfilter = NULL,
  type = c("index", "names", "full"),
  num.trees = 1000,
  mtry = ncol(x) * 0.2,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ranger_filter_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="ranger_filter_+3A_x">x</code></td>
<td>
<p>Matrix or dataframe of predictors</p>
</td></tr>
<tr><td><code id="ranger_filter_+3A_nfilter">nfilter</code></td>
<td>
<p>Number of predictors to return. If <code>NULL</code> all predictors are
returned.</p>
</td></tr>
<tr><td><code id="ranger_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices,
&quot;names&quot; returns predictor names, &quot;full&quot; returns a named vector of variable
importance.</p>
</td></tr>
<tr><td><code id="ranger_filter_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees to grow. See <a href="ranger.html#topic+ranger">ranger::ranger</a>.</p>
</td></tr>
<tr><td><code id="ranger_filter_+3A_mtry">mtry</code></td>
<td>
<p>Number of predictors randomly sampled as candidates at each
split. See <a href="ranger.html#topic+ranger">ranger::ranger</a>.</p>
</td></tr>
<tr><td><code id="ranger_filter_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to <a href="ranger.html#topic+ranger">ranger::ranger</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This filter uses the <code>ranger()</code> function from the <code>ranger</code> package. Variable
importance is calculated using mean decrease in gini impurity.
</p>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (type = &quot;index&quot;) or
character vector of names (type = &quot;names&quot;) of filtered parameters. If
<code>type</code> is <code>"full"</code> a named vector of variable importance is returned.
</p>

<hr>
<h2 id='relieff_filter'>ReliefF filter</h2><span id='topic+relieff_filter'></span>

<h3>Description</h3>

<p>Uses ReliefF algorithm from the CORElearn package to rank predictors in order
of importance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relieff_filter(
  y,
  x,
  nfilter = NULL,
  estimator = "ReliefFequalK",
  type = c("index", "names", "full"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relieff_filter_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="relieff_filter_+3A_x">x</code></td>
<td>
<p>Matrix or dataframe of predictors</p>
</td></tr>
<tr><td><code id="relieff_filter_+3A_nfilter">nfilter</code></td>
<td>
<p>Number of predictors to return. If <code>NULL</code> all predictors are
returned.</p>
</td></tr>
<tr><td><code id="relieff_filter_+3A_estimator">estimator</code></td>
<td>
<p>Type of algorithm used, see <a href="CORElearn.html#topic+attrEval">CORElearn::attrEval</a></p>
</td></tr>
<tr><td><code id="relieff_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices,
&quot;names&quot; returns predictor names, &quot;full&quot; returns a named vector of variable
importance.</p>
</td></tr>
<tr><td><code id="relieff_filter_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <a href="CORElearn.html#topic+attrEval">CORElearn::attrEval</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (type = &quot;index&quot;) or
character vector of names (type = &quot;names&quot;) of filtered parameters. If
<code>type</code> is <code>"full"</code> a named vector of variable importance is returned.
</p>


<h3>See Also</h3>

<p><code><a href="CORElearn.html#topic+attrEval">CORElearn::attrEval()</a></code>
</p>

<hr>
<h2 id='repeatcv'>Repeated nested CV</h2><span id='topic+repeatcv'></span>

<h3>Description</h3>

<p>Performs repeated calls to a <code>nestedcv</code> model to determine performance across
repeated runs of nested CV.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>repeatcv(
  expr,
  n = 5,
  repeat_folds = NULL,
  keep = TRUE,
  extra = FALSE,
  progress = TRUE,
  rep.cores = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="repeatcv_+3A_expr">expr</code></td>
<td>
<p>An expression containing a call to <code><a href="#topic+nestcv.glmnet">nestcv.glmnet()</a></code>,
<code><a href="#topic+nestcv.train">nestcv.train()</a></code>, <code><a href="#topic+nestcv.SuperLearner">nestcv.SuperLearner()</a></code> or <code><a href="#topic+outercv">outercv()</a></code>.</p>
</td></tr>
<tr><td><code id="repeatcv_+3A_n">n</code></td>
<td>
<p>Number of repeats</p>
</td></tr>
<tr><td><code id="repeatcv_+3A_repeat_folds">repeat_folds</code></td>
<td>
<p>Optional list containing fold indices to be applied to
the outer CV folds.</p>
</td></tr>
<tr><td><code id="repeatcv_+3A_keep">keep</code></td>
<td>
<p>Logical whether to save repeated outer CV predictions for ROC
curves etc.</p>
</td></tr>
<tr><td><code id="repeatcv_+3A_extra">extra</code></td>
<td>
<p>Logical whether additional performance metrics are gathered for
binary classification models. See <code><a href="#topic+metrics">metrics()</a></code>.</p>
</td></tr>
<tr><td><code id="repeatcv_+3A_progress">progress</code></td>
<td>
<p>Logical whether to show progress.</p>
</td></tr>
<tr><td><code id="repeatcv_+3A_rep.cores">rep.cores</code></td>
<td>
<p>Integer specifying number of cores/threads to invoke.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We recommend using this with the R pipe <code style="white-space: pre;">&#8288;|&gt;&#8288;</code> (see examples).
</p>
<p>When comparing models, it is recommended to fix the sets of outer CV folds
used across each repeat for comparing performance between models. The
function <code><a href="#topic+repeatfolds">repeatfolds()</a></code> can be used to create a fixed set of outer CV folds
for each repeat.
</p>
<p>Parallelisation over repeats is performed using <code>parallel::mclapply</code> (not
available on windows). Beware that <code>cv.cores</code> can still be set within calls
to <code>nestedcv</code> models (= nested parallelisation). This means that <code>rep.cores</code>
x <code>cv.cores</code> number of processes/forks will be spawned, so be careful not to
overload your CPU. In general parallelisation of repeats using <code>rep.cores</code> is
faster than parallelisation using <code>cv.cores</code>.
</p>


<h3>Value</h3>

<p>List of S3 class 'repeatcv' containing:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the model call</p>
</td></tr>
<tr><td><code>result</code></td>
<td>
<p>matrix of performance metrics</p>
</td></tr>
<tr><td><code>output</code></td>
<td>
<p>(if <code>keep = TRUE</code>) a matrix or dataframe containing the outer CV
predictions from each repeat</p>
</td></tr>
<tr><td><code>roc</code></td>
<td>
<p>(binary classification models only) a ROC curve object based on
predictions across all repeats as returned in <code>output</code>, generated by
<code>pROC::roc()</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("iris")
dat &lt;- iris
y &lt;- dat$Species
x &lt;- dat[, 1:4]

res &lt;- nestcv.glmnet(y, x, family = "multinomial", alphaSet = 1,
                     n_outer_folds = 4) |&gt;
       repeatcv(3, rep.cores = 2)
res
summary(res)

## set up fixed fold indices
set.seed(123, "L'Ecuyer-CMRG")
folds &lt;- repeatfolds(y, repeats = 3, n_outer_folds = 4)
res &lt;- nestcv.glmnet(y, x, family = "multinomial", alphaSet = 1,
                     n_outer_folds = 4) |&gt;
       repeatcv(3, repeat_folds = folds, rep.cores = 2)
res

</code></pre>

<hr>
<h2 id='repeatfolds'>Create folds for repeated nested CV</h2><span id='topic+repeatfolds'></span>

<h3>Description</h3>

<p>Create folds for repeated nested CV
</p>


<h3>Usage</h3>

<pre><code class='language-R'>repeatfolds(y, repeats = 5, n_outer_folds = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="repeatfolds_+3A_y">y</code></td>
<td>
<p>Outcome vector</p>
</td></tr>
<tr><td><code id="repeatfolds_+3A_repeats">repeats</code></td>
<td>
<p>Number of repeats</p>
</td></tr>
<tr><td><code id="repeatfolds_+3A_n_outer_folds">n_outer_folds</code></td>
<td>
<p>Number of outer CV folds</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing indices of outer CV folds
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("iris")
dat &lt;- iris
y &lt;- dat$Species
x &lt;- dat[, 1:4]

## set up fixed fold indices
set.seed(123, "L'Ecuyer-CMRG")
folds &lt;- repeatfolds(y, repeats = 3, n_outer_folds = 4)

res &lt;- nestcv.glmnet(y, x, family = "multinomial", alphaSet = 1,
                     n_outer_folds = 4, cv.cores = 2) |&gt;
       repeatcv(3, repeat_folds = folds)
res

</code></pre>

<hr>
<h2 id='rf_filter'>Random forest filter</h2><span id='topic+rf_filter'></span>

<h3>Description</h3>

<p>Fits a random forest model and ranks variables by variable importance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rf_filter(
  y,
  x,
  nfilter = NULL,
  type = c("index", "names", "full"),
  ntree = 1000,
  mtry = ncol(x) * 0.2,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rf_filter_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="rf_filter_+3A_x">x</code></td>
<td>
<p>Matrix or dataframe of predictors</p>
</td></tr>
<tr><td><code id="rf_filter_+3A_nfilter">nfilter</code></td>
<td>
<p>Number of predictors to return. If <code>NULL</code> all predictors are
returned.</p>
</td></tr>
<tr><td><code id="rf_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices,
&quot;names&quot; returns predictor names, &quot;full&quot; returns a named vector of variable
importance.</p>
</td></tr>
<tr><td><code id="rf_filter_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees to grow. See <a href="randomForest.html#topic+randomForest">randomForest::randomForest</a>.</p>
</td></tr>
<tr><td><code id="rf_filter_+3A_mtry">mtry</code></td>
<td>
<p>Number of predictors randomly sampled as candidates at each
split. See <a href="randomForest.html#topic+randomForest">randomForest::randomForest</a>.</p>
</td></tr>
<tr><td><code id="rf_filter_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to <a href="randomForest.html#topic+randomForest">randomForest::randomForest</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This filter uses the <code>randomForest()</code> function from the <code>randomForest</code>
package. Variable importance is calculated using the
<a href="randomForest.html#topic+importance">randomForest::importance</a> function, specifying type 1 = mean decrease in
accuracy. See <a href="randomForest.html#topic+importance">randomForest::importance</a>.
</p>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (type = &quot;index&quot;) or
character vector of names (type = &quot;names&quot;) of filtered parameters. If
<code>type</code> is <code>"full"</code> a named vector of variable importance is returned.
</p>

<hr>
<h2 id='smote'>SMOTE</h2><span id='topic+smote'></span>

<h3>Description</h3>

<p>Synthetic Minority Oversampling Technique (SMOTE) algorithm for imbalanced
classification data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smote(y, x, k = 5, over = NULL, yminor = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smote_+3A_y">y</code></td>
<td>
<p>Vector of response outcome as a factor</p>
</td></tr>
<tr><td><code id="smote_+3A_x">x</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="smote_+3A_k">k</code></td>
<td>
<p>Range of KNN to consider for generation of new data</p>
</td></tr>
<tr><td><code id="smote_+3A_over">over</code></td>
<td>
<p>Amount of oversampling of the minority class. If set to <code>NULL</code>
then all classes will be oversampled up to the number of samples in the
majority class.</p>
</td></tr>
<tr><td><code id="smote_+3A_yminor">yminor</code></td>
<td>
<p>Optional character value specifying the level in <code>y</code> which is
to be oversampled. If <code>NULL</code>, this is set automatically to the class with
the smallest sample size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing extended matrix <code>x</code> of synthesised data and extended
response vector <code>y</code>
</p>


<h3>References</h3>

<p>Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. (2002).
<em>Smote: Synthetic minority over-sampling technique</em>. Journal of
Artificial Intelligence Research, 16:321-357.
</p>

<hr>
<h2 id='stat_filter'>Univariate filter for binary classification with mixed predictor datatypes</h2><span id='topic+stat_filter'></span><span id='topic+bin_stat_filter'></span><span id='topic+class_stat_filter'></span><span id='topic+cor_stat_filter'></span>

<h3>Description</h3>

<p>Univariate statistic filter for dataframes of predictors with mixed numeric
and categorical datatypes. Different statistical tests are used depending on
the data type of response vector and predictors:
</p>

<dl>
<dt>Binary class response: <code>bin_stat_filter()</code></dt><dd><p>t-test for continuous
data, chi-squared test for categorical data</p>
</dd>
<dt>Multiclass response: <code>class_stat_filter()</code></dt><dd><p>one-way ANOVA for
continuous data, chi-squared test for categorical data</p>
</dd>
<dt>Continuous response: <code>cor_stat_filter()</code></dt><dd><p>correlation (or linear
regression) for continuous data and binary data, one-way ANOVA for
categorical data</p>
</dd>
</dl>



<h3>Usage</h3>

<pre><code class='language-R'>stat_filter(y, x, ...)

bin_stat_filter(
  y,
  x,
  force_vars = NULL,
  nfilter = NULL,
  p_cutoff = 0.05,
  rsq_cutoff = NULL,
  type = c("index", "names", "full", "list"),
  ...
)

class_stat_filter(
  y,
  x,
  force_vars = NULL,
  nfilter = NULL,
  p_cutoff = 0.05,
  rsq_cutoff = NULL,
  type = c("index", "names", "full", "list"),
  ...
)

cor_stat_filter(
  y,
  x,
  cor_method = c("pearson", "spearman", "lm"),
  force_vars = NULL,
  nfilter = NULL,
  p_cutoff = 0.05,
  rsq_cutoff = NULL,
  rsq_method = "pearson",
  type = c("index", "names", "full", "list"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat_filter_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="stat_filter_+3A_x">x</code></td>
<td>
<p>Matrix or dataframe of predictors</p>
</td></tr>
<tr><td><code id="stat_filter_+3A_...">...</code></td>
<td>
<p>optional arguments, e.g. <code>rsq_method</code>: see <code><a href="#topic+collinear">collinear()</a></code>.</p>
</td></tr>
<tr><td><code id="stat_filter_+3A_force_vars">force_vars</code></td>
<td>
<p>Vector of column names within <code>x</code> which are always retained
in the model (i.e. not filtered). Default <code>NULL</code> means all predictors will
be passed to <code>filterFUN</code>.</p>
</td></tr>
<tr><td><code id="stat_filter_+3A_nfilter">nfilter</code></td>
<td>
<p>Number of predictors to return. If <code>NULL</code> all predictors with
p-values &lt; <code>p_cutoff</code> are returned.</p>
</td></tr>
<tr><td><code id="stat_filter_+3A_p_cutoff">p_cutoff</code></td>
<td>
<p>p value cut-off</p>
</td></tr>
<tr><td><code id="stat_filter_+3A_rsq_cutoff">rsq_cutoff</code></td>
<td>
<p>r^2 cutoff for removing predictors due to collinearity.
Default <code>NULL</code> means no collinearity filtering. Predictors are ranked based
on t-test. If 2 or more predictors are collinear, the first ranked
predictor by t-test is retained, while the other collinear predictors are
removed. See <code><a href="#topic+collinear">collinear()</a></code>.</p>
</td></tr>
<tr><td><code id="stat_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices, &quot;names&quot;
returns predictor names, &quot;full&quot; returns a dataframe of statistics, &quot;list&quot;
returns a list of 2 matrices of statistics, one for continuous predictors,
one for categorical predictors.</p>
</td></tr>
<tr><td><code id="stat_filter_+3A_cor_method">cor_method</code></td>
<td>
<p>For <code>cor_stat_filter()</code> only, either <code>"pearson"</code>,
<code>"spearman"</code> or <code>"lm"</code> controlling whether continuous predictors are
filtered by correlation (faster) or regression (slower but allows inclusion
of covariates via <code>force_vars</code>).</p>
</td></tr>
<tr><td><code id="stat_filter_+3A_rsq_method">rsq_method</code></td>
<td>
<p>character string indicating which correlation coefficient
is to be computed. One of &quot;pearson&quot; (default), &quot;kendall&quot;, or &quot;spearman&quot;.
See <code><a href="#topic+collinear">collinear()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>stat_filter()</code> is a wrapper which calls <code>bin_stat_filter()</code>,
<code>class_stat_filter()</code> or <code>cor_stat_filter()</code> depending on whether <code>y</code> is
binary, multiclass or continuous respectively. Ordered factors are converted
to numeric (integer) levels and analysed as if continuous.
</p>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (type = &quot;index&quot;) or
character vector of names (type = &quot;names&quot;) of filtered parameters in order
of test p-value. If <code>type</code> is <code>"full"</code> full output is
returned containing a dataframe of statistical results. If <code>type</code> is
<code>"list"</code> the output is returned as a list of 2 matrices containing
statistical results separated by continuous and categorical predictors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mlbench)
data(BostonHousing2)
dat &lt;- BostonHousing2
y &lt;- dat$cmedv  ## continuous outcome
x &lt;- subset(dat, select = -c(cmedv, medv, town))

stat_filter(y, x, type = "full")
stat_filter(y, x, nfilter = 5, type = "names")
stat_filter(y, x)

data(iris)
y &lt;- iris$Species  ## 3 class outcome
x &lt;- subset(iris, select = -Species)
stat_filter(y, x, type = "full")

</code></pre>

<hr>
<h2 id='summary_vars'>Summarise variables</h2><span id='topic+summary_vars'></span>

<h3>Description</h3>

<p>Summarise variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summary_vars(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary_vars_+3A_x">x</code></td>
<td>
<p>Matrix or dataframe with variables in columns</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with variables in rows and mean, median and SD for each
variable or number of levels if the variable is a factor. If <code>NA</code> are
detected, an extra column <code>n.NA</code> is added with the numbers of <code>NA</code> for each
variable.
</p>

<hr>
<h2 id='supervisedPCA'>Supervised PCA plot</h2><span id='topic+supervisedPCA'></span>

<h3>Description</h3>

<p>Performs supervised principle component analysis (PCA) after filtering
dataset to help determine whether filtering has been useful for separating
samples according to the outcome variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>supervisedPCA(y, x, filterFUN = NULL, filter_options = NULL, plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="supervisedPCA_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="supervisedPCA_+3A_x">x</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="supervisedPCA_+3A_filterfun">filterFUN</code></td>
<td>
<p>Filter function, e.g. <a href="#topic+ttest_filter">ttest_filter</a> or <a href="#topic+relieff_filter">relieff_filter</a>.
Any function can be provided and is passed <code>y</code> and <code>x</code>. Must return a
character vector with names of filtered predictors.</p>
</td></tr>
<tr><td><code id="supervisedPCA_+3A_filter_options">filter_options</code></td>
<td>
<p>List of additional arguments passed to the filter
function specified by <code>filterFUN</code>.</p>
</td></tr>
<tr><td><code id="supervisedPCA_+3A_plot">plot</code></td>
<td>
<p>Logical whether to plot a ggplot2 object or return the PC scores</p>
</td></tr>
<tr><td><code id="supervisedPCA_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to <code><a href="stats.html#topic+princomp">princomp()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>plot=TRUE</code> returns a ggplot2 plot, otherwise returns the
principle component scores.
</p>

<hr>
<h2 id='train_preds'>Outer training fold predictions</h2><span id='topic+train_preds'></span>

<h3>Description</h3>

<p>Obtain predictions on outer training folds which can be used for performance
metrics and ROC curves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_preds(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_preds_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code>, <code>nestcv.train</code> or <code>outercv</code> fitted object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: the argument <code>outer_train_predict</code> must be set to <code>TRUE</code> in
the original call to either <code>nestcv.glmnet</code>, <code>nestcv.train</code> or <code>outercv</code>.
</p>


<h3>Value</h3>

<p>Dataframe with columns <code>ytrain</code> and <code>predy</code> containing observed and
predicted values from training folds. For binomial and multinomial models
additional columns are added with class probabilities or log likelihood
values.
</p>

<hr>
<h2 id='train_roc'>Build ROC curve from outer CV training folds</h2><span id='topic+train_roc'></span>

<h3>Description</h3>

<p>Build ROC (receiver operating characteristic) curve from outer training
folds. Object can be plotted using <code>plot()</code> or passed to functions <code><a href="glmnet.html#topic+auc">auc()</a></code>
etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_roc(x, direction = "&lt;", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_roc_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code>, <code>nestcv.train</code> or <code>outercv</code> object</p>
</td></tr>
<tr><td><code id="train_roc_+3A_direction">direction</code></td>
<td>
<p>Set ROC directionality <a href="pROC.html#topic+roc">pROC::roc</a></p>
</td></tr>
<tr><td><code id="train_roc_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <a href="pROC.html#topic+roc">pROC::roc</a></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: the argument <code>outer_train_predict</code> must be set to <code>TRUE</code> in
the original call to either <code>nestcv.glmnet</code>, <code>nestcv.train</code> or <code>outercv</code>.
</p>


<h3>Value</h3>

<p><code>"roc"</code> object, see <a href="pROC.html#topic+roc">pROC::roc</a>
</p>

<hr>
<h2 id='train_summary'>Summarise performance on outer training folds</h2><span id='topic+train_summary'></span>

<h3>Description</h3>

<p>Calculates performance metrics on outer training folds: confusion matrix,
accuracy and balanced accuracy for classification; ROC AUC for binary
classification; RMSE, R^2 and mean absolute error (MAE) for regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_summary(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_summary_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code>, <code>nestcv.train</code> or <code>outercv</code> object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: the argument <code>outer_train_predict</code> must be set to <code>TRUE</code> in
the original call to either <code>nestcv.glmnet</code>, <code>nestcv.train</code> or <code>outercv</code>.
</p>


<h3>Value</h3>

<p>Returns performance metrics from outer training folds, see
<a href="#topic+predSummary">predSummary</a>
</p>


<h3>See Also</h3>

<p><a href="#topic+predSummary">predSummary</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
x &lt;- iris[, 1:4]
y &lt;- iris[, 5]

fit &lt;- nestcv.glmnet(y, x,
                     family = "multinomial",
                     alpha = 1,
                     outer_train_predict = TRUE,
                     n_outer_folds = 3)
summary(fit)
innercv_summary(fit)
train_summary(fit)

fit2 &lt;- nestcv.train(y, x,
                    model="svm",
                    outer_train_predict = TRUE,
                    n_outer_folds = 3,
                    cv.cores = 2)
summary(fit2)
innercv_summary(fit2)
train_summary(fit2)

</code></pre>

<hr>
<h2 id='ttest_filter'>Univariate filters</h2><span id='topic+ttest_filter'></span><span id='topic+anova_filter'></span><span id='topic+wilcoxon_filter'></span><span id='topic+correl_filter'></span>

<h3>Description</h3>

<p>A selection of simple univariate filters using t-test, Wilcoxon test, one-way
ANOVA or correlation (Pearson or Spearman) for ranking variables. These
filters are designed for speed. <code>ttest_filter</code> uses the <code>Rfast</code> package,
<code>wilcoxon_filter</code> (Mann-Whitney) test uses
<a href="matrixTests.html#topic+wilcoxon">matrixTests::row_wilcoxon_twosample</a>, <code>anova_filter</code> uses
<a href="matrixTests.html#topic+oneway">matrixTests::col_oneway_welch</a> (Welch's F-test) from the <code>matrixTests</code>
package. Can be applied to all or a subset of predictors. For mixed datasets
(combined continuous &amp; categorical) see <code><a href="#topic+stat_filter">stat_filter()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttest_filter(
  y,
  x,
  force_vars = NULL,
  nfilter = NULL,
  p_cutoff = 0.05,
  rsq_cutoff = NULL,
  type = c("index", "names", "full"),
  keep_factors = TRUE,
  ...
)

anova_filter(
  y,
  x,
  force_vars = NULL,
  nfilter = NULL,
  p_cutoff = 0.05,
  rsq_cutoff = NULL,
  type = c("index", "names", "full"),
  keep_factors = TRUE,
  ...
)

wilcoxon_filter(
  y,
  x,
  force_vars = NULL,
  nfilter = NULL,
  p_cutoff = 0.05,
  rsq_cutoff = NULL,
  type = c("index", "names", "full"),
  exact = FALSE,
  keep_factors = TRUE,
  ...
)

correl_filter(
  y,
  x,
  method = "pearson",
  force_vars = NULL,
  nfilter = NULL,
  p_cutoff = 0.05,
  rsq_cutoff = NULL,
  type = c("index", "names", "full"),
  keep_factors = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ttest_filter_+3A_y">y</code></td>
<td>
<p>Response vector</p>
</td></tr>
<tr><td><code id="ttest_filter_+3A_x">x</code></td>
<td>
<p>Matrix or dataframe of predictors</p>
</td></tr>
<tr><td><code id="ttest_filter_+3A_force_vars">force_vars</code></td>
<td>
<p>Vector of column names within <code>x</code> which are always retained
in the model (i.e. not filtered). Default <code>NULL</code> means all predictors will
be passed to <code>filterFUN</code>.</p>
</td></tr>
<tr><td><code id="ttest_filter_+3A_nfilter">nfilter</code></td>
<td>
<p>Number of predictors to return. If <code>NULL</code> all predictors with
p-values &lt; <code>p_cutoff</code> are returned.</p>
</td></tr>
<tr><td><code id="ttest_filter_+3A_p_cutoff">p_cutoff</code></td>
<td>
<p>p value cut-off</p>
</td></tr>
<tr><td><code id="ttest_filter_+3A_rsq_cutoff">rsq_cutoff</code></td>
<td>
<p>r^2 cutoff for removing predictors due to collinearity.
Default <code>NULL</code> means no collinearity filtering. Predictors are ranked based
on t-test. If 2 or more predictors are collinear, the first ranked
predictor by t-test is retained, while the other collinear predictors are
removed. See <code><a href="#topic+collinear">collinear()</a></code>.</p>
</td></tr>
<tr><td><code id="ttest_filter_+3A_type">type</code></td>
<td>
<p>Type of vector returned. Default &quot;index&quot; returns indices, &quot;names&quot;
returns predictor names, &quot;full&quot; returns a matrix of p values.</p>
</td></tr>
<tr><td><code id="ttest_filter_+3A_keep_factors">keep_factors</code></td>
<td>
<p>Logical affecting factors with 3 or more levels.
Dataframes are coerced to a matrix using <a href="base.html#topic+data.matrix">data.matrix</a>. Binary
factors are converted to numeric values 0/1 and analysed as such. If
<code>keep_factors</code> is <code>TRUE</code> (the default), factors with 3 or more levels are
not filtered and are retained. If <code>keep_factors</code> is <code>FALSE</code>, they are
removed.</p>
</td></tr>
<tr><td><code id="ttest_filter_+3A_...">...</code></td>
<td>
<p>optional arguments, including <code>rsq_method</code> passed to <code><a href="#topic+collinear">collinear()</a></code>
or arguments passed to <a href="matrixTests.html#topic+wilcoxon">matrixTests::row_wilcoxon_twosample</a> in
<code><a href="#topic+wilcoxon_filter">wilcoxon_filter()</a></code>.</p>
</td></tr>
<tr><td><code id="ttest_filter_+3A_exact">exact</code></td>
<td>
<p>Logical whether exact or approximate p-value is calculated.
Default is <code>FALSE</code> for speed.</p>
</td></tr>
<tr><td><code id="ttest_filter_+3A_method">method</code></td>
<td>
<p>Type of correlation, either &quot;pearson&quot; or &quot;spearman&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer vector of indices of filtered parameters (type = &quot;index&quot;) or
character vector of names (type = &quot;names&quot;) of filtered parameters in order
of t-test p-value. If <code>type</code> is <code>"full"</code> full output from
<a href="Rfast.html#topic+ttests">Rfast::ttests</a> is returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm_filter">lm_filter()</a></code> <code><a href="#topic+stat_filter">stat_filter()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## sigmoid function
sigmoid &lt;- function(x) {1 / (1 + exp(-x))}

## load iris dataset and simulate a binary outcome
data(iris)
dt &lt;- iris[, 1:4]
colnames(dt) &lt;- c("marker1", "marker2", "marker3", "marker4")
dt &lt;- as.data.frame(apply(dt, 2, scale))
y2 &lt;- sigmoid(0.5 * dt$marker1 + 2 * dt$marker2) &gt; runif(nrow(dt))
y2 &lt;- factor(y2, labels = c("C1", "C2"))

ttest_filter(y2, dt)  # returns index of filtered predictors
ttest_filter(y2, dt, type = "name")  # shows names of predictors
ttest_filter(y2, dt, type = "full")  # full results table

data(iris)
dt &lt;- iris[, 1:4]
y3 &lt;- iris[, 5]
anova_filter(y3, dt)  # returns index of filtered predictors
anova_filter(y3, dt, type = "full")  # shows names of predictors
anova_filter(y3, dt, type = "name")  # full results table

</code></pre>

<hr>
<h2 id='txtProgressBar2'>Text Progress Bar 2</h2><span id='topic+txtProgressBar2'></span>

<h3>Description</h3>

<p>Text progress bar in the R console. Modified from <code>utils::txtProgressBar()</code>
to include title and timing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txtProgressBar2(
  min = 0,
  max = 1,
  initial = 0,
  char = "=",
  width = NA,
  title = ""
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="txtProgressBar2_+3A_min">min</code></td>
<td>
<p>Numeric value for minimum of the progress bar.</p>
</td></tr>
<tr><td><code id="txtProgressBar2_+3A_max">max</code></td>
<td>
<p>Numeric value for maximum of the progress bar.</p>
</td></tr>
<tr><td><code id="txtProgressBar2_+3A_initial">initial</code></td>
<td>
<p>Initial value for the progress bar.</p>
</td></tr>
<tr><td><code id="txtProgressBar2_+3A_char">char</code></td>
<td>
<p>The character (or character string) to form the progress bar.</p>
</td></tr>
<tr><td><code id="txtProgressBar2_+3A_width">width</code></td>
<td>
<p>The width of the progress bar, as a multiple of the width of
<code>char</code>. If <code>NA</code>, the default, the number of characters is that which fits
into <code>getOption("width")</code>.</p>
</td></tr>
<tr><td><code id="txtProgressBar2_+3A_title">title</code></td>
<td>
<p>Title for the progress bar.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use <code>utils::setTxtProgressBar()</code> to set the progress bar and <code>close()</code> to
close it.
</p>


<h3>Value</h3>

<p>An object of class &quot;<code>txtProgressBar</code>&quot;.
</p>

<hr>
<h2 id='var_direction'>Variable directionality</h2><span id='topic+var_direction'></span>

<h3>Description</h3>

<p>Determines directionality of final predictors for binary or regression
models, using the sign of the t-statistic or correlation coefficient
respectively for each variable compared to the outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_direction(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_direction_+3A_object">object</code></td>
<td>
<p>a <code>nestcv.glmnet</code> or <code>nestcv.train</code> fitted model</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Categorical features with &gt;2 levels are assumed to have a meaningful order
for the purposes of directionality. Factors are coerced to ordinal using
<code>data.matrix()</code>. If factors are multiclass then directionality results should
be ignored.
</p>


<h3>Value</h3>

<p>named vector showing the directionality of final predictors. If the
response vector is multinomial <code>NULL</code> is returned.
</p>

<hr>
<h2 id='var_stability'>Variable stability</h2><span id='topic+var_stability'></span><span id='topic+var_stability.nestcv.glmnet'></span><span id='topic+var_stability.nestcv.train'></span>

<h3>Description</h3>

<p>Uses variable importance across models trained and tested across outer CV
folds to assess stability of variable importance. For glmnet, variable
importance is measured as the absolute model coefficients, optionally scaled
as a percentage. The frequency with which each variable is selected in outer
folds as well as the final model is also returned which is helpful for sparse
models or with filters to determine how often variables end up in the model
in each fold. For glmnet, the direction of effect is taken directly from the
sign of model coefficients. For <code>caret</code> models, direction of effect is not
readily available, so as a substitute, the directionality of each predictor
is determined by the function <code><a href="#topic+var_direction">var_direction()</a></code> using the sign of a t-test
for binary classification or the sign of regression coefficient for
continuous outcomes (not available for multiclass caret models). To better
understand direction of effect of each predictor within the final model, we
recommend using SHAP values - see the vignette &quot;Explaining nestedcv models
with Shapley values&quot;. See <code><a href="#topic+pred_train">pred_train()</a></code> for an example.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_stability(x, ...)

## S3 method for class 'nestcv.glmnet'
var_stability(x, percent = TRUE, level = 1, sort = TRUE, ...)

## S3 method for class 'nestcv.train'
var_stability(x, sort = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_stability_+3A_x">x</code></td>
<td>
<p>a <code>nestcv.glmnet</code> or <code>nestcv.train</code> fitted object</p>
</td></tr>
<tr><td><code id="var_stability_+3A_...">...</code></td>
<td>
<p>Optional arguments for compatibility</p>
</td></tr>
<tr><td><code id="var_stability_+3A_percent">percent</code></td>
<td>
<p>Logical for <code>nestcv.glmnet</code> objects only, whether to scale
coefficients to percentage of the largest coefficient in each model</p>
</td></tr>
<tr><td><code id="var_stability_+3A_level">level</code></td>
<td>
<p>For multinomial <code>nestcv.glmnet</code> models only, either an integer
specifying which level of outcome is being examined, or the level can be
specified as a character value</p>
</td></tr>
<tr><td><code id="var_stability_+3A_sort">sort</code></td>
<td>
<p>Logical whether to sort variables by mean importance</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that for caret models <code><a href="caret.html#topic+varImp">caret::varImp()</a></code> may require the model package to
be fully loaded in order to function. During the fitting process <code>caret</code>
often only loads the package by namespace.
</p>


<h3>Value</h3>

<p>Dataframe containing mean, sd, sem of variable importance and
frequency by which each variable is selected in outer folds.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv_coef">cv_coef()</a></code> <code><a href="#topic+cv_varImp">cv_varImp()</a></code> <code><a href="#topic+pred_train">pred_train()</a></code>
</p>

<hr>
<h2 id='weight'>Calculate weights for class imbalance</h2><span id='topic+weight'></span>

<h3>Description</h3>

<p>Calculate weights for class imbalance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weight(y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weight_+3A_y">y</code></td>
<td>
<p>Factor or character response vector. If a character vector is
supplied it is coerced into a factor. Unused levels are dropped.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of weights
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
