<!DOCTYPE html><html><head><title>Help for package LexisNexisTools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {LexisNexisTools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#lnt_add'><p>Adds or replaces articles</p></a></li>
<li><a href='#lnt_asDate'><p>Convert Strings to dates</p></a></li>
<li><a href='#lnt_convert'><p>Convert LNToutput to other formats</p></a></li>
<li><a href='#lnt_diff'><p>Display diff of similar articles</p></a></li>
<li><a href='#lnt_lookup'><p>Lookup keywords in articles</p></a></li>
<li><a href='#lnt_read'><p>Read in a LexisNexis file</p></a></li>
<li><a href='#lnt_rename'><p>Assign proper names to LexisNexis files</p></a></li>
<li><a href='#lnt_sample'><p>Provides a small sample TXT/DOCX file</p></a></li>
<li><a href='#lnt_similarity'><p>Check for highly similar articles.</p></a></li>
<li><a href='#lnt2bibtex'><p>Convert LNToutput to other formats</p></a></li>
<li><a href='#LNToutput'><p>An S4 class to store the three data.frames created with lnt_read</p></a></li>
<li><a href='#LNToutput_methods'><p>Methods for LNToutput output objects</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Working with Files from 'LexisNexis'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.7</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-07-05</td>
</tr>
<tr>
<td>Description:</td>
<td>My PhD supervisor once told me that everyone doing newspaper
    analysis starts by writing code to read in files from the 'LexisNexis' newspaper
    archive (retrieved e.g., from <a href="https://www.lexisnexis.com/">https://www.lexisnexis.com/</a> or any of the partner
    sites). However, while this is a nice exercise I do recommend, not everyone has
    the time. This package takes files downloaded from the newspaper archive of
    'LexisNexis', reads them into R and offers functions for further processing.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table (&ge; 1.10.0), methods (&ge; 3.3.0), parallel (&ge;
3.3.0), pbapply (&ge; 1.3.4), quanteda (&ge; 1.1.0),
quanteda.textstats, stats (&ge; 3.3.0), stringdist (&ge; 0.9.4.0),
stringi (&ge; 1.1.7), tibble (&ge; 1.4.0), utils (&ge; 3.3.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>corpustools, covr, diffobj, dplyr, RSQLite, testthat,
tidytext, tm, kableExtra, knitr, pdftools, rmarkdown, spelling,
striprtf, xml2</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/JBGruber/LexisNexisTools">https://github.com/JBGruber/LexisNexisTools</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/JBGruber/LexisNexisTools/issues">https://github.com/JBGruber/LexisNexisTools/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Language:</td>
<td>en-GB</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-07-05 13:17:14 UTC; johannes</td>
</tr>
<tr>
<td>Author:</td>
<td>Johannes Gruber [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Johannes Gruber &lt;JohannesB.Gruber@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-07-05 13:33:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='lnt_add'>Adds or replaces articles</h2><span id='topic+lnt_add'></span>

<h3>Description</h3>

<p>This functions adds a dataframe to a slot in an LNToutput object
or overwrites existing entries. The main use of the function is to add an
extract of one of the data.frames back to an LNToutput object after
operations were performed on it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnt_add(to, what, where = "meta", replace = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnt_add_+3A_to">to</code></td>
<td>
<p>an LNToutput object to which something should be added.</p>
</td></tr>
<tr><td><code id="lnt_add_+3A_what">what</code></td>
<td>
<p>A data.frame which is added.</p>
</td></tr>
<tr><td><code id="lnt_add_+3A_where">where</code></td>
<td>
<p>Either &quot;meta&quot;, &quot;articles&quot; or &quot;paragraphs&quot; to indicate the slot
to which data is added.</p>
</td></tr>
<tr><td><code id="lnt_add_+3A_replace">replace</code></td>
<td>
<p>If TRUE, will overwrite entries which have the same ID as</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note, that when adding paragraphs, the Par_ID column is used to
determine if entries are already present in the set. For the other data
frames the article ID is used.
</p>


<h3>Author(s)</h3>

<p>Johannes Gruber
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Make LNToutput object from sample
LNToutput &lt;- lnt_read(lnt_sample(copy = FALSE))

# extract meta and make corrections
correction &lt;- LNToutput@meta[grepl("Wikipedia", LNToutput@meta$Headline), ]
correction$Newspaper &lt;- "Wikipedia"

# replace corrected meta information
LNToutput &lt;- lnt_add(to = LNToutput, what = correction, where = "meta", replace = TRUE)
</code></pre>

<hr>
<h2 id='lnt_asDate'>Convert Strings to dates</h2><span id='topic+lnt_asDate'></span>

<h3>Description</h3>

<p>Converts dates from string formats common in LexisNexis to a
date object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnt_asDate(x, format = "auto", locale = "auto", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnt_asDate_+3A_x">x</code></td>
<td>
<p>A character object to be converted.</p>
</td></tr>
<tr><td><code id="lnt_asDate_+3A_format">format</code></td>
<td>
<p>Either &quot;auto&quot; to guess the format based on a common order of
day, month and year or provide a custom format (see
<a href="stringi.html#topic+stri_datetime_format">stri_datetime_format</a> for format options).</p>
</td></tr>
<tr><td><code id="lnt_asDate_+3A_locale">locale</code></td>
<td>
<p>A ISO 639-1 locale code (see
<a href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes">https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes</a>).</p>
</td></tr>
<tr><td><code id="lnt_asDate_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns an object of class <a href="base.html#topic+date">date</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>LNToutput &lt;- lnt_read(lnt_sample(copy = FALSE), convert_date = FALSE)
d &lt;- lnt_asDate(LNToutput@meta$Date)
d
</code></pre>

<hr>
<h2 id='lnt_convert'>Convert LNToutput to other formats</h2><span id='topic+lnt_convert'></span><span id='topic+lnt2df'></span><span id='topic+lnt2rDNA'></span><span id='topic+lnt2quanteda'></span><span id='topic+lnt2tm'></span><span id='topic+lnt2cptools'></span><span id='topic+lnt2tidy'></span><span id='topic+lnt2SQLite'></span>

<h3>Description</h3>

<p>Takes output from <a href="#topic+lnt_read">lnt_read</a> and converts it to other formats. You can
either use <code>lnt_convert()</code> and choose the output format via <code>to</code> or
use the individual functions directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnt_convert(
  x,
  to = "data.frame",
  what = "articles",
  collapse = FALSE,
  file = "LNT.sqlite",
  ...
)

lnt2df(x, what = "articles", ...)

lnt2rDNA(x, what = "articles", collapse = TRUE)

lnt2quanteda(x, what = "articles", collapse = NULL, ...)

lnt2tm(x, what = "articles", collapse = NULL, ...)

lnt2cptools(x, what = "articles", ...)

lnt2tidy(x, what = "articles", ...)

lnt2SQLite(x, file = "LNT.sqlite", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnt_convert_+3A_x">x</code></td>
<td>
<p>An object of class LNToutput.</p>
</td></tr>
<tr><td><code id="lnt_convert_+3A_to">to</code></td>
<td>
<p>Which format to convert into. Possible values are &quot;rDNA&quot;,
&quot;corpustools&quot;, &quot;tidytext&quot;, &quot;tm&quot;, &quot;SQLite&quot; and &quot;quanteda&quot;.</p>
</td></tr>
<tr><td><code id="lnt_convert_+3A_what">what</code></td>
<td>
<p>Either &quot;articles&quot; or &quot;paragraphs&quot; to use articles or paragraphs as
text in the output object.</p>
</td></tr>
<tr><td><code id="lnt_convert_+3A_collapse">collapse</code></td>
<td>
<p>Only has an effect when <code>what = "articles"</code>. If set to
TRUE, an empty line will be added after each paragraphs. Alternatively you
can enter a custom string (such as <code>"\n"</code> for newline). <code>NULL</code>
or <code>FALSE</code> turns off this feature.</p>
</td></tr>
<tr><td><code id="lnt_convert_+3A_file">file</code></td>
<td>
<p>The name of the database to be written to (for lnt2SQLite only).</p>
</td></tr>
<tr><td><code id="lnt_convert_+3A_...">...</code></td>
<td>
<p>Passed on to different methods (see details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>lnt_convert() provides conversion methods into several formats
commonly used in prominent R packages for text analysis. Besides the
options set here, the ... (ellipsis) is passed on to the individual methods
for tuning the outcome:
</p>

<ul>
<li><p> data.frame, rDNA ... not used.
</p>
</li>
<li><p> quanteda ... passed on to <code><a href="quanteda.html#topic+corpus">quanteda::corpus()</a></code>.
</p>
</li>
<li><p> corpustools ... passed on to <code><a href="corpustools.html#topic+create_tcorpus">corpustools::create_tcorpus()</a></code>.
</p>
</li>
<li><p> tm ... passed on to <code><a href="tm.html#topic+Corpus">tm::Corpus()</a></code>.
</p>
</li>
<li><p> tidytext ... passed on to <code><a href="tidytext.html#topic+unnest_tokens">tidytext::unnest_tokens()</a></code>.
</p>
</li>
<li><p> lnt2SQLite ... passed on to <code><a href="RSQLite.html#topic+dbWriteTable">RSQLite::dbWriteTable-method()</a></code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>LNToutput &lt;- lnt_read(lnt_sample(copy = FALSE))

df &lt;- lnt_convert(LNToutput, to = "data.frame")

docs &lt;- lnt_convert(LNToutput, to = "rDNA")

corpus &lt;- lnt_convert(LNToutput, to = "quanteda")

## Not run: 

tCorpus &lt;- lnt_convert(LNToutput, to = "corpustools")

tidy &lt;- lnt_convert(LNToutput, to = "tidytext")

Corpus &lt;- lnt_convert(LNToutput, to = "tm")

dbloc &lt;- lnt_convert(LNToutput, to = "SQLite")

## End(Not run)

</code></pre>

<hr>
<h2 id='lnt_diff'>Display diff of similar articles</h2><span id='topic+lnt_diff'></span>

<h3>Description</h3>

<p>This function is a wrapper for <a href="diffobj.html#topic+diffPrint">diffPrint</a>. It is
intended to help performing a manual assessment of the difference between
highly similar articles identified via <a href="#topic+lnt_similarity">lnt_similarity</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnt_diff(x, min = 0.15, max = 0.3, n = 25, output_html = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnt_diff_+3A_x">x</code></td>
<td>
<p>lnt_sim object as returned by <a href="#topic+lnt_similarity">lnt_similarity</a>.</p>
</td></tr>
<tr><td><code id="lnt_diff_+3A_min">min</code></td>
<td>
<p>Minimum value of rel_dist to include in diff.</p>
</td></tr>
<tr><td><code id="lnt_diff_+3A_max">max</code></td>
<td>
<p>Maximum value of rel_dist to include in diff.</p>
</td></tr>
<tr><td><code id="lnt_diff_+3A_n">n</code></td>
<td>
<p>Size of displayed sample.</p>
</td></tr>
<tr><td><code id="lnt_diff_+3A_output_html">output_html</code></td>
<td>
<p>Set to TRUE to output html code, e.g. to use for knitting
an rmarkdown document to html. Chunk option must be set to
<code>results='asis'</code> in that case.</p>
</td></tr>
<tr><td><code id="lnt_diff_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Johannes Gruber
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Test similarity of articles
duplicates.df &lt;- lnt_similarity(
  LNToutput = lnt_read(lnt_sample(copy = FALSE)),
  threshold = 0.97
)

lnt_diff(duplicates.df, min = 0.18, max = 0.30)

## End(Not run)
</code></pre>

<hr>
<h2 id='lnt_lookup'>Lookup keywords in articles</h2><span id='topic+lnt_lookup'></span>

<h3>Description</h3>

<p>This function looks for the provided pattern in the string or
LNToutput object. This can be useful, for example, to see which of the
keywords you used when retrieving the data was used in each article.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnt_lookup(
  x,
  pattern,
  case_insensitive = FALSE,
  unique_pattern = FALSE,
  word_boundaries = c("both", "before", "after"),
  cores = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnt_lookup_+3A_x">x</code></td>
<td>
<p>An LNToutput object or a string or vector of strings.</p>
</td></tr>
<tr><td><code id="lnt_lookup_+3A_pattern">pattern</code></td>
<td>
<p>A character vector of keywords. Word boundaries before and
after the keywords are honoured (see <code>word_boundaries</code>). Regular
expression can be used.</p>
</td></tr>
<tr><td><code id="lnt_lookup_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>If FALSE, the pattern matching is case sensitive and
if TRUE, case is ignored during matching.</p>
</td></tr>
<tr><td><code id="lnt_lookup_+3A_unique_pattern">unique_pattern</code></td>
<td>
<p>If TRUE, duplicated mentions of the same pattern are
removed.</p>
</td></tr>
<tr><td><code id="lnt_lookup_+3A_word_boundaries">word_boundaries</code></td>
<td>
<p>If TRUE or &quot;both&quot;, lookup is performed with word
boundaries at beginning and end of the pattern (i.e., pattern &quot;protest&quot;
will not identify &quot;protesters&quot; etc.). Additionally word boundaries can be
either just in front of the pattern (&quot;before&quot;) or after the pattern
(&quot;after&quot;). FALSE searches without word boundaries.</p>
</td></tr>
<tr><td><code id="lnt_lookup_+3A_cores">cores</code></td>
<td>
<p>The number of CPU cores to use. Use <code>NULL</code> or <code>1</code> to
turn off.</p>
</td></tr>
<tr><td><code id="lnt_lookup_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag indicating whether a status bar is printed to
the screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If an LNToutput object is provided, the function will look for the
pattern in the headlines and articles. The returned object is a list of
hits. If a regular expression is provided, the returned word will be the
actual value from the text.
</p>


<h3>Value</h3>

<p>A list of keyword hits.
</p>


<h3>Author(s)</h3>

<p>Johannes Gruber
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Make LNToutput object from sample
LNToutput &lt;- lnt_read(lnt_sample(copy = FALSE))

# Lookup keywords
LNToutput@meta$Keyword &lt;- lnt_lookup(
  LNToutput,
  "statistical computing"
)

# Keep only articles which mention the keyword
LNToutput_stat &lt;- LNToutput[!sapply(LNToutput@meta$Keyword, is.null)]

# Convert list of keywords to string
LNToutput@meta$Keyword &lt;- sapply(LNToutput@meta$Keyword, toString)
</code></pre>

<hr>
<h2 id='lnt_read'>Read in a LexisNexis file</h2><span id='topic+lnt_read'></span>

<h3>Description</h3>

<p>Read a file from LexisNexis in a supported format and convert it to an object
of class <a href="#topic+LNToutput">LNToutput</a>. Supported formats are TXT, DOC, RTF and PDF files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnt_read(
  x,
  encoding = "UTF-8",
  extract_paragraphs = TRUE,
  convert_date = TRUE,
  start_keyword = "auto",
  end_keyword = "auto",
  length_keyword = "auto",
  author_keyword = "auto",
  exclude_lines = "^LOAD-DATE: |^UPDATE: |^GRAFIK: |^GRAPHIC: |^DATELINE: ",
  recursive = FALSE,
  file_type = c("txt", "rtf", "doc", "pdf", "docx", "zip"),
  remove_cover = TRUE,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnt_read_+3A_x">x</code></td>
<td>
<p>Name(s) of file(s) or one or multiple directories containing files
from LexisNexis to be converted.</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_encoding">encoding</code></td>
<td>
<p>Encoding to be assumed for input files. Defaults to UTF-8
(the LexisNexis standard value).</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_extract_paragraphs">extract_paragraphs</code></td>
<td>
<p>A logical flag indicating if the returned object
will include a third data frame with paragraphs.</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_convert_date">convert_date</code></td>
<td>
<p>A logical flag indicating if it should be tried to
convert the date of each article into Date format. For non-standard dates
provided by LexisNexis it might be safer to convert dates afterwards (see
<a href="#topic+lnt_asDate">lnt_asDate</a>).</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_start_keyword">start_keyword</code></td>
<td>
<p>Is used to indicate the beginning of an article. All
articles should have the same number of Beginnings, ends and lengths (which
indicate the last line of metadata). Use regex expression such as &quot;\d+ of
\d+ DOCUMENTS$&quot; (which would catch e.g., the format &quot;2 of 100 DOCUMENTS&quot;)
or &quot;auto&quot; to try all common keywords. Keyword search is case sensitive.</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_end_keyword">end_keyword</code></td>
<td>
<p>Is used to indicate the end of an article. Works the same
way as start_keyword. A common regex would be &quot;^LANGUAGE: &quot; which catches
language in all caps at the beginning of the line (usually the last line of
an article).</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_length_keyword">length_keyword</code></td>
<td>
<p>Is used to indicate the end of the metadata. Works the
same way as start_keyword and end_keyword. A common regex would be
&quot;^LENGTH: &quot; which catches length in all caps at the beginning of the line
(usually the last line of the metadata).</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_author_keyword">author_keyword</code></td>
<td>
<p>A keyword to identify the author(s) in the metadata.</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_exclude_lines">exclude_lines</code></td>
<td>
<p>Lines in which these keywords are found are excluded.
Set to <code>character()</code> if you want to turn off this feature.</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_recursive">recursive</code></td>
<td>
<p>A logical flag indicating whether subdirectories are
searched for more files.</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_file_type">file_type</code></td>
<td>
<p>File types/extensions to be included in search for files.</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_remove_cover">remove_cover</code></td>
<td>
<p>Logical. Should the cover page be removed.</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag indicating whether information should be
printed to the screen.</p>
</td></tr>
<tr><td><code id="lnt_read_+3A_...">...</code></td>
<td>
<p>Additional arguments passed on to <a href="#topic+lnt_asDate">lnt_asDate</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can produce an <a href="#topic+LNToutput">LNToutput</a> S4 object with two or
three data.frame: meta, containing all meta information such as date,
author and headline and articles, containing just the article ID and the
text of the articles. When extract_paragraphs is set to TRUE, the output
contains a third data.frame, similar to articles but with articles split
into paragraphs.
</p>
<p>When left to 'auto', the keywords will use the following defaults, which
should be the standard keywords in all languages used by 'LexisNexis':
</p>
<p>* <code>start_keyword = "\d+ of \d+ DOCUMENTS$| Dokument \d+ von \d+$|
  Document \d+ de \d+$"</code>.
</p>
<p>* <code>end_keyword = "^LANGUAGE: |^SPRACHE: |^LANGUE: "</code>.
</p>


<h3>Value</h3>

<p>An LNToutput S4 object consisting of 3 data.frames for metadata,
articles and paragraphs.
</p>


<h3>Author(s)</h3>

<p>Johannes B. Gruber
</p>


<h3>Examples</h3>

<pre><code class='language-R'>LNToutput &lt;- lnt_read(lnt_sample(copy = FALSE))
meta.df &lt;- LNToutput@meta
articles.df &lt;- LNToutput@articles
paragraphs.df &lt;- LNToutput@paragraphs
</code></pre>

<hr>
<h2 id='lnt_rename'>Assign proper names to LexisNexis files</h2><span id='topic+lnt_rename'></span>

<h3>Description</h3>

<p>Give proper names to files downloaded from 'LexisNexis' based on search
term and period retrieved from each file cover page. This information is not
always delivered by LexisNexis though. If the information is not present in
the file, new file names will be empty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnt_rename(
  x,
  encoding = "UTF-8",
  recursive = FALSE,
  report = TRUE,
  simulate = TRUE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnt_rename_+3A_x">x</code></td>
<td>
<p>Can be either a character vector of LexisNexis file name(s),
folder name(s) or can be left blank (see example).</p>
</td></tr>
<tr><td><code id="lnt_rename_+3A_encoding">encoding</code></td>
<td>
<p>Encoding to be assumed for input files. Defaults to UTF-8
(the LexisNexis standard value).</p>
</td></tr>
<tr><td><code id="lnt_rename_+3A_recursive">recursive</code></td>
<td>
<p>A logical flag indicating whether subdirectories are
searched for more files.</p>
</td></tr>
<tr><td><code id="lnt_rename_+3A_report">report</code></td>
<td>
<p>A logical flag indicating whether the function will return a
report which files were renamed.</p>
</td></tr>
<tr><td><code id="lnt_rename_+3A_simulate">simulate</code></td>
<td>
<p>Should the renaming be simulated instead of actually done?
This can help prevent accidental renaming of unrelated files which
happen to be in the same directory as the files from 'LexisNexis'.</p>
</td></tr>
<tr><td><code id="lnt_rename_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag indicating whether information should be
printed to the screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Warning: This will rename all supported files in a give folder.
</p>


<h3>Author(s)</h3>

<p>Johannes B. Gruber
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Copy sample file to current wd
lnt_sample()

# Rename files in current wd and report back if successful

report.df &lt;- lnt_rename(
  recursive = FALSE,
  report = TRUE
)


# Or provide file name(s)
my_files &lt;- list.files(
  pattern = ".txt", full.names = TRUE,
  recursive = TRUE, ignore.case = TRUE
)
report.df &lt;- lnt_rename(
  x = my_files,
  recursive = FALSE,
  report = TRUE
)

# Or provide folder name(s)
report.df &lt;- lnt_rename(x = getwd())

report.df

## End(Not run)
</code></pre>

<hr>
<h2 id='lnt_sample'>Provides a small sample TXT/DOCX file</h2><span id='topic+lnt_sample'></span>

<h3>Description</h3>

<p>Copies a small TXT sample file (as used by the old Nexis) or a DOCX (as used
by Nexis Uni or Lexis Advance) to the current working directory and returns
the location of this newly created file. The content of the file is made up
or copied from Wikipedia since real articles from LexisNexis fall under
copyright laws and can not be shared.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnt_sample(
  format = "txt",
  overwrite = FALSE,
  verbose = TRUE,
  path = NULL,
  copy = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnt_sample_+3A_format">format</code></td>
<td>
<p>Either &quot;txt&quot; to get the sample.TXT file or &quot;docx&quot; to get the
format used by Nexis Uni.</p>
</td></tr>
<tr><td><code id="lnt_sample_+3A_overwrite">overwrite</code></td>
<td>
<p>Should the sample file be overwritten if found in the
current working directory?</p>
</td></tr>
<tr><td><code id="lnt_sample_+3A_verbose">verbose</code></td>
<td>
<p>Display warning message if file exists in current wd.</p>
</td></tr>
<tr><td><code id="lnt_sample_+3A_path">path</code></td>
<td>
<p>The destination path for the sample file (current working
directory if <code>NULL</code>)</p>
</td></tr>
<tr><td><code id="lnt_sample_+3A_copy">copy</code></td>
<td>
<p>Logical. Should the file be copied to path/working directory? If
<code>FALSE</code>, the function only returns the location of the sample file.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A small sample database to test the functions of LexisNexisTools
</p>


<h3>Author(s)</h3>

<p>Johannes Gruber
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  lnt_sample()

## End(Not run)
</code></pre>

<hr>
<h2 id='lnt_similarity'>Check for highly similar articles.</h2><span id='topic+lnt_similarity'></span>

<h3>Description</h3>

<p>Check for highly similar articles by comparing all articles published on the
same date. This function implements two measures to test if articles are
almost identical. The function <a href="quanteda.textstats.html#topic+textstat_simil">textstat_simil</a>, which
compares the word similarity of two given texts; and a relative modification
of the generalized Levenshtein (edit) distance implementation in
<a href="stringdist.html#topic+stringdist">stringdist</a>. The relative distance is calculated by
dividing the string distance by the number of characters in the longer
article (resulting in a minimum of 0 if articles are exactly alike and 1 if
strings are completely different). Using both methods cancels out the
disadvantages of each method: the similarity measure is fast but does not
take the word order into account. Two widely different texts could,
therefore, be identified as the same, if they employ the exact same
vocabulary for some reason. The generalized Levenshtein distance is more
accurate but is very computationally demanding, especially if more than two
texts are compared at once.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnt_similarity(
  texts,
  dates,
  LNToutput,
  IDs = NULL,
  threshold = 0.99,
  rel_dist = TRUE,
  length_diff = Inf,
  nthread = getOption("sd_num_thread"),
  max_length = Inf,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnt_similarity_+3A_texts">texts</code></td>
<td>
<p>Provide texts to check for similarity.</p>
</td></tr>
<tr><td><code id="lnt_similarity_+3A_dates">dates</code></td>
<td>
<p>Provide corresponding dates, same length as <code>text</code>.</p>
</td></tr>
<tr><td><code id="lnt_similarity_+3A_lntoutput">LNToutput</code></td>
<td>
<p>Alternatively to providing texts and dates individually, you
can provide an LNToutput object.</p>
</td></tr>
<tr><td><code id="lnt_similarity_+3A_ids">IDs</code></td>
<td>
<p>IDs of articles.</p>
</td></tr>
<tr><td><code id="lnt_similarity_+3A_threshold">threshold</code></td>
<td>
<p>At which threshold of similarity is an article considered a
duplicate. Note that lower threshold values will increase the time to
calculate the relative difference (as more articles are considered).</p>
</td></tr>
<tr><td><code id="lnt_similarity_+3A_rel_dist">rel_dist</code></td>
<td>
<p>Calculate the relative Levenshtein distance between two
articles if set to TRUE (can take very long). The main difference between
the similarity and distance value is that the distance takes word order
into account while similarity employs the bag of words approach.</p>
</td></tr>
<tr><td><code id="lnt_similarity_+3A_length_diff">length_diff</code></td>
<td>
<p>Before calculating the relative distance between articles,
the length of the articles in characters is calculated. If the difference
surpasses this value, calculation is omitted and the distance will set to
NA.</p>
</td></tr>
<tr><td><code id="lnt_similarity_+3A_nthread">nthread</code></td>
<td>
<p>Maximum number of threads to use (see
<a href="stringdist.html#topic+stringdist-parallelization">stringdist-parallelization</a>).</p>
</td></tr>
<tr><td><code id="lnt_similarity_+3A_max_length">max_length</code></td>
<td>
<p>If the article is too long, calculation of the relative
distance can cause R to crash (see
<a href="https://github.com/markvanderloo/stringdist/issues/59">https://github.com/markvanderloo/stringdist/issues/59</a>). To prevent
this you can set a maximum length (longer articles will not be evaluated).</p>
</td></tr>
<tr><td><code id="lnt_similarity_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag indicating whether information should be
printed to the screen.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table consisting of information about duplicated
articles. Articles with a lower similarity than the threshold will be
removed, while all relative distances are still in the returned object.
Before you use the duplicated information to subset your dataset, you
should, therefore, filter out results with a high relative distance (e.g.
larger than 0.2).
</p>


<h3>Author(s)</h3>

<p>Johannes B. Gruber
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Copy sample file to current wd
lnt_sample()

## End(Not run)

# Convert raw file to LNToutput object
LNToutput &lt;- lnt_read(lnt_sample(copy = FALSE))

# Test similarity of articles
duplicates.df &lt;- lnt_similarity(
  texts = LNToutput@articles$Article,
  dates = LNToutput@meta$Date,
  IDs = LNToutput@articles$ID
)

# Remove instances with a high relative distance
duplicates.df &lt;- duplicates.df[duplicates.df$rel_dist &lt; 0.2]

# Create three separate data.frames from cleaned LNToutput object
LNToutput &lt;- LNToutput[!LNToutput@meta$ID %in%
  duplicates.df$ID_duplicate]
meta.df &lt;- LNToutput@meta
articles.df &lt;- LNToutput@articles
paragraphs.df &lt;- LNToutput@paragraphs
</code></pre>

<hr>
<h2 id='lnt2bibtex'>Convert LNToutput to other formats</h2><span id='topic+lnt2bibtex'></span>

<h3>Description</h3>

<p>Takes output from <a href="#topic+lnt_read">lnt_read</a> and converts chosen articles to a BibTeX
citation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnt2bibtex(x, art_id, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnt2bibtex_+3A_x">x</code></td>
<td>
<p>An object of class LNToutput.</p>
</td></tr>
<tr><td><code id="lnt2bibtex_+3A_art_id">art_id</code></td>
<td>
<p>The ID(s) of the article(s) to convert.</p>
</td></tr>
<tr><td><code id="lnt2bibtex_+3A_...">...</code></td>
<td>
<p>unused.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>LNToutput &lt;- lnt_read(lnt_sample(copy = FALSE))

bib &lt;- lnt2bibtex(LNToutput, art_id = 1)
</code></pre>

<hr>
<h2 id='LNToutput'>An S4 class to store the three data.frames created with <a href="#topic+lnt_read">lnt_read</a></h2><span id='topic+LNToutput'></span>

<h3>Description</h3>

<p>This S4 class stores the output from <a href="#topic+lnt_read">lnt_read</a>. Just like a spreadsheet
with multiple worksheets, an LNToutput object consist of three data.frames
which you can select using <code>@</code>. This object class is intended to be an
intermediate container. As it stores articles and paragraphs in two separate
data.frames, nested in an S4 object, the relevant text data is stored twice
in almost the same format. This has the advantage, that there is no need to
use special characters, such as &quot;\n&quot; to indicate a new paragraph. However,
it makes the files rather big when you save them directly. They should thus
usually be subsetted using <code>@</code> or converted to a different format using
<a href="#topic+lnt_convert">lnt_convert</a>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>meta</code></dt><dd><p>The metadata of the articles read in.</p>
</dd>
<dt><code>articles</code></dt><dd><p>The article texts and respective IDs.</p>
</dd>
<dt><code>paragraphs</code></dt><dd><p>The paragraphs (if the data.frame exists) and respective
article and paragraph IDs.</p>
</dd>
</dl>

<hr>
<h2 id='LNToutput_methods'>Methods for LNToutput output objects</h2><span id='topic+LNToutput_methods'></span><span id='topic+dim+2CLNToutput-method'></span><span id='topic+show+2CLNToutput-method'></span><span id='topic++5B+2CLNToutput+2CANY+2CANY+2CANY-method'></span><span id='topic++2B+2CLNToutput+2CLNToutput-method'></span>

<h3>Description</h3>

<p>Methods for LNToutput output objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'LNToutput'
dim(x)

## S4 method for signature 'LNToutput'
show(object)

## S4 method for signature 'LNToutput,ANY,ANY,ANY'
x[i, j, invert = FALSE, ..., drop = TRUE]

## S4 method for signature 'LNToutput,LNToutput'
e1 + e2
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LNToutput_methods_+3A_x">x</code>, <code id="LNToutput_methods_+3A_object">object</code></td>
<td>
<p>An LNToutput object.</p>
</td></tr>
<tr><td><code id="LNToutput_methods_+3A_i">i</code></td>
<td>
<p>Rows of the meta data.frame (default) or values of j.</p>
</td></tr>
<tr><td><code id="LNToutput_methods_+3A_j">j</code></td>
<td>
<p>The column you want to use to subset the LNToutput object. Takes
character strings.</p>
</td></tr>
<tr><td><code id="LNToutput_methods_+3A_invert">invert</code></td>
<td>
<p>Invert the selection of i.</p>
</td></tr>
<tr><td><code id="LNToutput_methods_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
<tr><td><code id="LNToutput_methods_+3A_drop">drop</code></td>
<td>
<p>not used (it's here so tests do not complain).</p>
</td></tr>
<tr><td><code id="LNToutput_methods_+3A_e1">e1</code>, <code id="LNToutput_methods_+3A_e2">e2</code></td>
<td>
<p>LNToutput objects which will be combined.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
