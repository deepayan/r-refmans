<!DOCTYPE html><html><head><title>Help for package VHDClassification</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {VHDClassification}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#VHDClassification-package'>
<p>Discrimination-Classification in very high dimension with linear and quadratic rules.</p></a></li>
<li><a href='#.EvaluateLogLikeRatio-methods'><p> ~~ Methods for Function .EvaluateLogLikeRatio  ~~</p></a></li>
<li><a href='#getBinaryRule'>
<p>Getter set of binary rules (object PartitionWithLLR)</p></a></li>
<li><a href='#getBinaryRule-methods'><p> ~~ Methods for Function getBinaryRule  ~~</p></a></li>
<li><a href='#getLogLikeRatio'>
<p>Get the log-likelihood ratio from a binary rule (QuadraticRule or LinearRule)</p></a></li>
<li><a href='#getLogLikeRatio-methods'><p> ~~ Methods for Function getLogLikeRatio  ~~</p></a></li>
<li><a href='#learnBinaryRule'>
<p>Function to learn a binary classification rule</p></a></li>
<li><a href='#learnPartitionWithLLR'>
<p>A function to learn a rule in case of 2 classes or more</p></a></li>
<li><a href='#LinearRule-class'><p>Class &quot;LinearRule&quot; ~~~</p></a></li>
<li><a href='#PartitionWithLLR-class'><p>Class &quot;PartitionWithLLR&quot; ~~~</p></a></li>
<li><a href='#plotClassifRule'><p>A</p>
plot function for classification rules (binary or not, quadratic or linear)</a></li>
<li><a href='#QuadraticRule-class'><p>Class &quot;QuadraticRule&quot; ~~~</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Discrimination/Classification in very high dimension with linear
and quadratic rules.</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2013-12-11</td>
</tr>
<tr>
<td>Author:</td>
<td>Robin Girard</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Robin Girard &lt;robin.girard@mines-paristech.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>This package provides an implementation of Linear discriminant analysis and quadratic discriminant analysis that works fine in very high dimension (when there are many more variables than observations). </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Depends:</td>
<td>methods, e1071, lattice, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>pamr,randomForest</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2013-12-11 14:42:03 UTC; robin.girard</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2013-12-18 14:13:37</td>
</tr>
</table>
<hr>
<h2 id='VHDClassification-package'>
Discrimination-Classification in very high dimension with linear and quadratic rules.
</h2><span id='topic+VHDClassification-package'></span><span id='topic+VHDClassification'></span><span id='topic+predict-methods'></span>

<h3>Description</h3>

<p>This package provides an implementation of Linear disciminant analysis 
and quadratic discriminant analysis that works fine in very high dimension 
(when there are many more variables than observations). 
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> VHDClassification</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2010-04-15</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> methods, e1071, lattice, stats</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>This package provides learning procedure for classification in very high dimension. 
Binary learning is done with <code><a href="#topic+learnBinaryRule">learnBinaryRule</a></code> while K-class (K&gt;=2) learning is done 
with function <code><a href="#topic+learnPartitionWithLLR">learnPartitionWithLLR</a></code>. 
</p>
<p><code><a href="#topic+learnBinaryRule">learnBinaryRule</a></code> can return an object <code><a href="#topic+LinearRule-class">LinearRule-class</a></code> or an object <code><a href="#topic+QuadraticRule-class">QuadraticRule-class</a></code> depending 
whether type='linear' or 'quadratic'. 
<code><a href="#topic+learnPartitionWithLLR">learnPartitionWithLLR</a></code> basically returns a set of binary rules which is represented by the class <code><a href="#topic+PartitionWithLLR-class">PartitionWithLLR-class</a></code>. 
The used procedure for the learning are described in the papers cited below. The 
</p>
<p>The method predict (<a href="#topic+predict-methods">predict-methods</a>) is implemented for class <code><a href="#topic+LinearRule-class">LinearRule-class</a></code> <code><a href="#topic+QuadraticRule-class">QuadraticRule-class</a></code> <code><a href="#topic+learnPartitionWithLLR">learnPartitionWithLLR</a></code>. 
It predicts the class of a new observation.  
</p>


<h3>Author(s)</h3>

<p>Maintainer-author: Robin Girard &lt;robin.girard@mines-paristech.fr&gt;
</p>


<h3>References</h3>

<p>Fast rate of convergence in high dimensional linear discriminant analysis. R. Girard To appear in Journal of Nonparametric Statistics.\
Very high dimensional discriminant analysis with thresholding estimation. R. Girard.  Submitted. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>############ Tests 2 classes when the true rule should be quadratic
#library(VHDClassification)
p=500; n=50 ; mu=array(0,c(p,2)) ; C=array(c(1,20),c(p,2)); C[c(1,3,5),1]=40
x=NULL; y=NULL;
for (k in 1:2)
{
  M=matrix(rnorm(p*n),nrow=p,ncol=n)
  tmp=array(C[,k]^(1/2),c(p,n))*(M)+array(mu[,k],c(p,n))
  x=rbind(x,t(tmp));
  y=c(y,array(k,n))
  }
#Learning
LearnedQuadBinaryRule=learnBinaryRule(x,y,type='quadratic')
LearnedLinBinaryRule=learnBinaryRule(x,y) # default is linear type
# for comparison with SVM
# require(e1071)
# svmRule=best.tune('svm',
#              train.x=x,
#              train.y=factor(y),
#              ranges=list(gamma=c(2^(-4:4),
#              cost = 2^(-2:2))))
# for comparison with randomForest
require(randomForest)
RF &lt;- best.tune('randomForest',x,factor(y),ranges=list(ntree = c(100,500)))
# for comparison with nearest chrunken centroid
#require(pamr)
#myTrainingdata=list(x=t(x),y=y)
#mytrain &lt;- pamr.train(myTrainingdata)
#mycv &lt;- pamr.cv(mytrain,myTrainingdata)
#thresh=try(mycv$threshold[which.min(mycv$error)],silent = TRUE)




#Testing Set
x=NULL; y=NULL;
for (k in 1:2){
    M=matrix(rnorm(p*n),nrow=p,ncol=n)
    x=rbind(x,t(array(C[,k]^(1/2),c(p,n))*(M)+array(mu[,k],c(p,n))));
    y=c(y,array(k,n))    
}
#Testing
myTestingdata=list(x=x,y=y)
QDAScore=mean((y!=predict(LearnedQuadBinaryRule,myTestingdata$x))) ;
LDAScore=mean((y!=predict(LearnedLinBinaryRule,myTestingdata$x))) ;
RFScore=mean((y!=predict(RF,myTestingdata$x))) ;
#SVMScore=mean((y!=predict(svmRule,x))) ;
#comparison with nearest chrunken centroid
myTestingdata=list(x=t(x),y=y)
#V=as.numeric(pamr.predict(mytrain, myTestingdata$x,threshold=thresh,type="class"))
#SCScore=mean((myTestingdata$y!=V))
cat('\n')
cat('What does it cost to use type=linear  when the rule  is quadratic ? ','\n',
'Score of the linear rule: ',LDAScore,'\n',
'Score of the quadratic rule: ',QDAScore,'\n',
#'Score of the nearest shrunken centroid rule: ',SCScore,'\n',
'Score of the random forest rule: ',RFScore,'\n',
#'Score of the support vector machine rule: ',SVMScore,'\n',
'Note: These scores should be average for a large number of experiment or interpreted carefully \n')
plotClassifRule(LearnedQuadBinaryRule)

############ Tests 2 classes quadratic and linear. when the true is linear 
#library(VHDClassification)
#p=100; n=50 ; mu=array(0,c(p,2)); mu[1:10,1]=1 ;C=array(c(1,20),p)
#x=NULL; y=NULL;
#for (k in 1:2){
# M=matrix(rnorm(p*n),nrow=p,ncol=n)
#	x=rbind(x,t(array(C^(1/2),c(p,n))*(M)+array(mu[,k],c(p,n))));
#    y=c(y,array(k,n))}
#Learning
#LearnedQuadBinaryRule=learnBinaryRule(x,y,type='quadratic')
#LearnedLinBinaryRule=learnBinaryRule(x,y) # default is linear type
#comparison with nearest chrunken centroid
#require(pamr)
#myTrainingdata=list(x=t(x),y=y)
#mytrain &lt;- pamr.train(myTrainingdata)
#mycv &lt;- pamr.cv(mytrain,myTrainingdata)
#thresh=try(mycv$threshold[which.min(mycv$error)],silent = TRUE)


#Testing Set
#x=NULL; y=NULL;
#for (k in 1:2){
# M=matrix(rnorm(p*n),nrow=p,ncol=n)
#    x=rbind(x,t(array(C^(1/2),c(p,n))*(M)+array(mu[,k],c(p,n))));
#    y=c(y,array(k,n))    
#}
#Testing
#myTestingdata=list(x=x,y=y)
#QDAScore=mean((y!=predict(LearnedQuadBinaryRule,myTestingdata$x))) ;
#LDAScore=mean((y!=predict(LearnedLinBinaryRule,myTestingdata$x))) ;
#comparison with nearest shrunken centroid
#myTestingdata=list(x=t(x),y=y)
#tmp=as.numeric(pamr.predict(mytrain,threshold=thresh,
# myTestingdata$x,type="class"))
#SCScore=mean((myTestingdata$y!=tmp))
#cat('\n',
#'What does it cost to use type=
# quadratic rule when the true optimal rule is linear ? ','\n',
#'Score of the linear rule: ',LDAScore,'\n',
#'Score of the  rule with type=quadratic : ',QDAScore,'\n',
# 'it detects that the true rule is linear?\n',
#'Score of the nearest shrunken centroid rule: ',SCScore,'\n')

#plotClassifRule(LearnedQuadBinaryRule)

############ Tests 3 classes
#library(VHDClassification)
#p=1000; n=40 ; mu=array(0,c(p,3)); mu[1:10,1]=4; C=array(c(1,20),p)

#x=NULL; y=NULL;
#for (k in 1:3){
#    if (k&lt;3){
#     M=matrix(rnorm(p*n),nrow=p,ncol=n)
#    x=rbind(x,t(array(C^(1/2),c(p,n))*(M)+array(mu[,k],c(p,n))));
#    y=c(y,array(k,n))}
#    else
#    {
#    tildeC=C; tildeC[1:10]=40; 
#     M=matrix(rnorm(p*n),nrow=p,ncol=n)
#    x=rbind(x,t(array(tildeC^(1/2),c(p,n))*(M)+array(mu[,k],c(p,n))));
#    y=c(y,array(k,n))
#    }
#    }
#Learning
#LearnedLinearPartitionWithLLR=learnPartitionWithLLR(x,y,type='linear')
#LearnedQuadraticPartitionWithLLR=learnPartitionWithLLR(x,y,type='quadratic')
#plotClassifRule(LearnedQuadraticPartitionWithLLR)
#require(randomForest)
#RF &lt;- best.tune('randomForest',x,factor(y),ranges=list(ntree = c(500)))

#Testing Set
#x=NULL; y=NULL;
#for (k in 1:3){
#    if (k&lt;3){
#     M=matrix(rnorm(p*n),nrow=p,ncol=n)
#    x=rbind(x,t(array(C^(1/2),c(p,n))*(M)+array(mu[,k],c(p,n))));
#    y=c(y,array(k,n))}
#    else
#    {
#    tildeC=C; tildeC[1:10]=40; 
#     M=matrix(rnorm(p*n),nrow=p,ncol=n)
#    x=rbind(x,t(array(tildeC^(1/2),c(p,n))*(M)+array(mu[,k],c(p,n))));
#    y=c(y,array(k,n))
#    }
#    }
#Testing
#myTestingdata=list(x=x,y=y)
#LDAScore=mean((y!=factor(predict(LearnedLinearPartitionWithLLR,myTestingdata$x)))) ;
#QDAScore=mean((y!=factor(predict(LearnedQuadraticPartitionWithLLR,myTestingdata$x)))) ;
#RFScore=mean((y!=predict(RF,myTestingdata$x))) ;

#cat('Score of the quadratic rule: ',QDAScore,'\n',
#'Score of the linear rule: ',LDAScore,'\n',
#'Score of the random Forest Rule: ',RFScore,'\n')


</code></pre>

<hr>
<h2 id='.EvaluateLogLikeRatio-methods'> ~~ Methods for Function .EvaluateLogLikeRatio  ~~</h2><span id='topic+.EvaluateLogLikeRatio-methods'></span><span id='topic+.EvaluateLogLikeRatio+2Cnumeric+2CLinearRule-method'></span><span id='topic+.EvaluateLogLikeRatio+2Cnumeric+2CQuadraticRule-method'></span>

<h3>Description</h3>

<p>~~ Methods for function <code>.EvaluateLogLikeRatio</code>  ~~
</p>


<h3>Methods</h3>


<dl>
<dt>
<code>signature(x = "numeric", object = "LinearRule")</code></dt><dd><p> hidden.</p>
</dd>
</dl>


<hr>
<h2 id='getBinaryRule'>
Getter set of binary rules (object PartitionWithLLR)
</h2><span id='topic+getBinaryRule'></span>

<h3>Description</h3>

<p>This function returns the binary rule for discrimination between data from class k and data from class l
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getBinaryRule(object, k, l)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getBinaryRule_+3A_object">object</code></td>
<td>

<p>An object of class PartitionWithLLR as returned by learnPartitionWithLLR
</p>
</td></tr>
<tr><td><code id="getBinaryRule_+3A_k">k</code></td>
<td>

<p>an existing label 
</p>
</td></tr>
<tr><td><code id="getBinaryRule_+3A_l">l</code></td>
<td>

<p>an existing label 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A binary classification rule. Can either be an object of class LinearRule or an object of class QuadraticRule
</p>


<h3>Author(s)</h3>

<p>Robin Girard
</p>


<h3>References</h3>

<p>Fast rate of convergence in high dimensional linear discriminant analysis. R. Girard To appear in Journal of Nonparametric Statistics.\
Very high dimensional discriminant analysis with thresholding estimation. R. Girard.  Submitted. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getLogLikeRatio">getLogLikeRatio</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#try p=1000 , 5000, ...
p=100; n=20 ; mu=array(0,c(p,4)); mu[1:10,1]=2 ;mu[11:20,2]=2;C=array(c(1,20),p)
mu[21:30,3]=2
x=NULL; y=NULL;
for (k in 1:4){
    x=rbind(x,t(array(C^(1/2),c(p,n))*(matrix(rnorm(p*n),nrow=p,ncol=n))+array(mu[,k],c(p,n))));
    y=c(y,array(k,n))}
#Learning
LearnedLinearPartitionWithLLR=learnPartitionWithLLR(x,y,procedure='FDRThresh')
Rule=getBinaryRule(LearnedLinearPartitionWithLLR,1,2)
show(Rule)
</code></pre>

<hr>
<h2 id='getBinaryRule-methods'> ~~ Methods for Function getBinaryRule  ~~</h2><span id='topic+getBinaryRule-methods'></span><span id='topic+getBinaryRule+2CPartitionWithLLR-method'></span>

<h3>Description</h3>

<p>~~ Methods for function <code>getBinaryRule</code>  ~~
</p>


<h3>Methods</h3>


<dl>
<dt><code>signature(object = "PartitionWithLLR")</code></dt><dd><p> see <code><a href="#topic+getBinaryRule">getBinaryRule</a></code> </p>
</dd>
</dl>

<hr>
<h2 id='getLogLikeRatio'>
Get the log-likelihood ratio from a binary rule (QuadraticRule or LinearRule) 
</h2><span id='topic+getLogLikeRatio'></span>

<h3>Description</h3>

<p>Binary rules can be expressed 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getLogLikeRatio(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getLogLikeRatio_+3A_object">object</code></td>
<td>

<p>an object of type LinearRule or QuadraticRule.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Get everything that defines a log likelihood ratio between two gaussian measures. 
</p>


<h3>Value</h3>

<p>A list, see <code><a href="#topic+getLogLikeRatio-methods">getLogLikeRatio-methods</a></code>
</p>


<h3>Author(s)</h3>

<p>Robin Girard
</p>


<h3>References</h3>

<p>Fast rate of convergence in high dimensional linear discriminant analysis. R. Girard To appear in Journal of Nonparametric Statistics.\
Very high dimensional discriminant analysis with thresholding estimation. R. Girard.  Submitted. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=100; n=20 ; mu=array(0,c(p,4)); mu[1:10,1]=2 ;mu[11:20,2]=2;C=array(c(1,20),p)
mu[21:30,3]=2
x=NULL; y=NULL;
for (k in 1:4){
    x=rbind(x,t(array(C^(1/2),c(p,n))*(matrix(rnorm(p*n),nrow=p,ncol=n))+array(mu[,k],c(p,n))));
    y=c(y,array(k,n))}
#Learning
LearnedLinearPartitionWithLLR=learnPartitionWithLLR(x,y,procedure='FDRThresh')

Rule=getBinaryRule(LearnedLinearPartitionWithLLR,1,2)
LLR=getLogLikeRatio(Rule)
print(LLR)</code></pre>

<hr>
<h2 id='getLogLikeRatio-methods'> ~~ Methods for Function getLogLikeRatio  ~~</h2><span id='topic+getLogLikeRatio-methods'></span><span id='topic+getLogLikeRatio+2CLinearRule-method'></span><span id='topic+getLogLikeRatio+2CQuadraticRule-method'></span>

<h3>Description</h3>

<p>~~ Methods for function <code>getLogLikeRatio</code>  ~~
</p>


<h3>Methods</h3>


<dl>
<dt><code>signature(object = "LinearRule")</code></dt><dd><p> Returns a list with NormalVector and CenterVector. The loglikelihood ratio on x can be evaluated by 
L(x)=1/2&lt;NormalVector,x-CenterVector&gt;. </p>
</dd>
<dt><code>signature(object = "QuadraticRule")</code></dt><dd><p> returns a list with a NormalVector, CenterVector, FormVector (3 vectors) and a numeric constant Constant. The loglikelihood ratio on x can be evaluated by
L(x)=-1/2&lt;diag(FormVector)(x-CenterVector),x-CenterVector&gt;+&lt;NormalVector,x-CenterVector&gt; -Constant </p>
</dd>
</dl>

<hr>
<h2 id='learnBinaryRule'>
Function to learn a binary classification rule
</h2><span id='topic+learnBinaryRule'></span>

<h3>Description</h3>

<p>Function to learn a binary classification rule. 
For more than two class, use <code><a href="#topic+learnPartitionWithLLR">learnPartitionWithLLR</a></code> instead. 
The learned rule can be linear or quadratic. 
There are reduction dimension methods (accessible via argument procedure)
to make the procedure efficient when the number of features is larger than the number of observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learnBinaryRule(x, y,type='linear', procedure = "FDRThresh", 
covariance = "diagonal", ql = NULL, qq = NULL,prior=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learnBinaryRule_+3A_x">x</code></td>
<td>

<p>The Matrix with input data of size pxn (p feature space dimension, and n number of observations)
</p>
</td></tr>
<tr><td><code id="learnBinaryRule_+3A_y">y</code></td>
<td>

<p>A vector of n factors with 2 LEVELS (labels) associated to observations (can also be numeric)
</p>
</td></tr>
<tr><td><code id="learnBinaryRule_+3A_type">type</code></td>
<td>
<p>'quadratic' or 'linear' are valid types. </p>
</td></tr>
<tr><td><code id="learnBinaryRule_+3A_procedure">procedure</code></td>
<td>

<p>Procedure gives the used procedure to reduce the dimensionality of the estimated NormalVector and FormVector.
use 'noThresh' for no dimensionality reduction. UnivTresh is the universal threshold and FDRThresh is an FDR thresolding procedure.
When type=='linear' 'FANThresh' and 'FDRstudent' are also available. For type linear, the thresholding procedures are fully described in the Paper 
&quot;Fast rate of convergence in high dimensional linear discriminant analysis&quot;</p>
</td></tr>
<tr><td><code id="learnBinaryRule_+3A_covariance">covariance</code></td>
<td>

<p>Unused argument ... further development comming soon
</p>
</td></tr>
<tr><td><code id="learnBinaryRule_+3A_ql">ql</code></td>
<td>

<p>The parameter associated to the thresholding procedure for the estimation of NormalVector. 
If a vector of values is given a 10 fold cross validation is performed
</p>
</td></tr>
<tr><td><code id="learnBinaryRule_+3A_qq">qq</code></td>
<td>

<p>The parameter associated to the thresholding procedure for the estimation of FormVector (only when type='quadratic'). 
If a vector of values is given a 10 fold cross validation is performed
</p>
</td></tr>
<tr><td><code id="learnBinaryRule_+3A_prior">prior</code></td>
<td>
<p> Do we put a prior on y (taking into account the proportion of the different class in the learning set to build the classification rule</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A classification rule of class LinearRule if type='linear' and of class QuadraticRule if type='quadratic'.
</p>


<h3>Author(s)</h3>

<p>Robin Girard
</p>


<h3>References</h3>

<p>Fast rate of convergence in high dimensional linear discriminant analysis. R. Girard To appear in Journal of Nonparametric Statistics.\
Very high dimensional discriminant analysis with thresholding estimation. R. Girard.  Submitted. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+learnPartitionWithLLR">learnPartitionWithLLR</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=100; n=50 ; mu=array(0,c(p,2)); mu[1:10,1]=1 ;C=array(c(1,20),p)
x=NULL; y=NULL;

for (k in 1:2){    
  M=matrix(rnorm(p*n),nrow=p,ncol=n)
  x=rbind(x,t(array(C^(1/2),c(p,n))*(M)+array(mu[,k],c(p,n))));
    y=c(y,array(k,n))    }
#Learning
LearnedBinaryRule=learnBinaryRule(x,y)
show(LearnedBinaryRule)</code></pre>

<hr>
<h2 id='learnPartitionWithLLR'>
A function to learn a rule in case of 2 classes or more
</h2><span id='topic+learnPartitionWithLLR'></span>

<h3>Description</h3>

<p>A function to learn a rule in case of 2 classes or more.
There are reduction dimension methods (accessible via argument procedure)
to make the procedure efficient when the number of features is larger than the number of observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learnPartitionWithLLR(x, y, type = "linear", procedure = "FDRThresh", 
ql = NULL, qq = NULL, BinaryLearningProcedure = NULL,prior=FALSE)
</code></pre>


<h3>Arguments</h3>

<p>The Argument are exactly the same as in <code><a href="#topic+learnBinaryRule">learnBinaryRule</a></code> except that y may have more than 2 levels 
</p>
<table>
<tr><td><code id="learnPartitionWithLLR_+3A_x">x</code></td>
<td>

<p>see <code><a href="#topic+learnBinaryRule">learnBinaryRule</a></code>
</p>
</td></tr>
<tr><td><code id="learnPartitionWithLLR_+3A_y">y</code></td>
<td>

<p>vector of factors with two or more levels
</p>
</td></tr>
<tr><td><code id="learnPartitionWithLLR_+3A_type">type</code></td>
<td>

</td></tr>
<tr><td><code id="learnPartitionWithLLR_+3A_procedure">procedure</code></td>
<td>

</td></tr>
<tr><td><code id="learnPartitionWithLLR_+3A_ql">ql</code></td>
<td>

</td></tr>
<tr><td><code id="learnPartitionWithLLR_+3A_qq">qq</code></td>
<td>

</td></tr>
<tr><td><code id="learnPartitionWithLLR_+3A_binarylearningprocedure">BinaryLearningProcedure</code></td>
<td>

</td></tr>
<tr><td><code id="learnPartitionWithLLR_+3A_prior">prior</code></td>
<td>
<p> Do we put a prior on y (taking into account the proportion of the different class in the learning set to build the classification rule</p>
</td></tr>
</table>

<hr>
<h2 id='LinearRule-class'>Class &quot;LinearRule&quot; ~~~ </h2><span id='topic+LinearRule-class'></span><span id='topic+plotClassifRule+2CLinearRule-method'></span><span id='topic+predict+2CLinearRule-method'></span><span id='topic+show+2CLinearRule-method'></span>

<h3>Description</h3>

<p>	 ~~ A concise (1-5 lines) description of what the class is.  ~~</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("LinearRule", ...)</code>.
~~ describe objects here ~~ 
</p>


<h3>Slots</h3>


<dl>
<dt><code>labels</code>:</dt><dd><p>Object of class <code>"factor"</code> ~~ </p>
</dd>
<dt><code>normalVector</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
<dt><code>normalIndex</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
<dt><code>centerVector</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
<dt><code>prior</code>:</dt><dd><p>Object of class <code>"logical"</code> ~~ </p>
</dd>
<dt><code>proportions</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>.EvaluateLogLikeRatio</dt><dd><p><code>signature(x = "numeric", object = "LinearRule")</code>: ... </p>
</dd>
<dt>getLogLikeRatio</dt><dd><p><code>signature(object = "LinearRule")</code>: ... </p>
</dd>
<dt>plotClassifRule</dt><dd><p><code>signature(object = "LinearRule")</code>: ... </p>
</dd>
<dt>predict</dt><dd><p><code>signature(object = "LinearRule")</code>: ... </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "LinearRule")</code>: ... </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Robin Girard </p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("LinearRule")
</code></pre>

<hr>
<h2 id='PartitionWithLLR-class'>Class &quot;PartitionWithLLR&quot; ~~~ </h2><span id='topic+PartitionWithLLR-class'></span><span id='topic+plotClassifRule+2CPartitionWithLLR-method'></span><span id='topic+predict+2CPartitionWithLLR-method'></span><span id='topic+show+2CPartitionWithLLR-method'></span>

<h3>Description</h3>

<p>	 ~~ A concise (1-5 lines) description of what the class is.  ~~</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("PartitionWithLLR", ...)</code>.
~~ describe objects here ~~ 
</p>


<h3>Slots</h3>


<dl>
<dt><code>LogLikeRatio</code>:</dt><dd><p>Object of class <code>"list"</code> ~~ </p>
</dd>
<dt><code>labels</code>:</dt><dd><p>Object of class <code>"ordered"</code> ~~ </p>
</dd>
<dt><code>ThreshProc</code>:</dt><dd><p>Object of class <code>"character"</code> ~~ </p>
</dd>
<dt><code>ql</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
<dt><code>qq</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>getBinaryRule</dt><dd><p><code>signature(object = "PartitionWithLLR")</code>: ... </p>
</dd>
<dt>plotClassifRule</dt><dd><p><code>signature(object = "PartitionWithLLR")</code>: ... </p>
</dd>
<dt>predict</dt><dd><p><code>signature(object = "PartitionWithLLR")</code>: ... </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "PartitionWithLLR")</code>: ... </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Robin Girard </p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("PartitionWithLLR")
</code></pre>

<hr>
<h2 id='plotClassifRule'>A
plot function for classification rules (binary or not, quadratic or linear)
</h2><span id='topic+plotClassifRule'></span>

<h3>Description</h3>

<p>plot function for classification rules (binary or not, quadratic or linear). Essentially a wrapper to xyplot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotClassifRule(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotClassifRule_+3A_object">object</code></td>
<td>

</td></tr>
<tr><td><code id="plotClassifRule_+3A_...">...</code></td>
<td>

<p>other argument that can be passed through xyplot
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robin Girard
</p>

<hr>
<h2 id='QuadraticRule-class'>Class &quot;QuadraticRule&quot; ~~~ </h2><span id='topic+QuadraticRule-class'></span><span id='topic+plotClassifRule+2CQuadraticRule-method'></span><span id='topic+predict+2CQuadraticRule-method'></span><span id='topic+show+2CQuadraticRule-method'></span>

<h3>Description</h3>

<p>	This class implements a high dimensional binary quadratic classification rule</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of <code>learnBinaryRule(x,y,type='quadratic')</code> see <code><a href="#topic+learnBinaryRule">learnBinaryRule</a></code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>formVector</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
<dt><code>formIndex</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
<dt><code>constant</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
<dt><code>normalVector</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
<dt><code>normalIndex</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
<dt><code>centerVector</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+LinearRule-class">LinearRule</a>"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>getLogLikeRatio</dt><dd><p><code>signature(object = "QuadraticRule")</code>: ... </p>
</dd>
<dt>plotClassifRule</dt><dd><p><code>signature(object = "QuadraticRule")</code>: ... </p>
</dd>
<dt>predict</dt><dd><p><code>signature(object = "QuadraticRule")</code>: ... </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "QuadraticRule")</code>: ... </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> robin girard </p>


<h3>References</h3>

<p> See my preprint Preprint </p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("QuadraticRule")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
