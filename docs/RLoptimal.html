<!DOCTYPE html><html lang="en-US"><head><title>Help for package RLoptimal</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RLoptimal}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adjust_significance_level'><p>Adjust Significance Level on a Simulation Basis</p></a></li>
<li><a href='#AllocationRule'><p>Allocation Rule Class</p></a></li>
<li><a href='#clean_python_settings'><p>Clean the Python Virtual Environment</p></a></li>
<li><a href='#learn_allocation_rule'><p>Build an Optimal Adaptive Allocation Rule using Reinforcement Learning</p></a></li>
<li><a href='#rl_config_set'><p>Configuration of Reinforcement Learning</p></a></li>
<li><a href='#rl_dnn_config'><p>DNN Configuration for Reinforcement Learning</p></a></li>
<li><a href='#setup_python'><p>Setting up a Python Virtual Environment</p></a></li>
<li><a href='#simulate_one_trial'><p>Simulate One Trial Using an Obtained Optimal Adaptive Allocation Rule</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Optimal Adaptive Allocation Using Deep Reinforcement Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation to compute an optimal adaptive allocation rule
    using deep reinforcement learning in a dose-response study
    (Matsuura et al. (2022) &lt;<a href="https://doi.org/10.1002%2Fsim.9247">doi:10.1002/sim.9247</a>&gt;).
    The adaptive allocation rule can directly optimize a performance metric,
    such as power, accuracy of the estimated target dose, or mean absolute error
    over the estimated dose-response curve.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/MatsuuraKentaro/RLoptimal">https://github.com/MatsuuraKentaro/RLoptimal</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/MatsuuraKentaro/RLoptimal/issues">https://github.com/MatsuuraKentaro/RLoptimal/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Imports:</td>
<td>DoseFinding, glue, R6, reticulate, stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Collate:</td>
<td>'timer.R' 'train_algo.R' 'utils.R' 'allocation_rule.R'
'generate_setup_code.R' 'rl_dnn_config.R' 'rl_config_set.R'
'learn_allocation_rule.R' 'setup_python.R' 'zzz.R'
'simulate_one_trial.R' 'adjust_significance_level.R'</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-13 12:49:35 UTC; kmatsuu</td>
</tr>
<tr>
<td>Author:</td>
<td>Kentaro Matsuura <a href="https://orcid.org/0000-0001-5262-055X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph],
  Koji Makiyama [aut, ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kentaro Matsuura &lt;matsuurakentaro55@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-13 13:50:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='adjust_significance_level'>Adjust Significance Level on a Simulation Basis</h2><span id='topic+adjust_significance_level'></span>

<h3>Description</h3>

<p>Adjust Significance Level on a Simulation Basis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjust_significance_level(
  allocation_rule,
  models,
  N_total,
  N_ini,
  N_block,
  outcome_type = c("continuous", "binary"),
  sd_normal = NULL,
  alpha = 0.025,
  n_sim = 10000L,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="adjust_significance_level_+3A_allocation_rule">allocation_rule</code></td>
<td>
<p>An object of class <a href="#topic+AllocationRule">AllocationRule</a>
specifying an obtained optimal adaptive allocation rule.</p>
</td></tr>
<tr><td><code id="adjust_significance_level_+3A_models">models</code></td>
<td>
<p>An object of class <a href="DoseFinding.html#topic+Mods">Mods</a> specifying assumed
dose-response models. This is used in the MCPMod method at the end of
this study.</p>
</td></tr>
<tr><td><code id="adjust_significance_level_+3A_n_total">N_total</code></td>
<td>
<p>A positive integer value. The total number of subjects.</p>
</td></tr>
<tr><td><code id="adjust_significance_level_+3A_n_ini">N_ini</code></td>
<td>
<p>A positive integer vector in which each element is greater than
or equal to 2. The number of subjects initially assigned to each dose.</p>
</td></tr>
<tr><td><code id="adjust_significance_level_+3A_n_block">N_block</code></td>
<td>
<p>A positive integer value. The number of subjects allocated
adaptively in each round.</p>
</td></tr>
<tr><td><code id="adjust_significance_level_+3A_outcome_type">outcome_type</code></td>
<td>
<p>A character value specifying the outcome type.
Possible values are &quot;continuous&quot; (default), and &quot;binary&quot;.</p>
</td></tr>
<tr><td><code id="adjust_significance_level_+3A_sd_normal">sd_normal</code></td>
<td>
<p>A positive numeric value. The standard deviation of the
observation noise. When <code>outcome_type</code> is &quot;continuous&quot;,
<code>sd_normal</code> must be specified.</p>
</td></tr>
<tr><td><code id="adjust_significance_level_+3A_alpha">alpha</code></td>
<td>
<p>A positive numeric value. The original significance level.
Default is 0.025.</p>
</td></tr>
<tr><td><code id="adjust_significance_level_+3A_n_sim">n_sim</code></td>
<td>
<p>A positive integer value. The number of simulation studies
to calculate the adjusted significance level. Default is 10000.</p>
</td></tr>
<tr><td><code id="adjust_significance_level_+3A_seed">seed</code></td>
<td>
<p>An integer value. Random seed for data generation in the simulation
studies.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A positive numeric value specifying adjusted significance level.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RLoptimal)

doses &lt;- c(0, 2, 4, 6, 8)

models &lt;- DoseFinding::Mods(
  doses = doses, maxEff = 1.65,
  linear = NULL, emax = 0.79, sigEmax = c(4, 5)
)

## Not run: 
allocation_rule &lt;- learn_allocation_rule(
  models,
  N_total = 150, N_ini = rep(10, 5), N_block = 10, Delta = 1.3,
  outcome_type = "continuous", sd_normal = sqrt(4.5), 
  seed = 123, rl_config = rl_config_set(iter = 1000),
  alpha = 0.025
)

# Simulation-based adjustment of the significance level using `allocation_rule`
adjusted_alpha &lt;- adjust_significance_level(
  allocation_rule, models,
  N_total = 150, N_ini = rep(10, 5), N_block = 10,
  outcome_type = "continuous", sd_normal = sqrt(4.5),
  alpha = 0.025, n_sim = 10000, seed = 123
)
## End(Not run)

</code></pre>

<hr>
<h2 id='AllocationRule'>Allocation Rule Class</h2><span id='topic+AllocationRule'></span>

<h3>Description</h3>

<p>This class represents an allocation rule that generates a next allocation.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>policy</code></dt><dd><p>The RLlib policy that is a Python object.</p>
</dd>
<dt><code>dir</code></dt><dd><p>Directory path of the allocation rule (policy).</p>
</dd>
<dt><code>dirpath</code></dt><dd><p>Full path to the directory of the allocation rule.</p>
</dd>
<dt><code>created_at</code></dt><dd><p>Created time of this object.</p>
</dd>
<dt><code>info</code></dt><dd><p>Information when learning the allocation rule.</p>
</dd>
<dt><code>input</code></dt><dd><p>Inputs for learning the allocation rule.</p>
</dd>
<dt><code>log</code></dt><dd><p>The log of scores during the learning of the allocation rule.</p>
</dd>
<dt><code>checkpoints</code></dt><dd><p>The integer vector of iteration counts for checkpoints.</p>
</dd>
<dt><code>checkpoints_paths</code></dt><dd><p>The paths to the directories where each checkpoint is stored.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-AllocationRule-new"><code>AllocationRule$new()</code></a>
</p>
</li>
<li> <p><a href="#method-AllocationRule-opt_allocation_probs"><code>AllocationRule$opt_allocation_probs()</code></a>
</p>
</li>
<li> <p><a href="#method-AllocationRule-resume_learning"><code>AllocationRule$resume_learning()</code></a>
</p>
</li>
<li> <p><a href="#method-AllocationRule-set_info"><code>AllocationRule$set_info()</code></a>
</p>
</li>
<li> <p><a href="#method-AllocationRule-print"><code>AllocationRule$print()</code></a>
</p>
</li>
<li> <p><a href="#method-AllocationRule-clone"><code>AllocationRule$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-AllocationRule-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new AllocationRule object.
</p>


<h5>Usage</h5>

<div class="r"><pre>AllocationRule$new(dir = "latest", base_dir = "allocation_rules")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir</code></dt><dd><p>A character value. A directory name or path where an
allocation rule is outputted. By default, the latest allocation
rule is searched in 'base_dir'.</p>
</dd>
<dt><code>base_dir</code></dt><dd><p>A character value. A directory path that is used as the
parent directory if the 'dir' argument is a directory name and is
not used otherwise.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-AllocationRule-opt_allocation_probs"></a>



<h4>Method <code>opt_allocation_probs()</code></h4>

<p>Compute optimal allocation probabilities using the obtained allocation rule for dose and response data.
</p>


<h5>Usage</h5>

<div class="r"><pre>AllocationRule$opt_allocation_probs(data_doses, data_resps)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_doses</code></dt><dd><p>A numeric vector. The doses actually administered to each
subject in your clinical trial. It must include all previous
doses.</p>
</dd>
<dt><code>data_resps</code></dt><dd><p>A numeric vector. The values of responses corresponding to
each subject for the 'data_doses' argument.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A vector of the probabilities of the doses.
</p>


<hr>
<a id="method-AllocationRule-resume_learning"></a>



<h4>Method <code>resume_learning()</code></h4>

<p>Resume learning the allocation rule. This function updates the original
AllocationRule object.
</p>


<h5>Usage</h5>

<div class="r"><pre>AllocationRule$resume_learning(iter)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>iter</code></dt><dd><p>A number of additional iterations.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>An updated <a href="#topic+AllocationRule">AllocationRule</a> object.
</p>


<hr>
<a id="method-AllocationRule-set_info"></a>



<h4>Method <code>set_info()</code></h4>

<p>Set information when learning the allocation rule.
</p>


<h5>Usage</h5>

<div class="r"><pre>AllocationRule$set_info(info, input, log, checkpoints)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>info</code></dt><dd><p>Information when learning the allocation rule.</p>
</dd>
<dt><code>input</code></dt><dd><p>Inputs for learning the allocation rule.</p>
</dd>
<dt><code>log</code></dt><dd><p>The log of scores during the learning of the allocation rule.</p>
</dd>
<dt><code>checkpoints</code></dt><dd><p>The paths to the directories where each checkpoint is stored.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-AllocationRule-print"></a>



<h4>Method <code>print()</code></h4>

<p>Print function for AllocationRule object
</p>


<h5>Usage</h5>

<div class="r"><pre>AllocationRule$print()</pre></div>


<hr>
<a id="method-AllocationRule-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>AllocationRule$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='clean_python_settings'>Clean the Python Virtual Environment</h2><span id='topic+clean_python_settings'></span>

<h3>Description</h3>

<p>Clean the Python Virtual Environment
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean_python_settings(envname = "r-RLoptimal")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="clean_python_settings_+3A_envname">envname</code></td>
<td>
<p>Python virtual environment name.</p>
</td></tr>
</table>

<hr>
<h2 id='learn_allocation_rule'>Build an Optimal Adaptive Allocation Rule using Reinforcement Learning</h2><span id='topic+learn_allocation_rule'></span>

<h3>Description</h3>

<p>Build an Optimal Adaptive Allocation Rule using Reinforcement Learning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_allocation_rule(
  models,
  N_total,
  N_ini,
  N_block,
  Delta,
  outcome_type = c("continuous", "binary"),
  sd_normal = NULL,
  optimization_metric = c("MAE", "power", "TD", "power and MAE"),
  rl_models = models,
  rl_models_prior = NULL,
  seed = NULL,
  rl_config = rl_config_set(),
  alpha = 0.025,
  selModel = c("AIC", "maxT", "aveAIC"),
  Delta_range = c(0.9, 1.1) * Delta,
  output_dir = format(Sys.time(), "%Y%m%d_%H%M%S"),
  output_base_dir = "allocation_rules",
  checkpoint_dir = "checkpoints"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="learn_allocation_rule_+3A_models">models</code></td>
<td>
<p>An object of class <a href="DoseFinding.html#topic+Mods">Mods</a> specifying assumed
dose-response models. When <code>outcome_type</code> is &quot;binary&quot;, <code>models</code> should
be specified on the logit scale.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_n_total">N_total</code></td>
<td>
<p>A positive integer value. The total number of subjects.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_n_ini">N_ini</code></td>
<td>
<p>A positive integer vector in which each element is greater than
or equal to 2. The number of subjects initially assigned to each dose.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_n_block">N_block</code></td>
<td>
<p>A positive integer value. The number of subjects allocated
adaptively in each round.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_delta">Delta</code></td>
<td>
<p>A positive numeric value. The clinically relevant target effect.
When <code>outcome_type</code> is &quot;binary&quot;, <code>Delta</code> should be specified
on the logit scale. See <a href="DoseFinding.html#topic+TD">TD</a> for details.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_outcome_type">outcome_type</code></td>
<td>
<p>A character value specifying the outcome type.
Possible values are &quot;continuous&quot; (default), and &quot;binary&quot;.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_sd_normal">sd_normal</code></td>
<td>
<p>A positive numeric value. The standard deviation of the
observation noise. When <code>outcome_type</code> is &quot;continuous&quot;,
<code>sd_normal</code> must be specified.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_optimization_metric">optimization_metric</code></td>
<td>
<p>A character value specifying the metric to
optimize. Possible values are &quot;MAE&quot; (default), &quot;power&quot;, &quot;TD&quot;, or
&quot;power and MAE&quot;. See Section 2.2 of the original paper for details.
&quot;power and MAE&quot; shows performance between &quot;power&quot; and &quot;MAE&quot;
by setting the reward based on MAE to 0 when not significant.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_rl_models">rl_models</code></td>
<td>
<p>An object of class <a href="DoseFinding.html#topic+Mods">Mods</a>. True dose-response
models in simulations for reinforcement learning. The default is the
same as the 'models' argument. Empirically, the inclusion of a wide
variety of models tends to stabilize performance (See RL-MAE incl. exp
in the supporting information of the original paper).</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_rl_models_prior">rl_models_prior</code></td>
<td>
<p>A positive numeric vector. The probability or weight
with which each model in rl_models is selected as the true model in
the simulation. The default is NULL, which specifies equal probability
for each model.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_seed">seed</code></td>
<td>
<p>An integer value. Random seed for reinforcement learning.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_rl_config">rl_config</code></td>
<td>
<p>A list. Other settings for reinforcement learning. See
<a href="#topic+rl_config_set">rl_config_set</a> for details.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_alpha">alpha</code></td>
<td>
<p>A positive numeric value. The significance level. Default is 0.025.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_selmodel">selModel</code></td>
<td>
<p>A character value specifying the model selection criterion
for dose estimation. Possible values are &quot;AIC&quot; (default), &quot;maxT&quot;, or
&quot;aveAIC&quot;. See <a href="DoseFinding.html#topic+MCPMod">MCPMod</a> for details.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_delta_range">Delta_range</code></td>
<td>
<p>A numeric vector of length 2. The lower and upper bounds
of Delta where the estimated target dose is correct. Default is
<code>c(0.9, 1.1) * Delta</code>.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_output_dir">output_dir</code></td>
<td>
<p>A character value. Directory name or path to store the
built allocation rule. Default is the current datetime.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_output_base_dir">output_base_dir</code></td>
<td>
<p>A character value. Parent directory path where the
built allocation rule will be stored. Valid only if 'output_dir' does
not contain '/'. Default is &quot;allocation_rules&quot;.</p>
</td></tr>
<tr><td><code id="learn_allocation_rule_+3A_checkpoint_dir">checkpoint_dir</code></td>
<td>
<p>A character value. Parent directory path to save
checkpoints. It enables you to resume learning from that point onwards.
Default is &quot;checkpoints&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <a href="#topic+AllocationRule">AllocationRule</a> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RLoptimal)

doses &lt;- c(0, 2, 4, 6, 8)

# We build the dose-response models to be used in the MCPMod method, 
# which we plan to execute at the end of the clinical trial.
models &lt;- DoseFinding::Mods(
  doses = doses, maxEff = 1.65,
  linear = NULL, emax = 0.79, sigEmax = c(4, 5)
)

# We obtain an optimal adaptive allocation rule by executing 
# `learn_allocation_rule()` with the `models`.
## Not run: 
allocation_rule &lt;- learn_allocation_rule(
  models,
  N_total = 150, N_ini = rep(10, 5), N_block = 10, Delta = 1.3,
  outcome_type = "continuous", sd_normal = sqrt(4.5), 
  seed = 123, rl_config = rl_config_set(iter = 1000),
  alpha = 0.025
)
## End(Not run)

# It is recommended that the models used in reinforcement learning include 
# possible models in addition to the models used in the MCPMod method. 
# Here, we add the exponential model according to the supporting information 
# in the original paper.
rl_models &lt;- DoseFinding::Mods(
  doses = doses, maxEff = 1.65,
  linear = NULL, emax = 0.79, sigEmax = c(4, 5), exponential = 1
)

# Then, we specify the argument `rl_models` in `learn_allocation_rule` function.
## Not run: 
allocation_rule &lt;- learn_allocation_rule(
  models,
  N_total = 150, N_ini = rep(10, 5), N_block = 10, Delta = 1.3,
  outcome_type = "continuous", sd_normal = sqrt(4.5), 
  seed = 123, rl_models = rl_models, rl_config = rl_config_set(iter = 1000),
  alpha = 0.025
)
## End(Not run)

</code></pre>

<hr>
<h2 id='rl_config_set'>Configuration of Reinforcement Learning</h2><span id='topic+rl_config_set'></span>

<h3>Description</h3>

<p>Mainly settings for the arguments of the training() function.
Not compatible with the new API stack introduced in Ray 2.10.0.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rl_config_set(
  iter = 1000L,
  save_start_iter = NULL,
  save_every_iter = NULL,
  cores = 4L,
  gamma = 1,
  lr = 5e-05,
  train_batch_size = 10000L,
  model = rl_dnn_config(),
  sgd_minibatch_size = 200L,
  num_sgd_iter = 20L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rl_config_set_+3A_iter">iter</code></td>
<td>
<p>A positive integer value. Number of iterations.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_save_start_iter">save_start_iter</code>, <code id="rl_config_set_+3A_save_every_iter">save_every_iter</code></td>
<td>
<p>An integer value. Save checkpoints every
'save_every_iter' iterations starting from 'save_start_iter' or later.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_cores">cores</code></td>
<td>
<p>A positive integer value. Number of CPU cores used for learning.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_gamma">gamma</code></td>
<td>
<p>A positive numeric value. Discount factor of the Markov decision
process. Default is 1.0 (not discount).</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_lr">lr</code></td>
<td>
<p>A positive numeric value. Learning rate (default 5e-5). You can set
a learning schedule instead of a learning rate.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_train_batch_size">train_batch_size</code></td>
<td>
<p>A positive integer value. Training batch size.
Deprecated on the new API stack.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_model">model</code></td>
<td>
<p>A list. Arguments passed into the policy model. See
<a href="#topic+rl_dnn_config">rl_dnn_config</a> for details.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_sgd_minibatch_size">sgd_minibatch_size</code></td>
<td>
<p>A positive integer value. Total SGD batch size
across all devices for SGD. Deprecated on the new API stack.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_num_sgd_iter">num_sgd_iter</code></td>
<td>
<p>A positive integer value. Number of SGD iterations in
each outer loop.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_...">...</code></td>
<td>
<p>Other settings for training(). See the arguments of the training()
function in the source code of RLlib.
https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py
https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of reinforcement learning configuration parameters
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
allocation_rule &lt;- learn_allocation_rule(
  models, 
  N_total = 150, N_ini = rep(10, 5), N_block = 10, Delta = 1.3,
  outcome_type = "continuous", sd_normal = sqrt(4.5), 
  seed = 123, 
  # We change `iter` to 200 and `cores` for reinforcement learning to 2
  rl_config = rl_config_set(iter = 200, cores = 2), 
  alpha = 0.025
)
## End(Not run) 

</code></pre>

<hr>
<h2 id='rl_dnn_config'>DNN Configuration for Reinforcement Learning</h2><span id='topic+rl_dnn_config'></span>

<h3>Description</h3>

<p>DNN (deep neural network) configuration for reinforcement learning.
For detail, see Section 3.2.6 of the original paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rl_dnn_config(
  fcnet_hiddens = c(256L, 256L),
  fcnet_activation = c("relu", "tanh", "swish", "silu", "linear"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rl_dnn_config_+3A_fcnet_hiddens">fcnet_hiddens</code></td>
<td>
<p>A positive integer vector. Numbers of units of the
intermediate layers.</p>
</td></tr>
<tr><td><code id="rl_dnn_config_+3A_fcnet_activation">fcnet_activation</code></td>
<td>
<p>A character value specifying the activation function.
Possible values are &quot;ReLU&quot; (default), &quot;tanh&quot;, &quot;Swish&quot; (or &quot;SiLU&quot;), or
&quot;linear&quot;.</p>
</td></tr>
<tr><td><code id="rl_dnn_config_+3A_...">...</code></td>
<td>
<p>Other configurations. See source code of RLlib.
https://github.com/ray-project/ray/blob/master/rllib/models/catalog.py</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of DNN configuration parameters
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
allocation_rule &lt;- learn_allocation_rule(
  models, 
  N_total = 150, N_ini = rep(10, 5), N_block = 10, Delta = 1.3,
  outcome_type = "continuous", sd_normal = sqrt(4.5), 
  seed = 123, 
  rl_config = rl_config_set(
    iter = 1000, 
    # We change the DNN model
    model = rl_dnn_config(fcnet_hiddens = c(512L, 512L), fcnet_activation = "tanh")
  ), 
  alpha = 0.025
)
## End(Not run) 

</code></pre>

<hr>
<h2 id='setup_python'>Setting up a Python Virtual Environment</h2><span id='topic+setup_python'></span>

<h3>Description</h3>

<p>Setting up a Python virtual environment for the Ray package, which includes
the RLlib library for reinforcement learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setup_python(envname = "r-RLoptimal")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setup_python_+3A_envname">envname</code></td>
<td>
<p>Python virtual environment name.</p>
</td></tr>
</table>

<hr>
<h2 id='simulate_one_trial'>Simulate One Trial Using an Obtained Optimal Adaptive Allocation Rule</h2><span id='topic+simulate_one_trial'></span>

<h3>Description</h3>

<p>Simulate One Trial Using an Obtained Optimal Adaptive Allocation Rule
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_one_trial(
  allocation_rule,
  models,
  true_response,
  N_total,
  N_ini,
  N_block,
  Delta,
  outcome_type = c("continuous", "binary"),
  sd_normal = NULL,
  alpha = 0.025,
  selModel = c("AIC", "maxT", "aveAIC"),
  seed = NULL,
  eval_type = c("all", "pVal")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simulate_one_trial_+3A_allocation_rule">allocation_rule</code></td>
<td>
<p>An object of class <a href="#topic+AllocationRule">AllocationRule</a>
specifying an obtained optimal adaptive allocation rule.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_models">models</code></td>
<td>
<p>An object of class <a href="DoseFinding.html#topic+Mods">Mods</a> specifying assumed
dose-response models. When <code>outcome_type</code> is &quot;binary&quot;, <code>models</code> should
be specified on the logit scale. This is used in the MCPMod method
at the end of this trial.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_true_response">true_response</code></td>
<td>
<p>A numeric vector specifying the true response values of
the true model. When <code>outcome_type</code> is &quot;binary&quot;, <code>true_response</code> should
be specified on the logit scale.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_n_total">N_total</code></td>
<td>
<p>A positive integer value. The total number of subjects.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_n_ini">N_ini</code></td>
<td>
<p>A positive integer vector in which each element is greater than
or equal to 2. The number of subjects initially assigned to each dose.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_n_block">N_block</code></td>
<td>
<p>A positive integer value. The number of subjects allocated
adaptively in each round.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_delta">Delta</code></td>
<td>
<p>A positive numeric value. The clinically relevant target effect.
When <code>outcome_type</code> is &quot;binary&quot;, <code>Delta</code> should be specified
on the logit scale. See <a href="DoseFinding.html#topic+TD">TD</a> for details.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_outcome_type">outcome_type</code></td>
<td>
<p>A character value specifying the outcome type.
Possible values are &quot;continuous&quot; (default), and &quot;binary&quot;.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_sd_normal">sd_normal</code></td>
<td>
<p>A positive numeric value. The standard deviation of the
observation noise. When <code>outcome_type</code> is &quot;continuous&quot;,
<code>sd_normal</code> must be specified.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_alpha">alpha</code></td>
<td>
<p>A positive numeric value. The significance level. Default is 0.025.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_selmodel">selModel</code></td>
<td>
<p>A character value specifying the model selection criterion
for dose estimation. Possible values are &quot;AIC&quot; (default), &quot;maxT&quot;, or
&quot;aveAIC&quot;. See <a href="DoseFinding.html#topic+MCPMod">MCPMod</a> for details.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_seed">seed</code></td>
<td>
<p>An integer value. Random seed for data generation in this trial.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_eval_type">eval_type</code></td>
<td>
<p>A character value specifying the evaluation type. Possible
values are &quot;all&quot; (default) and &quot;pVal&quot;. &quot;all&quot; returns all metrics,
which contain the minimum p value, the selected model name,
the estimated target dose, and the MAE. &quot;pVal&quot; returns only the
minimum p value without fitting models.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list which contains the minimum p value, the selected model name,
the estimated target dose, the MAE, and the proportions of subjects
allocated to each dose.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RLoptimal)

doses &lt;- c(0, 2, 4, 6, 8)

models &lt;- DoseFinding::Mods(
  doses = doses, maxEff = 1.65,
  linear = NULL, emax = 0.79, sigEmax = c(4, 5)
)

## Not run: 
allocation_rule &lt;- learn_allocation_rule(
  models,
  N_total = 150, N_ini = rep(10, 5), N_block = 10, Delta = 1.3,
  outcome_type = "continuous", sd_normal = sqrt(4.5), 
  seed = 123, rl_config = rl_config_set(iter = 1000),
  alpha = 0.025
)

# Simulation-based adjustment of the significance level using `allocation_rule`
adjusted_alpha &lt;- adjust_significance_level(
  allocation_rule, models,
  N_total = 150, N_ini = rep(10, 5), N_block = 10,
  outcome_type = "continuous", sd_normal = sqrt(4.5),
  alpha = 0.025, n_sim = 10000, seed = 123
)
## End(Not run)

eval_models &lt;- DoseFinding::Mods(
  doses = doses, maxEff = 1.65,
  linear = NULL, emax = 0.79, sigEmax = c(4, 5), exponential = 1, quadratic = - 1/12
)
true_response_matrix &lt;- DoseFinding::getResp(eval_models, doses = doses)
true_response_list &lt;- as.list(data.frame(true_response_matrix, check.names = FALSE))

true_model_name &lt;- "emax"

# Simulate one trial using the obtained `allocation_rule` When the true model is "emax"
## Not run: 
res_one &lt;- simulate_one_trial(
  allocation_rule, models, 
  true_response = true_response_list[[true_model_name]],
  N_total = 150, N_ini = rep(10, 5), N_block = 10, 
  Delta = 1.3, outcome_type = "continuous", sd_normal = sqrt(4.5),
  alpha = adjusted_alpha, seed = 123, eval_type = "all"
)
## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
