<!DOCTYPE html><html><head><title>Help for package KRLS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {KRLS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#fdskrls'>
<p>Compute first differences with KRLS</p></a></li>
<li><a href='#gausskernel'>
<p>Gaussian Kernel Distance Computation</p></a></li>
<li><a href='#krls'>
<p>Kernel-based Regularized Least Squares (KRLS)</p></a></li>
<li><a href='#lambdasearch'>
<p>Leave-one-out optimization to find <code class="reqn">\lambda</code></p></a></li>
<li><a href='#looloss'>
<p>Loss Function for Leave One Out Error</p></a></li>
<li><a href='#plot.krls'>
<p>Plot method for Kernel-based Regularized Least Squares (KRLS) Model Fits</p></a></li>
<li><a href='#predict.krls'>
<p>Predict method for Kernel-based Regularized Least Squares (KRLS) Model Fits</p></a></li>
<li><a href='#solveforc'>
<p>Solve for Choice Coefficients in KRLS</p></a></li>
<li><a href='#summary.krls'>
<p>Summary method for Kernel-based Regularized Least Squares (KRLS) Model Fits</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Kernel-Based Regularized Least Squares</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0-0</td>
</tr>
<tr>
<td>Date:</td>
<td>2017-07-08</td>
</tr>
<tr>
<td>Author:</td>
<td>Jens Hainmueller (Stanford) Chad Hazlett (UCLA)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jens Hainmueller &lt;jhain@stanford.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Package implements Kernel-based Regularized Least Squares (KRLS), a machine learning method to fit multidimensional functions y=f(x) for regression and classification problems without relying on linearity or additivity assumptions. KRLS finds the best fitting function by minimizing the squared loss of a Tikhonov regularization problem, using Gaussian kernels as radial basis functions. For further details see Hainmueller and Hazlett (2014).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Suggests:</td>
<td>lattice</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.r-project.org">https://www.r-project.org</a>, <a href="https://www.stanford.edu/~jhain/">https://www.stanford.edu/~jhain/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-07-10 05:24:25 UTC; chad</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-07-10 13:55:59 UTC</td>
</tr>
</table>
<hr>
<h2 id='fdskrls'>
Compute first differences with KRLS
</h2><span id='topic+fdskrls'></span>

<h3>Description</h3>

<p>Internal function that is called by  <code><a href="#topic+krls">krls</a></code> to computes first differences for binary predictors in the X matrix. It would normally not be called by the user directly.</p>


<h3>Usage</h3>

<pre><code class='language-R'>fdskrls(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fdskrls_+3A_object">object</code></td>
<td>
<p>Object from call to <code><a href="#topic+krls">krls</a></code>.
</p>
</td></tr>
<tr><td><code id="fdskrls_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to lower level functions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A object of class <code>krls</code> where the derivatives, average derivatives, and the varinaces of the average derivatives are 
replaced with the first differences for binary predictors. The binaryindicator is also updated and set to TRUE for binary predictors.
</p>


<h3>Author(s)</h3>

<p>Jens Hainmueller (Stanford) and Chad Hazlett (MIT)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+krls">krls</a></code>
</p>

<hr>
<h2 id='gausskernel'>
Gaussian Kernel Distance Computation
</h2><span id='topic+gausskernel'></span>

<h3>Description</h3>

<p>Given a <var>N</var> by <var>D</var> numeric data matrix, this function computes the <var>N</var> by <var>N</var> distance matrix with the pairwise distances between the rows of the data matrix as measured by a Gaussian Kernel. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gausskernel(X = NULL, sigma = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gausskernel_+3A_x">X</code></td>
<td>
<p><var>N</var> by <var>N</var> numeric data matrix.
</p>
</td></tr>
<tr><td><code id="gausskernel_+3A_sigma">sigma</code></td>
<td>
<p>Positive scalar that specifies the bandwidth of the Gaussian kernel (see details).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given two <var>D</var> dimensional vectors <code class="reqn">x_i</code> and <code class="reqn">x_j</code>. The Gaussian kernel is defined as
</p>
<p style="text-align: center;"><code class="reqn">k(x_i,x_j)=exp(\frac{-|| x_i - x_j ||^2}{\sigma^2})</code>
</p>

<p>where <code class="reqn">||x_i - x_j||</code> is the Euclidean distance given by
</p>
<p style="text-align: center;"><code class="reqn">||x_i - x_j||=((x_i1-x_j1)^2 + (x_i2-x_j2)^2 + ... + (x_iD-x_jD)^2)^.5</code>
</p>

<p>and <code class="reqn">\sigma^2</code> is the bandwidth of the kernel.
</p>
<p>Note that the Gaussian kernel is a measure of similarity between <code class="reqn">x_i</code> and <code class="reqn">x_j</code>. It evalues to 1 if the <code class="reqn">x_i</code> and <code class="reqn">x_j</code> are identical, and approaches 0 as <code class="reqn">x_i</code> and <code class="reqn">x_j</code> move further apart. 
</p>
<p>The function relies on the <code><a href="stats.html#topic+dist">dist</a></code> function in the stats package for an initial estimate of the euclidean distance.
</p>


<h3>Value</h3>

<p>An <var>N</var> by <var>N</var> numeric distance matrix that contains the pairwise distances between the rows in <var>X</var>.
</p>


<h3>Author(s)</h3>

<p>Jens Hainmueller (Stanford) and Chad Hazlett (MIT)
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dist">dist</a></code> function in the stats package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(rnorm(6),ncol=2)
gausskernel(X=X,sigma=1)
</code></pre>

<hr>
<h2 id='krls'>
Kernel-based Regularized Least Squares (KRLS)
</h2><span id='topic+krls'></span>

<h3>Description</h3>

<p>Function implements Kernel-Based Regularized Least Squares (KRLS), a machine learning method described in Hainmueller and Hazlett (2014) that allows users to solve regression and classification problems without manual specification search and strong functional form assumptions. KRLS finds the best fitting function by minimizing a Tikhonov regularization problem with a squared loss, using Gaussian Kernels as radial basis functions. KRLS reduces misspecification bias since it learns the functional form from the data. Yet, it nevertheless allows for interpretability and inference in ways similar to ordinary regression models. In particular, KRLS provides closed-form estimates for the predicted values, variances, and the pointwise partial derivatives that characterize the marginal effects of each independent variable at each data point in the covariate space. The distribution of pointwise marginal effects can be used to examine effect heterogeneity and or interactions. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>krls(X = NULL, y = NULL, whichkernel = "gaussian", lambda = NULL,
sigma = NULL, derivative = TRUE, binary= TRUE, vcov=TRUE, 
print.level = 1,L=NULL,U=NULL,tol=NULL,eigtrunc=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="krls_+3A_x">X</code></td>
<td>

<p><var>N</var> by <var>D</var> data numeric matrix that contains the values of <var>D</var> predictor variables for <code class="reqn">i=1,\ldots,N</code> observations. The matrix may not contain missing values or constants. Note that no intercept is required since the function operates on demeaned data and subtracting the mean of <var>y</var> is equivalent to including an (unpenalized) intercept into the model.
</p>
</td></tr>
<tr><td><code id="krls_+3A_y">y</code></td>
<td>

<p><var>N</var> by <var>1</var> data numeric matrix or vector that contains the values of the response variable for all observations. This vector may not contain missing values. 
</p>
</td></tr>
<tr><td><code id="krls_+3A_whichkernel">whichkernel</code></td>
<td>

<p>String vector that specifies which kernel should be used. Must be one of <code>gaussian</code>, <code>linear</code>, <code>poly1</code>, <code>poly2</code>, <code>poly3</code>, or <code>poly4</code> (see details). Default is <code>gaussian</code>.
</p>
</td></tr>
<tr><td><code id="krls_+3A_lambda">lambda</code></td>
<td>

<p>A positive scalar that specifies the <code class="reqn">\lambda</code> parameter for the regularizer (see details). It governs the tradeoff between model fit and complexity. By default, this parameter is chosen by minimizing the sum of the squared leave-one-out errors.
</p>
</td></tr>
<tr><td><code id="krls_+3A_sigma">sigma</code></td>
<td>

<p>A positive scalar that specifies the bandwidth of the Gaussian kernel (see <code><a href="#topic+gausskernel">gausskernel</a></code> for details). 
By default, the bandwidth is set equal to <var>D</var> (the number of dimensions) which typically yields a reasonable scaling of the distances between observations in the standardized data that is used for the fitting. 
</p>
</td></tr>
<tr><td><code id="krls_+3A_derivative">derivative</code></td>
<td>

<p>Logical that specifies whether pointwise partial derivatives should be computed. Currently, derivatives are only implemented for the Gaussian Kernel.
</p>
</td></tr>
<tr><td><code id="krls_+3A_binary">binary</code></td>
<td>

<p>Logical that specifies whether first-differences instead of pointwise partial derivatives should be computed for binary predictors. Ignored unless <code>derivative=TRUE</code>. 
</p>
</td></tr>
<tr><td><code id="krls_+3A_vcov">vcov</code></td>
<td>

<p>Logical that specifies whether variance-covariance matrix for the choice coefficients <var>c</var> and fitted values should be computed. Note that <code>derivative=TRUE</code> requires that <code>vcov=TRUE</code>. 
</p>
</td></tr>
<tr><td><code id="krls_+3A_print.level">print.level</code></td>
<td>

<p>Positive integer that determines the level of printing. Set to 0 for no printing and 2 for more printing.
</p>
</td></tr>
<tr><td><code id="krls_+3A_l">L</code></td>
<td>

<p>Non-negative scalar that determines the lower bound of the search window for the leave-one-out optimization to find <code class="reqn">\lambda</code>. Default is <code>NULL</code> which means that the lower bound is found by using an algorithm outlined in <code><a href="#topic+lambdasearch">lambdasearch</a></code>.
</p>
</td></tr>
<tr><td><code id="krls_+3A_u">U</code></td>
<td>

<p>Positive scalar that determines the upper bound of the search window for the leave-one-out optimization to find <code class="reqn">\lambda</code>. Default is <code>NULL</code> which means that the upper bound is found by using an algorithm outlined in <code><a href="#topic+lambdasearch">lambdasearch</a></code>.
</p>
</td></tr>
<tr><td><code id="krls_+3A_tol">tol</code></td>
<td>

<p>Positive scalar that determines the tolerance used in the optimization routine used to find <code class="reqn">\lambda</code>. Default is <code>NULL</code> which means that convergence is achieved when the difference in the sum of squared leave-one-out errors between the <var>i</var> and the <var>i+1</var> iteration is less than <var>N * 10^-3</var>.
</p>
</td></tr>
<tr><td><code id="krls_+3A_eigtrunc">eigtrunc</code></td>
<td>

<p>Positive scalar that determines how much eignvalues should be trunacted for finding the upper bound of the search window in the algorithm outlined in <code><a href="#topic+lambdasearch">lambdasearch</a></code>. If <code>eigtrunc</code> is set to <var>10^-6</var> this means that we keep only eigenvalues that are <var>10^-6</var> as large as the first. Default is <code>eigtrunc=NULL</code> which means no truncation is used. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>krls</code> implements the Kernel-based Regularized Least Squares (KRLS) estimator as described in Hainmueller and Hazlett (2014). Please consult this reference for any details.
</p>
<p>Kernel-based Regularized Least Squares (KRLS) arises as a Tikhonov minimization problem with a squared loss. Assume we have data of the from <code class="reqn">y_i,\,x_i</code> where <var>i</var> indexes observations, <code class="reqn">y_i \in R</code> is the outcome and <code class="reqn">x_i \in R^D</code> is a <var>D</var>-dimensional vector of predictor values. Then KRLS searches over a space of functions <code class="reqn">H</code> and chooses the best fitting function <code class="reqn">f</code> according to the rule: 
</p>
<p style="text-align: center;"><code class="reqn">argmin_{f \in H} \sum_i^N (y_i - f(x_i))^2 + \lambda ||f||_{H^2}</code>
</p>
 
<p>where <code class="reqn">(y_i - f(x_i))^2</code> is a loss function that computes how &lsquo;wrong&rsquo; the function 
is at each observation <var>i</var> and <code class="reqn">|| f ||_{H^2}</code> is the regularizer that measures the complexity of the function according to the <code class="reqn">L_2</code> norm <code class="reqn">||f||^2 = \int f(x)^2 dx</code>. <code class="reqn">\lambda</code> is the scalar regularization parameter that governs the tradeoff between model fit and complexity. By default, <code class="reqn">\lambda</code> is chosen by minimizing the sum of the squared leave-one-out errors, but it can also be specified by the user in the <code>lambda</code> argument to implement other approaches. 
</p>
<p>Under fairly general conditions, the function that minimizes the regularized loss 
within the hypothesis space established by the choice of a (positive semidefinite) kernel function <code class="reqn">k(x_i,x_j)</code> is of the form
</p>
<p style="text-align: center;"><code class="reqn">f(x_j)= \sum_i^N c_i k(x_i,x_j)</code>
</p>
 
<p>where the kernel function <code class="reqn">k(x_i,x_j)</code> measures the distance 
between two observations <code class="reqn">x_i</code> and <code class="reqn">x_j</code> and <code class="reqn">c_i</code> is the choice coefficient for each observation <code class="reqn">i</code>. Let <code class="reqn">K</code> be the <code class="reqn">N</code> by <code class="reqn">N</code> kernel matrix with all pairwise distances <code class="reqn">K_ij=k(x_i,x_j)</code> and <code class="reqn">c</code> be the  <code class="reqn">N</code> by <code class="reqn">1</code> vector of choice coefficients for all observations then in matrix notation the space is <code class="reqn">y=Kc</code>. 
</p>
<p>Accordingly, the <code>krls</code> function solves the following minimization problem
</p>
<p style="text-align: center;"><code class="reqn">argmin_{f \in H} \sum_i^n (y - Kc)'(y-Kc)+ \lambda c'Kc</code>
</p>

<p>which is convex in <code class="reqn">c</code> and solved by <code class="reqn">c=(K +\lambda I)^-1 y</code> where <code class="reqn">I</code> is the identity matrix. Note that this linear solution provides a flexible fitted response surface that typically reduces misspecification bias because it can learn a wide range of nonlinear and or nonadditive functions of the predictors. 
</p>
<p>If <code>vcov=TRUE</code> is specified, <code>krls</code> also computes the variance-covariance matrix for the choice coefficients <code class="reqn">c</code> and fitted values <code class="reqn">y=Kc</code> based on a variance estimator developed in Hainmueller and Hazlett (2014). Note that both matrices are <var>N</var> by <var>N</var> and therefore this results in increased memory and computing time.
</p>
<p>By default, <code>krls</code> uses the Gaussian Kernel (<code>whichkernel = "gaussian"</code>) given by 
</p>
<p style="text-align: center;"><code class="reqn">k(x_i,x_j)=exp(\frac{-|| x_i - x_j ||^2}{\sigma^2})</code>
</p>

<p>where <code class="reqn">||x_i - x_j||</code> is the Euclidean distance. The kernel bandwidth <code class="reqn">\sigma^2</code> is set to <code class="reqn">D</code>, the number of dimensions, by default, but the user can also specify other values using the <code>sigma</code> argument to implement other approaches. 
</p>
<p>If <code>derivative=TRUE</code> is specified, <code>krls</code> also computes the pointwise partial derivatives of the fitted function wrt to each predictor using the estimators developed in Hainmueller and Hazlett (2014). These can be used to examine the marginal effects of each predictor and how the marginal effects vary across the covariate space. Average derivatives are also computed with variances. Note that the <code>derivative=TRUE</code> option results in increased computing time and is only supported for the Gaussian kernel, i.e. when <code>whichkernel = "gaussian"</code>. Also <code>derivative=TRUE</code> requires that <code>vcov=TRUE</code>. 
</p>
<p>If <code>binary=TRUE</code> is also specified, the function will identify binary predictors and return first differences for these predictors instead of partial derivatives. First differences are computed going from the minimum to the maximum value of each binary predictor. Note that first differences are more appropriate to summarize the effects for binary predictors (see Hainmueller and Hazlett (2014) for details). 
</p>
<p>A few other kernels are also implemented, but derivatives are currently not supported for these: &quot;linear&quot;: <code class="reqn">k(x_i,x_j)=x_i'x_j</code>, &quot;poly1&quot;, &quot;poly2&quot;, &quot;poly3&quot;, &quot;poly4&quot; are polynomial kernels based on  <code class="reqn">k(x_i,x_j)=(x_i'x_j +1)^p</code> where <code class="reqn">p</code> is the order.
</p>


<h3>Value</h3>

<p>A list object of class <code>krls</code> with the following elements:
</p>
<table>
<tr><td><code>K</code></td>
<td>
<p><var>N</var> by <var>N</var> matrix of pairwise kernel distances between observations.</p>
</td></tr>
<tr><td><code>coeffs</code></td>
<td>
<p><var>N</var> by 1 vector of choice coefficients <var>c</var>.</p>
</td></tr>
<tr><td><code>Le</code></td>
<td>
<p>scalar with sum of squared leave-one-out errors.</p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p><var>N</var> by 1 vector of fitted values.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>original <var>N</var> by <var>D</var> predictor data matrix.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>original <var>N</var> by 1 matrix of values of the outcome variable.</p>
</td></tr> 
<tr><td><code>sigma</code></td>
<td>
<p>scalar with value of bandwidth, <code class="reqn">\sigma^2</code>, used for the Gaussian kernel.</p>
</td></tr>   
<tr><td><code>lambda</code></td>
<td>
<p>scalar with value of regularization parameter, <code class="reqn">\lambda</code>, used (user specified or based on leave-one-out cross-validation).</p>
</td></tr>  
<tr><td><code>R2</code></td>
<td>
<p>scalar with value of R-squared</p>
</td></tr>
<tr><td><code>vcov.c</code></td>
<td>
<p><var>N</var> by <var>N</var> variance covariance matrix for choice coefficients (<code>NULL</code> unless <code>vcov=TRUE</code> is specified).</p>
</td></tr>
<tr><td><code>vcov.fitted</code></td>
<td>
<p><var>N</var> by <var>N</var> variance covariance matrix for fitted values (<code>NULL</code> unless <code>vcov=TRUE</code> is specified).</p>
</td></tr>  
<tr><td><code>derivatives</code></td>
<td>
<p><var>N</var> by <var>D</var> matrix of pointwise partial derivatives based on the Gaussian kernel (<code>NULL</code> unless <code>derivative=TRUE</code> is specified. If <code>binary=TRUE</code> is specified, first differences are returned for binary predictors.</p>
</td></tr>
<tr><td><code>avgderivatives</code></td>
<td>
<p>1 by <var>D</var> matrix of average derivative based on the Gaussian kernel (<code>NULL</code> unless <code>derivative=TRUE</code> is specified. If <code>binary=TRUE</code> is specified, average first differences are returned for binary predictors.</p>
</td></tr>
<tr><td><code>var.avgderivatives</code></td>
<td>
<p>1 by <var>D</var> matrix of variances for average derivative based on gaussian kernel (<code>NULL</code> unless <code>derivative=TRUE</code> is specified. If <code>binary=TRUE</code> is specified, variances for average first differences are returned for binary predictors.</p>
</td></tr>  
<tr><td><code>binaryindicator</code></td>
<td>
<p>1 by <var>D</var> matrix that indicates for each predictor if it is treated as binary or not (evaluates to FALSE unless <code>binary=TRUE</code> is specified and a predictor is recognized binary.</p>
</td></tr>  
</table>


<h3>Note</h3>

<p>The function requires the storage of a <var>N</var> by <var>N</var> kernel matrix and can therefore exceed the memory limits for very large datasets.
</p>
<p>Setting <code>derivative=FALSE</code> and <code>vcov=FALSE</code> is useful to reduce computing time if pointwise partial derivatives and or variance covariance matrices are not needed.
</p>


<h3>Author(s)</h3>

<p>Jens Hainmueller (Stanford) and Chad Hazlett (MIT)
</p>


<h3>References</h3>

<p>Jeremy Ferwerda, Jens Hainmueller, Chad J. Hazlett (2017). Kernel-Based Regularized Least Squares in R (KRLS) and Stata (krls).
Journal of Statistical Software, 79(3), 1-26. doi:10.18637/jss.v079.i03
</p>
<p>Hainmueller, J. and Hazlett, C. (2014). Kernel Regularized Least Squares: Reducing Misspecification Bias with a Flexible and Interpretable Machine Learning Approach. Political Analysis, 22(2)
</p>
<p>Rifkin, R. 2002. Everything Old is New Again: A fresh look at historical approaches in machine learning. Thesis, MIT. September, 2002.
</p>
<p>Evgeniou, T., Pontil, M., and Poggio, T. (2000). Regularization networks and support vector machines. Advances In Computational Mathematics, 13(1):1-50.
</p>
<p>Schoelkopf, B., Herbrich, R. and Smola, A.J. (2001) A generalized representer theorem. In 14th Annual Conference on Computational Learning Theory, pages 416-426.
</p>
<p>Kimeldorf, G.S. Wahba, G. 1971. Some results on Tchebycheffian spline functions. Journal of Mathematical Analysis and Applications, 33:82-95.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.krls">predict.krls</a></code> for fitted values and predictions. <code><a href="#topic+summary.krls">summary.krls</a></code> for summary of the fit. <code><a href="#topic+plot.krls">plot.krls</a></code> for plots of the fit. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Linear example
# set up data
N &lt;- 200
x1 &lt;- rnorm(N)
x2 &lt;- rbinom(N,size=1,prob=.2)
y &lt;- x1 + .5*x2 + rnorm(N,0,.15)
X &lt;- cbind(x1,x2)
# fit model
krlsout &lt;- krls(X=X,y=y)
# summarize marginal effects and contribution of each variable
summary(krlsout)
# plot marginal effects and conditional expectation plots
plot(krlsout)


# non-linear example
# set up data
N &lt;- 200
x1 &lt;- rnorm(N)
x2 &lt;- rbinom(N,size=1,prob=.2)
y &lt;- x1^3 + .5*x2 + rnorm(N,0,.15)
X &lt;- cbind(x1,x2)

# fit model
krlsout &lt;- krls(X=X,y=y)
# summarize marginal effects and contribution of each variable
summary(krlsout)
# plot marginal effects and conditional expectation plots
plot(krlsout)

## 2D example:
# predictor data
X &lt;- matrix(seq(-3,3,.1))
# true function
Ytrue &lt;- sin(X)
# add noise 
Y     &lt;- sin(X) + rnorm(length(X),sd=.3)
# approximate function using KRLS
out &lt;- krls(y=Y,X=X)
# get fitted values and ses
fit &lt;- predict(out,newdata=X,se.fit=TRUE)
# results
par(mfrow=c(2,1))
plot(y=Ytrue,x=X,type="l",col="red",ylim=c(-1.2,1.2),lwd=2,main="f(x)")
points(y=fit$fit,X,col="blue",pch=19)
arrows(y1=fit$fit+1.96*fit$se.fit,
       y0=fit$fit-1.96*fit$se.fit,
       x1=X,x0=X,col="blue",length=0)
legend("bottomright",legend=c("true f(x)=sin(x)","KRLS fitted f(x)"),
       lty=c(1,NA),pch=c(NA,19),lwd=c(2,NA),col=c("red","blue"),cex=.8)

plot(y=cos(X),x=X,type="l",col="red",ylim=c(-1.2,1.2),lwd=2,main="df(x)/dx")
points(y=out$derivatives,X,col="blue",pch=19)

legend("bottomright",legend=c("true df(x)/dx=cos(x)","KRLS fitted df(x)/dx"),
       lty=c(1,NA),pch=c(NA,19),lwd=c(2,NA),col=c("red","blue"),,cex=.8)

## 3D example
# plot true function
par(mfrow=c(1,2))
f&lt;-function(x1,x2){ sin(x1)*cos(x2)}
x1 &lt;- x2 &lt;-seq(0,2*pi,.2)
z   &lt;-outer(x1,x2,f)
persp(x1, x2, z,theta=30,main="true f(x1,x2)=sin(x1)cos(x2)")
# approximate function with KRLS
# data and outcomes
X &lt;- cbind(sample(x1,200,replace=TRUE),sample(x2,200,replace=TRUE))
y   &lt;- f(X[,1],X[,2])+ runif(nrow(X))
# fit surface
krlsout &lt;- krls(X=X,y=y)
# plot fitted surface
ff  &lt;- function(x1i,x2i,krlsout){predict(object=krlsout,newdata=cbind(x1i,x2i))$fit}
z   &lt;- outer(x1,x2,ff,krlsout=krlsout)
persp(x1, x2, z,theta=30,main="KRLS fitted f(x1,x2)")

</code></pre>

<hr>
<h2 id='lambdasearch'>
Leave-one-out optimization to find <code class="reqn">\lambda</code>
</h2><span id='topic+lambdasearch'></span>

<h3>Description</h3>

<p>Function conducts leave-one-out optimization to find <code class="reqn">\lambda</code> using a golden search search with caching. This function is called internally by <code><a href="#topic+krls">krls</a></code>. It would normally not be called by the user directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lambdasearch(L=NULL,
           U=NULL,
           y=NULL,
           Eigenobject=NULL,
           tol=NULL,
           noisy=FALSE,
           eigtrunc=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lambdasearch_+3A_l">L</code></td>
<td>
<p>Non-negative scalar that determines the lower bound of the search window. Default is <code>NULL</code> which means that the lower bound is found using an algorithm (see details).</p>
</td></tr>
<tr><td><code id="lambdasearch_+3A_u">U</code></td>
<td>
<p>Positive scalar that determines the upper bound of the search window. Default is <code>NULL</code> which means that the upper bound is found using an algorithm (see details).</p>
</td></tr>
<tr><td><code id="lambdasearch_+3A_y">y</code></td>
<td>
<p><var>N</var> by <var>1</var> matrix of outcomes.</p>
</td></tr>
<tr><td><code id="lambdasearch_+3A_eigenobject">Eigenobject</code></td>
<td>
<p>List that contains the eigenvalues and eigenvectors of the kernel matrix <var>K</var>.</p>
</td></tr>
<tr><td><code id="lambdasearch_+3A_tol">tol</code></td>
<td>
<p>Positive scalar that determines the tolerance used in the optimization routine used to find <code class="reqn">\lambda</code>. Default is <code>NULL</code> which means that convergence is achieved when the difference in the sum of squared leave-one-out errors between the <code>i</code> and the <code>i+1</code> iteration is less than <code>N * 10^-3</code>.</p>
</td></tr>
<tr><td><code id="lambdasearch_+3A_noisy">noisy</code></td>
<td>
<p>If <code>TRUE</code>, the function will print details of the golden section search.</p>
</td></tr>
<tr><td><code id="lambdasearch_+3A_eigtrunc">eigtrunc</code></td>
<td>

<p>Positive scalar value that determines truncation of eigenvalues for lamnda search window. See <code><a href="#topic+krls">krls</a></code> for details. Default is <code>NULL</code> which means no truncation.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, upper bound is found as follows:
Set j to n, decrease by one until the following is longer true:
sum(EigenValues / (EigenValues + j)) &lt; 1.
</p>
<p>By default, upper bound is found as follows:
Get the position, q, of the eigenvalue that is closest to max(Eigenvalue)/1000.
Set j to 0, increase in steps of 0.05 until the below is longer true: 
sum(EigenValues / (EigenValues + j)) &gt; q.
</p>


<h3>Value</h3>

<p>A scalar that contains the <code class="reqn">\lambda</code> that minimizes the sum of squared leave-one-out errors.
</p>


<h3>Author(s)</h3>

<p>Jens Hainmueller (Stanford) and Chad Hazlett (MIT)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+krls">krls</a></code>
</p>

<hr>
<h2 id='looloss'>
Loss Function for Leave One Out Error
</h2><span id='topic+looloss'></span>

<h3>Description</h3>

<p>Internal function that computes Leave-On-Out (LOO) Error for KRLS given a fixed value for lambda (the parameter that governs the tradeoff between model fit and complexity in KRLS).
This function is called internally by <code>krls</code> to find value of lambda that minimizes the LOO error. It would normally not be called by the user directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>looloss(y = NULL, Eigenobject = NULL,
lambda = NULL,eigtrunc=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="looloss_+3A_y">y</code></td>
<td>

<p>n by 1 vector of outcomes.
</p>
</td></tr>
<tr><td><code id="looloss_+3A_eigenobject">Eigenobject</code></td>
<td>

<p>Object from call to <code>eigen</code> that contains spectral decomposition of the n by n Kernel matrix.
</p>
</td></tr>
<tr><td><code id="looloss_+3A_lambda">lambda</code></td>
<td>

<p>Positive scalar value for lamnbda parameter.
</p>
</td></tr>
<tr><td><code id="looloss_+3A_eigtrunc">eigtrunc</code></td>
<td>

<p>Positive scalar value that determines truncation of eigenvalues for lamnda search window. See <code><a href="#topic+krls">krls</a></code> for details. Default is <code>NULL</code> which means no truncation.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar value for LOO error.
</p>


<h3>Author(s)</h3>

<p>Jens Hainmueller (Stanford) and Chad Hazlett (MIT)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+krls">krls</a></code>
</p>

<hr>
<h2 id='plot.krls'>
Plot method for Kernel-based Regularized Least Squares (KRLS) Model Fits
</h2><span id='topic+plot.krls'></span>

<h3>Description</h3>

<p>Produces two types of plots. The first type of plot shows histograms for the pointwise partial derivatives to examine the heterogeneity in the marginal effects of each predictor (<code>which</code>==1). The second type of plot shows estimates of the conditional expectation functions of <code class="reqn">E[Y|X]</code> for each predictor (<code>which==2</code>). For each plot, the predictor of interest varies from its 1st to its 3rd quartile values, while the other predictors are kept at the means (or other values specified in <code>setx</code>). For binary varibales the <code class="reqn">E[Y|X]</code> are predicted at the max and the min value of the predictor (instead of the range from the 1st to the 3rd quantile).</p>


<h3>Usage</h3>

<pre><code class='language-R'> ## S3 method for class 'krls'
plot(x,which=c(1:2),
 main="distributions of pointwise marginal effects",
 setx="mean",ask = prod(par("mfcol")) &lt; nplots,nvalues=50,probs=c(.25,.75),...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.krls_+3A_x">x</code></td>
<td>
<p>An object of class &quot;<code>krls</code>&quot; that results from call to <code><a href="#topic+krls">krls</a></code>.</p>
</td></tr>
<tr><td><code id="plot.krls_+3A_which">which</code></td>
<td>
<p>if a subset of the plots is required, specify a subset of the numbers <code>1:2</code>.</p>
</td></tr>
<tr><td><code id="plot.krls_+3A_main">main</code></td>
<td>
<p>main title for histograms of pointwise partial derivatives.</p>
</td></tr>
<tr><td><code id="plot.krls_+3A_setx">setx</code></td>
<td>
<p>either one of <code>mean</code> or <code>median</code> to hold other predictors at their mean or median values for the conditional expectation plots. Alternativley the user can specific a numeric vector with predictor values at which the other predictors should be fixed for the conditional expectation plots. If specifed in this way there must be one value per predictor and the order of the values much match the order of the predictor used in the predictor matrix of the krls fit passed in <code>x</code>.</p>
</td></tr>
<tr><td><code id="plot.krls_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is asked before each plot, see <code><a href="graphics.html#topic+par">par</a></code> (<code>ask=.</code>).</p>
</td></tr>
<tr><td><code id="plot.krls_+3A_nvalues">nvalues</code></td>
<td>
<p>scalar that specifies the number of values at which conditional expectations should be plotted.</p>
</td></tr>
<tr><td><code id="plot.krls_+3A_probs">probs</code></td>
<td>
<p>vector with numbers between 0 and 1 that specify the quantiles that determine the range for of the predictor values for which the conditional expectation should be plotted. By default we vary each predictor from the 1st quartile to the 3rd quartile value.</p>
</td></tr>
<tr><td><code id="plot.krls_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to lower level functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Notice that the historgrams for the partial derivatives can only be plotted if the KRLS object was computed with <code>krls(,derivatives=TRUE)</code>.
</p>


<h3>Author(s)</h3>

<p>Jens Hainmueller (Stanford) and Chad Hazlett (MIT)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+krls">krls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># non-linear example
# set up data
N &lt;- 200
x1 &lt;- rnorm(N)
x2 &lt;- rbinom(N,size=1,prob=.2)
y &lt;- x1^3 + .5*x2 + rnorm(N,0,.15)
X &lt;- cbind(x1,x2)

# fit model
krlsout &lt;- krls(X=X,y=y)
# summarize marginal effects and contribution of each variable
summary(krlsout)
# plot marginal effects and conditional expectation plots
plot(krlsout)

</code></pre>

<hr>
<h2 id='predict.krls'>
Predict method for Kernel-based Regularized Least Squares (KRLS) Model Fits
</h2><span id='topic+predict.krls'></span>

<h3>Description</h3>

<p>Predicted values and standard errors based on krls model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> ## S3 method for class 'krls'
predict(object, newdata, se.fit = FALSE , ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.krls_+3A_object">object</code></td>
<td>
<p>Fitted KRLS model, i.e. an object of class <code>krls</code></p>
</td></tr>
<tr><td><code id="predict.krls_+3A_newdata">newdata</code></td>
<td>
<p>A data frame or matrix with variables values at which to predict the outcome. Number and order of columns in <code>newdata</code> have to match the corresponding predictors used in the fitted krls model given in <code>object</code>.
</p>
</td></tr>
<tr><td><code id="predict.krls_+3A_se.fit">se.fit</code></td>
<td>
<p>logical flag if standard errors should be computed for pointwise predictions.</p>
</td></tr>
<tr><td><code id="predict.krls_+3A_...">...</code></td>
<td>
<p>additional arguments affecting the predictions produced.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function produces predicted values, obtained by evaluating the fitted krls function with 
the newdata (ie. the test points). The prediction at a new test point <code class="reqn">x_i</code> is based on 
</p>
<p style="text-align: center;"><code class="reqn">f(x_i)= \sum_j=1^n c_j K_{x_j}(x_i)</code>
</p>
 
<p>where <code class="reqn">K</code> is the kernel matrix and thus <code class="reqn">K_{x_j}</code>
is a vector whose j-th entry is <code class="reqn">K(x_j,x_i)</code> (e.g. the distance between the test point <code class="reqn">x_i</code> and the training point <code class="reqn">x_j</code>). The training points are passed to the function with the krls fit in <code>object</code>.
</p>
<p>When data are missing in <code>newdata</code> during prediction, the value of each <code class="reqn">k(x_i,x_j)</code> is computed by using an adjusted Euclidean distance in the kernel definition. Assume <code class="reqn">x</code> is <var>D</var>-dimensional but a given pair of observations <code class="reqn">x_i</code> and <code class="reqn">x_j</code> have only <code class="reqn">D' &lt; D</code> non-missing dimensions in common. The adjusted Euclidean distance computes the sum of squared differences over the <code class="reqn">D'</code> non-missing dimensions, rescales this sum by <code class="reqn">D/D'</code>, and takes the square root.   The result corresponds to an assumption that conditional on the observed data, the missing values would not have contributed new information predictive of the outcome.
</p>


<h3>Value</h3>

<table>
<tr><td><code>fit</code></td>
<td>
<p><var>M</var> by 1 vector of fitted values for <var>M</var> test points.</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p><var>M</var> by 1 vector of standard errors for the fitted values for <var>M</var> test points (<code>NULL</code> unless <code>se.fit=TRUE</code> is specified).</p>
</td></tr>
<tr><td><code>vcov.fit</code></td>
<td>
<p><var>M</var> by <var>M</var> variance-covariance matrix for the fitted values for <var>M</var> test points (<code>NULL</code> unless <code>se.fit=TRUE</code> is specified).</p>
</td></tr>
<tr><td><code>newdata</code></td>
<td>
<p><var>M</var> by <var>D</var> data matrix of of <var>M</var> test points with <var>D</var> predictors.</p>
</td></tr>
<tr><td><code>newdataK</code></td>
<td>
<p><var>M</var> by <var>N</var> data matrix for pairwise Gauss Kernel distances between <var>M</var> test points and <var>N</var> training points from krls model fit in <code>object</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jens Hainmueller (Stanford) and Chad Hazlett (MIT)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+krls">krls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# make up data
X &lt;- seq(-3,3,.1)
Y &lt;- sin(X) + rnorm(length(X),.1)

# fit krls
krlsout &lt;- krls(y=Y,X=X)

# get in-sample prediction 
predin &lt;- predict(krlsout,newdata=X,se.fit=TRUE)

# get out-of-sample prediction
X2 &lt;- runif(5)
predout &lt;- predict(krlsout,newdata=X2,se.fit=TRUE)

# plot true function and predictions
plot(y=sin(X),x=X,type="l",col="red",ylim=c(-1.8,1.8),lwd=2,ylab="f(X)")
points(y=predin$fit,x=X,col="blue",pch=19)
arrows(y1=predin$fit+2*predin$se.fit,
       y0=predin$fit-2*predin$se.fit,
       x1=X,x0=X,col="blue",length=0)
       
points(y=predout$fit,x=X2,col="green",pch=17)
arrows(y1=predout$fit+2*predout $se.fit,
       y0=predout$fit-2*predout $se.fit,
       x1=X2,x0=X2,col="green",length=0)

legend("bottomright",
       legend=c("true f(x)=sin(X)",
                "KRLS fitted in-sample",
                "KRLS fitted out-of-sample"),
       lty=c(1,NA,NA),pch=c(NA,19,17),
       lwd=c(2,NA,NA),
       col=c("red","blue","green"),
       cex=.8)

</code></pre>

<hr>
<h2 id='solveforc'>
Solve for Choice Coefficients in KRLS 
</h2><span id='topic+solveforc'></span>

<h3>Description</h3>

<p>Internal function that computes choice coefficients for KRLS given a fixed value for lambda (the parameter that governs the tradeoff between model fit and complexity in KRLS).
This function is called internally by <code><a href="#topic+krls">krls</a></code>. It would normally not be called by the user directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solveforc(y = NULL, Eigenobject = NULL,
lambda = NULL,eigtrunc=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solveforc_+3A_y">y</code></td>
<td>

<p>n by 1 matrix of outcomes.
</p>
</td></tr>
<tr><td><code id="solveforc_+3A_eigenobject">Eigenobject</code></td>
<td>

<p>Object from call to <code><a href="base.html#topic+eigen">eigen</a></code> that contains spectral decomposition of the n by n Kernel matrix.
</p>
</td></tr>
<tr><td><code id="solveforc_+3A_lambda">lambda</code></td>
<td>

<p>Positive scalar value for lamnbda parameter.
</p>
</td></tr>
<tr><td><code id="solveforc_+3A_eigtrunc">eigtrunc</code></td>
<td>

<p>Positive scalar value that determines truncation of eigenvalues for lamnda search window. See <code><a href="#topic+krls">krls</a></code> for details. Default is <code>NULL</code> which means no truncation.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function relies on fast eigenvalue decomposition method described in  method Rifkin and Lippert (2007).
</p>


<h3>Value</h3>

<table>
<tr><td><code>coeffs</code></td>
<td>
<p>n by 1 one matrix of choice coefficients for KRLS model.</p>
</td></tr>
<tr><td><code>Le</code></td>
<td>
<p>n by 1 matrix of errors from leave-one-out validation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jens Hainmueller (Stanford) and Chad Hazlett (MIT)
</p>


<h3>References</h3>

<p>Rifkin, Ryan M. and Lippert, Ross A. (2007). Notes on Regularized Least Squares. MIT-CSAIL-TR-2007-025. CBCL-268
</p>


<h3>See Also</h3>

<p><code><a href="#topic+krls">krls</a></code>
</p>

<hr>
<h2 id='summary.krls'>
Summary method for Kernel-based Regularized Least Squares (KRLS) Model Fits
</h2><span id='topic+summary.krls'></span>

<h3>Description</h3>

<p>Summarizes average partial derivatives (i.e. marginal effects) and the distribution of the partial derivatives for each predictor. For binary predictors, the marginal effects are the first differences if <code>krls(,derivatives=TRUE,binary=TRUE)</code> was specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
 ## S3 method for class 'krls'
summary(object, probs=c(.25,.5,.75),...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.krls_+3A_object">object</code></td>
<td>
<p>Fitted krls model, i.e. an object of class krls</p>
</td></tr>
<tr><td><code id="summary.krls_+3A_probs">probs</code></td>
<td>
<p>numeric vector with numbers between 0 and 1 that specify the quantiles of the pointwise marginal effects for the summary (see the <code><a href="stats.html#topic+quantile">quantile</a></code> function for details).</p>
</td></tr>
<tr><td><code id="summary.krls_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to lower level functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Notice that the partial derivatives can only be summarized if the krls object was computed with <code>krls(,derivatives=TRUE)</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>coefficients</code></td>
<td>
<p>matrix with average partial derivates and or first differences (point estimates, standart errors, t-values, p-values).</p>
</td></tr>
<tr><td><code>qcoefficients</code></td>
<td>
<p>matrix with 1st, 2nd, and 3rd quatriles of distribution of pointwise marinal effects.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jens Hainmueller (Stanford) and Chad Hazlett (MIT)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+krls">krls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># non-linear example
# set up data
N &lt;- 200
x1 &lt;- rnorm(N)
x2 &lt;- rbinom(N,size=1,prob=.2)
y &lt;- x1^3 + .5*x2 + rnorm(N,0,.15)
X &lt;- cbind(x1,x2)

# fit model
krlsout &lt;- krls(X=X,y=y)
# summarize marginal effects and contribution of each variable
summary(krlsout)
# plot marginal effects and conditional expectation plots
plot(krlsout)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
