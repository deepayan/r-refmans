<!DOCTYPE html><html><head><title>Help for package plsgenomics</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {plsgenomics}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Colon'><p>Gene expression data from Alon et al. (1999)</p></a></li>
<li><a href='#Ecoli'><p>Ecoli gene expression and connectivity data from Kao et al. (2003)</p></a></li>
<li><a href='#gsim'><p>GSIM for binary data</p></a></li>
<li><a href='#gsim.cv'><p>Determination of the ridge regularization parameter and the bandwidth to be used for</p>
classification with GSIM for binary data</a></li>
<li><a href='#leukemia'><p>Gene expression data from Golub et al. (1999)</p></a></li>
<li><a href='#logit.spls'><p>Classification procedure for binary response based on a logistic model,</p>
solved by a combination of the Ridge Iteratively Reweighted Least Squares
(RIRLS) algorithm and the Adaptive Sparse PLS (SPLS) regression</a></li>
<li><a href='#logit.spls.cv'><p>Cross-validation procedure to calibrate the parameters (ncomp, lambda.l1,</p>
lambda.ridge) for the LOGIT-SPLS method</a></li>
<li><a href='#logit.spls.stab'><p>Stability selection procedure to estimate probabilities of selection of</p>
covariates for the LOGIT-SPLS method</a></li>
<li><a href='#matrix.heatmap'><p>Heatmap visualization for matrix</p></a></li>
<li><a href='#mgsim'><p>GSIM for categorical data</p></a></li>
<li><a href='#mgsim.cv'><p>Determination of the ridge regularization parameter and the bandwidth to be used for</p>
classification with GSIM for categorical data</a></li>
<li><a href='#mrpls'><p>Ridge Partial Least Square for categorical data</p></a></li>
<li><a href='#mrpls.cv'><p>Determination of the ridge regularization parameter and the number of PLS</p>
components to be used for classification with RPLS for categorical data</a></li>
<li><a href='#multinom.spls'><p>Classification procedure for multi-label response based on a multinomial</p>
model,  solved by a combination of the multinomial Ridge Iteratively
Reweighted Least Squares (multinom-RIRLS) algorithm and
the Adaptive Sparse PLS (SPLS) regression</a></li>
<li><a href='#multinom.spls.cv'><p>Cross-validation procedure to calibrate the parameters (ncomp, lambda.l1,</p>
lambda.ridge) for the multinomial-SPLS method</a></li>
<li><a href='#multinom.spls.stab'><p>Stability selection procedure  to estimate probabilities of selection of</p>
covariates for the multinomial-SPLS method</a></li>
<li><a href='#pls.lda'><p>Classification with PLS Dimension Reduction and Linear Discriminant</p>
Analysis</a></li>
<li><a href='#pls.lda.cv'><p>Determination of the number of latent components to be used for</p>
classification with PLS and LDA</a></li>
<li><a href='#pls.regression'><p>Multivariate Partial Least Squares Regression</p></a></li>
<li><a href='#pls.regression.cv'><p>Determination of the number of latent components to be used in PLS regression</p></a></li>
<li><a href='#plsgenomics-deprecated'><p>Deprecated function(s) in the 'plsgenomics' package</p></a></li>
<li><a href='#plsgenomics-internal'><p>Internal Functions for the 'plsgenomics' package</p></a></li>
<li><a href='#preprocess'><p>preprocess for microarray data</p></a></li>
<li><a href='#rpls'><p>Ridge Partial Least Square for binary data</p></a></li>
<li><a href='#rpls.cv'><p>Determination of the ridge regularization parameter and the number of PLS</p>
components to be used for classification with RPLS for binary data</a></li>
<li><a href='#sample.bin'><p>Generates covariate matrix X with correlated block of covariates and</p>
a binary random reponse depening on X through a logistic model</a></li>
<li><a href='#sample.cont'><p>Generates design matrix X with correlated block of covariates and a continuous random</p>
reponse Y depening on X through gaussian linear model Y=XB+E</a></li>
<li><a href='#sample.multinom'><p>Generates covariate matrix X with correlated block of covariates and</p>
a multi-label random reponse depening on X through a multinomial model</a></li>
<li><a href='#spls'><p>Adaptive Sparse Partial Least Squares (SPLS) regression</p></a></li>
<li><a href='#spls.cv'><p>Cross-validation procedure to calibrate the parameters (ncomp, lambda.l1)</p>
of the Adaptive Sparse PLS regression</a></li>
<li><a href='#spls.stab'><p>Stability selection procedure to estimate probabilities of selection of</p>
covariates for the sparse PLS method</a></li>
<li><a href='#SRBCT'><p>Gene expression data from Khan et al. (2001)</p></a></li>
<li><a href='#stability.selection'><p>Stability selection procedure to select covariates for the sparse PLS,</p>
LOGIT-SPLS and multinomial-SPLS methods</a></li>
<li><a href='#stability.selection.heatmap'><p>Heatmap visualization of estimated probabilities of selection for each</p>
covariate</a></li>
<li><a href='#TFA.estimate'><p>Prediction of Transcription Factor Activities using PLS</p></a></li>
<li><a href='#variable.selection'><p>Variable selection using the PLS weights</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.5-3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-26</td>
</tr>
<tr>
<td>Title:</td>
<td>PLS Analyses for Genomics</td>
</tr>
<tr>
<td>Author:</td>
<td>Anne-Laure Boulesteix &lt;boulesteix@ibe.med.uni-muenchen.de&gt;,
        Ghislain Durif &lt;gd.dev@libertymail.net&gt;,
        Sophie Lambert-Lacroix 
        &lt;sophie.lambert-lacroix@univ-grenoble-alpes.fr&gt;, Julie Peyre
        &lt;Julie.Peyre@univ-grenoble-alpes.fr&gt;, and Korbinian Strimmer
        &lt;k.strimmer@imperial.ac.uk&gt;.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ghislain Durif &lt;gd.dev@libertymail.net&gt;</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, boot, parallel, reshape2, plyr, fields, RhpcBLASctl</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Description:</td>
<td>Routines for PLS-based genomic analyses,
        implementing PLS methods for classification with
        microarray data and prediction of transcription factor
        activities from combined ChIP-chip analysis. The &gt;=1.2-1
        versions include two new classification methods for microarray
        data: GSIM and Ridge PLS. The &gt;=1.3 versions includes a
        new classification method combining variable selection and
        compression in logistic regression context: logit-SPLS; and
        an adaptive version of the sparse PLS.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/gdurif/plsgenomics">https://github.com/gdurif/plsgenomics</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/gdurif/plsgenomics/issues">https://github.com/gdurif/plsgenomics/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-27 08:53:02 UTC; drg</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-28 08:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='Colon'>Gene expression data from Alon et al. (1999)</h2><span id='topic+Colon'></span>

<h3>Description</h3>

<p>Gene expression data (2000 genes for 62 samples) from the
microarray experiments of Colon tissue samples of Alon et al. (1999). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Colon)
</code></pre>


<h3>Details</h3>

<p> This data set contains 62 samples 
with 2000 genes: 40 tumor tissues, coded 2 and 22 normal tissues, coded 1.  
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>a (62 x 2000) matrix giving the expression levels of 2000 
genes for the 62 Colon tissue samples. Each row corresponds to a patient, each column to a
gene.</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>a numeric vector of length 62 giving the type of tissue sample (tumor or normal).</p>
</td></tr> 
<tr><td><code>gene.names</code></td>
<td>
<p>a vector containing the names of the 2000 genes for the gene
expression matrix <code>X</code>.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The data are described in Alon et al. (1999). The data was originally 
collected from <code>http://microarray.princeton.edu/oncology/affydata/index.html</code> 
that is now (in 2024) <a href="http://genomics-pubs.princeton.edu/oncology/affydata/index.html">http://genomics-pubs.princeton.edu/oncology/affydata/index.html</a>.</p>


<h3>References</h3>

<p>Alon, U. and Barkai, N. and Notterman, D.A. and Gish, K. and Ybarra, S. and Mack, D. and Levine, 
A.J. (1999). Broad patterns of gene expression revealed by clustering analysis of tumor and normal 
colon tissues probed by oligonucleotide arrays, Proc. Natl. Acad. Sci. USA,<b>96</b>(12), 
6745&ndash;6750.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load data set
data(Colon)

# how many samples and how many genes ?
dim(Colon$X)

# how many samples of class 1 and 2 respectively ?
sum(Colon$Y==1)
sum(Colon$Y==2)
</code></pre>

<hr>
<h2 id='Ecoli'>Ecoli gene expression and connectivity data from Kao et al. (2003)</h2><span id='topic+Ecoli'></span>

<h3>Description</h3>

<p>Gene expression data obtained during Escherichia coli carbon source transition
and connectivity data from the RegulonDB data base (Salgado et al., 2001). 
The experiments  and data sets are described in Kao et al. (2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Ecoli)
</code></pre>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>CONNECdata</code></td>
<td>
<p>a (100 x 16) matrix containing the connectivity data for 100 genes and 16 
regulators. The data are coded as 1 (positive interaction), 0 (no interaction) and -1 (negative
interaction).</p>
</td></tr>
<tr><td><code>GEdata</code></td>
<td>
<p>a (100 x 23) matrix containing gene expression data for 100 genes and 23 
samples corresponding to differents times during carbon source transition.</p>
</td></tr>
<tr><td><code>timepoint</code></td>
<td>
<p>a numeric vector of length 23 containing the time points (in
hours) for the 23 samples.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The data are described in Kao et al. (2004). The data was originally 
collected from 
<code>http://www.seas.ucla.edu/~liaoj/downloads.html</code> that became
<a href="https://www.seas.ucla.edu/liao_lab//downloads.html">https://www.seas.ucla.edu/liao_lab//downloads.html</a> but the dataset
does not appear to be available anymore  and we could not find a 
replacement link in 2024.</p>


<h3>References</h3>

<p>K. Kao, Y.-L. Yang, R. Boscolo, C. Sabatti, V. Roychowdhury and J. C. Liao
(2004). Transcriptome-based determination of multiple transcription regulator
activities in Escherichia coli by using network component analysis, PNAS
<b>101</b>, 641&ndash;646. 
</p>
<p>H. Salgado, A. Santos-Zavaleta, S. Gama-Castro, D. Millan-Zarate, E.
Diaz-Peredo, F. Sanchez-Solano, E. Perez-Rueda, C. Bonavides-Martinez and J.
Collado-Vides (2001). RegulonDB (version 3.2): transcriptional regulation and
operon organization in Escherichia coli K-12, Nucleic Acids Research <b>29</b>,
72&ndash;74. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load data set
data(Ecoli)

# how many genes and how many transcription factors ?
dim(Ecoli$CONNECdata)


</code></pre>

<hr>
<h2 id='gsim'>GSIM for binary data</h2><span id='topic+gsim'></span>

<h3>Description</h3>

<p>The function <code>gsim</code> performs prediction using Lambert-Lacroix and Peyre's GSIM algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsim(Xtrain, Ytrain, Xtest=NULL, Lambda, hA, hB=NULL, NbIterMax=50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsim_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictors. <code>Xtrain</code> must be a matrix. 
Each row corresponds to an observation and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="gsim_+3A_ytrain">Ytrain</code></td>
<td>
<p>a ntrain vector of responses. <code>Ytrain</code> must be a vector. 
<code>Ytrain</code> is a {1,2}-valued vector and contains the response variable for each
observation.</p>
</td></tr>
<tr><td><code id="gsim_+3A_xtest">Xtest</code></td>
<td>
<p>a (ntest x p) matrix containing the predictors for the test data
set. <code>Xtest</code> may also be a vector of length p (corresponding to only one
test observation). If <code>Xtest</code> is not equal to NULL, then the prediction 
step is made for these new predictor variables.</p>
</td></tr>
<tr><td><code id="gsim_+3A_lambda">Lambda</code></td>
<td>
<p>a positive real value. <code>Lambda</code> is the ridge regularization parameter.</p>
</td></tr>
<tr><td><code id="gsim_+3A_ha">hA</code></td>
<td>
<p>a strictly positive real value. <code>hA</code> is the bandwidth for GSIM step A.</p>
</td></tr>
<tr><td><code id="gsim_+3A_hb">hB</code></td>
<td>
<p>a strictly positive real value. <code>hB</code> is the bandwidth for 
GSIM step B. if <code>hB</code> is equal to NULL, then hB value is chosen using a
plug-in method.</p>
</td></tr>
<tr><td><code id="gsim_+3A_nbitermax">NbIterMax</code></td>
<td>
<p>a positive integer. <code>NbIterMax</code> is the maximal number of
iterations in the Newton-Rapson parts.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>Xtrain</code> and <code>Xtest</code> may not be standardized, 
since standardizing is performed by the function <code>gsim</code> as a preliminary step
before the algorithm is run. 
</p>
<p>The procedure described in Lambert-Lacroix and Peyre (2005) is used to estimate 
the projection direction beta. When <code>Xtest</code> 
is not equal to NULL, the procedure predicts the labels for these new predictor variables.  
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Ytest</code></td>
<td>
<p>the ntest vector containing the predicted labels for the observations from 
<code>Xtest</code>.</p>
</td></tr> 
<tr><td><code>beta</code></td>
<td>
<p>the p vector giving the projection direction estimated.</p>
</td></tr>
<tr><td><code>hB</code></td>
<td>
<p>the value of hB used in step B of GSIM (value given by the user or
estimated by plug-in if the argument value was equal to NULL)</p>
</td></tr>
<tr><td><code>DeletedCol</code></td>
<td>
<p>the vector containing the column number of <code>Xtrain</code> when the 
variance of the corresponding predictor variable is null. Otherwise <code>DeletedCol</code>=NULL</p>
</td></tr>
<tr><td><code>Cvg</code></td>
<td>
<p>the 0-1 value indicating convergence of the algorithm (1 for convergence, 
0 otherwise).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sophie Lambert-Lacroix 
(<a href="http://membres-timc.imag.fr/Sophie.Lambert/">http://membres-timc.imag.fr/Sophie.Lambert/</a>) 
and 
Julie Peyre (<a href="https://membres-ljk.imag.fr/Julie.Peyre/">https://membres-ljk.imag.fr/Julie.Peyre/</a>).
</p>


<h3>References</h3>

<p>S. Lambert-Lacroix,  J. Peyre . (2006) Local likelyhood regression in  generalized linear 
single-index models with applications to microarrays data. Computational Statistics and 
Data Analysis, vol 51, n 3, 2091-2113. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsim.cv">gsim.cv</a></code>, <code><a href="#topic+mgsim">mgsim</a></code>, <code><a href="#topic+mgsim.cv">mgsim.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load Colon data
data(Colon)
IndexLearn &lt;- c(sample(which(Colon$Y==2),12),sample(which(Colon$Y==1),8))

Xtrain &lt;- Colon$X[IndexLearn,]
Ytrain &lt;- Colon$Y[IndexLearn]
Xtest &lt;- Colon$X[-IndexLearn,]

# preprocess data
resP &lt;- preprocess(Xtrain= Xtrain, Xtest=Xtest,Threshold = c(100,16000),Filtering=c(5,500),
		log10.scale=TRUE,row.stand=TRUE)

# perform prediction by GSIM
res &lt;- gsim(Xtrain=resP$pXtrain,Ytrain= Ytrain,Xtest=resP$pXtest,Lambda=10,hA=50,hB=NULL)
   
res$Cvg
sum(res$Ytest!=Colon$Y[-IndexLearn])

</code></pre>

<hr>
<h2 id='gsim.cv'>Determination of the ridge regularization parameter and the bandwidth to be used for
classification with GSIM for binary data</h2><span id='topic+gsim.cv'></span>

<h3>Description</h3>

<p>The function <code>gsim.cv</code> determines the best ridge regularization parameter and bandwidth to be
used for classification with GSIM as described in Lambert-Lacroix and Peyre (2005).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsim.cv(Xtrain, Ytrain,LambdaRange,hARange,hB=NULL, NbIterMax=50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsim.cv_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictors. <code>Xtrain</code> must be a matrix. 
Each row corresponds to an observation and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="gsim.cv_+3A_ytrain">Ytrain</code></td>
<td>
<p>a ntrain vector of responses. <code>Ytrain</code> must be a vector. 
<code>Ytrain</code> is a {1,2}-valued vector and contains the response variable for each
observation.</p>
</td></tr>
<tr><td><code id="gsim.cv_+3A_lambdarange">LambdaRange</code></td>
<td>
<p>the vector of positive real value from which the best ridge regularization 
parameter has to be chosen by cross-validation.</p>
</td></tr>
<tr><td><code id="gsim.cv_+3A_harange">hARange</code></td>
<td>
<p>the vector of strictly positive real value from which the best bandwidth
has to be chosen by cross-validation for GSIM step A.</p>
</td></tr>
<tr><td><code id="gsim.cv_+3A_hb">hB</code></td>
<td>
<p>a strictly positive real value. <code>hB</code> is the bandwidth for 
GSIM step B. if <code>hB</code> is equal to NULL, then hB value is chosen using a
plug-in method.</p>
</td></tr>
<tr><td><code id="gsim.cv_+3A_nbitermax">NbIterMax</code></td>
<td>
<p>a positive integer. <code>NbIterMax</code> is the maximal number of
iterations in the Newton-Rapson parts.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cross-validation procedure described in Lambert-Lacroix and Peyre (2005)
is used to determine the best ridge regularization parameter and bandwidth to be
used for classification with GSIM for binary data (for categorical data see 
<code><a href="#topic+mgsim">mgsim</a></code> and <code><a href="#topic+mgsim.cv">mgsim.cv</a></code>).
At each cross-validation run, <code>Xtrain</code> is split into a pseudo training
set (ntrain - 1 samples) and a pseudo test set (1 sample) and the classification error rate is 
determined for each value of ridge regularization parameter and bandwidth. Finally, the function 
<code>gsim.cv</code> returns the values of the ridge regularization parameter and 
bandwidth for which the mean classification error rate is minimal. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Lambda</code></td>
<td>
<p>the optimal regularization parameter.</p>
</td></tr> 
<tr><td><code>hA</code></td>
<td>
<p>the optimal bandwidth parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sophie Lambert-Lacroix 
(<a href="http://membres-timc.imag.fr/Sophie.Lambert/">http://membres-timc.imag.fr/Sophie.Lambert/</a>) 
and 
Julie Peyre (<a href="https://membres-ljk.imag.fr/Julie.Peyre/">https://membres-ljk.imag.fr/Julie.Peyre/</a>).
</p>


<h3>References</h3>

<p>S. Lambert-Lacroix,  J. Peyre . (2006) Local likelyhood regression in  generalized linear 
single-index models with applications to microarrays data. Computational Statistics and 
Data Analysis, vol 51, n 3, 2091-2113.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mgsim">mgsim</a></code>, <code><a href="#topic+gsim">gsim</a></code>, <code><a href="#topic+gsim.cv">gsim.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## between 5~15 seconds
# load plsgenomics library
library(plsgenomics)

# load Colon data
data(Colon)
IndexLearn &lt;- c(sample(which(Colon$Y==2),12),sample(which(Colon$Y==1),8))

Xtrain &lt;- Colon$X[IndexLearn,]
Ytrain &lt;- Colon$Y[IndexLearn]
Xtest &lt;- Colon$X[-IndexLearn,]

# preprocess data
resP &lt;- preprocess(Xtrain= Xtrain, Xtest=Xtest,Threshold = c(100,16000),Filtering=c(5,500),
				log10.scale=TRUE,row.stand=TRUE)

# Determine optimum h and lambda
hl &lt;- gsim.cv(Xtrain=resP$pXtrain,Ytrain=Ytrain,hARange=c(7,20),LambdaRange=c(0.1,1),hB=NULL)

# perform prediction by GSIM  
res &lt;- gsim(Xtrain=resP$pXtrain,Ytrain=Ytrain,Xtest=resP$pXtest,Lambda=hl$Lambda,hA=hl$hA,hB=NULL)
res$Cvg
sum(res$Ytest!=Colon$Y[-IndexLearn])

## End(Not run)
</code></pre>

<hr>
<h2 id='leukemia'>Gene expression data from Golub et al. (1999)</h2><span id='topic+leukemia'></span>

<h3>Description</h3>

<p>Gene expression data (3051 genes and 38 tumor mRNA samples) from the
leukemia microarray study of Golub et al. (1999). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(leukemia)
</code></pre>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>a (38 x 3051) matrix giving the expression levels of 3051 
genes for 38 leukemia patients. Each row corresponds to a patient, each column to a
gene.</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>a numeric vector of length 38 giving the cancer class of each
patient.</p>
</td></tr> 
<tr><td><code>gene.names</code></td>
<td>
<p>a matrix containing the names of the 3051 genes for the gene
expression matrix <code>X</code>. The three columns correspond to
the gene 'index', 'ID', and 'Name', respectively.
</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The dataset was taken from
the R package multtest. The data are described in Golub et al. (1999). 
The data was originally collected from 
<code>http://www.broadinstitute.org/cgi-bin/cancer/publications/pub_paper.cgi?paper_id=43</code>
but the URL is not working anymore and we could not find a replacement link 
in 2024.</p>


<h3>References</h3>

<p>S. Dudoit, J. Fridlyand and T. P. Speed (2002). Comparison of discrimination
methods for the classification of tumors using gene expression data, Journal of
the American Statistical Association <b>97</b>, 77&ndash;87.  
</p>
<p>Golub et al. (1999). Molecular classification of cancer: class discovery
and class prediction by gene expression monitoring, Science <b>286</b>,
531&ndash;537.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load data set
data(leukemia)

# how many samples and how many genes ?
dim(leukemia$X)

# how many samples of class 1 and 2, respectively ?
sum(leukemia$Y==1)
sum(leukemia$Y==2)</code></pre>

<hr>
<h2 id='logit.spls'>Classification procedure for binary response based on a logistic model, 
solved by a combination of the Ridge Iteratively Reweighted Least Squares 
(RIRLS) algorithm and the Adaptive Sparse PLS (SPLS) regression</h2><span id='topic+logit.spls'></span>

<h3>Description</h3>

<p>The function <code>logit.spls</code> performs compression and variable selection 
in the context of binary classification (with possible prediction) 
using Durif et al. (2018) algorithm based on Ridge IRLS and sparse PLS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logit.spls(
  Xtrain,
  Ytrain,
  lambda.ridge,
  lambda.l1,
  ncomp,
  Xtest = NULL,
  adapt = TRUE,
  maxIter = 100,
  svd.decompose = TRUE,
  center.X = TRUE,
  scale.X = FALSE,
  weighted.center = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logit.spls_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictor values. 
<code>Xtrain</code> must be a matrix. Each row corresponds to an observation 
and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_ytrain">Ytrain</code></td>
<td>
<p>a (ntrain) vector of (continuous) responses. <code>Ytrain</code> 
must be a vector or a one column matrix, and contains the response variable 
for each observation. <code>Ytrain</code> should take values in {0,1}.</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_lambda.ridge">lambda.ridge</code></td>
<td>
<p>a positive real value. <code>lambda.ridge</code> is the Ridge 
regularization parameter for the RIRLS algorithm (see details).</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_lambda.l1">lambda.l1</code></td>
<td>
<p>a positive real value, in [0,1]. <code>lambda.l1</code> is the 
sparse penalty parameter for the dimension reduction step by sparse PLS 
(see details).</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_ncomp">ncomp</code></td>
<td>
<p>a positive integer. <code>ncomp</code> is the number of 
PLS components. If <code>ncomp=0</code>,then the Ridge regression is performed 
without any dimension reduction (no SPLS step).</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_xtest">Xtest</code></td>
<td>
<p>a (ntest x p) matrix containing the predictor values for the 
test data set. <code>Xtest</code> may also be a vector of length p 
(corresponding to only one test observation). Default value is NULL, 
meaning that no prediction is performed.</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_adapt">adapt</code></td>
<td>
<p>a boolean value, indicating whether the sparse PLS selection 
step sould be adaptive or not (see details).</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_maxiter">maxIter</code></td>
<td>
<p>a positive integer. <code>maxIter</code> is the maximal number of 
iterations in the Newton-Raphson parts in the RIRLS algorithm (see details).</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_svd.decompose">svd.decompose</code></td>
<td>
<p>a boolean parameter. <code>svd.decompose</code> indicates 
wether or not the predictor matrix <code>Xtrain</code> should be decomposed by 
SVD (singular values decomposition) for the RIRLS step (see details).</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_center.x">center.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be centered or not.</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_scale.x">scale.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be scaled or not 
(<code>scale.X=TRUE</code> implies <code>center.X=TRUE</code>) in the spls step.</p>
</td></tr>
<tr><td><code id="logit.spls_+3A_weighted.center">weighted.center</code></td>
<td>
<p>a boolean value indicating whether the centering 
should take into account the weighted l2 metric or not in the SPLS step.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>Xtrain</code> and <code>Xtest</code> may 
not be standardized, since standardizing can be performed by the function 
<code>logit.spls</code> as a preliminary step.
</p>
<p>The procedure described in Durif et al. (2018) is used to compute
latent sparse components that are used in a logistic regression model.
In addition, when a matrix <code>Xtest</code> is supplied, the procedure 
predicts the response associated to these new values of the predictors.
</p>


<h3>Value</h3>

<p>An object of class <code>logit.spls</code> with the following attributes
</p>
<table>
<tr><td><code>Coefficients</code></td>
<td>
<p>the (p+1) vector containing the linear coefficients 
associated to the predictors and intercept in the logistic model 
explaining the response Y.</p>
</td></tr>
<tr><td><code>hatY</code></td>
<td>
<p>the (ntrain) vector containing the estimated response value on 
the train set <code>Xtrain</code>.</p>
</td></tr>
<tr><td><code>hatYtest</code></td>
<td>
<p>the (ntest) vector containing the predicted labels 
for the observations from <code>Xtest</code> (if provided).</p>
</td></tr>
<tr><td><code>DeletedCol</code></td>
<td>
<p>the vector containing the indexes of columns with null 
variance in <code>Xtrain</code> that were skipped in the procedure.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>the active set of predictors selected by the procedures. 
<code>A</code> is a subset of 1:p.</p>
</td></tr>
<tr><td><code>Anames</code></td>
<td>
<p>Vector of selected predictor names, i.e. the names of the 
columns from <code>Xtrain</code> that are in <code>A</code>.</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>a {0,1} value indicating whether the RIRLS algorithm did
converge in less than <code>maxIter</code> iterations or not.</p>
</td></tr>
<tr><td><code>X.score</code></td>
<td>
<p>a (n x ncomp) matrix being the observations coordinates or 
scores in the new component basis produced by the SPLS step (sparse PLS). 
Each column t.k of <code>X.score</code> is a SPLS component.</p>
</td></tr>
<tr><td><code>X.weight</code></td>
<td>
<p>a (p x ncomp) matrix being the coefficients of predictors 
in each components produced by sparse PLS. Each column w.k of 
<code>X.weight</code> verifies t.k = Xtrain x w.k (as a matrix product).</p>
</td></tr>
<tr><td><code>Xtrain</code></td>
<td>
<p>the design matrix.</p>
</td></tr>
<tr><td><code>sXtrain</code></td>
<td>
<p>the scaled predictor matrix.</p>
</td></tr>
<tr><td><code>Ytrain</code></td>
<td>
<p>the response observations.</p>
</td></tr>
<tr><td><code>sPseudoVar</code></td>
<td>
<p>the scaled pseudo-response produced by the RIRLS
algorithm.</p>
</td></tr>
<tr><td><code>lambda.ridge</code></td>
<td>
<p>the Ridge hyper-parameter used to fit the model.</p>
</td></tr>
<tr><td><code>lambda.l1</code></td>
<td>
<p>the sparse hyper-parameter used to fit the model.</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p>the number of components used to fit the model.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the (ntrain x ntrain) matrix used to weight the metric in the 
sparse PLS step. <code>V</code> is the inverse of the covariance matrix of the 
pseudo-response produced by the RIRLS step.</p>
</td></tr>
<tr><td><code>proba</code></td>
<td>
<p>the (ntrain) vector of estimated probabilities for the 
observations in code <code>Xtrain</code>, that are used to estimate the 
<code>hatY</code> labels.</p>
</td></tr>
<tr><td><code>proba.test</code></td>
<td>
<p>the (ntest) vector of predicted probabilities for the 
new observations in <code>Xtest</code>, that are used to predict the 
<code>hatYtest</code> labels.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spls">spls</a></code>, <code><a href="#topic+logit.spls.cv">logit.spls.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
sample1 &lt;- sample.bin(n=n, p=p, kstar=20, lstar=2, 
                      beta.min=0.25, beta.max=0.75, 
                      mean.H=0.2, sigma.H=10, sigma.F=5)
X &lt;- sample1$X
Y &lt;- sample1$Y

### splitting between learning and testing set
index.train &lt;- sort(sample(1:n, size=round(0.7*n)))
index.test &lt;- (1:n)[-index.train]

Xtrain &lt;- X[index.train,]
Ytrain &lt;- Y[index.train,]
Xtest &lt;- X[index.test,]
Ytest &lt;- Y[index.test,]

### fitting the model, and predicting new observations
model1 &lt;- logit.spls(Xtrain=Xtrain, Ytrain=Ytrain, lambda.ridge=2, 
                     lambda.l1=0.5, ncomp=2, Xtest=Xtest, adapt=TRUE, 
                     maxIter=100, svd.decompose=TRUE)
                     
str(model1)

### prediction error rate
sum(model1$hatYtest!=Ytest) / length(index.test)

## End(Not run)

</code></pre>

<hr>
<h2 id='logit.spls.cv'>Cross-validation procedure to calibrate the parameters (ncomp, lambda.l1, 
lambda.ridge) for the LOGIT-SPLS method</h2><span id='topic+logit.spls.cv'></span>

<h3>Description</h3>

<p>The function <code>logit.spls.cv</code> chooses the optimal values for the 
hyper-parameter of the <code>logit.spls</code> procedure, by minimizing the 
averaged error of prediction over the hyper-parameter grid, 
using Durif et al. (2018) LOGIT-SPLS algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logit.spls.cv(
  X,
  Y,
  lambda.ridge.range,
  lambda.l1.range,
  ncomp.range,
  adapt = TRUE,
  maxIter = 100,
  svd.decompose = TRUE,
  return.grid = FALSE,
  ncores = 1,
  nfolds = 10,
  nrun = 1,
  center.X = TRUE,
  scale.X = FALSE,
  weighted.center = TRUE,
  seed = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logit.spls.cv_+3A_x">X</code></td>
<td>
<p>a (n x p) data matrix of predictors. <code>X</code> must be a matrix. 
Each row corresponds to an observation and each column to a 
predictor variable.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_y">Y</code></td>
<td>
<p>a (n) vector of (continuous) responses. <code>Y</code> must be a 
vector or a one column matrix. It contains the response variable for 
each observation. <code>Y</code> should take values in {0,1}.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_lambda.ridge.range">lambda.ridge.range</code></td>
<td>
<p>a vector of positive real values. 
<code>lambda.ridge</code> is the Ridge regularization parameter for the 
RIRLS algorithm (see details), the optimal value will be chosen among
<code>lambda.ridge.range</code>.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_lambda.l1.range">lambda.l1.range</code></td>
<td>
<p>a vecor of positive real values, in [0,1]. 
<code>lambda.l1</code> is the sparse penalty parameter for the dimension 
reduction step by sparse PLS (see details), the optimal value will be 
chosen among <code>lambda.l1.range</code>.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_ncomp.range">ncomp.range</code></td>
<td>
<p>a vector of positive integers. <code>ncomp</code> is the 
number of PLS components. The optimal value will be chosen 
among <code>ncomp.range</code>.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_adapt">adapt</code></td>
<td>
<p>a boolean value, indicating whether the sparse PLS selection 
step sould be adaptive or not (see details).</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_maxiter">maxIter</code></td>
<td>
<p>a positive integer, the maximal number of iterations in the 
RIRLS algorithm (see details).</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_svd.decompose">svd.decompose</code></td>
<td>
<p>a boolean parameter. <code>svd.decompose</code> indicates 
wether or not the predictor matrix <code>Xtrain</code> should be decomposed by 
SVD (singular values decomposition) for the RIRLS step (see details).</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_return.grid">return.grid</code></td>
<td>
<p>a boolean values indicating whether the grid of 
hyper-parameters values with corresponding mean prediction error rate over 
the folds should be returned or not.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_ncores">ncores</code></td>
<td>
<p>a positve integer, indicating the number of cores that the 
cross-validation is allowed to use for parallel computation (see details).</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_nfolds">nfolds</code></td>
<td>
<p>a positive integer indicating the number of folds in the 
K-folds cross-validation procedure, <code>nfolds=n</code> corresponds 
to the leave-one-out cross-validation, default is 10.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_nrun">nrun</code></td>
<td>
<p>a positive integer indicating how many times the K-folds cross-
validation procedure should be repeated, default is 1.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_center.x">center.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be centered or not.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_scale.x">scale.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be scaled or not 
(<code>scale.X=TRUE</code> implies <code>center.X=TRUE</code>) in the spls step.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_weighted.center">weighted.center</code></td>
<td>
<p>a boolean value indicating whether the centering 
should take into account the weighted l2 metric or not in the SPLS step.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_seed">seed</code></td>
<td>
<p>a positive integer value (default is NULL). If non NULL, 
the seed for pseudo-random number generation is set accordingly.</p>
</td></tr>
<tr><td><code id="logit.spls.cv_+3A_verbose">verbose</code></td>
<td>
<p>a boolean parameter indicating the verbosity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>X</code> may not be standardized, 
since standardizing is performed by the function <code>logit.spls.cv</code> 
as a preliminary step. 
</p>
<p>The procedure is described in Durif et al. (2018). The K-fold 
cross-validation can be summarize as follow: the train set is partitioned 
into K folds, for each value of hyper-parameters the model is fit K times, 
using each fold to compute the prediction error rate, and fitting the 
model on the remaining observations. The cross-validation procedure returns 
the optimal hyper-parameters values, meaning the one that minimize 
the averaged error of prediction averaged over all the folds.
</p>
<p>This procedures uses <code>mclapply</code> from the <code>parallel</code> package, 
available on GNU/Linux and MacOS. Users of Microsoft Windows can refer to 
the README file in the source to be able to use a mclapply type function.
</p>


<h3>Value</h3>

<p>An object of class <code>logit.spls</code> with the following attributes
</p>
<table>
<tr><td><code>lambda.ridge.opt</code></td>
<td>
<p>the optimal value in <code>lambda.ridge.range</code>.</p>
</td></tr>
<tr><td><code>lambda.l1.opt</code></td>
<td>
<p>the optimal value in <code>lambda.l1.range</code>.</p>
</td></tr>
<tr><td><code>ncomp.opt</code></td>
<td>
<p>the optimal value in <code>ncomp.range</code>.</p>
</td></tr>
<tr><td><code>conv.per</code></td>
<td>
<p>the overall percentage of models that converge during the 
cross-validation procedure.</p>
</td></tr>
<tr><td><code>cv.grid</code></td>
<td>
<p>the grid of hyper-parameters and corresponding prediction 
error rate averaged over the folds. <code>cv.grid</code> is NULL if 
<code>return.grid</code> is set to FALSE.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logit.spls">logit.spls</a></code>, <code><a href="#topic+logit.spls.stab">logit.spls.stab</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
sample1 &lt;- sample.bin(n=n, p=p, kstar=10, lstar=2, 
                      beta.min=0.25, beta.max=0.75, mean.H=0.2, 
                      sigma.H=10, sigma.F=5)

X &lt;- sample1$X
Y &lt;- sample1$Y

### hyper-parameters values to test
lambda.l1.range &lt;- seq(0.05,0.95,by=0.1) # between 0 and 1
ncomp.range &lt;- 1:10
# log-linear range between 0.01 a,d 1000 for lambda.ridge.range
logspace &lt;- function( d1, d2, n) exp(log(10)*seq(d1, d2, length.out=n))
lambda.ridge.range &lt;- signif(logspace(d1 &lt;- -2, d2 &lt;- 3, n=21), digits=3)

### tuning the hyper-parameters
cv1 &lt;- logit.spls.cv(X=X, Y=Y, lambda.ridge.range=lambda.ridge.range, 
                     lambda.l1.range=lambda.l1.range, 
                     ncomp.range=ncomp.range, 
                     adapt=TRUE, maxIter=100, svd.decompose=TRUE, 
                     return.grid=TRUE, ncores=1, nfolds=10)
                       
str(cv1)

## End(Not run)

</code></pre>

<hr>
<h2 id='logit.spls.stab'>Stability selection procedure to estimate probabilities of selection of 
covariates for the LOGIT-SPLS method</h2><span id='topic+logit.spls.stab'></span>

<h3>Description</h3>

<p>The function <code>logit.spls.stab</code> train a logit-spls model for each 
candidate values <code>(ncomp, lambda.l1, lambda.ridge)</code> of hyper-parameters 
on multiple sub-samplings in the data. The stability selection procedure 
selects the covariates that are selected by most of the models among the 
grid of hyper-parameters, following the procedure described in 
Durif et al. (2018). Candidates values for <code>ncomp</code>, <code>lambda.l1</code> 
and <code>lambda.l2</code> are respectively given by 
the input arguments <code>ncomp.range</code>, <code>lambda.l1.range</code> 
and <code>lambda.l2.range</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logit.spls.stab(
  X,
  Y,
  lambda.ridge.range,
  lambda.l1.range,
  ncomp.range,
  adapt = TRUE,
  maxIter = 100,
  svd.decompose = TRUE,
  ncores = 1,
  nresamp = 100,
  center.X = TRUE,
  scale.X = FALSE,
  weighted.center = TRUE,
  seed = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logit.spls.stab_+3A_x">X</code></td>
<td>
<p>a (n x p) data matrix of predictors. <code>X</code> must be a matrix. 
Each row corresponds to an observation and each column to a 
predictor variable.</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_y">Y</code></td>
<td>
<p>a (n) vector of (continuous) responses. <code>Y</code> must be a 
vector or a one column matrix. It contains the response variable for 
each observation. <code>Y</code> should take values in {0,1}.</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_lambda.ridge.range">lambda.ridge.range</code></td>
<td>
<p>a vector of positive real values. 
<code>lambda.ridge</code> is the Ridge regularization parameter for the 
RIRLS algorithm (see details), the optimal value will be chosen among
<code>lambda.ridge.range</code>.</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_lambda.l1.range">lambda.l1.range</code></td>
<td>
<p>a vecor of positive real values, in [0,1]. 
<code>lambda.l1</code> is the sparse penalty parameter for the dimension 
reduction step by sparse PLS (see details), the optimal value will be 
chosen among <code>lambda.l1.range</code>.</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_ncomp.range">ncomp.range</code></td>
<td>
<p>a vector of positive integers. <code>ncomp</code> is the 
number of PLS components. The optimal value will be chosen 
among <code>ncomp.range</code>.</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_adapt">adapt</code></td>
<td>
<p>a boolean value, indicating whether the sparse PLS selection 
step sould be adaptive or not (see details).</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_maxiter">maxIter</code></td>
<td>
<p>a positive integer, the maximal number of iterations in the 
RIRLS algorithm (see details).</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_svd.decompose">svd.decompose</code></td>
<td>
<p>a boolean parameter. <code>svd.decompose</code> indicates 
wether or not the predictor matrix <code>Xtrain</code> should be decomposed by 
SVD (singular values decomposition) for the RIRLS step (see details).</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_ncores">ncores</code></td>
<td>
<p>a positve integer, indicating the number of cores that the 
cross-validation is allowed to use for parallel computation (see details).</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_nresamp">nresamp</code></td>
<td>
<p>number of resamplings of the data to estimate the probility 
of selection for each covariate, default is 100.</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_center.x">center.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be centered or not.</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_scale.x">scale.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be scaled or not 
(<code>scale.X=TRUE</code> implies <code>center.X=TRUE</code>) in the spls step.</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_weighted.center">weighted.center</code></td>
<td>
<p>a boolean value indicating whether the centering 
should take into account the weighted l2 metric or not in the SPLS step.</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_seed">seed</code></td>
<td>
<p>a positive integer value (default is NULL). If non NULL, 
the seed for pseudo-random number generation is set accordingly.</p>
</td></tr>
<tr><td><code id="logit.spls.stab_+3A_verbose">verbose</code></td>
<td>
<p>a boolean parameter indicating the verbosity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>X</code> may not be standardized, 
since standardizing is performed by the function <code>logit.spls.stab</code> 
as a preliminary step. 
</p>
<p>The procedure is described in Durif et al. (2018). The stability selection 
procedure can be summarize as follow (c.f. Meinshausen and Buhlmann, 2010).
</p>
<p>(i) For each candidate values <code>(ncomp, lambda.l1, lambda.ridge)</code> of 
hyper-parameters, a logit-SPLS is trained on <code>nresamp</code> resamplings 
of the data. Then, for each triplet <code>(ncomp, lambda.l1, lambda.ridge)</code>, 
the probability that a covariate (i.e. a column in <code>X</code>) is selected is 
computed among the resamplings.
</p>
<p>(ii) Eventually, the set of &quot;stable selected&quot; variables corresponds to the 
set of covariates that were selected by most of the training among the 
grid of hyper-parameters candidate values.
</p>
<p>This function achieves the first step (i) of the stability selection 
procedure. The second step (ii) is achieved by the function 
<code><a href="#topic+stability.selection">stability.selection</a></code>.
</p>
<p>This procedures uses <code>mclapply</code> from the <code>parallel</code> package, 
available on GNU/Linux and MacOS. Users of Microsoft Windows can refer to 
the README file in the source to be able to use a mclapply type function.
</p>


<h3>Value</h3>

<p>An object with the following attributes
</p>
<table>
<tr><td><code>q.Lambda</code></td>
<td>
<p>A table with values of q.Lambda (c.f. Durif 
et al. (2018) for the notation), being the averaged number of covariates
selected among the entire grid of hyper-parameters candidates values,
for increasing size of hyper-parameter grid.</p>
</td></tr>
<tr><td><code>probs.lambda</code></td>
<td>
<p>A table with estimated probability of selection for each 
covariates depending on the candidates values for hyper-parameters.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>An integer values indicating the number of covariates in the 
model.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>
<p>Meinshausen, N., Buhlmann P. (2010). Stability Selection. Journal of the 
Royal Statistical Society: Series B (Statistical Methodology) 
72, no. 4, 417-473.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logit.spls">logit.spls</a></code>, <code><a href="#topic+stability.selection">stability.selection</a></code>, 
<code><a href="#topic+stability.selection.heatmap">stability.selection.heatmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
sample1 &lt;- sample.bin(n=n, p=p, kstar=10, lstar=2, 
                      beta.min=0.25, beta.max=0.75, mean.H=0.2, 
                      sigma.H=10, sigma.F=5)

X &lt;- sample1$X
Y &lt;- sample1$Y

### pertinent covariates id
sample1$sel

### hyper-parameters values to test
lambda.l1.range &lt;- seq(0.05,0.95,by=0.1) # between 0 and 1
ncomp.range &lt;- 1:10
# log-linear range between 0.01 a,d 1000 for lambda.ridge.range
logspace &lt;- function( d1, d2, n) exp(log(10)*seq(d1, d2, length.out=n))
lambda.ridge.range &lt;- signif(logspace(d1 &lt;- -2, d2 &lt;- 3, n=21), digits=3)

### tuning the hyper-parameters
stab1 &lt;- logit.spls.stab(X=X, Y=Y, lambda.ridge.range=lambda.ridge.range, 
                         lambda.l1.range=lambda.l1.range, 
                         ncomp.range=ncomp.range, 
                         adapt=TRUE, maxIter=100, svd.decompose=TRUE, 
                         ncores=1, nresamp=100)
                       
str(stab1)

### heatmap of estimated probabilities
stability.selection.heatmap(stab1)

### selected covariates
stability.selection(stab1, piThreshold=0.6, rhoError=10)

## End(Not run)

</code></pre>

<hr>
<h2 id='matrix.heatmap'>Heatmap visualization for matrix</h2><span id='topic+matrix.heatmap'></span>

<h3>Description</h3>

<p>Visualization of matrix entries in heatmap format, the color scale 
depends on the numerical values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrix.heatmap(mat, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matrix.heatmap_+3A_mat">mat</code></td>
<td>
<p>the matrix to visualize</p>
</td></tr>
<tr><td><code id="matrix.heatmap_+3A_...">...</code></td>
<td>
<p>any argument that could be pass to the functions 
<code><a href="fields.html#topic+image.plot">image.plot</a></code> or <code><a href="graphics.html#topic+image">image</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>matrix.heatmap</code> is a wrapper for the function 
<code><a href="fields.html#topic+image.plot">image.plot</a></code> from the 'fields' package.
</p>


<h3>Value</h3>

<p>No return, just plot the heatmap in the current graphic window.
</p>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logit.spls">logit.spls</a></code>, <code><a href="#topic+stability.selection">stability.selection</a></code>, 
<code><a href="#topic+stability.selection.heatmap">stability.selection.heatmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### load plsgenomics library
library(plsgenomics)

### generate a matrix
A = matrix(runif(10*10), ncol=10)

### heatmap of estimated probabilities
matrix.heatmap(A)

</code></pre>

<hr>
<h2 id='mgsim'>GSIM for categorical data</h2><span id='topic+mgsim'></span>

<h3>Description</h3>

<p>The function <code>mgsim</code> performs prediction using Lambert-Lacroix and Peyre's MGSIM algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mgsim(Ytrain,Xtrain,Lambda,h,Xtest=NULL,NbIterMax=50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mgsim_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictors. <code>Xtrain</code> must be a matrix. 
Each row corresponds to an observation and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="mgsim_+3A_ytrain">Ytrain</code></td>
<td>
<p>a ntrain vector of responses. <code>Ytrain</code> must be a vector. 
<code>Ytrain</code> is a {1,...,c+1}-valued vector and contains the response variable for each
observation. c+1 is the number of classes.</p>
</td></tr>
<tr><td><code id="mgsim_+3A_xtest">Xtest</code></td>
<td>
<p>a (ntest x p) matrix containing the predictors for the test data
set. <code>Xtest</code> may also be a vector of length p (corresponding to only one 
test observation). If <code>Xtest</code> is not equal to NULL, then the prediction 
step is made for these new predictor variables.</p>
</td></tr>
<tr><td><code id="mgsim_+3A_lambda">Lambda</code></td>
<td>
<p>a positive real value. <code>Lambda</code> is the ridge regularization parameter.</p>
</td></tr>
<tr><td><code id="mgsim_+3A_h">h</code></td>
<td>
<p>a strictly positive real value. <code>h</code> is the bandwidth for GSIM step A.</p>
</td></tr>
<tr><td><code id="mgsim_+3A_nbitermax">NbIterMax</code></td>
<td>
<p>a positive integer. <code>NbIterMax</code> is the maximal number of iterations in the 
Newton-Rapson parts.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>Xtrain</code> and <code>Xtest</code> may not be standardized, 
since standardizing is performed by the function <code>mgsim</code> as a preliminary step
before the algorithm is run. 
</p>
<p>The procedure described in Lambert-Lacroix and Peyre (2005) is used to estimate 
the c projection directions and the coefficients of the parametric fit obtained 
after projecting predictor variables onto the estimated directions. When <code>Xtest</code> 
is not equal to NULL, the procedure predicts the labels for these new predictor variables.  
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Ytest</code></td>
<td>
<p>the ntest vector containing the predicted labels for the observations from 
<code>Xtest</code>.</p>
</td></tr> 
<tr><td><code>beta</code></td>
<td>
<p>the (p x c) matrix containing the c estimated projection directions.</p>
</td></tr>
<tr><td><code>Coefficients</code></td>
<td>
<p>the (2 x c) matrix containing the coefficients of the parametric fit obtained 
after projecting predictor variables onto these estimated directions. </p>
</td></tr>
<tr><td><code>DeletedCol</code></td>
<td>
<p>the vector containing the column number of <code>Xtrain</code> when the 
variance of the corresponding predictor variable is null. Otherwise <code>DeletedCol</code>=NULL</p>
</td></tr>
<tr><td><code>Cvg</code></td>
<td>
<p>the 0-1 value indicating convergence of the algorithm (1 for convergence, 
0 otherwise).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sophie Lambert-Lacroix 
(<a href="http://membres-timc.imag.fr/Sophie.Lambert/">http://membres-timc.imag.fr/Sophie.Lambert/</a>) 
and 
Julie Peyre (<a href="https://membres-ljk.imag.fr/Julie.Peyre/">https://membres-ljk.imag.fr/Julie.Peyre/</a>).
</p>


<h3>References</h3>

<p>S. Lambert-Lacroix,  J. Peyre . (2006) Local likelyhood regression in  generalized linear 
single-index models with applications to microarrays data. Computational Statistics and 
Data Analysis, vol 51, n 3, 2091-2113. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mgsim.cv">mgsim.cv</a></code>, <code><a href="#topic+gsim">gsim</a></code>, <code><a href="#topic+gsim.cv">gsim.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load SRBCT data
data(SRBCT)
IndexLearn &lt;- c(sample(which(SRBCT$Y==1),10),sample(which(SRBCT$Y==2),4),
			sample(which(SRBCT$Y==3),7),sample(which(SRBCT$Y==4),9))

# perform prediction by MGSIM
res &lt;- mgsim(Ytrain=SRBCT$Y[IndexLearn],Xtrain=SRBCT$X[IndexLearn,],Lambda=0.001,h=19,
			Xtest=SRBCT$X[-IndexLearn,])
res$Cvg
sum(res$Ytest!=SRBCT$Y[-IndexLearn])

# prediction for another sample
Xnew &lt;- SRBCT$X[83,]
# projection of Xnew onto the c estimated direction
Xproj &lt;- Xnew %*% res$beta
# Compute the linear predictor for each classes expect class 1
eta &lt;- diag(cbind(rep(1,3),t(Xproj)) %*% res$Coefficients)
Ypred &lt;- which.max(c(0,eta))
Ypred
SRBCT$Y[83]

</code></pre>

<hr>
<h2 id='mgsim.cv'>Determination of the ridge regularization parameter and the bandwidth to be used for
classification with GSIM for categorical data</h2><span id='topic+mgsim.cv'></span>

<h3>Description</h3>

<p>The function <code>mgsim.cv</code> determines the best ridge regularization parameter and bandwidth to 
be used for classification with MGSIM as described in Lambert-Lacroix and Peyre (2005).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mgsim.cv(Ytrain,Xtrain,LambdaRange,hRange,NbIterMax=50)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mgsim.cv_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictors. <code>Xtrain</code> must be a matrix. 
Each row corresponds to an observation and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="mgsim.cv_+3A_ytrain">Ytrain</code></td>
<td>
<p>a ntrain vector of responses. <code>Ytrain</code> must be a vector. 
<code>Ytrain</code> is a {1,...,c+1}-valued vector and contains the response variable for each
observation. c+1 is the number of classes.</p>
</td></tr>
<tr><td><code id="mgsim.cv_+3A_lambdarange">LambdaRange</code></td>
<td>
<p>the vector of positive real value from which the best ridge regularization 
parameter has to be chosen by cross-validation.</p>
</td></tr>
<tr><td><code id="mgsim.cv_+3A_hrange">hRange</code></td>
<td>
<p>the vector of strictly positive real value from which the best bandwidth
has to be chosen by cross-validation.</p>
</td></tr>
<tr><td><code id="mgsim.cv_+3A_nbitermax">NbIterMax</code></td>
<td>
<p>a positive integer. <code>NbIterMax</code> is the maximal number of iterations in the 
Newton-Rapson parts.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cross-validation procedure described in Lambert-Lacroix and Peyre (2005)
is used to determine the best ridge regularization parameter and bandwidth to be
used for classification with GSIM for categorical data (for binary data see 
<code><a href="#topic+gsim">gsim</a></code> and <code><a href="#topic+gsim.cv">gsim.cv</a></code>).
At each cross-validation run, <code>Xtrain</code> is split into a pseudo training
set (ntrain-1 samples) and a pseudo test set (1 sample) and the 
classification error rate is determined for each
value of ridge regularization parameter and bandwidth. Finally, the function 
<code>mgsim.cv</code> returns the values of the ridge regularization parameter and 
bandwidth for which the mean classification error rate is minimal. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Lambda</code></td>
<td>
<p>the optimal regularization parameter.</p>
</td></tr> 
<tr><td><code>h</code></td>
<td>
<p>the optimal bandwidth parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sophie Lambert-Lacroix 
(<a href="http://membres-timc.imag.fr/Sophie.Lambert/">http://membres-timc.imag.fr/Sophie.Lambert/</a>) 
and 
Julie Peyre (<a href="https://membres-ljk.imag.fr/Julie.Peyre/">https://membres-ljk.imag.fr/Julie.Peyre/</a>).
</p>


<h3>References</h3>

<p>S. Lambert-Lacroix,  J. Peyre . (2006) Local likelyhood regression in  generalized linear 
single-index models with applications to microarrays data. Computational Statistics and 
Data Analysis, vol 51, n 3, 2091-2113. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mgsim">mgsim</a></code>, <code><a href="#topic+gsim">gsim</a></code>, <code><a href="#topic+gsim.cv">gsim.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## between 5~15 seconds
# load plsgenomics library
library(plsgenomics)

# load SRBCT data
data(SRBCT)
IndexLearn &lt;- c(sample(which(SRBCT$Y==1),10),sample(which(SRBCT$Y==2),4),
                sample(which(SRBCT$Y==3),7),sample(which(SRBCT$Y==4),9))

### Determine optimum h and lambda
# /!\ take 30 secondes to run
#hl &lt;- mgsim.cv(Ytrain=SRBCT$Y[IndexLearn],Xtrain=SRBCT$X[IndexLearn,],
#                            LambdaRange=c(0.1),hRange=c(7,20))

### perform prediction by MGSIM
#res &lt;- mgsim(Ytrain=SRBCT$Y[IndexLearn],Xtrain=SRBCT$X[IndexLearn,],Lambda=hl$Lambda,
#             h=hl$h,Xtest=SRBCT$X[-IndexLearn,])
#res$Cvg
#sum(res$Ytest!=SRBCT$Y[-IndexLearn])


## End(Not run)
</code></pre>

<hr>
<h2 id='mrpls'>Ridge Partial Least Square for categorical data</h2><span id='topic+mrpls'></span>

<h3>Description</h3>

<p>The function <code>mrpls</code> performs prediction using Fort et al. (2005) MRPLS algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mrpls(Ytrain,Xtrain,Lambda,ncomp,Xtest=NULL,NbIterMax=50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mrpls_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictors. <code>Xtrain</code> must be a matrix. 
Each row corresponds to an observation and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="mrpls_+3A_ytrain">Ytrain</code></td>
<td>
<p>a ntrain vector of responses. <code>Ytrain</code> must be a vector. 
<code>Ytrain</code> is a {0,...,c}-valued vector and contains the response variable for each
observation. c+1 is the number of classes.</p>
</td></tr>
<tr><td><code id="mrpls_+3A_xtest">Xtest</code></td>
<td>
<p>a (ntest x p) matrix containing the predictors for the test data
set. <code>Xtest</code> may also be a vector of length p (corresponding to only one 
test observation).If <code>Xtest</code> is not equal to NULL, then the prediction 
step is made for these new predictor variables.</p>
</td></tr>
<tr><td><code id="mrpls_+3A_lambda">Lambda</code></td>
<td>
<p>a positive real value. <code>Lambda</code> is the ridge regularization parameter.</p>
</td></tr>
<tr><td><code id="mrpls_+3A_ncomp">ncomp</code></td>
<td>
<p>a positive integer. <code>ncomp</code> is the number of PLS components. 
If <code>ncomp</code>=0,then the Ridge regression is performed without reduction 
dimension. </p>
</td></tr>
<tr><td><code id="mrpls_+3A_nbitermax">NbIterMax</code></td>
<td>
<p>a positive integer. <code>NbIterMax</code> is the maximal number of iterations in the 
Newton-Rapson parts.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>Xtrain</code> and <code>Xtest</code> may not be standardized, 
since standardizing is performed by the function <code>mrpls</code> as a preliminary step
before the algorithm is run. 
</p>
<p>The procedure described in Fort et al. (2005) is used to determine
latent components to be used for classification and when <code>Xtest</code> 
is not equal to NULL, the procedure predicts the labels for these new 
predictor variables.  
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Coefficients</code></td>
<td>
<p>the (p+1) x c matrix containing the coefficients weighting the block 
design matrix.</p>
</td></tr>
<tr><td><code>hatY</code></td>
<td>
<p>the ntrain vector containing the estimated {0,...,c}-valued labels for the 
observations from <code>Xtrain</code>.</p>
</td></tr>
<tr><td><code>hatYtest</code></td>
<td>
<p>the ntest vector containing the predicted {0,...,c}-valued labels for the 
observations from <code>Xtest</code>.</p>
</td></tr>
<tr><td><code>proba</code></td>
<td>
<p>the ntrain vector containing the estimated probabilities for the 
observations from <code>Xtrain</code>.</p>
</td></tr>
<tr><td><code>proba.test</code></td>
<td>
<p>the ntest vector containing the predicted probabilities for the 
observations from <code>Xtest</code>.</p>
</td></tr>
<tr><td><code>DeletedCol</code></td>
<td>
<p>the vector containing the column number of <code>Xtrain</code> when the 
variance of the corresponding predictor variable is null. Otherwise <code>DeletedCol</code>=NULL</p>
</td></tr>
<tr><td><code>hatYtest_k</code></td>
<td>
<p>If <code>ncomp</code> is greater than 1, <code>hatYtest_k</code> is a matrix 
of size ntest x ncomp in such a way that the kth column corresponds to the 
predicted label obtained with k PLS components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sophie Lambert-Lacroix 
(<a href="http://membres-timc.imag.fr/Sophie.Lambert/">http://membres-timc.imag.fr/Sophie.Lambert/</a>). 
</p>


<h3>References</h3>

<p>G. Fort, S. Lambert-Lacroix and Julie Peyre (2005). Reduction de dimension dans les modeles 
lineaires generalises : application a la classification supervisee de donnees issues des biopuces.
Journal de la SFDS, tome 146, n1-2, 117-152. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mrpls.cv">mrpls.cv</a></code>, <code><a href="#topic+rpls">rpls</a></code>, <code><a href="#topic+rpls.cv">rpls.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load SRBCT data
data(SRBCT)
IndexLearn &lt;- c(sample(which(SRBCT$Y==1),10),sample(which(SRBCT$Y==2),4),
			sample(which(SRBCT$Y==3),7),sample(which(SRBCT$Y==4),9))

# perform prediction by MRPLS
res &lt;- mrpls(Ytrain=SRBCT$Y[IndexLearn]-1,Xtrain=SRBCT$X[IndexLearn,],Lambda=0.001,ncomp=2,
			Xtest=SRBCT$X[-IndexLearn,])
sum(res$Ytest!=SRBCT$Y[-IndexLearn]-1)

# prediction for another sample
Xnew &lt;- SRBCT$X[83,]
# Compute the linear predictor for each classes expect class 1
eta &lt;- diag(t(cbind(c(1,Xnew),c(1,Xnew),c(1,Xnew))) %*% res$Coefficients)
Ypred &lt;- which.max(c(0,eta))
Ypred+1
SRBCT$Y[83]

</code></pre>

<hr>
<h2 id='mrpls.cv'>Determination of the ridge regularization parameter and the number of PLS 
components to be used for classification with RPLS for categorical data</h2><span id='topic+mrpls.cv'></span>

<h3>Description</h3>

<p>The function <code>mrpls.cv</code> determines the best ridge regularization parameter and the best 
number of PLS components to be used for classification for Fort et al. (2005) MRPLS algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mrpls.cv(Ytrain, Xtrain, LambdaRange, ncompMax, NbIterMax=50, ncores=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mrpls.cv_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictors. <code>Xtrain</code> must be a matrix. 
Each row corresponds to an observation and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="mrpls.cv_+3A_ytrain">Ytrain</code></td>
<td>
<p>a ntrain vector of responses. <code>Ytrain</code> must be a vector. 
<code>Ytrain</code> is a {0,...,c}-valued vector and contains the response variable for each
observation. c+1 is the number of classes.</p>
</td></tr>
<tr><td><code id="mrpls.cv_+3A_lambdarange">LambdaRange</code></td>
<td>
<p>the vector of positive real value from which the best ridge regularization 
parameter has to be chosen by cross-validation.</p>
</td></tr>
<tr><td><code id="mrpls.cv_+3A_ncompmax">ncompMax</code></td>
<td>
<p>a positive integer. The best number of components is chosen from  
1,...,<code>ncompMax</code>. If <code>ncompMax</code>=0,then the Ridge regression is performed without 
reduction dimension. </p>
</td></tr>
<tr><td><code id="mrpls.cv_+3A_nbitermax">NbIterMax</code></td>
<td>
<p>a positive integer. <code>NbIterMax</code> is the maximal number of iterations in the 
Newton-Rapson parts.</p>
</td></tr>
<tr><td><code id="mrpls.cv_+3A_ncores">ncores</code></td>
<td>
<p>a positive integer. The number of cores to be used for parallel computing 
(if different from 1)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A cross-validation procedure is used to determine the best ridge regularization parameter and 
number of PLS components to be used for classification with MRPLS for categorical data 
(for binary data see <code><a href="#topic+rpls">rpls</a></code> and <code><a href="#topic+rpls.cv">rpls.cv</a></code>).
At each cross-validation run, <code>Xtrain</code> is split into a pseudo training
set (ntrain-1 samples) and a pseudo test set (1 sample) and the classification error rate is 
determined for each value of ridge regularization parameter and number of components. Finally, 
the function <code>mrpls.cv</code> returns the values of the ridge regularization parameter and 
bandwidth for which the mean classification error rate is minimal. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Lambda</code></td>
<td>
<p>the optimal regularization parameter.</p>
</td></tr> 
<tr><td><code>ncomp</code></td>
<td>
<p>the optimal number of PLS components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sophie Lambert-Lacroix 
(<a href="http://membres-timc.imag.fr/Sophie.Lambert/">http://membres-timc.imag.fr/Sophie.Lambert/</a>). 
</p>


<h3>References</h3>

<p>G. Fort, S. Lambert-Lacroix and Julie Peyre (2005). Reduction de dimension dans les modeles 
lineaires generalises : application a la classification supervisee de donnees issues des biopuces.
Journal de la SFDS, tome 146, n1-2, 117-152. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mrpls">mrpls</a></code>, <code><a href="#topic+rpls">rpls</a></code>, <code><a href="#topic+rpls.cv">rpls.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## between 5~15 seconds
# load plsgenomics library
library(plsgenomics)

# load SRBCT data
data(SRBCT)
IndexLearn &lt;- c(sample(which(SRBCT$Y==1),10),sample(which(SRBCT$Y==2),4),
			sample(which(SRBCT$Y==3),7),sample(which(SRBCT$Y==4),9))

# Determine optimum ncomp and Lambda
nl &lt;- mrpls.cv(Ytrain=SRBCT$Y[IndexLearn]-1,Xtrain=SRBCT$X[IndexLearn,],
			LambdaRange=c(0.1,1),ncompMax=3)

# perform prediction by MRPLS
res &lt;- mrpls(Ytrain=SRBCT$Y[IndexLearn]-1,Xtrain=SRBCT$X[IndexLearn,],Lambda=nl$Lambda,
			ncomp=nl$ncomp,Xtest=SRBCT$X[-IndexLearn,])
sum(res$Ytest!=SRBCT$Y[-IndexLearn]-1)

## End(Not run)
</code></pre>

<hr>
<h2 id='multinom.spls'>Classification procedure for multi-label response based on a multinomial 
model,  solved by a combination of the multinomial Ridge Iteratively 
Reweighted Least Squares (multinom-RIRLS) algorithm and 
the Adaptive Sparse PLS (SPLS) regression</h2><span id='topic+multinom.spls'></span>

<h3>Description</h3>

<p>The function <code>multinom.spls</code> performs compression and variable selection 
in the context of multi-label ('nclass' &gt; 2) classification 
(with possible prediction) using Durif et al. (2018) algorithm 
based on Ridge IRLS and sparse PLS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinom.spls(
  Xtrain,
  Ytrain,
  lambda.ridge,
  lambda.l1,
  ncomp,
  Xtest = NULL,
  adapt = TRUE,
  maxIter = 100,
  svd.decompose = TRUE,
  center.X = TRUE,
  scale.X = FALSE,
  weighted.center = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multinom.spls_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictor values. 
<code>Xtrain</code> must be a matrix. Each row corresponds to an observation 
and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_ytrain">Ytrain</code></td>
<td>
<p>a (ntrain) vector of (continuous) responses. <code>Ytrain</code> 
must be a vector or a one column matrix, and contains the response variable 
for each observation. <code>Ytrain</code> should take values in 
{0,...,nclass-1}, where nclass is the number of class.</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_lambda.ridge">lambda.ridge</code></td>
<td>
<p>a positive real value. <code>lambda.ridge</code> is the Ridge 
regularization parameter for the RIRLS algorithm (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_lambda.l1">lambda.l1</code></td>
<td>
<p>a positive real value, in [0,1]. <code>lambda.l1</code> is the 
sparse penalty parameter for the dimension reduction step by sparse PLS 
(see details).</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_ncomp">ncomp</code></td>
<td>
<p>a positive integer. <code>ncomp</code> is the number of 
PLS components. If <code>ncomp=0</code>,then the Ridge regression is performed 
without any dimension reduction (no SPLS step).</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_xtest">Xtest</code></td>
<td>
<p>a (ntest x p) matrix containing the predictor values for the 
test data set. <code>Xtest</code> may also be a vector of length p 
(corresponding to only one test observation). Default value is NULL, 
meaning that no prediction is performed.</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_adapt">adapt</code></td>
<td>
<p>a boolean value, indicating whether the sparse PLS selection 
step sould be adaptive or not (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_maxiter">maxIter</code></td>
<td>
<p>a positive integer. <code>maxIter</code> is the maximal number of 
iterations in the Newton-Raphson parts in the RIRLS algorithm (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_svd.decompose">svd.decompose</code></td>
<td>
<p>a boolean parameter. <code>svd.decompose</code> indicates 
wether or not the predictor matrix <code>Xtrain</code> should be decomposed by 
SVD (singular values decomposition) for the RIRLS step (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_center.x">center.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be centered or not.</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_scale.x">scale.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be scaled or not 
(<code>scale.X=TRUE</code> implies <code>center.X=TRUE</code>) in the spls step.</p>
</td></tr>
<tr><td><code id="multinom.spls_+3A_weighted.center">weighted.center</code></td>
<td>
<p>a boolean value indicating whether the centering 
should take into account the weighted l2 metric or not in the SPLS step.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>Xtrain</code> and <code>Xtest</code> may 
not be standardized, since standardizing can be performed by the function 
<code>multinom.spls</code> as a preliminary step.
</p>
<p>The procedure described in Durif et al. (2018) is used to compute
latent sparse components that are used in a multinomial regression model.
In addition, when a matrix <code>Xtest</code> is supplied, the procedure 
predicts the response associated to these new values of the predictors.
</p>


<h3>Value</h3>

<p>An object of class <code>multinom.spls</code> with the following attributes
</p>
<table>
<tr><td><code>Coefficients</code></td>
<td>
<p>a (p+1) x (nclass-1) matrix containing the linear 
coefficients associated to the predictors and intercept in the multinomial 
model 
explaining the response Y.</p>
</td></tr>
<tr><td><code>hatY</code></td>
<td>
<p>the (ntrain) vector containing the estimated response value on 
the train set <code>Xtrain</code>.</p>
</td></tr>
<tr><td><code>hatYtest</code></td>
<td>
<p>the (ntest) vector containing the predicted labels 
for the observations from <code>Xtest</code> (if provided).</p>
</td></tr>
<tr><td><code>DeletedCol</code></td>
<td>
<p>the vector containing the indexes of columns with null 
variance in <code>Xtrain</code> that were skipped in the procedure.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>a list of size nclass-1 with predictors selected by the procedures 
for each set of coefficients in the multinomial model (i.e. indexes of the 
corresponding non null entries in each columns of <code>Coefficients</code>. Each 
elements of <code>A</code> is a subset of 1:p.</p>
</td></tr>
<tr><td><code>A.full</code></td>
<td>
<p>union of elements in A, corresponding to predictors 
selected in the full model.</p>
</td></tr>
<tr><td><code>Anames</code></td>
<td>
<p>Vector of selected predictor names, i.e. the names of the 
columns from <code>Xtrain</code> that are in <code>A.full</code>.</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>a {0,1} value indicating whether the RIRLS algorithm did
converge in less than <code>maxIter</code> iterations or not.</p>
</td></tr>
<tr><td><code>X.score</code></td>
<td>
<p>list of nclass-1 different (n x ncomp) matrices being 
the observations coordinates or scores in the new component basis produced 
for each class in the multinomial model by the SPLS step (sparse PLS), 
see Durif et al. (2018) for details.</p>
</td></tr>
<tr><td><code>X.weight</code></td>
<td>
<p>list of nclass-1 different (p x ncomp) matrices being 
the coefficients of predictors in each components produced for each class 
in the multinomial model by the sparse PLS, 
see Durif et al. (2018) for details.</p>
</td></tr>
<tr><td><code>X.score.full</code></td>
<td>
<p>a ((n x (nclass-1)) x ncomp) matrix being the 
observations coordinates or scores in the new component basis produced 
by the SPLS step (sparse PLS) in the linearized multinomial model, see 
Durif et al. (2018). Each column t.k of <code>X.score</code> is a SPLS component.</p>
</td></tr>
<tr><td><code>X.weight.full</code></td>
<td>
<p>a (p x ncomp) matrix being the coefficients of predictors 
in each components produced by sparse PLS in the linearized multinomial 
model, see Durif et al. (2018). Each column w.k of 
<code>X.weight</code> verifies t.k = Xtrain x w.k (as a matrix product).</p>
</td></tr>
<tr><td><code>lambda.ridge</code></td>
<td>
<p>the Ridge hyper-parameter used to fit the model.</p>
</td></tr>
<tr><td><code>lambda.l1</code></td>
<td>
<p>the sparse hyper-parameter used to fit the model.</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p>the number of components used to fit the model.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the (ntrain x ntrain) matrix used to weight the metric in the 
sparse PLS step. <code>V</code> is the inverse of the covariance matrix of the 
pseudo-response produced by the RIRLS step.</p>
</td></tr>
<tr><td><code>proba</code></td>
<td>
<p>the (ntrain) vector of estimated probabilities for the 
observations in code <code>Xtrain</code>, that are used to estimate the 
<code>hatY</code> labels.</p>
</td></tr>
<tr><td><code>proba.test</code></td>
<td>
<p>the (ntest) vector of predicted probabilities for the 
new observations in <code>Xtest</code>, that are used to predict the 
<code>hatYtest</code> labels.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spls">spls</a></code>, <code><a href="#topic+logit.spls">logit.spls</a></code>, 
<code><a href="#topic+multinom.spls.cv">multinom.spls.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
nclass &lt;- 3
sample1 &lt;- sample.multinom(n, p, nb.class=nclass, kstar=20, lstar=2, 
                           beta.min=0.25, beta.max=0.75, 
                           mean.H=0.2, sigma.H=10, sigma.F=5)
X &lt;- sample1$X
Y &lt;- sample1$Y

### splitting between learning and testing set
index.train &lt;- sort(sample(1:n, size=round(0.7*n)))
index.test &lt;- (1:n)[-index.train]

Xtrain &lt;- X[index.train,]
Ytrain &lt;- Y[index.train,]
Xtest &lt;- X[index.test,]
Ytest &lt;- Y[index.test,]

### fitting the model, and predicting new observations
model1 &lt;- multinom.spls(Xtrain=Xtrain, Ytrain=Ytrain, lambda.ridge=2, 
                        lambda.l1=0.5, ncomp=2, Xtest=Xtest, adapt=TRUE, 
                        maxIter=100, svd.decompose=TRUE)
                     
str(model1)

### prediction error rate
sum(model1$hatYtest!=Ytest) / length(index.test)

## End(Not run)

</code></pre>

<hr>
<h2 id='multinom.spls.cv'>Cross-validation procedure to calibrate the parameters (ncomp, lambda.l1, 
lambda.ridge) for the multinomial-SPLS method</h2><span id='topic+multinom.spls.cv'></span>

<h3>Description</h3>

<p>The function <code>multinom.spls.cv</code> chooses the optimal values for the 
hyper-parameter of the <code>multinom.spls</code> procedure, by minimizing the 
averaged error of prediction over the hyper-parameter grid, 
using Durif et al. (2018) multinomial-SPLS algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinom.spls.cv(
  X,
  Y,
  lambda.ridge.range,
  lambda.l1.range,
  ncomp.range,
  adapt = TRUE,
  maxIter = 100,
  svd.decompose = TRUE,
  return.grid = FALSE,
  ncores = 1,
  nfolds = 10,
  nrun = 1,
  center.X = TRUE,
  scale.X = FALSE,
  weighted.center = TRUE,
  seed = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multinom.spls.cv_+3A_x">X</code></td>
<td>
<p>a (n x p) data matrix of predictors. <code>X</code> must be a matrix. 
Each row corresponds to an observation and each column to a 
predictor variable.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_y">Y</code></td>
<td>
<p>a (n) vector of (continuous) responses. <code>Y</code> must be a 
vector or a one column matrix. It contains the response variable for 
each observation. <code>Y</code> should take values in {0,...,nclass-1},
where nclass is the number of class.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_lambda.ridge.range">lambda.ridge.range</code></td>
<td>
<p>a vector of positive real values. 
<code>lambda.ridge</code> is the Ridge regularization parameter for the 
RIRLS algorithm (see details), the optimal value will be chosen among
<code>lambda.ridge.range</code>.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_lambda.l1.range">lambda.l1.range</code></td>
<td>
<p>a vecor of positive real values, in [0,1]. 
<code>lambda.l1</code> is the sparse penalty parameter for the dimension 
reduction step by sparse PLS (see details), the optimal value will be 
chosen among <code>lambda.l1.range</code>.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_ncomp.range">ncomp.range</code></td>
<td>
<p>a vector of positive integers. <code>ncomp</code> is the 
number of PLS components. The optimal value will be chosen 
among <code>ncomp.range</code>.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_adapt">adapt</code></td>
<td>
<p>a boolean value, indicating whether the sparse PLS selection 
step sould be adaptive or not (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_maxiter">maxIter</code></td>
<td>
<p>a positive integer, the maximal number of iterations in the 
RIRLS algorithm (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_svd.decompose">svd.decompose</code></td>
<td>
<p>a boolean parameter. <code>svd.decompose</code> indicates 
wether or not the predictor matrix <code>Xtrain</code> should be decomposed by 
SVD (singular values decomposition) for the RIRLS step (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_return.grid">return.grid</code></td>
<td>
<p>a boolean values indicating whether the grid of 
hyper-parameters values with corresponding mean prediction error rate over 
the folds should be returned or not.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_ncores">ncores</code></td>
<td>
<p>a positve integer, indicating the number of cores that the 
cross-validation is allowed to use for parallel computation (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_nfolds">nfolds</code></td>
<td>
<p>a positive integer indicating the number of folds in the 
K-folds cross-validation procedure, <code>nfolds=n</code> corresponds 
to the leave-one-out cross-validation, default is 10.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_nrun">nrun</code></td>
<td>
<p>a positive integer indicating how many times the K-folds cross-
validation procedure should be repeated, default is 1.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_center.x">center.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be centered or not.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_scale.x">scale.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be scaled or not 
(<code>scale.X=TRUE</code> implies <code>center.X=TRUE</code>) in the spls step.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_weighted.center">weighted.center</code></td>
<td>
<p>a boolean value indicating whether the centering 
should take into account the weighted l2 metric or not in the SPLS step.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_seed">seed</code></td>
<td>
<p>a positive integer value (default is NULL). If non NULL, 
the seed for pseudo-random number generation is set accordingly.</p>
</td></tr>
<tr><td><code id="multinom.spls.cv_+3A_verbose">verbose</code></td>
<td>
<p>a boolean parameter indicating the verbosity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>X</code> may not be standardized, 
since standardizing is performed by the function <code>multinom.spls.cv</code> 
as a preliminary step. 
</p>
<p>The procedure is described in Durif et al. (2018). The K-fold 
cross-validation can be summarize as follow: the train set is partitioned 
into K folds, for each value of hyper-parameters the model is fit K times, 
using each fold to compute the prediction error rate, and fitting the 
model on the remaining observations. The cross-validation procedure returns 
the optimal hyper-parameters values, meaning the one that minimize 
the averaged error of prediction averaged over all the folds.
</p>
<p>This procedures uses <code>mclapply</code> from the <code>parallel</code> package, 
available on GNU/Linux and MacOS. Users of Microsoft Windows can refer to 
the README file in the source to be able to use a mclapply type function.
</p>


<h3>Value</h3>

<p>An object of class <code>multinom.spls</code> with the following attributes
</p>
<table>
<tr><td><code>lambda.ridge.opt</code></td>
<td>
<p>the optimal value in <code>lambda.ridge.range</code>.</p>
</td></tr>
<tr><td><code>lambda.l1.opt</code></td>
<td>
<p>the optimal value in <code>lambda.l1.range</code>.</p>
</td></tr>
<tr><td><code>ncomp.opt</code></td>
<td>
<p>the optimal value in <code>ncomp.range</code>.</p>
</td></tr>
<tr><td><code>conv.per</code></td>
<td>
<p>the overall percentage of models that converge during the 
cross-validation procedure.</p>
</td></tr>
<tr><td><code>cv.grid</code></td>
<td>
<p>the grid of hyper-parameters and corresponding prediction 
error rate averaged over the folds. <code>cv.grid</code> is NULL if 
<code>return.grid</code> is set to FALSE.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+multinom.spls">multinom.spls</a></code>, <code><a href="#topic+multinom.spls.stab">multinom.spls.stab</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
nclass &lt;- 3
sample1 &lt;- sample.multinom(n=n, p=p, nb.class=nclass, kstar=10, lstar=2, 
                           beta.min=0.25, beta.max=0.75, mean.H=0.2, 
                           sigma.H=10, sigma.F=5)

X &lt;- sample1$X
Y &lt;- sample1$Y

### hyper-parameters values to test
lambda.l1.range &lt;- seq(0.05,0.95,by=0.1) # between 0 and 1
ncomp.range &lt;- 1:10
# log-linear range between 0.01 a,d 1000 for lambda.ridge.range
logspace &lt;- function( d1, d2, n) exp(log(10)*seq(d1, d2, length.out=n))
lambda.ridge.range &lt;- signif(logspace(d1 &lt;- -2, d2 &lt;- 3, n=21), digits=3)

### tuning the hyper-parameters
cv1 &lt;- multinom.spls.cv(X=X, Y=Y, lambda.ridge.range=lambda.ridge.range, 
                        lambda.l1.range=lambda.l1.range, 
                        ncomp.range=ncomp.range, 
                        adapt=TRUE, maxIter=100, svd.decompose=TRUE, 
                        return.grid=TRUE, ncores=1, nfolds=10)
                       
str(cv1)

## End(Not run)

</code></pre>

<hr>
<h2 id='multinom.spls.stab'>Stability selection procedure  to estimate probabilities of selection of
covariates for the multinomial-SPLS method</h2><span id='topic+multinom.spls.stab'></span>

<h3>Description</h3>

<p>The function <code>multinom.spls.stab</code> train a multinomial-spls model for 
each candidate values <code>(ncomp, lambda.l1, lambda.ridge)</code> of 
hyper-parameters on multiple sub-samplings in the data. The stability 
selection procedure selects the covariates that are selected by most of the 
models among the grid of hyper-parameters, following the procedure 
described in Durif et al.  (2018). Candidates values for <code>ncomp</code>, 
<code>lambda.l1</code> and <code>lambda.l2</code> are respectively given by 
the input arguments <code>ncomp.range</code>, <code>lambda.l1.range</code> 
and <code>lambda.l2.range</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinom.spls.stab(
  X,
  Y,
  lambda.ridge.range,
  lambda.l1.range,
  ncomp.range,
  adapt = TRUE,
  maxIter = 100,
  svd.decompose = TRUE,
  ncores = 1,
  nresamp = 100,
  center.X = TRUE,
  scale.X = FALSE,
  weighted.center = TRUE,
  seed = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multinom.spls.stab_+3A_x">X</code></td>
<td>
<p>a (n x p) data matrix of predictors. <code>X</code> must be a matrix. 
Each row corresponds to an observation and each column to a 
predictor variable.</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_y">Y</code></td>
<td>
<p>a (n) vector of (continuous) responses. <code>Y</code> must be a 
vector or a one column matrix. It contains the response variable for 
each observation.  <code>Y</code> should take values in {0,...,nclass-1}, 
where nclass is the number of class.</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_lambda.ridge.range">lambda.ridge.range</code></td>
<td>
<p>a vector of positive real values. 
<code>lambda.ridge</code> is the Ridge regularization parameter for the 
RIRLS algorithm (see details), the optimal value will be chosen among
<code>lambda.ridge.range</code>.</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_lambda.l1.range">lambda.l1.range</code></td>
<td>
<p>a vecor of positive real values, in [0,1]. 
<code>lambda.l1</code> is the sparse penalty parameter for the dimension 
reduction step by sparse PLS (see details), the optimal value will be 
chosen among <code>lambda.l1.range</code>.</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_ncomp.range">ncomp.range</code></td>
<td>
<p>a vector of positive integers. <code>ncomp</code> is the 
number of PLS components. The optimal value will be chosen 
among <code>ncomp.range</code>.</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_adapt">adapt</code></td>
<td>
<p>a boolean value, indicating whether the sparse PLS selection 
step sould be adaptive or not (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_maxiter">maxIter</code></td>
<td>
<p>a positive integer, the maximal number of iterations in the 
RIRLS algorithm (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_svd.decompose">svd.decompose</code></td>
<td>
<p>a boolean parameter. <code>svd.decompose</code> indicates 
wether or not the predictor matrix <code>Xtrain</code> should be decomposed by 
SVD (singular values decomposition) for the RIRLS step (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_ncores">ncores</code></td>
<td>
<p>a positve integer, indicating the number of cores that the 
cross-validation is allowed to use for parallel computation (see details).</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_nresamp">nresamp</code></td>
<td>
<p>number of resamplings of the data to estimate the probility 
of selection for each covariate, default is 100.</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_center.x">center.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be centered or not.</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_scale.x">scale.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be scaled or not 
(<code>scale.X=TRUE</code> implies <code>center.X=TRUE</code>) in the spls step.</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_weighted.center">weighted.center</code></td>
<td>
<p>a boolean value indicating whether the centering 
should take into account the weighted l2 metric or not in the SPLS step.</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_seed">seed</code></td>
<td>
<p>a positive integer value (default is NULL). If non NULL, 
the seed for pseudo-random number generation is set accordingly.</p>
</td></tr>
<tr><td><code id="multinom.spls.stab_+3A_verbose">verbose</code></td>
<td>
<p>a boolean parameter indicating the verbosity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>X</code> may not be standardized, 
since standardizing is performed by the function <code>multinom.spls.stab</code> 
as a preliminary step. 
</p>
<p>The procedure is described in Durif et al. (2018). The stability selection 
procedure can be summarize as follow (c.f. Meinshausen and Buhlmann, 2010).
</p>
<p>(i) For each candidate values <code>(ncomp, lambda.l1, lambda.ridge)</code> of 
hyper-parameters, a multinomial-spls is trained on <code>nresamp</code> 
resamplings of the data. Then, for each triplet 
<code>(ncomp, lambda.l1, lambda.ridge)</code>, the probability that a covariate 
(i.e. a column in <code>X</code>) is selected is computed among the resamplings.
</p>
<p>The estimated probabilities can be visualized as a heatmap with the 
function <code><a href="#topic+stability.selection.heatmap">stability.selection.heatmap</a></code>.
</p>
<p>(ii) Eventually, the set of &quot;stable selected&quot; variables corresponds to the 
set of covariates that were selected by most of the training among the 
grid of hyper-parameters candidate values.
</p>
<p>This function achieves the first step (i) of the stability selection 
procedure. The second step (ii) is achieved by the function 
<code><a href="#topic+stability.selection">stability.selection</a></code>
</p>
<p>This procedures uses <code>mclapply</code> from the <code>parallel</code> package, 
available on GNU/Linux and MacOS. Users of Microsoft Windows can refer to 
the README file in the source to be able to use a mclapply type function.
</p>


<h3>Value</h3>

<p>An object with the following attributes
</p>
<table>
<tr><td><code>q.Lambda</code></td>
<td>
<p>A table with values of q.Lambda (c.f. Durif 
et al. (2018) for the notation), being the averaged number of covariates
selected among the entire grid of hyper-parameters candidates values,
for increasing size of hyper-parameter grid.</p>
</td></tr>
<tr><td><code>probs.lambda</code></td>
<td>
<p>A table with estimated probability of selection for each 
covariates depending on the candidates values for hyper-parameters.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>An integer values indicating the number of covariates in the 
model.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>
<p>Meinshausen, N., Buhlmann P. (2010). Stability Selection. Journal of the 
Royal Statistical Society: Series B (Statistical Methodology) 
72, no. 4, 417-473.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+multinom.spls">multinom.spls</a></code>, <code><a href="#topic+stability.selection">stability.selection</a></code>, 
<code><a href="#topic+stability.selection.heatmap">stability.selection.heatmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
nclass &lt;- 3
sample1 &lt;- sample.multinom(n, p, nb.class=nclass, kstar=20, lstar=2, 
                           beta.min=0.25, beta.max=0.75, 
                           mean.H=0.2, sigma.H=10, sigma.F=5)

X &lt;- sample1$X
Y &lt;- sample1$Y

### pertinent covariates id
sample1$sel

### hyper-parameters values to test
lambda.l1.range &lt;- seq(0.05,0.95,by=0.1) # between 0 and 1
ncomp.range &lt;- 1:10
# log-linear range between 0.01 a,d 1000 for lambda.ridge.range
logspace &lt;- function( d1, d2, n) exp(log(10)*seq(d1, d2, length.out=n))
lambda.ridge.range &lt;- signif(logspace(d1 &lt;- -2, d2 &lt;- 3, n=21), digits=3)

### tuning the hyper-parameters
stab1 &lt;- multinom.spls.stab(X=X, Y=Y, lambda.ridge.range=lambda.ridge.range, 
                            lambda.l1.range=lambda.l1.range, 
                            ncomp.range=ncomp.range, 
                            adapt=TRUE, maxIter=100, svd.decompose=TRUE, 
                            ncores=1, nresamp=100)
                       
str(stab1)

### heatmap of estimated probabilities
stability.selection.heatmap(stab1)

### selected covariates
stability.selection(stab1, piThreshold=0.6, rhoError=10)

## End(Not run)

</code></pre>

<hr>
<h2 id='pls.lda'>Classification with PLS Dimension Reduction and Linear Discriminant
Analysis</h2><span id='topic+pls.lda'></span>

<h3>Description</h3>

<p>The function <code>pls.lda</code>  performs binary or multicategorical classification using the method 
described in Boulesteix (2004) which consists in PLS dimension reduction and linear
discriminant analysis applied on the PLS components. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pls.lda(Xtrain, Ytrain, Xtest=NULL, ncomp, nruncv=0, alpha=2/3, priors=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pls.lda_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix containing the predictors for the training data set.
Xtrain may be a matrix or a data frame. Each row is an observation and each column is a 
predictor variable.</p>
</td></tr>
<tr><td><code id="pls.lda_+3A_ytrain">Ytrain</code></td>
<td>
<p>a vector of length ntrain giving the classes of the ntrain
observations. The classes must be coded as 1,...,K (K&gt;=2).</p>
</td></tr>
<tr><td><code id="pls.lda_+3A_xtest">Xtest</code></td>
<td>
<p>a (ntest x p) data matrix containing the predictors for the test
data set.  <code>Xtest</code> may also be a
vector of length p (corresponding to only one test observation). If
<code>Xtest=NULL</code>, the training data set is considered as test data set as
well.</p>
</td></tr>
<tr><td><code id="pls.lda_+3A_ncomp">ncomp</code></td>
<td>
<p>if <code>nruncv=0</code>, <code>ncomp</code> is the number of latent components to 
be used for PLS dimension reduction. If
<code>nruncv&gt;0</code>, the cross-validation procedure described in Boulesteix (2004) is used
to choose the best number of components from the vector of integers <code>ncomp</code> or from 
1,...,<code>ncomp</code> if <code>ncomp</code> is of length 1.</p>
</td></tr>
<tr><td><code id="pls.lda_+3A_nruncv">nruncv</code></td>
<td>
<p>the number of cross-validation iterations to be performed for the choice of
the number of latent components. If <code>nruncv=0</code>, cross-validation is not performed 
and <code>ncomp</code> latent components are used.</p>
</td></tr>
<tr><td><code id="pls.lda_+3A_alpha">alpha</code></td>
<td>
<p>the proportion of observations to be included in the training set at
each cross-validation iteration.</p>
</td></tr>
<tr><td><code id="pls.lda_+3A_priors">priors</code></td>
<td>
<p>The class priors to be used for linear discriminant analysis. If
unspecified, the class proportions in the training set are used. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>pls.lda</code> proceeds as follows to predict the class of the
observations from the test data set. 
First, the SIMPLS algorithm is run on <code>Xtrain</code> and <code>Ytrain</code> to
determine the new PLS components based on the training observations only. 
The new PLS components are then computed for the test
data set. Classification is performed by applying classical linear 
discriminant analysis (LDA) to the new components. Of course, the LDA
classifier is built using the training observations only. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>predclass</code></td>
<td>
<p>the vector containing the predicted classes of the ntest observations from
<code>Xtest</code>.</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p>the number of latent components used for classification.</p>
</td></tr>
<tr><td><code>pls.out</code></td>
<td>
<p>an object containing the results from the call of the <code>pls.regression</code>
function (from the <code>plsgenomics</code> package).</p>
</td></tr>
<tr><td><code>lda.out</code></td>
<td>
<p>an object containing the results from the call of the <code>lda</code> 
function (from the <code>MASS</code> package).</p>
</td></tr>
<tr><td><code>pred.lda.out</code></td>
<td>
<p>an object containing the results from the call of the <code>predict.lda</code> 
function (from the <code>MASS</code> package).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Anne-Laure Boulesteix 
(<a href="https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html">https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html</a>) 
</p>


<h3>References</h3>

<p>A. L. Boulesteix (2004). PLS dimension reduction for classification with microarray data,
Statistical Applications in Genetics and Molecular Biology <b>3</b>, Issue 1, Article 33.
</p>
<p>A. L. Boulesteix, K. Strimmer (2007). Partial least squares: a versatile tool for the analysis 
of high-dimensional genomic data. Briefings in Bioinformatics 7:32-44.
</p>
<p>S. de Jong (1993). SIMPLS: an alternative approach to partial least squares 
regression, Chemometrics Intell. Lab. Syst. <b>18</b>, 251&ndash;263.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.regression">pls.regression</a></code>, <code><a href="#topic+variable.selection">variable.selection</a></code>,
<code><a href="#topic+pls.lda.cv">pls.lda.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load leukemia data
data(leukemia)

# Classify observations 1,2,3 (test set) using observations 4 to 38 (training set), 
# with 2 PLS components
pls.lda(Xtrain=leukemia$X[-(1:3),],Ytrain=leukemia$Y[-(1:3)],Xtest=leukemia$X[1:3,],
     	ncomp=2,nruncv=0)

# Classify observations 1,2,3 (test set) using observations 4 to 38 (training set), 
# with the best number of components as determined by cross-validation
pls.lda(Xtrain=leukemia$X[-(1:3),],Ytrain=leukemia$Y[-(1:3)],Xtest=leukemia$X[1:3,],
		ncomp=1:4,nruncv=20)

</code></pre>

<hr>
<h2 id='pls.lda.cv'>Determination of the number of latent components to be used for
classification with PLS and LDA</h2><span id='topic+pls.lda.cv'></span>

<h3>Description</h3>

<p>The function <code>pls.lda.cv</code> determines the best number of latent components to be used for 
classification with PLS dimension reduction and linear discriminant analysis as described in
Boulesteix (2004). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pls.lda.cv(Xtrain, Ytrain,  ncomp, nruncv=20, alpha=2/3, priors=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pls.lda.cv_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix containing the predictors for the training data set.
<code>Xtrain</code> may be a matrix or a data frame. Each row is an observation and each column is a 
predictor variable.</p>
</td></tr>
<tr><td><code id="pls.lda.cv_+3A_ytrain">Ytrain</code></td>
<td>
<p>a vector of length ntrain giving the classes of the ntrain
observations. The classes must be coded as 1,...,K (K&gt;=2).</p>
</td></tr>
<tr><td><code id="pls.lda.cv_+3A_ncomp">ncomp</code></td>
<td>
<p>the vector of integers from which the best number of latent 
components has to be chosen by cross-validation. If <code>ncomp</code> is of length
1, the best number of components is chosen from  1,...,<code>ncomp</code>.</p>
</td></tr>  <tr><td><code id="pls.lda.cv_+3A_nruncv">nruncv</code></td>
<td>
<p>the number 
of cross-validation iterations to be performed for the choice of the number of latent components.</p>
</td></tr>
<tr><td><code id="pls.lda.cv_+3A_alpha">alpha</code></td>
<td>
<p>the proportion of observations to be included in the training set at
each cross-validation iteration.</p>
</td></tr>
<tr><td><code id="pls.lda.cv_+3A_priors">priors</code></td>
<td>
<p>The class priors to be used for linear discriminant analysis. If
unspecified, the class proportions in the training set are used. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cross-validation procedure described in Boulesteix (2004) is used to
determine the best number of latent components to be used for classification.
At each cross-validation run, <code>Xtrain</code> is split into a pseudo training
set and a pseudo test set and the classification error rate is determined for each
number of latent components. Finally, the function <code>pls.lda.cv</code> returns
the number of latent components for which the mean classification rate over
the <code>nrun</code> partitions is minimal. 
</p>


<h3>Value</h3>

<p>The number of latent components to be used for classification.
</p>


<h3>Author(s)</h3>

<p>Anne-Laure Boulesteix 
(<a href="https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html">https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html</a>) 
</p>


<h3>References</h3>

<p>A. L. Boulesteix (2004). PLS dimension reduction for classification with microarray data,
Statistical Applications in Genetics and Molecular Biology <b>3</b>, Issue 1, Article 33.
</p>
<p>A. L. Boulesteix, K. Strimmer (2007). Partial least squares: a versatile tool for the analysis 
of high-dimensional genomic data. Briefings in Bioinformatics 7:32-44.
</p>
<p>S. de Jong (1993). SIMPLS: an alternative approach to partial least squares 
regression, Chemometrics Intell. Lab. Syst. <b>18</b>, 251&ndash;263.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.lda">pls.lda</a></code>, <code><a href="#topic+pls.regression.cv">pls.regression.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## between 5~15 seconds
# load plsgenomics library
library(plsgenomics)

# load leukemia data
data(leukemia)

# Determine the best number of components to be used for classification using the 
# cross-validation procedure
# choose the best number from 2,3,4
pls.lda.cv(Xtrain=leukemia$X,Ytrain=leukemia$Y,ncomp=2:4,nruncv=20)
# choose the best number from 1,2,3
pls.lda.cv(Xtrain=leukemia$X,Ytrain=leukemia$Y,ncomp=3,nruncv=20)


## End(Not run)
</code></pre>

<hr>
<h2 id='pls.regression'>Multivariate Partial Least Squares Regression</h2><span id='topic+pls.regression'></span>

<h3>Description</h3>

<p>The function <code>pls.regression</code> performs pls multivariate regression (with several response 
variables and several predictor variables) using de Jong's SIMPLS algorithm. This function 
is an adaptation of R. Wehrens' code from the package pls.pcr. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pls.regression(Xtrain, Ytrain, Xtest=NULL, ncomp=NULL,  unit.weights=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pls.regression_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictors. <code>Xtrain</code> may be a matrix or a
data frame. Each row corresponds to an observation and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="pls.regression_+3A_ytrain">Ytrain</code></td>
<td>
<p>a (ntrain x m) data matrix of responses. <code>Ytrain</code> may be a vector (if m=1), 
a matrix or a data frame. If <code>Ytrain</code> is a matrix or a data frame, each row corresponds 
to an observation and each column to a response variable. If <code>Ytrain</code> is a vector, it 
contains the unique response variable for each observation.</p>
</td></tr>
<tr><td><code id="pls.regression_+3A_xtest">Xtest</code></td>
<td>
<p>a (ntest x p) matrix containing the predictors for the test data
set. <code>Xtest</code> may also be a
vector of length p (corresponding to only one test observation).</p>
</td></tr>
<tr><td><code id="pls.regression_+3A_ncomp">ncomp</code></td>
<td>
<p>the number of latent components to be used for regression. If
<code>ncomp</code> is a vector of integers, the regression model is built
successively with each number of components. If <code>ncomp=NULL</code>, the maximal
number of components min(ntrain,p) is chosen.</p>
</td></tr>
<tr><td><code id="pls.regression_+3A_unit.weights">unit.weights</code></td>
<td>
<p>if <code>TRUE</code> then the latent components
will be constructed from weight vectors that are standardized to length 1, 
otherwise the weight vectors do not have length 1 but the latent components have
norm 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>Xtrain</code> and <code>Ytrain</code> must not be centered to have 
mean zero, since centering is performed by the function <code>pls.regression</code> as a preliminary 
step before the SIMPLS algorithm is run. 
</p>
<p>In the original definition of SIMPLS by de Jong (1993), the weight vectors have
length 1. If the weight vectors are standardized to have length 1, 
they satisfy a simple optimality criterion (de Jong, 1993). However, it is
also usual (and computationally efficient) to standardize the latent
components to have length 1. 
</p>
<p>In contrast to the original version found in the package <code>pls.pcr</code>, 
the prediction for the observations from <code>Xtest</code> is performed after
centering the columns of <code>Xtest</code> by substracting the columns means
calculated from <code>Xtrain</code>.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>B</code></td>
<td>
<p>the (p x m x length(<code>ncomp</code>)) matrix containing the regression coefficients. Each 
row corresponds to a predictor variable and each column to a response variable. The third
dimension of the matrix <code>B</code> corresponds to the number of PLS components used
to compute the regression coefficients. If <code>ncomp</code> has length 1, <code>B</code>
is just a (p x m) matrix.</p>
</td></tr>
<tr><td><code>Ypred</code></td>
<td>
<p>the (ntest x m x length(<code>ncomp</code>)) containing the predicted
values of the response variables for the observations from <code>Xtest</code>. The
third dimension of the matrix <code>Ypred</code> corresponds to the number of PLS
components used to compute the regression coefficients. </p>
</td></tr> 
<tr><td><code>P</code></td>
<td>
<p>the (p x max(<code>ncomp</code>)) matrix containing the X-loadings.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>the (m x max(<code>ncomp</code>)) matrix containing the Y-loadings.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the (ntrain x max(<code>ncomp</code>)) matrix containing the X-scores (latent components)</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>the (p x max(<code>ncomp</code>)) matrix containing the weights used to construct the
latent components.</p>
</td></tr>
<tr><td><code>meanX</code></td>
<td>
<p>the p-vector containing the means of the columns of <code>Xtrain</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Anne-Laure Boulesteix 
(<a href="https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html">https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html</a>) 
and 
Korbinian Strimmer (<a href="https://strimmerlab.github.io/korbinian.html">https://strimmerlab.github.io/korbinian.html</a>).
</p>
<p>Adapted in part from 
pls.pcr code by R. Wehrens (in a former version of the 'pls' package
<a href="https://CRAN.R-project.org/package=pls">https://CRAN.R-project.org/package=pls</a>). 
</p>


<h3>References</h3>

<p>S. de Jong (1993). SIMPLS: an alternative approach to partial least squares 
regression, Chemometrics Intell. Lab. Syst. <b>18</b>, 251&ndash;263.
</p>
<p>C. J. F. ter Braak and S. de Jong (1993). The objective function of partial
least squares regression, Journal of Chemometrics <b>12</b>, 41&ndash;54.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.lda">pls.lda</a></code>, <code><a href="#topic+TFA.estimate">TFA.estimate</a></code>,
<code><a href="#topic+pls.regression.cv">pls.regression.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load the Ecoli data
data(Ecoli)

# perform pls regression
# with unit latent components
pls.regression(Xtrain=Ecoli$CONNECdata,Ytrain=Ecoli$GEdata,Xtest=Ecoli$CONNECdata,
			ncomp=1:3,unit.weights=FALSE)

# with unit weight vectors
pls.regression(Xtrain=Ecoli$CONNECdata,Ytrain=Ecoli$GEdata,Xtest=Ecoli$CONNECdata,
			ncomp=1:3,unit.weights=TRUE)


</code></pre>

<hr>
<h2 id='pls.regression.cv'>Determination of the number of latent components to be used in PLS regression</h2><span id='topic+pls.regression.cv'></span>

<h3>Description</h3>

<p>The function <code>pls.regression.cv</code> determines the best number of latent components to be used 
for PLS regression using the cross-validation approach described in Boulesteix and Strimmer (2005).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pls.regression.cv(Xtrain, Ytrain, ncomp, nruncv=20, alpha=2/3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pls.regression.cv_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix containing the predictors for the training data set.
<code>Xtrain</code> may be a matrix or a data frame. Each row is an observation and each column is a 
predictor variable.</p>
</td></tr>
<tr><td><code id="pls.regression.cv_+3A_ytrain">Ytrain</code></td>
<td>
<p>a (ntrain x m) data matrix of responses. <code>Ytrain</code> may be a vector (if m=1), 
a matrix or a data frame. If <code>Ytrain</code> is a matrix or a data frame, each row is an 
observation and each column is a response variable. If <code>Ytrain</code> is a vector, it contains 
the unique response variable for each observation.</p>
</td></tr>
<tr><td><code id="pls.regression.cv_+3A_ncomp">ncomp</code></td>
<td>
<p>the vector of integers from which the best number of latent 
components has to be chosen by cross-validation. If <code>ncomp</code> is of length
1, the best number of components is chosen from  1,...,<code>ncomp</code>.</p>
</td></tr>
<tr><td><code id="pls.regression.cv_+3A_nruncv">nruncv</code></td>
<td>
<p>the number of cross-validation iterations to be performed for the choice of
the number of latent components.</p>
</td></tr>
<tr><td><code id="pls.regression.cv_+3A_alpha">alpha</code></td>
<td>
<p>the proportion of observations to be included in the training set at
each cross-validation iteration.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cross-validation procedure described in Boulesteix and Strimmer (2005) 
is used to  determine the best number of latent components to be used for classification.
At each cross-validation run, <code>Xtrain</code> is split into a pseudo training
set and a pseudo test set and the squared error is determined for each
number of latent components. Finally, the function <code>pls.regression.cv</code> returns
the number of latent components for which the mean squared error over
the <code>nrun</code> partitions is minimal. 
</p>


<h3>Value</h3>

<p>The number of latent components to be used in PLS regression, as
determined by cross-validation.
</p>


<h3>Author(s)</h3>

<p>Anne-Laure Boulesteix 
(<a href="https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html">https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html</a>) 
and 
Korbinian Strimmer (<a href="https://strimmerlab.github.io/korbinian.html">https://strimmerlab.github.io/korbinian.html</a>).
</p>


<h3>References</h3>

<p>A. L. Boulesteix and K. Strimmer (2005). Predicting Transcription Factor Activities 
from Combined Analysis of Microarray and ChIP Data: A Partial Least Squares Approach.
</p>
<p>A. L. Boulesteix, K. Strimmer (2007). Partial least squares: a versatile tool for the analysis of 
high-dimensional genomic data. Briefings in Bioinformatics 7:32-44.
</p>
<p>S. de Jong (1993). SIMPLS: an alternative approach to partial least squares 
regression, Chemometrics Intell. Lab. Syst. <b>18</b>, 251&ndash;263.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.regression">pls.regression</a></code>, <code><a href="#topic+TFA.estimate">TFA.estimate</a></code>,
<code><a href="#topic+pls.lda.cv">pls.lda.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## between 5~15 seconds
# load plsgenomics library
library(plsgenomics)

# load Ecoli data
data(Ecoli)

# determine the best number of components for PLS regression using the cross-validation approach
# choose the best number from 1,2,3,4
pls.regression.cv(Xtrain=Ecoli$CONNECdata,Ytrain=Ecoli$GEdata,ncomp=4,nruncv=20)
# choose the best number from 2,3
pls.regression.cv(Xtrain=Ecoli$CONNECdata,Ytrain=Ecoli$GEdata,ncomp=c(2,3),nruncv=20)


## End(Not run)
</code></pre>

<hr>
<h2 id='plsgenomics-deprecated'>Deprecated function(s) in the 'plsgenomics' package</h2><span id='topic+plsgenomics-deprecated'></span><span id='topic+m.rirls.spls'></span><span id='topic+rirls.spls'></span><span id='topic+rirls.spls.tune'></span><span id='topic+rirls.spls.stab'></span><span id='topic+m.rirls.spls.tune'></span><span id='topic+m.rirls.spls.stab'></span><span id='topic+spls.adapt'></span><span id='topic+spls.adapt.tune'></span>

<h3>Description</h3>

<p>These functions are provided for compatibility with older version of
the 'plsgenomics' package. They may eventually be completely
removed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>m.rirls.spls(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plsgenomics-deprecated_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the modern version of the function</p>
</td></tr>
</table>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: right;">
    <code>rirls.spls</code> </td><td style="text-align: left;"> is replaced by <code><a href="#topic+logit.spls">logit.spls</a></code></td>
</tr>
<tr>
 <td style="text-align: right;">
    <code>rirls.spls.tune</code> </td><td style="text-align: left;"> is replaced by <code><a href="#topic+logit.spls.cv">logit.spls.cv</a></code></td>
</tr>
<tr>
 <td style="text-align: right;">
    <code>rirls.spls.stab</code> </td><td style="text-align: left;"> is replaced by <code><a href="#topic+logit.spls.stab">logit.spls.stab</a></code></td>
</tr>
<tr>
 <td style="text-align: right;">
    <code>m.rirls.spls</code> </td><td style="text-align: left;"> is replaced by <code><a href="#topic+multinom.spls">multinom.spls</a></code></td>
</tr>
<tr>
 <td style="text-align: right;">
    <code>m.rirls.spls.tune</code> </td><td style="text-align: left;"> is replaced by <code><a href="#topic+multinom.spls.cv">multinom.spls.cv</a></code></td>
</tr>
<tr>
 <td style="text-align: right;">
    <code>m.rirls.spls.stab</code> </td><td style="text-align: left;"> is replaced by <code><a href="#topic+multinom.spls.stab">multinom.spls.stab</a></code></td>
</tr>
<tr>
 <td style="text-align: right;">
    <code>spls.adapt</code> </td><td style="text-align: left;"> is replaced by <code><a href="#topic+spls">spls</a></code></td>
</tr>
<tr>
 <td style="text-align: right;">
    <code>spls.adapt.tune</code> </td><td style="text-align: left;"> is replaced by <code><a href="#topic+spls.cv">spls.cv</a></code></td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>


<hr>
<h2 id='plsgenomics-internal'>Internal Functions for the 'plsgenomics' package</h2><span id='topic+plsgenomics-internal'></span><span id='topic+standard.simpls'></span><span id='topic+unitr.simpls'></span><span id='topic+transformy'></span><span id='topic+pls.regression.sample'></span><span id='topic+pls.lda.sample'></span><span id='topic+gsim.aux'></span><span id='topic+mgsimaux'></span><span id='topic+rplsaux'></span><span id='topic+mrplsaux'></span><span id='topic+hplugin'></span><span id='topic+wirrls'></span><span id='topic+mwirrls'></span><span id='topic+ust'></span><span id='topic+ust.adapt'></span><span id='topic+wpls'></span><span id='topic+spls.aux'></span><span id='topic+spls.in'></span><span id='topic+logit.spls.aux'></span><span id='topic+multinom.spls.aux'></span><span id='topic+safeExp'></span><span id='topic+safeExpMat'></span><span id='topic+safeSum'></span><span id='topic+softMax'></span>

<h3>Description</h3>

<p>These are not to be called by the user.
</p>

<hr>
<h2 id='preprocess'>preprocess for microarray data</h2><span id='topic+preprocess'></span>

<h3>Description</h3>

<p>The function <code>preprocess</code> performs a preprocessing of microarray data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess(Xtrain, Xtest=NULL,Threshold=c(100,16000),Filtering=c(5,500),
		log10.scale=TRUE,row.stand=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictors. <code>Xtrain</code> must be a matrix. 
Each row corresponds to an observation and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="preprocess_+3A_xtest">Xtest</code></td>
<td>
<p>a (ntest x p) matrix containing the predictors for the test data
set. <code>Xtest</code> may also be a vector of length p (corresponding to only one 
test observation).</p>
</td></tr>
<tr><td><code id="preprocess_+3A_threshold">Threshold</code></td>
<td>
<p> a vector of length 2 containing the values (threshmin,threshmax) for 
thresholding data in preprocess. Data is thresholded to value threshmin and ceiled to value 
threshmax. If <code>Threshold</code> is NULL then no thresholding is done. By default, if the value 
given for <code>Threshold</code> is not valid, no thresholding is done.</p>
</td></tr>
<tr><td><code id="preprocess_+3A_filtering">Filtering</code></td>
<td>
<p> a vector of length 2 containing the values (FiltMin,FiltMax) for filtering genes 
in preprocess. Genes with max/min$&lt;= FiltMin$ and (max-min)$&lt;= FiltMax$ are excluded. 
If <code>Filtering</code> is NULL then no thresholding is done. By default, if the value given for 
<code>Filtering</code> is not valid, no filtering is done.</p>
</td></tr>
<tr><td><code id="preprocess_+3A_log10.scale">log10.scale</code></td>
<td>
<p>a logical value equal to TRUE if a log10-transformation has to be done.</p>
</td></tr>
<tr><td><code id="preprocess_+3A_row.stand">row.stand</code></td>
<td>
<p>a logical value equal to TRUE if a standardisation in row has to be done.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The pre-processing steps recommended by Dudoit et al. (2002) are performed.
The default values are those adapted for Colon data.
</p>


<h3>Value</h3>

<p> A list with the following components:
</p>
<table>
<tr><td><code>pXtrain</code></td>
<td>
<p>the (ntrain x p') matrix containing the preprocessed train data.</p>
</td></tr>
<tr><td><code>pXtest</code></td>
<td>
<p>the (ntest x p') matrix containing the preprocessed test data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sophie Lambert-Lacroix 
(<a href="http://membres-timc.imag.fr/Sophie.Lambert/">http://membres-timc.imag.fr/Sophie.Lambert/</a>) 
and 
Julie Peyre (<a href="https://membres-ljk.imag.fr/Julie.Peyre/">https://membres-ljk.imag.fr/Julie.Peyre/</a>).
</p>


<h3>References</h3>

<p>Dudoit, S. and Fridlyand, J. and Speed, T. (2002). Comparison of discrimination methods 
for the classification of tumors using gene expression data, 
Journal of the American Statistical Association, 97, 77&ndash;87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load Colon data
data(Colon)
IndexLearn &lt;- c(sample(which(Colon$Y==2),27),sample(which(Colon$Y==1),14))

Xtrain &lt;- Colon$X[IndexLearn,]
Ytrain &lt;- Colon$Y[IndexLearn]
Xtest &lt;- Colon$X[-IndexLearn,]

# preprocess data
resP &lt;- preprocess(Xtrain= Xtrain, Xtest=Xtest,Threshold = c(100,16000),Filtering=c(5,500),
				log10.scale=TRUE,row.stand=TRUE)

# how many genes after preprocess ?
dim(resP$pXtrain)[2]
</code></pre>

<hr>
<h2 id='rpls'>Ridge Partial Least Square for binary data</h2><span id='topic+rpls'></span><span id='topic+logit.pls'></span>

<h3>Description</h3>

<p>The function <code>mrpls</code> performs prediction using Fort and Lambert-Lacroix (2005) RPLS 
algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rpls(Ytrain,Xtrain,Lambda,ncomp,Xtest=NULL,NbIterMax=50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rpls_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictors. <code>Xtrain</code> must be a matrix. 
Each row corresponds to an observation and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="rpls_+3A_ytrain">Ytrain</code></td>
<td>
<p>a ntrain vector of responses. <code>Ytrain</code> must be a vector. 
<code>Ytrain</code> is a {0,1}-valued vector and contains the response variable for each
observation.</p>
</td></tr>
<tr><td><code id="rpls_+3A_xtest">Xtest</code></td>
<td>
<p>a (ntest x p) matrix containing the predictors for the test data
set. <code>Xtest</code> may also be a vector of length p (corresponding to only one 
test observation).If <code>Xtest</code> is not equal to NULL, then the prediction 
step is made for these new predictor variables.</p>
</td></tr>
<tr><td><code id="rpls_+3A_lambda">Lambda</code></td>
<td>
<p>a positive real value. <code>Lambda</code> is the ridge regularization parameter.</p>
</td></tr>
<tr><td><code id="rpls_+3A_ncomp">ncomp</code></td>
<td>
<p>a positive integer. <code>ncomp</code> is the number of PLS components. 
If <code>ncomp</code>=0,then the Ridge regression is performed without reduction 
dimension. </p>
</td></tr>
<tr><td><code id="rpls_+3A_nbitermax">NbIterMax</code></td>
<td>
<p>a positive integer. <code>NbIterMax</code> is the maximal number of iterations in the 
Newton-Rapson parts.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>Xtrain</code> and <code>Xtest</code> may not be standardized, 
since standardizing is performed by the function <code>rpls</code> as a preliminary step
before the algorithm is run. 
</p>
<p>The procedure described in Fort and Lambert-Lacroix (2005) is used to determine
latent components to be used for classification and when <code>Xtest</code> 
is not equal to NULL, the procedure predicts the labels for these new 
predictor variables.  
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Coefficients</code></td>
<td>
<p>the (p+1) vector containing the coefficients weighting the 
design matrix.</p>
</td></tr>
<tr><td><code>hatY</code></td>
<td>
<p>the ntrain vector containing the estimated {0,1}-valued labels for the 
observations from <code>Xtrain</code>.</p>
</td></tr>
<tr><td><code>hatYtest</code></td>
<td>
<p>the ntest vector containing the predicted {0,1}-valued labels for the 
observations from <code>Xtest</code>.</p>
</td></tr>
<tr><td><code>proba</code></td>
<td>
<p>the ntrain vector containing the estimated probabilities for the 
observations from <code>Xtrain</code>.</p>
</td></tr>
<tr><td><code>proba.test</code></td>
<td>
<p>the ntest vector containing the predicted probabilities for the 
observations from <code>Xtest</code>.</p>
</td></tr>
<tr><td><code>DeletedCol</code></td>
<td>
<p>the vector containing the column number of <code>Xtrain</code> when the 
variance of the corresponding predictor variable is null. Otherwise <code>DeletedCol</code>=NULL</p>
</td></tr>
<tr><td><code>hatYtest_k</code></td>
<td>
<p>If <code>ncomp</code> is greater than 1, <code>hatYtest_k</code> is a {0,1}-valued matrix of 
size ntest x ncomp in such a way that the kth column corresponds to the predicted label 
obtained with k PLS components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sophie Lambert-Lacroix 
(<a href="http://membres-timc.imag.fr/Sophie.Lambert/">http://membres-timc.imag.fr/Sophie.Lambert/</a>). 
</p>


<h3>References</h3>

<p>G. Fort and S. Lambert-Lacroix (2005). Classification using Partial Least Squares with 
Penalized Logistic Regression, Bioinformatics, vol 21,  n 8, 1104-1111. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rpls.cv">rpls.cv</a></code>, <code><a href="#topic+mrpls">mrpls</a></code>, <code><a href="#topic+mrpls.cv">mrpls.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load Colon data
data(Colon)
IndexLearn &lt;- c(sample(which(Colon$Y==2),12),sample(which(Colon$Y==1),8))

# preprocess data
res &lt;- preprocess(Xtrain= Colon$X[IndexLearn,], Xtest=Colon$X[-IndexLearn,],
                    Threshold = c(100,16000),Filtering=c(5,500),
                    log10.scale=TRUE,row.stand=TRUE)
# the results are given in res$pXtrain and res$pXtest

# perform prediction by RPLS
resrpls &lt;- rpls(Ytrain=Colon$Y[IndexLearn]-1,Xtrain=res$pXtrain,Lambda=0.6,ncomp=1,Xtest=res$pXtest)
resrpls$hatY
sum(resrpls$Ytest!=Colon$Y[-IndexLearn])

# prediction for another sample
Xnew &lt;- res$pXtest[1,]
# Compute the linear predictor for each classes expect class 0
eta &lt;- c(1,Xnew) %*% resrpls$Coefficients
Ypred &lt;- which.max(c(0,eta))
Ypred+1



</code></pre>

<hr>
<h2 id='rpls.cv'>Determination of the ridge regularization parameter and the number of PLS 
components to be used for classification with RPLS for binary data</h2><span id='topic+rpls.cv'></span><span id='topic+logit.pls.cv'></span>

<h3>Description</h3>

<p>The function <code>rpls.cv</code> determines the best ridge regularization parameter and the best 
number of PLS components to be used for classification for Fort and Lambert-Lacroix (2005) 
RPLS algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rpls.cv(Ytrain, Xtrain, LambdaRange, ncompMax, NbIterMax=50, ncores=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rpls.cv_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictors. <code>Xtrain</code> must be a matrix. 
Each row corresponds to an observation and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="rpls.cv_+3A_ytrain">Ytrain</code></td>
<td>
<p>a ntrain vector of responses. <code>Ytrain</code> must be a vector. 
<code>Ytrain</code> is a {0,1}-valued vector and contains the response variable for each
observation.</p>
</td></tr>
<tr><td><code id="rpls.cv_+3A_lambdarange">LambdaRange</code></td>
<td>
<p>the vector of positive real value from which the best ridge regularization 
parameter has to be chosen by cross-validation.</p>
</td></tr>
<tr><td><code id="rpls.cv_+3A_ncompmax">ncompMax</code></td>
<td>
<p>a positive integer. the best number of components is chosen from  
1,...,<code>ncompMax</code>. If <code>ncompMax</code>=0,then the Ridge regression is performed without 
reduction dimension. </p>
</td></tr>
<tr><td><code id="rpls.cv_+3A_nbitermax">NbIterMax</code></td>
<td>
<p>a positive integer. <code>NbIterMax</code> is the maximal number of iterations in the 
Newton-Rapson parts.</p>
</td></tr>
<tr><td><code id="rpls.cv_+3A_ncores">ncores</code></td>
<td>
<p>a positive integer. The number of cores to be used for parallel computing 
(if different from 1)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A cross-validation procedure is used to determine the best ridge regularization parameter and 
number of PLS components to be used for classification with RPLS for binary data 
(for categorical data see <code><a href="#topic+mrpls">mrpls</a></code> and <code><a href="#topic+mrpls.cv">mrpls.cv</a></code>).
At each cross-validation run, <code>Xtrain</code> is split into a pseudo training
set (ntrain-1 samples) and a pseudo test set (1 sample) and the classification error rate is 
determined for each value of ridge regularization parameter and number of components. Finally, 
the function <code>mrpls.cv</code> returns the values of the ridge regularization parameter and 
bandwidth for which the mean classification error rate is minimal. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Lambda</code></td>
<td>
<p>the optimal regularization parameter.</p>
</td></tr> 
<tr><td><code>ncomp</code></td>
<td>
<p>the optimal number of PLS components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sophie Lambert-Lacroix 
(<a href="http://membres-timc.imag.fr/Sophie.Lambert/">http://membres-timc.imag.fr/Sophie.Lambert/</a>). 
</p>


<h3>References</h3>

<p>G. Fort and S. Lambert-Lacroix (2005). Classification using Partial Least Squares with 
Penalized Logistic Regression, Bioinformatics, vol 21,  n 8, 1104-1111. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rpls">rpls</a></code>, <code><a href="#topic+mrpls">mrpls</a></code>, <code><a href="#topic+mrpls.cv">mrpls.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## between 5~15 seconds
# load plsgenomics library
library(plsgenomics)

# load Colon data
data(Colon)
IndexLearn &lt;- c(sample(which(Colon$Y==2),12),sample(which(Colon$Y==1),8))

# preprocess data
res &lt;- preprocess(Xtrain= Colon$X[IndexLearn,], Xtest=Colon$X[-IndexLearn,],
                    Threshold = c(100,16000),Filtering=c(5,500),
                    log10.scale=TRUE,row.stand=TRUE)
# the results are given in res$pXtrain and res$pXtest

# Determine optimum ncomp and lambda
nl &lt;- rpls.cv(Ytrain=Colon$Y[IndexLearn]-1,Xtrain=res$pXtrain,LambdaRange=c(0.1,1),ncompMax=3)

# perform prediction by RPLS
resrpls &lt;- rpls(Ytrain=Colon$Y[IndexLearn]-1,Xtrain=res$pXtrain,Lambda=nl$Lambda,
			ncomp=nl$ncomp,Xtest=res$pXtest)
sum(resrpls$Ytest!=Colon$Y[-IndexLearn]-1)


## End(Not run)
</code></pre>

<hr>
<h2 id='sample.bin'>Generates covariate matrix X with correlated block of covariates and 
a binary random reponse depening on X through a logistic model</h2><span id='topic+sample.bin'></span>

<h3>Description</h3>

<p>The function <code>sample.bin</code> generates a random sample of n observations, 
composed of p predictors, collected in the n x p matrix X, and a binary 
response, in a vector Y of length n, thanks to a logistic model, where the 
response Y is generated as a Bernoulli random variable of parameter 
logit^{-1}(XB), the coefficients B are sparse. In addition, the covariate 
matrix X is composed of correlated blocks of predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample.bin(
  n,
  p,
  kstar,
  lstar,
  beta.min,
  beta.max,
  mean.H = 0,
  sigma.H = 1,
  sigma.F = 1,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.bin_+3A_n">n</code></td>
<td>
<p>the number of observations in the sample.</p>
</td></tr>
<tr><td><code id="sample.bin_+3A_p">p</code></td>
<td>
<p>the number of covariates in the sample.</p>
</td></tr>
<tr><td><code id="sample.bin_+3A_kstar">kstar</code></td>
<td>
<p>the number of underlying latent variables used to generates 
the covariate matrix <code>X</code>, <code>kstar &lt;= p</code>. <code>kstar</code> is also the 
number of blocks in the covariate matrix (see details).</p>
</td></tr>
<tr><td><code id="sample.bin_+3A_lstar">lstar</code></td>
<td>
<p>the number of blocks in the covariate matrix <code>X</code> that are 
used to generates the response <code>Y</code>, i.e. with non null coefficients 
in vector <code>B</code>, <code>lstar &lt;= kstar</code>.</p>
</td></tr>
<tr><td><code id="sample.bin_+3A_beta.min">beta.min</code></td>
<td>
<p>the inf bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code id="sample.bin_+3A_beta.max">beta.max</code></td>
<td>
<p>the sup bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code id="sample.bin_+3A_mean.h">mean.H</code></td>
<td>
<p>the mean of latent variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code id="sample.bin_+3A_sigma.h">sigma.H</code></td>
<td>
<p>the standard deviation of latent variables used to 
generates <code>X</code>.</p>
</td></tr>
<tr><td><code id="sample.bin_+3A_sigma.f">sigma.F</code></td>
<td>
<p>the standard deviation of the noise added to latent 
variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code id="sample.bin_+3A_seed">seed</code></td>
<td>
<p>an positive integer, if non NULL it fix the seed (with the 
command <code>set.seed</code>) used for random number generation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The set (1:p) of predictors is partitioned into kstar block. 
Each block k (k=1,...,kstar) depends on a latent variable H.k which are 
independent and identically distributed following a Gaussian distribution 
N(mean.H, sigma.H^2). Each columns X.j of the matrix X is generated 
as H.k + F.j for j in the block k, where F.j is independent and identically 
distributed gaussian noise N(0,sigma.F^2).
</p>
<p>The coefficients B are generated as random between beta.min and beta.max 
on lstar blocks, randomly chosen, and null otherwise. The variables with 
non null coefficients are then relevant to explain the response, whereas 
the ones with null coefficients are not.
</p>
<p>The response is generated as by drawing one observation of n different 
Bernoulli random variables of parameters logit^{-1}(XB).
</p>
<p>The details of the procedure are developped by Durif et al. (2018).
</p>


<h3>Value</h3>

<p>An object with the following attributes:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>the (n x p) covariate matrix, containing the <code>n</code> observations 
for each of the <code>p</code> predictors.</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>the (n) vector of Y observations.</p>
</td></tr>
<tr><td><code>proba</code></td>
<td>
<p>the n vector of Bernoulli parameters used to generate the 
response, in particular <code>logit^{-1}(X %*% B)</code>.</p>
</td></tr>
<tr><td><code>sel</code></td>
<td>
<p>the index in (1:p) of covariates with non null coefficients 
in <code>B</code>.</p>
</td></tr>
<tr><td><code>nosel</code></td>
<td>
<p>the index in (1:p) of covariates with null coefficients 
in <code>B</code>.</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>the (n) vector of coefficients.</p>
</td></tr>
<tr><td><code>block.partition</code></td>
<td>
<p>a (p) vector indicating the block of each predictors 
in (1:kstar).</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>the number of covariates in the sample.</p>
</td></tr>
<tr><td><code>kstar</code></td>
<td>
<p>the number of underlying latent variables used to generates 
the covariate matrix <code>X</code>, <code>kstar &lt;= p</code>. <code>kstar</code> is also 
the number of blocks in the covariate matrix (see details).</p>
</td></tr>
<tr><td><code>lstar</code></td>
<td>
<p>the number of blocks in the covariate matrix <code>X</code> that 
are used to generates the response <code>Y</code>, i.e. with non null 
coefficients in vector <code>B</code>, <code>lstar &lt;= kstar</code>.</p>
</td></tr>
<tr><td><code>p0</code></td>
<td>
<p>the number of predictors with non null coefficients in <code>B</code>.</p>
</td></tr>
<tr><td><code>block.sel</code></td>
<td>
<p>a (lstar) vector indicating the index in (1:kstar) of 
blocks with predictors having non null coefficient in <code>B</code>.</p>
</td></tr>
<tr><td><code>beta.min</code></td>
<td>
<p>the inf bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code>beta.max</code></td>
<td>
<p>the sup bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code>mean.H</code></td>
<td>
<p>the mean of latent variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code>sigma.H</code></td>
<td>
<p>the standard deviation of latent variables used to 
generates <code>X</code>.</p>
</td></tr>
<tr><td><code>sigma.F</code></td>
<td>
<p>the standard deviation of the noise added to latent 
variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code>seed</code></td>
<td>
<p>an positive integer, if non NULL it fix the seed 
(with the command <code>set.seed</code>) used for random number generation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sample.cont">sample.cont</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 1000
sample1 &lt;- sample.bin(n=n, p=p, kstar=20, lstar=2, beta.min=0.25, 
                      beta.max=0.75, mean.H=0.2, 
                      sigma.H=10, sigma.F=5)

str(sample1)

</code></pre>

<hr>
<h2 id='sample.cont'>Generates design matrix X with correlated block of covariates and a continuous random 
reponse Y depening on X through gaussian linear model Y=XB+E</h2><span id='topic+sample.cont'></span>

<h3>Description</h3>

<p>The function <code>sample.cont</code> generates a random sample with p predictors X, a response Y, 
and n observations, through a linear model Y=XB+E, where the noise E is gaussian, 
the coefficients B are sparse, and the design matrix X is composed of correlated blocks of
predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample.cont(n, p, kstar, lstar, beta.min, beta.max, mean.H=0, sigma.H, 
               sigma.F, sigma.E, seed=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.cont_+3A_n">n</code></td>
<td>
<p>the number of observations in the sample.</p>
</td></tr>
<tr><td><code id="sample.cont_+3A_p">p</code></td>
<td>
<p>the number of covariates in the sample.</p>
</td></tr>
<tr><td><code id="sample.cont_+3A_kstar">kstar</code></td>
<td>
<p>the number of underlying latent variables used to generates the design matrix 
<code>X</code>, <code>kstar &lt;= p</code>. <code>kstar</code> is also the number of blocks in the design matrix 
(see details).</p>
</td></tr>
<tr><td><code id="sample.cont_+3A_lstar">lstar</code></td>
<td>
<p>the number of blocks in the design matrix <code>X</code> that are used to generates the 
response <code>Y</code>, i.e. with non null coefficients in vector <code>B</code>, <code>lstar &lt;= kstar</code>.</p>
</td></tr>
<tr><td><code id="sample.cont_+3A_beta.min">beta.min</code></td>
<td>
<p>the inf bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code id="sample.cont_+3A_beta.max">beta.max</code></td>
<td>
<p>the sup bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code id="sample.cont_+3A_mean.h">mean.H</code></td>
<td>
<p>the mean of latent variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code id="sample.cont_+3A_sigma.h">sigma.H</code></td>
<td>
<p>the standard deviation of latent variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code id="sample.cont_+3A_sigma.f">sigma.F</code></td>
<td>
<p>the standard deviation of the noise added to latent variables used to 
generates <code>X</code>.</p>
</td></tr>
<tr><td><code id="sample.cont_+3A_sigma.e">sigma.E</code></td>
<td>
<p>the standard deviation of the noise in the linear model 
<code>Y = X %*%* B + E</code> used to generates <code>Y</code>.</p>
</td></tr>
<tr><td><code id="sample.cont_+3A_seed">seed</code></td>
<td>
<p>an positive integer, if non NULL it fix the seed (with the command 
<code>set.seed</code>) used for random number generation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The set (1:p) of predictors is partitioned into kstar block. Each block k (k=1,...,kstar) 
depends on a latent variable H.k which are independent and identically distributed following a 
distribution N(mean.H, sigma.H^2). Each columns X.j of the matrix X is generated as H.k + F.j 
for j in the block k, where F.j is independent and identically distributed gaussian noise 
N(0,sigma.F^2).
</p>
<p>The coefficients B are generated as random between beta.min and beta.max on lstar blocks, 
randomly chosen, and null otherwise. The variables with non null coefficients are then 
relevant to explain the response, whereas the ones with null coefficients are not.
</p>
<p>The response is generated as Y = X %*% B + E, where E is some gaussian noise N(0,sigma.E^2).
</p>
<p>The details of the procedure are developped by Durif et al. (2018).
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>the (n x p) design matrix, containing the <code>n</code> observations for each of the 
<code>p</code> predictors.</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>the (n) vector of Y observations.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the (n) vector corresponding to the noise <code>E</code> in the model 
<code>Y = X %*% B + E</code>.</p>
</td></tr>
<tr><td><code>sel</code></td>
<td>
<p>the index in (1:p) of covariates with non null coefficients in <code>B</code>.</p>
</td></tr>
<tr><td><code>nosel</code></td>
<td>
<p>the index in (1:p) of covariates with null coefficients in <code>B</code>.</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>the (n) vector of coefficients.</p>
</td></tr>
<tr><td><code>block.partition</code></td>
<td>
<p>a (p) vector indicating the block of each predictors in (1:kstar).</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>the number of covariates in the sample.</p>
</td></tr>
<tr><td><code>kstar</code></td>
<td>
<p>the number of underlying latent variables used to generates the design matrix 
<code>X</code>, <code>kstar &lt;= p</code>. <code>kstar</code> is also the number of blocks in the design matrix 
(see details).</p>
</td></tr>
<tr><td><code>lstar</code></td>
<td>
<p>the number of blocks in the design matrix <code>X</code> that are used to generates the 
response <code>Y</code>, i.e. with non null coefficients in vector <code>B</code>, <code>lstar &lt;= kstar</code>.</p>
</td></tr>
<tr><td><code>p0</code></td>
<td>
<p>the number of predictors with non null coefficients in <code>B</code>.</p>
</td></tr>
<tr><td><code>block.sel</code></td>
<td>
<p>a (lstar) vector indicating the index in (1:kstar) of blocks with predictors 
having non null coefficient in <code>B</code>.</p>
</td></tr>
<tr><td><code>beta.min</code></td>
<td>
<p>the inf bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code>beta.max</code></td>
<td>
<p>the sup bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code>mean.H</code></td>
<td>
<p>the mean of latent variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code>sigma.H</code></td>
<td>
<p>the standard deviation of latent variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code>sigma.F</code></td>
<td>
<p>the standard deviation of the noise added to latent variables used to 
generates <code>X</code>.</p>
</td></tr>
<tr><td><code>sigma.E</code></td>
<td>
<p>the standard deviation of the noise in the linear model.</p>
</td></tr>
<tr><td><code>seed</code></td>
<td>
<p>an positive integer, if non NULL it fix the seed (with the command 
<code>set.seed</code>) used for random number generation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>). 
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sample.bin">sample.bin</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 1000
sample1 &lt;- sample.cont(n=n, p=p, kstar=20, lstar=2, beta.min=0.25, beta.max=0.75, mean.H=0.2, 
					sigma.H=10, sigma.F=5, sigma.E=5)
str(sample1)

</code></pre>

<hr>
<h2 id='sample.multinom'>Generates covariate matrix X with correlated block of covariates and 
a multi-label random reponse depening on X through a multinomial model</h2><span id='topic+sample.multinom'></span>

<h3>Description</h3>

<p>The function <code>sample.multinom</code> generates a random sample of n observations, 
composed of p predictors, collected in the n x p matrix X, and a binary 
response, in a vector Y of length n, thanks to a logistic model, where the 
response Y is generated as a Bernoulli random variable of parameter 
logit^{-1}(XB), the coefficients B are sparse. In addition, the covariate 
matrix X is composed of correlated blocks of predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample.multinom(
  n,
  p,
  nb.class = 2,
  kstar,
  lstar,
  beta.min,
  beta.max,
  mean.H = 0,
  sigma.H = 1,
  sigma.F = 1,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.multinom_+3A_n">n</code></td>
<td>
<p>the number of observations in the sample.</p>
</td></tr>
<tr><td><code id="sample.multinom_+3A_p">p</code></td>
<td>
<p>the number of covariates in the sample.</p>
</td></tr>
<tr><td><code id="sample.multinom_+3A_nb.class">nb.class</code></td>
<td>
<p>the number of groups in the data.</p>
</td></tr>
<tr><td><code id="sample.multinom_+3A_kstar">kstar</code></td>
<td>
<p>the number of underlying latent variables used to generates 
the covariate matrix <code>X</code>, <code>kstar &lt;= p</code>. <code>kstar</code> is also the 
number of blocks in the covariate matrix (see details).</p>
</td></tr>
<tr><td><code id="sample.multinom_+3A_lstar">lstar</code></td>
<td>
<p>the number of blocks in the covariate matrix <code>X</code> that are 
used to generates the response <code>Y</code>, i.e. with non null coefficients 
in vector <code>B</code>, <code>lstar &lt;= kstar</code>.</p>
</td></tr>
<tr><td><code id="sample.multinom_+3A_beta.min">beta.min</code></td>
<td>
<p>the inf bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code id="sample.multinom_+3A_beta.max">beta.max</code></td>
<td>
<p>the sup bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code id="sample.multinom_+3A_mean.h">mean.H</code></td>
<td>
<p>the mean of latent variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code id="sample.multinom_+3A_sigma.h">sigma.H</code></td>
<td>
<p>the standard deviation of latent variables used to 
generates <code>X</code>.</p>
</td></tr>
<tr><td><code id="sample.multinom_+3A_sigma.f">sigma.F</code></td>
<td>
<p>the standard deviation of the noise added to latent 
variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code id="sample.multinom_+3A_seed">seed</code></td>
<td>
<p>an positive integer, if non NULL it fix the seed (with the 
command <code>set.seed</code>) used for random number generation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The set (1:p) of predictors is partitioned into kstar block. 
Each block k (k=1,...,kstar) depends on a latent variable H.k which are 
independent and identically distributed following a Gaussian distribution 
N(mean.H, sigma.H^2). Each columns X.j of the matrix X is generated 
as H.k + F.j for j in the block k, where F.j is independent and identically 
distributed gaussian noise N(0,sigma.F^2).
</p>
<p>The coefficients B are generated as random between beta.min and beta.max 
on lstar blocks, randomly chosen, and null otherwise. The variables with 
non null coefficients are then relevant to explain the response, whereas 
the ones with null coefficients are not.
</p>
<p>The response is generated as by drawing one observation of n different 
Bernoulli random variables of parameters logit^{-1}(XB).
</p>
<p>The details of the procedure are developped by Durif et al. (2018).
</p>


<h3>Value</h3>

<p>An object with the following attributes:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>the (n x p) covariate matrix, containing the <code>n</code> observations 
for each of the <code>p</code> predictors.</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>the (n) vector of Y observations.</p>
</td></tr>
<tr><td><code>proba</code></td>
<td>
<p>the n vector of Bernoulli parameters used to generate the 
response, in particular <code>logit^{-1}(X %*% B)</code>.</p>
</td></tr>
<tr><td><code>sel</code></td>
<td>
<p>the index in (1:p) of covariates with non null coefficients 
in <code>B</code>.</p>
</td></tr>
<tr><td><code>nosel</code></td>
<td>
<p>the index in (1:p) of covariates with null coefficients 
in <code>B</code>.</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>the (n) vector of coefficients.</p>
</td></tr>
<tr><td><code>block.partition</code></td>
<td>
<p>a (p) vector indicating the block of each predictors 
in (1:kstar).</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>the number of covariates in the sample.</p>
</td></tr>
<tr><td><code>kstar</code></td>
<td>
<p>the number of underlying latent variables used to generates 
the covariate matrix <code>X</code>, <code>kstar &lt;= p</code>. <code>kstar</code> is also 
the number of blocks in the covariate matrix (see details).</p>
</td></tr>
<tr><td><code>lstar</code></td>
<td>
<p>the number of blocks in the covariate matrix <code>X</code> that 
are used to generates the response <code>Y</code>, i.e. with non null 
coefficients in vector <code>B</code>, <code>lstar &lt;= kstar</code>.</p>
</td></tr>
<tr><td><code>p0</code></td>
<td>
<p>the number of predictors with non null coefficients in <code>B</code>.</p>
</td></tr>
<tr><td><code>block.sel</code></td>
<td>
<p>a (lstar) vector indicating the index in (1:kstar) of 
blocks with predictors having non null coefficient in <code>B</code>.</p>
</td></tr>
<tr><td><code>beta.min</code></td>
<td>
<p>the inf bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code>beta.max</code></td>
<td>
<p>the sup bound for non null coefficients (see details).</p>
</td></tr>
<tr><td><code>mean.H</code></td>
<td>
<p>the mean of latent variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code>sigma.H</code></td>
<td>
<p>the standard deviation of latent variables used to 
generates <code>X</code>.</p>
</td></tr>
<tr><td><code>sigma.F</code></td>
<td>
<p>the standard deviation of the noise added to latent 
variables used to generates <code>X</code>.</p>
</td></tr>
<tr><td><code>seed</code></td>
<td>
<p>an positive integer, if non NULL it fix the seed 
(with the command <code>set.seed</code>) used for random number generation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sample.cont">sample.cont</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 1000
nclass &lt;- 3
sample1 &lt;- sample.multinom(n=n, p=p, nb.class=nclass,
                           kstar=20, lstar=2, beta.min=0.25,
                           beta.max=0.75, mean.H=0.2,
                           sigma.H=10, sigma.F=5)

str(sample1)

</code></pre>

<hr>
<h2 id='spls'>Adaptive Sparse Partial Least Squares (SPLS) regression</h2><span id='topic+spls'></span>

<h3>Description</h3>

<p>The function <code>spls.adapt</code> performs compression and variable selection 
in the context of linear regression (with possible prediction) 
using Durif et al. (2018) adaptive SPLS algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spls(
  Xtrain,
  Ytrain,
  lambda.l1,
  ncomp,
  weight.mat = NULL,
  Xtest = NULL,
  adapt = TRUE,
  center.X = TRUE,
  center.Y = TRUE,
  scale.X = TRUE,
  scale.Y = TRUE,
  weighted.center = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spls_+3A_xtrain">Xtrain</code></td>
<td>
<p>a (ntrain x p) data matrix of predictor values. 
<code>Xtrain</code> must be a matrix. Each row corresponds to an observation 
and each column to a predictor variable.</p>
</td></tr>
<tr><td><code id="spls_+3A_ytrain">Ytrain</code></td>
<td>
<p>a (ntrain) vector of (continuous) responses. <code>Ytrain</code> 
must be a vector or a one column matrix, and contains the response variable 
for each observation.</p>
</td></tr>
<tr><td><code id="spls_+3A_lambda.l1">lambda.l1</code></td>
<td>
<p>a positive real value, in [0,1]. <code>lambda.l1</code> is the 
sparse penalty parameter for the dimension reduction step by sparse PLS 
(see details).</p>
</td></tr>
<tr><td><code id="spls_+3A_ncomp">ncomp</code></td>
<td>
<p>a positive integer. <code>ncomp</code> is the number of PLS 
components.</p>
</td></tr>
<tr><td><code id="spls_+3A_weight.mat">weight.mat</code></td>
<td>
<p>a (ntrain x ntrain) matrix used to weight the l2 metric 
in the observation space, it can be the covariance inverse of the Ytrain 
observations in a heteroskedastic context. If NULL, the l2 metric is the 
standard one, corresponding to homoskedastic model (<code>weight.mat</code> is the 
identity matrix).</p>
</td></tr>
<tr><td><code id="spls_+3A_xtest">Xtest</code></td>
<td>
<p>a (ntest x p) matrix containing the predictor values for the 
test data set. <code>Xtest</code> may also be a vector of length p 
(corresponding to only one test observation). Default value is NULL, 
meaning that no prediction is performed.</p>
</td></tr>
<tr><td><code id="spls_+3A_adapt">adapt</code></td>
<td>
<p>a boolean value, indicating whether the sparse PLS selection 
step sould be adaptive or not (see details).</p>
</td></tr>
<tr><td><code id="spls_+3A_center.x">center.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be centered or not.</p>
</td></tr>
<tr><td><code id="spls_+3A_center.y">center.Y</code></td>
<td>
<p>a boolean value indicating whether the response values 
<code>Ytrain</code> set should be centered or not.</p>
</td></tr>
<tr><td><code id="spls_+3A_scale.x">scale.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be scaled or not 
(<code>scale.X=TRUE</code> implies <code>center.X=TRUE</code>).</p>
</td></tr>
<tr><td><code id="spls_+3A_scale.y">scale.Y</code></td>
<td>
<p>a boolean value indicating whether the response values 
<code>Ytrain</code> should be scaled or not (<code>scale.Y=TRUE</code> implies 
<code>center.Y=TRUE</code>).</p>
</td></tr>
<tr><td><code id="spls_+3A_weighted.center">weighted.center</code></td>
<td>
<p>a boolean value indicating whether the centering 
should take into account the weighted l2 metric or not 
(if TRUE, it requires that weighted.mat is non NULL).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>Xtrain</code> and <code>Xtest</code> may 
not be standardized, since standardizing can be performed by the function 
<code>spls</code> as a preliminary step.
</p>
<p>The procedure described in Durif et al. (2018) is used to compute 
latent sparse components that are used in a regression model.
In addition, when a matrix <code>Xtest</code> is supplied, the procedure 
predicts the response associated to these new values of the predictors.
</p>


<h3>Value</h3>

<p>An object of class <code>spls</code> with the following attributes
</p>
<table>
<tr><td><code>Xtrain</code></td>
<td>
<p>the ntrain x p predictor matrix.</p>
</td></tr>
<tr><td><code>Ytrain</code></td>
<td>
<p>the response observations.</p>
</td></tr>
<tr><td><code>sXtrain</code></td>
<td>
<p>the centered if so and scaled if so predictor matrix.</p>
</td></tr>
<tr><td><code>sYtrain</code></td>
<td>
<p>the centered if so and scaled if so response.</p>
</td></tr>
<tr><td><code>betahat</code></td>
<td>
<p>the linear coefficients in model 
<code>sYtrain = sXtrain %*% betahat + residuals</code>.</p>
</td></tr>
<tr><td><code>betahat.nc</code></td>
<td>
<p>the (p+1) vector containing the coefficients and intercept 
for the non centered and non scaled model 
<code>Ytrain = cbind(rep(1,ntrain),Xtrain) %*% betahat.nc + residuals.nc</code>.</p>
</td></tr>
<tr><td><code>meanXtrain</code></td>
<td>
<p>the (p) vector of Xtrain column mean, 
used for centering if so.</p>
</td></tr>
<tr><td><code>sigmaXtrain</code></td>
<td>
<p>the (p) vector of Xtrain column standard deviation, 
used for scaling if so.</p>
</td></tr>
<tr><td><code>meanYtrain</code></td>
<td>
<p>the mean of Ytrain, used for centering if so.</p>
</td></tr>
<tr><td><code>sigmaYtrain</code></td>
<td>
<p>the standard deviation of Ytrain, used for centering 
if so.</p>
</td></tr>
<tr><td><code>X.score</code></td>
<td>
<p>a (n x ncomp) matrix being the observations coordinates or 
scores in the new component basis produced by the compression step 
(sparse PLS). Each column t.k of <code>X.score</code> is a SPLS component.</p>
</td></tr>
<tr><td><code>X.score.low</code></td>
<td>
<p>a (n x ncomp) matrix being the PLS components only 
computed with the selected predictors.</p>
</td></tr>
<tr><td><code>X.loading</code></td>
<td>
<p>the (ncomp x p) matrix of coefficients in regression of 
Xtrain over the new components <code>X.score</code>.</p>
</td></tr>
<tr><td><code>Y.loading</code></td>
<td>
<p>the (ncomp) vector of coefficients in regression of Ytrain 
over the SPLS components <code>X.score</code>.</p>
</td></tr>
<tr><td><code>X.weight</code></td>
<td>
<p>a (p x ncomp) matrix being the coefficients of predictors 
in each components produced by sparse PLS. Each column w.k of 
<code>X.weight</code> verifies t.k = Xtrain x w.k (as a matrix product).</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the (ntrain) vector of residuals in the model 
<code>sYtrain = sXtrain %*% betahat + residuals</code>.</p>
</td></tr>
<tr><td><code>residuals.nc</code></td>
<td>
<p>the (ntrain) vector of residuals in the non centered 
and non scaled model 
<code>Ytrain = cbind(rep(1,ntrain),Xtrain) %*% betahat.nc + residuals.nc</code>.</p>
</td></tr>
<tr><td><code>hatY</code></td>
<td>
<p>the (ntrain) vector containing the estimated reponse values 
on the train set of centered and scaled (if so) predictors 
<code>sXtrain</code>, <code>hatY = sXtrain %*% betahat</code>.</p>
</td></tr>
<tr><td><code>hatY.nc</code></td>
<td>
<p>the (ntrain) vector containing the estimated reponse value 
on the train set of non centered and non scaled predictors <code>Xtrain</code>, 
<code>hatY.nc = cbind(rep(1,ntrain),Xtrain) %*% betahat.nc</code>.</p>
</td></tr>
<tr><td><code>hatYtest</code></td>
<td>
<p>the (ntest) vector containing the predicted values 
for the response on the centered and scaled test set <code>sXtest</code>
(if provided), <code>hatYtest = sXtest %*% betahat</code>.</p>
</td></tr>
<tr><td><code>hatYtest.nc</code></td>
<td>
<p>the (ntest) vector containing the predicted values 
for the response on the non centered and non scaled test set <code>Xtest</code> 
(if provided), 
<code>hatYtest.nc = cbind(rep(1,ntest),Xtest) %*% betahat.nc</code>.</p>
</td></tr>	
<tr><td><code>A</code></td>
<td>
<p>the active set of predictors selected by the procedures. <code>A</code> 
is a subset of <code>1:p</code>.</p>
</td></tr>
<tr><td><code>betamat</code></td>
<td>
<p>a (ncomp) list of coefficient vector betahat in the model 
with <code>k</code> components, for <code>k=1,...,ncomp</code>.</p>
</td></tr>
<tr><td><code>new2As</code></td>
<td>
<p>a (ncomp) list of subset of <code>(1:p)</code> indicating the 
variables that are selected when constructing the 
components <code>k</code>, for <code>k=1,...,ncomp</code>.</p>
</td></tr>
<tr><td><code>lambda.l1</code></td>
<td>
<p>the sparse hyper-parameter used to fit the model.</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p>the number of components used to fit the model.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>the (ntrain x ntrain) matrix used to weight the metric in 
the sparse PLS step.</p>
</td></tr>
<tr><td><code>adapt</code></td>
<td>
<p>a boolean value, indicating whether the sparse PLS selection 
step was adaptive or not.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>). 
</p>
<p>Adapted in part from spls code by H. Chun, D. Chung and S.Keles 
(<a href="https://CRAN.R-project.org/package=spls">https://CRAN.R-project.org/package=spls</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>
<p>Chun, H., &amp; Keles, S. (2010). Sparse partial least squares regression for 
simultaneous dimension reduction and variable selection.  Journal of the 
Royal Statistical Society. Series B (Methodological), 72(1), 3-25. 
doi:10.1111/j.1467-9868.2009.00723.x
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spls.cv">spls.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
sample1 &lt;- sample.cont(n=n, p=p, kstar=10, lstar=2, beta.min=0.25, 
                       beta.max=0.75, mean.H=0.2, sigma.H=10, 
                       sigma.F=5, sigma.E=5)
X &lt;- sample1$X
Y &lt;- sample1$Y
### splitting between learning and testing set
index.train &lt;- sort(sample(1:n, size=round(0.7*n)))
index.test &lt;- (1:n)[-index.train]
Xtrain &lt;- X[index.train,]
Ytrain &lt;- Y[index.train,]
Xtest &lt;- X[index.test,]
Ytest &lt;- Y[index.test,]

### fitting the model, and predicting new observations
model1 &lt;- spls(Xtrain=Xtrain, Ytrain=Ytrain, lambda.l1=0.5, ncomp=2, 
               weight.mat=NULL, Xtest=Xtest, adapt=TRUE, center.X=TRUE, 
               center.Y=TRUE, scale.X=TRUE, scale.Y=TRUE, 
               weighted.center=FALSE)

str(model1)

### plotting the estimation versus real values for the non centered response
plot(model1$Ytrain, model1$hatY.nc, 
     xlab="real Ytrain", ylab="Ytrain estimates")
points(-1000:1000,-1000:1000, type="l")

### plotting residuals versus centered response values
plot(model1$sYtrain, model1$residuals, xlab="sYtrain", ylab="residuals")

### plotting the predictor coefficients
plot(model1$betahat.nc, xlab="variable index", ylab="coeff")

### mean squares error of prediction on test sample
sYtest &lt;- as.matrix(scale(Ytest, center=model1$meanYtrain, scale=model1$sigmaYtrain))
sum((model1$hatYtest - sYtest)^2) / length(index.test)

### plotting predicted values versus non centered real response values 
## on the test set
plot(model1$hatYtest, sYtest, xlab="real Ytest", ylab="predicted values")
points(-1000:1000,-1000:1000, type="l")

</code></pre>

<hr>
<h2 id='spls.cv'>Cross-validation procedure to calibrate the parameters (ncomp, lambda.l1) 
of the Adaptive Sparse PLS regression</h2><span id='topic+spls.cv'></span>

<h3>Description</h3>

<p>The function <code>spls.cv</code> chooses the optimal values for the 
hyper-parameter of the <code>spls</code> procedure, by minimizing the mean 
squared error of prediction over the hyper-parameter grid, 
using Durif et al. (2018) adaptive SPLS algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spls.cv(
  X,
  Y,
  lambda.l1.range,
  ncomp.range,
  weight.mat = NULL,
  adapt = TRUE,
  center.X = TRUE,
  center.Y = TRUE,
  scale.X = TRUE,
  scale.Y = TRUE,
  weighted.center = FALSE,
  return.grid = FALSE,
  ncores = 1,
  nfolds = 10,
  nrun = 1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spls.cv_+3A_x">X</code></td>
<td>
<p>a (n x p) data matrix of predictors. <code>X</code> must be a matrix. 
Each row corresponds to an observation and each column to a 
predictor variable.</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_y">Y</code></td>
<td>
<p>a (n) vector of (continuous) responses. <code>Y</code> must be a 
vector or a one column matrix. It contains the response variable for 
each observation.</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_lambda.l1.range">lambda.l1.range</code></td>
<td>
<p>a vecor of positive real values, in [0,1]. 
<code>lambda.l1</code> is the sparse penalty parameter for the dimension 
reduction step by sparse PLS (see details), the optimal value will be 
chosen among <code>lambda.l1.range</code>.</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_ncomp.range">ncomp.range</code></td>
<td>
<p>a vector of positive integers. <code>ncomp</code> is the 
number of PLS components. The optimal value will be chosen 
among <code>ncomp.range</code>.</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_weight.mat">weight.mat</code></td>
<td>
<p>a (ntrain x ntrain) matrix used to weight the l2 metric 
in the observation space, it can be the covariance inverse of the Ytrain 
observations in a heteroskedastic context. If NULL, the l2 metric is the 
standard one, corresponding to homoskedastic model (<code>weight.mat</code> is the 
identity matrix).</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_adapt">adapt</code></td>
<td>
<p>a boolean value, indicating whether the sparse PLS selection 
step sould be adaptive or not (see details).</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_center.x">center.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be centered or not.</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_center.y">center.Y</code></td>
<td>
<p>a boolean value indicating whether the response values 
<code>Ytrain</code> set should be centered or not.</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_scale.x">scale.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be scaled or not 
(<code>scale.X=TRUE</code> implies <code>center.X=TRUE</code>).</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_scale.y">scale.Y</code></td>
<td>
<p>a boolean value indicating whether the response values 
<code>Ytrain</code> should be scaled or not (<code>scale.Y=TRUE</code> implies 
<code>center.Y=TRUE</code>).</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_weighted.center">weighted.center</code></td>
<td>
<p>a boolean value indicating whether the centering 
should take into account the weighted l2 metric or not 
(if TRUE, it requires that weighted.mat is non NULL).</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_return.grid">return.grid</code></td>
<td>
<p>a boolean values indicating whether the grid of 
hyper-parameters values with corresponding mean prediction error rate over 
the folds should be returned or not.</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_ncores">ncores</code></td>
<td>
<p>a positve integer, indicating the number of cores that the 
cross-validation is allowed to use for parallel computation (see details).</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_nfolds">nfolds</code></td>
<td>
<p>a positive integer indicating the number of folds in the 
K-folds cross-validation procedure, <code>nfolds=n</code> corresponds 
to the leave-one-out cross-validation, default is 10.</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_nrun">nrun</code></td>
<td>
<p>a positive integer indicating how many times the K-folds cross-
validation procedure should be repeated, default is 1.</p>
</td></tr>
<tr><td><code id="spls.cv_+3A_verbose">verbose</code></td>
<td>
<p>a boolean value indicating verbosity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>Xtrain</code> and <code>Xtest</code> may not 
be standardized, since standardizing can be performed by the function 
<code>spls.cv</code> as a preliminary step.
</p>
<p>The procedure is described in Durif et al. (2018). The K-fold 
cross-validation can be summarize as follow: the train set is partitioned 
into K folds, for each value of hyper-parameters the model is fit K times, 
using each fold to compute the prediction error rate, and fitting the 
model on the remaining observations. The cross-validation procedure returns 
the optimal hyper-parameters values, meaning the one that minimize 
the mean squared error of prediction averaged over all the folds.
</p>
<p>This procedures uses the <code>mclapply</code> from the <code>parallel</code> package, 
available on GNU/Linux and MacOS. Users of Microsoft Windows can refer to 
the README file in the source to be able to use a mclapply type function.
</p>


<h3>Value</h3>

<p>An object with the following attributes
</p>
<table>
<tr><td><code>lambda.l1.opt</code></td>
<td>
<p>the optimal value in <code>lambda.l1.range</code>.</p>
</td></tr>
<tr><td><code>ncomp.opt</code></td>
<td>
<p>the optimal value in <code>ncomp.range</code>.</p>
</td></tr>
<tr><td><code>cv.grid</code></td>
<td>
<p>the grid of hyper-parameters and corresponding prediction 
error rate over the folds. 
<code>cv.grid</code> is NULL if <code>return.grid</code> is set to FALSE.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spls">spls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
sample1 &lt;- sample.cont(n=n, p=p, kstar=10, lstar=2, 
                       beta.min=0.25, beta.max=0.75, mean.H=0.2, 
                       sigma.H=10, sigma.F=5, sigma.E=5)
                       
X &lt;- sample1$X
Y &lt;- sample1$Y

### hyper-parameters values to test
lambda.l1.range &lt;- seq(0.05,0.95,by=0.1) # between 0 and 1
ncomp.range &lt;- 1:10

### tuning the hyper-parameters
cv1 &lt;- spls.cv(X=X, Y=Y, lambda.l1.range=lambda.l1.range, 
               ncomp.range=ncomp.range, weight.mat=NULL, adapt=TRUE, 
               center.X=TRUE, center.Y=TRUE, 
               scale.X=TRUE, scale.Y=TRUE, weighted.center=FALSE, 
               return.grid=TRUE, ncores=1, nfolds=10, nrun=1)
str(cv1)

### otpimal values
cv1$lambda.l1.opt
cv1$ncomp.opt

## End(Not run)

</code></pre>

<hr>
<h2 id='spls.stab'>Stability selection procedure to estimate probabilities of selection of 
covariates for the sparse PLS method</h2><span id='topic+spls.stab'></span>

<h3>Description</h3>

<p>The function <code>spls.stab</code> train a sparse PLS model for each 
candidate values <code>(ncomp, lambda.l1)</code> of hyper-parameters 
on multiple sub-samplings in the data. The stability selection procedure 
selects the covariates that are selected by most of the models among the 
grid of hyper-parameters, following the procedure described in 
Durif et al. (2018). Candidates values for <code>ncomp</code> and <code>lambda.l1</code> 
are respectively given by the input arguments <code>ncomp.range</code> and 
<code>lambda.l1.range</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spls.stab(
  X,
  Y,
  lambda.l1.range,
  ncomp.range,
  weight.mat = NULL,
  adapt = TRUE,
  center.X = TRUE,
  center.Y = TRUE,
  scale.X = TRUE,
  scale.Y = TRUE,
  weighted.center = FALSE,
  ncores = 1,
  nresamp = 100,
  seed = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spls.stab_+3A_x">X</code></td>
<td>
<p>a (n x p) data matrix of predictors. <code>X</code> must be a matrix. 
Each row corresponds to an observation and each column to a 
predictor variable.</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_y">Y</code></td>
<td>
<p>a (n) vector of (continuous) responses. <code>Y</code> must be a 
vector or a one column matrix. It contains the response variable for 
each observation. <code>Y</code> should take values in {0,1}.</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_lambda.l1.range">lambda.l1.range</code></td>
<td>
<p>a vecor of positive real values, in [0,1]. 
<code>lambda.l1</code> is the sparse penalty parameter for the dimension 
reduction step by sparse PLS (see details), the optimal value will be 
chosen among <code>lambda.l1.range</code>.</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_ncomp.range">ncomp.range</code></td>
<td>
<p>a vector of positive integers. <code>ncomp</code> is the 
number of PLS components. The optimal value will be chosen 
among <code>ncomp.range</code>.</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_weight.mat">weight.mat</code></td>
<td>
<p>a (ntrain x ntrain) matrix used to weight the l2 metric 
in the observation space, it can be the covariance inverse of the Ytrain 
observations in a heteroskedastic context. If NULL, the l2 metric is the 
standard one, corresponding to homoskedastic model (<code>weight.mat</code> is the 
identity matrix).</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_adapt">adapt</code></td>
<td>
<p>a boolean value, indicating whether the sparse PLS selection 
step sould be adaptive or not (see details).</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_center.x">center.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be centered or not.</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_center.y">center.Y</code></td>
<td>
<p>a boolean value indicating whether the response values 
<code>Ytrain</code> set should be centered or not.</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_scale.x">scale.X</code></td>
<td>
<p>a boolean value indicating whether the data matrices 
<code>Xtrain</code> and <code>Xtest</code> (if provided) should be scaled or not 
(<code>scale.X=TRUE</code> implies <code>center.X=TRUE</code>).</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_scale.y">scale.Y</code></td>
<td>
<p>a boolean value indicating whether the response values 
<code>Ytrain</code> should be scaled or not (<code>scale.Y=TRUE</code> implies 
<code>center.Y=TRUE</code>).</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_weighted.center">weighted.center</code></td>
<td>
<p>a boolean value indicating whether the centering 
should take into account the weighted l2 metric or not 
(if TRUE, it requires that weighted.mat is non NULL).</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_ncores">ncores</code></td>
<td>
<p>a positve integer, indicating the number of cores that the 
cross-validation is allowed to use for parallel computation (see details).</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_nresamp">nresamp</code></td>
<td>
<p>number of resamplings of the data to estimate the probility 
of selection for each covariate, default is 100.</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_seed">seed</code></td>
<td>
<p>a positive integer value (default is NULL). If non NULL, 
the seed for pseudo-random number generation is set accordingly.</p>
</td></tr>
<tr><td><code id="spls.stab_+3A_verbose">verbose</code></td>
<td>
<p>a boolean parameter indicating the verbosity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The columns of the data matrices <code>X</code> may not be standardized, 
since standardizing is performed by the function <code>spls.stab</code> 
as a preliminary step. 
</p>
<p>The procedure is described in Durif et al. (2018). The stability selection 
procedure can be summarize as follow (c.f. Meinshausen and Buhlmann, 2010).
</p>
<p>(i) For each candidate values <code>(ncomp, lambda.l1)</code> of 
hyper-parameters, a logit-SPLS is trained on <code>nresamp</code> resamplings 
of the data. Then, for each pair <code>(ncomp, lambda.l1)</code>, 
the probability that a covariate (i.e. a column in <code>X</code>) is selected is 
computed among the resamplings.
</p>
<p>(ii) Eventually, the set of &quot;stable selected&quot; variables corresponds to the 
set of covariates that were selected by most of the training among the 
grid of hyper-parameters candidate values.
</p>
<p>This function achieves the first step (i) of the stability selection 
procedure. The second step (ii) is achieved by the function 
<code><a href="#topic+stability.selection">stability.selection</a></code>.
</p>
<p>This procedures uses <code>mclapply</code> from the <code>parallel</code> package, 
available on GNU/Linux and MacOS. Users of Microsoft Windows can refer to 
the README file in the source to be able to use a mclapply type function.
</p>


<h3>Value</h3>

<p>An object with the following attributes
</p>
<table>
<tr><td><code>q.Lambda</code></td>
<td>
<p>A table with values of q.Lambda (c.f. Durif 
et al. (2018) for the notation), being the averaged number of covariates
selected among the entire grid of hyper-parameters candidates values,
for increasing size of hyper-parameter grid.</p>
</td></tr>
<tr><td><code>probs.lambda</code></td>
<td>
<p>A table with estimated probability of selection for each 
covariates depending on the candidates values for hyper-parameters.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>An integer values indicating the number of covariates in the 
model.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>
<p>Meinshausen, N., Buhlmann P. (2010). Stability Selection. Journal of the 
Royal Statistical Society: Series B (Statistical Methodology) 
72, no. 4, 417-473.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spls">spls</a></code>, <code><a href="#topic+stability.selection">stability.selection</a></code>, 
<code><a href="#topic+stability.selection.heatmap">stability.selection.heatmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
sample1 &lt;- sample.cont(n=n, p=p, kstar=10, lstar=2, 
                       beta.min=0.25, beta.max=0.75, mean.H=0.2, 
                       sigma.H=10, sigma.F=5, sigma.E=5)
                       
X &lt;- sample1$X
Y &lt;- sample1$Y

### hyper-parameters values to test
lambda.l1.range &lt;- seq(0.05,0.95,by=0.1) # between 0 and 1
ncomp.range &lt;- 1:10

### tuning the hyper-parameters
stab1 &lt;- spls.stab(X=X, Y=Y, lambda.l1.range=lambda.l1.range, 
                   ncomp.range=ncomp.range, 
                   adapt=TRUE, 
                   ncores=1, nresamp=100)
                       
str(stab1)

### heatmap of estimated probabilities
stability.selection.heatmap(stab1)

### selected covariates
stability.selection(stab1, piThreshold=0.6, rhoError=10)

## End(Not run)

</code></pre>

<hr>
<h2 id='SRBCT'>Gene expression data from Khan et al. (2001)</h2><span id='topic+SRBCT'></span>

<h3>Description</h3>

<p>Gene expression data (2308 genes for 83 samples) from the
microarray experiments of Small Round Blue Cell Tumors (SRBCT) 
of childhood cancer study of Khan et al. (2001). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(SRBCT)
</code></pre>


<h3>Details</h3>

<p> This data set contains 83 samples 
with 2308 genes: 29 cases of Ewing sarcoma (EWS), coded 1, 
11 cases of Burkitt lymphoma (BL), coded 2, 18 cases of 
neuroblastoma (NB), coded 3, 25 cases of rhabdomyosarcoma (RMS), coded 4.
A total of 63 training samples and 25 test samples are 
provided in Khan et al. (2001). Five of the test set are
non-SRBCT and are not considered here. The training sample indexes correspond
to 1:65 and the test sample indexes (without non-SRBCT sample) correspond to 66:83.
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>a (88 x 2308) matrix giving the expression levels of 2308 
genes for 88 SRBCT patients. Each row corresponds to a patient, each column to a
gene.</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>a numeric vector of length 88 giving the cancer class of each
patient.</p>
</td></tr> 
<tr><td><code>gene.names</code></td>
<td>
<p>a matrix containing the names of the 2308 genes for the gene
expression matrix <code>X</code>. The two columns correspond to
the gene 'Image.Id.' and 'Gene.Description', respectively.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The data are described in Khan et al. (2001). The data was originally 
collected from <code>http://research.nhgri.nih.gov/microarray/Supplement/</code> 
but the URL is not working anymore and we could not find a 
replacement link in 2024.</p>


<h3>References</h3>

<p>Khan, J. and Wei, J. S. and Ringner, M. and Saal, L. H. and Ladanyi,
M. and Westermann, F. and Berthold, F. and Schwab,
M. and Antonescu, C. R. and Peterson, C. and Meltzer, P. S. (2001).
Classification and diagnostic prediction of cancers using gene expression profiling and artificial
neural networks, Nature Medecine, 7, 673&ndash;679.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load data set
data(SRBCT)

# how many samples and how many genes ?
dim(SRBCT$X)

# how many samples of class 1, 2, 3 and 4, respectively ?
sum(SRBCT$Y==1)
sum(SRBCT$Y==2)
sum(SRBCT$Y==3)
sum(SRBCT$Y==4)</code></pre>

<hr>
<h2 id='stability.selection'>Stability selection procedure to select covariates for the sparse PLS, 
LOGIT-SPLS and multinomial-SPLS methods</h2><span id='topic+stability.selection'></span>

<h3>Description</h3>

<p>The function <code>stability.selection</code> returns the list of selected 
covariates, when following the stability selection procedure described in 
Durif et al. (2018). In particular, it selects covariates that are selected 
by most of the sparse PLS, the logit-SPLS or the multinomial-SPLS models 
when exploring the grid of hyper-parameter candidate values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stability.selection(stab.out, piThreshold = 0.9, rhoError = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stability.selection_+3A_stab.out">stab.out</code></td>
<td>
<p>the output of the functions <code><a href="#topic+spls.stab">spls.stab</a></code>, 
<code><a href="#topic+logit.spls.stab">logit.spls.stab</a></code> or <code><a href="#topic+multinom.spls.stab">multinom.spls.stab</a></code>.</p>
</td></tr>
<tr><td><code id="stability.selection_+3A_pithreshold">piThreshold</code></td>
<td>
<p>a value in (0,1], corresponding to the threshold 
probability used to select covariate (c.f. Durif et al., 2018).</p>
</td></tr>
<tr><td><code id="stability.selection_+3A_rhoerror">rhoError</code></td>
<td>
<p>a positive value used to restrict the grid of 
hyper-parameter candidate values (c.f. Durif et al., 2018).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The procedure is described in Durif et al. (2018). The stability selection 
procedure can be summarize as follow (c.f. Meinshausen and Buhlmann, 2010).
</p>
<p>(i) For each candidate values of hyper-parameters, a model is trained 
on <code>nresamp</code> resamplings of the data. Then, for each candidate value of
the hyper-parameters, the probability that a covariate 
(i.e. a column in <code>X</code>) is selected is computed among the resamplings.
</p>
<p>The estimated probabilities can be visualized as a heatmap with the 
function <code><a href="#topic+stability.selection.heatmap">stability.selection.heatmap</a></code>.
</p>
<p>(ii) Eventually, the set of &quot;stable selected&quot; variables corresponds to the 
set of covariates that were selected by most of the training among the 
grid of hyper-parameters candidate values, based on a threshold probability
<code>piThreshold</code> and a restriction of the grid of hyper-parameters based 
on <code>rhoError</code> (c.f. Durif et al., 2018, for details).
</p>
<p>This function achieves the second step (ii) of the stability selection 
procedure. The first step (i) is achieved by the functions 
<code><a href="#topic+spls.stab">spls.stab</a></code>, <code><a href="#topic+logit.spls.stab">logit.spls.stab</a></code> 
or <code><a href="#topic+multinom.spls.stab">multinom.spls.stab</a></code>.
</p>


<h3>Value</h3>

<p>An object with the following attributes:
</p>
<table>
<tr><td><code>selected.predictors</code></td>
<td>
<p>The list of the name of covariates that 
are selected.</p>
</td></tr>
<tr><td><code>max.probs</code></td>
<td>
<p>The corresponding estimated probabilities of selection for 
each covariate, i.e. the maximal values on the reduced grid of 
hyper-parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>References</h3>

<p>Durif, G., Modolo, L., Michaelsson, J., Mold, J.E., Lambert-Lacroix, S., 
Picard, F., 2018. High dimensional classification with combined 
adaptive sparse PLS and logistic regression. Bioinformatics 34, 
485&ndash;493. <a href="https://doi.org/10.1093/bioinformatics/btx571">doi:10.1093/bioinformatics/btx571</a>.
Available at <a href="http://arxiv.org/abs/1502.05933">http://arxiv.org/abs/1502.05933</a>.
</p>
<p>Meinshausen, N., Buhlmann P. (2010). Stability Selection. Journal of the 
Royal Statistical Society: Series B (Statistical Methodology) 
72, no. 4, 417-473.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spls.stab">spls.stab</a></code>, <code><a href="#topic+logit.spls.stab">logit.spls.stab</a></code>, 
<code><a href="#topic+multinom.spls.stab">multinom.spls.stab</a></code>, <code><a href="#topic+stability.selection.heatmap">stability.selection.heatmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
sample1 &lt;- sample.cont(n=n, p=p, kstar=10, lstar=2, 
                       beta.min=0.25, beta.max=0.75, mean.H=0.2, 
                       sigma.H=10, sigma.F=5, sigma.E=5)
                       
X &lt;- sample1$X
Y &lt;- sample1$Y

### hyper-parameters values to test
lambda.l1.range &lt;- seq(0.05,0.95,by=0.1) # between 0 and 1
ncomp.range &lt;- 1:10

### tuning the hyper-parameters
stab1 &lt;- spls.stab(X=X, Y=Y, lambda.l1.range=lambda.l1.range, 
                   ncomp.range=ncomp.range, weight.mat=NULL, 
                   adapt=FALSE, center.X=TRUE, center.Y=TRUE, 
                   scale.X=TRUE, scale.Y=TRUE, weighted.center=FALSE, 
                   ncores=1, nresamp=100)
                       
str(stab1)

### selected covariates
stability.selection(stab1, piThreshold=0.6, rhoError=10)

## End(Not run)

</code></pre>

<hr>
<h2 id='stability.selection.heatmap'>Heatmap visualization of estimated probabilities of selection for each
covariate</h2><span id='topic+stability.selection.heatmap'></span>

<h3>Description</h3>

<p>The function <code>stability.selection.heatmap</code> allows to visualize 
estimated probabilities to be selected for each covariate depending on the
value of hyper-parameters in the spls, logit-spls or multinomial-spls models. 
These estimated probabilities are used in the stability selection procedure 
described in Durif et al. (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stability.selection.heatmap(stab.out, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stability.selection.heatmap_+3A_stab.out">stab.out</code></td>
<td>
<p>the output of the functions <code><a href="#topic+spls.stab">spls.stab</a></code>,
<code><a href="#topic+logit.spls.stab">logit.spls.stab</a></code> or <code><a href="#topic+multinom.spls.stab">multinom.spls.stab</a></code>.</p>
</td></tr>
<tr><td><code id="stability.selection.heatmap_+3A_...">...</code></td>
<td>
<p>any argument that could be pass to the functions 
<code><a href="fields.html#topic+image.plot">image.plot</a></code> or <code><a href="graphics.html#topic+image">image</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The procedure is described in Durif et al. (2018). The stability selection 
procedure can be summarize as follow (c.f. Meinshausen and Buhlmann, 2010).
</p>
<p>(i) For each candidate values of hyper-parameters, a model is trained 
on <code>nresamp</code> resamplings of the data. Then, for each candidate value of
the hyper-parameters, the probability that a covariate 
(i.e. a column in <code>X</code>) is selected is computed among the resamplings.
</p>
<p>The estimated probabilities can be visualized as a heatmap with the 
function <code><a href="#topic+stability.selection.heatmap">stability.selection.heatmap</a></code>.
</p>
<p>(ii) Eventually, the set of &quot;stable selected&quot; variables corresponds to the 
set of covariates that were selected by most of the training among the 
grid of hyper-parameters candidate values, based on a threshold probability
<code>piThreshold</code> and a restriction of the grid of hyper-parameters based 
on <code>rhoError</code> (c.f. Durif et al., 2018, for details).
</p>
<p>This function allows to visualize probabalities estimated at the first 
step (i) of the stability selection by the functions <code><a href="#topic+spls.stab">spls.stab</a></code>,
<code><a href="#topic+logit.spls.stab">logit.spls.stab</a></code> or <code><a href="#topic+multinom.spls.stab">multinom.spls.stab</a></code>.
</p>
<p>This function use the function <code><a href="#topic+matrix.heatmap">matrix.heatmap</a></code>.
</p>


<h3>Value</h3>

<p>No return, just plot the heatmap in the current graphic window.
</p>


<h3>Author(s)</h3>

<p>Ghislain Durif (<a href="https://gdurif.perso.math.cnrs.fr/">https://gdurif.perso.math.cnrs.fr/</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logit.spls">logit.spls</a></code>, <code><a href="#topic+stability.selection">stability.selection</a></code>, 
<code><a href="#topic+stability.selection.heatmap">stability.selection.heatmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
### load plsgenomics library
library(plsgenomics)

### generating data
n &lt;- 100
p &lt;- 100
sample1 &lt;- sample.cont(n=n, p=p, kstar=10, lstar=2, 
                       beta.min=0.25, beta.max=0.75, mean.H=0.2, 
                       sigma.H=10, sigma.F=5, sigma.E=5)
                       
X &lt;- sample1$X
Y &lt;- sample1$Y

### hyper-parameters values to test
lambda.l1.range &lt;- seq(0.05,0.95,by=0.1) # between 0 and 1
ncomp.range &lt;- 1:10

### tuning the hyper-parameters
stab1 &lt;- spls.stab(X=X, Y=Y, lambda.l1.range=lambda.l1.range, 
                   ncomp.range=ncomp.range, weight.mat=NULL, 
                   adapt=FALSE, center.X=TRUE, center.Y=TRUE, 
                   scale.X=TRUE, scale.Y=TRUE, weighted.center=FALSE, 
                   ncores=1, nresamp=100)
                       
str(stab1)

### heatmap of estimated probabilities
stability.selection.heatmap(stab1)

## End(Not run)

</code></pre>

<hr>
<h2 id='TFA.estimate'>Prediction of Transcription Factor Activities using PLS</h2><span id='topic+TFA.estimate'></span>

<h3>Description</h3>

<p>The function <code>TFA.estimate</code> estimates the transcription factor activities from gene 
expression data and ChIP data using the PLS multivariate regression approach described 
in Boulesteix and Strimmer (2005).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TFA.estimate(CONNECdata, GEdata, ncomp=NULL, nruncv=0, alpha=2/3, unit.weights=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TFA.estimate_+3A_connecdata">CONNECdata</code></td>
<td>
<p>a (n x p) matrix containing the ChIP data for the n genes and the
p predictors. The n genes must be the same as the n genes of <code>GEdata</code> and the ordering
of the genes must also be the same. Each row of <code>ChIPdata</code> corresponds to a gene, each
column to a transcription factor. <code>CONNECdata</code> might have either binary (e.g. 0-1) or 
numeric entries.</p>
</td></tr>
<tr><td><code id="TFA.estimate_+3A_gedata">GEdata</code></td>
<td>
<p>a (n x m) matrix containing the gene expression levels of the n
considered genes for m samples. Each row of <code>GEdata</code> corresponds to a gene, each
column to a sample.</p>
</td></tr>
<tr><td><code id="TFA.estimate_+3A_ncomp">ncomp</code></td>
<td>
<p>if <code>nruncv=0</code>, <code>ncomp</code> is the number of latent 
components to be  constructed. If <code>nruncv&gt;0</code>, the  number of 
latent components to be used for PLS
regression is chosen from 1,...,<code>ncomp</code> using the cross-validation procedure described
in Boulesteix and Strimmer (2005). If <code>ncomp=NULL</code>, <code>ncomp</code>
is set to min(n,p).</p>
</td></tr>
<tr><td><code id="TFA.estimate_+3A_nruncv">nruncv</code></td>
<td>
<p>the number of cross-validation iterations to be performed for the choice of
the number of latent components. If <code>nruncv=0</code>, cross-validation is not performed and
<code>ncomp</code> latent components are used.</p>
</td></tr>
<tr><td><code id="TFA.estimate_+3A_alpha">alpha</code></td>
<td>
<p>the proportion of genes to be included in the training set for
the cross-validation procedure.</p>
</td></tr>
<tr><td><code id="TFA.estimate_+3A_unit.weights">unit.weights</code></td>
<td>
<p>If <code>TRUE</code> then the latent components
will be constructed from weight vectors that are standardized to length 1, 
otherwise the weight vectors do not have length 1 but the latent components have
norm 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The gene expression data as well as the ChIP data are assumed to have been
properly normalized. However, they do not have to be centered or scaled, since
centering and scaling are performed by the function <code>TFA.estimate</code> as a
preliminary step. 
</p>
<p>The matrix <code>ChIPdata</code> containing the ChIP data for the n genes and p transcription 
factors might be replaced by  any 'connectivity' matrix whose entries give the strength
of the interactions between the genes and transcription factors. For instance, a connectivity
matrix obtained by aggregating qualitative information from various genomic databases 
might be used as argument  in place of ChIP data.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>TFA</code></td>
<td>
<p>a (p x m) matrix containing the estimated transcription factor
activities for the p transcription factors and the m samples.</p>
</td></tr>
<tr><td><code>metafactor</code></td>
<td>
<p>a (m x <code>ncomp</code>) matrix containing the metafactors for the m
samples. Each row corresponds to a sample, each column to a metafactor.</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p>the number of latent components used in the PLS regression.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Anne-Laure Boulesteix 
(<a href="https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html">https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html</a>) 
and 
Korbinian Strimmer (<a href="https://strimmerlab.github.io/korbinian.html">https://strimmerlab.github.io/korbinian.html</a>).
</p>


<h3>References</h3>

<p>A. L. Boulesteix and K. Strimmer (2005). Predicting Transcription Factor Activities 
from Combined Analysis of Microarray and ChIP Data: A Partial Least Squares Approach.
</p>
<p>A. L. Boulesteix, K. Strimmer (2007). Partial least squares: a versatile tool for the analysis 
of high-dimensional genomic data. Briefings in Bioinformatics 7:32-44.
</p>
<p>S. de Jong (1993). SIMPLS: an alternative approach to partial least squares 
regression, Chemometrics Intell. Lab. Syst. <b>18</b>, 251&ndash;263.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.regression">pls.regression</a></code>, <code><a href="#topic+pls.regression.cv">pls.regression.cv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# load Ecoli data
data(Ecoli)

# estimate TFAs based on 3 latent components
TFA.estimate(Ecoli$CONNECdata,Ecoli$GEdata,ncomp=3,nruncv=0)

# estimate TFAs and determine the best number of latent components simultaneously
TFA.estimate(Ecoli$CONNECdata,Ecoli$GEdata,ncomp=1:5,nruncv=20)

</code></pre>

<hr>
<h2 id='variable.selection'>Variable selection using the PLS weights</h2><span id='topic+variable.selection'></span>

<h3>Description</h3>

<p>The function <code>variable.selection</code>  performs variable selection for binary classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>variable.selection(X, Y, nvar=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="variable.selection_+3A_x">X</code></td>
<td>
<p>a (n x p) data matrix of predictors. X may be a matrix or a
data frame. Each row corresponds to an observation and each column corresponds
to a predictor variable.</p>
</td></tr>
<tr><td><code id="variable.selection_+3A_y">Y</code></td>
<td>
<p>a vector of length n giving the classes of the n observations.  The two classes 
must be coded as 1,2.</p>
</td></tr>
<tr><td><code id="variable.selection_+3A_nvar">nvar</code></td>
<td>
<p>the number of variables to be returned. If <code>nvar=NULL</code>, all the variables are
returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>variable.selection</code> orders the variables according to 
the absolute value of the weight defining the first PLS
component. This ordering is equivalent to the ordering obtained with the 
F-statistic and t-test with equal variances (Boulesteix, 2004).
</p>
<p>For computational reasons, the function <code>variable.selection</code> does not use 
the pls algorithm, but the obtained ordering of the variables is exactly 
equivalent to the ordering obtained using the PLS weights output by
<code><a href="#topic+pls.regression">pls.regression</a></code>.
</p>


<h3>Value</h3>

<p>A vector of length <code>nvar</code> (or of length p if <code>nvar=NULL</code>) containing the indices of 
the variables to be selected. The variables are ordered from the best to the worst variable. 
</p>


<h3>Author(s)</h3>

<p>Anne-Laure Boulesteix 
(<a href="https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html">https://www.ibe.med.uni-muenchen.de/mitarbeiter/professoren/boulesteix/index.html</a>) 
</p>


<h3>References</h3>

<p>A. L. Boulesteix (2004). PLS dimension reduction for classification with microarray data,
Statistical Applications in Genetics and Molecular Biology <b>3</b>, Issue 1, Article 33.
</p>
<p>A. L. Boulesteix, K. Strimmer (2007). Partial least squares: a versatile tool for the analysis 
of high-dimensional genomic data. Briefings in Bioinformatics 7:32-44.
</p>
<p>S. de Jong (1993). SIMPLS: an alternative approach to partial least squares 
regression, Chemometrics Intell. Lab. Syst. <b>18</b>, 251&ndash;263.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.regression">pls.regression</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load plsgenomics library
library(plsgenomics)

# generate X and Y (4 observations and 3 variables)
X&lt;-matrix(c(4,3,3,4,1,0,6,7,3,5,5,9),4,3,byrow=FALSE)
Y&lt;-c(1,1,2,2)

# select the 2 best variables
variable.selection(X,Y,nvar=2)
# order the 3 variables
variable.selection(X,Y)

# load the leukemia data 
data(leukemia)

# select the 50 best variables from the leukemia data
variable.selection(leukemia$X,leukemia$Y,nvar=50)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
