<!DOCTYPE html><html lang="en"><head><title>Help for package QAEnsemble</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {QAEnsemble}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ensemblealg'><p>Ensemble MCMC algorithm (either Quadratic or Affine Invariant method)</p></a></li>
<li><a href='#hpdparameter'><p>Highest Posterior Density (HPD) for a parameter</p></a></li>
<li><a href='#psrfdiagnostic'><p>Potential Scale Reduction Factor computation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Ensemble Quadratic and Affine Invariant Markov Chain Monte Carlo</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>The Ensemble Quadratic and Affine Invariant Markov chain Monte 
    Carlo algorithms provide an efficient way to perform Bayesian inference in 
    difficult parameter space geometries. The Ensemble Quadratic Monte Carlo 
    algorithm was developed by Militzer (2023) &lt;<a href="https://doi.org/10.3847%2F1538-4357%2Face1f1">doi:10.3847/1538-4357/ace1f1</a>&gt;. 
    The Ensemble Affine Invariant algorithm was developed by Goodman and Weare 
    (2010) &lt;<a href="https://doi.org/10.2140%2Fcamcos.2010.5.65">doi:10.2140/camcos.2010.5.65</a>&gt; and it was implemented in Python by 
    Foreman-Mackey et al (2013) &lt;<a href="https://doi.org/10.48550%2FarXiv.1202.3665">doi:10.48550/arXiv.1202.3665</a>&gt;. The Quadratic 
    Monte Carlo method was shown to perform better than the Affine Invariant 
    method in the paper by Militzer (2023) &lt;<a href="https://doi.org/10.3847%2F1538-4357%2Face1f1">doi:10.3847/1538-4357/ace1f1</a>&gt; and 
    the Quadratic Monte Carlo method is the default method used. The Chen-Shao 
    Highest Posterior Density Estimation algorithm is used for obtaining 
    credible intervals and the potential scale reduction factor diagnostic is 
    used for checking the convergence of the chains.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>coda, diagram, expm, knitr, rmarkdown, svMisc</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-08 18:47:26 UTC; wroda</td>
</tr>
<tr>
<td>Author:</td>
<td>Weston Roda <a href="https://orcid.org/0000-0001-7200-7605"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Karsten Hempel <a href="https://orcid.org/0000-0003-3273-4247"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Sasha van Katwyk <a href="https://orcid.org/0000-0003-3026-2063"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Diepreye Ayabina <a href="https://orcid.org/0000-0002-7005-6734"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Children's Hospital of Eastern Ontario [fnd],
  Canada's Drug Agency [fnd],
  Institute of Health Economics [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Weston Roda &lt;wroda@ihe.ca&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-09 14:50:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='ensemblealg'>Ensemble MCMC algorithm (either Quadratic or Affine Invariant method)</h2><span id='topic+ensemblealg'></span>

<h3>Description</h3>

<p>This function runs the Ensemble Quadratic or Affine Invariant
MCMC algorithm for Bayesian inference parameter estimation and it is based
off of the papers by Militzer (2023), Goodman and Weare (2010), and
Foreman-Mackey, Hogg, Lang, and Goodman (2013). The Ensemble Quadratic Monte
Carlo algorithm was developed by Militzer (2023). The Ensemble Affine
Invariant algorithm was developed by Goodman and Weare (2010) and it was
implemented in Python by Foreman-Mackey, Hogg, Lang, and Goodman (2013). The
Quadratic Monte Carlo method was shown to perform better than the Affine
Invariant method in the paper by Militzer (2023) and the Quadratic method is
the default method used in the 'ensemblealg' function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ensemblealg(
  theta0,
  logfuns,
  T_iter,
  Thin_val,
  UseQuad = TRUE,
  a_par = NULL,
  ShowProgress = FALSE,
  ReturnCODA = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ensemblealg_+3A_theta0">theta0</code></td>
<td>
<p>The matrix of initial guesses for the MCMC chains</p>
</td></tr>
<tr><td><code id="ensemblealg_+3A_logfuns">logfuns</code></td>
<td>
<p>A list object containing the log prior function and log
likelihood function</p>
</td></tr>
<tr><td><code id="ensemblealg_+3A_t_iter">T_iter</code></td>
<td>
<p>The number of iterations to run for each chain in the Ensemble
MCMC algorithm</p>
</td></tr>
<tr><td><code id="ensemblealg_+3A_thin_val">Thin_val</code></td>
<td>
<p>Every nth iteration is saved, where n is equal to the
&quot;Thin_val&quot; parameter</p>
</td></tr>
<tr><td><code id="ensemblealg_+3A_usequad">UseQuad</code></td>
<td>
<p>If this bool is true, then the Ensemble Quadratic MCMC
algorithm is used. Otherwise, the Ensemble Affine Invariant MCMC algorithm is
used. (The default setting is true.)</p>
</td></tr>
<tr><td><code id="ensemblealg_+3A_a_par">a_par</code></td>
<td>
<p>The parameter 'a_par' is a performance parameter for the MCMC
ensemble algorithm. (The default setting for the Quadratic algorithm is
'a_par' equal to 1.5 and the default setting for the Affine Invariant
algorithm is 'a_par' equal to 2.)</p>
</td></tr>
<tr><td><code id="ensemblealg_+3A_showprogress">ShowProgress</code></td>
<td>
<p>If this bool is true, then the progress of the algorithm
is shown. Otherwise, the progress of the algorithm is not shown.(The default
setting is false.)</p>
</td></tr>
<tr><td><code id="ensemblealg_+3A_returncoda">ReturnCODA</code></td>
<td>
<p>If this bool is true, then the 'coda' package 'mcmc.list'
object is returned along with the other outputs (The default setting is
false.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object is returned that contains three matrices: theta_sample,
log_like_sample, and log_prior_sample.<br />
</p>
<p>theta_sample: this is the matrix of
parameter samples returned from the Ensemble MCMC algorithm, the matrix
dimensions are given by
(Number of parameters) x (Number of chains) x (Number of iterations) <br />
</p>
<p>log_like_sample: this is the matrix of log likelihood samples returned from
the Ensemble MCMC algorithm, the matrix dimensions are given by
(Number of chains) x (Number of iterations) <br />
</p>
<p>log_prior_sample: this is the matrix of log prior samples returned from the
Ensemble MCMC algorithm, the matrix dimensions are given by
(Number of chains) x (Number of iterations) <br />
</p>
<p>mcmc_list_coda: (optional) this is the 'coda' package 'mcmc.list' object that
can be used with various MCMC diagnostic functions in the 'coda' package
</p>


<h3>References</h3>

<p>Militzer B (2023) Study of Jupiterâ€™s Interior with Quadratic
Monte Carlo Simulations. ApJ 953(111):20pp.
https://doi.org/10.3847/1538-4357/ace1f1 <br />
</p>
<p>Goodman J and Weare J (2010) Ensemble samplers with affine invariance. Commun
Appl Math Comput Sci 5(1):65-80.
https://doi.org/10.2140/camcos.2010.5.65 <br />
</p>
<p>Foreman-Mackey D, Hogg DW, Lang D, Goodman J (2013) emcee: The MCMC Hammer.
PASP 125(925):306. https://doi.org/10.48550/arXiv.1202.3665
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Ensemble Quadratic MCMC algorithm example for fitting a Weibull
#distribution

#Assume the true parameters are

a_shape = 20
sigma_scale = 900

#Random sample from the Weibull distribution with a = 20 and sigma = 900,
#Y~WEI(a = 20, sigma = 900)

num_ran_samples = 50

data_weibull = matrix(NA, nrow = 1, ncol = num_ran_samples)

#Set the seed for this example
set.seed(10)

data_weibull = rweibull(num_ran_samples, shape = a_shape, scale = sigma_scale)

#We want to estimate a_shape and sigma_scale

#Log prior function for a_shape and sigma_scale
#(assumed priors a_shape ~ U(1e-2, 1e2) and sigma_scale ~ U(1, 1e4))
logp &lt;- function(param)
{
 a_shape_use = param[1]
 sigma_scale_use = param[2]

 logp_val = dunif(a_shape_use, min = 1e-2, max = 1e2, log = TRUE) +
 dunif(sigma_scale_use, min = 1, max = 1e4, log = TRUE)

 return(logp_val)
}

#Log likelihood function for a_shape and sigma_scale
logl &lt;- function(param)
{
 a_shape_use = param[1]
 sigma_scale_use = param[2]

 logl_val = sum(dweibull(data_weibull, shape = a_shape_use, scale = sigma_scale_use, log = TRUE))

 return(logl_val)
}

logfuns = list(logp = logp, logl = logl)

num_par = 2

#It is recommended to use at least twice as many chains as the number of
#parameters to be estimated.
num_chains = 2*num_par

#Generate initial guesses for the MCMC chains
theta0 = matrix(0, nrow = num_par, ncol = num_chains)

temp_val = 0
j = 0

while(j &lt; num_chains)
{
 initial = c(runif(1, 1e-2, 1e2), runif(1, 1, 1e4))
 temp_val = logl(initial) + logp(initial)

 while(is.na(temp_val) || is.infinite(temp_val))
 {
   initial = c(runif(1, 1e-2, 1e2), runif(1, 1, 1e4))
   temp_val = logl(initial) + logp(initial)
 }

 j = j + 1

 message(paste('j:', j))

 theta0[1,j] = initial[1]
 theta0[2,j] = initial[2]

}

num_chain_iterations = 1e4
thin_val_par = 10

#The total number of returned samples is given by
#(num_chain_iterations/thin_val_par)*num_chains = 4e3

#Ensemble Quadratic MCMC algorithm

Weibull_Quad_result = ensemblealg(theta0, logfuns,
T_iter = num_chain_iterations, Thin_val = thin_val_par)

my_samples = Weibull_Quad_result$theta_sample

my_log_prior = Weibull_Quad_result$log_prior_sample

my_log_like = Weibull_Quad_result$log_like_sample

#Burn-in 25% of each chain
my_samples_burn_in = my_samples[,,-c(1:floor((num_chain_iterations/thin_val_par)*0.25))]

my_log_prior_burn_in = my_log_prior[,-c(1:floor((num_chain_iterations/thin_val_par)*0.25))]

my_log_like_burn_in = my_log_like[,-c(1:floor((num_chain_iterations/thin_val_par)*0.25))]

#Calculate potential scale reduction factors
diagnostic_result = psrfdiagnostic(my_samples_burn_in, 0.05)

diagnostic_result$p_s_r_f_vec

#log unnormalized posterior samples
log_un_post_vec = as.vector(my_log_prior_burn_in + my_log_like_burn_in)

#a_shape posterior samples
k1 = as.vector(my_samples_burn_in[1,,])

#sigma_scale posterior samples
k2 = as.vector(my_samples_burn_in[2,,])

#Calculate posterior median, 95% credible intervals, and maximum posterior for
#the parameters
median(k1)
hpdparameter(k1, 0.05)

median(k2)
hpdparameter(k2, 0.05)

k1[which.max(log_un_post_vec)]

k2[which.max(log_un_post_vec)]

#These plots display the silhouette of the unnormalized posterior surface from
#the chosen parameter's perspective

plot(k1, exp(log_un_post_vec), xlab="a_shape", ylab="unnormalized posterior density")

plot(k2, exp(log_un_post_vec), xlab="sigma_scale", ylab="unnormalized posterior density")
</code></pre>

<hr>
<h2 id='hpdparameter'>Highest Posterior Density (HPD) for a parameter</h2><span id='topic+hpdparameter'></span>

<h3>Description</h3>

<p>This function returns the upper and lower bound of the Highest
Posterior Density (HPD) for a parameter based on the Chen-Shao Highest
Posterior Density (HPD) Estimation Algorithm found in the book by Chen, Shao,
and Ibrahim (2000). (The smallest 95% credible interval will be given by the
HPD using alpha = 0.05)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hpdparameter(parameter_MCMC, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hpdparameter_+3A_parameter_mcmc">parameter_MCMC</code></td>
<td>
<p>a vector of the parameter samples for a single
estimated parameter</p>
</td></tr>
<tr><td><code id="hpdparameter_+3A_alpha">alpha</code></td>
<td>
<p>100(1 - alpha)% credible interval with the default value as
alpha = 0.05</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector is returned that contains the lower and upper bound of the
Highest Posterior Density (HPD) for a parameter (this will be the smallest
95% credible interval using alpha = 0.05)
</p>


<h3>References</h3>

<p>Chen M, Shao Q, Ibrahim JG (2000) Monte Carlo Methods in Bayesian
Computation. New York-New York: Springer-Verlag.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x_parameter = rnorm(75, mean = 0, sd = 1)

hpdparameter(x_parameter, 0.05)
</code></pre>

<hr>
<h2 id='psrfdiagnostic'>Potential Scale Reduction Factor computation</h2><span id='topic+psrfdiagnostic'></span>

<h3>Description</h3>

<p>This function computes the potential scale reduction factor for
each parameter to formally test the convergence of the MCMC sampling to the
estimated posterior distribution which was developed by Gelman
and Brooks (1998). This potential scale reduction factor is based on empirical
interval lengths with the following formula:
<code class="reqn">\hat{R} = \frac{S}{\sum_{i=1}^{K} \frac{s_i}{K}}</code>, where <code class="reqn">S</code> is the
distance between the upper and lower values of the <code class="reqn">100 (1 - \alpha)\%</code>
interval for the pooled samples, <code class="reqn">s_i</code> is the distance between the upper
and lower values of the <code class="reqn">100 (1 - \alpha)\%</code> interval for the
<code class="reqn">i^{\textrm{th}}</code> chain, and <code class="reqn">K</code> is the total number of chains used.
When the potential scale reduction factor is close to 1 for all the estimated
parameters, this indicates that the MCMC sampling converged to the estimated
posterior distribution for each parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psrfdiagnostic(my_samples_burn_in, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="psrfdiagnostic_+3A_my_samples_burn_in">my_samples_burn_in</code></td>
<td>
<p>This parameter is a matrix of parameter samples
returned from the Ensemble MCMC algorithm 'ensemblealg', the matrix
dimensions are given by
(Number of parameters) x (Number of chains) x (Number of iterations - Number
of burn in iterations).
It is recommended to burn-in the parameter samples from the starting
iterations before running the 'psrfdiagnostic' to assess the convergence.</p>
</td></tr>
<tr><td><code id="psrfdiagnostic_+3A_alpha">alpha</code></td>
<td>
<p>the alpha value here corresponds to the 100(1 - alpha)% credible
intervals to be estimated, with the default value as alpha = 0.05</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object is returned that contains two vectors and one matrix:
p_s_r_f_vec, L_vec, and d_matrix.<br />
</p>
<p>p_s_r_f_vec: this is the vector of potential scale reduction factors in order
of the parameters <br />
</p>
<p>L_vec: this is the vector of distances between the upper and lower values of
the 95% interval for the pooled samples and these distances are in order of
the parameters <br />
</p>
<p>d_matrix: this is the matrix of distances between the upper and lower values
of the 95% interval for the samples in each of the chains, the matrix
dimensions are given by
(Number of parameters) x (Number of chains)
</p>


<h3>References</h3>

<p>Brooks SP and Gelman A (1998) General methods for monitoring
convergence of iterative simulations. J Comp Graph Stat 7(4):434-455.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Take 100 random samples from a multivariate normal distribution
#with mean c(1, 2) and covariance matrix matrix(c(1, 0.75, 0.75, 1), nrow = 2, ncol = 2)
#for each of four chains.

my_samples_example = array(0, dim=c(2, 4, 100))

for(j in 1:4)
{
  for(i in 1:100)
  {
    my_samples_example[,j,i] = solve(matrix(c(1, 0.75, 0.75, 1), nrow = 2, ncol = 2))%*%
    rnorm(2, mean = 0, sd = 1) +  matrix(c(1, 2), nrow = 2, ncol = 1, byrow = TRUE)
  }
}

#The potential scale reduction factors for each parameter are close to 1
psrfdiagnostic(my_samples_example)$p_s_r_f_vec
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
