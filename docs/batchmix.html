<!DOCTYPE html><html><head><title>Help for package batchmix</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {batchmix}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#batchmix-package'><p>Bayesian Mixture Modelling for Joint Model-Based</p>
Clustering/Classification and Batch Correction</a></li>
<li><a href='#batchSemiSupervisedMixtureModel'><p>Batch semisupervised mixture model</p></a></li>
<li><a href='#calcAllocProb'><p>Calculate allocation probabilities</p></a></li>
<li><a href='#checkDataGenerationInputs'><p>Check data generation inputs</p></a></li>
<li><a href='#checkProposalWindows'><p>Check proposal windows</p></a></li>
<li><a href='#collectAcceptanceRates'><p>Collect acceptance rate</p></a></li>
<li><a href='#continueChain'><p>Continue chain</p></a></li>
<li><a href='#continueChains'><p>Continue chains</p></a></li>
<li><a href='#createSimilarityMat'><p>Create Similarity Matrix</p></a></li>
<li><a href='#gammaLogLikelihood'><p>Gamma log-likelihood</p></a></li>
<li><a href='#generateBatchData'><p>Generate batch data</p></a></li>
<li><a href='#generateBatchDataLogPoisson'><p>Generate batch data</p></a></li>
<li><a href='#generateBatchDataMVT'><p>Generate batch data from a multivariate t distribution</p></a></li>
<li><a href='#generateBatchDataVaryingRepresentation'><p>Generate batch data</p></a></li>
<li><a href='#generateGroupIDsInSimulator'><p>Generate group IDs</p></a></li>
<li><a href='#generateInitialLabels'><p>Generate initial labels</p></a></li>
<li><a href='#getLikelihood'><p>Get likelihood</p></a></li>
<li><a href='#getSampledBatchScale'><p>Get sampled batch shift</p></a></li>
<li><a href='#getSampledBatchShift'><p>Get sampled batch shift</p></a></li>
<li><a href='#getSampledClusterMeans'><p>Get sampled cluster means</p></a></li>
<li><a href='#invGammaLogLikelihood'><p>Inverse gamma log-likelihood</p></a></li>
<li><a href='#invWishartLogLikelihood'><p>Inverse-Wishart log-likelihood</p></a></li>
<li><a href='#minVI'><p>Minimium VI</p></a></li>
<li><a href='#plotAcceptanceRates'><p>Plot acceptance rates</p></a></li>
<li><a href='#plotLikelihoods'><p>Plot likelihoods</p></a></li>
<li><a href='#plotSampledBatchMeans'><p>Plot sampled batch means</p></a></li>
<li><a href='#plotSampledBatchScales'><p>Plot sampled batch scales</p></a></li>
<li><a href='#plotSampledClusterMeans'><p>Plot sampled cluster means</p></a></li>
<li><a href='#plotSampledParameter'><p>Plot sampled vector parameter</p></a></li>
<li><a href='#predictClass'><p>Predict class</p></a></li>
<li><a href='#predictFromMultipleChains'><p>Predict from multiple MCMC chains</p></a></li>
<li><a href='#prepareInitialParameters'><p>Prepare initial values</p></a></li>
<li><a href='#processMCMCChain'><p>Process MCMC chain</p></a></li>
<li><a href='#processMCMCChains'><p>Process MCMC chains</p></a></li>
<li><a href='#rStickBreakingPrior'><p>Random Draw From Stick Breaking Prior</p></a></li>
<li><a href='#runBatchMix'><p>Run Batch Mixture Model</p></a></li>
<li><a href='#runMCMCChains'><p>Run MCMC Chains</p></a></li>
<li><a href='#sampleMVN'><p>Sample mixture of multivariate normal distributions with batch effects</p></a></li>
<li><a href='#sampleMVT'><p>Sample mixture of multivariate t-distributions with batch effects</p></a></li>
<li><a href='#samplePriorLabels'><p>Sample prior labels</p></a></li>
<li><a href='#sampleSemisupervisedMVN'><p>Sample semi-supervised MVN Mixture model</p></a></li>
<li><a href='#sampleSemisupervisedMVT'><p>Sample semi-supervised MVT Mixture model</p></a></li>
<li><a href='#VI.lb'><p>Minimum VI lower bound</p></a></li>
<li><a href='#wishartLogLikelihood'><p>Wishart log-likelihood</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Semi-Supervised Bayesian Mixture Models Incorporating Batch
Correction</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Semi-supervised and unsupervised Bayesian mixture models that
  simultaneously infer the cluster/class structure and a batch correction.
  Densities available are the multivariate normal and the multivariate t.
  The model sampler is implemented in C++. This package is aimed at analysis of
  low-dimensional data generated across several batches. See Coleman et al.
  (2022) &lt;<a href="https://doi.org/10.1101%2F2022.01.14.476352">doi:10.1101/2022.01.14.476352</a>&gt; for details of the model.</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.5), tidyr, ggplot2</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, testthat</td>
</tr>
<tr>
<td>Suggests:</td>
<td>xml2, knitr, rmarkdown</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/stcolema/batchmix">https://github.com/stcolema/batchmix</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/stcolema/batchmix/issues">https://github.com/stcolema/batchmix/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-25 01:29:54 UTC; colemste</td>
</tr>
<tr>
<td>Author:</td>
<td>Stephen Coleman [aut, cre],
  Paul Kirk [aut],
  Chris Wallace [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Stephen Coleman &lt;stcolema@tcd.ie&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-25 03:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='batchmix-package'>Bayesian Mixture Modelling for Joint Model-Based
Clustering/Classification and Batch Correction</h2><span id='topic+batchmix'></span><span id='topic+batchmix-package'></span><span id='topic+batchmix+2C'></span><span id='topic+BatchMixtureModel'></span>

<h3>Description</h3>

<p>Semi-supervised and unsupervised Bayesian mixture models that
simultaneously infer the cluster/class structure and a batch correction.
Densities available are the multivariate normal and the multivariate t.
The model sampler is implemented in C++. This package is aimed at analysis of
low-dimensional data generated across several batches. See
(Coleman et al. (2022))[https://doi.org/10.1101/2022.01.14.476352] for
details of the model.
</p>


<h3>Author(s)</h3>

<p>Stephen Coleman &lt;stcolema@tcd.ie&gt;, Paul D.W. Kirk, Chris Wallace
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/stcolema/batchmix">https://github.com/stcolema/batchmix</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/stcolema/batchmix/issues">https://github.com/stcolema/batchmix/issues</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

# Which labels are observed
fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Sampling parameters
R &lt;- 1000
thin &lt;- 50

# Classification
samples &lt;- runBatchMix(X,
  R,
  thin,
  batch_vec,
  "MVN",
  initial_labels = labels,
  fixed = fixed,
)

# Clustering
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVT")

</code></pre>

<hr>
<h2 id='batchSemiSupervisedMixtureModel'>Batch semisupervised mixture model</h2><span id='topic+batchSemiSupervisedMixtureModel'></span>

<h3>Description</h3>

<p>A Bayesian mixture model with batch effects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>batchSemiSupervisedMixtureModel(
  X,
  R,
  thin,
  initial_labels,
  fixed,
  batch_vec,
  type,
  K_max = length(unique(initial_labels)),
  alpha = NULL,
  concentration = NULL,
  mu_proposal_window = 0.5^2,
  cov_proposal_window = 0.002,
  m_proposal_window = 0.3^2,
  S_proposal_window = 0.01,
  t_df_proposal_window = 0.015,
  m_scale = NULL,
  rho = 3,
  theta = 1,
  initial_class_means = NULL,
  initial_class_covariance = NULL,
  initial_batch_shift = NULL,
  initial_batch_scale = NULL,
  initial_class_df = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_x">X</code></td>
<td>
<p>Data to cluster as a matrix with the items to cluster held in rows.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_r">R</code></td>
<td>
<p>The number of iterations in the sampler.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_thin">thin</code></td>
<td>
<p>The factor by which the samples generated are thinned, e.g. if
&ldquo;thin=50&ldquo; only every 50th sample is kept.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_initial_labels">initial_labels</code></td>
<td>
<p>Initial clustering.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_fixed">fixed</code></td>
<td>
<p>Which items are fixed in their initial label.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_batch_vec">batch_vec</code></td>
<td>
<p>Labels identifying which batch each item being clustered is
from.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_type">type</code></td>
<td>
<p>Character indicating density type to use. One of 'MVN'
(multivariate normal distribution) or 'MVT' (multivariate t distribution).</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_k_max">K_max</code></td>
<td>
<p>The number of components to include (the upper bound on the
number of clusters in each sample). Defaults to the number of unique labels
in &ldquo;initial_labels&ldquo;.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_alpha">alpha</code></td>
<td>
<p>The concentration parameter for the stick-breaking prior and the
weights in the model.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_concentration">concentration</code></td>
<td>
<p>Initial concentration vector for component weights.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_mu_proposal_window">mu_proposal_window</code></td>
<td>
<p>The proposal window for the cluster mean proposal
kernel. The proposal density is a Gaussian distribution, the window is the
variance.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_cov_proposal_window">cov_proposal_window</code></td>
<td>
<p>The proposal window for the cluster covariance
proposal kernel. The proposal density is a Wishart distribution, this
argument is the reciprocal of the degree of freedom.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_m_proposal_window">m_proposal_window</code></td>
<td>
<p>The proposal window for the batch mean proposal
kernel. The proposal density is a Gaussian distribution, the window is the
variance.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_s_proposal_window">S_proposal_window</code></td>
<td>
<p>The proposal window for the batch standard deviation
proposal kernel. The proposal density is a Gamma distribution, this
argument is the reciprocal of the rate.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_t_df_proposal_window">t_df_proposal_window</code></td>
<td>
<p>The proposal window for the degrees of freedom
for the multivariate t distribution (not used if type is not 'MVT'). The
proposal density is a Gamma distribution, this argument is the reciprocal of
the rate.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_m_scale">m_scale</code></td>
<td>
<p>The scale hyperparameter for the batch shift prior
distribution. This defines the scale of the batch effect upon the mean and
should be in (0, 1].</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_rho">rho</code></td>
<td>
<p>The shape of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_theta">theta</code></td>
<td>
<p>The scale of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_initial_class_means">initial_class_means</code></td>
<td>
<p>A $P x K$ matrix of initial values for the class
means. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_initial_class_covariance">initial_class_covariance</code></td>
<td>
<p>A $P x P x K$ array of initial values for
the class covariance matrices. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_initial_batch_shift">initial_batch_shift</code></td>
<td>
<p>A $P x B$ matrix of initial values for the batch
shift effect Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_initial_batch_scale">initial_batch_scale</code></td>
<td>
<p>A $P x B$ matrix of initial values for the batch
scales Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_initial_class_df">initial_class_df</code></td>
<td>
<p>A $K$ vector of initial values for the class degrees
of freedom. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="batchSemiSupervisedMixtureModel_+3A_verbose">verbose</code></td>
<td>
<p>Logiccal indicating if warning about proposal windows should
be printed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing the sampled partitions, cluster and batch
parameters, model fit measures and some details on the model call.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Density choice
type &lt;- "MVN"

# Sampling parameters
R &lt;- 1000
thin &lt;- 50

# MCMC samples and BIC vector
samples &lt;- batchSemiSupervisedMixtureModel(
  X,
  R,
  thin,
  labels,
  fixed,
  batch_vec,
  type
)

# Given an initial value for the parameters
initial_class_means &lt;- matrix(c(1, 1, 3, 4), nrow = 2)
initial_class_covariance &lt;- array(c(1, 0, 0, 1, 1, 0, 0, 1),
  dim = c(2, 2, 2)
)

# We can use values from a previous chain
initial_batch_shift &lt;- samples$batch_shift[, , R / thin]
initial_batch_scale &lt;- matrix(
  c(1.2, 1.3, 1.7, 1.1, 1.4, 1.3, 1.2, 1.2, 1.1, 2.0),
  nrow = 2
)

samples &lt;- batchSemiSupervisedMixtureModel(X,
  R,
  thin,
  labels,
  fixed,
  batch_vec,
  type,
  initial_class_means = initial_class_means,
  initial_class_covariance = initial_class_covariance,
  initial_batch_shift = initial_batch_shift,
  initial_batch_scale = initial_batch_scale
)

</code></pre>

<hr>
<h2 id='calcAllocProb'>Calculate allocation probabilities</h2><span id='topic+calcAllocProb'></span>

<h3>Description</h3>

<p>Calculate the empirical allocation probability for each class
based on the sampled allocation probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcAllocProb(mcmc_samples, burn = 0, method = "median")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcAllocProb_+3A_mcmc_samples">mcmc_samples</code></td>
<td>
<p>Output from &ldquo;batchSemiSupervisedMixtureModel&ldquo;.</p>
</td></tr>
<tr><td><code id="calcAllocProb_+3A_burn">burn</code></td>
<td>
<p>The number of samples to discard.</p>
</td></tr>
<tr><td><code id="calcAllocProb_+3A_method">method</code></td>
<td>
<p>The point estimate to use. &ldquo;method = 'mean'&ldquo; or
&ldquo;method = 'median'&ldquo;. &ldquo;'median'&ldquo; is the default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An N x K matrix of class probabilities.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Sampling parameters
R &lt;- 1000
thin &lt;- 50

# MCMC samples and BIC vector
samples &lt;- batchSemiSupervisedMixtureModel(X, R, thin, labels, fixed, batch_vec, "MVN")

# Burn in
burn &lt;- 20
eff_burn &lt;- burn / thin

# Probability across classes
probs &lt;- calcAllocProb(samples, burn = burn)

</code></pre>

<hr>
<h2 id='checkDataGenerationInputs'>Check data generation inputs</h2><span id='topic+checkDataGenerationInputs'></span>

<h3>Description</h3>

<p>Checks that the inputs for the &ldquo;generateBatchData&ldquo; function
are correct. For internal use only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkDataGenerationInputs(
  N,
  P,
  group_means,
  group_std_devs,
  batch_shift,
  batch_scale,
  group_weights,
  batch_weights,
  type,
  group_dfs,
  frac_known,
  permute_variables,
  scale_data
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkDataGenerationInputs_+3A_n">N</code></td>
<td>
<p>The number of items (rows) to generate.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_p">P</code></td>
<td>
<p>The number of columns in the generated dataset.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_group_means">group_means</code></td>
<td>
<p>A vector of the group means for a column.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_group_std_devs">group_std_devs</code></td>
<td>
<p>A vector of group standard deviations for a column.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_batch_shift">batch_shift</code></td>
<td>
<p>A vector of batch means in a column.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_batch_scale">batch_scale</code></td>
<td>
<p>A vector of batch standard deviations within a column.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_group_weights">group_weights</code></td>
<td>
<p>A K x B matrix of the expected proportion of N in each group in each batch.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_batch_weights">batch_weights</code></td>
<td>
<p>A vector of the expected proportion of N in each batch.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_type">type</code></td>
<td>
<p>A string indicating if data should be generated from
multivariate normal (&quot;MVN&quot;) or multivariate t (&quot;MVT&quot;) densities.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_group_dfs">group_dfs</code></td>
<td>
<p>A K-vector of the group specific degrees of freedom.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_frac_known">frac_known</code></td>
<td>
<p>The number of items with known labels.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_permute_variables">permute_variables</code></td>
<td>
<p>Logical indicating if group and batch means and
standard deviations should be permuted in each column or not.</p>
</td></tr>
<tr><td><code id="checkDataGenerationInputs_+3A_scale_data">scale_data</code></td>
<td>
<p>Logical indicating if data should be mean centred and
standardised.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 500
P &lt;- 2
K &lt;- 2
B &lt;- 5
mean_dist &lt;- 4
batch_dist &lt;- 0.3
group_means &lt;- seq(1, K) * mean_dist
batch_shift &lt;- rnorm(B, mean = batch_dist, sd = batch_dist)
group_std_devs &lt;- rep(2, K)
batch_scale &lt;- rep(1.2, B)
group_weights &lt;- rep(1 / K, K)
batch_weights &lt;- rep(1 / B, B)
type &lt;- "MVT"
group_dfs &lt;- c(4, 7)
frac_known &lt;- 0.3
permute_variables &lt;- TRUE
scale_data &lt;- FALSE

checkDataGenerationInputs(
  N,
  P,
  group_means,
  group_std_devs,
  batch_shift,
  batch_scale,
  group_weights,
  batch_weights,
  type,
  group_dfs,
  frac_known,
  permute_variables,
  scale_data
)
</code></pre>

<hr>
<h2 id='checkProposalWindows'>Check proposal windows</h2><span id='topic+checkProposalWindows'></span>

<h3>Description</h3>

<p>Checks the proposal windows are acceptable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkProposalWindows(
  mu_proposal_window,
  cov_proposal_window,
  m_proposal_window,
  S_proposal_window,
  t_df_proposal_window,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkProposalWindows_+3A_mu_proposal_window">mu_proposal_window</code></td>
<td>
<p>The proposal window for the cluster mean proposal
kernel. The proposal density is a Gaussian distribution, the window is the
variance.</p>
</td></tr>
<tr><td><code id="checkProposalWindows_+3A_cov_proposal_window">cov_proposal_window</code></td>
<td>
<p>The proposal window for the cluster covariance
proposal kernel. The proposal density is a Wishart distribution, this
argument is the reciprocal of the degree of freedom.</p>
</td></tr>
<tr><td><code id="checkProposalWindows_+3A_m_proposal_window">m_proposal_window</code></td>
<td>
<p>The proposal window for the batch mean proposal
kernel. The proposal density is a Gaussian distribution, the window is the
variance.</p>
</td></tr>
<tr><td><code id="checkProposalWindows_+3A_s_proposal_window">S_proposal_window</code></td>
<td>
<p>The proposal window for the batch standard deviation
proposal kernel. The proposal density is a Gamma distribution, this
argument is the reciprocal of the rate.</p>
</td></tr>
<tr><td><code id="checkProposalWindows_+3A_t_df_proposal_window">t_df_proposal_window</code></td>
<td>
<p>The proposal window for the degrees of freedom
for the multivariate t distribution (not used if type is not 'MVT'). The
proposal density is a Gamma distribution, this argument is the reciprocal of
the rate.</p>
</td></tr>
<tr><td><code id="checkProposalWindows_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating if a warning should be printed if proposal
windows are outside their expected scale.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>checkProposalWindows(0.1, 0.2, 0.3, 0.1, 0.4, 0.3)
</code></pre>

<hr>
<h2 id='collectAcceptanceRates'>Collect acceptance rate</h2><span id='topic+collectAcceptanceRates'></span>

<h3>Description</h3>

<p>Collects the acceptance rates for each parameter into a data.frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>collectAcceptanceRates(samples)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collectAcceptanceRates_+3A_samples">samples</code></td>
<td>
<p>The output of &ldquo;runBatchMix&ldquo;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A wide data.frame of all the sampled parameters and the iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Sampling parameters
R &lt;- 1000
thin &lt;- 50

# MCMC samples
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVN",
  initial_labels = labels,
  fixed = fixed
)

# Acceptance rates
collectAcceptanceRates(samples)

</code></pre>

<hr>
<h2 id='continueChain'>Continue chain</h2><span id='topic+continueChain'></span>

<h3>Description</h3>

<p>Continues sampling from a previous position for a given chain.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>continueChain(mcmc_output, X, fixed, batch_vec, R, keep_old_samples = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="continueChain_+3A_mcmc_output">mcmc_output</code></td>
<td>
<p>Chain to be continued.</p>
</td></tr>
<tr><td><code id="continueChain_+3A_x">X</code></td>
<td>
<p>Data to cluster as a matrix with the items to cluster held in rows.</p>
</td></tr>
<tr><td><code id="continueChain_+3A_fixed">fixed</code></td>
<td>
<p>The indicator vector for which labels are observed.</p>
</td></tr>
<tr><td><code id="continueChain_+3A_batch_vec">batch_vec</code></td>
<td>
<p>The vector of the batch labels for the data.</p>
</td></tr>
<tr><td><code id="continueChain_+3A_r">R</code></td>
<td>
<p>The number of iterations to run in this continuation (thinning
factor is the same as initial chain).</p>
</td></tr>
<tr><td><code id="continueChain_+3A_keep_old_samples">keep_old_samples</code></td>
<td>
<p>Logical indicating if the original samples should be
kept or only the new samples returned. Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing the sampled partitions, cluster and batch
parameters, model fit measures and some details on the model call.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Density choice
type &lt;- "MVT"

# Sampling parameters
R &lt;- 1000
thin &lt;- 50

# MCMC samples and BIC vector
mcmc_output &lt;- runBatchMix(
  X,
  R,
  thin,
  batch_vec,
  type,
  initial_labels = labels,
  fixed = fixed
)

# Given an initial value for the parameters
mcmc_output &lt;- continueChain(
  mcmc_output,
  X,
  fixed,
  batch_vec,
  R,
)

</code></pre>

<hr>
<h2 id='continueChains'>Continue chains</h2><span id='topic+continueChains'></span>

<h3>Description</h3>

<p>Continues sampling from a list of previous chains.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>continueChains(mcmc_output, X, fixed, batch_vec, R, keep_old_samples = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="continueChains_+3A_mcmc_output">mcmc_output</code></td>
<td>
<p>Chains to be continued.</p>
</td></tr>
<tr><td><code id="continueChains_+3A_x">X</code></td>
<td>
<p>Data to cluster as a matrix with the items to cluster held in rows.</p>
</td></tr>
<tr><td><code id="continueChains_+3A_fixed">fixed</code></td>
<td>
<p>The indicator vector for which labels are observed.</p>
</td></tr>
<tr><td><code id="continueChains_+3A_batch_vec">batch_vec</code></td>
<td>
<p>The vector of the batch labels for the data.</p>
</td></tr>
<tr><td><code id="continueChains_+3A_r">R</code></td>
<td>
<p>The number of iterations to run in this continuation (thinning
factor is the same as initial chain).</p>
</td></tr>
<tr><td><code id="continueChains_+3A_keep_old_samples">keep_old_samples</code></td>
<td>
<p>Logical indicating if the original samples should be
kept or only the new samples returned. Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing the sampled partitions, cluster and batch
parameters, model fit measures and some details on the model call.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Density choice
type &lt;- "MVT"

# Sampling parameters
R &lt;- 1000
thin &lt;- 50
n_chains &lt;- 4

# MCMC samples
mcmc_output &lt;- runMCMCChains(
  X,
  n_chains,
  R,
  thin,
  batch_vec,
  type,
  initial_labels = labels,
  fixed = fixed
)

# Given an initial value for the parameters
new_output &lt;- continueChains(
  mcmc_output,
  X,
  fixed,
  batch_vec,
  R,
  keep_old_samples = TRUE
)
</code></pre>

<hr>
<h2 id='createSimilarityMat'>Create Similarity Matrix</h2><span id='topic+createSimilarityMat'></span>

<h3>Description</h3>

<p>Constructs a similarity matrix of the pairwise coclustering 
rate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createSimilarityMat(allocations)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createSimilarityMat_+3A_allocations">allocations</code></td>
<td>
<p>Matrix of sampled partitions. Columns correspond to 
items/samples being clustered, each row is a sampled partition.//'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A symmetric n x n matrix (for n rows in cluster record) describing 
the fraction of iterations for which each pairwise combination of points are
assigned the same label.
</p>

<hr>
<h2 id='gammaLogLikelihood'>Gamma log-likelihood</h2><span id='topic+gammaLogLikelihood'></span>

<h3>Description</h3>

<p>Used in calculating model probability in Metropolis-Hastings 
algorithm when proposals are from the Gamma distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gammaLogLikelihood(x, shape, rate)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gammaLogLikelihood_+3A_x">x</code></td>
<td>
<p>- double; the value to calculate the unnormalised likelihood of.</p>
</td></tr>
<tr><td><code id="gammaLogLikelihood_+3A_shape">shape</code></td>
<td>
<p>- double; the shape of the Gamma distribution.</p>
</td></tr>
<tr><td><code id="gammaLogLikelihood_+3A_rate">rate</code></td>
<td>
<p>- double; the rate of the Gamma distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the unnormalised log-likelihood of x in a Gamma with parameters shape 
and rate.
</p>

<hr>
<h2 id='generateBatchData'>Generate batch data</h2><span id='topic+generateBatchData'></span>

<h3>Description</h3>

<p>Generate data from K multivaraite normal or multivariate t
distributions with additional noise from batches. Assumes independence across
columns. In each column the parameters are randomly permuted for both the
groups and batches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateBatchData(
  N,
  P,
  group_means,
  group_std_devs,
  batch_shift,
  batch_scale,
  group_weights,
  batch_weights,
  type = "MVN",
  group_dfs = NULL,
  frac_known = 0.2,
  permute_variables = TRUE,
  scale_data = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateBatchData_+3A_n">N</code></td>
<td>
<p>The number of items (rows) to generate.</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_p">P</code></td>
<td>
<p>The number of columns in the generated dataset.</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_group_means">group_means</code></td>
<td>
<p>A vector of the group means for a column.</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_group_std_devs">group_std_devs</code></td>
<td>
<p>A vector of group standard deviations for a column.</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_batch_shift">batch_shift</code></td>
<td>
<p>A vector of batch means in a column.</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_batch_scale">batch_scale</code></td>
<td>
<p>A vector of batch standard deviations within a column.</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_group_weights">group_weights</code></td>
<td>
<p>One of either a K x B matrix of the expected proportion
of each batch in each group or a K-vector of the expected proportion of the
entire dataset in each group.</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_batch_weights">batch_weights</code></td>
<td>
<p>A vector of the expected proportion of N in each batch.</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_type">type</code></td>
<td>
<p>A string indicating if data should be generated from
multivariate normal (&quot;MVN&quot;) or multivariate t (&quot;MVT&quot;) densities (defaults to
&quot;MVN&quot;).</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_group_dfs">group_dfs</code></td>
<td>
<p>A K-vector of the group specific degrees of freedom.</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_frac_known">frac_known</code></td>
<td>
<p>The number of items with known labels.</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_permute_variables">permute_variables</code></td>
<td>
<p>Logical indicating if group and batch means and
standard deviations should be permuted in each column or not (defaults to
&ldquo;TRUE&ldquo;).</p>
</td></tr>
<tr><td><code id="generateBatchData_+3A_scale_data">scale_data</code></td>
<td>
<p>Logical indicating if data should be mean centred and
standardised (defaults to &ldquo;FALSE&ldquo;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of 5 objects; the data generated from the groups with and
without batch effects, the label indicating the generating group, the
batch label and the vector indicating training versus test.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 500
P &lt;- 2
K &lt;- 2
B &lt;- 5
mean_dist &lt;- 4
batch_dist &lt;- 0.3
group_means &lt;- seq(1, K) * mean_dist
batch_shift &lt;- rnorm(B, mean = batch_dist, sd = batch_dist)
std_dev &lt;- rep(2, K)
batch_var &lt;- rep(1.2, B)
group_weights &lt;- rep(1 / K, K)
batch_weights &lt;- rep(1 / B, B)
dfs &lt;- c(4, 7)
my_data &lt;- generateBatchData(
  N,
  P,
  group_means,
  std_dev,
  batch_shift,
  batch_var,
  group_weights,
  batch_weights,
  type = "MVT",
  group_dfs = dfs
)
</code></pre>

<hr>
<h2 id='generateBatchDataLogPoisson'>Generate batch data</h2><span id='topic+generateBatchDataLogPoisson'></span>

<h3>Description</h3>

<p>Generate data from K multivaraite normal or multivariate t
distributions with additional noise from batches. Assumes independence across
columns. In each column the parameters are randomly permuted for both the
groups and batches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateBatchDataLogPoisson(
  N,
  P,
  group_rates,
  batch_rates,
  group_weights,
  batch_weights,
  frac_known = 0.2,
  permute_variables = TRUE,
  scale_data = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateBatchDataLogPoisson_+3A_n">N</code></td>
<td>
<p>The number of items (rows) to generate.</p>
</td></tr>
<tr><td><code id="generateBatchDataLogPoisson_+3A_p">P</code></td>
<td>
<p>The number of columns in the generated dataset.</p>
</td></tr>
<tr><td><code id="generateBatchDataLogPoisson_+3A_group_rates">group_rates</code></td>
<td>
<p>A vector of the group rates for the classes within a column.</p>
</td></tr>
<tr><td><code id="generateBatchDataLogPoisson_+3A_batch_rates">batch_rates</code></td>
<td>
<p>A vector of the batch rates for the classes within a column.
This is used to create a variable which has the sum of the appropriate batch
and class rate, it might be better interpreted as the batch effect on the
observed rate.</p>
</td></tr>
<tr><td><code id="generateBatchDataLogPoisson_+3A_group_weights">group_weights</code></td>
<td>
<p>One of either a K x B matrix of the expected proportion
of each batch in each group or a K-vector of the expected proportion of the
entire dataset in each group.</p>
</td></tr>
<tr><td><code id="generateBatchDataLogPoisson_+3A_batch_weights">batch_weights</code></td>
<td>
<p>A vector of the expected proportion of N in each batch.</p>
</td></tr>
<tr><td><code id="generateBatchDataLogPoisson_+3A_frac_known">frac_known</code></td>
<td>
<p>The number of items with known labels.</p>
</td></tr>
<tr><td><code id="generateBatchDataLogPoisson_+3A_permute_variables">permute_variables</code></td>
<td>
<p>Logical indicating if group and batch means and
standard deviations should be permuted in each column or not (defaults to
&ldquo;TRUE&ldquo;).</p>
</td></tr>
<tr><td><code id="generateBatchDataLogPoisson_+3A_scale_data">scale_data</code></td>
<td>
<p>Logical indicating if data should be mean centred and
standardised (defaults to &ldquo;FALSE&ldquo;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of 5 objects; the data generated from the groups with and
without batch effects, the label indicating the generating group, the
batch label and the vector indicating training versus test.
</p>

<hr>
<h2 id='generateBatchDataMVT'>Generate batch data from a multivariate t distribution</h2><span id='topic+generateBatchDataMVT'></span>

<h3>Description</h3>

<p>Generate data from K multivariate t distributions with
additional noise from batches. Assumes independence across columns. In each
column the parameters are randomly permuted for both the groups and batches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateBatchDataMVT(
  N,
  P,
  group_means,
  group_std_devs,
  batch_shift,
  batch_scale,
  group_weights,
  batch_weights,
  dfs,
  frac_known = 0.2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateBatchDataMVT_+3A_n">N</code></td>
<td>
<p>The number of items (rows) to generate.</p>
</td></tr>
<tr><td><code id="generateBatchDataMVT_+3A_p">P</code></td>
<td>
<p>The number of columns in the generated dataset.</p>
</td></tr>
<tr><td><code id="generateBatchDataMVT_+3A_group_means">group_means</code></td>
<td>
<p>A vector of the group means for a column.</p>
</td></tr>
<tr><td><code id="generateBatchDataMVT_+3A_group_std_devs">group_std_devs</code></td>
<td>
<p>A vector of group standard deviations for a column.</p>
</td></tr>
<tr><td><code id="generateBatchDataMVT_+3A_batch_shift">batch_shift</code></td>
<td>
<p>A vector of batch means in a column.</p>
</td></tr>
<tr><td><code id="generateBatchDataMVT_+3A_batch_scale">batch_scale</code></td>
<td>
<p>A vector of batch standard deviations within a column.</p>
</td></tr>
<tr><td><code id="generateBatchDataMVT_+3A_group_weights">group_weights</code></td>
<td>
<p>A K x B matrix of the expected proportion of N in each group in each batch.</p>
</td></tr>
<tr><td><code id="generateBatchDataMVT_+3A_batch_weights">batch_weights</code></td>
<td>
<p>A vector of the expected proportion of N in each batch.</p>
</td></tr>
<tr><td><code id="generateBatchDataMVT_+3A_dfs">dfs</code></td>
<td>
<p>A K-vector of the group specific degrees of freedom.</p>
</td></tr>
<tr><td><code id="generateBatchDataMVT_+3A_frac_known">frac_known</code></td>
<td>
<p>The number of items with known labels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of 5 objects; the data generated from the groups with and
without batch effects, the label indicating the generating group, the
batch label and the vector indicating training versus test.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 500
P &lt;- 2
K &lt;- 2
B &lt;- 5
mean_dist &lt;- 4
batch_dist &lt;- 0.3
group_means &lt;- seq(1, K) * mean_dist
batch_shift &lt;- rnorm(B, mean = batch_dist, sd = batch_dist)
std_dev &lt;- rep(2, K)
batch_var &lt;- rep(1.2, B)
group_weights &lt;- rep(1 / K, K)
batch_weights &lt;- rep(1 / B, B)
dfs &lt;- c(4, 7)
my_data &lt;- generateBatchDataMVT(
  N,
  P,
  group_means,
  std_dev,
  batch_shift,
  batch_var,
  group_weights,
  batch_weights,
  dfs
)
</code></pre>

<hr>
<h2 id='generateBatchDataVaryingRepresentation'>Generate batch data</h2><span id='topic+generateBatchDataVaryingRepresentation'></span>

<h3>Description</h3>

<p>Generate data from groups across batches. Assumes independence
across columns. In each column the parameters are randomly permuted for both
the groups and batches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateBatchDataVaryingRepresentation(
  N,
  P,
  group_means,
  group_std_dev,
  batch_shift,
  batch_scale,
  group_weights,
  batch_weights,
  frac_known = 0.2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateBatchDataVaryingRepresentation_+3A_n">N</code></td>
<td>
<p>The number of items (rows) to generate.</p>
</td></tr>
<tr><td><code id="generateBatchDataVaryingRepresentation_+3A_p">P</code></td>
<td>
<p>The number of columns in the generated dataset.</p>
</td></tr>
<tr><td><code id="generateBatchDataVaryingRepresentation_+3A_group_means">group_means</code></td>
<td>
<p>A vector of the group means for a column.</p>
</td></tr>
<tr><td><code id="generateBatchDataVaryingRepresentation_+3A_group_std_dev">group_std_dev</code></td>
<td>
<p>A vector of group standard deviations for a column.</p>
</td></tr>
<tr><td><code id="generateBatchDataVaryingRepresentation_+3A_batch_shift">batch_shift</code></td>
<td>
<p>A vector of batch means in a column.</p>
</td></tr>
<tr><td><code id="generateBatchDataVaryingRepresentation_+3A_batch_scale">batch_scale</code></td>
<td>
<p>A vector of batch standard deviations within a column.</p>
</td></tr>
<tr><td><code id="generateBatchDataVaryingRepresentation_+3A_group_weights">group_weights</code></td>
<td>
<p>A K x B matrix of the expected proportion of N in each group in each batch.</p>
</td></tr>
<tr><td><code id="generateBatchDataVaryingRepresentation_+3A_batch_weights">batch_weights</code></td>
<td>
<p>A vector of the expected proportion of N in each batch.</p>
</td></tr>
<tr><td><code id="generateBatchDataVaryingRepresentation_+3A_frac_known">frac_known</code></td>
<td>
<p>The expected fraction of observed labels. Used to generate
a &ldquo;fixed&ldquo; vector to feed into the &ldquo;batchSemiSupervisedMixtureModel&ldquo; function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of 4 objects; the data generated from the groups with and
without batch effects, the label indicating the generating group and the
batch label.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 500
P &lt;- 2
K &lt;- 2
B &lt;- 5
mean_dist &lt;- 4
batch_dist &lt;- 0.3
group_means &lt;- seq(1, K) * mean_dist
batch_shift &lt;- rnorm(B, mean = batch_dist, sd = batch_dist)
std_dev &lt;- rep(2, K)
batch_var &lt;- rep(1.2, B)
group_weights &lt;- matrix(
  c(
    0.8, 0.6, 0.4, 0.2, 0.2,
    0.2, 0.4, 0.6, 0.8, 0.8
  ),
  nrow = K, ncol = B, byrow = TRUE
)
batch_weights &lt;- rep(1 / B, B)

my_data &lt;- generateBatchDataVaryingRepresentation(
  N,
  P,
  group_means,
  std_dev,
  batch_shift,
  batch_var,
  group_weights,
  batch_weights
)
</code></pre>

<hr>
<h2 id='generateGroupIDsInSimulator'>Generate group IDs</h2><span id='topic+generateGroupIDsInSimulator'></span>

<h3>Description</h3>

<p>Generate group IDs within &ldquo;generateBatchData&ldquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateGroupIDsInSimulator(
  N,
  K,
  B,
  batch_IDs,
  group_weights,
  varying_group_within_batch
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateGroupIDsInSimulator_+3A_n">N</code></td>
<td>
<p>The number of items (rows) to generate.</p>
</td></tr>
<tr><td><code id="generateGroupIDsInSimulator_+3A_k">K</code></td>
<td>
<p>The number of groups to genetare.</p>
</td></tr>
<tr><td><code id="generateGroupIDsInSimulator_+3A_b">B</code></td>
<td>
<p>The number of batches present in &ldquo;batch_IDs&ldquo;.</p>
</td></tr>
<tr><td><code id="generateGroupIDsInSimulator_+3A_batch_ids">batch_IDs</code></td>
<td>
<p>The batch membership of each item.</p>
</td></tr>
<tr><td><code id="generateGroupIDsInSimulator_+3A_group_weights">group_weights</code></td>
<td>
<p>One of either a K x B matrix of the expected proportion
of each batch in each group or a K-vector of the expected proportion of the
entire dataset in each group.</p>
</td></tr>
<tr><td><code id="generateGroupIDsInSimulator_+3A_varying_group_within_batch">varying_group_within_batch</code></td>
<td>
<p>Flag indicating if the groups are vvarying
across batches.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A N-vector of group membership.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 500
K &lt;- 2
B &lt;- 5
group_weights &lt;- rep(1 / K, K)
batch_weights &lt;- rep(1 / B, B)
batch_IDs &lt;- sample(seq(1, B), N, replace = TRUE, prob = batch_weights)
varying_group_within_batch &lt;- FALSE
group_IDs &lt;- generateGroupIDsInSimulator(
  N,
  K,
  B,
  batch_IDs,
  group_weights,
  varying_group_within_batch
)
</code></pre>

<hr>
<h2 id='generateInitialLabels'>Generate initial labels</h2><span id='topic+generateInitialLabels'></span>

<h3>Description</h3>

<p>For simulated data, generates an initial labelling for sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateInitialLabels(alpha, K, fixed, labels = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateInitialLabels_+3A_alpha">alpha</code></td>
<td>
<p>The mass in the stick breaking prior</p>
</td></tr>
<tr><td><code id="generateInitialLabels_+3A_k">K</code></td>
<td>
<p>The number of classes available.</p>
</td></tr>
<tr><td><code id="generateInitialLabels_+3A_fixed">fixed</code></td>
<td>
<p>The vector of 0s and 1s indicating which labels are to be held
fixed.</p>
</td></tr>
<tr><td><code id="generateInitialLabels_+3A_labels">labels</code></td>
<td>
<p>The initial labelling. Defaults to NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An N vector of labels.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 500
P &lt;- 2
K &lt;- 2
B &lt;- 5
mean_dist &lt;- 4
batch_dist &lt;- 0.3
cluster_means &lt;- seq(1, K) * mean_dist
batch_shift &lt;- rnorm(B, mean = batch_dist, sd = batch_dist)
std_dev &lt;- rep(2, K)
batch_var &lt;- rep(1.2, B)
cluster_weights &lt;- rep(1 / K, K)
batch_weights &lt;- rep(1 / B, B)

my_data &lt;- generateBatchData(
  N,
  P,
  cluster_means,
  std_dev,
  batch_shift,
  batch_var,
  cluster_weights,
  batch_weights
)

initial_labels &lt;- generateInitialLabels(1, K, my_data$fixed)
</code></pre>

<hr>
<h2 id='getLikelihood'>Get likelihood</h2><span id='topic+getLikelihood'></span>

<h3>Description</h3>

<p>Extracts the model fit score from the mixture model output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getLikelihood(mcmc_output, choice = "complete_likelihood")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getLikelihood_+3A_mcmc_output">mcmc_output</code></td>
<td>
<p>The output from the mixture model.</p>
</td></tr>
<tr><td><code id="getLikelihood_+3A_choice">choice</code></td>
<td>
<p>The model fit score to use. Must be one of
&ldquo;'observed_likelihood'&ldquo;, &ldquo;'complete_likelihood'&ldquo; or &ldquo;'BIC'&ldquo;. Defaults
to &ldquo;'complete_likelihood'&ldquo;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame containing the model fit score of choice and the
iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Sampling parameters
R &lt;- 100
thin &lt;- 5

# MCMC samples and BIC vector
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVN")

lkl_df &lt;- getLikelihood(samples)

</code></pre>

<hr>
<h2 id='getSampledBatchScale'>Get sampled batch shift</h2><span id='topic+getSampledBatchScale'></span>

<h3>Description</h3>

<p>Given an array of sampled batch scales from the
&ldquo;mixtureModel&ldquo; function, acquire a tidy version ready for &ldquo;ggplot2&ldquo; use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSampledBatchScale(
  sampled_batch_scale,
  B = dim(sampled_batch_scale)[2],
  P = dim(sampled_batch_scale)[1],
  R = dim(sampled_batch_scale)[3],
  thin = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getSampledBatchScale_+3A_sampled_batch_scale">sampled_batch_scale</code></td>
<td>
<p>A 3D array of sampled batch mean shifts.</p>
</td></tr>
<tr><td><code id="getSampledBatchScale_+3A_b">B</code></td>
<td>
<p>The number of batches present. Defaults to the number of columns in
the batch mean matrix from the first sample.</p>
</td></tr>
<tr><td><code id="getSampledBatchScale_+3A_p">P</code></td>
<td>
<p>The dimension of the batch mean shifts. Defaults to the number of
rows in the batch mean matrix from the first sample.</p>
</td></tr>
<tr><td><code id="getSampledBatchScale_+3A_r">R</code></td>
<td>
<p>The number of iterations run. Defaults to the number of slices in
the sampled batch mean array.</p>
</td></tr>
<tr><td><code id="getSampledBatchScale_+3A_thin">thin</code></td>
<td>
<p>The thinning factor of the sampler. Defaults to 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame of three columns; the parameter, the sampled value and the iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Observed batches represented by integers
batch_vec &lt;- sample(seq(1, 5), size = 100, replace = TRUE)

# MCMC iterations (this is too low for real use)
R &lt;- 100
thin &lt;- 5

# MCMC samples
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVN")

batch_scale_df &lt;- getSampledBatchShift(samples$batch_scale, R = R, thin = thin)

</code></pre>

<hr>
<h2 id='getSampledBatchShift'>Get sampled batch shift</h2><span id='topic+getSampledBatchShift'></span>

<h3>Description</h3>

<p>Given an array of sampled batch mean shifts from the
&ldquo;mixtureModel&ldquo; function, acquire a tidy version ready for &ldquo;ggplot2&ldquo; use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSampledBatchShift(
  sampled_batch_shift,
  B = dim(sampled_batch_shift)[2],
  P = dim(sampled_batch_shift)[1],
  R = dim(sampled_batch_shift)[3],
  thin = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getSampledBatchShift_+3A_sampled_batch_shift">sampled_batch_shift</code></td>
<td>
<p>A 3D array of sampled batch mean shifts.</p>
</td></tr>
<tr><td><code id="getSampledBatchShift_+3A_b">B</code></td>
<td>
<p>The number of batches present. Defaults to the number of columns in
the batch mean matrix from the first sample.</p>
</td></tr>
<tr><td><code id="getSampledBatchShift_+3A_p">P</code></td>
<td>
<p>The dimension of the batch mean shifts. Defaults to the number of
rows in the batch mean matrix from the first sample.</p>
</td></tr>
<tr><td><code id="getSampledBatchShift_+3A_r">R</code></td>
<td>
<p>The number of iterations run. Defaults to the number of slices in
the sampled batch mean array.</p>
</td></tr>
<tr><td><code id="getSampledBatchShift_+3A_thin">thin</code></td>
<td>
<p>The thinning factor of the sampler. Defaults to 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame of three columns; the parameter, the sampled value and the iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Observed batches represented by integers
batch_vec &lt;- sample(seq(1, 5), size = 100, replace = TRUE)

# MCMC iterations (this is too low for real use)
R &lt;- 100
thin &lt;- 5

# MCMC samples
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVN")

batch_shift_df &lt;- getSampledBatchShift(samples$batch_shift,
  R = R,
  thin = thin
)

</code></pre>

<hr>
<h2 id='getSampledClusterMeans'>Get sampled cluster means</h2><span id='topic+getSampledClusterMeans'></span>

<h3>Description</h3>

<p>Given an array of sampled cluster means from the
&ldquo;mixtureModel&ldquo; function, acquire a tidy version ready for &ldquo;ggplot2&ldquo; use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSampledClusterMeans(
  sampled_cluster_means,
  K = dim(sampled_cluster_means)[2],
  P = dim(sampled_cluster_means)[1],
  R = dim(sampled_cluster_means)[3],
  thin = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getSampledClusterMeans_+3A_sampled_cluster_means">sampled_cluster_means</code></td>
<td>
<p>A 3D array of sampled cluster means.</p>
</td></tr>
<tr><td><code id="getSampledClusterMeans_+3A_k">K</code></td>
<td>
<p>The number of clusters present. Defaults to the number of columns in
the batch mean matrix from the first sample.</p>
</td></tr>
<tr><td><code id="getSampledClusterMeans_+3A_p">P</code></td>
<td>
<p>The dimension of the batch mean shifts. Defaults to the number of
rows in the batch mean matrix from the first sample.</p>
</td></tr>
<tr><td><code id="getSampledClusterMeans_+3A_r">R</code></td>
<td>
<p>The number of iterations run. Defaults to the number of slices in
the sampled batch mean array.</p>
</td></tr>
<tr><td><code id="getSampledClusterMeans_+3A_thin">thin</code></td>
<td>
<p>The thinning factor of the sampler. Defaults to 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame of three columns; the parameter, the sampled value and the iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Observed batches represented by integers
batch_vec &lt;- sample(seq(1, 5), size = 100, replace = TRUE)

# MCMC iterations (this is too low for real use)
R &lt;- 100
thin &lt;- 5

# MCMC samples
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVN")

batch_shift_df &lt;- getSampledClusterMeans(samples$means, R = R, thin = thin)

</code></pre>

<hr>
<h2 id='invGammaLogLikelihood'>Inverse gamma log-likelihood</h2><span id='topic+invGammaLogLikelihood'></span>

<h3>Description</h3>

<p>Used in calculating model probability in Metropolis-Hastings 
algorithm when proposals are from the inverse-Gamma distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>invGammaLogLikelihood(x, shape, scale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="invGammaLogLikelihood_+3A_x">x</code></td>
<td>
<p>- double; the value to calculate the likelihood of.</p>
</td></tr>
<tr><td><code id="invGammaLogLikelihood_+3A_shape">shape</code></td>
<td>
<p>- double; the shape of the inverse-Gamma distribution.</p>
</td></tr>
<tr><td><code id="invGammaLogLikelihood_+3A_scale">scale</code></td>
<td>
<p>- double; the scale of the inverse-Gamma distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the unnormalised log-likelihood of x in a inverse-Gamma with parameters 
shape and scale.
</p>

<hr>
<h2 id='invWishartLogLikelihood'>Inverse-Wishart log-likelihood</h2><span id='topic+invWishartLogLikelihood'></span>

<h3>Description</h3>

<p>Used in calculating model probability in Metropolis-Hastings 
algorithm when proposals are from the Wishart distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>invWishartLogLikelihood(X, Psi, nu, P)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="invWishartLogLikelihood_+3A_x">X</code></td>
<td>
<p>- matrix; the matrix to calculate the likelihood of.</p>
</td></tr>
<tr><td><code id="invWishartLogLikelihood_+3A_psi">Psi</code></td>
<td>
<p>- matrix; the scale of the inverse-Wishart distribution.</p>
</td></tr>
<tr><td><code id="invWishartLogLikelihood_+3A_nu">nu</code></td>
<td>
<p>- double; the degrees of freedom for the inverse-Wishart distribution.</p>
</td></tr>
<tr><td><code id="invWishartLogLikelihood_+3A_p">P</code></td>
<td>
<p>- unsigned integer; the dimension of X.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the unnormalised log-likelihood of X in a inverse-Wishart with parameters Psi 
and nu.
</p>

<hr>
<h2 id='minVI'>Minimium VI</h2><span id='topic+minVI'></span>

<h3>Description</h3>

<p>Local implementation of S. Wade's 'minVI' function from their
'mcclust.ext' package (available at 'sarawade/mcclust.ext' on github).
Reimplemented here to avoid dependency on a non-CRAN package and we have
dropped the 'greedy' method. Finds the optimal partition by minimising the
lower bound to the Variation of Information obtained from Jensen's inequality
where the expectation and log are reversed. For full details please see the
aforementioned package and Wade and Ghahramani, 2018, 'Bayesian Cluster
Analysis: Point Estimation and Credible Balls (with Discussion)'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minVI(psm, cls.draw = NULL, method = "avg", max.k = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="minVI_+3A_psm">psm</code></td>
<td>
<p>The posterior similarity matrix for a set of clustering MCMC
samples such as is returned by the 'createSimilarityMat' function.</p>
</td></tr>
<tr><td><code id="minVI_+3A_cls.draw">cls.draw</code></td>
<td>
<p>The set of clustering MCMC samples used to generate 'psm'.
Only required if &lsquo;method' is one of '&rsquo;draws'&lsquo; or '&rsquo;all''.</p>
</td></tr>
<tr><td><code id="minVI_+3A_method">method</code></td>
<td>
<p>String indicating which method is used to find the point
estimate clustering. Must be one of ''avg'&lsquo;, '&rsquo;comp'&lsquo;, '&rsquo;draws'&lsquo; or '&rsquo;all''.
Defaults to ''avg'&lsquo;. If '&rsquo;all'' is passed the three methods are all applied
to return different choices of point clustering.</p>
</td></tr>
<tr><td><code id="minVI_+3A_max.k">max.k</code></td>
<td>
<p>The maximum number of clusters to consider. Only used by the
''comp'&lsquo; and '&rsquo;avg'' methods. Defaults to one-quarter the number of data
points rounded up.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If &lsquo;method' is '&rsquo;all'' returns a matrix of four clusterings, one for
each method and a repeat of that which performs best based on minimising the
Variation of Information between the clustering and the PSM. Otherwise
returns a vector. This is annotated with the attribute '&quot;info&quot;', a named list
describing:
</p>
<p>* '.$loss': the loss score used (Variation of Information)
</p>
<p>* &lsquo;.$maxNClusters': the 'max.k' value used by the '&rsquo;comp'&lsquo; and '&rsquo;avg'' methods
</p>
<p>* '.$expectedLoss': the estimated minimum Variation of Information for the point
clustering(s)
</p>
<p>* '.$method': the point method used to infer the clustering(s)
</p>
<p>Names are due to legacy reasons - this function is replacing the
'salso::salso' function and name choices are to minimise workflow damage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# MCMC samples and BIC vector
mcmc_outputs &lt;- runMCMCChains(
  X,
  n_chains,
  R,
  thin,
  batch_vec,
  type
)

# Note that in this toy example we have not applied a burn in
psm &lt;- createSimilarityMat(mcmc_outputs[[1]]$samples)
cl_est &lt;- minVI(psm, mcmc_outputs[[1]]$samples)

## End(Not run)
</code></pre>

<hr>
<h2 id='plotAcceptanceRates'>Plot acceptance rates</h2><span id='topic+plotAcceptanceRates'></span>

<h3>Description</h3>

<p>Plot the acceptance rates for the parameters sampled in a
Metropolis-Hastings step. Aiming for acceptance rates in [0.2, 0.5] for the
class means, the batch effect on location and scale is a good rule of thumb.
The class covariance should be in [0.35, 0.8] based on the authors'
experience. The class degree of freedom appears to be prone to high
acceptance rates, but aim to keep this above 0.2 at a minimum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotAcceptanceRates(mcmc_lst)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotAcceptanceRates_+3A_mcmc_lst">mcmc_lst</code></td>
<td>
<p>The output of the &ldquo;runMCMCChains&ldquo; function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object of the boxplots of acceptance rates for each
parameter across chains.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Sampling parameters
R &lt;- 500
thin &lt;- 10
n_chains &lt;- 4

# MCMC samples and BIC vector
mcmc_lst &lt;- runMCMCChains(X, n_chains, R, thin, batch_vec, "MVN",
  initial_labels = labels,
  fixed = fixed
)

# Plot the acceptance rate of each parameter in the 4 chains
plotAcceptanceRates(mcmc_lst)

</code></pre>

<hr>
<h2 id='plotLikelihoods'>Plot likelihoods</h2><span id='topic+plotLikelihoods'></span>

<h3>Description</h3>

<p>Plots the model fit for multiple chains.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotLikelihoods(
  mcmc_outputs,
  choice = "complete_likelihood",
  colour_by_chain = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotLikelihoods_+3A_mcmc_outputs">mcmc_outputs</code></td>
<td>
<p>The output from &ldquo;runMCMCChains&ldquo;.</p>
</td></tr>
<tr><td><code id="plotLikelihoods_+3A_choice">choice</code></td>
<td>
<p>The model fit score to use. Must be one of
&ldquo;'observed_likelihood'&ldquo;, &ldquo;'complete_likelihood'&ldquo; or &ldquo;'BIC'&ldquo;. Defaults
to &ldquo;'complete_likelihood'&ldquo;.</p>
</td></tr>
<tr><td><code id="plotLikelihoods_+3A_colour_by_chain">colour_by_chain</code></td>
<td>
<p>Logical indcating if plots should be coloured by chain
or all the same colour. Defaults to &ldquo;TRUE&ldquo;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 object. Line plot of likelihood across iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Sampling parameters
R &lt;- 1000
thin &lt;- 50
n_chains &lt;- 4

# MCMC samples
samples &lt;- runMCMCChains(X, n_chains, R, thin, batch_vec, "MVN",
  initial_labels = labels,
  fixed = fixed
)

p &lt;- plotLikelihoods(samples)

</code></pre>

<hr>
<h2 id='plotSampledBatchMeans'>Plot sampled batch means</h2><span id='topic+plotSampledBatchMeans'></span>

<h3>Description</h3>

<p>Plot the sampled values for the batch mean shifts in each
dimension from the output of the mixture model functions. Not recommended
for large B or P.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSampledBatchMeans(samples, burn_in = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotSampledBatchMeans_+3A_samples">samples</code></td>
<td>
<p>The output of the &ldquo;batchUnsupervisedMixtureModel&ldquo; or
&ldquo;batchSemiSupervisedMixtureModel&ldquo; functions.</p>
</td></tr>
<tr><td><code id="plotSampledBatchMeans_+3A_burn_in">burn_in</code></td>
<td>
<p>The samples at the beginning of the chain to drop. Defaults to 0.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object of the values in each sampled batch mean per iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Observed batches represented by integers
batch_vec &lt;- sample(seq(1, 5), size = 100, replace = TRUE)

# MCMC iterations (this is too low for real use)
R &lt;- 100
thin &lt;- 5

# MCMC samples and BIC vector
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVN")

# Plot the sampled value of the batch mean shift against MCMC iteration
plotSampledBatchMeans(samples)

</code></pre>

<hr>
<h2 id='plotSampledBatchScales'>Plot sampled batch scales</h2><span id='topic+plotSampledBatchScales'></span>

<h3>Description</h3>

<p>Plot the sampled values for the batch scale in each
dimension from the output of the mixture model functions. Not recommended
for large B or P.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSampledBatchScales(samples, burn_in = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotSampledBatchScales_+3A_samples">samples</code></td>
<td>
<p>The output of the &ldquo;batchUnsupervisedMixtureModel&ldquo; or
&ldquo;batchSemiSupervisedMixtureModel&ldquo; functions.</p>
</td></tr>
<tr><td><code id="plotSampledBatchScales_+3A_burn_in">burn_in</code></td>
<td>
<p>The samples at the beginning of the chain to drop. Defaults to 0.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object of the values in each sampled batch mean per iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Data in matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Observed batches represented by integers
batch_vec &lt;- sample(seq(1, 5), size = 100, replace = TRUE)

# MCMC iterations (this is too low for real use)
R &lt;- 100
thin &lt;- 5

# MCMC samples and BIC vector
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVN")

# Plot the sampled value of the batch scales against MCMC iteration
plotSampledBatchScales(samples)
</code></pre>

<hr>
<h2 id='plotSampledClusterMeans'>Plot sampled cluster means</h2><span id='topic+plotSampledClusterMeans'></span>

<h3>Description</h3>

<p>Plot the sampled values for the cluster means in each
dimension from the output of the mixture model functions. Not recommended
for large K or P.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSampledClusterMeans(samples, burn_in = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotSampledClusterMeans_+3A_samples">samples</code></td>
<td>
<p>The output of the &ldquo;batchUnsupervisedMixtureModel&ldquo; or
&ldquo;batchSemiSupervisedMixtureModel&ldquo; functions.</p>
</td></tr>
<tr><td><code id="plotSampledClusterMeans_+3A_burn_in">burn_in</code></td>
<td>
<p>The samples at the beginning of the chain to drop. Defaults to 0.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object of the values in each sampled cluster mean per iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Observed batches represented by integers
batch_vec &lt;- sample(seq(1, 5), size = 100, replace = TRUE)

# MCMC iterations (this is too low for real use)
R &lt;- 100
thin &lt;- 5

# MCMC samples and BIC vector
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVN")

# Plot the sampled value of the cluster means against MCMC iteration
plotSampledClusterMeans(samples)

</code></pre>

<hr>
<h2 id='plotSampledParameter'>Plot sampled vector parameter</h2><span id='topic+plotSampledParameter'></span>

<h3>Description</h3>

<p>Plot the sampled values for a sampled vector from the output of
the &ldquo;mixtureModel&ldquo; function. Not recommended for large B or P.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSampledParameter(samples, parameter, R = NULL, thin = 1, burn_in = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotSampledParameter_+3A_samples">samples</code></td>
<td>
<p>The output of the &ldquo;mixtureModel&ldquo; function.</p>
</td></tr>
<tr><td><code id="plotSampledParameter_+3A_parameter">parameter</code></td>
<td>
<p>The name of the parameter to be plotted (a string).</p>
</td></tr>
<tr><td><code id="plotSampledParameter_+3A_r">R</code></td>
<td>
<p>The number of iterations run. Defaults to the number of samples for
the cluster membership.</p>
</td></tr>
<tr><td><code id="plotSampledParameter_+3A_thin">thin</code></td>
<td>
<p>The thinning factor of the sampler. Defaults to 1.</p>
</td></tr>
<tr><td><code id="plotSampledParameter_+3A_burn_in">burn_in</code></td>
<td>
<p>The samples at the beginning of the chain to drop. Defaults to 0.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object of the values in each sampled batch mean per iteration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Data in matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Observed batches represented by integers
batch_vec &lt;- sample(seq(1, 3), size = 100, replace = TRUE)

# MCMC iterations (this is too low for real use)
R &lt;- 50
thin &lt;- 1

# MCMC samples and BIC vector
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVN", K_max = 8)

# Plot the sampled value of the cluster means against MCMC iteration
parameter &lt;- "means"
plotSampledParameter(samples, parameter, R, thin)
</code></pre>

<hr>
<h2 id='predictClass'>Predict class</h2><span id='topic+predictClass'></span>

<h3>Description</h3>

<p>Predicts a final class for each item given a matrix of
allocation probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictClass(prob)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predictClass_+3A_prob">prob</code></td>
<td>
<p>Output from the &ldquo;calcAllocProb&ldquo; function, a N x K matrix of
allocation probabilities.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An N vector of class allocations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Sampling parameters
R &lt;- 1000
thin &lt;- 50

# MCMC samples and BIC vector
samples &lt;- batchSemiSupervisedMixtureModel(
  X,
  R,
  thin,
  labels,
  fixed,
  batch_vec,
  "MVN"
)

# Burn in
burn &lt;- 200
eff_burn &lt;- burn / thin

# Probability across classes
probs &lt;- calcAllocProb(samples, burn = burn)

# Predict the class
preds &lt;- predictClass(probs)

</code></pre>

<hr>
<h2 id='predictFromMultipleChains'>Predict from multiple MCMC chains</h2><span id='topic+predictFromMultipleChains'></span>

<h3>Description</h3>

<p>Applies a burn in to and finds a point estimate by combining
multiple chains of &ldquo;callMDI&ldquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictFromMultipleChains(
  mcmc_outputs,
  burn,
  point_estimate_method = "median",
  chains_already_processed = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predictFromMultipleChains_+3A_mcmc_outputs">mcmc_outputs</code></td>
<td>
<p>Output from &ldquo;runMCMCChains&ldquo;</p>
</td></tr>
<tr><td><code id="predictFromMultipleChains_+3A_burn">burn</code></td>
<td>
<p>The number of MCMC samples to drop as part of a burn in.</p>
</td></tr>
<tr><td><code id="predictFromMultipleChains_+3A_point_estimate_method">point_estimate_method</code></td>
<td>
<p>Summary statistic used to define the point
estimate. Must be &ldquo;'mean'&ldquo; or &ldquo;'median'&ldquo;. &ldquo;'median'&ldquo; is the default.</p>
</td></tr>
<tr><td><code id="predictFromMultipleChains_+3A_chains_already_processed">chains_already_processed</code></td>
<td>
<p>Logical indicating if the chains have already
had a burn-in applied.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of quantities related to prediction/clustering:
</p>
<p>* &ldquo;allocation_probability&ldquo;: List with an $(N x K)$ matrix if the model is
semi-supervised. The point estimate of the allocation probabilities for
each data point to each class.
</p>
<p>* &ldquo;prob&ldquo;: $N$ vector of the point estimate of the probability of being
allocated to the class with the highest probability.
</p>
<p>* &ldquo;pred&ldquo;: $N$ vector of the predicted class for each sample. If the model
is unsupervised then the &ldquo;salso&ldquo; function from Dahl et al. (2021) is
used on the sampled partitions using the default settings.
</p>
<p>* &ldquo;samples&ldquo;: List of sampled allocations for each view. Columns
correspond to items being clustered, rows to MCMC samples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data dimensions
N &lt;- 600
P &lt;- 4
K &lt;- 5
B &lt;- 7

# Generating model parameters
mean_dist &lt;- 2.25
batch_dist &lt;- 0.3
group_means &lt;- seq(1, K) * mean_dist
batch_shift &lt;- rnorm(B, mean = batch_dist, sd = batch_dist)
std_dev &lt;- rep(2, K)
batch_var &lt;- rep(1.2, B)
group_weights &lt;- rep(1 / K, K)
batch_weights &lt;- rep(1 / B, B)
dfs &lt;- c(4, 7, 15, 60, 120)

my_data &lt;- generateBatchData(
  N,
  P,
  group_means,
  std_dev,
  batch_shift,
  batch_var,
  group_weights,
  batch_weights,
  type = "MVT",
  group_dfs = dfs
)


X &lt;- my_data$observed_data

true_labels &lt;- my_data$group_IDs
fixed &lt;- my_data$fixed
batch_vec &lt;- my_data$batch_IDs

alpha &lt;- 1
initial_labels &lt;- generateInitialLabels(alpha, K, fixed, true_labels)

# Sampling parameters
R &lt;- 1000
thin &lt;- 25
burn &lt;- 100
n_chains &lt;- 2

# Density choice
type &lt;- "MVT"

# MCMC samples and BIC vector
mcmc_outputs &lt;- runMCMCChains(
  X,
  n_chains,
  R,
  thin,
  batch_vec,
  type,
  initial_labels = initial_labels,
  fixed = fixed
)
ensemble_mod &lt;- predictFromMultipleChains(mcmc_outputs, burn)

</code></pre>

<hr>
<h2 id='prepareInitialParameters'>Prepare initial values</h2><span id='topic+prepareInitialParameters'></span>

<h3>Description</h3>

<p>Prepares user given values for input into the C++ function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepareInitialParameters(
  initial_class_means,
  initial_class_covariance,
  initial_batch_shift,
  initial_batch_scale,
  initial_class_df,
  P,
  K,
  B,
  type
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepareInitialParameters_+3A_initial_class_means">initial_class_means</code></td>
<td>
<p>A $P x K$ matrix of initial values for the class
means. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="prepareInitialParameters_+3A_initial_class_covariance">initial_class_covariance</code></td>
<td>
<p>A $P x P x K$ array of initial values for the
class covariance matrices. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="prepareInitialParameters_+3A_initial_batch_shift">initial_batch_shift</code></td>
<td>
<p>A $P x B$ matrix of initial values for the batch
shift effect Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="prepareInitialParameters_+3A_initial_batch_scale">initial_batch_scale</code></td>
<td>
<p>A $P x B$ matrix of initial values for the batch
scales Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="prepareInitialParameters_+3A_initial_class_df">initial_class_df</code></td>
<td>
<p>A $K$ vector of initial values for the class degrees
of freedom. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="prepareInitialParameters_+3A_p">P</code></td>
<td>
<p>Integer. The number of measurements for each sample in the dataset
being modelled.</p>
</td></tr>
<tr><td><code id="prepareInitialParameters_+3A_k">K</code></td>
<td>
<p>The number of classes/clusters being modelled.</p>
</td></tr>
<tr><td><code id="prepareInitialParameters_+3A_b">B</code></td>
<td>
<p>The number of batches being modelled.</p>
</td></tr>
<tr><td><code id="prepareInitialParameters_+3A_type">type</code></td>
<td>
<p>The type of mixture model used; one of &quot;MVN&quot; or &quot;MVT&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing the different parameters.
</p>

<hr>
<h2 id='processMCMCChain'>Process MCMC chain</h2><span id='topic+processMCMCChain'></span>

<h3>Description</h3>

<p>Applies a burn in to and finds a point estimate for the output
of &ldquo;batchSemiSupervisedMixtureModel&ldquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>processMCMCChain(mcmc_output, burn, point_estimate_method = "median")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="processMCMCChain_+3A_mcmc_output">mcmc_output</code></td>
<td>
<p>Output from &ldquo;batchSemiSupervisedMixtureModel&ldquo;</p>
</td></tr>
<tr><td><code id="processMCMCChain_+3A_burn">burn</code></td>
<td>
<p>The number of MCMC samples to drop as part of a burn in.</p>
</td></tr>
<tr><td><code id="processMCMCChain_+3A_point_estimate_method">point_estimate_method</code></td>
<td>
<p>Summary statistic used to define the point
estimate. Must be &ldquo;'mean'&ldquo; or &ldquo;'median'&ldquo;. &ldquo;'median'&ldquo; is the default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list similar to the output of
&ldquo;batchSemiSupervisedMixtureModel&ldquo; with some additional entries:
</p>
<p>* &ldquo;mean_est&ldquo;: $(P x K)$ matrix. The point estimate of the cluster
means with columns  corresponding to clusters.
</p>
<p>* &ldquo;cov_est&ldquo;: $(P x P x K)$ array. The point estimate of the
cluster covariance matrices with slices corresponding to clusters.
</p>
<p>* &ldquo;shift_est&ldquo;: $(P x B)$ matrix. The point estimate of the batch
shift effect with columns  corresponding to batches.
</p>
<p>* &ldquo;scale_est&ldquo;: $(P x B)$ matrix. The point estimate of the batch
scale effects. The $bth$ column contains the diagonal entries of the scaling
matrix for the $bth£ batch.
</p>
<p>* &ldquo;mean_sum_est&ldquo;: $(P x K x B)$ array. The point estimate of the
sum of the cluster  means and the batch shift effect with columns
corresponding to clusters and slices to batches.
</p>
<p>* &ldquo;cov_comb_est&ldquo;: List of length $B$, with each entry being a
$(P x P x K)$ array. The point estimate of the combination of the
cluster covariance matrices and the batch scale effect with list entries
corresponding to batches and slices of each array corresponding to clusters.
</p>
<p>* &ldquo;inferred_dataset&ldquo;: $(N x P)$ matrix. The inferred &ldquo;batch-free&rdquo;
dataset.
</p>
<p>* &ldquo;allocation_probability&ldquo;: $(N x K)$ matrix. The point estimate of
the allocation probabilities for each data point to each class.
</p>
<p>* &ldquo;prob&ldquo;: $N$ vector. The point estimate of the probability of being
allocated to the class with the highest probability.
</p>
<p>* &ldquo;pred&ldquo;: $N$ vector. The predicted class for each sample.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Sampling parameters
R &lt;- 1000
burn &lt;- 250
thin &lt;- 50

# MCMC samples
samples &lt;- runBatchMix(X, R, thin, batch_vec, "MVN",
  initial_labels = labels,
  fixed = fixed
)

# Process the MCMC samples
processed_samples &lt;- processMCMCChain(samples, burn)

</code></pre>

<hr>
<h2 id='processMCMCChains'>Process MCMC chains</h2><span id='topic+processMCMCChains'></span>

<h3>Description</h3>

<p>Applies a burn in to and finds a point estimate for each of the
chains outputted from &ldquo;runMCMCChains&ldquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>processMCMCChains(mcmc_lst, burn, point_estimate_method = "median")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="processMCMCChains_+3A_mcmc_lst">mcmc_lst</code></td>
<td>
<p>Output from &ldquo;runMCMCChains&ldquo;</p>
</td></tr>
<tr><td><code id="processMCMCChains_+3A_burn">burn</code></td>
<td>
<p>The number of MCMC samples to drop as part of a burn in.</p>
</td></tr>
<tr><td><code id="processMCMCChains_+3A_point_estimate_method">point_estimate_method</code></td>
<td>
<p>Summary statistic used to define the point
estimate. Must be &ldquo;'mean'&ldquo; or &ldquo;'median'&ldquo;. &ldquo;'median'&ldquo; is the default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list similar to the output of
&ldquo;batchSemiSupervisedMixtureModel&ldquo; with some additional entries:
</p>
<p>* &ldquo;mean_est&ldquo;:  $(P x K)$ matrix. The point estimate of the cluster
means with columns  corresponding to clusters.
</p>
<p>* &ldquo;cov_est&ldquo;:  $(P x P x K)$ array. The point estimate of the
cluster covariance matrices with slices corresponding to clusters.
</p>
<p>* &ldquo;shift_est&ldquo;:  $(P x B)$ matrix. The point estimate of the batch
shift effect with columns  corresponding to batches.
</p>
<p>* &ldquo;scale_est&ldquo;:  $(P x B)$ matrix. The point estimate of the batch
scale effects. The $bth$ column contains the diagonal entries of the scaling
matrix for the $bth£ batch.
</p>
<p>* &ldquo;mean_sum_est&ldquo;:  $(P x K x B)$ array. The point estimate of the
sum of the cluster  means and the batch shift effect with columns
corresponding to clusters and slices to batches.
</p>
<p>* &ldquo;cov_comb_est&ldquo;:  List of length $B$, with each entry being a
$(P x P x K)$ array. The point estimate of the combination of the
cluster covariance matrices and the batch scale effect with list entries
corresponding to batches and slices of each array corresponding to clusters.
</p>
<p>* &ldquo;inferred_dataset&ldquo;:  $(N x P)$ matrix. The inferred &ldquo;batch-free&rdquo;
dataset.
</p>
<p>* &ldquo;allocation_probability&ldquo;:  $(N x K)$ matrix. The point estimate of
the allocation probabilities for each data point to each class.
</p>
<p>* &ldquo;prob&ldquo;:  $N$ vector. The point estimate of the probability of being
allocated to the class with the highest probability.
</p>
<p>* &ldquo;pred&ldquo;:  $N$ vector. The predicted class for each sample.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Sampling parameters
R &lt;- 1000
burn &lt;- 250
thin &lt;- 50
n_chains &lt;- 4

# MCMC samples
samples &lt;- runMCMCChains(X, n_chains, R, thin, batch_vec, "MVN",
  initial_labels = labels,
  fixed = fixed
)

# Process the MCMC samples
processed_samples &lt;- processMCMCChains(samples, burn)

</code></pre>

<hr>
<h2 id='rStickBreakingPrior'>Random Draw From Stick Breaking Prior</h2><span id='topic+rStickBreakingPrior'></span>

<h3>Description</h3>

<p>Draw weights from the stick-breaking prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rStickBreakingPrior(alpha, K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rStickBreakingPrior_+3A_alpha">alpha</code></td>
<td>
<p>The concentration parameter.</p>
</td></tr>
<tr><td><code id="rStickBreakingPrior_+3A_k">K</code></td>
<td>
<p>The number of weights to generate.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of component weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>weights &lt;- rStickBreakingPrior(1, 50)
</code></pre>

<hr>
<h2 id='runBatchMix'>Run Batch Mixture Model</h2><span id='topic+runBatchMix'></span>

<h3>Description</h3>

<p>Runs a MCMC chain for a Bayesian mixture model which models
both batch effects and class/cluster structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runBatchMix(
  X,
  R,
  thin,
  batch_vec,
  type,
  K_max = NULL,
  initial_labels = NULL,
  fixed = NULL,
  alpha = 1,
  mu_proposal_window = 0.5^2,
  cov_proposal_window = 0.002,
  m_proposal_window = 0.3^2,
  S_proposal_window = 0.01,
  t_df_proposal_window = 0.015,
  m_scale = NULL,
  rho = 3,
  theta = 1,
  initial_class_means = NULL,
  initial_class_covariance = NULL,
  initial_batch_shift = NULL,
  initial_batch_scale = NULL,
  initial_class_df = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="runBatchMix_+3A_x">X</code></td>
<td>
<p>Data to cluster as a matrix with the items to cluster held in rows.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_r">R</code></td>
<td>
<p>The number of iterations in the sampler.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_thin">thin</code></td>
<td>
<p>The factor by which the samples generated are thinned, e.g. if
&ldquo;thin=50&ldquo; only every 50th sample is kept.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_batch_vec">batch_vec</code></td>
<td>
<p>Labels identifying which batch each item being clustered is
from.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_type">type</code></td>
<td>
<p>Character indicating density type to use. One of 'MVN'
(multivariate normal distribution) or 'MVT' (multivariate t distribution).</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_k_max">K_max</code></td>
<td>
<p>The number of components to include (the upper bound on the
number of clusters in each sample). Defaults to the number of unique labels
in &ldquo;initial_labels&ldquo;.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_initial_labels">initial_labels</code></td>
<td>
<p>Initial clustering.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_fixed">fixed</code></td>
<td>
<p>Which items are fixed in their initial label. If not given,
defaults to a vector of 0 meaning the model is run unsupervised.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_alpha">alpha</code></td>
<td>
<p>The concentration parameter for the stick-breaking prior and the
weights in the model.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_mu_proposal_window">mu_proposal_window</code></td>
<td>
<p>The proposal window for the cluster mean proposal
kernel. Making this smaller will normally increase the acceptance rate for
the proposed values in the Metropolis-Hastings sampler. The proposal density
is a Gaussian distribution, the window is the variance.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_cov_proposal_window">cov_proposal_window</code></td>
<td>
<p>The proposal window for the cluster covariance
proposal kernel. The proposal density is a Wishart distribution, this
argument is the reciprocal of the degree of freedom. It is recommended to
set this aiming for accpetance rates of greater than 0.5 for the covariance
matrices (e.g., between 2e-03 and 1e-04 is a good range to consider
initially). As the entire covariance matrix is sampled at once exploration is
difficult.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_m_proposal_window">m_proposal_window</code></td>
<td>
<p>The proposal window for the batch mean proposal
kernel. The proposal density is a Gaussian distribution, the window is the
variance.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_s_proposal_window">S_proposal_window</code></td>
<td>
<p>The proposal window for the batch standard deviation
proposal kernel. The proposal density is a Gamma distribution, this
argument is the reciprocal of the rate. Recommended range to initially
consider is 0.015 to 2e-03, though smaller values might be necessary
particularly in higher dimensional data.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_t_df_proposal_window">t_df_proposal_window</code></td>
<td>
<p>The proposal window for the degrees of freedom
for the multivariate t distribution (not used if type is not 'MVT'). The
proposal density is a Gamma distribution, this argument is the reciprocal of
the rate. If the data is more Gaussian than the degrees of freedom might have
high acceptance rates regardless of the value chosen.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_m_scale">m_scale</code></td>
<td>
<p>The scale hyperparameter for the batch shift prior
distribution. This defines the scale of the batch effect upon the mean and
should be in (0, 1].</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_rho">rho</code></td>
<td>
<p>The shape of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_theta">theta</code></td>
<td>
<p>The scale of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_initial_class_means">initial_class_means</code></td>
<td>
<p>A $P x K$ matrix of initial values for the class
means. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_initial_class_covariance">initial_class_covariance</code></td>
<td>
<p>A $P x P x K$ array of initial values for
the class covariance matrices. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_initial_batch_shift">initial_batch_shift</code></td>
<td>
<p>A $P x B$ matrix of initial values for the batch
shift effect Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_initial_batch_scale">initial_batch_scale</code></td>
<td>
<p>A $P x B$ matrix of initial values for the batch
scales Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_initial_class_df">initial_class_df</code></td>
<td>
<p>A $K$ vector of initial values for the class degrees
of freedom. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="runBatchMix_+3A_verbose">verbose</code></td>
<td>
<p>Logiccal indicating if warning about proposal windows should
be printed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing the sampled partitions, cluster and batch
parameters, model fit measures and some details on the model call.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Density choice
type &lt;- "MVN"

# Sampling parameters
R &lt;- 1000
thin &lt;- 50

# MCMC samples
mcmc_out &lt;- runBatchMix(
  X,
  R,
  thin,
  batch_vec,
  type,
  initial_labels = labels,
  fixed = fixed
)

# Given an initial value for the parameters
initial_class_means &lt;- matrix(c(1, 1, 3, 4), nrow = 2)
initial_class_covariance &lt;- array(c(1, 0, 0, 1, 1, 0, 0, 1),
  dim = c(2, 2, 2)
)

# We can use values from a previous chain
initial_batch_shift &lt;- mcmc_out$batch_shift[, , R / thin]
initial_batch_scale &lt;- matrix(
  c(1.2, 1.3, 1.7, 1.1, 1.4, 1.3, 1.2, 1.2, 1.1, 2.0),
  nrow = 2
)

mcmc_out &lt;- runBatchMix(X,
  R,
  thin,
  batch_vec,
  type,
  initial_labels = labels,
  fixed = fixed,
  initial_class_means = initial_class_means,
  initial_class_covariance = initial_class_covariance,
  initial_batch_shift = initial_batch_shift,
  initial_batch_scale = initial_batch_scale
)

</code></pre>

<hr>
<h2 id='runMCMCChains'>Run MCMC Chains</h2><span id='topic+runMCMCChains'></span>

<h3>Description</h3>

<p>Run multiple chains of the batch mixture model of the same type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runMCMCChains(
  X,
  n_chains,
  R,
  thin,
  batch_vec,
  type,
  K_max = NULL,
  initial_labels = NULL,
  fixed = NULL,
  alpha = 1,
  mu_proposal_window = 0.5^2,
  cov_proposal_window = 0.002,
  m_proposal_window = 0.3^2,
  S_proposal_window = 0.01,
  t_df_proposal_window = 0.015,
  m_scale = 0.01,
  rho = 3,
  theta = 1,
  initial_class_means = NULL,
  initial_class_covariance = NULL,
  initial_batch_shift = NULL,
  initial_batch_scale = NULL,
  initial_class_df = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="runMCMCChains_+3A_x">X</code></td>
<td>
<p>Data to cluster as a matrix with the items to cluster held in rows.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_n_chains">n_chains</code></td>
<td>
<p>Integer. Number of MCMC chains to run.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_r">R</code></td>
<td>
<p>The number of iterations in the sampler.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_thin">thin</code></td>
<td>
<p>The factor by which the samples generated are thinned, e.g. if
&ldquo;thin=50&ldquo; only every 50th sample is kept.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_batch_vec">batch_vec</code></td>
<td>
<p>Labels identifying which batch each item being clustered is
from.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_type">type</code></td>
<td>
<p>Character indicating density type to use. One of 'MVN'
(multivariate normal distribution) or 'MVT' (multivariate t distribution).
&ldquo;weights&ldquo; which is a matrix with K x B columns. The columns are ordered by
batch, i.e. the first K columns contain the class weights in the first batch,
the second K are the class weights in the second batch, etc. If generic
weights are used then this matrix has K columns, one for each component weight.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_k_max">K_max</code></td>
<td>
<p>The number of components to include (the upper bound on the
number of clusters in each sample). Defaults to the number of unique labels
in &ldquo;initial_labels&ldquo;.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_initial_labels">initial_labels</code></td>
<td>
<p>Initial clustering, if none given defaults to a random draw.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_fixed">fixed</code></td>
<td>
<p>Which items are fixed in their initial label. If not given,
defaults to a vector of 0 meaning the model is run unsupervised.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_alpha">alpha</code></td>
<td>
<p>The concentration parameter for the stick-breaking prior and the
weights in the model.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_mu_proposal_window">mu_proposal_window</code></td>
<td>
<p>The proposal window for the cluster mean proposal
kernel. The proposal density is a Gaussian distribution, the window is the
variance.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_cov_proposal_window">cov_proposal_window</code></td>
<td>
<p>The proposal window for the cluster covariance
proposal kernel. The proposal density is a Wishart distribution, this
argument is the reciprocal of the degree of freedom.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_m_proposal_window">m_proposal_window</code></td>
<td>
<p>The proposal window for the batch mean proposal
kernel. The proposal density is a Gaussian distribution, the window is the
variance.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_s_proposal_window">S_proposal_window</code></td>
<td>
<p>The proposal window for the batch standard deviation
proposal kernel. The proposal density is a Gamma distribution, this
argument is the reciprocal of the rate.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_t_df_proposal_window">t_df_proposal_window</code></td>
<td>
<p>The proposal window for the degrees of freedom
for the multivariate t distribution (not used if type is not 'MVT'). The
proposal density is a Gamma distribution, this argument is the reciprocal of
the rate.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_m_scale">m_scale</code></td>
<td>
<p>The scale hyperparameter for the batch shift prior
distribution. This defines the scale of the batch effect upon the mean and
should be in (0, 1].</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_rho">rho</code></td>
<td>
<p>The shape of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_theta">theta</code></td>
<td>
<p>The scale of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_initial_class_means">initial_class_means</code></td>
<td>
<p>A $P x K$ matrix of initial values for the class
means. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_initial_class_covariance">initial_class_covariance</code></td>
<td>
<p>A $P x P x K$ array of initial values for
the class covariance matrices. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_initial_batch_shift">initial_batch_shift</code></td>
<td>
<p>A $P x B$ matrix of initial values for the batch
shift effect Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_initial_batch_scale">initial_batch_scale</code></td>
<td>
<p>A $P x B$ matrix of initial values for the batch
scales Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_initial_class_df">initial_class_df</code></td>
<td>
<p>A $K$ vector of initial values for the class degrees
of freedom. Defaults to draws from the prior distribution.</p>
</td></tr>
<tr><td><code id="runMCMCChains_+3A_verbose">verbose</code></td>
<td>
<p>Logiccal indicating if warning about proposal windows should
be printed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of named lists. Each entry is the output of
&ldquo;runBatchMix&ldquo;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Data in a matrix format
X &lt;- matrix(c(rnorm(100, 0, 1), rnorm(100, 3, 1)), ncol = 2, byrow = TRUE)

# Initial labelling
labels &lt;- c(
  rep(1, 10),
  sample(c(1, 2), size = 40, replace = TRUE),
  rep(2, 10),
  sample(c(1, 2), size = 40, replace = TRUE)
)

fixed &lt;- c(rep(1, 10), rep(0, 40), rep(1, 10), rep(0, 40))

# Batch
batch_vec &lt;- sample(seq(1, 5), replace = TRUE, size = 100)

# Sampling parameters
R &lt;- 1000
thin &lt;- 50
n_chains &lt;- 4

# MCMC samples
samples &lt;- runMCMCChains(X, n_chains, R, thin, batch_vec, "MVN",
  initial_labels = labels,
  fixed = fixed
)

</code></pre>

<hr>
<h2 id='sampleMVN'>Sample mixture of multivariate normal distributions with batch effects</h2><span id='topic+sampleMVN'></span>

<h3>Description</h3>

<p>Performs MCMC sampling for a mixture model with batch effects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleMVN(
  X,
  K,
  B,
  labels,
  batch_vec,
  mu_proposal_window,
  cov_proposal_window,
  m_proposal_window,
  S_proposal_window,
  R,
  thin,
  concentration,
  m_scale,
  rho,
  theta,
  initial_mu,
  initial_cov,
  initial_m,
  initial_S,
  mu_initialised,
  cov_initialised,
  m_initialised,
  S_initialised,
  sample_m_scale
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampleMVN_+3A_x">X</code></td>
<td>
<p>The data matrix to perform clustering upon (items to cluster in rows).</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_k">K</code></td>
<td>
<p>The number of components to model (upper limit on the number of 
clusters found).</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_b">B</code></td>
<td>
<p>The number of batches to model.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_labels">labels</code></td>
<td>
<p>Vector item labels to initialise from.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_batch_vec">batch_vec</code></td>
<td>
<p>Observed batch labels.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_mu_proposal_window">mu_proposal_window</code></td>
<td>
<p>The standard deviation for the Gaussian proposal
density of the cluster means.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_cov_proposal_window">cov_proposal_window</code></td>
<td>
<p>The degrees of freedom for the Wishart proposal
density of the cluster covariances.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_m_proposal_window">m_proposal_window</code></td>
<td>
<p>The standard deviation for the Gaussian proposal
density of the batch mean effects.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_s_proposal_window">S_proposal_window</code></td>
<td>
<p>The rate for the Gamma proposal density of the 
batch scale.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_r">R</code></td>
<td>
<p>The number of iterations to run for.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_thin">thin</code></td>
<td>
<p>thinning factor for samples recorded.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_concentration">concentration</code></td>
<td>
<p>Vector of concentrations for mixture weights
(recommended to be symmetric).</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_m_scale">m_scale</code></td>
<td>
<p>The scale hyperparameter for the batch shift prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_rho">rho</code></td>
<td>
<p>The shape of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_theta">theta</code></td>
<td>
<p>The scale of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_initial_mu">initial_mu</code></td>
<td>
<p>A P x K matrix of initial values for the class means.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_initial_cov">initial_cov</code></td>
<td>
<p>A P x P x K cube of initial values for the class 
covariance matrices.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_initial_m">initial_m</code></td>
<td>
<p>A P x B matrix of initial values for the batch shift 
effects.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_initial_s">initial_S</code></td>
<td>
<p>A P x B matrix of initial values for the batch scales.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_mu_initialised">mu_initialised</code></td>
<td>
<p>Bool indicating if the class means are initialised by
the user. If &ldquo;false&ldquo; then initial values are drawn from the prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_cov_initialised">cov_initialised</code></td>
<td>
<p>Bool indicating if the class covariance matrices are 
initialised by the user. If &ldquo;false&ldquo; then initial values are drawn from the
prior distribution.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_m_initialised">m_initialised</code></td>
<td>
<p>Bool indicating if the batch shift effects are 
initialised by the user. If &ldquo;false&ldquo; then initial values are drawn from the
prior distribution.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_s_initialised">S_initialised</code></td>
<td>
<p>Bool indicating if the batch scales are initialised by 
the user. If &ldquo;false&ldquo; then initial values are drawn from the prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleMVN_+3A_sample_m_scale">sample_m_scale</code></td>
<td>
<p>Bool indicating if the hyperparameter on the batch 
shift effect is sampled or given as fixed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list of the different quantities drawn by the sampler.
</p>

<hr>
<h2 id='sampleMVT'>Sample mixture of multivariate t-distributions with batch effects</h2><span id='topic+sampleMVT'></span>

<h3>Description</h3>

<p>Performs MCMC sampling for a MVT mixture model with batch effects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleMVT(
  X,
  K,
  B,
  labels,
  batch_vec,
  mu_proposal_window,
  cov_proposal_window,
  m_proposal_window,
  S_proposal_window,
  t_df_proposal_window,
  R,
  thin,
  concentration,
  m_scale,
  rho,
  theta,
  initial_mu,
  initial_cov,
  initial_df,
  initial_m,
  initial_S,
  mu_initialised,
  cov_initialised,
  df_initialised,
  m_initialised,
  S_initialised,
  sample_m_scale
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampleMVT_+3A_x">X</code></td>
<td>
<p>The data matrix to perform clustering upon (items to cluster in rows).</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_k">K</code></td>
<td>
<p>The number of components to model (upper limit on the number of 
clusters found).</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_b">B</code></td>
<td>
<p>The number of batches to model.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_labels">labels</code></td>
<td>
<p>Vector item labels to initialise from.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_batch_vec">batch_vec</code></td>
<td>
<p>Observed batch labels.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_mu_proposal_window">mu_proposal_window</code></td>
<td>
<p>The standard deviation for the Gaussian proposal 
density of the cluster means.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_cov_proposal_window">cov_proposal_window</code></td>
<td>
<p>The degrees of freedom for the Wishart proposal 
density of the cluster covariances.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_m_proposal_window">m_proposal_window</code></td>
<td>
<p>The standard deviation for the Gaussian proposal 
density of the batch mean effects.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_s_proposal_window">S_proposal_window</code></td>
<td>
<p>The rate for the Gamma proposal density of the 
batch scale.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_t_df_proposal_window">t_df_proposal_window</code></td>
<td>
<p>The rate for the Gamma proposal density of the 
cluster degrees of freedom.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_r">R</code></td>
<td>
<p>The number of iterations to run for.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_thin">thin</code></td>
<td>
<p>thinning factor for samples recorded.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_concentration">concentration</code></td>
<td>
<p>Vector of concentrations for mixture weights 
(recommended to be symmetric).</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_m_scale">m_scale</code></td>
<td>
<p>The scale hyperparameter for the batch shift prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_rho">rho</code></td>
<td>
<p>The shape of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_theta">theta</code></td>
<td>
<p>The scale of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_initial_mu">initial_mu</code></td>
<td>
<p>A P x K matrix of initial values for the class means.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_initial_cov">initial_cov</code></td>
<td>
<p>A P x P x K cube of initial values for the class 
covariance matrices.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_initial_df">initial_df</code></td>
<td>
<p>A K vector of initial values for the class degrees of
freedom.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_initial_m">initial_m</code></td>
<td>
<p>A P x B matrix of initial values for the batch shift 
effects.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_initial_s">initial_S</code></td>
<td>
<p>A P x B matrix of initial values for the batch scales.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_mu_initialised">mu_initialised</code></td>
<td>
<p>Bool indicating if the class means are initialised by
the user. If &ldquo;false&ldquo; then initial values are drawn from the prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_cov_initialised">cov_initialised</code></td>
<td>
<p>Bool indicating if the class covariance matrices are 
initialised by the user. If &ldquo;false&ldquo; then initial values are drawn from the
prior distribution.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_df_initialised">df_initialised</code></td>
<td>
<p>Bool indicating if the class degrees of freedom are 
initialised by the user. If &ldquo;false&ldquo; then initial values are drawn from the 
prior distribution.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_m_initialised">m_initialised</code></td>
<td>
<p>Bool indicating if the batch shift effects are 
initialised by the user. If &ldquo;false&ldquo; then initial values are drawn from the
prior distribution.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_s_initialised">S_initialised</code></td>
<td>
<p>Bool indicating if the batch scales are initialised by 
the user. If &ldquo;false&ldquo; then initial values are drawn from the prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleMVT_+3A_sample_m_scale">sample_m_scale</code></td>
<td>
<p>Bool indicating if the hyperparameter on the batch 
shift effect is sampled or given as fixed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list of the different quantities drawn by the sampler.
</p>

<hr>
<h2 id='samplePriorLabels'>Sample prior labels</h2><span id='topic+samplePriorLabels'></span>

<h3>Description</h3>

<p>Generate labels from the stick-breaking prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>samplePriorLabels(alpha, K, N)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="samplePriorLabels_+3A_alpha">alpha</code></td>
<td>
<p>The concentration parameter for the stick-breaking prior.</p>
</td></tr>
<tr><td><code id="samplePriorLabels_+3A_k">K</code></td>
<td>
<p>The number of components to include (the upper bound on the number of unique labels generated).</p>
</td></tr>
<tr><td><code id="samplePriorLabels_+3A_n">N</code></td>
<td>
<p>The number of labels to generate.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of labels.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>initial_labels &lt;- samplePriorLabels(1, 50, 100)
</code></pre>

<hr>
<h2 id='sampleSemisupervisedMVN'>Sample semi-supervised MVN Mixture model</h2><span id='topic+sampleSemisupervisedMVN'></span>

<h3>Description</h3>

<p>Performs MCMC sampling for a mixture model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleSemisupervisedMVN(
  X,
  K,
  B,
  labels,
  batch_vec,
  fixed,
  mu_proposal_window,
  cov_proposal_window,
  m_proposal_window,
  S_proposal_window,
  R,
  thin,
  concentration,
  m_scale,
  rho,
  theta,
  initial_mu,
  initial_cov,
  initial_m,
  initial_S,
  mu_initialised,
  cov_initialised,
  m_initialised,
  S_initialised,
  sample_m_scale
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampleSemisupervisedMVN_+3A_x">X</code></td>
<td>
<p>The data matrix to perform clustering upon (items to cluster in
rows).</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_k">K</code></td>
<td>
<p>The number of components to model (upper limit on the number of
clusters found).</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_b">B</code></td>
<td>
<p>The number of batches to model.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_labels">labels</code></td>
<td>
<p>Vector item labels to initialise from.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_batch_vec">batch_vec</code></td>
<td>
<p>Observed batch labels.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_fixed">fixed</code></td>
<td>
<p>Binary vector of the items that are fixed in their initial
label.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_mu_proposal_window">mu_proposal_window</code></td>
<td>
<p>The standard deviation for the Gaussian proposal
density of the cluster means.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_cov_proposal_window">cov_proposal_window</code></td>
<td>
<p>The degrees of freedom for the Wishart proposal
density of the cluster covariances.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_m_proposal_window">m_proposal_window</code></td>
<td>
<p>The standard deviation for the Gaussian proposal
density of the batch mean effects.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_s_proposal_window">S_proposal_window</code></td>
<td>
<p>The rate for the Gamma proposal density of the batch scale.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_r">R</code></td>
<td>
<p>The number of iterations to run for.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_thin">thin</code></td>
<td>
<p>thinning factor for samples recorded.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_concentration">concentration</code></td>
<td>
<p>Vector of concentrations for mixture weights 
(recommended to be symmetric).</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_m_scale">m_scale</code></td>
<td>
<p>The scale hyperparameter for the batch shift prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_rho">rho</code></td>
<td>
<p>The shape of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_theta">theta</code></td>
<td>
<p>The scale of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_initial_mu">initial_mu</code></td>
<td>
<p>A P x K matrix of initial values for the class means.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_initial_cov">initial_cov</code></td>
<td>
<p>A P x P x K cube of initial values for the class 
covariance matrices.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_initial_m">initial_m</code></td>
<td>
<p>A P x B matrix of initial values for the batch shift 
effects.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_initial_s">initial_S</code></td>
<td>
<p>A P x B matrix of initial values for the batch scales.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_mu_initialised">mu_initialised</code></td>
<td>
<p>Bool indicating if the class means are initialised by
the user. If &ldquo;false&ldquo; then initial values are drawn from the prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_cov_initialised">cov_initialised</code></td>
<td>
<p>Bool indicating if the class covariance matrices are 
initialised by the user. If &ldquo;false&ldquo; then initial values are drawn from the
prior distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_m_initialised">m_initialised</code></td>
<td>
<p>Bool indicating if the batch shift effects are 
initialised by the user. If &ldquo;false&ldquo; then initial values are drawn from the
prior distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_s_initialised">S_initialised</code></td>
<td>
<p>Bool indicating if the batch scales are initialised by 
the user. If &ldquo;false&ldquo; then initial values are drawn from the prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVN_+3A_sample_m_scale">sample_m_scale</code></td>
<td>
<p>Bool indicating if the hyperparameter on the batch 
shift effect is sampled or given as fixed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list of the different quantities drawn by the sampler.
</p>

<hr>
<h2 id='sampleSemisupervisedMVT'>Sample semi-supervised MVT Mixture model</h2><span id='topic+sampleSemisupervisedMVT'></span>

<h3>Description</h3>

<p>Performs MCMC sampling for a mixture model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleSemisupervisedMVT(
  X,
  K,
  B,
  labels,
  batch_vec,
  fixed,
  mu_proposal_window,
  cov_proposal_window,
  m_proposal_window,
  S_proposal_window,
  t_df_proposal_window,
  R,
  thin,
  concentration,
  m_scale,
  rho,
  theta,
  initial_mu,
  initial_cov,
  initial_df,
  initial_m,
  initial_S,
  mu_initialised,
  cov_initialised,
  df_initialised,
  m_initialised,
  S_initialised,
  sample_m_scale
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampleSemisupervisedMVT_+3A_x">X</code></td>
<td>
<p>The data matrix to perform clustering upon (items to cluster in rows).</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_k">K</code></td>
<td>
<p>The number of components to model (upper limit on the number of clusters found).</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_b">B</code></td>
<td>
<p>The number of batches to model.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_labels">labels</code></td>
<td>
<p>Vector item labels to initialise from.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_batch_vec">batch_vec</code></td>
<td>
<p>Observed batch labels.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_fixed">fixed</code></td>
<td>
<p>Binary vector of the items that are fixed in their initial label.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_mu_proposal_window">mu_proposal_window</code></td>
<td>
<p>The standard deviation for the Gaussian proposal density of the cluster means.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_cov_proposal_window">cov_proposal_window</code></td>
<td>
<p>The degrees of freedom for the Wishart proposal density of the cluster covariances.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_m_proposal_window">m_proposal_window</code></td>
<td>
<p>The standard deviation for the Gaussian proposal density of the batch mean effects.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_s_proposal_window">S_proposal_window</code></td>
<td>
<p>The rate for the Gamma proposal density of the batch scale.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_t_df_proposal_window">t_df_proposal_window</code></td>
<td>
<p>The rate for the Gamma proposal density of the cluster degrees of freedom.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_r">R</code></td>
<td>
<p>The number of iterations to run for.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_thin">thin</code></td>
<td>
<p>thinning factor for samples recorded.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_concentration">concentration</code></td>
<td>
<p>Vector of concentrations for mixture weights (recommended to be symmetric).</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_m_scale">m_scale</code></td>
<td>
<p>The scale hyperparameter for the batch shift prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_rho">rho</code></td>
<td>
<p>The shape of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_theta">theta</code></td>
<td>
<p>The scale of the prior distribution for the batch scale.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_initial_mu">initial_mu</code></td>
<td>
<p>A P x K matrix of initial values for the class means.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_initial_cov">initial_cov</code></td>
<td>
<p>A P x P x K cube of initial values for the class 
covariance matrices.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_initial_df">initial_df</code></td>
<td>
<p>A K vector of initial values for the class degrees of
freedom.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_initial_m">initial_m</code></td>
<td>
<p>A P x B matrix of initial values for the batch shift 
effects.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_initial_s">initial_S</code></td>
<td>
<p>A P x B matrix of initial values for the batch scales.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_mu_initialised">mu_initialised</code></td>
<td>
<p>Bool indicating if the class means are initialised by
the user. If &ldquo;false&ldquo; then initial values are drawn from the prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_cov_initialised">cov_initialised</code></td>
<td>
<p>Bool indicating if the class covariance matrices are 
initialised by the user. If &ldquo;false&ldquo; then initial values are drawn from the
prior distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_df_initialised">df_initialised</code></td>
<td>
<p>Bool indicating if the class degrees of freedom are 
initialised by the user. If &ldquo;false&ldquo; then initial values are drawn from the 
prior distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_m_initialised">m_initialised</code></td>
<td>
<p>Bool indicating if the batch shift effects are 
initialised by the user. If &ldquo;false&ldquo; then initial values are drawn from the
prior distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_s_initialised">S_initialised</code></td>
<td>
<p>Bool indicating if the batch scales are initialised by 
the user. If &ldquo;false&ldquo; then initial values are drawn from the prior 
distribution.</p>
</td></tr>
<tr><td><code id="sampleSemisupervisedMVT_+3A_sample_m_scale">sample_m_scale</code></td>
<td>
<p>Bool indicating if the hyperparameter on the batch 
shift effect is sampled or given as fixed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list of the different quantities drawn by the sampler.
</p>

<hr>
<h2 id='VI.lb'>Minimum VI lower bound</h2><span id='topic+VI.lb'></span>

<h3>Description</h3>

<p>Local implementation of S. Wade's 'minVI' function from their
'mcclust.ext' package (available at 'sarawade/mcclust.ext' on github).
Reimplemented here to avoid dependency on a non-CRAN package. Computes the
lower bound to the posterior expected Variation of Information. For full
details please see the aforementioned package and Wade and Ghahramani, 2018,
'Bayesian Cluster Analysis: Point Estimation and Credible Balls (with
Discussion)'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VI.lb(cls, psm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VI.lb_+3A_cls">cls</code></td>
<td>
<p>A clustering for which the lower bound of the Variation of
Information is calculated.</p>
</td></tr>
<tr><td><code id="VI.lb_+3A_psm">psm</code></td>
<td>
<p>The posterior similarity matrix which 'cls' is a summary thereof.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of the the lower bound of the Variation of Information
for
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# MCMC samples and BIC vector
mcmc_outputs &lt;- runMCMCChains(
  X,
  n_chains,
  R,
  thin,
  batch_vec,
  type
)

# Note that in this toy example we have not applied a burn in
psm &lt;- createSimilarityMat(mcmc_outputs[[1]]$samples)
VI.lb(mcmc_outputs[[1]]$samples[1, ], psm)

## End(Not run)
</code></pre>

<hr>
<h2 id='wishartLogLikelihood'>Wishart log-likelihood</h2><span id='topic+wishartLogLikelihood'></span>

<h3>Description</h3>

<p>Used in calculating model probability in Metropolis-Hastings 
algorithm when proposals are from the Wishart distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wishartLogLikelihood(X, V, n, P)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wishartLogLikelihood_+3A_x">X</code></td>
<td>
<p>- matrix; the matrix to calculate the likelihood of.</p>
</td></tr>
<tr><td><code id="wishartLogLikelihood_+3A_v">V</code></td>
<td>
<p>- matrix; the scale of the Wishart distribution.</p>
</td></tr>
<tr><td><code id="wishartLogLikelihood_+3A_n">n</code></td>
<td>
<p>- double; the degrees of freedom for the Wishart distribution.</p>
</td></tr>
<tr><td><code id="wishartLogLikelihood_+3A_p">P</code></td>
<td>
<p>- unsigned integer; the dimension of X.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the unnormalised log-likelihood of X in a Wishart with parameters V 
and n.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
