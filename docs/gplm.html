<!DOCTYPE html><html><head><title>Help for package gplm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gplm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bandwidth.scott'><p>Scott's rule of thumb</p></a></li>
<li><a href='#convol'><p>Kernel convolution</p></a></li>
<li><a href='#create.grid'><p>Create a grid for kernel estimation</p></a></li>
<li><a href='#glm.inverse.link'><p>Link function for GLM</p></a></li>
<li><a href='#glm.link'><p>(Inverse) Link function for GLM</p></a></li>
<li><a href='#glm.ll'><p>Log-likelihood for GLM</p></a></li>
<li><a href='#glm.lld'><p>Log-likelihood derivatives for GLM</p></a></li>
<li><a href='#kbackfit'><p>Backfitting for an additive model using kernel regression</p></a></li>
<li><a href='#kde'><p>Kernel density estimation</p></a></li>
<li><a href='#kernel.constants'><p>Kernel constants</p></a></li>
<li><a href='#kernel.function'><p>Kernel function</p></a></li>
<li><a href='#kgplm'><p>Generalized partial linear model</p></a></li>
<li><a href='#kreg'><p>Kernel regression</p></a></li>
<li><a href='#sgplm1'><p>Generalized partial linear model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Generalized Partial Linear Models (GPLM)</td>
</tr>
<tr>
<td>Version:</td>
<td>0.7-4</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-08-28</td>
</tr>
<tr>
<td>Author:</td>
<td>Marlene Mueller</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marlene Mueller &lt;marlene.mueller@gmx.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions for estimating a generalized partial
	     linear model, a semiparametric variant of the generalized linear model
	     (GLM) which replaces the linear predictor by the sum of a linear
	     and a nonparametric function.</td>
</tr>
<tr>
<td>Depends:</td>
<td>AER</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-08-28 17:31:09 UTC; mm</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-08-29 08:18:41</td>
</tr>
</table>
<hr>
<h2 id='bandwidth.scott'>Scott's rule of thumb</h2><span id='topic+bandwidth.scott'></span>

<h3>Description</h3>

<p>Calculates Scott's rule of thumb bandwidth vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bandwidth.scott(x, kernel = "biweight", product = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bandwidth.scott_+3A_x">x</code></td>
<td>
<p>n x d matrix, data</p>
</td></tr>
<tr><td><code id="bandwidth.scott_+3A_kernel">kernel</code></td>
<td>
<p>text string, see <code><a href="#topic+kernel.function">kernel.function</a></code></p>
</td></tr>
<tr><td><code id="bandwidth.scott_+3A_product">product</code></td>
<td>
<p>(if d&gt;1) product or spherical kernel</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default
bandwidth vector is computed by Scott's rule of thumb for the Gaussian
kernel and adapted to the chosen kernel function.
</p>


<h3>Value</h3>

<p>d x 1 bandwidth vector used for calculation
</p>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>References</h3>

<p>Scott, D.W. (1992).
<em>Multivariate Density Estimation: Theory, Practice, and
Visualization</em>. New York, Chichester: Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel.function">kernel.function</a></code>, <code><a href="#topic+kde">kde</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## two-dimensional data
  n &lt;- 1000
  u &lt;- runif(n)
  thresh &lt;- 0.4
  x1 &lt;- rnorm(n)*(u&lt;thresh) +rnorm(n,mean=3)*(u&gt;=thresh)
  x2 &lt;- rnorm(n)*(u&lt;thresh) +rnorm(n,mean=9)*(u&gt;=thresh)
  bandwidth.scott( cbind(x1,x2) )
</code></pre>

<hr>
<h2 id='convol'>Kernel convolution</h2><span id='topic+convol'></span>

<h3>Description</h3>

<p>Calculates the convolution of data with a kernel function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convol(x, h = 1, grid = NULL, y = 1, w = 1, p = 2, q = 2,
       product = TRUE, sort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convol_+3A_x">x</code></td>
<td>
<p>n x d matrix, data</p>
</td></tr>
<tr><td><code id="convol_+3A_h">h</code></td>
<td>
<p>scalar or 1 x d, bandwidth(s)</p>
</td></tr>
<tr><td><code id="convol_+3A_grid">grid</code></td>
<td>
<p>m x d matrix, where to calculate the convolution (default = x)</p>
</td></tr>
<tr><td><code id="convol_+3A_y">y</code></td>
<td>
<p>n x c matrix, optional responses</p>
</td></tr>
<tr><td><code id="convol_+3A_w">w</code></td>
<td>
<p>scalar or  n x 1 or 1 x m or n x m, optional weights</p>
</td></tr>
<tr><td><code id="convol_+3A_p">p</code></td>
<td>
<p>integer or text, see <code><a href="#topic+kernel.function">kernel.function</a></code></p>
</td></tr>
<tr><td><code id="convol_+3A_q">q</code></td>
<td>
<p>integer, see <code><a href="#topic+kernel.function">kernel.function</a></code></p>
</td></tr>
<tr><td><code id="convol_+3A_product">product</code></td>
<td>
<p>(if d&gt;1) product or spherical kernel</p>
</td></tr>
<tr><td><code id="convol_+3A_sort">sort</code></td>
<td>
<p>logical, TRUE if data need to be sorted</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kernel convolution which is calculated is
<code class="reqn">\sum_i K_h(x_i - grid_{j})\,y_i\,w_{ij}</code> for
<code class="reqn">i=1,...,n</code> and <code class="reqn">j=1,...,m</code>. The kernel function is determined
by the kernel parameters p and q, see
<code><a href="#topic+kernel.function">kernel.function</a></code>. The default kernel is the biweight
(quartic) kernel function.   Note that the DLL requires the data matrix
to be sorted by its first column.
</p>


<h3>Value</h3>

<p>m x c matrix
</p>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel.function">kernel.function</a></code>, <code><a href="#topic+kde">kde</a></code>, <code><a href="#topic+kreg">kreg</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  n &lt;- 100
  x &lt;- rnorm(n)
  convol(x,h=0.8,grid=-3:3)/n  ## estimates density of x at points -3:3
</code></pre>

<hr>
<h2 id='create.grid'>Create a grid for kernel estimation</h2><span id='topic+create.grid'></span>

<h3>Description</h3>

<p>Helps to define a grid for kernel denity or regression estimates
(univariate or multivariate).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create.grid(grid.list, sort=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create.grid_+3A_grid.list">grid.list</code></td>
<td>
<p>list of 1-dimensional vectors containing the grid
values for each dimension</p>
</td></tr>
<tr><td><code id="create.grid_+3A_sort">sort</code></td>
<td>
<p>sort the vectors (can be set to FALSE if vectors are
already sorted in ascending order)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function allows easily to define grids for the &quot;gplm&quot; package.
If the data are d-dimensional and the grid vector lengths are n1,
... nd, then the output is a (n1*...*nd) x d matrix with each row
corresponding to one d-dimensional data point at which the function
estimate is to be calculated.
</p>


<h3>Value</h3>

<p>m x d grid matrix
</p>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+expand.grid">expand.grid</a></code>, <code><a href="#topic+kde">kde</a></code>, <code><a href="#topic+kreg">kreg</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  v1 &lt;- 1:5
  v2 &lt;- 3:1
  grid &lt;- create.grid(list(v1,v2))

  x &lt;- matrix(rnorm(60),30,2)
  v1 &lt;- seq(min(x[,1]),max(x[,1]),length=10)
  v2 &lt;- seq(min(x[,2]),max(x[,2]),length=5)
  grid &lt;- create.grid(list(v1,v2))
</code></pre>

<hr>
<h2 id='glm.inverse.link'>Link function for GLM</h2><span id='topic+glm.inverse.link'></span>

<h3>Description</h3>

<p>Defines the link function for a GLM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm.inverse.link(mu, family="gaussian", link="identity", k=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm.inverse.link_+3A_mu">mu</code></td>
<td>
<p>n x 1, linear predictors</p>
</td></tr>
<tr><td><code id="glm.inverse.link_+3A_family">family</code></td>
<td>
<p>text string, family of distributions (e.g.
&quot;gaussian&quot; or &quot;bernoulli&quot;, see details for <code><a href="#topic+glm.ll">glm.ll</a></code>)</p>
</td></tr>
<tr><td><code id="glm.inverse.link_+3A_link">link</code></td>
<td>
<p>text string, link function (depending on family,
see details for <code><a href="#topic+glm.ll">glm.ll</a></code>)</p>
</td></tr>
<tr><td><code id="glm.inverse.link_+3A_k">k</code></td>
<td>
<p>integer &gt; 0, parameter for the negative binomial</p>
</td></tr>
</table>


<h3>Value</h3>

<p>n x 1, vector eta (predictors)
</p>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.ll">glm.ll</a></code>, <code><a href="#topic+glm.lld">glm.lld</a></code>, <code><a href="#topic+glm.link">glm.link</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  glm.inverse.link(c(0.25,0.5), family="bernoulli", link="logit")
</code></pre>

<hr>
<h2 id='glm.link'>(Inverse) Link function for GLM</h2><span id='topic+glm.link'></span>

<h3>Description</h3>

<p>Defines the inverse link function for a GLM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm.link(eta, family="gaussian", link="identity", k=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm.link_+3A_eta">eta</code></td>
<td>
<p>n x 1, linear predictors</p>
</td></tr>
<tr><td><code id="glm.link_+3A_family">family</code></td>
<td>
<p>text string, family of distributions (e.g.
&quot;gaussian&quot; or &quot;bernoulli&quot;, see details for <code><a href="#topic+glm.ll">glm.ll</a></code>)</p>
</td></tr>
<tr><td><code id="glm.link_+3A_link">link</code></td>
<td>
<p>text string, link function (depending on family,
see details for <code><a href="#topic+glm.ll">glm.ll</a></code>)</p>
</td></tr>
<tr><td><code id="glm.link_+3A_k">k</code></td>
<td>
<p>integer &gt; 0, parameter for the negative binomial</p>
</td></tr>
</table>


<h3>Value</h3>

<p>n x 1, vector mu (responses)
</p>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.ll">glm.ll</a></code>, <code><a href="#topic+glm.lld">glm.lld</a></code>, <code><a href="#topic+glm.inverse.link">glm.inverse.link</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  glm.link(c(-1,2), family="bernoulli", link="logit")
</code></pre>

<hr>
<h2 id='glm.ll'>Log-likelihood for GLM</h2><span id='topic+glm.ll'></span>

<h3>Description</h3>

<p>Calculates the log-likelihood function of a GLM.
Currently only the gaussian and the bernoulli family
are implemented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm.ll(mu, y, phi=1, family="gaussian", k=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm.ll_+3A_mu">mu</code></td>
<td>
<p>n x 1, predicted regression function</p>
</td></tr>
<tr><td><code id="glm.ll_+3A_y">y</code></td>
<td>
<p>n x 1, responses</p>
</td></tr>
<tr><td><code id="glm.ll_+3A_phi">phi</code></td>
<td>
<p>scalar, nuisance parameter (sigma^2 for the gaussian and
inverse gaussian families, nu for the gamma family)</p>
</td></tr>
<tr><td><code id="glm.ll_+3A_family">family</code></td>
<td>
<p>text string, family of distributions (e.g.
&quot;gaussian&quot; or &quot;bernoulli&quot;, see details below)</p>
</td></tr>
<tr><td><code id="glm.ll_+3A_k">k</code></td>
<td>
<p>integer &gt; 0, parameter for the negative binomial</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Implemented are the &quot;gaussian&quot; family
(with links &quot;identity&quot; and &quot;log&quot;), the &quot;bernoulli&quot; family
(with links &quot;logit&quot; and &quot;probit&quot;), the &quot;gamma&quot; family
(with link &quot;inverse&quot;), the &quot;poisson&quot; family (with link &quot;log&quot;),
the &quot;inverse.gaussian&quot; family (with link &quot;inverse.squared&quot;) and
the &quot;negative.binomial&quot; (with its canonical &quot;log&quot; type link).
</p>
<p>The default value k=1 leads to the geometric distribution (as a
special case of the negative binomial). 
</p>


<h3>Value</h3>

<p>log-likelihood value
</p>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.lld">glm.lld</a></code>, <code><a href="#topic+glm.link">glm.link</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  glm.ll(rep(0.4,2), c(0,1), family="bernoulli")
</code></pre>

<hr>
<h2 id='glm.lld'>Log-likelihood derivatives for GLM</h2><span id='topic+glm.lld'></span>

<h3>Description</h3>

<p>Computes first and second derivatives of the individual 
log-likelihood with respect to the linear predictor.
Currently only the gaussian (with identity link) and the bernoulli family
(with logit and probit links) are implemented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm.lld(eta, y, family="gaussian", link="identity", k=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm.lld_+3A_eta">eta</code></td>
<td>
<p>n x 1, linear predictors</p>
</td></tr>
<tr><td><code id="glm.lld_+3A_y">y</code></td>
<td>
<p>n x 1, responses</p>
</td></tr>
<tr><td><code id="glm.lld_+3A_family">family</code></td>
<td>
<p>text string, family of distributions (e.g.
&quot;gaussian&quot; or &quot;bernoulli&quot;, see details for <code><a href="#topic+glm.ll">glm.ll</a></code>)</p>
</td></tr>
<tr><td><code id="glm.lld_+3A_link">link</code></td>
<td>
<p>text string, link function (depending on family,
see details for <code><a href="#topic+glm.ll">glm.ll</a></code>)</p>
</td></tr>
<tr><td><code id="glm.lld_+3A_k">k</code></td>
<td>
<p>integer &gt; 0, parameter for the negative binomial</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See details for <code><a href="#topic+glm.ll">glm.ll</a></code>.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>ll1</code></td>
<td>
<p>n x 1, vector of first derivatives</p>
</td></tr>
<tr><td><code>ll2</code></td>
<td>
<p>n x 1, vector of second derivatives</p>
</td></tr>
<tr><td><code>ll1.2</code></td>
<td>
<p>n x 1, ratio <code>ll1/ll2</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.ll">glm.ll</a></code>, <code><a href="#topic+glm.link">glm.link</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  glm.lld(c(-1,2), c(0,1), family="bernoulli", link="logit")
</code></pre>

<hr>
<h2 id='kbackfit'>Backfitting for an additive model using kernel regression</h2><span id='topic+kbackfit'></span>

<h3>Description</h3>

<p>Implements kernel-based backfitting in an additive model,
optional with a partial linear term.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kbackfit(t, y, h, x = NULL, grid = NULL, weights.conv = 1,
           offset = 0, method = "generic",
           max.iter = 50, eps.conv = 1e-04, m.start = NULL,
           kernel = "biweight")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kbackfit_+3A_y">y</code></td>
<td>
<p>n x 1 vector, responses</p>
</td></tr>
<tr><td><code id="kbackfit_+3A_t">t</code></td>
<td>
<p>n x q matrix, data for nonparametric part</p>
</td></tr>
<tr><td><code id="kbackfit_+3A_h">h</code></td>
<td>
<p>scalar or 1 x q, bandwidth(s)</p>
</td></tr>
<tr><td><code id="kbackfit_+3A_x">x</code></td>
<td>
<p>optional, n x p matrix, data for linear part</p>
</td></tr>
<tr><td><code id="kbackfit_+3A_grid">grid</code></td>
<td>
<p>m x q matrix, where to calculate the nonparametric function (default = t)</p>
</td></tr>
<tr><td><code id="kbackfit_+3A_weights.conv">weights.conv</code></td>
<td>
<p>weights for convergence criterion</p>
</td></tr>
<tr><td><code id="kbackfit_+3A_offset">offset</code></td>
<td>
<p>offset</p>
</td></tr>
<tr><td><code id="kbackfit_+3A_method">method</code></td>
<td>
<p>one of <code>"generic"</code>, <code>"linit"</code> or <code>"modified"</code></p>
</td></tr>
<tr><td><code id="kbackfit_+3A_max.iter">max.iter</code></td>
<td>
<p>maximal number of iterations</p>
</td></tr>
<tr><td><code id="kbackfit_+3A_eps.conv">eps.conv</code></td>
<td>
<p>convergence criterion</p>
</td></tr>
<tr><td><code id="kbackfit_+3A_m.start">m.start</code></td>
<td>
<p>n x q matrix, start values for m</p>
</td></tr>
<tr><td><code id="kbackfit_+3A_kernel">kernel</code></td>
<td>
<p>text string, see <code><a href="#topic+kernel.function">kernel.function</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>c</code></td>
<td>
<p>constant</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>p x 1 vector, linear coefficients</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>n x q matrix, nonparametric marginal function estimates</p>
</td></tr>
<tr><td><code>m.grid</code></td>
<td>
<p>m x q matrix, nonparametric marginal function estimates
on grid</p>
</td></tr>
<tr><td><code>rss</code></td>
<td>
<p>residual sum of squares</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel.function">kernel.function</a></code>, <code><a href="#topic+kreg">kreg</a></code></p>

<hr>
<h2 id='kde'>Kernel density estimation</h2><span id='topic+kde'></span>

<h3>Description</h3>

<p>Calculates a kernel density estimate (univariate or multivariate).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kde(x, bandwidth = NULL, grid = TRUE, kernel = "biweight",
    product = TRUE, sort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kde_+3A_x">x</code></td>
<td>
<p>n x d matrix, data</p>
</td></tr>
<tr><td><code id="kde_+3A_bandwidth">bandwidth</code></td>
<td>
<p>scalar or 1 x d, bandwidth(s)</p>
</td></tr>
<tr><td><code id="kde_+3A_grid">grid</code></td>
<td>
<p>logical or m x d matrix (where to calculate the density)</p>
</td></tr>
<tr><td><code id="kde_+3A_kernel">kernel</code></td>
<td>
<p>text string, see <code><a href="#topic+kernel.function">kernel.function</a></code></p>
</td></tr>
<tr><td><code id="kde_+3A_product">product</code></td>
<td>
<p>(if d&gt;1) product or spherical kernel</p>
</td></tr>
<tr><td><code id="kde_+3A_sort">sort</code></td>
<td>
<p>logical, TRUE if data need to be sorted</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kernel density estimator is calculated as
<code class="reqn">\frac{1}{n} \sum_i K_h(x_i - grid_{j})</code> for
<code class="reqn">i=1,...,n</code> and <code class="reqn">j=1,...,m</code>. The default
bandwidth vector is computed by Scott's rule of thumb
(adapted to the chosen kernel function).
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>m x d matrix, where density has been calculated</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>m x 1 vector, density estimates</p>
</td></tr>
<tr><td><code>bandwidth</code></td>
<td>
<p>bandwidth vector used for calculation</p>
</td></tr>
<tr><td><code>rearrange</code></td>
<td>
<p>if sort=TRUE, index to rearrange x and y to its original order.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel.function">kernel.function</a></code>, <code><a href="#topic+convol">convol</a></code>, <code><a href="#topic+kreg">kreg</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  n &lt;- 1000
  x &lt;- rnorm(n)
  plot(kde(x), type="l")

  ## mixed normal data
  n &lt;- 1000
  u &lt;- runif(n)
  thresh &lt;- 0.4
  x &lt;- rnorm(n)*(u&lt;thresh) +rnorm(n,mean=3)*(u&gt;=thresh)
  h &lt;- 1
  fh &lt;- kde(x,bandwidth=h)
  plot(kde(x,bandwidth=h),type="l",lwd=2); rug(x)
  lines(kde(x,bandwidth=h*1.2),col="red")
  lines(kde(x,bandwidth=h*1.4),col="orange")
  lines(kde(x,bandwidth=h/1.2),col="blue")
  lines(kde(x,bandwidth=h/1.4),col="cyan")

  ## two-dimensional data
  n &lt;- 1000
  u &lt;- runif(n)
  thresh &lt;- 0.4
  x1 &lt;- rnorm(n)*(u&lt;thresh) +rnorm(n,mean=3)*(u&gt;=thresh)
  x2 &lt;- rnorm(n)*(u&lt;thresh) +rnorm(n,mean=9)*(u&gt;=thresh)

  grid1 &lt;- seq(min(x1),max(x1),length=20)  ## grid for x1
  grid2 &lt;- seq(min(x2),max(x2),length=25)  ## grid for x2

  fh &lt;- kde( cbind(x1,x2), grid=create.grid(list(grid1,grid2)) )
  o &lt;- order(fh$x[,2],fh$x[,1])
  density &lt;- (matrix(fh$y[o],length(grid1),length(grid2)))
  
  par(mfrow=c(2,2))
  plot(kde(x1),type="l",main="x1"); rug(x1)
  plot(kde(x2),type="l",main="x2"); rug(x2)
  persp(grid1,grid2,density,main="KDE",
        theta=30,phi=30,expand=0.5,col="lightblue",shade=0.5)
  contour(grid1,grid2,density, main="KDE Contours")
  points(x1,x2,col="red",pch=18,cex=0.5)
  par(mfrow=c(1,1))
</code></pre>

<hr>
<h2 id='kernel.constants'>Kernel constants</h2><span id='topic+kernel.constants'></span>

<h3>Description</h3>

<p>Calculates several constants of a (product) kernel function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel.constants(kernel = "biweight", d = 1, product = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel.constants_+3A_kernel">kernel</code></td>
<td>
<p>text string, see <code><a href="#topic+kernel.function">kernel.function</a></code></p>
</td></tr>
<tr><td><code id="kernel.constants_+3A_d">d</code></td>
<td>
<p>integer (dimension of the kernel)</p>
</td></tr>
<tr><td><code id="kernel.constants_+3A_product">product</code></td>
<td>
<p>(if d&gt;1) product or spherical kernel</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The constants which are calculated are the second moment,
the square norm and the canonical bandwidth of the kernel
(only the two latter terms depend on the dimension d).
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>m2</code></td>
<td>
<p>second moment</p>
</td></tr>
<tr><td><code>c2</code></td>
<td>
<p>square norm</p>
</td></tr>
<tr><td><code>d0</code></td>
<td>
<p>canonical bandwidth</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel.function">kernel.function</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  kernel.constants()                  ## default (biweight), d=1
  kernel.constants("epanechnikov",1)  ## epanechnikov, d=1
  kernel.constants("epanechnikov",2)  ## product epanechnikov, d=2
</code></pre>

<hr>
<h2 id='kernel.function'>Kernel function</h2><span id='topic+kernel.function'></span>

<h3>Description</h3>

<p>Calculates several kernel functions (uniform, triangle, epanechnikov,
biweight, triweight, gaussian).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel.function(u, kernel = "biweight", product = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel.function_+3A_u">u</code></td>
<td>
<p>n x d matrix</p>
</td></tr>
<tr><td><code id="kernel.function_+3A_kernel">kernel</code></td>
<td>
<p>text string</p>
</td></tr>
<tr><td><code id="kernel.function_+3A_product">product</code></td>
<td>
<p>(if d&gt;1) product or spherical kernel</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kernel parameter is  a text string specifying
the univariate kernel function which is either the gaussian pdf
or proportional to <code class="reqn">(1-|u|^p)^q</code>.
Possible text strings are &quot;triangle&quot; (p=q=1),
&quot;uniform&quot; (p=1, q=0), &quot;epanechnikov&quot; (p=2, q=1),
&quot;biweight&quot; or &quot;quartic&quot; (p=q=2),
&quot;triweight&quot; (p=2, q=3), &quot;gaussian&quot; or &quot;normal&quot; (gaussian pdf).
</p>
<p>The multivariate kernels are obtained by a
product of unvariate kernels <code class="reqn">K(u_1)...K(u_d)</code>
or by a spherical (radially symmetric) kernel
proportional to <code class="reqn">K(||u||)</code>. (The resulting kernel
is a density, i.e. integrates to 1.)
</p>


<h3>Value</h3>

<p>n x 1 vector of kernel weights
</p>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>Examples</h3>

<pre><code class='language-R'>  kernel.function(0)                         ## default (biweight)
  kernel.function(0, kernel="epanechnikov")  ## epanechnikov
  kernel.function(0, kernel="gaussian")      ## equals dnorm(0)
</code></pre>

<hr>
<h2 id='kgplm'>Generalized partial linear model</h2><span id='topic+kgplm'></span>

<h3>Description</h3>

<p>Fits a generalized partial linear model (kernel-based)
using the (generalized) Speckman estimator or backfitting (in the
generalized case combined with local scoring) for two additive
component functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kgplm(x, t, y, h, family, link,
          b.start=NULL, m.start=NULL, grid = NULL, 
          offset = 0, method = "speckman", sort = TRUE, weights = 1,
          weights.trim = 1, weights.conv = 1, max.iter = 25, eps.conv = 1e-8,
          kernel = "biweight", kernel.product = TRUE, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kgplm_+3A_x">x</code></td>
<td>
<p>n x p matrix, data for linear part</p>
</td></tr>
<tr><td><code id="kgplm_+3A_y">y</code></td>
<td>
<p>n x 1 vector, responses</p>
</td></tr>
<tr><td><code id="kgplm_+3A_t">t</code></td>
<td>
<p>n x q matrix, data for nonparametric part</p>
</td></tr>
<tr><td><code id="kgplm_+3A_h">h</code></td>
<td>
<p>scalar or 1 x q, bandwidth(s)</p>
</td></tr>
<tr><td><code id="kgplm_+3A_family">family</code></td>
<td>
<p>text string, family of distributions (e.g.
&quot;gaussian&quot; or &quot;bernoulli&quot;, see details for <code><a href="#topic+glm.ll">glm.ll</a></code>)</p>
</td></tr>
<tr><td><code id="kgplm_+3A_link">link</code></td>
<td>
<p>text string, link function (depending on family,
see details for <code><a href="#topic+glm.ll">glm.ll</a></code>)</p>
</td></tr>
<tr><td><code id="kgplm_+3A_b.start">b.start</code></td>
<td>
<p>p x 1 vector, start values for linear part</p>
</td></tr>
<tr><td><code id="kgplm_+3A_m.start">m.start</code></td>
<td>
<p>n x 1 vector, start values for nonparametric part</p>
</td></tr>
<tr><td><code id="kgplm_+3A_grid">grid</code></td>
<td>
<p>m x q matrix, where to calculate the nonparametric function (default = t)</p>
</td></tr>
<tr><td><code id="kgplm_+3A_offset">offset</code></td>
<td>
<p>offset</p>
</td></tr>
<tr><td><code id="kgplm_+3A_method">method</code></td>
<td>
<p>&quot;speckman&quot; or &quot;backfit&quot;</p>
</td></tr>
<tr><td><code id="kgplm_+3A_sort">sort</code></td>
<td>
<p>logical, TRUE if data need to be sorted</p>
</td></tr>
<tr><td><code id="kgplm_+3A_weights">weights</code></td>
<td>
<p>binomial weights</p>
</td></tr>
<tr><td><code id="kgplm_+3A_weights.trim">weights.trim</code></td>
<td>
<p>trimming weights for fitting the linear part</p>
</td></tr>
<tr><td><code id="kgplm_+3A_weights.conv">weights.conv</code></td>
<td>
<p>weights for convergence criterion</p>
</td></tr>
<tr><td><code id="kgplm_+3A_max.iter">max.iter</code></td>
<td>
<p>maximal number of iterations</p>
</td></tr>
<tr><td><code id="kgplm_+3A_eps.conv">eps.conv</code></td>
<td>
<p>convergence criterion</p>
</td></tr>
<tr><td><code id="kgplm_+3A_kernel">kernel</code></td>
<td>
<p>text string, see <code><a href="#topic+kernel.function">kernel.function</a></code></p>
</td></tr>
<tr><td><code id="kgplm_+3A_kernel.product">kernel.product</code></td>
<td>
<p>(if p&gt;1) product or spherical kernel</p>
</td></tr>
<tr><td><code id="kgplm_+3A_verbose">verbose</code></td>
<td>
<p>print additional convergence information</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>b</code></td>
<td>
<p>p x 1 vector, linear coefficients</p>
</td></tr>
<tr><td><code>b.cov</code></td>
<td>
<p>p x p matrix, linear coefficients</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>n x 1 vector, nonparametric function estimate</p>
</td></tr>
<tr><td><code>m.grid</code></td>
<td>
<p>m x 1 vector, nonparametric function estimate on grid</p>
</td></tr>
<tr><td><code>it</code></td>
<td>
<p>number of iterations</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>deviance</p>
</td></tr>
<tr><td><code>df.residual</code></td>
<td>
<p>approximate degrees of freedom (residuals)</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>Akaike's information criterion</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>References</h3>

<p>Mueller, M. (2001).
Estimation and testing in generalized partial linear models &ndash; A
comparative study. <em>Statistics and Computing</em>, 11:299&ndash;309.
</p>
<p>Hastie, T. and Tibshirani, R. (1990).
<em>Generalized Additive Models</em>. London: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel.function">kernel.function</a></code>, <code><a href="#topic+kreg">kreg</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## data
  n &lt;- 1000; b &lt;- c(1,-1); rho &lt;- 0.7
  m &lt;- function(t){ 1.5*sin(pi*t) }
  x1 &lt;- runif(n,min=-1,max=1); u  &lt;- runif(n,min=-1,max=1)
  t  &lt;- runif(n,min=-1,max=1); x2 &lt;- round(m(rho*t + (1-rho)*u))
  x  &lt;- cbind(x1,x2)
  y  &lt;- x %*% b + m(t) + rnorm(n)

  ## partial linear model (PLM)
  gh &lt;- kgplm(x,t,y,h=0.25,family="gaussian",link="identity")
  o &lt;- order(t)
  plot(t[o],m(t[o]),type="l",col="green")
  lines(t[o],gh$m[o]); rug(t)

  ## partial linear probit model (GPLM)
  y &lt;- (y&gt;0)
  gh &lt;- kgplm(x,t,y,h=0.25,family="bernoulli",link="probit")

  o &lt;- order(t)
  plot(t[o],m(t[o]),type="l",col="green")
  lines(t[o],gh$m[o]); rug(t)

  ## data with two-dimensional m-function 
  n &lt;- 1000; b &lt;- c(1,-1); rho &lt;- 0.7
  m &lt;- function(t1,t2){ 1.5*sin(pi*t1)+t2 }
  x1 &lt;- runif(n,min=-1,max=1); u  &lt;- runif(n,min=-1,max=1)
  t1 &lt;- runif(n,min=-1,max=1); t2 &lt;- runif(n,min=-1,max=1)
  x2 &lt;- round( m( rho*t1 + (1-rho)*u , t2 ) )
  x  &lt;- cbind(x1,x2); t  &lt;- cbind(t1,t2)
  y  &lt;- x %*% b + m(t1,t2) + rnorm(n)

  ## partial linear model (PLM)
  grid1 &lt;- seq(min(t[,1]),max(t[,1]),length=20)
  grid2 &lt;- seq(min(t[,2]),max(t[,2]),length=25)
  grid  &lt;- create.grid(list(grid1,grid2))

  gh &lt;- kgplm(x,t,y,h=0.5,grid=grid,family="gaussian",link="identity")

  o &lt;- order(grid[,2],grid[,1])
  est.m  &lt;- (matrix(gh$m.grid[o],length(grid1),length(grid2)))
  orig.m &lt;- outer(grid1,grid2,m)
  par(mfrow=c(1,2))
  persp(grid1,grid2,orig.m,main="Original Function",
        theta=30,phi=30,expand=0.5,col="lightblue",shade=0.5)
  persp(grid1,grid2,est.m,main="Estimated Function",
        theta=30,phi=30,expand=0.5,col="lightblue",shade=0.5)
  par(mfrow=c(1,1))
</code></pre>

<hr>
<h2 id='kreg'>Kernel regression</h2><span id='topic+kreg'></span>

<h3>Description</h3>

<p>Calculates a kernel regression estimate (univariate or multivariate).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kreg(x, y, bandwidth = NULL, grid = TRUE, kernel = "biweight",
     product = TRUE, sort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kreg_+3A_x">x</code></td>
<td>
<p>n x d matrix, data</p>
</td></tr>
<tr><td><code id="kreg_+3A_y">y</code></td>
<td>
<p>n x 1 vector, responses</p>
</td></tr>
<tr><td><code id="kreg_+3A_bandwidth">bandwidth</code></td>
<td>
<p>scalar or 1 x d, bandwidth(s)</p>
</td></tr>
<tr><td><code id="kreg_+3A_grid">grid</code></td>
<td>
<p>logical or m x d matrix (where to calculate the regression)</p>
</td></tr>
<tr><td><code id="kreg_+3A_kernel">kernel</code></td>
<td>
<p>text string, see <code><a href="#topic+kernel.function">kernel.function</a></code></p>
</td></tr>
<tr><td><code id="kreg_+3A_product">product</code></td>
<td>
<p>(if d&gt;1) product or spherical kernel</p>
</td></tr>
<tr><td><code id="kreg_+3A_sort">sort</code></td>
<td>
<p>logical, TRUE if data need to be sorted</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimator is calculated by Nadaraya-Watson kernel regression.
Future extension to local linear (d&gt;1) or polynomial (d=1) estimates
is planned. The default bandwidth is computed by Scott's rule of thumb
for kde (adapted to the chosen kernel function).
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>m x d matrix, where regression has been calculated</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>m x 1 vector, regression estimates</p>
</td></tr>
<tr><td><code>bandwidth</code></td>
<td>
<p>bandwidth used for calculation</p>
</td></tr>
<tr><td><code>df.residual</code></td>
<td>
<p>approximate degrees of freedom (residuals)</p>
</td></tr>
<tr><td><code>rearrange</code></td>
<td>
<p>if sort=TRUE, index to rearrange x and y to its original order.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel.function">kernel.function</a></code>, <code><a href="#topic+convol">convol</a></code>, <code><a href="#topic+kde">kde</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  n &lt;- 1000
  x &lt;- rnorm(n)
  m &lt;- sin(x)
  y &lt;- m + rnorm(n)
  plot(x,y,col="gray")
  o &lt;- order(x); lines(x[o],m[o],col="green")
  lines(kreg(x,y),lwd=2)

  ## two-dimensional
  n &lt;- 100
  x &lt;- 6*cbind(runif(n), runif(n))-3
  m &lt;- function(x1,x2){ 4*sin(x1) + x2 }
  y &lt;- m(x[,1],x[,2]) + rnorm(n)
  mh &lt;- kreg(x,y)##,bandwidth=1)

  grid1 &lt;- unique(mh$x[,1])
  grid2 &lt;- unique(mh$x[,2])
  est.m  &lt;- t(matrix(mh$y,length(grid1),length(grid2)))
  orig.m &lt;- outer(grid1,grid2,m)
  par(mfrow=c(1,2))
  persp(grid1,grid2,orig.m,main="Original Function",
        theta=30,phi=30,expand=0.5,col="lightblue",shade=0.5)
  persp(grid1,grid2,est.m,main="Estimated Function",
	theta=30,phi=30,expand=0.5,col="lightblue",shade=0.5)
  par(mfrow=c(1,1))
  
  ## now with normal x, note the boundary problem,
  ## which can be somewhat reduced by a gaussian kernel
  n &lt;- 1000
  x &lt;- cbind(rnorm(n), rnorm(n))
  m &lt;- function(x1,x2){ 4*sin(x1) + x2 }
  y &lt;- m(x[,1],x[,2]) + rnorm(n)
  mh &lt;- kreg(x,y)##,p="gaussian")

  grid1 &lt;- unique(mh$x[,1])
  grid2 &lt;- unique(mh$x[,2])
  est.m  &lt;- t(matrix(mh$y,length(grid1),length(grid2)))
  orig.m &lt;- outer(grid1,grid2,m)
  par(mfrow=c(1,2))
  persp(grid1,grid2,orig.m,main="Original Function",
        theta=30,phi=30,expand=0.5,col="lightblue",shade=0.5)
  persp(grid1,grid2,est.m,main="Estimated Function",
	theta=30,phi=30,expand=0.5,col="lightblue",shade=0.5)
  par(mfrow=c(1,1))
</code></pre>

<hr>
<h2 id='sgplm1'>Generalized partial linear model</h2><span id='topic+sgplm1'></span>

<h3>Description</h3>

<p>Fits a generalized partial linear model (based on smoothing spline)
using the (generalized) Speckman estimator or backfitting (in the
generalized case combined with local scoring) for two additive
component functions.
In contrast to <code><a href="#topic+kgplm">kgplm</a></code>, this function can be used
only for a 1-dimensional nonparametric function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sgplm1(x, t, y, spar, df=4, family, link,
       b.start=NULL, m.start=NULL, grid = NULL, offset = 0, 
       method = "speckman", weights = 1, weights.trim = 1, 
       weights.conv = 1, max.iter = 25, eps.conv = 1e-8,
       verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sgplm1_+3A_x">x</code></td>
<td>
<p>n x p matrix, data for linear part</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_y">y</code></td>
<td>
<p>n x 1 vector, responses</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_t">t</code></td>
<td>
<p>n x 1 matrix, data for nonparametric part</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_spar">spar</code></td>
<td>
<p>scalar smoothing parameter, as in <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code></p>
</td></tr>
<tr><td><code id="sgplm1_+3A_df">df</code></td>
<td>
<p>scalar equivalent number of degrees of freedom (trace of
the smoother matrix), as in <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code></p>
</td></tr>
<tr><td><code id="sgplm1_+3A_family">family</code></td>
<td>
<p>text string, family of distributions (e.g.
&quot;gaussian&quot; or &quot;bernoulli&quot;, see details for <code><a href="#topic+glm.ll">glm.ll</a></code>)</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_link">link</code></td>
<td>
<p>text string, link function (depending on family,
see details for <code><a href="#topic+glm.ll">glm.ll</a></code>)</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_b.start">b.start</code></td>
<td>
<p>p x 1 vector, start values for linear part</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_m.start">m.start</code></td>
<td>
<p>n x 1 vector, start values for nonparametric part</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_grid">grid</code></td>
<td>
<p>m x q matrix, where to calculate the nonparametric function (default = t)</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_offset">offset</code></td>
<td>
<p>offset</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_method">method</code></td>
<td>
<p>&quot;speckman&quot; or &quot;backfit&quot;</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_weights">weights</code></td>
<td>
<p>binomial weights</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_weights.trim">weights.trim</code></td>
<td>
<p>trimming weights for fitting the linear part</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_weights.conv">weights.conv</code></td>
<td>
<p>weights for convergence criterion</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_max.iter">max.iter</code></td>
<td>
<p>maximal number of iterations</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_eps.conv">eps.conv</code></td>
<td>
<p>convergence criterion</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_verbose">verbose</code></td>
<td>
<p>print additional convergence information</p>
</td></tr>
<tr><td><code id="sgplm1_+3A_...">...</code></td>
<td>
<p>further parameters to be passed to <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>b</code></td>
<td>
<p>p x 1 vector, linear coefficients</p>
</td></tr>
<tr><td><code>b.cov</code></td>
<td>
<p>p x p matrix, linear coefficients</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>n x 1 vector, nonparametric function estimate</p>
</td></tr>
<tr><td><code>m.grid</code></td>
<td>
<p>m x 1 vector, nonparametric function estimate on grid</p>
</td></tr>
<tr><td><code>it</code></td>
<td>
<p>number of iterations</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>deviance</p>
</td></tr>
<tr><td><code>df.residual</code></td>
<td>
<p>approximate degrees of freedom (residuals)</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>Akaike's information criterion</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is mainly implemented for comparison. It is not
really optimized for performance, however since it is spline-based, it
should be sufficiently fast. Nevertheless, there might be several
possibilities to improve for speed, in particular I guess that the
sorting that <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code> performs in every iteration
is slowing down the procedure quite a bit.
</p>


<h3>Author(s)</h3>

<p>Marlene Mueller</p>


<h3>References</h3>

<p>Mueller, M. (2001)
Estimation and testing in generalized partial linear models &ndash; A
comparative study. <em>Statistics and Computing,</em> 11:299&ndash;309.
</p>
<p>Hastie, T. and Tibshirani, R. (1990)
<em>Generalized Additive Models.</em> London: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kgplm">kgplm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## generate data
  n &lt;- 1000; b &lt;- c(1,-1); rho &lt;- 0.7
  mm &lt;- function(t){ 1.5*sin(pi*t) }
  x1 &lt;- runif(n,min=-1,max=1); u  &lt;- runif(n,min=-1,max=1)
  t  &lt;- runif(n,min=-1,max=1); x2 &lt;- round(mm(rho*t + (1-rho)*u))
  x  &lt;- cbind(x1,x2)
  y  &lt;- x %*% b + mm(t) + rnorm(n)

  ## fit partial linear model (PLM)
  k.plm &lt;- kgplm(x,t,y,h=0.35,family="gaussian",link="identity")
  s.plm &lt;- sgplm1(x,t,y,spar=0.95,family="gaussian",link="identity")

  o &lt;- order(t)
  ylim &lt;- range(c(mm(t[o]),k.plm$m,s.plm$m),na.rm=TRUE)
  plot(t[o],mm(t[o]),type="l",ylim=ylim)
  lines(t[o],k.plm$m[o], col="green")
  lines(t[o],s.plm$m[o], col="blue")
  rug(t); title("Kernel PLM vs. Spline PLM")

  ## fit partial linear probit model (GPLM)
  y &lt;- (y&gt;0)
  k.gplm &lt;- kgplm(x,t,y,h=0.35,family="bernoulli",link="probit")
  s.gplm &lt;- sgplm1(x,t,y,spar=0.95,family="bernoulli",link="probit")

  o &lt;- order(t)
  ylim &lt;- range(c(mm(t[o]),k.gplm$m,s.gplm$m),na.rm=TRUE)
  plot(t[o],mm(t[o]),type="l",ylim=ylim)
  lines(t[o],k.gplm$m[o], col="green")
  lines(t[o],s.gplm$m[o], col="blue")
  rug(t); title("Kernel GPLM vs. Spline GPLM (Probit)")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
