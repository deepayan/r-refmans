<!DOCTYPE html><html lang="en"><head><title>Help for package gplite</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gplite}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#gplite-package'><p>The 'gplite' package.</p></a></li>
<li><a href='#approx'><p>Approximations to the posterior of the latent values</p></a></li>
<li><a href='#cf'><p>Initialize covariance function</p></a></li>
<li><a href='#gp_draw'><p>Make predictions with a GP model</p></a></li>
<li><a href='#gp_energy'><p>Energy of a GP model</p></a></li>
<li><a href='#gp_fit'><p>Fit a GP model</p></a></li>
<li><a href='#gp_init'><p>Initialize a GP model</p></a></li>
<li><a href='#gp_loo'><p>Model assessment and comparison</p></a></li>
<li><a href='#gp_optim'><p>Optimize hyperparameters of a GP model</p></a></li>
<li><a href='#gp_saveload'><p>Save and load a GP model</p></a></li>
<li><a href='#lik'><p>Initialize likelihood</p></a></li>
<li><a href='#method'><p>Initialize method or type of the model</p></a></li>
<li><a href='#param'><p>Get or set GP model parameters</p></a></li>
<li><a href='#priors'><p>Initialize prior for hyperparameter</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>General Purpose Gaussian Process Modelling</td>
</tr>
<tr>
<td>Version:</td>
<td>0.13.0</td>
</tr>
<tr>
<td>Description:</td>
<td>
  Implements the most common Gaussian process (GP) models using Laplace and
  expectation propagation (EP) approximations, maximum marginal likelihood
  (or posterior) inference for the hyperparameters, and sparse approximations
  for larger datasets.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4.0),</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, methods, Rcpp</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Biarch:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.0</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, ggplot2</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-08-24 06:45:39 UTC; juho</td>
</tr>
<tr>
<td>Author:</td>
<td>Juho Piironen [cre, aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Juho Piironen &lt;juho.t.piironen@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-08-24 07:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='gplite-package'>The 'gplite' package.</h2><span id='topic+gplite-package'></span><span id='topic+gplite'></span>

<h3>Description</h3>

<p><span class="pkg">gplite</span> implements some of the most common Gaussian process (GP) models. 
The package offers tools for integrating out the latent values analytically 
using Laplace or expectation propagation (EP) approximation and for estimating the
hyperparameters based on maximizing the (approximate) marginal likelihood or posterior.
The package also implements some common sparse approximations for larger datasets.
</p>


<h3>Functions</h3>

<p>Here's a list of the most important functions:
</p>

<dl>
<dt><a href="#topic+gp_init">gp_init</a></dt><dd>
<p>Set up the GP model.
</p>
</dd>
<dt><a href="#topic+cf">cf</a>, <a href="#topic+lik">lik</a>, <a href="#topic+method">method</a>, <a href="#topic+approx">approx</a></dt><dd>
<p>Choose the covariance functions, likelihood (observation model), type of the GP
(full or some sparse approximation) and the latent function approximation method
(Laplace, EP).
</p>
</dd>
<dt><a href="#topic+gp_optim">gp_optim</a>, <a href="#topic+gp_fit">gp_fit</a></dt><dd>
<p>Optimize the model hyperparameters, or just fit the model with the current
hyperparameter values.
</p>
</dd>
<dt><a href="#topic+gp_pred">gp_pred</a>, <a href="#topic+gp_draw">gp_draw</a></dt><dd>
<p>Make predictions with the fitted model. Can also be used before fitting to obtain
prior predictive distribution or draws.
</p>
</dd>
<dt><a href="#topic+gp_loo">gp_loo</a>, <a href="#topic+gp_compare">gp_compare</a></dt><dd>
<p>Model assessment and comparison using leave-one-out (LOO) cross-validation.
</p>
</dd>
</dl>


<hr>
<h2 id='approx'>Approximations to the posterior of the latent values</h2><span id='topic+approx'></span><span id='topic+approx_laplace'></span><span id='topic+approx_ep'></span>

<h3>Description</h3>

<p>Functions for initializing the approximation for the latent values, which can
then be passed to <code><a href="#topic+gp_init">gp_init</a></code>.
The supported methods are:
</p>

<dl>
<dt><code>approx_laplace</code></dt><dd><p> Laplace's method, that is, based on local second
order approximation to the log likelihood. For Gaussian likelihood, this means exact inference
(no approximation). </p>
</dd>
<dt><code>approx_ep</code></dt><dd><p> Expectation propagation, EP. Approximates the likelihood by
introducing Gaussian pseudo-data so that the posterior marginals match to the so called
tilted distributions (leave-one-out posterior times the true likelihood factor) as
closely as possible.  Typically more accurate than
Laplace, but slower. </p>
</dd>
</dl>



<h3>Usage</h3>

<pre><code class='language-R'>approx_laplace(maxiter = 30, tol = 1e-04)

approx_ep(damping = 0.9, quad_order = 11, maxiter = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="approx_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations in the Laplace/EP iteration.</p>
</td></tr>
<tr><td><code id="approx_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="approx_+3A_damping">damping</code></td>
<td>
<p>Damping factor for EP. Should be between 0 and 1. Smaller values
typically lead to more stable iterations, but also increase the number of iterations,
and thus make the algorithm slower.</p>
</td></tr>
<tr><td><code id="approx_+3A_quad_order">quad_order</code></td>
<td>
<p>Order of the Gauss-Hermite quadrature used to evaluate the required
tilted moments in EP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The approximation object.
</p>


<h3>References</h3>

<p>Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning.
MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Basic usage
gp &lt;- gp_init(
  cfs = cf_sexp(),
  lik = lik_bernoulli(),
  method = method_fitc(num_inducing = 100),
  approx = approx_ep()
)

</code></pre>

<hr>
<h2 id='cf'>Initialize covariance function</h2><span id='topic+cf'></span><span id='topic+cf_const'></span><span id='topic+cf_lin'></span><span id='topic+cf_sexp'></span><span id='topic+cf_matern32'></span><span id='topic+cf_matern52'></span><span id='topic+cf_nn'></span><span id='topic+cf_periodic'></span><span id='topic+cf_prod'></span><span id='topic++2A.cf'></span>

<h3>Description</h3>

<p>Functions for initializing the covariance functions which can then be passed
to <code><a href="#topic+gp_init">gp_init</a></code>. See section Details for explanation of what covariance
function is what.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cf_const(magn = 1, prior_magn = prior_logunif())

cf_lin(vars = NULL, magn = 1, prior_magn = prior_logunif(), normalize = FALSE)

cf_sexp(
  vars = NULL,
  lscale = 0.3,
  magn = 1,
  prior_lscale = prior_logunif(),
  prior_magn = prior_logunif(),
  normalize = FALSE
)

cf_matern32(
  vars = NULL,
  lscale = 0.3,
  magn = 1,
  prior_lscale = prior_logunif(),
  prior_magn = prior_logunif(),
  normalize = FALSE
)

cf_matern52(
  vars = NULL,
  lscale = 0.3,
  magn = 1,
  prior_lscale = prior_logunif(),
  prior_magn = prior_logunif(),
  normalize = FALSE
)

cf_nn(
  vars = NULL,
  sigma0 = 1,
  sigma = 3,
  magn = 1,
  prior_sigma0 = prior_half_t(),
  prior_sigma = prior_half_t(),
  prior_magn = prior_logunif(),
  normalize = TRUE
)

cf_periodic(
  vars = NULL,
  period = 1,
  cf_base = cf_sexp(),
  prior_period = prior_logunif()
)

cf_prod(...)

## S3 method for class 'cf'
cf1 * cf2
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cf_+3A_magn">magn</code></td>
<td>
<p>Initial value for the magnitude hyperparameter (depicts the magnitude of
the variation captured by the given covariance function).</p>
</td></tr>
<tr><td><code id="cf_+3A_prior_magn">prior_magn</code></td>
<td>
<p>Prior for hypeparameter <code>magn</code>. See <code><a href="#topic+priors">priors</a></code>.</p>
</td></tr>
<tr><td><code id="cf_+3A_vars">vars</code></td>
<td>
<p>Indices of the inputs which are taken into account when calculating this
covariance. If the input matrix has named columns, can also be a vector of column names.
Default is all the inputs.</p>
</td></tr>
<tr><td><code id="cf_+3A_normalize">normalize</code></td>
<td>
<p>Whether to automatically scale and center the inputs for the given
covariance function. Can be useful for inputs with mean and variance far from 0 and 1, respectively.</p>
</td></tr>
<tr><td><code id="cf_+3A_lscale">lscale</code></td>
<td>
<p>Initial value for the length-scale hyperparameter.</p>
</td></tr>
<tr><td><code id="cf_+3A_prior_lscale">prior_lscale</code></td>
<td>
<p>Prior for hyperparameter <code>lscale</code>. See <code><a href="#topic+priors">priors</a></code>.</p>
</td></tr>
<tr><td><code id="cf_+3A_sigma0">sigma0</code></td>
<td>
<p>Prior std for the bias in the neural network covariance function.</p>
</td></tr>
<tr><td><code id="cf_+3A_sigma">sigma</code></td>
<td>
<p>Prior std for the weights in the hidden layers of the neural network
covariance function.</p>
</td></tr>
<tr><td><code id="cf_+3A_prior_sigma0">prior_sigma0</code></td>
<td>
<p>Prior for hyperparameter <code>sigma0</code>. See <code><a href="#topic+priors">priors</a></code>.</p>
</td></tr>
<tr><td><code id="cf_+3A_prior_sigma">prior_sigma</code></td>
<td>
<p>Prior for hyperparameter <code>sigma</code>. See <code><a href="#topic+priors">priors</a></code>.</p>
</td></tr>
<tr><td><code id="cf_+3A_period">period</code></td>
<td>
<p>Period length for the periodic covariance function.</p>
</td></tr>
<tr><td><code id="cf_+3A_cf_base">cf_base</code></td>
<td>
<p>Base covariance function that is used to model the variability within each period
in periodic covariance function.</p>
</td></tr>
<tr><td><code id="cf_+3A_prior_period">prior_period</code></td>
<td>
<p>Prior for hyperparameter <code>period</code>. See <code><a href="#topic+priors">priors</a></code>.</p>
</td></tr>
<tr><td><code id="cf_+3A_...">...</code></td>
<td>
<p>Meaning depends on context. For <code>cf_prod</code> pass in the covariance functions in the product.</p>
</td></tr>
<tr><td><code id="cf_+3A_cf1">cf1</code></td>
<td>
<p>Instance of a covariance function.</p>
</td></tr>
<tr><td><code id="cf_+3A_cf2">cf2</code></td>
<td>
<p>Instance of a covariance function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The supported covariance functions are (see Rasmussen and Williams, 2006):
</p>

<dl>
<dt><code>cf_const</code></dt><dd><p> Constant covariance function. Can be used to model the intercept. </p>
</dd>
<dt><code>cf_lin</code></dt><dd><p> Linear covariance function. Produces linear functions. </p>
</dd>
<dt><code>cf_sexp</code></dt><dd><p> Squared exponential (or exponentiated quadratic, or Gaussian) covariance function.</p>
</dd>
<dt><code>cf_matern32</code></dt><dd><p> Matern nu=3/2 covariance function. </p>
</dd>
<dt><code>cf_matern52</code></dt><dd><p> Matern nu=5/2 covariance function. </p>
</dd>
<dt><code>cf_nn</code></dt><dd><p> Neural network covariance function. </p>
</dd>
<dt><code>cf_periodic</code></dt><dd><p> Periodic covariance function. The periodicity is achieved by mapping the
original inputs through sine and cosine functions, and then applying the base kernel in this new space.</p>
</dd>
<dt><code>cf_prod</code></dt><dd><p> Product of two or more covariance functions. </p>
</dd>
</dl>



<h3>Value</h3>

<p>The covariance function object.
</p>


<h3>References</h3>

<p>Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning. MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate some toy data
set.seed(1242)
n &lt;- 50
x &lt;- matrix(rnorm(n * 3), nrow = n)
f &lt;- sin(x[, 1]) + 0.5 * x[, 2]^2 + x[, 3]
y &lt;- f + 0.5 * rnorm(n)
x &lt;- data.frame(x1 = x[, 1], x2 = x[, 2], x3 = x[, 3])

# Basic usage (single covariance function)
cf &lt;- cf_sexp()
lik &lt;- lik_gaussian()
gp &lt;- gp_init(cf, lik)
gp &lt;- gp_optim(gp, x, y)
plot(gp_pred(gp, x)$mean, y)

# More than one covariance function; one for x1 and x2, and another one for x3
cf1 &lt;- cf_sexp(c("x1", "x2"))
cf2 &lt;- cf_lin("x3")
cfs &lt;- list(cf1, cf2)
lik &lt;- lik_gaussian()
gp &lt;- gp_init(cfs, lik)
gp &lt;- gp_optim(gp, x, y, maxiter = 500)
plot(gp_pred(gp, x)$mean, y)
plot(x[, 3], gp_pred(gp, x, cfind = 2)$mean) # plot effect w.r.t x3 only


</code></pre>

<hr>
<h2 id='gp_draw'>Make predictions with a GP model</h2><span id='topic+gp_draw'></span><span id='topic+pred'></span><span id='topic+gp_pred'></span>

<h3>Description</h3>

<p>Function <code>gp_pred</code> can be used to make analytic predictions for the latent function
values at test points, whereas <code>gp_draw</code>
can be used to draw from the predictive distribution (or from the prior if the GP has
not been fitted yet.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gp_draw(
  gp,
  xnew,
  draws = NULL,
  transform = TRUE,
  target = FALSE,
  marginal = FALSE,
  cfind = NULL,
  jitter = NULL,
  seed = NULL,
  ...
)

gp_pred(
  gp,
  xnew,
  var = FALSE,
  quantiles = NULL,
  transform = FALSE,
  cfind = NULL,
  jitter = NULL,
  quad_order = 15,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gp_draw_+3A_gp">gp</code></td>
<td>
<p>A GP model object.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_xnew">xnew</code></td>
<td>
<p>N-by-d matrix of input values (N is the number of test points and d
the input dimension).
Can also be a vector of length N if the model has only a single input.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_draws">draws</code></td>
<td>
<p>Number of draws to generate from the predictive distribution for the
latent values.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_transform">transform</code></td>
<td>
<p>Whether to transform the draws of latent values to the same scale
as the target y, that is, through the response (or inverse-link) function.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_target">target</code></td>
<td>
<p>If TRUE, draws values for the target variable <code>y</code> instead of the latent
function values.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_marginal">marginal</code></td>
<td>
<p>If TRUE, then draws for each test point are only marginally correct, but the
covariance structure between test points is not retained. However, this will make the sampling
considerably faster in some cases, and can be useful if one is interested only in looking
at the marginal predictive distributions for a large number of test locations
(for example, in posterior predictive checking).</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_cfind">cfind</code></td>
<td>
<p>Indices of covariance functions to be used in the prediction. By default uses
all covariance functions.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_jitter">jitter</code></td>
<td>
<p>Magnitude of diagonal jitter for covariance matrices for numerical stability.
Default is 1e-6.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_seed">seed</code></td>
<td>
<p>Random seed for draws.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_...">...</code></td>
<td>
<p>Additional parameters that might be needed. For example <code>offset</code> or 
keyword <code>trials</code> for binomial and beta-binomial likelihoods.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_var">var</code></td>
<td>
<p>Whether to compute the predictive variances along with predictive mean.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_quantiles">quantiles</code></td>
<td>
<p>Vector of probabilities between 0 and 1 indicating which quantiles are to
be predicted.</p>
</td></tr>
<tr><td><code id="gp_draw_+3A_quad_order">quad_order</code></td>
<td>
<p>Quadrature order in order to compute the mean and variance on
the transformed scale.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>gp_pred</code> returns a list with fields giving the predictive mean, variance and
quantiles (the last two are computed only if requested). <code>gp_draw</code> returns an N-by-draws
matrix of random draws from the predictive distribution, where N is the number of test points.
</p>


<h3>References</h3>

<p>Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning. MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate some toy data
set.seed(1242)
n &lt;- 50
x &lt;- matrix(rnorm(n * 3), nrow = n)
f &lt;- sin(x[, 1]) + 0.5 * x[, 2]^2 + x[, 3]
y &lt;- f + 0.5 * rnorm(n)
x &lt;- data.frame(x1 = x[, 1], x2 = x[, 2], x3 = x[, 3])

# More than one covariance function; one for x1 and x2, and another one for x3
cf1 &lt;- cf_nn(c("x1", "x2"), prior_sigma0 = prior_half_t(df = 4, scale = 2))
cf2 &lt;- cf_sexp("x3")
cfs &lt;- list(cf1, cf2)
lik &lt;- lik_gaussian()
gp &lt;- gp_init(cfs, lik)
gp &lt;- gp_optim(gp, x, y, maxiter = 500)

# plot the predictions with respect to x1, when x2 = x3 = 0
xt &lt;- cbind(x1 = seq(-3, 3, len = 50), x2 = 0, x3 = 0)
pred &lt;- gp_pred(gp, xt)
plot(xt[, "x1"], pred$mean, type = "l")

# draw from the predictive distribution
xt &lt;- cbind(x1 = seq(-3, 3, len = 50), x2 = 0, x3 = 0)
draws &lt;- gp_draw(gp, xt, draws = 100)
plot(xt[, "x1"], draws[, 1], type = "l")
for (i in 2:50) {
  lines(xt[, "x1"], draws[, i])
}

# plot effect with respect to x3 only
xt &lt;- cbind("x3" = seq(-3, 3, len = 50))
pred &lt;- gp_pred(gp, xt, cfind = 2)
plot(xt, pred$mean, type = "l")


</code></pre>

<hr>
<h2 id='gp_energy'>Energy of a GP model</h2><span id='topic+gp_energy'></span>

<h3>Description</h3>

<p>Returns the energy (negative log marginal likelihood) of a fitted GP model with the
current hyperparameters. The result is exact for the Gaussian likelihood and
dependent on the <code><a href="#topic+approx">approx</a></code> for other cases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gp_energy(gp, include_prior = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gp_energy_+3A_gp">gp</code></td>
<td>
<p>The fitted GP model.</p>
</td></tr>
<tr><td><code id="gp_energy_+3A_include_prior">include_prior</code></td>
<td>
<p>Whether to add log density of the prior to the result (in which case
the result is -(log marginal likelihood + log prior))</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The energy value (negative log marginal likelihood).
</p>


<h3>References</h3>

<p>Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning. MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Generate some toy data
set.seed(1242)
n &lt;- 500
x &lt;- matrix(rnorm(n * 3), nrow = n)
f &lt;- sin(x[, 1]) + 0.5 * x[, 2]^2 + x[, 3]
y &lt;- f + 0.5 * rnorm(n)
x &lt;- data.frame(x1 = x[, 1], x2 = x[, 2], x3 = x[, 3])

# Basic usage
gp &lt;- gp_init(cf_sexp(), lik_gaussian())
gp &lt;- gp_fit(gp, x, y)
e &lt;- gp_energy(gp)


</code></pre>

<hr>
<h2 id='gp_fit'>Fit a GP model</h2><span id='topic+gp_fit'></span>

<h3>Description</h3>

<p>Function <code>gp_fit</code> fits a GP model with the current hyperparameters.
Notice that this function does not optimize the hyperparameters in any way,
but only finds the analytical posterior approximation (depending on chosen
<code><a href="#topic+approx">approx</a></code>) for the latent values with the current hyperparameters.
For optimizing the hyperparameter
values, see <code>gp_optim</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gp_fit(gp, x, y, trials = NULL, offset = NULL, jitter = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gp_fit_+3A_gp">gp</code></td>
<td>
<p>The gp model object to be fitted.</p>
</td></tr>
<tr><td><code id="gp_fit_+3A_x">x</code></td>
<td>
<p>n-by-d matrix of input values (n is the number of observations and d the input dimension).
Can also be a vector of length n if the model has only a single input.</p>
</td></tr>
<tr><td><code id="gp_fit_+3A_y">y</code></td>
<td>
<p>Vector of n output (target) values.</p>
</td></tr>
<tr><td><code id="gp_fit_+3A_trials">trials</code></td>
<td>
<p>Vector of length n giving the number of trials for each observation in binomial
(and beta binomial) model.</p>
</td></tr>
<tr><td><code id="gp_fit_+3A_offset">offset</code></td>
<td>
<p>Vector of constant values added to the latent values f_i (i = 1,...,n).
For Poisson models, this is the logarithm of the exposure time in each observation.</p>
</td></tr>
<tr><td><code id="gp_fit_+3A_jitter">jitter</code></td>
<td>
<p>Magnitude of diagonal jitter for covariance matrices for numerical stability.
Default is 1e-6.</p>
</td></tr>
<tr><td><code id="gp_fit_+3A_...">...</code></td>
<td>
<p>Currently ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated GP model object.
</p>


<h3>References</h3>

<p>Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning.
MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate some toy data
set.seed(32004)
n &lt;- 150
sigma &lt;- 0.1
x &lt;- rnorm(n)
ycont &lt;- sin(3 * x) * exp(-abs(x)) + rnorm(n) * sigma
y &lt;- rep(0, n)
y[ycont &gt; 0] &lt;- 1
trials &lt;- rep(1, n)

# Fit the model using Laplace approximation (with the specified hyperparameters)
cf &lt;- cf_sexp(lscale = 0.3, magn = 3)
gp &lt;- gp_init(cf, lik_binomial())
gp &lt;- gp_fit(gp, x, y, trials = trials)


</code></pre>

<hr>
<h2 id='gp_init'>Initialize a GP model</h2><span id='topic+gp_init'></span>

<h3>Description</h3>

<p>Initializes a GP model with given covariance function(s) and likelihood. 
The model can then be fitted using <code><a href="#topic+gp_fit">gp_fit</a></code>. For hyperparameter 
optimization, see <code><a href="#topic+gp_optim">gp_optim</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gp_init(
  cfs = cf_sexp(),
  lik = lik_gaussian(),
  method = method_full(),
  approx = approx_laplace()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gp_init_+3A_cfs">cfs</code></td>
<td>
<p>The covariance function(s). Either a single covariance function 
or a list of them. See <code><a href="#topic+cf">cf</a></code>.</p>
</td></tr>
<tr><td><code id="gp_init_+3A_lik">lik</code></td>
<td>
<p>Likelihood (observation model). See <code><a href="#topic+lik">lik</a></code>.</p>
</td></tr>
<tr><td><code id="gp_init_+3A_method">method</code></td>
<td>
<p>Method for approximating the covariance function.
See <code><a href="#topic+method">method</a></code>.</p>
</td></tr>
<tr><td><code id="gp_init_+3A_approx">approx</code></td>
<td>
<p>Approximate inference method for Gaussian approximation
for the posterior of the latent values. See <code><a href="#topic+approx">approx</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A GP model object that can be passed to other functions, for example 
when optimizing the hyperparameters or making predictions.
</p>


<h3>References</h3>

<p>Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning.
MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Full exact GP with Gaussian likelihood
gp &lt;- gp_init(
  cfs = cf_sexp(),
  lik = lik_gaussian(),
  method = method_full()
)

# Binary classification model with EP approximation for the latent values
# and FITC sparse approximation to facilitate large datasets
gp &lt;- gp_init(
  cfs = cf_sexp(),
  lik = lik_bernoulli(),
  approx = approx_ep(),
  method = method_fitc(num_inducing = 100)
)


</code></pre>

<hr>
<h2 id='gp_loo'>Model assessment and comparison</h2><span id='topic+gp_loo'></span><span id='topic+gp_compare'></span>

<h3>Description</h3>

<p>Function <code>gp_loo</code> computes the approximate leave-one-out (LOO)
cross-validation statistics for the given GP model with the current
hyperparameters.
Function <code>gp_compare</code> estimates the difference in the expected
predictive accuracy of two or more GP models given their LOO statistics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gp_loo(
  gp,
  x,
  y,
  quadrature = TRUE,
  quad_order = 11,
  draws = 4000,
  jitter = NULL,
  seed = NULL,
  ...
)

gp_compare(..., ref = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gp_loo_+3A_gp">gp</code></td>
<td>
<p>The gp model object to be fitted.</p>
</td></tr>
<tr><td><code id="gp_loo_+3A_x">x</code></td>
<td>
<p>n-by-d matrix of input values (n is the number of observations and d the input dimension).
Can also be a vector of length n if the model has only a single input.</p>
</td></tr>
<tr><td><code id="gp_loo_+3A_y">y</code></td>
<td>
<p>Vector of n output (target) values.</p>
</td></tr>
<tr><td><code id="gp_loo_+3A_quadrature">quadrature</code></td>
<td>
<p>Whether to use deterministic Gauss-Hermite quadrature to estimate the
required integrals. If FALSE, then Monte Carlo estimate is used.</p>
</td></tr>
<tr><td><code id="gp_loo_+3A_quad_order">quad_order</code></td>
<td>
<p>Order of the numerical quadrature
(only applicable if <code>quadrature=TRUE</code>).</p>
</td></tr>
<tr><td><code id="gp_loo_+3A_draws">draws</code></td>
<td>
<p>Number of posterior draws to estimate the required integrals (only applicable
if <code>quadrature=FALSE</code>).</p>
</td></tr>
<tr><td><code id="gp_loo_+3A_jitter">jitter</code></td>
<td>
<p>Magnitude of diagonal jitter for covariance matrices for numerical stability.
Default is 1e-6.</p>
</td></tr>
<tr><td><code id="gp_loo_+3A_seed">seed</code></td>
<td>
<p>Random seed.</p>
</td></tr>
<tr><td><code id="gp_loo_+3A_...">...</code></td>
<td>
<p>For <code>gp_compare</code>, LOO statistics for the models to compare. For
<code>gp_loo</code>, possible additional data that is required for LOO predictions (for example,
argument <code>trials</code> in case of binomial likelihood).</p>
</td></tr>
<tr><td><code id="gp_loo_+3A_ref">ref</code></td>
<td>
<p>Index of the model against which to compare the other models (pairwise
comparison for LOO difference). If not given, then the model with the best LOO is
used as the reference for comparisons.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>gp_loo</code> returns a list with LOO statistics.
<code>gp_compare</code> returns a matrix with comparison statistics (LOO differences 
and stardard errors in the estimates).
</p>


<h3>References</h3>

<p>Vehtari A., Mononen T., Tolvanen V., Sivula T. and Winther O. (2016).
Bayesian Leave-One-Out Cross-Validation Approximations for Gaussian Latent
Variable Models. Journal of Machine Learning Research 17(103):1-38.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate some toy data
set.seed(32004)
n &lt;- 50
sigma &lt;- 0.1
x &lt;- rnorm(n)
ycont &lt;- sin(3 * x) * exp(-abs(x)) + rnorm(n) * sigma
y &lt;- rep(0, n)
y[ycont &gt; 0] &lt;- 1
trials &lt;- rep(1, n)

# Set up two models
gp1 &lt;- gp_init(cf_sexp(), lik_binomial())
gp2 &lt;- gp_init(cf_matern32(), lik_binomial())

# Optimize
gp1 &lt;- gp_optim(gp1, x, y, trials = trials)
gp2 &lt;- gp_optim(gp2, x, y, trials = trials)

# Compare
loo1 &lt;- gp_loo(gp1, x, y, trials = trials)
loo2 &lt;- gp_loo(gp2, x, y, trials = trials)
gp_compare(loo1, loo2)


</code></pre>

<hr>
<h2 id='gp_optim'>Optimize hyperparameters of a GP model</h2><span id='topic+gp_optim'></span>

<h3>Description</h3>

<p>This function can be used to optimize the hyperparameters of the model to the maximum
marginal likelihood (or maximum marginal posterior if priors are used), using Nelder-Mead
algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gp_optim(
  gp,
  x,
  y,
  tol = 1e-04,
  tol_param = 0.1,
  maxiter = 500,
  restarts = 1,
  verbose = TRUE,
  warnings = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gp_optim_+3A_gp">gp</code></td>
<td>
<p>The gp model object to be fitted.</p>
</td></tr>
<tr><td><code id="gp_optim_+3A_x">x</code></td>
<td>
<p>n-by-d matrix of input values (n is the number of observations and d the input
dimension). Can also be a vector of length n if the model has only a single input.</p>
</td></tr>
<tr><td><code id="gp_optim_+3A_y">y</code></td>
<td>
<p>Vector of n output (target) values.</p>
</td></tr>
<tr><td><code id="gp_optim_+3A_tol">tol</code></td>
<td>
<p>Relative change in the objective function value (marginal log posterior) after 
which the optimization is terminated. This will be passed to the function stats::optim 
as a convergence criterion.</p>
</td></tr>
<tr><td><code id="gp_optim_+3A_tol_param">tol_param</code></td>
<td>
<p>After the optimizer (Nelder-Mead) has terminated, the found hyperparameter
values will be checked for convergence within tolerance tol_param. More precisely, if we perturb
any of the hyperparameters by the amount tol_param or -tol_param, then the resulting
log posterior must be smaller than the value with the found hyperparameter values. If not,
then the optimizer will automatically attempt a restart (see argument restarts). Note:
tol_param will be applied for the logarithms of the parameters (e.g. log length-scale), not
for the native parameter values.</p>
</td></tr>
<tr><td><code id="gp_optim_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="gp_optim_+3A_restarts">restarts</code></td>
<td>
<p>Number of possible restarts during optimization. The Nelder-Mead 
iteration can sometimes terminate prematurely before a local optimum is found, and
this argument can be used to specify how many times the optimization is allowed to
restart from where it left when Nelder-Mead terminated. By setting restarts &gt; 0, 
one can often find local optimum without having to call gp_optim several times.
Note: usually there is no need to allow more than a few (say 1-3) restarts; 
if the optimization does not converge with a few restarts, then one usually 
must try to reduce argument tol in order to achieve convergence. If this does not help
either, then the optimization problem is usually ill-conditioned somehow.</p>
</td></tr>
<tr><td><code id="gp_optim_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, then some information about the progress of the optimization is
printed to the console.</p>
</td></tr>
<tr><td><code id="gp_optim_+3A_warnings">warnings</code></td>
<td>
<p>Whether to print out some potential warnings (such as maximum number of
iterations reached) during the optimization.</p>
</td></tr>
<tr><td><code id="gp_optim_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code><a href="#topic+gp_fit">gp_fit</a></code> that are needed
in the fitting process, for example <code>trials</code> in the case of binomial likelihood.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated GP model object.
</p>


<h3>References</h3>

<p>Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning. MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate some toy data
set.seed(1242)
n &lt;- 50
x &lt;- matrix(rnorm(n * 3), nrow = n)
f &lt;- sin(x[, 1]) + 0.5 * x[, 2]^2 + x[, 3]
y &lt;- f + 0.5 * rnorm(n)
x &lt;- data.frame(x1 = x[, 1], x2 = x[, 2], x3 = x[, 3])

# Basic usage
cf &lt;- cf_sexp()
lik &lt;- lik_gaussian()
gp &lt;- gp_init(cf, lik)
gp &lt;- gp_optim(gp, x, y)


</code></pre>

<hr>
<h2 id='gp_saveload'>Save and load a GP model</h2><span id='topic+gp_saveload'></span><span id='topic+gp_save'></span><span id='topic+gp_load'></span>

<h3>Description</h3>

<p>Convenience functions for saving and loading GP models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gp_save(gp, filename)

gp_load(filename)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gp_saveload_+3A_gp">gp</code></td>
<td>
<p>The gp model object to be saved.</p>
</td></tr>
<tr><td><code id="gp_saveload_+3A_filename">filename</code></td>
<td>
<p>Where to save or load from.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>gp_load</code> returns the loaded GP model object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
gp &lt;- gp_init()

# fit the model (skipped here)

# save the model
filename &lt;- file.path(tempdir(), 'gp.rda')
gp_save(gp, filename)

# load the model and remove the file
gp &lt;- gp_load(filename)
file.remove(filename)


</code></pre>

<hr>
<h2 id='lik'>Initialize likelihood</h2><span id='topic+lik'></span><span id='topic+lik_gaussian'></span><span id='topic+lik_bernoulli'></span><span id='topic+lik_binomial'></span><span id='topic+lik_betabinom'></span><span id='topic+lik_poisson'></span>

<h3>Description</h3>

<p>Functions for initializing the likelihood (observation model) which can then be passed to <code><a href="#topic+gp_init">gp_init</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lik_gaussian(sigma = 0.5, prior_sigma = prior_logunif())

lik_bernoulli(link = "logit")

lik_binomial(link = "logit")

lik_betabinom(link = "logit", phi = 1, prior_phi = prior_logunif())

lik_poisson(link = "log")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lik_+3A_sigma">sigma</code></td>
<td>
<p>Initial value for the noise standard deviation.</p>
</td></tr>
<tr><td><code id="lik_+3A_prior_sigma">prior_sigma</code></td>
<td>
<p>Prior for hyperparameter <code>sigma</code>. See <code><a href="#topic+priors">priors</a></code>.</p>
</td></tr>
<tr><td><code id="lik_+3A_link">link</code></td>
<td>
<p>Link function if the likelihood supports non-identity links. See Details for
information about possible links for each likelihood.</p>
</td></tr>
<tr><td><code id="lik_+3A_phi">phi</code></td>
<td>
<p>The over dispersion parameter for beta binomial likelihood.</p>
</td></tr>
<tr><td><code id="lik_+3A_prior_phi">prior_phi</code></td>
<td>
<p>Prior for hyperparameter <code>phi</code>. See <code><a href="#topic+priors">priors</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The supported likelihoods are:
</p>

<dl>
<dt><code>lik_gaussian</code></dt><dd><p>Gaussian likelihood. Has no links (uses identity link).</p>
</dd>
<dt><code>lik_bernoulli</code></dt><dd><p>Bernoulli likelihood. Possible links: 'logit' or 'probit'.</p>
</dd>
<dt><code>lik_binomial</code></dt><dd><p>Binomial likelihood. Possible links: 'logit' or 'probit'.</p>
</dd>
<dt><code>lik_betabinom</code></dt><dd><p>Beta binomial likelihood. Possible links: 'logit' or 'probit'.</p>
</dd>
<dt><code>lik_poisson</code></dt><dd><p>Poisson likelihood. Possible links: 'log'.</p>
</dd>
</dl>



<h3>Value</h3>

<p>The likelihood object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Basic usage
cf &lt;- cf_sexp()
lik &lt;- lik_binomial()
gp &lt;- gp_init(cf, lik)


</code></pre>

<hr>
<h2 id='method'>Initialize method or type of the model</h2><span id='topic+method'></span><span id='topic+method_full'></span><span id='topic+method_fitc'></span><span id='topic+method_rf'></span>

<h3>Description</h3>

<p>Functions for initializing the method or type of the model, which can then be passed to
<code><a href="#topic+gp_init">gp_init</a></code>.
The supported methods are:
</p>

<dl>
<dt><code>method_full</code></dt><dd><p> Full GP, so full exact covariance function is used, meaning
that the inference will be for the <code>n</code> latent
function values (fitting time scales cubicly in <code>n</code>).</p>
</dd>
<dt><code>method_fitc</code></dt><dd><p> Fully independent training (and test) conditional,
or FITC, approximation (see Quiñonero-Candela and Rasmussen, 2005;
Snelson and Ghahramani, 2006).
The fitting time scales <code>O(n*m^2)</code>, where n is the number of data points and
m the number of inducing points <code>num_inducing</code>.
The inducing point locations are chosen using the k-means algorithm.
</p>
</dd>
<dt><code>method_rf</code></dt><dd><p> Random features, that is, linearized GP.
Uses random features (or basis functions) for approximating the covariance function,
which means the inference
time scales cubicly in the number of approximating basis functions <code>num_basis</code>.
For stationary covariance functions random Fourier features (Rahimi and Recht, 2007)
is used, and for non-stationary kernels using case specific method when possible
(for example, drawing the hidden layer parameters randomly for <code>cf_nn</code>). For
<code>cf_const</code> and <code>cf_lin</code> this means using standard linear model, and the
inference is performed on the weight space (not in the function space). Thus if
the model is linear (only <code>cf_const</code> and <code>cf_lin</code> are used), this will give
a potentially huge speed-up if the number of features is considerably smaller than
the number of data points.</p>
</dd>
</dl>



<h3>Usage</h3>

<pre><code class='language-R'>method_full()

method_fitc(
  inducing = NULL,
  num_inducing = 100,
  bin_along = NULL,
  bin_count = 10,
  seed = 12345
)

method_rf(num_basis = 400, seed = 12345)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="method_+3A_inducing">inducing</code></td>
<td>
<p>Inducing points to use. If not given, then <code>num_inducing</code>
points will be placed in the input space using a clustering algorithm.</p>
</td></tr>
<tr><td><code id="method_+3A_num_inducing">num_inducing</code></td>
<td>
<p>Number of inducing points for the approximation. Will be ignored
if the inducing points are given by the user.</p>
</td></tr>
<tr><td><code id="method_+3A_bin_along">bin_along</code></td>
<td>
<p>Either an index or a name of the input variable along which to bin the
values before placing the inducing inputs. For example, if <code>bin_along=3</code>, then the
input data is divided into <code>bin_count</code> bins along 3rd input variable, and each bin
will have the same number inducing points (or as close as possible). This can sometimes
be useful to ensure that inducing points are spaced evenly with respect to some particular
variable, for example time in spatio-temporal models.</p>
</td></tr>
<tr><td><code id="method_+3A_bin_count">bin_count</code></td>
<td>
<p>The number of bins to use if <code>bin_along</code> given. Has effect only if
<code>bin_along</code> is given.</p>
</td></tr>
<tr><td><code id="method_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducible results.</p>
</td></tr>
<tr><td><code id="method_+3A_num_basis">num_basis</code></td>
<td>
<p>Number of basis functions for the approximation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The method object.
</p>


<h3>References</h3>

<p>Rahimi, A. and Recht, B. (2008). Random features for large-scale kernel machines.
In Advances in Neural Information Processing Systems 20.
</p>
<p>Quiñonero-Candela, J. and Rasmussen, C. E (2005). A unifying view of sparse approximate
Gaussian process regression. Journal of Machine Learning Research 6:1939-1959.
</p>
<p>Snelson, E. and Ghahramani, Z. (2006). Sparse Gaussian processes using pseudo-inputs.
In Advances in Neural Information Processing Systems 18.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

#' # Generate some toy data
# NOTE: this is so small dataset that in reality there would be no point
# use sparse approximation here; we use this small dataset only to make this
# example run fast
set.seed(1242)
n &lt;- 50
x &lt;- matrix(rnorm(n * 3), nrow = n)
f &lt;- sin(x[, 1]) + 0.5 * x[, 2]^2 + x[, 3]
y &lt;- f + 0.5 * rnorm(n)
x &lt;- data.frame(x1 = x[, 1], x2 = x[, 2], x3 = x[, 3])

# Full exact GP with Gaussian likelihood
gp &lt;- gp_init(cf_sexp())
gp &lt;- gp_optim(gp, x, y)

# Approximate solution using random features (here we use a very small 
# number of random features only to make this example run fast)
gp &lt;- gp_init(cf_sexp(), method = method_rf(num_basis = 30))
gp &lt;- gp_optim(gp, x, y)

# Approximate solution using FITC (here we use a very small 
# number of incuding points only to make this example run fast)
gp &lt;- gp_init(cf_sexp(), method = method_fitc(num_inducing = 10))
gp &lt;- gp_optim(gp, x, y)

</code></pre>

<hr>
<h2 id='param'>Get or set GP model parameters</h2><span id='topic+param'></span><span id='topic+get_param'></span><span id='topic+set_param'></span>

<h3>Description</h3>

<p><code>get_param</code> returns the current hyperparameters of the GP model in a vector.
<code>set_param</code> can be used to set the parameters. Note that these functions
are intended mainly for internal usage, and there is typically
no need to use these functions directly but instead create a new GP model using
<code>gp_init</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_param(object, ...)

set_param(object, param, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="param_+3A_object">object</code></td>
<td>
<p>The model object.</p>
</td></tr>
<tr><td><code id="param_+3A_...">...</code></td>
<td>
<p>Ignored currently.</p>
</td></tr>
<tr><td><code id="param_+3A_param">param</code></td>
<td>
<p>The parameters to be set. Call first <code>get_param</code> to see the order in which the parameters should be given for a particular model. Notice that all positive parameters should be given in a log-scale.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>get_param</code> returns the current hyperparameters and <code>set_param</code> the GP model structure with the new parameter values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Set up some model
gp &lt;- gp_init(cf = cf_sexp(), lik = lik_gaussian())

# print out to see the parameter ordering
param &lt;- get_param(gp)
print(param)

# set some new values
param_new &lt;- log(c(0.1, 0.8, 0.3))
names(param_new) &lt;- names(param)
gp &lt;- set_param(gp, param_new)

# check the result
print(get_param(gp))


</code></pre>

<hr>
<h2 id='priors'>Initialize prior for hyperparameter</h2><span id='topic+priors'></span><span id='topic+prior_fixed'></span><span id='topic+prior_logunif'></span><span id='topic+prior_lognormal'></span><span id='topic+prior_half_t'></span>

<h3>Description</h3>

<p>Functions for initializing hyperparameter priors which can then be passed
to <code><a href="#topic+gp_init">gp_init</a></code>. See section Details for the prior explanations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prior_fixed()

prior_logunif()

prior_lognormal(loc = 0, scale = 1)

prior_half_t(df = 1, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="priors_+3A_loc">loc</code></td>
<td>
<p>Location parameter of the distribution</p>
</td></tr>
<tr><td><code id="priors_+3A_scale">scale</code></td>
<td>
<p>Scale parameter of the distribution</p>
</td></tr>
<tr><td><code id="priors_+3A_df">df</code></td>
<td>
<p>Degrees of freedom</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The supported priors are:
</p>

<dl>
<dt><code>prior_fixed</code></dt><dd><p> The hyperparameter is fixed to its initial value,
and is not optimized by <code>gp_optim</code>. </p>
</dd>
<dt><code>prior_logunif</code></dt><dd><p> Improper uniform prior on the log of the parameter. </p>
</dd>
<dt><code>prior_lognormal</code></dt><dd><p> Log-normal prior (Gaussian prior on the logarithm of the parameter). </p>
</dd>
<dt><code>prior_half_t</code></dt><dd><p> Half Student-t prior for a positive parameter. </p>
</dd>
</dl>



<h3>Value</h3>

<p>The hyperprior object.
</p>


<h3>References</h3>

<p>Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning. MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Quasi-periodic covariance function, with fixed period
cf1 &lt;- cf_periodic(
  period = 5,
  prior_period = prior_fixed(),
  cf_base = cf_sexp(lscale = 2)
)
cf2 &lt;- cf_sexp(lscale = 40)
cf &lt;- cf1 * cf2
gp &lt;- gp_init(cf)

# draw from the prior
set.seed(104930)
xt &lt;- seq(-10, 10, len = 500)
plot(xt, gp_draw(gp, xt), type = "l")


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
