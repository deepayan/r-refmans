<!DOCTYPE html><html><head><title>Help for package PwrGSD</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PwrGSD}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#agghaz'><p>Aggregated Hazard</p></a></li>
<li><a href='#as.boundaries'><p>Convert a &quot;PwrGSD&quot; object to a &quot;boundaries&quot; object</p></a></li>
<li><a href='#CDFOR2LRR'><p>Convert CDF Odds Ratio to Logged Relative Risks</p></a></li>
<li><a href='#CondPower'><p>Conditional type I and type II error probabilities given current value of the test</p>
statistic</a></li>
<li><a href='#cpd.PwrGSD'><p>Create a skeleton compound PwrGSD object</p></a></li>
<li><a href='#CRRtoRR'><p>Cumulative-risk ratios to risk ratios</p></a></li>
<li><a href='#CY2TOShaz'><p>Calender year rates to Study Year Rates</p></a></li>
<li><a href='#detail'>
<p>Function to extract the 'detail' component from a <code>PwrGSD</code> object</p></a></li>
<li><a href='#DX'><p>A utility function for forming differences</p></a></li>
<li><a href='#Elements'><p>Create a subset of a &quot;cpd.PwrGSD&quot; object</p></a></li>
<li><a href='#EX1gXK'>
<p>A function for computing the bias adjusted point estimate for a</p>
statistic observed to cross the efficacy boundary.</a></li>
<li><a href='#GrpSeqBnds'><p>Computes efficacy and futility boundaries</p></a></li>
<li><a href='#gsd.dens'>
<p>A function for computing the probability density for the group</p>
sequentially monitored test statistic.</a></li>
<li><a href='#Haybittle'><p>The Haybittle method of Boundary Construction</p></a></li>
<li><a href='#IntSurvDiff'><p>Weighted Integrated Survival function test</p></a></li>
<li><a href='#LanDemets'><p>The Lan-Demets method of Boundary Construction</p></a></li>
<li><a href='#lookup'><p>Lookup values for a piecewise constant function</p></a></li>
<li><a href='#lung'><p>Mayo Clinic Lung Cancer Data</p></a></li>
<li><a href='#mystack'><p>Stack a dataset</p></a></li>
<li><a href='#mysurvfit'><p>My Survfit</p></a></li>
<li><a href='#ObrienFleming'><p>The O'Brien-Fleming Alpha Spending Function</p></a></li>
<li><a href='#plot.cpd.PwrGSD'><p>Plot Method for cpd.PwrGSD objects</p></a></li>
<li><a href='#Pocock'><p>The Pocock Alpha Spending Function</p></a></li>
<li><a href='#Pow'><p>The Wang-Tsiatis Power Alpha Spending Function</p></a></li>
<li><a href='#Power'><p>Extract the Power results</p></a></li>
<li><a href='#PwrGSD'><p>Calculate Power in a Group Sequential Design</p></a></li>
<li><a href='#RCM2RR'><p>Relative cumulative mortality to Relative Risk</p></a></li>
<li><a href='#RR2RCM'><p>Relative risk to Relative Cumulative Mortality</p></a></li>
<li><a href='#SC'><p>The Stochastic Curtailment method of Boundary Construction</p></a></li>
<li><a href='#SCtoBdry'><p>Converts a stochastic curtailment boundary (conditional type I or</p>
II error probability) into a (efficacy or futility) boundary on the
standardized Z scale</a></li>
<li><a href='#SimGSB'><p>Verifies the results of &quot;GrpSeqBnds&quot; via simulation</p></a></li>
<li><a href='#wtdlogrank'><p>Weighted log-rank test</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Power in a Group Sequential Design</td>
</tr>
<tr>
<td>Version:</td>
<td>2.3.6</td>
</tr>
<tr>
<td>Author:</td>
<td>Grant Izmirlian &lt;izmirlig@mail.nih.gov&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>survival</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools for the evaluation of interim analysis plans for sequentially
 monitored trials on a survival endpoint; tools to construct efficacy and 
 futility boundaries, for deriving power of a sequential design at a specified
 alternative, template for evaluating the performance of candidate plans at a 
 set of time varying alternatives. See Izmirlian, G. (2014) &lt;<a href="https://doi.org/10.4310%2FSII.2014.v7.n1.a4">doi:10.4310/SII.2014.v7.n1.a4</a>&gt;.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Grant Izmirlian &lt;izmirlig@mail.nih.gov&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-12-10 20:16:36 UTC; izmirlig</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-12-10 23:40:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='agghaz'>Aggregated Hazard</h2><span id='topic+agghaz'></span>

<h3>Description</h3>

<p>Computes the MLE for the model that assumes piecewise constant hazards on intervals
defined by a grid of points. One applications for example is to calculate monthly hazard
rates given numbers of events, numbers at risk and event times reported to the day. Can
also handle time to event data stratified on a blocking factor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>agghaz(t.agg, time, nrisk, nevent)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="agghaz_+3A_t.agg">t.agg</code></td>
<td>
<p>Vector defining intervals upon which the user wants constant hazard rates.</p>
</td></tr>
<tr><td><code id="agghaz_+3A_time">time</code></td>
<td>
<p>Event times, possibly stratified on a blocking factor into multiple columns,
in units that occur in enough numbers per interval specified above. If there is just a
single column then it must be in column form (see example below).</p>
</td></tr>
<tr><td><code id="agghaz_+3A_nrisk">nrisk</code></td>
<td>
<p>Numbers at risk at specified event times</p>
</td></tr>
<tr><td><code id="agghaz_+3A_nevent">nevent</code></td>
<td>
<p>Numbers of events at specified event times</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>time.a</code></td>
<td>
<p>User supplied left-hand endpoints of intervals of hazard constancy</p>
</td></tr>
<tr><td><code>nrisk.a</code></td>
<td>
<p>Numbers at risk on specified intervals</p>
</td></tr>
<tr><td><code>nevent.a</code></td>
<td>
<p>Numbers of events on specified intervals</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(PwrGSD)
  data(lung)
  fit.msf &lt;- mysurvfit(Surv(time, I(status==2)) ~ sex, data=lung)

  ## A single stratum:
  with(fit.msf$Table, agghaz(30, time, cbind(nrisk.sex1), cbind(nevent.sex1)))

  ## Multiple strata--pooled and group 1:
  with(fit.msf$Table, agghaz(30, time, cbind(nrisk.sex1+nrisk.sex2,nrisk.sex1),
                                       cbind(nevent.sex1+nevent.sex2,nevent.sex1)))
</code></pre>

<hr>
<h2 id='as.boundaries'>Convert a &quot;PwrGSD&quot; object to a &quot;boundaries&quot; object</h2><span id='topic+as.boundaries'></span>

<h3>Description</h3>

<p>Convert a <code>PwrGSD</code> object to a <code>boundaries</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  as.boundaries(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.boundaries_+3A_object">object</code></td>
<td>
<p>an object of class <code>PwrGSD</code></p>
</td></tr>
<tr><td><code id="as.boundaries_+3A_...">...</code></td>
<td>
<p>if <code>object</code> is of class <code>PwrGSD</code> and there are
more than one statistic under investigation, then you may specify an
argument <code>stat</code>. The default value is 1, meaning the first one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>boundaries</code>.  See the documentation for
<code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code>
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov</p>


<h3>See Also</h3>

<p><code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## none as yet
</code></pre>

<hr>
<h2 id='CDFOR2LRR'>Convert CDF Odds Ratio to Logged Relative Risks</h2><span id='topic+CDFOR2LRR'></span>

<h3>Description</h3>

<p>Given the values of the baseline hazard and odds ratio of the CDF at
a grid of time points find the corresponding logged risk ratio.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  CDFOR2LRR(tcut, tmax, h0, CDFOR)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CDFOR2LRR_+3A_tcut">tcut</code></td>
<td>
<p>Grid of time points (left endpoints)</p>
</td></tr>
<tr><td><code id="CDFOR2LRR_+3A_tmax">tmax</code></td>
<td>
<p>The right endpoint of the last interval</p>
</td></tr>
<tr><td><code id="CDFOR2LRR_+3A_h0">h0</code></td>
<td>
<p>Values of the baseline hazard function on given intervals</p>
</td></tr>
<tr><td><code id="CDFOR2LRR_+3A_cdfor">CDFOR</code></td>
<td>
<p>Values of the odds ratio of the CDF's on the given intervals</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An m by 2 matrix, where m=length(tcut), having columns 'tcut' and
logged RR.
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov</p>

<hr>
<h2 id='CondPower'>Conditional type I and type II error probabilities given current value of the test
statistic</h2><span id='topic+CondPower'></span>

<h3>Description</h3>

<p>Computes conditional type I and type II error probabilities given current value of the
test statistic for monitoring based upon stochastic curtailment. This is now obsolete
and included in the functionality of &ldquo;GrpSeqBnds&rdquo; and is here for instructional
purposes only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CondPower(Z, frac, drift, drift.end, err.I, sided = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CondPower_+3A_z">Z</code></td>
<td>
<p>Current value of test statistic standardized to unit variance.</p>
</td></tr>
<tr><td><code id="CondPower_+3A_frac">frac</code></td>
<td>
<p>Current value of the information fraction (variance fraction).</p>
</td></tr>
<tr><td><code id="CondPower_+3A_drift">drift</code></td>
<td>
<p>Current value of the drift, i.e. the expected value of the test statistic
normalized to have variance equal to the information fraction. Required if you want to
compute conditional type II error, otherwise enter 0.</p>
</td></tr>
<tr><td><code id="CondPower_+3A_drift.end">drift.end</code></td>
<td>
<p>Projected value of the drift at the end of the trial.</p>
</td></tr>
<tr><td><code id="CondPower_+3A_err.i">err.I</code></td>
<td>
<p>Overall (total) type I error probability</p>
</td></tr>
<tr><td><code id="CondPower_+3A_sided">sided</code></td>
<td>
<p>Enter 1 or 2 for sided-ness of the test.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named numeric vector containing the two components &ldquo;Pr.cond.typeIerr&rdquo; and
&ldquo;Pr.cond.typeIIerr&rdquo;
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>References</h3>

 
<p>A General Theory on Stochastic Curtailment for Censored Survival Data D. Y. Lin, Q. Yao,
Zhiliang Ying Journal of the American Statistical Association, Vol. 94, No. 446 (Jun.,
1999), pp. 510-521
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## None as yet
</code></pre>

<hr>
<h2 id='cpd.PwrGSD'>Create a skeleton compound PwrGSD object</h2><span id='topic+cpd.PwrGSD'></span>

<h3>Description</h3>

<p>Given a user defined indexing dataframe as its only argument,
creates a skeleton compound PwrGSD object having a component
<code>Elements</code>, a list of <code>PwrGSD</code> objects, of length
equal to the number of rows in the indexing dataframe
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpd.PwrGSD(descr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpd.PwrGSD_+3A_descr">descr</code></td>
<td>
<p>A dataframe of a number of rows equal to the length
of the resulting list, <code>Elements</code>, of <code>PwrGSD</code> objects.
The user defines the mapping between rows of <code>descr</code>
and components of <code>Elements</code> and uses it to set up a loop
over scenarios.  There are several S3 classes and methods
for example <code>plot.cpd.PwrGSD</code>, which exploit this mapping
between characteristics of a run and the rows of <code>desr</code> for
subsetting and constructing conditioned plots. See the example below.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>cpd.PwrGSD</code> containing elements:
</p>
<table>
<tr><td><code>date</code></td>
<td>
<p>the POSIX date that the object was created&ndash;its quite useful</p>
</td></tr>
<tr><td><code>Elements</code></td>
<td>
<p>a list of length equal to the number of rows of <code>descr</code>
which will later contain objects of class <code>PwrGSD</code></p>
</td></tr>
<tr><td><code>descr</code></td>
<td>
<p>a copy of the indexing dataframe argument for use in
navigating the compound object in subsequent calls to other
functions such as the related <code>plot</code> method, and the subset
extractor, <code>Elements</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p>A <code>cpd.PwrGSD</code> object essentially a list of <code>PwrGSD</code> objects
that a user may set up in order to investigate the space of possible trial
scenarios, test statistics, and boundary construction options. One
could store a list of results without appealing at all to these
internal indexing capabilities. The advantage of setting up a
<code>cpd.PwrGSD</code> object is the nice summarization functionality
provided, for example the plot method for the <code>cpd.PwrGSD</code> class.
</p>
<p>The key ingredient to (i) the construction of the empty
object, (ii) and summarizing the results in tabular or plotted form
via its manipulation in subsequent function calls, is
the indexing dataset, <code>descr</code> (for description).  The correspondence
between rows of <code>descr</code> and elements in the list of <code>PwrGSD</code>
objects is purposely left very loose. In the example outlined below,
the user creates a &ldquo;base case&rdquo; call to <code>PwrGSD</code> and then decides
which quantities in this &ldquo;base case&rdquo; call to vary in order to
navigate the space of possible trial scenarios, monitoring statistics
and boundary construction methods.  Next, for each one of these
settings being varied, a variable with levels that determine each
possible setting is created.  The dataset <code>descr</code> is created
with one line corresponding to each combination of the selection
variables so created.  In order to ensure that there is 1-1
correspondence between the order of the rows in <code>descr</code> and the
order in the list <code>Elements</code> of <code>PwrGSD</code> objects, the user
carries out the computation in a loop over rows of <code>descr</code> in
which the values of the selection variables in each given row of
<code>descr</code> are used to create the corresponding component of
<code>Elements</code> via an update the &ldquo;base case&rdquo; call.
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>See Also</h3>

<p><code><a href="#topic+Elements">Elements</a></code>, <code><a href="#topic+plot.cpd.PwrGSD">plot.cpd.PwrGSD</a></code> and <code><a href="#topic+Power">Power</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## don't worry--these examples are guaranteed to work,
## its just inconvenient for the package checker
## Not run: 
  library(PwrGSD)

## In order to set up a compound object of class `cpd.PwrGSD'
## we first construct a base case: a two arm trial randomized in just
## under eight years with a maximum of 20 years of follow-up.
## We compute power at a specific alternative, `rhaz', under
## an interim analysis plan with roughly one annual analysis, some
## crossover between intervention and control arms, with Efficacy
## and futility boundaries constructed via the Lan-Demets procedure
## with O'Brien-Fleming spending on the hybrid scale. Investigate
## the behavior of three weighted log-rank statistics.

test.example &lt;-
  PwrGSD(EfficacyBoundary = LanDemets(alpha = 0.05, spending = ObrienFleming),
         FutilityBoundary = LanDemets(alpha = 0.1, spending = ObrienFleming),
	 RR.Futility = 0.82, sided="1&lt;",method="A",accru =7.73, accrat =9818.65,
         tlook =c(7.14, 8.14, 9.14, 10.14, 10.64, 11.15, 12.14, 13.14,
                  14.14, 15.14, 16.14, 17.14, 18.14, 19.14, 20.14),
         tcut0 =0:19, h0 =c(rep(3.73e-04, 2), rep(7.45e-04, 3),
                            rep(1.49e-03, 15)),
         tcut1 =0:19, rhaz =c(1, 0.9125, 0.8688, 0.7814, 0.6941,
                              0.6943, 0.6072, 0.5202, 0.4332, 0.6520,
                              0.6524, 0.6527, 0.6530, 0.6534, 0.6537,
                              0.6541, 0.6544, 0.6547, 0.6551, 0.6554),
         tcutc0 =0:19, hc0 =c(rep(1.05e-02, 2), rep(2.09e-02, 3),
                              rep(4.19e-02, 15)),
         tcutc1 =0:19, hc1 =c(rep(1.05e-02, 2), rep(2.09e-02, 3),
                              rep(4.19e-02, 15)),
         tcutd0B =c(0, 13), hd0B =c(0.04777, 0),
         tcutd1B =0:6, hd1B =c(0.1109, 0.1381, 0.1485, 0.1637, 0.2446,
                               0.2497, 0),
         noncompliance =crossover, gradual =TRUE,
         WtFun =c("FH", "SFH", "Ramp"),
         ppar =c(0, 1, 0, 1, 10, 10))

## we will construct a variety of alternate hypotheses relative to the
## base case specified above

  rhaz &lt;- 
    c(1, 0.9125, 0.8688, 0.7814, 0.6941, 0.6943, 0.6072, 0.5202, 0.4332,
    0.652, 0.6524, 0.6527, 0.653, 0.6534, 0.6537, 0.6541, 0.6544,
    0.6547, 0.6551, 0.6554)

  max.effect &lt;- 0.80 + 0.05*(0:8)
  n.me &lt;- length(max.effect)

## we will also vary extent of censoring relative to the base case
## specified above

  hc &lt;- c(rep(0.0105, 2), rep(0.0209, 3), rep(0.0419, 15))

  cens.amt &lt;- 0.75 + 0.25*(0:2)
  n.ca &lt;- length(cens.amt)

## we may also wish to compare the Lan-Demets/O'Brien-Fleming efficacy
## boundary with a Lan-Demets/linear spending boundary

  Eff.bound.choice &lt;- 1:2
  ebc.nms &lt;- c("LanDemets(alpha=0.05, spending=ObrienFleming)",
               "LanDemets(alpha=0.05, spending=Pow(1))")
  n.ec &lt;- length(Eff.bound.choice)

## The following line creates the indexing dataframe, `descr', with one
## line for each possible combination of the selection variables we've
## created. 


  descr &lt;- as.data.frame(
              cbind(Eff.bound.choice=rep(Eff.bound.choice, each=n.ca*n.me),
                    cens.amt=rep(rep(cens.amt, each=n.me), n.ec),
                    max.effect=rep(max.effect, n.ec*n.ca)))

  descr$Eff.bound.choice &lt;- ebc.nms[descr$Eff.bound.choice]

## Now descr contains one row for each combination of the levels of
## the user defined selection variables, `Eff.bound.choice',
## `max.effect' and `cens.amt'. Keep in mind that the names and number
## of these variables is arbitrary. Next we create a skeleton
## `cpd.PwrGSD' object with a call to the function `cpd.PwrGSD' with
## argument `descr' 

  test.example.set &lt;- cpd.PwrGSD(descr)

## Now, the newly created object, of class `cpd.PwrGSD', contains
## an element `descr', a component `date', the date created 
## and a component `Elements', an empty list of length equal
## to the number of rows in `descr'.  Next we do the computation in
## a loop over the rows of `descr'.

  n.descr &lt;- nrow(descr)

  for(k in 1:n.descr){

    ## First, we copy the original call to the current call,
    ## `Elements[[k]]$call'

    test.example.set$Elements[[k]]$call &lt;- test.example$call

    ## Use the efficacy boundary choice in the kth row of `descr'
    ## to set the efficacy boundary choice in the current call

    test.example.set$Elements[[k]]$call$EfficacyBoundary &lt;- 
    parse(text=as.character(descr[k,"Eff.bound.choice"]))[[1]]

    ## Derive the `rhaz' defined by the selection variable "max.effect"
    ## in the kth row of `descr' and use this to set the `rhaz'
    ## components of the current call

    test.example.set$Elements[[k]]$call$rhaz &lt;-
                            exp(descr[k,"max.effect"] * log(rhaz))

    ## Derive the censoring components from the selection variable
    ## "cens.amt" in the kth row of `descr' and place that result
    ## into the current call
    
    test.example.set$Elements[[k]]$call$hc0 &lt;-
    test.example.set$Elements[[k]]$call$hc1 &lt;- descr[k, "cens.amt"] * hc

    ## Now the current call corresponds exactly to the selection
    ## variable values in row `k' of `descr'. The computation is
    ## done by calling `update'

    test.example.set$Elements[[k]] &lt;- update(test.example.set$Elements[[k]])
    cat(k/n.descr, "\r")
  }

  ## We can create a new `cpd.PwrGSD' object by subsetting on
  ## the selection variables in `descr':

  Elements(test.example.set, 
           subset=(substring(Eff.bound.choice, 32, 34)=="Obr" &amp;
                            max.effect &gt;= 1))


  ## or we can plot the results -- see the help under `plot.cpd.PwrGSD'

  plot(test.example.set, formula = ~ max.effect | stat * cens.amt,
       subset=(substring(Eff.bound.choice, 32, 34)=="Obr"))

  plot(test.example.set, formula = ~ max.effect | stat * cens.amt,
       subset=(substring(Eff.bound.choice, 32, 34)=="Pow"))

  ## Notice the appearance of the selection variable `stat' which was
  ## not defined in the dataset `descr'. 

  ## Recall that each single "PwrGSD" object can contain results
  ## for a list of test statistics, as in the example shown here where
  ## we have results on three statistics per component of `Elements'.
  ## For this reason the variable `stat' can be also be referenced in
  ## the `subset' or `formula' arguments of calls to this `plot' method,
  ## and in the `subset' argument of the function `Power' shown below.

  ## The function `Power' is used to convert the `cpd.PwrGSD' object
  ## into  a dataframe, stacked by rows of `descr' and by `stat'
  ## (there are three statistics being profiled per each component of
  ## `Elements'), for generating tables or performing other 
  ## computations.

  Power(test.example.set,
        subset=(substring(Eff.bound.choice, 32, 34)=="Pow" &amp; stat %in% c(1,3)))

  
## End(Not run)
</code></pre>

<hr>
<h2 id='CRRtoRR'>Cumulative-risk ratios to risk ratios</h2><span id='topic+CRRtoRR'></span>

<h3>Description</h3>

<p>Given a vector of cumulative-risk ratios, computes risk ratios
</p>


<h3>Usage</h3>

<pre><code class='language-R'> CRRtoRR(CRR, DT, h = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CRRtoRR_+3A_crr">CRR</code></td>
<td>
<p>vector of cumulative risk ratios of length <code>m</code></p>
</td></tr>
<tr><td><code id="CRRtoRR_+3A_dt">DT</code></td>
<td>
<p>vector of time increments upon which the cumulative ratios
represent. For example if the hazard takes values
$h_1, h_2, ..., h_m$ on the intervals $[t_1, t_2), [t_2, t_3),
..., [t_m, t_m+1)$ then <code>DT</code> will be c($t_2-t_1,t_3 -t_2,
..., t_m+1 - t_m )$
</p>
</td></tr>
<tr><td><code id="CRRtoRR_+3A_h">h</code></td>
<td>
<p>The hazard in the reference arm, of length <code>m</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vector of risk ratios at the <code>m</code> time points
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## none as yet
</code></pre>

<hr>
<h2 id='CY2TOShaz'>Calender year rates to Study Year Rates</h2><span id='topic+CY2TOShaz'></span>

<h3>Description</h3>

<p>Given the cutpoints at which the hazard is to be constant, the 
values taken by the calender year rates and the calender time
offset from the start of the trial at which randomization ended,
this function converts to time on study rates, assuming uniform
accrual.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  CY2TOShaz(tcut, t.eor, m, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CY2TOShaz_+3A_tcut">tcut</code></td>
<td>
<p>Left hand endpoints of intervals on which time on study
hazard is taken to be constant</p>
</td></tr>
<tr><td><code id="CY2TOShaz_+3A_t.eor">t.eor</code></td>
<td>
<p>Time offsest from the beginning of the trial at which
randomization ended</p>
</td></tr>
<tr><td><code id="CY2TOShaz_+3A_m">m</code></td>
<td>
<p>Annual calender time rates</p>
</td></tr>
<tr><td><code id="CY2TOShaz_+3A_verbose">verbose</code></td>
<td>
<p>do you want to see alot of debugging info&ndash;defaults to
<code>FALSE</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>hazard = h, table = attr(obj., &quot;tbl&quot;)
</p>
<table>
<tr><td><code>hazard</code></td>
<td>
<p>time on study hazard values taken on intervals specified
by the argument <code>tcut</code></p>
</td></tr>
<tr><td><code>table</code></td>
<td>
<p>a table containg the observed and fitted values</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## none as yet
</code></pre>

<hr>
<h2 id='detail'>
Function to extract the 'detail' component from a <code>PwrGSD</code> object
</h2><span id='topic+detail'></span>

<h3>Description</h3>

<p>Extracts the 'detail' component from an object of class <code>PwrGSD</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>detail(obj)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="detail_+3A_obj">obj</code></td>
<td>

<p>An object of class <code>PwrGSD</code> returned from the fucntion
<code>PwrGSD</code> or a component of the list returned by the funtion 
<code>cpd.PwrGSD</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The 'detail' component of the object. For the Asymptotic method,
this will be most of the quantities involved in the computation,
the input parameters such as the various incidence rates, cross over
rates etc, as well as intermediate computations such as the drift
function variance function as well. For the simulation method, some of
these are returned an in addition, the simulated event histories.
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian at nih dot gov&gt;
</p>

<hr>
<h2 id='DX'>A utility function for forming differences</h2><span id='topic+DX'></span>

<h3>Description</h3>

<p>DX(x) returns c(x[1], diff(x))
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  DX(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DX_+3A_x">x</code></td>
<td>
<p>A grid of time points (increasing)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>DX(x) returns c(x[1], diff(x))
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov</p>

<hr>
<h2 id='Elements'>Create a subset of a &quot;cpd.PwrGSD&quot; object</h2><span id='topic+Elements'></span>

<h3>Description</h3>

<p>Create a subset of a <code>cpd.PwrGSD</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  Elements(object, subset, na.action = na.pass)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Elements_+3A_object">object</code></td>
<td>
<p>an object of class <code>cpd.PwrGSD</code></p>
</td></tr>
<tr><td><code id="Elements_+3A_subset">subset</code></td>
<td>
<p>you may extract a subset via a logical expression in the
variables of the index dataframe, <code>descr</code></p>
</td></tr>
<tr><td><code id="Elements_+3A_na.action">na.action</code></td>
<td>
<p>a method for handling <code>NA</code> values in the
variables in the subset expression.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>cpd.PwrGSD</code>. See help on that topic for
details.
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>See Also</h3>

<p><code><a href="#topic+cpd.PwrGSD">cpd.PwrGSD</a></code> and <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## See the `cpd.PwrGSD' example
</code></pre>

<hr>
<h2 id='EX1gXK'>
A function for computing the bias adjusted point estimate for a
statistic observed to cross the efficacy boundary. 
</h2><span id='topic+EX1gXK'></span>

<h3>Description</h3>

<p>A function for computing the bias adjusted point estimate for a
statistic, on the Brownian scale, observed to cross the efficacy boundary. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EX1gXK(xk, b.eff, frac)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EX1gXK_+3A_xk">xk</code></td>
<td>

<p>The observed value of the statistic, on the &ldquo;Brownian&rdquo; scale.
</p>
</td></tr>
<tr><td><code id="EX1gXK_+3A_b.eff">b.eff</code></td>
<td>

<p>Efficacy boundary points at current and prior analyses
</p>
</td></tr>
<tr><td><code id="EX1gXK_+3A_frac">frac</code></td>
<td>

<p>Information fraction at current and prior analyses
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the expected value of X_1 given X_K, which is the bias adjusted point estimate
</p>


<h3>Note</h3>

<p>This works for the unweighted, proportional hazards case, but also works
in the case of the weighted log-rank statistic when we assume the chosen
weights are proportional to the true shape.
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlig@mail.nih.gov&gt;
</p>


<h3>References</h3>

<p>Emerson, S. S. (1993). Computation of the uniform minimum variance unibiased estimator of
a normal mean following a group sequential trialdiscrete sequential boundaries for
clinical trials. Computers and Biomedical Research 26 68&ndash;73.
</p>
<p>Izmirlian, G. (2014). Estimation of the relative risk following group sequential procedure
based upon the weighted log-rank statistic. Statistics and its Interface 00 00&ndash;00
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gsd.dens">gsd.dens</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># if Z.K = U_K/V_K^0.5 is the log-rank statistic on the standard normal
# scale, then we obtain an estimate of the logged relative risk as follows
# Suppose we've stopped at analysis number K=4, and Z.K = 2.5
# suppose the end of trial variance of the log-rank statistic
# (specified in design and used to compute 'frac') is V.end = 100

K &lt;- 4
Z.K &lt;- 2.5
V.end &lt;- 100

# Information fraction
frac &lt;- c(0.15, 0.37, 0.64, 0.76)

# Efficacy Boundary 
gsb &lt;- GrpSeqBnds(frac=frac, EfficacyBoundary=LanDemets(spending=ObrienFleming, alpha=0.05))

# Efficacy boundary points
be &lt;- gsb$table[,"b.e"] 

# Brownian scale
X.K &lt;- Z.K*frac[K]

# expected value of X_1 given X_K
ex1gxk &lt;- EX1gXK(X.K, be, frac)

# Crude estimate of logged relative risk
X.K/(frac[K]*V.end^0.5)

# Bias adjusted estimate of logged relative risk
ex1gxk/(frac[1]*V.end^0.5)
</code></pre>

<hr>
<h2 id='GrpSeqBnds'>Computes efficacy and futility boundaries</h2><span id='topic+GrpSeqBnds'></span>

<h3>Description</h3>

<p>This computes efficacy and futility boundaries for interim analysis
and sequential designs.  Two sided symmetric efficacy boundaries can
be computed by specifying half of the intended total type I error
probability in the argument, <code>Alpha.Efficacy</code>.  Otherwise,
especially in the case of efficacy and futility bounds only one sided
boundaries are currently computed.  The computation allows for two
different time scales&ndash;one must be the variance ratio, and the second
can be a user chosen increasing scale beginning with 0 that takes the
value 1 at the conclusion of the trial.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GrpSeqBnds(EfficacyBoundary = LanDemets(alpha = 0.05, spending = ObrienFleming),
           FutilityBoundary = LanDemets(alpha = 0.1, spending = ObrienFleming),
           NonBindingFutility = TRUE, frac, frac.ii = NULL, drift = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GrpSeqBnds_+3A_efficacyboundary">EfficacyBoundary</code></td>
<td>
<p>This specifies the method used to construct the
efficacy boundary. The available choices are:
</p>
<p>&lsquo;<span class="samp">&#8288;(i) &#8288;</span>&rsquo;<code>Lan-Demets</code>(<code>alpha</code>=&lt;total type I error&gt;,
<code>spending</code>=&lt;spending function&gt;). The Lan-Demets method is based upon a error
probability spending approach. The spending function can be set to
<code>ObrienFleming</code>, <code>Pocock</code>, or <code>Power(rho)</code>, where <code>rho</code> is the the
power argument for the power spending function: rho=3 is roughly equivalent to the
O'Brien-Fleming spending function and smaller powers result in a less conservative
spending function.
</p>
<p>&lsquo;<span class="samp">&#8288;(ii) &#8288;</span>&rsquo;<code>Haybittle</code>(<code>alpha</code>=&lt;total type I error&gt;,
<code>b.Haybittle</code>=&lt;user specified boundary point&gt;).  The Haybittle approach is
conceptually the simplest of all methods for efficacy boundary construction. However,
as it spends nearly no alpha until the end, is for all practical purposes equivalent
to a single analysis design and to be considered overly conservative. This method sets
all the boundary points equal to <code>b.Haybittle</code>, a user specified value (try 3)
for all analyses except the last, which is calculated so as to result in the total
type I error, set with the argument <code>alpha</code>.
</p>
<p>&lsquo;<span class="samp">&#8288;(iii) &#8288;</span>&rsquo;<code>SC</code>(be.end=&lt;efficacy boundary point at trial end&gt;,
<code>prob</code>=&lt;threshold for conditional type I error for efficacy stopping&gt;). The
stochastic curtailment method is based upon the conditional probability of type I
error given the current value of the statistic. Under this method, a sequence of
boundary points on the standard normal scale (as are boundary points under all other
methods) is calculated so that the total probability of type I error is
maintained. This is done by considering the joint probabilities of continuing to the
current analysis and then exceeding the threshold at the current analysis. A good
value for the threshold value for the conditional type I error, <code>prob</code> is 0.90 or
greater.
</p>
<p>&lsquo;<span class="samp">&#8288;(iv)  &#8288;</span>&rsquo;User supplied boundary points in the form <code>c(b1, b2, b3, ..., b_m)</code>,
where <code>m</code> is the number of looks.
</p>
</td></tr>
<tr><td><code id="GrpSeqBnds_+3A_futilityboundary">FutilityBoundary</code></td>
<td>
<p>This specifies the method used to construct the futility
boundary. The available choices are:
</p>
<p>&lsquo;<span class="samp">&#8288;(i) &#8288;</span>&rsquo;<code>Lan-Demets(alpha</code>=&lt;total type II error&gt;, <code>spending</code>=&lt;spending
function&gt;). The Lan-Demets method is based upon a error probability spending
approach. The spending function can be set to <code>ObrienFleming</code>, <code>Pocock</code>, or
<code>Power(rho)</code>, where <code>rho</code> is the the power argument for the power spending
function: rho=3 is roughly equivalent to the O'Brien-Fleming spending function and
smaller powers result in a less conservative spending function.
</p>
<p>&lsquo;<span class="samp">&#8288;NOTE: &#8288;</span>&rsquo;there is no implementation of the <code>Haybittle</code> method for
futility boundary construction. Given that the futility boundary depends upon
values of the drift function, this method doesn't apply.
</p>
<p>&lsquo;<span class="samp">&#8288;(ii) &#8288;</span>&rsquo;<code>SC</code>(be.end=&lt;efficacy boundary point at trial end&gt;,
<code>prob</code>=&lt;threshold for conditional type II error for futility stopping&gt;,
<code>drift.end</code>=&lt;projected drift at end of trial&gt;). The stochastic curtailment method
is based upon the conditional probability of type II error given the current value of
the statistic. Under this method, a sequence of boundary points on the standard normal
scale (as are boundary points under all other methods) is calculated so that the total
probability of type II error, is maintained. This is done by considering the joint
probabilities of continuing to the current analysis and then exceeding the threshold
at the current analysis. A good value for the threshold value for the conditional type
I error, <code>prob</code> is 0.90 or greater.
</p>
<p>&lsquo;<span class="samp">&#8288;(iii) &#8288;</span>&rsquo;User supplied boundary points in the form <code>c(b1, b2, b3, ..., b_m)</code>,
where <code>m</code> is the number of looks.
</p>
</td></tr>
<tr><td><code id="GrpSeqBnds_+3A_nonbindingfutility">NonBindingFutility</code></td>
<td>
<p>When using a futility boundary and this is set to 'TRUE', the
efficacy boundary will be constructed in the absence of the futility boundary, and
then the futility boundary will be constructed given the resulting efficacy
boundary. This results in a more conservative efficacy boundary with true type I error
less than the nominal level. This is recommended due to the fact that futility
crossings are viewed by DSMB's with much less gravity than an efficacy crossing and as
such, the consensus is that efficacy bounds should not be discounted towards the null
hypothesis because of paths which cross a futility boundary. Default value is 'TRUE'.
</p>
</td></tr>
<tr><td><code id="GrpSeqBnds_+3A_frac">frac</code></td>
<td>
<p>The variance ratio. If the end of trial variance is unknown then normalize
all previous variances by the current variance. In this case you must specify a second
scale that is monotone increasing from 0 to 1 at the end of the trial. Required.
</p>
</td></tr>
<tr><td><code id="GrpSeqBnds_+3A_frac.ii">frac.ii</code></td>
<td>
<p>The second information scale that is used for type I and type II error
probability spending. Optional (see above)
</p>
</td></tr>
<tr><td><code id="GrpSeqBnds_+3A_drift">drift</code></td>
<td>
<p>The drift function of the underlying brownian motion, which is the expected
value under the design alternative of the un-normalized weighted log-rank statistic,
then normalized to have variance one when the variance ratio equals 1. See the
examples below.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>boundaries</code> with components:
&quot;table&quot;   &quot;frac&quot;    &quot;frac.ii&quot; &quot;drift&quot;   &quot;call&quot;
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>The call that produced the returned results.</p>
</td></tr>
<tr><td><code>frac</code></td>
<td>
<p>The vector of variance ratios.</p>
</td></tr>
<tr><td><code>frac.ii</code></td>
<td>
<p>The vector of information ratios for type I and type II error probability
spending, which differs from the above if the user sets the argument <code>frac.ii</code> to
a second scale as mentioned above.</p>
</td></tr>
<tr><td><code>drift</code></td>
<td>
<p>The drift vector that is required as an argument when futility boundaries
are calculated.</p>
</td></tr>
<tr><td><code>table</code></td>
<td>
<p>A matrix with components
</p>
<p>&lsquo;<span class="samp">&#8288;frac &#8288;</span>&rsquo;The information ratio for type I and type II error probability spending.
</p>
<p>&lsquo;<span class="samp">&#8288;b.f &#8288;</span>&rsquo;The calculated futility boundary (if requested).
</p>
<p>&lsquo;<span class="samp">&#8288;alpha.f &#8288;</span>&rsquo;The type II error probability spent at that analysis (if doing futility bounds).
</p>
<p>&lsquo;<span class="samp">&#8288;cum-alpha.f &#8288;</span>&rsquo;Cumulative sum of <code>alpha.f</code> (if doing futility bounds).
</p>
<p>&lsquo;<span class="samp">&#8288;b.e &#8288;</span>&rsquo;The calculated efficacy boundary.
</p>
<p>&lsquo;<span class="samp">&#8288;alpha.e &#8288;</span>&rsquo;The type I error probability spent at that analysis.
</p>
<p>&lsquo;<span class="samp">&#8288;cum-alpha.e &#8288;</span>&rsquo;Cumulative sum of <code>alpha.e</code>.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>References</h3>

<p>Gu, M.-G. and Lai, T.-L.  &ldquo;Determination of power and sample size in the design
of clinical trials with failure-time endpoints and interim analyses.&rdquo;  Controlled
Clinical Trials 20 (5): 423-438. 1999
</p>
<p>Izmirlian, G.  &ldquo;The PwrGSD package.&rdquo;  NCI Div. of Cancer Prevention Technical
Report. 2004
</p>
<p>Jennison, C. and Turnbull, B.W. (1999) Group Sequential Methods: Applications to
Clinical Trials Chapman &amp; Hall/Crc, Boca Raton FL
</p>
<p>Proschan, M.A., Lan, K.K.G., Wittes, J.T.  (2006), corr 2nd printing (2008) Statistical
Monitoring of Clinical Trials A Unified Approach Springer Verlag, New York 
</p>
<p>Izmirlian G. (2014). Estimation of the relative risk following group
sequential procedure based upon the weighted log-rank statistic.
Statistics and its Interface 7(1), 27-42
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
    ## NOTE: In an unweighted analysis, the variance ratios and event ratios
    ## are the same, whereas in a weighted analysis, they are quite different.
    ##
    ## For example, in a trial with 7 or so years of accrual and maximum follow-up of 20 years
    ## using the stopped Fleming-Harrington weights, `WtFun' = "SFH", with paramaters
    ## `ppar' = c(0, 1, 10) we might get the following vector of variance ratios:

    frac    &lt;- c(0.006995655, 0.01444565, 0.02682463, 0.04641363, 0.0585665,
                 0.07614902, 0.1135391, 0.168252, 0.2336901, 0.3186155, 0.4164776,
                 0.5352199, 0.670739, 0.8246061, 1)


    ## and the following vector of event ratios:

    frac.ii &lt;- c(0.1494354, 0.1972965, 0.2625075, 0.3274323, 0.3519184, 0.40231,
                 0.4673037, 0.5579035, 0.6080742, 0.6982293, 0.7671917, 0.8195019,
                 0.9045182, 0.9515884, 1)

    ## and the following drift under a given alternative hypothesis
               
    drift &lt;-   c(0.06214444, 0.1061856, 0.1731267, 0.2641265, 0.3105231, 0.3836636,
                 0.5117394, 0.6918584, 0.8657705, 1.091984, 1.311094, 1.538582,
                 1.818346, 2.081775, 2.345386)

    ## JUST ONE SIDED EFFICACY BOUNDARY
    ## In this call, we calculate a one sided efficacy boundary at each of 15 analyses
    ## which will occur at the given (known) variance ratios, and we use the variance
    ## ratio for type I error probability spending, with a total type I error probabilty
    ## of 0.05, using the Lan-Demets method with Obrien-Fleming spending (the default).

    gsb.all.just.eff &lt;- GrpSeqBnds(frac=frac, 
                                   EfficacyBoundary=LanDemets(alpha=0.05, spending=ObrienFleming))

    ## ONE SIDED EFFICACY AND FUTILTY BOUNDARIES
    ## In this call, we calculate a one sided efficacy boundary at each of 15 analyses
    ## which will occur at the given (known) variance ratios, and we use the variance
    ## ratio for type I and type II error probability spending, with a total type I error
    ## probabilty of 0.05 and a total type II error probability of 0.10, using the Lan-Demets
    ## method with Obrien-Fleming spending (the default) for both efficacy and futilty.

    gsb.all.eff.fut &lt;- GrpSeqBnds(frac=frac,  
                                  EfficacyBoundary=LanDemets(alpha=0.05, spending=ObrienFleming),
                                  FutilityBoundary=LanDemets(alpha=0.10, spending=ObrienFleming),
                                  drift=drift)

    ## Now suppose that we are performing the 7th interim analysis. We don't know what the variance
    ## will be at the end of the trial, so we normalize variances of the current and previous
    ## statistics by the variance of the current statistic.  This is equivalent to the following
    ## length 7 vector of variance ratios:

    frac7 &lt;- frac[1:7]/frac[7]

    ## To proceed under the "unknown variance at end of trial" case, we must use a second
    ## scale for spending type I and II error probabilty. Unlike the above scale
    ## which is renormalized at each analysis to have value 1 at the current analysis, the
    ## alpha spending scale must be monotone increasing and attain the value 1 only at the
    ## end of the trial. A natural choice is the event ratio, which is known in advance if
    ## the trial is run until a required number of events is obtained, a so called
    ## maximum information trial:

    frac7.ii &lt;- frac.ii[1:7]

    ## the first seven values of the drift function

    drift7 &lt;- drift[1:7]/frac[7]^0.5

    gsb.1st7.eff.fut &lt;- GrpSeqBnds(frac=frac7, frac.ii=frac7.ii,  
                                   EfficacyBoundary=LanDemets(alpha=0.05, spending=ObrienFleming),
                                   FutilityBoundary=LanDemets(alpha=0.10, spending=ObrienFleming),
				   drift=drift7)

    ## Of course there are other options not covered in these examples but this should get you
    ## started
</code></pre>

<hr>
<h2 id='gsd.dens'>
A function for computing the probability density for the group
sequentially monitored test statistic.
</h2><span id='topic+gsd.dens'></span>

<h3>Description</h3>

<p>A function for computing the probability density for a sequentially monitored test. This
is the joint density, in the rejection region, of (X_K, K), where X_K is the observed
value of the test statistic upon efficacy boundary crossing, and K is the analysis
number at which the efficacy boundary was crossed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsd.dens(x, frac = NULL, scale="Standard")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsd.dens_+3A_x">x</code></td>
<td>

<p>The main argument, <code>x</code>, is either a object of class &ldquo;boundaries&rdquo; or a
numeric vector. If it is of class &ldquo;boundaries&rdquo; then no other arguments are
required. If it is a numeric vector then the <code>frac</code> argument must be
specified. See below. In this case, <code>x</code> will be the observed values of the
statistic at the current and all prior analyses, either on the standard normal scale
(the default) or on the &ldquo;Brownian&rdquo; scale. For &ldquo;Brownian&rdquo; scale, set
argument <code>scale</code> to &ldquo;Brownian&rdquo;.
</p>
</td></tr>
<tr><td><code id="gsd.dens_+3A_frac">frac</code></td>
<td>

<p>Required only when the main argument, <code>x</code>, is a numeric vector, and must be a
vector of the same length. In this case, <code>frac</code> will be the information at the
current and all prior interim analyses.
</p>
</td></tr>
<tr><td><code id="gsd.dens_+3A_scale">scale</code></td>
<td>

<p>Required only when the main argument, <code>x</code>, is a numeric vector. A switch
indicating whether the elements of the numeric vector, <code>x</code>, are specified on the
standard normal scale, <code>x</code>=&ldquo;Standard&rdquo;, or on the Brownian scale,
<code>x</code>=&ldquo;Brownian&rdquo;.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements <code>x</code>, <code>dF</code>, <code>x1c</code>, and
<code>dF1c</code>:
</p>
<table>
<tr><td><code>x</code></td>
<td>

<p>Node points used in Gaussian quadrature. See examples below.
</p>
</td></tr>
<tr><td><code>dF</code></td>
<td>

<p>Probability mass at each node point. See examples below.
</p>
</td></tr>
<tr><td><code>x1c</code></td>
<td>

<p>Node points in the continuation region at the first analysis.
</p>
</td></tr>
<tr><td><code>dF1c</code></td>
<td>

<p>Probability mass at each node point in the continuation region
at the first analysis.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Also used in computation of Rao-Blackwell-ized bias adjusted point
estimate for statistic observed to cross the efficacy boundary.
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlig@mail.nih.gov&gt;
</p>


<h3>References</h3>

<p>Emerson, S. S. (1993). Computation of the uniform minimum variance unibiased estimator of
a normal mean following a group sequential trialdiscrete sequential boundaries for
clinical trials. Computers and Biomedical Research 26 68&ndash;73.
</p>
<p>Izmirlian, G. (2014). Estimation of the relative risk following group sequential procedure
based upon the weighted log-rank statistic. Statistics and its Interface 00 00&ndash;00
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EX1gXK">EX1gXK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # Information fraction
  frac &lt;- c(0.15, 0.37, 0.64, 0.76)

  # Efficacy Boundary 
  gsb &lt;- GrpSeqBnds(frac=frac, EfficacyBoundary=LanDemets(spending=ObrienFleming, alpha=0.05))

  # To compute the p-value under the stagewise ordering, for an observed
  #  value of the monitoring statistic 2.1, crossing the efficacy
  #  boundary at the 4th analysis, we do the following

  be &lt;- gsb$table[,"b.e"] 
  be[4] &lt;- 2.1

  sum(gsd.dens(be, frac, scale="Standard")$dF)

</code></pre>

<hr>
<h2 id='Haybittle'>The Haybittle method of Boundary Construction</h2><span id='topic+Haybittle'></span>

<h3>Description</h3>

<p>The function <code>Haybittle</code> is used in calls to the functions <code>GrpSeqBnds</code> and
<code>PwrGSD</code> as a possible setting for the argument <code>EfficacyBoundary</code>. NOTE: the
Haybittle method is not implemented as a futility boundary method The Haybittle method
is one of four currently availiable choices (efficacy only), the others being
<code>LanDemets</code>, <code>SC</code> (stochastic curtailment), and user specified.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>  Haybittle(alpha, b.Haybittle, from = NULL, to = NULL) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Haybittle_+3A_alpha">alpha</code></td>
<td>
<p>The total probability of type I error.</p>
</td></tr>
<tr><td><code id="Haybittle_+3A_b.haybittle">b.Haybittle</code></td>
<td>
<p>User specified efficacy boundary at all but the
last analysis.</p>
</td></tr>
<tr><td><code id="Haybittle_+3A_from">from</code></td>
<td>
<p>WARNING EXPERIMENTAL: See the documentation under
<code>LanDemets</code> or <code>SC</code>. I'm not quite sure if this works
or even makes sense. Don't use it, ok?</p>
</td></tr>
<tr><td><code id="Haybittle_+3A_to">to</code></td>
<td>
<p>See above.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Haybittle approach is conceptually the simplest of all methods for efficacy boundary
construction. However, as it spends nearly no alpha until the end, is for all practical
purposes equivalent to a single analysis design and to be considered overly
conservative. This method sets all the boundary points equal to <code>b.Haybittle</code>, a
user specified value (try 3) for all analyses except the last, which is calculated so as
to result in the total type I error, set with the argument <code>alpha</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>boundary.construction.method</code> which is really a list
with the following components. The print method displays the original
call.
</p>
<table>
<tr><td><code>type</code></td>
<td>
<p>Gives the boundary construction method type, which is the character
string &quot;Haybittle&quot;</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>The numeric value passed to the argument 'alpha' which is
the total probability of type I error.</p>
</td></tr>
<tr><td><code>b.Haybittle</code></td>
<td>
<p>The numeric value passed to the argument 'b.Haybittle' which is
the user specified efficacy boundary at all but the
last analysis.</p>
</td></tr>
<tr><td><code>from</code></td>
<td>
<p>Description of 'comp2'</p>
</td></tr>
<tr><td><code>to</code></td>
<td>
<p>You're not using this, right? </p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>see above.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The print method returns the call by default</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian</p>


<h3>References</h3>

<p>see references under <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>See Also</h3>

<p><code><a href="#topic+LanDemets">LanDemets</a></code>, <code><a href="#topic+SC">SC</a></code>, <code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code>, and 
<code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## example 1: what is the result of calling a Boundary Construction Method function
    ## A call to 'Haybittle' just returns the call
    Haybittle(alpha=0.05, b.Haybittle=3)
    
    ## It does arguement checking...this results in an error
    ## Not run: 
      Haybittle(alpha=0.05)
    
## End(Not run)
    
    ## but really its value is a list with the a component containing
    ## the boundary method type, "LanDemts", and components for each
    ## of the arguments.
    names(Haybittle(alpha=0.05, b.Haybittle=3))

    Haybittle(alpha=0.05, b.Haybittle=3)$type
    Haybittle(alpha=0.05, b.Haybittle=3)$alpha
    Haybittle(alpha=0.05, b.Haybittle=3)$b.Haybittle
    Haybittle(alpha=0.05, b.Haybittle=3)$call

## example 2: ...But the intended purpose of the spending functions 
## is in constructing calls to 'GrpSeqBnds' and to 'PwrGSD':
     

    frac &lt;- c(0.07614902,0.1135391,0.168252,0.2336901,0.3186155,
              0.4164776,0.5352199,0.670739,0.8246061,1)

    test &lt;- GrpSeqBnds(frac=frac, EfficacyBoundary=Haybittle(alpha=0.025, b.Haybittle=3))
</code></pre>

<hr>
<h2 id='IntSurvDiff'>Weighted Integrated Survival function test</h2><span id='topic+IntSurvDiff'></span>

<h3>Description</h3>

<p>Computes a two sample weighted integrated survival function log-rank
statistic with events weighted according to one of the available
weighting function choices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  IntSurvDiff(formula =formula(data), data =parent.frame(), WtFun =c("FH", "SFH", "Ramp"),
              param = c(0, 0), sided = c(2, 1), subset, na.action, w = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IntSurvDiff_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>Surv(Time, Event) ~ arm</code> where <code>arm</code>
is a dichotomous variable with values 0 and 1.</p>
</td></tr>
<tr><td><code id="IntSurvDiff_+3A_data">data</code></td>
<td>
<p>a dataframe</p>
</td></tr>
<tr><td><code id="IntSurvDiff_+3A_wtfun">WtFun</code></td>
<td>
<p>a selection from the available list: &ldquo;FH&rdquo; (Fleming-Harrington),
&ldquo;SFH&rdquo; (stopped Fleming-Harrington) or &ldquo;Ramp&rdquo;.  See <code>param</code> in
the following line.</p>
</td></tr>
<tr><td><code id="IntSurvDiff_+3A_param">param</code></td>
<td>
<p>Weight function parameters. Length and interpretation depends upon
the selected value of <code>WtFun</code>:<br />
If <code>WtFun</code>==&ldquo;FH&rdquo; then <code>param</code> is a length 2 vector specifying
the power of the pooled (across arms) kaplan meier estimate and its complement.<br />
If <code>WtFun</code>==&ldquo;SFH&rdquo; then <code>param</code> is a length 3 vector with first
two components as in the &ldquo;FH&rdquo; case, and third component the time (in the same
units as the time to event) at which the &ldquo;FH&rdquo; weight function is capped off
at its current value.<br />
If <code>WtFun==SFH</code> then <code>param</code> is of length 1 specifying the time
(same units as time to event) at which events begin to get equal weight. The
&ldquo;Ramp&rdquo; weight function is a linearly increasing deterministic weight function
which is capped off at 1 at the user specified time.
</p>
</td></tr>
<tr><td><code id="IntSurvDiff_+3A_sided">sided</code></td>
<td>
<p>One or Two sided test?  Set to 1 or 2</p>
</td></tr>
<tr><td><code id="IntSurvDiff_+3A_subset">subset</code></td>
<td>
<p>Analysis can be applied to a subset of the dataframe based upon a logical
expression in its variables</p>
</td></tr>
<tr><td><code id="IntSurvDiff_+3A_na.action">na.action</code></td>
<td>
<p>Method for handling <code>NA</code> values in the covariate, <code>arm</code></p>
</td></tr>
<tr><td><code id="IntSurvDiff_+3A_w">w</code></td>
<td>
<p>currently no effect</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>survtest</code> containing components
</p>
<table>
<tr><td><code>pn</code></td>
<td>
<p>sample size</p>
</td></tr>
<tr><td><code>wttyp</code></td>
<td>
<p>internal representation of the <code>WtFun</code> argument</p>
</td></tr>
<tr><td><code>par</code></td>
<td>
<p>internal representation of the <code>param</code> argument</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>unique times of events accross all arms</p>
</td></tr>
<tr><td><code>nrisk</code></td>
<td>
<p>number at risk accross all arms at each event time</p>
</td></tr>
<tr><td><code>nrisk1</code></td>
<td>
<p>Number at risk in the experimental arm at each event time</p>
</td></tr>
<tr><td><code>nevent</code></td>
<td>
<p>Number of events accross all arms at each event time</p>
</td></tr>
<tr><td><code>nevent1</code></td>
<td>
<p>Number of events in the experimental arm at each event time</p>
</td></tr>
<tr><td><code>wt</code></td>
<td>
<p>Values of the weight function at each event time</p>
</td></tr>
<tr><td><code>pntimes</code></td>
<td>
<p>Number of event times</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>
<p>The un-normalized weighted log-rank statistic, i.e. the summed
weighted observed minus expected differences at each event time</p>
</td></tr>
<tr><td><code>var</code></td>
<td>
<p>Variance estimate for the above</p>
</td></tr>
<tr><td><code>pu0</code></td>
<td>
<p>person units of follow-up time in the control arm</p>
</td></tr>
<tr><td><code>pu1</code></td>
<td>
<p>person units of follow-up time in the intervention arm</p>
</td></tr>
<tr><td><code>n0</code></td>
<td>
<p>events in the control arm</p>
</td></tr>
<tr><td><code>n1</code></td>
<td>
<p>events in the intervention arm</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>sample size, same as <code>pn</code></p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that created the object</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov</p>


<h3>References</h3>

<p>Weiand S, Gail MH, James BR, James KL. (1989). A family of nonparametric statistics for
comparing diagnostic makers with paired or unpaired data.  <em>Biometrika</em> <b>76</b>,
585-592.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wtdlogrank">wtdlogrank</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(PwrGSD)
  data(lung)
  fit.isd&lt;-IntSurvDiff(Surv(time,I(status==2))~I(sex==2), data=lung, WtFun="SFH", param=c(0,1,300))
</code></pre>

<hr>
<h2 id='LanDemets'>The Lan-Demets method of Boundary Construction</h2><span id='topic+LanDemets'></span>

<h3>Description</h3>

<p>The function <code>LanDemets</code> is used in calls to the functions <code>GrpSeqBnds</code> and
<code>PwrGSD</code> as a possible setting for the arguments <code>EfficacyBoundary</code> and
<code>FutilityBoundary</code>, in specification of the method whereby efficacy and or futility
boundaries are to be constructed. The Lan-Demets method is one of four currently
availiable choices, the others being <code>SC</code> (stochastic curtailment),
<code>Haybittle</code> (efficacy only) and user specified.  </p>


<h3>Usage</h3>

<pre><code class='language-R'> LanDemets(alpha,
  spending, from = NULL, to = NULL) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LanDemets_+3A_alpha">alpha</code></td>
<td>
<p>If <code>LanDemets</code> is used to specify the <code>EfficacyBoundary</code> 
then the argument <code>alpha</code> is the total probability of type I error.  
If <code>LanDemets</code> is used to specify the <code>FutilityBoundary</code> then the 
argument <code>alpha</code> is the total probability of type II error.</p>
</td></tr>
<tr><td><code id="LanDemets_+3A_spending">spending</code></td>
<td>
<p>Specify the alpha spending function. Set this to
<code>ObrienFleming</code>, <code>Pow(rho=&lt;x&gt;)</code>, or <code>Pocock</code>. See
help files for these spending functions.</p>
</td></tr>
<tr><td><code id="LanDemets_+3A_from">from</code></td>
<td>
<p>WARNING EXPERIMENTAL: you can actually construct boundaries via a
hybrid of the 3 boundary construction methods, <code>LanDemets</code>, <code>SC</code>,
and 'user specified'. When using a hybrid boundry, set the argument
<code>EfficacyBoundary</code> or <code>FutilityBoundary</code> respectively,
to a list with components <code>LanDemets</code>, <code>SC</code>, or user
specified numbers. In the former two cases, <code>from</code> and
<code>to</code> are used in <code>LanDemets</code> and also in <code>SC</code>
to stipulate how many interim analyses they are in effect.
See the help for <code>GrpSeqBnds</code> and <code>PwrGSD</code></p>
</td></tr>
<tr><td><code id="LanDemets_+3A_to">to</code></td>
<td>
<p>See above.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cornerstone of the Lan-Demets method is that the
amount of alpha (type I or II error probability) that is &quot;spent&quot; at
a given interim analysis is determined via a user specified
&quot;spending function&quot;. A spending function is a monotone increasing
mapping on (0,1) with range (0,alpha).  The 'alpha' spent at a given
analysis is determined by the increment in the values of the spending
function at the current and at the most recent information fractions.
</p>


<h3>Value</h3>

<p>An object of class <code>boundary.construction.method</code> which is really a list
with the following components. The print method displays the original
call.
</p>
<table>
<tr><td><code>type</code></td>
<td>
<p>Gives the boundary construction method type, which is the character
string &quot;LanDemets&quot;</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>The numeric value passed to the argument 'alpha' which is
the total probability of type I (efficacy) or type II (futility) error.</p>
</td></tr>
<tr><td><code>spending</code></td>
<td>
<p>The spending function that was passed to the argument
'spending'. Note that this will be of class 'name'  for
'ObrienFleming' and 'Pocock', but will be of class 'function' for
'Pow'</p>
</td></tr>
<tr><td><code>from</code></td>
<td>
<p>The numeric value passed to the argument 'from'. See above.</p>
</td></tr>
<tr><td><code>to</code></td>
<td>
<p>The numeric value passed to the argument 'to'. See above.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>returns the call</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The print method returns the call by default</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian</p>


<h3>References</h3>

<p>see references under <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>See Also</h3>

<p><code><a href="#topic+SC">SC</a></code>, <code><a href="#topic+ObrienFleming">ObrienFleming</a></code>,
<code><a href="#topic+Pow">Pow</a></code>, <code><a href="#topic+Pocock">Pocock</a></code>, <code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code>, and
<code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## example 1: what is the result of calling a Boundary Construction Method function
    ## A call to 'LanDemets' just returns the call
    LanDemets(alpha=0.05, spending=ObrienFleming)
    
    ## It does arguement checking...this results in an error
    ## Not run: 
      LanDemets(alpha=0.05)
    
## End(Not run)
    
    ## but really its value is a list with the a component containing
    ## the boundary method type, "LanDemts", and components for each
    ## of the arguments.
    names(LanDemets(alpha=0.05, spending=ObrienFleming))

    LanDemets(alpha=0.05, spending=ObrienFleming)$type
    LanDemets(alpha=0.05, spending=ObrienFleming)$alpha
    LanDemets(alpha=0.05, spending=ObrienFleming)$spending
    class(LanDemets(alpha=0.05, spending=ObrienFleming)$spending)
    LanDemets(alpha=0.05, spending=Pow(2))$spending
    class(LanDemets(alpha=0.05, spending=Pow(2))$spending)
    LanDemets(alpha=0.05, spending=ObrienFleming)$call

## example 2: ...But the intended purpose of the spending functions is
## in constructing calls to 'GrpSeqBnds' and to 'PwrGSD':
     

    frac &lt;- c(0.07614902,0.1135391,0.168252,0.2336901,0.3186155,
              0.4164776,0.5352199,0.670739,0.8246061,1)
    drift &lt;- c(0.3836636,0.5117394,0.6918584,0.8657705,1.091984,
               1.311094,1.538582,1.818346,2.081775,2.345386)

    test &lt;- GrpSeqBnds(frac=frac, EfficacyBoundary=LanDemets(alpha=0.05, spending=ObrienFleming),
                       FutilityBoundary=LanDemets(alpha=0.10, spending=Pocock),
                       drift=drift)
</code></pre>

<hr>
<h2 id='lookup'>Lookup values for a piecewise constant function</h2><span id='topic+lookup'></span>

<h3>Description</h3>

<p>Given the values and lefthand endpoints for intervals of constancy,
lookup values of the function at arbitrary values of the independent
variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  lookup(xgrid, ygrid, x, y0 = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lookup_+3A_xgrid">xgrid</code></td>
<td>
<p>Lefthand endpoints of intervals of constancy</p>
</td></tr>
<tr><td><code id="lookup_+3A_ygrid">ygrid</code></td>
<td>
<p>Values on these intervals, of same length as <code>xgrid</code></p>
</td></tr>
<tr><td><code id="lookup_+3A_x">x</code></td>
<td>
<p>Input vector of arbitrary independent variables.</p>
</td></tr>
<tr><td><code id="lookup_+3A_y0">y0</code></td>
<td>
<p>Value to be returned for values of <code>x</code> that are smaller
than <code>min(xgrid)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>~Describe the value returned
If it is a LIST, use
</p>
<table>
<tr><td><code>comp1</code></td>
<td>
<p>Description of 'comp1'</p>
</td></tr>
<tr><td><code>comp2</code></td>
<td>
<p>Description of 'comp2'</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## none as yet
</code></pre>

<hr>
<h2 id='lung'>Mayo Clinic Lung Cancer Data</h2><span id='topic+lung'></span>

<h3>Description</h3>

<p>Survival in patients with lung cancer at Mayo
Clinic. Performance scores rate how well the patient can perform
usual daily activities.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(lung)
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    inst:</td><td style="text-align: left;"> Institution code</td>
</tr>
<tr>
 <td style="text-align: left;">
    time:</td><td style="text-align: left;"> Survival time in days</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> censoring status 1=censored, 2=dead</td>
</tr>
<tr>
 <td style="text-align: left;">
    age:</td><td style="text-align: left;"> Age in years</td>
</tr>
<tr>
 <td style="text-align: left;">
    sex:</td><td style="text-align: left;">  Male=1 Female=2</td>
</tr>
<tr>
 <td style="text-align: left;">
    ph.ecog:</td><td style="text-align: left;"> ECOG performance score (0=good 5=dead)</td>
</tr>
<tr>
 <td style="text-align: left;">
    ph.karno:</td><td style="text-align: left;"> Karnofsky performance score (bad=0-good=100) rated by physician</td>
</tr>
<tr>
 <td style="text-align: left;">
    pat.karno:</td><td style="text-align: left;"> Karnofsky performance score  rated by patient</td>
</tr>
<tr>
 <td style="text-align: left;">
    meal.cal:</td><td style="text-align: left;"> Calories consumed at meals</td>
</tr>
<tr>
 <td style="text-align: left;">
    wt.loss:</td><td style="text-align: left;"> Weight loss in last six months</td>
</tr>
<tr>
 <td style="text-align: left;">
     </td>
</tr>

</table>



<h3>Source</h3>

<p>Terry Therneau</p>

<hr>
<h2 id='mystack'>Stack a dataset</h2><span id='topic+mystack'></span>

<h3>Description</h3>

<p>Given a dataframe containing one or more variables named with a common
prefix, this function creates a stacked dataset with one set of
observed values of the variables (in order of occurence) per line.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mystack(object, fu.vars, create.idvar = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mystack_+3A_object">object</code></td>
<td>
<p>a dataframe containing one or more variables named with a common
prefix</p>
</td></tr>
<tr><td><code id="mystack_+3A_fu.vars">fu.vars</code></td>
<td>
<p>a list of the unique prefixes</p>
</td></tr>
<tr><td><code id="mystack_+3A_create.idvar">create.idvar</code></td>
<td>
<p>Do you want to add an ID variable with a common
value given to all records resulting from a given input record?
Default is <code>FALSE</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A stacked dataframe
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## none as yet
</code></pre>

<hr>
<h2 id='mysurvfit'>My Survfit</h2><span id='topic+mysurvfit'></span>

<h3>Description</h3>

<p>Computes numbers at risk, numbers of events at each unique event
time within levels of a blocking factor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mysurvfit(formula = formula(data), data = parent.frame(), subset, na.action = na.fail)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mysurvfit_+3A_formula">formula</code></td>
<td>
<p>Should be a formula of the form <code>Surv(ti, ev) ~ block</code>
where <code>block</code> is the blocking factor. It need not be a 
factor per se but should have relatively few discrete levels.  Sorry,
no staggered entry allowed at present</p>
</td></tr>
<tr><td><code id="mysurvfit_+3A_data">data</code></td>
<td>
<p>a dataframe</p>
</td></tr>
<tr><td><code id="mysurvfit_+3A_subset">subset</code></td>
<td>
<p>you can subset the analysis via logical expression
in variables in the dataframe</p>
</td></tr>
<tr><td><code id="mysurvfit_+3A_na.action">na.action</code></td>
<td>
<p>pass a method for handling <code>NA</code> values in
<code>block</code> such as <code>na.omit</code>, or <code>na.fail</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe of <code>2*NLEV + 1</code> columns where <code>NLEV</code> is the
number of levels of the factor <code>block</code>.
</p>
<table>
<tr><td><code>time</code></td>
<td>
<p>The sorted vector of unique event times from all blocks</p>
</td></tr>
<tr><td><code>nrisk1</code></td>
<td>
<p>The number at risk in block level 1 at each event time</p>
</td></tr>
<tr><td><code>nevent1</code></td>
<td>
<p>The number of events in block level 1 at each event time</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
<tr><td><code>nriskNLEV</code></td>
<td>
<p>The number at risk in block level NLEV at each event time</p>
</td></tr>
<tr><td><code>neventNLEV</code></td>
<td>
<p>The number of events in block level NLEV at each event time</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(PwrGSD)
  data(lung)

  fit.msf &lt;- mysurvfit(Surv(time, I(status==2)) ~ sex, data=lung)

  fit.msf
  ## Not run: 
  plot(fit.msf)
  
## End(Not run)
</code></pre>

<hr>
<h2 id='ObrienFleming'>The O'Brien-Fleming Alpha Spending Function</h2><span id='topic+ObrienFleming'></span>

<h3>Description</h3>

<p>Stipulates alpha spending according to the O'Brien-Fleming
spending function in the Lan-Demets boundary construction method. Its
intended purpose is in constructing calls to <code>GrpSeqBnds</code> and
<code>PwrGSD</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ObrienFleming()
</code></pre>


<h3>Value</h3>

<p>An object of class <code>spending.function</code> which is really a list
with the following components. The print method displays the original
call.
</p>
<table>
<tr><td><code>type</code></td>
<td>
<p>Gives the spending function type, which is the character
string &quot;ObrienFleming&quot;</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>returns the call</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The print method returns the call by default</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian</p>


<h3>References</h3>

<p>see references under <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>See Also</h3>

<p><code><a href="#topic+LanDemets">LanDemets</a></code>, <code><a href="#topic+Pow">Pow</a></code>, <code><a href="#topic+Pocock">Pocock</a></code>,
<code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code>, <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## example 1: what is the result of calling a spending function
    ## A call to 'ObrienFleming' just returns the call
    ObrienFleming()

    ## but really its value is a list with a component named
    ## 'type' equal to "ObrienFleming" and a component named
    ## 'call' equal to the call.
    names(ObrienFleming)

    ObrienFleming()$type

    ObrienFleming()$call

## example 2: ...But the intended purpose of the spending functions is
## in constructing calls to 'GrpSeqBnds' and to 'PwrGSD':
     

    frac &lt;- c(0.07614902,0.1135391,0.168252,0.2336901,0.3186155,
              0.4164776,0.5352199,0.670739,0.8246061,1)
    drift &lt;- c(0.3836636,0.5117394,0.6918584,0.8657705,1.091984,
               1.311094,1.538582,1.818346,2.081775,2.345386)

    test &lt;- GrpSeqBnds(frac=frac, EfficacyBoundary=LanDemets(alpha=0.05, spending=ObrienFleming),
                       FutilityBoundary=LanDemets(alpha=0.10, spending=Pocock),
                       drift=drift)
</code></pre>

<hr>
<h2 id='plot.cpd.PwrGSD'>Plot Method for cpd.PwrGSD objects</h2><span id='topic+plot.cpd.PwrGSD'></span>

<h3>Description</h3>

<p>Creates a trellis plot of type II error probability and
power at each interim analysis, stacked, versus an effect size
variable, conditioned upon levels of up to two factors.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cpd.PwrGSD'
plot(x, formula, subset, na.action,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cpd.PwrGSD_+3A_x">x</code></td>
<td>
<p>an object of class <code>cpd.PwrGSD</code></p>
</td></tr>
<tr><td><code id="plot.cpd.PwrGSD_+3A_formula">formula</code></td>
<td>
<p>a one sided formula of the form <code>~ effect | f1</code>
or <code>~ effect | f1 * f2</code> where <code>effect</code>, <code>f1</code>, and
<code>f2</code> are variables in the indexing dataframe <code>descr</code>, or
the special variable <code>stat</code> which may be used when there are
multiple test statistics per component of <code>Elements</code>.  See
the example in the documentation for <code>cpd.PwrGSD</code></p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="plot.cpd.PwrGSD_+3A_subset">subset</code></td>
<td>
<p>the plot can be applied to a subset of rows of
<code>descr</code> via a logical expression on its variables in
combination with the special variable, <code>stat</code> when applicable.</p>
</td></tr>
<tr><td><code id="plot.cpd.PwrGSD_+3A_na.action">na.action</code></td>
<td>
<p>a <code>na.action</code> method for handling <code>NA</code>
values</p>
</td></tr>
<tr><td><code id="plot.cpd.PwrGSD_+3A_...">...</code></td>
<td>
<p>other parameters to pass to the R function <code>coplot</code>
usually not neccesary</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the object, <code>x</code>, invisibly
</p>


<h3>Note</h3>

<p>This processes the <code>cpd.PwrGSD</code> object into a dataframe,
stacked on interim looks and then passes the results to the R
function <code>coplot</code></p>


<h3>Author(s)</h3>

<p>Abovementioned <code>cpd.PwrGSD</code> processing done by Grant
Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>References</h3>

<p>Chambers, J. M. (1992)
<em>Data for models.</em>
Chapter 3 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth \&amp; Brooks/Cole.
</p>
<p>Cleveland, W. S. (1993) <em>Visualizing Data.</em> New Jersey: Summit Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cpd.PwrGSD">cpd.PwrGSD</a></code> <code><a href="#topic+Power">Power</a></code> and <code><a href="#topic+Elements">Elements</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## See the example in the 'cpd.PwrGSD' documentation
</code></pre>

<hr>
<h2 id='Pocock'>The Pocock Alpha Spending Function</h2><span id='topic+Pocock'></span>

<h3>Description</h3>

<p>Stipulates alpha spending according to the Pocock 
spending function in the Lan-Demets boundary construction method. Its
intended purpose is in constructing calls to <code>GrpSeqBnds</code> and
<code>PwrGSD</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Pocock()
</code></pre>


<h3>Value</h3>

<p>An object of class <code>spending.function</code>
</p>
<table>
<tr><td><code>type</code></td>
<td>
<p>Gives the spending function type, which is the character
string &quot;Pocock&quot;</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>returns the call</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The print method returns the call by default</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian</p>


<h3>References</h3>

<p>see references under <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>See Also</h3>

<p><code><a href="#topic+LanDemets">LanDemets</a></code>, <code><a href="#topic+ObrienFleming">ObrienFleming</a></code>, <code><a href="#topic+Pow">Pow</a></code>,
<code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code>, <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## example 1: what is the result of calling a spending function

    ## A call to 'Pocock' just returns the call
    Pocock()

    ## but really its value is a list with a component named
    ## 'type' equal to "Pocock" and a component named
    ## 'call' equal to the call.
    names(Pocock)

    Pocock()$type

    Pocock()$call

## example 2: ...But the intended purpose of the spending functions is
## in constructing calls to 'GrpSeqBnds' and to 'PwrGSD':
     

    frac &lt;- c(0.07614902,0.1135391,0.168252,0.2336901,0.3186155,
              0.4164776,0.5352199,0.670739,0.8246061,1)
    drift &lt;- c(0.3836636,0.5117394,0.6918584,0.8657705,1.091984,
               1.311094,1.538582,1.818346,2.081775,2.345386)

    test &lt;- GrpSeqBnds(frac=frac, EfficacyBoundary=LanDemets(alpha=0.05, spending=Pocock),
                       FutilityBoundary=LanDemets(alpha=0.10, spending=ObrienFleming),
                       drift=drift)
</code></pre>

<hr>
<h2 id='Pow'>The Wang-Tsiatis Power Alpha Spending Function</h2><span id='topic+Pow'></span>

<h3>Description</h3>

<p>Stipulates alpha spending according to the Wang-Tsiatis
Power function in the Lan-Demets boundary construction method. Its
intended purpose is in constructing calls to <code>GrpSeqBnds</code> and
<code>PwrGSD</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Pow(rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Pow_+3A_rho">rho</code></td>
<td>
<p>The exponent for the Wang-Tsiatis power spending function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Larger <code>rho</code> results in more conservative
boundaries.  <code>rho</code>=3 is roughly equivalent to Obrien-Fleming
spending. <code>rho</code>=1 spends <code>alpha</code> linearly in the
information fraction
</p>


<h3>Value</h3>

<p>An object of class <code>spending.function</code> which is really a list
with the following components. The print method displays the original
call.
</p>
<table>
<tr><td><code>type</code></td>
<td>
<p>Gives the spending function type, which is the character
string &quot;Pow&quot;</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>the numeric value passed to the single argument, <code>rho</code></p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>returns the call</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The print method returns the call by default</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian</p>


<h3>References</h3>

<p>see references under <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>See Also</h3>

<p><code><a href="#topic+LanDemets">LanDemets</a></code>, <code><a href="#topic+ObrienFleming">ObrienFleming</a></code>, <code><a href="#topic+Pocock">Pocock</a></code>,
<code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code>, <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## example 1: what is the result of calling a spending function
    ## A call to 'Pow' just returns the call
    Pow(rho=2)

    ## It does argument checking...the following results in an error:
    ## Not run: 
      Pow()
    
## End(Not run)
    
    ## it doesn't matter whether the argument is named or not,
    ## either produces the same result
    Pow(2)

    ## but really its value is a list with a component named
    ## 'type' equal to "Pow", a component named 'rho' equal
    ## to the numeric value passed to the single argument 'rho'
    ## and a component  named 'call' equal to the call.
    names(Pow(rho=2))
    
    names(Pow(2))
    
    Pow(rho=2)$type
    Pow(rho=2)$rho
    Pow(rho=2)$call    

## example 2: ...But the intended purpose of the spending functions is
## in constructing calls to 'GrpSeqBnds' and to 'PwrGSD':
     

    frac &lt;- c(0.07614902,0.1135391,0.168252,0.2336901,0.3186155,
              0.4164776,0.5352199,0.670739,0.8246061,1)
    drift &lt;- c(0.3836636,0.5117394,0.6918584,0.8657705,1.091984,
               1.311094,1.538582,1.818346,2.081775,2.345386)

    test &lt;- GrpSeqBnds(frac=frac, EfficacyBoundary=LanDemets(alpha=0.05, spending=Pow(2)),
                       FutilityBoundary=LanDemets(alpha=0.10, spending=ObrienFleming),
                       drift=drift)
</code></pre>

<hr>
<h2 id='Power'>Extract the Power results</h2><span id='topic+Power'></span>

<h3>Description</h3>

<p>The function &lsquo;Power&rsquo; is used to summarize the &lsquo;cpd.PwrGSD&rsquo; object into  a
dataframe containing power and type II error, summed over analysis
times. The data frame is stacked by rows of &lsquo;descr&rsquo; and by &lsquo;stat&rsquo;
(if there are multiple statistics being profiled per each component of
&lsquo;Elements&rsquo;), for generating tables or performing other computations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Power(object, subset, nlook.ind = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Power_+3A_object">object</code></td>
<td>
<p>an object of class <code>cpd.PwrGSD</code></p>
</td></tr>
<tr><td><code id="Power_+3A_subset">subset</code></td>
<td>
<p>you may extract a subset via a logical expression in the
variables of the index dataframe, <code>descr</code></p>
</td></tr>
<tr><td><code id="Power_+3A_nlook.ind">nlook.ind</code></td>
<td>
<p>(optional) a vector containing a subset of the
indices of analysis times over which the sum is formed. Use this
for example if you want to know the probability of stopping by the
kth analysis under an unfavorable alternative.  Set nlook.ind to
1:k</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a dataframe, stacked by rows of &lsquo;descr&rsquo; and then by choices of &lsquo;stat&rsquo;
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>See Also</h3>

<p><code><a href="#topic+cpd.PwrGSD">cpd.PwrGSD</a></code> and <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## See the `cpd.PwrGSD' example
</code></pre>

<hr>
<h2 id='PwrGSD'>Calculate Power in a Group Sequential Design</h2><span id='topic+PwrGSD'></span>

<h3>Description</h3>

<p>Derives power in a two arm clinical trial under a group sequential design.  Allows for
arbitrary number of interim analyses, arbitrary specification of arm-0/arm-1 time to
event distributions (via survival or hazard), arm-0/arm-1 censoring distribution,
provisions for two types of continuous time non-compliance according to arm-0/arm-1 rate
followed by switch to new hazard rate.  Allows for analyses using (I) weighted log-rank
statistic, with weighting function (a) a member of the Flemming-Harrington G-Rho class,
or (b) a stopped version thereof, or (c) the ramp-plateau deterministic weights, or (II)
the integrated survival distance (currently under method==&quot;S&quot; without futility only).
Stopping boundaries are computed via the Lan-Demets method, Haybittle method, converted
from the stochastic curtailment procedure, or be completely specified by the user.  The
Lan-Demets boundaries can be constructed usign either O'Brien-Flemming, Pocock or
Wang-Tsiatis power alpha-spending.  The C kernel is readily extensible, and further
options will become available in the near future.</p>


<h3>Usage</h3>

<pre><code class='language-R'>PwrGSD(EfficacyBoundary = LanDemets(alpha = 0.05, spending = ObrienFleming),
    FutilityBoundary = LanDemets(alpha = 0.1, spending = ObrienFleming),
    NonBindingFutility = TRUE, sided = c("2&gt;", "2&lt;", "1&gt;", "1&lt;"),
    method = c("S", "A"), accru, accrat, tlook,
    tcut0 = NULL, h0 = NULL, s0 = NULL, tcut1 = NULL,
    rhaz = NULL, h1 = NULL, s1 = NULL, tcutc0 = NULL, hc0 = NULL,
    sc0 = NULL, tcutc1 = NULL, hc1 = NULL, sc1 = NULL, tcutd0A = NULL,
    hd0A = NULL, sd0A = NULL, tcutd0B = NULL, hd0B = NULL, sd0B = NULL,
    tcutd1A = NULL, hd1A = NULL, sd1A = NULL, tcutd1B = NULL,
    hd1B = NULL, sd1B = NULL, tcutx0A = NULL, hx0A = NULL, sx0A = NULL,
    tcutx0B = NULL, hx0B = NULL, sx0B = NULL, tcutx1A = NULL,
    hx1A = NULL, sx1A = NULL, tcutx1B = NULL, hx1B = NULL, sx1B = NULL,
    noncompliance = c("none", "crossover", "mixed", "user"),
    gradual = FALSE, WtFun = c("FH", "SFH", "Ramp"), ppar = cbind(c(0, 0)), 
    Spend.Info = c("Variance", "Events", "Hybrid(k)", "Calendar"), RR.Futility = NULL, 
    qProp.one.or.Q = c("one", "Q"), Nsim = NULL, detail = FALSE, StatType = c("WLR",
        "ISD"), doProj=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PwrGSD_+3A_efficacyboundary">EfficacyBoundary</code></td>
<td>
<p>This specifies the method used to construct the efficacy
boundary. The available choices are:
</p>
<p>&lsquo;<span class="samp">&#8288;(i) &#8288;</span>&rsquo;<code>Lan-Demets</code>(<code>alpha</code>=&lt;total type I error&gt;, <code>spending</code>
=&lt;spending function&gt;). The Lan-Demets method is based upon a error probability
spending approach. The spending function can be set to <code>ObrienFleming</code>,
<code>Pocock</code>, or <code>Power(rho)</code>, where <code>rho</code> is the the power argument for
the power spending function: rho=3 is roughly equivalent to the O'Brien-Fleming
spending function and smaller powers result in a less conservative spending function.
</p>
<p>&lsquo;<span class="samp">&#8288;(ii) &#8288;</span>&rsquo;<code>Haybittle</code>(<code>alpha</code>=&lt;total type I error&gt;,
<code>b.Haybittle</code>=&lt;user specified boundary point&gt;).  The Haybittle approach is
conceptually the simplest of all methods for efficacy boundary construction. However,
as it spends nearly no alpha until the end, is for all practical purposes equivalent
to a single analysis design and to be considered overly conservative. This method sets
all the boundary points equal to <code>b.Haybittle</code>, a user specified value (try 3)
for all analyses except the last, which is calculated so as to result in the total
type I error, set with the argument <code>alpha</code>.
</p>
<p>&lsquo;<span class="samp">&#8288;(iii) &#8288;</span>&rsquo;<code>SC</code>(<code>be.end</code>=&lt;efficacy boundary point at trial end&gt;,
<code>prob</code>=&lt;threshold for conditional type I error for efficacy stopping&gt;).
The stochastic curtailment method is based upon the conditional probability of type I
error given the current value of the statistic. Under this method, a sequence of
boundary points on the standard normal scale (as are boundary points under all other
methods) is calculated so that the total probability of type I error is maintained.
This is done by considering the joint probabilities of continuing to the current
analysis and then exceeding the threshold at the current analysis. A good value for
the threshold value for the conditional type I error, <code>prob</code> is 0.90 or greater.
</p>
<p>&lsquo;<span class="samp">&#8288;(iv) &#8288;</span>&rsquo;User supplied boundary points in the form <code>c(b1, b2, b3, ..., b_m)</code>,
where <code>m</code> is the number of looks.
</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_futilityboundary">FutilityBoundary</code></td>
<td>
<p>This specifies the method used to construct the futility
boundary. The available choices are:
</p>
<p>&lsquo;<span class="samp">&#8288;(i) &#8288;</span>&rsquo;<code>Lan-Demets</code>(<code>alpha</code>=&lt;total type II error&gt;,  <code>spending</code>=
&lt;spending function&gt;). The Lan-Demets method is based upon a error probability spending
approach. The spending function can be set to <code>ObrienFleming</code>, <code>Pocock</code>, or
<code>Power(rho)</code>, where <code>rho</code> is the the power argument for the power spending
function: rho=3 is roughly equivalent to the O'Brien-Fleming spending function and
smaller powers result in a less conservative spending function.
</p>
<p>&lsquo;<span class="samp">&#8288;NOTE: &#8288;</span>&rsquo;there is no implementation of the <code>Haybittle</code> method for
futility boundary construction. Given that the futility boundary depends upon
values of the drift function, this method doesn't apply.
</p>
<p>&lsquo;<span class="samp">&#8288;(ii) &#8288;</span>&rsquo;<code>SC</code>(<code>be.end</code>=&lt;efficacy boundary point at trial end&gt;,
<code>prob</code>=&lt;threshold for conditional type II error for futility stopping&gt;,
<code>drift.end</code>=&lt;projected drift at end of trial&gt;). The stochastic curtailment
method is based upon the conditional probability of type II error given the current
value of the statistic. Under this method, a sequence of boundary points on the
standard normal scale (as are boundary points under all other methods) is calculated
so that the total probability of type II error, is maintained. This is done by
considering the joint probabilities of continuing to the current analysis and then
exceeding the threshold at the current analysis. A good value for the threshold value
for the conditional type I error, <code>prob</code> is 0.90 or greater.
</p>
<p>&lsquo;<span class="samp">&#8288;(iii) &#8288;</span>&rsquo;User supplied boundary points in the form <code>c(b1, b2, b3, ..., b_m)</code>,
where <code>m</code> is the number of looks.
</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_nonbindingfutility">NonBindingFutility</code></td>
<td>
<p>When using a futility boundary and this is set to 'TRUE', the
efficacy boundary will be constructed in the absence of the futility boundary, and
then the futility boundary will be constructed given the resulting efficacy
boundary. This results in a more conservative efficacy boundary with true type I error
less than the nominal level. This is recommended due to the fact that futility
crossings are viewed by DSMB's with much less gravity than an efficacy crossing and as
such, the consensus is that efficacy bounds should not be discounted towards the null
hypothesis because of paths which cross a futility boundary. Default value is 'TRUE'.
</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_sided">sided</code></td>
<td>
<p>Set to &ldquo;2&gt;&rdquo; (quoted) for two sided tests of the null hypothesis when
a positive drift corresponds to efficacy. Set to &ldquo;2&lt;&rdquo; (quoted) for two sided
tests of the null hypothesis when a negative drift corresponds to efficacy. Set to
&ldquo;1&gt;&rdquo; or &ldquo;1&lt;&rdquo; for one sided tests of H0 when efficacy corresponds to a
positive or negative drift, respectively. If <code>method</code>==&ldquo;S&rdquo; then this must
be of the same length as <code>StatType</code> because the interpretation of <code>sided</code> is
different depending upon whether <code>StatType</code>==&ldquo;WLR&rdquo; (negative is benefit)
or <code>StatType</code>==&ldquo;ISD&rdquo; (positive is benefit)
</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_method">method</code></td>
<td>
<p>Determines how to calculate the power. Set to &ldquo;A&rdquo; (Asymptotic
method, the default) or &ldquo;S&rdquo; (Simulation method)</p>
</td></tr> 
<tr><td><code id="PwrGSD_+3A_accru">accru</code></td>
<td>
<p>The upper endpoint of the accrual period beginning with time 0.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_accrat">accrat</code></td>
<td>
<p>The rate of accrual per unit of time.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tlook">tlook</code></td>
<td>
<p>The times of planned interim analyses.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tcut0">tcut0</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-0 specific mortality
is constant. The last given component is the left hand endpoint of the interval having
right hand endpoint infinity.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_h0">h0</code></td>
<td>
<p>A vector of the same length as <code>tcut0</code> which specifies the piecewise
constant arm-0 mortality rate.</p>
</td></tr> 
<tr><td><code id="PwrGSD_+3A_s0">s0</code></td>
<td>
<p>Alternatively, the arm-0 mortality distribution can be supplied via this
argument, in terms of of the corresponding survival function values at the times given
in the vector <code>tcut0</code>. If <code>s0</code> is supplied, then <code>h0</code>is derived
internally, assuming the piecewise exponential distrubiton. If you specify <code>s0</code>,
the first element must be 1, and correspondingly, the first component of <code>tcut0</code>
will be the lower support point of the distribution. You must supply either <code>h0</code>
or <code>s0</code> but not both.</p>
</td></tr>  
<tr><td><code id="PwrGSD_+3A_tcut1">tcut1</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-1 specific mortality
is constant.  The last given component is the left hand endpoint of the interval
having right hand endpoint infinity.</p>
</td></tr>  
<tr><td><code id="PwrGSD_+3A_rhaz">rhaz</code></td>
<td>
<p>A vector of piecewise constant arm-1 versus arm-0 mortality rate ratios.
If <code>tcut1</code> and <code>tcut0</code> are not identical, then <code>tcut1</code>, <code>h0</code>, and
<code>rhaz</code> are internally rederived at the union of the sequences <code>tcut0</code> and
<code>tcut1</code>. In all cases the arm-1 mortality rate is then derived at the time
cutpoints <code>tcut1</code> as <code>rhaz</code> times<code>h0</code>.</p>
</td></tr>   
<tr><td><code id="PwrGSD_+3A_h1">h1</code></td>
<td>
<p>Alternatively, the arm-1 mortality distribution can be supplied via this
argument by specifying the piecewise constant arm-1 mortality rate. See the comments
above.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_s1">s1</code></td>
<td>
<p>Alternatively, the arm-1 mortality distribution can be supplied via this
argument, in terms of of the corresponding survival function values at the times given
in the vector <code>tcut1</code>. Comments regarding <code>s0</code> above apply here as well. You
must supply exactly one of the following: <code>h1</code>, <code>rhaz</code>, or <code>s1</code>.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tcutc0">tcutc0</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-0 specific censoring
distribution hazard function is constant. The last given component is the left hand
endpoint of the interval having right hand endpoint infinity.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_hc0">hc0</code></td>
<td>
<p>A vector of the same length as <code>tcutc0</code> which specifies the arm-0
censoring distribution in terms of a piecewise constant hazard function.</p>
</td></tr> 
<tr><td><code id="PwrGSD_+3A_sc0">sc0</code></td>
<td>
<p>Alternatively, the arm-0 censoring distribution can be supplied via this
argument, in terms of of the corresponding survival function values at the times
given in the vector <code>tcutc0</code>. See comments above. You must supply either
<code>hc0</code> or <code>sc0</code> but not both.</p>
</td></tr>  
<tr><td><code id="PwrGSD_+3A_tcutc1">tcutc1</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-1 specific censoring
distribution hazard function is constant. The last given component is the left hand
endpoint of the interval having right hand endpoint infinity.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_hc1">hc1</code></td>
<td>
<p>A vector of the same length as <code>tcutc1</code> which specifies the arm-1
censoring distribution in terms of a piecewise constant hazard function.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_sc1">sc1</code></td>
<td>
<p>Alternatively, the arm-1 censoring distribution can be supplied via this
argument, in terms of of the corresponding survival function values at the times given
in the vector <code>tcutc1</code>. See comments above. You must supply either <code>hc1</code> or
<code>sc1</code> but not both.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_noncompliance">noncompliance</code></td>
<td>
<p>(i) Seting <code>noncompliance</code> to &ldquo;none&rdquo; for no
non-compliance will automatically set the non-compliance arguments, below, to
appropriate values for no compliance. This requires no additional user specification
of non-compliance parameters.  (ii) Setting <code>noncompliance</code> to &ldquo;crossover&rdquo;
will automatically set crossover values in the arm 0/1 specific
<em>post-cause-B-delay-mortality</em> for cross-over, i.e. simple interchange of the arm
0 and arm 1 mortalities. The user is required to specify all parameters corresponding
to the arm 0/1 specific <em>cause-B-delay</em> distributions. The <em>cause-A-delay</em>
and <em>post-cause-A-delay-mortality</em> are automatically set so as not to influence
the calculations. Setting <code>noncompliance</code> to &ldquo;mixed&rdquo; will set the arm 0/1
specific <em>post-cause-B-delay-mortality</em> distributions for crossover as defined
above. The user specifies the arm 0/1 specific <em>cause-B-delay</em> distribution as
above, and in addition, all parameters related to the arm 0/1 specific
<em>cause-A-delay</em> distributions and corresponding arm 0/1 specific
<em>post-cause-A-delay-mortality</em> distributions. (iii) Setting <code>noncompliance</code>
to &ldquo;user&rdquo; requires the user to specify all non-compliance distribution
parameters.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tcutd0a">tcutd0A</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-0 specific
<em>cause-A delay</em> distribution hazard function is constant. The last given
component is the left hand endpoint of the interval having right hand endpoint
infinity. Required only when <code>noncompliance</code> is set to &ldquo;mixed&rdquo; or
&ldquo;user&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_hd0a">hd0A</code></td>
<td>
<p>A vector of the same length as <code>tcutd0A</code> containing peicewise constant
hazard rates for the arm-0 <em>cause-A delay</em> distribution.  Required only when
<code>noncompliance</code> is set to &ldquo;mixed&rdquo; or &ldquo;user&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_sd0a">sd0A</code></td>
<td>
<p>When required, the arm-0 <em>cause-A-delay</em> distribution is alternately
specified via a survival function. A vector of the same length as <code>tcutd0A</code>.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tcutd0b">tcutd0B</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-0 specific
<em>cause-B delay</em> distribution hazard function is constant. The last given
component is the left hand endpoint of the interval having right hand endpoint
infinity. Always required when <code>noncompliance</code> is set to any value other than
&ldquo;none&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_hd0b">hd0B</code></td>
<td>
<p>A vector of the same length as <code>tcutd0B</code> containing peicewise constant
hazard rates for the arm-0 <em>cause-B delay</em> distribution. Always required when
<code>noncompliance</code> is set to any value other than &ldquo;none&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_sd0b">sd0B</code></td>
<td>
<p>When required, the arm-0 <em>cause-B-delay</em> distribution is alternately
specified via a survival function.  A vector of the same length as <code>tcutd0B</code>.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tcutd1a">tcutd1A</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-1 specific
<em>cause-A delay</em> distribution hazard function is constant. The last given
component is the left hand endpoint of the interval having right hand endpoint
infinity. Required only when <code>noncompliance</code> is set to &ldquo;mixed&rdquo; or
&ldquo;user&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_hd1a">hd1A</code></td>
<td>
<p>A vector of the same length as <code>tcutd1A</code> containing peicewise constant
hazard rates for the arm-1 <em>cause-A delay</em> distribution. Required only when
<code>noncompliance</code> is set to &ldquo;mixed&rdquo; or &ldquo;user&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_sd1a">sd1A</code></td>
<td>
<p>When required, the arm-1 <em>cause-A-delay</em> distribution is alternately
specified via a survival function. A vector of the same length as <code>tcutd1A</code>.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tcutd1b">tcutd1B</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-1 specific
<em>cause-B delay</em> distribution hazard function is constant. The last given
component is the left hand endpoint of the interval having right hand endpoint
infinity. Always required when <code>noncompliance</code> is set to any value other than
&ldquo;none&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_hd1b">hd1B</code></td>
<td>
<p>A vector of the same length as <code>tcutd1B</code> containing peicewise constant
hazard rates for the arm-1 <em>cause-B delay</em> distribution. Always required when
<code>noncompliance</code> is set to any value other than &ldquo;none&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_sd1b">sd1B</code></td>
<td>
<p>When required, the arm-1 <em>cause-A-delay</em> distribution is alternately
specified via a survival function. A vector of the same length as <code>tcutd1A</code>.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tcutx0a">tcutx0A</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-0 specific
<em>post-cause-A-delay-mortality</em> rate is constant. The last given component is the
left hand endpoint of the interval having right hand endpoint infinity. Required only
when <code>noncompliance</code> is set to &ldquo;mixed&rdquo; or &ldquo;user&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_hx0a">hx0A</code></td>
<td>
<p>A vector of the same length as <code>tcutx0A</code> containing the arm-0
<em>post-cause-A-delay mortality</em> rates.  Required only when <code>noncompliance</code> is
set to &ldquo;mixed&rdquo; or &ldquo;user&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_sx0a">sx0A</code></td>
<td>
<p>When required, the arm-0 <em>post-cause-A-delay mortality</em> distribution is
alternately specified via a survival function. A vector of the same length as
<code>tcutx0A</code>.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tcutx0b">tcutx0B</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-0 specific
<em>post-cause-B-delay-mortality</em> rate is constant. The last given component is the
left hand endpoint of the interval having right hand endpoint infinity. Always
required when <code>noncompliance</code> is set to any value other than &ldquo;none&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_hx0b">hx0B</code></td>
<td>
<p>A vector of the same length as <code>tcutx0B</code> containing the arm-0
<em>post-cause-B-delay mortality</em> rates. Always required when <code>noncompliance</code>
is set to any value other than &ldquo;none&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_sx0b">sx0B</code></td>
<td>
<p>When required, the arm-0 <em>post-cause-B-delay mortality</em> distribution is
alternately specified via a survival function.  A vector of the same length as
<code>tcutx0B</code>.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tcutx1a">tcutx1A</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-1 specific
<em>post-cause-A-delay-mortality</em> rate is constant. The last given component is the
left hand endpoint of the interval having right hand endpoint infinity. Required only
when <code>noncompliance</code> is set to &ldquo;mixed&rdquo; or &ldquo;user&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_hx1a">hx1A</code></td>
<td>
<p>A vector of the same length as <code>tcutx1A</code> containing the arm-1
<em>post-cause-A-delay mortality</em> rates. Required only when <code>noncompliance</code> is
set to &ldquo;mixed&rdquo; or &ldquo;user&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_sx1a">sx1A</code></td>
<td>
<p>When required, the arm-1 <em>post-cause-A-delay mortality</em> distribution is
alternately specified via a survival function. A vector of the same length as
<code>tcutx1A</code>.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_tcutx1b">tcutx1B</code></td>
<td>
<p>Left hand endpoints for intervals upon which the arm-1 specific
<em>post-cause-B-delay-mortality</em> rate is constant. The last given component is the
left hand endpoint of the interval having right hand endpoint infinity. Always
required when <code>noncompliance</code> is set to any value other than &ldquo;none&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_hx1b">hx1B</code></td>
<td>
<p>A vector of the same length as <code>tcutx1B</code> containing the arm-1
<em>post-cause-B-delay mortality</em> rates. Always required when <code>noncompliance</code>
is set to any value other than &ldquo;none&rdquo;.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_sx1b">sx1B</code></td>
<td>
<p>When required, the arm-1 <em>post-cause-B-delay mortality</em> distribution is
alternately specified via a survival function. A vector of the same length as
<code>tcutx1B</code>.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_gradual">gradual</code></td>
<td>
<p>Should the conversion to post-noncompliance mortality be gradual. Under
the default behavior, <code>gradual</code>=<code>FALSE</code>, there is an immediate conversion to
the post-noncompliance mortality rate function. If <code>gradual</code> is set to
<code>TRUE</code> then this conversion is done &ldquo;gradually&rdquo;. In truth, at the
individual level what is done is that the new mortality rate function is a convex
combination of the pre-noncompliance mortality and the post-noncompliance mortality,
with the weighting in proportion to the time spent in compliance with the study arm
protocal.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_wtfun">WtFun</code></td>
<td>
<p>Specifies the name of a weighting function (of time) for assigning relative
weights to events according to the times at which they occur. The default,
&ldquo;FH&rdquo;, a two parameter weight function, specifies the
&lsquo;Fleming-Harrington&rsquo; <code>g-rho</code> family of weighting functions defined as the
pooled arm survival function (Kaplan-Meier estimate) raised to the <code>g</code> times its
complement raised to the <code>rho</code>. Note that <code>g</code>=<code>rho</code>=0 corresponds to
the unweighted log-rank statistic. A second choice is the &ldquo;SFH&rdquo; function, (for
&lsquo;Stopped Fleming-Harrington&rsquo;), meaning that the &ldquo;FH&rdquo; weights are capped
at their value at a user specified time, which has a total of 3 parameters.  A third
choice is <code>Ramp(tcut)</code>. Under this choice, weights are assigned in a linearly
manner from time 0 until a user specified cut-off time, <code>tcut</code>, after which
events are weighted equally. It is possible to conduct computations on <code>nstat</code>
candidate statistics within a single run. In this case, <code>WtFun</code> should be a
character vector of length <code>nstat</code> having components set from among the available
choices.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_ppar">ppar</code></td>
<td>
<p>A vector containing all the weight function parameters, in the order
determined by that of &ldquo;WtFun&rdquo;.  For example, if <code>WtFun</code> is set to
<code>c("FH","SFH","Ramp")</code> then <code>ppar</code> should be a vector of length six, with
the &ldquo;FH&rdquo; parameters in the first two elements, &ldquo;SFH&rdquo; parameters in the
next 3 elements, and &ldquo;Ramp&rdquo; parameter in the last element.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_rr.futility">RR.Futility</code></td>
<td>
<p>The relative risk corresponding to the alternative alternative
hypothesis that is required in the construction of the futility boundary. Required if
<code>Boundary.Futility</code> is set to a non-null value.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_spend.info">Spend.Info</code></td>
<td>
<p>When the test statistic is something other than the unweighted
log-rank statistic, the variance information, i.e. the ratio of variance at interim
analysis to variance at the end of trial, is something other than the ratio of events
at interim analysis to the events at trial end.  The problem is that in practice one
doesn't necessarily have a good idea what the end of trial variance should be.  In
this case the user may wish to spend the type I and type II error probabilities
according to a different time scale. Possible choices are &ldquo;Variance&rdquo;,
(default), which just uses the variance ratio scale, &ldquo;Events&rdquo;, which uses the
events ratio scale, &ldquo;Hybrid(k)&rdquo;, which makes a linear transition from the
&ldquo;Variance&rdquo; scale to the &ldquo;Events&rdquo; scale beginning with analysis number
<code>k</code>.  The last choice, &ldquo;Calendar&rdquo;, uses the calendar time scale</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_qprop.one.or.q">qProp.one.or.Q</code></td>
<td>
<p>If a futility boundary is specified, what assumption should be
made about the drift function (the mean value of the weighted log-rank statistic at
analysis <code>j</code> normalized by the square root of the variance function at analysis
<code>k</code>).  In practice we don't presume to know the shape of the drift function. Set
to &ldquo;one&rdquo; or &ldquo;Q&rdquo;.  The choice &ldquo;one&rdquo; results in a more conservative
boundary.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_nsim">Nsim</code></td>
<td>
<p>If you specify <code>method</code>==&ldquo;S&rdquo;, then you must specify the number
of simulations.  1000 should be sufficient.</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_detail">detail</code></td>
<td>
<p>If you specify <code>method</code>==&ldquo;S&rdquo;, and want to see the full level
of detail regarding arguments returned from the C level code, specify
<code>detail</code>==TRUE</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_stattype">StatType</code></td>
<td>
<p>If you specify <code>method</code>==&ldquo;S&rdquo;, then the available choices are
&ldquo;WLR&rdquo; (weighted log-rank) and &ldquo;ISD&rdquo; (integrated survival difference).</p>
</td></tr>
<tr><td><code id="PwrGSD_+3A_doproj">doProj</code></td>
<td>
<p>Works only when <code>method</code>==&ldquo;S&rdquo;. If a weighted log-rank
statistic is specified without maximum information having been stipulated in the
design then certain functionals, the <code>Q</code> first and second moments, must be
projected. Setting this argument to <code>TRUE</code> includes this projection into the
simulation runs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> Returns a value of class <code>PwrGSD</code> which has components listed below.  Note
that the print method will display a summary table of estimated powers and type I errors
as a <code>nstat</code> by 2 matrix.  The summary method returns the same object invisibly,
but after computing the summary table mentioned above, and it is included in the
returned value as a commponent <code>TBL</code>.  See examples below.
</p>
<table>
<tr><td><code>dPower</code></td>
<td>
<p>A <code>length(tlook)</code> by <code>nstat</code> matrix containing in each column,
an increment in power that resulted at that analysis time for the given statistic.</p>
</td></tr>
<tr><td><code>dErrorI</code></td>
<td>
<p>A <code>length(tlook)</code> by <code>nstat</code> matrix containing in each column,
an increment in type I error that resulted at that analysis time for the given
statistic.  Always sums to the total alpha specified in <code>alphatot</code></p>
</td></tr>
<tr><td><code>detail</code></td>
<td>
<p>A list with components equal to the arguments of the C-call, which
correspond in a natural way to the arguments specified in the R call, along with the
computed results in <code>palpha0vec</code>, <code>palpha1vec</code>, <code>inffrac</code>, and
<code>mu</code>.  The first two are identical to <code>dErrorI</code> and <code>dPower</code>, explained
above.  The last two are <code>length(tlook)</code> by <code>nstat</code> matrices. For each
statistic specified in <code>par</code>, the corresponding columns of <code>pinffrac</code> and
<code>mu</code> contain the information fraction and drift at each of the analysis times.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Grant Izmirlian <a href="mailto:izmirlian@nih.gov">izmirlian@nih.gov</a></p>


<h3>References</h3>

<p> Gu, M.-G. and Lai, T.-L.  &ldquo;Determination of power and sample size in
the design of clinical trials with failure-time endpoints and interim analyses.&rdquo;
Controlled Clinical Trials 20 (5): 423-438. 1999
</p>
<p>Izmirlian, G.  &ldquo;The PwrGSD package.&rdquo;  NCI Div. of Cancer Prevention Technical 
Report. 2004
</p>
<p>Jennison, C. and Turnbull, B.W. (1999) Group Sequential Methods: Applications to
Clinical Trials Chapman &amp; Hall/Crc, Boca Raton FL
</p>
<p>Proschan, M.A., Lan, K.K.G., Wittes, J.T.  (2006), corr 2nd printing (2008) Statistical
Monitoring of Clinical Trials A Unified Approach Springer Verlag, New
York 
</p>
<p>Izmirlian G. (2014). Estimation of the relative risk following group
sequential procedure based upon the weighted log-rank statistic.
Statistics and its Interface 7(1), 27-42
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cpd.PwrGSD">cpd.PwrGSD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(PwrGSD)

test.example &lt;-
  PwrGSD(EfficacyBoundary = LanDemets(alpha = 0.05, spending = ObrienFleming),
         FutilityBoundary = LanDemets(alpha = 0.1, spending = ObrienFleming),
	 RR.Futility = 0.82, sided="1&lt;",method="A",accru =7.73, accrat =9818.65,
         tlook =c(7.14, 8.14, 9.14, 10.14, 10.64, 11.15, 12.14, 13.14,
                  14.14, 15.14, 16.14, 17.14, 18.14, 19.14, 20.14),
         tcut0 =0:19, h0 =c(rep(3.73e-04, 2), rep(7.45e-04, 3),
                            rep(1.49e-03, 15)),
         tcut1 =0:19, rhaz =c(1, 0.9125, 0.8688, 0.7814, 0.6941,
                              0.6943, 0.6072, 0.5202, 0.4332, 0.6520,
                              0.6524, 0.6527, 0.6530, 0.6534, 0.6537,
                              0.6541, 0.6544, 0.6547, 0.6551, 0.6554),
         tcutc0 =0:19, hc0 =c(rep(1.05e-02, 2), rep(2.09e-02, 3),
                              rep(4.19e-02, 15)),
         tcutc1 =0:19, hc1 =c(rep(1.05e-02, 2), rep(2.09e-02, 3),
                              rep(4.19e-02, 15)),
         tcutd0B =c(0, 13), hd0B =c(0.04777, 0),
         tcutd1B =0:6, hd1B =c(0.1109, 0.1381, 0.1485, 0.1637, 0.2446,
                               0.2497, 0),
         noncompliance =crossover, gradual =TRUE,
         WtFun =c("FH", "SFH", "Ramp"),
         ppar =c(0, 1, 0, 1, 10, 10))
</code></pre>

<hr>
<h2 id='RCM2RR'>Relative cumulative mortality to Relative Risk</h2><span id='topic+RCM2RR'></span>

<h3>Description</h3>

<p>Given the relative cumulative mortality (ratio of CDFs),
the baseline hazard and censoring hazard at a grid of time points,
calculates the corresponding risk ratio at a second specified
grid of time points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  RCM2RR(tlook, tcut.i, h.i, hOth, accru, rcm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RCM2RR_+3A_tlook">tlook</code></td>
<td>
<p>Second grid of time points at which you desire risk ratios</p>
</td></tr>
<tr><td><code id="RCM2RR_+3A_tcut.i">tcut.i</code></td>
<td>
<p>First grid of time points at which baseline hazard,
censoring hazard and relative cumulative mortality are specified
(left hand endpoints of intervals)</p>
</td></tr>
<tr><td><code id="RCM2RR_+3A_h.i">h.i</code></td>
<td>
<p>Values of baseline hazard on intervals given by tcut.i</p>
</td></tr>
<tr><td><code id="RCM2RR_+3A_hoth">hOth</code></td>
<td>
<p>Values of censoring hazard on intervals given by tcut.i</p>
</td></tr>
<tr><td><code id="RCM2RR_+3A_accru">accru</code></td>
<td>
<p>Time at which uniform accrual is completed (starts at 0)</p>
</td></tr>
<tr><td><code id="RCM2RR_+3A_rcm">rcm</code></td>
<td>
<p>Values of relative cumulative mortality (ratio of CDFs) on
interals given by tcut.i</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Values of risk ratio on intervals given by tlook
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>

<hr>
<h2 id='RR2RCM'>Relative risk to Relative Cumulative Mortality</h2><span id='topic+RR2RCM'></span>

<h3>Description</h3>

<p>Relative risk to Relative Cumulative Mortality
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RR2RCM(tlook, tcut.i, tcut.ii, h, rr, hOth, accru)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RR2RCM_+3A_tlook">tlook</code></td>
<td>
<p>Grid of time points at which you desire cumlative
relative mortality</p>
</td></tr>
<tr><td><code id="RR2RCM_+3A_tcut.i">tcut.i</code></td>
<td>
<p>Grid of time points at which baseline hazard,
censoring hazard and relative cumulative mortality are specified
(left hand endpoints of intervals)</p>
</td></tr>
<tr><td><code id="RR2RCM_+3A_tcut.ii">tcut.ii</code></td>
<td>
<p>Grid of time points at which study arm hazard
is specified (left hand endpoints of intervals)</p>
</td></tr>
<tr><td><code id="RR2RCM_+3A_h">h</code></td>
<td>
<p>Values of baseline hazard on intervals given by tcut.i</p>
</td></tr>
<tr><td><code id="RR2RCM_+3A_rr">rr</code></td>
<td>
<p>Values of risk ratio on intervals given by tcut.i</p>
</td></tr>
<tr><td><code id="RR2RCM_+3A_hoth">hOth</code></td>
<td>
<p>Values of censoring hazard on intervals given by tcut.i</p>
</td></tr>
<tr><td><code id="RR2RCM_+3A_accru">accru</code></td>
<td>
<p>Time at which uniform accrual is completed (starts at 0)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Values of relative cumulative mortality (ratio of CDFs) on interals
given by tlook
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov</p>

<hr>
<h2 id='SC'>The Stochastic Curtailment method of Boundary Construction</h2><span id='topic+SC'></span>

<h3>Description</h3>

<p>The function <code>SC</code> is used in calls to the functions
<code>GrpSeqBnds</code> and <code>PwrGSD</code> as a possible setting for the
arguments <code>EfficacyBoundary</code> and <code>FutilityBoundary</code>, in
specification of the method whereby efficacy and or futility
boundaries are to be constructed. The Stochastic Curtailment method is
one of four currently availiable choices, the others being
<code>LanDemets</code>, <code>Haybittle</code> (efficacy only) and user specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SC(be.end, prob, drift.end = NULL, from = NULL, to = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SC_+3A_be.end">be.end</code></td>
<td>
<p>The value of the efficacy criterion in the scale of a
standardized normal. This should be set to something further from
the null than the single test $Z_alpha$. For example if the total type
I error probability is 0.05 in a two sided test of the null than
set be.end to 2.10 or larger (instead of 1.96).</p>
</td></tr>
<tr><td><code id="SC_+3A_prob">prob</code></td>
<td>
<p>The criterion, a probability to be exceeded in
order to stop. 0.90 or above is a good choice. See detail below.</p>
</td></tr>
<tr><td><code id="SC_+3A_drift.end">drift.end</code></td>
<td>
<p>Required only if you are using <code>SC</code> to set the
<code>FutilityBoundary</code>. In this case, set <code>drift.end</code> to the
value of the drift function anticipated at the end of the
trial. See detail below.</p>
</td></tr>
<tr><td><code id="SC_+3A_from">from</code></td>
<td>
<p>WARNING EXPERIMENTAL: you can actually construct boundaries via a
hybrid of the 3 boundary construction methods, <code>LanDemets</code>, <code>SC</code>,
and 'user specified'. When using a hybrid boundry, set the argument
<code>EfficacyBoundary</code> or <code>FutilityBoundary</code> respectively,
to a list with components <code>LanDemets</code>, <code>SC</code>, or user
specified numbers. In the former two cases, <code>from</code> and
<code>to</code> are used in <code>LanDemets</code> and also in <code>SC</code>
to stipulate how many interim analyses they are in effect.
See the help for <code>GrpSeqBnds</code> and <code>PwrGSD</code></p>
</td></tr>
<tr><td><code id="SC_+3A_to">to</code></td>
<td>
<p>See above.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When the stochastic curtailment procedure is used to construct the
efficacy boundary, i.e. <code>EfficacyBoundary</code>=<code>SC(...)</code>,
the efficacy criterion is reached when the conditional probability,
under the null hypothesis, that the last analysis results in
statistical significance, given the present value of the statistic,
exceeds 'prob'. In of itself, this doesn't produce a boundary on the
scale of a standard normal, but it is easily converted to one as is
done here. When this is used to construct a futility boundary, i.e.
<code>FutilityBoundary</code>=<code>SC(...)</code>, the futility criterion is
reached when the conditional probability, under the design alternative
hypothesis, that the last analysis does not result in statistical
significance, given the present value of the statistic, exceeds 'prob'.
The design alternative corresponds to a drift function, which is the
expected value of the statistic normalized to have variance equal to
the information fraction at each interim analysis. For the unweighted
log-rank statistic, the drift function is $(V_T)^(1/2)$ B f, where
B is the logged relative risk, $V_T$ is the variance at the end of the
trial and f is the information fraction.  If the two trial arms are
balanced and the number at risk is roughly constant throughout the
trial then $V_T = pi (1-pi) N_T$, where $pi$ is the constant proportion
at risk in one of the trial arms and $N_T$ is the anticipated number of
events.
</p>


<h3>Value</h3>

<p>An object of class <code>boundary.construction.method</code> which is really a list
with the following components. The print method displays the original
call.
</p>
<table>
<tr><td><code>type</code></td>
<td>
<p>Gives the boundary construction method type, which is the character
string &quot;SC&quot;</p>
</td></tr>
<tr><td><code>be.end</code></td>
<td>
<p>The numeric value passed to the argument 'be.end', which is the 
value of the efficacy criterion in the scale of a standardized normal.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>The numeric value passed to the argument 'prob', which is the probability to
be exceeded in order to stop.</p>
</td></tr>
<tr><td><code>drift.end</code></td>
<td>
<p>The numeric value passed to the argument 'drift.end', which is the
value of the drift function at the end of the trial. See details.</p>
</td></tr>
<tr><td><code>from</code></td>
<td>
<p>The numeric value passed to the argument 'from'. See above.</p>
</td></tr>
<tr><td><code>to</code></td>
<td>
<p>The numeric value passed to the argument 'to'. See above.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>returns the call</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The print method returns the call by default</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian</p>


<h3>References</h3>

<p>see references under <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>See Also</h3>

<p><code><a href="#topic+LanDemets">LanDemets</a></code>, <code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code>, <code><a href="#topic+PwrGSD">PwrGSD</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## example 1: what is the result of calling a Boundary Construction Method function
    ## A call to 'SC' just returns the call
    SC(be.end=2.10, prob=0.90)
    
    ## It does arguement checking...this results in an error
    ## Not run: 
      SC(be.end=2.10)
    
## End(Not run)
    
    ## but really its value is a list with the a component containing
    ## the boundary method type, "LanDemts", and components for each
    ## of the arguments.
    names(SC(be.end=2.10, prob=0.90))

    SC(be.end=2.10, prob=0.90, drift.end=2.34)$type
    SC(be.end=2.10, prob=0.90, drift.end=2.34)$be.end
    SC(be.end=2.10, prob=0.90, drift.end=2.34)$prob
    SC(be.end=2.10, prob=0.90, drift.end=2.34)$drift.end

## example 2: ...But the intended purpose of the spending functions is
## in constructing calls to 'GrpSeqBnds' and to 'PwrGSD':
     

    frac &lt;- c(0.07614902,0.1135391,0.168252,0.2336901,0.3186155,
              0.4164776,0.5352199,0.670739,0.8246061,1)
    drift &lt;- c(0.3836636,0.5117394,0.6918584,0.8657705,1.091984,
	      1.311094,1.538582,1.818346,2.081775,2.345386)

    test &lt;- GrpSeqBnds(frac=frac, EfficacyBoundary=LanDemets(alpha=0.05, spending=ObrienFleming),
                       FutilityBoundary=SC(be.end=2.10, prob=0.90, drift.end=drift[10]),
                       drift=drift)
</code></pre>

<hr>
<h2 id='SCtoBdry'>Converts a stochastic curtailment boundary (conditional type I or
II error probability) into a (efficacy or futility) boundary on the
standardized Z scale</h2><span id='topic+SCtoBdry'></span>

<h3>Description</h3>

<p>Converts a stochastic curtailment boundary (conditional type I or
II error probability) into a (efficacy or futility) boundary on the
standardized Z scale</p>


<h3>Usage</h3>

<pre><code class='language-R'>SCtoBdry(prob, frac, be.end, drift = NULL, drift.end = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SCtoBdry_+3A_prob">prob</code></td>
<td>
<p>The stochastic curtailment thresh-hold probability, which
is the complement of the type I (efficacy) or II (futility) error.
We typically use 0.90 which will stop for efficacy if the
probability under the null that the final analysis results in an
efficacious decision given the data so far exceeds 0.90, and stops
for futility of the probability under the alternative corresponding
to the drift arguments, that the final analysis results in a
futility decision given the data so far, exceeds 0.90. </p>
</td></tr> 
<tr><td><code id="SCtoBdry_+3A_frac">frac</code></td>
<td>
<p>The variance ratio. See the <code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code>
documentation for details.</p>
</td></tr>
<tr><td><code id="SCtoBdry_+3A_be.end">be.end</code></td>
<td>
<p>Value of efficacy (futility) boundary at the final analysis</p>
</td></tr>
<tr><td><code id="SCtoBdry_+3A_drift">drift</code></td>
<td>
<p>The drift function. See the <code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code>
documentation for details.</p>
</td></tr>
<tr><td><code id="SCtoBdry_+3A_drift.end">drift.end</code></td>
<td>
<p>Required if using a futility boundary. This is the
value of the drift function at the final analysis. Must be projected
using the trial design.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A efficacy or futility boundary on the standard normal scale</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  ## Here we show how to convert a stochastic curtailment procedure for
  ## futility into a futility boundary on the standard normal scale
  library(PwrGSD)

  ## Values of the information fraction at interim analyses --
  ## the sequence does not have to include the last analysis
  frac &lt;- c(0.16, 0.32, 0.54, 0.83, 1.0)

  ## values drift at interim analyses corresponding to values of
  ## frac given above
  drift &lt;- c(0.69, 1.09, 1.54, 2.08, 2.35)

  ## value of the drift at the final analysis (from the design or
  ## projected
  drift.end &lt;- drift[5]

  ## value of the efficacy boundary at the final analysis 
  be.end &lt;- 1.69

  ## stochastic curtailment threshhold probability -- if the probability of rejecting the
  ## null hypothesis by the scheduled end of the trial, under the alternative hypothesis,
  ## and conditional upon the current value of the statistic, is not greater than
  ## prob.thresh, then stop for futility.
  prob.thresh &lt;- 0.90

  ## computes equivalent futility boundary points on the standard normal scale
  SCtoBdry(prob.thresh, frac=frac, be.end=be.end, drift=drift, drift.end=drift.end)
</code></pre>

<hr>
<h2 id='SimGSB'>Verifies the results of &quot;GrpSeqBnds&quot; via simulation</h2><span id='topic+SimGSB'></span>

<h3>Description</h3>

<p>Verifies the results of <code>GrpSeqBnds</code> via simulation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SimGSB(object, nsim = 1e+05, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SimGSB_+3A_object">object</code></td>
<td>
<p>an object of class either <code>boundaries</code> or <code>PwrGSD</code></p>
</td></tr>
<tr><td><code id="SimGSB_+3A_nsim">nsim</code></td>
<td>
<p>number of simulations to do</p>
</td></tr>
<tr><td><code id="SimGSB_+3A_...">...</code></td>
<td>
<p>if <code>object</code> is of class <code>PwrGSD</code> and there are
more than one statistic under investigation, then you may specify an
argument <code>stat</code>. The default value is 1, meaning the first one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tabulation of the results
</p>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov</p>


<h3>See Also</h3>

<p><code><a href="#topic+GrpSeqBnds">GrpSeqBnds</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## none as yet
</code></pre>

<hr>
<h2 id='wtdlogrank'>Weighted log-rank test</h2><span id='topic+wtdlogrank'></span>

<h3>Description</h3>

<p>Computes a two sample weighted log-rank statistic with events weighted
according to one of the available weighting function choices</p>


<h3>Usage</h3>

<pre><code class='language-R'>  wtdlogrank(formula =formula(data), data =parent.frame(), WtFun = c("FH", "SFH", "Ramp"),
  param = c(0, 0), sided = c(2, 1), subset, na.action, w = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wtdlogrank_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>Surv(Time, Event) ~ arm</code> where <code>arm</code>
is a dichotomous variable with values 0 and 1.</p>
</td></tr>
<tr><td><code id="wtdlogrank_+3A_data">data</code></td>
<td>
<p>a dataframe</p>
</td></tr>
<tr><td><code id="wtdlogrank_+3A_wtfun">WtFun</code></td>
<td>
<p>a selection from the available list: &ldquo;FH&rdquo; (Fleming-Harrington),
&ldquo;SFH&rdquo; (stopped Fleming-Harrington) or &ldquo;Ramp&rdquo;.  See <code>param</code> in
the following line.</p>
</td></tr>
<tr><td><code id="wtdlogrank_+3A_param">param</code></td>
<td>
<p>Weight function parameters. Length and interpretation depends upon
the selected value of <code>WtFun</code>:<br />
If <code>WtFun</code>==&ldquo;FH&rdquo; then <code>param</code> is a length 2 vector specifying
the power of the pooled (across arms) kaplan meier estimate and its complement.<br />
If <code>WtFun</code>==&ldquo;SFH&rdquo; then <code>param</code> is a length 3 vector with
first two components as in the &ldquo;FH&rdquo; case, and third component the time (in
the same units as the time to event) at which the &ldquo;FH&rdquo; weight function is
capped off at its current value.<br />
If <code>WtFun</code>==&ldquo;Ramp&rdquo; then <code>param</code> is of length 1 specifying the
time (same units as time to event) at which events begin to get equal weight. The
&ldquo;Ramp&rdquo; weight function is a linearly increasing deterministic weight function
which is capped off at 1 at the user specified time.
</p>
</td></tr>
<tr><td><code id="wtdlogrank_+3A_sided">sided</code></td>
<td>
<p>One or Two sided test?  Set to 1 or 2</p>
</td></tr>
<tr><td><code id="wtdlogrank_+3A_subset">subset</code></td>
<td>
<p>Analysis can be applied to a subset of the dataframe based upon a logical
expression in its variables</p>
</td></tr>
<tr><td><code id="wtdlogrank_+3A_na.action">na.action</code></td>
<td>
<p>Method for handling <code>NA</code> values in the covariate, <code>arm</code></p>
</td></tr>
<tr><td><code id="wtdlogrank_+3A_w">w</code></td>
<td>
<p>currently no effect</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>survtest</code> containing components
</p>
<table>
<tr><td><code>pn</code></td>
<td>
<p>sample size</p>
</td></tr>
<tr><td><code>wttyp</code></td>
<td>
<p>internal representation of the <code>WtFun</code> argument</p>
</td></tr>
<tr><td><code>par</code></td>
<td>
<p>internal representation of the <code>param</code> argument</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>unique times of events accross all arms</p>
</td></tr>
<tr><td><code>nrisk</code></td>
<td>
<p>number at risk accross all arms at each event time</p>
</td></tr>
<tr><td><code>nrisk1</code></td>
<td>
<p>Number at risk in the experimental arm at each event time</p>
</td></tr>
<tr><td><code>nevent</code></td>
<td>
<p>Number of events accross all arms at each event time</p>
</td></tr>
<tr><td><code>nevent1</code></td>
<td>
<p>Number of events in the experimental arm at each event time</p>
</td></tr>
<tr><td><code>wt</code></td>
<td>
<p>Values of the weight function at each event time</p>
</td></tr>
<tr><td><code>pntimes</code></td>
<td>
<p>Number of event times</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>
<p>The un-normalized weighted log-rank statistic, i.e. the summed
weighted observed minus expected differences at each event time</p>
</td></tr>
<tr><td><code>var</code></td>
<td>
<p>Variance estimate for the above</p>
</td></tr>
<tr><td><code>UQt</code></td>
<td>
<p>Cumulative sum of increments in the sum resulting in <code>stat</code> above</p>
</td></tr>
<tr><td><code>varQt</code></td>
<td>
<p>Cumulative sum of increments in the sum resulting in <code>var</code> above</p>
</td></tr>
<tr><td><code>var1t</code></td>
<td>
<p>Cumulative sum of increments in the sum resulting in the variance of an
unweighted version of the statistic</p>
</td></tr>
<tr><td><code>pu0</code></td>
<td>
<p>person units of follow-up time in the control arm</p>
</td></tr>
<tr><td><code>pu1</code></td>
<td>
<p>person units of follow-up time in the intervention arm</p>
</td></tr>
<tr><td><code>n0</code></td>
<td>
<p>events in the control arm</p>
</td></tr>
<tr><td><code>n1</code></td>
<td>
<p>events in the intervention arm</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>sample size, same as <code>pn</code></p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that created the object</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Grant Izmirlian &lt;izmirlian@nih.gov&gt;</p>


<h3>References</h3>

<p>Harrington, D. P. and Fleming, T. R. (1982).
A class of rank test procedures for censored survival data.
<em>Biometrika</em>
<b>69</b>, 553-566.</p>


<h3>See Also</h3>

<p><code><a href="#topic+IntSurvDiff">IntSurvDiff</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(PwrGSD)
  data(lung)
  fit.wlr &lt;-wtdlogrank(Surv(time, I(status==2))~I(sex==2), data=lung, WtFun="SFH", param=c(0,1,300))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
