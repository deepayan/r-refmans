<!DOCTYPE html><html><head><title>Help for package quanteda</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {quanteda}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#%&gt;%'><p>Pipe operator</p></a></li>
<li><a href='#as.character.corpus'><p>Coercion and checking methods for corpus objects</p></a></li>
<li><a href='#as.data.frame.dfm'><p>Convert a dfm to a data.frame</p></a></li>
<li><a href='#as.dfm'><p>Coercion and checking functions for dfm objects</p></a></li>
<li><a href='#as.dictionary'><p>Coercion and checking functions for dictionary objects</p></a></li>
<li><a href='#as.fcm'><p>Coercion and checking functions for fcm objects</p></a></li>
<li><a href='#as.list.tokens'><p>Coercion, checking, and combining functions for tokens objects</p></a></li>
<li><a href='#as.matrix.dfm'><p>Coerce a dfm to a matrix or data.frame</p></a></li>
<li><a href='#as.yaml'><p>Convert quanteda dictionary objects to the YAML format</p></a></li>
<li><a href='#attributes&lt;-'><p>Function extending base::attributes()</p></a></li>
<li><a href='#bootstrap_dfm'><p>Bootstrap a dfm</p></a></li>
<li><a href='#cbind.dfm'><p>Combine dfm objects by Rows or Columns</p></a></li>
<li><a href='#char_select'><p>Select or remove elements from a character vector</p></a></li>
<li><a href='#char_tolower'><p>Convert the case of character objects</p></a></li>
<li><a href='#check_class'><p>Check object class for functions</p></a></li>
<li><a href='#check_dots'><p>Check arguments passed to other functions via ...</p></a></li>
<li><a href='#check_integer'><p>Validate input vectors</p></a></li>
<li><a href='#convert'><p>Convert quanteda objects to non-quanteda formats</p></a></li>
<li><a href='#convert-wrappers'><p>Convenience wrappers for dfm convert</p></a></li>
<li><a href='#corpus'><p>Construct a corpus object</p></a></li>
<li><a href='#corpus_group'><p>Combine documents in corpus by a grouping variable</p></a></li>
<li><a href='#corpus_reshape'><p>Recast the document units of a corpus</p></a></li>
<li><a href='#corpus_sample'><p>Randomly sample documents from a corpus</p></a></li>
<li><a href='#corpus_segment'><p>Segment texts on a pattern match</p></a></li>
<li><a href='#corpus_subset'><p>Extract a subset of a corpus</p></a></li>
<li><a href='#corpus_trim'><p>Remove sentences based on their token lengths or a pattern match</p></a></li>
<li><a href='#corpus-class'><p>Base method extensions for corpus objects</p></a></li>
<li><a href='#data_char_sampletext'><p>A paragraph of text for testing various text-based functions</p></a></li>
<li><a href='#data_char_ukimmig2010'><p>Immigration-related sections of 2010 UK party manifestos</p></a></li>
<li><a href='#data_corpus_inaugural'><p>US presidential inaugural address texts</p></a></li>
<li><a href='#data_dfm_lbgexample'><p>dfm from data in Table 1 of Laver, Benoit, and Garry (2003)</p></a></li>
<li><a href='#data_dictionary_LSD2015'><p>Lexicoder Sentiment Dictionary (2015)</p></a></li>
<li><a href='#data-internal'><p>Internal data sets</p></a></li>
<li><a href='#data-relocated'><p>Formerly included data objects</p></a></li>
<li><a href='#dfm'><p>Create a document-feature matrix</p></a></li>
<li><a href='#dfm_compress'><p>Recombine a dfm or fcm by combining identical dimension elements</p></a></li>
<li><a href='#dfm_group'><p>Combine documents in a dfm by a grouping variable</p></a></li>
<li><a href='#dfm_lookup'><p>Apply a dictionary to a dfm</p></a></li>
<li><a href='#dfm_match'><p>Match the feature set of a dfm to given feature names</p></a></li>
<li><a href='#dfm_replace'><p>Replace features in dfm</p></a></li>
<li><a href='#dfm_sample'><p>Randomly sample documents from a dfm</p></a></li>
<li><a href='#dfm_select'><p>Select features from a dfm or fcm</p></a></li>
<li><a href='#dfm_sort'><p>Sort a dfm by frequency of one or more margins</p></a></li>
<li><a href='#dfm_subset'><p>Extract a subset of a dfm</p></a></li>
<li><a href='#dfm_tfidf'><p>Weight a dfm by <em>tf-idf</em></p></a></li>
<li><a href='#dfm_tolower'><p>Convert the case of the features of a dfm and combine</p></a></li>
<li><a href='#dfm_trim'><p>Trim a dfm using frequency threshold-based feature selection</p></a></li>
<li><a href='#dfm_weight'><p>Weight the feature frequencies in a dfm</p></a></li>
<li><a href='#dfm-class'><p>Virtual class &quot;dfm&quot; for a document-feature matrix</p></a></li>
<li><a href='#dfm-internal'><p>Internal functions for dfm objects</p></a></li>
<li><a href='#dfm2lsa'><p>Convert a dfm to an lsa &quot;textmatrix&quot;</p></a></li>
<li><a href='#dictionary'><p>Create a dictionary</p></a></li>
<li><a href='#dictionary2-class'><p>dictionary class objects and functions</p></a></li>
<li><a href='#docfreq'><p>Compute the (weighted) document frequency of a feature</p></a></li>
<li><a href='#docnames'><p>Get or set document names</p></a></li>
<li><a href='#docvars'><p>Get or set document-level variables</p></a></li>
<li><a href='#escape_regex'><p>Internal function for <code>select_types()</code> to escape regular expressions</p></a></li>
<li><a href='#expand'><p>Simpler and faster version of expand.grid() in base package</p></a></li>
<li><a href='#fcm'><p>Create a feature co-occurrence matrix</p></a></li>
<li><a href='#fcm_sort'><p>Sort an fcm in alphabetical order of the features</p></a></li>
<li><a href='#fcm-class'><p>Virtual class &quot;fcm&quot; for a feature co-occurrence matrix</p></a></li>
<li><a href='#featfreq'><p>Compute the frequencies of features</p></a></li>
<li><a href='#featnames'><p>Get the feature labels from a dfm</p></a></li>
<li><a href='#field_system'><p>Shortcut functions to access or assign metadata</p></a></li>
<li><a href='#flatten_dictionary'><p>Flatten a hierarchical dictionary into a list of character vectors</p></a></li>
<li><a href='#flatten_list'><p>Internal function to flatten a nested list</p></a></li>
<li><a href='#format_sparsity'><p>format a sparsity value for printing</p></a></li>
<li><a href='#get_docvars'><p>Internal function to extract docvars</p></a></li>
<li><a href='#get_object_version'><p>Get the package version that created an object</p></a></li>
<li><a href='#groups'><p>Grouping variable(s) for various functions</p></a></li>
<li><a href='#head.dfm'><p>Return the first or last part of a dfm</p></a></li>
<li><a href='#index'><p>Locate a pattern in a tokens object</p></a></li>
<li><a href='#is_glob'><p>Check if patterns contains glob wildcard</p></a></li>
<li><a href='#is_indexed'><p>Check if a glob pattern is indexed by index_types</p></a></li>
<li><a href='#is_regex'><p>Check if a string is a regular expression</p></a></li>
<li><a href='#is.collocations'><p>Check if an object is collocations</p></a></li>
<li><a href='#kwic'><p>Locate keywords-in-context</p></a></li>
<li><a href='#list2dictionary'><p>Internal function to convert a list to a dictionary</p></a></li>
<li><a href='#lowercase_dictionary_values'><p>Internal function to lowercase dictionary values</p></a></li>
<li><a href='#make_docvars'><p>Internal function to make new system-level docvars</p></a></li>
<li><a href='#make_meta'><p>Internal functions to create a list of the meta fields</p></a></li>
<li><a href='#matrix2dfm'><p>Converts a Matrix to a dfm</p></a></li>
<li><a href='#matrix2fcm'><p>Converts a Matrix to a fcm</p></a></li>
<li><a href='#merge_dictionary_values'><p>Internal function to merge values of duplicated keys</p></a></li>
<li><a href='#message_error'><p>Return an error message</p></a></li>
<li><a href='#meta'><p>Get or set object metadata</p></a></li>
<li><a href='#meta_system'><p>Internal function to get, set or initialize system metadata</p></a></li>
<li><a href='#msg'><p>Conditionally format messages</p></a></li>
<li><a href='#names-quanteda'><p>Special handling for names of quanteda objects</p></a></li>
<li><a href='#ndoc'><p>Count the number of documents or features</p></a></li>
<li><a href='#nest_dictionary'><p>Utility function to generate a nested list</p></a></li>
<li><a href='#nsentence'><p>Count the number of sentences</p></a></li>
<li><a href='#ntoken'><p>Count the number of tokens or types</p></a></li>
<li><a href='#object-builders'><p>Object builders</p></a></li>
<li><a href='#object2id'><p>Match quanteda objects against token types</p></a></li>
<li><a href='#pattern'><p>Pattern for feature, token and keyword matching</p></a></li>
<li><a href='#pattern2id'><p>Match patterns against token types</p></a></li>
<li><a href='#phrase'><p>Declare a pattern to be a sequence of separate patterns</p></a></li>
<li><a href='#print-methods'><p>Print methods for quanteda core objects</p></a></li>
<li><a href='#print.phrases'><p>Print a phrase object</p></a></li>
<li><a href='#quanteda_options'><p>Get or set package options for quanteda</p></a></li>
<li><a href='#quanteda-package'><p>An R package for the quantitative analysis of textual data</p></a></li>
<li><a href='#read_dict_functions'><p>Internal functions to import dictionary files</p></a></li>
<li><a href='#readtext-methods'><p>Extensions for readtext objects</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#remove_empty_keys'><p>Utility function to remove empty keys</p></a></li>
<li><a href='#replace_dictionary_values'><p>Internal function to replace dictionary values</p></a></li>
<li><a href='#resample'><p>Sample a vector</p></a></li>
<li><a href='#reshape_docvars'><p>Internal function to subset or duplicate docvar rows</p></a></li>
<li><a href='#search_glob'><p>Select types without performing slow regex search</p></a></li>
<li><a href='#search_index'><p>Internal function for <code>select_types</code> to search the index using</p>
fastmatch.</a></li>
<li><a href='#serialize_tokens'><p>Function to serialize list-of-character tokens</p></a></li>
<li><a href='#set_dfm_dimnames&lt;-'><p>Internal functions to set dimnames</p></a></li>
<li><a href='#spacyr-methods'><p>Extensions for and from spacy_parse objects</p></a></li>
<li><a href='#sparsity'><p>Compute the sparsity of a document-feature matrix</p></a></li>
<li><a href='#split_values'><p>Internal function for special handling of multi-word dictionary values</p></a></li>
<li><a href='#summary_metadata'><p>Functions to add or retrieve corpus summary metadata</p></a></li>
<li><a href='#summary.corpus'><p>Summarize a corpus</p></a></li>
<li><a href='#textmodels'><p>Models for scaling and classification of textual data</p></a></li>
<li><a href='#textplots'><p>Plots for textual data</p></a></li>
<li><a href='#texts'><p>Get or assign corpus texts [deprecated]</p></a></li>
<li><a href='#textstats'><p>Statistics for textual data</p></a></li>
<li><a href='#tokenize_custom'><p>Customizable tokenizer</p></a></li>
<li><a href='#tokenize_internal'><p>quanteda tokenizers</p></a></li>
<li><a href='#tokens'><p>Construct a tokens object</p></a></li>
<li><a href='#tokens_chunk'><p>Segment tokens object by chunks of a given size</p></a></li>
<li><a href='#tokens_compound'><p>Convert token sequences into compound tokens</p></a></li>
<li><a href='#tokens_group'><p>Combine documents in a tokens object by a grouping variable</p></a></li>
<li><a href='#tokens_lookup'><p>Apply a dictionary to a tokens object</p></a></li>
<li><a href='#tokens_ngrams'><p>Create n-grams and skip-grams from tokens</p></a></li>
<li><a href='#tokens_recompile'><p>recompile a serialized tokens object</p></a></li>
<li><a href='#tokens_replace'><p>Replace tokens in a tokens object</p></a></li>
<li><a href='#tokens_restore'><p>Restore special tokens</p></a></li>
<li><a href='#tokens_sample'><p>Randomly sample documents from a tokens object</p></a></li>
<li><a href='#tokens_segment'><p>Segment tokens object by patterns</p></a></li>
<li><a href='#tokens_select'><p>Select or remove tokens from a tokens object</p></a></li>
<li><a href='#tokens_split'><p>Split tokens by a separator pattern</p></a></li>
<li><a href='#tokens_subset'><p>Extract a subset of a tokens</p></a></li>
<li><a href='#tokens_tolower'><p>Convert the case of tokens</p></a></li>
<li><a href='#tokens_wordstem'><p>Stem the terms in an object</p></a></li>
<li><a href='#tokens-class'><p>Base method extensions for tokens objects</p></a></li>
<li><a href='#topfeatures'><p>Identify the most frequent features in a dfm</p></a></li>
<li><a href='#types'><p>Get word types from a tokens object</p></a></li>
<li><a href='#unlist_character'><p>Unlist a list of character vectors safely</p></a></li>
<li><a href='#unlist_integer'><p>Unlist a list of integer vectors safely</p></a></li>
<li><a href='#valuetype'><p>Pattern matching using valuetype</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>3.3.1</td>
</tr>
<tr>
<td>Title:</td>
<td>Quantitative Analysis of Textual Data</td>
</tr>
<tr>
<td>Description:</td>
<td>A fast, flexible, and comprehensive framework for 
    quantitative text analysis in R.  Provides functionality for corpus management,
    creating and manipulating tokens and n-grams, exploring keywords in context, 
    forming and manipulating sparse matrices
    of documents by features and feature co-occurrences, analyzing keywords, computing feature similarities and
    distances, applying content dictionaries, applying supervised and unsupervised machine learning, 
    visually representing text and text analyses, and more. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>fastmatch, jsonlite, magrittr, Matrix (&ge; 1.5-0), Rcpp (&ge;
0.12.12), RcppParallel, SnowballC, stopwords, stringi, xml2,
yaml</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppParallel, RcppArmadillo (&ge; 0.7.600.1.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, spelling, testthat, formatR, tm (&ge; 0.6),
tokenizers, knitr, lda, lsa, dplyr, purrr, quanteda.textmodels,
quanteda.textstats, quanteda.textplots, RColorBrewer, slam,
spacyr, stm, text2vec, topicmodels, tibble, tidytext, xtable,
ggplot2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://quanteda.io">https://quanteda.io</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/quanteda/quanteda/issues">https://github.com/quanteda/quanteda/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Language:</td>
<td>en-GB</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Collate:</td>
<td>'RcppExports.R' 'tokenizers.R' 'meta.R'
'quanteda-documentation.R' 'aaa.R' 'bootstrap_dfm.R'
'casechange-functions.R' 'char_select.R' 'convert.R'
'corpus-addsummary-metadata.R' 'corpus-methods.R' 'corpus.R'
'corpus_group.R' 'corpus_reshape.R' 'corpus_sample.R'
'corpus_segment.R' 'corpus_subset.R' 'corpus_trim.R'
'data-documentation.R' 'dfm-classes.R' 'dfm-methods.R'
'dfm-print.R' 'dfm-subsetting.R' 'dfm.R' 'dfm_compress.R'
'dfm_group.R' 'dfm_lookup.R' 'dfm_match.R' 'dfm_replace.R'
'dfm_sample.R' 'dfm_select.R' 'dfm_sort.R' 'dfm_subset.R'
'dfm_trim.R' 'dfm_weight.R' 'dictionaries.R' 'dimnames.R'
'fcm-classes.R' 'docnames.R' 'docvars.R' 'fcm-methods.R'
'fcm-print.R' 'fcm-subsetting.R' 'fcm.R' 'fcm_select.R'
'index.R' 'kwic.R' 'message.R' 'nfunctions.R'
'object-builder.R' 'object2fixed.R' 'pattern2fixed.R'
'phrases.R' 'quanteda_options.R' 'readtext-methods.R'
'spacyr-methods.R' 'stopwords.R' 'summary.R' 'textmodel.R'
'textplot.R' 'texts.R' 'textstat.R' 'tokens-methods.R'
'tokens.R' 'tokens_chunk.R' 'tokens_compound.R'
'tokens_group.R' 'tokens_lookup.R' 'tokens_ngrams.R'
'tokens_replace.R' 'tokens_restore.R' 'tokens_sample.R'
'tokens_segment.R' 'tokens_select.R' 'tokens_split.R'
'tokens_subset.R' 'utils.R' 'validator.R' 'wordstem.R' 'zzz.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-17 15:53:38 UTC; kbenoit</td>
</tr>
<tr>
<td>Author:</td>
<td>Kenneth Benoit <a href="https://orcid.org/0000-0002-0797-564X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre, aut, cph],
  Kohei Watanabe <a href="https://orcid.org/0000-0001-6519-5265"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Haiyan Wang <a href="https://orcid.org/0000-0003-4992-4311"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Paul Nulty <a href="https://orcid.org/0000-0002-7214-4666"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Adam Obeng <a href="https://orcid.org/0000-0002-2906-4775"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Stefan Müller <a href="https://orcid.org/0000-0002-6315-4125"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Akitaka Matsuo <a href="https://orcid.org/0000-0002-3323-6330"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  William Lowe <a href="https://orcid.org/0000-0002-1549-6163"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Christian Müller [ctb],
  Olivier Delmarcelle
    <a href="https://orcid.org/0000-0003-4347-070X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  European Research Council [fnd] (ERC-2011-StG 283794-QUANTESS)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kenneth Benoit &lt;kbenoit@lse.ac.uk&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-18 09:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>

<hr>
<h2 id='as.character.corpus'>Coercion and checking methods for corpus objects</h2><span id='topic+as.character.corpus'></span><span id='topic+is.corpus'></span><span id='topic+as.corpus'></span>

<h3>Description</h3>

<p>Coercion functions to and from <a href="#topic+corpus">corpus</a> objects, including conversion to a
plain <a href="base.html#topic+character">character</a> object; and checks for whether an object is a corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'corpus'
as.character(x, use.names = TRUE, ...)

is.corpus(x)

as.corpus(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.character.corpus_+3A_x">x</code></td>
<td>
<p>object to be coerced or checked</p>
</td></tr>
<tr><td><code id="as.character.corpus_+3A_use.names">use.names</code></td>
<td>
<p>logical; preserve (document) names if <code>TRUE</code></p>
</td></tr>
<tr><td><code id="as.character.corpus_+3A_...">...</code></td>
<td>
<p>additional arguments used by specific methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>as.character()</code> returns the corpus as a plain character vector, with
or without named elements.
</p>
<p><code>is.corpus</code> returns <code>TRUE</code> if the object is a corpus.
</p>
<p><code>as.corpus()</code> upgrades a corpus object to the newest format.
object.
</p>


<h3>Note</h3>

<p><code>as.character(x)</code> where <code>x</code> is a corpus is equivalent to
calling the deprecated <code>texts(x)</code>.
</p>

<hr>
<h2 id='as.data.frame.dfm'>Convert a dfm to a data.frame</h2><span id='topic+as.data.frame.dfm'></span>

<h3>Description</h3>

<p>Deprecated function to convert a dfm into a data.frame.
Recommended that you use <code>convert(x, to = "data.frame")</code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dfm'
as.data.frame(
  x,
  row.names = NULL,
  ...,
  document = docnames(x),
  docid_field = "doc_id",
  check.names = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.data.frame.dfm_+3A_x">x</code></td>
<td>
<p>any <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="as.data.frame.dfm_+3A_row.names">row.names</code></td>
<td>
<p><code>NULL</code> or a character vector giving the row
names for the data frame.  Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="as.data.frame.dfm_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
<tr><td><code id="as.data.frame.dfm_+3A_document">document</code></td>
<td>
<p>optional first column of mode <code>character</code> in the
data.frame, defaults <code>docnames(x)</code>.  Set to <code>NULL</code> to exclude.</p>
</td></tr>
<tr><td><code id="as.data.frame.dfm_+3A_docid_field">docid_field</code></td>
<td>
<p>character; the name of the column containing document
names used when <code>to = "data.frame"</code>.  Unused for other conversions.</p>
</td></tr>
<tr><td><code id="as.data.frame.dfm_+3A_check.names">check.names</code></td>
<td>
<p>logical; passed to the <code><a href="base.html#topic+data.frame">data.frame</a>()</code> call.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+convert">convert()</a></code>
</p>

<hr>
<h2 id='as.dfm'>Coercion and checking functions for dfm objects</h2><span id='topic+as.dfm'></span><span id='topic+is.dfm'></span>

<h3>Description</h3>

<p>Convert an eligible input object into a dfm, or check whether an object is a
dfm.  Current eligible inputs for coercion to a dfm are: <a href="base.html#topic+matrix">matrix</a>,
(sparse) <a href="Matrix.html#topic+Matrix">Matrix</a>,
<a href="tm.html#topic+matrix">TermDocumentMatrix</a> and <a href="tm.html#topic+matrix">DocumentTermMatrix</a>
(from the <span class="pkg">tm</span> package), <a href="base.html#topic+data.frame">data.frame</a>, and other <a href="#topic+dfm">dfm</a> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.dfm(x)

is.dfm(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.dfm_+3A_x">x</code></td>
<td>
<p>a candidate object for checking or coercion to <a href="#topic+dfm">dfm</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>as.dfm</code> converts an input object into a <a href="#topic+dfm">dfm</a>.  Row names
are used for docnames, and column names for featnames, of the resulting
dfm.
</p>
<p><code>is.dfm</code> returns <code>TRUE</code> if and only if its argument is a <a href="#topic+dfm">dfm</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.data.frame.dfm">as.data.frame.dfm()</a></code>, <code><a href="#topic+as.matrix.dfm">as.matrix.dfm()</a></code>,
<code><a href="#topic+convert">convert()</a></code>
</p>

<hr>
<h2 id='as.dictionary'>Coercion and checking functions for dictionary objects</h2><span id='topic+as.dictionary'></span><span id='topic+is.dictionary'></span>

<h3>Description</h3>

<p>Convert a dictionary from a different format into a <span class="pkg">quanteda</span>
dictionary, or check to see if an object is a dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.dictionary(x, format = c("tidytext"), separator = " ", tolower = FALSE)

is.dictionary(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.dictionary_+3A_x">x</code></td>
<td>
<p>a dictionary-like object to be coerced or checked</p>
</td></tr>
<tr><td><code id="as.dictionary_+3A_format">format</code></td>
<td>
<p>input format for the object to be coerced to a
<a href="#topic+dictionary">dictionary</a>; current legal values are a data.frame with the fields
<code>word</code> and <code>sentiment</code> (as per the <strong>tidytext</strong> package)</p>
</td></tr>
<tr><td><code id="as.dictionary_+3A_separator">separator</code></td>
<td>
<p>the character in between multi-word dictionary values. This
defaults to <code>" "</code>.</p>
</td></tr>
<tr><td><code id="as.dictionary_+3A_tolower">tolower</code></td>
<td>
<p>if <code>TRUE</code>, convert all dictionary values to lowercase</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>as.dictionary</code> returns a <span class="pkg">quanteda</span> <a href="#topic+dictionary">dictionary</a>
object.  This conversion function differs from the <code><a href="#topic+dictionary">dictionary()</a></code>
constructor function in that it converts an existing object rather than
creates one from components or from a file.
</p>
<p><code>is.dictionary</code> returns <code>TRUE</code> if an object is a
<span class="pkg">quanteda</span> <a href="#topic+dictionary">dictionary</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(sentiments, package = "tidytext")
as.dictionary(subset(sentiments, lexicon == "nrc"))
as.dictionary(subset(sentiments, lexicon == "bing"))
# to convert AFINN into polarities - adjust thresholds if desired
datafinn &lt;- subset(sentiments, lexicon == "AFINN")
datafinn[["sentiment"]] &lt;-
    with(datafinn,
         sentiment &lt;- ifelse(score &lt; 0, "negative",
                             ifelse(score &gt; 0, "positive", "netural"))
    )
with(datafinn, table(score, sentiment))
as.dictionary(datafinn)

dat &lt;- data.frame(
    word = c("Great", "Horrible"),
    sentiment = c("positive", "negative")
    )
as.dictionary(dat)
as.dictionary(dat, tolower = FALSE)

## End(Not run)

is.dictionary(dictionary(list(key1 = c("val1", "val2"), key2 = "val3")))
# [1] TRUE
is.dictionary(list(key1 = c("val1", "val2"), key2 = "val3"))
# [1] FALSE
</code></pre>

<hr>
<h2 id='as.fcm'>Coercion and checking functions for fcm objects</h2><span id='topic+as.fcm'></span>

<h3>Description</h3>

<p>Convert an eligible input object into a fcm, or check whether an object is a
fcm.  Current eligible inputs for coercion to a dfm are: <a href="base.html#topic+matrix">matrix</a>,
(sparse) <a href="Matrix.html#topic+Matrix">Matrix</a> and other <a href="#topic+fcm">fcm</a> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.fcm(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.fcm_+3A_x">x</code></td>
<td>
<p>a candidate object for checking or coercion to <a href="#topic+dfm">dfm</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>as.fcm</code> converts an input object into a <a href="#topic+fcm">fcm</a>.
</p>

<hr>
<h2 id='as.list.tokens'>Coercion, checking, and combining functions for tokens objects</h2><span id='topic+as.list.tokens'></span><span id='topic+as.character.tokens'></span><span id='topic+is.tokens'></span><span id='topic+as.tokens'></span><span id='topic+as.tokens.spacyr_parsed'></span>

<h3>Description</h3>

<p>Coercion functions to and from <a href="#topic+tokens">tokens</a> objects, checks for whether an
object is a <a href="#topic+tokens">tokens</a> object, and functions to combine <a href="#topic+tokens">tokens</a>
objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tokens'
as.list(x, ...)

## S3 method for class 'tokens'
as.character(x, use.names = FALSE, ...)

is.tokens(x)

as.tokens(x, concatenator = "_", ...)

## S3 method for class 'spacyr_parsed'
as.tokens(
  x,
  concatenator = "/",
  include_pos = c("none", "pos", "tag"),
  use_lemma = FALSE,
  ...
)

is.tokens(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.list.tokens_+3A_x">x</code></td>
<td>
<p>object to be coerced or checked</p>
</td></tr>
<tr><td><code id="as.list.tokens_+3A_...">...</code></td>
<td>
<p>additional arguments used by specific methods.  For
<a href="#topic+c.tokens">c.tokens</a>, these are the <a href="#topic+tokens">tokens</a> objects to be concatenated.</p>
</td></tr>
<tr><td><code id="as.list.tokens_+3A_use.names">use.names</code></td>
<td>
<p>logical; preserve names if <code>TRUE</code>.  For
<code>as.character</code> and <code>unlist</code> only.</p>
</td></tr>
<tr><td><code id="as.list.tokens_+3A_concatenator">concatenator</code></td>
<td>
<p>character between multi-word expressions, default is the
underscore character.  See Details.</p>
</td></tr>
<tr><td><code id="as.list.tokens_+3A_include_pos">include_pos</code></td>
<td>
<p>character; whether and which part-of-speech tag to use:
<code>"none"</code> do not use any part of speech indicator, <code>"pos"</code> use the
<code>pos</code> variable, <code>"tag"</code> use the <code>tag</code> variable.  The POS
will be added to the token after <code>"concatenator"</code>.</p>
</td></tr>
<tr><td><code id="as.list.tokens_+3A_use_lemma">use_lemma</code></td>
<td>
<p>logical; if <code>TRUE</code>, use the lemma rather than the raw
token</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>concatenator</code> is used to automatically generate dictionary
values for multi-word expressions in <code><a href="#topic+tokens_lookup">tokens_lookup()</a></code> and
<code><a href="#topic+dfm_lookup">dfm_lookup()</a></code>. The underscore character is commonly used to join
elements of multi-word expressions (e.g. &quot;piece_of_cake&quot;, &quot;New_York&quot;), but
other characters (e.g. whitespace &quot; &quot; or a hyphen &quot;-&quot;) can also be used.
In those cases, users have to tell the system what is the concatenator in
your tokens so that the conversion knows to treat this character as the
inter-word delimiter, when reading in the elements that will become the
tokens.
</p>


<h3>Value</h3>

<p><code>as.list</code> returns a simple list of characters from a
<a href="#topic+tokens">tokens</a> object.
</p>
<p><code>as.character</code> returns a character vector from a
<a href="#topic+tokens">tokens</a> object.
</p>
<p><code>is.tokens</code> returns <code>TRUE</code> if the object is of class
tokens, <code>FALSE</code> otherwise.
</p>
<p><code>as.tokens</code> returns a quanteda <a href="#topic+tokens">tokens</a> object.
</p>
<p><code>is.tokens</code> returns <code>TRUE</code> if the object is of class
tokens, <code>FALSE</code> otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create tokens object from list of characters with custom concatenator
dict &lt;- dictionary(list(country = "United States",
                   sea = c("Atlantic Ocean", "Pacific Ocean")))
lis &lt;- list(c("The", "United-States", "has", "the", "Atlantic-Ocean",
              "and", "the", "Pacific-Ocean", "."))
toks &lt;- as.tokens(lis, concatenator = "-")
tokens_lookup(toks, dict)

</code></pre>

<hr>
<h2 id='as.matrix.dfm'>Coerce a dfm to a matrix or data.frame</h2><span id='topic+as.matrix.dfm'></span>

<h3>Description</h3>

<p>Methods for coercing a <a href="#topic+dfm">dfm</a> object to a matrix or data.frame object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dfm'
as.matrix(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.matrix.dfm_+3A_x">x</code></td>
<td>
<p>dfm to be coerced</p>
</td></tr>
<tr><td><code id="as.matrix.dfm_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># coercion to matrix
as.matrix(data_dfm_lbgexample[, 1:10])

</code></pre>

<hr>
<h2 id='as.yaml'>Convert quanteda dictionary objects to the YAML format</h2><span id='topic+as.yaml'></span>

<h3>Description</h3>

<p>Converts a <span class="pkg">quanteda</span> dictionary object constructed by the
<a href="#topic+dictionary">dictionary</a> function into the YAML format. The YAML
files can be edited in text editors and imported into
<span class="pkg">quanteda</span> again.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.yaml(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.yaml_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+dictionary">dictionary</a> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>as.yaml</code> a dictionary in the YAML format, as a character object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dict &lt;- dictionary(list(one = c("a b", "c*"), two = c("x", "y", "z??")))
cat(yaml &lt;- as.yaml(dict))
cat(yaml, file = (yamlfile &lt;- paste0(tempfile(), ".yml")))
dictionary(file = yamlfile)

## End(Not run)
</code></pre>

<hr>
<h2 id='attributes+26lt+3B-'>Function extending base::attributes()</h2><span id='topic+attributes+3C-'></span>

<h3>Description</h3>

<p>Function extending base::attributes()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>attributes(x, overwrite = TRUE) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="attributes+2B26lt+2B3B-_+3A_x">x</code></td>
<td>
<p>an object</p>
</td></tr>
<tr><td><code id="attributes+2B26lt+2B3B-_+3A_overwrite">overwrite</code></td>
<td>
<p>if <code>TRUE</code>, overwrite old attributes</p>
</td></tr>
<tr><td><code id="attributes+2B26lt+2B3B-_+3A_value">value</code></td>
<td>
<p>new attributes</p>
</td></tr>
</table>

<hr>
<h2 id='bootstrap_dfm'>Bootstrap a dfm</h2><span id='topic+bootstrap_dfm'></span>

<h3>Description</h3>

<p>Create an array of resampled dfms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_dfm(x, n = 10, ..., verbose = quanteda_options("verbose"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrap_dfm_+3A_x">x</code></td>
<td>
<p>a character or <a href="#topic+corpus">corpus</a> object</p>
</td></tr>
<tr><td><code id="bootstrap_dfm_+3A_n">n</code></td>
<td>
<p>number of resamples</p>
</td></tr>
<tr><td><code id="bootstrap_dfm_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="#topic+dfm">dfm()</a></code></p>
</td></tr>
<tr><td><code id="bootstrap_dfm_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code> print status messages</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function produces multiple, resampled <a href="#topic+dfm">dfm</a> objects, based on
resampling sentences (with replacement) from each document, recombining
these into new &quot;documents&quot; and computing a dfm for each. Resampling of
sentences is done strictly within document, so that every resampled
document will contain at least some of its original tokens.
</p>


<h3>Value</h3>

<p>A named list of <a href="#topic+dfm">dfm</a> objects, where the first, <code>dfm_0</code>, is
the dfm from the original texts, and subsequent elements are the
sentence-resampled dfms.
</p>


<h3>Author(s)</h3>

<p>Kenneth Benoit
</p>


<h3>Examples</h3>

<pre><code class='language-R'># bootstrapping from the original text
set.seed(10)
txt &lt;- c(textone = "This is a sentence.  Another sentence.  Yet another.",
         texttwo = "Premiere phrase.  Deuxieme phrase.")
bootstrap_dfm(txt, n = 3, verbose = TRUE)

</code></pre>

<hr>
<h2 id='cbind.dfm'>Combine dfm objects by Rows or Columns</h2><span id='topic+cbind.dfm'></span><span id='topic+rbind.dfm'></span>

<h3>Description</h3>

<p>Combine a <a href="#topic+dfm">dfm</a> with another dfm, or numeric, or matrix object,
returning a dfm with the combined documents or features, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dfm'
cbind(...)

## S3 method for class 'dfm'
rbind(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cbind.dfm_+3A_...">...</code></td>
<td>
<p><a href="#topic+dfm">dfm</a>, numeric, or matrix  objects to be joined column-wise
(<code>cbind</code>) or row-wise (<code>rbind</code>) to the first.  Numeric objects
not confirming to the row or column dimension will be recycled as normal.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cbind(x, y, ...)</code> combines dfm objects by columns, returning a
dfm object with combined features from input dfm objects.  Note that this
should be used with extreme caution, as joining dfms with different
documents will result in a new row with the docname(s) of the first dfm,
merging in those from the second.  Furthermore, if features are shared
between the dfms being cbinded, then duplicate feature labels will result.
In both instances, warning messages will result.
</p>
<p><code>rbind(x, y, ...)</code> combines dfm objects by rows, returning a
dfm object with combined features from input dfm objects.  Features are
matched between the two dfm objects, so that the order and names of the
features do not need to match.  The order of the features in the resulting
dfm is not guaranteed.  The attributes and settings of this new dfm are not
currently preserved.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># cbind() for dfm objects
(dfmat1 &lt;- dfm(tokens(c("a b c d", "c d e f"))))
(dfmat2 &lt;- dfm(tokens(c("a b", "x y z"))))
cbind(dfmat1, dfmat2)
cbind(dfmat1, 100)
cbind(100, dfmat1)
cbind(dfmat1, matrix(c(101, 102), ncol = 1))
cbind(matrix(c(101, 102), ncol = 1), dfmat1)


# rbind() for dfm objects
(dfmat1 &lt;- dfm(tokens(c(doc1 = "This is one sample text sample."))))
(dfmat2 &lt;- dfm(tokens(c(doc2 = "One two three text text."))))
(dfmat3 &lt;- dfm(tokens(c(doc3 = "This is the fourth sample text."))))
rbind(dfmat1, dfmat2)
rbind(dfmat1, dfmat2, dfmat3)
</code></pre>

<hr>
<h2 id='char_select'>Select or remove elements from a character vector</h2><span id='topic+char_select'></span><span id='topic+char_remove'></span><span id='topic+char_keep'></span>

<h3>Description</h3>

<p>These function select or discard elements from a <a href="base.html#topic+character">character</a> object.  For
convenience, the functions <code>char_remove</code> and <code>char_keep</code> are defined as
shortcuts for <code>char_select(x, pattern, selection = "remove")</code> and
<code>char_select(x, pattern, selection = "keep")</code>, respectively.
</p>
<p>These functions make it easy to change, for instance, stopwords
based on pattern matching.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>char_select(
  x,
  pattern,
  selection = c("keep", "remove"),
  valuetype = c("glob", "fixed", "regex"),
  case_insensitive = TRUE
)

char_remove(x, ...)

char_keep(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="char_select_+3A_x">x</code></td>
<td>
<p>an input <a href="base.html#topic+character">character</a> vector</p>
</td></tr>
<tr><td><code id="char_select_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="#topic+pattern">pattern</a> for details.</p>
</td></tr>
<tr><td><code id="char_select_+3A_selection">selection</code></td>
<td>
<p>whether to <code>"keep"</code> or <code>"remove"</code> the tokens matching
<code>pattern</code></p>
</td></tr>
<tr><td><code id="char_select_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="char_select_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="char_select_+3A_...">...</code></td>
<td>
<p>additional arguments passed by <code>char_remove</code> and <code>char_keep</code> to
<code>char_select</code>. Cannot include <code>selection</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a modified <a href="base.html#topic+character">character</a> vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'># character selection
mykeywords &lt;- c("natural", "national", "denatured", "other")
char_select(mykeywords, "nat*", valuetype = "glob")
char_select(mykeywords, "nat", valuetype = "regex")
char_select(mykeywords, c("natur*", "other"))
char_select(mykeywords, c("natur*", "other"), selection = "remove")

# character removal
char_remove(letters[1:5], c("a", "c", "x"))
words &lt;- c("any", "and", "Anna", "as", "announce", "but")
char_remove(words, "an*")
char_remove(words, "an*", case_insensitive = FALSE)
char_remove(words, "^.n.+$", valuetype = "regex")

# remove some of the system stopwords
stopwords("en", source = "snowball")[1:6]
stopwords("en", source = "snowball")[1:6] %&gt;%
  char_remove(c("me", "my*"))
  
# character keep
char_keep(letters[1:5], c("a", "c", "x"))
</code></pre>

<hr>
<h2 id='char_tolower'>Convert the case of character objects</h2><span id='topic+char_tolower'></span><span id='topic+char_toupper'></span>

<h3>Description</h3>

<p><code>char_tolower</code> and <code>char_toupper</code> are replacements for
<a href="base.html#topic+chartr">base::tolower()</a> and <a href="base.html#topic+chartr">base::tolower()</a>
based on the <span class="pkg">stringi</span> package.  The <span class="pkg">stringi</span> functions for case
conversion are superior to the <span class="pkg">base</span> functions because they correctly
handle case conversion for Unicode.  In addition, the <code style="white-space: pre;">&#8288;*_tolower()&#8288;</code> functions
provide an option for preserving acronyms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>char_tolower(x, keep_acronyms = FALSE)

char_toupper(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="char_tolower_+3A_x">x</code></td>
<td>
<p>the input object whose character/tokens/feature elements will be
case-converted</p>
</td></tr>
<tr><td><code id="char_tolower_+3A_keep_acronyms">keep_acronyms</code></td>
<td>
<p>logical; if <code>TRUE</code>, do not lowercase any
all-uppercase words (applies only to <code style="white-space: pre;">&#8288;*_tolower()&#8288;</code> functions)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>txt1 &lt;- c(txt1 = "b A A", txt2 = "C C a b B")
char_tolower(txt1)
char_toupper(txt1)

# with acronym preservation
txt2 &lt;- c(text1 = "England and France are members of NATO and UNESCO",
          text2 = "NASA sent a rocket into space.")
char_tolower(txt2)
char_tolower(txt2, keep_acronyms = TRUE)
char_toupper(txt2)
</code></pre>

<hr>
<h2 id='check_class'>Check object class for functions</h2><span id='topic+check_class'></span>

<h3>Description</h3>

<p>Checks if the method is defined for the class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_class(class, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_class_+3A_class">class</code></td>
<td>
<p>the object class to check</p>
</td></tr>
<tr><td><code id="check_class_+3A_method">method</code></td>
<td>
<p>the name of functions to be called</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
quanteda:::check_class("tokens", "dfm_select")

## End(Not run)
</code></pre>

<hr>
<h2 id='check_dots'>Check arguments passed to other functions via ...</h2><span id='topic+check_dots'></span>

<h3>Description</h3>

<p>Check arguments passed to other functions via ...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_dots(..., method = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_dots_+3A_...">...</code></td>
<td>
<p>dots to check</p>
</td></tr>
<tr><td><code id="check_dots_+3A_method">method</code></td>
<td>
<p>the names of functions <code>...</code> is passed to</p>
</td></tr>
</table>

<hr>
<h2 id='check_integer'>Validate input vectors</h2><span id='topic+check_integer'></span><span id='topic+check_double'></span><span id='topic+check_logical'></span><span id='topic+check_character'></span>

<h3>Description</h3>

<p>Check the range of values and the length of input vectors
before used in control flow or passed to C++ functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_integer(
  x,
  min_len = 1,
  max_len = 1,
  min = -Inf,
  max = Inf,
  strict = FALSE,
  allow_null = FALSE
)

check_double(
  x,
  min_len = 1,
  max_len = 1,
  min = -Inf,
  max = Inf,
  strict = FALSE,
  allow_null = FALSE
)

check_logical(x, min_len = 1, max_len = 1, strict = FALSE, allow_null = FALSE)

check_character(
  x,
  min_len = 1,
  max_len = 1,
  min_nchar = 0,
  max_nchar = Inf,
  strict = FALSE,
  allow_null = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_integer_+3A_min_len">min_len</code></td>
<td>
<p>minimum length of the vector</p>
</td></tr>
<tr><td><code id="check_integer_+3A_max_len">max_len</code></td>
<td>
<p>maximum length of the vector</p>
</td></tr>
<tr><td><code id="check_integer_+3A_min">min</code></td>
<td>
<p>minimum value in the vector</p>
</td></tr>
<tr><td><code id="check_integer_+3A_max">max</code></td>
<td>
<p>maximum value in the vector</p>
</td></tr>
<tr><td><code id="check_integer_+3A_strict">strict</code></td>
<td>
<p>raise error when <code>x</code> is a different type</p>
</td></tr>
<tr><td><code id="check_integer_+3A_allow_null">allow_null</code></td>
<td>
<p>if <code>TRUE</code>, returns <code>NULL</code> when <code>is.null(x)</code></p>
</td></tr>
<tr><td><code id="check_integer_+3A_min_nchar">min_nchar</code></td>
<td>
<p>minimum character length of values in the vector</p>
</td></tr>
<tr><td><code id="check_integer_+3A_max_nchar">max_nchar</code></td>
<td>
<p>maximum character length of values in the vector</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that value checks are performed after coercion to expected input types.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_integer(0, min = 1) # error
check_integer(-0.1, min = 0) # return 0
check_double(-0.1, min = 0) # error
check_double(numeric(), min_len = 0) # return numeric()
check_double("1.1", min = 1) # returns 1.1
check_double("1.1", min = 1, strict = TRUE) # error
check_double("xyz", min = 1) # error
check_logical(c(TRUE, FALSE), min_len = 3) # error
check_character("_", min_nchar = 1) # return "_"
check_character("", min_nchar = 1) # error

## End(Not run)
</code></pre>

<hr>
<h2 id='convert'>Convert quanteda objects to non-quanteda formats</h2><span id='topic+convert'></span><span id='topic+convert.dfm'></span><span id='topic+convert.corpus'></span>

<h3>Description</h3>

<p>Convert a quanteda <a href="#topic+dfm">dfm</a> or <a href="#topic+corpus">corpus</a> object to a format useable by other
packages. The general function <code>convert</code> provides easy conversion from a dfm
to the document-term representations used in all other text analysis packages
for which conversions are defined.  For <a href="#topic+corpus">corpus</a> objects, <code>convert</code> provides
an easy way to make a corpus and its document variables into a data.frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert(x, to, ...)

## S3 method for class 'dfm'
convert(
  x,
  to = c("lda", "tm", "stm", "austin", "topicmodels", "lsa", "matrix", "data.frame",
    "tripletlist"),
  docvars = NULL,
  omit_empty = TRUE,
  docid_field = "doc_id",
  ...
)

## S3 method for class 'corpus'
convert(x, to = c("data.frame", "json"), pretty = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+dfm">dfm</a> or <a href="#topic+corpus">corpus</a> to be converted</p>
</td></tr>
<tr><td><code id="convert_+3A_to">to</code></td>
<td>
<p>target conversion format, one of:
</p>
 <dl>
<dt><code>"lda"</code></dt><dd><p>a list with components &quot;documents&quot; and &quot;vocab&quot; as
needed by the function
<a href="lda.html#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a> from the
<span class="pkg">lda</span> package</p>
</dd>
<dt><code>"tm"</code></dt><dd><p>a <a href="tm.html#topic+matrix">DocumentTermMatrix</a> from the <span class="pkg">tm</span>
package.  Note: The <span class="pkg">tm</span> package version of <code>as.TermDocumentMatrix()</code>
allows a <code>weighting</code> argument, which supplies a weighting function for
<code><a href="tm.html#topic+matrix">TermDocumentMatrix()</a></code>.  Here the default is for
term frequency weighting. If you want a different weighting, apply the
weights after converting using one of the <span class="pkg">tm</span> functions. For other
available weighting functions from the <span class="pkg">tm</span> package, see
<a href="tm.html#topic+matrix">TermDocumentMatrix</a>.</p>
</dd>
<dt><code>"stm"</code></dt><dd><p>the  format for the <span class="pkg">stm</span> package</p>
</dd> <dt><code>"austin"</code></dt><dd><p>the
<code>wfm</code> format from the <strong>austin</strong> package</p>
</dd>
<dt><code>"topicmodels"</code></dt><dd><p>the &quot;dtm&quot; format as used by the <span class="pkg">topicmodels</span>
package</p>
</dd>
<dt><code>"lsa"</code></dt><dd><p>the &quot;textmatrix&quot; format as
used by the <span class="pkg">lsa</span> package</p>
</dd>
<dt><code>"data.frame"</code></dt><dd><p>a data.frame of without row.names, in which documents
are rows, and each feature is a variable (for a dfm),
or each text and its document variables form a row (for a corpus)</p>
</dd>
<dt><code>"json"</code></dt><dd><p>(corpus only) convert a corpus and its document variables
into JSON format, using the format described in
<a href="jsonlite.html#topic+fromJSON">jsonlite::toJSON()</a></p>
</dd>
<dt><code>"tripletlist"</code></dt><dd><p>a named &quot;triplet&quot; format list consisting of
<code>document</code>, <code>feature</code>, and <code>frequency</code></p>
</dd>
</dl>
</td></tr>
<tr><td><code id="convert_+3A_...">...</code></td>
<td>
<p>unused directly</p>
</td></tr>
<tr><td><code id="convert_+3A_docvars">docvars</code></td>
<td>
<p>optional data.frame of document variables used as the
<code>meta</code> information in conversion to the <span class="pkg">stm</span> package format.
This aids in selecting the document variables only corresponding to the
documents with non-zero counts.  Only affects the &quot;stm&quot; format.</p>
</td></tr>
<tr><td><code id="convert_+3A_omit_empty">omit_empty</code></td>
<td>
<p>logical; if <code>TRUE</code>, omit empty documents and features
from the converted dfm. This is required for some formats (such as STM)
that do not accept empty documents.  Only used when <code>to = "lda"</code> or
<code>to = "topicmodels"</code>.  For <code>to = "stm"</code> format, <code>omit_empty</code> is
always <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="convert_+3A_docid_field">docid_field</code></td>
<td>
<p>character; the name of the column containing document
names used when <code>to = "data.frame"</code>.  Unused for other conversions.</p>
</td></tr>
<tr><td><code id="convert_+3A_pretty">pretty</code></td>
<td>
<p>adds indentation whitespace to JSON output. Can be TRUE/FALSE or a number specifying the number of spaces to indent. See <code><a href="jsonlite.html#topic+prettify">prettify()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A converted object determined by the value of <code>to</code> (see above).
See conversion target package documentation for more detailed descriptions
of the return formats.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## convert a dfm

toks &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1970) %&gt;%
    tokens()
dfmat1 &lt;- dfm(toks)

# austin's wfm format
identical(dim(dfmat1), dim(convert(dfmat1, to = "austin")))

# stm package format
stmmat &lt;- convert(dfmat1, to = "stm")
str(stmmat)

# triplet
tripletmat &lt;- convert(dfmat1, to = "tripletlist")
str(tripletmat)

## Not run: 
# tm's DocumentTermMatrix format
tmdfm &lt;- convert(dfmat1, to = "tm")
str(tmdfm)

# topicmodels package format
str(convert(dfmat1, to = "topicmodels"))

# lda package format
str(convert(dfmat1, to = "lda"))

## End(Not run)

## convert a corpus into a data.frame

corp &lt;- corpus(c(d1 = "Text one.", d2 = "Text two."),
               docvars = data.frame(dvar1 = 1:2, dvar2 = c("one", "two"),
                                    stringsAsFactors = FALSE))
convert(corp, to = "data.frame")
convert(corp, to = "json")
</code></pre>

<hr>
<h2 id='convert-wrappers'>Convenience wrappers for dfm convert</h2><span id='topic+convert-wrappers'></span><span id='topic+dfm2austin'></span><span id='topic+dfm2tm'></span><span id='topic+dfm2lda'></span><span id='topic+dtm2lda'></span><span id='topic+dfm2dtm'></span><span id='topic+dfm2stm'></span>

<h3>Description</h3>

<p>To make the usage as consistent as possible with other packages, quanteda
also provides shortcut wrappers to <code><a href="#topic+convert">convert()</a></code>, designed to be
similar in syntax to analogous commands in the packages to whose format they
are converting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm2austin(x)

dfm2tm(x, weighting = tm::weightTf)

dfm2lda(x, omit_empty = TRUE)

dtm2lda(x, omit_empty = TRUE)

dfm2dtm(x, omit_empty = TRUE)

dfm2stm(x, docvars = NULL, omit_empty = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert-wrappers_+3A_x">x</code></td>
<td>
<p>the dfm to be converted</p>
</td></tr>
<tr><td><code id="convert-wrappers_+3A_weighting">weighting</code></td>
<td>
<p>a <span class="pkg">tm</span> weight, see <code><a href="tm.html#topic+weightTf">tm::weightTf()</a></code></p>
</td></tr>
<tr><td><code id="convert-wrappers_+3A_omit_empty">omit_empty</code></td>
<td>
<p>logical; if <code>TRUE</code>, omit empty documents and features
from the converted dfm. This is required for some formats (such as STM)
that do not accept empty documents.  Only used when <code>to = "lda"</code> or
<code>to = "topicmodels"</code>.  For <code>to = "stm"</code> format, <code>omit_empty</code> is
always <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="convert-wrappers_+3A_docvars">docvars</code></td>
<td>
<p>optional data.frame of document variables used as the
<code>meta</code> information in conversion to the <span class="pkg">stm</span> package format.
This aids in selecting the document variables only corresponding to the
documents with non-zero counts.  Only affects the &quot;stm&quot; format.</p>
</td></tr>
<tr><td><code id="convert-wrappers_+3A_...">...</code></td>
<td>
<p>additional arguments used only by <code>as.DocumentTermMatrix</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dfm2lda</code> provides converts a <a href="#topic+dfm">dfm</a> into the list representation
of terms in documents used by the <span class="pkg">lda</span> package (a list with components
&quot;documents&quot; and &quot;vocab&quot; as needed by
<code><a href="lda.html#topic+lda.collapsed.gibbs.sampler">lda::lda.collapsed.gibbs.sampler()</a></code>).
</p>
<p><code>dfm2ldaformat</code> provides converts a <a href="#topic+dfm">dfm</a> into the list
representation of terms in documents used by the <span class="pkg">lda</span> package (a list
with components &quot;documents&quot; and &quot;vocab&quot; as needed by
<code><a href="lda.html#topic+lda.collapsed.gibbs.sampler">lda::lda.collapsed.gibbs.sampler()</a></code>).
</p>


<h3>Value</h3>

<p>A converted object determined by the value of <code>to</code> (see above).
See conversion target package documentation for more detailed descriptions
of the return formats.
</p>


<h3>Note</h3>

<p>Additional coercion methods to base R objects are also available:
</p>
 <dl>
<dt><code style="white-space: pre;">&#8288;[as.data.frame](x)&#8288;</code></dt><dd><p>converts a <a href="#topic+dfm">dfm</a> into
a <a href="base.html#topic+data.frame">data.frame</a></p>
</dd>
<dt><code style="white-space: pre;">&#8288;[as.matrix](x)&#8288;</code></dt><dd><p>converts a <a href="#topic+dfm">dfm</a> into a
<a href="base.html#topic+matrix">matrix</a></p>
</dd> </dl>



<h3>Examples</h3>

<pre><code class='language-R'>dfmat &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1970) %&gt;%
    tokens() %&gt;%
    dfm()

## Not run: 
# shortcut conversion to lda package list format
identical(quanteda:::dfm2lda(dfmat), convert(dfmat, to = "lda"))

## End(Not run)

## Not run: 
# shortcut conversion to lda package list format
identical(dfm2ldaformat(dfmat), convert(dfmat, to = "lda"))

## End(Not run)
</code></pre>

<hr>
<h2 id='corpus'>Construct a corpus object</h2><span id='topic+corpus'></span><span id='topic+corpus.corpus'></span><span id='topic+corpus.character'></span><span id='topic+corpus.data.frame'></span><span id='topic+corpus.kwic'></span><span id='topic+corpus.Corpus'></span>

<h3>Description</h3>

<p>Creates a corpus object from available sources.  The currently available
sources are:
</p>

<ul>
<li><p> a <a href="base.html#topic+character">character</a> vector, consisting of one document per
element; if the elements are named, these names will be used as document
names.
</p>
</li>
<li><p> a <a href="base.html#topic+data.frame">data.frame</a> (or a <span class="pkg">tibble</span> <code>tbl_df</code>), whose default
document id is a variable identified by <code>docid_field</code>; the text of the
document is a variable identified by <code>text_field</code>; and other variables
are imported as document-level meta-data.  This matches the format of
data.frames constructed by the the <span class="pkg">readtext</span> package.
</p>
</li>
<li><p> a <a href="#topic+kwic">kwic</a> object constructed by <code><a href="#topic+kwic">kwic()</a></code>.
</p>
</li>
<li><p> a <span class="pkg">tm</span> <a href="tm.html#topic+VCorpus">VCorpus</a> or <a href="tm.html#topic+SimpleCorpus">SimpleCorpus</a>
class  object, with the fixed metadata fields imported as <a href="#topic+docvars">docvars</a> and
corpus-level metadata imported as <a href="#topic+meta">meta</a> information.
</p>
</li>
<li><p> a <a href="#topic+corpus">corpus</a> object.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>corpus(x, ...)

## S3 method for class 'corpus'
corpus(
  x,
  docnames = quanteda::docnames(x),
  docvars = quanteda::docvars(x),
  meta = quanteda::meta(x),
  ...
)

## S3 method for class 'character'
corpus(
  x,
  docnames = NULL,
  docvars = NULL,
  meta = list(),
  unique_docnames = TRUE,
  ...
)

## S3 method for class 'data.frame'
corpus(
  x,
  docid_field = "doc_id",
  text_field = "text",
  meta = list(),
  unique_docnames = TRUE,
  ...
)

## S3 method for class 'kwic'
corpus(x, split_context = TRUE, extract_keyword = TRUE, meta = list(), ...)

## S3 method for class 'Corpus'
corpus(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpus_+3A_x">x</code></td>
<td>
<p>a valid corpus source object</p>
</td></tr>
<tr><td><code id="corpus_+3A_...">...</code></td>
<td>
<p>not used directly</p>
</td></tr>
<tr><td><code id="corpus_+3A_docnames">docnames</code></td>
<td>
<p>Names to be assigned to the texts.  Defaults to the names of
the character vector (if any); <code>doc_id</code> for a data.frame; the document
names in a <span class="pkg">tm</span> corpus; or a vector of user-supplied labels equal in
length to the number of documents.  If none of these are round, then
&quot;text1&quot;, &quot;text2&quot;, etc. are assigned automatically.</p>
</td></tr>
<tr><td><code id="corpus_+3A_docvars">docvars</code></td>
<td>
<p>a data.frame of document-level variables associated with each
text</p>
</td></tr>
<tr><td><code id="corpus_+3A_meta">meta</code></td>
<td>
<p>a named list that will be added to the corpus as corpus-level,
user meta-data.  This can later be accessed or updated using
<code><a href="#topic+meta">meta()</a></code>.</p>
</td></tr>
<tr><td><code id="corpus_+3A_unique_docnames">unique_docnames</code></td>
<td>
<p>logical; if <code>TRUE</code>, enforce strict uniqueness in
<code>docnames</code>; otherwise, rename duplicated docnames using an added serial
number, and treat them as segments of the same document.</p>
</td></tr>
<tr><td><code id="corpus_+3A_docid_field">docid_field</code></td>
<td>
<p>optional column index of a document identifier; defaults
to &quot;doc_id&quot;, but if this is not found, then will use the rownames of the
data.frame; if the rownames are not set, it will use the default sequence
based on <code style="white-space: pre;">&#8288;([quanteda_options]("base_docname")&#8288;</code>.</p>
</td></tr>
<tr><td><code id="corpus_+3A_text_field">text_field</code></td>
<td>
<p>the character name or numeric index of the source
<code>data.frame</code> indicating the variable to be read in as text, which must
be a character vector. All other variables in the data.frame will be
imported as docvars.  This argument is only used for <code>data.frame</code>
objects (including those created by <span class="pkg">readtext</span>).</p>
</td></tr>
<tr><td><code id="corpus_+3A_split_context">split_context</code></td>
<td>
<p>logical; if <code>TRUE</code>, split each kwic row into two
&quot;documents&quot;, one for &quot;pre&quot; and one for &quot;post&quot;, with this designation saved
in a new docvar <code>context</code> and with the new number of documents
therefore being twice the number of rows in the kwic.</p>
</td></tr>
<tr><td><code id="corpus_+3A_extract_keyword">extract_keyword</code></td>
<td>
<p>logical; if  <code>TRUE</code>, save the keyword matching
<code>pattern</code> as a new docvar <code>keyword</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The texts and document variables of corpus objects can also be
accessed using index notation and the <code>$</code> operator for accessing or assigning
docvars.  For details, see <code><a href="#topic+corpus-class">[.corpus()</a></code>.
</p>


<h3>Value</h3>

<p>A <a href="#topic+corpus-class">corpus</a> class object containing the original texts,
document-level variables, document-level metadata, corpus-level metadata,
and default settings for subsequent processing of the corpus.
</p>
<p>For <span class="pkg">quanteda</span> &gt;= 2.0, this is a specially classed character vector. It
has many additional attributes but <strong>you should not access these
attributes directly</strong>, especially if you are another package author. Use the
extractor and replacement functions instead, or else your code is not only
going to be uglier, but also likely to break should the internal structure
of a corpus object change.  Using the accessor and replacement functions
ensures that future code to manipulate corpus objects will continue to work.
</p>


<h3>See Also</h3>

<p><a href="#topic+corpus-class">corpus</a>, <code><a href="#topic+docvars">docvars()</a></code>,
<code><a href="#topic+meta">meta()</a></code>, <code><a href="#topic+as.character.corpus">as.character.corpus()</a></code>, <code><a href="#topic+ndoc">ndoc()</a></code>,
<code><a href="#topic+docnames">docnames()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a corpus from texts
corpus(data_char_ukimmig2010)

# create a corpus from texts and assign meta-data and document variables
summary(corpus(data_char_ukimmig2010,
               docvars = data.frame(party = names(data_char_ukimmig2010))), 5)

# import a tm VCorpus
if (requireNamespace("tm", quietly = TRUE)) {
    data(crude, package = "tm")    # load in a tm example VCorpus
    vcorp &lt;- corpus(crude)
    summary(vcorp)

    data(acq, package = "tm")
    summary(corpus(acq), 5)

    vcorp2 &lt;- tm::VCorpus(tm::VectorSource(data_char_ukimmig2010))
    corp &lt;- corpus(vcorp2)
    summary(corp)
}

# construct a corpus from a data.frame
dat &lt;- data.frame(letter_factor = factor(rep(letters[1:3], each = 2)),
                  some_ints = 1L:6L,
                  some_text = paste0("This is text number ", 1:6, "."),
                  stringsAsFactors = FALSE,
                  row.names = paste0("fromDf_", 1:6))
dat
summary(corpus(dat, text_field = "some_text",
               meta = list(source = "From a data.frame called mydf.")))

# from a kwic
kw &lt;- kwic(tokens(data_char_sampletext, remove_separators = FALSE),
           pattern = "econom*", separator = "")
summary(corpus(kw))
summary(corpus(kw, split_context = FALSE))
as.character(corpus(kw, split_context = FALSE))

</code></pre>

<hr>
<h2 id='corpus_group'>Combine documents in corpus by a grouping variable</h2><span id='topic+corpus_group'></span>

<h3>Description</h3>

<p>Combine documents in a <a href="#topic+corpus">corpus</a> object by a grouping variable, by
concatenating their texts in the order of the documents within each grouping
variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corpus_group(x, groups = docid(x), fill = FALSE, concatenator = " ")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpus_group_+3A_x">x</code></td>
<td>
<p><a href="#topic+corpus">corpus</a> object</p>
</td></tr>
<tr><td><code id="corpus_group_+3A_groups">groups</code></td>
<td>
<p>grouping variable for sampling, equal in length to the number
of documents. This will be evaluated in the docvars data.frame, so that
docvars may be referred to by name without quoting. This also changes
previous behaviours for <code>groups</code>. See <code>news(Version &gt;= "3.0", package = "quanteda")</code> for details.</p>
</td></tr>
<tr><td><code id="corpus_group_+3A_fill">fill</code></td>
<td>
<p>logical; if <code>TRUE</code> and <code>groups</code> is a factor, then use all levels
of the factor when forming the new documents of the grouped object.  This
will result in a new &quot;document&quot; with empty content for levels not observed,
but for which an empty document may be needed.  If <code>groups</code> is a factor of
dates, for instance, then <code>fill = TRUE</code> ensures that the new object will
consist of one new &quot;document&quot; by date, regardless of whether any documents
previously existed with that date.  Has no effect if the <code>groups</code>
variable(s) are not factors.</p>
</td></tr>
<tr><td><code id="corpus_group_+3A_concatenator">concatenator</code></td>
<td>
<p>the concatenation character that will connect the grouped
documents.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+corpus">corpus</a> object whose documents are equal to the unique group
combinations, and whose texts are the concatenations of the texts by group.
Document-level variables that have no variation within groups are saved in
<a href="#topic+docvars">docvars</a>.  Document-level variables that are lists are dropped from
grouping, even when these exhibit no variation within groups.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corp &lt;- corpus(c("a a b", "a b c c", "a c d d", "a c c d"),
               docvars = data.frame(grp = c("grp1", "grp1", "grp2", "grp2")))
corpus_group(corp, groups = grp)
corpus_group(corp, groups = c(1, 1, 2, 2))
corpus_group(corp, groups = factor(c(1, 1, 2, 2), levels = 1:3))

# with fill
corpus_group(corp, groups = factor(c(1, 1, 2, 2), levels = 1:3), fill = TRUE)
</code></pre>

<hr>
<h2 id='corpus_reshape'>Recast the document units of a corpus</h2><span id='topic+corpus_reshape'></span>

<h3>Description</h3>

<p>For a corpus, reshape (or recast) the documents to a different level of aggregation.
Units of aggregation can be defined as documents, paragraphs, or sentences.
Because the corpus object records its current &quot;units&quot; status, it is possible
to move from recast units back to original units, for example from documents,
to sentences, and then back to documents (possibly after modifying the sentences).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corpus_reshape(
  x,
  to = c("sentences", "paragraphs", "documents"),
  use_docvars = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpus_reshape_+3A_x">x</code></td>
<td>
<p>corpus whose document units will be reshaped</p>
</td></tr>
<tr><td><code id="corpus_reshape_+3A_to">to</code></td>
<td>
<p>new document units in which the corpus will be recast</p>
</td></tr>
<tr><td><code id="corpus_reshape_+3A_use_docvars">use_docvars</code></td>
<td>
<p>if <code>TRUE</code>, repeat the docvar values for each
segmented text; if <code>FALSE</code>, drop the docvars in the segmented corpus.
Dropping the docvars might be useful in order to conserve space or if these
are not desired for the segmented corpus.</p>
</td></tr>
<tr><td><code id="corpus_reshape_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="#topic+tokens">tokens()</a></code>, since the
syntactic segmenter uses this function)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A corpus object with the documents defined as the new units,
including document-level meta-data identifying the original documents.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simple example
corp1 &lt;- corpus(c(textone = "This is a sentence.  Another sentence.  Yet another.",
                 textwo = "Premiere phrase.  Deuxieme phrase."),
                 docvars = data.frame(country=c("UK", "USA"), year=c(1990, 2000)))
summary(corp1)
summary(corpus_reshape(corp1, to = "sentences"))

# example with inaugural corpus speeches
(corp2 &lt;- corpus_subset(data_corpus_inaugural, Year&gt;2004))
corp2para &lt;- corpus_reshape(corp2, to = "paragraphs")
corp2para
summary(corp2para, 50, showmeta = TRUE)
## Note that Bush 2005 is recorded as a single paragraph because that text
## used a single \n to mark the end of a paragraph.
</code></pre>

<hr>
<h2 id='corpus_sample'>Randomly sample documents from a corpus</h2><span id='topic+corpus_sample'></span>

<h3>Description</h3>

<p>Take a random sample of documents of the specified size from a corpus, with
or without replacement, optionally by grouping variables or with probability
weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corpus_sample(x, size = ndoc(x), replace = FALSE, prob = NULL, by = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpus_sample_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+corpus">corpus</a> object whose documents will be sampled</p>
</td></tr>
<tr><td><code id="corpus_sample_+3A_size">size</code></td>
<td>
<p>a positive number, the number of documents to select; when used
with <code>by</code>, the number to select from each group or a vector equal in
length to the number of groups defining the samples to be chosen in each
category of <code>by</code>.  By defining a size larger than the number of documents,
it is possible to oversample when <code>replace = TRUE</code>.</p>
</td></tr>
<tr><td><code id="corpus_sample_+3A_replace">replace</code></td>
<td>
<p>if <code>TRUE</code>, sample  with replacement</p>
</td></tr>
<tr><td><code id="corpus_sample_+3A_prob">prob</code></td>
<td>
<p>a vector of probability weights for obtaining the elements of the
vector being sampled.  May not be applied when <code>by</code> is used.</p>
</td></tr>
<tr><td><code id="corpus_sample_+3A_by">by</code></td>
<td>
<p>optional grouping variable for sampling.  This will be evaluated in
the docvars data.frame, so that docvars may be referred to by name without
quoting.  This also changes previous behaviours for <code>by</code>.
See <code>news(Version &gt;= "2.9", package = "quanteda")</code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+corpus">corpus</a> object (re)sampled on the documents, containing the document
variables for the documents sampled.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
# sampling from a corpus
summary(corpus_sample(data_corpus_inaugural, size = 5))
summary(corpus_sample(data_corpus_inaugural, size = 10, replace = TRUE))

# sampling with by
corp &lt;- data_corpus_inaugural
corp$century &lt;- paste(floor(corp$Year / 100) + 1)
corp$century &lt;- paste0(corp$century, ifelse(corp$century &lt; 21, "th", "st"))
corpus_sample(corp, size = 2, by = century) %&gt;% summary()
# needs drop = TRUE to avoid empty interactions
corpus_sample(corp, size = 1, by = interaction(Party, century, drop = TRUE), replace = TRUE) %&gt;%
    summary()

# sampling sentences by document
corp &lt;- corpus(c(one = "Sentence one.  Sentence two.  Third sentence.",
                 two = "First sentence, doc2.  Second sentence, doc2."),
               docvars = data.frame(var1 = c("a", "a"), var2 = c(1, 2)))
corpus_reshape(corp, to = "sentences") %&gt;%
    corpus_sample(replace = TRUE, by = docid(.))

# oversampling
corpus_sample(corp, size = 5, replace = TRUE)
</code></pre>

<hr>
<h2 id='corpus_segment'>Segment texts on a pattern match</h2><span id='topic+corpus_segment'></span><span id='topic+char_segment'></span>

<h3>Description</h3>

<p>Segment corpus text(s) or a character vector, splitting
on a pattern match.  This is useful for breaking the texts into smaller
documents based on a regular pattern (such as a speaker identifier in a
transcript) or a user-supplied annotation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corpus_segment(
  x,
  pattern = "##*",
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  extract_pattern = TRUE,
  pattern_position = c("before", "after"),
  use_docvars = TRUE
)

char_segment(
  x,
  pattern = "##*",
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  remove_pattern = TRUE,
  pattern_position = c("before", "after")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpus_segment_+3A_x">x</code></td>
<td>
<p>character or <a href="#topic+corpus">corpus</a> object whose texts will be segmented</p>
</td></tr>
<tr><td><code id="corpus_segment_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="#topic+pattern">pattern</a> for details.</p>
</td></tr>
<tr><td><code id="corpus_segment_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="corpus_segment_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="corpus_segment_+3A_extract_pattern">extract_pattern</code></td>
<td>
<p>extracts matched patterns from the texts and save in docvars if
<code>TRUE</code></p>
</td></tr>
<tr><td><code id="corpus_segment_+3A_pattern_position">pattern_position</code></td>
<td>
<p>either <code>"before"</code> or <code>"after"</code>, depending
on whether the pattern precedes the text (as with a user-supplied tag, such
as <code style="white-space: pre;">&#8288;##INTRO&#8288;</code> in the examples below) or follows the text (as with
punctuation delimiters)</p>
</td></tr>
<tr><td><code id="corpus_segment_+3A_use_docvars">use_docvars</code></td>
<td>
<p>if <code>TRUE</code>, repeat the docvar values for each
segmented text; if <code>FALSE</code>, drop the docvars in the segmented corpus.
Dropping the docvars might be useful in order to conserve space or if these
are not desired for the segmented corpus.</p>
</td></tr>
<tr><td><code id="corpus_segment_+3A_remove_pattern">remove_pattern</code></td>
<td>
<p>removes matched patterns from the texts if <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For segmentation into syntactic units defined by the locale (such as
sentences), use <code><a href="#topic+corpus_reshape">corpus_reshape()</a></code> instead.  In cases where more
fine-grained segmentation is needed, such as that based on commas or
semi-colons (phrase delimiters within a sentence),
<code><a href="#topic+corpus_segment">corpus_segment()</a></code> offers greater user control than
<code><a href="#topic+corpus_reshape">corpus_reshape()</a></code>.
</p>


<h3>Value</h3>

<p><code>corpus_segment</code> returns a corpus of segmented texts
</p>
<p><code>char_segment</code> returns a character vector of segmented texts
</p>


<h3>Boundaries and segmentation explained</h3>

<p>The <code>pattern</code> acts as a
boundary delimiter that defines the segmentation points for splitting a
text into new &quot;document&quot; units.  Boundaries are always defined as the
pattern matches, plus the end and beginnings of each document.  The new
&quot;documents&quot; that are created following the segmentation will then be the
texts found between boundaries.
</p>
<p>The pattern itself will be saved as a new document variable named
<code>pattern</code>.  This is most useful when segmenting a text according to
tags such as names in a transcript, section titles, or user-supplied
annotations.  If the beginning of the file precedes a pattern match, then
the extracted text will have a <code>NA</code> for the extracted <code>pattern</code>
document variable (or when <code>pattern_position = "after"</code>, this will be
true for the text split between the last pattern match and the end of the
document).
</p>
<p>To extract syntactically defined sub-document units such as sentences and
paragraphs, use <code><a href="#topic+corpus_reshape">corpus_reshape()</a></code> instead.
</p>


<h3>Using patterns</h3>

<p>One of the most common uses for
<code>corpus_segment</code> is to partition a corpus into sub-documents using
tags.  The default pattern value is designed for a user-annotated tag that
is a term beginning with double &quot;hash&quot; signs, followed by a whitespace, for
instance as <code style="white-space: pre;">&#8288;##INTRODUCTION The text&#8288;</code>.
</p>
<p>Glob and fixed pattern types use a whitespace character to signal the end
of the pattern.
</p>
<p>For more advanced pattern matches that could include whitespace or
newlines, a regex pattern type can be used, for instance a text such as
</p>
<p><code style="white-space: pre;">&#8288;Mr. Smith: Text&#8288;</code> <br /> <code style="white-space: pre;">&#8288;Mrs. Jones: More text&#8288;</code>
</p>
<p>could have as <code>pattern = "\\b[A-Z].+\\.\\s[A-Z][a-z]+:"</code>, which
would catch the title, the name, and the colon.
</p>
<p>For custom boundary delimitation using punctuation characters that come
come at the end of a clause or sentence (such as <code style="white-space: pre;">&#8288;,&#8288;</code> and<code>.</code>,
these can be specified manually and <code>pattern_position</code> set to
<code>"after"</code>. To keep the punctuation characters in the text (as with
sentence segmentation), set <code>extract_pattern = FALSE</code>.  (With most tag
applications, users will want to remove the patterns from the text, as they
are annotations rather than parts of the text itself.)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+corpus_reshape">corpus_reshape()</a></code>, for segmenting texts into pre-defined
syntactic units such as sentences, paragraphs, or fixed-length chunks
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## segmenting a corpus

# segmenting a corpus using tags
corp1 &lt;- corpus(c("##INTRO This is the introduction.
                  ##DOC1 This is the first document.  Second sentence in Doc 1.
                  ##DOC3 Third document starts here.  End of third document.",
                 "##INTRO Document ##NUMBER Two starts before ##NUMBER Three."))
corpseg1 &lt;- corpus_segment(corp1, pattern = "##*")
cbind(corpseg1, docvars(corpseg1))

# segmenting a transcript based on speaker identifiers
corp2 &lt;- corpus("Mr. Smith: Text.\nMrs. Jones: More text.\nMr. Smith: I'm speaking, again.")
corpseg2 &lt;- corpus_segment(corp2, pattern = "\\b[A-Z].+\\s[A-Z][a-z]+:",
                           valuetype = "regex")
cbind(corpseg2, docvars(corpseg2))

# segmenting a corpus using crude end-of-sentence segmentation
corpseg3 &lt;- corpus_segment(corp1, pattern = ".", valuetype = "fixed",
                           pattern_position = "after", extract_pattern = FALSE)
cbind(corpseg3, docvars(corpseg3))

## segmenting a character vector

# segment into paragraphs and removing the "- " bullet points
cat(data_char_ukimmig2010[4])
char_segment(data_char_ukimmig2010[4],
             pattern = "\\n\\n(-\\s){0,1}", valuetype = "regex",
             remove_pattern = TRUE)

# segment a text into clauses
txt &lt;- c(d1 = "This, is a sentence?  You: come here.", d2 = "Yes, yes okay.")
char_segment(txt, pattern = "\\p{P}", valuetype = "regex",
             pattern_position = "after", remove_pattern = FALSE)
</code></pre>

<hr>
<h2 id='corpus_subset'>Extract a subset of a corpus</h2><span id='topic+corpus_subset'></span>

<h3>Description</h3>

<p>Returns subsets of a corpus that meet certain conditions, including direct
logical operations on docvars (document-level variables).  <code>corpus_subset</code>
functions identically to <code><a href="base.html#topic+subset.data.frame">subset.data.frame()</a></code>, using non-standard
evaluation to evaluate conditions based on the <a href="#topic+docvars">docvars</a> in the corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corpus_subset(x, subset, drop_docid = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpus_subset_+3A_x">x</code></td>
<td>
<p><a href="#topic+corpus">corpus</a> object to be subsetted</p>
</td></tr>
<tr><td><code id="corpus_subset_+3A_subset">subset</code></td>
<td>
<p>logical expression indicating the documents to keep: missing
values are taken as false</p>
</td></tr>
<tr><td><code id="corpus_subset_+3A_drop_docid">drop_docid</code></td>
<td>
<p>if <code>TRUE</code>, <code>docid</code> for documents are removed as the result
of subsetting.</p>
</td></tr>
<tr><td><code id="corpus_subset_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>corpus object, with a subset of documents (and docvars) selected according to arguments
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+subset.data.frame">subset.data.frame()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(corpus_subset(data_corpus_inaugural, Year &gt; 1980))
summary(corpus_subset(data_corpus_inaugural, Year &gt; 1930 &amp; President == "Roosevelt"))
</code></pre>

<hr>
<h2 id='corpus_trim'>Remove sentences based on their token lengths or a pattern match</h2><span id='topic+corpus_trim'></span><span id='topic+char_trim'></span>

<h3>Description</h3>

<p>Removes sentences from a corpus or a character vector shorter than a
specified length.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corpus_trim(
  x,
  what = c("sentences", "paragraphs", "documents"),
  min_ntoken = 1,
  max_ntoken = NULL,
  exclude_pattern = NULL
)

char_trim(
  x,
  what = c("sentences", "paragraphs", "documents"),
  min_ntoken = 1,
  max_ntoken = NULL,
  exclude_pattern = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpus_trim_+3A_x">x</code></td>
<td>
<p><a href="#topic+corpus">corpus</a> or character object whose sentences will be selected.</p>
</td></tr>
<tr><td><code id="corpus_trim_+3A_what">what</code></td>
<td>
<p>units of trimming, <code>"sentences"</code> or <code>"paragraphs"</code>, or
<code>"documents"</code></p>
</td></tr>
<tr><td><code id="corpus_trim_+3A_min_ntoken">min_ntoken</code>, <code id="corpus_trim_+3A_max_ntoken">max_ntoken</code></td>
<td>
<p>minimum and maximum lengths in word tokens
(excluding punctuation)</p>
</td></tr>
<tr><td><code id="corpus_trim_+3A_exclude_pattern">exclude_pattern</code></td>
<td>
<p>a <span class="pkg">stringi</span> regular expression whose match (at the
sentence level) will be used to exclude sentences</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+corpus">corpus</a> or character vector equal in length to the input.  If
the input was a corpus, then the all docvars and metadata are preserved.
For documents whose sentences have been removed entirely, a null string
(<code>""</code>) will be returned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt &lt;- c("PAGE 1. This is a single sentence.  Short sentence. Three word sentence.",
         "PAGE 2. Very short! Shorter.",
         "Very long sentence, with multiple parts, separated by commas.  PAGE 3.")
corp &lt;- corpus(txt, docvars = data.frame(serial = 1:3))
corp

# exclude sentences shorter than 3 tokens
corpus_trim(corp, min_ntoken = 3)
# exclude sentences that start with "PAGE &lt;digit(s)&gt;"
corpus_trim(corp, exclude_pattern = "^PAGE \\d+")

# trimming character objects
char_trim(txt, "sentences", min_ntoken = 3)
char_trim(txt, "sentences", exclude_pattern = "sentence\\.")
</code></pre>

<hr>
<h2 id='corpus-class'>Base method extensions for corpus objects</h2><span id='topic+corpus-class'></span><span id='topic++2B.corpus'></span><span id='topic+c.corpus'></span><span id='topic++5B.corpus'></span><span id='topic+print.summary.corpus'></span>

<h3>Description</h3>

<p>Extensions of base R functions for corpus objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'corpus'
c1 + c2

## S3 method for class 'corpus'
c(..., recursive = FALSE)

## S3 method for class 'corpus'
x[i, drop_docid = TRUE]

## S3 method for class 'summary.corpus'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpus-class_+3A_c1">c1</code></td>
<td>
<p>corpus one to be added</p>
</td></tr>
<tr><td><code id="corpus-class_+3A_c2">c2</code></td>
<td>
<p>corpus two to be added</p>
</td></tr>
<tr><td><code id="corpus-class_+3A_recursive">recursive</code></td>
<td>
<p>logical used by <code>c()</code> method, always set to <code>FALSE</code></p>
</td></tr>
<tr><td><code id="corpus-class_+3A_x">x</code></td>
<td>
<p>a corpus object</p>
</td></tr>
<tr><td><code id="corpus-class_+3A_i">i</code></td>
<td>
<p>document names or indices for documents to extract.</p>
</td></tr>
<tr><td><code id="corpus-class_+3A_if">if</code></td>
<td>
<p><code>TRUE</code>, drop_docid drop <code>docid</code> for documents removed as the result of extraction.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>+</code> operator for a corpus object will combine two corpus
objects, resolving any non-matching <code><a href="#topic+docvars">docvars()</a></code> by making them
into <code>NA</code> values for the corpus lacking that field. Corpus-level meta
data is concatenated, except for <code>source</code> and <code>notes</code>, which are
stamped with information pertaining to the creation of the new joined
corpus.
</p>
<p>The <code>c()</code> operator is also defined for corpus class objects, and provides
an easy way to combine multiple corpus objects.
</p>
<p>There are some issues that need to be addressed in future revisions of
quanteda concerning the use of factors to store document variables and
meta-data.  Currently most or all of these are not recorded as factors,
because we use <code>stringsAsFactors=FALSE</code> in the
<code><a href="base.html#topic+data.frame">data.frame()</a></code> calls that are used to create and store the
document-level information, because the texts should always be stored as
character vectors and never as factors.
</p>


<h3>Value</h3>

<p>The <code>+</code> and <code>c()</code> operators return a <code><a href="#topic+corpus">corpus()</a></code> object.
</p>
<p>Indexing a corpus works in three ways, as of v2.x.x:
</p>

<ul>
<li> <p><code>[</code> returns a subsetted corpus
</p>
</li>
<li> <p><code>[[</code> returns the textual contents of a subsetted corpus (similar to <code><a href="base.html#topic+as.character">as.character()</a></code>)
</p>
</li>
<li> <p><code>$</code> returns a vector containing the single named <a href="#topic+docvars">docvars</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+summary.corpus">summary.corpus()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># concatenate corpus objects
corp1 &lt;- corpus(data_char_ukimmig2010[1:2])
corp2 &lt;- corpus(data_char_ukimmig2010[3:4])
corp3 &lt;- corpus(data_char_ukimmig2010[5:6])
summary(c(corp1, corp2, corp3))

# two ways to index corpus elements
data_corpus_inaugural["1793-Washington"]
data_corpus_inaugural[2]

# return the text itself
data_corpus_inaugural[["1793-Washington"]]
</code></pre>

<hr>
<h2 id='data_char_sampletext'>A paragraph of text for testing various text-based functions</h2><span id='topic+data_char_sampletext'></span>

<h3>Description</h3>

<p>This is a long paragraph (2,914 characters) of text taken from a debate on
Joe Higgins, delivered December 8, 2011.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_char_sampletext
</code></pre>


<h3>Format</h3>

<p>character vector with one element
</p>


<h3>Source</h3>

<p>Dáil Éireann Debate,
<a href="https://www.oireachtas.ie/en/debates/find/">Financial Resolution No. 13: General (Resumed).</a>
7 December 2011.  vol. 749, no. 1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tokens(data_char_sampletext, remove_punct = TRUE)
</code></pre>

<hr>
<h2 id='data_char_ukimmig2010'>Immigration-related sections of 2010 UK party manifestos</h2><span id='topic+data_char_ukimmig2010'></span>

<h3>Description</h3>

<p>Extracts from the election manifestos of 9 UK political parties from 2010, related
to immigration or asylum-seekers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_char_ukimmig2010
</code></pre>


<h3>Format</h3>

<p>A named character vector of plain ASCII texts
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data_corpus_ukimmig2010 &lt;- 
    corpus(data_char_ukimmig2010, 
           docvars = data.frame(party = names(data_char_ukimmig2010)))
summary(data_corpus_ukimmig2010, showmeta = TRUE)
</code></pre>

<hr>
<h2 id='data_corpus_inaugural'>US presidential inaugural address texts</h2><span id='topic+data_corpus_inaugural'></span>

<h3>Description</h3>

<p>US presidential inaugural address texts, and metadata (for the corpus), from
1789 to present.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_corpus_inaugural
</code></pre>


<h3>Format</h3>

<p>a <a href="#topic+corpus">corpus</a> object with the following docvars:
</p>

<ul>
<li> <p><code>Year</code> a four-digit integer year
</p>
</li>
<li> <p><code>President</code> character; President's last name
</p>
</li>
<li> <p><code>FirstName</code> character; President's first name (and possibly middle initial)
</p>
</li>
<li> <p><code>Party</code> factor; name of the President's political party
</p>
</li></ul>



<h3>Details</h3>

<p><code>data_corpus_inaugural</code> is the <a href="#topic+quanteda-package">quanteda-package</a> corpus
object of US presidents' inaugural addresses since 1789. Document variables
contain the year of the address and the last name of the president.
</p>


<h3>Source</h3>

<p><a href="https://archive.org/details/Inaugural-Address-Corpus-1789-2009">https://archive.org/details/Inaugural-Address-Corpus-1789-2009</a> and
<a href="https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses">https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># some operations on the inaugural corpus
summary(data_corpus_inaugural)
head(docvars(data_corpus_inaugural), 10)
</code></pre>

<hr>
<h2 id='data_dfm_lbgexample'>dfm from data in Table 1 of Laver, Benoit, and Garry (2003)</h2><span id='topic+data_dfm_lbgexample'></span>

<h3>Description</h3>

<p>Constructed example data to demonstrate the Wordscores algorithm, from Laver
Benoit and Garry (2003), Table 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_dfm_lbgexample
</code></pre>


<h3>Format</h3>

<p>A <a href="#topic+dfm">dfm</a> object with 6 documents and 37 features.
</p>


<h3>Details</h3>

<p>This is the example word count data from Laver, Benoit and Garry's
(2003) Table 1. Documents R1 to R5 are assumed to have known positions:
-1.5, -0.75, 0, 0.75, 1.5.  Document V1 is assumed unknown, and will have a
raw text score of approximately -0.45 when computed as per LBG (2003).
</p>


<h3>References</h3>

<p>Laver, M., Benoit, K.R., &amp; Garry, J. (2003).
<a href="https://kenbenoit.net/pdfs/WORDSCORESAPSR.pdf">Estimating Policy Positions from Political Text using Words as Data</a>. <em>American
Political Science Review</em>, 97(2), 311&ndash;331.
</p>

<hr>
<h2 id='data_dictionary_LSD2015'>Lexicoder Sentiment Dictionary (2015)</h2><span id='topic+data_dictionary_LSD2015'></span>

<h3>Description</h3>

<p>The 2015 Lexicoder Sentiment Dictionary in <span class="pkg">quanteda</span> <a href="#topic+dictionary">dictionary</a>
format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_dictionary_LSD2015
</code></pre>


<h3>Format</h3>

<p>A <a href="#topic+dictionary">dictionary</a> of four keys containing glob-style <a href="#topic+valuetype">pattern matches</a>.
</p>

<dl>
<dt><code>negative</code></dt><dd><p>2,858 word patterns indicating negative sentiment</p>
</dd>
<dt><code>positive</code></dt><dd><p>1,709 word patterns indicating positive sentiment</p>
</dd>
<dt><code>neg_positive</code></dt><dd><p>1,721 word patterns indicating a positive word preceded by a negation (used to convey negative sentiment)</p>
</dd>
<dt><code>neg_negative</code></dt><dd><p>2,860 word patterns indicating a negative word preceded by a negation (used to convey positive sentiment)</p>
</dd>
</dl>



<h3>Details</h3>

<p>The dictionary consists of 2,858 &quot;negative&quot; sentiment words and 1,709
&quot;positive&quot; sentiment words. A further set of 2,860 and 1,721 negations of
negative and positive words, respectively, is also included. While many users
will find the non-negation sentiment forms of the LSD adequate for sentiment
analysis, Young and Soroka (2012) did find a small, but non-negligible
increase in performance when accounting for negations. Users wishing to test
this or include the negations are encouraged to subtract negated positive
words from the count of positive words, and subtract the negated negative
words from the negative count.
</p>
<p>Young and Soroka (2012) also suggest the use of a pre-processing script to
remove specific cases of some words (i.e., &quot;good bye&quot;, or &quot;nobody better&quot;,
which should not be counted as positive). Pre-processing scripts are
available at <a href="https://www.snsoroka.com/data-lexicoder/">https://www.snsoroka.com/data-lexicoder/</a>.
</p>


<h3>License and Conditions</h3>

<p>The LSD is available for non-commercial academic purposes only. By using
<code>data_dictionary_LSD2015</code>, you accept these terms.
</p>
<p>Please cite the references below when using the dictionary.
</p>


<h3>References</h3>

<p>The objectives, development and reliability of the dictionary are discussed
in detail in Young and Soroka (2012). Please cite this article when using the
Lexicoder Sentiment Dictionary and related resources.
Young, L. &amp; Soroka, S. (2012). <em>Lexicoder Sentiment
Dictionary</em>. Available at <a href="https://www.snsoroka.com/data-lexicoder/">https://www.snsoroka.com/data-lexicoder/</a>.
</p>
<p>Young, L. &amp; Soroka, S. (2012). Affective News: The Automated Coding of
Sentiment in Political Texts]. <a href="https://doi.org/10.1080/10584609.2012.671234">doi:10.1080/10584609.2012.671234</a>.
<em>Political Communication</em>, 29(2), 205&ndash;231.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simple example
txt &lt;- "This aggressive policy will not win friends."

tokens_lookup(tokens(txt), dictionary = data_dictionary_LSD2015, exclusive = FALSE)
## tokens from 1 document.
## text1 :
## [1] "This"   "NEGATIVE"   "policy"   "will"   "NEG_POSITIVE"   "POSITIVE"   "POSITIVE" "."

# notice that double-counting of negated and non-negated terms is avoided 
# when using nested_scope = "dictionary"
tokens_lookup(tokens(txt), dictionary = data_dictionary_LSD2015, 
              exclusive = FALSE, nested_scope = "dictionary")
## tokens from 1 document.
## text1 :
## [1] "This"   "NEGATIVE"   "policy"   "will"   "NEG_POSITIVE" "POSITIVE."   

# on larger examples - notice that few negations are used
dfm(data_char_ukimmig2010[1:5], dictionary = data_dictionary_LSD2015)

# compound neg_negative and neg_positive tokens before creating a dfm object
toks &lt;- tokens_compound(tokens(txt), data_dictionary_LSD2015)

dfm_lookup(dfm(toks), data_dictionary_LSD2015)
</code></pre>

<hr>
<h2 id='data-internal'>Internal data sets</h2><span id='topic+data-internal'></span>

<h3>Description</h3>

<p>Data sets used for mainly internal purposes by the <span class="pkg">quanteda</span> package.
</p>

<hr>
<h2 id='data-relocated'>Formerly included data objects</h2><span id='topic+data-relocated'></span><span id='topic+data_corpus_dailnoconf1991'></span><span id='topic+data_corpus_irishbudget2010'></span>

<h3>Description</h3>

<p>The following corpus objects have been relocated to the <span class="pkg">quanteda.textmodels</span>
package:
</p>

<ul>
<li> <p><code>data_corpus_dailnoconf1991</code>
</p>
</li>
<li> <p><code>data_corpus_irishbudget2010</code>
</p>
</li></ul>



<h3>See Also</h3>

<p><code>quanteda.textmodels::quanteda.textmodels-package</code>
</p>

<hr>
<h2 id='dfm'>Create a document-feature matrix</h2><span id='topic+dfm'></span>

<h3>Description</h3>

<p>Construct a sparse document-feature matrix, from a character, <a href="#topic+corpus">corpus</a>,
<a href="#topic+tokens">tokens</a>, or even other <a href="#topic+dfm">dfm</a> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm(
  x,
  tolower = TRUE,
  remove_padding = FALSE,
  verbose = quanteda_options("verbose"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+tokens">tokens</a> or <a href="#topic+dfm">dfm</a> object</p>
</td></tr>
<tr><td><code id="dfm_+3A_tolower">tolower</code></td>
<td>
<p>convert all features to lowercase</p>
</td></tr>
<tr><td><code id="dfm_+3A_remove_padding">remove_padding</code></td>
<td>
<p>logical; if <code>TRUE</code>, remove the &quot;pads&quot; left as empty tokens after
calling <code><a href="#topic+tokens">tokens()</a></code> or <code><a href="#topic+tokens_remove">tokens_remove()</a></code> with <code>padding = TRUE</code></p>
</td></tr>
<tr><td><code id="dfm_+3A_verbose">verbose</code></td>
<td>
<p>display messages if <code>TRUE</code></p>
</td></tr>
<tr><td><code id="dfm_+3A_...">...</code></td>
<td>
<p>not used directly</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+dfm-class">dfm</a> object
</p>


<h3>Changes in version 3</h3>

<p>In <span class="pkg">quanteda</span> v3, many convenience functions formerly available in
<code>dfm()</code> were deprecated. Formerly, <code>dfm()</code> could be called directly on a
<code>character</code> or <code>corpus</code> object, but we now steer users to tokenise their
inputs first using <code><a href="#topic+tokens">tokens()</a></code>.  Other convenience arguments to <code>dfm()</code> were
also removed, such as <code>select</code>, <code>dictionary</code>, <code>thesaurus</code>, and <code>groups</code>.  All
of these functions are available elsewhere, e.g. through <code><a href="#topic+dfm_group">dfm_group()</a></code>.
See <code>news(Version &gt;= "2.9", package = "quanteda")</code> for details.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfm_select">dfm_select()</a></code>, <a href="#topic+dfm-class">dfm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## for a corpus
toks &lt;- data_corpus_inaugural %&gt;%
  corpus_subset(Year &gt; 1980) %&gt;%
  tokens()
dfm(toks)

# removal options
toks &lt;- tokens(c("a b c", "A B C D")) %&gt;%
    tokens_remove("b", padding = TRUE)
toks
dfm(toks)
dfm(toks) %&gt;%
 dfm_remove(pattern = "") # remove "pads"

# preserving case
dfm(toks, tolower = FALSE)
</code></pre>

<hr>
<h2 id='dfm_compress'>Recombine a dfm or fcm by combining identical dimension elements</h2><span id='topic+dfm_compress'></span><span id='topic+fcm_compress'></span>

<h3>Description</h3>

<p>&quot;Compresses&quot; or groups a <a href="#topic+dfm">dfm</a> or <a href="#topic+fcm">fcm</a> whose dimension names are
the same, for either documents or features.  This may happen, for instance,
if features are made equivalent through application of a thesaurus.  It could also be needed after a
<code><a href="#topic+cbind.dfm">cbind.dfm()</a></code> or <code><a href="#topic+rbind.dfm">rbind.dfm()</a></code> operation.  In most cases, you will not
need to call <code>dfm_compress</code>, since it is called automatically by functions that change the
dimensions of the dfm, e.g. <code><a href="#topic+dfm_tolower">dfm_tolower()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_compress(x, margin = c("both", "documents", "features"))

fcm_compress(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_compress_+3A_x">x</code></td>
<td>
<p>input object, a <a href="#topic+dfm">dfm</a> or <a href="#topic+fcm">fcm</a></p>
</td></tr>
<tr><td><code id="dfm_compress_+3A_margin">margin</code></td>
<td>
<p>character indicating on which margin to compress a dfm, either
<code>"documents"</code>, <code>"features"</code>, or <code>"both"</code> (default).  For fcm
objects, <code>"documents"</code> has no effect.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>dfm_compress</code> returns a <a href="#topic+dfm">dfm</a> whose dimensions have been
recombined by summing the cells across identical dimension names
(<a href="#topic+docnames">docnames</a> or <a href="#topic+featnames">featnames</a>).  The <a href="#topic+docvars">docvars</a> will be
preserved for combining by features but not when documents are combined.
</p>
<p><code>fcm_compress</code> returns an <a href="#topic+fcm">fcm</a> whose features have been
recombined by combining counts of identical features, summing their counts.
</p>


<h3>Note</h3>

<p><code>fcm_compress</code> works only when the <a href="#topic+fcm">fcm</a> was created with a
document context.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># dfm_compress examples
dfmat &lt;- rbind(dfm(tokens(c("b A A", "C C a b B")), tolower = FALSE),
               dfm(tokens("A C C C C C"), tolower = FALSE))
colnames(dfmat) &lt;- char_tolower(featnames(dfmat))
dfmat
dfm_compress(dfmat, margin = "documents")
dfm_compress(dfmat, margin = "features")
dfm_compress(dfmat)

# no effect if no compression needed
dfmatsubset &lt;- dfm(tokens(data_corpus_inaugural[1:5]))
dim(dfmatsubset)
dim(dfm_compress(dfmatsubset))

# compress an fcm
fcmat1 &lt;- fcm(tokens("A D a C E a d F e B A C E D"),
             context = "window", window = 3)
## this will produce an error:
# fcm_compress(fcmat1)

txt &lt;- c("The fox JUMPED over the dog.",
         "The dog jumped over the fox.")
toks &lt;- tokens(txt, remove_punct = TRUE)
fcmat2 &lt;- fcm(toks, context = "document")
colnames(fcmat2) &lt;- rownames(fcmat2) &lt;- tolower(colnames(fcmat2))
colnames(fcmat2)[5] &lt;- rownames(fcmat2)[5] &lt;- "fox"
fcmat2
fcm_compress(fcmat2)
</code></pre>

<hr>
<h2 id='dfm_group'>Combine documents in a dfm by a grouping variable</h2><span id='topic+dfm_group'></span>

<h3>Description</h3>

<p>Combine documents in a <a href="#topic+dfm">dfm</a> by a grouping variable, by summing the cell
frequencies within group and creating new &quot;documents&quot; with the group labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_group(x, groups = docid(x), fill = FALSE, force = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_group_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+dfm">dfm</a></p>
</td></tr>
<tr><td><code id="dfm_group_+3A_groups">groups</code></td>
<td>
<p>grouping variable for sampling, equal in length to the number
of documents. This will be evaluated in the docvars data.frame, so that
docvars may be referred to by name without quoting. This also changes
previous behaviours for <code>groups</code>. See <code>news(Version &gt;= "3.0", package = "quanteda")</code> for details.</p>
</td></tr>
<tr><td><code id="dfm_group_+3A_fill">fill</code></td>
<td>
<p>logical; if <code>TRUE</code> and <code>groups</code> is a factor, then use all levels
of the factor when forming the new documents of the grouped object.  This
will result in a new &quot;document&quot; with empty content for levels not observed,
but for which an empty document may be needed.  If <code>groups</code> is a factor of
dates, for instance, then <code>fill = TRUE</code> ensures that the new object will
consist of one new &quot;document&quot; by date, regardless of whether any documents
previously existed with that date.  Has no effect if the <code>groups</code>
variable(s) are not factors.</p>
</td></tr>
<tr><td><code id="dfm_group_+3A_force">force</code></td>
<td>
<p>logical; if <code>TRUE</code>, group by summing existing counts, even if
the dfm has been weighted.  This can result in invalid sums, such as adding
log counts (when a dfm has been weighted by <code>"logcount"</code> for instance using
<code><a href="#topic+dfm_weight">dfm_weight()</a></code>).  Not needed when the term weight schemes &quot;count&quot; and
&quot;prop&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>dfm_group</code> returns a <a href="#topic+dfm">dfm</a> whose documents are equal to
the unique group combinations, and whose cell values are the sums of the
previous values summed by group. Document-level variables that have no
variation within groups are saved in <a href="#topic+docvars">docvars</a>.  Document-level
variables that are lists are dropped from grouping, even when these exhibit
no variation within groups.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corp &lt;- corpus(c("a a b", "a b c c", "a c d d", "a c c d"),
               docvars = data.frame(grp = c("grp1", "grp1", "grp2", "grp2")))
dfmat &lt;- dfm(tokens(corp))
dfm_group(dfmat, groups = grp)
dfm_group(dfmat, groups = c(1, 1, 2, 2))

# with fill = TRUE
dfm_group(dfmat, fill = TRUE,
          groups = factor(c("A", "A", "B", "C"), levels = LETTERS[1:4]))
</code></pre>

<hr>
<h2 id='dfm_lookup'>Apply a dictionary to a dfm</h2><span id='topic+dfm_lookup'></span>

<h3>Description</h3>

<p>Apply a dictionary to a dfm by looking up all dfm features for matches in a a
set of <a href="#topic+dictionary">dictionary</a> values, and replace those features with a count of
the dictionary's keys.  If <code>exclusive = FALSE</code> then the behaviour is to
apply a &quot;thesaurus&quot;, where each value match is replaced by the dictionary
key, converted to capitals if <code>capkeys = TRUE</code> (so that the replacements
are easily distinguished from features that were terms found originally in
the document).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_lookup(
  x,
  dictionary,
  levels = 1:5,
  exclusive = TRUE,
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  capkeys = !exclusive,
  nomatch = NULL,
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_lookup_+3A_x">x</code></td>
<td>
<p>the dfm to which the dictionary will be applied</p>
</td></tr>
<tr><td><code id="dfm_lookup_+3A_dictionary">dictionary</code></td>
<td>
<p>a <a href="#topic+dictionary">dictionary</a>-class object</p>
</td></tr>
<tr><td><code id="dfm_lookup_+3A_levels">levels</code></td>
<td>
<p>levels of entries in a hierarchical dictionary that will be
applied</p>
</td></tr>
<tr><td><code id="dfm_lookup_+3A_exclusive">exclusive</code></td>
<td>
<p>if <code>TRUE</code>, remove all features not in dictionary,
otherwise, replace values in dictionary with keys while leaving other
features unaffected</p>
</td></tr>
<tr><td><code id="dfm_lookup_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="dfm_lookup_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="dfm_lookup_+3A_capkeys">capkeys</code></td>
<td>
<p>if <code>TRUE</code>, convert dictionary keys to uppercase to
distinguish them from other features</p>
</td></tr>
<tr><td><code id="dfm_lookup_+3A_nomatch">nomatch</code></td>
<td>
<p>an optional character naming a new feature that will contain
the counts of features of <code>x</code> not matched to a dictionary key.  If
<code>NULL</code> (default), do not tabulate unmatched features.</p>
</td></tr>
<tr><td><code id="dfm_lookup_+3A_verbose">verbose</code></td>
<td>
<p>print status messages if <code>TRUE</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p>If using <code>dfm_lookup</code> with dictionaries containing multi-word
values, matches will only occur if the features themselves are multi-word
or formed from n-grams. A better way to match dictionary values that include
multi-word patterns is to apply <code><a href="#topic+tokens_lookup">tokens_lookup()</a></code> to the tokens,
and then construct the dfm.
</p>


<h3>See Also</h3>

<p>dfm_replace
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dict &lt;- dictionary(list(christmas = c("Christmas", "Santa", "holiday"),
                          opposition = c("Opposition", "reject", "notincorpus"),
                          taxglob = "tax*",
                          taxregex = "tax.+$",
                          country = c("United_States", "Sweden")))
dfmat &lt;- dfm(tokens(c("My Christmas was ruined by your opposition tax plan.",
                      "Does the United_States or Sweden have more progressive taxation?")),
             remove = stopwords("english"))
dfmat

# glob format
dfm_lookup(dfmat, dict, valuetype = "glob")
dfm_lookup(dfmat, dict, valuetype = "glob", case_insensitive = FALSE)

# regex v. glob format: note that "united_states" is a regex match for "tax*"
dfm_lookup(dfmat, dict, valuetype = "glob")
dfm_lookup(dfmat, dict, valuetype = "regex", case_insensitive = TRUE)

# fixed format: no pattern matching
dfm_lookup(dfmat, dict, valuetype = "fixed")
dfm_lookup(dfmat, dict, valuetype = "fixed", case_insensitive = FALSE)

# show unmatched tokens
dfm_lookup(dfmat, dict, nomatch = "_UNMATCHED")

</code></pre>

<hr>
<h2 id='dfm_match'>Match the feature set of a dfm to given feature names</h2><span id='topic+dfm_match'></span>

<h3>Description</h3>

<p>Match the feature set of a <a href="#topic+dfm">dfm</a> to a specified vector of feature names.
For existing features in <code>x</code> for which there is an exact match for an
element of <code>features</code>, these will be included.  Any features in <code>x</code>
not <code>features</code> will be discarded, and any feature names specified in
<code>features</code> but not found in <code>x</code> will be added with all zero counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_match(x, features)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_match_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+dfm">dfm</a></p>
</td></tr>
<tr><td><code id="dfm_match_+3A_features">features</code></td>
<td>
<p>character; the feature names to be matched in the output dfm</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Selecting on another <a href="#topic+dfm">dfm</a>'s <code><a href="#topic+featnames">featnames()</a></code> is useful when you
have trained a model on one dfm, and need to project this onto a test set
whose features must be identical. It is also used in
<code><a href="#topic+bootstrap_dfm">bootstrap_dfm()</a></code>.
</p>


<h3>Value</h3>

<p>A <a href="#topic+dfm">dfm</a> whose features are identical to those specified in
<code>features</code>.
</p>


<h3>Note</h3>

<p>Unlike <code><a href="#topic+dfm_select">dfm_select()</a></code>, this function will add feature names
not already present in <code>x</code>. It also provides only fixed,
case-sensitive matches.  For more flexible feature selection, see
<code><a href="#topic+dfm_select">dfm_select()</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfm_select">dfm_select()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># matching a dfm to a feature vector
dfm_match(dfm(""), letters[1:5])
dfm_match(data_dfm_lbgexample, c("A", "B", "Z"))
dfm_match(data_dfm_lbgexample, c("B", "newfeat1", "A", "newfeat2"))

# matching one dfm to another
txt &lt;- c("This is text one", "The second text", "This is text three")
(dfmat1 &lt;- dfm(tokens(txt[1:2])))
(dfmat2 &lt;- dfm(tokens(txt[2:3])))
(dfmat3 &lt;- dfm_match(dfmat1, featnames(dfmat2)))
setequal(featnames(dfmat2), featnames(dfmat3))
</code></pre>

<hr>
<h2 id='dfm_replace'>Replace features in dfm</h2><span id='topic+dfm_replace'></span>

<h3>Description</h3>

<p>Substitute features based on vectorized one-to-one matching for lemmatization
or user-defined stemming.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_replace(
  x,
  pattern,
  replacement,
  case_insensitive = TRUE,
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_replace_+3A_x">x</code></td>
<td>
<p><a href="#topic+dfm">dfm</a> whose features will be replaced</p>
</td></tr>
<tr><td><code id="dfm_replace_+3A_pattern">pattern</code></td>
<td>
<p>a character vector.  See <a href="#topic+pattern">pattern</a>
for more details.</p>
</td></tr>
<tr><td><code id="dfm_replace_+3A_replacement">replacement</code></td>
<td>
<p>if <code>pattern</code> is a character vector, then
<code>replacement</code> must be character vector of equal length, for a 1:1
match.</p>
</td></tr>
<tr><td><code id="dfm_replace_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="dfm_replace_+3A_verbose">verbose</code></td>
<td>
<p>print status messages if <code>TRUE</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat1 &lt;- dfm(data_corpus_inaugural)

# lemmatization
taxwords &lt;- c("tax", "taxing", "taxed", "taxed", "taxation")
lemma &lt;- rep("TAX", length(taxwords))
featnames(dfm_select(dfmat1, pattern = taxwords))
dfmat2 &lt;- dfm_replace(dfmat1, pattern = taxwords, replacement = lemma)
featnames(dfm_select(dfmat2, pattern = taxwords))

# stemming
feat &lt;- featnames(dfmat1)
featstem &lt;- char_wordstem(feat, "porter")
dfmat3 &lt;- dfm_replace(dfmat1, pattern = feat, replacement = featstem, case_insensitive = FALSE)
identical(dfmat3, dfm_wordstem(dfmat1, "porter"))
</code></pre>

<hr>
<h2 id='dfm_sample'>Randomly sample documents from a dfm</h2><span id='topic+dfm_sample'></span>

<h3>Description</h3>

<p>Take a random sample of documents of the specified size from a dfm, with
or without replacement, optionally by grouping variables or with probability
weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_sample(x, size = NULL, replace = FALSE, prob = NULL, by = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_sample_+3A_x">x</code></td>
<td>
<p>the <a href="#topic+dfm">dfm</a> object whose documents will be sampled</p>
</td></tr>
<tr><td><code id="dfm_sample_+3A_size">size</code></td>
<td>
<p>a positive number, the number of documents to select; when used
with <code>by</code>, the number to select from each group or a vector equal in
length to the number of groups defining the samples to be chosen in each
category of <code>by</code>.  By defining a size larger than the number of documents,
it is possible to oversample when <code>replace = TRUE</code>.</p>
</td></tr>
<tr><td><code id="dfm_sample_+3A_replace">replace</code></td>
<td>
<p>if <code>TRUE</code>, sample  with replacement</p>
</td></tr>
<tr><td><code id="dfm_sample_+3A_prob">prob</code></td>
<td>
<p>a vector of probability weights for obtaining the elements of the
vector being sampled.  May not be applied when <code>by</code> is used.</p>
</td></tr>
<tr><td><code id="dfm_sample_+3A_by">by</code></td>
<td>
<p>optional grouping variable for sampling.  This will be evaluated in
the docvars data.frame, so that docvars may be referred to by name without
quoting.  This also changes previous behaviours for <code>by</code>.
See <code>news(Version &gt;= "2.9", package = "quanteda")</code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+dfm">dfm</a> object (re)sampled on the documents, containing the document
variables for the documents sampled.
</p>


<h3>See Also</h3>

<p><a href="base.html#topic+sample">sample</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(10)
dfmat &lt;- dfm(tokens(c("a b c c d", "a a c c d d d", "a b b c")))
dfmat
dfm_sample(dfmat)
dfm_sample(dfmat, replace = TRUE)

# by groups
dfmat &lt;- dfm(tokens(data_corpus_inaugural[50:58]))
dfm_sample(dfmat, by = Party, size = 2)
</code></pre>

<hr>
<h2 id='dfm_select'>Select features from a dfm or fcm</h2><span id='topic+dfm_select'></span><span id='topic+dfm_remove'></span><span id='topic+dfm_keep'></span><span id='topic+fcm_select'></span><span id='topic+fcm_remove'></span><span id='topic+fcm_keep'></span>

<h3>Description</h3>

<p>This function selects or removes features from a <a href="#topic+dfm">dfm</a> or <a href="#topic+fcm">fcm</a>,
based on feature name matches with <code>pattern</code>.  The most common usages
are to eliminate features from a dfm already constructed, such as stopwords,
or to select only terms of interest from a dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_select(
  x,
  pattern = NULL,
  selection = c("keep", "remove"),
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  min_nchar = NULL,
  max_nchar = NULL,
  padding = FALSE,
  verbose = quanteda_options("verbose")
)

dfm_remove(x, ...)

dfm_keep(x, ...)

fcm_select(
  x,
  pattern = NULL,
  selection = c("keep", "remove"),
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  verbose = quanteda_options("verbose"),
  ...
)

fcm_remove(x, ...)

fcm_keep(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_select_+3A_x">x</code></td>
<td>
<p>the <a href="#topic+dfm">dfm</a> or <a href="#topic+fcm">fcm</a> object whose features will be selected</p>
</td></tr>
<tr><td><code id="dfm_select_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="#topic+pattern">pattern</a> for details.</p>
</td></tr>
<tr><td><code id="dfm_select_+3A_selection">selection</code></td>
<td>
<p>whether to <code>keep</code> or <code>remove</code> the features</p>
</td></tr>
<tr><td><code id="dfm_select_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="dfm_select_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="dfm_select_+3A_min_nchar">min_nchar</code>, <code id="dfm_select_+3A_max_nchar">max_nchar</code></td>
<td>
<p>optional numerics specifying the minimum and
maximum length in characters for tokens to be removed or kept; defaults are
<code>NULL</code> for no limits.  These are applied after (and hence, in addition
to) any selection based on pattern matches.</p>
</td></tr>
<tr><td><code id="dfm_select_+3A_padding">padding</code></td>
<td>
<p>if <code>TRUE</code>, record the number of removed tokens in the first column.</p>
</td></tr>
<tr><td><code id="dfm_select_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code>, print message about how many pattern were
removed</p>
</td></tr>
<tr><td><code id="dfm_select_+3A_...">...</code></td>
<td>
<p>used only for passing arguments from <code>dfm_remove</code> or
<code>dfm_keep</code> to <code>dfm_select</code>. Cannot include
<code>selection</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dfm_remove</code> and <code>fcm_remove</code> are simply a convenience
wrappers to calling <code>dfm_select</code> and <code>fcm_select</code> with
<code>selection = "remove"</code>.
</p>
<p><code>dfm_keep</code> and <code>fcm_keep</code> are simply a convenience wrappers to
calling <code>dfm_select</code> and <code>fcm_select</code> with <code>selection = "keep"</code>.
</p>


<h3>Value</h3>

<p>A <a href="#topic+dfm">dfm</a> or <a href="#topic+fcm">fcm</a> object, after the feature selection has
been applied.
</p>
<p>For compatibility with earlier versions, when <code>pattern</code> is a
<a href="#topic+dfm">dfm</a> object and <code>selection = "keep"</code>, then this will be
equivalent to calling <code><a href="#topic+dfm_match">dfm_match()</a></code>.  In this case, the following
settings are always used: <code>case_insensitive = FALSE</code>, and
<code>valuetype = "fixed"</code>.  This functionality is deprecated, however, and
you should use <code><a href="#topic+dfm_match">dfm_match()</a></code> instead.
</p>


<h3>Note</h3>

<p>This function selects features based on their labels.  To select
features based on the values of the document-feature matrix, use
<code><a href="#topic+dfm_trim">dfm_trim()</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfm_match">dfm_match()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat &lt;- tokens(c("My Christmas was ruined by your opposition tax plan.",
               "Does the United_States or Sweden have more progressive taxation?")) %&gt;%
    dfm(tolower = FALSE)
dict &lt;- dictionary(list(countries = c("United_States", "Sweden", "France"),
                        wordsEndingInY = c("by", "my"),
                        notintext = "blahblah"))
dfm_select(dfmat, pattern = dict)
dfm_select(dfmat, pattern = dict, case_insensitive = FALSE)
dfm_select(dfmat, pattern = c("s$", ".y"), selection = "keep", valuetype = "regex")
dfm_select(dfmat, pattern = c("s$", ".y"), selection = "remove", valuetype = "regex")
dfm_select(dfmat, pattern = stopwords("english"), selection = "keep", valuetype = "fixed")
dfm_select(dfmat, pattern = stopwords("english"), selection = "remove", valuetype = "fixed")

# select based on character length
dfm_select(dfmat, min_nchar = 5)

dfmat &lt;- dfm(tokens(c("This is a document with lots of stopwords.",
                      "No if, and, or but about it: lots of stopwords.")))
dfmat
dfm_remove(dfmat, stopwords("english"))
toks &lt;- tokens(c("this contains lots of stopwords",
                 "no if, and, or but about it: lots"),
               remove_punct = TRUE)
fcmat &lt;- fcm(toks)
fcmat
fcm_remove(fcmat, stopwords("english"))
</code></pre>

<hr>
<h2 id='dfm_sort'>Sort a dfm by frequency of one or more margins</h2><span id='topic+dfm_sort'></span>

<h3>Description</h3>

<p>Sorts a <a href="#topic+dfm">dfm</a> by descending frequency of total features, total features
in documents, or both.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_sort(x, decreasing = TRUE, margin = c("features", "documents", "both"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_sort_+3A_x">x</code></td>
<td>
<p>Document-feature matrix created by <code><a href="#topic+dfm">dfm()</a></code></p>
</td></tr>
<tr><td><code id="dfm_sort_+3A_decreasing">decreasing</code></td>
<td>
<p>logical; if <code>TRUE</code>, the sort will be in descending
order, otherwise sort in increasing order</p>
</td></tr>
<tr><td><code id="dfm_sort_+3A_margin">margin</code></td>
<td>
<p>which margin to sort on <code>features</code> to sort by frequency of
features, <code>documents</code> to sort by total feature counts in documents,
and <code>both</code> to sort by both</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sorted <a href="#topic+dfm">dfm</a> matrix object
</p>


<h3>Author(s)</h3>

<p>Ken Benoit
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat &lt;- dfm(data_corpus_inaugural)
head(dfmat)
head(dfm_sort(dfmat))
head(dfm_sort(dfmat, decreasing = FALSE, "both"))
</code></pre>

<hr>
<h2 id='dfm_subset'>Extract a subset of a dfm</h2><span id='topic+dfm_subset'></span>

<h3>Description</h3>

<p>Returns document subsets of a dfm that meet certain conditions,
including direct logical operations on docvars (document-level variables).
<code>dfm_subset</code> functions identically to <code><a href="base.html#topic+subset.data.frame">subset.data.frame()</a></code>,
using non-standard evaluation to evaluate conditions based on the
<a href="#topic+docvars">docvars</a> in the dfm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_subset(x, subset, drop_docid = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_subset_+3A_x">x</code></td>
<td>
<p><a href="#topic+dfm">dfm</a> object to be subsetted</p>
</td></tr>
<tr><td><code id="dfm_subset_+3A_subset">subset</code></td>
<td>
<p>logical expression indicating the documents to keep: missing
values are taken as false</p>
</td></tr>
<tr><td><code id="dfm_subset_+3A_drop_docid">drop_docid</code></td>
<td>
<p>if <code>TRUE</code>, <code>docid</code> for documents are removed as the result
of subsetting.</p>
</td></tr>
<tr><td><code id="dfm_subset_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To select or subset <em>features</em>, see <code><a href="#topic+dfm_select">dfm_select()</a></code> instead.
</p>
<p>When <code>select</code> is a dfm, then the returned dfm will be equal in
document dimension and order to the dfm used for selection.  This is the
document-level version of using <code><a href="#topic+dfm_select">dfm_select()</a></code> where
<code>pattern</code> is a dfm: that function matches features, while
<code>dfm_subset</code> will match documents.
</p>


<h3>Value</h3>

<p><a href="#topic+dfm">dfm</a> object, with a subset of documents (and docvars) selected
according to arguments
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+subset.data.frame">subset.data.frame()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corp &lt;- corpus(c(d1 = "a b c d", d2 = "a a b e",
                 d3 = "b b c e", d4 = "e e f a b"),
               docvars = data.frame(grp = c(1, 1, 2, 3)))
dfmat &lt;- dfm(tokens(corp))
# selecting on a docvars condition
dfm_subset(dfmat, grp &gt; 1)
# selecting on a supplied vector
dfm_subset(dfmat, c(TRUE, FALSE, TRUE, FALSE))
</code></pre>

<hr>
<h2 id='dfm_tfidf'>Weight a dfm by <em>tf-idf</em></h2><span id='topic+dfm_tfidf'></span>

<h3>Description</h3>

<p>Weight a dfm by term frequency-inverse document frequency (<em>tf-idf</em>),
with full control over options.  Uses fully sparse methods for efficiency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_tfidf(
  x,
  scheme_tf = "count",
  scheme_df = "inverse",
  base = 10,
  force = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_tfidf_+3A_x">x</code></td>
<td>
<p>object for which idf or tf-idf will be computed (a document-feature
matrix)</p>
</td></tr>
<tr><td><code id="dfm_tfidf_+3A_scheme_tf">scheme_tf</code></td>
<td>
<p>scheme for <code><a href="#topic+dfm_weight">dfm_weight()</a></code>; defaults to <code>"count"</code></p>
</td></tr>
<tr><td><code id="dfm_tfidf_+3A_scheme_df">scheme_df</code></td>
<td>
<p>scheme for <code><a href="#topic+docfreq">docfreq()</a></code>; defaults to
<code>"inverse"</code>.</p>
</td></tr>
<tr><td><code id="dfm_tfidf_+3A_base">base</code></td>
<td>
<p>the base for the logarithms in the <code><a href="#topic+dfm_weight">dfm_weight()</a></code> and
<code><a href="#topic+docfreq">docfreq()</a></code> calls; default is 10</p>
</td></tr>
<tr><td><code id="dfm_tfidf_+3A_force">force</code></td>
<td>
<p>logical; if <code>TRUE</code>, apply weighting scheme even if the dfm
has been weighted before.  This can result in invalid weights, such as as
weighting by <code>"prop"</code> after applying <code>"logcount"</code>, or after
having grouped a dfm using <code><a href="#topic+dfm_group">dfm_group()</a></code>.</p>
</td></tr>
<tr><td><code id="dfm_tfidf_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="#topic+docfreq">docfreq</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dfm_tfidf</code> computes term frequency-inverse document frequency
weighting.  The default is to use counts instead of normalized term
frequency (the relative term frequency within document), but this
can be overridden using <code>scheme_tf = "prop"</code>.
</p>


<h3>References</h3>

<p>Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008).
<em>Introduction to Information Retrieval</em>. Cambridge: Cambridge University Press.
<a href="https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf">https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat1 &lt;- as.dfm(data_dfm_lbgexample)
head(dfmat1[, 5:10])
head(dfm_tfidf(dfmat1)[, 5:10])
docfreq(dfmat1)[5:15]
head(dfm_weight(dfmat1)[, 5:10])

# replication of worked example from
# https://en.wikipedia.org/wiki/Tf-idf#Example_of_tf.E2.80.93idf
dfmat2 &lt;-
    matrix(c(1,1,2,1,0,0, 1,1,0,0,2,3),
           byrow = TRUE, nrow = 2,
           dimnames = list(docs = c("document1", "document2"),
                           features = c("this", "is", "a", "sample",
                                        "another", "example"))) %&gt;%
    as.dfm()
dfmat2
docfreq(dfmat2)
dfm_tfidf(dfmat2, scheme_tf = "prop") %&gt;% round(digits = 2)

## Not run: 
# comparison with tm
if (requireNamespace("tm")) {
    convert(dfmat2, to = "tm") %&gt;% tm::weightTfIdf() %&gt;% as.matrix()
    # same as:
    dfm_tfidf(dfmat2, base = 2, scheme_tf = "prop")
}

## End(Not run)
</code></pre>

<hr>
<h2 id='dfm_tolower'>Convert the case of the features of a dfm and combine</h2><span id='topic+dfm_tolower'></span><span id='topic+dfm_toupper'></span><span id='topic+fcm_tolower'></span><span id='topic+fcm_toupper'></span>

<h3>Description</h3>

<p><code>dfm_tolower()</code> and <code>dfm_toupper()</code> convert the features of the dfm or
fcm to lower and upper case, respectively, and then recombine the counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_tolower(x, keep_acronyms = FALSE)

dfm_toupper(x)

fcm_tolower(x, keep_acronyms = FALSE)

fcm_toupper(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_tolower_+3A_x">x</code></td>
<td>
<p>the input object whose character/tokens/feature elements will be
case-converted</p>
</td></tr>
<tr><td><code id="dfm_tolower_+3A_keep_acronyms">keep_acronyms</code></td>
<td>
<p>logical; if <code>TRUE</code>, do not lowercase any
all-uppercase words (applies only to <code style="white-space: pre;">&#8288;*_tolower()&#8288;</code> functions)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fcm_tolower()</code> and <code>fcm_toupper()</code> convert both dimensions of
the <a href="#topic+fcm">fcm</a> to lower and upper case, respectively, and then recombine
the counts. This works only on fcm objects created with <code>context = "document"</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># for a document-feature matrix
dfmat &lt;- dfm(tokens(c("b A A", "C C a b B")), tolower = FALSE)
dfmat
dfm_tolower(dfmat)
dfm_toupper(dfmat)

# for a feature co-occurrence matrix
fcmat &lt;- fcm(tokens(c("b A A d", "C C a b B e")),
             context = "document")
fcmat
fcm_tolower(fcmat)
fcm_toupper(fcmat)
</code></pre>

<hr>
<h2 id='dfm_trim'>Trim a dfm using frequency threshold-based feature selection</h2><span id='topic+dfm_trim'></span>

<h3>Description</h3>

<p>Returns a document by feature matrix reduced in size based on
document and term frequency, usually in terms of a minimum frequency, but
may also be in terms of maximum frequencies.  Setting a combination of
minimum and maximum frequencies will select features based on a range.
</p>
<p>Feature selection is implemented by considering features across
all documents, by summing them for term frequency, or counting the
documents in which they occur for document frequency. Rank and quantile
versions of these are also implemented, for taking the first <code class="reqn">n</code>
features in terms of descending order of overall global counts or document
frequencies, or as a quantile of all frequencies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_trim(
  x,
  min_termfreq = NULL,
  max_termfreq = NULL,
  termfreq_type = c("count", "prop", "rank", "quantile"),
  min_docfreq = NULL,
  max_docfreq = NULL,
  docfreq_type = c("count", "prop", "rank", "quantile"),
  sparsity = NULL,
  verbose = quanteda_options("verbose"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_trim_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+dfm">dfm</a> object</p>
</td></tr>
<tr><td><code id="dfm_trim_+3A_min_termfreq">min_termfreq</code>, <code id="dfm_trim_+3A_max_termfreq">max_termfreq</code></td>
<td>
<p>minimum/maximum values of feature frequencies
across all documents, below/above which features will
be removed</p>
</td></tr>
<tr><td><code id="dfm_trim_+3A_termfreq_type">termfreq_type</code></td>
<td>
<p>how <code>min_termfreq</code> and <code>max_termfreq</code> are
interpreted.  <code>"count"</code> sums the frequencies; <code>"prop"</code> divides the
term frequencies by the total sum; <code>"rank"</code> is matched against the
inverted ranking of features in terms of overall frequency, so that 1, 2,
... are the highest and second highest frequency features, and so on;
<code>"quantile"</code> sets the cutoffs according to the quantiles (see
<code><a href="stats.html#topic+quantile">quantile()</a></code>) of term frequencies.</p>
</td></tr>
<tr><td><code id="dfm_trim_+3A_min_docfreq">min_docfreq</code>, <code id="dfm_trim_+3A_max_docfreq">max_docfreq</code></td>
<td>
<p>minimum/maximum values of a feature's document
frequency, below/above which features will be removed</p>
</td></tr>
<tr><td><code id="dfm_trim_+3A_docfreq_type">docfreq_type</code></td>
<td>
<p>specify how <code>min_docfreq</code> and <code>max_docfreq</code> are
interpreted.   <code>"count"</code> is the same as <code style="white-space: pre;">&#8288;[docfreq](x, scheme = "count")&#8288;</code>; <code>"prop"</code> divides the document frequencies by the total
sum; <code>"rank"</code> is matched against the inverted ranking of document
frequency, so that 1, 2, ... are the features with the highest and second
highest document frequencies, and so on; <code>"quantile"</code> sets the cutoffs
according to the quantiles (see <code><a href="stats.html#topic+quantile">quantile()</a></code>) of document
frequencies.</p>
</td></tr>
<tr><td><code id="dfm_trim_+3A_sparsity">sparsity</code></td>
<td>
<p>equivalent to <code>1 - min_docfreq</code>, included for comparison
with <span class="pkg">tm</span></p>
</td></tr>
<tr><td><code id="dfm_trim_+3A_verbose">verbose</code></td>
<td>
<p>print messages</p>
</td></tr>
<tr><td><code id="dfm_trim_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+dfm">dfm</a> reduced in features (with the same number of documents)
</p>


<h3>Note</h3>

<p>Trimming a <a href="#topic+dfm">dfm</a> object is an operation based on the <em>values</em>
in the document-feature matrix.  To select subsets of a dfm based on the
features themselves (meaning the feature labels from
<code><a href="#topic+featnames">featnames()</a></code>) &ndash; such as those matching a regular expression, or
removing features matching a stopword list, use <code><a href="#topic+dfm_select">dfm_select()</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfm_select">dfm_select()</a></code>, <code><a href="#topic+dfm_sample">dfm_sample()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>(dfmat &lt;- dfm(data_corpus_inaugural[1:5]))

# keep only words occurring &gt;= 10 times and in &gt;= 2 documents
dfm_trim(dfmat, min_termfreq = 10, min_docfreq = 2)

# keep only words occurring &gt;= 10 times and in at least 0.4 of the documents
dfm_trim(dfmat, min_termfreq = 10, min_docfreq = 0.4)

# keep only words occurring &lt;= 10 times and in &lt;=2 documents
dfm_trim(dfmat, max_termfreq = 10, max_docfreq = 2)

# keep only words occurring &lt;= 10 times and in at most 3/4 of the documents
dfm_trim(dfmat, max_termfreq = 10, max_docfreq = 0.75)

# keep only words occurring 5 times in 1000, and in 2 of 5 of documents
dfm_trim(dfmat, min_docfreq = 0.4, min_termfreq = 0.005, termfreq_type = "prop")

# keep only words occurring frequently (top 20%) and in &lt;=2 documents
dfm_trim(dfmat, min_termfreq = 0.2, max_docfreq = 2, termfreq_type = "quantile")

## Not run: 
# compare to removeSparseTerms from the tm package
(dfmattm &lt;- convert(dfmat, "tm"))
tm::removeSparseTerms(dfmattm, 0.7)
dfm_trim(dfmat, min_docfreq = 0.3)
dfm_trim(dfmat, sparsity = 0.7)

## End(Not run)

</code></pre>

<hr>
<h2 id='dfm_weight'>Weight the feature frequencies in a dfm</h2><span id='topic+dfm_weight'></span><span id='topic+dfm_smooth'></span>

<h3>Description</h3>

<p>Weight the feature frequencies in a dfm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_weight(
  x,
  scheme = c("count", "prop", "propmax", "logcount", "boolean", "augmented", "logave"),
  weights = NULL,
  base = 10,
  k = 0.5,
  smoothing = 0.5,
  force = FALSE
)

dfm_smooth(x, smoothing = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_weight_+3A_x">x</code></td>
<td>
<p>document-feature matrix created by <a href="#topic+dfm">dfm</a></p>
</td></tr>
<tr><td><code id="dfm_weight_+3A_scheme">scheme</code></td>
<td>
<p>a label of the weight type:
</p>

<dl>
<dt><code>count</code></dt><dd><p><code class="reqn">tf_{ij}</code>, an integer feature count (default when a dfm
is created)</p>
</dd>
<dt><code>prop</code></dt><dd><p>the proportion of the feature counts of total feature counts
(aka relative frequency), calculated as <code class="reqn">tf_{ij} / \sum_j tf_{ij}</code></p>
</dd>
<dt><code>propmax</code></dt><dd><p>the proportion of the feature counts of the highest
feature count in a document, <code class="reqn">tf_{ij} / \textrm{max}_j tf_{ij}</code></p>
</dd>
<dt><code>logcount</code></dt><dd><p>take the 1 + the logarithm of each count, for the
given base, or 0 if the count was zero: <code class="reqn">1 +
  \textrm{log}_{base}(tf_{ij})</code> if <code class="reqn">tf_{ij} &gt; 0</code>, or 0 otherwise.</p>
</dd>
<dt><code>boolean</code></dt><dd><p>recode all non-zero counts as 1</p>
</dd>
<dt><code>augmented</code></dt><dd><p>equivalent to <code class="reqn">k + (1 - k) *</code> <code>dfm_weight(x, "propmax")</code></p>
</dd>
<dt><code>logave</code></dt><dd><p>(1 + the log of the counts) / (1 + log of the average count
within document), or </p>
<p style="text-align: center;"><code class="reqn">\frac{1 + \textrm{log}_{base} tf_{ij}}{1 +
  \textrm{log}_{base}(\sum_j tf_{ij} / N_i)}</code>
</p>
</dd>
<dt><code>logsmooth</code></dt><dd><p>log of the counts + <code>smooth</code>, or <code class="reqn">tf_{ij} + s</code></p>
</dd>
</dl>
</td></tr>
<tr><td><code id="dfm_weight_+3A_weights">weights</code></td>
<td>
<p>if <code>scheme</code> is unused, then <code>weights</code> can be a named
numeric vector of weights to be applied to the dfm, where the names of the
vector correspond to feature labels of the dfm, and the weights will be
applied as multipliers to the existing feature counts for the corresponding
named features.  Any features not named will be assigned a weight of 1.0
(meaning they will be unchanged).</p>
</td></tr>
<tr><td><code id="dfm_weight_+3A_base">base</code></td>
<td>
<p>base for the logarithm when <code>scheme</code> is <code>"logcount"</code> or
<code>logave</code></p>
</td></tr>
<tr><td><code id="dfm_weight_+3A_k">k</code></td>
<td>
<p>the k for the augmentation when <code>scheme = "augmented"</code></p>
</td></tr>
<tr><td><code id="dfm_weight_+3A_smoothing">smoothing</code></td>
<td>
<p>constant added to the dfm cells for smoothing, default is 1
for <code>dfm_smooth()</code> and 0.5 for <code>dfm_weight()</code></p>
</td></tr>
<tr><td><code id="dfm_weight_+3A_force">force</code></td>
<td>
<p>logical; if <code>TRUE</code>, apply weighting scheme even if the dfm
has been weighted before.  This can result in invalid weights, such as as
weighting by <code>"prop"</code> after applying <code>"logcount"</code>, or after
having grouped a dfm using <code><a href="#topic+dfm_group">dfm_group()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>dfm_weight</code> returns the dfm with weighted values.  Note the
because the default weighting scheme is <code>"count"</code>, simply calling this
function on an unweighted dfm will return the same object.  Many users will
want the normalized dfm consisting of the proportions of the feature counts
within each document, which requires setting <code>scheme = "prop"</code>.
</p>
<p><code>dfm_smooth</code> returns a dfm whose values have been smoothed by
adding the <code>smoothing</code> amount. Note that this effectively converts a
matrix from sparse to dense format, so may exceed memory requirements
depending on the size of your input matrix.
</p>


<h3>References</h3>

<p>Manning, C.D., Raghavan, P., &amp; Schütze, H. (2008).
<em>An Introduction to Information Retrieval</em>. Cambridge: Cambridge University Press.
<a href="https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf">https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+docfreq">docfreq()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat1 &lt;- dfm(data_corpus_inaugural)

dfmat2 &lt;- dfm_weight(dfmat1, scheme = "prop")
topfeatures(dfmat2)
dfmat3 &lt;- dfm_weight(dfmat1)
topfeatures(dfmat3)
dfmat4 &lt;- dfm_weight(dfmat1, scheme = "logcount")
topfeatures(dfmat4)
dfmat5 &lt;- dfm_weight(dfmat1, scheme = "logave")
topfeatures(dfmat5)

# combine these methods for more complex dfm_weightings, e.g. as in Section 6.4
# of Introduction to Information Retrieval
head(dfm_tfidf(dfmat1, scheme_tf = "logcount"))

# apply numeric weights
str &lt;- c("apple is better than banana", "banana banana apple much better")
(dfmat6 &lt;- dfm(str, remove = stopwords("english")))
dfm_weight(dfmat6, weights = c(apple = 5, banana = 3, much = 0.5))

# smooth the dfm
dfmat &lt;- dfm(data_corpus_inaugural)
dfm_smooth(dfmat, 0.5)
</code></pre>

<hr>
<h2 id='dfm-class'>Virtual class &quot;dfm&quot; for a document-feature matrix</h2><span id='topic+dfm-class'></span><span id='topic+t+2Cdfm-method'></span><span id='topic+colSums+2Cdfm-method'></span><span id='topic+rowSums+2Cdfm-method'></span><span id='topic+colMeans+2Cdfm-method'></span><span id='topic+rowMeans+2Cdfm-method'></span><span id='topic+Arith+2Cdfm+2Cnumeric-method'></span><span id='topic+Arith+2Cnumeric+2Cdfm-method'></span><span id='topic++5B+2Cdfm+2Cindex+2Cindex+2Cmissing-method'></span><span id='topic++5B+2Cdfm+2Cindex+2Cindex+2Clogical-method'></span><span id='topic++5B+2Cdfm+2Cmissing+2Cmissing+2Cmissing-method'></span><span id='topic++5B+2Cdfm+2Cmissing+2Cmissing+2Clogical-method'></span><span id='topic++5B+2Cdfm+2Cindex+2Cmissing+2Cmissing-method'></span><span id='topic++5B+2Cdfm+2Cindex+2Cmissing+2Clogical-method'></span><span id='topic++5B+2Cdfm+2Cmissing+2Cindex+2Cmissing-method'></span><span id='topic++5B+2Cdfm+2Cmissing+2Cindex+2Clogical-method'></span>

<h3>Description</h3>

<p>The dfm class of object is a type of <a href="Matrix.html#topic+Matrix-class">Matrix-class</a> object with
additional slots, described below.  <span class="pkg">quanteda</span> uses two subclasses of the
<code>dfm</code> class, depending on whether the object can be represented by a
sparse matrix, in which case it is a <code>dfm</code> class object, or if dense,
then a <code>dfmDense</code> object.  See Details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'dfm'
t(x)

## S4 method for signature 'dfm'
colSums(x, na.rm = FALSE, dims = 1, ...)

## S4 method for signature 'dfm'
rowSums(x, na.rm = FALSE, dims = 1, ...)

## S4 method for signature 'dfm'
colMeans(x, na.rm = FALSE, dims = 1, ...)

## S4 method for signature 'dfm'
rowMeans(x, na.rm = FALSE, dims = 1, ...)

## S4 method for signature 'dfm,numeric'
Arith(e1, e2)

## S4 method for signature 'numeric,dfm'
Arith(e1, e2)

## S4 method for signature 'dfm,index,index,missing'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'dfm,index,index,logical'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'dfm,missing,missing,missing'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'dfm,missing,missing,logical'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'dfm,index,missing,missing'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'dfm,index,missing,logical'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'dfm,missing,index,missing'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'dfm,missing,index,logical'
x[i, j, ..., drop = TRUE]
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm-class_+3A_x">x</code></td>
<td>
<p>the dfm object</p>
</td></tr>
<tr><td><code id="dfm-class_+3A_na.rm">na.rm</code></td>
<td>
<p>if <code>TRUE</code>, omit missing values (including <code>NaN</code>) from
the calculations</p>
</td></tr>
<tr><td><code id="dfm-class_+3A_dims">dims</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="dfm-class_+3A_...">...</code></td>
<td>
<p>additional arguments not used here</p>
</td></tr>
<tr><td><code id="dfm-class_+3A_e1">e1</code></td>
<td>
<p>first quantity in an <a href="methods.html#topic+S4groupGeneric">Arith</a> operation for dfm</p>
</td></tr>
<tr><td><code id="dfm-class_+3A_e2">e2</code></td>
<td>
<p>second quantity in an <a href="methods.html#topic+S4groupGeneric">Arith</a> operation for dfm</p>
</td></tr>
<tr><td><code id="dfm-class_+3A_i">i</code></td>
<td>
<p>document names or indices for documents to extract.</p>
</td></tr>
<tr><td><code id="dfm-class_+3A_j">j</code></td>
<td>
<p>feature names or indices for documents to extract.</p>
</td></tr>
<tr><td><code id="dfm-class_+3A_drop_docid">drop_docid</code></td>
<td>
<p>if <code>TRUE</code>, <code>docid</code> for documents are removed as the result
of extraction.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>dfm</code> class is a virtual class that will contain
<a href="Matrix.html#topic+dgCMatrix-class">dgCMatrix-class</a>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>weightTf</code></dt><dd><p>the type of term frequency weighting applied to the dfm.  Default is
<code>"frequency"</code>, indicating that the values in the cells of the dfm are
simple feature counts. To change this, use the <code><a href="#topic+dfm_weight">dfm_weight()</a></code>
method.</p>
</dd>
<dt><code>weightFf</code></dt><dd><p>the type of document frequency weighting applied to the dfm. See
<code><a href="#topic+docfreq">docfreq()</a></code>.</p>
</dd>
<dt><code>smooth</code></dt><dd><p>a smoothing parameter, defaults to zero.  Can be changed using
the <code><a href="#topic+dfm_smooth">dfm_smooth()</a></code> method.</p>
</dd>
<dt><code>Dimnames</code></dt><dd><p>These are inherited from <a href="Matrix.html#topic+Matrix-class">Matrix-class</a> but are
named <code>docs</code> and <code>features</code> respectively.</p>
</dd>
</dl>


<h3>See Also</h3>

<p><a href="#topic+dfm">dfm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># dfm subsetting
dfmat &lt;- dfm(tokens(c("this contains lots of stopwords",
                  "no if, and, or but about it: lots",
                  "and a third document is it"),
                remove_punct = TRUE))
dfmat[1:2, ]
dfmat[1:2, 1:5]
</code></pre>

<hr>
<h2 id='dfm-internal'>Internal functions for dfm objects</h2><span id='topic+dfm-internal'></span><span id='topic+Compare+2Cdfm+2Cnumeric-method'></span>

<h3>Description</h3>

<p>Internal function documentation for <a href="#topic+dfm">dfm</a> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'dfm,numeric'
Compare(e1, e2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm-internal_+3A_e1">e1</code></td>
<td>
<p>a <a href="#topic+dfm">dfm</a></p>
</td></tr>
<tr><td><code id="dfm-internal_+3A_e2">e2</code></td>
<td>
<p>a numeric value to compare with values in a dfm</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="base.html#topic+Comparison">Comparison</a> operators
</p>

<hr>
<h2 id='dfm2lsa'>Convert a dfm to an lsa &quot;textmatrix&quot;</h2><span id='topic+dfm2lsa'></span>

<h3>Description</h3>

<p>Converts a dfm to a textmatrix for use with the lsa package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm2lsa(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm2lsa_+3A_x">x</code></td>
<td>
<p>dfm to be converted</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
(dfmat &lt;- dfm(tokens(c(d1 = "this is a first matrix",
                       d2 = "this is second matrix as example"))))
lsa::lsa(convert(dfmat, to = "lsa"))

## End(Not run)
</code></pre>

<hr>
<h2 id='dictionary'>Create a dictionary</h2><span id='topic+dictionary'></span>

<h3>Description</h3>

<p>Create a <span class="pkg">quanteda</span> dictionary class object, either from a list or by
importing from a foreign format.  Currently supported input file formats are
the WordStat, LIWC, Lexicoder v2 and v3, and Yoshikoder formats.  The import
using the LIWC format works with all currently available dictionary files
supplied as part of the LIWC 2001, 2007, and 2015 software (see References).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dictionary(
  x,
  file = NULL,
  format = NULL,
  separator = " ",
  tolower = TRUE,
  encoding = "utf-8"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dictionary_+3A_x">x</code></td>
<td>
<p>a named list of character vector dictionary entries, including
<a href="#topic+valuetype">valuetype</a> pattern matches, and including multi-word expressions
separated by <code>concatenator</code>.  See examples. This argument may be
omitted if the dictionary is read from <code>file</code>.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_file">file</code></td>
<td>
<p>file identifier for a foreign dictionary</p>
</td></tr>
<tr><td><code id="dictionary_+3A_format">format</code></td>
<td>
<p>character identifier for the format of the foreign dictionary.
If not supplied, the format is guessed from the dictionary file's
extension. Available options are: </p>

<dl>
<dt><code>"wordstat"</code></dt><dd><p>format used by Provalis Research's WordStat
software</p>
</dd> <dt><code>"LIWC"</code></dt><dd><p>format used by the Linguistic Inquiry and
Word Count software</p>
</dd> <dt><code>"yoshikoder"</code></dt><dd><p> format used by Yoshikoder
software</p>
</dd> <dt><code>"lexicoder"</code></dt><dd><p>format used by Lexicoder</p>
</dd>
<dt><code>"YAML"</code></dt><dd><p>the standard YAML format</p>
</dd></dl>
</td></tr>
<tr><td><code id="dictionary_+3A_separator">separator</code></td>
<td>
<p>the character in between multi-word dictionary values. This
defaults to <code>" "</code>.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_tolower">tolower</code></td>
<td>
<p>if <code>TRUE</code>, convert all dictionary values to lowercase</p>
</td></tr>
<tr><td><code id="dictionary_+3A_encoding">encoding</code></td>
<td>
<p>additional optional encoding value for reading in imported
dictionaries. This uses the <a href="base.html#topic+iconv">iconv</a> labels for encoding.  See the
&quot;Encoding&quot; section of the help for <a href="base.html#topic+file">file</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Dictionaries can be subsetted using
<code><a href="#topic+dictionary2-class">[</a></code> and
<code><a href="#topic+dictionary2-class">[[</a></code>, operating the same as the equivalent
<a href="#topic+dictionary2-class">list</a> operators.
</p>
<p>Dictionaries can be coerced from lists using <code><a href="#topic+as.dictionary">as.dictionary()</a></code>,
coerced to named lists of characters using
<code><a href="#topic+dictionary2-class">as.list()</a></code>, and checked using
<code><a href="#topic+is.dictionary">is.dictionary()</a></code>.
</p>


<h3>Value</h3>

<p>A dictionary class object, essentially a specially classed named list
of characters.
</p>


<h3>References</h3>

<p>WordStat dictionaries page, from Provalis Research
<a href="https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/">https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/</a>.
</p>
<p>Pennebaker, J.W., Chung, C.K., Ireland, M., Gonzales, A., &amp; Booth, R.J.
(2007). The development and psychometric properties of LIWC2007. [Software
manual]. Austin, TX (<a href="https://www.liwc.app/">https://www.liwc.app/</a>).
</p>
<p>Yoshikoder page, from Will Lowe
<a href="https://conjugateprior.org/software/yoshikoder/">https://conjugateprior.org/software/yoshikoder/</a>.
</p>
<p>Lexicoder format, <a href="https://www.snsoroka.com/data-lexicoder/">https://www.snsoroka.com/data-lexicoder/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.dictionary">as.dictionary()</a></code>,
<code><a href="#topic+dictionary2-class">as.list()</a></code>, <code><a href="#topic+is.dictionary">is.dictionary()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corp &lt;- corpus_subset(data_corpus_inaugural, Year&gt;1900)
dict &lt;- dictionary(list(christmas = c("Christmas", "Santa", "holiday"),
                          opposition = c("Opposition", "reject", "notincorpus"),
                          taxing = "taxing",
                          taxation = "taxation",
                          taxregex = "tax*",
                          country = "america"))
head(dfm(tokens(corp), dictionary = dict))

# subset a dictionary
dict[1:2]
dict[c("christmas", "opposition")]
dict[["opposition"]]

# combine dictionaries
c(dict["christmas"], dict["country"])

## Not run: 
# import the Laver-Garry dictionary from Provalis Research
dictfile &lt;- tempfile()
download.file("https://provalisresearch.com/Download/LaverGarry.zip",
              dictfile, mode = "wb")
unzip(dictfile, exdir = (td &lt;- tempdir()))
dictlg &lt;- dictionary(file = paste(td, "LaverGarry.cat", sep = "/"))
head(dfm(data_corpus_inaugural, dictionary = dictlg))

# import a LIWC formatted dictionary from http://www.moralfoundations.org
download.file("http://bit.ly/37cV95h", tf &lt;- tempfile())
dictliwc &lt;- dictionary(file = tf, format = "LIWC")
head(dfm(data_corpus_inaugural, dictionary = dictliwc))

## End(Not run)
</code></pre>

<hr>
<h2 id='dictionary2-class'>dictionary class objects and functions</h2><span id='topic+dictionary2-class'></span><span id='topic+as.list+2Cdictionary2-method'></span><span id='topic++5B+2Cdictionary2+2Cindex+2CANY+2CANY-method'></span><span id='topic++5B+5B+2Cdictionary2+2Cindex-method'></span><span id='topic++24.dictionary2'></span><span id='topic+c+2Cdictionary2-method'></span>

<h3>Description</h3>

<p>The <code>dictionary2</code> class constructed by <code><a href="#topic+dictionary">dictionary()</a></code>, and associated core
class functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'dictionary2'
as.list(x, flatten = FALSE, levels = 1:100)

## S4 method for signature 'dictionary2,index,ANY,ANY'
x[i]

## S4 method for signature 'dictionary2,index'
x[[i]]

## S3 method for class 'dictionary2'
x$name

## S4 method for signature 'dictionary2'
c(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dictionary2-class_+3A_flatten">flatten</code></td>
<td>
<p>flatten the nested structure if <code>TRUE</code></p>
</td></tr>
<tr><td><code id="dictionary2-class_+3A_levels">levels</code></td>
<td>
<p>integer vector indicating levels in the dictionary. Used only
when <code>flatten = TRUE</code>.</p>
</td></tr>
<tr><td><code id="dictionary2-class_+3A_i">i</code></td>
<td>
<p>index for entries</p>
</td></tr>
<tr><td><code id="dictionary2-class_+3A_name">name</code></td>
<td>
<p>the dictionary key</p>
</td></tr>
<tr><td><code id="dictionary2-class_+3A_...">...</code></td>
<td>
<p><a href="#topic+dictionary">dictionary</a> objects to be concatenated</p>
</td></tr>
<tr><td><code id="dictionary2-class_+3A_object">object</code></td>
<td>
<p>the dictionary to be extracted</p>
</td></tr>
</table>


<h3>Slots</h3>


<dl>
<dt><code>.Data</code></dt><dd><p>named list of mode character, where each element name is a
dictionary &quot;key&quot; and each element is one or more dictionary entry &quot;values&quot;
consisting of a pattern match</p>
</dd>
<dt><code>meta</code></dt><dd><p>list of object metadata</p>
</dd>
</dl>

<hr>
<h2 id='docfreq'>Compute the (weighted) document frequency of a feature</h2><span id='topic+docfreq'></span>

<h3>Description</h3>

<p>For a <a href="#topic+dfm">dfm</a> object, returns a (weighted) document frequency for each
term.  The default is a simple count of the number of documents in which a
feature occurs more than a given frequency threshold.  (The default threshold
is  zero, meaning that any feature occurring at least once in a document will
be counted.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>docfreq(
  x,
  scheme = c("count", "inverse", "inversemax", "inverseprob", "unary"),
  base = 10,
  smoothing = 0,
  k = 0,
  threshold = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="docfreq_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+dfm">dfm</a></p>
</td></tr>
<tr><td><code id="docfreq_+3A_scheme">scheme</code></td>
<td>
<p>type of document frequency weighting, computed as
follows, where <code class="reqn">N</code> is defined as the number of documents in the dfm and
<code class="reqn">s</code> is the smoothing constant:
</p>

<dl>
<dt><code>count</code></dt><dd><p><code class="reqn">df_j</code>, the number of documents for which <code class="reqn">n_{ij} &gt; threshold</code></p>
</dd>
<dt><code>inverse</code></dt><dd><p style="text-align: center;"><code class="reqn">\textrm{log}_{base}\left(s + \frac{N}{k + df_j}\right)</code>
</p>
</dd>
<dt><code>inversemax</code></dt><dd><p style="text-align: center;"><code class="reqn">\textrm{log}_{base}\left(s + \frac{\textrm{max}(df_j)}{k + df_j}\right)</code>
</p>
</dd>
<dt><code>inverseprob</code></dt><dd><p style="text-align: center;"><code class="reqn">\textrm{log}_{base}\left(\frac{N - df_j}{k + df_j}\right)</code>
</p>
</dd>
<dt><code>unary</code></dt><dd><p>1 for each feature</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="docfreq_+3A_base">base</code></td>
<td>
<p>the base with respect to which logarithms in the inverse document
frequency weightings are computed; default is 10 (see Manning, Raghavan,
and Schütze 2008, p123).</p>
</td></tr>
<tr><td><code id="docfreq_+3A_smoothing">smoothing</code></td>
<td>
<p>added to the quotient before taking the logarithm</p>
</td></tr>
<tr><td><code id="docfreq_+3A_k">k</code></td>
<td>
<p>added to the denominator in the &quot;inverse&quot; weighting types, to
prevent a zero document count for a term</p>
</td></tr>
<tr><td><code id="docfreq_+3A_threshold">threshold</code></td>
<td>
<p>numeric value of the threshold <em>above which</em> a feature
will considered in the computation of document frequency.  The default is
0, meaning that a feature's document frequency will be the number of
documents in which it occurs greater than zero times.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of document frequencies for each feature
</p>


<h3>References</h3>

<p>Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008).
<em>Introduction to Information Retrieval</em>. Cambridge: Cambridge University Press.
<a href="https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf">https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat1 &lt;- dfm(data_corpus_inaugural[1:2])
docfreq(dfmat1[, 1:20])

# replication of worked example from
# https://en.wikipedia.org/wiki/Tf-idf#Example_of_tf.E2.80.93idf
dfmat2 &lt;-
    matrix(c(1,1,2,1,0,0, 1,1,0,0,2,3),
           byrow = TRUE, nrow = 2,
           dimnames = list(docs = c("document1", "document2"),
                           features = c("this", "is", "a", "sample",
                                        "another", "example"))) %&gt;%
    as.dfm()
dfmat2
docfreq(dfmat2)
docfreq(dfmat2, scheme = "inverse")
docfreq(dfmat2, scheme = "inverse", k = 1, smoothing = 1)
docfreq(dfmat2, scheme = "unary")
docfreq(dfmat2, scheme = "inversemax")
docfreq(dfmat2, scheme = "inverseprob")
</code></pre>

<hr>
<h2 id='docnames'>Get or set document names</h2><span id='topic+docnames'></span><span id='topic+docnames+3C-'></span><span id='topic+docid'></span><span id='topic+segid'></span>

<h3>Description</h3>

<p>Get or set the document names of a <a href="#topic+corpus">corpus</a>, <a href="#topic+tokens">tokens</a>, or <a href="#topic+dfm">dfm</a> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>docnames(x)

docnames(x) &lt;- value

docid(x)

segid(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="docnames_+3A_x">x</code></td>
<td>
<p>the object with docnames</p>
</td></tr>
<tr><td><code id="docnames_+3A_value">value</code></td>
<td>
<p>a character vector of the same length as <code>x</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>docnames</code> returns a character vector of the document names
</p>
<p><code style="white-space: pre;">&#8288;docnames &lt;-&#8288;</code> assigns new values to the document names of an object.
docnames can only be character, so any non-character value assigned to be a
docname will be coerced to mode <code>character</code>.
</p>
<p><code>docid</code> returns an internal variable denoting the original &quot;docname&quot;
from which a document came.  If an object has been reshaped (e.g.
<code><a href="#topic+corpus_reshape">corpus_reshape()</a></code> or segmented (e.g. <code><a href="#topic+corpus_segment">corpus_segment()</a></code>), <code>docid(x)</code> returns
the original docnames but <code>segid(x)</code> does the serial number of those segments
within the original document.
</p>


<h3>Note</h3>

<p><code>docid</code> and <code>segid</code> are designed primarily for developers, not for end users.  In
most cases, you will want <code>docnames</code> instead.  It is, however, the
default for <a href="#topic+groups">groups</a>, so that documents that have been previously reshaped
(e.g. <code><a href="#topic+corpus_reshape">corpus_reshape()</a></code> or segmented (e.g.
<code><a href="#topic+corpus_segment">corpus_segment()</a></code>) will be regrouped into their original <code>docnames</code> when
<code>groups = docid(x)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+featnames">featnames()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># get and set doument names to a corpus
corp &lt;- data_corpus_inaugural
docnames(corp) &lt;- char_tolower(docnames(corp))

# get and set doument names to a tokens
toks &lt;- tokens(data_corpus_inaugural)
docnames(toks) &lt;- char_tolower(docnames(toks))

# get and set doument names to a dfm
dfmat &lt;- dfm(data_corpus_inaugural[1:5])
docnames(dfmat) &lt;- char_tolower(docnames(dfmat))

# reassign the document names of the inaugural speech corpus
docnames(data_corpus_inaugural) &lt;- paste("Speech", 1:ndoc(data_corpus_inaugural), sep="")


corp &lt;- corpus(c(textone = "This is a sentence.  Another sentence.  Yet another.",
                 textwo = "Sentence 1. Sentence 2."))
corpsent &lt;- corp %&gt;%
    corpus_reshape(to = "sentences")
docnames(corpsent)

# docid
docid(corpsent)
docid(tokens(corpsent))
docid(dfm(tokens(corpsent)))

# segid
segid(corpsent)
segid(tokens(corpsent))
segid(dfm(tokens(corpsent)))
</code></pre>

<hr>
<h2 id='docvars'>Get or set document-level variables</h2><span id='topic+docvars'></span><span id='topic+docvars+3C-'></span><span id='topic++24.corpus'></span><span id='topic++24+3C-.corpus'></span><span id='topic++24.tokens'></span><span id='topic++24+3C-.tokens'></span><span id='topic++24.dfm'></span><span id='topic++24+3C-.dfm'></span>

<h3>Description</h3>

<p>Get or set variables associated with a document in a <a href="#topic+corpus">corpus</a>,
<a href="#topic+tokens">tokens</a> or <a href="#topic+dfm">dfm</a> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>docvars(x, field = NULL)

docvars(x, field = NULL) &lt;- value

## S3 method for class 'corpus'
x$name

## S3 replacement method for class 'corpus'
x$name &lt;- value

## S3 method for class 'tokens'
x$name

## S3 replacement method for class 'tokens'
x$name &lt;- value

## S3 method for class 'dfm'
x$name

## S3 replacement method for class 'dfm'
x$name &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="docvars_+3A_x">x</code></td>
<td>
<p><a href="#topic+corpus">corpus</a>, <a href="#topic+tokens">tokens</a>, or <a href="#topic+dfm">dfm</a> object whose
document-level variables will be read or set</p>
</td></tr>
<tr><td><code id="docvars_+3A_field">field</code></td>
<td>
<p>string containing the document-level variable name</p>
</td></tr>
<tr><td><code id="docvars_+3A_value">value</code></td>
<td>
<p>a vector of document variable values to be assigned to <code>name</code></p>
</td></tr>
<tr><td><code id="docvars_+3A_name">name</code></td>
<td>
<p>a literal character string specifying a single <a href="#topic+docvars">docvars</a> name</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>docvars</code> returns a data.frame of the document-level variables,
dropping the second dimension to form a vector if a single docvar is
returned.
</p>
<p><code style="white-space: pre;">&#8288;docvars&lt;-&#8288;</code> assigns <code>value</code> to the named <code>field</code>
</p>


<h3>Accessing or assigning docvars using the <code>$</code> operator</h3>

<p>As of <span class="pkg">quanteda</span> v2, it is possible to access and assign a docvar using
the <code>$</code> operator.  See Examples.
</p>


<h3>Note</h3>

<p>Reassigning document variables for a <a href="#topic+tokens">tokens</a> or <a href="#topic+dfm">dfm</a> object
is allowed, but discouraged.  A better, more reproducible workflow is to
create your docvars as desired in the <a href="#topic+corpus">corpus</a>, and let these continue
to be attached &quot;downstream&quot; after tokenization and forming a document-feature
matrix.  Recognizing that in some cases, you may need to modify or add
document variables to downstream objects, the assignment operator is defined
for <a href="#topic+tokens">tokens</a> or <a href="#topic+dfm">dfm</a> objects as well.  Use with caution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># retrieving docvars from a corpus
head(docvars(data_corpus_inaugural))
tail(docvars(data_corpus_inaugural, "President"), 10)
head(data_corpus_inaugural$President)

# assigning document variables to a corpus
corp &lt;- data_corpus_inaugural
docvars(corp, "President") &lt;- paste("prez", 1:ndoc(corp), sep = "")
head(docvars(corp))
corp$fullname &lt;- paste(data_corpus_inaugural$FirstName,
                       data_corpus_inaugural$President)
tail(corp$fullname)


# accessing or assigning docvars for a corpus using "$"
data_corpus_inaugural$Year
data_corpus_inaugural$century &lt;- floor(data_corpus_inaugural$Year / 100)
data_corpus_inaugural$century

# accessing or assigning docvars for tokens using "$"
toks &lt;- tokens(corpus_subset(data_corpus_inaugural, Year &lt;= 1805))
toks$Year
toks$Year &lt;- 1991:1995
toks$Year
toks$nonexistent &lt;- TRUE
docvars(toks)

# accessing or assigning docvars for a dfm using "$"
dfmat &lt;- dfm(toks)
dfmat$Year
dfmat$Year &lt;- 1991:1995
dfmat$Year
dfmat$nonexistent &lt;- TRUE
docvars(dfmat)
</code></pre>

<hr>
<h2 id='escape_regex'>Internal function for <code>select_types()</code> to escape regular expressions</h2><span id='topic+escape_regex'></span>

<h3>Description</h3>

<p>This function escapes glob patterns before <code>utils:glob2rx()</code>, therefore * and
? are unescaped.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>escape_regex(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="escape_regex_+3A_x">x</code></td>
<td>
<p>character vector to be escaped</p>
</td></tr>
</table>

<hr>
<h2 id='expand'>Simpler and faster version of expand.grid() in base package</h2><span id='topic+expand'></span>

<h3>Description</h3>

<p>Simpler and faster version of expand.grid() in base package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expand(elem)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expand_+3A_elem">elem</code></td>
<td>
<p>list of elements to be combined</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>quanteda:::expand(list(c("a", "b", "c"), c("x", "y")))
</code></pre>

<hr>
<h2 id='fcm'>Create a feature co-occurrence matrix</h2><span id='topic+fcm'></span><span id='topic+is.fcm'></span>

<h3>Description</h3>

<p>Create a sparse feature co-occurrence matrix, measuring co-occurrences of
features within a user-defined context. The context can be defined as a
document or a window within a collection of documents, with an optional
vector of weights applied to the co-occurrence counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fcm(
  x,
  context = c("document", "window"),
  count = c("frequency", "boolean", "weighted"),
  window = 5L,
  weights = NULL,
  ordered = FALSE,
  tri = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fcm_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+tokens">tokens</a>, or <a href="#topic+dfm">dfm</a> object from which to generate the feature
co-occurrence matrix</p>
</td></tr>
<tr><td><code id="fcm_+3A_context">context</code></td>
<td>
<p>the context in which to consider term co-occurrence:
<code>"document"</code> for co-occurrence counts within document; <code>"window"</code>
for co-occurrence within a defined window of words, which requires a
positive integer value for <code>window</code>.  Note: if <code>x</code> is a dfm
object, then <code>context</code> can only be <code>"document"</code>.</p>
</td></tr>
<tr><td><code id="fcm_+3A_count">count</code></td>
<td>
<p>how to count co-occurrences:
</p>

<dl>
<dt><code>"frequency"</code></dt><dd><p>count the number of co-occurrences within the
context</p>
</dd>
<dt><code>"boolean"</code></dt><dd><p>count only the co-occurrence or not within the
context, irrespective of how many times it occurs.</p>
</dd>
<dt><code>"weighted"</code></dt><dd><p>count a weighted function of counts, typically as
a function of distance from the target feature.  Only makes sense for
<code>context = "window"</code>.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="fcm_+3A_window">window</code></td>
<td>
<p>positive integer value for the size of a window on either side
of the target feature, default is 5, meaning 5 words before and after the
target feature</p>
</td></tr>
<tr><td><code id="fcm_+3A_weights">weights</code></td>
<td>
<p>a vector of weights applied to each distance from
<code>1:window</code>, strictly decreasing by default; can be a custom-defined
vector of the same length as <code>window</code></p>
</td></tr>
<tr><td><code id="fcm_+3A_ordered">ordered</code></td>
<td>
<p>if <code>TRUE</code>, count only the forward co-occurrences for each
target token for bigram models, so that the <code style="white-space: pre;">&#8288;i, j&#8288;</code> cell of the fcm is the
number of times that token <code>j</code> occurs before the target token <code>i</code> within
the window. Only makes sense for <code>context = "window"</code>, and when <code>ordered = TRUE</code>, the argument <code>tri</code> has no effect.</p>
</td></tr>
<tr><td><code id="fcm_+3A_tri">tri</code></td>
<td>
<p>if <code>TRUE</code> return only upper triangle (including diagonal).
Ignored if <code>ordered = TRUE</code>.</p>
</td></tr>
<tr><td><code id="fcm_+3A_...">...</code></td>
<td>
<p>not used here</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code><a href="#topic+fcm">fcm()</a></code> provides a very general
implementation of a &quot;context-feature&quot; matrix, consisting of a count of
feature co-occurrence within a defined context.  This context, following
Momtazi et. al. (2010), can be defined as the <em>document</em>,
<em>sentences</em> within documents, <em>syntactic relationships</em> between
features (nouns within a sentence, for instance), or according to a
<em>window</em>.  When the context is a window, a weighting function is
typically applied that is a function of distance from the target word (see
Jurafsky and Martin 2015, Ch. 16) and ordered co-occurrence of the two
features is considered (see Church &amp; Hanks 1990).
</p>
<p><a href="#topic+fcm">fcm</a> provides all of this functionality, returning a <code class="reqn">V * V</code>
matrix (where <code class="reqn">V</code> is the vocabulary size, returned by
<code><a href="#topic+nfeat">nfeat()</a></code>). The <code>tri = TRUE</code> option will only return the
upper part of the matrix.
</p>
<p>Unlike some implementations of co-occurrences, <a href="#topic+fcm">fcm</a> counts feature
co-occurrences with themselves, meaning that the diagonal will not be zero.
</p>
<p><a href="#topic+fcm">fcm</a> also provides &quot;boolean&quot; counting within the context of &quot;window&quot;,
which differs from the counting within &quot;document&quot;.
</p>
<p><code>is.fcm(x)</code> returns <code>TRUE</code> if and only if its x is an object of
type <a href="#topic+fcm">fcm</a>.
</p>


<h3>Author(s)</h3>

<p>Kenneth Benoit (R), Haiyan Wang (R, C++), Kohei Watanabe (C++)
</p>


<h3>References</h3>

<p>Momtazi, S., Khudanpur, S., &amp; Klakow, D. (2010). &quot;A comparative study of
word co-occurrence for term clustering in language model-based sentence
retrieval. <em>Human Language Technologies: The 2010 Annual Conference of the
North American Chapter of the ACL</em>, Los Angeles, California, June 2010,
325-328.  https://aclanthology.org/N10-1046/
</p>
<p>Jurafsky, D. &amp; Martin, J.H. (2018). From <em>Speech and Language Processing:
An Introduction to Natural Language Processing, Computational Linguistics,
and Speech Recognition</em>. Draft of September 23, 2018 (Chapter 6, Vector
Semantics). Available at <a href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a>.
</p>
<p>Church, K. W. &amp; P. Hanks (1990). <a href="https://dl.acm.org/doi/10.5555/89086.89095">Word association norms, mutual information, and lexicography</a>.
<em>Computational Linguistics</em>, 16(1), 22-29.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># see http://bit.ly/29b2zOA
toks1 &lt;- tokens(c("A D A C E A D F E B A C E D"))
fcm(toks1, context = "window", window = 2)
fcm(toks1, context = "window", count = "weighted", window = 3)
fcm(toks1, context = "window", count = "weighted", window = 3,
    weights = c(3, 2, 1), ordered = TRUE, tri = FALSE)

# with multiple documents
toks2 &lt;- tokens(c("a a a b b c", "a a c e", "a c e f g"))
fcm(toks2, context = "document", count = "frequency")
fcm(toks2, context = "document", count = "boolean")
fcm(toks2, context = "window", window = 2)

txt3 &lt;- c("The quick brown fox jumped over the lazy dog.",
         "The dog jumped and ate the fox.")
toks3 &lt;- tokens(char_tolower(txt3), remove_punct = TRUE)
fcm(toks3, context = "document")
fcm(toks3, context = "window", window = 3)
</code></pre>

<hr>
<h2 id='fcm_sort'>Sort an fcm in alphabetical order of the features</h2><span id='topic+fcm_sort'></span>

<h3>Description</h3>

<p>Sorts an <a href="#topic+fcm">fcm</a> in alphabetical order of the features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fcm_sort(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fcm_sort_+3A_x">x</code></td>
<td>
<p><a href="#topic+fcm">fcm</a> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+fcm">fcm</a> object whose features have been alphabetically sorted.
Differs from <code><a href="#topic+fcm_sort">fcm_sort()</a></code> in that this function sorts the fcm by
the feature labels, not the counts of the features.
</p>


<h3>Author(s)</h3>

<p>Kenneth Benoit
</p>


<h3>Examples</h3>

<pre><code class='language-R'># with tri = FALSE
fcmat1 &lt;- fcm(tokens(c("A X Y C B A", "X Y C A B B")), tri = FALSE)
rownames(fcmat1)[3] &lt;- colnames(fcmat1)[3] &lt;- "Z"
fcmat1
fcm_sort(fcmat1)

# with tri = TRUE
fcmat2 &lt;- fcm(tokens(c("A X Y C B A", "X Y C A B B")), tri = TRUE)
rownames(fcmat2)[3] &lt;- colnames(fcmat2)[3] &lt;- "Z"
fcmat2
fcm_sort(fcmat2)
</code></pre>

<hr>
<h2 id='fcm-class'>Virtual class &quot;fcm&quot; for a feature co-occurrence matrix</h2><span id='topic+fcm-class'></span><span id='topic+t+2Cfcm-method'></span><span id='topic+Arith+2Cfcm+2Cnumeric-method'></span><span id='topic+Arith+2Cnumeric+2Cfcm-method'></span><span id='topic++5B+2Cfcm+2Cindex+2Cindex+2Cmissing-method'></span><span id='topic++5B+2Cfcm+2Cindex+2Cindex+2Clogical-method'></span><span id='topic++5B+2Cfcm+2Cmissing+2Cmissing+2Cmissing-method'></span><span id='topic++5B+2Cfcm+2Cmissing+2Cmissing+2Clogical-method'></span><span id='topic++5B+2Cfcm+2Cindex+2Cmissing+2Cmissing-method'></span><span id='topic++5B+2Cfcm+2Cindex+2Cmissing+2Clogical-method'></span><span id='topic++5B+2Cfcm+2Cmissing+2Cindex+2Cmissing-method'></span><span id='topic++5B+2Cfcm+2Cmissing+2Cindex+2Clogical-method'></span>

<h3>Description</h3>

<p>The fcm class of object is a special type of <a href="#topic+fcm">fcm</a> object with
additional slots, described below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'fcm'
t(x)

## S4 method for signature 'fcm,numeric'
Arith(e1, e2)

## S4 method for signature 'numeric,fcm'
Arith(e1, e2)

## S4 method for signature 'fcm,index,index,missing'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'fcm,index,index,logical'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'fcm,missing,missing,missing'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'fcm,missing,missing,logical'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'fcm,index,missing,missing'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'fcm,index,missing,logical'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'fcm,missing,index,missing'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'fcm,missing,index,logical'
x[i, j, ..., drop = TRUE]
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fcm-class_+3A_x">x</code></td>
<td>
<p>the fcm object</p>
</td></tr>
<tr><td><code id="fcm-class_+3A_e1">e1</code></td>
<td>
<p>first quantity in &quot;+&quot; operation for fcm</p>
</td></tr>
<tr><td><code id="fcm-class_+3A_e2">e2</code></td>
<td>
<p>second quantity in &quot;+&quot; operation for fcm</p>
</td></tr>
<tr><td><code id="fcm-class_+3A_i">i</code></td>
<td>
<p>index for features</p>
</td></tr>
<tr><td><code id="fcm-class_+3A_j">j</code></td>
<td>
<p>index for features</p>
</td></tr>
<tr><td><code id="fcm-class_+3A_...">...</code></td>
<td>
<p>additional arguments not used here</p>
</td></tr>
<tr><td><code id="fcm-class_+3A_drop">drop</code></td>
<td>
<p>always set to <code>FALSE</code></p>
</td></tr>
</table>


<h3>Slots</h3>


<dl>
<dt><code>context</code></dt><dd><p>the context definition</p>
</dd>
<dt><code>window</code></dt><dd><p>the size of the window, if <code>context = "window"</code></p>
</dd>
<dt><code>count</code></dt><dd><p>how co-occurrences are counted</p>
</dd>
<dt><code>weights</code></dt><dd><p>context weighting for distance from target feature, equal in length to <code>window</code></p>
</dd>
<dt><code>margin</code></dt><dd><p>frequencies of features in the original <a href="#topic+dfm">dfm</a> or <a href="#topic+tokens">tokens</a></p>
</dd>
<dt><code>tri</code></dt><dd><p>whether the lower triangle of the symmetric <code class="reqn">V \times V</code> matrix is recorded</p>
</dd>
<dt><code>ordered</code></dt><dd><p>whether a term appears before or after the target feature
are counted separately</p>
</dd>
</dl>


<h3>See Also</h3>

<p><a href="#topic+fcm">fcm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># fcm subsetting
fcmat &lt;- fcm(tokens(c("this contains lots of stopwords",
                  "no if, and, or but about it: lots"),
                remove_punct = TRUE))
fcmat[1:3, ]
fcmat[4:5, 1:5]


</code></pre>

<hr>
<h2 id='featfreq'>Compute the frequencies of features</h2><span id='topic+featfreq'></span>

<h3>Description</h3>

<p>For a <a href="#topic+dfm">dfm</a> object, returns a frequency for each feature, computed
across all documents in the dfm. This is equivalent to <code>colSums(x)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>featfreq(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="featfreq_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+dfm">dfm</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a (named) numeric vector of feature frequencies
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfm_tfidf">dfm_tfidf()</a></code>, <code><a href="#topic+dfm_weight">dfm_weight()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat &lt;- dfm(data_char_sampletext)
featfreq(dfmat)
</code></pre>

<hr>
<h2 id='featnames'>Get the feature labels from a dfm</h2><span id='topic+featnames'></span>

<h3>Description</h3>

<p>Get the features from a document-feature matrix, which are stored as the
column names of the <a href="#topic+dfm">dfm</a> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>featnames(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="featnames_+3A_x">x</code></td>
<td>
<p>the dfm whose features will be extracted</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character vector of the feature labels
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat &lt;- dfm(data_corpus_inaugural)

# first 50 features (in original text order)
head(featnames(dfmat), 50)

# first 50 features alphabetically
head(sort(featnames(dfmat)), 50)

# contrast with descending total frequency order from topfeatures()
names(topfeatures(dfmat, 50))
</code></pre>

<hr>
<h2 id='field_system'>Shortcut functions to access or assign metadata</h2><span id='topic+field_system'></span><span id='topic+field_system+3C-'></span><span id='topic+field_object'></span><span id='topic+field_object+3C-'></span><span id='topic+field_user'></span><span id='topic+field_user+3C-'></span>

<h3>Description</h3>

<p>Internal functions to access or replace an object metadata field without
going through attribute trees. <code>field_system()</code>, <code>field_object()</code> and
<code>field_user()</code> correspond to the system, object and user meta fields,
respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>field_system(x, field = NULL)

field_system(x, field = NULL) &lt;- value

field_object(x, field = NULL)

field_object(x, field = NULL) &lt;- value

field_user(x, field = NULL)

field_user(x, field = NULL) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="field_system_+3A_x">x</code></td>
<td>
<p>a list of attributes extracted from a <code>dfm</code>, <code>tokens</code>, or <code>corpus</code>
object by <code>attributes(x)</code></p>
</td></tr>
<tr><td><code id="field_system_+3A_field">field</code></td>
<td>
<p>name of the sub-field to access or assign values</p>
</td></tr>
</table>

<hr>
<h2 id='flatten_dictionary'>Flatten a hierarchical dictionary into a list of character vectors</h2><span id='topic+flatten_dictionary'></span>

<h3>Description</h3>

<p>Converts a hierarchical dictionary (a named list of named lists, ending in
character vectors at the lowest level) into a flat list of character
vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flatten_dictionary(dictionary, levels = 1:100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flatten_dictionary_+3A_dictionary">dictionary</code></td>
<td>
<p>a <a href="#topic+dictionary">dictionary</a>-class object to be flattened</p>
</td></tr>
<tr><td><code id="flatten_dictionary_+3A_levels">levels</code></td>
<td>
<p>an integer vector indicating levels in the dictionary</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of character vectors
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dict1 &lt;- dictionary(
    list(populism=c("elit*", "consensus*", "undemocratic*", "referend*",
                    "corrupt*", "propagand", "politici*", "*deceit*",
                    "*deceiv*", "*betray*", "shame*", "scandal*", "truth*",
                    "dishonest*", "establishm*", "ruling*"))
     )
flatten_dictionary(dict1)

dict2 &lt;- dictionary(
    list(level1a = list(level1a1 = c("l1a11", "l1a12"),
         level1a2 = c("l1a21", "l1a22")),
         level1b = list(level1b1 = c("l1b11", "l1b12"),
         level1b2 = c("l1b21", "l1b22", "l1b23")),
         level1c = list(level1c1a = list(level1c1a1 = c("lowest1", "lowest2")),
         level1c1b = list(level1c1b1 = c("lowestalone"))))
     )
flatten_dictionary(dict2)
flatten_dictionary(dict2, 2)
flatten_dictionary(dict2, 1:2)

</code></pre>

<hr>
<h2 id='flatten_list'>Internal function to flatten a nested list</h2><span id='topic+flatten_list'></span>

<h3>Description</h3>

<p>Internal function to flatten a nested list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flatten_list(
  lis,
  levels = 1:100,
  level = 1,
  key_parent = "",
  lis_flat = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flatten_list_+3A_lis">lis</code></td>
<td>
<p>a nested list</p>
</td></tr>
<tr><td><code id="flatten_list_+3A_levels">levels</code></td>
<td>
<p>an integer vector indicating levels in the list</p>
</td></tr>
<tr><td><code id="flatten_list_+3A_level">level</code></td>
<td>
<p>an internal argument to pass current levels</p>
</td></tr>
<tr><td><code id="flatten_list_+3A_key_parent">key_parent</code></td>
<td>
<p>an internal argument to pass for parent keys</p>
</td></tr>
<tr><td><code id="flatten_list_+3A_lis_flat">lis_flat</code></td>
<td>
<p>an internal argument to pass the flattened list</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>lis &lt;- list("A" = list("B" = c("b", "B"), c("a", "A", "aa")))
quanteda:::flatten_list(lis, 1:2)
quanteda:::flatten_list(lis, 1)
</code></pre>

<hr>
<h2 id='format_sparsity'>format a sparsity value for printing</h2><span id='topic+format_sparsity'></span>

<h3>Description</h3>

<p>Inputs a dfm sparsity value from <code><a href="#topic+sparsity">sparsity()</a></code> and formats it for
printing in <code>print.dfm()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_sparsity(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="format_sparsity_+3A_x">x</code></td>
<td>
<p>input sparsity value, ranging from 0 to 1.0</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>ss &lt;- c(1, .99999, .9999, .999, .99, .9,
       .1, .01, .001, .0001, .000001, .0000001, .00000001, .000000000001, 0)
for (s in ss) 
    cat(format(s, width = 10),  ":", quanteda:::format_sparsity(s), "\n")
</code></pre>

<hr>
<h2 id='get_docvars'>Internal function to extract docvars</h2><span id='topic+get_docvars'></span>

<h3>Description</h3>

<p>Internal function to extract docvars
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_docvars(x, field = NULL, user = TRUE, system = FALSE, drop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_docvars_+3A_x">x</code></td>
<td>
<p>an object from which docvars are extracted</p>
</td></tr>
<tr><td><code id="get_docvars_+3A_field">field</code></td>
<td>
<p>name of docvar fields</p>
</td></tr>
<tr><td><code id="get_docvars_+3A_user">user</code></td>
<td>
<p>if <code>TRUE</code>, return user variables</p>
</td></tr>
<tr><td><code id="get_docvars_+3A_system">system</code></td>
<td>
<p>if <code>TRUE</code>, return system variables</p>
</td></tr>
<tr><td><code id="get_docvars_+3A_drop">drop</code></td>
<td>
<p>if <code>TRUE</code>, convert data.frame with one variable to a vector</p>
</td></tr>
</table>

<hr>
<h2 id='get_object_version'>Get the package version that created an object</h2><span id='topic+get_object_version'></span><span id='topic+is_pre2'></span>

<h3>Description</h3>

<p>Return the the <span class="pkg">quanteda</span> package version in which a <a href="#topic+dfm">dfm</a>,
<a href="#topic+tokens">tokens</a>, or <a href="#topic+corpus">corpus</a> object was created.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_object_version(x)

is_pre2(x)
</code></pre>


<h3>Value</h3>

<p>A three-element integer vector of class &quot;package_version&quot;. For
versions of the package &lt; 1.5 for which no version was recorded in the
object, <code>c(1, 4, 0)</code> is returned.
</p>
<p><code>ispre2()</code> returns <code>TRUE</code> if the object was created before
<span class="pkg">quanteda</span> version 2, or <code>FALSE</code> otherwise
</p>

<hr>
<h2 id='groups'>Grouping variable(s) for various functions</h2><span id='topic+groups'></span>

<h3>Description</h3>

<p>Groups for aggregation by various functions that take grouping options.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="groups_+3A_groups">groups</code></td>
<td>
<p>grouping variable for sampling, equal in length to the number
of documents. This will be evaluated in the docvars data.frame, so that
docvars may be referred to by name without quoting. This also changes
previous behaviours for <code>groups</code>. See <code>news(Version &gt;= "3.0", package = "quanteda")</code> for details.</p>
</td></tr>
<tr><td><code id="groups_+3A_fill">fill</code></td>
<td>
<p>logical; if <code>TRUE</code> and <code>groups</code> is a factor, then use all levels
of the factor when forming the new documents of the grouped object.  This
will result in a new &quot;document&quot; with empty content for levels not observed,
but for which an empty document may be needed.  If <code>groups</code> is a factor of
dates, for instance, then <code>fill = TRUE</code> ensures that the new object will
consist of one new &quot;document&quot; by date, regardless of whether any documents
previously existed with that date.  Has no effect if the <code>groups</code>
variable(s) are not factors.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+corpus_group">corpus_group()</a></code>, <code><a href="#topic+tokens_group">tokens_group()</a></code>, <code><a href="#topic+dfm_group">dfm_group()</a></code>
</p>

<hr>
<h2 id='head.dfm'>Return the first or last part of a dfm</h2><span id='topic+head.dfm'></span><span id='topic+tail.dfm'></span>

<h3>Description</h3>

<p>For a <a href="#topic+dfm">dfm</a> object, return the dfm with only the first or last <code>n</code> documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dfm'
head(x, n = 6L, ...)

## S3 method for class 'dfm'
tail(x, n = 6L, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="head.dfm_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+dfm">dfm</a> object</p>
</td></tr>
<tr><td><code id="head.dfm_+3A_n">n</code></td>
<td>
<p>an integer vector of length up to <code>dim(x)</code> (or 1,
for non-dimensioned objects).  A <code>logical</code> is silently coerced to
integer.  Values specify the indices to be
selected in the corresponding dimension (or along the length) of the
object. A positive value of <code>n[i]</code> includes the first/last
<code>n[i]</code> indices in that dimension, while a negative value
excludes the last/first <code>abs(n[i])</code>, including all remaining
indices. <code>NA</code> or non-specified values (when <code>length(n) &lt;
      length(dim(x))</code>) select all indices in that dimension. Must
contain at least one non-missing value.</p>
</td></tr>
<tr><td><code id="head.dfm_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+dfm">dfm</a> class object corresponding to the subset of documents
determined by by <code>n</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>head(data_dfm_lbgexample, 3)
head(data_dfm_lbgexample, -4)

tail(data_dfm_lbgexample)
tail(data_dfm_lbgexample, n = 3)
</code></pre>

<hr>
<h2 id='index'>Locate a pattern in a tokens object</h2><span id='topic+index'></span><span id='topic+is.index'></span>

<h3>Description</h3>

<p>Locates a <a href="#topic+pattern">pattern</a> within a tokens object, returning the index positions of
the beginning and ending tokens in the pattern.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>index(
  x,
  pattern,
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE
)

is.index(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="index_+3A_x">x</code></td>
<td>
<p>an input <a href="#topic+tokens">tokens</a> object</p>
</td></tr>
<tr><td><code id="index_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="#topic+pattern">pattern</a> for details.</p>
</td></tr>
<tr><td><code id="index_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="index_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame consisting of one row per pattern match, with columns
for the document name, index positions <code>from</code> and <code>to</code>, and the pattern
matched.
</p>
<p><code>is.index</code> returns <code>TRUE</code> if the object was created by
<code><a href="#topic+index">index()</a></code>; <code>FALSE</code> otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>toks &lt;- tokens(data_corpus_inaugural[1:8])
index(toks, pattern = "secure*")
index(toks, pattern = c("secure*", phrase("united states"))) %&gt;% head()
</code></pre>

<hr>
<h2 id='is_glob'>Check if patterns contains glob wildcard</h2><span id='topic+is_glob'></span>

<h3>Description</h3>

<p>Check if patterns contains glob wildcard
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_glob(pattern)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_glob_+3A_pattern">pattern</code></td>
<td>
<p>a glob pattern to be tested</p>
</td></tr>
</table>

<hr>
<h2 id='is_indexed'>Check if a glob pattern is indexed by index_types</h2><span id='topic+is_indexed'></span>

<h3>Description</h3>

<p>Internal function for <code>select_types</code> to check if a glob pattern is indexed by
<code>index_types</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_indexed(pattern)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_indexed_+3A_pattern">pattern</code></td>
<td>
<p>a glob pattern to be tested</p>
</td></tr>
</table>

<hr>
<h2 id='is_regex'>Check if a string is a regular expression</h2><span id='topic+is_regex'></span>

<h3>Description</h3>

<p>Internal function for <code>select_types()</code> to check if a string is a regular expression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_regex(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_regex_+3A_x">x</code></td>
<td>
<p>a character string to be tested</p>
</td></tr>
</table>

<hr>
<h2 id='is.collocations'>Check if an object is collocations</h2><span id='topic+is.collocations'></span>

<h3>Description</h3>

<p>Function to check if an object is a collocations object, created by
<code><a href="quanteda.textstats.html#topic+textstat_collocations">quanteda.textstats::textstat_collocations()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.collocations(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.collocations_+3A_x">x</code></td>
<td>
<p>object to be checked</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object is of class <code>collocations</code>, <code>FALSE</code> otherwise
</p>

<hr>
<h2 id='kwic'>Locate keywords-in-context</h2><span id='topic+kwic'></span><span id='topic+is.kwic'></span><span id='topic+as.data.frame.kwic'></span>

<h3>Description</h3>

<p>For a text or a collection of texts (in a quanteda corpus object), return a
list of a keyword supplied by the user in its immediate context, identifying
the source text and the word index number within the source text.  (Not the
line number, since the text may or may not be segmented using end-of-line
delimiters.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kwic(
  x,
  pattern,
  window = 5,
  valuetype = c("glob", "regex", "fixed"),
  separator = " ",
  case_insensitive = TRUE,
  index = NULL,
  ...
)

is.kwic(x)

## S3 method for class 'kwic'
as.data.frame(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kwic_+3A_x">x</code></td>
<td>
<p>a character, <a href="#topic+corpus">corpus</a>, or <a href="#topic+tokens">tokens</a> object</p>
</td></tr>
<tr><td><code id="kwic_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="#topic+pattern">pattern</a> for details.</p>
</td></tr>
<tr><td><code id="kwic_+3A_window">window</code></td>
<td>
<p>the number of context words to be displayed around the keyword</p>
</td></tr>
<tr><td><code id="kwic_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="kwic_+3A_separator">separator</code></td>
<td>
<p>a character to separate words in the output</p>
</td></tr>
<tr><td><code id="kwic_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="kwic_+3A_index">index</code></td>
<td>
<p>an <a href="#topic+index">index</a> object to specify keywords</p>
</td></tr>
<tr><td><code id="kwic_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>kwic</code> classed data.frame, with the document name
(<code>docname</code>) and the token index positions (<code>from</code> and <code>to</code>,
which will be the same for single-word patterns, or a sequence equal in
length to the number of elements for multi-word phrases).
</p>


<h3>Note</h3>

<p><code>pattern</code> will be a keyword pattern or phrase, possibly multiple
patterns, that may include punctuation.  If a pattern contains whitespace,
it is best to wrap it in <code><a href="#topic+phrase">phrase()</a></code> to make this explicit. However if
<code>pattern</code> is a <a href="quanteda.textstats.html#topic+textstat_collocations">collocations</a> or
<a href="#topic+dictionary">dictionary</a> object, then the collocations or multi-word dictionary keys
will automatically be considered phrases where each whitespace-separated
element matches a token in sequence.
</p>


<h3>See Also</h3>

<p><a href="#topic+print-methods">print-methods</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># single token matching
toks &lt;- tokens(data_corpus_inaugural[1:8])
kwic(toks, pattern = "secure*", valuetype = "glob", window = 3)
kwic(toks, pattern = "secur", valuetype = "regex", window = 3)
kwic(toks, pattern = "security", valuetype = "fixed", window = 3)

# phrase matching
kwic(toks, pattern = phrase("secur* against"), window = 2)
kwic(toks, pattern = phrase("war against"), valuetype = "regex", window = 2)

# use index
idx &lt;- index(toks, phrase("secur* against"))
kwic(toks, index = idx, window = 2)
kw &lt;- kwic(tokens(data_corpus_inaugural[1:20]), "provident*")
is.kwic(kw)
is.kwic("Not a kwic")
is.kwic(kw[, c("pre", "post")])

toks &lt;- tokens(data_corpus_inaugural[1:8])
kw &lt;- kwic(toks, pattern = "secure*", valuetype = "glob", window = 3)
as.data.frame(kw)

</code></pre>

<hr>
<h2 id='list2dictionary'>Internal function to convert a list to a dictionary</h2><span id='topic+list2dictionary'></span>

<h3>Description</h3>

<p>A dictionary is internally a list of list to keys and values to coexist in
the same level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list2dictionary(dict)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="list2dictionary_+3A_dict">dict</code></td>
<td>
<p>list of object</p>
</td></tr>
</table>

<hr>
<h2 id='lowercase_dictionary_values'>Internal function to lowercase dictionary values</h2><span id='topic+lowercase_dictionary_values'></span>

<h3>Description</h3>

<p>Internal function to lowercase dictionary values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lowercase_dictionary_values(dict)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lowercase_dictionary_values_+3A_dict">dict</code></td>
<td>
<p>the dictionary whose values will be lowercased</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>dict &lt;- list(KEY1 = list(SUBKEY1 = c("A", "B"),
                          SUBKEY2 = c("C", "D")),
              KEY2 = list(SUBKEY3 = c("E", "F"),
                          SUBKEY4 = c("G", "F", "I")),
              KEY3 = list(SUBKEY5 = list(SUBKEY7 = c("J", "K")),
                          SUBKEY6 = list(SUBKEY8 = c("L"))))
quanteda:::lowercase_dictionary_values(dict)
</code></pre>

<hr>
<h2 id='make_docvars'>Internal function to make new system-level docvars</h2><span id='topic+make_docvars'></span>

<h3>Description</h3>

<p>Internal function to make new system-level docvars
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_docvars(n, docname = NULL, unique = TRUE, drop_docid = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_docvars_+3A_n">n</code></td>
<td>
<p>the number of documents</p>
</td></tr>
<tr><td><code id="make_docvars_+3A_docname">docname</code></td>
<td>
<p>a character vector for the names of documents. Must be the
same length as <code>n</code> or <code>NULL</code>. If NULL, names are generated automatically.</p>
</td></tr>
<tr><td><code id="make_docvars_+3A_unique">unique</code></td>
<td>
<p>if <code>TRUE</code>, names must be all unique. If <code>FALSE</code>, documents with the same
names are treated as segments from the same document and given serial number.</p>
</td></tr>
<tr><td><code id="make_docvars_+3A_drop_docid">drop_docid</code></td>
<td>
<p>if <code>TRUE</code>, drop unused names of documents.</p>
</td></tr>
</table>

<hr>
<h2 id='make_meta'>Internal functions to create a list of the meta fields</h2><span id='topic+make_meta'></span><span id='topic+make_meta_system'></span><span id='topic+make_meta_corpus'></span><span id='topic+make_meta_tokens'></span><span id='topic+make_meta_dfm'></span><span id='topic+make_meta_fcm'></span><span id='topic+make_meta_dictionary2'></span><span id='topic+update_meta'></span>

<h3>Description</h3>

<p>Internal functions to create a list of the meta fields
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_meta(class, inherit = NULL, ...)

make_meta_system(inherit = NULL)

make_meta_corpus(inherit = NULL, ...)

make_meta_tokens(inherit = NULL, ...)

make_meta_dfm(inherit = NULL, ...)

make_meta_fcm(inherit = NULL, ...)

make_meta_dictionary2(inherit = NULL, ...)

update_meta(default, inherit, ..., warn = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_meta_+3A_class">class</code></td>
<td>
<p>object class either <code>dfm</code>, <code>tokens</code> or <code>corpus</code></p>
</td></tr>
<tr><td><code id="make_meta_+3A_inherit">inherit</code></td>
<td>
<p>list from the meta attribute</p>
</td></tr>
<tr><td><code id="make_meta_+3A_...">...</code></td>
<td>
<p>values assigned to the object meta fields</p>
</td></tr>
<tr><td><code id="make_meta_+3A_default">default</code></td>
<td>
<p>default values for the meta attribute</p>
</td></tr>
</table>

<hr>
<h2 id='matrix2dfm'>Converts a Matrix to a dfm</h2><span id='topic+matrix2dfm'></span>

<h3>Description</h3>

<p>Converts a Matrix to a dfm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrix2dfm(x, docvars = NULL, meta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matrix2dfm_+3A_x">x</code></td>
<td>
<p>a Matrix</p>
</td></tr>
<tr><td><code id="matrix2dfm_+3A_meta">meta</code></td>
<td>
<p>a list of values to be assigned to slots</p>
</td></tr>
</table>

<hr>
<h2 id='matrix2fcm'>Converts a Matrix to a fcm</h2><span id='topic+matrix2fcm'></span>

<h3>Description</h3>

<p>Converts a Matrix to a fcm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrix2fcm(x, meta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matrix2fcm_+3A_x">x</code></td>
<td>
<p>a Matrix</p>
</td></tr>
<tr><td><code id="matrix2fcm_+3A_slots">slots</code></td>
<td>
<p>slots a list of values to be assigned to slots</p>
</td></tr>
</table>

<hr>
<h2 id='merge_dictionary_values'>Internal function to merge values of duplicated keys</h2><span id='topic+merge_dictionary_values'></span>

<h3>Description</h3>

<p>Internal function to merge values of duplicated keys
</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_dictionary_values(dict)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge_dictionary_values_+3A_dict">dict</code></td>
<td>
<p>a dictionary object</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>dict &lt;- list("A" = list(AA = list("aaaaa"), "a"),
             "B" = list("b"),
             "C" = list("c"),
             "A" = list("aa"))
quanteda:::merge_dictionary_values(dict)
</code></pre>

<hr>
<h2 id='message_error'>Return an error message</h2><span id='topic+message_error'></span>

<h3>Description</h3>

<p>Return an error message
</p>


<h3>Usage</h3>

<pre><code class='language-R'>message_error(key = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="message_error_+3A_key">key</code></td>
<td>
<p>type of error message</p>
</td></tr>
</table>

<hr>
<h2 id='meta'>Get or set object metadata</h2><span id='topic+meta'></span><span id='topic+meta+3C-'></span>

<h3>Description</h3>

<p>Get or set the object metadata in a <a href="#topic+corpus">corpus</a>, <a href="#topic+tokens">tokens</a>, <a href="#topic+dfm">dfm</a>, or
<a href="#topic+dictionary">dictionary</a> object. With the exception of dictionaries, this will be
corpus-level metadata.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meta(x, field = NULL, type = c("user", "object", "system", "all"))

meta(x, field = NULL) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="meta_+3A_x">x</code></td>
<td>
<p>an object for which the metadata will be read or set</p>
</td></tr>
<tr><td><code id="meta_+3A_field">field</code></td>
<td>
<p>metadata field name(s); if <code>NULL</code> (default), return all
metadata names</p>
</td></tr>
<tr><td><code id="meta_+3A_type">type</code></td>
<td>
<p><code>"user"</code> for user-provided corpus-level metadata;
<code>"system"</code> for metadata set automatically when the corpus is created;
or <code>"all"</code> for all metadata.</p>
</td></tr>
<tr><td><code id="meta_+3A_value">value</code></td>
<td>
<p>new value of the metadata field</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>meta</code>, a named list of the metadata fields in the corpus.
</p>
<p>For <code style="white-space: pre;">&#8288;meta &lt;-&#8288;</code>, the corpus with the updated user-level metadata.  Only
user-level metadata may be assigned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>meta(data_corpus_inaugural)
meta(data_corpus_inaugural, "source")
meta(data_corpus_inaugural, "citation") &lt;- "Presidential Speeches Online Project (2014)."
meta(data_corpus_inaugural, "citation")
</code></pre>

<hr>
<h2 id='meta_system'>Internal function to get, set or initialize system metadata</h2><span id='topic+meta_system'></span><span id='topic+meta_system+3C-'></span><span id='topic+meta_system+3C-.corpus'></span><span id='topic+meta_system+3C-.tokens'></span><span id='topic+meta_system+3C-.dfm'></span><span id='topic+meta_system+3C-.dictionary'></span><span id='topic+meta_system_defaults'></span>

<h3>Description</h3>

<p>Sets or initializes system metadata for new objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meta_system(x, field = NULL)

meta_system(x, field = NULL) &lt;- value

## S3 replacement method for class 'corpus'
meta_system(x, field = NULL) &lt;- value

## S3 replacement method for class 'tokens'
meta_system(x, field = NULL) &lt;- value

## S3 replacement method for class 'dfm'
meta_system(x, field = NULL) &lt;- value

## S3 replacement method for class 'dictionary'
meta_system(x, field = NULL) &lt;- value

meta_system_defaults()
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="meta_system_+3A_x">x</code></td>
<td>
<p>an object for which the metadata will be read or set</p>
</td></tr>
<tr><td><code id="meta_system_+3A_field">field</code></td>
<td>
<p>metadata field name(s); if <code>NULL</code> (default), return all
metadata names</p>
</td></tr>
<tr><td><code id="meta_system_+3A_value">value</code></td>
<td>
<p>new value of the metadata field</p>
</td></tr>
<tr><td><code id="meta_system_+3A_source">source</code></td>
<td>
<p>character; the input object class</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>meta_system</code> returns a list with the object's system metadata.
It is literally a wrapper to <code><a href="#topic+meta">meta(x, field, type = &quot;system&quot;)()</a></code>.
</p>
<p><code style="white-space: pre;">&#8288;meta_system&lt;-&#8288;</code> returns the object with the system metadata
modified. This is an internal function and not designed for users!
</p>
<p><code>meta_system_defaults</code> returns a list of default system
values, with the user setting the &quot;source&quot; value.  This should be used
to set initial system meta information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corp &lt;- corpus(c(d1 = "one two three", d2 = "two three four"))
# quanteda:::`meta_system&lt;-`(corp, value = quanteda:::meta_system_defaults("example"))
quanteda:::meta_system(corp)
</code></pre>

<hr>
<h2 id='msg'>Conditionally format messages</h2><span id='topic+msg'></span>

<h3>Description</h3>

<p>Conditionally format messages
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msg(x, values = NULL, indices = NULL, pretty = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msg_+3A_x">x</code></td>
<td>
<p>message template to be passed to <code>stri_sprintf()</code>.</p>
</td></tr>
<tr><td><code id="msg_+3A_values">values</code></td>
<td>
<p>list of values to be used in the template. Coerced to list if vector is given.</p>
</td></tr>
<tr><td><code id="msg_+3A_indices">indices</code></td>
<td>
<p>list of integer to specify which value to be used.</p>
</td></tr>
<tr><td><code id="msg_+3A_pretty">pretty</code></td>
<td>
<p>if <code>TRUE</code>, message is passed to <code><a href="base.html#topic+prettyNum">prettyNum()</a></code>.</p>
</td></tr>
<tr><td><code id="msg_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="base.html#topic+prettyNum">prettyNum()</a></code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
quanteda:::msg("you cannot delete %s", 
               c("a document", "documents"), indices = TRUE)
quanteda:::msg("tokens has %s", 
               c("sentences", "paragraphs", "documents"), indices = 2)

dfmat &lt;- data_dfm_lbgexample
quanteda:::msg("dfm has %d %s and %d %s", 
     list(ndoc(dfmat), c("document", "documents"),
          nfeat(dfmat), c("feature", "features")), 
     list(1, ndoc(dfmat) &gt; 1, 
          1, nfeat(dfmat) &gt; 1))

## End(Not run)      
</code></pre>

<hr>
<h2 id='names-quanteda'>Special handling for names of quanteda objects</h2><span id='topic+names-quanteda'></span><span id='topic+names+3C-.corpus'></span><span id='topic+names+3C-.tokens'></span><span id='topic+rownames+3C-+2Cdfm-method'></span><span id='topic+rownames+3C-.dfm'></span><span id='topic+rownames+3C-+2Cfcm-method'></span><span id='topic+rownames+3C-.fcm'></span>

<h3>Description</h3>

<p>Keeps the element names and rownames in sync with the system docvar
<code>docname_</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 replacement method for class 'corpus'
names(x) &lt;- value

## S3 replacement method for class 'tokens'
names(x) &lt;- value

## S4 replacement method for signature 'dfm'
rownames(x) &lt;- value

## S4 replacement method for signature 'fcm'
rownames(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="names-quanteda_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="names-quanteda_+3A_value">value</code></td>
<td>
<p>a character vector of up to the same length as <code>x</code>, or
<code>NULL</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='ndoc'>Count the number of documents or features</h2><span id='topic+ndoc'></span><span id='topic+nfeat'></span>

<h3>Description</h3>

<p>Get the number of documents or features in an object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ndoc(x)

nfeat(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ndoc_+3A_x">x</code></td>
<td>
<p>a <span class="pkg">quanteda</span> object: a <a href="#topic+corpus">corpus</a>, <a href="#topic+dfm">dfm</a>, or
<a href="#topic+tokens">tokens</a> object, or a readtext object from the <span class="pkg">readtext</span> package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ndoc</code> returns the number of documents in an object
whose texts are organized as &quot;documents&quot; (a <a href="#topic+corpus">corpus</a>,
<a href="#topic+dfm">dfm</a>, or <a href="#topic+tokens">tokens</a> object, a readtext object from the
<span class="pkg">readtext</span> package).
</p>
<p><code>nfeat</code> returns the number of features from a dfm; it is an
alias for <code>ntype</code> when applied to dfm objects.  This function is only
defined for <a href="#topic+dfm">dfm</a> objects because only these have &quot;features&quot;.  (To count
tokens, see <code><a href="#topic+ntoken">ntoken()</a></code>.)
</p>


<h3>Value</h3>

<p>an integer (count) of the number of documents or features
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ntoken">ntoken()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># number of documents
ndoc(data_corpus_inaugural)
ndoc(corpus_subset(data_corpus_inaugural, Year &gt; 1980))
ndoc(tokens(data_corpus_inaugural))
ndoc(dfm(tokens(corpus_subset(data_corpus_inaugural, Year &gt; 1980))))

# number of features
toks1 &lt;- tokens(corpus_subset(data_corpus_inaugural, Year &gt; 1980), remove_punct = FALSE)
toks2 &lt;- tokens(corpus_subset(data_corpus_inaugural, Year &gt; 1980), remove_punct = TRUE)
nfeat(dfm(toks1))
nfeat(dfm(toks2))
</code></pre>

<hr>
<h2 id='nest_dictionary'>Utility function to generate a nested list</h2><span id='topic+nest_dictionary'></span>

<h3>Description</h3>

<p>Utility function to generate a nested list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nest_dictionary(dict, depth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nest_dictionary_+3A_dict">dict</code></td>
<td>
<p>a flat dictionary</p>
</td></tr>
<tr><td><code id="nest_dictionary_+3A_depth">depth</code></td>
<td>
<p>depths of nested element</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>lis &lt;- list("A" = c("a", "aa", "aaa"), "B" = c("b", "bb"), "C" = c("c", "cc"), "D" = c("ddd"))
dict &lt;- quanteda:::list2dictionary(lis)
quanteda:::nest_dictionary(dict, c(1, 1, 2, 2))
quanteda:::nest_dictionary(dict, c(1, 2, 1, 2))

</code></pre>

<hr>
<h2 id='nsentence'>Count the number of sentences</h2><span id='topic+nsentence'></span>

<h3>Description</h3>

<p>Return the count of sentences in a corpus or character object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nsentence(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nsentence_+3A_x">x</code></td>
<td>
<p>a character or <a href="#topic+corpus">corpus</a> whose sentences will be counted</p>
</td></tr>
</table>


<h3>Value</h3>

<p>count(s) of the total sentences per text
</p>


<h3>Note</h3>

<p><code>nsentence()</code> relies on the boundaries definitions in the <span class="pkg">stringi</span>
package (see <a href="stringi.html#topic+stri_opts_brkiter">stri_opts_brkiter</a>).  It does not
count sentences correctly if the text has been transformed to lower case,
and for this reason <code>nsentence()</code> will issue a warning if it detects all
lower-cased text.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simple example
txt &lt;- c(text1 = "This is a sentence: second part of first sentence.",
         text2 = "A word. Repeated repeated.",
         text3 = "Mr. Jones has a PhD from the LSE.  Second sentence.")
nsentence(txt)
</code></pre>

<hr>
<h2 id='ntoken'>Count the number of tokens or types</h2><span id='topic+ntoken'></span><span id='topic+ntype'></span>

<h3>Description</h3>

<p>Get the count of tokens (total features) or types (unique tokens).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ntoken(x, ...)

ntype(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ntoken_+3A_x">x</code></td>
<td>
<p>a <span class="pkg">quanteda</span> object: a character, <a href="#topic+corpus">corpus</a>,
<a href="#topic+tokens">tokens</a>, or <a href="#topic+dfm">dfm</a> object</p>
</td></tr>
<tr><td><code id="ntoken_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="#topic+tokens">tokens()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The precise definition of &quot;tokens&quot; for objects not yet tokenized (e.g.
<a href="base.html#topic+character">character</a> or <a href="#topic+corpus">corpus</a> objects) can be controlled through optional
arguments passed to <code><a href="#topic+tokens">tokens()</a></code> through <code>...</code>.
</p>
<p>For <a href="#topic+dfm">dfm</a> objects, <code>ntype</code> will only return the count of features
that occur more than zero times in the dfm.
</p>


<h3>Value</h3>

<p>named integer vector of the counts of the total tokens or types
</p>


<h3>Note</h3>

<p>Due to differences between raw text tokens and features that have been
defined for a <a href="#topic+dfm">dfm</a>, the counts may be different for dfm objects and the
texts from which the dfm was generated.  Because the method tokenizes the
text in order to count the tokens, your results will depend on the options
passed through to <code><a href="#topic+tokens">tokens()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simple example
txt &lt;- c(text1 = "This is a sentence, this.", text2 = "A word. Repeated repeated.")
ntoken(txt)
ntype(txt)
ntoken(char_tolower(txt))  # same
ntype(char_tolower(txt))   # fewer types
ntoken(char_tolower(txt), remove_punct = TRUE)
ntype(char_tolower(txt), remove_punct = TRUE)

# with some real texts
ntoken(corpus_subset(data_corpus_inaugural, Year &lt; 1806), remove_punct = TRUE)
ntype(corpus_subset(data_corpus_inaugural, Year &lt; 1806), remove_punct = TRUE)
ntoken(dfm(tokens(corpus_subset(data_corpus_inaugural, Year &lt; 1800))))
ntype(dfm(tokens(corpus_subset(data_corpus_inaugural, Year &lt; 1800))))
</code></pre>

<hr>
<h2 id='object-builders'>Object builders</h2><span id='topic+object-builders'></span><span id='topic+build_dfm'></span><span id='topic+rebuild_dfm'></span><span id='topic+upgrade_dfm'></span><span id='topic+build_tokens'></span><span id='topic+rebuild_tokens'></span><span id='topic+upgrade_tokens'></span><span id='topic+build_corpus'></span><span id='topic+rebuild_corpus'></span><span id='topic+upgrade_corpus'></span><span id='topic+build_dictionary2'></span><span id='topic+rebuild_dictionary2'></span><span id='topic+upgrade_dictionary2'></span><span id='topic+build_fcm'></span><span id='topic+rebuild_fcm'></span><span id='topic+upgrade_fcm'></span>

<h3>Description</h3>

<p>Functions to build or re-build core objects, or to upgrade earlier versions
of these objects to the current format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_dfm(
  x,
  features,
  docvars = data.frame(),
  meta = list(),
  class = "dfm",
  ...
)

rebuild_dfm(x, attrs)

upgrade_dfm(x)

build_tokens(
  x,
  types,
  padding = FALSE,
  docvars = data.frame(),
  meta = list(),
  class = "tokens",
  ...
)

rebuild_tokens(x, attrs)

upgrade_tokens(x)

build_corpus(x, docvars = data.frame(), meta = list(), class = "corpus", ...)

rebuild_corpus(x, attrs)

upgrade_corpus(x)

build_dictionary2(x, meta = list(), class = "dictionary2", ...)

rebuild_dictionary2(x, attrs)

upgrade_dictionary2(x)

build_fcm(
  x,
  features1,
  features2 = features1,
  meta = list(),
  class = "fcm",
  ...
)

rebuild_fcm(x, attrs)

upgrade_fcm(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="object-builders_+3A_x">x</code></td>
<td>
<p>an input <a href="#topic+corpus">corpus</a>, <a href="#topic+tokens">tokens</a>, <a href="#topic+dfm">dfm</a>, <a href="#topic+fcm">fcm</a> or <a href="#topic+dictionary">dictionary</a> object.</p>
</td></tr>
<tr><td><code id="object-builders_+3A_features">features</code></td>
<td>
<p>character for feature of resulting <code>dfm</code>.</p>
</td></tr>
<tr><td><code id="object-builders_+3A_docvars">docvars</code></td>
<td>
<p>data.frame for document level variables created by
<code><a href="#topic+make_docvars">make_docvars()</a></code>. Names of documents are extracted from the
<code>docname_</code> column.</p>
</td></tr>
<tr><td><code id="object-builders_+3A_meta">meta</code></td>
<td>
<p>list for meta fields</p>
</td></tr>
<tr><td><code id="object-builders_+3A_class">class</code></td>
<td>
<p>class labels to be attached to the object.</p>
</td></tr>
<tr><td><code id="object-builders_+3A_...">...</code></td>
<td>
<p>values saved in the object meta fields. They overwrite values
passed via <code>meta</code>. If not specified, default values in
<code><a href="#topic+make_meta">make_meta()</a></code> will be used.</p>
</td></tr>
<tr><td><code id="object-builders_+3A_attrs">attrs</code></td>
<td>
<p>a list of attributes to be reassigned</p>
</td></tr>
<tr><td><code id="object-builders_+3A_types">types</code></td>
<td>
<p>character for types of resulting the <code>tokens</code> object.</p>
</td></tr>
<tr><td><code id="object-builders_+3A_padding">padding</code></td>
<td>
<p>logical indicating if the <code>tokens</code> object contains paddings.</p>
</td></tr>
<tr><td><code id="object-builders_+3A_features1">features1</code></td>
<td>
<p>character for row feature of resulting <code>fcm</code>.</p>
</td></tr>
<tr><td><code id="object-builders_+3A_features2">features2</code></td>
<td>
<p>character for column feature of resulting <code>fcm</code> iff.
different from <code>feature1</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>quanteda:::build_tokens(
    list(c(1, 2, 3), c(4, 5, 6)),
    docvars = quanteda:::make_docvars(n = 2L),
    types = c("a", "b", "c", "d", "e", "f"),
    padding = FALSE
)
quanteda:::build_corpus(
    c("a b c", "d e f"),
    docvars = quanteda:::make_docvars(n = 2L),
    unit = "sentence"
)
</code></pre>

<hr>
<h2 id='object2id'>Match quanteda objects against token types</h2><span id='topic+object2id'></span><span id='topic+object2fixed'></span>

<h3>Description</h3>

<p>Developer function to match patterns in quanteda objects against token types.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>object2id(
  x,
  types,
  valuetype = c("glob", "fixed", "regex"),
  case_insensitive = TRUE,
  concatenator = "_",
  levels = 1,
  remove_unigram = FALSE,
  keep_nomatch = FALSE
)

object2fixed(
  x,
  types,
  valuetype = c("glob", "fixed", "regex"),
  case_insensitive = TRUE,
  concatenator = "_",
  levels = 1,
  remove_unigram = FALSE,
  keep_nomatch = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="object2id_+3A_x">x</code></td>
<td>
<p>a list of character vectors, <a href="#topic+dictionary">dictionary</a> or collocations object</p>
</td></tr>
<tr><td><code id="object2id_+3A_types">types</code></td>
<td>
<p>token types against which patterns are matched</p>
</td></tr>
<tr><td><code id="object2id_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="object2id_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="object2id_+3A_concatenator">concatenator</code></td>
<td>
<p>the concatenation character that join multi-word
expression in <code>types</code></p>
</td></tr>
<tr><td><code id="object2id_+3A_levels">levels</code></td>
<td>
<p>integers specifying the levels of entries in a hierarchical
dictionary that will be applied.  The top level is 1, and subsequent levels
describe lower nesting levels.  Values may be combined, even if these
levels are not contiguous, e.g. <code>levels = c(1:3)</code> will collapse the second
level into the first, but record the third level (if present) collapsed
below the first (see examples).</p>
</td></tr>
<tr><td><code id="object2id_+3A_remove_unigram">remove_unigram</code></td>
<td>
<p>if <code>TRUE</code>, ignores single-word patterns</p>
</td></tr>
<tr><td><code id="object2id_+3A_keep_nomatch">keep_nomatch</code></td>
<td>
<p>keep patterns that did not match</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>object2fixed()</code> returns a list of character vectors of matched
types. <code>object2id()</code> returns a list of indices of matched types with
attributes. The &quot;pattern&quot; attribute records the indices of the matched patterns
in <code>x</code>; the &quot;key&quot; attribute records the keys of the matched patterns when <code>x</code> is
<a href="#topic+dictionary">dictionary</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pattern2id">pattern2id()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>types &lt;- c("A", "AA", "B", "BB", "B_B", "C", "C-C")

# dictionary
dict &lt;- dictionary(list(A = c("a", "aa"), 
                        B = c("BB", "B B"),
                        C = c("C", "C-C")))
object2fixed(dict, types)
object2fixed(dict, types, remove_unigram = TRUE)

# phrase
pats &lt;- phrase(c("a", "aa", "zz", "bb", "b b"))
object2fixed(pats, types)
object2fixed(pats, types, keep_nomatch = TRUE)
</code></pre>

<hr>
<h2 id='pattern'>Pattern for feature, token and keyword matching</h2><span id='topic+pattern'></span>

<h3>Description</h3>

<p>Pattern(s) for use in matching features, tokens, and keywords through a
<a href="#topic+valuetype">valuetype</a> pattern.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="pattern_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="#topic+pattern">pattern</a> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>pattern</code> argument is a vector of patterns, including
sequences, to match in a target object, whose match type is specified by
<code><a href="#topic+valuetype">valuetype()</a></code>. Note that an empty pattern (<code>""</code>) will match
&quot;padding&quot; in a <a href="#topic+tokens">tokens</a> object.
</p>

<dl>
<dt><code>character</code></dt><dd><p>A character vector of token patterns to be selected
or removed. Whitespace is not privileged, so that in a character vector,
white space is interpreted literally. If you wish to consider
whitespace-separated elements as sequences of tokens, wrap the argument in
<code><a href="#topic+phrase">phrase()</a></code>. </p>
</dd>
<dt><code style="white-space: pre;">&#8288;list of character objects&#8288;</code></dt><dd><p>If the list elements are character
vectors of length 1, then this is equivalent to a vector of characters.  If
a list element contains a vector of characters longer than length 1, then
for matching will consider these as sequences of matches, equivalent to
wrapping the argument in <code><a href="#topic+phrase">phrase()</a></code>, except for matching to
<a href="#topic+dfm">dfm</a> features where this does not apply. </p>
</dd>
<dt><code>dictionary</code></dt><dd><p>Values in <a href="#topic+dictionary">dictionary</a> are used as patterns,
for literal matches. Multi-word values are automatically converted into
phrases, so performing selection or compounding using a dictionary is the
same as wrapping the dictionary in <code><a href="#topic+phrase">phrase()</a></code>. </p>
</dd>
<dt><code>collocations</code></dt><dd><p>Collocations objects created from
<code>quanteda.textstats::textstat_collocations()</code>, which are treated as phrases
automatically.
</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'># these are interpreted literally
(patt1 &lt;- c("president", "white house", "house of representatives"))
# as multi-word sequences
phrase(patt1)

# three single-word patterns
(patt2 &lt;- c("president", "white_house", "house_of_representatives"))
phrase(patt2)

# this is equivalent to phrase(patt1)
(patt3 &lt;- list(c("president"), c("white", "house"),
               c("house", "of", "representatives")))

# glob expression can be used
phrase(patt4 &lt;- c("president?", "white house", "house * representatives"))

# this is equivalent to phrase(patt4)
(patt5 &lt;- list(c("president?"), c("white", "house"), c("house", "*", "representatives")))

# dictionary with multi-word matches
(dict1 &lt;- dictionary(list(us = c("president", "white house", "house of representatives"))))
phrase(dict1)
</code></pre>

<hr>
<h2 id='pattern2id'>Match patterns against token types</h2><span id='topic+pattern2id'></span><span id='topic+pattern2fixed'></span><span id='topic+index_types'></span>

<h3>Description</h3>

<p>Developer function to match regex, fixed or glob patterns against token
types. This allows C++ function to perform fast searches in tokens object.
C++ functions use a list of type IDs to construct a hash table, against which
sub-vectors of tokens object are matched. This function constructs an index
of glob patterns for faster matching.
</p>
<p><code>pattern2fixed</code> converts regex and glob patterns to fixed patterns.
</p>
<p><code>index_types</code> is an auxiliary function for <code>pattern2id</code> that
constructs an index of &quot;glob&quot; or &quot;fixed&quot; patterns to avoid expensive
sequential search. For example, a type &quot;cars&quot; is index by keys &quot;cars&quot;,
&quot;car?&quot;, &quot;c*&quot;, &quot;ca*&quot;, &quot;car*&quot; and &quot;cars*&quot; when <code>valuetype="glob"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pattern2id(
  pattern,
  types,
  valuetype = c("glob", "fixed", "regex"),
  case_insensitive = TRUE,
  keep_nomatch = FALSE,
  use_index = TRUE
)

pattern2fixed(
  pattern,
  types,
  valuetype = c("glob", "fixed", "regex"),
  case_insensitive = TRUE,
  keep_nomatch = FALSE,
  use_index = TRUE
)

index_types(
  pattern,
  types,
  valuetype = c("glob", "fixed", "regex"),
  case_insensitive = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pattern2id_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="#topic+pattern">pattern</a> for details.</p>
</td></tr>
<tr><td><code id="pattern2id_+3A_types">types</code></td>
<td>
<p>token types against which patterns are matched</p>
</td></tr>
<tr><td><code id="pattern2id_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="pattern2id_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="pattern2id_+3A_keep_nomatch">keep_nomatch</code></td>
<td>
<p>keep patterns that did not match</p>
</td></tr>
<tr><td><code id="pattern2id_+3A_use_index">use_index</code></td>
<td>
<p>construct index of types for quick search</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of integer vectors containing indices of matched types
</p>
<p><code>pattern2fixed</code> returns a list of character vectors containing
types
</p>
<p><code>index_types</code> returns a list of integer vectors containing type
IDs with index keys as an attribute
</p>


<h3>Examples</h3>

<pre><code class='language-R'>types &lt;- c("A", "AA", "B", "BB", "BBB", "C", "CC")

pats_regex &lt;- list(c("^a$", "^b"), c("c"), c("d"))
pattern2id(pats_regex, types, "regex", case_insensitive = TRUE)

pats_glob &lt;- list(c("a*", "b*"), c("c"), c("d"))
pattern2id(pats_glob, types, "glob", case_insensitive = TRUE)

pattern &lt;- list(c("^a$", "^b"), c("c"), c("d"))
types &lt;- c("A", "AA", "B", "BB", "BBB", "C", "CC")
pattern2fixed(pattern, types, "regex", case_insensitive = TRUE)
index &lt;- index_types("yy*", c("xxx", "yyyy", "ZZZ"), "glob", FALSE)
quanteda:::search_glob("yy*", attr(index, "types_search"), index)
</code></pre>

<hr>
<h2 id='phrase'>Declare a pattern to be a sequence of separate patterns</h2><span id='topic+phrase'></span><span id='topic+as.phrase'></span><span id='topic+is.phrase'></span>

<h3>Description</h3>

<p>Declares that a character expression consists of multiple patterns, separated
by an element such as whitespace.  This is typically used as a wrapper around
<code><a href="#topic+pattern">pattern()</a></code> to make it explicit that the pattern elements are to be used for
matches to multi-word sequences, rather than individual, unordered matches to
single words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phrase(x, separator = " ")

as.phrase(x)

is.phrase(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="phrase_+3A_x">x</code></td>
<td>
<p>character, <a href="#topic+dictionary">dictionary</a>, list, collocations, or tokens object; the
compound patterns to be treated as a sequence separated by <code>separator</code>.
For list, collocations, or tokens objects, use <code>as.phrase()</code>.</p>
</td></tr>
<tr><td><code id="phrase_+3A_separator">separator</code></td>
<td>
<p>character; the character in between the patterns. This
defaults to &quot; &quot;.  For <code>phrase()</code> only.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>phrase()</code> and <code>as.phrase()</code> return a specially classed list whose
elements have been split into separate <code>character</code> (pattern) elements.
</p>
<p><code>is.phrase</code> returns <code>TRUE</code> if the object was created by
<code><a href="#topic+phrase">phrase()</a></code>; <code>FALSE</code> otherwise.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.phrase">as.phrase()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># make phrases from characters
phrase(c("natural language processing"))
phrase(c("natural_language_processing", "text_analysis"), separator = "_")

# from a dictionary
phrase(dictionary(list(catone = c("a b"), cattwo = "c d e", catthree = "f")))

# from a list
as.phrase(list(c("natural", "language", "processing")))

# from tokens
as.phrase(tokens("natural language processing"))
</code></pre>

<hr>
<h2 id='print-methods'>Print methods for quanteda core objects</h2><span id='topic+print-methods'></span><span id='topic+print.corpus'></span><span id='topic+print+2Cdfm-method'></span><span id='topic+print.dfm'></span><span id='topic+print+2Cdictionary2-method'></span><span id='topic+print.dictionary'></span><span id='topic+print+2Cfcm-method'></span><span id='topic+print.kwic'></span><span id='topic+print.tokens'></span>

<h3>Description</h3>

<p>Print method for <span class="pkg">quanteda</span> objects.  In each <code style="white-space: pre;">&#8288;max_n*&#8288;</code> option, 0 shows none, and
-1 shows all.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'corpus'
print(
  x,
  max_ndoc = quanteda_options("print_corpus_max_ndoc"),
  max_nchar = quanteda_options("print_corpus_max_nchar"),
  show_summary = quanteda_options("print_corpus_summary"),
  ...
)

## S4 method for signature 'dfm'
print(
  x,
  max_ndoc = quanteda_options("print_dfm_max_ndoc"),
  max_nfeat = quanteda_options("print_dfm_max_nfeat"),
  show_summary = quanteda_options("print_dfm_summary"),
  ...
)

## S4 method for signature 'dictionary2'
print(
  x,
  max_nkey = quanteda_options("print_dictionary_max_nkey"),
  max_nval = quanteda_options("print_dictionary_max_nval"),
  show_summary = quanteda_options("print_dictionary_summary"),
  ...
)

## S4 method for signature 'fcm'
print(
  x,
  max_nfeat = quanteda_options("print_dfm_max_nfeat"),
  show_summary = TRUE,
  ...
)

## S3 method for class 'kwic'
print(
  x,
  max_nrow = quanteda_options("print_kwic_max_nrow"),
  show_summary = quanteda_options("print_kwic_summary"),
  ...
)

## S3 method for class 'tokens'
print(
  x,
  max_ndoc = quanteda_options("print_tokens_max_ndoc"),
  max_ntoken = quanteda_options("print_tokens_max_ntoken"),
  show_summary = quanteda_options("print_tokens_summary"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print-methods_+3A_x">x</code></td>
<td>
<p>the object to be printed</p>
</td></tr>
<tr><td><code id="print-methods_+3A_max_ndoc">max_ndoc</code></td>
<td>
<p>max number of documents to print; default is from the
<code style="white-space: pre;">&#8288;print_*_max_ndoc&#8288;</code> setting of <code><a href="#topic+quanteda_options">quanteda_options()</a></code></p>
</td></tr>
<tr><td><code id="print-methods_+3A_max_nchar">max_nchar</code></td>
<td>
<p>max number of tokens to print; default is from the
<code>print_corpus_max_nchar</code> setting of <code><a href="#topic+quanteda_options">quanteda_options()</a></code></p>
</td></tr>
<tr><td><code id="print-methods_+3A_show_summary">show_summary</code></td>
<td>
<p>print a brief summary indicating the number of documents
and other characteristics of the object, such as docvars or sparsity.</p>
</td></tr>
<tr><td><code id="print-methods_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="print-methods_+3A_max_nfeat">max_nfeat</code></td>
<td>
<p>max number of features to print; default is from the
<code>print_dfm_max_nfeat</code> setting of <code><a href="#topic+quanteda_options">quanteda_options()</a></code></p>
</td></tr>
<tr><td><code id="print-methods_+3A_max_nkey">max_nkey</code></td>
<td>
<p>max number of keys to print; default is from the
<code>print_dictionary_max_max_nkey</code> setting of <code><a href="#topic+quanteda_options">quanteda_options()</a></code></p>
</td></tr>
<tr><td><code id="print-methods_+3A_max_nval">max_nval</code></td>
<td>
<p>max number of values to print; default is from the
<code>print_dictionary_max_nval</code> setting of <code><a href="#topic+quanteda_options">quanteda_options()</a></code></p>
</td></tr>
<tr><td><code id="print-methods_+3A_max_nrow">max_nrow</code></td>
<td>
<p>max number of documents to print; default is from the
<code>print_kwic_max_nrow</code> setting of <code><a href="#topic+quanteda_options">quanteda_options()</a></code></p>
</td></tr>
<tr><td><code id="print-methods_+3A_max_ntoken">max_ntoken</code></td>
<td>
<p>max number of tokens to print; default is from the
<code>print_tokens_max_ntoken</code> setting of <code><a href="#topic+quanteda_options">quanteda_options()</a></code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+quanteda_options">quanteda_options()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corp &lt;- corpus(data_char_ukimmig2010)
print(corp, max_ndoc = 3, max_nchar = 40)

toks &lt;- tokens(corp)
print(toks, max_ndoc = 3, max_ntoken = 6)

dfmat &lt;- dfm(toks)
print(dfmat, max_ndoc = 3, max_nfeat = 10)
</code></pre>

<hr>
<h2 id='print.phrases'>Print a phrase object</h2><span id='topic+print.phrases'></span>

<h3>Description</h3>

<p>prints a phrase object in a way that looks like a standard list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'phrases'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.phrases_+3A_x">x</code></td>
<td>
<p>a phrases (constructed by <code><a href="#topic+phrase">phrase()</a></code> object to be printed</p>
</td></tr>
<tr><td><code id="print.phrases_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods</p>
</td></tr>
</table>

<hr>
<h2 id='quanteda_options'>Get or set package options for quanteda</h2><span id='topic+quanteda_options'></span>

<h3>Description</h3>

<p>Get or set global options affecting functions across <span class="pkg">quanteda</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quanteda_options(..., reset = FALSE, initialize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quanteda_options_+3A_...">...</code></td>
<td>
<p>options to be set, as key-value pair, same as
<code><a href="base.html#topic+options">options()</a></code>. This may be a list of valid key-value pairs, useful
for setting a group of options at once (see examples).</p>
</td></tr>
<tr><td><code id="quanteda_options_+3A_reset">reset</code></td>
<td>
<p>logical; if <code>TRUE</code>, reset all <span class="pkg">quanteda</span> options to
their default values</p>
</td></tr>
<tr><td><code id="quanteda_options_+3A_initialize">initialize</code></td>
<td>
<p>logical; if <code>TRUE</code>, reset only the <span class="pkg">quanteda</span>
options that are not already defined.  Used for setting initial values when
some have been defined previously, such as in <code>.Rprofile</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently available options are: </p>

<dl>
<dt><code>verbose</code></dt><dd><p>logical; if <code>TRUE</code> then use this as the default
for all functions with a <code>verbose</code> argument</p>
</dd>
<dt><code>threads</code></dt><dd><p>integer; specifies the number of threads to use in
parallelized functions; defaults to <code>RcppParallel::defaultNumThreads()</code>;
the number of threads can be changed only once in a session</p>
</dd>
<dt><code>print_dfm_max_ndoc</code>, <code>print_corpus_max_ndoc</code>, <code>print_tokens_max_ndoc</code></dt><dd><p>integer;
specify the number of documents to display when using the defaults for
printing a dfm, corpus, or tokens object</p>
</dd>
<dt><code>print_dfm_max_nfeat</code>, <code>print_corpus_max_nchar</code>, <code>print_tokens_max_ntoken</code></dt><dd><p>integer;
specifies the number of features to display when printing a dfm,
the number of characters to display when printing corpus documents, or the
number of tokens to display when printing tokens objects</p>
</dd>
<dt><code>print_dfm_summary</code></dt><dd><p>integer; specifies the number of documents
to display when using the defaults for printing a dfm</p>
</dd>
<dt><code>print_dictionary_max_nkey</code>, <code>print_dictionary_max_nval</code></dt><dd><p>the number of
keys or values (respectively) to display when printing a dictionary</p>
</dd>
<dt><code>print_kwic_max_nrow</code></dt><dd><p>the number of rows to display when printing a
kwic object</p>
</dd>
<dt><code>base_docname</code></dt><dd><p>character; stem name for documents that are
unnamed when a corpus, tokens, or dfm are created or when a dfm is converted
from another object</p>
</dd>
<dt><code>base_featname</code></dt><dd><p>character; stem name for
features that are unnamed when they are added, for whatever reason, to a dfm
through an operation that adds features</p>
</dd>
<dt><code>base_compname</code></dt><dd><p>character; stem name for components that are
created by matrix factorization</p>
</dd>
<dt><code>language_stemmer</code></dt><dd><p>character; language option for <code><a href="#topic+char_wordstem">char_wordstem()</a></code>,
<code><a href="#topic+tokens_wordstem">tokens_wordstem()</a></code>, and <code><a href="#topic+dfm_wordstem">dfm_wordstem()</a></code></p>
</dd>
<dt><code>pattern_hashtag</code>, <code>pattern_username</code></dt><dd><p>character; regex patterns for
(social media) hashtags and usernames respectively, used to avoid segmenting
these in the default internal &quot;word&quot; tokenizer</p>
</dd>
<dt><code>tokens_block_size</code></dt><dd><p>integer; specifies the
number of documents to be tokenized at a time in blocked tokenization.
When the number is large, tokenization becomes faster but also memory-intensive.</p>
</dd>
<dt><code>tokens_locale</code></dt><dd><p>character; specify locale in stringi boundary detection in
tokenization and corpus reshaping. See <code><a href="stringi.html#topic+stri_opts_brkiter">stringi::stri_opts_brkiter()</a></code>.</p>
</dd>
<dt><code>tokens_tokenizer_word</code></dt><dd><p>character; the current word tokenizer version
used as a default for <code>what = "word"</code> in <code><a href="#topic+tokens">tokens()</a></code>, one of <code>"word1"</code>,
<code>"word2"</code>, <code>"word3"</code> (same as <code>"word2"</code>), or <code>"word4"</code>.</p>
</dd>
</dl>



<h3>Value</h3>

<p>When called using a <code>key = value</code> pair (where <code>key</code> can be
a label or quoted character name)), the option is set and <code>TRUE</code> is
returned invisibly.
</p>
<p>When called with no arguments, a named list of the package options is
returned.
</p>
<p>When called with <code>reset = TRUE</code> as an argument, all arguments are
options are reset to their default values, and <code>TRUE</code> is returned
invisibly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>(opt &lt;- quanteda_options())

quanteda_options(verbose = TRUE)
quanteda_options("verbose" = FALSE)
quanteda_options("threads")
quanteda_options(print_dfm_max_ndoc = 50L)
# reset to defaults
quanteda_options(reset = TRUE)
# reset to saved options
quanteda_options(opt)

</code></pre>

<hr>
<h2 id='quanteda-package'>An R package for the quantitative analysis of textual data</h2><span id='topic+quanteda'></span><span id='topic+quanteda-package'></span>

<h3>Description</h3>

<p>Functions for creating and managing textual corpora, extracting
features from textual data, and analyzing those features using quantitative
methods.
</p>


<h3>Details</h3>

<p><span class="pkg">quanteda</span> makes it easy to manage texts in the form of a
corpus, defined as a collection of texts that includes document-level
variables specific to each text, as well as meta-data. <span class="pkg">quanteda</span>
includes tools to make it easy and fast to manipulate the texts in a
corpus, by performing the most common natural language processing tasks
simply and quickly, such as tokenizing, stemming, or forming ngrams.
<span class="pkg">quanteda</span>'s functions for tokenizing texts and forming multiple
tokenized documents into a document-feature matrix are both extremely fast
and very simple to use. <span class="pkg">quanteda</span> can segment texts easily by words,
paragraphs, sentences, or even user-supplied delimiters and tags.
</p>
<p>Built on the text processing functions in the <span class="pkg">stringi</span> package,
which is in turn built on C++ implementation of the ICU libraries for
Unicode text handling, <span class="pkg">quanteda</span> pays special attention to fast and
correct implementation of Unicode and the handling of text in any character
set.
</p>
<p><span class="pkg">quanteda</span> is built for efficiency and speed, through its design
around three infrastructures: the <span class="pkg">stringi</span> package for text
processing, the <span class="pkg">Matrix</span> package for sparse matrix objects, and
computationally intensive processing (e.g. for tokens) handled in
parallelized C++. If you can fit it into memory, <span class="pkg">quanteda</span> will handle
it quickly. (And eventually, we will make it possible to process objects
even larger than available memory.)
</p>
<p><span class="pkg">quanteda</span> is principally designed to allow users a fast and
convenient method to go from a corpus of texts to a selected matrix of
documents by features, after defining what the documents and features. The
package makes it easy to redefine documents, for instance by splitting them
into sentences or paragraphs, or by tags, as well as to group them into
larger documents by document variables, or to subset them based on logical
conditions or combinations of document variables. The package also
implements common NLP feature selection functions, such as removing
stopwords and stemming in numerous languages, selecting words found in
dictionaries, treating words as equivalent based on a user-defined
&quot;thesaurus&quot;, and trimming and weighting features based on document
frequency, feature frequency, and related measures such as tf-idf.
</p>
<p>Tools for working with dictionaries are one of <span class="pkg">quanteda</span>'s
principal strengths, and the package includes several core functions for
preparing and applying dictionaries to texts, for example for lexicon-based
sentiment analysis.
</p>
<p>Once constructed, a <span class="pkg">quanteda</span> document-feature matrix (&quot;<a href="#topic+dfm">dfm</a>&quot;)
can be easily analyzed using either <span class="pkg">quanteda</span>'s built-in tools for
scaling document positions, or used with a number of other text analytic
tools, such as: topic models (including converters for direct use with the
topicmodels, LDA, and stm packages) document scaling (using the
<span class="pkg">quanteda.textmodels</span> package's functions for the &quot;wordfish&quot; and
&quot;Wordscores&quot; models, or direct use with the <strong>ca</strong> package for
correspondence analysis), or machine learning through a variety of other
packages that take matrix or matrix-like inputs. <span class="pkg">quanteda</span> includes
functions for converting its core objects, but especially a dfm, into other
formats so that these are easy to use with other analytic packages.
</p>
<p>Additional features of <span class="pkg">quanteda</span> include:
</p>

<ul>
<li><p> powerful, flexible tools for working with <a href="#topic+dictionary">dictionaries</a>;
</p>
</li>
<li><p> the ability to identify <a href="quanteda.textstats.html#topic+textstat_keyness">keywords</a>
associated with documents or groups of documents;
</p>
</li>
<li><p> the ability to explore texts using <a href="#topic+kwic">key-words-in-context</a>;
</p>
</li>
<li><p> quick computation of word or document
<a href="quanteda.textstats.html#topic+textstat_simil">similarities</a>, for clustering or to
compute distances for other purposes;
</p>
</li>
<li><p> a comprehensive suite of <a href="#topic+summary.corpus">descriptive statistics on text</a>
such as the number of sentences, words, characters, or syllables per
document; and
</p>
</li>
<li><p> flexible, easy to use graphical tools to portray many of the analyses
available in the package.
</p>
</li></ul>



<h3>Source code and additional information</h3>

<p><a href="https://github.com/quanteda/quanteda">https://github.com/quanteda/quanteda</a>
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Kenneth Benoit <a href="mailto:kbenoit@lse.ac.uk">kbenoit@lse.ac.uk</a> (<a href="https://orcid.org/0000-0002-0797-564X">ORCID</a>) [copyright holder]
</p>
<p>Authors:
</p>

<ul>
<li><p> Kohei Watanabe <a href="mailto:watanabe.kohei@gmail.com">watanabe.kohei@gmail.com</a> (<a href="https://orcid.org/0000-0001-6519-5265">ORCID</a>)
</p>
</li>
<li><p> Haiyan Wang <a href="mailto:whyinsa@yahoo.com">whyinsa@yahoo.com</a> (<a href="https://orcid.org/0000-0003-4992-4311">ORCID</a>)
</p>
</li>
<li><p> Paul Nulty <a href="mailto:paul.nulty@gmail.com">paul.nulty@gmail.com</a> (<a href="https://orcid.org/0000-0002-7214-4666">ORCID</a>)
</p>
</li>
<li><p> Adam Obeng <a href="mailto:quanteda@binaryeagle.com">quanteda@binaryeagle.com</a> (<a href="https://orcid.org/0000-0002-2906-4775">ORCID</a>)
</p>
</li>
<li><p> Stefan Müller <a href="mailto:stefan.mueller@ucd.ie">stefan.mueller@ucd.ie</a> (<a href="https://orcid.org/0000-0002-6315-4125">ORCID</a>)
</p>
</li>
<li><p> Akitaka Matsuo <a href="mailto:a.matsuo@essex.ac.uk">a.matsuo@essex.ac.uk</a> (<a href="https://orcid.org/0000-0002-3323-6330">ORCID</a>)
</p>
</li>
<li><p> William Lowe <a href="mailto:lowe@hertie-school.org">lowe@hertie-school.org</a> (<a href="https://orcid.org/0000-0002-1549-6163">ORCID</a>)
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Christian Müller <a href="mailto:C.Mueller@lse.ac.uk">C.Mueller@lse.ac.uk</a> [contributor]
</p>
</li>
<li><p> Olivier Delmarcelle <a href="mailto:olivier.delmarcelle@ugent.be">olivier.delmarcelle@ugent.be</a> (<a href="https://orcid.org/0000-0003-4347-070X">ORCID</a>) [contributor]
</p>
</li>
<li><p> European Research Council (ERC-2011-StG 283794-QUANTESS) [funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://quanteda.io">https://quanteda.io</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/quanteda/quanteda/issues">https://github.com/quanteda/quanteda/issues</a>
</p>
</li></ul>


<hr>
<h2 id='read_dict_functions'>Internal functions to import dictionary files</h2><span id='topic+read_dict_functions'></span><span id='topic+read_dict_lexicoder'></span><span id='topic+read_dict_wordstat'></span><span id='topic+read_dict_liwc'></span><span id='topic+read_dict_yoshikoder'></span>

<h3>Description</h3>

<p>Internal functions to import dictionary files in a variety of formats
</p>
<p><code>read_dict_lexicoder</code> imports Lexicoder files in the <code>.lc3</code> format.
</p>
<p><code>read_dict_wordstat</code> imports WordStat files in the
<code>.cat</code> format.
</p>
<p><code>read_dict_liwc</code> imports LIWC dictionary files in the
<code>.dic</code> format.
</p>
<p><code>read_dict_yoshikoder</code> imports Yoshikoder files in the
<code>.ykd</code> format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_dict_lexicoder(path)

read_dict_wordstat(path, encoding = "utf-8")

read_dict_liwc(path, encoding = "utf-8")

read_dict_yoshikoder(path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_dict_functions_+3A_path">path</code></td>
<td>
<p>the full path and filename of the dictionary file to be read</p>
</td></tr>
<tr><td><code id="read_dict_functions_+3A_encoding">encoding</code></td>
<td>
<p>the encoding of the file to be imported</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <span class="pkg">quanteda</span> <a href="#topic+dictionary">dictionary</a> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dict &lt;- quanteda:::read_dict_lexicoder(
    system.file("extdata", "LSD2015.lc3", package = "quanteda")
)


## Not run: 
dict &lt;- quanteda:::read_dict_wordstat(system.file("extdata", "RID.cat", package = "quanteda"))
# dict &lt;- read_dict_wordstat("/home/kohei/Documents/Dictionary/LaverGarry.txt", "utf-8")
# dict &lt;- read_dict_wordstat("/home/kohei/Documents/Dictionary/Wordstat/ROGET.cat", "utf-8")
# dict &lt;- read_dict_wordstat("/home/kohei/Documents/Dictionary/Wordstat/WordStat Sentiments.cat",
#                            encoding = "iso-8859-1")

## End(Not run)

dict &lt;- quanteda:::read_dict_liwc(
    system.file("extdata", "moral_foundations_dictionary.dic", package = "quanteda")
)

dict &lt;- quanteda:::read_dict_yoshikoder(system.file("extdata", "laver_garry.ykd",
                                                    package = "quanteda"))
</code></pre>

<hr>
<h2 id='readtext-methods'>Extensions for readtext objects</h2><span id='topic+readtext-methods'></span><span id='topic+docnames.readtext'></span><span id='topic+docvars.readtext'></span><span id='topic+ndoc.readtext'></span>

<h3>Description</h3>

<p>These functions provide <span class="pkg">quanteda</span> methods for <span class="pkg">readtext</span> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'readtext'
docnames(x)

## S3 method for class 'readtext'
docvars(x, field = NULL)

## S3 method for class 'readtext'
ndoc(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readtext-methods_+3A_x">x</code></td>
<td>
<p>an object read by <code>readtext()</code> from the <span class="pkg">readtext</span> package</p>
</td></tr>
<tr><td><code id="readtext-methods_+3A_field">field</code></td>
<td>
<p>string containing the document-level variable name</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>docnames(x)</code> returns a character vector of the document names from a
readtext object
</p>
<p><code>docvars(x, field = NULL)</code> returns a data.frame of the document
variables from a readtext object or a vector if <code>field</code> is a single value
</p>
<p><code>ndoc(x)</code> returns the number of documents from a readtext object
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+stopwords'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>stopwords</dt><dd><p><code><a href="stopwords.html#topic+stopwords">stopwords</a></code></p>
</dd>
</dl>


<h3>Stopwords</h3>

<p>Stopword lists were formerly built into <span class="pkg">quanteda</span>, but have been moved to the
<span class="pkg">stopwords</span> package. See <code><a href="stopwords.html#topic+stopwords">stopwords::stopwords()</a></code>.
</p>

<hr>
<h2 id='remove_empty_keys'>Utility function to remove empty keys</h2><span id='topic+remove_empty_keys'></span>

<h3>Description</h3>

<p>Utility function to remove empty keys
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_empty_keys(dict)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove_empty_keys_+3A_dict">dict</code></td>
<td>
<p>a flat or hierarchical dictionary</p>
</td></tr>
</table>

<hr>
<h2 id='replace_dictionary_values'>Internal function to replace dictionary values</h2><span id='topic+replace_dictionary_values'></span>

<h3>Description</h3>

<p>Internal function to replace dictionary values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>replace_dictionary_values(dict, from, to)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="replace_dictionary_values_+3A_dict">dict</code></td>
<td>
<p>a <a href="#topic+dictionary">dictionary</a> object</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>dict &lt;- list(KEY1 = list(SUBKEY1 = list("A_B"),
                          SUBKEY2 = list("C_D")),
              KEY2 = list(SUBKEY3 = list("E_F"),
                          SUBKEY4 = list("G_F_I")),
              KEY3 = list(SUBKEY5 = list(SUBKEY7 = list("J_K")),
                          SUBKEY6 = list(SUBKEY8 = list("L"))))
quanteda:::replace_dictionary_values(dict, "_", " ")
</code></pre>

<hr>
<h2 id='resample'>Sample a vector</h2><span id='topic+resample'></span>

<h3>Description</h3>

<p>Return a sample from a vector within a grouping variable if specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resample(x, size = NULL, replace = FALSE, prob = NULL, by = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="resample_+3A_x">x</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="resample_+3A_size">size</code></td>
<td>
<p>the number of items to sample within each group, as a positive
number or a vector of numbers equal in length to the number of groups. If
<code>NULL</code>, the sampling is stratified by group in the original group
sizes.</p>
</td></tr>
<tr><td><code id="resample_+3A_replace">replace</code></td>
<td>
<p>if <code>TRUE</code>, sample with replacement</p>
</td></tr>
<tr><td><code id="resample_+3A_prob">prob</code></td>
<td>
<p>a vector of probability weights for values in <code>x</code></p>
</td></tr>
<tr><td><code id="resample_+3A_by">by</code></td>
<td>
<p>a grouping vector equal in length to <code>length(x)</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code> resampled within groups
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(100)
grvec &lt;- c(rep("a", 3), rep("b", 4), rep("c", 3))
quanteda:::resample(1:10, replace = FALSE, by = grvec)
quanteda:::resample(1:10, replace = TRUE, by = grvec)
quanteda:::resample(1:10, size = 2, replace = TRUE, by = grvec)
quanteda:::resample(1:10, size = c(1, 1, 3), replace = TRUE, by = grvec)
</code></pre>

<hr>
<h2 id='reshape_docvars'>Internal function to subset or duplicate docvar rows</h2><span id='topic+reshape_docvars'></span>

<h3>Description</h3>

<p>Internal function to subset or duplicate docvar rows
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reshape_docvars(x, i = NULL, unique = FALSE, drop_docid = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reshape_docvars_+3A_x">x</code></td>
<td>
<p>docvar data.frame</p>
</td></tr>
<tr><td><code id="reshape_docvars_+3A_i">i</code></td>
<td>
<p>numeric or logical vector for subsetting/duplicating rows</p>
</td></tr>
<tr><td><code id="reshape_docvars_+3A_unique">unique</code></td>
<td>
<p>if <code>TRUE</code>, names must be all unique. If <code>FALSE</code>, documents with the same
names are treated as segments from the same document and given serial number.</p>
</td></tr>
<tr><td><code id="reshape_docvars_+3A_drop_docid">drop_docid</code></td>
<td>
<p>if <code>TRUE</code>, drop unused names of documents.</p>
</td></tr>
</table>

<hr>
<h2 id='search_glob'>Select types without performing slow regex search</h2><span id='topic+search_glob'></span><span id='topic+search_glob_multi'></span><span id='topic+search_regex'></span><span id='topic+search_regex_multi'></span><span id='topic+search_fixed'></span><span id='topic+search_fixed_multi'></span>

<h3>Description</h3>

<p>This is an internal function for <code>pattern2id</code> that select types using
keys in index when available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>search_glob(pattern, types_search, case_insensitive, index = NULL)

search_glob_multi(patterns, types_search, case_insensitive, index)

search_regex(pattern, types_search, case_insensitive)

search_regex_multi(patterns, types_search, case_insensitive)

search_fixed(pattern, types_search, index = NULL)

search_fixed_multi(patterns, types_search, index)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="search_glob_+3A_pattern">pattern</code></td>
<td>
<p>a &quot;glob&quot;, &quot;fixed&quot; or &quot;regex&quot; pattern</p>
</td></tr>
<tr><td><code id="search_glob_+3A_types_search">types_search</code></td>
<td>
<p>lowercased types when <code>case_insensitive=TRUE</code>, but
not used in glob and fixed matching as types are in the index.</p>
</td></tr>
<tr><td><code id="search_glob_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="search_glob_+3A_index">index</code></td>
<td>
<p>index object created by <code>index_types</code></p>
</td></tr>
<tr><td><code id="search_glob_+3A_patterns">patterns</code></td>
<td>
<p>a list of &quot;glob&quot;, &quot;fixed&quot; or &quot;regex&quot; patterns</p>
</td></tr>
</table>

<hr>
<h2 id='search_index'>Internal function for <code>select_types</code> to search the index using
fastmatch.</h2><span id='topic+search_index'></span>

<h3>Description</h3>

<p>Internal function for <code>select_types</code> to search the index using
fastmatch.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>search_index(pattern, index)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="search_index_+3A_index">index</code></td>
<td>
<p>an index object created by <code>index_types</code></p>
</td></tr>
<tr><td><code id="search_index_+3A_regex">regex</code></td>
<td>
<p>a glob expression to search</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+index_types">index_types()</a></code>
</p>

<hr>
<h2 id='serialize_tokens'>Function to serialize list-of-character tokens</h2><span id='topic+serialize_tokens'></span>

<h3>Description</h3>

<p>Creates a serialized object of tokens, called by <code><a href="#topic+tokens">tokens()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>serialize_tokens(x, types_reserved = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="serialize_tokens_+3A_x">x</code></td>
<td>
<p>a list of character vectors</p>
</td></tr>
<tr><td><code id="serialize_tokens_+3A_types_reserved">types_reserved</code></td>
<td>
<p>optional pre-existing types for mapping of tokens</p>
</td></tr>
<tr><td><code id="serialize_tokens_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list the serialized tokens found in each text
</p>

<hr>
<h2 id='set_dfm_dimnames+26lt+3B-'>Internal functions to set dimnames</h2><span id='topic+set_dfm_dimnames+3C-'></span><span id='topic+set_dfm_docnames+3C-'></span><span id='topic+set_dfm_featnames+3C-'></span><span id='topic+set_fcm_dimnames+3C-'></span><span id='topic+set_fcm_featnames+3C-'></span>

<h3>Description</h3>

<p>Default <code>dimnames()</code> converts a zero-length character vector to NULL,
leading to the improper functioning of subsetting functions. These are safer
methods to set the dimnames of a dfm or fcm object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_dfm_dimnames(x) &lt;- value

set_dfm_docnames(x) &lt;- value

set_dfm_featnames(x) &lt;- value

set_fcm_dimnames(x) &lt;- value

set_fcm_featnames(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_dfm_dimnames+2B26lt+2B3B-_+3A_x">x</code></td>
<td>
<p><a href="#topic+dfm">dfm</a> or <a href="#topic+fcm">fcm</a></p>
</td></tr>
<tr><td><code id="set_dfm_dimnames+2B26lt+2B3B-_+3A_value">value</code></td>
<td>
<p>character a vector for docnames or featnames or a list of them
for dimnames</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat &lt;- dfm(tokens(c("a a b b c", "b b b c")))
quanteda:::set_dfm_featnames(dfmat) &lt;- paste0("feature", 1:3)
quanteda:::set_dfm_docnames(dfmat) &lt;- paste0("DOC", 1:2)
quanteda:::set_dfm_dimnames(dfmat) &lt;- list(c("docA", "docB"), LETTERS[1:3])
</code></pre>

<hr>
<h2 id='spacyr-methods'>Extensions for and from spacy_parse objects</h2><span id='topic+spacyr-methods'></span>

<h3>Description</h3>

<p>These functions provide <span class="pkg">quanteda</span> methods for <span class="pkg">spacyr</span> objects, and
also extend <a href="spacyr.html#topic+spacy_parse">spacy_parse</a> and
<a href="spacyr.html#topic+spacy_tokenize">spacy_tokenize</a> to work directly with <a href="#topic+corpus">corpus</a>
objects.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="spacyr-methods_+3A_x">x</code></td>
<td>
<p>an object returned by <code>spacy_parse</code>, or (for
<code>spacy_parse</code>) a <a href="#topic+corpus">corpus</a> object</p>
</td></tr>
<tr><td><code id="spacyr-methods_+3A_...">...</code></td>
<td>
<p>not used for these functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>spacy_parse(x, ...)</code> and <code>spacy_tokenize(x, ...)</code> work directly on
<span class="pkg">quanteda</span> <a href="#topic+corpus">corpus</a> objects.
</p>
<p><code>docnames(x)</code> returns the document names
</p>
<p><code>ndoc(x)</code> returns the number of documents
</p>
<p><code>ntoken(x, ...)</code> returns the number of tokens by document
</p>
<p><code>ntype(x, ...)</code> returns the number of types (unique tokens) by document
</p>
<p><code>nsentence(x)</code> returns the number of sentences by document
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library("spacyr")
spacy_initialize()

corp &lt;- corpus(c(doc1 = "And now, now, now for something completely different.",
                 doc2 = "Jack and Jill are children."))
spacy_tokenize(corp)
(parsed &lt;- spacy_parse(corp))

ntype(parsed)
ntoken(parsed)
ndoc(parsed)
docnames(parsed)

## End(Not run)
</code></pre>

<hr>
<h2 id='sparsity'>Compute the sparsity of a document-feature matrix</h2><span id='topic+sparsity'></span>

<h3>Description</h3>

<p>Return the proportion of sparseness of a document-feature matrix, equal
to the proportion of cells that have zero counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparsity(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparsity_+3A_x">x</code></td>
<td>
<p>the document-feature matrix</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat &lt;- dfm(data_corpus_inaugural)
sparsity(dfmat)
sparsity(dfm_trim(dfmat, min_termfreq = 5))
</code></pre>

<hr>
<h2 id='split_values'>Internal function for special handling of multi-word dictionary values</h2><span id='topic+split_values'></span>

<h3>Description</h3>

<p>Internal function for special handling of multi-word dictionary values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_values(dict, concatenator_dictionary, concatenator_tokens)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="split_values_+3A_dict">dict</code></td>
<td>
<p>a flatten dictionary</p>
</td></tr>
<tr><td><code id="split_values_+3A_concatenator_dictionary">concatenator_dictionary</code></td>
<td>
<p>concatenator from a dictionary object</p>
</td></tr>
<tr><td><code id="split_values_+3A_concatenator_tokens">concatenator_tokens</code></td>
<td>
<p>concatenator from a tokens object</p>
</td></tr>
</table>

<hr>
<h2 id='summary_metadata'>Functions to add or retrieve corpus summary metadata</h2><span id='topic+summary_metadata'></span><span id='topic+add_summary_metadata'></span><span id='topic+get_summary_metadata'></span><span id='topic+summarize_texts_extended'></span>

<h3>Description</h3>

<p>Functions to add or retrieve corpus summary metadata
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_summary_metadata(x, extended = FALSE, ...)

get_summary_metadata(x, ...)

summarize_texts_extended(x, stop_words = stopwords("en"), n = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary_metadata_+3A_x">x</code></td>
<td>
<p><a href="#topic+corpus">corpus</a> object</p>
</td></tr>
<tr><td><code id="summary_metadata_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="#topic+tokens">tokens()</a></code> when computing the
summary information</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is provided so that a <a href="#topic+corpus">corpus</a> object can be stored with
summary information to avoid having to compute this every time
<code style="white-space: pre;">&#8288;[summary.corpus()]&#8288;</code> is called.
</p>
<p>So in future calls, if <code style="white-space: pre;">&#8288;!is.null(meta(x, "summary", type = "system") &amp;&amp; !length(list(...))&#8288;</code>, then <code>summary.corpus()</code> will simply return
<code>get_system_meta()</code> rather than compute the summary statistics on the fly,
which requires tokenizing the text.
</p>


<h3>Value</h3>

<p><code>add_summary_metadata()</code> returns a corpus with summary metadata added
as a data.frame, with the top-level list element names <code>summary</code>.
</p>
<p><code>get_summary_metadata()</code> returns the summary metadata as a data.frame.
</p>
<p><code>summarize_texts_extended()</code> returns extended summary information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corp &lt;- corpus(data_char_ukimmig2010)
corp &lt;- quanteda:::add_summary_metadata(corp)
quanteda:::get_summary_metadata(corp)

## Not run: 
# using extended summary

extended_data &lt;- quanteda:::summarize_texts_extended(data_corpus_inaugural)

textplot_wordcloud(extended_data$top_dfm, max_words = 100)

\dontrun{
library("ggplot2")
ggplot(data.frame(all_tokens = extended_data$all_tokens), aes(x = all_tokens)) +
   geom_histogram(color = "darkblue", fill = "lightblue") +
   xlab("Total length in tokens")
}

## End(Not run)
</code></pre>

<hr>
<h2 id='summary.corpus'>Summarize a corpus</h2><span id='topic+summary.corpus'></span>

<h3>Description</h3>

<p>Displays information about a corpus, including attributes and metadata such
as date of number of texts, creation and source.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'corpus'
summary(object, n = 100, tolower = FALSE, showmeta = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.corpus_+3A_object">object</code></td>
<td>
<p>corpus to be summarized</p>
</td></tr>
<tr><td><code id="summary.corpus_+3A_n">n</code></td>
<td>
<p>maximum number of texts to describe, default=100</p>
</td></tr>
<tr><td><code id="summary.corpus_+3A_tolower">tolower</code></td>
<td>
<p>convert texts to lower case before counting types</p>
</td></tr>
<tr><td><code id="summary.corpus_+3A_showmeta">showmeta</code></td>
<td>
<p>set to <code>TRUE</code> to include document-level
meta-data</p>
</td></tr>
<tr><td><code id="summary.corpus_+3A_...">...</code></td>
<td>
<p>additional arguments passed through to <code><a href="#topic+tokens">tokens()</a></code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>summary(data_corpus_inaugural)
summary(data_corpus_inaugural, n = 10)
corp &lt;- corpus(data_char_ukimmig2010,
               docvars = data.frame(party=names(data_char_ukimmig2010)))
summary(corp, showmeta = TRUE) # show the meta-data
sumcorp &lt;- summary(corp) # (quietly) assign the results
sumcorp$Types / sumcorp$Tokens # crude type-token ratio
</code></pre>

<hr>
<h2 id='textmodels'>Models for scaling and classification of textual data</h2><span id='topic+textmodels'></span>

<h3>Description</h3>

<p>The <code style="white-space: pre;">&#8288;textmodel_*()&#8288;</code> functions formerly in <span class="pkg">quanteda</span> have now been moved
to the <span class="pkg">quanteda.textmodels</span> package.
</p>


<h3>See Also</h3>

<p><code>quanteda.textmodels::quanteda.textmodels-package</code>
</p>

<hr>
<h2 id='textplots'>Plots for textual data</h2><span id='topic+textplots'></span>

<h3>Description</h3>

<p>The <code style="white-space: pre;">&#8288;textplot_*()&#8288;</code> functions formerly in <span class="pkg">quanteda</span> have now been moved
to the <span class="pkg">quanteda.textplots</span> package.
</p>


<h3>See Also</h3>

<p><code>quanteda.textplots::quanteda.textplots-package</code>
</p>

<hr>
<h2 id='texts'>Get or assign corpus texts [deprecated]</h2><span id='topic+texts'></span><span id='topic+texts+3C-'></span>

<h3>Description</h3>

<p>This function is <strong>deprecated</strong> and will be removed in the next
major release.
</p>

<ul>
<li><p> Use <code><a href="#topic+as.character.corpus">as.character.corpus()</a></code> to turn a corpus into a simple named character
vector.
</p>
</li>
<li><p> Use <code><a href="#topic+corpus_group">corpus_group()</a></code> instead of <code>texts(x, groups = ...)</code> to aggregate texts
by a grouping variable.
</p>
</li>
<li><p> Use <code><a href="Matrix.html#topic++5B+3C-">[&lt;-</a></code> instead of <code style="white-space: pre;">&#8288;texts()&lt;-&#8288;</code> for replacing texts in a corpus object.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>texts(x, groups = NULL, spacer = " ")

texts(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="texts_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+corpus">corpus</a></p>
</td></tr>
<tr><td><code id="texts_+3A_groups">groups</code></td>
<td>
<p>grouping variable for sampling, equal in length to the number
of documents. This will be evaluated in the docvars data.frame, so that
docvars may be referred to by name without quoting. This also changes
previous behaviours for <code>groups</code>. See <code>news(Version &gt;= "3.0", package = "quanteda")</code> for details.</p>
</td></tr>
<tr><td><code id="texts_+3A_spacer">spacer</code></td>
<td>
<p>when concatenating texts by using <code>groups</code>, this will be the
spacing added between texts.  (Default is two spaces.)</p>
</td></tr>
<tr><td><code id="texts_+3A_value">value</code></td>
<td>
<p>character vector of the new texts</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Get or replace the texts in a <a href="#topic+corpus">corpus</a>, with grouping options.
Works for plain character vectors too, if <code>groups</code> is a factor.
</p>


<h3>Value</h3>

<p>For <code>texts</code>, a character vector of the texts in the corpus.
</p>
<p>For <code style="white-space: pre;">&#8288;texts &lt;-&#8288;</code>, the corpus with the updated texts.
</p>
<p>for <code style="white-space: pre;">&#8288;texts &lt;-&#8288;</code>, a corpus with the texts replaced by <code>value</code>
</p>


<h3>Note</h3>

<p>The <code>groups</code> will be used for concatenating the texts based on shared
values of <code>groups</code>, without any specified order of aggregation.
</p>
<p>You are strongly encouraged as a good practice of text analysis
workflow <em>not</em> to modify the substance of the texts in a corpus.
Rather, this sort of processing is better performed through downstream
operations.  For instance, do not lowercase the texts in a corpus, or you
will never be able to recover the original case.  Rather, apply
<code><a href="#topic+tokens_tolower">tokens_tolower()</a></code> after applying <code><a href="#topic+tokens">tokens()</a></code> to a
corpus, or use the option <code>tolower = TRUE</code> in <code><a href="#topic+dfm">dfm()</a></code>.
</p>

<hr>
<h2 id='textstats'>Statistics for textual data</h2><span id='topic+textstats'></span>

<h3>Description</h3>

<p>The <code style="white-space: pre;">&#8288;textstat_*()&#8288;</code> functions formerly in <span class="pkg">quanteda</span> have now been moved
to the <span class="pkg">quanteda.textstats</span> package.
</p>


<h3>See Also</h3>

<p><code>quanteda.textstats::quanteda.textstats-package</code>
</p>

<hr>
<h2 id='tokenize_custom'>Customizable tokenizer</h2><span id='topic+tokenize_custom'></span><span id='topic+breakrules_get'></span><span id='topic+breakrules_set'></span><span id='topic+breakrules_reset'></span>

<h3>Description</h3>

<p>Allows users to tokenize texts using customized boundary rules. See the <a href="https://unicode-org.github.io/icu/userguide/boundaryanalysis/break-rules.html">ICU website</a>
for how to define boundary rules.
</p>
<p>Tools for custom word and sentence breakrules, to retrieve, set, or reset
them to package defaults.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_custom(x, rules)

breakrules_get(what = c("word", "sentence"))

breakrules_set(x, what = c("word", "sentence"))

breakrules_reset(what = c("word", "sentence"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_custom_+3A_x">x</code></td>
<td>
<p>character vector for texts to tokenize</p>
</td></tr>
<tr><td><code id="tokenize_custom_+3A_rules">rules</code></td>
<td>
<p>a list of rules for rule-based boundary detection</p>
</td></tr>
<tr><td><code id="tokenize_custom_+3A_what">what</code></td>
<td>
<p>character; which set of rules to return, one of <code>"word"</code> or
<code>"sentence"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The package contains internal sets of rules for word and sentence
breaks, which are lists
of rules for word and sentence boundary detection. <code>base</code> is copied from
the ICU library. Other rules are created by the package maintainers in
<code>system.file("breakrules/breakrules_custom.yml")</code>.
</p>
<p>This function allows modification of those rules, and applies them as a new
tokenizer.
</p>
<p>Custom word rules:
</p>

<dl>
<dt><code>base</code></dt><dd><p>ICU's rules for detecting word/sentence boundaries</p>
</dd>
<dt><code>keep_hyphens</code></dt><dd><p>quanteda's rule for preserving hyphens</p>
</dd>
<dt><code>keep_url</code></dt><dd><p>quanteda's rule for preserving URLs</p>
</dd>
<dt><code>keep_email</code></dt><dd><p>quanteda's rule for preserving emails</p>
</dd>
<dt><code>keep_tags</code></dt><dd><p>quanteda's rule for preserving tags</p>
</dd>
<dt><code>split_elisions</code></dt><dd><p>quanteda's rule for splitting elisions</p>
</dd>
<dt><code>split_tags</code></dt><dd><p>quanteda's rule for splitting tags</p>
</dd>
</dl>



<h3>Value</h3>

<p><code>tokenize_custom()</code> returns a list of characters containing tokens.
</p>
<p><code>breakrules_get()</code> returns the existing break rules as a list.
</p>
<p><code>breakrules_set()</code> returns nothing but reassigns the global
breakrules to <code>x</code>.
</p>
<p><code>breakrules_reset()</code> returns nothing but reassigns the global
breakrules to the system defaults.  These rules are defined in
<code>system.file("breakrules/")</code>.
</p>


<h3>Source</h3>

<p><a href="https://raw.githubusercontent.com/unicode-org/icu/main/icu4c/source/data/brkitr/rules/word.txt">https://raw.githubusercontent.com/unicode-org/icu/main/icu4c/source/data/brkitr/rules/word.txt</a>
</p>
<p><a href="https://raw.githubusercontent.com/unicode-org/icu/main/icu4c/source/data/brkitr/rules/sent.txt">https://raw.githubusercontent.com/unicode-org/icu/main/icu4c/source/data/brkitr/rules/sent.txt</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lis &lt;- tokenize_custom("a well-known http://example.com", rules = breakrules_get("word"))
tokens(lis, remove_separators = TRUE)
breakrules_get("word")
breakrules_get("sentence")

brw &lt;- breakrules_get("word")
brw$keep_email &lt;- "@[a-zA-Z0-9_]+"
breakrules_set(brw, what = "word")
breakrules_reset("sentence")
breakrules_reset("word")
</code></pre>

<hr>
<h2 id='tokenize_internal'>quanteda tokenizers</h2><span id='topic+tokenize_internal'></span><span id='topic+tokenize'></span><span id='topic+tokenize_word2'></span><span id='topic+tokenize_word3'></span><span id='topic+tokenize_word4'></span><span id='topic+tokenize_word1'></span><span id='topic+tokenize_character'></span><span id='topic+tokenize_sentence'></span><span id='topic+tokenize_fasterword'></span><span id='topic+tokenize_fastestword'></span>

<h3>Description</h3>

<p>Internal methods for tokenization providing default and legacy methods for
text segmentation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_word2(
  x,
  split_hyphens = FALSE,
  verbose = quanteda_options("verbose"),
  ...
)

tokenize_word3(
  x,
  split_hyphens = FALSE,
  verbose = quanteda_options("verbose"),
  ...
)

tokenize_word4(
  x,
  split_hyphens = FALSE,
  split_tags = FALSE,
  split_elisions = FALSE,
  verbose = quanteda_options("verbose"),
  ...
)

tokenize_word1(
  x,
  split_hyphens = FALSE,
  verbose = quanteda_options("verbose"),
  ...
)

tokenize_character(x, ...)

tokenize_sentence(x, verbose = FALSE, ...)

tokenize_fasterword(x, ...)

tokenize_fastestword(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_internal_+3A_x">x</code></td>
<td>
<p>(named) character; input texts</p>
</td></tr>
<tr><td><code id="tokenize_internal_+3A_split_hyphens">split_hyphens</code></td>
<td>
<p>logical; if <code>FALSE</code>, do not split words that are
connected by hyphenation and hyphenation-like characters in between words,
e.g. <code>"self-aware"</code> becomes <code>c("self", "-", "aware")</code></p>
</td></tr>
<tr><td><code id="tokenize_internal_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code>, print timing messages to the console</p>
</td></tr>
<tr><td><code id="tokenize_internal_+3A_...">...</code></td>
<td>
<p>used to pass arguments among the functions</p>
</td></tr>
<tr><td><code id="tokenize_internal_+3A_split_tags">split_tags</code></td>
<td>
<p>logical; if <code>FALSE</code>, do not split social media tags defined
in <code>quanteda_options()</code>. The default patterns are <code>pattern_hashtag = "#\\w+#?"</code> and <code>pattern_username = "@[a-zA-Z0-9_]+"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each of the word tokenizers corresponds to a major version of <span class="pkg">quanteda</span>,
kept here for backward compatibility and comparison.  <code>tokenize_word3()</code> is
identical to <code>tokenize_word2()</code>.
</p>


<h3>Value</h3>

<p>a list of characters corresponding to the (most conservative)
tokenization, including whitespace where applicable; except for
<code>tokenize_word1()</code>, which is a special tokenizer for Internet language that
includes URLs, #hashtags, @usernames, and email addresses.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
txt &lt;- c(doc1 = "Tweet https://quanteda.io using @quantedainit and #rstats.",
         doc2 = "The £1,000,000 question.",
         doc4 = "Line 1.\nLine2\n\nLine3.",
         doc5 = "?",
         doc6 = "Self-aware machines! \U0001f600",
         doc7 = "Qu'est-ce que c'est?")
tokenize_word2(txt)
tokenize_word2(txt, split_hyphens = FALSE)
tokenize_word1(txt, split_hyphens = FALSE)
tokenize_word4(txt, split_hyphens = FALSE, split_elisions = TRUE)
tokenize_fasterword(txt)
tokenize_fastestword(txt)
tokenize_sentence(txt)
tokenize_character(txt[2])

## End(Not run)
</code></pre>

<hr>
<h2 id='tokens'>Construct a tokens object</h2><span id='topic+tokens'></span>

<h3>Description</h3>

<p>Construct a tokens object, either by importing a named list of characters
from an external tokenizer, or by calling the internal <span class="pkg">quanteda</span>
tokenizer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens(
  x,
  what = "word",
  remove_punct = FALSE,
  remove_symbols = FALSE,
  remove_numbers = FALSE,
  remove_url = FALSE,
  remove_separators = TRUE,
  split_hyphens = FALSE,
  split_tags = FALSE,
  include_docvars = TRUE,
  padding = FALSE,
  verbose = quanteda_options("verbose"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_+3A_x">x</code></td>
<td>
<p>the input object to the tokens constructor; a <a href="#topic+tokens">tokens</a>, <a href="#topic+corpus">corpus</a> or
<a href="base.html#topic+character">character</a> object to tokenize.</p>
</td></tr>
<tr><td><code id="tokens_+3A_what">what</code></td>
<td>
<p>character; which tokenizer to use.  The default <code>what = "word"</code>
is the version 2 <span class="pkg">quanteda</span> tokenizer.  Legacy tokenizers (version &lt; 2)
are also supported, including the default <code>what = "word1"</code>. See the Details
and quanteda Tokenizers below.</p>
</td></tr>
<tr><td><code id="tokens_+3A_remove_punct">remove_punct</code></td>
<td>
<p>logical; if <code>TRUE</code> remove all characters in the Unicode
&quot;Punctuation&quot; <code style="white-space: pre;">&#8288;[P]&#8288;</code> class, with exceptions for those used as prefixes for
valid social media tags if <code>preserve_tags = TRUE</code></p>
</td></tr>
<tr><td><code id="tokens_+3A_remove_symbols">remove_symbols</code></td>
<td>
<p>logical; if <code>TRUE</code> remove all characters in the Unicode
&quot;Symbol&quot; <code style="white-space: pre;">&#8288;[S]&#8288;</code> class</p>
</td></tr>
<tr><td><code id="tokens_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>logical; if <code>TRUE</code> remove tokens that consist only of
numbers, but not words that start with digits, e.g. <code style="white-space: pre;">&#8288;2day&#8288;</code></p>
</td></tr>
<tr><td><code id="tokens_+3A_remove_url">remove_url</code></td>
<td>
<p>logical; if <code>TRUE</code> find and eliminate URLs beginning with
http(s)</p>
</td></tr>
<tr><td><code id="tokens_+3A_remove_separators">remove_separators</code></td>
<td>
<p>logical; if <code>TRUE</code> remove separators and separator
characters (Unicode &quot;Separator&quot; <code style="white-space: pre;">&#8288;[Z]&#8288;</code> and &quot;Control&quot; <code style="white-space: pre;">&#8288;[C]&#8288;</code> categories)</p>
</td></tr>
<tr><td><code id="tokens_+3A_split_hyphens">split_hyphens</code></td>
<td>
<p>logical; if <code>FALSE</code>, do not split words that are
connected by hyphenation and hyphenation-like characters in between words,
e.g. <code>"self-aware"</code> becomes <code>c("self", "-", "aware")</code></p>
</td></tr>
<tr><td><code id="tokens_+3A_split_tags">split_tags</code></td>
<td>
<p>logical; if <code>FALSE</code>, do not split social media tags defined
in <code>quanteda_options()</code>. The default patterns are <code>pattern_hashtag = "#\\w+#?"</code> and <code>pattern_username = "@[a-zA-Z0-9_]+"</code>.</p>
</td></tr>
<tr><td><code id="tokens_+3A_include_docvars">include_docvars</code></td>
<td>
<p>if <code>TRUE</code>, pass docvars through to the tokens object.
Does not apply when the input is a character data or a list of characters.</p>
</td></tr>
<tr><td><code id="tokens_+3A_padding">padding</code></td>
<td>
<p>if <code>TRUE</code>, leave an empty string where the removed tokens
previously existed.  This is useful if a positional match is needed between
the pre- and post-selected tokens, for instance if a window of adjacency
needs to be computed.</p>
</td></tr>
<tr><td><code id="tokens_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code>, print timing messages to the console</p>
</td></tr>
<tr><td><code id="tokens_+3A_...">...</code></td>
<td>
<p>used to pass arguments among the functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>tokens()</code> works on tokens class objects, which means that the removal rules
can be applied post-tokenization, although it should be noted that it will
not be possible to remove things that are not present.  For instance, if the
<code>tokens</code> object has already had punctuation removed, then <code>tokens(x, remove_punct = TRUE)</code> will have no additional effect.
</p>


<h3>Value</h3>

<p><span class="pkg">quanteda</span> <code>tokens</code> class object, by default a serialized list of
integers corresponding to a vector of types.
</p>


<h3>Details</h3>

<p>As of version 2, the choice of tokenizer is left more to
the user, and <code>tokens()</code> is treated more as a constructor (from a named
list) than a tokenizer. This allows users to use any other tokenizer that
returns a named list, and to use this as an input to <code>tokens()</code>, with
removal and splitting rules applied after this has been constructed (passed
as arguments).  These removal and splitting rules are conservative and will
not remove or split anything, however, unless the user requests it.
</p>
<p>You usually do not want to split hyphenated words or social media tags, but
extra steps required to preserve such special tokens. If there are many
random characters in your texts, you should <code>split_hyphens = TRUE</code> and
<code>split_tags = TRUE</code> to avoid a slowdown in tokenization.
</p>
<p>Using external tokenizers is best done by piping the output from these
other tokenizers into the <code>tokens()</code> constructor, with additional removal
and splitting options applied at the construction stage.  These will only
have an effect, however, if the tokens exist for which removal is specified
at in the <code>tokens()</code> call.  For instance, it is impossible to remove
punctuation if the input list to <code>tokens()</code> already had its punctuation
tokens removed at the external tokenization stage.
</p>
<p>To construct a tokens object from a list with no additional processing,
call <code><a href="#topic+as.tokens">as.tokens()</a></code> instead of <code>tokens()</code>.
</p>
<p>Recommended tokenizers are those from the <span class="pkg">tokenizers</span> package, which
are generally faster than the default (built-in) tokenizer but always
splits infix hyphens, or <span class="pkg">spacyr</span>.  The default tokenizer in
<strong>quanteda</strong> is very smart, however, and if you do not have special
requirements, it works extremely well for most languages as well as text
from social media (including hashtags and usernames).
</p>


<h3>quanteda Tokenizers</h3>

<p>The default word tokenizer <code>what = "word"</code>
splits tokens using <a href="stringi.html#topic+stri_split_boundaries">stri_split_boundaries(x, type = &quot;word&quot;)</a> but by default preserves infix
hyphens (e.g. &quot;self-funding&quot;), URLs, and social media &quot;tag&quot; characters
(#hashtags and @usernames), and email addresses.  The rules defining a
valid &quot;tag&quot; can be found at
https://www.hashtags.org/featured/what-characters-can-a-hashtag-include/
for hashtags and at
https://help.twitter.com/en/managing-your-account/twitter-username-rules
for usernames.
</p>
<p>In versions &lt; 2, the argument <code>remove_twitter</code> controlled whether social
media tags were preserved or removed, even when <code>remove_punct = TRUE</code>. This
argument is not longer functional in versions &gt;= 2.  If greater control
over social media tags is desired, you should user an alternative
tokenizer, including non-<span class="pkg">quanteda</span> options.
</p>
<p>For backward compatibility, the following older tokenizers are also
supported through <code>what</code>: </p>
 <dl>
<dt><code>"word1"</code></dt><dd><p>(legacy) implements
similar behaviour to the version of <code>what = "word"</code> found in pre-version 2.
(It preserves social media tags and infix hyphens, but splits URLs.)
&quot;word1&quot; is also slower than &quot;word&quot;.</p>
</dd> <dt><code>"fasterword"</code></dt><dd><p>(legacy) splits
on whitespace and control characters, using
<code>stringi::stri_split_charclass(x, "[\\p{Z}\\p{C}]+")</code></p>
</dd>
<dt><code>"fastestword"</code></dt><dd><p>(legacy) splits on the space character, using
<code>stringi::stri_split_fixed(x, " ")</code></p>
</dd> <dt><code>"character"</code></dt><dd><p>tokenization into
individual characters</p>
</dd> <dt><code>"sentence"</code></dt><dd><p>sentence segmenter based on
<a href="stringi.html#topic+stri_split_boundaries">stri_split_boundaries</a>, but with
additional rules to avoid splits on words like &quot;Mr.&quot; that would otherwise
incorrectly be detected as sentence boundaries.  For better sentence
tokenization, consider using <span class="pkg">spacyr</span>.</p>
</dd> </dl>

<p>For forward compatibility including use of a more advanced tokenizer that
will be used in major version 4, there is also a &quot;word4&quot; tokenizer that is
even smarter than the default, which is also aliased as &quot;word2&quot; and &quot;word3&quot;
(these are identical).  See <code><a href="#topic+tokenize_word4">tokenize_word4()</a></code> for full details.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tokens_ngrams">tokens_ngrams()</a></code>, <code><a href="#topic+tokens_skipgrams">tokens_skipgrams()</a></code>, <code><a href="#topic+as.list.tokens">as.list.tokens()</a></code>,
<code><a href="#topic+as.tokens">as.tokens()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt &lt;- c(doc1 = "A sentence, showing how tokens() works.",
         doc2 = "@quantedainit and #textanalysis https://example.com?p=123.",
         doc3 = "Self-documenting code??",
         doc4 = "£1,000,000 for 50¢ is gr8 4ever \U0001f600")
tokens(txt)
tokens(txt, what = "word1")

# removing punctuation marks but keeping tags and URLs
tokens(txt[1:2], remove_punct = TRUE)

# splitting hyphenated words
tokens(txt[3])
tokens(txt[3], split_hyphens = TRUE)

# symbols and numbers
tokens(txt[4])
tokens(txt[4], remove_numbers = TRUE)
tokens(txt[4], remove_numbers = TRUE, remove_symbols = TRUE)

## Not run: # using other tokenizers
tokens(tokenizers::tokenize_words(txt[4]), remove_symbols = TRUE)
tokenizers::tokenize_words(txt, lowercase = FALSE, strip_punct = FALSE) %&gt;%
    tokens(remove_symbols = TRUE)
tokenizers::tokenize_characters(txt[3], strip_non_alphanum = FALSE) %&gt;%
    tokens(remove_punct = TRUE)
tokenizers::tokenize_sentences(
    "The quick brown fox.  It jumped over the lazy dog.") %&gt;%
    tokens()

## End(Not run)

</code></pre>

<hr>
<h2 id='tokens_chunk'>Segment tokens object by chunks of a given size</h2><span id='topic+tokens_chunk'></span>

<h3>Description</h3>

<p>Segment tokens into new documents of equally sized token lengths, with the
possibility of overlapping the chunks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_chunk(x, size, overlap = 0, use_docvars = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_chunk_+3A_x">x</code></td>
<td>
<p><a href="#topic+tokens">tokens</a> object whose token elements will be segmented into
chunks</p>
</td></tr>
<tr><td><code id="tokens_chunk_+3A_size">size</code></td>
<td>
<p>integer; the token length of the chunks</p>
</td></tr>
<tr><td><code id="tokens_chunk_+3A_overlap">overlap</code></td>
<td>
<p>integer; the number of tokens in a chunk to be taken from the
last <code>overlap</code> tokens from the preceding chunk</p>
</td></tr>
<tr><td><code id="tokens_chunk_+3A_use_docvars">use_docvars</code></td>
<td>
<p>if <code>TRUE</code>, repeat the docvar values for each chunk;
if <code>FALSE</code>, drop the docvars in the chunked tokens</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+tokens">tokens</a> object whose documents have been split into chunks of
length <code>size</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tokens_segment">tokens_segment()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txts &lt;- c(doc1 = "Fellow citizens, I am again called upon by the voice of
                  my country to execute the functions of its Chief Magistrate.",
          doc2 = "When the occasion proper for it shall arrive, I shall
                  endeavor to express the high sense I entertain of this
                  distinguished honor.")
toks &lt;- tokens(txts)
tokens_chunk(toks, size = 5)
tokens_chunk(toks, size = 5, overlap = 4)
</code></pre>

<hr>
<h2 id='tokens_compound'>Convert token sequences into compound tokens</h2><span id='topic+tokens_compound'></span>

<h3>Description</h3>

<p>Replace multi-token sequences with a multi-word, or &quot;compound&quot; token.  The
resulting compound tokens will represent a phrase or multi-word expression,
concatenated with <code>concatenator</code> (by default, the &quot;<code style="white-space: pre;">&#8288;_&#8288;</code>&quot; character) to form a
single &quot;token&quot;.  This ensures that the sequences will be processed
subsequently as single tokens, for instance in constructing a <a href="#topic+dfm">dfm</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_compound(
  x,
  pattern,
  valuetype = c("glob", "regex", "fixed"),
  concatenator = "_",
  window = 0L,
  case_insensitive = TRUE,
  join = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_compound_+3A_x">x</code></td>
<td>
<p>an input <a href="#topic+tokens">tokens</a> object</p>
</td></tr>
<tr><td><code id="tokens_compound_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="#topic+pattern">pattern</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_compound_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_compound_+3A_concatenator">concatenator</code></td>
<td>
<p>the concatenation character that will connect the words
making up the multi-word sequences.  The default <code style="white-space: pre;">&#8288;_&#8288;</code> is recommended since
it will not be removed during normal cleaning and tokenization (while
nearly all other punctuation characters, at least those in the Unicode
punctuation class <code style="white-space: pre;">&#8288;[P]&#8288;</code> will be removed).</p>
</td></tr>
<tr><td><code id="tokens_compound_+3A_window">window</code></td>
<td>
<p>integer; a vector of length 1 or 2 that specifies size of the
window of tokens adjacent to <code>pattern</code> that will be compounded with matches
to <code>pattern</code>.  The window can be asymmetric if two elements are specified,
with the first giving the window size before <code>pattern</code> and the second the
window size after.  If paddings (empty <code>""</code> tokens) are found, window will
be shrunk to exclude them.</p>
</td></tr>
<tr><td><code id="tokens_compound_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="tokens_compound_+3A_join">join</code></td>
<td>
<p>logical; if <code>TRUE</code>, join overlapping compounds into a single
compound; otherwise, form these separately.  See examples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+tokens">tokens</a> object in which the token sequences matching <code>pattern</code>
have been replaced by new compounded &quot;tokens&quot; joined by the concatenator.
</p>


<h3>Note</h3>

<p>Patterns to be compounded (naturally) consist of multi-word sequences,
and how these are expected in <code>pattern</code> is very specific.  If the elements
to be compounded are supplied as space-delimited elements of a character
vector, wrap the vector in <code><a href="#topic+phrase">phrase()</a></code>.  If the elements to be compounded
are separate elements of a character vector, supply it as a list where each
list element is the sequence of character elements.
</p>
<p>See the examples below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt &lt;- "The United Kingdom is leaving the European Union."
toks &lt;- tokens(txt, remove_punct = TRUE)

# character vector - not compounded
tokens_compound(toks, c("United", "Kingdom", "European", "Union"))

# elements separated by spaces - not compounded
tokens_compound(toks, c("United Kingdom", "European Union"))

# list of characters - is compounded
tokens_compound(toks, list(c("United", "Kingdom"), c("European", "Union")))

# elements separated by spaces, wrapped in phrase() - is compounded
tokens_compound(toks, phrase(c("United Kingdom", "European Union")))

# supplied as values in a dictionary (same as list) - is compounded
# (keys do not matter)
tokens_compound(toks, dictionary(list(key1 = "United Kingdom",
                                      key2 = "European Union")))
# pattern as dictionaries with glob matches
tokens_compound(toks, dictionary(list(key1 = c("U* K*"))), valuetype = "glob")

# note the differences caused by join = FALSE
compounds &lt;- list(c("the", "European"), c("European", "Union"))
tokens_compound(toks, pattern = compounds, join = TRUE)
tokens_compound(toks, pattern = compounds, join = FALSE)

# use window to form ngrams
tokens_remove(toks, pattern = stopwords("en")) %&gt;%
    tokens_compound(pattern = "leav*", join = FALSE, window = c(0, 3))
    
</code></pre>

<hr>
<h2 id='tokens_group'>Combine documents in a tokens object by a grouping variable</h2><span id='topic+tokens_group'></span>

<h3>Description</h3>

<p>Combine documents in a <a href="#topic+tokens">tokens</a> object by a grouping variable, by
concatenating the tokens in the order of the documents within each grouping
variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_group(x, groups = docid(x), fill = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_group_+3A_x">x</code></td>
<td>
<p><a href="#topic+tokens">tokens</a> object</p>
</td></tr>
<tr><td><code id="tokens_group_+3A_groups">groups</code></td>
<td>
<p>grouping variable for sampling, equal in length to the number
of documents. This will be evaluated in the docvars data.frame, so that
docvars may be referred to by name without quoting. This also changes
previous behaviours for <code>groups</code>. See <code>news(Version &gt;= "3.0", package = "quanteda")</code> for details.</p>
</td></tr>
<tr><td><code id="tokens_group_+3A_fill">fill</code></td>
<td>
<p>logical; if <code>TRUE</code> and <code>groups</code> is a factor, then use all levels
of the factor when forming the new documents of the grouped object.  This
will result in a new &quot;document&quot; with empty content for levels not observed,
but for which an empty document may be needed.  If <code>groups</code> is a factor of
dates, for instance, then <code>fill = TRUE</code> ensures that the new object will
consist of one new &quot;document&quot; by date, regardless of whether any documents
previously existed with that date.  Has no effect if the <code>groups</code>
variable(s) are not factors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+tokens">tokens</a> object whose documents are equal to the unique group
combinations, and whose tokens are the concatenations of the tokens by
group. Document-level variables that have no variation within groups are
saved in <a href="#topic+docvars">docvars</a>.  Document-level variables that are lists are dropped
from grouping, even when these exhibit no variation within groups.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corp &lt;- corpus(c("a a b", "a b c c", "a c d d", "a c c d"),
               docvars = data.frame(grp = c("grp1", "grp1", "grp2", "grp2")))
toks &lt;- tokens(corp)
tokens_group(toks, groups = grp)
tokens_group(toks, groups = c(1, 1, 2, 2))

# with fill
tokens_group(toks, groups = factor(c(1, 1, 2, 2), levels = 1:3))
tokens_group(toks, groups = factor(c(1, 1, 2, 2), levels = 1:3), fill = TRUE)
</code></pre>

<hr>
<h2 id='tokens_lookup'>Apply a dictionary to a tokens object</h2><span id='topic+tokens_lookup'></span>

<h3>Description</h3>

<p>Convert tokens into equivalence classes defined by values of a dictionary
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_lookup(
  x,
  dictionary,
  levels = 1:5,
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  capkeys = !exclusive,
  exclusive = TRUE,
  nomatch = NULL,
  nested_scope = c("key", "dictionary"),
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_lookup_+3A_x">x</code></td>
<td>
<p>tokens object to which dictionary or thesaurus will be supplied</p>
</td></tr>
<tr><td><code id="tokens_lookup_+3A_dictionary">dictionary</code></td>
<td>
<p>the <a href="#topic+dictionary">dictionary</a>-class object that will be applied to
<code>x</code></p>
</td></tr>
<tr><td><code id="tokens_lookup_+3A_levels">levels</code></td>
<td>
<p>integers specifying the levels of entries in a hierarchical
dictionary that will be applied.  The top level is 1, and subsequent levels
describe lower nesting levels.  Values may be combined, even if these
levels are not contiguous, e.g. <code>levels = c(1:3)</code> will collapse the second
level into the first, but record the third level (if present) collapsed
below the first (see examples).</p>
</td></tr>
<tr><td><code id="tokens_lookup_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_lookup_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="tokens_lookup_+3A_capkeys">capkeys</code></td>
<td>
<p>if TRUE, convert dictionary keys to uppercase to distinguish
them from other features</p>
</td></tr>
<tr><td><code id="tokens_lookup_+3A_exclusive">exclusive</code></td>
<td>
<p>if <code>TRUE</code>, remove all features not in dictionary,
otherwise, replace values in dictionary with keys while leaving other
features unaffected</p>
</td></tr>
<tr><td><code id="tokens_lookup_+3A_nomatch">nomatch</code></td>
<td>
<p>an optional character naming a new key for tokens that do not
matched to a dictionary values  If <code>NULL</code> (default), do not record
unmatched tokens.</p>
</td></tr>
<tr><td><code id="tokens_lookup_+3A_nested_scope">nested_scope</code></td>
<td>
<p>how to treat matches from different dictionary keys that
are nested.  When one value is nested within another, such as &quot;a b&quot; being
nested within &quot;a b c&quot;, then <code>tokens_lookup()</code> will match the longer.  When
<code>nested_scope = "key"</code>, this longer-match priority is applied only
within the key, while <code>"dictionary"</code> applies it across keys, matching only
the key with the longer pattern, not the matches nested within that longer
pattern from other keys.  See Details.</p>
</td></tr>
<tr><td><code id="tokens_lookup_+3A_verbose">verbose</code></td>
<td>
<p>print status messages if <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Dictionary values may consist of sequences, and there are different
methods of counting key matches based on values that are nested or that
overlap.
</p>
<p>When two different keys in a dictionary are nested matches of one another,
the <code>nested_scope</code> options provide the choice of matching each key's
values independently (the <code>"key"</code>) option, or just counting the
longest match (the <code>"dictionary"</code> option).  Values that are nested
<em>within</em> the same key are always counted as a single match.  See the
last example below comparing the <em>New York</em> and <em>New York Times</em>
for these two different behaviours.
</p>
<p><em>Overlapping values</em>, such as <code>"a b"</code> and <code>"b a"</code> are
currently always considered as separate matches if they are in different
keys, or as one match if the overlap is within the same key.
</p>


<h3>See Also</h3>

<p>tokens_replace
</p>


<h3>Examples</h3>

<pre><code class='language-R'>toks1 &lt;- tokens(data_corpus_inaugural)
dict1 &lt;- dictionary(list(country = "united states",
                   law=c("law*", "constitution"),
                   freedom=c("free*", "libert*")))
dfm(tokens_lookup(toks1, dict1, valuetype = "glob", verbose = TRUE))
dfm(tokens_lookup(toks1, dict1, valuetype = "glob", verbose = TRUE, nomatch = "NONE"))

dict2 &lt;- dictionary(list(country = "united states",
                       law = c("law", "constitution"),
                       freedom = c("freedom", "liberty")))
# dfm(applyDictionary(toks1, dict2, valuetype = "fixed"))
dfm(tokens_lookup(toks1, dict2, valuetype = "fixed"))

# hierarchical dictionary example
txt &lt;- c(d1 = "The United States has the Atlantic Ocean and the Pacific Ocean.",
         d2 = "Britain and Ireland have the Irish Sea and the English Channel.")
toks2 &lt;- tokens(txt)
dict3 &lt;- dictionary(list(US = list(Countries = c("States"),
                                  oceans = c("Atlantic", "Pacific")),
                        Europe = list(Countries = c("Britain", "Ireland"),
                                      oceans = list(west = "Irish Sea",
                                                    east = "English Channel"))))
tokens_lookup(toks2, dict3, levels = 1)
tokens_lookup(toks2, dict3, levels = 2)
tokens_lookup(toks2, dict3, levels = 1:2)
tokens_lookup(toks2, dict3, levels = 3)
tokens_lookup(toks2, dict3, levels = c(1,3))
tokens_lookup(toks2, dict3, levels = c(2,3))

# show unmatched tokens
tokens_lookup(toks2, dict3, nomatch = "_UNMATCHED")

# nested matching differences
dict4 &lt;- dictionary(list(paper = "New York Times", city = "New York"))
toks4 &lt;- tokens("The New York Times is a New York paper.")
tokens_lookup(toks4, dict4, nested_scope = "key", exclusive = FALSE)
tokens_lookup(toks4, dict4, nested_scope = "dictionary", exclusive = FALSE)

</code></pre>

<hr>
<h2 id='tokens_ngrams'>Create n-grams and skip-grams from tokens</h2><span id='topic+tokens_ngrams'></span><span id='topic+char_ngrams'></span><span id='topic+tokens_skipgrams'></span>

<h3>Description</h3>

<p>Create a set of n-grams (tokens in sequence) from already tokenized text
objects, with an optional skip argument to form skip-grams. Both the n-gram
length and the skip lengths take vectors of arguments to form multiple
lengths or skips in one pass.  Implemented in C++ for efficiency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_ngrams(x, n = 2L, skip = 0L, concatenator = "_")

char_ngrams(x, n = 2L, skip = 0L, concatenator = "_")

tokens_skipgrams(x, n, skip, concatenator = "_")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_ngrams_+3A_x">x</code></td>
<td>
<p>a tokens object, or a character vector, or a list of characters</p>
</td></tr>
<tr><td><code id="tokens_ngrams_+3A_n">n</code></td>
<td>
<p>integer vector specifying the number of elements to be concatenated
in each n-gram.  Each element of this vector will define a <code class="reqn">n</code> in the
<code class="reqn">n</code>-gram(s) that are produced.</p>
</td></tr>
<tr><td><code id="tokens_ngrams_+3A_skip">skip</code></td>
<td>
<p>integer vector specifying the adjacency skip size for tokens
forming the n-grams, default is 0 for only immediately neighbouring words.
For <code>skipgrams</code>, <code>skip</code> can be a vector of integers, as the
&quot;classic&quot; approach to forming skip-grams is to set skip = <code class="reqn">k</code> where
<code class="reqn">k</code> is the distance for which <code class="reqn">k</code> or fewer skips are used to
construct the <code class="reqn">n</code>-gram.  Thus a &quot;4-skip-n-gram&quot; defined as <code>skip = 0:4</code> produces results that include 4 skips, 3 skips, 2 skips, 1 skip, and 0
skips (where 0 skips are typical n-grams formed from adjacent words).  See
Guthrie et al (2006).</p>
</td></tr>
<tr><td><code id="tokens_ngrams_+3A_concatenator">concatenator</code></td>
<td>
<p>character for combining words, default is <code style="white-space: pre;">&#8288;_&#8288;</code>
(underscore) character</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Normally, these functions will be called through
<code style="white-space: pre;">&#8288;[tokens](x, ngrams = , ...)&#8288;</code>, but these functions are provided
in case a user wants to perform lower-level n-gram construction on tokenized
texts.
</p>
<p><code><a href="#topic+tokens_skipgrams">tokens_skipgrams()</a></code> is a wrapper to <code><a href="#topic+tokens_ngrams">tokens_ngrams()</a></code> that requires
arguments to be supplied for both <code>n</code> and <code>skip</code>. For <code class="reqn">k</code>-skip
skip-grams, set <code>skip</code> to <code style="white-space: pre;">&#8288;0:&#8288;</code><code class="reqn">k</code>, in order to conform to the
definition of skip-grams found in Guthrie et al (2006): A <code class="reqn">k</code> skip-gram
is an n-gram which is a superset of all n-grams and each <code class="reqn">(k-i)</code>
skip-gram until <code class="reqn">(k-i)==0</code> (which includes 0 skip-grams).
</p>


<h3>Value</h3>

<p>a tokens object consisting a list of character vectors of n-grams, one
list element per text, or a character vector if called on a simple
character vector
</p>


<h3>Note</h3>

<p><code>char_ngrams</code> is a convenience wrapper for a (non-list)
vector of characters, so named to be consistent with <span class="pkg">quanteda</span>'s naming
scheme.
</p>


<h3>References</h3>

<p>Guthrie, David, Ben Allison, Wei Liu, Louise Guthrie, and Yorick Wilks. 2006.
&quot;A Closer Look at Skip-Gram Modelling.&quot; <code style="white-space: pre;">&#8288;https://aclanthology.org/L06-1210/&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ngrams
tokens_ngrams(tokens(c("a b c d e", "c d e f g")), n = 2:3)

toks &lt;- tokens(c(text1 = "the quick brown fox jumped over the lazy dog"))
tokens_ngrams(toks, n = 1:3)
tokens_ngrams(toks, n = c(2,4), concatenator = " ")
tokens_ngrams(toks, n = c(2,4), skip = 1, concatenator = " ")
# on character
char_ngrams(letters[1:3], n = 1:3)

# skipgrams
toks &lt;- tokens("insurgents killed in ongoing fighting")
tokens_skipgrams(toks, n = 2, skip = 0:1, concatenator = " ")
tokens_skipgrams(toks, n = 2, skip = 0:2, concatenator = " ")
tokens_skipgrams(toks, n = 3, skip = 0:2, concatenator = " ")
</code></pre>

<hr>
<h2 id='tokens_recompile'>recompile a serialized tokens object</h2><span id='topic+tokens_recompile'></span>

<h3>Description</h3>

<p>This function recompiles a serialized tokens object when the vocabulary has
been changed in a way that makes some of its types identical, such as
lowercasing when a lowercased version of the type already exists in the type
table, or introduces gaps in the integer map of the types.  It also
re-indexes the types attribute to account for types that may have become
duplicates, through a procedure such as stemming or lowercasing; or the
addition of new tokens through compounding.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_recompile(x, method = c("C++", "R"), gap = TRUE, dup = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_recompile_+3A_x">x</code></td>
<td>
<p>the <a href="#topic+tokens">tokens</a> object to be recompiled</p>
</td></tr>
<tr><td><code id="tokens_recompile_+3A_method">method</code></td>
<td>
<p><code>"C++"</code> for C++ implementation or <code>"R"</code> for an older
R-based method</p>
</td></tr>
<tr><td><code id="tokens_recompile_+3A_gap">gap</code></td>
<td>
<p>if <code>TRUE</code>, remove gaps between token IDs</p>
</td></tr>
<tr><td><code id="tokens_recompile_+3A_dup">dup</code></td>
<td>
<p>if <code>TRUE</code>, merge duplicated token types into the same ID</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># lowercasing
toks1 &lt;- tokens(c(one = "a b c d A B C D",
                 two = "A B C d"))
attr(toks1, "types") &lt;- char_tolower(attr(toks1, "types"))
unclass(toks1)
unclass(quanteda:::tokens_recompile(toks1))

# stemming
toks2 &lt;- tokens("Stemming stemmed many word stems.")
unclass(toks2)
unclass(quanteda:::tokens_recompile(tokens_wordstem(toks2)))

# compounding
toks3 &lt;- tokens("One two three four.")
unclass(toks3)
unclass(tokens_compound(toks3, "two three"))

# lookup
dict &lt;- dictionary(list(test = c("one", "three")))
unclass(tokens_lookup(toks3, dict))

# empty pads
unclass(tokens_select(toks3, dict))
unclass(tokens_select(toks3, dict, pad = TRUE))

# ngrams
unclass(tokens_ngrams(toks3, n = 2:3))

</code></pre>

<hr>
<h2 id='tokens_replace'>Replace tokens in a tokens object</h2><span id='topic+tokens_replace'></span>

<h3>Description</h3>

<p>Substitute token types based on vectorized one-to-one matching. Since this
function is created for lemmatization or user-defined stemming. It supports
substitution of multi-word features by multi-word features, but substitution
is fastest when <code>pattern</code> and <code>replacement</code> are character vectors
and <code>valuetype = "fixed"</code> as the function only substitute types of
tokens. Please use <code><a href="#topic+tokens_lookup">tokens_lookup()</a></code> with <code>exclusive = FALSE</code>
to replace <a href="#topic+dictionary">dictionary</a> values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_replace(
  x,
  pattern,
  replacement,
  valuetype = "glob",
  case_insensitive = TRUE,
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_replace_+3A_x">x</code></td>
<td>
<p><a href="#topic+tokens">tokens</a> object whose token elements will be replaced</p>
</td></tr>
<tr><td><code id="tokens_replace_+3A_pattern">pattern</code></td>
<td>
<p>a character vector or list of character vectors.  See
<a href="#topic+pattern">pattern</a> for more details.</p>
</td></tr>
<tr><td><code id="tokens_replace_+3A_replacement">replacement</code></td>
<td>
<p>a character vector or (if <code>pattern</code> is a list) list
of character vectors of the same length as <code>pattern</code></p>
</td></tr>
<tr><td><code id="tokens_replace_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_replace_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="tokens_replace_+3A_verbose">verbose</code></td>
<td>
<p>print status messages if <code>TRUE</code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p>tokens_lookup
</p>


<h3>Examples</h3>

<pre><code class='language-R'>toks1 &lt;- tokens(data_corpus_inaugural, remove_punct = TRUE)

# lemmatization
taxwords &lt;- c("tax", "taxing", "taxed", "taxed", "taxation")
lemma &lt;- rep("TAX", length(taxwords))
toks2 &lt;- tokens_replace(toks1, taxwords, lemma, valuetype = "fixed")
kwic(toks2, "TAX") %&gt;% 
    tail(10)

# stemming
type &lt;- types(toks1)
stem &lt;- char_wordstem(type, "porter")
toks3 &lt;- tokens_replace(toks1, type, stem, valuetype = "fixed", case_insensitive = FALSE)
identical(toks3, tokens_wordstem(toks1, "porter"))

# multi-multi substitution
toks4 &lt;- tokens_replace(toks1, phrase(c("Supreme Court")),
                        phrase(c("Supreme Court of the United States")))
kwic(toks4, phrase(c("Supreme Court of the United States")))
</code></pre>

<hr>
<h2 id='tokens_restore'>Restore special tokens</h2><span id='topic+tokens_restore'></span>

<h3>Description</h3>

<p>Compounds segments of tokens marked by special markers. The beginning and
the end of the segments should be marked by U+E001 and U+E002 respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_restore(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_restore_+3A_x">x</code></td>
<td>
<p>tokens object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a modified tokens object
</p>

<hr>
<h2 id='tokens_sample'>Randomly sample documents from a tokens object</h2><span id='topic+tokens_sample'></span>

<h3>Description</h3>

<p>Take a random sample of documents of the specified size from a corpus, with
or without replacement, optionally by grouping variables or with probability
weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_sample(x, size = NULL, replace = FALSE, prob = NULL, by = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_sample_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+tokens">tokens</a> object whose documents will be sampled</p>
</td></tr>
<tr><td><code id="tokens_sample_+3A_size">size</code></td>
<td>
<p>a positive number, the number of documents to select; when used
with <code>by</code>, the number to select from each group or a vector equal in
length to the number of groups defining the samples to be chosen in each
category of <code>by</code>.  By defining a size larger than the number of documents,
it is possible to oversample when <code>replace = TRUE</code>.</p>
</td></tr>
<tr><td><code id="tokens_sample_+3A_replace">replace</code></td>
<td>
<p>if <code>TRUE</code>, sample  with replacement</p>
</td></tr>
<tr><td><code id="tokens_sample_+3A_prob">prob</code></td>
<td>
<p>a vector of probability weights for obtaining the elements of the
vector being sampled.  May not be applied when <code>by</code> is used.</p>
</td></tr>
<tr><td><code id="tokens_sample_+3A_by">by</code></td>
<td>
<p>optional grouping variable for sampling.  This will be evaluated in
the docvars data.frame, so that docvars may be referred to by name without
quoting.  This also changes previous behaviours for <code>by</code>.
See <code>news(Version &gt;= "2.9", package = "quanteda")</code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+tokens">tokens</a> object (re)sampled on the documents, containing the document
variables for the documents sampled.
</p>


<h3>See Also</h3>

<p><a href="base.html#topic+sample">sample</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
toks &lt;- tokens(data_corpus_inaugural[1:6])
toks
tokens_sample(toks)
tokens_sample(toks, replace = TRUE) %&gt;% docnames()
tokens_sample(toks, size = 3, replace = TRUE) %&gt;% docnames()

# sampling using by
docvars(toks)
tokens_sample(toks, size = 2, replace = TRUE, by = Party) %&gt;% docnames()

</code></pre>

<hr>
<h2 id='tokens_segment'>Segment tokens object by patterns</h2><span id='topic+tokens_segment'></span>

<h3>Description</h3>

<p>Segment tokens by splitting on a pattern match. This is useful for breaking
the tokenized texts into smaller document units, based on a regular pattern
or a user-supplied annotation. While it normally makes more sense to do this
at the corpus level (see <code><a href="#topic+corpus_segment">corpus_segment()</a></code>), <code>tokens_segment</code>
provides the option to perform this operation on tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_segment(
  x,
  pattern,
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  extract_pattern = FALSE,
  pattern_position = c("before", "after"),
  use_docvars = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_segment_+3A_x">x</code></td>
<td>
<p><a href="#topic+tokens">tokens</a> object whose token elements will be segmented</p>
</td></tr>
<tr><td><code id="tokens_segment_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="#topic+pattern">pattern</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_segment_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_segment_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="tokens_segment_+3A_extract_pattern">extract_pattern</code></td>
<td>
<p>remove matched patterns from the texts and save in
<a href="#topic+docvars">docvars</a>, if <code>TRUE</code></p>
</td></tr>
<tr><td><code id="tokens_segment_+3A_pattern_position">pattern_position</code></td>
<td>
<p>either <code>"before"</code> or <code>"after"</code>, depending
on whether the pattern precedes the text (as with a tag) or follows the
text (as with punctuation delimiters)</p>
</td></tr>
<tr><td><code id="tokens_segment_+3A_use_docvars">use_docvars</code></td>
<td>
<p>if <code>TRUE</code>, repeat the docvar values for each
segmented text; if <code>FALSE</code>, drop the docvars in the segmented corpus.
Dropping the docvars might be useful in order to conserve space or if these
are not desired for the segmented corpus.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>tokens_segment</code> returns a <a href="#topic+tokens">tokens</a> object whose documents
have been split by patterns
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txts &lt;- "Fellow citizens, I am again called upon by the voice of my country to
execute the functions of its Chief Magistrate. When the occasion proper for
it shall arrive, I shall endeavor to express the high sense I entertain of
this distinguished honor."
toks &lt;- tokens(txts)

# split by any punctuation
tokens_segment(toks, "^\\p{Sterm}$", valuetype = "regex",
               extract_pattern = TRUE,
               pattern_position = "after")
tokens_segment(toks, c(".", "?", "!"), valuetype = "fixed",
               extract_pattern = TRUE,
               pattern_position = "after")
</code></pre>

<hr>
<h2 id='tokens_select'>Select or remove tokens from a tokens object</h2><span id='topic+tokens_select'></span><span id='topic+tokens_remove'></span><span id='topic+tokens_keep'></span>

<h3>Description</h3>

<p>These function select or discard tokens from a <a href="#topic+tokens">tokens</a> object.  For
convenience, the functions <code>tokens_remove</code> and <code>tokens_keep</code> are defined as
shortcuts for <code>tokens_select(x, pattern, selection = "remove")</code> and
<code>tokens_select(x, pattern, selection = "keep")</code>, respectively.  The most
common usage for <code>tokens_remove</code> will be to eliminate stop words from a text
or text-based object, while the most common use of <code>tokens_select</code> will be to
select tokens with only positive pattern matches from a list of regular
expressions, including a dictionary. <code>startpos</code> and <code>endpos</code> determine the
positions of tokens searched for <code>pattern</code> and areas affected are expanded by
<code>window</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_select(
  x,
  pattern,
  selection = c("keep", "remove"),
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  padding = FALSE,
  window = 0,
  min_nchar = NULL,
  max_nchar = NULL,
  startpos = 1L,
  endpos = -1L,
  verbose = quanteda_options("verbose")
)

tokens_remove(x, ...)

tokens_keep(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_select_+3A_x">x</code></td>
<td>
<p><a href="#topic+tokens">tokens</a> object whose token elements will be removed or kept</p>
</td></tr>
<tr><td><code id="tokens_select_+3A_pattern">pattern</code></td>
<td>
<p>a character vector, list of character vectors, <a href="#topic+dictionary">dictionary</a>,
or collocations object.  See <a href="#topic+pattern">pattern</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_select_+3A_selection">selection</code></td>
<td>
<p>whether to <code>"keep"</code> or <code>"remove"</code> the tokens matching
<code>pattern</code></p>
</td></tr>
<tr><td><code id="tokens_select_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_select_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
<tr><td><code id="tokens_select_+3A_padding">padding</code></td>
<td>
<p>if <code>TRUE</code>, leave an empty string where the removed tokens
previously existed.  This is useful if a positional match is needed between
the pre- and post-selected tokens, for instance if a window of adjacency
needs to be computed.</p>
</td></tr>
<tr><td><code id="tokens_select_+3A_window">window</code></td>
<td>
<p>integer of length 1 or 2; the size of the window of tokens
adjacent to <code>pattern</code> that will be selected. The window is symmetric unless
a vector of two elements is supplied, in which case the first element will
be the token length of the window before <code>pattern</code>, and the second will be
the token length of the window after <code>pattern</code>. The default is <code>0</code>, meaning
that only the pattern matched token(s) are selected, with no adjacent
terms.
</p>
<p>Terms from overlapping windows are never double-counted, but simply
returned in the pattern match. This is because <code>tokens_select</code> never
redefines the document units; for this, see <code><a href="#topic+kwic">kwic()</a></code>.</p>
</td></tr>
<tr><td><code id="tokens_select_+3A_min_nchar">min_nchar</code>, <code id="tokens_select_+3A_max_nchar">max_nchar</code></td>
<td>
<p>optional numerics specifying the minimum and
maximum length in characters for tokens to be removed or kept; defaults are
<code>NULL</code> for no limits.  These are applied after (and hence, in addition to)
any selection based on pattern matches.</p>
</td></tr>
<tr><td><code id="tokens_select_+3A_startpos">startpos</code>, <code id="tokens_select_+3A_endpos">endpos</code></td>
<td>
<p>integer; position of tokens in documents where pattern
matching starts and ends, where 1 is the first token in a document.  For
negative indexes, counting starts at the ending token of the document, so
that -1 denotes the last token in the document, -2 the second to last, etc.
When the length of the vector is equal to <code>ndoc</code>, tokens in corresponding
positions will be selected; when it is less than <code>ndoc</code>, values are
repeated to make them equal in length.</p>
</td></tr>
<tr><td><code id="tokens_select_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code> print messages about how many tokens were selected
or removed</p>
</td></tr>
<tr><td><code id="tokens_select_+3A_...">...</code></td>
<td>
<p>additional arguments passed by <code>tokens_remove</code> and
<code>tokens_keep</code> to <code>tokens_select</code>. Cannot include
<code>selection</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+tokens">tokens</a> object with tokens selected or removed based on their
match to <code>pattern</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## tokens_select with simple examples
toks &lt;- as.tokens(list(letters, LETTERS))
tokens_select(toks, c("b", "e", "f"), selection = "keep", padding = FALSE)
tokens_select(toks, c("b", "e", "f"), selection = "keep", padding = TRUE)
tokens_select(toks, c("b", "e", "f"), selection = "remove", padding = FALSE)
tokens_select(toks, c("b", "e", "f"), selection = "remove", padding = TRUE)

# how case_insensitive works
tokens_select(toks, c("b", "e", "f"), selection = "remove", case_insensitive = TRUE)
tokens_select(toks, c("b", "e", "f"), selection = "remove", case_insensitive = FALSE)

# use window
tokens_select(toks, c("b", "f"), selection = "keep", window = 1)
tokens_select(toks, c("b", "f"), selection = "remove", window = 1)
tokens_remove(toks, c("b", "f"), window = c(0, 1))
tokens_select(toks, pattern = c("e", "g"), window = c(1, 2))

# tokens_remove example: remove stopwords
txt &lt;- c(wash1 &lt;- "Fellow citizens, I am again called upon by the voice of my
                   country to execute the functions of its Chief Magistrate.",
         wash2 &lt;- "When the occasion proper for it shall arrive, I shall
                   endeavor to express the high sense I entertain of this
                   distinguished honor.")
tokens_remove(tokens(txt, remove_punct = TRUE), stopwords("english"))

# token_keep example: keep two-letter words
tokens_keep(tokens(txt, remove_punct = TRUE), "??")

</code></pre>

<hr>
<h2 id='tokens_split'>Split tokens by a separator pattern</h2><span id='topic+tokens_split'></span>

<h3>Description</h3>

<p>Replaces tokens by multiple replacements consisting of elements split by a
separator pattern, with the option of retaining the separator.  This function
effectively reverses the operation of <code><a href="#topic+tokens_compound">tokens_compound()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_split(
  x,
  separator = " ",
  valuetype = c("fixed", "regex"),
  remove_separator = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_split_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+tokens">tokens</a> object</p>
</td></tr>
<tr><td><code id="tokens_split_+3A_separator">separator</code></td>
<td>
<p>a single-character pattern match by which tokens are separated</p>
</td></tr>
<tr><td><code id="tokens_split_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="tokens_split_+3A_remove_separator">remove_separator</code></td>
<td>
<p>if <code>TRUE</code>, remove separator from new tokens</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># undo tokens_compound()
toks1 &lt;- tokens("pork barrel is an idiomatic multi-word expression")
tokens_compound(toks1, phrase("pork barrel"))
tokens_compound(toks1, phrase("pork barrel")) %&gt;%
    tokens_split(separator = "_")
    
# similar to tokens(x, remove_hyphen = TRUE) but post-tokenization 
toks2 &lt;- tokens("UK-EU negotiation is not going anywhere as of 2018-12-24.")
tokens_split(toks2, separator = "-", remove_separator = FALSE)
</code></pre>

<hr>
<h2 id='tokens_subset'>Extract a subset of a tokens</h2><span id='topic+tokens_subset'></span>

<h3>Description</h3>

<p>Returns document subsets of a tokens that meet certain conditions, including
direct logical operations on docvars (document-level variables).
<code>tokens_subset()</code> functions identically to <code><a href="base.html#topic+subset.data.frame">subset.data.frame()</a></code>, using
non-standard evaluation to evaluate conditions based on the <a href="#topic+docvars">docvars</a> in the
tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_subset(x, subset, drop_docid = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_subset_+3A_x">x</code></td>
<td>
<p><a href="#topic+tokens">tokens</a> object to be subsetted</p>
</td></tr>
<tr><td><code id="tokens_subset_+3A_subset">subset</code></td>
<td>
<p>logical expression indicating the documents to keep: missing
values are taken as false</p>
</td></tr>
<tr><td><code id="tokens_subset_+3A_drop_docid">drop_docid</code></td>
<td>
<p>if <code>TRUE</code>, <code>docid</code> for documents are removed as the result
of subsetting.</p>
</td></tr>
<tr><td><code id="tokens_subset_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="#topic+tokens">tokens</a> object, with a subset of documents (and docvars)
selected according to arguments
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+subset.data.frame">subset.data.frame()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>corp &lt;- corpus(c(d1 = "a b c d", d2 = "a a b e",
                 d3 = "b b c e", d4 = "e e f a b"),
                 docvars = data.frame(grp = c(1, 1, 2, 3)))
toks &lt;- tokens(corp)
# selecting on a docvars condition
tokens_subset(toks, grp &gt; 1)
# selecting on a supplied vector
tokens_subset(toks, c(TRUE, FALSE, TRUE, FALSE))
</code></pre>

<hr>
<h2 id='tokens_tolower'>Convert the case of tokens</h2><span id='topic+tokens_tolower'></span><span id='topic+tokens_toupper'></span>

<h3>Description</h3>

<p><code>tokens_tolower()</code> and <code>tokens_toupper()</code> convert the features of a
<a href="#topic+tokens">tokens</a> object and re-index the types.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_tolower(x, keep_acronyms = FALSE)

tokens_toupper(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_tolower_+3A_x">x</code></td>
<td>
<p>the input object whose character/tokens/feature elements will be
case-converted</p>
</td></tr>
<tr><td><code id="tokens_tolower_+3A_keep_acronyms">keep_acronyms</code></td>
<td>
<p>logical; if <code>TRUE</code>, do not lowercase any
all-uppercase words (applies only to <code style="white-space: pre;">&#8288;*_tolower()&#8288;</code> functions)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># for a document-feature matrix
toks &lt;- tokens(c(txt1 = "b A A", txt2 = "C C a b B"))
tokens_tolower(toks)
tokens_toupper(toks)
</code></pre>

<hr>
<h2 id='tokens_wordstem'>Stem the terms in an object</h2><span id='topic+tokens_wordstem'></span><span id='topic+char_wordstem'></span><span id='topic+dfm_wordstem'></span>

<h3>Description</h3>

<p>Apply a stemmer to words.  This is a wrapper to <a href="SnowballC.html#topic+wordStem">wordStem</a>
designed to allow this function to be called without loading the entire
<span class="pkg">SnowballC</span> package.  <a href="SnowballC.html#topic+wordStem">wordStem</a>  uses Martin Porter's
stemming algorithm and the C libstemmer library generated by Snowball.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_wordstem(x, language = quanteda_options("language_stemmer"))

char_wordstem(
  x,
  language = quanteda_options("language_stemmer"),
  check_whitespace = TRUE
)

dfm_wordstem(x, language = quanteda_options("language_stemmer"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_wordstem_+3A_x">x</code></td>
<td>
<p>a character, tokens, or dfm object whose word stems are to be
removed.  If tokenized texts, the tokenization must be word-based.</p>
</td></tr>
<tr><td><code id="tokens_wordstem_+3A_language">language</code></td>
<td>
<p>the name of a recognized language, as returned by
<a href="SnowballC.html#topic+getStemLanguages">getStemLanguages</a>, or a two- or three-letter ISO-639 code
corresponding to one of these languages (see references for the list of
codes)</p>
</td></tr>
<tr><td><code id="tokens_wordstem_+3A_check_whitespace">check_whitespace</code></td>
<td>
<p>logical; if <code>TRUE</code>, stop with a warning when trying
to stem inputs containing whitespace</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>tokens_wordstem</code> returns a <a href="#topic+tokens">tokens</a> object whose word
types have been stemmed.
</p>
<p><code>char_wordstem</code> returns a <a href="base.html#topic+character">character</a> object whose word
types have been stemmed.
</p>
<p><code>dfm_wordstem</code> returns a <a href="#topic+dfm">dfm</a> object whose word
types (features) have been stemmed, and recombined to consolidate features made
equivalent because of stemming.
</p>


<h3>References</h3>

<p><a href="http://snowball.tartarus.org/">http://snowball.tartarus.org/</a>
</p>
<p><a href="http://www.iso.org/iso/home/standards/language_codes.htm">http://www.iso.org/iso/home/standards/language_codes.htm</a> for the
ISO-639 language codes
</p>


<h3>See Also</h3>

<p><a href="SnowballC.html#topic+wordStem">wordStem</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example applied to tokens
txt &lt;- c(one = "eating eater eaters eats ate",
         two = "taxing taxes taxed my tax return")
th &lt;- tokens(txt)
tokens_wordstem(th)

# simple example
char_wordstem(c("win", "winning", "wins", "won", "winner"))

# example applied to a dfm
(origdfm &lt;- dfm(tokens(txt)))
dfm_wordstem(origdfm)

</code></pre>

<hr>
<h2 id='tokens-class'>Base method extensions for tokens objects</h2><span id='topic+tokens-class'></span><span id='topic+unlist.tokens'></span><span id='topic++5B.tokens'></span><span id='topic++2B.tokens'></span><span id='topic+c.tokens'></span>

<h3>Description</h3>

<p>Extensions of base R functions for tokens objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tokens'
unlist(x, recursive = FALSE, use.names = TRUE)

## S3 method for class 'tokens'
x[i, drop_docid = TRUE]

## S3 method for class 'tokens'
t1 + t2

## S3 method for class 'tokens'
c(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens-class_+3A_x">x</code></td>
<td>
<p>a tokens object</p>
</td></tr>
<tr><td><code id="tokens-class_+3A_recursive">recursive</code></td>
<td>
<p>a required argument for <a href="base.html#topic+unlist">unlist</a> but inapplicable to
<a href="#topic+tokens">tokens</a> objects</p>
</td></tr>
<tr><td><code id="tokens-class_+3A_i">i</code></td>
<td>
<p>document names or indices for documents to extract.</p>
</td></tr>
<tr><td><code id="tokens-class_+3A_drop_docid">drop_docid</code></td>
<td>
<p>if <code>TRUE</code>, <code>docid</code> for documents are removed as the result
of extraction.</p>
</td></tr>
<tr><td><code id="tokens-class_+3A_t1">t1</code></td>
<td>
<p>tokens one to be added</p>
</td></tr>
<tr><td><code id="tokens-class_+3A_t2">t2</code></td>
<td>
<p>tokens two to be added</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>unlist</code> returns a simple vector of characters from a
<a href="#topic+tokens">tokens</a> object.
</p>
<p><code>c(...)</code> and <code>+</code> return a tokens object whose documents
have been added as a single sequence of documents.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>toks &lt;- tokens(c(d1 = "one two three", d2 = "four five six", d3 = "seven eight"))
str(toks)
toks[c(1,3)]
# combining tokens
toks1 &lt;- tokens(c(doc1 = "a b c d e", doc2 = "f g h"))
toks2 &lt;- tokens(c(doc3 = "1 2 3"))
toks1 + toks2
c(toks1, toks2)

</code></pre>

<hr>
<h2 id='topfeatures'>Identify the most frequent features in a dfm</h2><span id='topic+topfeatures'></span>

<h3>Description</h3>

<p>List the most (or least) frequently occurring features in a <a href="#topic+dfm">dfm</a>, either
as a whole or separated by document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topfeatures(
  x,
  n = 10,
  decreasing = TRUE,
  scheme = c("count", "docfreq"),
  groups = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="topfeatures_+3A_x">x</code></td>
<td>
<p>the object whose features will be returned</p>
</td></tr>
<tr><td><code id="topfeatures_+3A_n">n</code></td>
<td>
<p>how many top features should be returned</p>
</td></tr>
<tr><td><code id="topfeatures_+3A_decreasing">decreasing</code></td>
<td>
<p>If <code>TRUE</code>, return the <code>n</code> most frequent features;
otherwise return the <code>n</code> least frequent features</p>
</td></tr>
<tr><td><code id="topfeatures_+3A_scheme">scheme</code></td>
<td>
<p>one of <code>count</code> for total feature frequency (within
<code>group</code> if applicable), or <code>docfreq</code> for the document frequencies
of features</p>
</td></tr>
<tr><td><code id="topfeatures_+3A_groups">groups</code></td>
<td>
<p>grouping variable for sampling, equal in length to the number
of documents. This will be evaluated in the docvars data.frame, so that
docvars may be referred to by name without quoting. This also changes
previous behaviours for <code>groups</code>. See <code>news(Version &gt;= "3.0", package = "quanteda")</code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named numeric vector of feature counts, where the names are the
feature labels, or a list of these if <code>groups</code> is given.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dfmat1 &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1980) %&gt;%
    tokens(remove_punct = TRUE) %&gt;%
    dfm()
dfmat2 &lt;- dfm_remove(dfmat1, stopwords("en"))

# most frequent features
topfeatures(dfmat1)
topfeatures(dfmat2)

# least frequent features
topfeatures(dfmat2, decreasing = FALSE)

# top features of individual documents
topfeatures(dfmat2, n = 5, groups = docnames(dfmat2))

# grouping by president last name
topfeatures(dfmat2, n = 5, groups = President)

# features by document frequencies
tail(topfeatures(dfmat1, scheme = "docfreq", n = 200))
</code></pre>

<hr>
<h2 id='types'>Get word types from a tokens object</h2><span id='topic+types'></span>

<h3>Description</h3>

<p>Get unique types of tokens from a <a href="#topic+tokens">tokens</a> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>types(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="types_+3A_x">x</code></td>
<td>
<p>a tokens object</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+featnames">featnames</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>toks &lt;- tokens(data_corpus_inaugural)
types(toks)
</code></pre>

<hr>
<h2 id='unlist_character'>Unlist a list of character vectors safely</h2><span id='topic+unlist_character'></span>

<h3>Description</h3>

<p>Unlist a list of character vectors safely
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unlist_character(x, unique = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unlist_character_+3A_x">x</code></td>
<td>
<p>a list of integers</p>
</td></tr>
<tr><td><code id="unlist_character_+3A_unique">unique</code></td>
<td>
<p>if <code>TRUE</code> remove duplicated elements</p>
</td></tr>
<tr><td><code id="unlist_character_+3A_...">...</code></td>
<td>
<p>passed to <code>unlist</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>character vector
</p>

<hr>
<h2 id='unlist_integer'>Unlist a list of integer vectors safely</h2><span id='topic+unlist_integer'></span>

<h3>Description</h3>

<p>Unlist a list of integer vectors safely
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unlist_integer(x, unique = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unlist_integer_+3A_x">x</code></td>
<td>
<p>a list of integers</p>
</td></tr>
<tr><td><code id="unlist_integer_+3A_unique">unique</code></td>
<td>
<p>if <code>TURE</code> remove duplicated elements</p>
</td></tr>
<tr><td><code id="unlist_integer_+3A_...">...</code></td>
<td>
<p>passed to <code>unlist</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>integer vector
</p>

<hr>
<h2 id='valuetype'>Pattern matching using valuetype</h2><span id='topic+valuetype'></span><span id='topic+case_insensitive'></span>

<h3>Description</h3>

<p>Pattern matching in <span class="pkg">quanteda</span> using the <code>valuetype</code> argument.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="valuetype_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="valuetype_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="#topic+dictionary">dictionary</a> values</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Pattern matching in in <span class="pkg">quanteda</span> uses &quot;glob&quot;-style pattern
matching as the default, because this is simpler than regular expression
matching while addressing most users' needs.  It is also has the advantage
of being identical to fixed pattern matching when the wildcard characters
(<code>*</code> and <code style="white-space: pre;">&#8288;?&#8288;</code>) are not used. Finally, most <a href="#topic+dictionary">dictionary</a> formats use glob
matching.
</p>
 <dl>
<dt><code>"glob"</code></dt><dd><p>&quot;glob&quot;-style wildcard expressions, the quanteda
default. The implementation used in <span class="pkg">quanteda</span> uses <code>*</code> to match any
number of any characters including none, and <code style="white-space: pre;">&#8288;?&#8288;</code> to match any single
character.  See also <code><a href="utils.html#topic+glob2rx">utils::glob2rx()</a></code> and References below.</p>
</dd>
<dt><code>"regex"</code></dt><dd><p>Regular expression matching.</p>
</dd> <dt><code>"fixed"</code></dt><dd><p>Fixed
(literal) pattern matching.</p>
</dd> </dl>



<h3>Note</h3>

<p>If &quot;fixed&quot; is used with <code>case_insensitive = TRUE</code>, features will
typically be lowercased internally prior to matching.  Also, glob matches
are converted to regular expressions (using <a href="utils.html#topic+glob2rx">glob2rx</a>) when
they contain wild card characters, and to fixed pattern matches when they
do not.
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+glob2rx">utils::glob2rx()</a></code>, <a href="https://en.wikipedia.org/wiki/Glob_(programming)">glob pattern matching (Wikipedia)</a>,
<code><a href="stringi.html#topic+about_search_regex">stringi::stringi-search-regex()</a></code>, <code><a href="stringi.html#topic+about_search_fixed">stringi::stringi-search-fixed()</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
