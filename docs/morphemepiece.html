<!DOCTYPE html><html><head><title>Help for package morphemepiece</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {morphemepiece}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.infer_case_from_vocab'><p>Determine Vocabulary Casedness</p></a></li>
<li><a href='#.mp_tokenize_single_string'><p>Tokenize an Input Word-by-word</p></a></li>
<li><a href='#.mp_tokenize_word'><p>Tokenize a Word</p></a></li>
<li><a href='#.mp_tokenize_word_bidir'><p>Tokenize a Word Bidirectionally</p></a></li>
<li><a href='#.mp_tokenize_word_lookup'><p>Tokenize a Word Including Lookup</p></a></li>
<li><a href='#.new_morphemepiece_vocabulary'><p>Constructor for Class morphemepiece_vocabulary</p></a></li>
<li><a href='#.process_mp_vocab'><p>Process a Morphemepiece Vocabulary for Tokenization</p></a></li>
<li><a href='#.validate_morphemepiece_vocabulary'><p>Validator for Objects of Class morphemepiece_vocabulary</p></a></li>
<li><a href='#load_lookup'><p>Load a morphemepiece lookup file</p></a></li>
<li><a href='#load_or_retrieve_lookup'><p>Load a lookup file, or retrieve from cache</p></a></li>
<li><a href='#load_or_retrieve_vocab'><p>Load a vocabulary file, or retrieve from cache</p></a></li>
<li><a href='#load_vocab'><p>Load a vocabulary file</p></a></li>
<li><a href='#morphemepiece_cache_dir'><p>Retrieve Directory for Morphemepiece Cache</p></a></li>
<li><a href='#morphemepiece_tokenize'><p>Tokenize Sequence with Morpheme Pieces</p></a></li>
<li><a href='#morphemepiece-package'><p>morphemepiece: Morpheme Tokenization</p></a></li>
<li><a href='#prepare_vocab'><p>Format a Token List as a Vocabulary</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#set_morphemepiece_cache_dir'><p>Set a Cache Directory for Morphemepiece</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Morpheme Tokenization</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Tokenize text into morphemes. The morphemepiece algorithm uses a 
  lookup table to determine the morpheme breakdown of words, and falls back on a 
  modified wordpiece tokenization algorithm for words not found in the lookup 
  table.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/macmillancontentscience/morphemepiece">https://github.com/macmillancontentscience/morphemepiece</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/macmillancontentscience/morphemepiece/issues">https://github.com/macmillancontentscience/morphemepiece/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Imports:</td>
<td>dlr (&ge; 1.0.0), fastmatch, magrittr, memoise (&ge; 2.0.0),
morphemepiece.data, piecemaker (&ge; 1.0.0), purrr (&ge; 0.3.4),
readr, rlang, stringr (&ge; 1.4.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>dplyr, fs, ggplot2, here, knitr, remotes, rmarkdown, testthat
(&ge; 3.0.0), utils</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-04-16 13:57:47 UTC; jonathan.bratt</td>
</tr>
<tr>
<td>Author:</td>
<td>Jonathan Bratt <a href="https://orcid.org/0000-0003-2859-0076"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Jon Harmon <a href="https://orcid.org/0000-0003-4781-4346"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Bedford Freeman &amp; Worth Pub Grp LLC DBA Macmillan Learning [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jonathan Bratt &lt;jonathan.bratt@macmillan.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-04-16 14:12:29 UTC</td>
</tr>
</table>
<hr>
<h2 id='.infer_case_from_vocab'>Determine Vocabulary Casedness</h2><span id='topic+.infer_case_from_vocab'></span>

<h3>Description</h3>

<p>Determine whether or not a wordpiece vocabulary is case-sensitive.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.infer_case_from_vocab(vocab)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".infer_case_from_vocab_+3A_vocab">vocab</code></td>
<td>
<p>The vocabulary as a character vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If none of the tokens in the vocabulary start with a capital letter, it will
be assumed to be uncased. Note that tokens like &quot;\[CLS\]&quot; contain uppercase
letters, but don't start with uppercase letters.
</p>


<h3>Value</h3>

<p>TRUE if the vocabulary is cased, FALSE if uncased.
</p>

<hr>
<h2 id='.mp_tokenize_single_string'>Tokenize an Input Word-by-word</h2><span id='topic+.mp_tokenize_single_string'></span>

<h3>Description</h3>

<p>Tokenize an Input Word-by-word
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.mp_tokenize_single_string(words, vocab, lookup, unk_token, max_chars)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".mp_tokenize_single_string_+3A_words">words</code></td>
<td>
<p>Character; a vector of words (generated by space-tokenizing a
single input).</p>
</td></tr>
<tr><td><code id=".mp_tokenize_single_string_+3A_vocab">vocab</code></td>
<td>
<p>A morphemepiece vocabulary.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_single_string_+3A_lookup">lookup</code></td>
<td>
<p>A morphemepiece lookup table.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_single_string_+3A_unk_token">unk_token</code></td>
<td>
<p>Token to represent unknown words.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_single_string_+3A_max_chars">max_chars</code></td>
<td>
<p>Maximum length of word recognized.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named integer vector of tokenized words.
</p>

<hr>
<h2 id='.mp_tokenize_word'>Tokenize a Word</h2><span id='topic+.mp_tokenize_word'></span>

<h3>Description</h3>

<p>Tokenize a single &quot;word&quot; (no whitespace). The word can technically contain
punctuation, but typically punctuation has been split off by this point.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.mp_tokenize_word(
  word,
  vocab_split,
  dir = 1,
  allow_compounds = TRUE,
  unk_token = "[UNK]",
  max_chars = 100
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".mp_tokenize_word_+3A_word">word</code></td>
<td>
<p>Word to tokenize.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_+3A_vocab_split">vocab_split</code></td>
<td>
<p>List of character vectors containing vocabulary words.
Should have components named &quot;prefixes&quot;, &quot;words&quot;, &quot;suffixes&quot;.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_+3A_dir">dir</code></td>
<td>
<p>Integer; if 1 (the default), look for tokens starting at the
beginning of the word. Otherwise, start at the end.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_+3A_allow_compounds">allow_compounds</code></td>
<td>
<p>Logical; whether to allow multiple whole words in the
breakdown.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_+3A_unk_token">unk_token</code></td>
<td>
<p>Token to represent unknown words.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_+3A_max_chars">max_chars</code></td>
<td>
<p>Maximum length of word recognized.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is an adaptation of wordpiece:::.tokenize_word. The main differences are
that it was designed to work with a morphemepiece vocabulary, which can
include prefixes (denoted like &quot;pre##&quot;). As in wordpiece, the algorithm uses
a repeated greedy search for the largest piece from the vocabulary found
within the word, but starting from either the beginning or the end of the
word (controlled by the <code>dir</code> parameter). The input vocabulary must be split
into prefixes, suffixes, and &quot;words&quot;.
</p>


<h3>Value</h3>

<p>Input word as a list of tokens.
</p>

<hr>
<h2 id='.mp_tokenize_word_bidir'>Tokenize a Word Bidirectionally</h2><span id='topic+.mp_tokenize_word_bidir'></span>

<h3>Description</h3>

<p>Apply .mp_tokenize_word from both directions and pick the result with fewer
pieces.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.mp_tokenize_word_bidir(
  word,
  vocab_split,
  unk_token,
  max_chars,
  allow_compounds = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".mp_tokenize_word_bidir_+3A_word">word</code></td>
<td>
<p>Character scalar; word to tokenize.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_bidir_+3A_vocab_split">vocab_split</code></td>
<td>
<p>List of character vectors containing vocabulary words.
Should have components named &quot;prefixes&quot;, &quot;words&quot;, &quot;suffixes&quot;.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_bidir_+3A_unk_token">unk_token</code></td>
<td>
<p>Token to represent unknown words.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_bidir_+3A_max_chars">max_chars</code></td>
<td>
<p>Maximum length of word recognized.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_bidir_+3A_allow_compounds">allow_compounds</code></td>
<td>
<p>Logical; whether to allow multiple whole words in the
breakdown. Default is TRUE. This option will not be exposed to end users;
it is kept here for documentation + development purposes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Input word as a list of tokens.
</p>

<hr>
<h2 id='.mp_tokenize_word_lookup'>Tokenize a Word Including Lookup</h2><span id='topic+.mp_tokenize_word_lookup'></span>

<h3>Description</h3>

<p>Look up a word in the table; go to fall-back otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.mp_tokenize_word_lookup(word, vocab, lookup, unk_token, max_chars)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".mp_tokenize_word_lookup_+3A_word">word</code></td>
<td>
<p>Character scalar; word to tokenize.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_lookup_+3A_vocab">vocab</code></td>
<td>
<p>A morphemepiece vocabulary.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_lookup_+3A_lookup">lookup</code></td>
<td>
<p>A morphemepiece lookup table.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_lookup_+3A_unk_token">unk_token</code></td>
<td>
<p>Token to represent unknown words.</p>
</td></tr>
<tr><td><code id=".mp_tokenize_word_lookup_+3A_max_chars">max_chars</code></td>
<td>
<p>Maximum length of word recognized.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Input word, broken into tokens.
</p>

<hr>
<h2 id='.new_morphemepiece_vocabulary'>Constructor for Class morphemepiece_vocabulary</h2><span id='topic+.new_morphemepiece_vocabulary'></span>

<h3>Description</h3>

<p>Constructor for Class morphemepiece_vocabulary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.new_morphemepiece_vocabulary(vocab, vocab_split, is_cased)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".new_morphemepiece_vocabulary_+3A_vocab">vocab</code></td>
<td>
<p>Character vector; the &quot;actual&quot; vocabulary.</p>
</td></tr>
<tr><td><code id=".new_morphemepiece_vocabulary_+3A_vocab_split">vocab_split</code></td>
<td>
<p>List of character vectors; the split vocabulary.</p>
</td></tr>
<tr><td><code id=".new_morphemepiece_vocabulary_+3A_is_cased">is_cased</code></td>
<td>
<p>Logical; whether the vocabulary is cased.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vocabulary with <code>is_cased</code> attached as an attribute, and the
class <code>morphemepiece_vocabulary</code> applied. The split vocabulary is also
attached as an attribute.
</p>

<hr>
<h2 id='.process_mp_vocab'>Process a Morphemepiece Vocabulary for Tokenization</h2><span id='topic+.process_mp_vocab'></span><span id='topic+.process_mp_vocab.default'></span><span id='topic+.process_mp_vocab.morphemepiece_vocabulary'></span><span id='topic+.process_mp_vocab.integer'></span><span id='topic+.process_mp_vocab.character'></span>

<h3>Description</h3>

<p>Process a Morphemepiece Vocabulary for Tokenization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.process_mp_vocab(v)

## Default S3 method:
.process_mp_vocab(v)

## S3 method for class 'morphemepiece_vocabulary'
.process_mp_vocab(v)

## S3 method for class 'integer'
.process_mp_vocab(v)

## S3 method for class 'character'
.process_mp_vocab(v)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".process_mp_vocab_+3A_v">v</code></td>
<td>
<p>An object of class <code>morphemepiece_vocabulary</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector of tokens for tokenization.
</p>

<hr>
<h2 id='.validate_morphemepiece_vocabulary'>Validator for Objects of Class morphemepiece_vocabulary</h2><span id='topic+.validate_morphemepiece_vocabulary'></span>

<h3>Description</h3>

<p>Validator for Objects of Class morphemepiece_vocabulary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.validate_morphemepiece_vocabulary(vocab)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".validate_morphemepiece_vocabulary_+3A_vocab">vocab</code></td>
<td>
<p>morphemepiece_vocabulary object to validate</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>vocab</code> if the object passes the checks. Otherwise, abort with
message.
</p>

<hr>
<h2 id='load_lookup'>Load a morphemepiece lookup file</h2><span id='topic+load_lookup'></span>

<h3>Description</h3>

<p>Usually you will want to use the included lookup that can be accessed via
<code>morphemepiece_lookup()</code>. This function can be used to load a different
lookup from a file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_lookup(lookup_file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_lookup_+3A_lookup_file">lookup_file</code></td>
<td>
<p>path to lookup file. File is assumed to be a text
file, with one word per line. The lookup value, if different from the word,
follows the word on the same line, after a space.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The lookup as a named list. Names are words in lookup.
</p>

<hr>
<h2 id='load_or_retrieve_lookup'>Load a lookup file, or retrieve from cache</h2><span id='topic+load_or_retrieve_lookup'></span>

<h3>Description</h3>

<p>Usually you will want to use the included lookup that can be accessed via
<code>morphemepiece_lookup()</code>. This function can be used to load (and cache) a
different lookup from a file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_or_retrieve_lookup(lookup_file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_or_retrieve_lookup_+3A_lookup_file">lookup_file</code></td>
<td>
<p>path to lookup file. File is assumed to be a text
file, with one word per line. The lookup value, if different from the word,
follows the word on the same line, after a space.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The lookup table as a named character vector.
</p>

<hr>
<h2 id='load_or_retrieve_vocab'>Load a vocabulary file, or retrieve from cache</h2><span id='topic+load_or_retrieve_vocab'></span>

<h3>Description</h3>

<p>Usually you will want to use the included vocabulary that can be accessed via
<code>morphemepiece_vocab()</code>. This function can be used to load (and cache) a
different vocabulary from a file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_or_retrieve_vocab(vocab_file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_or_retrieve_vocab_+3A_vocab_file">vocab_file</code></td>
<td>
<p>path to vocabulary file. File is assumed to be a text file,
with one token per line, with the line number (starting at zero)
corresponding to the index of that token in the vocabulary.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vocab as a character vector of tokens. The casedness of the
vocabulary is inferred and attached as the &quot;is_cased&quot; attribute. The
vocabulary indices are taken to be the positions of the tokens,
<em>starting at zero</em> for historical consistency.
</p>
<p>Note that from the perspective of a neural net, the numeric indices <em>are</em>
the tokens, and the mapping from token to index is fixed. If we changed the
indexing, it would break any pre-trained models using that vocabulary.
</p>

<hr>
<h2 id='load_vocab'>Load a vocabulary file</h2><span id='topic+load_vocab'></span>

<h3>Description</h3>

<p>Usually you will want to use the included vocabulary that can be accessed via
<code>morphemepiece_vocab()</code>. This function can be used to load a different
vocabulary from a file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_vocab(vocab_file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_vocab_+3A_vocab_file">vocab_file</code></td>
<td>
<p>path to vocabulary file. File is assumed to be a text file,
with one token per line, with the line number (starting at zero)
corresponding to the index of that token in the vocabulary.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vocab as a character vector of tokens. The casedness of the
vocabulary is inferred and attached as the &quot;is_cased&quot; attribute. The
vocabulary indices are taken to be the positions of the tokens,
<em>starting at zero</em> for historical consistency.
</p>
<p>Note that from the perspective of a neural net, the numeric indices <em>are</em>
the tokens, and the mapping from token to index is fixed. If we changed the
indexing, it would break any pre-trained models using that vocabulary.
</p>

<hr>
<h2 id='morphemepiece_cache_dir'>Retrieve Directory for Morphemepiece Cache</h2><span id='topic+morphemepiece_cache_dir'></span>

<h3>Description</h3>

<p>The morphemepiece cache directory is a platform- and user-specific path where
morphemepiece saves caches (such as a downloaded lookup). You can override
the default location in a few ways:
</p>

<ul>
<li><p>Option: <code>morphemepiece.dir</code>Use
<code><a href="#topic+set_morphemepiece_cache_dir">set_morphemepiece_cache_dir</a></code> to set a specific cache directory
for this session
</p>
</li>
<li><p>Environment: <code>MORPHEMEPIECE_CACHE_DIR</code>Set this environment
variable to specify a morphemepiece cache directory for all sessions.
</p>
</li>
<li><p>Environment: <code>R_USER_CACHE_DIR</code>Set this environment variable
to specify a cache directory root for all packages that use the caching
system.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>morphemepiece_cache_dir()
</code></pre>


<h3>Value</h3>

<p>A character vector with the normalized path to the cache.
</p>

<hr>
<h2 id='morphemepiece_tokenize'>Tokenize Sequence with Morpheme Pieces</h2><span id='topic+morphemepiece_tokenize'></span>

<h3>Description</h3>

<p>Given a single sequence of text and a morphemepiece vocabulary, tokenizes the
text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>morphemepiece_tokenize(
  text,
  vocab = morphemepiece_vocab(),
  lookup = morphemepiece_lookup(),
  unk_token = "[UNK]",
  max_chars = 100
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="morphemepiece_tokenize_+3A_text">text</code></td>
<td>
<p>Character scalar; text to tokenize.</p>
</td></tr>
<tr><td><code id="morphemepiece_tokenize_+3A_vocab">vocab</code></td>
<td>
<p>A morphemepiece vocabulary.</p>
</td></tr>
<tr><td><code id="morphemepiece_tokenize_+3A_lookup">lookup</code></td>
<td>
<p>A morphemepiece lookup table.</p>
</td></tr>
<tr><td><code id="morphemepiece_tokenize_+3A_unk_token">unk_token</code></td>
<td>
<p>Token to represent unknown words.</p>
</td></tr>
<tr><td><code id="morphemepiece_tokenize_+3A_max_chars">max_chars</code></td>
<td>
<p>Maximum length of word recognized.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector of tokenized text (later, this should be a named
integer vector, as in the wordpiece package.)
</p>

<hr>
<h2 id='morphemepiece-package'>morphemepiece: Morpheme Tokenization</h2><span id='topic+morphemepiece-package'></span>

<h3>Description</h3>

<p>Tokenize words into morphemes (the smallest unit of meaning).
</p>

<hr>
<h2 id='prepare_vocab'>Format a Token List as a Vocabulary</h2><span id='topic+prepare_vocab'></span>

<h3>Description</h3>

<p>We use a character vector with class morphemepiece_vocabulary to provide
information about tokens used in
<code><a href="#topic+morphemepiece_tokenize">morphemepiece_tokenize</a></code>. This function takes a character vector
of tokens and puts it into that format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_vocab(token_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_vocab_+3A_token_list">token_list</code></td>
<td>
<p>A character vector of tokens.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vocab as a character vector of tokens. The casedness of the
vocabulary is inferred and attached as the &quot;is_cased&quot; attribute. The
vocabulary indices are taken to be the positions of the tokens,
<em>starting at zero</em> for historical consistency.
</p>
<p>Note that from the perspective of a neural net, the numeric indices <em>are</em>
the tokens, and the mapping from token to index is fixed. If we changed the
indexing, it would break any pre-trained models using that vocabulary.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>my_vocab &lt;- prepare_vocab(c("some", "example", "tokens"))
class(my_vocab)
attr(my_vocab, "is_cased")
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic++25+3E+25'></span><span id='topic+.data'></span><span id='topic++25+7C+7C+25'></span><span id='topic++25fin+25'></span><span id='topic+morphemepiece_vocab'></span><span id='topic+morphemepiece_lookup'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>fastmatch</dt><dd><p><code><a href="fastmatch.html#topic+fmatch">%fin%</a></code></p>
</dd>
<dt>magrittr</dt><dd><p><code><a href="magrittr.html#topic+pipe">%&gt;%</a></code></p>
</dd>
<dt>morphemepiece.data</dt><dd><p><code><a href="morphemepiece.data.html#topic+morphemepiece_lookup">morphemepiece_lookup</a></code>, <code><a href="morphemepiece.data.html#topic+morphemepiece_vocab">morphemepiece_vocab</a></code></p>
</dd>
<dt>rlang</dt><dd><p><code><a href="rlang.html#topic+op-null-default">%||%</a></code>, <code><a href="rlang.html#topic+dot-data">.data</a></code></p>
</dd>
</dl>

<hr>
<h2 id='set_morphemepiece_cache_dir'>Set a Cache Directory for Morphemepiece</h2><span id='topic+set_morphemepiece_cache_dir'></span>

<h3>Description</h3>

<p>Use this function to override the cache path used by morphemepiece for the
current session. Set the <code>MORPHEMEPIECE_CACHE_DIR</code> environment variable
for a more permanent change.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_morphemepiece_cache_dir(cache_dir = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_morphemepiece_cache_dir_+3A_cache_dir">cache_dir</code></td>
<td>
<p>Character scalar; a path to a cache directory.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A normalized path to a cache directory. The directory is created if
the user has write access and the directory does not exist.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
