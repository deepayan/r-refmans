<!DOCTYPE html><html lang="en"><head><title>Help for package stackgbm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {stackgbm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#stackgbm-package'><p>stackgbm: Stacked Gradient Boosting Machines</p></a></li>
<li><a href='#catboost_load_pool'><p>Create a dataset</p></a></li>
<li><a href='#catboost_predict'><p>Predict based on the model</p></a></li>
<li><a href='#catboost_train'><p>Train the model</p></a></li>
<li><a href='#cv_catboost'><p>catboost - parameter tuning and model selection with k-fold cross-validation</p>
and grid search</a></li>
<li><a href='#cv_lightgbm'><p>lightgbm - parameter tuning and model selection with k-fold cross-validation</p>
and grid search</a></li>
<li><a href='#cv_param_grid'><p>Generate a parameter grid for cross-validation</p></a></li>
<li><a href='#cv_xgboost'><p>xgboost - parameter tuning and model selection with k-fold cross-validation</p>
and grid search</a></li>
<li><a href='#is_installed_catboost'><p>Is catboost installed?</p></a></li>
<li><a href='#is_installed_lightgbm'><p>Is lightgbm installed?</p></a></li>
<li><a href='#is_installed_xgboost'><p>Is xgboost installed?</p></a></li>
<li><a href='#lightgbm_train'><p>Train lightgbm model</p></a></li>
<li><a href='#predict.stackgbm'><p>Make predictions from a stackgbm model object</p></a></li>
<li><a href='#stackgbm'><p>Model stacking for boosted trees</p></a></li>
<li><a href='#xgboost_dmatrix'><p>Create xgb.DMatrix object</p></a></li>
<li><a href='#xgboost_train'><p>Train xgboost model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Stacked Gradient Boosting Machines</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>A minimalist implementation of model stacking by
    Wolpert (1992) &lt;<a href="https://doi.org/10.1016%2FS0893-6080%2805%2980023-1">doi:10.1016/S0893-6080(05)80023-1</a>&gt; for boosted tree models.
    A classic, two-layer stacking model is implemented, where the first layer
    generates features using gradient boosting trees, and the second layer
    employs a logistic regression model that uses these features as inputs.
    Utilities for training the base models and parameters tuning are provided,
    allowing users to experiment with different ensemble configurations easily.
    It aims to provide a simple and efficient way to combine multiple
    gradient boosting models to improve predictive model performance
    and robustness.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://nanx.me/stackgbm/">https://nanx.me/stackgbm/</a>, <a href="https://github.com/nanxstats/stackgbm">https://github.com/nanxstats/stackgbm</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/nanxstats/stackgbm/issues">https://github.com/nanxstats/stackgbm/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>pROC, progress, rlang</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, lightgbm, msaenet, rmarkdown, xgboost</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-29 04:16:05 UTC; nanx</td>
</tr>
<tr>
<td>Author:</td>
<td>Nan Xiao <a href="https://orcid.org/0000-0002-0250-5673"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nan Xiao &lt;me@nanx.me&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-30 11:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='stackgbm-package'>stackgbm: Stacked Gradient Boosting Machines</h2><span id='topic+stackgbm-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>A minimalist implementation of model stacking by Wolpert (1992) <a href="https://doi.org/10.1016/S0893-6080%2805%2980023-1">doi:10.1016/S0893-6080(05)80023-1</a> for boosted tree models. A classic, two-layer stacking model is implemented, where the first layer generates features using gradient boosting trees, and the second layer employs a logistic regression model that uses these features as inputs. Utilities for training the base models and parameters tuning are provided, allowing users to experiment with different ensemble configurations easily. It aims to provide a simple and efficient way to combine multiple gradient boosting models to improve predictive model performance and robustness.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Nan Xiao <a href="mailto:me@nanx.me">me@nanx.me</a> (<a href="https://orcid.org/0000-0002-0250-5673">ORCID</a>) [copyright holder]
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://nanx.me/stackgbm/">https://nanx.me/stackgbm/</a>
</p>
</li>
<li> <p><a href="https://github.com/nanxstats/stackgbm">https://github.com/nanxstats/stackgbm</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/nanxstats/stackgbm/issues">https://github.com/nanxstats/stackgbm/issues</a>
</p>
</li></ul>


<hr>
<h2 id='catboost_load_pool'>Create a dataset</h2><span id='topic+catboost_load_pool'></span>

<h3>Description</h3>

<p>Create a dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>catboost_load_pool(data, label = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="catboost_load_pool_+3A_data">data</code></td>
<td>
<p>Predictors.</p>
</td></tr>
<tr><td><code id="catboost_load_pool_+3A_label">label</code></td>
<td>
<p>Labels.</p>
</td></tr>
<tr><td><code id="catboost_load_pool_+3A_...">...</code></td>
<td>
<p>Additional parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>catboost.Pool</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 100,
  p = 10,
  rho = 0.6,
  coef = rnorm(5, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

catboost_load_pool(data = sim_data$x.tr, label = sim_data$y.tr)
catboost_load_pool(data = sim_data$x.tr, label = NULL)
catboost_load_pool(data = sim_data$x.te, label = NULL)

</code></pre>

<hr>
<h2 id='catboost_predict'>Predict based on the model</h2><span id='topic+catboost_predict'></span>

<h3>Description</h3>

<p>Predict based on the model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>catboost_predict(model, pool, prediction_type = "Probability", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="catboost_predict_+3A_model">model</code></td>
<td>
<p>The trained model.</p>
</td></tr>
<tr><td><code id="catboost_predict_+3A_pool">pool</code></td>
<td>
<p>The dataset to predict on.</p>
</td></tr>
<tr><td><code id="catboost_predict_+3A_prediction_type">prediction_type</code></td>
<td>
<p>Prediction type.</p>
</td></tr>
<tr><td><code id="catboost_predict_+3A_...">...</code></td>
<td>
<p>Additional parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predicted values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 100,
  p = 10,
  rho = 0.6,
  coef = rnorm(5, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

x_train &lt;- catboost_load_pool(data = sim_data$x.tr, label = sim_data$y.tr)
x_test &lt;- catboost_load_pool(data = sim_data$x.te, label = NULL)

fit &lt;- catboost_train(
  x_train,
  NULL,
  params = list(
    loss_function = "Logloss",
    iterations = 100,
    depth = 3,
    logging_level = "Silent"
  )
)

catboost_predict(fit, x_test)

</code></pre>

<hr>
<h2 id='catboost_train'>Train the model</h2><span id='topic+catboost_train'></span>

<h3>Description</h3>

<p>Train the model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>catboost_train(learn_pool, test_pool = NULL, params = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="catboost_train_+3A_learn_pool">learn_pool</code></td>
<td>
<p>Training dataset.</p>
</td></tr>
<tr><td><code id="catboost_train_+3A_test_pool">test_pool</code></td>
<td>
<p>Testing dataset.</p>
</td></tr>
<tr><td><code id="catboost_train_+3A_params">params</code></td>
<td>
<p>A list of training parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A model object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 100,
  p = 10,
  rho = 0.6,
  coef = rnorm(5, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

x_train &lt;- catboost_load_pool(data = sim_data$x.tr, label = sim_data$y.tr)

fit &lt;- catboost_train(
  x_train,
  NULL,
  params = list(
    loss_function = "Logloss",
    iterations = 100,
    depth = 3,
    logging_level = "Silent"
  )
)

fit

</code></pre>

<hr>
<h2 id='cv_catboost'>catboost - parameter tuning and model selection with k-fold cross-validation
and grid search</h2><span id='topic+cv_catboost'></span>

<h3>Description</h3>

<p>catboost - parameter tuning and model selection with k-fold cross-validation
and grid search
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_catboost(
  x,
  y,
  params = cv_param_grid(),
  n_folds = 5,
  n_threads = 1,
  seed = 42,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv_catboost_+3A_x">x</code></td>
<td>
<p>Predictor matrix.</p>
</td></tr>
<tr><td><code id="cv_catboost_+3A_y">y</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code id="cv_catboost_+3A_params">params</code></td>
<td>
<p>Parameter grid generated by <code><a href="#topic+cv_param_grid">cv_param_grid()</a></code>.</p>
</td></tr>
<tr><td><code id="cv_catboost_+3A_n_folds">n_folds</code></td>
<td>
<p>Number of folds. Default is 5.</p>
</td></tr>
<tr><td><code id="cv_catboost_+3A_n_threads">n_threads</code></td>
<td>
<p>The number of parallel threads. For optimal speed,
match this to the number of physical CPU cores, not threads.
See respective model documentation for more details. Default is 1.</p>
</td></tr>
<tr><td><code id="cv_catboost_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducibility.</p>
</td></tr>
<tr><td><code id="cv_catboost_+3A_verbose">verbose</code></td>
<td>
<p>Show progress?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing the complete tuning grid and the AUC values,
with the best parameter combination and the highest AUC value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 100,
  p = 10,
  rho = 0.6,
  coef = rnorm(5, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

params &lt;- cv_catboost(
  sim_data$x.tr,
  sim_data$y.tr,
  params = cv_param_grid(
    n_iterations = c(100, 200),
    max_depth = c(3, 5),
    learning_rate = c(0.1, 0.5)
  ),
  n_folds = 5,
  n_threads = 1,
  seed = 42,
  verbose = FALSE
)

params$df

</code></pre>

<hr>
<h2 id='cv_lightgbm'>lightgbm - parameter tuning and model selection with k-fold cross-validation
and grid search</h2><span id='topic+cv_lightgbm'></span>

<h3>Description</h3>

<p>lightgbm - parameter tuning and model selection with k-fold cross-validation
and grid search
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_lightgbm(
  x,
  y,
  params = cv_param_grid(),
  n_folds = 5,
  n_threads = 1,
  seed = 42,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv_lightgbm_+3A_x">x</code></td>
<td>
<p>Predictor matrix.</p>
</td></tr>
<tr><td><code id="cv_lightgbm_+3A_y">y</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code id="cv_lightgbm_+3A_params">params</code></td>
<td>
<p>Parameter grid generated by <code><a href="#topic+cv_param_grid">cv_param_grid()</a></code>.</p>
</td></tr>
<tr><td><code id="cv_lightgbm_+3A_n_folds">n_folds</code></td>
<td>
<p>Number of folds. Default is 5.</p>
</td></tr>
<tr><td><code id="cv_lightgbm_+3A_n_threads">n_threads</code></td>
<td>
<p>The number of parallel threads. For optimal speed,
match this to the number of physical CPU cores, not threads.
See respective model documentation for more details. Default is 1.</p>
</td></tr>
<tr><td><code id="cv_lightgbm_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducibility.</p>
</td></tr>
<tr><td><code id="cv_lightgbm_+3A_verbose">verbose</code></td>
<td>
<p>Show progress?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing the complete tuning grid and the AUC values,
with the best parameter combination and the highest AUC value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 100,
  p = 10,
  rho = 0.6,
  coef = rnorm(5, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

params &lt;- suppressWarnings(
  cv_lightgbm(
    sim_data$x.tr,
    sim_data$y.tr,
    params = cv_param_grid(
      n_iterations = c(100, 200),
      max_depth = c(3, 5),
      learning_rate = c(0.1, 0.5)
    ),
    n_folds = 5,
    n_threads = 1,
    seed = 42,
    verbose = FALSE
  )
)

params$df

</code></pre>

<hr>
<h2 id='cv_param_grid'>Generate a parameter grid for cross-validation</h2><span id='topic+cv_param_grid'></span>

<h3>Description</h3>

<p>This function generates a parameter grid to be used in the
cross-validation of gradient boosting decision tree (GBDT) models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_param_grid(
  n_iterations = c(100, 200, 500, 1000),
  max_depth = c(3, 5, 7, 9),
  learning_rate = c(0.01, 0.05, 0.1, 0.2)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv_param_grid_+3A_n_iterations">n_iterations</code></td>
<td>
<p>A numeric vector of the number of iterations (trees)
for the GBDT model. This is equivalent to <code>nrounds</code> in XGBoost,
<code>num_iterations</code> in LightGBM, and <code>iterations</code> in CatBoost.</p>
</td></tr>
<tr><td><code id="cv_param_grid_+3A_max_depth">max_depth</code></td>
<td>
<p>A numeric vector of the maximum tree depths.
This parameter is equivalent to <code>max_depth</code> in XGBoost and LightGBM,
and <code>depth</code> in CatBoost.</p>
</td></tr>
<tr><td><code id="cv_param_grid_+3A_learning_rate">learning_rate</code></td>
<td>
<p>A numeric vector of learning rates for the GBDT model.
This parameter is equivalent to <code>eta</code> in XGBoost,
<code>learning_rate</code> in LightGBM, and ignored in CatBoost.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list where the names are the parameter names and the values
are vectors of possible values for those parameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>params &lt;- cv_param_grid(
  n_iterations = c(10, 100),
  max_depth = c(3, 5),
  learning_rate = c(0.01, 0.1)
)
</code></pre>

<hr>
<h2 id='cv_xgboost'>xgboost - parameter tuning and model selection with k-fold cross-validation
and grid search</h2><span id='topic+cv_xgboost'></span>

<h3>Description</h3>

<p>xgboost - parameter tuning and model selection with k-fold cross-validation
and grid search
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_xgboost(
  x,
  y,
  params = cv_param_grid(),
  n_folds = 5,
  n_threads = 1,
  seed = 42,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv_xgboost_+3A_x">x</code></td>
<td>
<p>Predictor matrix.</p>
</td></tr>
<tr><td><code id="cv_xgboost_+3A_y">y</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code id="cv_xgboost_+3A_params">params</code></td>
<td>
<p>Parameter grid generated by <code><a href="#topic+cv_param_grid">cv_param_grid()</a></code>.</p>
</td></tr>
<tr><td><code id="cv_xgboost_+3A_n_folds">n_folds</code></td>
<td>
<p>Number of folds. Default is 5.</p>
</td></tr>
<tr><td><code id="cv_xgboost_+3A_n_threads">n_threads</code></td>
<td>
<p>The number of parallel threads. For optimal speed,
match this to the number of physical CPU cores, not threads.
See respective model documentation for more details. Default is 1.</p>
</td></tr>
<tr><td><code id="cv_xgboost_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducibility.</p>
</td></tr>
<tr><td><code id="cv_xgboost_+3A_verbose">verbose</code></td>
<td>
<p>Show progress?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing the complete tuning grid and the AUC values,
with the best parameter combination and the highest AUC value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 100,
  p = 10,
  rho = 0.6,
  coef = rnorm(5, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

params &lt;- cv_xgboost(
  sim_data$x.tr,
  sim_data$y.tr,
  params = cv_param_grid(
    n_iterations = c(100, 200),
    max_depth = c(3, 5),
    learning_rate = c(0.1, 0.5)
  ),
  n_folds = 5,
  n_threads = 1,
  seed = 42,
  verbose = FALSE
)

params$df

</code></pre>

<hr>
<h2 id='is_installed_catboost'>Is catboost installed?</h2><span id='topic+is_installed_catboost'></span>

<h3>Description</h3>

<p>Is catboost installed?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_installed_catboost()
</code></pre>


<h3>Value</h3>

<p><code>TRUE</code> if installed, <code>FALSE</code> if not.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>is_installed_catboost()
</code></pre>

<hr>
<h2 id='is_installed_lightgbm'>Is lightgbm installed?</h2><span id='topic+is_installed_lightgbm'></span>

<h3>Description</h3>

<p>Is lightgbm installed?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_installed_lightgbm()
</code></pre>


<h3>Value</h3>

<p><code>TRUE</code> if installed, <code>FALSE</code> if not.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>is_installed_lightgbm()
</code></pre>

<hr>
<h2 id='is_installed_xgboost'>Is xgboost installed?</h2><span id='topic+is_installed_xgboost'></span>

<h3>Description</h3>

<p>Is xgboost installed?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_installed_xgboost()
</code></pre>


<h3>Value</h3>

<p><code>TRUE</code> if installed, <code>FALSE</code> if not.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>is_installed_xgboost()
</code></pre>

<hr>
<h2 id='lightgbm_train'>Train lightgbm model</h2><span id='topic+lightgbm_train'></span>

<h3>Description</h3>

<p>Train lightgbm model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lightgbm_train(data, label, params, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lightgbm_train_+3A_data">data</code></td>
<td>
<p>Training data.</p>
</td></tr>
<tr><td><code id="lightgbm_train_+3A_label">label</code></td>
<td>
<p>Labels.</p>
</td></tr>
<tr><td><code id="lightgbm_train_+3A_params">params</code></td>
<td>
<p>A list of parameters.</p>
</td></tr>
<tr><td><code id="lightgbm_train_+3A_...">...</code></td>
<td>
<p>Additional parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A model object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 100,
  p = 10,
  rho = 0.6,
  coef = rnorm(5, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

fit &lt;- suppressWarnings(
  lightgbm_train(
    data = sim_data$x.tr,
    label = sim_data$y.tr,
    params = list(
      objective = "binary",
      learning_rate = 0.1,
      num_iterations = 100,
      max_depth = 3,
      num_leaves = 2^3 - 1,
      num_threads = 1
    ),
    verbose = -1
  )
)

fit

</code></pre>

<hr>
<h2 id='predict.stackgbm'>Make predictions from a stackgbm model object</h2><span id='topic+predict.stackgbm'></span>

<h3>Description</h3>

<p>Make predictions from a stackgbm model object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stackgbm'
predict(object, newx, threshold = 0.5, classes = c(1L, 0L), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.stackgbm_+3A_object">object</code></td>
<td>
<p>A stackgbm model object.</p>
</td></tr>
<tr><td><code id="predict.stackgbm_+3A_newx">newx</code></td>
<td>
<p>New predictor matrix.</p>
</td></tr>
<tr><td><code id="predict.stackgbm_+3A_threshold">threshold</code></td>
<td>
<p>Decision threshold. Default is 0.5.</p>
</td></tr>
<tr><td><code id="predict.stackgbm_+3A_classes">classes</code></td>
<td>
<p>The class encoding vector of the predicted outcome.
The naming and order will be respected.</p>
</td></tr>
<tr><td><code id="predict.stackgbm_+3A_...">...</code></td>
<td>
<p>Unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of two vectors presenting the predicted classification
probabilities and predicted response.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 1000,
  p = 50,
  rho = 0.6,
  coef = rnorm(25, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

params_xgboost &lt;- structure(
  list("nrounds" = 200, "eta" = 0.05, "max_depth" = 3),
  class = c("cv_params", "cv_xgboost")
)
params_lightgbm &lt;- structure(
  list("num_iterations" = 200, "max_depth" = 3, "learning_rate" = 0.05),
  class = c("cv_params", "cv_lightgbm")
)
params_catboost &lt;- structure(
  list("iterations" = 100, "depth" = 3),
  class = c("cv_params", "cv_catboost")
)

fit &lt;- stackgbm(
  sim_data$x.tr,
  sim_data$y.tr,
  params = list(
    params_xgboost,
    params_lightgbm,
    params_catboost
  )
)

predict(fit, newx = sim_data$x.te)

</code></pre>

<hr>
<h2 id='stackgbm'>Model stacking for boosted trees</h2><span id='topic+stackgbm'></span>

<h3>Description</h3>

<p>Model stacking with a two-layer architecture: first layer being boosted
tree models fitted by xgboost, lightgbm, and catboost; second layer being
a logistic regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stackgbm(x, y, params, n_folds = 5L, seed = 42, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stackgbm_+3A_x">x</code></td>
<td>
<p>Predictor matrix.</p>
</td></tr>
<tr><td><code id="stackgbm_+3A_y">y</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code id="stackgbm_+3A_params">params</code></td>
<td>
<p>A list of optimal parameter objects for boosted tree models
derived from <code><a href="#topic+cv_xgboost">cv_xgboost()</a></code>, <code><a href="#topic+cv_lightgbm">cv_lightgbm()</a></code>, and <code><a href="#topic+cv_catboost">cv_catboost()</a></code>.
The order does not matter.</p>
</td></tr>
<tr><td><code id="stackgbm_+3A_n_folds">n_folds</code></td>
<td>
<p>Number of folds. Default is 5.</p>
</td></tr>
<tr><td><code id="stackgbm_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducibility.</p>
</td></tr>
<tr><td><code id="stackgbm_+3A_verbose">verbose</code></td>
<td>
<p>Show progress?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Fitted boosted tree models and stacked tree model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 1000,
  p = 50,
  rho = 0.6,
  coef = rnorm(25, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

params_xgboost &lt;- structure(
  list("nrounds" = 200, "eta" = 0.05, "max_depth" = 3),
  class = c("cv_params", "cv_xgboost")
)
params_lightgbm &lt;- structure(
  list("num_iterations" = 200, "max_depth" = 3, "learning_rate" = 0.05),
  class = c("cv_params", "cv_lightgbm")
)
params_catboost &lt;- structure(
  list("iterations" = 100, "depth" = 3),
  class = c("cv_params", "cv_catboost")
)

fit &lt;- stackgbm(
  sim_data$x.tr,
  sim_data$y.tr,
  params = list(
    params_xgboost,
    params_lightgbm,
    params_catboost
  )
)

predict(fit, newx = sim_data$x.te)

</code></pre>

<hr>
<h2 id='xgboost_dmatrix'>Create xgb.DMatrix object</h2><span id='topic+xgboost_dmatrix'></span>

<h3>Description</h3>

<p>Create xgb.DMatrix object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgboost_dmatrix(data, label = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgboost_dmatrix_+3A_data">data</code></td>
<td>
<p>Matrix or file.</p>
</td></tr>
<tr><td><code id="xgboost_dmatrix_+3A_label">label</code></td>
<td>
<p>Labels (optional).</p>
</td></tr>
<tr><td><code id="xgboost_dmatrix_+3A_...">...</code></td>
<td>
<p>Additional parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>xgb.DMatrix</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 100,
  p = 10,
  rho = 0.6,
  coef = rnorm(5, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

xgboost_dmatrix(sim_data$x.tr, label = sim_data$y.tr)
xgboost_dmatrix(sim_data$x.te)

</code></pre>

<hr>
<h2 id='xgboost_train'>Train xgboost model</h2><span id='topic+xgboost_train'></span>

<h3>Description</h3>

<p>Train xgboost model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgboost_train(params, data, nrounds, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgboost_train_+3A_params">params</code></td>
<td>
<p>A list of parameters.</p>
</td></tr>
<tr><td><code id="xgboost_train_+3A_data">data</code></td>
<td>
<p>Training data.</p>
</td></tr>
<tr><td><code id="xgboost_train_+3A_nrounds">nrounds</code></td>
<td>
<p>The Maximum number of boosting iterations.</p>
</td></tr>
<tr><td><code id="xgboost_train_+3A_...">...</code></td>
<td>
<p>Additional parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A model object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sim_data &lt;- msaenet::msaenet.sim.binomial(
  n = 100,
  p = 10,
  rho = 0.6,
  coef = rnorm(5, mean = 0, sd = 10),
  snr = 1,
  p.train = 0.8,
  seed = 42
)

x_train &lt;- xgboost_dmatrix(sim_data$x.tr, label = sim_data$y.tr)

fit &lt;- xgboost_train(
  params = list(
    objective = "binary:logistic",
    eval_metric = "auc",
    max_depth = 3,
    eta = 0.1
  ),
  data = x_train,
  nrounds = 100,
  nthread = 1
)

fit

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
