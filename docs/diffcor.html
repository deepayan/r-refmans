<!DOCTYPE html><html lang="en"><head><title>Help for package diffcor</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {diffcor}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bootcor.dep'><p>Bootstrapped Correlation Difference Test for Dependent Correlations</p></a></li>
<li><a href='#bootcor.one'><p>Bootstrapped Correlation Difference Test between an Empirical and an</p>
Expected Correlation</a></li>
<li><a href='#bootcor.two'><p>Bootstrapped Correlation Difference Test between Correlations from Two</p>
Independent Samples</a></li>
<li><a href='#diffcor.dep'><p>Fisher's z-Tests of dependent correlations</p></a></li>
<li><a href='#diffcor.one'><p>Fisher's z-test of difference between an empirical and a hypothesized</p>
correlation</a></li>
<li><a href='#diffcor.two'><p>Fisher's z-Tests for differences of correlations in two independent</p>
samples</a></li>
<li><a href='#diffpwr.dep'><p>Monte Carlo Simulation for the correlation difference between dependent</p>
correlations</a></li>
<li><a href='#diffpwr.one'><p>Difference Between an Assumed Sample Correlation and a Population</p>
Correlation</a></li>
<li><a href='#diffpwr.two'><p>Monte Carlo Simulation for the correlation difference between two</p>
correlations that were observed in two independent samples</a></li>
<li><a href='#visual_mc'><p>Visualization of the simulated parameters</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fisher's z-Tests Concerning Differences Between Correlations</td>
</tr>
<tr>
<td>Version:</td>
<td>0.8.4</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.3.0), MASS</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-09-11</td>
</tr>
<tr>
<td>Description:</td>
<td>Computations of Fisher's z-tests concerning different kinds of correlation differences. The 'diffpwr' family entails approaches to estimating statistical power via Monte Carlo simulations. Important to note, the Pearson correlation coefficient is sensitive to linear association, but also to a host of statistical issues such as univariate and bivariate outliers, range restrictions, and heteroscedasticity (e.g., Duncan &amp; Layard, 1973 &lt;<a href="https://doi.org/10.1093%2FBIOMET%2F60.3.551">doi:10.1093/BIOMET/60.3.551</a>&gt;; Wilcox, 2013 &lt;<a href="https://doi.org/10.1016%2FC2010-0-67044-1">doi:10.1016/C2010-0-67044-1</a>&gt;). Thus, every power analysis requires that specific statistical prerequisites are fulfilled and can be invalid if the prerequisites do not hold. To this end, the 'bootcor' family provides bootstrapping confidence intervals for the incorporated correlation difference tests.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-12 08:32:54 UTC; christianblotner</td>
</tr>
<tr>
<td>Author:</td>
<td>Christian Blötner [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Christian Blötner &lt;c.bloetner@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-12 15:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bootcor.dep'>Bootstrapped Correlation Difference Test for Dependent Correlations</h2><span id='topic+bootcor.dep'></span>

<h3>Description</h3>

<p>Derivation of bootstrap confidence intervals for the calculation of
correlation differences for dependent correlations.</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootcor.dep(target,
            x1,
            x2,
            k = 5000,
            alpha = .05,
            digit = 3,
            seed = 1234)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootcor.dep_+3A_target">target</code></td>
<td>
<p>A vector containing the values for the target variable for
which the correlations of the two competing variables x1 and x2 should be
compared.</p>
</td></tr>
<tr><td><code id="bootcor.dep_+3A_x1">x1</code></td>
<td>
<p>A vector containing the values of the first variable being
correlated with the target variable.</p>
</td></tr>
<tr><td><code id="bootcor.dep_+3A_x2">x2</code></td>
<td>
<p>A vector containing the values of the second variable being
correlated with the target variable.</p>
</td></tr>
<tr><td><code id="bootcor.dep_+3A_k">k</code></td>
<td>
<p>The number of bootstrap samples that should be drawn. The default is
5000.</p>
</td></tr>
<tr><td><code id="bootcor.dep_+3A_alpha">alpha</code></td>
<td>
<p>Likelihood of Type I error. The default is .05.</p>
</td></tr>
<tr><td><code id="bootcor.dep_+3A_digit">digit</code></td>
<td>
<p>Number of digits in the output. The default is 3.</p>
</td></tr>
<tr><td><code id="bootcor.dep_+3A_seed">seed</code></td>
<td>
<p>A random seed to make the results reproducible.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bivariate correlation analyses as well as correlation difference tests possess
very strict statistical requirements that are not necessarily fulfilled when
using the basic <code>diffcor.dep()</code> function from this package (Wilcox, 2013
&lt;doi:10.1016/C2010-0-67044-1&gt;). For instance, if the assumption of a normal
distribution does not hold, the significance test can lead to false positive or
false negative conclusions. To address potential deviations from normal
distribution, the present function applies bootstrapping to the data. The output
provides a confidence interval for the difference between the empirically
observed correlations of two competing variables with a target variable,
whereby the interval is derived from bootstrapping..</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>r_target_1</code></td>
<td>
<p>The empircally observed correlation between the first
variable and the target variable.</p>
</td></tr>
<tr><td><code>r_target_2</code></td>
<td>
<p>The empircally observed correlation between the second
variable and the target variable.</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p>Mean of the confidence interval of the correlation difference between
<code>r_target_1</code> and <code>r_target_2</code>.</p>
</td></tr>
<tr><td><code>LL</code></td>
<td>
<p>Lower limit of the confidence interval of the correlation difference
between <code>r_target_1</code> and <code>r_target_2</code>, given the entered Type I-level.
</p>
</td></tr>
<tr><td><code>UL</code></td>
<td>
<p>Upper limit of the confidence interval of the correlation difference
between <code>r_target_1</code> and <code>r_target_2</code>, given the entered Type I-level.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Blötner
<a href="mailto:c.bloetner@gmail.com">c.bloetner@gmail.com</a></p>


<h3>References</h3>

<p>Wilcox, R. (2013). Introduction to robust estimation and hypothesis testing.
Elsevier. https://doi.org/10.1016/C2010-0-67044-1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
df &lt;- data.frame(target = rnorm(1000),
                 var1 = rnorm(1000),
                 var2 = rnorm(1000))

bootcor.dep(target = df$target,
            x1 = df$var1,
            x2 = df$var2,
            k = 5000,
            alpha = .05,
            digit = 3,
            seed = 1234)</code></pre>

<hr>
<h2 id='bootcor.one'>Bootstrapped Correlation Difference Test between an Empirical and an
Expected Correlation</h2><span id='topic+bootcor.one'></span>

<h3>Description</h3>

<p>Derivation of bootstrap confidence intervals for the calculation of
correlation differences between the empirically observed correlation coefficient
and a threshold against which this coefficient is tested.</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootcor.one(x,
            y,
            r_target,
            k = 5000,
            alpha = .05,
            digit = 3,
            seed = 1234)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootcor.one_+3A_x">x</code></td>
<td>
<p>A vector containing the values of the first variable being involved
in the correlation.</p>
</td></tr>
<tr><td><code id="bootcor.one_+3A_y">y</code></td>
<td>
<p>A vector containing the values of the second variable being involved
in the correlation.</p>
</td></tr>
<tr><td><code id="bootcor.one_+3A_r_target">r_target</code></td>
<td>
<p>A single value against which the correlation between x and y
is tested.</p>
</td></tr>
<tr><td><code id="bootcor.one_+3A_k">k</code></td>
<td>
<p>The number of bootstrap samples to be drawn. The default is 5000.</p>
</td></tr>
<tr><td><code id="bootcor.one_+3A_alpha">alpha</code></td>
<td>
<p>Likelihood of Type I error. The default is .05.</p>
</td></tr>
<tr><td><code id="bootcor.one_+3A_digit">digit</code></td>
<td>
<p>Number of digits in the output. The default is 3.</p>
</td></tr>
<tr><td><code id="bootcor.one_+3A_seed">seed</code></td>
<td>
<p>A random seed to make the results reproducible.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bivariate correlation analyses as well as correlation difference tests possess
very strict statistical requirements that are not necessarily fulfilled when
using the basic <code>diffcor.one()</code> function from this package (Wilcox, 2013
&lt;doi:10.1016/C2010-0-67044-1&gt;). For instance, if the assumption of a normal
distribution does not hold, the significance test can lead to false positive or
false negative conclusions. To address potential deviations from normal
distribution, the present function applies bootstrapping to the data. The output
provides a confidence interval for the difference between the empirically
observed correlation coefficient and the threshold against which this
coefficient should be tested, whereby the interval is derived from bootstrapping
samples.</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>r_emp</code></td>
<td>
<p>The empircally observed correlation between x and y.</p>
</td></tr>
<tr><td><code>r_target</code></td>
<td>
<p>The threshold against which r_emp is tested.</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p>Mean of the confidence interval of the correlation difference between
<code>r_emp</code> and <code>r_target</code>.</p>
</td></tr>
<tr><td><code>LL</code></td>
<td>
<p>Lower limit of the confidence interval of the correlation difference
between <code>r_emp</code> and <code>r_target</code>, given the entered Type I-level.</p>
</td></tr>
<tr><td><code>UL</code></td>
<td>
<p>Upper limit of the confidence interval of the correlation difference
between <code>r_emp</code> and <code>r_target</code>, given the entered Type I-level.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Blötner
<a href="mailto:c.bloetner@gmail.com">c.bloetner@gmail.com</a></p>


<h3>References</h3>

<p>Wilcox, R. (2013). Introduction to robust estimation and hypothesis testing.
Elsevier. https://doi.org/10.1016/C2010-0-67044-1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
df &lt;- data.frame(a = rnorm(1000),
                 b = rnorm(1000))

bootcor.one(x = df$a,
            y = df$b,
            r_target = .10,
            k = 5000,
            alpha = .05,
            digit = 3,
            seed = 1234)
</code></pre>

<hr>
<h2 id='bootcor.two'>Bootstrapped Correlation Difference Test between Correlations from Two
Independent Samples</h2><span id='topic+bootcor.two'></span>

<h3>Description</h3>

<p>Derivation of bootstrap confidence intervals for the calculation of
correlation differences between the empirically observed correlations obtained
from two independent samples.</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootcor.two(x1,
            y1,
            x2,
            y2,
            k = 5000,
            alpha = .05,
            digit = 3,
            seed = 1234)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootcor.two_+3A_x1">x1</code></td>
<td>
<p>A vector containing the values of the first variable being involved
in the correlation in Sample 1.</p>
</td></tr>
<tr><td><code id="bootcor.two_+3A_y1">y1</code></td>
<td>
<p>A vector containing the values of the second variable being involved
in the correlation in Sample 1.</p>
</td></tr>
<tr><td><code id="bootcor.two_+3A_x2">x2</code></td>
<td>
<p>A vector containing the values of the first variable being involved
in the correlation in Sample 2.</p>
</td></tr>
<tr><td><code id="bootcor.two_+3A_y2">y2</code></td>
<td>
<p>A vector containing the values of the second variable being involved
in the correlation in Sample 2.</p>
</td></tr>
<tr><td><code id="bootcor.two_+3A_k">k</code></td>
<td>
<p>The number of bootstrap samples that should be drawn. The default is
5000.</p>
</td></tr>
<tr><td><code id="bootcor.two_+3A_alpha">alpha</code></td>
<td>
<p>Likelihood of Type I error. The default is .05.</p>
</td></tr>
<tr><td><code id="bootcor.two_+3A_digit">digit</code></td>
<td>
<p>Number of digits in the output. The default is 3.</p>
</td></tr>
<tr><td><code id="bootcor.two_+3A_seed">seed</code></td>
<td>
<p>A random seed to make the results reproducible.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bivariate correlation analyses as well as correlation difference tests possess
very strict statistical requirements that are not necessarily fulfilled when
using the basic <code>diffcor.two()</code> function from this package (Wilcox, 2013
&lt;doi:10.1016/C2010-0-67044-1&gt;). For instance, if the assumption of a normal
distribution does not hold, the significance test can lead to false positive or
false negative conclusions. To address potential deviations from normal
distribution, the present function applies bootstrapping to the data. The output
provides a confidence interval for the difference between the empirically
observed correlation coefficients obtained from two independent samples, whereby
the interval is derived from bootstrapping.</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>r1</code></td>
<td>
<p>The empircally observed correlation between x and y in Sample 1.</p>
</td></tr>
<tr><td><code>r2</code></td>
<td>
<p>The empircally observed correlation between x and y in Sample 2.</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p>Mean of the confidence interval of the correlation difference between
the correlations from the two samples.</p>
</td></tr>
<tr><td><code>LL</code></td>
<td>
<p>Lower limit of the confidence interval of the correlation difference
between the correlations from the two samples, given the entered Type I-level.</p>
</td></tr>
<tr><td><code>UL</code></td>
<td>
<p>Upper limit of the confidence interval of the correlation difference
between the correlations from the two samples, given the entered Type I-level.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Blötner
<a href="mailto:c.bloetner@gmail.com">c.bloetner@gmail.com</a></p>


<h3>References</h3>

<p>Wilcox, R. (2013). Introduction to robust estimation and hypothesis testing.
Elsevier. https://doi.org/10.1016/C2010-0-67044-1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df1 &lt;- data.frame(a = rnorm(1000),
                  b = rnorm(1000))

df2 &lt;- data.frame(x = rnorm(600),
                  y = rnorm(600))

bootcor.two(x1 = df1$a,
            y1 = df1$b,
            x2 = df2$x,
            y2 = df2$y,
            k = 5000,
            alpha = .05,
            digit = 3,
            seed = 1234)</code></pre>

<hr>
<h2 id='diffcor.dep'>Fisher's z-Tests of dependent correlations</h2><span id='topic+diffcor.dep'></span>

<h3>Description</h3>

<p>Tests if the correlation between two variables (r12) differs from the
correlation between the first and a third one (r13), given the intercorrelation
of the compared constructs (r23). All correlations are automatically transformed
with the Fisher z-transformation prior to computations. The output provides the
compared correlations, test statistic as z-score, and p-values.</p>


<h3>Usage</h3>

<pre><code class='language-R'>diffcor.dep(r12, r13, r23, n, cor.names = NULL,
alternative = c("one.sided", "two.sided"), digit = 3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diffcor.dep_+3A_r12">r12</code></td>
<td>
<p>Correlation between the criterion with which both competing
variables are correlated and the first of the two competing variables.</p>
</td></tr>
<tr><td><code id="diffcor.dep_+3A_r13">r13</code></td>
<td>
<p>Correlation between the criterion with which both competing
variables are correlated and the second of the two competing variables.</p>
</td></tr>
<tr><td><code id="diffcor.dep_+3A_r23">r23</code></td>
<td>
<p>Intercorrelation between the two competing variables.</p>
</td></tr>
<tr><td><code id="diffcor.dep_+3A_n">n</code></td>
<td>
<p>Sample size in which the observed effect was found</p>
</td></tr>
<tr><td><code id="diffcor.dep_+3A_cor.names">cor.names</code></td>
<td>
<p>OPTIONAL, label for the correlation. DEFAULT is NULL</p>
</td></tr>
<tr><td><code id="diffcor.dep_+3A_alternative">alternative</code></td>
<td>
<p>A character string specifying if you wish to test one-sided
or two-sided differences</p>
</td></tr>
<tr><td><code id="diffcor.dep_+3A_digit">digit</code></td>
<td>
<p>Number of digits in the output for all parameters, DEFAULT = 3</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>r12</code></td>
<td>
<p>Correlation between the criterion with which both competing
variables are correlated and the first of the two competing variables.</p>
</td></tr>
<tr><td><code>r13</code></td>
<td>
<p>Correlation between the criterion with which both competing
variables are correlated and the second of the two competing variables.</p>
</td></tr>
<tr><td><code>r23</code></td>
<td>
<p>Intercorrelation between the two competing variables.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>Test statistic for correlation difference in units of z distribution</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>p value for one- or two-sided testing, depending on alternative =
c(&quot;one.sided&quot;, &quot;two.sided)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Blötner
<a href="mailto:c.bloetner@gmail.com">c.bloetner@gmail.com</a></p>


<h3>References</h3>

<p>Cohen, J. (1988). Statistical power analysis for the behavioral
sciences (2nd ed.). Lawrence Erlbaum.
</p>
<p>Eid, M., Gollwitzer, M., &amp; Schmitt, M. (2015). Statistik und Forschungsmethoden
(4.Auflage) [Statistics and research methods (4th ed.)]. Beltz.
</p>
<p>Steiger, J. H. (1980). Tests for comparing elements of a correlation matrix.
Psychological Bulletin, 87, 245-251.</p>


<h3>Examples</h3>

<pre><code class='language-R'>diffcor.dep(r12 = .76, r13 = .70, r23 = .50, n = 271, digit = 4,
cor.names = NULL, alternative = "two.sided")
</code></pre>

<hr>
<h2 id='diffcor.one'>Fisher's z-test of difference between an empirical and a hypothesized
correlation</h2><span id='topic+diffcor.one'></span>

<h3>Description</h3>

<p>The function tests whether an observed correlation differs from an expected one,
for example, in construct validation. All correlations are automatically
transformed with the Fisher z-transformation prior to computations. The output
provides the compared correlations, a z-score, a p-value, a confidence interval,
and the effect size Cohens q. According to Cohen (1988), q = |.10|, |.30| and
|.50| are considered small, moderate, and large differences, respectively.</p>


<h3>Usage</h3>

<pre><code class='language-R'>diffcor.one(emp.r, hypo.r, n, alpha = .05, cor.names = NULL,
alternative = c("one.sided", "two.sided"), digit = 3)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diffcor.one_+3A_emp.r">emp.r</code></td>
<td>
<p>Empirically observed correlation</p>
</td></tr>
<tr><td><code id="diffcor.one_+3A_hypo.r">hypo.r</code></td>
<td>
<p>Hypothesized correlation which shall be tested</p>
</td></tr>
<tr><td><code id="diffcor.one_+3A_n">n</code></td>
<td>
<p>Sample size in which the observed effect was found</p>
</td></tr>
<tr><td><code id="diffcor.one_+3A_alpha">alpha</code></td>
<td>
<p>Likelihood of Type I error, DEFAULT = .05</p>
</td></tr>
<tr><td><code id="diffcor.one_+3A_cor.names">cor.names</code></td>
<td>
<p>OPTIONAL, label for the correlation (e.g., &quot;IQ-performance&quot;).
DEFAULT is NULL</p>
</td></tr>
<tr><td><code id="diffcor.one_+3A_digit">digit</code></td>
<td>
<p>Number of digits in the output for all parameters, DEFAULT = 3</p>
</td></tr>
<tr><td><code id="diffcor.one_+3A_alternative">alternative</code></td>
<td>
<p>A character string specifying if you wish to test one-sided
or two-sided differences</p>
</td></tr></table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>r_exp</code></td>
<td>
<p>Vector of the expected correlations</p>
</td></tr>
<tr><td><code>r_obs</code></td>
<td>
<p>Vector of the empirically observed correlations</p>
</td></tr>
<tr><td><code>LL</code></td>
<td>
<p>Lower limit of the confidence interval of the empirical correlation,
given the specified alpha level, DEFAULT = 95 percent</p>
</td></tr>
<tr><td><code>UL</code></td>
<td>
<p>Upper limit of the confidence interval of the empirical correlation,
given the specified alpha level, DEFAULT = 95 percent</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>Test statistic for correlation difference in units of z distribution</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>p value for one- or two-sided testing, depending on alternative =
c(&quot;one.sided&quot;, &quot;two.sided)</p>
</td></tr>
<tr><td><code>Cohen_q</code></td>
<td>
<p>Effect size measure for differences of independent correlations</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Blötner
<a href="mailto:c.bloetner@gmail.com">c.bloetner@gmail.com</a></p>


<h3>References</h3>

<p>Cohen, J. (1988). Statistical power analysis for the behavioral
sciences (2nd ed.). Lawrence Erlbaum.
</p>
<p>Eid, M., Gollwitzer, M., &amp; Schmitt, M. (2015). Statistik und Forschungsmethoden
(4.Auflage) [Statistics and research methods (4th ed.)]. Beltz.
</p>
<p>Steiger, J. H. (1980). Tests for comparing elements of a correlation matrix.
Psychological Bulletin, 87, 245-251.</p>


<h3>Examples</h3>

<pre><code class='language-R'>diffcor.one(c(.76, .53, -.32), c(.70, .35, -.40),
  c(225, 250, 210),
  cor.names = c("a-b", "c-d", "e-f"), digit = 2, alternative = "one.sided")
</code></pre>

<hr>
<h2 id='diffcor.two'>Fisher's z-Tests for differences of correlations in two independent
samples</h2><span id='topic+diffcor.two'></span>

<h3>Description</h3>

<p>Tests whether the correlation between two variables differs across two
independent studies/samples. The correlations are automatically transformed with
the Fisher z-transformation prior to computations. The output provides the
compared correlations, test statistic as z-score, p-values, confidence intervals
of the empirical correlations, and the effect size Cohens q. According to Cohen
(1988), q = |.10|, |.30| and |.50| are considered small, moderate, and large
differences, respectively.</p>


<h3>Usage</h3>

<pre><code class='language-R'>diffcor.two(r1, r2, n1, n2, alpha = .05, cor.names = NULL,
alternative = c("one.sided", "two.sided"), digit = 3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diffcor.two_+3A_r1">r1</code></td>
<td>
<p>Correlation coefficient in first sample</p>
</td></tr>
<tr><td><code id="diffcor.two_+3A_r2">r2</code></td>
<td>
<p>Correlation coefficient in second sample</p>
</td></tr>
<tr><td><code id="diffcor.two_+3A_n1">n1</code></td>
<td>
<p>First sample size</p>
</td></tr>
<tr><td><code id="diffcor.two_+3A_n2">n2</code></td>
<td>
<p>Second sample size</p>
</td></tr>
<tr><td><code id="diffcor.two_+3A_alpha">alpha</code></td>
<td>
<p>Likelihood of Type I error, DEFAULT = .05</p>
</td></tr>
<tr><td><code id="diffcor.two_+3A_cor.names">cor.names</code></td>
<td>
<p>OPTIONAL, label for the correlation (e.g., &quot;IQ-performance&quot;).
DEFAULT is NULL</p>
</td></tr>
<tr><td><code id="diffcor.two_+3A_digit">digit</code></td>
<td>
<p>Number of digits in the output for all parameters, DEFAULT = 3</p>
</td></tr>
<tr><td><code id="diffcor.two_+3A_alternative">alternative</code></td>
<td>
<p>A character string specifying if you wish to test one-sided
or two-sided differences</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>r1</code></td>
<td>
<p>Vector of the empirically observed correlations in the first sample</p>
</td></tr>
<tr><td><code>r2</code></td>
<td>
<p>Vector of the empirically observed correlations in the second sample</p>
</td></tr>
<tr><td><code>LL1</code></td>
<td>
<p>Lower limit of the confidence interval of the first empirical
correlation, given the specified alpha level, DEFAULT = 95 percent</p>
</td></tr>
<tr><td><code>UL1</code></td>
<td>
<p>Upper limit of the confidence interval of the first empirical
correlation, given the specified alpha level, DEFAULT = 95 percent</p>
</td></tr>
<tr><td><code>LL2</code></td>
<td>
<p>Lower limit of the confidence interval of the second empirical
correlation, given the specified alpha level, DEFAULT = 95 percent</p>
</td></tr>
<tr><td><code>UL2</code></td>
<td>
<p>Upper limit of the confidence interval of the second empirical
correlation, given the specified alpha level, DEFAULT = 95 percent</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>Test statistic for correlation difference in units of z distribution</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>p value for one- or two-sided testing, depending on alternative =
c(&quot;one.sided&quot;, &quot;two.sided)</p>
</td></tr>
<tr><td><code>Cohen_q</code></td>
<td>
<p>Effect size measure for differences of independent correlations</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Blötner
<a href="mailto:c.bloetner@gmail.com">c.bloetner@gmail.com</a></p>


<h3>References</h3>

<p>Cohen, J. (1988). Statistical power analysis for the behavioral
sciences (2nd ed.). Lawrence Erlbaum.
</p>
<p>Eid, M., Gollwitzer, M., &amp; Schmitt, M. (2015). Statistik und Forschungsmethoden
(4.Auflage) [Statistics and research methods (4th ed.)]. Beltz.
</p>
<p>Steiger, J. H. (1980). Tests for comparing elements of a correlation matrix.
Psychological Bulletin, 87, 245-251.</p>


<h3>Examples</h3>

<pre><code class='language-R'>diffcor.two(r1 = c(.39, .52, .22),
  r2 = c(.29, .44, .12),
  n1 = c(66, 66, 66), n2 = c(96, 96, 96), alpha = .01,
  cor.names = c("a-b", "c-d", "e-f"), alternative = "one.sided")
</code></pre>

<hr>
<h2 id='diffpwr.dep'>Monte Carlo Simulation for the correlation difference between dependent
correlations</h2><span id='topic+diffpwr.dep'></span>

<h3>Description</h3>

<p>Computation of a Monte Carlo simulation to estimate the statistical power of the
comparison between the correlations of a variable with two competing variables
that are also correlated with each other.</p>


<h3>Usage</h3>

<pre><code class='language-R'>diffpwr.dep(n,
            rho12,
            rho13,
            rho23,
            alpha = 0.05,
            n.samples = 1000,
            seed = 1234)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diffpwr.dep_+3A_n">n</code></td>
<td>
<p>Sample size to be tested in the Monte Carlo simulation.</p>
</td></tr>
<tr><td><code id="diffpwr.dep_+3A_rho12">rho12</code></td>
<td>
<p>Assumed population correlation between the criterion with which
both competing variables are correlated and the first of the two competing
variables.</p>
</td></tr>
<tr><td><code id="diffpwr.dep_+3A_rho13">rho13</code></td>
<td>
<p>Assumed population correlation between the criterion with which
both competing variables are correlated and the second of the two competing
variables.</p>
</td></tr>
<tr><td><code id="diffpwr.dep_+3A_rho23">rho23</code></td>
<td>
<p>Assumed population correlation between the two competing variables.</p>
</td></tr>
<tr><td><code id="diffpwr.dep_+3A_alpha">alpha</code></td>
<td>
<p>Type I error. Default is .05.</p>
</td></tr>
<tr><td><code id="diffpwr.dep_+3A_n.samples">n.samples</code></td>
<td>
<p>Number of samples generated in the Monte Carlo simulation. The
recommended minimum is 1,000 iterations, which is also the default.</p>
</td></tr>
<tr><td><code id="diffpwr.dep_+3A_seed">seed</code></td>
<td>
<p>To make the results reproducible, it is recommended to set a random
seed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Depending on the number of generated samples (n.samples), correlation
coefficients simulated. For each simulated sample, it is checked whether the
correlations r12 and r13 differ, given the correlation r23. The ratio of
simulated z-tests of the correlation difference tests exceeding the critical
z-value, given the intended alpha-level and sample size, equals the achieved
statistical power(see Muthén &amp; Muthén, 2002 &lt;doi:10.1207/S15328007SEM0904_8&gt;;
Robert &amp; Casella, 2010 &lt;doi:10.1007/978-1-4419-1576-4&gt;, for overviews of the
Monte Carlo method).
</p>
<p>It should be noted that the Pearson correlation coefficient is sensitive to
linear association, but also to a host of statistical issues such as univariate
and bivariate outliers, range restrictions, and heteroscedasticity (e.g., Duncan
&amp; Layard, 1973 &lt;doi:10.1093/BIOMET/60.3.551&gt;; Wilcox, 2013
&lt;doi:10.1016/C2010-0-67044-1&gt;). Thus, every power analysis requires that specific
statistical prerequisites are fulfilled and can be invalid with regard to the
actual data if the prerequisites do not hold, potentially biasing Type I error
rates.</p>


<h3>Value</h3>

<p>As dataframe with the following parameters
</p>
<table role = "presentation">
<tr><td><code>rho12</code></td>
<td>
<p>Assumed population correlation between the criterion with which
both competing variables are correlated and the first of the two competing
variables.</p>
</td></tr>
<tr><td><code>cov12</code></td>
<td>
<p>Coverage. Indicates the ratio of simulated confidence intervals
including the assumed effect size rho12.</p>
</td></tr>
<tr><td><code>bias12_M</code></td>
<td>
<p>Difference between the mean of the distribution of the simulated
correlations and rho12, divided by rho12.</p>
</td></tr>
<tr><td><code>bias12_Md</code></td>
<td>
<p>Difference between the median of the distribution of the
simulated correlations and rho12, divided by rho12.</p>
</td></tr>
<tr><td><code>rho13</code></td>
<td>
<p>Assumed population correlation between the criterion with which
both competing variables are correlated and the second of the two competing
variables.</p>
</td></tr>
<tr><td><code>cov13</code></td>
<td>
<p>Coverage. Indicates the ratio of simulated confidence intervals
including the assumed effect size rho13.</p>
</td></tr>
<tr><td><code>bias13_M</code></td>
<td>
<p>Difference between the mean of the distribution of the simulated
correlations and rho13, divided by rho13.</p>
</td></tr>
<tr><td><code>bias13_Md</code></td>
<td>
<p>Difference between the median of the distribution of the
simulated correlations and rho13, divided by rho13.</p>
</td></tr>
<tr><td><code>rho23</code></td>
<td>
<p>Assumed population correlation between the two competing variables.
</p>
</td></tr>
<tr><td><code>cov23</code></td>
<td>
<p>Coverage. Indicates the ratio of simulated confidence intervals
including the assumed effect size rho23.</p>
</td></tr>
<tr><td><code>bias23_M</code></td>
<td>
<p>Difference between the mean of the distribution of the simulated
correlations and rho23, divided by rho23.</p>
</td></tr>
<tr><td><code>bias23_Md</code></td>
<td>
<p>Difference between the median of the distribution of the
simulated correlations and rho23, divided by rho23.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Sample size to be tested in the Monte Carlo simulation.</p>
</td></tr>
<tr><td><code>pwr</code></td>
<td>
<p>Statistical power as the ratio of simulated difference tests that
yielded statistical significance.</p>
</td></tr>
</table>
<p>Biases should be as close to zero as possible and coverage should be ideally
between .91 and .98 (Muthén &amp; Muthén, 2002 &lt;doi:10.1207/S15328007SEM0904_8&gt;).</p>


<h3>Author(s)</h3>

<p>Christian Blötner
<a href="mailto:c.bloetner@gmail.com">c.bloetner@gmail.com</a></p>


<h3>References</h3>

<p>Duncan, G. T., &amp; Layard, M. W. (1973). A Monte-Carlo study of asymptotically
robust tests for correlation coefficients. Biometrika, 60, 551–558.
https://doi.org/10.1093/BIOMET/60.3.551
</p>
<p>Muthén, L. K., &amp; Muthén, B. O. (2002). How to use a Monte Carlo study to decide
on sample size and determine power. Structural Equation Modeling: A
Multidisciplinary Journal, 9(4), 599–620.
https://doi.org/10.1207/S15328007SEM0904_8
</p>
<p>Robert, C., &amp; Casella, G. (2010). Introducing Monte Carlo methods with R.
Springer. https://doi.org/10.1007/978-1-4419-1576-4
</p>
<p>Wilcox, R. (2013). Introduction to robust estimation and hypothesis testing.
Elsevier. https://doi.org/10.1016/C2010-0-67044-1</p>


<h3>Examples</h3>

<pre><code class='language-R'>diffpwr.dep(n.samples = 1000,
                      n = 250,
                      rho12 = .30,
                      rho13 = .45,
                      rho23 = .50,
                      alpha = .05,
                      seed = 1234)</code></pre>

<hr>
<h2 id='diffpwr.one'>Difference Between an Assumed Sample Correlation and a Population
Correlation</h2><span id='topic+diffpwr.one'></span>

<h3>Description</h3>

<p>Computation of a Monte Carlo simulation to estimate the statistical
power the correlation difference between an assumed sample correlation and an
assumed population correlation against which the correlation should be tested.</p>


<h3>Usage</h3>

<pre><code class='language-R'>diffpwr.one(n,
            r,
            rho,
            alpha = .05,
            n.samples = 1000,
            seed = 1234)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diffpwr.one_+3A_n">n</code></td>
<td>
<p>Sample size to be tested in the Monte Carlo simulation.</p>
</td></tr>
<tr><td><code id="diffpwr.one_+3A_r">r</code></td>
<td>
<p>Assumed observed correlation.</p>
</td></tr>
<tr><td><code id="diffpwr.one_+3A_rho">rho</code></td>
<td>
<p>Correlation coefficient against which to test (reflects the null
hypothesis).</p>
</td></tr>
<tr><td><code id="diffpwr.one_+3A_alpha">alpha</code></td>
<td>
<p>Type I error. Default is .05.</p>
</td></tr>
<tr><td><code id="diffpwr.one_+3A_n.samples">n.samples</code></td>
<td>
<p>Number of samples generated in the Monte Carlo simulation. The
recommended minimum is 1,000 iterations, which is also the default.</p>
</td></tr>
<tr><td><code id="diffpwr.one_+3A_seed">seed</code></td>
<td>
<p>To make the results reproducible, it is recommended to set a random
seed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Depending on the number of generated samples (n.samples), correlation
coefficients of size r are simulated. Confidence intervals are constructed
around the simulated correlation coefficients. For each simulated coefficient,
it is then checked whether the hypothesized correlation cofficient (rho) falls
within this interval. All correlations are automatically transformed with the
Fisher z-transformation prior to computations. The ratio of simulated confidence
intervals excluding the hypothesized coefficient equals the statistical power,
given the intended alpha-level and sample size (see Robert &amp; Casella, 2010
&lt;doi:10.1007/978-1-4419-1576-4&gt;, for an overview of the Monte Carlo method).
</p>
<p>It should be noted that the Pearson correlation coefficient is sensitive to
linear association, but also to a host of statistical issues such as univariate
and bivariate outliers, range restrictions, and heteroscedasticity (e.g., Duncan
&amp; Layard, 1973 &lt;doi:10.1093/BIOMET/60.3.551&gt;; Wilcox, 2013
&lt;doi:10.1016/C2010-0-67044-1&gt;). Thus, every power analysis requires that specific
statistical prerequisites are fulfilled and can be invalid with regard to the
actual data if the prerequisites do not hold, potentially biasing Type I error
rates.</p>


<h3>Value</h3>

<p>As dataframe with the following parameters
</p>
<table role = "presentation">
<tr><td><code>r</code></td>
<td>
<p>Empirically observed correlation.</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>Correlation against which r should be tested.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>The sample size entered in the function.</p>
</td></tr>
<tr><td><code>cov</code></td>
<td>
<p>Coverage. Indicates the ratio of simulated confidence intervals
including the assumed correlation r. Should be between .91 and .98 (Muthén
&amp; Muthén, 2002 &lt;doi:10.1207/S15328007SEM0904_8&gt;).</p>
</td></tr>
<tr><td><code>bias_M</code></td>
<td>
<p>Difference between the mean of the distribution of the simulated
correlations and rho, divided by rho.</p>
</td></tr>
<tr><td><code>bias_Md</code></td>
<td>
<p>Difference between the median of the distribution of the
simulated correlations and rho, divided by rho.</p>
</td></tr>
<tr><td><code>pwr</code></td>
<td>
<p>Statistical power as the ratio of simulated confidence intervals
excluding rho.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Blötner
<a href="mailto:c.bloetner@gmail.com">c.bloetner@gmail.com</a></p>


<h3>References</h3>

<p>Duncan, G. T., &amp; Layard, M. W. (1973). A Monte-Carlo study of asymptotically
robust tests for correlation coefficients. Biometrika, 60, 551–558.
https://doi.org/10.1093/BIOMET/60.3.551
</p>
<p>Muthén, L. K., &amp; Muthén, B. O. (2002). How to use a Monte Carlo study to decide
on sample size and determine power. Structural Equation Modeling: A
Multidisciplinary Journal, 9(4), 599–620.
https://doi.org/10.1207/S15328007SEM0904_8
</p>
<p>Robert, C., &amp; Casella, G. (2010). Introducing Monte Carlo methods with R.
Springer. https://doi.org/10.1007/978-1-4419-1576-4
</p>
<p>Wilcox, R. (2013). Introduction to robust estimation and hypothesis testing.
Elsevier. https://doi.org/10.1016/C2010-0-67044-1</p>


<h3>Examples</h3>

<pre><code class='language-R'>diffpwr.one(n = 500,
            r = .30,
            rho = .40,
            alpha = .05,
            n.samples = 1000,
            seed = 1234)
</code></pre>

<hr>
<h2 id='diffpwr.two'>Monte Carlo Simulation for the correlation difference between two
correlations that were observed in two independent samples</h2><span id='topic+diffpwr.two'></span>

<h3>Description</h3>

<p>Computation of a Monte Carlo simulation to estimate the statistical
power the correlation difference between the correlation coefficients detected
in two independent samples (e.g., original study and replication study).</p>


<h3>Usage</h3>

<pre><code class='language-R'>diffpwr.two(n1,
            n2,
            rho1,
            rho2,
            alpha = .05,
            n.samples = 1000,
            seed = 1234)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diffpwr.two_+3A_n1">n1</code></td>
<td>
<p>Sample size to be tested in the Monte Carlo simulation for the first
sample.</p>
</td></tr>
<tr><td><code id="diffpwr.two_+3A_n2">n2</code></td>
<td>
<p>Sample size to be tested in the Monte Carlo simulation for the
second sample.</p>
</td></tr>
<tr><td><code id="diffpwr.two_+3A_rho1">rho1</code></td>
<td>
<p>Assumed population correlation to be observed in the first sample.</p>
</td></tr>
<tr><td><code id="diffpwr.two_+3A_rho2">rho2</code></td>
<td>
<p>Assumed population correlation to be observed in the second sample.</p>
</td></tr>
<tr><td><code id="diffpwr.two_+3A_alpha">alpha</code></td>
<td>
<p>Type I error. Default is .05.</p>
</td></tr>
<tr><td><code id="diffpwr.two_+3A_n.samples">n.samples</code></td>
<td>
<p>Number of samples generated in the Monte Carlo simulation. The
recommended minimum is 1,000 iterations, which is also the default.</p>
</td></tr>
<tr><td><code id="diffpwr.two_+3A_seed">seed</code></td>
<td>
<p>To make the results reproducible, a random seed is specified.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Depending on the number of generated samples (n.samples), correlation
coefficients are simulated. For each simulated pair of coefficients, it is then
checked whether the confidence intervals (with given alpha level) of the
correlations overlap. All correlations are automatically transformed with the
Fisher z-transformation prior to computations. The ratio of simulated non-
overlapping confidence intervals equals the statistical power, given the
alpha-level and sample sizes (see Robert &amp; Casella, 2010
&lt;doi:10.1007/978-1-4419-1576-4&gt;, for an overview of the Monte Carlo method).
</p>
<p>It should be noted that the Pearson correlation coefficient is sensitive to
linear association, but also to a host of statistical issues such as univariate
and bivariate outliers, range restrictions, and heteroscedasticity (e.g., Duncan
&amp; Layard, 1973 &lt;doi:10.1093/BIOMET/60.3.551&gt;; Wilcox, 2013
&lt;doi:10.1016/C2010-0-67044-1&gt;). Thus, every power analysis requires that specific
statistical prerequisites are fulfilled and can be invalid with regard to the
actual data if the prerequisites do not hold, potentially biasing Type I error
rates.</p>


<h3>Value</h3>

<p>As dataframe with the following parameters
</p>
<table role = "presentation">
<tr><td><code>rho1</code></td>
<td>
<p>Assumed population correlation to be observed in the first sample.</p>
</td></tr>
<tr><td><code>n1</code></td>
<td>
<p>Sample size of the first sample.</p>
</td></tr>
<tr><td><code>cov1</code></td>
<td>
<p>Coverage. Ratio of simulated confidence intervals including rho1.</p>
</td></tr>
<tr><td><code>bias1_M</code></td>
<td>
<p>Difference between the mean of the distribution of the simulated
correlations and rho1, divided by rho1.</p>
</td></tr>
<tr><td><code>bias1_Md</code></td>
<td>
<p>Difference between the median of the distribution of the
simulated correlations and rho1, divided by rho1.</p>
</td></tr>
<tr><td><code>rho2</code></td>
<td>
<p>Assumed population correlation to be observed in the second sample.</p>
</td></tr>
<tr><td><code>n2</code></td>
<td>
<p>The sample size of the second sample.</p>
</td></tr>
<tr><td><code>cov2</code></td>
<td>
<p>Coverage. Ratio of simulated confidence intervals including rho2.</p>
</td></tr>
<tr><td><code>bias2_M</code></td>
<td>
<p>Difference between the mean of the distribution of the simulated
correlations and rho2, divided by rho2.</p>
</td></tr>
<tr><td><code>bias2_Md</code></td>
<td>
<p>Difference between the median of the distribution of the
simulated correlations and rho2, divided by rho2.</p>
</td></tr>
<tr><td><code>pwr</code></td>
<td>
<p>Statistical power as the ratio of simulated non-verlapping confidence
intervals.</p>
</td></tr>
</table>
<p>Biases should be as close to zero as possible and coverage should be ideally
between .91 and .98 (Muthén &amp; Muthén, 2002 &lt;doi:10.1207/S15328007SEM0904_8&gt;).</p>


<h3>Author(s)</h3>

<p>Christian Blötner
<a href="mailto:c.bloetner@gmail.com">c.bloetner@gmail.com</a></p>


<h3>References</h3>

<p>Duncan, G. T., &amp; Layard, M. W. (1973). A Monte-Carlo study of asymptotically
robust tests for correlation coefficients. Biometrika, 60, 551–558.
https://doi.org/10.1093/BIOMET/60.3.551
</p>
<p>Muthén, L. K., &amp; Muthén, B. O. (2002). How to use a Monte Carlo study to decide
on sample size and determine power. Structural Equation Modeling: A
Multidisciplinary Journal, 9(4), 599–620.
https://doi.org/10.1207/S15328007SEM0904_8
</p>
<p>Robert, C., &amp; Casella, G. (2010). Introducing Monte Carlo methods with R.
Springer. https://doi.org/10.1007/978-1-4419-1576-4
</p>
<p>Wilcox, R. (2013). Introduction to robust estimation and hypothesis testing.
Elsevier. https://doi.org/10.1016/C2010-0-67044-1</p>


<h3>Examples</h3>

<pre><code class='language-R'>diffpwr.two(n1 = 1000,
            n2 = 594,
            rho1 = .45,
            rho2 = .39,
            alpha = .05,
            n.samples = 1000,
            seed = 1234)</code></pre>

<hr>
<h2 id='visual_mc'>Visualization of the simulated parameters</h2><span id='topic+visual_mc'></span>

<h3>Description</h3>

<p>To evaluate the quality of the Monte Carlo simulation beyond bias
and coverage parameters (Muthén &amp; Muthén, 2002), it can be helpful to also
inspect the simulated parameters visually. To this end, visual_mc() can be used
to visualize the simulated parameters (including corresponding confidence
intervals) in relation to the targeted parameter.</p>


<h3>Usage</h3>

<pre><code class='language-R'>visual_mc(rho,
          n,
          alpha = .05,
          n.intervals = 100,
          seed = 1234)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="visual_mc_+3A_rho">rho</code></td>
<td>
<p>Targeted correlation coefficient of the simulation.</p>
</td></tr>
<tr><td><code id="visual_mc_+3A_n">n</code></td>
<td>
<p>An integer reflecting the sample size.</p>
</td></tr>
<tr><td><code id="visual_mc_+3A_alpha">alpha</code></td>
<td>
<p>Type I error. Default is .05.</p>
</td></tr>
<tr><td><code id="visual_mc_+3A_n.intervals">n.intervals</code></td>
<td>
<p>An integer reflecting the number of simulated parameters
that should be visualized in the graphic. Default is 100.</p>
</td></tr>
<tr><td><code id="visual_mc_+3A_seed">seed</code></td>
<td>
<p>To make the results reproducible, a random seed is specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot in which the targeted correlation coefficient is visualized with a dashed
red line and the simulated correlation coefficients are visualized by black
squares and confidence intervals (level depending on the specification made in
the argument alpha).</p>


<h3>Author(s)</h3>

<p>Christian Blötner
<a href="mailto:c.bloetner@gmail.com">c.bloetner@gmail.com</a></p>


<h3>References</h3>

<p>Muthén, L. K., &amp; Muthén, B. O. (2002). How to use a Monte Carlo study to decide
on sample size and determine power. Structural Equation Modeling: A
Multidisciplinary Journal, 9(4), 599–620.
https://doi.org/10.1207/S15328007SEM0904_8</p>


<h3>Examples</h3>

<pre><code class='language-R'>visual_mc(rho = .25,
                    n = 300,
                    alpha = .05,
                    n.intervals = 100,
                    seed = 1234)</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
