<!DOCTYPE html><html><head><title>Help for package cuda.ml</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cuda.ml}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cuda_ml_agglomerative_clustering'><p>Perform Single-Linkage Agglomerative Clustering.</p></a></li>
<li><a href='#cuda_ml_can_predict_class_probabilities'><p>Determine whether a CuML model can predict class probabilities.</p></a></li>
<li><a href='#cuda_ml_dbscan'><p>Run the DBSCAN clustering algorithm.</p></a></li>
<li><a href='#cuda_ml_elastic_net'><p>Train a linear model using elastic regression.</p></a></li>
<li><a href='#cuda_ml_fil_enabled'><p>Determine whether Forest Inference Library (FIL) functionalities are enabled</p>
in the current installation of cuda.ml.</a></li>
<li><a href='#cuda_ml_fil_load_model'><p>Load a XGBoost or LightGBM model file.</p></a></li>
<li><a href='#cuda_ml_inverse_transform'><p>Apply the inverse transformation defined by a trained cuML model.</p></a></li>
<li><a href='#cuda_ml_is_classifier'><p>Determine whether a CuML model is a classifier.</p></a></li>
<li><a href='#cuda_ml_kmeans'><p>Run the K means clustering algorithm.</p></a></li>
<li><a href='#cuda_ml_knn'><p>Build a KNN model.</p></a></li>
<li><a href='#cuda_ml_knn_algo_ivfflat'><p>Build a specification for the &quot;ivfflat&quot; KNN query algorithm.</p></a></li>
<li><a href='#cuda_ml_knn_algo_ivfpq'><p>Build a specification for the &quot;ivfpq&quot; KNN query algorithm.</p></a></li>
<li><a href='#cuda_ml_knn_algo_ivfsq'><p>Build a specification for the &quot;ivfsq&quot; KNN query algorithm.</p></a></li>
<li><a href='#cuda_ml_lasso'><p>Train a linear model using LASSO regression.</p></a></li>
<li><a href='#cuda_ml_logistic_reg'><p>Train a logistic regression model.</p></a></li>
<li><a href='#cuda_ml_ols'><p>Train a OLS model.</p></a></li>
<li><a href='#cuda_ml_pca'><p>Perform principal component analysis.</p></a></li>
<li><a href='#cuda_ml_rand_forest'><p>Train a random forest model.</p></a></li>
<li><a href='#cuda_ml_rand_proj'><p>Random projection for dimensionality reduction.</p></a></li>
<li><a href='#cuda_ml_ridge'><p>Train a linear model using ridge regression.</p></a></li>
<li><a href='#cuda_ml_serialize'><p>Serialize a CuML model</p></a></li>
<li><a href='#cuda_ml_sgd'><p>Train a MBSGD linear model.</p></a></li>
<li><a href='#cuda_ml_svm'><p>Train a SVM model.</p></a></li>
<li><a href='#cuda_ml_transform'><p>Transform data using a trained cuML model.</p></a></li>
<li><a href='#cuda_ml_tsne'><p>t-distributed Stochastic Neighbor Embedding.</p></a></li>
<li><a href='#cuda_ml_tsvd'><p>Truncated SVD.</p></a></li>
<li><a href='#cuda_ml_umap'><p>Uniform Manifold Approximation and Projection (UMAP) for dimension reduction.</p></a></li>
<li><a href='#cuda_ml_unserialize'><p>Unserialize a CuML model state</p></a></li>
<li><a href='#cuda.ml'><p>cuda.ml</p></a></li>
<li><a href='#cuML_major_version'><p>Get the major version of the RAPIDS cuML shared library cuda.ml was linked</p>
to.</a></li>
<li><a href='#cuML_minor_version'><p>Get the minor version of the RAPIDS cuML shared library cuda.ml was linked</p>
to.</a></li>
<li><a href='#has_cuML'><p>Determine whether cuda.ml was linked to a valid version of the RAPIDS cuML</p>
shared library.</a></li>
<li><a href='#predict.cuda_ml_fil'><p>Make predictions on new data points.</p></a></li>
<li><a href='#predict.cuda_ml_knn'><p>Make predictions on new data points.</p></a></li>
<li><a href='#predict.cuda_ml_linear_model'><p>Make predictions on new data points.</p></a></li>
<li><a href='#predict.cuda_ml_logistic_reg'><p>Make predictions on new data points.</p></a></li>
<li><a href='#predict.cuda_ml_rand_forest'><p>Make predictions on new data points.</p></a></li>
<li><a href='#predict.cuda_ml_svm'><p>Make predictions on new data points.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>R Interface for the RAPIDS cuML Suite of Libraries</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.2</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Falbel &lt;daniel@rstudio.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>R interface for RAPIDS cuML (<a href="https://github.com/rapidsai/cuml">https://github.com/rapidsai/cuml</a>),
    a suite of GPU-accelerated machine learning libraries powered by CUDA
    (<a href="https://en.wikipedia.org/wiki/CUDA">https://en.wikipedia.org/wiki/CUDA</a>).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://mlverse.github.io/cuda.ml/">https://mlverse.github.io/cuda.ml/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlverse/cuda.ml/issues">https://github.com/mlverse/cuda.ml/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>ellipsis, hardhat, parsnip, Rcpp (&ge; 1.0.6), rlang (&ge; 0.1.4)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>callr, glmnet, MASS, magrittr, mlbench, purrr, reticulate,
testthat, xgboost</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>OS_type:</td>
<td>unix</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>RAPIDS cuML (see https://rapids.ai/start.html)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-01-07 22:00:35 UTC; yitaoli</td>
</tr>
<tr>
<td>Author:</td>
<td>Yitao Li <a href="https://orcid.org/0000-0002-1261-905X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cph],
  Tomasz Kalinowski [cph, ctb],
  Daniel Falbel [aut, cre, cph],
  RStudio [cph, fnd]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-01-08 01:42:47 UTC</td>
</tr>
</table>
<hr>
<h2 id='cuda_ml_agglomerative_clustering'>Perform Single-Linkage Agglomerative Clustering.</h2><span id='topic+cuda_ml_agglomerative_clustering'></span>

<h3>Description</h3>

<p>Recursively merge the pair of clusters that minimally increases a given
linkage distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_agglomerative_clustering(
  x,
  n_clusters = 2L,
  metric = c("euclidean", "l1", "l2", "manhattan", "cosine"),
  connectivity = c("pairwise", "knn"),
  n_neighbors = 15L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_agglomerative_clustering_+3A_x">x</code></td>
<td>
<p>The input matrix or dataframe. Each data point should be a row
and should consist of numeric values only.</p>
</td></tr>
<tr><td><code id="cuda_ml_agglomerative_clustering_+3A_n_clusters">n_clusters</code></td>
<td>
<p>The number of clusters to find. Default: 2L.</p>
</td></tr>
<tr><td><code id="cuda_ml_agglomerative_clustering_+3A_metric">metric</code></td>
<td>
<p>Metric used for linkage computation. Must be one of
&quot;euclidean&quot;, &quot;l1&quot;, &quot;l2&quot;, &quot;manhattan&quot;, &quot;cosine&quot;. If connectivity is
&quot;knn&quot; then only &quot;euclidean&quot; is accepted. Default: &quot;euclidean&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_agglomerative_clustering_+3A_connectivity">connectivity</code></td>
<td>
<p>The type of connectivity matrix to compute. Must be one
of &quot;pairwise&quot;, &quot;knn&quot;. Default: &quot;pairwise&quot;.
- 'pairwise' will compute the entire fully-connected graph of pairwise
distances between each set of points. This is the fastest to compute
and can be very fast for smaller datasets but requires O(n^2) space.
- 'knn' will sparsify the fully-connected connectivity matrix to save
memory and enable much larger inputs. &quot;n_neighbors&quot; will control the
amount of memory used and the graph will be connected automatically in
the event &quot;n_neighbors&quot; was not large enough to connect it.</p>
</td></tr>
<tr><td><code id="cuda_ml_agglomerative_clustering_+3A_n_neighbors">n_neighbors</code></td>
<td>
<p>The number of neighbors to compute when
<code>connectivity</code> is &quot;knn&quot;. Default: 15L.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A clustering object with the following attributes:
&quot;n_clusters&quot;: The number of clusters found by the algorithm.
&quot;children&quot;: The children of each non-leaf node. Values less than
<code>nrow(x)</code> correspond to leaves of the tree which are the original
samples. <code>children[i + 1][1]</code> and <code>children[i + 1][2]</code> were
merged to form node <code>(nrow(x) + i)</code> in the <code>i</code>-th iteration.
&quot;labels&quot;: cluster label of each data point.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)
library(MASS)
library(magrittr)
library(purrr)

set.seed(0L)

gen_pts &lt;- function() {
  centers &lt;- list(c(1000, 1000), c(-1000, -1000), c(-1000, 1000))
  pts &lt;- centers %&gt;%
    map(~ mvrnorm(50, mu = .x, Sigma = diag(2)))

  rlang::exec(rbind, !!!pts) %&gt;% as.matrix()
}

clust &lt;- cuda_ml_agglomerative_clustering(
  x = gen_pts(),
  metric = "euclidean",
  n_clusters = 3L
)

print(clust$labels)
</code></pre>

<hr>
<h2 id='cuda_ml_can_predict_class_probabilities'>Determine whether a CuML model can predict class probabilities.</h2><span id='topic+cuda_ml_can_predict_class_probabilities'></span>

<h3>Description</h3>

<p>Given a trained CuML model, return <code>TRUE</code> if the model is a classifier
and is capable of outputting class probabilities as prediction results (e.g.,
if the model is a KNN or an ensemble classifier), otherwise return
<code>FALSE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_can_predict_class_probabilities(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_can_predict_class_probabilities_+3A_model">model</code></td>
<td>
<p>A trained CuML model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical value indicating whether the model supports outputting
class probabilities.
</p>

<hr>
<h2 id='cuda_ml_dbscan'>Run the DBSCAN clustering algorithm.</h2><span id='topic+cuda_ml_dbscan'></span>

<h3>Description</h3>

<p>Run the DBSCAN (Density-based spatial clustering of applications with noise)
clustering algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_dbscan(
  x,
  min_pts,
  eps,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_dbscan_+3A_x">x</code></td>
<td>
<p>The input matrix or dataframe. Each data point should be a row
and should consist of numeric values only.</p>
</td></tr>
<tr><td><code id="cuda_ml_dbscan_+3A_min_pts">min_pts</code>, <code id="cuda_ml_dbscan_+3A_eps">eps</code></td>
<td>
<p>A point 'p' is a core point if at least 'min_pts' are
within distance 'eps' from it.</p>
</td></tr>
<tr><td><code id="cuda_ml_dbscan_+3A_cuml_log_level">cuML_log_level</code></td>
<td>
<p>Log level within cuML library functions. Must be one of
&quot;off&quot;, &quot;critical&quot;, &quot;error&quot;, &quot;warn&quot;, &quot;info&quot;, &quot;debug&quot;, &quot;trace&quot;.
Default: off.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the cluster assignments of all data points. A data
point not belonging to any cluster (i.e., &quot;noise&quot;) will have NA its cluster
assignment.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(cuda.ml)
library(magrittr)

gen_pts &lt;- function() {
  centroids &lt;- list(c(1000, 1000), c(-1000, -1000), c(-1000, 1000))

  pts &lt;- centroids %&gt;%
    purrr::map(~ MASS::mvrnorm(10, mu = .x, Sigma = diag(2)))

  rlang::exec(rbind, !!!pts)
}

m &lt;- gen_pts()
clusters &lt;- cuda_ml_dbscan(m, min_pts = 5, eps = 3)

print(clusters)
</code></pre>

<hr>
<h2 id='cuda_ml_elastic_net'>Train a linear model using elastic regression.</h2><span id='topic+cuda_ml_elastic_net'></span><span id='topic+cuda_ml_elastic_net.default'></span><span id='topic+cuda_ml_elastic_net.data.frame'></span><span id='topic+cuda_ml_elastic_net.matrix'></span><span id='topic+cuda_ml_elastic_net.formula'></span><span id='topic+cuda_ml_elastic_net.recipe'></span>

<h3>Description</h3>

<p>Train a linear model with combined L1 and L2 priors as the regularizer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_elastic_net(x, ...)

## Default S3 method:
cuda_ml_elastic_net(x, ...)

## S3 method for class 'data.frame'
cuda_ml_elastic_net(
  x,
  y,
  alpha = 1,
  l1_ratio = 0.5,
  max_iter = 1000L,
  tol = 0.001,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  selection = c("cyclic", "random"),
  ...
)

## S3 method for class 'matrix'
cuda_ml_elastic_net(
  x,
  y,
  alpha = 1,
  l1_ratio = 0.5,
  max_iter = 1000L,
  tol = 0.001,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  selection = c("cyclic", "random"),
  ...
)

## S3 method for class 'formula'
cuda_ml_elastic_net(
  formula,
  data,
  alpha = 1,
  l1_ratio = 0.5,
  max_iter = 1000L,
  tol = 0.001,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  selection = c("cyclic", "random"),
  ...
)

## S3 method for class 'recipe'
cuda_ml_elastic_net(
  x,
  data,
  alpha = 1,
  l1_ratio = 0.5,
  max_iter = 1000L,
  tol = 0.001,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  selection = c("cyclic", "random"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_elastic_net_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>
<p>* A __data frame__ of predictors.
* A __matrix__ of predictors.
* A __recipe__ specifying a set of preprocessing steps
* created from [recipes::recipe()].
* A __formula__ specifying the predictors and the outcome.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_y">y</code></td>
<td>
<p>A numeric vector (for regression) or factor (for classification) of
desired responses.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_alpha">alpha</code></td>
<td>
<p>Multiplier of the penalty term (i.e., the result would become
and Ordinary Least Square model if <code>alpha</code> were set to 0). Default: 1.
For numerical reasons, running elastic regression with <code>alpha</code> set to
0 is not advised. For the <code>alpha</code>-equals-to-0 scenario, one should use
<code>cuda_ml_ols</code> to train an OLS model instead.
Default: 1.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_l1_ratio">l1_ratio</code></td>
<td>
<p>The ElasticNet mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1
penalty.
For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.
The penalty term is computed using the following formula:
penalty = <code>alpha</code> * <code>l1_ratio</code> * ||w||_1 +
0.5 * <code>alpha</code> * (1 - <code>l1_ratio</code>) * ||w||^2_2
where ||w||_1 is the L1 norm of the coefficients, and ||w||_2 is the L2
norm of the coefficients.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of coordinate descent iterations.
Default: 1000L.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_tol">tol</code></td>
<td>
<p>Stop the coordinate descent when the duality gap is below this
threshold. Default: 1e-3.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>If TRUE, then the model tries to correct for the global
mean of the response variable. If FALSE, then the model expects data to be
centered. Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_normalize_input">normalize_input</code></td>
<td>
<p>Ignored when <code>fit_intercept</code> is FALSE. If TRUE,
then the predictors will be normalized to have a L2 norm of 1.
Default: FALSE.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_selection">selection</code></td>
<td>
<p>If &quot;random&quot;, then instead of updating coefficients in cyclic
order, a random coefficient is updated in each iteration. Default: &quot;cyclic&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="cuda_ml_elastic_net_+3A_data">data</code></td>
<td>
<p>When a __recipe__ or __formula__ is used, <code>data</code> is
specified as a  __data frame__ containing the predictors and (if
applicable) the outcome.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An elastic net regressor that can be used with the 'predict' S3
generic to make predictions on new data points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

model &lt;- cuda_ml_elastic_net(
  formula = mpg ~ ., data = mtcars, alpha = 1e-3, l1_ratio = 0.6
)
cuda_ml_predictions &lt;- predict(model, mtcars)

# predictions will be comparable to those from a `glmnet` model with `lambda`
# set to 1e-3 and `alpha` set to 0.6
# (in `glmnet`, `lambda` is the weight of the penalty term, and `alpha` is
#  the elastic mixing parameter between L1 and L2 penalties.

library(glmnet)

glmnet_model &lt;- glmnet(
  x = as.matrix(mtcars[names(mtcars) != "mpg"]), y = mtcars$mpg,
  alpha = 0.6, lambda = 1e-3, nlambda = 1, standardize = FALSE
)

glm_predictions &lt;- predict(
  glmnet_model, as.matrix(mtcars[names(mtcars) != "mpg"]),
  s = 0
)

print(
  all.equal(
    as.numeric(glm_predictions),
    cuda_ml_predictions$.pred,
    tolerance = 1e-2
  )
)
</code></pre>

<hr>
<h2 id='cuda_ml_fil_enabled'>Determine whether Forest Inference Library (FIL) functionalities are enabled
in the current installation of cuda.ml.</h2><span id='topic+cuda_ml_fil_enabled'></span>

<h3>Description</h3>

<p>CuML Forest Inference Library (FIL) functionalities (see
https://github.com/rapidsai/cuml/tree/main/python/cuml/fil#readme) will
require Treelite C API. If you need FIL to run tree-based model ensemble on
GPU, and <code>fil_enabled()</code> returns FALSE, then please consider installing
Treelite and then re-installing cuda.ml.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_fil_enabled()
</code></pre>


<h3>Value</h3>

<p>A logical value indicating whether the Forest Inference Library (FIL)
functionalities are enabled.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (cuda_ml_fil_enabled()) {
  # run GPU-accelerated Forest Inference Library (FIL) functionalities
} else {
  message(
    "FIL functionalities are disabled in the current installation of ",
    "{cuda.ml}. Please reinstall Treelite C library first, and then re-install",
    " {cuda.ml} to enable FIL."
  )
}
</code></pre>

<hr>
<h2 id='cuda_ml_fil_load_model'>Load a XGBoost or LightGBM model file.</h2><span id='topic+cuda_ml_fil_load_model'></span>

<h3>Description</h3>

<p>Load a XGBoost or LightGBM model file using Treelite. The resulting model
object can be used to perform high-throughput batch inference on new data
points using the GPU acceleration functionality from the CuML Forest
Inference Library (FIL).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_fil_load_model(
  filename,
  mode = c("classification", "regression"),
  model_type = c("xgboost", "lightgbm"),
  algo = c("auto", "naive", "tree_reorg", "batch_tree_reorg"),
  threshold = 0.5,
  storage_type = c("auto", "dense", "sparse"),
  threads_per_tree = 1L,
  n_items = 0L,
  blocks_per_sm = 0L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_fil_load_model_+3A_filename">filename</code></td>
<td>
<p>Path to the saved model file.</p>
</td></tr>
<tr><td><code id="cuda_ml_fil_load_model_+3A_mode">mode</code></td>
<td>
<p>Type of task to be performed by the model. Must be one of
&quot;classification&quot;, &quot;regression&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_fil_load_model_+3A_model_type">model_type</code></td>
<td>
<p>Format of the saved model file. Notice if <code>filename</code>
ends with &quot;.json&quot; and <code>model_type</code> is &quot;xgboost&quot;, then cuda.ml will
assume the model file is in XGBoost JSON (instead of binary) format.
Default: &quot;xgboost&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_fil_load_model_+3A_algo">algo</code></td>
<td>
<p>Type of the algorithm for inference, must be one of the
following.
- &quot;auto&quot;:
Choose the algorithm automatically. Currently 'batch_tree_reorg' is
used for dense storage, and 'naive' for sparse storage.
- &quot;naive&quot;:
Simple inference using shared memory.
- &quot;tree_reorg&quot;:
Similar to naive but with trees rearranged to be more coalescing-
friendly.
- &quot;batch_tree_reorg&quot;:
Similar to 'tree_reorg' but predicting multiple rows per thread
block.
Default: &quot;auto&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_fil_load_model_+3A_threshold">threshold</code></td>
<td>
<p>Class probability threshold for classification. Ignored for
regression tasks. Default: 0.5.</p>
</td></tr>
<tr><td><code id="cuda_ml_fil_load_model_+3A_storage_type">storage_type</code></td>
<td>
<p>In-memory storage format of the FIL model. Must be one of
the following.
- &quot;auto&quot;:
Choose the storage type automatically,
- &quot;dense&quot;:
Create a dense forest,
- &quot;sparse&quot;:
Create a sparse forest. Requires <code>algo</code> to be 'naive' or 'auto'.</p>
</td></tr>
<tr><td><code id="cuda_ml_fil_load_model_+3A_threads_per_tree">threads_per_tree</code></td>
<td>
<p>If &gt;1, then have multiple (neighboring) threads infer
on the same tree within a block, which will improve memory bandwith near
tree root (but consuming more shared memory). Default: 1L.</p>
</td></tr>
<tr><td><code id="cuda_ml_fil_load_model_+3A_n_items">n_items</code></td>
<td>
<p>Number of input samples each thread processes. If 0, then
choose (up to 4) that fit into shared memory. Default: 0L.</p>
</td></tr>
<tr><td><code id="cuda_ml_fil_load_model_+3A_blocks_per_sm">blocks_per_sm</code></td>
<td>
<p>Indicates how CuML should determine the number of thread
blocks to lauch for the inference kernel.
- 0:
Launches the number of blocks proportional to the number of data points.
- &gt;= 1:
Attempts to lauch <code>blocks_per_sm</code> blocks for each streaming
multiprocessor.
This will fail if <code>blocks_per_sm</code> blocks result in more threads than
the maximum supported number of threads per GPU. Even if successful, it
is not guaranteed that <code>blocks_per_sm</code> blocks will run on an SM
concurrently.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A GPU-accelerated FIL model that can be used with the 'predict' S3
generic to make predictions on new data points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)
library(xgboost)

model_path &lt;- file.path(tempdir(), "xgboost.model")

model &lt;- xgboost(
  data = as.matrix(mtcars[names(mtcars) != "mpg"]),
  label = as.matrix(mtcars["mpg"]),
  max.depth = 6,
  eta = 1,
  nthread = 2,
  nrounds = 20,
  objective = "reg:squarederror"
)

xgb.save(model, model_path)

model &lt;- cuda_ml_fil_load_model(
  model_path,
  mode = "regression",
  model_type = "xgboost"
)

preds &lt;- predict(model, mtcars[names(mtcars) != "mpg"])

print(preds)
</code></pre>

<hr>
<h2 id='cuda_ml_inverse_transform'>Apply the inverse transformation defined by a trained cuML model.</h2><span id='topic+cuda_ml_inverse_transform'></span>

<h3>Description</h3>

<p>Given a trained cuML model, apply the inverse transformation defined by that
model to an input dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_inverse_transform(model, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_inverse_transform_+3A_model">model</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="cuda_ml_inverse_transform_+3A_x">x</code></td>
<td>
<p>The dataset to be transformed.</p>
</td></tr>
<tr><td><code id="cuda_ml_inverse_transform_+3A_...">...</code></td>
<td>
<p>Additional model-specific parameters (if any).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The transformed data points.
</p>

<hr>
<h2 id='cuda_ml_is_classifier'>Determine whether a CuML model is a classifier.</h2><span id='topic+cuda_ml_is_classifier'></span>

<h3>Description</h3>

<p>Given a trained CuML model, return <code>TRUE</code> if the model is a classifier,
otherwise <code>FALSE</code> (e.g., if the model is a regressor).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_is_classifier(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_is_classifier_+3A_model">model</code></td>
<td>
<p>A trained CuML model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical value indicating whether the model is a classifier.
</p>

<hr>
<h2 id='cuda_ml_kmeans'>Run the K means clustering algorithm.</h2><span id='topic+cuda_ml_kmeans'></span>

<h3>Description</h3>

<p>Run the K means clustering algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_kmeans(
  x,
  k,
  max_iters = 300,
  tol = 0,
  init_method = c("kmeans++", "random"),
  seed = 0L,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_kmeans_+3A_x">x</code></td>
<td>
<p>The input matrix or dataframe. Each data point should be a row
and should consist of numeric values only.</p>
</td></tr>
<tr><td><code id="cuda_ml_kmeans_+3A_k">k</code></td>
<td>
<p>The number of clusters.</p>
</td></tr>
<tr><td><code id="cuda_ml_kmeans_+3A_max_iters">max_iters</code></td>
<td>
<p>Maximum number of iterations. Default: 300.</p>
</td></tr>
<tr><td><code id="cuda_ml_kmeans_+3A_tol">tol</code></td>
<td>
<p>Relative tolerance with regards to inertia to declare convergence.
Default: 0 (i.e., do not use inertia-based stopping criterion).</p>
</td></tr>
<tr><td><code id="cuda_ml_kmeans_+3A_init_method">init_method</code></td>
<td>
<p>Method for initializing the centroids. Valid methods
include &quot;kmeans++&quot;, &quot;random&quot;, or a matrix of k rows, each row specifying
the initial value of a centroid. Default: &quot;kmeans++&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_kmeans_+3A_seed">seed</code></td>
<td>
<p>Seed to the random number generator. Default: 0.</p>
</td></tr>
<tr><td><code id="cuda_ml_kmeans_+3A_cuml_log_level">cuML_log_level</code></td>
<td>
<p>Log level within cuML library functions. Must be one of
&quot;off&quot;, &quot;critical&quot;, &quot;error&quot;, &quot;warn&quot;, &quot;info&quot;, &quot;debug&quot;, &quot;trace&quot;.
Default: off.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the cluster assignments and the centroid of each
cluster. Each centroid will be a column within the 'centroids' matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

kclust &lt;- cuda_ml_kmeans(
  iris[names(iris) != "Species"],
  k = 3, max_iters = 100
)

print(kclust)
</code></pre>

<hr>
<h2 id='cuda_ml_knn'>Build a KNN model.</h2><span id='topic+cuda_ml_knn'></span><span id='topic+cuda_ml_knn.default'></span><span id='topic+cuda_ml_knn.data.frame'></span><span id='topic+cuda_ml_knn.matrix'></span><span id='topic+cuda_ml_knn.formula'></span><span id='topic+cuda_ml_knn.recipe'></span>

<h3>Description</h3>

<p>Build a k-nearest-model for classification or regression tasks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_knn(x, ...)

## Default S3 method:
cuda_ml_knn(x, ...)

## S3 method for class 'data.frame'
cuda_ml_knn(
  x,
  y,
  algo = c("brute", "ivfflat", "ivfpq", "ivfsq"),
  metric = c("euclidean", "l2", "l1", "cityblock", "taxicab", "manhattan",
    "braycurtis", "canberra", "minkowski", "chebyshev", "jensenshannon", "cosine",
    "correlation"),
  p = 2,
  neighbors = 5L,
  ...
)

## S3 method for class 'matrix'
cuda_ml_knn(
  x,
  y,
  algo = c("brute", "ivfflat", "ivfpq", "ivfsq"),
  metric = c("euclidean", "l2", "l1", "cityblock", "taxicab", "manhattan",
    "braycurtis", "canberra", "minkowski", "chebyshev", "jensenshannon", "cosine",
    "correlation"),
  p = 2,
  neighbors = 5L,
  ...
)

## S3 method for class 'formula'
cuda_ml_knn(
  formula,
  data,
  algo = c("brute", "ivfflat", "ivfpq", "ivfsq"),
  metric = c("euclidean", "l2", "l1", "cityblock", "taxicab", "manhattan",
    "braycurtis", "canberra", "minkowski", "chebyshev", "jensenshannon", "cosine",
    "correlation"),
  p = 2,
  neighbors = 5L,
  ...
)

## S3 method for class 'recipe'
cuda_ml_knn(
  x,
  data,
  algo = c("brute", "ivfflat", "ivfpq", "ivfsq"),
  metric = c("euclidean", "l2", "l1", "cityblock", "taxicab", "manhattan",
    "braycurtis", "canberra", "minkowski", "chebyshev", "jensenshannon", "cosine",
    "correlation"),
  p = 2,
  neighbors = 5L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_knn_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>
<p>* A __data frame__ of predictors.
* A __matrix__ of predictors.
* A __recipe__ specifying a set of preprocessing steps
* created from [recipes::recipe()].
* A __formula__ specifying the predictors and the outcome.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_+3A_y">y</code></td>
<td>
<p>A numeric vector (for regression) or factor (for classification) of
desired responses.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_+3A_algo">algo</code></td>
<td>
<p>The query algorithm to use. Must be one of
&quot;brute&quot;, &quot;ivfflat&quot;, &quot;ivfpq&quot;, &quot;ivfsq&quot; or a KNN algorithm specification
constructed using the <code>cuda_ml_knn_algo_*</code> family of functions.
If the algorithm is specified by one of the <code>cuda_ml_knn_algo_*</code>
functions, then values of all required parameters of the algorithm will
need to be specified explicitly.
If the algorithm is specified by a character vector, then parameters for
the algorithm are generated automatically.
</p>
<p>Descriptions of supported algorithms:
- &quot;brute&quot;: for brute-force, slow but produces exact results.
- &quot;ivfflat&quot;: for inverted file, divide the dataset in partitions
and perform search on relevant partitions only.
- &quot;ivfpq&quot;: for inverted file and product quantization (vectors
are divided into sub-vectors, and each sub-vector is encoded
using intermediary k-means clusterings to provide partial
information).
- &quot;ivfsq&quot;: for inverted file and scalar quantization (vectors components
are quantized into reduced binary representation allowing
faster distances calculations).
</p>
<p>Default: &quot;brute&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_+3A_metric">metric</code></td>
<td>
<p>Distance metric to use. Must be one of &quot;euclidean&quot;, &quot;l2&quot;,
&quot;l1&quot;, &quot;cityblock&quot;, &quot;taxicab&quot;, &quot;manhattan&quot;, &quot;braycurtis&quot;, &quot;canberra&quot;,
&quot;minkowski&quot;, &quot;lp&quot;, &quot;chebyshev&quot;, &quot;linf&quot;, &quot;jensenshannon&quot;, &quot;cosine&quot;,
&quot;correlation&quot;.
Default: &quot;euclidean&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_+3A_p">p</code></td>
<td>
<p>Parameter for the Minkowski metric. If p = 1, then the metric is
equivalent to manhattan distance (l1). If p = 2, the metric is equivalent
to euclidean distance (l2).</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_+3A_neighbors">neighbors</code></td>
<td>
<p>Number of nearest neighbors to query. Default: 5L.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_+3A_data">data</code></td>
<td>
<p>When a __recipe__ or __formula__ is used, <code>data</code> is
specified as a  __data frame__ containing the predictors and (if
applicable) the outcome.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A KNN model that can be used with the 'predict' S3 generic to make
predictions on new data points.
The model object contains the following:
- &quot;knn_index&quot;: a GPU pointer to the KNN index.
- &quot;algo&quot;: enum value of the algorithm being used for the KNN query.
- &quot;metric&quot;: enum value of the distance metric used in KNN computations.
- &quot;p&quot;: parameter for the Minkowski metric.
- &quot;n_samples&quot;: number of input data points.
- &quot;n_dims&quot;: dimension of each input data point.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)
library(MASS)
library(magrittr)
library(purrr)

set.seed(0L)

centers &lt;- list(c(3, 3), c(-3, -3), c(-3, 3))

gen_pts &lt;- function(cluster_sz) {
  pts &lt;- centers %&gt;%
    map(~ mvrnorm(cluster_sz, mu = .x, Sigma = diag(2)))

  rlang::exec(rbind, !!!pts) %&gt;% as.matrix()
}

gen_labels &lt;- function(cluster_sz) {
  seq_along(centers) %&gt;%
    sapply(function(x) rep(x, cluster_sz)) %&gt;%
    factor()
}

sample_cluster_sz &lt;- 1000
sample_pts &lt;- cbind(
  gen_pts(sample_cluster_sz) %&gt;% as.data.frame(),
  label = gen_labels(sample_cluster_sz)
)

model &lt;- cuda_ml_knn(label ~ ., sample_pts, algo = "ivfflat", metric = "euclidean")

test_cluster_sz &lt;- 10
test_pts &lt;- gen_pts(test_cluster_sz) %&gt;% as.data.frame()

predictions &lt;- predict(model, test_pts)
print(predictions, n = 30)
</code></pre>

<hr>
<h2 id='cuda_ml_knn_algo_ivfflat'>Build a specification for the &quot;ivfflat&quot; KNN query algorithm.</h2><span id='topic+cuda_ml_knn_algo_ivfflat'></span>

<h3>Description</h3>

<p>Build a specification of the flat-inverted-file KNN query algorithm, with all
required parameters specified explicitly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_knn_algo_ivfflat(nlist, nprobe)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_knn_algo_ivfflat_+3A_nlist">nlist</code></td>
<td>
<p>Number of cells to partition dataset into.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_algo_ivfflat_+3A_nprobe">nprobe</code></td>
<td>
<p>At query time, the number of cells used for approximate nearest
neighbor search.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object encapsulating all required parameters of the &quot;ivfflat&quot; KNN
query algorithm.
</p>

<hr>
<h2 id='cuda_ml_knn_algo_ivfpq'>Build a specification for the &quot;ivfpq&quot; KNN query algorithm.</h2><span id='topic+cuda_ml_knn_algo_ivfpq'></span>

<h3>Description</h3>

<p>Build a specification of the inverted-file-product-quantization KNN query
algorithm, with all required parameters specified explicitly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_knn_algo_ivfpq(
  nlist,
  nprobe,
  m,
  n_bits,
  use_precomputed_tables = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_knn_algo_ivfpq_+3A_nlist">nlist</code></td>
<td>
<p>Number of cells to partition dataset into.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_algo_ivfpq_+3A_nprobe">nprobe</code></td>
<td>
<p>At query time, the number of cells used for approximate nearest
neighbor search.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_algo_ivfpq_+3A_m">m</code></td>
<td>
<p>Number of subquantizers.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_algo_ivfpq_+3A_n_bits">n_bits</code></td>
<td>
<p>Bits allocated per subquantizer.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_algo_ivfpq_+3A_use_precomputed_tables">use_precomputed_tables</code></td>
<td>
<p>Whether to use precomputed tables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object encapsulating all required parameters of the &quot;ivfpq&quot; KNN
query algorithm.
</p>

<hr>
<h2 id='cuda_ml_knn_algo_ivfsq'>Build a specification for the &quot;ivfsq&quot; KNN query algorithm.</h2><span id='topic+cuda_ml_knn_algo_ivfsq'></span>

<h3>Description</h3>

<p>Build a specification of the inverted-file-scalar-quantization KNN query
algorithm, with all required parameters specified explicitly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_knn_algo_ivfsq(
  nlist,
  nprobe,
  qtype = c("QT_8bit", "QT_4bit", "QT_8bit_uniform", "QT_4bit_uniform", "QT_fp16",
    "QT_8bit_direct", "QT_6bit"),
  encode_residual = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_knn_algo_ivfsq_+3A_nlist">nlist</code></td>
<td>
<p>Number of cells to partition dataset into.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_algo_ivfsq_+3A_nprobe">nprobe</code></td>
<td>
<p>At query time, the number of cells used for approximate nearest
neighbor search.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_algo_ivfsq_+3A_qtype">qtype</code></td>
<td>
<p>Quantizer type. Must be one of &quot;QT_8bit&quot;, &quot;QT_4bit&quot;,
&quot;QT_8bit_uniform&quot;, &quot;QT_4bit_uniform&quot;, &quot;QT_fp16&quot;, &quot;QT_8bit_direct&quot;,
&quot;QT_6bit&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_knn_algo_ivfsq_+3A_encode_residual">encode_residual</code></td>
<td>
<p>Whether to encode residuals.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object encapsulating all required parameters of the &quot;ivfsq&quot; KNN
query algorithm.
</p>

<hr>
<h2 id='cuda_ml_lasso'>Train a linear model using LASSO regression.</h2><span id='topic+cuda_ml_lasso'></span><span id='topic+cuda_ml_lasso.default'></span><span id='topic+cuda_ml_lasso.data.frame'></span><span id='topic+cuda_ml_lasso.matrix'></span><span id='topic+cuda_ml_lasso.formula'></span><span id='topic+cuda_ml_lasso.recipe'></span>

<h3>Description</h3>

<p>Train a linear model using LASSO (Least Absolute Shrinkage and Selection
Operator) regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_lasso(x, ...)

## Default S3 method:
cuda_ml_lasso(x, ...)

## S3 method for class 'data.frame'
cuda_ml_lasso(
  x,
  y,
  alpha = 1,
  max_iter = 1000L,
  tol = 0.001,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  selection = c("cyclic", "random"),
  ...
)

## S3 method for class 'matrix'
cuda_ml_lasso(
  x,
  y,
  alpha = 1,
  max_iter = 1000L,
  tol = 0.001,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  selection = c("cyclic", "random"),
  ...
)

## S3 method for class 'formula'
cuda_ml_lasso(
  formula,
  data,
  alpha = 1,
  max_iter = 1000L,
  tol = 0.001,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  selection = c("cyclic", "random"),
  ...
)

## S3 method for class 'recipe'
cuda_ml_lasso(
  x,
  data,
  alpha = 1,
  max_iter = 1000L,
  tol = 0.001,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  selection = c("cyclic", "random"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_lasso_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>
<p>* A __data frame__ of predictors.
* A __matrix__ of predictors.
* A __recipe__ specifying a set of preprocessing steps
* created from [recipes::recipe()].
* A __formula__ specifying the predictors and the outcome.</p>
</td></tr>
<tr><td><code id="cuda_ml_lasso_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="cuda_ml_lasso_+3A_y">y</code></td>
<td>
<p>A numeric vector (for regression) or factor (for classification) of
desired responses.</p>
</td></tr>
<tr><td><code id="cuda_ml_lasso_+3A_alpha">alpha</code></td>
<td>
<p>Multiplier of the L1 penalty term (i.e., the result would become
and Ordinary Least Square model if <code>alpha</code> were set to 0). Default: 1.</p>
</td></tr>
<tr><td><code id="cuda_ml_lasso_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of coordinate descent iterations.
Default: 1000L.</p>
</td></tr>
<tr><td><code id="cuda_ml_lasso_+3A_tol">tol</code></td>
<td>
<p>Stop the coordinate descent when the duality gap is below this
threshold. Default: 1e-3.</p>
</td></tr>
<tr><td><code id="cuda_ml_lasso_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>If TRUE, then the model tries to correct for the global
mean of the response variable. If FALSE, then the model expects data to be
centered. Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_lasso_+3A_normalize_input">normalize_input</code></td>
<td>
<p>Ignored when <code>fit_intercept</code> is FALSE. If TRUE,
then the predictors will be normalized to have a L2 norm of 1.
Default: FALSE.</p>
</td></tr>
<tr><td><code id="cuda_ml_lasso_+3A_selection">selection</code></td>
<td>
<p>If &quot;random&quot;, then instead of updating coefficients in cyclic
order, a random coefficient is updated in each iteration. Default: &quot;cyclic&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_lasso_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="cuda_ml_lasso_+3A_data">data</code></td>
<td>
<p>When a __recipe__ or __formula__ is used, <code>data</code> is
specified as a  __data frame__ containing the predictors and (if
applicable) the outcome.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A LASSO regressor that can be used with the 'predict' S3 generic to
make predictions on new data points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

model &lt;- cuda_ml_lasso(formula = mpg ~ ., data = mtcars, alpha = 1e-3)
cuda_ml_predictions &lt;- predict(model, mtcars)

# predictions will be comparable to those from a `glmnet` model with `lambda`
# set to 1e-3 and `alpha` set to 1
# (in `glmnet`, `lambda` is the weight of the penalty term, and `alpha` is
#  the elastic mixing parameter between L1 and L2 penalties.

library(glmnet)

glmnet_model &lt;- glmnet(
  x = as.matrix(mtcars[names(mtcars) != "mpg"]), y = mtcars$mpg,
  alpha = 1, lambda = 1e-3, nlambda = 1, standardize = FALSE
)

glm_predictions &lt;- predict(
  glmnet_model, as.matrix(mtcars[names(mtcars) != "mpg"]),
  s = 0
)

print(
  all.equal(
    as.numeric(glm_predictions),
    cuda_ml_predictions$.pred,
    tolerance = 1e-2
  )
)
</code></pre>

<hr>
<h2 id='cuda_ml_logistic_reg'>Train a logistic regression model.</h2><span id='topic+cuda_ml_logistic_reg'></span><span id='topic+cuda_ml_logistic_reg.default'></span><span id='topic+cuda_ml_logistic_reg.data.frame'></span><span id='topic+cuda_ml_logistic_reg.matrix'></span><span id='topic+cuda_ml_logistic_reg.formula'></span><span id='topic+cuda_ml_logistic_reg.recipe'></span>

<h3>Description</h3>

<p>Train a logistic regression model using Quasi-Newton (QN) algorithms (i.e.,
Orthant-Wise Limited Memory Quasi-Newton (OWL-QN) if there is L1
regularization, Limited Memory BFGS (L-BFGS) otherwise).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_logistic_reg(x, ...)

## Default S3 method:
cuda_ml_logistic_reg(x, ...)

## S3 method for class 'data.frame'
cuda_ml_logistic_reg(
  x,
  y,
  fit_intercept = TRUE,
  penalty = c("l2", "l1", "elasticnet", "none"),
  tol = 1e-04,
  C = 1,
  class_weight = NULL,
  sample_weight = NULL,
  max_iters = 1000L,
  linesearch_max_iters = 50L,
  l1_ratio = NULL,
  ...
)

## S3 method for class 'matrix'
cuda_ml_logistic_reg(
  x,
  y,
  fit_intercept = TRUE,
  penalty = c("l2", "l1", "elasticnet", "none"),
  tol = 1e-04,
  C = 1,
  class_weight = NULL,
  sample_weight = NULL,
  max_iters = 1000L,
  linesearch_max_iters = 50L,
  l1_ratio = NULL,
  ...
)

## S3 method for class 'formula'
cuda_ml_logistic_reg(
  formula,
  data,
  fit_intercept = TRUE,
  penalty = c("l2", "l1", "elasticnet", "none"),
  tol = 1e-04,
  C = 1,
  class_weight = NULL,
  sample_weight = NULL,
  max_iters = 1000L,
  linesearch_max_iters = 50L,
  l1_ratio = NULL,
  ...
)

## S3 method for class 'recipe'
cuda_ml_logistic_reg(
  x,
  data,
  fit_intercept = TRUE,
  penalty = c("l2", "l1", "elasticnet", "none"),
  tol = 1e-04,
  C = 1,
  class_weight = NULL,
  sample_weight = NULL,
  max_iters = 1000L,
  linesearch_max_iters = 50L,
  l1_ratio = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_logistic_reg_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>
<p>* A __data frame__ of predictors.
* A __matrix__ of predictors.
* A __recipe__ specifying a set of preprocessing steps
* created from [recipes::recipe()].
* A __formula__ specifying the predictors and the outcome.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_y">y</code></td>
<td>
<p>A numeric vector (for regression) or factor (for classification) of
desired responses.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>If TRUE, then the model tries to correct for the global
mean of the response variable. If FALSE, then the model expects data to be
centered. Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_penalty">penalty</code></td>
<td>
<p>The penalty type, must be one of
&quot;none&quot;, &quot;l1&quot;, &quot;l2&quot;, &quot;elasticnet&quot;.
If &quot;none&quot; or &quot;l2&quot; is selected, then L-BFGS solver will be used.
If &quot;l1&quot; is selected, solver OWL-QN will be used.
If &quot;elasticnet&quot; is selected, OWL-QN will be used if l1_ratio &gt; 0, otherwise
L-BFGS will be used. Default: &quot;l2&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_tol">tol</code></td>
<td>
<p>Tolerance for stopping criteria. Default: 1e-4.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_c">C</code></td>
<td>
<p>Inverse of regularization strength; must be a positive float.
Default: 1.0.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_class_weight">class_weight</code></td>
<td>
<p>If <code>NULL</code>, then each class has equal weight of
<code>1</code>.
If <code>class_weight</code> is set to <code>"balanced"</code>, then weights will be
inversely proportional to class frequencies in the input data.
If otherwise, then <code>class_weight</code> must be a named numeric vector of
weight values, with names being class labels.
If <code>class_weight</code> is not <code>NULL</code>, then each entry in
<code>sample_weight</code> will be adjusted by multiplying its original value
with the class weight of the corresponding sample's class.
Default: NULL.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_sample_weight">sample_weight</code></td>
<td>
<p>Array of weights assigned to individual samples.
If <code>NULL</code>, then each sample has an equal weight of 1. Default: NULL.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_max_iters">max_iters</code></td>
<td>
<p>Maximum number of solver iterations. Default: 1000L.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_linesearch_max_iters">linesearch_max_iters</code></td>
<td>
<p>Max number of linesearch iterations per outer
iteration used in the LBFGS- and OWL- QN solvers. Default: 50L.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_l1_ratio">l1_ratio</code></td>
<td>
<p>The Elastic-Net mixing parameter, must <code>NULL</code> or be
within the range of [0, 1]. Default: NULL.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="cuda_ml_logistic_reg_+3A_data">data</code></td>
<td>
<p>When a __recipe__ or __formula__ is used, <code>data</code> is
specified as a  __data frame__ containing the predictors and (if
applicable) the outcome.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(cuda.ml)

X &lt;- scale(as.matrix(iris[names(iris) != "Species"]))
y &lt;- iris$Species

model &lt;- cuda_ml_logistic_reg(X, y, max_iters = 100)
predictions &lt;- predict(model, X)

# NOTE: if we were only performing binary classifications (e.g., by having
# `iris_data &lt;- iris %&gt;% mutate(Species = (Species == "setosa"))`), then the
# above would be conceptually equivalent to the following:
#
# iris_data &lt;- iris %&gt;% mutate(Species = (Species == "setosa"))
# model &lt;- glm(
#   Species ~ ., data = iris_data, family = binomial(link = "logit"),
#   control = glm.control(epsilon = 1e-8, maxit = 100)
# )
#
# predict(model, iris_data, type = "response")
</code></pre>

<hr>
<h2 id='cuda_ml_ols'>Train a OLS model.</h2><span id='topic+cuda_ml_ols'></span><span id='topic+cuda_ml_ols.default'></span><span id='topic+cuda_ml_ols.data.frame'></span><span id='topic+cuda_ml_ols.matrix'></span><span id='topic+cuda_ml_ols.formula'></span><span id='topic+cuda_ml_ols.recipe'></span>

<h3>Description</h3>

<p>Train an Ordinary Least Square (OLS) model for regression tasks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_ols(x, ...)

## Default S3 method:
cuda_ml_ols(x, ...)

## S3 method for class 'data.frame'
cuda_ml_ols(
  x,
  y,
  method = c("svd", "eig", "qr"),
  fit_intercept = TRUE,
  normalize_input = FALSE,
  ...
)

## S3 method for class 'matrix'
cuda_ml_ols(
  x,
  y,
  method = c("svd", "eig", "qr"),
  fit_intercept = TRUE,
  normalize_input = FALSE,
  ...
)

## S3 method for class 'formula'
cuda_ml_ols(
  formula,
  data,
  method = c("svd", "eig", "qr"),
  fit_intercept = TRUE,
  normalize_input = FALSE,
  ...
)

## S3 method for class 'recipe'
cuda_ml_ols(
  x,
  data,
  method = c("svd", "eig", "qr"),
  fit_intercept = TRUE,
  normalize_input = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_ols_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>
<p>* A __data frame__ of predictors.
* A __matrix__ of predictors.
* A __recipe__ specifying a set of preprocessing steps
* created from [recipes::recipe()].
* A __formula__ specifying the predictors and the outcome.</p>
</td></tr>
<tr><td><code id="cuda_ml_ols_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="cuda_ml_ols_+3A_y">y</code></td>
<td>
<p>A numeric vector (for regression) or factor (for classification) of
desired responses.</p>
</td></tr>
<tr><td><code id="cuda_ml_ols_+3A_method">method</code></td>
<td>
<p>Must be one of &quot;svd&quot;, &quot;eig&quot;, &quot;qr&quot;.
</p>
<p>- &quot;svd&quot;: compute SVD decomposition using Jacobi iterations.
- &quot;eig&quot;: use an eigendecomposition of the covariance matrix.
- &quot;qr&quot;: use the QR decomposition algorithm and solve 'Rx = Q^T y'.
</p>
<p>If the number of features is larger than the sample size, then the
&quot;svd&quot; algorithm will be force-selected because it is the only
algorithm that can support this type of scenario.
</p>
<p>Default: &quot;svd&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_ols_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>If TRUE, then the model tries to correct for the global
mean of the response variable. If FALSE, then the model expects data to be
centered. Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_ols_+3A_normalize_input">normalize_input</code></td>
<td>
<p>Ignored when <code>fit_intercept</code> is FALSE. If TRUE,
then the predictors will be normalized to have a L2 norm of 1.
Default: FALSE.</p>
</td></tr>
<tr><td><code id="cuda_ml_ols_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="cuda_ml_ols_+3A_data">data</code></td>
<td>
<p>When a __recipe__ or __formula__ is used, <code>data</code> is
specified as a  __data frame__ containing the predictors and (if
applicable) the outcome.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A OLS regressor that can be used with the 'predict' S3 generic to
make predictions on new data points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

model &lt;- cuda_ml_ols(formula = mpg ~ ., data = mtcars, method = "qr")
predictions &lt;- predict(model, mtcars[names(mtcars) != "mpg"])

# predictions will be comparable to those from a `stats::lm` model
lm_model &lt;- stats::lm(formula = mpg ~ ., data = mtcars, method = "qr")
lm_predictions &lt;- predict(lm_model, mtcars[names(mtcars) != "mpg"])

print(
  all.equal(
    as.numeric(lm_predictions),
    predictions$.pred,
    tolerance = 1e-3
  )
)
</code></pre>

<hr>
<h2 id='cuda_ml_pca'>Perform principal component analysis.</h2><span id='topic+cuda_ml_pca'></span>

<h3>Description</h3>

<p>Compute principal component(s) of the input data. Each feature from the input
will be mean-centered (but not scaled) before the SVD computation takes
place.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_pca(
  x,
  n_components = NULL,
  eig_algo = c("dq", "jacobi"),
  tol = 1e-07,
  n_iters = 15L,
  whiten = FALSE,
  transform_input = TRUE,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_pca_+3A_x">x</code></td>
<td>
<p>The input matrix or dataframe. Each data point should be a row
and should consist of numeric values only.</p>
</td></tr>
<tr><td><code id="cuda_ml_pca_+3A_n_components">n_components</code></td>
<td>
<p>Number of principal component(s) to keep. Default:
min(nrow(x), ncol(x)).</p>
</td></tr>
<tr><td><code id="cuda_ml_pca_+3A_eig_algo">eig_algo</code></td>
<td>
<p>Eigen decomposition algorithm to be applied to the covariance
matrix. Valid choices are &quot;dq&quot; (divid-and-conquer method for symmetric
matrices) and &quot;jacobi&quot; (the Jacobi method for symmetric matrices).
Default: &quot;dq&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_pca_+3A_tol">tol</code></td>
<td>
<p>Tolerance for singular values computed by the Jacobi method.
Default: 1e-7.</p>
</td></tr>
<tr><td><code id="cuda_ml_pca_+3A_n_iters">n_iters</code></td>
<td>
<p>Maximum number of iterations for the Jacobi method.
Default: 15.</p>
</td></tr>
<tr><td><code id="cuda_ml_pca_+3A_whiten">whiten</code></td>
<td>
<p>If TRUE, then de-correlate all components, making each
component have unit variance  and removing multi-collinearity.
Default: FALSE.</p>
</td></tr>
<tr><td><code id="cuda_ml_pca_+3A_transform_input">transform_input</code></td>
<td>
<p>If TRUE, then compute an approximate representation
of the input data. Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_pca_+3A_cuml_log_level">cuML_log_level</code></td>
<td>
<p>Log level within cuML library functions. Must be one of
&quot;off&quot;, &quot;critical&quot;, &quot;error&quot;, &quot;warn&quot;, &quot;info&quot;, &quot;debug&quot;, &quot;trace&quot;.
Default: off.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A PCA model object with the following attributes:
- &quot;components&quot;: a matrix of <code>n_components</code> rows containing the top
principal components.
- &quot;explained_variance&quot;: amount of variance within the input data explained
by each component.
- &quot;explained_variance_ratio&quot;: fraction of variance within the input data
explained by each component.
- &quot;singular_values&quot;: singular values (non-negative) corresponding to the
top principal components.
- &quot;mean&quot;: the column wise mean of <code>x</code> which was used to mean-center
<code>x</code> first.
- &quot;transformed_data&quot;: (only present if &quot;transform_input&quot; is set to TRUE)
an approximate representation of input data based on principal
components.
- &quot;pca_params&quot;: opaque pointer to PCA parameters which will be used for
performing inverse transforms.
</p>
<p>The model object can be used as input to the inverse_transform() function to
map a representation based on principal components back to the original
feature space.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

iris.pca &lt;- cuda_ml_pca(iris[1:4], n_components = 3)
print(iris.pca)
</code></pre>

<hr>
<h2 id='cuda_ml_rand_forest'>Train a random forest model.</h2><span id='topic+cuda_ml_rand_forest'></span><span id='topic+cuda_ml_rand_forest.default'></span><span id='topic+cuda_ml_rand_forest.data.frame'></span><span id='topic+cuda_ml_rand_forest.matrix'></span><span id='topic+cuda_ml_rand_forest.formula'></span><span id='topic+cuda_ml_rand_forest.recipe'></span>

<h3>Description</h3>

<p>Train a random forest model for classification or regression tasks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_rand_forest(x, ...)

## Default S3 method:
cuda_ml_rand_forest(x, ...)

## S3 method for class 'data.frame'
cuda_ml_rand_forest(
  x,
  y,
  mtry = NULL,
  trees = NULL,
  min_n = 2L,
  bootstrap = TRUE,
  max_depth = 16L,
  max_leaves = Inf,
  max_predictors_per_note_split = NULL,
  n_bins = 128L,
  min_samples_leaf = 1L,
  split_criterion = NULL,
  min_impurity_decrease = 0,
  max_batch_size = 128L,
  n_streams = 8L,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace"),
  ...
)

## S3 method for class 'matrix'
cuda_ml_rand_forest(
  x,
  y,
  mtry = NULL,
  trees = NULL,
  min_n = 2L,
  bootstrap = TRUE,
  max_depth = 16L,
  max_leaves = Inf,
  max_predictors_per_note_split = NULL,
  n_bins = 128L,
  min_samples_leaf = 1L,
  split_criterion = NULL,
  min_impurity_decrease = 0,
  max_batch_size = 128L,
  n_streams = 8L,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace"),
  ...
)

## S3 method for class 'formula'
cuda_ml_rand_forest(
  formula,
  data,
  mtry = NULL,
  trees = NULL,
  min_n = 2L,
  bootstrap = TRUE,
  max_depth = 16L,
  max_leaves = Inf,
  max_predictors_per_note_split = NULL,
  n_bins = 128L,
  min_samples_leaf = 1L,
  split_criterion = NULL,
  min_impurity_decrease = 0,
  max_batch_size = 128L,
  n_streams = 8L,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace"),
  ...
)

## S3 method for class 'recipe'
cuda_ml_rand_forest(
  x,
  data,
  mtry = NULL,
  trees = NULL,
  min_n = 2L,
  bootstrap = TRUE,
  max_depth = 16L,
  max_leaves = Inf,
  max_predictors_per_note_split = NULL,
  n_bins = 128L,
  min_samples_leaf = 1L,
  split_criterion = NULL,
  min_impurity_decrease = 0,
  max_batch_size = 128L,
  n_streams = 8L,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_rand_forest_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>
<p>* A __data frame__ of predictors.
* A __matrix__ of predictors.
* A __recipe__ specifying a set of preprocessing steps
* created from [recipes::recipe()].
* A __formula__ specifying the predictors and the outcome.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_y">y</code></td>
<td>
<p>A numeric vector (for regression) or factor (for classification) of
desired responses.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_mtry">mtry</code></td>
<td>
<p>The number of predictors that will be randomly sampled at each
split when creating the tree models. Default: the square root of the total
number of predictors.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_trees">trees</code></td>
<td>
<p>An integer for the number of trees contained in the ensemble.
Default: 100L.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_min_n">min_n</code></td>
<td>
<p>An integer for the minimum number of data points in a node that
are required for the node to be split further. Default: 2L.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_bootstrap">bootstrap</code></td>
<td>
<p>Whether to perform bootstrap.
If TRUE, each tree in the forest is built on a bootstrapped sample with
replacement.
If FALSE, the whole dataset is used to build each tree.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_max_depth">max_depth</code></td>
<td>
<p>Maximum tree depth. Default: 16L.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_max_leaves">max_leaves</code></td>
<td>
<p>Maximum leaf nodes per tree. Soft constraint. Default: Inf
(unlimited).</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_max_predictors_per_note_split">max_predictors_per_note_split</code></td>
<td>
<p>Number of predictor to consider per node
split. Default: square root of the total number predictors.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_n_bins">n_bins</code></td>
<td>
<p>Number of bins used by the split algorithm. Default: 128L.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_min_samples_leaf">min_samples_leaf</code></td>
<td>
<p>The minimum number of data points in each leaf node.
Default: 1L.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_split_criterion">split_criterion</code></td>
<td>
<p>The criterion used to split nodes, can be &quot;gini&quot; or
&quot;entropy&quot; for classifications, and &quot;mse&quot; or &quot;mae&quot; for regressions.
Default: &quot;gini&quot; for classification; &quot;mse&quot; for regression.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_min_impurity_decrease">min_impurity_decrease</code></td>
<td>
<p>Minimum decrease in impurity requried for node
to be spilt. Default: 0.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_max_batch_size">max_batch_size</code></td>
<td>
<p>Maximum number of nodes that can be processed in a
given batch. Default: 128L.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_n_streams">n_streams</code></td>
<td>
<p>Number of CUDA streams to use for building trees.
Default: 8L.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_cuml_log_level">cuML_log_level</code></td>
<td>
<p>Log level within cuML library functions. Must be one of
&quot;off&quot;, &quot;critical&quot;, &quot;error&quot;, &quot;warn&quot;, &quot;info&quot;, &quot;debug&quot;, &quot;trace&quot;.
Default: off.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_forest_+3A_data">data</code></td>
<td>
<p>When a __recipe__ or __formula__ is used, <code>data</code> is
specified as a  __data frame__ containing the predictors and (if
applicable) the outcome.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A random forest classifier / regressor object that can be used with
the 'predict' S3 generic to make predictions on new data points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(cuda.ml)

# Classification

model &lt;- cuda_ml_rand_forest(
  formula = Species ~ .,
  data = iris,
  trees = 100
)

predictions &lt;- predict(model, iris[names(iris) != "Species"])

# Regression

model &lt;- cuda_ml_rand_forest(
  formula = mpg ~ .,
  data = mtcars,
  trees = 100
)

predictions &lt;- predict(model, mtcars[names(mtcars) != "mpg"])
</code></pre>

<hr>
<h2 id='cuda_ml_rand_proj'>Random projection for dimensionality reduction.</h2><span id='topic+cuda_ml_rand_proj'></span>

<h3>Description</h3>

<p>Generate a random projection matrix for dimensionality reduction, and
optionally transform input data to a projection in a lower dimension space
using the generated random matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_rand_proj(
  x,
  n_components = NULL,
  eps = 0.1,
  gaussian_method = TRUE,
  density = NULL,
  transform_input = TRUE,
  seed = 0L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_rand_proj_+3A_x">x</code></td>
<td>
<p>The input matrix or dataframe. Each data point should be a row
and should consist of numeric values only.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_proj_+3A_n_components">n_components</code></td>
<td>
<p>Dimensionality of the target projection space. If NULL,
then the parameter is deducted using the Johnson-Lindenstrauss lemma,
taking into consideration the number of samples and the <code>eps</code>
parameter. Default: NULL.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_proj_+3A_eps">eps</code></td>
<td>
<p>Error tolerance during projection. Default: 0.1.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_proj_+3A_gaussian_method">gaussian_method</code></td>
<td>
<p>If TRUE, then use the Gaussian random projection
method. Otherwise, use the sparse random projection method.
See https://en.wikipedia.org/wiki/Random_projection for details.
Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_proj_+3A_density">density</code></td>
<td>
<p>Ratio of non-zero component in the random projection matrix.
If NULL, then the value is set to the minimum density as recommended by
Ping Li et al.: 1 / sqrt(n_features). Default: NULL.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_proj_+3A_transform_input">transform_input</code></td>
<td>
<p>Whether to project input data onto a lower dimension
space using the random matrix. Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_rand_proj_+3A_seed">seed</code></td>
<td>
<p>Seed for the pseudorandom number generator. Default: 0L.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A context object containing GPU pointer to a random matrix that can
be used as input to the <code>cuda_ml_transform()</code> function.
If <code>transform_input</code> is set to TRUE, then the context object will also
contain a &quot;transformed_data&quot; attribute containing the lower dimensional
projection of the input data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(cuda.ml)
library(mlbench)

data(Vehicle)
vehicle_data &lt;- Vehicle[order(Vehicle$Class), which(names(Vehicle) != "Class")]

model &lt;- cuda_ml_rand_proj(vehicle_data, n_components = 4)

set.seed(0L)
print(kmeans(model$transformed_data, centers = 4, iter.max = 1000))
</code></pre>

<hr>
<h2 id='cuda_ml_ridge'>Train a linear model using ridge regression.</h2><span id='topic+cuda_ml_ridge'></span><span id='topic+cuda_ml_ridge.default'></span><span id='topic+cuda_ml_ridge.data.frame'></span><span id='topic+cuda_ml_ridge.matrix'></span><span id='topic+cuda_ml_ridge.formula'></span><span id='topic+cuda_ml_ridge.recipe'></span>

<h3>Description</h3>

<p>Train a linear model with L2 regularization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_ridge(x, ...)

## Default S3 method:
cuda_ml_ridge(x, ...)

## S3 method for class 'data.frame'
cuda_ml_ridge(
  x,
  y,
  alpha = 1,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  ...
)

## S3 method for class 'matrix'
cuda_ml_ridge(
  x,
  y,
  alpha = 1,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  ...
)

## S3 method for class 'formula'
cuda_ml_ridge(
  formula,
  data,
  alpha = 1,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  ...
)

## S3 method for class 'recipe'
cuda_ml_ridge(
  x,
  data,
  alpha = 1,
  fit_intercept = TRUE,
  normalize_input = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_ridge_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>
<p>* A __data frame__ of predictors.
* A __matrix__ of predictors.
* A __recipe__ specifying a set of preprocessing steps
* created from [recipes::recipe()].
* A __formula__ specifying the predictors and the outcome.</p>
</td></tr>
<tr><td><code id="cuda_ml_ridge_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="cuda_ml_ridge_+3A_y">y</code></td>
<td>
<p>A numeric vector (for regression) or factor (for classification) of
desired responses.</p>
</td></tr>
<tr><td><code id="cuda_ml_ridge_+3A_alpha">alpha</code></td>
<td>
<p>Multiplier of the L2 penalty term (i.e., the result would become
and Ordinary Least Square model if <code>alpha</code> were set to 0). Default: 1.</p>
</td></tr>
<tr><td><code id="cuda_ml_ridge_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>If TRUE, then the model tries to correct for the global
mean of the response variable. If FALSE, then the model expects data to be
centered. Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_ridge_+3A_normalize_input">normalize_input</code></td>
<td>
<p>Ignored when <code>fit_intercept</code> is FALSE. If TRUE,
then the predictors will be normalized to have a L2 norm of 1.
Default: FALSE.</p>
</td></tr>
<tr><td><code id="cuda_ml_ridge_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="cuda_ml_ridge_+3A_data">data</code></td>
<td>
<p>When a __recipe__ or __formula__ is used, <code>data</code> is
specified as a  __data frame__ containing the predictors and (if
applicable) the outcome.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ridge regressor that can be used with the 'predict' S3 generic to
make predictions on new data points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

model &lt;- cuda_ml_ridge(formula = mpg ~ ., data = mtcars, alpha = 1e-3)
cuda_ml_predictions &lt;- predict(model, mtcars[names(mtcars) != "mpg"])

# predictions will be comparable to those from a `glmnet` model with `lambda`
# set to 2e-3 and `alpha` set to 0
# (in `glmnet`, `lambda` is the weight of the penalty term, and `alpha` is
#  the elastic mixing parameter between L1 and L2 penalties.

library(glmnet)

glmnet_model &lt;- glmnet(
  x = as.matrix(mtcars[names(mtcars) != "mpg"]), y = mtcars$mpg,
  alpha = 0, lambda = 2e-3, nlambda = 1, standardize = FALSE
)

glmnet_predictions &lt;- predict(
  glmnet_model, as.matrix(mtcars[names(mtcars) != "mpg"]),
  s = 0
)

print(
  all.equal(
    as.numeric(glmnet_predictions),
    cuda_ml_predictions$.pred,
    tolerance = 1e-3
  )
)
</code></pre>

<hr>
<h2 id='cuda_ml_serialize'>Serialize a CuML model</h2><span id='topic+cuda_ml_serialize'></span><span id='topic+cuda_ml_serialise'></span>

<h3>Description</h3>

<p>Given a CuML model, serialize its state into a connection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_serialize(model, connection = NULL, ...)

cuda_ml_serialise(model, connection = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_serialize_+3A_model">model</code></td>
<td>
<p>The model object.</p>
</td></tr>
<tr><td><code id="cuda_ml_serialize_+3A_connection">connection</code></td>
<td>
<p>An open connection or <code>NULL</code>. If <code>NULL</code>, then the
model state is serialized to a raw vector. Default: NULL.</p>
</td></tr>
<tr><td><code id="cuda_ml_serialize_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>base::serialize()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>NULL</code> unless <code>connection</code> is <code>NULL</code>, in which case
the serialized model state is returned as a raw vector.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+serialize">serialize</a></code>
</p>

<hr>
<h2 id='cuda_ml_sgd'>Train a MBSGD linear model.</h2><span id='topic+cuda_ml_sgd'></span><span id='topic+cuda_ml_sgd.default'></span><span id='topic+cuda_ml_sgd.data.frame'></span><span id='topic+cuda_ml_sgd.matrix'></span><span id='topic+cuda_ml_sgd.formula'></span><span id='topic+cuda_ml_sgd.recipe'></span>

<h3>Description</h3>

<p>Train a linear model using mini-batch stochastic gradient descent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_sgd(x, ...)

## Default S3 method:
cuda_ml_sgd(x, ...)

## S3 method for class 'data.frame'
cuda_ml_sgd(
  x,
  y,
  fit_intercept = TRUE,
  loss = c("squared_loss", "log", "hinge"),
  penalty = c("none", "l1", "l2", "elasticnet"),
  alpha = 1e-04,
  l1_ratio = 0.5,
  epochs = 1000L,
  tol = 0.001,
  shuffle = TRUE,
  learning_rate = c("constant", "invscaling", "adaptive"),
  eta0 = 0.001,
  power_t = 0.5,
  batch_size = 32L,
  n_iters_no_change = 5L,
  ...
)

## S3 method for class 'matrix'
cuda_ml_sgd(
  x,
  y,
  fit_intercept = TRUE,
  loss = c("squared_loss", "log", "hinge"),
  penalty = c("none", "l1", "l2", "elasticnet"),
  alpha = 1e-04,
  l1_ratio = 0.5,
  epochs = 1000L,
  tol = 0.001,
  shuffle = TRUE,
  learning_rate = c("constant", "invscaling", "adaptive"),
  eta0 = 0.001,
  power_t = 0.5,
  batch_size = 32L,
  n_iters_no_change = 5L,
  ...
)

## S3 method for class 'formula'
cuda_ml_sgd(
  formula,
  data,
  fit_intercept = TRUE,
  loss = c("squared_loss", "log", "hinge"),
  penalty = c("none", "l1", "l2", "elasticnet"),
  alpha = 1e-04,
  l1_ratio = 0.5,
  epochs = 1000L,
  tol = 0.001,
  shuffle = TRUE,
  learning_rate = c("constant", "invscaling", "adaptive"),
  eta0 = 0.001,
  power_t = 0.5,
  batch_size = 32L,
  n_iters_no_change = 5L,
  ...
)

## S3 method for class 'recipe'
cuda_ml_sgd(
  x,
  data,
  fit_intercept = TRUE,
  loss = c("squared_loss", "log", "hinge"),
  penalty = c("none", "l1", "l2", "elasticnet"),
  alpha = 1e-04,
  l1_ratio = 0.5,
  epochs = 1000L,
  tol = 0.001,
  shuffle = TRUE,
  learning_rate = c("constant", "invscaling", "adaptive"),
  eta0 = 0.001,
  power_t = 0.5,
  batch_size = 32L,
  n_iters_no_change = 5L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_sgd_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>
<p>* A __data frame__ of predictors.
* A __matrix__ of predictors.
* A __recipe__ specifying a set of preprocessing steps
* created from [recipes::recipe()].
* A __formula__ specifying the predictors and the outcome.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_y">y</code></td>
<td>
<p>A numeric vector (for regression) or factor (for classification) of
desired responses.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_fit_intercept">fit_intercept</code></td>
<td>
<p>If TRUE, then the model tries to correct for the global
mean of the response variable. If FALSE, then the model expects data to be
centered. Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_loss">loss</code></td>
<td>
<p>Loss function, must be one of &quot;squared_loss&quot;, &quot;log&quot;, &quot;hinge&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_penalty">penalty</code></td>
<td>
<p>Type of regularization to perform, must be one of
&quot;none&quot;, &quot;l1&quot;, &quot;l2&quot;, &quot;elasticnet&quot;.
</p>
<p>- &quot;none&quot;: no regularization.
- &quot;l1&quot;: perform regularization based on the L1-norm (LASSO) which tries to
minimize the sum of the absolute values of the coefficients.
- &quot;l2&quot;: perform regularization based on the L2 norm (Ridge) which tries to
minimize the sum of the square of the coefficients.
- &quot;elasticnet&quot;: perform the Elastic Net regularization which is based on
the weighted averable of L1 and L2 norms.
Default: &quot;none&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_alpha">alpha</code></td>
<td>
<p>Multiplier of the penalty term. Default: 1e-4.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_l1_ratio">l1_ratio</code></td>
<td>
<p>The ElasticNet mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1
penalty.
For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.
The penalty term is computed using the following formula:
penalty = <code>alpha</code> * <code>l1_ratio</code> * ||w||_1 +
0.5 * <code>alpha</code> * (1 - <code>l1_ratio</code>) * ||w||^2_2
where ||w||_1 is the L1 norm of the coefficients, and ||w||_2 is the L2
norm of the coefficients.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_epochs">epochs</code></td>
<td>
<p>The number of times the model should iterate through the entire
dataset during training. Default: 1000L.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_tol">tol</code></td>
<td>
<p>Threshold for stopping training. Training will stop if
(loss in current epoch) &gt; (loss in previous epoch) - <code>tol</code>.
Default: 1e-3.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_shuffle">shuffle</code></td>
<td>
<p>Whether to shuffles the training data after each epoch.
Default: True.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_learning_rate">learning_rate</code></td>
<td>
<p>Must be one of &quot;constant&quot;, &quot;invscaling&quot;, &quot;adaptive&quot;.
</p>
<p>- &quot;constant&quot;: the learning rate will be kept constant.
- &quot;invscaling&quot;: (learning rate) = (initial learning rate) / pow(t, power_t)
where <code>t</code> is the number of epochs and
<code>power_t</code> is a tunable parameter of this model.
- &quot;adaptive&quot;: (learning rate) = (initial learning rate) as long as the
training loss keeps decreasing. Each time the last
<code>n_iter_no_change</code> consecutive epochs fail to decrease
the training loss by <code>tol</code>, the current learning rate is
divided by 5.
Default: &quot;constant&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_eta0">eta0</code></td>
<td>
<p>The initial learning rate. Default: 1e-3.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_power_t">power_t</code></td>
<td>
<p>The exponent used in the invscaling learning rate
calculations.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_batch_size">batch_size</code></td>
<td>
<p>The number of samples that will be included in each batch.
Default: 32L.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_n_iters_no_change">n_iters_no_change</code></td>
<td>
<p>The maximum number of epochs to train if there is no
imporvement in the model. Default: 5.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="cuda_ml_sgd_+3A_data">data</code></td>
<td>
<p>When a __recipe__ or __formula__ is used, <code>data</code> is
specified as a  __data frame__ containing the predictors and (if
applicable) the outcome.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A linear model that can be used with the 'predict' S3 generic to make
predictions on new data points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

model &lt;- cuda_ml_sgd(
  mpg ~ ., mtcars,
  batch_size = 4L, epochs = 50000L,
  learning_rate = "adaptive", eta0 = 1e-5,
  penalty = "l2", alpha = 1e-5, tol = 1e-6,
  n_iters_no_change = 10L
)

preds &lt;- predict(model, mtcars[names(mtcars) != "mpg"])
print(all.equal(preds$.pred, mtcars$mpg, tolerance = 0.09))
</code></pre>

<hr>
<h2 id='cuda_ml_svm'>Train a SVM model.</h2><span id='topic+cuda_ml_svm'></span><span id='topic+cuda_ml_svm.default'></span><span id='topic+cuda_ml_svm.data.frame'></span><span id='topic+cuda_ml_svm.matrix'></span><span id='topic+cuda_ml_svm.formula'></span><span id='topic+cuda_ml_svm.recipe'></span>

<h3>Description</h3>

<p>Train a Support Vector Machine model for classification or regression tasks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_svm(x, ...)

## Default S3 method:
cuda_ml_svm(x, ...)

## S3 method for class 'data.frame'
cuda_ml_svm(
  x,
  y,
  cost = 1,
  kernel = c("rbf", "tanh", "polynomial", "linear"),
  gamma = NULL,
  coef0 = 0,
  degree = 3L,
  tol = 0.001,
  max_iter = NULL,
  nochange_steps = 1000L,
  cache_size = 1024,
  epsilon = 0.1,
  sample_weights = NULL,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace"),
  ...
)

## S3 method for class 'matrix'
cuda_ml_svm(
  x,
  y,
  cost = 1,
  kernel = c("rbf", "tanh", "polynomial", "linear"),
  gamma = NULL,
  coef0 = 0,
  degree = 3L,
  tol = 0.001,
  max_iter = NULL,
  nochange_steps = 1000L,
  cache_size = 1024,
  epsilon = 0.1,
  sample_weights = NULL,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace"),
  ...
)

## S3 method for class 'formula'
cuda_ml_svm(
  formula,
  data,
  cost = 1,
  kernel = c("rbf", "tanh", "polynomial", "linear"),
  gamma = NULL,
  coef0 = 0,
  degree = 3L,
  tol = 0.001,
  max_iter = NULL,
  nochange_steps = 1000L,
  cache_size = 1024,
  epsilon = 0.1,
  sample_weights = NULL,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace"),
  ...
)

## S3 method for class 'recipe'
cuda_ml_svm(
  x,
  data,
  cost = 1,
  kernel = c("rbf", "tanh", "polynomial", "linear"),
  gamma = NULL,
  coef0 = 0,
  degree = 3L,
  tol = 0.001,
  max_iter = NULL,
  nochange_steps = 1000L,
  cache_size = 1024,
  epsilon = 0.1,
  sample_weights = NULL,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_svm_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>
<p>* A __data frame__ of predictors.
* A __matrix__ of predictors.
* A __recipe__ specifying a set of preprocessing steps
* created from [recipes::recipe()].
* A __formula__ specifying the predictors and the outcome.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_y">y</code></td>
<td>
<p>A numeric vector (for regression) or factor (for classification) of
desired responses.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_cost">cost</code></td>
<td>
<p>A positive number for the cost of predicting a sample within or
on the wrong side of the margin. Default: 1.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_kernel">kernel</code></td>
<td>
<p>Type of the SVM kernel function (must be one of &quot;rbf&quot;, &quot;tanh&quot;,
&quot;polynomial&quot;, or &quot;linear&quot;). Default: &quot;rbf&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_gamma">gamma</code></td>
<td>
<p>The gamma coefficient (only relevant to polynomial, RBF, and
tanh kernel functions, see explanations below).
Default: 1 / (num features).
</p>
<p>The following kernels are implemented:
- RBF K(x_1, x_2) = exp(-gamma |x_1-x_2|^2)
- TANH K(x_1, x_2) = tanh(gamma &lt;x_1,x_2&gt; + coef0)
- POLYNOMIAL K(x_1, x_2) = (gamma &lt;x_1,x_2&gt; + coef0)^degree
- LINEAR K(x_1,x_2) = &lt;x_1,x_2&gt;,
where &lt; , &gt; denotes the dot product.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_coef0">coef0</code></td>
<td>
<p>The 0th coefficient (only applicable to polynomial and tanh
kernel functions, see explanations below). Default: 0.
</p>
<p>The following kernels are implemented:
- RBF K(x_1, x_2) = exp(-gamma |x_1-x_2|^2)
- TANH K(x_1, x_2) = tanh(gamma &lt;x_1,x_2&gt; + coef0)
- POLYNOMIAL K(x_1, x_2) = (gamma &lt;x_1,x_2&gt; + coef0)^degree
- LINEAR K(x_1,x_2) = &lt;x_1,x_2&gt;,
where &lt; , &gt; denotes the dot product.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_degree">degree</code></td>
<td>
<p>Degree of the polynomial kernel function (note: not applicable
to other kernel types, see explanations below). Default: 3.
</p>
<p>The following kernels are implemented:
- RBF K(x_1, x_2) = exp(-gamma |x_1-x_2|^2)
- TANH K(x_1, x_2) = tanh(gamma &lt;x_1,x_2&gt; + coef0)
- POLYNOMIAL K(x_1, x_2) = (gamma &lt;x_1,x_2&gt; + coef0)^degree
- LINEAR K(x_1,x_2) = &lt;x_1,x_2&gt;,
where &lt; , &gt; denotes the dot product.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_tol">tol</code></td>
<td>
<p>Tolerance to stop fitting. Default: 1e-3.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of outer iterations in SmoSolver.
Default: 100 * (num samples).</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_nochange_steps">nochange_steps</code></td>
<td>
<p>Number of steps with no change w.r.t convergence.
Default: 1000.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_cache_size">cache_size</code></td>
<td>
<p>Size of kernel cache (MiB) in device memory. Default: 1024.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_epsilon">epsilon</code></td>
<td>
<p>Espsilon parameter of the epsilon-SVR model. There is no
penalty for points that are predicted within the epsilon-tube around the
target values. Please note this parameter is only relevant for regression
tasks. Default: 0.1.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_sample_weights">sample_weights</code></td>
<td>
<p>Optional weight assigned to each input data point.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_cuml_log_level">cuML_log_level</code></td>
<td>
<p>Log level within cuML library functions. Must be one of
&quot;off&quot;, &quot;critical&quot;, &quot;error&quot;, &quot;warn&quot;, &quot;info&quot;, &quot;debug&quot;, &quot;trace&quot;.
Default: off.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="cuda_ml_svm_+3A_data">data</code></td>
<td>
<p>When a __recipe__ or __formula__ is used, <code>data</code> is
specified as a  __data frame__ containing the predictors and (if
applicable) the outcome.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A SVM classifier / regressor object that can be used with the
'predict' S3 generic to make predictions on new data points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

# Classification

model &lt;- cuda_ml_svm(
  formula = Species ~ .,
  data = iris,
  kernel = "rbf"
)

predictions &lt;- predict(model, iris[names(iris) != "Species"])

# Regression

model &lt;- cuda_ml_svm(
  formula = mpg ~ .,
  data = mtcars,
  kernel = "rbf"
)

predictions &lt;- predict(model, mtcars)
</code></pre>

<hr>
<h2 id='cuda_ml_transform'>Transform data using a trained cuML model.</h2><span id='topic+cuda_ml_transform'></span>

<h3>Description</h3>

<p>Given a trained cuML model, transform an input dataset using that model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_transform(model, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_transform_+3A_model">model</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="cuda_ml_transform_+3A_x">x</code></td>
<td>
<p>The dataset to be transformed.</p>
</td></tr>
<tr><td><code id="cuda_ml_transform_+3A_...">...</code></td>
<td>
<p>Additional model-specific parameters (if any).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The transformed data points.
</p>

<hr>
<h2 id='cuda_ml_tsne'>t-distributed Stochastic Neighbor Embedding.</h2><span id='topic+cuda_ml_tsne'></span>

<h3>Description</h3>

<p>t-distributed Stochastic Neighbor Embedding (TSNE) for visualizing high-
dimensional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_tsne(
  x,
  n_components = 2L,
  n_neighbors = ceiling(3 * perplexity),
  method = c("barnes_hut", "fft", "exact"),
  angle = 0.5,
  n_iter = 1000L,
  learning_rate = 200,
  learning_rate_method = c("adaptive", "none"),
  perplexity = 30,
  perplexity_max_iter = 100L,
  perplexity_tol = 1e-05,
  early_exaggeration = 12,
  late_exaggeration = 1,
  exaggeration_iter = 250L,
  min_grad_norm = 1e-07,
  pre_momentum = 0.5,
  post_momentum = 0.8,
  square_distances = TRUE,
  seed = NULL,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_tsne_+3A_x">x</code></td>
<td>
<p>The input matrix or dataframe. Each data point should be a row
and should consist of numeric values only.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_n_components">n_components</code></td>
<td>
<p>Dimension of the embedded space.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_n_neighbors">n_neighbors</code></td>
<td>
<p>The number of datapoints to use in the attractive forces.
Default: ceiling(3 * perplexity).</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_method">method</code></td>
<td>
<p>T-SNE method, must be one of &quot;barnes_hut&quot;, &quot;fft&quot;, &quot;exact&quot;.
The &quot;exact&quot; method will be more accurate but slower. Both &quot;barnes_hut&quot; and
&quot;fft&quot; methods are fast approximations.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_angle">angle</code></td>
<td>
<p>Valid values are between 0.0 and 1.0, which trade off speed and
accuracy, respectively. Generally, these values are set between 0.2 and
0.8. (Barnes-Hut only.)</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_n_iter">n_iter</code></td>
<td>
<p>Maximum number of iterations for the optimization. Should be
at least 250. Default: 1000L.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_learning_rate">learning_rate</code></td>
<td>
<p>Learning rate of the t-SNE algorithm, usually between
(10, 1000). If the learning rate is too high, then t-SNE result could look
like a cloud / ball of points.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_learning_rate_method">learning_rate_method</code></td>
<td>
<p>Must be one of &quot;adaptive&quot;, &quot;none&quot;. If
&quot;adaptive&quot;, then learning rate, early exaggeration, and perplexity are
automatically tuned based on input size. Default: &quot;adaptive&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_perplexity">perplexity</code></td>
<td>
<p>The target value of the conditional distribution's
perplexity (see
https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding
for details).</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_perplexity_max_iter">perplexity_max_iter</code></td>
<td>
<p>The number of epochs the best Gaussian bands are
found for. Default: 100L.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_perplexity_tol">perplexity_tol</code></td>
<td>
<p>Stop optimizing the Gaussian bands when the conditional
distribution's perplexity is within this desired tolerance compared to its
taget value. Default: 1e-5.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_early_exaggeration">early_exaggeration</code></td>
<td>
<p>Controls the space between clusters. Not critical
to tune this. Default: 12.0.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_late_exaggeration">late_exaggeration</code></td>
<td>
<p>Controls the space between clusters. It may be
beneficial to increase this slightly to improve cluster separation. This
will be applied after 'exaggeration_iter' iterations (FFT only).</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_exaggeration_iter">exaggeration_iter</code></td>
<td>
<p>Number of exaggeration iterations. Default: 250L.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_min_grad_norm">min_grad_norm</code></td>
<td>
<p>If the gradient norm is below this threshold, the
optimization will be stopped. Default: 1e-7.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_pre_momentum">pre_momentum</code></td>
<td>
<p>During the exaggeration iteration, more forcefully apply
gradients. Default: 0.5.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_post_momentum">post_momentum</code></td>
<td>
<p>During the late phases, less forcefully apply gradients.
Default: 0.8.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_square_distances">square_distances</code></td>
<td>
<p>Whether TSNE should square the distance values.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_seed">seed</code></td>
<td>
<p>Seed to the psuedorandom number generator. Setting this can make
repeated runs look more similar. Note, however, that this highly
parallelized t-SNE implementation is not completely deterministic between
runs, even with the same <code>seed</code> being used for each run.
Default: NULL.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsne_+3A_cuml_log_level">cuML_log_level</code></td>
<td>
<p>Log level within cuML library functions. Must be one of
&quot;off&quot;, &quot;critical&quot;, &quot;error&quot;, &quot;warn&quot;, &quot;info&quot;, &quot;debug&quot;, &quot;trace&quot;.
Default: off.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing the embedding of the input data in a low-
dimensional space, with each row representing an embedded data point.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(cuda.ml)

embedding &lt;- cuda_ml_tsne(iris[1:4], method = "exact")

set.seed(0L)
print(kmeans(embedding, centers = 3))
</code></pre>

<hr>
<h2 id='cuda_ml_tsvd'>Truncated SVD.</h2><span id='topic+cuda_ml_tsvd'></span>

<h3>Description</h3>

<p>Dimensionality reduction using Truncated Singular Value Decomposition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_tsvd(
  x,
  n_components = 2L,
  eig_algo = c("dq", "jacobi"),
  tol = 1e-07,
  n_iters = 15L,
  transform_input = TRUE,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_tsvd_+3A_x">x</code></td>
<td>
<p>The input matrix or dataframe. Each data point should be a row
and should consist of numeric values only.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsvd_+3A_n_components">n_components</code></td>
<td>
<p>Desired dimensionality of output data. Must be strictly
less than <code>ncol(x)</code> (i.e., the number of features in input data).
Default: 2.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsvd_+3A_eig_algo">eig_algo</code></td>
<td>
<p>Eigen decomposition algorithm to be applied to the covariance
matrix. Valid choices are &quot;dq&quot; (divid-and-conquer method for symmetric
matrices) and &quot;jacobi&quot; (the Jacobi method for symmetric matrices).
Default: &quot;dq&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsvd_+3A_tol">tol</code></td>
<td>
<p>Tolerance for singular values computed by the Jacobi method.
Default: 1e-7.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsvd_+3A_n_iters">n_iters</code></td>
<td>
<p>Maximum number of iterations for the Jacobi method.
Default: 15.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsvd_+3A_transform_input">transform_input</code></td>
<td>
<p>If TRUE, then compute an approximate representation
of the input data. Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_tsvd_+3A_cuml_log_level">cuML_log_level</code></td>
<td>
<p>Log level within cuML library functions. Must be one of
&quot;off&quot;, &quot;critical&quot;, &quot;error&quot;, &quot;warn&quot;, &quot;info&quot;, &quot;debug&quot;, &quot;trace&quot;.
Default: off.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A TSVD model object with the following attributes:
- &quot;components&quot;: a matrix of <code>n_components</code> rows to be used for
dimensionalitiy reduction on new data points.
- &quot;explained_variance&quot;: (only present if &quot;transform_input&quot; is set to TRUE)
amount of variance within the input data explained by each component.
- &quot;explained_variance_ratio&quot;: (only present if &quot;transform_input&quot; is set to
TRUE) fraction of variance within the input data explained by each
component.
- &quot;singular_values&quot;: The singular values corresponding to each component.
The singular values are equal to the 2-norms of the <code>n_components</code>
variables in the lower-dimensional space.
- &quot;tsvd_params&quot;: opaque pointer to TSVD parameters which will be used for
performing inverse transforms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(cuda.ml)

iris.tsvd &lt;- cuda_ml_tsvd(iris[1:4], n_components = 2)
print(iris.tsvd)
</code></pre>

<hr>
<h2 id='cuda_ml_umap'>Uniform Manifold Approximation and Projection (UMAP) for dimension reduction.</h2><span id='topic+cuda_ml_umap'></span>

<h3>Description</h3>

<p>Run the Uniform Manifold Approximation and Projection (UMAP) algorithm to
find a low dimensional embedding of the input data that approximates an
underlying manifold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_umap(
  x,
  y = NULL,
  n_components = 2L,
  n_neighbors = 15L,
  n_epochs = 500L,
  learning_rate = 1,
  init = c("spectral", "random"),
  min_dist = 0.1,
  spread = 1,
  set_op_mix_ratio = 1,
  local_connectivity = 1L,
  repulsion_strength = 1,
  negative_sample_rate = 5L,
  transform_queue_size = 4,
  a = NULL,
  b = NULL,
  target_n_neighbors = n_neighbors,
  target_metric = c("categorical", "euclidean"),
  target_weight = 0.5,
  transform_input = TRUE,
  seed = NULL,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_umap_+3A_x">x</code></td>
<td>
<p>The input matrix or dataframe. Each data point should be a row
and should consist of numeric values only.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_y">y</code></td>
<td>
<p>An optional numeric vector of target values for supervised dimension
reduction. Default: NULL.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_n_components">n_components</code></td>
<td>
<p>The dimension of the space to embed into. Default: 2.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_n_neighbors">n_neighbors</code></td>
<td>
<p>The size of local neighborhood (in terms of number of
neighboring sample points) used for manifold approximation. Default: 15.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_n_epochs">n_epochs</code></td>
<td>
<p>The number of training epochs to be used in optimizing the
low dimensional embedding. Default: 500.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_learning_rate">learning_rate</code></td>
<td>
<p>The initial learning rate for the embedding
optimization. Default: 1.0.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_init">init</code></td>
<td>
<p>Initialization mode of the low dimensional embedding. Must be
one of &quot;spectral&quot;, &quot;random&quot;. Default: &quot;spectral&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_min_dist">min_dist</code></td>
<td>
<p>The effective minimum distance between embedded points.
Default: 0.1.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_spread">spread</code></td>
<td>
<p>The effective scale of embedded points. In combination with
<code>min_dist</code> this determines how clustered/clumped the embedded points
are. Default: 1.0.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_set_op_mix_ratio">set_op_mix_ratio</code></td>
<td>
<p>Interpolate between (fuzzy) union and intersection as
the set operation used to combine local fuzzy simplicial sets to obtain a
global fuzzy simplicial sets. Both fuzzy set operations use the product
t-norm. The value of this parameter should be between 0.0 and 1.0; a value
of 1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy
intersection. Default: 1.0.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_local_connectivity">local_connectivity</code></td>
<td>
<p>The local connectivity required &ndash; i.e. the number
of nearest neighbors that should be assumed to be connected at a local
level. Default: 1.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_repulsion_strength">repulsion_strength</code></td>
<td>
<p>Weighting applied to negative samples in low
dimensional embedding optimization. Values higher than one will result in
greater weight being given to negative samples. Default: 1.0.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_negative_sample_rate">negative_sample_rate</code></td>
<td>
<p>The number of negative samples to select per
positive sample in the optimization process. Default: 5.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_transform_queue_size">transform_queue_size</code></td>
<td>
<p>For transform operations (embedding new points
using a trained model this will control how aggressively to search for
nearest neighbors. Default: 4.0.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_a">a</code>, <code id="cuda_ml_umap_+3A_b">b</code></td>
<td>
<p>More specific parameters controlling the embedding. If not set,
then these values are set automatically as determined by <code>min_dist</code>
and <code>spread</code>. Default: NULL.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_target_n_neighbors">target_n_neighbors</code></td>
<td>
<p>The number of nearest neighbors to use to construct
the target simplcial set. Default: n_neighbors.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_target_metric">target_metric</code></td>
<td>
<p>The metric for measuring distance between the actual and
and the target values (<code>y</code>) if using supervised dimension reduction.
Must be one of &quot;categorical&quot;, &quot;euclidean&quot;. Default: &quot;categorical&quot;.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_target_weight">target_weight</code></td>
<td>
<p>Weighting factor between data topology and target
topology. A value of 0.0 weights entirely on data, a value of 1.0 weights
entirely on target. The default of 0.5 balances the weighting equally
between data and target.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_transform_input">transform_input</code></td>
<td>
<p>If TRUE, then compute an approximate representation
of the input data. Default: TRUE.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_seed">seed</code></td>
<td>
<p>Optional seed for pseudo random number generator.  Default: NULL.
Setting a PRNG seed will enable consistency of trained embeddings, allowing
for reproducible results to 3 digits of precision, but at the expense of
potentially slower training and increased memory usage.
If the PRNG seed is not set, then the trained embeddings will not be
deterministic.</p>
</td></tr>
<tr><td><code id="cuda_ml_umap_+3A_cuml_log_level">cuML_log_level</code></td>
<td>
<p>Log level within cuML library functions. Must be one of
&quot;off&quot;, &quot;critical&quot;, &quot;error&quot;, &quot;warn&quot;, &quot;info&quot;, &quot;debug&quot;, &quot;trace&quot;.
Default: off.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A UMAP model object that can be used as input to the
<code>cuda_ml_transform()</code> function.
If <code>transform_input</code> is set to TRUE, then the model object will
contain a &quot;transformed_data&quot; attribute containing the lower dimensional
embedding of the input data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(cuda.ml)

model &lt;- cuda_ml_umap(
  x = iris[1:4],
  y = iris[[5]],
  n_components = 2,
  n_epochs = 200,
  transform_input = TRUE
)

set.seed(0L)
print(kmeans(model$transformed, iter.max = 100, centers = 3))
</code></pre>

<hr>
<h2 id='cuda_ml_unserialize'>Unserialize a CuML model state</h2><span id='topic+cuda_ml_unserialize'></span><span id='topic+cuda_ml_unserialise'></span>

<h3>Description</h3>

<p>Unserialize a CuML model state into a CuML model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuda_ml_unserialize(connection, ...)

cuda_ml_unserialise(connection, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cuda_ml_unserialize_+3A_connection">connection</code></td>
<td>
<p>An open connection or a raw vector.</p>
</td></tr>
<tr><td><code id="cuda_ml_unserialize_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>base::unserialize()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A unserialized CuML model.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+unserialize">unserialize</a></code>
</p>

<hr>
<h2 id='cuda.ml'>cuda.ml</h2><span id='topic+cuda.ml'></span>

<h3>Description</h3>

<p>This package provides a R interface for the RAPIDS cuML library.
</p>


<h3>Author(s)</h3>

<p>Yitao Li &lt;yitao@rstudio.com&gt;
</p>

<hr>
<h2 id='cuML_major_version'>Get the major version of the RAPIDS cuML shared library cuda.ml was linked
to.</h2><span id='topic+cuML_major_version'></span>

<h3>Description</h3>

<p>Get the major version of the RAPIDS cuML shared library cuda.ml was linked
to.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuML_major_version()
</code></pre>


<h3>Value</h3>

<p>The major version of the RAPIDS cuML shared library cuda.ml was
linked to in a character vector, or <code>NA_character_</code> if cuda.ml was not
linked to any version of RAPIDS cuML.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

print(cuML_major_version())
</code></pre>

<hr>
<h2 id='cuML_minor_version'>Get the minor version of the RAPIDS cuML shared library cuda.ml was linked
to.</h2><span id='topic+cuML_minor_version'></span>

<h3>Description</h3>

<p>Get the minor version of the RAPIDS cuML shared library cuda.ml was linked
to.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cuML_minor_version()
</code></pre>


<h3>Value</h3>

<p>The minor version of the RAPIDS cuML shared library cuda.ml was
linked to in a character vector, or <code>NA_character_</code> if cuda.ml was not
linked to any version of RAPIDS cuML.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

print(cuML_minor_version())
</code></pre>

<hr>
<h2 id='has_cuML'>Determine whether cuda.ml was linked to a valid version of the RAPIDS cuML
shared library.</h2><span id='topic+has_cuML'></span>

<h3>Description</h3>

<p>Determine whether cuda.ml was linked to a valid version of the RAPIDS cuML
shared library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>has_cuML()
</code></pre>


<h3>Value</h3>

<p>A logical value indicating whether the current installation cuda.ml
was linked to a valid version of the RAPIDS cuML shared library.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(cuda.ml)

if (!has_cuML()) {
  warning(
    "Please install the RAPIDS cuML shared library first, and then re-",
    "install {cuda.ml}."
  )
}
</code></pre>

<hr>
<h2 id='predict.cuda_ml_fil'>Make predictions on new data points.</h2><span id='topic+predict.cuda_ml_fil'></span>

<h3>Description</h3>

<p>Make predictions on new data points using a FIL model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cuda_ml_fil'
predict(object, x, output_class_probabilities = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cuda_ml_fil_+3A_object">object</code></td>
<td>
<p>A trained CuML model.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_fil_+3A_x">x</code></td>
<td>
<p>A matrix or dataframe containing new data points.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_fil_+3A_output_class_probabilities">output_class_probabilities</code></td>
<td>
<p>Whether to output class probabilities.
NOTE: setting <code>output_class_probabilities</code> to <code>TRUE</code> is only
valid when the model being applied is a classification model and supports
class probabilities output. CuML classification models supporting class
probabilities include <code>knn</code>, <code>fil</code>, and <code>rand_forest</code>.
A warning message will be emitted if <code>output_class_probabilities</code>
is set to <code>TRUE</code> or <code>FALSE</code> but the model being applied does
not support class probabilities output.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_fil_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>predict()</code>. Currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predictions on new data points.
</p>

<hr>
<h2 id='predict.cuda_ml_knn'>Make predictions on new data points.</h2><span id='topic+predict.cuda_ml_knn'></span>

<h3>Description</h3>

<p>Make predictions on new data points using a CuML KNN model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cuda_ml_knn'
predict(object, x, output_class_probabilities = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cuda_ml_knn_+3A_object">object</code></td>
<td>
<p>A trained CuML model.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_knn_+3A_x">x</code></td>
<td>
<p>A matrix or dataframe containing new data points.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_knn_+3A_output_class_probabilities">output_class_probabilities</code></td>
<td>
<p>Whether to output class probabilities.
NOTE: setting <code>output_class_probabilities</code> to <code>TRUE</code> is only
valid when the model being applied is a classification model and supports
class probabilities output. CuML classification models supporting class
probabilities include <code>knn</code>, <code>fil</code>, and <code>rand_forest</code>.
A warning message will be emitted if <code>output_class_probabilities</code>
is set to <code>TRUE</code> or <code>FALSE</code> but the model being applied does
not support class probabilities output.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_knn_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>predict()</code>. Currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predictions on new data points.
</p>

<hr>
<h2 id='predict.cuda_ml_linear_model'>Make predictions on new data points.</h2><span id='topic+predict.cuda_ml_linear_model'></span>

<h3>Description</h3>

<p>Make predictions on new data points using a linear model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cuda_ml_linear_model'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cuda_ml_linear_model_+3A_object">object</code></td>
<td>
<p>A trained CuML model.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_linear_model_+3A_x">x</code></td>
<td>
<p>A matrix or dataframe containing new data points.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_linear_model_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>predict()</code>. Currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predictions on new data points.
</p>

<hr>
<h2 id='predict.cuda_ml_logistic_reg'>Make predictions on new data points.</h2><span id='topic+predict.cuda_ml_logistic_reg'></span>

<h3>Description</h3>

<p>Make predictions on new data points using a CuML logistic regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cuda_ml_logistic_reg'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cuda_ml_logistic_reg_+3A_object">object</code></td>
<td>
<p>A trained CuML model.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_logistic_reg_+3A_x">x</code></td>
<td>
<p>A matrix or dataframe containing new data points.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_logistic_reg_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>predict()</code>. Currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predictions on new data points.
</p>

<hr>
<h2 id='predict.cuda_ml_rand_forest'>Make predictions on new data points.</h2><span id='topic+predict.cuda_ml_rand_forest'></span>

<h3>Description</h3>

<p>Make predictions on new data points using a CuML random forest model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cuda_ml_rand_forest'
predict(
  object,
  x,
  output_class_probabilities = NULL,
  cuML_log_level = c("off", "critical", "error", "warn", "info", "debug", "trace"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cuda_ml_rand_forest_+3A_object">object</code></td>
<td>
<p>A trained CuML model.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_rand_forest_+3A_x">x</code></td>
<td>
<p>A matrix or dataframe containing new data points.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_rand_forest_+3A_output_class_probabilities">output_class_probabilities</code></td>
<td>
<p>Whether to output class probabilities.
NOTE: setting <code>output_class_probabilities</code> to <code>TRUE</code> is only
valid when the model being applied is a classification model and supports
class probabilities output. CuML classification models supporting class
probabilities include <code>knn</code>, <code>fil</code>, and <code>rand_forest</code>.
A warning message will be emitted if <code>output_class_probabilities</code>
is set to <code>TRUE</code> or <code>FALSE</code> but the model being applied does
not support class probabilities output.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_rand_forest_+3A_cuml_log_level">cuML_log_level</code></td>
<td>
<p>Log level within cuML library functions. Must be one of
&quot;off&quot;, &quot;critical&quot;, &quot;error&quot;, &quot;warn&quot;, &quot;info&quot;, &quot;debug&quot;, &quot;trace&quot;.
Default: off.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_rand_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>predict()</code>. Currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predictions on new data points.
</p>

<hr>
<h2 id='predict.cuda_ml_svm'>Make predictions on new data points.</h2><span id='topic+predict.cuda_ml_svm'></span>

<h3>Description</h3>

<p>Make predictions on new data points using a CuML SVM model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cuda_ml_svm'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cuda_ml_svm_+3A_object">object</code></td>
<td>
<p>A trained CuML model.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_svm_+3A_x">x</code></td>
<td>
<p>A matrix or dataframe containing new data points.</p>
</td></tr>
<tr><td><code id="predict.cuda_ml_svm_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>predict()</code>. Currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predictions on new data points.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
