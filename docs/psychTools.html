<!DOCTYPE html><html><head><title>Help for package psychTools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {psychTools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ability'><p>16 ability items scored as correct or incorrect.</p></a></li>
<li><a href='#affect'><p>Two data sets of affect and arousal scores as a function of personality and movie conditions</p></a></li>
<li><a href='#Athenstaedt'><p>Gender Role Self Concept data from Athenstaedt (2003)</p></a></li>
<li><a href='#bfi'><p>25 Personality items representing 5 factors</p></a></li>
<li><a href='#BFI.adjectives.dictionary'><p>Dictionary for the  100 Big Five Adjectives</p></a></li>
<li><a href='#big5.100.adjectives'><p>100 adjectives describing the &quot;big 5&quot; for 502 subjects</p></a></li>
<li><a href='#blant'><p>A 29 x 29 matrix that produces weird factor analytic results</p></a></li>
<li><a href='#blot'><p>Bond's Logical Operations Test &ndash; BLOT</p></a></li>
<li><a href='#burt'><p>11 emotional variables from Burt (1915)</p></a></li>
<li><a href='#cities'><p>Distances between 11 US cities</p></a></li>
<li><a href='#colom'><p>Correlations of 14 ability tests from the Spanish version of the WAIS (taken from Colom et al. 2002.)</p></a></li>
<li><a href='#cubits'><p>Galton's example of the relationship between height and 'cubit' or forearm length</p></a></li>
<li><a href='#cushny'>
<p>A data set from Cushny and Peebles (1905) on the effect of three drugs on hours of sleep, used by Student (1908)</p></a></li>
<li><a href='#df2latex'><p>Convert a data frame, correlation matrix, or factor analysis output to a LaTeX or rtf table</p></a></li>
<li><a href='#dfOrder'><p>Sort (order) a dataframe or matrix by multiple columns</p></a></li>
<li><a href='#eminence'><p>Eminence of 69 American Psychologists</p></a></li>
<li><a href='#epi'><p>Eysenck Personality Inventory (EPI) data for 3570 participants</p></a></li>
<li><a href='#epi.bfi'><p>13 personality scales from the Eysenck Personality Inventory and Big 5 inventory</p></a></li>
<li><a href='#galton'><p>Galton's Mid parent child height data</p></a></li>
<li><a href='#GERAS'><p>Data from Gruber et al, 2020, Study 2: Gender Related Attributes Survey</p></a></li>
<li><a href='#globalWarm'><p>7 attitude items about Global Warming policy from Erik Nisbet</p></a></li>
<li><a href='#heights'><p>A data.frame of the Galton (1888) height and cubit data set.</p></a></li>
<li><a href='#holzinger.swineford'>
<p>The raw and transformed data from Holzinger and Swineford, 1939</p></a></li>
<li><a href='#income'><p>US family income from US census 2008</p>
</p></a></li>
<li><a href='#iqitems'><p>16 multiple choice IQ items</p></a></li>
<li><a href='#msq'><p>75 mood items from the Motivational State Questionnaire for 3896 participants</p></a></li>
<li><a href='#msqR'><p>75 mood items from the Motivational State Questionnaire for 3032 unique participants</p></a></li>
<li><a href='#neo'><p>NEO correlation matrix from the NEO_PI_R manual</p></a></li>
<li><a href='#peas'><p>Galton's Peas</p></a></li>
<li><a href='#Pollack'><p>Pollack et al (2012) correlation matrix for mediation example</p></a></li>
<li><a href='#psychTools'><p>psychTools:  datasets and utility functions to accompany the psych package</p></a></li>
<li><a href='#rd2html'><p>Convert all Rd files in a directory to HTML files in a new directory</p></a></li>
<li><a href='#read.file'><p>Shortcuts for reading from the clipboard or a file</p></a></li>
<li><a href='#recode'><p>Recode or rearrange or reshape variables or values to new values</p></a></li>
<li><a href='#sai'><p>State Anxiety data from the PMC lab over multiple occasions.</p></a></li>
<li><a href='#salary'><p>Salary example from Cohen, Cohen, Aiken and West (2003)</p></a></li>
<li><a href='#sat.act'><p>3 Measures of ability: SATV, SATQ, ACT</p></a></li>
<li><a href='#Schutz'>
<p>The Schutz correlation matrix example from Shapiro and ten Berge</p></a></li>
<li><a href='#selectBy'><p>Select a subset of rows (subjects) meeting one or more criteria for columns</p></a></li>
<li><a href='#Spengler'><p>Project Talent data set from Marion Spengler and Rodica Damian</p></a></li>
<li><a href='#spi'><p>A sample from the SAPA Personality Inventory including an item dictionary and scoring keys.</p></a></li>
<li><a href='#usaf'><p>17 anthropometric measures from the USAF showing a general factor</p></a></li>
<li><a href='#Utility'><p>Useful utility functions for file/directory exploration and manipulation.</p></a></li>
<li><a href='#vegetables'><p> Paired comparison of preferences for 9 vegetables</p></a></li>
<li><a href='#vJoin'><p>Combine two matrices or data frames into one based upon variable labels</p>
</p></a></li>
<li><a href='#zola'><p>Correlation matrix of 135 self report and 30 peer report personality items</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>2.4.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-12</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools to Accompany the 'psych' Package for Psychological
Research</td>
</tr>
<tr>
<td>Description:</td>
<td>Support functions,  data sets, and vignettes for the 'psych' package. Contains several of the biggest data sets for the 'psych' package as well as four vignettes. A few helper functions for file manipulation are included as well. For more information, see the <a href="https://personality-project.org/r/">https://personality-project.org/r/</a> web page.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>psych,foreign,rtf</td>
</tr>
<tr>
<td>Suggests:</td>
<td>parallel, GPArotation, lavaan,knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 2.10)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>TRUE</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://personality-project.org/r/psych/">https://personality-project.org/r/psych/</a>
<a href="https://personality-project.org/r/psych-manual.pdf">https://personality-project.org/r/psych-manual.pdf</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-18 22:18:00 UTC; WR</td>
</tr>
<tr>
<td>Author:</td>
<td>William Revelle <a href="https://orcid.org/0000-0003-4880-9610"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>William Revelle &lt;revelle@northwestern.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-19 17:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='ability'>16 ability items scored as correct or incorrect.</h2><span id='topic+ability'></span><span id='topic+ability.keys'></span>

<h3>Description</h3>

<p>16 multiple choice ability items 1525 subjects taken from the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project are saved as <code><a href="#topic+iqitems">iqitems</a></code>. Those data are shown as examples of how to score multiple choice tests and analyses of response alternatives.  When scored correct or incorrect, the data are useful for demonstrations of  tetrachoric based factor analysis <code>irt.fa</code> and finding tetrachoric correlations. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(iqitems)</code></pre>


<h3>Format</h3>

<p>A data frame with 1525 observations on the following 16 variables. The number following the name is the item number from SAPA.
</p>

<dl>
<dt><code>reason.4</code></dt><dd><p>Basic reasoning questions </p>
</dd>
<dt><code>reason.16</code></dt><dd><p>Basic reasoning question</p>
</dd>
<dt><code>reason.17</code></dt><dd><p>Basic reasoning question</p>
</dd>
<dt><code>reason.19</code></dt><dd><p>Basic reasoning question </p>
</dd>   
<dt><code>letter.7</code></dt><dd><p>In the following alphanumeric series, what letter comes next?</p>
</dd>
<dt><code>letter.33</code></dt><dd><p>In the following alphanumeric series, what letter comes next?</p>
</dd>
<dt><code>letter.34</code></dt><dd><p>In the following alphanumeric series, what letter comes next</p>
</dd>
<dt><code>letter.58</code></dt><dd><p>In the following alphanumeric series, what letter comes next?</p>
</dd>
<dt><code>matrix.45</code></dt><dd><p>A matrix reasoning task</p>
</dd>
<dt><code>matrix.46</code></dt><dd><p>A matrix reasoning task</p>
</dd>
<dt><code>matrix.47</code></dt><dd><p>A matrix reasoning task</p>
</dd>
<dt><code>matrix.55</code></dt><dd><p>A matrix reasoning task</p>
</dd>
<dt><code>rotate.3</code></dt><dd><p>Spatial Rotation of type 1.2</p>
</dd>
<dt><code>rotate.4</code></dt><dd><p>Spatial Rotation of type 1.2</p>
</dd>
<dt><code>rotate.6</code></dt><dd><p>Spatial Rotation of type 1.1</p>
</dd>
<dt><code>rotate.8</code></dt><dd><p>Spatial Rotation of type 2.3</p>
</dd>
</dl>



<h3>Details</h3>

<p>16 items were sampled from 80 items given as part of the SAPA (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) project  (Revelle, Wilt and Rosenthal, 2009; Condon and Revelle, 2014) to develop online measures of ability. These 16 items reflect four lower order factors (verbal reasoning, letter series, matrix reasoning, and spatial rotations.  These lower level factors all share a higher level factor ('g').  
</p>
<p>This data set may be used to demonstrate item response functions, <code>tetrachoric</code> correlations, or <code>irt.fa</code> as well as <code>omega</code> estimates of of reliability and hierarchical structure.
</p>
<p>In addition, the data set is a good example of doing item analysis to examine the empirical response probabilities of each item alternative as a function of the underlying latent trait.  When doing this, it appears that two of the matrix reasoning problems do not have monotonically increasing trace lines for the probability correct.  At moderately high ability (theta = 1) there is a decrease in the probability correct from theta = 0 and theta = 2.
</p>


<h3>Source</h3>

<p> The example data set is taken from the Synthetic Aperture Personality Assessment personality and ability test at <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>.  The data were collected with David Condon from 8/08/12 to 8/31/12.
</p>
<p>Similar data are available from the International Cognitive Ability Resource at <a href="https://www.icar-project.org/">https://www.icar-project.org/</a>.
</p>


<h3>References</h3>

<p>Condon, David and Revelle, William, (2014) The International Cognitive Ability Resource:  Development and initial validation of a public-domain measure. Intelligence, 43, 52-64.
</p>
<p>Revelle, William, Dworak, Elizabeth M. and Condon, David (2020) Cognitive ability in everyday life: the utility of open-source measures.  Current Directions in Psychological Science, 29, (4) 358-363. Open access at <a href="https://doi.org/10.1177/0963721420922178">doi:10.1177/0963721420922178</a>.
</p>
<p>Dworak, Elizabeth M., Revelle, William, Doebler, Philip and Condon, David (2021)  Using the International Cognitive Ability Resource as an open source tool to explore individual differences in cognitive ability.  Personality and Individual Differences, 169. Open access at <a href="https://doi.org/10.1016/j.paid.2020.109906">doi:10.1016/j.paid.2020.109906</a>.
Revelle, William, Wilt, Joshua,  and Rosenthal, Allen (2010)  Personality and Cognition: The Personality-Cognition Link. In Gruszka, Alexandra  and Matthews, Gerald   and Szymura, Blazej (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control, Springer. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ability)
cs&lt;- psych::cs
keys &lt;- list(ICAR16=colnames(ability),reasoning =  cs(reason.4,reason.16,reason.17,reason.19),
  letters=cs(letter.7, letter.33,letter.34,letter.58), 
  matrix=cs(matrix.45,matrix.46,matrix.47,matrix.55), 
  rotate=cs(rotate.3,rotate.4,rotate.6,rotate.8))
  psych::scoreOverlap(keys,ability)
    #this next step takes a few seconds to run and demonstrates IRT approaches
     ability.irt &lt;- psych::irt.fa(ability)
     ability.scores &lt;- psych::scoreIrt(ability.irt,ability)
     ability.sub.scores &lt;- psych::scoreIrt.2pl(keys,ability) #demonstrate irt scoring

#It is sometimes asked how to handle missing data when finding scores
#this next example compares 3 ways of scoring ability items from icar
#Just sum the items
#Sum the means for the items
#IRT score the items

total &lt;- rowSums(ability, na.rm=TRUE)
 means  &lt;- rowMeans(ability, na.rm=TRUE)
irt &lt;- psych::scoreIrt(items=ability)[1]
 df &lt;- data.frame(total, means,irt)
 psych:: pairs.panels(df)


</code></pre>

<hr>
<h2 id='affect'>Two data sets of affect and arousal scores as a function of personality and movie conditions
</h2><span id='topic+affect'></span><span id='topic+maps'></span><span id='topic+flat'></span>

<h3>Description</h3>

<p>A recurring question in the study of affect is the proper dimensionality and the relationship to various personality dimensions. Here is a data set taken from two studies of mood and arousal using movies to induce affective states.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(affect)</code></pre>


<h3>Details</h3>

<p>These are data from two studies conducted in the Personality, Motivation and Cognition Laboratory at Northwestern University.  Both studies used a similar methodology: 
</p>
<p>Collection of pretest data using 5 scales from the Eysenck Personality Inventory and items taken from the Motivational State Questionnaire (see <code><a href="#topic+msq">msq</a></code>. In addition, state and trait anxiety measures were given.  In the &ldquo;maps&quot; study, the Beck Depression Inventory was given also.
</p>
<p>Then subjects were randomly assigned to one of four movie conditions: 1: Frontline.  A documentary about the liberation of the Bergen-Belsen concentration camp.  2: Halloween.  A horror film.  3: National Geographic, a nature film about the Serengeti plain.  4: Parenthood.  A comedy.  Each film clip was shown for 9 minutes.  Following this the MSQ was given again.  
</p>
<p>Data from the MSQ were scored for Energetic and Tense Arousal (EA and TA) as well as Positive and Negative Affect (PA and NA).  
</p>
<p>Study flat had 170 participants, study maps had 160.
</p>
<p>These studies are described in more detail in various publications from the PMC lab.  In particular, Revelle and Anderson, 1997 and Rafaeli and Revelle (2006). An analysis of these data has also appeared in Smillie et al. (2012).
</p>
<p>For a much more complete data set involving film, caffeine, and time of day manipulations, see the <code><a href="#topic+msqR">msqR</a></code> data set.
</p>


<h3>Source</h3>

<p>Data collected at the Personality, Motivation, and Cognition Laboratory, Northwestern University. 
</p>


<h3>References</h3>

<p>Revelle,  William  and Anderson, Kristen Joan  (1997) Personality, motivation and cognitive performance: Final report to the Army Research Institute on  contract MDA 903-93-K-0008
</p>
<p>Rafaeli, Eshkol and Revelle, William (2006), A premature consensus: Are happiness and sadness truly opposite affects? Motivation and Emotion, 30, 1, 1-12.
</p>
<p>Smillie, Luke D.  and Cooper, Andrew  and Wilt, Joshua  and Revelle, William (2012) Do Extraverts Get More Bang for the Buck? Refining the Affective-Reactivity Hypothesis of Extraversion. Journal of Personality and Social Psychology, 103 (2), 206-326.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(affect)
psych::describeBy(affect[-1],group="Film")
psych::pairs.panels(affect[14:17],bg=c("red","black","white","blue")[affect$Film],pch=21,
    main="Affect varies by movies ")
psych::errorCircles("EA2","TA2",data=affect,group="Film",labels=c("Sad","Fear","Neutral","Humor")
, main="Enegetic and Tense Arousal by Movie condition")
psych::errorCircles(x="PA2",y="NA2",data=affect,group="Film",labels=c("Sad","Fear","Neutral","
Humor"),  main="Positive and Negative Affect by Movie condition")

</code></pre>

<hr>
<h2 id='Athenstaedt'>Gender Role Self Concept data from Athenstaedt (2003)</h2><span id='topic+Athenstaedt'></span><span id='topic+Athenstaedt.dictionary'></span><span id='topic+Athenstaedt.keys'></span>

<h3>Description</h3>

<p>Athenstaedt (2003) examined Gender Role Self-Concept. She reports two independent dimensions of Male and Female behaviors.  While there are large gender/sex differences on both of these dimensions, the two represent independent factorsl  Eagly and Revelle (2022) have used these data to explore the power of aggregation when examining sex differences.  This data set is also useful to show various graphical display procedures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Athenstaedt")</code></pre>


<h3>Format</h3>

<p>A data frame with 576 observations on the following 117 variables.
</p>

<dl>
<dt><code>STUDIE</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>gender</code></dt><dd><p>Male =1, Female= 2</p>
</dd>
<dt>V1 - V74</dt><dd><p>self report items (see Athenstaedt.dictionary)</p>
</dd>
<dt>V1</dt><dd><p>Gender   (Male = 1,  Female =2)</p>
</dd>
<dt>V2</dt><dd><p>To pay attention to ones appearance in the office</p>
</dd>
<dt>V3</dt><dd><p>Offer fire to somebody</p>
</dd>
<dt>V4</dt><dd><p>Paint an Apartment</p>
</dd>
<dt>V5</dt><dd><p>Mow the Lawn</p>
</dd>
<dt>V6</dt><dd><p>Make the Bed</p>
</dd>
<dt>V7</dt><dd><p>Hold the Door Open for your Partner</p>
</dd>
<dt>V8</dt><dd><p>Do the Dishes</p>
</dd>
<dt>V9</dt><dd><p>Do Extreme Sports</p>
</dd>
<dt>V10</dt><dd><p>Tinker with the Car</p>
</dd>
<dt>V11</dt><dd><p>Talk about Sports</p>
</dd>
<dt>V12</dt><dd><p>Assemble Prefabricated Furniture</p>
</dd>
<dt>V13</dt><dd><p>Drive a Car in a Risky Way</p>
</dd>
<dt>V14</dt><dd><p>Listen Attentively to Others</p>
</dd>
<dt>V15</dt><dd><p>Tell your Partner about Problems at Work</p>
</dd>
<dt>V16</dt><dd><p>Play on a Computer</p>
</dd>
<dt>V17</dt><dd><p>Set the Table</p>
</dd>
<dt>V18</dt><dd><p>Watch ones Weight</p>
</dd>
<dt>V19</dt><dd><p>Care for a Partner if he/she is Ill</p>
</dd>
<dt>V20</dt><dd><p>Play Chess</p>
</dd>
<dt>V21</dt><dd><p>Meet with friends at a Regulars Table</p>
</dd>
<dt>V22</dt><dd><p>Watch Soap Operas</p>
</dd>
<dt>V23</dt><dd><p>Take a Friends Arm</p>
</dd>
<dt>V24</dt><dd><p>Wrap Presents Beautifully</p>
</dd>
<dt>V25</dt><dd><p>In case of Vacation with Partner Packing the Luggage for Both</p>
</dd>
<dt>V26</dt><dd><p>To admit own Occupational Weekness</p>
</dd>
<dt>V27</dt><dd><p>Work Overtime</p>
</dd>
<dt>V28</dt><dd><p>Openly Show Vulnerability</p>
</dd>
<dt>V29</dt><dd><p>Babysit</p>
</dd>
<dt>V30</dt><dd><p>Change Fuses</p>
</dd>
<dt>V31</dt><dd><p>Clean a Drain</p>
</dd>
<dt>V32</dt><dd><p>Take Care of Somebody</p>
</dd>
<dt>V33</dt><dd><p>Do Repair Work</p>
</dd>
<dt>V34</dt><dd><p>Change Light Bulbs</p>
</dd>
<dt>V35</dt><dd><p>Wash the Car</p>
</dd>
<dt>V36</dt><dd><p>Ride a Motorcycle</p>
</dd>
<dt>V37</dt><dd><p>Cook Meat on the Grill</p>
</dd>
<dt>V38</dt><dd><p>Thump Carpets</p>
</dd>
<dt>V39</dt><dd><p>Dust the Furniture</p>
</dd>
<dt>V40</dt><dd><p>Buy Electric Appliances</p>
</dd>
<dt>V41</dt><dd><p>Go Dancing</p>
</dd>
<dt>V42</dt><dd><p>Go for a Walk through Town</p>
</dd>
<dt>V43</dt><dd><p>Go to the Ballet</p>
</dd>
<dt>V44</dt><dd><p>Hug a Friend</p>
</dd>
<dt>V45</dt><dd><p>Do Handiwork (e.g. Knitting)</p>
</dd>
<dt>V46</dt><dd><p>Change Bed Sheets</p>
</dd>
<dt>V47</dt><dd><p>Sew on a Button</p>
</dd>
<dt>V48</dt><dd><p>Do Aerobics</p>
</dd>
<dt>V49</dt><dd><p>Watch Sports on Television</p>
</dd>
<dt>V50</dt><dd><p>Talk about Problems</p>
</dd>
<dt>V51</dt><dd><p>Play Parlor Games</p>
</dd>
<dt>V52</dt><dd><p>Talk about Politics</p>
</dd>
<dt>V53</dt><dd><p>Take Care of Flowers</p>
</dd>
<dt>V54</dt><dd><p>Make Coffee in the Office</p>
</dd>
<dt>V55</dt><dd><p>Shovel Snow</p>
</dd>
<dt>V56</dt><dd><p>Read non-Fiction Books</p>
</dd>
<dt>V57</dt><dd><p>Organize Company Parties</p>
</dd>
<dt>V58</dt><dd><p>Do Home Improvement Jobs</p>
</dd>
<dt>V59</dt><dd><p>Plead for the Socially Disadvantaged</p>
</dd>
<dt>V60</dt><dd><p>Buy a Present for a Colleague</p>
</dd>
<dt>V61</dt><dd><p>To Talk with Colleagues about Family Matters</p>
</dd>
<dt>V62</dt><dd><p>Make Jam</p>
</dd>
<dt>V63</dt><dd><p>Frquently Ask Colleagues Questions</p>
</dd>
<dt>V64</dt><dd><p>Decorate the Office with Flowers</p>
</dd>
<dt>V65</dt><dd><p>Pick up the Dinner Bill</p>
</dd>
<dt>V66</dt><dd><p>Shop for the Family</p>
</dd>
<dt>V67</dt><dd><p>Have Problem using Technical Devices</p>
</dd>
<dt>V68</dt><dd><p>Care for Family Besides a Job</p>
</dd>
<dt>V69</dt><dd><p>Watch Action Movies</p>
</dd>
<dt>V70</dt><dd><p>Cook</p>
</dd>
<dt>V71</dt><dd><p>Help your Partner Put on His or Her Coat</p>
</dd>
<dt>V72</dt><dd><p>Wash Windows</p>
</dd>
<dt>V73</dt><dd><p>Do the Ironing</p>
</dd>
<dt>V74</dt><dd><p>Do the Laundry</p>
</dd>
<dt>V75</dt><dd><p>Put on Make-up</p>
</dd>
<dt>V76</dt><dd><p>Femininity Scale</p>
</dd>
<dt>V77</dt><dd><p>Masculinity Scale</p>
</dd>
<dt>V78</dt><dd><p>Femininity Scale</p>
</dd>
<dt>V79</dt><dd><p>Masculinity Scale</p>
</dd>
<dt>V80</dt><dd><p>Pooled Scale</p>
</dd>
<dt>MMINUS1 - MPLUS</dt><dd><p>see the original Athenstaedt paper</p>
</dd>
<dt><code>FBEHAV</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>MBEHAV</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>Femininity</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>Masculinity</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>MF</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p>Ursala Athenstaedt (2003) reported several analyses of items and scales measuring  Gender Role Self-Concept.  Eagly and Revelle (2022) have used these data in an analysis of the power of aggregation.  Here are the original items as well as the three scales Eagly and Revelle (2022).  The accompanying Athenstaedt.dictionary may be used to see the items.
</p>
<p>See the <code><a href="#topic+GERAS">GERAS</a></code> data set for a related example.
</p>


<h3>Source</h3>

<p>Ursala Athenstaedt, personal communication, 2022, provided a SPSS sav file with the original data from which the complete cases in this set were selected.
</p>


<h3>References</h3>

<p>Ursula Athenstaedt (2003) On the Content and Structure of the Gender Role Self-Concept: Including Gender-Stereotypical Behaviors in Addition to Traits.  Psychology of Women Quarterly, 27, 309-318. doi: 10.1111/1471-6402.00111.
</p>
<p>Alice Eagly and William Revelle (2022) Understanding the Magnitude of Psychological Differences Between Women and Men Requires Seeing the Forest and the Trees. Perspectives in Psychological Science doi:10.1177/17456916211046006.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Athenstaedt)
psych::scatterHist(Femininity ~ Masculinity + gender, data =Athenstaedt,
cex.point=.4,smooth=FALSE, correl=FALSE,d.arrow=TRUE,col=c("red","blue"),
   lwd=4,  cex.main=1.5,main="Scatter Plot and Density",cex.axis=2)
   
psych::cohen.d(Athenstaedt[2:76], group="gender", dictionary=Athenstaedt.dictionary)
#show the top 5 items for each scale
select &lt;- c(psych::selectFromKeys(Athenstaedt.keys$MF10),"gender")
psych::corPlot(Athenstaedt[,select], main="F and M items from Athenstaedt")

</code></pre>

<hr>
<h2 id='bfi'>25 Personality items representing 5 factors</h2><span id='topic+bfi'></span><span id='topic+bfi.dictionary'></span><span id='topic+bfi.keys'></span>

<h3>Description</h3>

<p>25 personality self report items taken from the International Personality Item Pool (ipip.ori.org) were included as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project.  The data from 2800 subjects are included here as a demonstration set for scale construction, factor analysis, and Item Response Theory analysis.  Three additional demographic variables (sex, education, and age) are also included.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bfi)
data(bfi.dictionary)

</code></pre>


<h3>Format</h3>

<p>A data frame with 2800 observations on the following 28 variables. (The q numbers are the SAPA item numbers).
</p>

<dl>
<dt><code>A1</code></dt><dd><p>Am indifferent to the feelings of others. (q_146)</p>
</dd>
<dt><code>A2</code></dt><dd><p>Inquire about others' well-being. (q_1162)</p>
</dd>
<dt><code>A3</code></dt><dd><p>Know how to comfort others. (q_1206) </p>
</dd>
<dt><code>A4</code></dt><dd><p>Love children. (q_1364)</p>
</dd>
<dt><code>A5</code></dt><dd><p>Make people feel at ease. (q_1419)</p>
</dd>
<dt><code>C1</code></dt><dd><p>Am exacting in my work. (q_124)</p>
</dd>
<dt><code>C2</code></dt><dd><p>Continue until everything is perfect. (q_530)</p>
</dd>
<dt><code>C3</code></dt><dd><p>Do things according to a plan. (q_619)</p>
</dd>
<dt><code>C4</code></dt><dd><p>Do things in a half-way manner. (q_626)</p>
</dd>
<dt><code>C5</code></dt><dd><p>Waste my time. (q_1949)</p>
</dd>
<dt><code>E1</code></dt><dd><p>Don't talk a lot. (q_712)</p>
</dd>
<dt><code>E2</code></dt><dd><p>Find it difficult to approach others. (q_901)</p>
</dd>
<dt><code>E3</code></dt><dd><p>Know how to captivate people. (q_1205)</p>
</dd>
<dt><code>E4</code></dt><dd><p>Make friends easily. (q_1410)</p>
</dd>
<dt><code>E5</code></dt><dd><p>Take charge. (q_1768)</p>
</dd>
<dt><code>N1</code></dt><dd><p>Get angry easily. (q_952)</p>
</dd>
<dt><code>N2</code></dt><dd><p>Get irritated easily. (q_974)</p>
</dd>
<dt><code>N3</code></dt><dd><p>Have frequent mood swings. (q_1099</p>
</dd>
<dt><code>N4</code></dt><dd><p>Often feel blue. (q_1479)</p>
</dd>
<dt><code>N5</code></dt><dd><p>Panic easily. (q_1505)</p>
</dd>
<dt><code>O1</code></dt><dd><p>Am full of ideas. (q_128)</p>
</dd>
<dt><code>O2</code></dt><dd><p>Avoid difficult reading material.(q_316)</p>
</dd>
<dt><code>O3</code></dt><dd><p>Carry the conversation to a higher level. (q_492)</p>
</dd>
<dt><code>O4</code></dt><dd><p>Spend time reflecting on things. (q_1738)</p>
</dd>
<dt><code>O5</code></dt><dd><p>Will not probe deeply into a subject. (q_1964)</p>
</dd>
<dt><code>gender</code></dt><dd><p>Males = 1, Females =2</p>
</dd>
<dt><code>education</code></dt><dd><p>1 = HS, 2 = finished HS, 3 = some college, 4 = college graduate 5 = graduate degree</p>
</dd>
<dt><code>age</code></dt><dd><p>age in years</p>
</dd>
</dl>



<h3>Details</h3>

<p>The first 25 items are organized by five putative factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Opennness.  The scoring key is created using  <code>make.keys</code>, the scores are found using  <code>score.items</code>.
</p>
<p>These five factors are a useful example of using <code>irt.fa</code> to do Item Response Theory based latent factor analysis of the <code>polychoric</code> correlation matrix.  The endorsement plots for each item, as well as the item information functions reveal that the items differ in their quality.
</p>
<p>The item data were collected using a 6 point response scale: 
1 Very Inaccurate
2 Moderately Inaccurate
3 Slightly Inaccurate
4 Slightly Accurate
5 Moderately Accurate
6 Very Accurate
</p>
<p>as part of the Synthetic Apeture Personality Assessment (SAPA <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) project.  To see an example of the data collection technique, visit <a href="https://www.SAPA-project.org/">https://www.SAPA-project.org/</a> or the International Cognitive Ability Resource at <a href="https://icar-project.org">https://icar-project.org</a>.  The items given were sampled from the International Personality Item Pool of Lewis Goldberg using the sampling technique of SAPA.  This is a sample data set taken from the much larger SAPA data bank.
</p>


<h3>Note</h3>

<p>The bfi data set and items should not be confused with the BFI (Big Five Inventory) of Oliver John and colleagues (John, O. P., Donahue, E. M., &amp; Kentle, R. L. (1991). The Big Five Inventory&ndash;Versions 4a and 54. Berkeley, CA: University of California,Berkeley, Institute of Personality and Social Research.)
</p>


<h3>Source</h3>

<p>The items are from the ipip (Goldberg, 1999).  The data are from the SAPA project (Revelle, Wilt and Rosenthal, 2010) , collected Spring, 2010 ( <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>).
</p>


<h3>References</h3>

<p>Goldberg, L.R. (1999) A broad-bandwidth, public domain, personality inventory measuring the lower-level facets of several five-factor models. In Mervielde, I. and Deary, I. and De Fruyt, F. and Ostendorf, F. (eds) Personality psychology in Europe. 7. Tilburg University Press. Tilburg, The Netherlands.
</p>
<p>Revelle, W., Wilt, J.,  and Rosenthal, A. (2010)  Individual Differences in Cognition: New Methods for examining the Personality-Cognition Link In Gruszka, A.  and Matthews, G. and Szymura, B. (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control, Springer.
</p>
<p>Revelle, W,  Condon, D.M.,  Wilt, J.,  French, J.A., Brown, A.,  and  Elleman, L.G. (2016) Web and phone based data collection using planned missing designs. In  Fielding, N.G.,  Lee, R.M. and  Blank, G. (Eds). SAGE Handbook of Online Research Methods (2nd Ed), Sage Publcations. </p>


<h3>See Also</h3>

<p><code>bi.bars</code> to show the data by age and gender, <code>irt.fa</code> for item factor analysis applying the irt model.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bfi)
psych::describe(bfi)
# create the bfi.keys (actually already saved in the data file)
 bfi.keys &lt;-
  list(agree=c("-A1","A2","A3","A4","A5"),conscientious=c("C1","C2","C3","-C4","-C5"),
extraversion=c("-E1","-E2","E3","E4","E5"),neuroticism=c("N1","N2","N3","N4","N5"),
openness = c("O1","-O2","O3","O4","-O5")) 

 scores &lt;- psych::scoreItems(bfi.keys,bfi,min=1,max=6) #specify the minimum and maximum values
 scores
 #show the use of the keys.lookup with a dictionary
 psych::keys.lookup(bfi.keys,bfi.dictionary[,1:4])
 
</code></pre>

<hr>
<h2 id='BFI.adjectives.dictionary'>Dictionary for the  100 Big Five Adjectives</h2><span id='topic+BFI.adjectives.dictionary'></span><span id='topic+bfi.adjectives.dictionary'></span><span id='topic+big5.adjectives.dictionary'></span>

<h3>Description</h3>

<p>Lew Goldberg organized 100 adjectives to measure 5 factors of personality (The Big5).  500 hundred participants were given these adjectives along with other personality measures.  This dictionary allows for easy item labeling of the results. 
~
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("BFI.adjectives.dictionary")
 </code></pre>


<h3>Format</h3>

<p>A data frame with 100 observations on the following 2 variables.
</p>

<dl>
<dt><code>numer</code></dt><dd><p>a character vector of the item label</p>
</dd>
<dt><code>Item</code></dt><dd><p>a character vector of the actual adjectives</p>
</dd>
</dl>



<h3>Details</h3>

<p>Keying information for the 100 adjectives:
</p>


<h3>Source</h3>

<p>Data collected at the Personality, Motivation, and Cognition Laboratory, Northwestern University.</p>


<h3>References</h3>

<p>Lewis R. Goldberg,(1992) The development of markers for the Big-Five factor structure, Psychological Assessment, 4 (1) 26-42.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+big5.100.adjectives">big5.100.adjectives</a></code> for examples of the data. 
<code><a href="#topic+msqR">msqR</a></code> for 3896 participants with scores on five scales of the EPI.  <code><a href="#topic+affect">affect</a></code> for an example of the use of some of these adjectives in a mood manipulation study.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(BFI.adjectives.dictionary) #this includes the bfi.adjectives.keys
bfi.adjectives.keys &lt;- list(
Agreeableness =																				
psych::cs(V2,	-V11, V14, V15, -V19,	-V21, V29,	-V31, V32,	V48, V55,-V61,	-V63,	
V69, V76, -V78,	-V79, -V90,	-V94,	V99), 
Conscientiousness	= psych::cs(V9,	-V10,	V13, -V20,	V22, -V30, -V37, -V38, -V39,	
     V50,  -V51, V53, V56, V57, -V67,	V68, V70, V73, -V82, -V95),
Extraversion = psych::cs(V1,V5,	-V6,V7,	V17, V24, V26, -V40,-V45, -V58,	-V60,-V65,
     V71,  -V74,	-V77,	V92, -V96,	V97, V98, -V100),
Neuroticism= psych::cs(V3, V23, V25, V27,V28, V33,-V36, V42, V46,V47, V49, V52,-V59,V62,
 V72, V75, -V81,-V83,-V84, -V85),
Openness = psych::cs(V4,V8,V12, V16, V18,V34, -V35,V41, V43, V44, V54,	-V64,-V66, -V80,
-V86, -V87, -V88, -V89, -V91, -V93)
	)
	
psych::lookupFromKeys(bfi.adjectives.keys,bfi.adjectives.dictionary,20)

</code></pre>

<hr>
<h2 id='big5.100.adjectives'>100 adjectives describing the &quot;big 5&quot; for 502 subjects</h2><span id='topic+big5.100.adjectives'></span><span id='topic+bfi.adjectives'></span><span id='topic+big5.adjectives.keys'></span><span id='topic+bfi.adjectives.keys'></span>

<h3>Description</h3>

<p>Lew Goldberg organized 100 adjectives to measure 5 factors of personality (The Big5).  500 hundred participants were given these adjectives along with other personality measures in the Personality, Motivation and Cognition (PMC) lab.  This data set is for demonstrations of factor and cluster analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("big5.100.adjectives")</code></pre>


<h3>Format</h3>

<p>A data frame with 554 observations on the following 102 variables.
</p>

<dl>
<dt><code>study</code></dt><dd><p>a character vector</p>
</dd>
<dt><code>id</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V1</code></dt><dd><p>numeric vector  (see big5.adjectives.dictionary) </p>
</dd>
<dt><code>V100</code></dt><dd><p>A numeric vector. (see big5.adjectives.dictionary)</p>
</dd>
<dt>bfi.adjectives.keys</dt><dd><p>a key list</p>
</dd>
</dl>



<h3>Details</h3>

<p>Procedure. The data were collected over nine years in the Personality, Motivation and Cognition laboratory at Northwestern, as part of a series of studies examining the effects of personality and situational factors on motivational state and subsequent cognitive performance. In each of 38 studies, prior to any manipulation of motivational state, participants signed a consent form and in some studies, consumed 0 or 4mg/kg of caffeine. In caffeine studies, they waited 30 minutes and then filled out the MSQ as well as other personality trait measures (e.g. the Big 5 adjectives)
</p>


<h3>Source</h3>

<p>Data collected at the Personality, Motivation, and Cognition Laboratory, Northwestern University.</p>


<h3>References</h3>

<p>Lewis R. Goldberg,(1992) The development of markers for the Big-Five factor structure, Psychological Assessment, 4 (1) 26-42.
</p>
<p>Revelle, W. and  Anderson, K.J. (1998) Personality, motivation and cognitive performance: Final report to the Army Research Institute on  contract MDA 903-93-K-0008. (<a href="https://www.personality-project.org/revelle/publications/ra.ari.98.pdf">https://www.personality-project.org/revelle/publications/ra.ari.98.pdf</a>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(big5.100.adjectives)
five.scores &lt;- psych::scoreItems(big5.adjectives.keys,big5.100.adjectives)
summary(five.scores)
</code></pre>

<hr>
<h2 id='blant'>A 29 x 29 matrix that produces weird factor analytic results</h2><span id='topic+blant'></span>

<h3>Description</h3>

<p>Normally, min.res factor analysis and maximum likelihood produce very similar results.  This data set (from Alexandra Blant) does not.  Warnings are given for the min.res solution, the pa solution, but not the old.min nor the mle solution.  Included as a test case for the factor analysis function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("blant")</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:29, 1:29] 1 0.77 0.813 0.68 0.717 ...
- attr(*, &quot;dimnames&quot;)=List of 2
..$ : NULL
..$ : chr [1:29] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; ...
</p>


<h3>Details</h3>

<p>This data matrix was sent by Alexandra Blant as an example of a problem with the minres solution in the <code>fa</code> function.  The default solution, using fm=&quot;minres&quot; issues a warning that the solution has improper factor score weights.  This is not the case for the fm=&quot;old.min&quot; and fm=&quot;mle&quot; options, but is for fm=&quot;pa&quot;, fm=&quot;ols&quot;.
</p>
<p>The residuals are indeed smaller for fm=&quot;minres&quot; than for fm=&quot;old.min&quot; or fm=&quot;mle&quot;.
</p>
<p>&quot;old.min&quot; attempts to find the minimum residual but uses the gradient for mle. This was the approach until version 1.7.5 but was changed (see the help page for fa) following extensive communication with Hao Wu.
</p>
<p>The problem with this matrix is probably that it is almost singular, with some smcs approaching 1 and the smallest three eigenvalues of .006, .004 and .001.
</p>
<p>This problem matrix was provided by Alexandra Blant.  
</p>


<h3>Source</h3>

<p>Alexandra Blant, personal communication</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(blant)
#compare
f5 &lt;- psych::fa(blant,5,rotate="none")  #the default minres 
f5.old &lt;- psych::fa(blant,5, fm="old.min",rotate="none") #old version of minres
f5.mle &lt;- psych::fa(blant,5,fm="mle",rotate= "none") #maximum likelihood
#compare solutions
psych::factor.congruence(list(f5,f5.old,f5.mle))
#compare sums of squared residuals
sum(residuals(f5,diag=FALSE)^2,na.rm=TRUE) # 1.355489
sum(residuals(f5.old,diag=FALSE)^2,na.rm=TRUE) # 1.539757
sum(residuals(f5.mle,diag=FALSE)^2,na.rm=TRUE) # 2.402092

#but, when we divide the squared residuals by the original (squared) correlations, we find 
#a different ordering of fit
f5$fit     # 0.9748177
f5.old$fit  # 0.9752774
f5.mle$fit   # 0.9603324

</code></pre>

<hr>
<h2 id='blot'>Bond's Logical Operations Test &ndash; BLOT
</h2><span id='topic+blot'></span>

<h3>Description</h3>

<p>35 items for 150 subjects from Bond's Logical Operations Test.  A good example of Item Response Theory analysis using the Rasch model.  One parameter (Rasch) analysis and two parameter IRT analyses produce somewhat different results. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(blot)</code></pre>


<h3>Format</h3>

<p>A data frame with 150 observations on  35 variables. The BLOT was developed as a paper and pencil test for children to measure Logical Thinking as discussed by Piaget and Inhelder. 
</p>


<h3>Details</h3>

<p>Bond and Fox apply Rasch modeling to a variety of data sets.  This one, Bond's Logical Operations Test, is used as an example of Rasch modeling for dichotomous items.  In their text (p 56), Bond and Fox report the results using WINSTEPS.  Those results are consistent (up to a scaling parameter) with those found by the rasch function in the ltm package. The WINSTEPS seem to produce difficulty estimates with a mean item difficulty of 0, whereas rasch from ltm has a mean difficulty of -1.52.  In addition, rasch seems to reverse the signs of the difficulty estimates when reporting the coefficients and is effectively reporting &quot;easiness&quot;.   
</p>
<p>However, when using a two parameter model, one of the items (V12) behaves very differently.
</p>
<p>This data set is useful when comparing 1PL, 2PL and 2PN IRT models.  
</p>


<h3>Source</h3>

<p>The data are taken (with kind permission from Trevor Bond) from the webpage https://www.winsteps.com/BF3/bondfox3.htm and read using read.fwf.  
</p>


<h3>References</h3>

<p>T.G. Bond. BLOT:Bond's Logical Operations Test. Townsville, Australia: James Cook Univer- sity. (Original work
published 1976), 1995.
</p>
<p>T. Bond and C. Fox. (2007)  Applying the Rasch model: Fundamental measurement in the human sciences. Lawrence Erlbaum, Mahwah, NJ, US, 2 edition.
</p>


<h3>See Also</h3>

<p> See also the <code>irt.fa</code> and associated plot functions. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(blot)
 
 #ltm is not required by psychTools, but if available, may be run to show a Rasch model

#do the same thing with functions in psych
blot.fa &lt;- psych::irt.fa(blot)  # a 2PN model
plot(blot.fa)
  
 
</code></pre>

<hr>
<h2 id='burt'>11 emotional variables from Burt (1915)</h2><span id='topic+burt'></span>

<h3>Description</h3>

<p>Cyril Burt reported an early factor analysis with a circumplex structure of 11 emotional variables in 1915.  8 of these were subsequently used by Harman in his text on factor analysis.  Unfortunately, it seems as if Burt made a mistake for the matrix is not positive definite.  With one change from .87 to .81 the matrix is positive definite.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(burt)</code></pre>


<h3>Format</h3>

<p>A correlation matrix based upon 172 &quot;normal school age children aged 9-12&quot;. 
</p>

<dl>
<dt>Sociality</dt><dd><p>Sociality</p>
</dd>
<dt>Sorrow</dt><dd><p>Sorrow</p>
</dd>
<dt>Tenderness</dt><dd><p>Tenderness</p>
</dd>
<dt>Joy</dt><dd><p>Joy</p>
</dd>
<dt>Wonder</dt><dd><p>Wonder</p>
</dd>
<dt>Elation</dt><dd><p>Elation</p>
</dd>
<dt>Disgust</dt><dd><p>Disgust</p>
</dd>
<dt>Anger</dt><dd><p>Anger</p>
</dd>
<dt>Sex</dt><dd><p>Sex</p>
</dd>
<dt>Fear</dt><dd><p>Fear</p>
</dd>
<dt>Subjection</dt><dd><p>Subjection</p>
</dd>
</dl>



<h3>Details</h3>

<p>The Burt data set is interesting for several reasons.  It seems to be an early example of the organizaton of emotions into an affective circumplex, a subset of it has been used for factor analysis examples (see <code>Harman.Burt</code>, and it is an example of how typos affect data.  The original data matrix has one negative eigenvalue. With the replacement of the correlation between Sorrow and Tenderness from .87 to .81, the matrix is positive definite.
</p>
<p>Alternatively, using <code>cor.smooth</code>, the matrix can be made positive definite as well, although cor.smooth makes more (but smaller) changes.
</p>


<h3>Source</h3>

<p>(retrieved from the web at https://www.biodiversitylibrary.org/item/95822#790)
Following a suggestion by Jan DeLeeuw.
</p>


<h3>References</h3>

<p>Burt, C.General and Specific Factors underlying the Primary Emotions. Reports of the British Association for the Advancement of Science, 85th meeting, 
held in Manchester,  September 7-11, 1915. 
London, John Murray, 1916, p. 694-696 
(retrieved from the web at https://www.biodiversitylibrary.org/item/95822#790) 
</p>


<h3>See Also</h3>

 <p><code>Harman.Burt</code> in the <code>Harman</code> dataset and <code>cor.smooth</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(burt)
eigen(burt)$values  #one is negative!
burt.new &lt;- burt
burt.new[2,3] &lt;- burt.new[3,2] &lt;- .81
eigen(burt.new)$values  #all are positive
bs &lt;- psych::cor.smooth(burt)
round(burt.new - bs,3)

</code></pre>

<hr>
<h2 id='cities'>Distances between 11 US cities</h2><span id='topic+cities'></span><span id='topic+city.location'></span>

<h3>Description</h3>

<p>Airline distances between 11 US cities may be used as an example for multidimensional scaling or cluster analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cities)</code></pre>


<h3>Format</h3>

<p>A data frame with 11 observations on the following 11 variables.
</p>

<dl>
<dt><code>ATL</code></dt><dd><p>Atlana, Georgia</p>
</dd>
<dt><code>BOS</code></dt><dd><p>Boston, Massachusetts</p>
</dd>
<dt><code>ORD</code></dt><dd><p>Chicago, Illinois</p>
</dd>
<dt><code>DCA</code></dt><dd><p>Washington, District of Columbia</p>
</dd>
<dt><code>DEN</code></dt><dd><p>Denver, Colorado</p>
</dd>
<dt><code>LAX</code></dt><dd><p>Los Angeles, California</p>
</dd>
<dt><code>MIA</code></dt><dd><p>Miami, Florida</p>
</dd>
<dt><code>JFK</code></dt><dd><p>New York, New York</p>
</dd>
<dt><code>SEA</code></dt><dd><p>Seattle, Washington</p>
</dd>
<dt><code>SFO</code></dt><dd><p>San Francisco, California</p>
</dd>
<dt><code>MSY</code></dt><dd><p>New Orleans, Lousianna</p>
</dd>
</dl>



<h3>Details</h3>

<p>An 11 x11  matrix of distances between major US airports. This is a useful demonstration of multiple dimensional scaling. 
</p>
<p>city.location is a dataframe of longitude and latitude for those cities.
</p>
<p>Note that the 2 dimensional MDS solution does not perfectly capture the data from these city distances.   Boston, New York and Washington, D.C. are located slightly too far west, and Seattle and LA are slightly too far south.
</p>


<h3>Source</h3>

 <p><a href="https://www.timeanddate.com/worldclock/distance.html">https://www.timeanddate.com/worldclock/distance.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(cities)
city.location[,1] &lt;- -city.location[,1] #included in the cities data set
plot(city.location, xlab="Dimension 1", ylab="Dimension 2",
   main ="Multidimensional scaling of US cities")
#do the mds   
city.loc &lt;- cmdscale(cities, k=2) #ask for a 2 dimensional solution  round(city.loc,0) 
city.loc &lt;- -city.loc  #flip the axes
 city.loc &lt;- psych::rescale(city.loc,apply(city.location,2,mean),apply(city.location,2,sd))
points(city.loc,type="n") #add the date point to the map
text(city.loc,labels=names(cities))

## Not run:    #we need the maps package to be available
#an overlay map can be added if the package maps is available
if(require(maps)) {
  map("usa",add=TRUE)
}

## End(Not run)
</code></pre>

<hr>
<h2 id='colom'>Correlations of 14 ability tests from the Spanish version of the WAIS (taken from Colom et al. 2002.)
</h2><span id='topic+colom'></span><span id='topic+colom.ed0'></span><span id='topic+colom.ed1'></span><span id='topic+colom.ed2'></span><span id='topic+colom.ed3'></span>

<h3>Description</h3>

<p>Colom et al. analyze 14 tests from the Spanish version of the WAIS. This is a nice example of a hierarchical structure using the <a href="psych.html#topic+omega">omega</a> function.  Here are the correlation matrices of the variables (colom), for 4 levels of education.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("colom")
 data("colom.ed0")
 data("colom.ed1")
 data("colom.ed2")
 data("colom.ed3")
 </code></pre>


<h3>Format</h3>

<p>The format is:
num [1:14, 1:14] 1 0.755 0.608 0.555 0.715 0.729 0.627 0.616 0.606 0.598 ...
- attr(*, &quot;dimnames&quot;)=List of 2
..$ : chr [1:14] &quot;Vocabulary&quot; &quot;Similarities&quot; &quot;Arithmetic&quot; &quot;Digit_span&quot; ...
..$ : chr [1:14] &quot;Vocabulary&quot; &quot;Similarities&quot; &quot;Arithmetic&quot; &quot;Digit_span&quot; ...
</p>


<h3>Details</h3>

<p>The Wechsler Adult Intelligence Scale (WAIS)  is the &quot;gold standard&quot; measure of intelligence.  Here is an example of the correlational structure of 14 tests.  It was used by Colom and his colleagues to find correlations of WAIS scores as a function of education.  Here we show the complete standardization sample.  
</p>
<p>The <a href="#topic+colom">colom</a> data set is the complete correlation matrix for all subjects (703 females, 666 males).  The four subset data sets for four levels of education. Ns = 301, 	432,  525, and  111.    
</p>


<h3>Source</h3>

<p>Colom et al, 2002
</p>


<h3>References</h3>

<p>Roberto Colom and Francisco J Abad and Luis F Garc  and Manuel Juan-Espinosa, 2002, Education, Wechsler's Full Scale IQ, and g.  Intelligence, 30, 449-462,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(colom)
psych::lowerMat(colom)
psych::omega(colom, 4)    #do the omega analysis  
</code></pre>

<hr>
<h2 id='cubits'>Galton's example of the relationship between height and 'cubit' or forearm length</h2><span id='topic+cubits'></span>

<h3>Description</h3>

<p>Francis Galton introduced the 'co-relation' in 1888 with a paper discussing how to measure the relationship between two variables.  His primary example was the relationship between height and forearm length.  The data table (cubits) is taken from Galton (1888).  Unfortunately, there seem to be some errors in the original data table in that the marginal totals do not match the table.
</p>
<p>The data frame, <code><a href="#topic+heights">heights</a></code>, is converted from this table. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cubits)</code></pre>


<h3>Format</h3>

<p>A data frame with 9 observations on the following 8 variables.
</p>

<dl>
<dt><code>16.5</code></dt><dd><p>Cubit length &lt; 16.5</p>
</dd>
<dt><code>16.75</code></dt><dd><p>16.5 &lt;= Cubit length &lt; 17.0</p>
</dd>
<dt><code>17.25</code></dt><dd><p>17.0 &lt;= Cubit length &lt; 17.5</p>
</dd>
<dt><code>17.75</code></dt><dd><p>17.5 &lt;= Cubit length &lt; 18.0</p>
</dd>
<dt><code>18.25</code></dt><dd><p>18.0 &lt;= Cubit length &lt; 18.5</p>
</dd>
<dt><code>18.75</code></dt><dd><p>18.5 &lt;= Cubit length &lt; 19.0</p>
</dd>
<dt><code>19.25</code></dt><dd><p>19.0 &lt;= Cubit length &lt; 19.5</p>
</dd>
<dt><code>19.75</code></dt><dd><p>19.5 &lt;= Cubit length  </p>
</dd>
</dl>



<h3>Details</h3>

<p>Sir Francis Galton (1888) published the first demonstration of the correlation coefficient.  The regression (or reversion to mediocrity) of the height to the length of the left forearm (a cubit) was found to .8.  There seem to be some errors in the table as published in that the row sums do not agree with the actual row sums. These data are used to create a matrix using <code>table2matrix</code> for demonstrations of analysis and displays of the data.
</p>


<h3>Source</h3>

<p>Galton (1888)
</p>


<h3>References</h3>

<p>Galton, Francis (1888) Co-relations and their measurement. Proceedings of the Royal Society. London Series,45,135-145,
</p>


<h3>See Also</h3>

  <p><code><a href="psych.html#topic+table2matrix">table2matrix</a></code>,  <code><a href="psych.html#topic+table2df">table2df</a></code>, <code><a href="psych.html#topic+ellipses">ellipses</a></code>, <code><a href="#topic+heights">heights</a></code>, <code><a href="#topic+peas">peas</a></code>,<code><a href="#topic+galton">galton</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cubits)
cubits
heights &lt;- psych::table2df(cubits,labs = c("height","cubit"))
psych::ellipses(heights,n=1,main="Galton's co-relation data set")
psych::ellipses(jitter(heights$height,3),jitter(heights$cubit,3),pch=".",
     main="Galton's co-relation data set",xlab="height",
     ylab="Forearm (cubit)") #add in some noise to see the points
psych::pairs.panels(heights,jiggle=TRUE,main="Galton's cubits data set")
</code></pre>

<hr>
<h2 id='cushny'>
A data set from Cushny and Peebles (1905) on the effect of three drugs on hours of sleep, used by Student (1908)
</h2><span id='topic+cushny'></span>

<h3>Description</h3>

<p>The classic data set used by Gossett (publishing as Student) for the introduction of the t-test.  The design was a within subjects study with hours of sleep in a control condition compared to those in 3 drug conditions.  Drug1 was 06mg of L Hscyamine, Drug 2L and Drug2R were said to be .6 mg of  Left and Right isomers of Hyoscine. As discussed by Zabell (2008) these were not optical isomers. The detal1, delta2L and delta2R are changes from the baseline control.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cushny)</code></pre>


<h3>Format</h3>

<p>A data frame with 10 observations on the following 7 variables.
</p>

<dl>
<dt><code>Control</code></dt><dd><p>Hours of sleep in a control condition</p>
</dd>
<dt><code>drug1</code></dt><dd><p>Hours of sleep in Drug condition 1</p>
</dd>
<dt><code>drug2L</code></dt><dd><p>Hours of sleep in Drug condition 2</p>
</dd>
<dt><code>drug2R</code></dt><dd><p>Hours of sleep in Drug condition 3 (an isomer of the drug in condition 2</p>
</dd>
<dt><code>delta1</code></dt><dd><p>Change from control, drug 1</p>
</dd>
<dt><code>delta2L</code></dt><dd><p>Change from control, drug 2L</p>
</dd>
<dt><code>delta2R</code></dt><dd><p>Change from control, drug 2R</p>
</dd>
</dl>



<h3>Details</h3>

<p>The original analysis by Student is used as an example for the t-test function, both as a paired t-test and a two group t-test.  The data are also useful for a repeated measures analysis of variance.  
</p>


<h3>Source</h3>

<p>Cushny, A.R. and Peebles, A.R. (1905) The action of optical isomers: II hyoscines.  The Journal of Physiology 32, 501-510.
</p>
<p>Student (1908) The probable error of the mean.  Biometrika, 6 (1) , 1-25.
</p>


<h3>References</h3>

<p>See also the data set sleep and the examples for the t.test
</p>
<p>S. L. Zabell.  On Student's 1908 Article &quot;The Probable Error of a Mean&quot; Journal of the American Statistical Association, Vol. 103, No. 481 (Mar., 2008), pp. 1- 20</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cushny)
with(cushny, t.test(drug1,drug2L,paired=TRUE)) #within subjects 

psych::error.bars(cushny[1:4],within=TRUE,ylab="Hours of sleep",xlab="Drug condition", 
       main="95% confidence of within subject effects")
</code></pre>

<hr>
<h2 id='df2latex'>Convert a data frame, correlation matrix, or factor analysis output to a LaTeX or rtf table</h2><span id='topic+df2latex'></span><span id='topic+cor2latex'></span><span id='topic+fa2latex'></span><span id='topic+omega2latex'></span><span id='topic+irt2latex'></span><span id='topic+ICC2latex'></span><span id='topic+df2rtf'></span><span id='topic+cor2rtf'></span><span id='topic+fa2rtf'></span>

<h3>Description</h3>

<p>A set of handy helper functions to convert data frames or matrices to LaTeX or rtf tables. Although Sweave is the preferred means of converting R output to LaTeX, it is sometimes useful to go directly from a data.frame or matrix to a LaTeX table.   cor2latex will find the correlations and then create a lower (or upper) triangular matrix for latex output. cor2rtf will do the same for rtf output. fa2latex and fa2rtf will create the latex commands for showing the loadings and factor intercorrelations. As the default option, tables are prepared in an approximation of APA format. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df2latex(x,digits=2,rowlabels=TRUE,apa=TRUE,short.names=TRUE,font.size ="scriptsize",
       big.mark=NULL,drop.na=TRUE, heading="A table from the psych package in R",
   caption="df2latex",label="default", char=FALSE, 
    stars=FALSE,silent=FALSE,file=NULL,append=FALSE,cut=0,big=0,abbrev=NULL,long=FALSE)
cor2latex(x,use = "pairwise", method="pearson", adjust="holm",stars=FALSE,
       digits=2,rowlabels=TRUE,lower=TRUE,apa=TRUE,short.names=TRUE,
     font.size ="scriptsize", heading="A correlation table from the psych package in R.",
      caption="cor2latex",label="default",silent=FALSE,file=NULL,append=FALSE,cut=0,big=0)
fa2latex(f,digits=2,rowlabels=TRUE,apa=TRUE,short.names=FALSE,cumvar=FALSE,
       cut=0,big=.3,alpha=.05,font.size ="scriptsize",long=FALSE,
       heading="A factor analysis table from the psych package in R",
       caption="fa2latex",label="default",silent=FALSE,file=NULL,append=FALSE) 
omega2latex(f,digits=2,rowlabels=TRUE,apa=TRUE,short.names=FALSE,cumvar=FALSE,cut=.2,
        big=.3,font.size ="scriptsize", 
        heading="An omega analysis table from the psych package in R",
        caption="omega2latex",label="default",silent=FALSE,file=NULL,append=FALSE)

irt2latex(f,digits=2,rowlabels=TRUE,apa=TRUE,short.names=FALSE,
       font.size ="scriptsize", heading="An IRT factor analysis table from R",
       caption="fa2latex",label="default",silent=FALSE,file=NULL,append=FALSE)
ICC2latex(icc,digits=2,rowlabels=TRUE,apa=TRUE,ci=TRUE,
   font.size ="scriptsize",big.mark=NULL, drop.na=TRUE,
    heading="A table from the psych package in R",
   caption="ICC2latex",label="default",char=FALSE,silent=FALSE,file=NULL,append=FALSE) 

#not all options are yet implemented in these next three functions.   
df2rtf(x,file=NULL, digits=2,rowlabels=TRUE,width=8.5,old=NULL, apa=TRUE,short.names=TRUE,
	font.size =10,big.mark=NULL, drop.na=TRUE, 
	heading="A table from the psych package in R",
	caption="Created with df2rtf",label="default",char=FALSE,stars=FALSE,silent=FALSE,
	append=FALSE,cut=0,big=.0,abbrev=NULL,long=FALSE) 

cor2rtf(x,file=NULL, use = "pairwise", method="pearson", adjust="holm", digits=2,
	rowlabels=TRUE,width=8.5,lower=TRUE,old=NULL, apa=TRUE,short.names=TRUE,
	font.size =10,big.mark=NULL, drop.na=TRUE, 
	heading="A correlation matrix from the psych package in R",
	caption="Created with cor2rtf.   left justify output if stars", 
	label="default",char=FALSE,stars=FALSE,silent=FALSE,
	append=FALSE,cut=0,big=.0,abbrev=NULL,long=FALSE) 
	
fa2rtf(f,file=NULL, use = "pairwise", method="pearson", adjust="holm", digits=2,
	rowlabels=TRUE,width=8.5,lower=TRUE,old=NULL, apa=TRUE,short.names=TRUE,
	font.size =10,big.mark=NULL, drop.na=TRUE, 
	heading="A Factor analysis   from the psych package in R",
	caption="Created with fa2rtf. ",label="default",char=FALSE,silent=FALSE,
	append=FALSE,cut=0,big=.0,abbrev=NULL)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="df2latex_+3A_x">x</code></td>
<td>
<p>A data frame or matrix to convert to LaTeX. If non-square, then correlations will be found prior to printing in cor2latex</p>
</td></tr>
<tr><td><code id="df2latex_+3A_digits">digits</code></td>
<td>
<p>Round the output to digits of accuracy.  NULL for formatting character data</p>
</td></tr>
<tr><td><code id="df2latex_+3A_abbrev">abbrev</code></td>
<td>
<p>How many characters should be used in column names &ndash;defaults to digits + 3</p>
</td></tr>
<tr><td><code id="df2latex_+3A_rowlabels">rowlabels</code></td>
<td>
<p>If TRUE, use the row names from the matrix or data.frame</p>
</td></tr>
<tr><td><code id="df2latex_+3A_short.names">short.names</code></td>
<td>
<p>Name the columns with abbreviated rownames to save space</p>
</td></tr>
<tr><td><code id="df2latex_+3A_apa">apa</code></td>
<td>
<p>If TRUE formats table in APA style</p>
</td></tr>
<tr><td><code id="df2latex_+3A_cumvar">cumvar</code></td>
<td>
<p>For factor analyses, should we show the cumulative variance accounted for?</p>
</td></tr>
<tr><td><code id="df2latex_+3A_font.size">font.size</code></td>
<td>
<p>e.g., &quot;scriptsize&quot;, &quot;tiny&quot; or anyother acceptable LaTeX font size.</p>
</td></tr>
<tr><td><code id="df2latex_+3A_heading">heading</code></td>
<td>
<p>The label appearing at the top of the table</p>
</td></tr>
<tr><td><code id="df2latex_+3A_caption">caption</code></td>
<td>
<p>The table caption</p>
</td></tr>
<tr><td><code id="df2latex_+3A_lower">lower</code></td>
<td>
<p>in cor2latex, just show the lower triangular matrix</p>
</td></tr>
<tr><td><code id="df2latex_+3A_f">f</code></td>
<td>
<p>The object returned from a factor analysis using <code>fa</code> or <code>irt.fa</code>. </p>
</td></tr>
<tr><td><code id="df2latex_+3A_label">label</code></td>
<td>
<p>The label for the table</p>
</td></tr>
<tr><td><code id="df2latex_+3A_big.mark">big.mark</code></td>
<td>
<p>Comma separate numbers large numbers (big.mark=&quot;,&quot;)</p>
</td></tr>
<tr><td><code id="df2latex_+3A_drop.na">drop.na</code></td>
<td>
<p>Do not print NA values</p>
</td></tr>
<tr><td><code id="df2latex_+3A_method">method</code></td>
<td>
<p>When finding correlations, which method should be used (pearson)</p>
</td></tr>
<tr><td><code id="df2latex_+3A_use">use</code></td>
<td>
<p>use=&quot;pairwise&quot; is the default when finding correlations in cor2latex</p>
</td></tr>
<tr><td><code id="df2latex_+3A_adjust">adjust</code></td>
<td>
<p>If showing probabilities, which adjustment should be used (holm)</p>
</td></tr>
<tr><td><code id="df2latex_+3A_stars">stars</code></td>
<td>
<p>Should probability 'magic astericks' be displayed in cor2latex (FALSE)</p>
</td></tr>
<tr><td><code id="df2latex_+3A_char">char</code></td>
<td>
<p>char=TRUE allows printing tables with character information, but does not allow for putting in commas into numbers</p>
</td></tr>
<tr><td><code id="df2latex_+3A_cut">cut</code></td>
<td>
<p>In omega2latex, df2latex and fa2latex, do not print abs(values) &lt; cut </p>
</td></tr>
<tr><td><code id="df2latex_+3A_big">big</code></td>
<td>
<p>In fa2latex and df2latex boldface those abs(values) &gt; big</p>
</td></tr>
<tr><td><code id="df2latex_+3A_alpha">alpha</code></td>
<td>
<p>If fa has returned confidence intervals, then what values of loadings should be boldfaced?</p>
</td></tr>
<tr><td><code id="df2latex_+3A_icc">icc</code></td>
<td>
<p>Either the output of an ICC, or the data to be analyzed.</p>
</td></tr>
<tr><td><code id="df2latex_+3A_ci">ci</code></td>
<td>
<p>Should confidence intervals of the ICC be displayed</p>
</td></tr>
<tr><td><code id="df2latex_+3A_silent">silent</code></td>
<td>
<p>If TRUE, do not print any output, just return silently &ndash; useful if using Sweave</p>
</td></tr>
<tr><td><code id="df2latex_+3A_file">file</code></td>
<td>
<p>If specified, write the output to this file</p>
</td></tr>
<tr><td><code id="df2latex_+3A_append">append</code></td>
<td>
<p>If file is specified, then should we append (append=TRUE) or just write to the file</p>
</td></tr>
<tr><td><code id="df2latex_+3A_long">long</code></td>
<td>
<p>if TRUE, then do long tables.  (requires the longtables package in latex)</p>
</td></tr>
<tr><td><code id="df2latex_+3A_old">old</code></td>
<td>
<p>When appending output with df2rtf, old is the output from the prior run.</p>
</td></tr>
<tr><td><code id="df2latex_+3A_width">width</code></td>
<td>
<p>page width in inches for df2rtf </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A LaTeX table.  Note that if showing &quot;stars&quot; for correlations, then one needs to use the siunitx  package in LaTex. 
The entire LaTeX output is also returned invisibly.  If using Sweave to create tables, then the silent option should be set to TRUE and the returned object saved as a file.  See the last example.
</p>
<p>Finally, some users have asked for the ability to convert these output tables into HTML.  This may be done using the tth package. 
</p>
<p>Three functions to write to rtf files (for use in various proprietary word processing languages) have been added with version 2.4.3.  These will write to an rtf file and may be formatted directly.  df2rtf takes a data frame and writes it as a table with header information.
</p>
<p>cor2rtf will take either a data matrix (and find the correlations) or just a correlation matrix.  &quot;magic astericks &quot; can be added to the correlations using the stars=TRUE option.  In this case, the result table can be left justified in a word processing language to get the numbers to appear correctly justified.
</p>
<p>fa2latex and fa2rtf can take the output from  either a factor analysis or from fa.lookup.
</p>


<h3>Author(s)</h3>

<p>William Revelle with suggestions from Jason French and David Condon and Davide Morselli</p>


<h3>See Also</h3>

<p>The many LaTeX conversion routines in Hmisc.
</p>
<p>To convert these LaTex objects to HTML, you should install the tth package.
</p>
<p>Consider the last example for creating HTML
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df2latex(psych::Thurstone,rowlabels=FALSE,apa=FALSE,short.names=FALSE,
        caption="Thurstone Correlation matrix")
df2latex(psych::Thurstone,heading="Thurstone Correlation matrix in APA style")

df2latex(psych::describe(psych::sat.act)[2:10],short.names=FALSE)
cor2latex(psych::Thurstone)
cor2latex(psych::sat.act,short.names=FALSE)
fa2latex(psych::fa(psych::Thurstone,3),heading="Factor analysis from R in quasi APA style")


#to write to rtf file
#replace the temporary file name with something more useful
fn &lt;- tempfile(pattern="example",fileext=".rtf")  #create a temporary file
#better is to create a local file
# e.g. fn &lt;- "rtf_example.rtf"

cor2rtf(sat.act, file=fn)   #write to the file

dd &lt;- psych::describe(sat.act)
temp &lt;- df2rtf(dd, file=fn, append=TRUE, width=12) #write and keep open
temp1 &lt;-  cor2rtf(sat.act,old=temp,caption=date(), append=TRUE)  #use date as caption 
cor2rtf(sat.act, old=temp1, stars=TRUE) #close the file
#now open this with your word processor and reformat with left justify

#now write a factor analysis output to an output file
# e.g. fn &lt;- "rtf_example.rtf"
f5 &lt;- psych::fa(bfi,5)
temp &lt;- fa2rtf(f5, width=12, file=fn, append=TRUE)  #a normal fa output
fl &lt;- psych::fa.lookup(f5, dictionary=bfi.dictionary)
fa2rtf(fl, old = temp)
##now open this with your word processor

#To convert these latex tables to HTML

#f3.lat &lt;- fa2latex(psych::fa(psych::Thurstone,3),
#    heading="Factor analysis from R in quasi APA style")
#library(tth)
#f3.ht &lt;- tth(f3.lat)
#print(as.data.frame(f3.ht),row.names=FALSE)

###

 #If using Sweave to create a LateX table as a separate file then set silent=TRUE
#e.g., 
#LaTex preamble 
#....
#&lt;&lt;print=FALSE,echo=FALSE&gt;&gt;= 
#f3 &lt;- fa(Thurstone,3)
#fa2latex(f3,silent=TRUE,file='testoutput.tex')
#@
#
#\input{testoutput.tex}

 </code></pre>

<hr>
<h2 id='dfOrder'>Sort (order) a dataframe or matrix by multiple columns
</h2><span id='topic+dfOrder'></span>

<h3>Description</h3>

<p>Although <code><a href="base.html#topic+order">order</a></code> will order a vector, and it is possible to order several columns of a data.frame by specifying each column individually in the call to order, <code><a href="#topic+dfOrder">dfOrder</a></code> will order a dataframe or matrix by as many columns as desired.  The default is to sort by columns in lexicographic order. If the object  is a correlation matrix, then the selected columns are sorted by the (abs) max value across the columns (similar to fa.lookup in psych).   If object is a correlation matrix,  rows and columns are sorted.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfOrder(object, columns,absolute=FALSE,ascending=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfOrder_+3A_object">object</code></td>
<td>
<p>The data.frame or matrix to be sorted</p>
</td></tr>
<tr><td><code id="dfOrder_+3A_columns">columns</code></td>
<td>
<p>Column numbers or names to use for sorting.  If positive, then they will be sorted in increasing order. If negative, then in decreasing order</p>
</td></tr>
<tr><td><code id="dfOrder_+3A_absolute">absolute</code></td>
<td>
<p>If TRUE, then sort the absolute values</p>
</td></tr>
<tr><td><code id="dfOrder_+3A_ascending">ascending</code></td>
<td>
<p>By default, order from smallest to largest.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is just a simple helper function to reorder data.frames and correlation matrices.  Originally developed to organize IRT  output from the ltm package. It is a basic add on to the order function.  
</p>
<p>(Completely rewritten for version 1.8.1. and then again for 2.2.1 to allow sorting correlation matrices by numeric values.) 
</p>


<h3>Value</h3>

<p>The original data frame is now in sorted order. If the input is a correlation matrix, the output is sorted by rows and columns.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

<p> Other useful file manipulation functions include <code><a href="#topic+read.file">read.file</a></code> to read in data from a file or <code><a href="#topic+read.clipboard">read.clipboard</a></code> from the clipboard,  <code><a href="#topic+fileScan">fileScan</a></code>, <code><a href="#topic+filesList">filesList</a></code>, <code><a href="#topic+filesInfo">filesInfo</a></code>,  and <code><a href="#topic+fileCreate">fileCreate</a></code>
</p>
<p><code><a href="#topic+dfOrder">dfOrder</a></code> code is used in the  <code>test.irt</code> function to combine ltm and <code>sim.irt</code> output.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#create a data frame and then sort it in lexicographic order
set.seed(42)
x &lt;- matrix(sample(1:4,64,replace=TRUE),ncol=4)
dfOrder(x)  # sort by all columns
dfOrder(x,c(1,4))  #sort by the first and 4th column
x.df &lt;- data.frame(x)
dfOrder(x.df,c(1,-2))  #sort by the first in increasing order, 
   #the second in decreasing order

#now show sorting correlation matrices  
r &lt;- cor(sat.act,use="pairwise")
r.ord &lt;- dfOrder(r,columns=c("education","ACT"),ascending=FALSE)
psych::corPlot(r.ord)
</code></pre>

<hr>
<h2 id='eminence'>Eminence of 69 American Psychologists</h2><span id='topic+eminence'></span>

<h3>Description</h3>

<p>Marco Del Giudice criticized an earlier study by Simonton for using partial regression weights to estimate the importance of various predictors of rated eminence. This is a nice example of the (mis)interpretation of beta weights of highly correlated predictors.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("eminence")</code></pre>


<h3>Format</h3>

<p>A data frame with 69 observations on the following 9 variables.
</p>

<dl>
<dt><code>name</code></dt><dd><p>a character vector</p>
</dd>
<dt><code>reputation</code></dt><dd><p>Log of rated reputation</p>
</dd>
<dt><code>birth.year</code></dt><dd><p>Year of birth</p>
</dd>
<dt><code>first.year</code></dt><dd><p>Year of first cited publicatin</p>
</dd>
<dt><code>last.year</code></dt><dd><p>Year of last cited publication</p>
</dd>
<dt><code>works</code></dt><dd><p>Log of number of publications</p>
</dd>
<dt><code>citations</code></dt><dd><p>Log of number of citations</p>
</dd>
<dt><code>composite</code></dt><dd><p>A composite index of publications</p>
</dd>
<dt><code>h</code></dt><dd><p>The 'h' index of citations</p>
</dd>
</dl>



<h3>Details</h3>

<p>Simonton (1997, 2014) discusses various estimates of eminence among 69  psychologists born between 1842 and 1912 and reports that the regression weights are small and interprets this as meaning  number of publications and citations are not very important.  Del Giudice (2020) points out that  citations and the number of publications are highly collinear and thus while their independent contributions are small, their joint effect is quite large (R= .69 ).  These data are given here as an example of multiple correlation and partial correlation
</p>


<h3>Source</h3>

<p>Del Giudice (2020) links to a web page with the data. 
</p>


<h3>References</h3>

<p>Marco Del Giudice (2020).   How Well Do Bibliometric Indicators Correlate With Scientific Eminence? A Comment on Simonton (2016).  Perspective in Psychological Science, 15, 202-203.
</p>
<p>Simonton, D. K. (1992). Leaders of American psychology, 1879-1967: Career development, creative output, and professional achievement. Journal of Personality and Social Psychology, 62, 5-17.
</p>
<p>Simonton, D. K. (2016). Giving credit where credit is due: Why it's so hard to do in psychological science. Perspectives on Psychological Science, 11, 888-892.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(eminence)
psych::lowerCor(eminence)
cs &lt;- psych::cs
psych::partial.r(eminence, x= cs(reputation, works, citations),y=cs(birth.year))
psych::setCor(reputation ~ works + h +  first.year,data=eminence)
</code></pre>

<hr>
<h2 id='epi'>Eysenck Personality Inventory (EPI) data for 3570 participants</h2><span id='topic+epi'></span><span id='topic+epi.dictionary'></span><span id='topic+epiR'></span><span id='topic+epi.keys'></span>

<h3>Description</h3>

<p>The EPI is and has been a very frequently administered personality test with 57 measuring two broad dimensions, Extraversion-Introversion and Stability-Neuroticism, with an additional Lie scale. Developed by Eysenck and Eysenck, 1964. Eventually replaced with the EPQ which measures three broad dimensions. This data set represents 3570 observations collected in the early 1990s at the Personality, Motivation and Cognition lab at Northwestern. An additional data set (epiR) has test and retest information for 474 participants. The data are included here as demonstration of scale construction and test-retest reliability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(epi)
data(epi.dictionary)
data(epiR)</code></pre>


<h3>Format</h3>

<p>A data frame with 3570 observations on the following 57 variables. 
</p>

<dl>
<dt><code>id</code></dt><dd><p>The identification number within the study</p>
</dd>
<dt><code>time</code></dt><dd><p>First (group testing) or 2nd time (before a lab experiment)
for the epiR data set.</p>
</dd>
<dt><code>study</code></dt><dd><p>Four lab based studies and their pretest data</p>
</dd>
<dt><code>V1</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V2</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V3</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V4</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V5</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V6</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V7</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V8</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V9</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V10</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V11</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V12</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V13</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V14</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V15</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V16</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V17</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V18</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V19</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V20</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V21</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V22</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V23</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V24</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V25</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V26</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V27</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V28</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V29</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V30</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V31</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V32</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V33</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V34</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V35</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V36</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V37</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V38</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V39</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V40</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V41</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V42</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V43</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V44</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V45</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V46</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V47</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V48</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V49</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V50</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V51</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V52</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V53</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V54</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V55</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V56</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>V57</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p>The original data were collected in a group testing framework for screening participants for subsequent studies. The participants were enrolled in an introductory psychology class between Fall, 1991 and Spring, 1995. 
</p>
<p>The actual items may be found in the <code><a href="#topic+epi.dictionary">epi.dictionary</a></code>.   
</p>
<p>The structure of the E scale has been shown by Rocklin and Revelle (1981) to have two subcomponents, Impulsivity and Sociability. These were subsequently used by Revelle, Humphreys, Simon and Gilliland (1980) to examine the relationship between personality, caffeine induced arousal, and cognitive performance. 
</p>
<p>The epiR data include the original group testing data and matched data for 474 participants collected several weeks later.  This is useful for showing that internal consistency estimates (e.g. <code><a href="ggplot2.html#topic+alpha">alpha</a></code> or <code>omega</code>) can be low even though the test is stable across time.  For more demonstrations of the distinction between immediate internal consistency and delayed test-retest reliability see the <code><a href="#topic+msqR">msqR</a></code> and <code><a href="#topic+sai">sai</a></code> data sets and <code>testRetest</code>.
</p>


<h3>Source</h3>

<p>Data from the PMC laboratory at Northwestern. 
</p>


<h3>References</h3>

<p>Eysenck, H.J. and Eysenck, S. B.G. (1968). Manual for the Eysenck Personality Inventory.Educational and Industrial Testing Service, San Diego, CA.
</p>
<p>Revelle, W. and Humphreys, M. S. and Simon, L. and Gilliland, K. (1980) Interactive effect of personality, time of day, and caffeine: A test of the arousal model,  Journal of Experimental Psychology General, 109, 1, 1-31,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(epi)
epi.keys &lt;- list(E = c("V1",  "V3",  "V8",  "V10", "V13", "V17", "V22", "V25", "V27", "V39",
  "V44", "V46", "V49", "V53", "V56", "-V5", "-V15", "-V20", "-V29", "-V32", "-V34","-V37",
   "-V41", "-V51"),
N = c( "V2", "V4", "V7", "V9", "V11", "V14", "V16", "V19", "V21", "V23", "V26", "V28", 
"V31", "V33", "V35", "V38", "V40","V43", "V45", "V47", "V50", "V52","V55", "V57"),
L = c("V6",  "V24", "V36", "-V12", "-V18", "-V30", "-V42", "-V48", "-V54"),
Imp = c( "V1",  "V3",  "V8",  "V10", "V13", "V22", "V39", "-V5", "-V41"),
Soc = c( "V17", "V25", "V27", "V44", "V46", "V53", "-V11", "-V15", "-V20", 
"-V29", "-V32", "-V37", "-V51")
)
scores &lt;- psych::scoreItems(epi.keys,epi)

psych::keys.lookup(epi.keys[1:3],epi.dictionary) #show the items and keying information

#a variety of demonstrations (not run) of test retest reliability versus alpha versus omega

E &lt;- psych::selectFromKeys(epi.keys$E)
#look at the testRetest help file for more examples 

</code></pre>

<hr>
<h2 id='epi.bfi'>13 personality scales from the Eysenck Personality Inventory and Big 5 inventory</h2><span id='topic+epi.bfi'></span>

<h3>Description</h3>

<p>A small data set of 5 scales from the Eysenck Personality Inventory, 5 from a Big 5 inventory, a Beck Depression Inventory, and State and Trait Anxiety measures.  Used for demonstrations of correlations, regressions, graphic displays.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(epi.bfi)</code></pre>


<h3>Format</h3>

<p>A data frame with 231 observations on the following 13 variables.
</p>

<dl>
<dt><code>epiE</code></dt><dd><p>EPI Extraversion </p>
</dd>
<dt><code>epiS</code></dt><dd><p>EPI Sociability (a subset of Extraversion items</p>
</dd>
<dt><code>epiImp</code></dt><dd><p>EPI Impulsivity (a subset of Extraversion items</p>
</dd>
<dt><code>epilie</code></dt><dd><p>EPI Lie scale</p>
</dd>
<dt><code>epiNeur</code></dt><dd><p>EPI neuroticism</p>
</dd>
<dt><code>bfagree</code></dt><dd><p>Big 5 inventory (from the IPIP) measure of Agreeableness</p>
</dd>
<dt><code>bfcon</code></dt><dd><p>Big 5 Conscientiousness</p>
</dd>
<dt><code>bfext</code></dt><dd><p>Big 5 Extraversion</p>
</dd>
<dt><code>bfneur</code></dt><dd><p>Big 5 Neuroticism</p>
</dd>
<dt><code>bfopen</code></dt><dd><p>Big 5 Openness</p>
</dd>
<dt><code>bdi</code></dt><dd><p>Beck Depression scale</p>
</dd>
<dt><code>traitanx</code></dt><dd><p>Trait Anxiety</p>
</dd>
<dt><code>stateanx</code></dt><dd><p>State Anxiety</p>
</dd>
</dl>



<h3>Details</h3>

<p>Self report personality scales tend to measure the &ldquo;Giant 2&quot; of Extraversion and Neuroticism or the &ldquo;Big 5&quot; of Extraversion, Neuroticism, Agreeableness, Conscientiousness, and Openness.  Here is a small data set from Northwestern University undergraduates with scores on the Eysenck Personality Inventory (EPI) and a Big 5 inventory taken from the International Personality Item Pool.  
</p>


<h3>Source</h3>

<p>Data were collected at the Personality, Motivation, and Cognition Lab (PMCLab) at Northwestern by William Revelle)
</p>


<h3>References</h3>

<p><a href="https://personality-project.org/pmc.html">https://personality-project.org/pmc.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(epi.bfi)
psych::pairs.panels(epi.bfi[,1:5])
psych::describe(epi.bfi)
</code></pre>

<hr>
<h2 id='galton'>Galton's Mid parent child height data</h2><span id='topic+galton'></span>

<h3>Description</h3>

<p>Two of the earliest examples of the correlation coefficient were Francis Galton's data sets on the relationship between mid parent and child height and the similarity of parent generation peas with child peas.  This is the data set for the Galton height.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(galton)</code></pre>


<h3>Format</h3>

<p>A data frame with 928 observations on the following 2 variables.
</p>

<dl>
<dt><code>parent</code></dt><dd><p>Mid Parent heights (in inches) </p>
</dd>
<dt><code>child</code></dt><dd><p>Child Height</p>
</dd>
</dl>



<h3>Details</h3>

<p>Female heights were adjusted by 1.08 to compensate for sex differences. (This was done in the original data set)
</p>


<h3>Source</h3>

<p>This is just the galton data set from UsingR, slightly rearranged.
</p>


<h3>References</h3>

<p>Stigler, S. M. (1999). Statistics on the Table: The History of Statistical 
Concepts and Methods. Harvard University Press. 
Galton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the Anthropological Institute of Great Britain and Ireland, 15:246-263. 
Galton, F. (1869). Hereditary Genius: An Inquiry into its Laws and Consequences. London: Macmillan. 
</p>
<p>Wachsmuth, A.W., Wilkinson L., Dallal G.E. (2003). Galton's bend: A 
previously undiscovered nonlinearity in Galton's family stature regression 
data. The American Statistician, 57, 190-192.
</p>


<h3>See Also</h3>

<p>The other Galton data sets:   <code><a href="#topic+heights">heights</a></code>, <code><a href="#topic+peas">peas</a></code>,<code><a href="#topic+cubits">cubits</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(galton)
psych::describe(galton)
 #show the scatter plot and the lowess fit 
psych::pairs.panels(galton,main="Galton's Parent child heights")  
#but this makes the regression lines look the same
psych::pairs.panels(galton,lm=TRUE,main="Galton's Parent child heights") 
 #better is to scale them 
psych::pairs.panels(galton,lm=TRUE,xlim=c(62,74),ylim=c(62,74),
              main="Galton's Parent child heights") 
</code></pre>

<hr>
<h2 id='GERAS'>Data from Gruber et al, 2020, Study 2: Gender Related Attributes Survey</h2><span id='topic+GERAS'></span><span id='topic+GERAS.items'></span><span id='topic+GERAS.keys'></span><span id='topic+GERAS.dictionary'></span><span id='topic+GERAS.scales'></span>

<h3>Description</h3>

<p>Gruber et al. (2020) report on the psychometric properties of a multifaceted Gender Related Attributes Survey. Here are the data from their 3 domains (Personality, Cognition and Activities and Interests from their study 2. Eagly and Revelle (2022) include these data in their review of the power of aggregation. The data  are included here as demonstrations of the <code>cohen.d</code> and <code>scatterHist</code> functions in the psych package and may be used to show the power of aggregation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("GERAS")
#These other objects are included in the file
# data("GERAS.scales")
# data("GERAS.dictionary")
# data("GERAS.items")
# data("GERAS.keys")
    </code></pre>


<h3>Format</h3>

<p>A data frame with 471 observations on the following 51 variables (selected from the original 93) The code numbers are item numbers from the bigger set.
</p>

<dl>
<dt><code>V15</code></dt><dd><p>  reckless</p>
</dd>
<dt><code>V22</code></dt><dd><p> willing to take risks</p>
</dd>
<dt><code>V11</code></dt><dd><p> courageous</p>
</dd>
<dt><code>V6</code></dt><dd><p>a  adventurous</p>
</dd>
<dt><code>V19</code></dt><dd><p>  dominant</p>
</dd>
<dt><code>V14</code></dt><dd><p>  controlling</p>
</dd>
<dt><code>V20</code></dt><dd><p>  boastful</p>
</dd>
<dt><code>V21</code></dt><dd><p>  rational</p>
</dd>
<dt><code>V23</code></dt><dd><p> analytical</p>
</dd>
<dt><code>V9</code></dt><dd><p>   pragmatic</p>
</dd>
<dt><code>V44</code></dt><dd><p>  to find an address for the first time</p>
</dd>
<dt><code>V45</code></dt><dd><p>   to find a way again</p>
</dd>
<dt><code>V46</code></dt><dd><p>  to understand equations</p>
</dd>
<dt><code>V50</code></dt><dd><p>  to follow directions</p>
</dd>
<dt><code>V51</code></dt><dd><p>  to understand equations</p>
</dd>
<dt><code>V53</code></dt><dd><p>    day-to-day calculations</p>
</dd>
<dt><code>V48</code></dt><dd><p>    to write a computer program</p>
</dd>
<dt><code>V69</code></dt><dd><p> paintball</p>
</dd>
<dt><code>V73</code></dt><dd><p>    driving go-cart</p>
</dd>
<dt><code>V71</code></dt><dd><p> drinking beer</p>
</dd>
<dt><code>V68</code></dt><dd><p> watching action movies</p>
</dd>
<dt><code>V75</code></dt><dd><p> playing cards (poker)</p>
</dd>
<dt><code>V72</code></dt><dd><p> watching sports on TV</p>
</dd>
<dt><code>V67</code></dt><dd><p>  doing certain sports (e.g. soccer, ...)</p>
</dd>
<dt><code>V74</code></dt><dd><p>    Gym (weightlifting)</p>
</dd>
<dt><code>V27</code></dt><dd><p>  warm-hearted</p>
</dd>
<dt><code>V28</code></dt><dd><p>   loving</p>
</dd>
<dt><code>V29</code></dt><dd><p>   caring</p>
</dd>
<dt><code>V26</code></dt><dd><p> compassionate</p>
</dd>
<dt><code>V32</code></dt><dd><p>  delicate</p>
</dd>
<dt><code>V30</code></dt><dd><p>   tender</p>
</dd>
<dt><code>V24</code></dt><dd><p>  familiy-oriented</p>
</dd>
<dt><code>V40</code></dt><dd><p>  anxious</p>
</dd>
<dt><code>V39</code></dt><dd><p>  thin-skinned</p>
</dd>
<dt><code>V41</code></dt><dd><p>  careful</p>
</dd>
<dt><code>V55</code></dt><dd><p>   to explain foreign words</p>
</dd>
<dt><code>V58</code></dt><dd><p>to find the right words to express certain content</p>
</dd>
<dt><code>V59</code></dt><dd><p> synonyms for a word in order to avoid repetitions</p>
</dd>
<dt><code>V60</code></dt><dd><p>  to phrase a text</p>
</dd>
<dt><code>V54</code></dt><dd><p>   remembering events from your own life</p>
</dd>
<dt><code>V63</code></dt><dd><p>    to notice small changes</p>
</dd>
<dt><code>V57</code></dt><dd><p>    to remember names and faces</p>
</dd>
<dt><code>V89</code></dt><dd><p>  shopping</p>
</dd>
<dt><code>V92</code></dt><dd><p> gossiping</p>
</dd>
<dt><code>V81</code></dt><dd><p>   watching a romantic movie</p>
</dd>
<dt><code>V80</code></dt><dd><p>  talking on the phone with a friend</p>
</dd>
<dt><code>V90</code></dt><dd><p>  yoga</p>
</dd>
<dt><code>V83</code></dt><dd><p>    rhythmic gymnastics</p>
</dd>
<dt><code>V84</code></dt><dd><p>  going for a walk</p>
</dd>
<dt><code>V86</code></dt><dd><p>  dancing</p>
</dd>
<dt><code>gender</code></dt><dd><p>gender (M=1 F=2)</p>
</dd>
</dl>



<h3>Details</h3>

<p>These 50 items (+ gender) may be formed into scales using the GERAS.keys
The first 10 items are Male Personality, the next 10  are  Female Personality, then 7 and 7 M and F Cognition, then 8 and 8 M and F Activity items.  The Pers, Cog and Act scales are formed from the M-F scales for the three domains.  M and F are the composites of the Male and then the Female scales.  MF.all is the composite of the  M - F scales.  See the GERAS.keys object for scoring directions.
</p>
<p>&quot;M.pers&quot; &quot;F.pers&quot; &quot;M.cog&quot;  &quot;F.cog&quot;  &quot;M.act&quot;  &quot;F.act&quot;  &quot;Pers&quot;   &quot;Cog&quot;    &quot;Act&quot;    &quot;M&quot;     
&quot;F&quot;      &quot;MF.all&quot; &quot;gender&quot;
</p>
<p>See the <code><a href="#topic+Athenstaedt">Athenstaedt</a></code> data set for a related data set.
</p>


<h3>Source</h3>

<p>Study 2 data  downloaded from the Open Science Framework https://osf.io/42jhr/ 
Used by kind permission of Freya M. Gruber, Tullia Ortner, and Belinda A. Pletzer.</p>


<h3>References</h3>

<p>Alice H. Eagly and William Revelle (2022), Understanding the Magnitude of Psychological Differences Between Women and Men Requires Seeing the Forest and the Tree. Perspectives in Psychological Science doi:10.1177/17456916211046006
</p>
<p>Gruber, Freya M. and Distlberger, Eva and Scherndl, Thomas and Ortner, Tuulia M. and Pletzer, Belinda (2020)   Psychometric properties of the multifaceted Gender-Related Attributes Survey (GERAS)  European Journal of Psychological Assessment, 36, (4) 612-623.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(GERAS)
GERAS.keys  #show the keys
#show the items from the dictionary
psych::lookupFromKeys(GERAS.keys, GERAS.dictionary[,4,drop=FALSE])


#now, use the GERAS.scales to show a scatterHist  plot showing univariate d and bivariate 
# Mahalanobis D.

psych::scatterHist(F ~ M + gender, data=GERAS.scales, cex.point=.3,smooth=FALSE, 
xlab="Masculine Scale",ylab="Feminine Scale",correl=FALSE, 
d.arrow=TRUE,col=c("red","blue"), bg=c("red","blue"), lwd=4, 
title="Combined  M and F scales",cex.cor=2,cex.arrow=1.25, cex.main=2)







</code></pre>

<hr>
<h2 id='globalWarm'>7 attitude items about Global Warming policy from Erik Nisbet
</h2><span id='topic+globalWarm'></span><span id='topic+glbwarm'></span>

<h3>Description</h3>

<p>Erik Nisbet reported the relationship between emotions, ideology, and party affiliation as predictors of attitudes towards government action on climate change. The data were used by Hayes (2013) in a discussion of regression.  They are available as the glbwarm data set in the processR package.  They are copied here for examples of mediation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("globalWarm")</code></pre>


<h3>Format</h3>

<p>A data frame with 815 observations on the following 7 variables.
</p>

<dl>
<dt><code>govact</code></dt><dd><p>Support for govermment action</p>
</dd>
<dt><code>posemot</code></dt><dd><p>Positive emotions about climate change</p>
</dd>
<dt><code>negemot</code></dt><dd><p>Negative emotions about climate change</p>
</dd> 
<dt><code>ideology</code></dt><dd><p>Political ideology (Liberal to conservative)</p>
</dd>
<dt><code>age</code></dt><dd><p>age</p>
</dd>
<dt><code>sex</code></dt><dd><p>female =0, male =1</p>
</dd>
<dt><code>partyid</code></dt><dd><p>Democratic =1, Independent =2, Republican =3</p>
</dd>
</dl>



<h3>Details</h3>

<p>This data set is discussed as an example of regression in Hayes (2013) p 24 - 30 and elsewhere.   It is a nice example of moderated regression.   It was collected by Erik Nisbet (no citation) who studies communication and the media. E. Nisbet is currently on the faculty at Northwestern School of Communication.  
</p>


<h3>Source</h3>

<p>The raw data are available from the processR package (Keon-Woong Moon, 2020) as the glbwarm data set as well as from Hayes' website.  The data set is used by Hayes in several examples. Used here by kind permission of Erik Nisbet. 
</p>
<p>Although the processR package has been removed from CRAN, an earlier version had the data. 
</p>


<h3>References</h3>

<p>Hayes, Andrew F. (2013) Introduction to mediation, moderation, and conditional process analysis: A regression-based approach. Guilford Press.
</p>
<p>Moon K (2023). processR: Implementation of the 'PROCESS' Macro_. R package version 0.2.8,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(globalWarm)
psych::lowerCor(globalWarm)
#compare to Hayes p 254-258
psych::lmCor(govact ~ negemot * age + posemot +ideology+sex,data=globalWarm,std=FALSE)
</code></pre>

<hr>
<h2 id='heights'>A data.frame of the Galton (1888) height and cubit data set.</h2><span id='topic+heights'></span>

<h3>Description</h3>

<p>Francis Galton introduced the 'co-relation' in 1888 with a paper discussing how to measure the relationship between two variables.  His primary example was the relationship between height and forearm length.  The data table (<code><a href="#topic+cubits">cubits</a></code>) is taken from Galton (1888).  Unfortunately, there seem to be some errors in the original data table in that the marginal totals do not match the table.
</p>
<p>The data frame, <code><a href="#topic+heights">heights</a></code>, is converted from this table using  <code>table2df</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(heights)</code></pre>


<h3>Format</h3>

<p>A data frame with 348 observations on the following 2 variables.
</p>

<dl>
<dt><code>height</code></dt><dd><p>Height in inches</p>
</dd>
<dt><code>cubit</code></dt><dd><p>Forearm length in inches</p>
</dd>
</dl>



<h3>Details</h3>

<p>Sir Francis Galton (1888) published the first demonstration of the correlation coefficient.  The regression (or reversion to mediocrity) of the height to the length of the left forearm (a cubit) was found to .8. The original table <code><a href="#topic+cubits">cubits</a></code> is taken from Galton (1888). There seem to be some errors in the table as published in that the row sums do not agree with the actual row sums. These data are used to create a matrix using <code>table2matrix</code> for demonstrations of analysis and displays of the data.
</p>


<h3>Source</h3>

<p>Galton (1888)
</p>


<h3>References</h3>

<p>Galton, Francis (1888) Co-relations and their measurement. Proceedings of the Royal Society. London Series,45,135-145,
</p>


<h3>See Also</h3>

  <p><code><a href="psych.html#topic+table2matrix">table2matrix</a></code>,  <code><a href="psych.html#topic+table2df">table2df</a></code>, <code><a href="#topic+cubits">cubits</a></code>, <code>ellipses</code>, <code><a href="#topic+galton">galton</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(heights)
psych::ellipses(heights,n=1,main="Galton's co-relation data set")

</code></pre>

<hr>
<h2 id='holzinger.swineford'>
The raw and transformed data from Holzinger and Swineford, 1939
</h2><span id='topic+holzinger.swineford'></span><span id='topic+holzinger.raw'></span><span id='topic+holzinger.dictionary'></span>

<h3>Description</h3>

<p>A classic data set in psychometrics is that from Holzinger and Swineford (1939).  A 4 and 5 factor solution to 24 of these variables problem is presented by Harman (1976), and 9 of these are used by the lavaan package.  The two data sets were supplied by Keith Widaman.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(holzinger.swineford)
     data(holzinger.raw)
     data(holzinger.dictionary)
     </code></pre>


<h3>Format</h3>

<p>A data frame with 301 observations on the following 33 variables. Longer descriptions taken from Thompson, (1998).
</p>

<dl>
<dt><code>case</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>school</code></dt><dd><p>School Pasteur  or Grant-White</p>
</dd>
<dt><code>grade</code></dt><dd><p>Grade (7 or 8)</p>
</dd>
<dt><code>female</code></dt><dd><p>male = 1, female = 2</p>
</dd>
<dt><code>ageyr</code></dt><dd><p>age in years</p>
</dd>
<dt><code>mo</code></dt><dd><p>months over year</p>
</dd>
<dt><code>agemo</code></dt><dd><p>Age in months </p>
</dd>
<dt><code>t01_visperc</code></dt><dd><p>Visual perception test from Spearman VPT  Part I</p>
</dd>
<dt><code>t02_cubes</code></dt><dd><p>Cubes, Simplification of Brighams Spatial Relations Test</p>
</dd>
<dt><code>t03_frmbord</code></dt><dd><p>Paper formboard-Shapes that can be combined to form a target</p>
</dd>
<dt><code>t04_lozenges</code></dt><dd><p>Lozenges from Thorndike-Shapes flipped over then identify target</p>
</dd>
<dt><code>t05_geninfo</code></dt><dd><p>General Information Verbal Test</p>
</dd>
<dt><code>t06_paracomp</code></dt><dd><p>Paragraph Comprehension Test</p>
</dd>
<dt><code>t07_sentcomp</code></dt><dd><p>Sentence Completion Test</p>
</dd>
<dt><code>t08_wordclas</code></dt><dd><p>Word clasification-Which word not belong in set</p>
</dd>
<dt><code>t09_wordmean</code></dt><dd><p>Word Meaning Test</p>
</dd>
<dt><code>t10_addition</code></dt><dd><p>Speeded addition test</p>
</dd>
<dt><code>t11_code</code></dt><dd><p>Speeded codetest-Transform shapes into alpha with code</p>
</dd>
<dt><code>t12_countdot</code></dt><dd><p>Speeded counting of dots in shap</p>
</dd>
<dt><code>t13_sccaps</code></dt><dd><p>Speeded discrimation of straight and curved caps</p>
</dd>
<dt><code>t14_wordrecg</code></dt><dd><p>Memory of Target Words</p>
</dd>
<dt><code>t15_numbrecg</code></dt><dd><p>Memory of Target Numbers</p>
</dd>
<dt><code>t16_figrrecg</code></dt><dd><p>Memory of Target Shapes</p>
</dd>
<dt><code>t17_objnumb</code></dt><dd><p>Memory of object-Number association targets</p>
</dd>
<dt><code>t18_numbfig</code></dt><dd><p>Memory of number-Object association targets</p>
</dd>
<dt><code>t19_figword</code></dt><dd><p>Memory of figure-Word association target</p>
</dd>
<dt><code>t20_deduction</code></dt><dd><p>Deductive Math Ability</p>
</dd>
<dt><code>t21_numbpuzz</code></dt><dd><p>Math number puzzles</p>
</dd>
<dt><code>t22_probreas</code></dt><dd><p>Math word problem reasoning</p>
</dd>
<dt><code>t23_series</code></dt><dd><p>Completion of a Math Number Series</p>
</dd>
<dt><code>t24_woody</code></dt><dd><p>Woody-McCall mixed math fundamentals test</p>
</dd>
<dt><code>t25_frmbord2</code></dt><dd><p>Revision of t3-Paper form board</p>
</dd>
<dt><code>t26_flags</code></dt><dd><p>Flags-possible substitute for t4 lozenges</p>
</dd>
</dl>



<h3>Details</h3>

<p>The following commentary was provided by Keith Widaman:
</p>
<p>&ldquo;The Holzinger and Swineford (1939) data have been used as a model data set by many investigators. For example, Harman (1976) used the &ldquo;24 Psychological Variables&quot; example prominently in his authoritative text on multiple factor analysis, and the data presented under this rubric consisted of 24 of the variables from the Grant-White school (N = 145). Meredith (1964a, 1964b) used several variables from the Holzinger and Swineford study in his work on factorial invariance under selection. Joreskog (1971) based his work on multiple-group confirmatory factor analysis using the Holzinger and Swineford data, subsetting the data into four groups.
</p>
<p>Rosseel, who developed the &lsquo;lavaan&rsquo; package for  R, included 9 of the manifest variables from Holzinger and Swineford (1939) as a &ldquo;resident&quot; data set when one downloads the &lsquo;lavaan&rsquo; package. Several background variables are included in this &ldquo;resident&quot; data set in addition to 9 of the psychological tests (which are named x1 &ndash; x9 in the data set). When analyzing these data, I found the distributions of the variables (means, SDs) did not match the sample statistics from the original article. For example, in the &ldquo;resident&quot; data set in &lsquo;lavaan&rsquo;, scores on all manifest variables ranged between 0 and 10, sample means varied between 3 and 6, and sample SDs varied between 1.0 and 1.5. In the original data set, scores ranges were rather different across tests, with some variables having scores that ranged between 0 and 20, but other manifest variables having scores ranging from 50 to over 300 &ndash; with obvious attendant differences in sample means and SDs.
</p>
<p>After a bit of snooping (i.e., data analysis), I discovered that the 9 variables in the &ldquo;resident&quot; data set in &lsquo;lavaan&rsquo; had been rescored through ratio transformations. The ratio transformations involved dividing the raw score for each person on a given test by a particular constant for that test that transformed scores on the test to have the desired range.
</p>
<p>I decided to perform transformations of all 26 variables so that two data sets could be available to interested researchers:&quot;
</p>
<p>holzinger.raw  are the raws scores on all variables from Holzinger &amp; Swineford (1939) 
</p>
<p>holzinger.swineford are  rescaled scores on all variables from Holzinger &amp; Swineford.
</p>
<p>holzinger.dictionary is a list of the variable names in short and long form.
</p>
<p>... Widaman continues:
</p>
<p>&ldquo;As several persons have noted, Harman (1976) used data only from the Grant-White school (N = 145) for his 24 Psychological Variables data set. In doing so, Harman replaced t03_frmbord and t04_lozenges with t25_frmbord2 and t26_flags, because the latter two tests were experimental tests that were designed to be more appropriate for this age level. This substitution is fine, as long as one analyzes data from only the Grant- White school. If one wishes to perform multiple-group analyses and uses school as a grouping variable (as Meredith, 1964a, 1964b, and Joreskog, 1971, did), then tests 25 and 26 should not be used.&quot;
</p>
<p>&ldquo;As have others, Gorsuch (1983) mentioned that analyses based on the raw data reported by Holzinger and Swineford (1939) will not produce statistics (means, SDs, correlations) that match precisely the values reported by Holzinger and Swineford or Harman (1976). Following Gorsuch, I have assumed that the raw data are correct. Applying factor analytic techniques to the raw data from the Grant-White school and to the summary data reported by Harman (1976) will produce slightly different results, but results that differ in only minor, unimportant details.&quot;
</p>
<p>These data are interesting not just for the historical completeness of having the original data, but also as an example of suppressor variables.  Age and grade are positively correlated, and scores are higher in the 8th grade than in the 7th grade. But age (particularly in months) is negatively correlated with many of the cognitive tasks, and when grade and age are both entered into regression, this negative correlation is enhanced.  That is, although increasing grade increases cognitive performance, younger children in both grades do better than the older children.  
</p>


<h3>Note</h3>

<p>As discussed by Widaman, the descriptive values reported in Harman (1967) (p 124) do not quite match the descriptive statistics in <code><a href="#topic+holzinger.raw">holzinger.raw</a></code>.  Further note that the correlation matrix and factor loadings are trivially different from the Harman.24 factor loadings in the GPA rotation package. 
</p>
<p>The purpose behind presenting both the raw and transformed data is to show that the fit statistics from factor analysis are identical for these two data sets. 
</p>
<p>The variables v1 ... v9 in the lavaan package correspond to tests 1, 2, 4, 6, 7, 9, 10, 12 and 13. 
</p>


<h3>Source</h3>

<p>Keith Widaman (2019, personal communication).  Original data from Holzinger and Swineford (1939). 
</p>


<h3>References</h3>

<p>Gorsuch, R. L. (1983). Factor analysis (2nd ed.). Hillsdale, NJ: Erlbaum.
</p>
<p>Harman, Harry Horace (1967), Modern factor analysis. Chicago, University of Chicago Press.
</p>
<p>Holzinger, K. J., &amp; Swineford, F. (1939). A study in factor analysis: The stability of a bi-factor solution. Supplementary Educational Monographs, no. 48. Chicago: University of
Chicago, Department of Education.
</p>
<p>Joreskog, K. G. (1971). Simultaneous factor analysis in several populations. Psychometrika, 36, 409-426.
</p>
<p>Meredith, W. (1964a). Notes on factorial invariance. Psychometrika, 29, 177-185.
</p>
<p>Meredith, W. (1964b). Rotation to achieve factorial invariance. Psychometrika, 29, 177-206.
</p>
<p>Meredith, W. (1977). On weighted Procrustes and hyperplane fitting in factor analytic rotation. Psychometrika, 42, 491-522.
</p>
<p>Thompson, Bruce. Five Methodology Errors in Educational  Research:The Pantheon of Statistical Significance and Other Faux Pas. Paper presented at the Annual Meeting of the American Educational Research Association(San Diego, CA, April 13-17,1998)
</p>


<h3>See Also</h3>

<p> psych::Holzinger </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(holzinger.raw)
psych::describe(holzinger.raw)
data(holzinger.dictionary)
holzinger.dictionary  #to see the longer names for these data (taken from Thompson)

#Compare these to the lavaan correlation matrix
psych::lowerCor(holzinger.swineford[ 7+ c(1, 2, 4, 6, 7, 9, 10, 12,  13)])

psych::lmCor(t01_visperc + t05_geninfo + t08_wordclas ~ grade + agemo,data = holzinger.raw)
psych::lmCor( t06_paracomp ~ grade + agemo, data=holzinger.swineford)
psych::mediate(t06_paracomp  ~ grade + (agemo),data = holzinger.raw,std=TRUE)

#show the omega structure of the 24 variables
 om4 &lt;- psych::omega(holzinger.swineford[8:31],4)
psych::omega.diagram(om4,sl=FALSE,main="26 variables from Holzinger-Swineford")

#these data also show an interesting suppression effect

psych::lowerCor(holzinger.swineford[c(3,7,12:14)])
psych::lmCor( t06_paracomp ~ grade + agemo, data=holzinger.swineford)
#or show as a mediation effect
mod &lt;- psych::mediate(t06_paracomp  ~ grade + (agemo),data = holzinger.raw,std=TRUE,n.iter=50)
summary(mod)

#now, show a plot of these effets
plot(t07_sentcomp ~ agemo, col=c("red","blue")[holzinger.swineford$grade -6],
  pch=26-holzinger.swineford$grade,data=holzinger.swineford,
   ylab="Sentence Comprehension",xlab="Age in Months",
   main="Sentence Comprehension varies by age and grade")
   #we use lmCor to figure out the lines 
   #note that we need to not plot the default graph
by(holzinger.swineford,holzinger.swineford$grade -6,function(x) abline(
     psych::lmCor(t07_sentcomp ~ agemo, data=x, std=FALSE, plot=FALSE), 
     lty=c("dashed","solid")[x$grade-6]))
text(190,3.3,"grade = 8")
text(190,2,"grade = 7") 
</code></pre>

<hr>
<h2 id='income'>US family income from US census 2008
</h2><span id='topic+income'></span><span id='topic+all.income'></span>

<h3>Description</h3>

<p>US census data on family income from 2008
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(income)</code></pre>


<h3>Format</h3>

<p>A data frame with 44 observations on the following 4 variables.
</p>

<dl>
<dt><code>value</code></dt><dd><p>lower boundary of the income group</p>
</dd>
<dt><code>count</code></dt><dd><p>Number of families within that income group</p>
</dd>
<dt><code>mean</code></dt><dd><p>Mean of the category</p>
</dd>
<dt><code>prop</code></dt><dd><p>proportion of families</p>
</dd>
</dl>



<h3>Details</h3>

<p>The distribution of income is a nice example of a log normal distribution.  It is also an interesting example of the power of graphics. It is quite clear when graphing the data that income statistics are bunched to the nearest 5K.  That is, there is a clear sawtooth pattern in the data.
</p>
<p>The all.income set is interpolates intervening values for 100-150K, 150-200K and 200-250K</p>


<h3>Source</h3>

<p>US Census: Table HINC-06. Income Distribution to $250,000 or More for Households: 2008
</p>
<p>https://www.census.gov/hhes/www/cpstables/032009/hhinc/new06_000.htm
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(income)
with(income[1:40,], plot(mean,prop, main="US family income for 2008",xlab="income", 
        ylab="Proportion of families",xlim=c(0,100000)))
with (income[1:40,], points(lowess(mean,prop,f=.3),typ="l"))
psych::describe(income)


with(all.income, plot(mean,prop, main="US family income for 2008",xlab="income", 
                ylab="Proportion of families",xlim=c(0,250000)))
with (all.income[1:50,], points(lowess(mean,prop,f=.25),typ="l"))

</code></pre>

<hr>
<h2 id='iqitems'>16 multiple choice IQ items</h2><span id='topic+iqitems'></span>

<h3>Description</h3>

<p>16 multiple choice ability items taken from  the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project.  The data from 1525 subjects are included here as a demonstration set for scoring multiple choice inventories and doing basic item statistics. For more information on the development of an open source measure of cognitive ability, consult the readings available at the <a href="https://personality-project.org/">https://personality-project.org/</a>.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(iqitems)</code></pre>


<h3>Format</h3>

<p>A data frame with 1525 observations on the following 16 variables. The number following the name is the item number from SAPA.
</p>

<dl>
<dt><code>reason.4</code></dt><dd><p>Basic reasoning questions </p>
</dd>
<dt><code>reason.16</code></dt><dd><p>Basic reasoning question</p>
</dd>
<dt><code>reason.17</code></dt><dd><p>Basic reasoning question</p>
</dd>
<dt><code>reason.19</code></dt><dd><p>Basic reasoning question </p>
</dd>   
<dt><code>letter.7</code></dt><dd><p>In the following alphanumeric series, what letter comes next?</p>
</dd>
<dt><code>letter.33</code></dt><dd><p>In the following alphanumeric series, what letter comes next?</p>
</dd>
<dt><code>letter.34</code></dt><dd><p>In the following alphanumeric series, what letter comes next</p>
</dd>
<dt><code>letter.58</code></dt><dd><p>In the following alphanumeric series, what letter comes next?</p>
</dd>
<dt><code>matrix.45</code></dt><dd><p>A matrix reasoning task</p>
</dd>
<dt><code>matrix.46</code></dt><dd><p>A matrix reasoning task</p>
</dd>
<dt><code>matrix.47</code></dt><dd><p>A matrix reasoning task</p>
</dd>
<dt><code>matrix.55</code></dt><dd><p>A matrix reasoning task</p>
</dd>
<dt><code>rotate.3</code></dt><dd><p>Spatial Rotation of type 1.2</p>
</dd>
<dt><code>rotate.4</code></dt><dd><p>Spatial Rotation of type 1.2</p>
</dd>
<dt><code>rotate.6</code></dt><dd><p>Spatial Rotation of type 1.1</p>
</dd>
<dt><code>rotate.8</code></dt><dd><p>Spatial Rotation of type 2.3</p>
</dd>
</dl>



<h3>Details</h3>

<p>16 items were sampled from 80 items given as part of the SAPA (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>) project  (Revelle, Wilt and Rosenthal, 2009; Condon and Revelle, 2014) to develop online measures of ability. These 16 items reflect four lower order factors (verbal reasoning, letter series, matrix reasoning, and spatial rotations.  These lower level factors all share a higher level factor ('g'). Similar data are available from the International Cognitive Abiity Resource at <a href="https://www.icar-project.org/">https://www.icar-project.org/</a> .
</p>
<p>This data set and the associated data set (<code><a href="#topic+ability">ability</a></code> based upon scoring these multiple choice items and converting them to correct/incorrect may be used to demonstrate item response functions, <code>tetrachoric</code> correlations, or <code>irt.fa</code> as well as <code>omega</code> estimates of of reliability and hierarchical structure.
</p>
<p>In addition, the data set is a good example of doing item analysis to examine the empirical response probabilities of each item alternative as a function of the underlying latent trait.  When doing this, it appears that two of the matrix reasoning problems do not have monotonically increasing trace lines for the probability correct.  At moderately high ability (theta = 1) there is a decrease in the probability correct from theta = 0 and theta = 2.
</p>


<h3>Source</h3>

<p> The example data set is taken from the Synthetic Aperture Personality Assessment personality and ability test at <a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>.  The data were collected with David Condon from 8/08/12 to 8/31/12.
</p>


<h3>References</h3>

<p>Condon, David and Revelle, William, (2014) The International Cognitive Ability Resource:  Development and initial validation of a public-domain measure. Intelligence, 43, 52-64.
</p>
<p>Revelle, William, Dworak, Elizabeth M. and Condon, David (2020) Cognitive ability in everyday life: the utility of open-source measures.  Current Directions in Psychological Science, 29, (4) 358-363. Open access at <a href="https://doi.org/10.1177/0963721420922178">doi:10.1177/0963721420922178</a>.
</p>
<p>Dworak, Elizabeth M., Revelle, William, Doebler, Philip and Condon, David (2021)  Using the International Cognitive Ability Resource as an open source tool to explore individual differences in cognitive ability.  Personality and Individual Differences, 169. Open access at <a href="https://doi.org/10.1016/j.paid.2020.109906">doi:10.1016/j.paid.2020.109906</a>.
</p>
<p>Revelle, W., Wilt, J.,  and Rosenthal, A. (2010)  Individual Differences in Cognition: New Methods for examining the Personality-Cognition Link In Gruszka, A.  and Matthews, G. and Szymura, B. (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control, Springer.
</p>
<p>Revelle, W,  Condon, D.M.,  Wilt, J.,  French, J.A., Brown, A.,  and  Elleman, L.G. (2016) Web and phone based data collection using planned missing designs. In  Fielding, N.G.,  Lee, R.M. and  Blank, G. (Eds). SAGE Handbook of Online Research Methods (2nd Ed), Sage Publcations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iqitems)
iq.keys &lt;- c(4,4,4, 6,  6,3,4,4,   5,2,2,4,   3,2,6,7)
psych::score.multiple.choice(iq.keys,iqitems)   #this just gives summary statisics
#convert them to true false 
iq.scrub &lt;- psych::scrub(iqitems,isvalue=0)  #first get rid of the zero responses
iq.tf &lt;-  psych::score.multiple.choice(iq.keys,iq.scrub,score=FALSE) 
              #convert to wrong (0) and correct (1) for analysis
psych::describe(iq.tf) 
#see the ability data set for these analyses
#now, for some item analysis
iq.irt &lt;- psych::irt.fa(iq.tf)  #do a basic irt
iq.sc &lt;- psych::scoreIrt(iq.irt,iq.tf)  #find the scores
op &lt;- par(mfrow=c(4,4))
psych::irt.responses(iq.sc[,1], iq.tf)  
op &lt;- par(mfrow=c(1,1))

</code></pre>

<hr>
<h2 id='msq'>75 mood items from the Motivational State Questionnaire for 3896 participants</h2><span id='topic+msq'></span>

<h3>Description</h3>

<p>Emotions may be described either as discrete emotions or in dimensional terms.  The Motivational State Questionnaire (MSQ) was developed to study emotions in laboratory and field settings.  The data can be well described in terms of a two dimensional solution of energy vs tiredness and tension versus calmness.  Additional items include what time of day the data were collected and a few personality questionnaire scores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(msq)</code></pre>


<h3>Format</h3>

<p>A data frame with 3896 observations on the following 92 variables.
</p>

<dl>
<dt><code>active</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>afraid</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>alert</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>angry</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>anxious</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>aroused</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>ashamed</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>astonished</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>at.ease</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>at.rest</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>attentive</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>blue</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>bored</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>calm</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>cheerful</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>clutched.up</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>confident</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>content</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>delighted</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>depressed</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>determined</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>distressed</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>drowsy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>dull</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>elated</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>energetic</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>enthusiastic</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>excited</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>fearful</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>frustrated</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>full.of.pep</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>gloomy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>grouchy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>guilty</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>happy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>hostile</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>idle</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>inactive</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>inspired</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>intense</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>interested</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>irritable</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>jittery</code></dt><dd><p>a numeric vector</p>
</dd>    
<dt><code>lively</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>lonely</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>nervous</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>placid</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>pleased</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>proud</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>quiescent</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>quiet</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>relaxed</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>sad</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>satisfied</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>scared</code></dt><dd><p>a numeric vector</p>
</dd>  
<dt><code>serene</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>sleepy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>sluggish</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>sociable</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>sorry</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>still</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>strong</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>surprised</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>tense</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>tired</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>tranquil</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>unhappy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>upset</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>vigorous</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>wakeful</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>warmhearted</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>wide.awake</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>alone</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>kindly</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>scornful</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>EA</code></dt><dd><p>Thayer's Energetic Arousal Scale</p>
</dd>
<dt><code>TA</code></dt><dd><p>Thayer's Tense Arousal Scale</p>
</dd>
<dt><code>PA</code></dt><dd><p>Positive Affect scale</p>
</dd>
<dt><code>NegAff</code></dt><dd><p>Negative Affect scale</p>
</dd>
<dt><code>Extraversion</code></dt><dd><p>Extraversion from the Eysenck Personality Inventory</p>
</dd>
<dt><code>Neuroticism</code></dt><dd><p>Neuroticism from the Eysenck Personality Inventory</p>
</dd>
<dt><code>Lie</code></dt><dd><p>Lie from the EPI</p>
</dd>
<dt><code>Sociability</code></dt><dd><p>The sociability subset of the Extraversion Scale</p>
</dd>
<dt><code>Impulsivity</code></dt><dd><p>The impulsivity subset of the Extraversions Scale</p>
</dd>
<dt><code>MSQ_Time</code></dt><dd><p>Time of day the data were collected</p>
</dd>
<dt><code>MSQ_Round</code></dt><dd><p>Rounded time of day</p>
</dd>
<dt><code>TOD</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>TOD24</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>ID</code></dt><dd><p>subject ID</p>
</dd>
<dt><code>condition</code></dt><dd><p>What was the experimental condition after the msq was given</p>
</dd>
<dt><code>scale</code></dt><dd><p>a factor with levels <code>msq</code> <code>r</code> original or revised msq</p>
</dd>
<dt><code>exper</code></dt><dd><p>Which study were the data collected: a factor with levels 
<code>AGES</code> <code>BING</code> <code>BORN</code> <code>CART</code> <code>CITY</code> <code>COPE</code> <code>EMIT</code> <code>FAST</code> <code>Fern</code> <code>FILM</code> <code>FLAT</code> <code>Gray</code> <code>imps</code> <code>item</code> <code>knob</code> <code>MAPS</code> <code>mite</code> <code>pat-1</code> <code>pat-2</code> <code>PATS</code> <code>post</code> <code>RAFT</code> <code>Rim.1</code> <code>Rim.2</code> <code>rob-1</code> <code>rob-2</code> <code>ROG1</code> <code>ROG2</code> <code>SALT</code> <code>sam-1</code> <code>sam-2</code> <code>SAVE/PATS</code> <code>sett</code> <code>swam</code> <code>swam-2</code> <code>TIME</code> <code>VALE-1</code> <code>VALE-2</code> <code>VIEW</code></p>
</dd>
</dl>



<h3>Details</h3>

<p>The Motivational States Questionnaire (MSQ) is composed of 72 items, which represent the full affective space (Revelle &amp; Anderson, 1998). The MSQ consists of 20 items taken from the Activation-Deactivation Adjective Check List (Thayer, 1986), 18 from the Positive and Negative Affect Schedule (PANAS, Watson, Clark, &amp; Tellegen, 1988) along with the items used by Larsen and Diener (1992). The response format was a four-point scale that corresponds to Russell and Carroll's (1999) &quot;ambiguous&ndash;likely-unipolar format&quot; and that asks the respondents to indicate their current standing (&ldquo;at this moment&quot;) with the following rating scale:<br />
0&mdash;&mdash;&mdash;&mdash;&mdash;-1&mdash;&mdash;&mdash;&mdash;&mdash;-2&mdash;&mdash;&mdash;&mdash;&mdash;-3
<br />
Not at all		A little		Moderately     	Very much <br />
</p>
<p>The original version of the MSQ included 70 items. Intermediate analyses (done with 1840 subjects) demonstrated a concentration of items in some sections of the two dimensional space, and a paucity of items in others. To begin correcting this, 3 items from redundantly measured sections (alone, kindly, scornful) were removed, and 5 new ones (anxious, cheerful, idle, inactive, and tranquil) were added.  Thus, the correlation matrix is missing the correlations between items anxious, cheerful, idle, inactive, and tranquil with alone, kindly, and scornful. 
</p>
<p>Procedure. The data were collected over nine years, as part of a series of studies examining the effects of personality and situational factors on motivational state and subsequent cognitive performance. In each of 38 studies, prior to any manipulation of motivational state, participants signed a consent form and filled out the MSQ. (The procedures of the individual studies are irrelevant to this data set and could not affect the responses to the MSQ, since this instrument was completed before any further instructions or tasks).  Some MSQ post test (after manipulations) is available in <code><a href="#topic+affect">affect</a></code>.
</p>
<p>The EA and TA scales are from Thayer, the PA and NA scales are from Watson et al. (1988).
Scales and items:
</p>
<p>Energetic Arousal: active, energetic, vigorous, wakeful, wide.awake, full.of.pep, lively, -sleepy, -tired, - drowsy  (ADACL)
</p>
<p>Tense Arousal: Intense, Jittery, fearful, tense, clutched up, -quiet, -still, - placid, - calm, -at rest  (ADACL)
</p>
<p>Positive Affect: active, alert, attentive, determined, enthusiastic, excited, inspired,  interested,  proud, strong  (PANAS)
</p>
<p>Negative Affect: afraid, ashamed,   distressed,  guilty,  hostile, irritable , jittery, nervous, scared, upset (PANAS)
</p>
<p>The PA and NA scales can in turn can be thought of as having subscales:  (See the PANAS-X)
Fear:  afraid, scared, nervous, jittery    (not included  frightened, shaky)
Hostility: angry, hostile, irritable, (not included:   scornful,  disgusted, loathing 
guilt: ashamed, guilty,   (not included: blameworthy, angry at self, disgusted with self, dissatisfied with self)
sadness: alone,  blue,  lonely, sad,  (not included: downhearted) 
joviality: cheerful, delighted, energetic, enthusiastic, excited,  happy, lively,     (not included:  joyful)
self-assurance: proud, strong, confident,       (not included: bold,  daring, fearless )   
attentiveness:  alert, attentive,  determined  (not included: concentrating)
</p>
<p>The next set of circumplex scales were taken (I think) from Larsen and Diener (1992).  
High activation: active, aroused, surprised, intense, astonished
Activated PA: elated, excited, enthusiastic, lively
Unactivated NA : calm, serene, relaxed, at rest, content, at ease
PA: happy, warmhearted, pleased, cheerful, delighted
Low Activation: quiet, inactive, idle, still, tranquil
Unactivated PA: dull, bored, sluggish, tired, drowsy
NA: sad, blue, unhappy, gloomy, grouchy
Activated NA: jittery, anxious, nervous, fearful, distressed.
</p>
<p>Keys for these separate scales are shown in the examples.  
</p>
<p>In addition to the MSQ, there are 5 scales from the Eysenck Personality Inventory (Extraversion, Impulsivity, Sociability, Neuroticism, Lie).  The Imp and Soc are subsets of the the total extraversion scale. 
</p>


<h3>Source</h3>

<p>Data collected at the Personality, Motivation, and Cognition Laboratory, Northwestern University. 
</p>


<h3>References</h3>

<p>Larsen, R. J., &amp; Diener, E. (1992). Promises and problems with the circumplex model of emotion. In M. S. Clark (Ed.), Review of personality and social psychology, No. 13. Emotion (pp. 25-59). Thousand Oaks, CA, US: Sage Publications, Inc.
</p>
<p>Rafaeli, Eshkol and Revelle, William (2006), A premature consensus: Are happiness and sadness truly opposite affects? Motivation and Emotion, 30, 1, 1-12.
</p>
<p>Revelle, W. and  Anderson, K.J. (1998) Personality, motivation and cognitive performance: Final report to the Army Research Institute on  contract MDA 903-93-K-0008. (<a href="https://www.personality-project.org/revelle/publications/ra.ari.98.pdf">https://www.personality-project.org/revelle/publications/ra.ari.98.pdf</a>).
</p>
<p>Thayer, R.E. (1989)  The biopsychology of mood and arousal.
Oxford University Press. New York, NY. 
</p>
<p>Watson,D., Clark,  L.A.  and Tellegen, A. (1988)  Development and validation of brief measures of positive and negative affect: The PANAS scales. Journal of Personality and Social Psychology, 54(6):1063-1070.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+msqR">msqR</a></code> for a larger data set with repeated measures for 3032 participants measured at least once, 2753 measured twice, 446 three times and 181 four times.  <code><a href="#topic+affect">affect</a></code> for an example of the use of some of these adjectives in a mood manipulation study.
</p>
<p><code>make.keys</code>, <code>scoreItems</code> and <code>scoreOverlap</code> for instructions on how to score multiple scales with and without item overlap. Also see  <code>fa</code> and <code>fa.extension</code> for instructions on how to do factor analyses or factor extension.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(msq)
   #in in the interests of time
#basic descriptive statistics
psych::describe(msq)

#score them for 20 short scales -- note that these have item overlap
#The first 2 are from Thayer
#The next 2 are classic positive and negative affect
#The next 9 are circumplex scales
#the last 7 are msq estimates of PANASX scales (missing some items)
keys.list &lt;- list(
EA = c("active", "energetic", "vigorous", "wakeful", "wide.awake", "full.of.pep",
       "lively", "-sleepy", "-tired", "-drowsy"),
TA =c("intense", "jittery", "fearful", "tense", "clutched.up", "-quiet", "-still", 
       "-placid", "-calm", "-at.rest") ,
PA =c("active", "excited", "strong", "inspired", "determined", "attentive", 
          "interested", "enthusiastic", "proud", "alert"),
NAf =c("jittery", "nervous", "scared", "afraid", "guilty", "ashamed", "distressed",  
         "upset", "hostile", "irritable" ),
HAct = c("active", "aroused", "surprised", "intense", "astonished"),
aPA = c("elated", "excited", "enthusiastic", "lively"),
uNA = c("calm", "serene", "relaxed", "at.rest", "content", "at.ease"),
pa = c("happy", "warmhearted", "pleased", "cheerful", "delighted" ),
LAct = c("quiet", "inactive", "idle", "still", "tranquil"),
uPA =c( "dull", "bored", "sluggish", "tired", "drowsy"),
naf = c( "sad", "blue", "unhappy", "gloomy", "grouchy"),
aNA = c("jittery", "anxious", "nervous", "fearful", "distressed"),
Fear = c("afraid" , "scared" , "nervous" , "jittery" ) ,
Hostility = c("angry" ,  "hostile", "irritable", "scornful" ), 
Guilt = c("guilty" , "ashamed" ),
Sadness = c( "sad"  , "blue" , "lonely",  "alone" ),
Joviality =c("happy","delighted", "cheerful", "excited", "enthusiastic", "lively", "energetic"), 
Self.Assurance=c( "proud","strong" , "confident" , "-fearful" ),
Attentiveness = c("alert" , "determined" , "attentive" )
#, acquiscence = c("sleepy" ,  "wakeful" ,  "relaxed","tense")   
#dropped because it has a negative alpha and throws warnings
   )
       
msq.scores &lt;- psych::scoreItems(keys.list,msq)

#show a circumplex structure for the non-overlapping items
fcirc &lt;- psych::fa(msq.scores$scores[,5:12],2)  
psych::fa.plot(fcirc,labels=colnames(msq.scores$scores)[5:12])

#now, find the correlations corrected for item overlap
msq.overlap &lt;- psych::scoreOverlap(keys.list,msq)
#a warning is thrown by smc  because of some NAs in the matrix

f2 &lt;- psych::fa(msq.overlap$cor,2)
psych::fa.plot(f2,labels=colnames(msq.overlap$cor),
      title="2 dimensions of affect, corrected for overlap")

#extend this solution to EA/TA  NA/PA space
fe  &lt;- psych::fa.extension(cor(msq.scores$scores[,5:12],msq.scores$scores[,1:4]),fcirc)
psych::fa.diagram(fcirc,fe=fe,
          main="Extending the circumplex structure to  EA/TA and PA/NA ")

#show the 2 dimensional structure
f2 &lt;- psych::fa(msq[1:72],2)
psych::fa.plot(f2,labels=colnames(msq)[1:72],
     title="2 dimensions of affect at the item level",cex=.5)

#sort them by polar coordinates
round(psych::polar(f2),2)

            

</code></pre>

<hr>
<h2 id='msqR'>75 mood items from the Motivational State Questionnaire for 3032 unique participants</h2><span id='topic+msqR'></span><span id='topic+msq.keys'></span>

<h3>Description</h3>

<p>Emotions may be described either as discrete emotions or in dimensional terms.  The Motivational State Questionnaire (MSQ) was developed to study emotions in laboratory and field settings.  The data can be well described in terms of a two dimensional solution of energy vs tiredness and tension versus calmness. Alternatively, this space can be organized by the two dimensions of Positive Affect and Negative Affect. Additional items include what time of day the data were collected and a few personality questionnaire scores. 3032 unique participants took the MSQ  at least once, 2753 at least twice, 446 three times, and 181 four times.  The 3032 participants also took the <code><a href="#topic+sai">sai</a></code> state anxiety inventory at the same time.  Some studies manipulated arousal by caffeine, others manipulations included affect inducing movies. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("msqR")</code></pre>


<h3>Format</h3>

<p>A data frame with 6411 observations on the following 88 variables.
</p>

<dl>
<dt><code>active</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>afraid</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>alert</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>alone</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>angry</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>aroused</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>ashamed</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>astonished</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>at.ease</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>at.rest</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>attentive</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>blue</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>bored</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>calm</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>clutched.up</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>confident</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>content</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>delighted</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>depressed</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>determined</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>distressed</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>drowsy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>dull</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>elated</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>energetic</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>enthusiastic</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>excited</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>fearful</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>frustrated</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>full.of.pep</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>gloomy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>grouchy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>guilty</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>happy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>hostile</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>inspired</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>intense</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>interested</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>irritable</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>jittery</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>lively</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>lonely</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>nervous</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>placid</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>pleased</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>proud</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>quiescent</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>quiet</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>relaxed</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>sad</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>satisfied</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>scared</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>serene</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>sleepy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>sluggish</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>sociable</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>sorry</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>still</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>strong</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>surprised</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>tense</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>tired</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>unhappy</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>upset</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>vigorous</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>wakeful</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>warmhearted</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>wide.awake</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>anxious</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>cheerful</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>idle</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>inactive</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>tranquil</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>kindly</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>scornful</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>Extraversion</code></dt><dd><p>Extraversion from the EPI</p>
</dd>
<dt><code>Neuroticism</code></dt><dd><p>Neuroticism from the EPI</p>
</dd>
<dt><code>Lie</code></dt><dd><p>Lie from the EPI</p>
</dd>
<dt><code>Sociability</code></dt><dd><p>Sociability from the EPI</p>
</dd>
<dt><code>Impulsivity</code></dt><dd><p>Impulsivity from the EPI</p>
</dd>
<dt><code>gender</code></dt><dd><p>1= male, 2 = female  (coded on presumed x chromosome). 
Slowly being added to the data set.</p>
</dd>
<dt><code>TOD</code></dt><dd><p>Time of day that the study was run</p>
</dd>
<dt><code>drug</code></dt><dd><p>1 if given placebo, 2 if given caffeine</p>
</dd>
<dt><code>film</code></dt><dd><p>1-4 if given a film: 1=Frontline, 2= Halloween, 3=Serengeti, 4 = Parenthood</p>
</dd>
<dt><code>time</code></dt><dd><p>Measurement occasion (1 and 2 are same session, 3 and 4 are
the same, but a later session)</p>
</dd>
<dt><code>id</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>form</code></dt><dd><p>msq versus msqR</p>
</dd>
<dt><code>study</code></dt><dd><p>a character vector of the experiment name</p>
</dd>
</dl>



<h3>Details</h3>

<p>The Motivational States Questionnaire (MSQ) is composed of 75 items, which represent the full affective space (Revelle &amp; Anderson, 1998). The MSQ consists of 20 items taken from the Activation-Deactivation Adjective Check List (Thayer, 1986), 18 from the Positive and Negative Affect Schedule (PANAS, Watson, Clark, &amp; Tellegen, 1988) along with the affective circumplex items used by Larsen and Diener (1992). The response format was a four-point scale that corresponds to Russell and Carroll's (1999) &quot;ambiguous&ndash;likely-unipolar format&quot; and that asks the respondents to indicate their current standing (&ldquo;at this moment&quot;) with the following rating scale:<br />
0&mdash;&mdash;&mdash;&mdash;&mdash;-1&mdash;&mdash;&mdash;&mdash;&mdash;-2&mdash;&mdash;&mdash;&mdash;&mdash;-3
<br />
Not at all		A little		Moderately     	Very much <br />
</p>
<p>The original version of the MSQ included 70 items. Intermediate analyses (done with 1840 subjects) demonstrated a concentration of items in some sections of the two dimensional space, and a paucity of items in others. To begin correcting this, 3 items from redundantly measured sections (alone, kindly, scornful) were removed, and 5 new ones (anxious, cheerful, idle, inactive, and tranquil) were added.  Thus, the correlation matrix is missing the correlations between items anxious, cheerful, idle, inactive, and tranquil with alone, kindly, and scornful. 
</p>
<p>2605 individuals took Form 1 version, 3806 the Form 2 version.  3032 people (1218 form 1, 1814 form 2) took the MSQ at least once.  2086 at least twice, 1112 three times, and 181 four times. 
</p>
<p>To see the relative frequencies by time and form, see the first example.
</p>
<p>Procedure. The data were collected over nine years in the Personality, Motivation and Cognition laboratory at Northwestern, as part of a series of studies examining the effects of personality and situational factors on motivational state and subsequent cognitive performance. In each of 38 studies, prior to any manipulation of motivational state, participants signed a consent form and in some studies, consumed 0 or 4mg/kg of caffeine.  In caffeine studies, they waited 30 minutes and then filled out the MSQ. (Normally, the procedures of the individual studies are irrelevant to this data set and could not affect the responses to the MSQ at time 1, since this instrument was completed before any further instructions or tasks.  However, caffeine does have an effect.)  The MSQ post test following a movie manipulation) is available in <code><a href="#topic+affect">affect</a></code> as well as here. 
</p>
<p>The XRAY study crossed four movie conditions with caffeine.  The first MSQ measures are showing the effects of the movies and caffeine, but after an additional 30 minutes, the second MSQ seems to mainly show the caffeine effects. The movies were 9 minute clips from 1) a BBC documentary on British troops arriving at the Bergen-Belsen concentration camp (sad); 2) an early scene from Halloween in which the heroine runs around shutting doors and windows (terror); 3) a documentary about lions on the Serengeti plain, and 4) the &quot;birthday party&quot; scene from Parenthood.
</p>
<p>The FLAT study measured affect before, immediately after, and then after 30 minutes following a movie manipulation. See the <code><a href="#topic+affect">affect</a></code> data set.
</p>
<p>To see which studies used which conditions, see the second and third examples.
</p>
<p>The EA and TA scales are from Thayer, the PA and NA scales are from Watson et al. (1988).
Scales and items:
</p>
<p>Energetic Arousal: active, energetic, vigorous, wakeful, wide.awake, full.of.pep, lively, -sleepy, -tired, - drowsy  (ADACL)
</p>
<p>Tense Arousal: Intense, Jittery, fearful, tense, clutched up, -quiet, -still, - placid, - calm, -at rest  (ADACL)
</p>
<p>Positive Affect: active, alert, attentive, determined, enthusiastic, excited, inspired,  interested,  proud, strong  (PANAS)
</p>
<p>Negative Affect: afraid, ashamed,   distressed,  guilty,  hostile, irritable , jittery, nervous, scared, upset (PANAS)
</p>
<p>The PA and NA scales can in turn can be thought of as having subscales:  (See the PANAS-X)
Fear:  afraid, scared, nervous, jittery    (not included  frightened, shaky)
Hostility: angry, hostile, irritable, (not included:   scornful,  disgusted, loathing 
guilt: ashamed, guilty,   (not included: blameworthy, angry at self, disgusted with self, dissatisfied with self)
sadness: alone,  blue,  lonely, sad,  (not included: downhearted) 
joviality: cheerful, delighted, energetic, enthusiastic, excited,  happy, lively,     (not included:  joyful)
self-assurance: proud, strong, confident,       (not included: bold,  daring, fearless )   
attentiveness:  alert, attentive,  determined  (not included: concentrating)
</p>
<p>The next set of circumplex scales were taken  from Larsen and Diener (1992).  
High activation: active, aroused, surprised, intense, astonished
Activated PA: elated, excited, enthusiastic, lively
Unactivated NA : calm, serene, relaxed, at rest, content, at ease
PA: happy, warmhearted, pleased, cheerful, delighted
Low Activation: quiet, inactive, idle, still, tranquil
Unactivated PA: dull, bored, sluggish, tired, drowsy
NA: sad, blue, unhappy, gloomy, grouchy
Activated NA: jittery, anxious, nervous, fearful, distressed.
</p>
<p>Keys for these separate scales are shown in the examples.  
</p>
<p>In addition to the MSQ, there are 5 scales from the Eysenck Personality Inventory (Extraversion, Impulsivity, Sociability, Neuroticism, Lie).  The Imp and Soc are subsets of the the total extraversion scale based upon a reanalysis of the EPI by Rocklin and Revelle (1983). This information is in the <code><a href="#topic+msq">msq</a></code> data set as well.
</p>


<h3>Note</h3>

<p>In December, 2018 the caffeine, film  and personality conditions were added. In the process of doing so, it was discovered that the EMIT data had been incorrectly entered.  This has been fixed.
</p>


<h3>Source</h3>

<p>Data collected at the Personality, Motivation, and Cognition Laboratory, Northwestern University. 
</p>


<h3>References</h3>

<p>Larsen, R. J., &amp; Diener, E. (1992). Promises and problems with the circumplex model of emotion. In M. S. Clark (Ed.), Review of personality and social psychology, No. 13. Emotion (pp. 25-59). Thousand Oaks, CA, US: Sage Publications, Inc.
</p>
<p>Rafaeli, Eshkol and Revelle, William (2006), A premature consensus: Are happiness and sadness truly opposite affects? Motivation and Emotion, 30, 1, 1-12.
</p>
<p>Revelle, W. and  Anderson, K.J. (1998) Personality, motivation and cognitive performance: Final report to the Army Research Institute on  contract MDA 903-93-K-0008. (<a href="https://www.personality-project.org/revelle/publications/ra.ari.98.pdf">https://www.personality-project.org/revelle/publications/ra.ari.98.pdf</a>).
</p>
<p>Smillie, Luke D.  and Cooper, Andrew  and Wilt, Joshua  and Revelle, William (2012) Do Extraverts Get More Bang for the Buck? Refining the Affective-Reactivity Hypothesis of Extraversion. Journal of Personality and Social Psychology, 103 (2), 206-326.
</p>
<p>Thayer, R.E. (1989)  The biopsychology of mood and arousal.
Oxford University Press. New York, NY. 
</p>
<p>Watson,D., Clark,  L.A.  and Tellegen, A. (1988)  Development and validation of brief measures of positive and negative affect: The PANAS scales. Journal of Personality and Social Psychology, 54(6):1063-1070.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+msq">msq</a></code> for 3896 participants with scores on five scales of the EPI.  <code><a href="#topic+affect">affect</a></code> for an example of the use of some of these adjectives in a mood manipulation study.
</p>
<p><code>make.keys</code>, <code>scoreItems</code> and <code>scoreOverlap</code> for instructions on how to score multiple scales with and without item overlap. Also see  <code>fa</code> and <code>fa.extension</code> for instructions on how to do factor analyses or factor extension.
</p>
<p>Given the temporal ordering of the <code><a href="#topic+sai">sai</a></code> data and the <code><a href="#topic+msqR">msqR</a></code> data, these data are useful for demonstrations of <code>testRetest</code> reliability.  See the examples in <code>testRetest</code>  for how to combine the  <code><a href="#topic+sai">sai</a></code> <code><a href="#topic+tai">tai</a></code>  and <code><a href="#topic+msqR">msqR</a></code> datasets.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(msqR)
table(msqR$form,msqR$time) #which forms?
table(msqR$study,msqR$drug) #Drug studies
table(msqR$study,msqR$film) #Film studies
table(msqR$study,msqR$TOD) #To examine time of day


#score them for 20 short scales -- note that these have item overlap
#The first 2 are from Thayer
#The next 2 are classic positive and negative affect 
#The next 9 are circumplex scales
#the last 7 are msq estimates of PANASX scales (missing some items)
keys.list &lt;- list(
EA = c("active", "energetic", "vigorous", "wakeful", "wide.awake", "full.of.pep",
       "lively", "-sleepy", "-tired", "-drowsy"),
TA =c("intense", "jittery", "fearful", "tense", "clutched.up", "-quiet", "-still", 
       "-placid", "-calm", "-at.rest") ,
PA =c("active", "excited", "strong", "inspired", "determined", "attentive", 
          "interested", "enthusiastic", "proud", "alert"),
NAf =c("jittery", "nervous", "scared", "afraid", "guilty", "ashamed", "distressed",  
         "upset", "hostile", "irritable" ),
HAct = c("active", "aroused", "surprised", "intense", "astonished"),
aPA = c("elated", "excited", "enthusiastic", "lively"),
uNA = c("calm", "serene", "relaxed", "at.rest", "content", "at.ease"),
pa = c("happy", "warmhearted", "pleased", "cheerful", "delighted" ),
LAct = c("quiet", "inactive", "idle", "still", "tranquil"),
uPA =c( "dull", "bored", "sluggish", "tired", "drowsy"),
naf = c( "sad", "blue", "unhappy", "gloomy", "grouchy"),
aNA = c("jittery", "anxious", "nervous", "fearful", "distressed"),
Fear = c("afraid" , "scared" , "nervous" , "jittery" ) ,
Hostility = c("angry" ,  "hostile", "irritable", "scornful" ), 
Guilt = c("guilty" , "ashamed" ),
Sadness = c( "sad"  , "blue" , "lonely",  "alone" ),
Joviality =c("happy","delighted", "cheerful", "excited", "enthusiastic", "lively", "energetic"), 
Self.Assurance=c( "proud","strong" , "confident" , "-fearful" ),
Attentiveness = c("alert" , "determined" , "attentive" ))

#acquiscence = c("sleepy" ,  "wakeful" ,  "relaxed","tense"))
#Yik Russell and Steiger list the following items
Yik.keys &lt;- list(
pleasure =psych::cs(happy,content,satisfied, pleased),
act.pleasure =psych::cs(proud,enthusiastic,euphoric),
pleasant.activation = psych::cs(energetic,full.of.pep,excited,wakeful,attentive,
   wide.awake,active,alert,vigorous),
activation = psych::cs(aroused,hyperactivated,intense),
unpleasant.act = psych::cs(anxious,frenzied,jittery,nervous),
activated.displeasure =psych::cs(scared,upset,shaky,fearful,clutched.up,tense,
    ashamed,guilty,agitated,hostile),
displeaure =psych::cs(troubled,miserable,unhappy,dissatisfied),
Ueactivated.Displeasure = psych::cs(sad,down,gloomy,blue,melancholy),
Unpleasant.Deactivation = psych::cs(droopy,drowsy,dull,bored,sluggish,tired),
Deactivation =psych::cs( quiet,still),
pleasant.deactivation = psych::cs(placid,relaxed,tranquil, at.rest,calm),
deactived.pleasure =psych::cs( serene,soothed,peaceful,at.ease,secure)
)

#of these 60 items, 46 appear in the msqR
Yik.msq.keys &lt;- list(
Pleasure =psych::cs(happy,content,satisfied, pleased),
Activated.Pleasure =psych::cs(proud,enthusiastic),
Pleasant.Activation = psych::cs(energetic,full.of.pep,excited,wakeful,attentive,
    wide.awake,active,alert,vigorous),
Activation = psych::cs(aroused,intense),
Unpleasant.Activation = psych::cs(anxious,jittery,nervous),
Activated.Displeasure =psych::cs(scared,upset,fearful,
          clutched.up,tense,ashamed,guilty,hostile),
Displeasure = psych::cs(unhappy),
Deactivated.Displeasure = psych::cs(sad,gloomy,blue),
Unpleasant.Deactivation = psych::cs(drowsy,dull,bored,sluggish,tired),
Deactivation =psych::cs( quiet,still),
Pleasant.Deactivation = psych::cs(placid,relaxed,tranquil, at.rest,calm),
Deactivated.Pleasure =psych::cs( serene,at.ease)
)   
yik.scores &lt;- psych::scoreItems(Yik.msq.keys,msqR)
yik &lt;- yik.scores$scores
f2.yik &lt;- psych::fa(yik,2) #factor the yik scores
psych::fa.plot(f2.yik,labels=colnames(yik),title="Yik-Russell-Steiger circumplex",cex=.8,
      pos=(c(1,1,2,1,1,1,3,1,4,1,2,4)))

       
msq.scores &lt;- psych::scoreItems(keys.list,msqR)

#show a circumplex structure for the non-overlapping items
fcirc &lt;- psych::fa(msq.scores$scores[,5:12],2)  
psych::fa.plot(fcirc,labels=colnames(msq.scores$scores)[5:12])


#now, find the correlations corrected for item overlap
msq.overlap &lt;- psych::scoreOverlap(keys.list,msqR)
f2 &lt;- psych::fa(msq.overlap$cor,2)
psych::fa.plot(f2,labels=colnames(msq.overlap$cor),
          title="2 dimensions of affect, corrected for overlap")

#extend this solution to EA/TA  NA/PA space
fe  &lt;- psych::fa.extension(cor(msq.scores$scores[,5:12],msq.scores$scores[,1:4]),fcirc)
psych::fa.diagram(fcirc,fe=fe,main="Extending the circumplex structure to  EA/TA and PA/NA ")

#show the 2 dimensional structure
f2 &lt;- psych::fa(msqR[1:72],2)
psych::fa.plot(f2,labels=colnames(msqR)[1:72],title="2 dimensions of affect at the item level")

#sort them by polar coordinates
round(psych::polar(f2),2)

#the msqR and sai data sets have 10 overlapping items which can be used for
#testRetest analysis.  We need to specify the keys, and then choose the appropriate
#data sets  
sai.msq.keys &lt;- list(pos =c( "at.ease" ,  "calm" , "confident", "content","relaxed"),
  neg = c("anxious", "jittery", "nervous" ,"tense"  ,   "upset"),
  anx = c("anxious", "jittery", "nervous" ,"tense", "upset","-at.ease" ,  "-calm" ,
  "-confident", "-content","-relaxed"))
   
select &lt;- psych::selectFromKeys(sai.msq.keys$anx)
#The following is useful for examining test retest reliabilities
msq.control &lt;- subset(msqR,is.element( msqR$study , c("Cart", "Fast", "SHED", "SHOP")))
msq.film &lt;- subset(msqR,(is.element( msqR$study ,  c("FIAT", "FILM","FLAT","MIXX","XRAY"))
    &amp; (msqR$time &lt; 3) )) 

msq.film[((msq.film$study == "FLAT") &amp; (msq.film$time ==3)) ,] &lt;- NA 
msq.drug &lt;- subset(msqR,(is.element( msqR$study ,  c("AGES","SALT", "VALE", "XRAY")))
   &amp;(msqR$time &lt; 3))

msq.day &lt;- subset(msqR,is.element( msqR$study ,  c("SAM", "RIM")))



</code></pre>

<hr>
<h2 id='neo'>NEO correlation matrix from the NEO_PI_R manual</h2><span id='topic+neo'></span>

<h3>Description</h3>

<p>The NEO.PI.R is a widely used personality test to assess 5 broad factors (Neuroticism, Extraversion, Openness, Agreeableness and Conscientiousness) with six facet scales for each factor.  The correlation matrix of the facets is reported in the NEO.PI.R manual for 1000 subjects. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(neo)</code></pre>


<h3>Format</h3>

<p>A data frame of a 30 x 30 correlation matrix with the following 30 variables.
</p>

<dl>
<dt>N1</dt><dd><p>Anxiety</p>
</dd>
<dt>N2</dt><dd><p>AngryHostility</p>
</dd>
<dt> N3</dt><dd><p>Depression </p>
</dd>
<dt> N4</dt><dd><p>Self-Consciousness </p>
</dd>
<dt> N5</dt><dd><p>Impulsiveness </p>
</dd>
<dt> N6</dt><dd><p>Vulnerability </p>
</dd>
<dt> E1</dt><dd><p>Warmth </p>
</dd>
<dt> E2</dt><dd><p>Gregariousness </p>
</dd>
<dt> E3</dt><dd><p>Assertiveness </p>
</dd>
<dt> E4</dt><dd><p>Activity </p>
</dd>
<dt> E5</dt><dd><p>Excitement-Seeking </p>
</dd>
<dt> E6</dt><dd><p>PositiveEmotions </p>
</dd>
<dt> O1</dt><dd><p>Fantasy </p>
</dd>
<dt> O2</dt><dd><p>Aesthetics </p>
</dd>
<dt> O3</dt><dd><p>Feelings </p>
</dd>
<dt> O4</dt><dd><p>Ideas </p>
</dd>
<dt> O5</dt><dd><p>Actions </p>
</dd>
<dt> O6</dt><dd><p>Values </p>
</dd>
<dt> A1</dt><dd><p>Trust </p>
</dd>
<dt> A2</dt><dd><p>Straightforwardness </p>
</dd>
<dt> A3</dt><dd><p>Altruism </p>
</dd>
<dt> A4</dt><dd><p>Compliance </p>
</dd>
<dt> A5</dt><dd><p>Modesty </p>
</dd>
<dt> A6</dt><dd><p>Tender-Mindedness </p>
</dd>
<dt> C1</dt><dd><p>Competence </p>
</dd>
<dt> C2</dt><dd><p>Order </p>
</dd>
<dt> C3</dt><dd><p>Dutifulness </p>
</dd>
<dt> C4</dt><dd><p>AchievementStriving </p>
</dd>
<dt> C5</dt><dd><p>Self-Discipline </p>
</dd>
<dt> C6</dt><dd><p>Deliberation </p>
</dd>
</dl>



<h3>Details</h3>

<p>The past thirty years of personality research has led to a general consensus on the identification of major dimensions of personality. Variously known as the &ldquo;Big 5&quot; or the &ldquo;Five Factor Model&quot;, the general solution represents 5 broad domains of personal and interpersonal experience.  Neuroticism and Extraversion are thought to reflect sensitivity to negative and positive cues from the environment and the tendency to withdraw or approach.  Openness is sometimes labeled as Intellect and reflects an interest in new ideas and experiences.  Agreeableness and Conscientiousness reflect tendencies to get along with others and to want to get ahead.
</p>
<p>The factor structure of the NEO suggests five correlated factors as well as two higher level factors.  The NEO was constructed with 6 &ldquo;facets&quot; for each of the five broad factors.  
</p>
<p>For a contrasting structure, examine the items of the <code>link{spi}</code> data set (Condon, 2017).
</p>


<h3>Source</h3>

<p>Costa, Paul T. and McCrae, Robert R. (1992) (NEO PI-R) professional manual. Psychological Assessment Resources, Inc. Odessa, FL. (with permission of the author and the publisher)	
</p>


<h3>References</h3>

<p>Condon, D. (2017) The SAPA Personality Inventory:An empirically-derived, hierarchically-organized self-report personality assessment model 
</p>
<p>Digman, John M. (1990) Personality structure: Emergence of the five-factor model. Annual Review of Psychology. 41, 417-440.
</p>
<p>John M. Digman (1997) Higher-order factors of the Big Five. Journal of Personality and Social Psychology, 73, 1246-1256. 
</p>
<p>McCrae, Robert R. and Costa, Paul T., Jr. (1999) A Five-Factor theory of personality. In Pervin, Lawrence A. and John, Oliver P. (eds)  Handbook of personality: Theory and research (2nd ed.) 139-153.  Guilford Press, New York. N.Y.
</p>
<p>Revelle, William (1995), Personality processes, Annual Review of Psychology, 46, 
295-328.
</p>
<p>Joshua Wilt and William Revelle (2009) Extraversion and Emotional Reactivity. In Mark Leary and Rick H. Hoyle (eds). Handbook of Individual Differences in Social Behavior. Guilford Press, New York, N.Y. 
</p>
<p>Joshua Wil and William Revelle (2016) Extraversion. In Thomas Widiger (ed) The Oxford Handbook of the Five Factor Model. Oxford University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(neo)
n5 &lt;- psych::fa(neo,5)
neo.keys &lt;- psych::make.keys(30,list(N=c(1:6),E=c(7:12),O=c(13:18),A=c(19:24),C=c(25:30)))
n5p &lt;- psych::target.rot(n5,neo.keys) #show a targeted rotation for simple structure
n5p

</code></pre>

<hr>
<h2 id='peas'>Galton's Peas</h2><span id='topic+peas'></span>

<h3>Description</h3>

<p>Francis Galton introduced the correlation coefficient with an analysis of the similarities of the parent and child generation of 700 sweet peas.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(peas)</code></pre>


<h3>Format</h3>

<p>A data frame with 700 observations on the following 2 variables.
</p>

<dl>
<dt><code>parent</code></dt><dd><p>The mean diameter of the mother pea for 700 peas</p>
</dd>
<dt><code>child</code></dt><dd><p>The mean diameter of the daughter pea for 700 sweet peas</p>
</dd>
</dl>



<h3>Details</h3>

<p>Galton's introduction of the correlation coefficient was perhaps the most important contribution to the study of individual differences.  This data set allows a graphical analysis of the data set.  There are two different graphic examples.  One shows the regression lines for both relationships, the other finds the correlation as well.
</p>


<h3>Source</h3>

<p>Stanton, Jeffrey M. (2001) Galton, Pearson, and the Peas: A brief history of linear regression for statistics intstructors, Journal of Statistics Education, 9. (retrieved from the web from https://www.amstat.org/publications/jse/v9n3/stanton.html) reproduces the table from Galton, 1894, Table 2. 
</p>
<p>The data were generated from this table.
</p>


<h3>References</h3>

<p>Galton, Francis (1877) Typical laws of heredity. paper presented to the weekly evening meeting of the Royal Institution, London. Volume VIII (66) is the first reference to this data set.  The data appear in 
</p>
<p>Galton, Francis (1894) Natural Inheritance (5th Edition), New York: MacMillan). 
</p>


<h3>See Also</h3>

<p>The other Galton data sets:   <code><a href="#topic+heights">heights</a></code>, <code><a href="#topic+galton">galton</a></code>,<code><a href="#topic+cubits">cubits</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(peas)
psych::pairs.panels(peas,lm=TRUE,xlim=c(14,22),ylim=c(14,22),main="Galton's Peas")
psych::describe(peas)
psych::pairs.panels(peas,main="Galton's Peas")
</code></pre>

<hr>
<h2 id='Pollack'>Pollack et al (2012) correlation matrix for mediation example
</h2><span id='topic+Pollack'></span><span id='topic+estress'></span>

<h3>Description</h3>

<p>A correlation matrix taken from Pollack (2012) with 9 variables.  Primarily used as an example for setCor and mediation.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Pollack")</code></pre>


<h3>Format</h3>

<p>A correlation matrix based upon 262 participants.
</p>

<dl>
<dt><code>sex</code></dt><dd><p>Male = 1, Female = 0, 62% male</p>
</dd>
<dt><code>age</code></dt><dd><p>mean =33</p>
</dd>
<dt><code>tenure</code></dt><dd><p>length of employent, mean = 5.9 years</p>
</dd>
<dt><code>self.efficacy</code></dt><dd><p>self ratings</p>
</dd>
<dt><code>competence</code></dt><dd><p>self rating of competence</p>
</dd>
<dt><code>social.ties</code></dt><dd><p>Contact with business-related social ties</p>
</dd>
<dt><code>economic.stress</code></dt><dd><p>mean of two items on economic stress</p>
</dd>
<dt><code>depression</code></dt><dd><p>6 items from MAACL measuring depression</p>
</dd>
<dt><code>withdrawal</code></dt><dd><p>Withdrawal intentions in domain of entrepreneurship</p>
</dd>
</dl>



<h3>Details</h3>

<p>This is the correlation matrix from Pollack et al. (2012) p 797.  The raw data are available from the processR package (Keon-Woong Moon, 2020). The data set is used by Hayes in example p 179 in example 3.
</p>


<h3>Source</h3>

<p>Pollack et al. 2012
</p>


<h3>References</h3>

<p>Pollack, Jeffrey M. and Vanepps, Eric M. and Hayes, Andrew F. (2012). The moderating role of social ties on entrepreneurs' depressed affect and withdrawal intentions in response to economic stress, Journal of Organizational Behavior 33 (6) 789-810.
</p>
<p>Hayes, Andrew F. (2013) Introduction to mediation, moderation, and conditional process analysis: A regression-based approach. Guilford Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>psych::lowerMat(Pollack)
</code></pre>

<hr>
<h2 id='psychTools'>psychTools:  datasets and utility functions to accompany the psych package
</h2><span id='topic+psychTools'></span>

<h3>Description</h3>

<p>PsychTools includes the larger data sets used by the <code><a href="psych.html#topic+psych">psych</a></code> package and also includes a few general utility functions such as the <code><a href="#topic+read.file">read.file</a></code> and <code><a href="#topic+read.clipboard">read.clipboard</a></code> functions. The data sets ara made available for demonstrations of a variety of psychometric functions.
</p>


<h3>Details</h3>

<p>See the various helpfiles listed in the index or as links from here.  Also see the main functions in the psych package <code><a href="psych.html#topic+00.psych-package">00.psych-package</a></code>.
</p>
<p>Data sets from the SAPA/ICAR project:
</p>

<table>
<tr>
 <td style="text-align: left;">

<code><a href="#topic+ability">ability</a></code> </td><td style="text-align: left;"> 16 ICAR ability items scored as correct or incorrect for 1525 participants.  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+iqitems">iqitems</a></code> </td><td style="text-align: left;"> multiple choice IQ items (raw responses) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+affect">affect</a></code>   </td><td style="text-align: left;">  Two data sets of affect and arousal scores as a function of personality and movie conditions </td>
</tr>
<tr>
 <td style="text-align: left;">

<code><a href="#topic+bfi">bfi</a></code>  </td><td style="text-align: left;"> 25 Personality items representing 5 factors from the SAPA project for 2800 participants </td>
</tr>
<tr>
 <td style="text-align: left;">
bfi.dictionary </td><td style="text-align: left;"> Dictionary of the bfi </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+big5.100.adjectives">big5.100.adjectives</a></code> 100 adjectives describing the "big 5" for 502 subjects (from Goldberg)
<code><a href="#topic+colom">colom</a></code> </td><td style="text-align: left;"> Correlations from the Spanish WAIS (14 scales) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+eminence">eminence</a></code> </td><td style="text-align: left;"> Eminence of 69 American Psychologists  </td>
</tr>
<tr>
 <td style="text-align: left;">


<code><a href="#topic+epi">epi</a></code>  </td><td style="text-align: left;"> Eysenck Personality Inventory (EPI) data for 3570 participants </td>
</tr>
<tr>
 <td style="text-align: left;">
epi.dictionary </td><td style="text-align: left;"> The items for the epi </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+epi.bfi">epi.bfi</a></code>  </td><td style="text-align: left;"> 13 personality scales from the Eysenck Personality Inventory and Big 5 inventory  </td>
</tr>
<tr>
 <td style="text-align: left;">

<code><a href="#topic+epiR">epiR</a></code>  </td><td style="text-align: left;"> 474 participants took the epi twice </td>
</tr>
<tr>
 <td style="text-align: left;">


<code><a href="#topic+msq">msq</a></code>  </td><td style="text-align: left;"> 75 mood items from the Motivational State Questionnaire for 3896 participants </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+msqR">msqR</a></code> </td><td style="text-align: left;">  75 mood items from the Motivational State Questionnaire for 3032 unique participants </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+tai">tai</a></code>  </td><td style="text-align: left;"> Trait Anxiety data from the PMC lab matching the sai sample. 3032 unique subjects </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+sai">sai</a></code>  </td><td style="text-align: left;">  State Anxiety data from the PMC lab over multiple occasions. 3032 unique subjects. </td>
</tr>
<tr>
 <td style="text-align: left;">
sai.dictionary </td><td style="text-align: left;">  items used in the sai </td>
</tr>
<tr>
 <td style="text-align: left;">

<code><a href="#topic+spi">spi</a></code>  </td><td style="text-align: left;"> 4000 cases  from the SAPA Personality Inventory (135 items, 10 demographics) including an item dictionary and scoring keys. </td>
</tr>
<tr>
 <td style="text-align: left;">
spi.dictionary </td><td style="text-align: left;"> The items for the spi </td>
</tr>
<tr>
 <td style="text-align: left;">
spi.keys </td><td style="text-align: left;">  Scoring keys for the spi </td>
</tr>
<tr>
 <td style="text-align: left;">



</td>
</tr>

</table>
  
<p>Historically interesting data sets 
</p>

<table>
<tr>
 <td style="text-align: left;">

<code><a href="#topic+burt">burt</a></code>  </td><td style="text-align: left;"> 11 emotional variables from Burt (1915) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+galton">galton</a></code>  </td><td style="text-align: left;"> Galtons Mid parent child height data </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+heights">heights</a></code>  </td><td style="text-align: left;">  A data.frame of the Galton (1888) height and cubit data set </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+cubits">cubits</a></code> </td><td style="text-align: left;"> Galtons example of the relationship between height and cubit or forearm length </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+peas">peas</a></code> </td><td style="text-align: left;"> Galtons Peas </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+cushny">cushny</a></code>  </td><td style="text-align: left;"> The data set from Cushny and Peebles (1905) on the effect of three drugs on hours of sleep, used by Student (1908) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+holzinger.swineford">holzinger.swineford</a></code> </td><td style="text-align: left;"> 26 cognitive variables + 7 demographic variables for   301 cases from Holzinger and Swineford. 
</td>
</tr>

</table>

<p>Miscellaneous example data sets
</p>

<table>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+blant">blant</a></code>  </td><td style="text-align: left;"> A 29 x 29 matrix that produces weird factor analytic results </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+blot">blot</a></code>  </td><td style="text-align: left;"> Bonds Logical Operations Test - BLOT </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+cities">cities</a></code>  </td><td style="text-align: left;"> Distances between 11 US cities </td>
</tr>
<tr>
 <td style="text-align: left;"> 
city.location </td><td style="text-align: left;"> and their geograpical location </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+income">income</a></code>   </td><td style="text-align: left;"> US family income from US census 2008 </td>
</tr>
<tr>
 <td style="text-align: left;">
all.income  </td><td style="text-align: left;"> US family income from US census 2008 </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+neo">neo</a></code>  </td><td style="text-align: left;"> NEO correlation matrix from the NEO_PI_R manual </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+Schutz">Schutz</a></code> </td><td style="text-align: left;"> The Schutz correlation matrix example from Shapiro and ten Berge </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+Spengler">Spengler</a></code> </td><td style="text-align: left;"> The Spengler and Damian correlation matrix example from Spengler, Damian and Roberts (2018) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+Damian">Damian</a></code> </td><td style="text-align: left;"> Another correlation matrix from Spengler, Damian and Roberts (2018) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+usaf">usaf</a></code> </td><td style="text-align: left;"> A correlation of  17 body size (anthropometric) measures from the US Air Force. Adapted from the Anthropometric package.</td>
</tr>
<tr>
 <td style="text-align: left;">
veg </td><td style="text-align: left;"> Paired comparison of preferences for 9 vegetables (scaling example) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Functions to convert various objects to latex
</p>

<table>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+fa2latex">fa2latex</a></code>  </td><td style="text-align: left;"> Convert a data frame, correlation matrix, or factor analysis output to a LaTeX table </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+df2latex">df2latex</a></code>  </td><td style="text-align: left;"> Convert a data frame, correlation matrix, or factor analysis output to a LaTeX table </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+ICC2latex">ICC2latex</a></code> </td><td style="text-align: left;"> Convert an ICC analyssis output to a LaTeX table </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+irt2latex">irt2latex</a></code> </td><td style="text-align: left;"> Convert an irt analysis output to a LaTeX table </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+cor2latex">cor2latex</a></code> </td><td style="text-align: left;"> Convert a  correlation matrix output to a LaTeX table </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+omega2latex">omega2latex</a></code> </td><td style="text-align: left;"> Convert a data frame, correlation matrix, or factor analysis output to a LaTeX table </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>File manipulation functions
</p>

<table>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+fileCreate">fileCreate</a></code>  </td><td style="text-align: left;">  Create a file </td>
</tr>
<tr>
 <td style="text-align: left;">
fileScan </td><td style="text-align: left;"> Show the first few lines of multitple files </td>
</tr>
<tr>
 <td style="text-align: left;">
filesInfo </td><td style="text-align: left;">  Show the information for all files in a directory </td>
</tr>
<tr>
 <td style="text-align: left;">
filesList </td><td style="text-align: left;"> Show the names of all files in a directory </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p><code><a href="#topic+dfOrder">dfOrder</a></code>  Sorts a data frame
<code><a href="#topic+vJoin">vJoin</a></code> Combine two matrices or data frames into one based upon variable labels
<code><a href="#topic+combineMatrices">combineMatrices</a></code>Takes a square matrix (x) and combines with a rectangular matrix y to produce a larger xy matrix.
</p>
<p>File input/output functions 
</p>

<table>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+read.clipboard">read.clipboard</a></code>  </td><td style="text-align: left;"> Shortcuts for reading from the clipboard or a file </td>
</tr>
<tr>
 <td style="text-align: left;">
read.clipboard.csv </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
read.clipboard.fwf </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
read.clipboard.lower </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
read.clipboard.tab </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
read.clipboard.upper </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+read.file">read.file</a></code> </td><td style="text-align: left;"> Read a file according to its suffix </td>
</tr>
<tr>
 <td style="text-align: left;">
read.file.csv </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
read.https </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+write.file">write.file</a></code> </td><td style="text-align: left;"> Write data to a file </td>
</tr>
<tr>
 <td style="text-align: left;">
write.file.csv </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Examples</h3>

<pre><code class='language-R'>psych::describe(ability)
</code></pre>

<hr>
<h2 id='rd2html'>Convert all Rd files in a directory to HTML files in a new directory</h2><span id='topic+rd2html'></span>

<h3>Description</h3>

<p>Just a wrapper for tools::RdHTML to find a directory (e.g., the Man directory of help files) and convert them to HTML files in a new directory.  Useful for adding HTML help files to a local web page.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rd2html(inDir =NULL,outDir=NULL, nfiles=NULL,package="psych",file=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rd2html_+3A_indir">inDir</code></td>
<td>
<p>The input directory.  If NULL,then a file in a directory will be searched for using file.choose()</p>
</td></tr>
<tr><td><code id="rd2html_+3A_outdir">outDir</code></td>
<td>
<p>Where to write the output files</p>
</td></tr>
<tr><td><code id="rd2html_+3A_nfiles">nfiles</code></td>
<td>
<p>If not NULL, then how many files should be written</p>
</td></tr>
<tr><td><code id="rd2html_+3A_package">package</code></td>
<td>
<p>name of package </p>
</td></tr>
<tr><td><code id="rd2html_+3A_file">file</code></td>
<td>
<p>If specified, just convert this one file to HTML</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Just a wrapper for Rd2HTML calling some file tools.  An interesting use of the function is to precheck whether all the help files are syntactically correct.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

<p>See Also as <code><a href="#topic+filesList">filesList</a></code>, <code><a href="#topic+filesInfo">filesInfo</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive()) {
#This is an  interactive function whic require interactive input and thus is not given as examples
rd2html()

}
</code></pre>

<hr>
<h2 id='read.file'>Shortcuts for reading from the clipboard or a file</h2><span id='topic+read.clipboard'></span><span id='topic+read.clipboard.csv'></span><span id='topic+read.clipboard.tab'></span><span id='topic+read.clipboard.lower'></span><span id='topic+read.clipboard.upper'></span><span id='topic+read.clipboard.fwf'></span><span id='topic+read.file'></span><span id='topic+read.file.csv'></span><span id='topic+write.file'></span><span id='topic+write.file.csv'></span><span id='topic+read.https'></span>

<h3>Description</h3>

<p>Input from a variety of sources may be read. Matrices or data.frames may be read from files with suffixes of .txt, .text, .TXT, .dat, .DATA,.data, .csv, .rds, rda, .xpt, XPT, or  .sav   (i.e., data from SPSS sav files may be  read as can files saved by SAS using the .xpt option). Data exported by JMP or EXCEL in the csv format are also able to be read. Fixed Width Files saved in .txt mode may be read if the  widths parameter is specified. Files saved with writeRDS have suffixes of  .rds or  Rds,  and are read using readRDS.  Files associated with objects with suffixes .rda and .Rda are  loaded (following a security prompt). The default values for read.spss are adjusted for more standard input from SPSS files.  
Input from the clipboard is easy but a bit obscure, particularly for Mac users. <code><a href="#topic+read.clipboard">read.clipboard</a></code> and its variations are just an easier way to do so.  Data may be copied to the clipboard from Excel spreadsheets, csv files, or fixed width formatted files and then into a data.frame. Data may also be read from lower  (or upper) triangular matrices and filled out to square matrices.  Writing text files may be done using <code><a href="#topic+write.file">write.file</a></code> which
will prompt for a file name (if not given) and then write or save to that file depending upon the suffix (text, txt, or csv will call write.table, R, or r will dput, rda, Rda will save,  Rds,rds will saveRDS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.file(file=NULL,header=TRUE,use.value.labels=FALSE,to.data.frame=TRUE,sep=",",
    quote="\"", widths=NULL,f=NULL, filetype=NULL,...)
   #for .txt, .text, TXT, .csv, .sav, .xpt, XPT,  R, r, Rds, .rds, or .rda, 
  # .Rda, .RData, .Rdata, .dat and .DAT  files

read.clipboard(header = TRUE, ...)   #assumes headers and tab or space delimited
read.clipboard.csv(header=TRUE,sep=',',...)   #assumes headers and comma delimited
read.clipboard.tab(header=TRUE,sep='\t',...)   #assumes headers and tab delimited
 #read in a matrix given the lower off diagonal
 read.clipboard.lower(diag=TRUE,names=FALSE,...) 
 read.clipboard.upper(diag=TRUE,names=FALSE,...)

#read in data using a fixed format width (see read.fwf for instructions)
read.clipboard.fwf(header=FALSE,widths=rep(1,10),...)  

read.https(filename,header=TRUE)
read.file.csv(file=NULL,header=TRUE,f=NULL,...)

#For output: 
#be sure to specify the file type in name
write.file(x,file=NULL,row.names=FALSE,f=NULL,...)
write.file.csv(x,file=NULL,row.names=FALSE,f=NULL,...)


</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.file_+3A_header">header</code></td>
<td>
<p>Does the first row have variable labels (generally assumed to be TRUE). </p>
</td></tr>
<tr><td><code id="read.file_+3A_sep">sep</code></td>
<td>
<p>What is the designated separater between data fields? For typical csv files, this will be a comma, but if commas designate decimals, then a ; can be used to designate different records.  </p>
</td></tr>
<tr><td><code id="read.file_+3A_quote">quote</code></td>
<td>
<p>Specified to </p>
</td></tr>
<tr><td><code id="read.file_+3A_diag">diag</code></td>
<td>
<p>for upper or lower triangular matrices, is the diagonal specified or not</p>
</td></tr>
<tr><td><code id="read.file_+3A_names">names</code></td>
<td>
<p>for read.clipboard.lower or upper, are colnames in the  the first column</p>
</td></tr>
<tr><td><code id="read.file_+3A_widths">widths</code></td>
<td>
<p>how wide are the columns in fixed width input.  The default is to read 10 columns of size 1. </p>
</td></tr>
<tr><td><code id="read.file_+3A_filename">filename</code></td>
<td>
<p>Name or address of remote https file to read.</p>
</td></tr>
<tr><td><code id="read.file_+3A_...">...</code></td>
<td>
<p> Other parameters to pass to read </p>
</td></tr>
<tr><td><code id="read.file_+3A_f">f</code></td>
<td>
<p>A file name to read from or write to.  If omitted, <code><a href="base.html#topic+file.choose">file.choose</a></code> is called to dynamically get the file name.</p>
</td></tr>
<tr><td><code id="read.file_+3A_file">file</code></td>
<td>
<p>A file name to read from or write to. (same as f, but perhaps more intuitive).  If omitted and if f is omitted,then <code><a href="base.html#topic+file.choose">file.choose</a></code> is called to dynamically get the file name.</p>
</td></tr>
<tr><td><code id="read.file_+3A_x">x</code></td>
<td>
<p>The data frame or matrix to write to f</p>
</td></tr>
<tr><td><code id="read.file_+3A_row.names">row.names</code></td>
<td>
<p>Should the output file include the rownames? By default, no.</p>
</td></tr> <tr><td><code id="read.file_+3A_to.data.frame">to.data.frame</code></td>
<td>
<p>Should the spss input be converted to a data frame?</p>
</td></tr>
<tr><td><code id="read.file_+3A_use.value.labels">use.value.labels</code></td>
<td>
<p>Should the SPSS input values be converted to numeric?</p>
</td></tr>
<tr><td><code id="read.file_+3A_filetype">filetype</code></td>
<td>
<p>If specified the reading will use this term rather than the suffix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A typical session of R might involve data stored in text files, generated online, etc. Although it is easy to just read from a file (particularly if using <code><a href="#topic+read.file">read.file</a></code>), an alternative is to use one's local system to copy from the file to the clipboard and then read from the clipboard using  <code><a href="#topic+read.clipboard">read.clipboard</a></code>. This is  very convenient (and somewhat more intuitive to the naive user). This is particularly useful when copying from a text book or article and just moving a section of text into R.  However, copying from a file and then reading the clipboard is hard to automate in a script.  Thus, <code><a href="#topic+read.file">read.file</a></code> will read from a file.
</p>
<p>The <code><a href="#topic+read.file">read.file</a></code> function combines the <code><a href="base.html#topic+file.choose">file.choose</a></code> and either <code><a href="utils.html#topic+read.table">read.table</a></code>, <code><a href="utils.html#topic+read.fwf">read.fwf</a></code>, <code><a href="foreign.html#topic+read.spss">read.spss</a></code> or  <code><a href="foreign.html#topic+read.xport">read.xport</a></code>(from foreign) or <code><a href="base.html#topic+load">load</a></code> or <code><a href="base.html#topic+readRDS">readRDS</a></code> commands. By examining the file suffix, it chooses the appropriate way to read the file. For more complicated file structures, see the foreign package. For even more complicated file structures, see the rio or haven packages. 
</p>
<p>Note that <code><a href="#topic+read.file">read.file</a></code> assumes by default that the first row has column labels (header =TRUE).  If this is not true, then make sure to specify header = FALSE.  If the file is fixed width, the assumption is that it does not have a header field.  In the unlikely case that a fwf file does have a header, then you probably should try fn &lt;- file.choose() and then my.data &lt;- read.fwf(fn,header=TRUE,widths=   widths).
</p>
<p>Further note:  If the file is a .Rda, .rda, etc. file, the read.file command will return the name and location of the file. It will prompt the user to load this file.  In this case, it is necessary to either assign the output (the file name) to an object that has a different name than any of the objects in the file, or to call read.file() without any specification. Notice that loading an .Rda file can overwrite existing objects.  Thus the warning and the need to do the second step.
</p>
<p>If the file has no suffix the default action is to quit with a warning.  However, if the filetype is specified, it will use that type in the reading (e.g. filetype=&quot;txt&quot; will read as text file, even if there is no suffix).
</p>
<p>If the file is specified and has a prefix of http:// or https:// it will be downloaded and then read.
</p>
<p>Currently supported input formats are 
</p>

<table>
<tr>
 <td style="text-align: left;">
.sav       </td><td style="text-align: left;">      SPSS.sav files</td>
</tr>
<tr>
 <td style="text-align: left;">
.csv       </td><td style="text-align: left;">      A comma separated file (e.g. from Excel or Qualtrics)</td>
</tr>
<tr>
 <td style="text-align: left;">
.txt       </td><td style="text-align: left;">      A typical text file </td>
</tr>
<tr>
 <td style="text-align: left;">
.TXT        </td><td style="text-align: left;">      A typical text file </td>
</tr>
<tr>
 <td style="text-align: left;">
.text      </td><td style="text-align: left;">     A typical text file </td>
</tr>
<tr>
 <td style="text-align: left;">
.data     </td><td style="text-align: left;">       A data file </td>
</tr>
<tr>
 <td style="text-align: left;">
.dat      </td><td style="text-align: left;">       A data file </td>
</tr>
<tr>
 <td style="text-align: left;">
.rds       </td><td style="text-align: left;">      A R data file </td>
</tr>
<tr>
 <td style="text-align: left;">
.Rds      </td><td style="text-align: left;">       A R data file  (created by a write) </td>
</tr>
<tr>
 <td style="text-align: left;">
.Rda      </td><td style="text-align: left;">       A R data structure (created using save) </td>
</tr>
<tr>
 <td style="text-align: left;">
.rda      </td><td style="text-align: left;">       A R data structure (created using save) </td>
</tr>
<tr>
 <td style="text-align: left;">
.RData     </td><td style="text-align: left;">       A R data structure (created using save) </td>
</tr>
<tr>
 <td style="text-align: left;">
.rdata     </td><td style="text-align: left;">       A R data structure (created using save) </td>
</tr>
<tr>
 <td style="text-align: left;">
.R        </td><td style="text-align: left;">       A R data structure created using dput </td>
</tr>
<tr>
 <td style="text-align: left;">
.r        </td><td style="text-align: left;">       A R data structure created using dput </td>
</tr>
<tr>
 <td style="text-align: left;">
.xpt      </td><td style="text-align: left;">       A SAS data file in xport format </td>
</tr>
<tr>
 <td style="text-align: left;">
.XPT      </td><td style="text-align: left;">       A SAS data file in XPORT format  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Some data files have an extra ' in the data ( e.g. the NYT covid data base).  These files can be read specifying quote &quot;&quot;
</p>
<p>The foreign function <code><a href="foreign.html#topic+read.spss">read.spss</a></code> is used to read SPSS .sav files using the most common options.  Just as  <code><a href="foreign.html#topic+read.spss">read.spss</a></code>  issues various warnings, so does  <code><a href="#topic+read.file">read.file</a></code>.   In general, these can be ignored.  For more detailed information about using  <code><a href="foreign.html#topic+read.spss">read.spss</a></code>, see the help pages in the foreign package.
</p>
<p>If you have a file written by JMP, you must first export to a csv or text file.
</p>
<p>The <code><a href="#topic+write.file">write.file</a></code> function combines the <code><a href="base.html#topic+file.choose">file.choose</a></code> and either <code><a href="utils.html#topic+write.table">write.table</a></code> or <code><a href="base.html#topic+saveRDS">saveRDS</a></code>. By examining the file suffix, it chooses the appropriate way to write. For more complicated file structures, see the foreign package, or the save function in R Base.  If no suffix is added, it will write as a .txt file.  <code><a href="#topic+write.file.csv">write.file.csv</a></code> will write in csv format to an arbitrary file name.
</p>
<p>Currently supported output formats are 
</p>

<table>
<tr>
 <td style="text-align: left;">
.csv       </td><td style="text-align: left;">      A comma separated file (e.g. for reading into Excel)</td>
</tr>
<tr>
 <td style="text-align: left;">
.txt       </td><td style="text-align: left;">      A typical text file </td>
</tr>
<tr>
 <td style="text-align: left;">
.text      </td><td style="text-align: left;">     A typical text file </td>
</tr>
<tr>
 <td style="text-align: left;">
.rds       </td><td style="text-align: left;">      A R data file </td>
</tr>
<tr>
 <td style="text-align: left;">
.Rds      </td><td style="text-align: left;">       A R data file  (created by a write) </td>
</tr>
<tr>
 <td style="text-align: left;">
.Rda      </td><td style="text-align: left;">       A R data structure (created using save) </td>
</tr>
<tr>
 <td style="text-align: left;">
.rda      </td><td style="text-align: left;">       A R data structure (created using save) </td>
</tr>
<tr>
 <td style="text-align: left;">
.R        </td><td style="text-align: left;">       A R data structure created using dput </td>
</tr>
<tr>
 <td style="text-align: left;">
.r        </td><td style="text-align: left;">       A R data structure created using dput </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Many Excel based files specify missing values as a blank field. When reading from the clipboard, using <code><a href="#topic+read.clipboard.tab">read.clipboard.tab</a></code> will change these blank fields to NA.
</p>
<p>Sometimes missing values are specified as &quot;.&quot; or &quot;999&quot;, or some other values. These can be converted by the read.file command specifying what values are missing  (e.g., na =&quot;.&quot;). See the example for the reading from the remote mtcars.csv file.
</p>
<p><code><a href="#topic+read.clipboard">read.clipboard</a></code> was based upon a suggestion by Ken Knoblauch to the R-help listserve.
</p>
<p>If the input file that was copied into the clipboard was an Excel file with blanks for missing data, then read.clipboard.tab() will correctly replace the blanks with NAs.  Similarly for a csv file with blank entries, read.clipboard.csv will replace empty fields with NA.  
</p>
<p><code><a href="#topic+read.clipboard.lower">read.clipboard.lower</a></code> and <code><a href="#topic+read.clipboard.upper">read.clipboard.upper</a></code> are adapted from John Fox's read.moments function in the sem package.  They will read a lower (or upper) triangular matrix from the clipboard and return a full, symmetric matrix for use by factanal, <code>fa</code> , <code>ICLUST</code>, <code>pca</code>. <code>omega</code> , etc.  If the diagonal is false, it will be replaced by 1.0s.  These two function were added to allow easy reading of examples from various texts and manuscripts with just triangular output. 
</p>
<p>Many articles will report lower triangular matrices with variable labels in the first column.  read.clipboard.lower  will handle this case. Names must be in the first column if names=TRUE is specified.
</p>
<p>Other articles will report upper triangular matrices with variable labels in the first row.  read.clipboard.upper will handle this.  Note that labels in the first column will not work for read.clipboard.upper.  The names, if present, must be in the first row.
</p>
<p>Consider the following lower triangular matrix.  To read it, copy it to the clipboard and read.clipboard.lower(names=TRUE)
</p>

<table>
<tr>
 <td style="text-align: left;">
 A1 1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
A2 -0.34  1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
A3 -0.27 0.49  1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
A4 -0.15  0.34  0.36  1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
A5 -0.18  0.39  0.50  0.31  1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
C1 0.03   0.09  0.10  0.09  0.12  1.00</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>However, if the data are strung out e.g.,
</p>

<table>
<tr>
 <td style="text-align: right;">
-.34  </td>
</tr>
<tr>
 <td style="text-align: right;">
-.27  </td>
</tr>
<tr>
 <td style="text-align: right;">
-.15  </td>
</tr>
<tr>
 <td style="text-align: right;">
-.18  </td>
</tr>
<tr>
 <td style="text-align: right;">
.03</td>
</tr>
<tr>
 <td style="text-align: right;">
.49</td>
</tr>
<tr>
 <td style="text-align: right;">
.34 </td>
</tr>
<tr>
 <td style="text-align: right;">
.39 </td>
</tr>
<tr>
 <td style="text-align: right;">
.09 </td>
</tr>
<tr>
 <td style="text-align: right;">
.36</td>
</tr>
<tr>
 <td style="text-align: right;">
.50 </td>
</tr>
<tr>
 <td style="text-align: right;">
.10 </td>
</tr>
<tr>
 <td style="text-align: right;">
.31</td>
</tr>
<tr>
 <td style="text-align: right;">
.09 </td>
</tr>
<tr>
 <td style="text-align: right;">
.12</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>Then one needs to read it using the read.clipboard.upper(names=FALSE,diag=FALSE) option.  
</p>
<p>read.clipboard.fwf will read fixed format files from the clipboard.  It includes a patch to read.fwf which will not read from the clipboard or from remote file.  See read.fwf for documentation of how to specify the widths. </p>


<h3>Value</h3>

<p>The contents of the file to be read or of the clipboard. Saved as a data.frame.
</p>


<h3>Author(s)</h3>

<p> William Revelle</p>


<h3>Examples</h3>

<pre><code class='language-R'>#All of these functions are meant for interactive Input
#Because these are dynamic functions, they need to be run interactively and 
# can not be run as examples.
#Thus they are not to be tested by CRAN

if(interactive()) {
 my.data &lt;- read.file()  #search the directory for a file and then read it.
                         #return the result into an object 
#or, if the file is a rda, etc. file
my.data &lt;- read.file()  #return the path and instructions of how to load
  # without assigning a value.

filesList()  #search the system for a particular file and then list all the files in that directory
fileCreate() #search for a particular directory and create a file there.
write.file(Thurstone) #open the search window, choose a location and name the output file,
# write the data file (e.g., Thurstone ) to the file chosen

#the example data set from read.delim in the readr package to read a remote csv file
my.data &lt;-read.file(
"https://github.com/tidyverse/readr/raw/master/inst/extdata/mtcars.csv", 
na=".")   #the na option is used for an example, but is not needed for these data


#These functions read from the local clipboard and thus are interactive
my.data &lt;- read.clipboard()   #space delimited columns
my.data &lt;- read.clipboard.csv()  # , delimited columns 
my.data &lt;- read.clipboard.tab()  #typical input if copied from a spreadsheet
my.data &lt;- read.clipboad(header=FALSE)  #data start on line 1
my.matrix &lt;- read.clipboard.lower()
}

</code></pre>

<hr>
<h2 id='recode'>Recode or rearrange or reshape variables or values to new values</h2><span id='topic+recode'></span><span id='topic+rearrange'></span><span id='topic+wide2long'></span>

<h3>Description</h3>

<p>Given a set of numeric codes, change their values to different values given a mapping function.  Also included are the ability to reorder columns or to convert wide sets of columns  to long form 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rearrange(x,pattern)   #reorder the variables
wide2long(x,width, cname=NULL, idname = NULL, idvalues=NULL ,pattern=NULL) 
recode(x, where, isvalue, newvalue)  #recode text values to numeric values
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recode_+3A_x">x</code></td>
<td>
<p>A matrix or data frame of numeric values
</p>
</td></tr>
<tr><td><code id="recode_+3A_where">where</code></td>
<td>
<p>The column numbers to fix</p>
</td></tr>
<tr><td><code id="recode_+3A_isvalue">isvalue</code></td>
<td>
<p>A vector of values to change</p>
</td></tr>
<tr><td><code id="recode_+3A_newvalue">newvalue</code></td>
<td>
<p>A vector of the new values</p>
</td></tr>
<tr><td><code id="recode_+3A_pattern">pattern</code></td>
<td>
<p>column order of repeating patterns</p>
</td></tr>
<tr><td><code id="recode_+3A_width">width</code></td>
<td>
<p>width of long format </p>
</td></tr>
<tr><td><code id="recode_+3A_cname">cname</code></td>
<td>
<p>Variable names of long format </p>
</td></tr>
<tr><td><code id="recode_+3A_idname">idname</code></td>
<td>
<p>Name of first column</p>
</td></tr>
<tr><td><code id="recode_+3A_idvalues">idvalues</code></td>
<td>
<p>Values to fill first column </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Three functions for basic recoding are included.
</p>
<p>recode: Sometime, data are entered as levels in an incorrect order.  Once converted to numeric values, this can lead to confusion.  recoding of the data to the correct order is straightforward, if tedious.
</p>
<p>rearrange: Another tedious problem is when the output of one function needs to be arranged for better data handling in subsequent function. Specify a pattern of choosing the new columns. 
</p>
<p>wide2long: And then, having rearranged the data, perhaps convert the file to long format.
</p>


<h3>Value</h3>

<p>The reordered data
</p>


<h3>Note</h3>

<p>Although perhaps  useful, the recode function is definitely ugly code. For smaller data sets, the results from char2numeric back to the original will not work.  char2numeric works column wise and orders the data in each column.  
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

<p>mlArrange in the psych package for a more general version of wide2long</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(1:120,ncol=12) 
new &lt;- rearrange(x,pattern = c(1,4, 7,10))
new 
long &lt;- wide2long(x,width=3,pattern=c(1,4, 7,10))  #rearrange and then make wide


temp &lt;- bfi[1:100,1:5]
isvalue &lt;- 1:6
newvalue &lt;- psych::cs(one,two,three,four,five,six)
newtemp &lt;- recode(temp,1:5,isvalue,newvalue)
newtemp  #characters
temp.num &lt;- psych::char2numeric(newtemp) #convert to numeric
temp.num  #notice the numerical values have changed
new.temp.num &lt;- recode(temp.num, 1:5, isvalue=c(3,6,5,2,1,4), newvalue=1:6)
#note that because char2numeric works column wise, this will fail for small sets

</code></pre>

<hr>
<h2 id='sai'>State Anxiety data from the PMC lab over multiple occasions.
</h2><span id='topic+sai'></span><span id='topic+tai'></span><span id='topic+sai.dictionary'></span>

<h3>Description</h3>

<p>State Anxiety was measured two-three times in 11 studies at the Personality-Motivation-Cognition laboratory.  Here are item responses for 11 studies (9 repeated twice, 2 repeated three times). In all studies, the first occasion was before a manipulation.  In some studies, caffeine, or movies or incentives were then given to some of the participants before the second and third STAI was given.  In addition, Trait measures are available and included in the tai data set (3032 subjects).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sai)
data(tai)
data(sai.dictionary)
</code></pre>


<h3>Format</h3>

<p>A data frame with 3032 unique observations on the following 23 variables. 
</p>

<dl>
<dt><code>id</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>study</code></dt><dd><p>a factor with levels <code>ages</code> <code>cart</code> <code>fast</code> <code>fiat</code> <code>film</code> <code>flat</code> <code>home</code> <code>pat</code> <code>rob</code> <code>salt</code> <code>shed</code><code>shop</code> <code>xray</code></p>
</dd>
<dt><code>time</code></dt><dd><p>1=First, 2 = Second, 3=third administration</p>
</dd>
<dt><code>TOD</code></dt><dd><p>TOD  (time of day 1= 8:50-9:30 am,2 = 1=3 pm, 3= 7:-8pm</p>
</dd>
<dt><code>drug</code></dt><dd><p>drug (placebo (0) vs. caffeine (1))</p>
</dd>
<dt><code>film</code></dt><dd><p>film (1=Frontline (concentration camp), 2 = Halloween
3= National Geographic (control), 4- Parenthood (humor)</p>
</dd>
<dt><code>anxious</code></dt><dd><p>anxious</p>
</dd>
<dt><code>at.ease</code></dt><dd><p>at ease</p>
</dd>
<dt><code>calm</code></dt><dd><p>calm</p>
</dd>
<dt><code>comfortable</code></dt><dd><p>comfortable</p>
</dd>
<dt><code>confident</code></dt><dd><p>confident</p>
</dd>
<dt><code>content</code></dt><dd><p>content</p>
</dd>
<dt><code>high.strung</code></dt><dd><p>high.strung</p>
</dd>
<dt><code>jittery</code></dt><dd><p>jittery</p>
</dd>
<dt><code>joyful</code></dt><dd><p>joyful</p>
</dd>
<dt><code>nervous</code></dt><dd><p>nervous</p>
</dd>
<dt><code>pleasant</code></dt><dd><p>pleasant</p>
</dd>
<dt><code>rattled</code></dt><dd><p>over-excited and rattled</p>
</dd>
<dt><code>regretful</code></dt><dd><p>regretful</p>
</dd>
<dt><code>relaxed</code></dt><dd><p>relaxed</p>
</dd>
<dt><code>rested</code></dt><dd><p>rested</p>
</dd>
<dt><code>secure</code></dt><dd><p>secure</p>
</dd>
<dt><code>tense</code></dt><dd><p>tense</p>
</dd>
<dt><code>upset</code></dt><dd><p>upset</p>
</dd>
<dt><code>worried</code></dt><dd><p>worried</p>
</dd>
<dt><code>worrying</code></dt><dd><p>worrying</p>
</dd>
</dl>



<h3>Details</h3>

<p>The standard experimental study at the Personality, Motivation and Cognition (PMC) laboratory (Revelle and Anderson, 1997)  was to administer a number of personality trait and state measures (e.g. the <code><a href="#topic+epi">epi</a></code>, <code><a href="#topic+msq">msq</a></code>, <code><a href="#topic+msqR">msqR</a></code> and <code><a href="#topic+sai">sai</a></code>)  to participants before some experimental manipulation of arousal/effort/anxiety.  Following the manipulation (with a 30 minute delay if giving caffeine/placebo), some performance task was given, followed once again by measures of state arousal/effort/anxiety.  
</p>
<p>Here are the item level data on the <code><a href="#topic+sai">sai</a></code> (state anxiety)  and the <code><a href="#topic+tai">tai</a></code> (trait anxiety). Scores on these scales may be found using the scoring keys.  The <code><a href="#topic+affect">affect</a></code> data set includes pre and post scores for two studies (flat and maps) which manipulated state by using four types of movies.
</p>
<p>In addition to being useful for studies of motivational state, these studies provide examples of test-retest and  alternate form reliabilities.  Given that 10 items overlap with the <code><a href="#topic+msqR">msqR</a></code> data, they also allow for a comparison of immediate duplication of items with 30 minute delays.
</p>
<p>Studies CART, FAST, SHED,  RAFT,  and SHOP  were either control groups, or did not experimentally vary arousal/effort/anxiety.   
</p>
<p>AGES, CITY, EMIT, RIM, SALT, and XRAY were caffeine manipulations between time 1 and 2
(RIM and VALE were repeated day 1 and day 2)
</p>
<p>FIAT, FLAT, MAPS, MIXX, and THRU  were 1 day studies with film manipulation between time 1 and time 2.
</p>
<p>SAM1 and SAM2 were the first and second day of a two day study. The STAI was given once per day.  MSQ not MSQR was given.
</p>
<p>VALE and PAT were two day studies with the STAI given pre and post on both days
</p>
<p>RIM was a two day study with the STAI and MSQ given once per day. 
</p>
<p>Usually, time of day 1 = 8:50-9am am, and 2 = 7:30 pm, however, in rob, with paid subjects, the times were 0530 and 22:30.  
</p>


<h3>Source</h3>

<p>Data collected at the Personality, Motivation, and Cognition Laboratory, Northwestern University, between 1991 and 1999.
</p>


<h3>References</h3>

<p>Charles D. Spielberger and Richard L. Gorsuch and R. E. Lushene, (1970) Manual for the State-Trait Anxiety Inventory.
</p>
<p>Revelle,  William  and Anderson, Kristen Joan  (1997) Personality, motivation and cognitive performance: Final report to the Army Research Institute on  contract MDA 903-93-K-0008
</p>
<p>Rafaeli, Eshkol and Revelle, William (2006), A premature consensus: Are happiness and sadness truly opposite affects? Motivation and Emotion, 30, 1, 1-12.
</p>
<p>Smillie, Luke D.  and Cooper, Andrew  and Wilt, Joshua  and Revelle, William (2012) Do Extraverts Get More Bang for the Buck? Refining the Affective-Reactivity Hypothesis of Extraversion. Journal of Personality and Social Psychology, 103 (2), 206-326.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sai)

table(sai$study,sai$time)  #show the counts for repeated measures

#Here are the keys to score the sai total score, positive and negative items
sai.keys &lt;- list(sai = c("tense","regretful" , "upset", "worrying", "anxious", "nervous" ,  
"jittery" , "high.strung", "worried" , "rattled","-calm", 
"-secure","-at.ease","-rested","-comfortable", "-confident" ,"-relaxed" , "-content" , 
"-joyful", "-pleasant"  ) ,
sai.p = c("calm","at.ease","rested","comfortable", "confident", "secure" ,"relaxed" ,     
       "content" , "joyful", "pleasant" ),  
sai.n = c( "tense" , "anxious", "nervous" , "jittery" , "rattled",     "high.strung",  
         "upset", "worrying","worried","regretful" )
) 
tai.keys &lt;- list(tai=c("-pleasant" ,"nervous" , "not.satisfied", "wish.happy",
   "failure","-rested", "-calm", "difficulties" , "worry" , "-happy" , 
   "disturbing.thoughts","lack.self.confidence",
   "-secure", "decisive" , "inadequate","-content","thoughts.bother","disappointments" ,    
   "-steady" , "tension"  ),
   tai.pos = c("pleasant", "-wish.happy", "rested","calm","happy" ,"secure",
   "content","steady" ),
   tai.neg = c("nervous", "not.satisfied", "failure","difficulties", "worry", 
    "disturbing.thoughts" ,"lack.self.confidence","decisive","inadequate" , 
    "thoughts.bother","disappointments","tension" )         )


#using the is.element function instead of the %in% function 
#just get the control subjects 
control &lt;- subset(sai,is.element(sai$study,c("Cart", "Fast", "SHED", "RAFT", "SHOP")) )

#pre and post drug studies
drug &lt;- subset(sai,is.element(sai$study, c("AGES","CITY","EMIT","SALT","VALE","XRAY"))) 

#pre and post film studies
film &lt;- subset(sai,is.element(sai$study, c("FIAT","FLAT", "MAPS", "MIXX") ))

#this next set allows us to score those sai items that overlap with the msq item sets
msq.items &lt;- c("anxious", "at.ease" ,"calm", "confident","content", "jittery", 
 "nervous" ,  "relaxed" ,  "tense"  ,  "upset" ) #these overlap with the msq
 
sai.msq.keys &lt;- list(pos =c( "at.ease" ,  "calm" , "confident", "content","relaxed"),
  neg = c("anxious", "jittery", "nervous" ,"tense"  ,   "upset"),
  anx = c("anxious", "jittery", "nervous" ,"tense", "upset","-at.ease" ,  "-calm" ,
  "-confident", "-content","-relaxed"))
sai.not.msq.keys &lt;- list(pos=c(  "secure","rested","comfortable" ,"joyful" , "pleasant" ),    
    neg=c("regretful","worrying", "high.strung","worried", "rattled" ),
    anx = c("regretful","worrying", "high.strung","worried", "rattled",     "-secure",      
    "-rested", "-comfortable", "-joyful",  "-pleasant" )) 
sai.alternate.forms &lt;- list( pos1 =c( "at.ease","calm","confident","content","relaxed"),
  neg1 = c("anxious", "jittery", "nervous" ,"tense"  ,   "upset"),
  anx1 = c("anxious", "jittery", "nervous" ,"tense", "upset","-at.ease" ,  "-calm" ,
       "-confident", "-content","-relaxed"),
  pos2=c(  "secure","rested","comfortable" ,"joyful" , "pleasant" ),    
  neg2=c("regretful","worrying", "high.strung","worried", "rattled" ),
  anx2 = c("regretful","worrying", "high.strung","worried", "rattled", "-secure",      
    "-rested", "-comfortable", "-joyful",  "-pleasant" )) 
  
sai.repeated &lt;- c("AGES","Cart","Fast","FIAT","FILM","FLAT","HOME","PAT","RIM","SALT",
    "SAM","SHED","SHOP","VALE","XRAY")
sai12 &lt;- subset(sai,is.element(sai$study,  sai.repeated)) #the subset with repeated measures

#Choose those studies with repeated measures by :
sai.control &lt;- subset(sai,is.element(sai$study, c("Cart", "Fast", "SHED", "SHOP")))
sai.film &lt;- subset(sai,is.element(sai$study, c("FIAT","FLAT") )  )
sai.drug &lt;- subset(sai,is.element(sai$study, c("AGES",  "SALT", "VALE", "XRAY")))
sai.day &lt;- subset(sai,is.element(sai$study, c("SAM", "RIM")))


</code></pre>

<hr>
<h2 id='salary'>Salary example from Cohen, Cohen, Aiken and West (2003)</h2><span id='topic+salary'></span>

<h3>Description</h3>

<p>Four predictors of academic salary are used as examples in Cohen, Cohen, Aiken, and West (2003) may be used for demonstration purposes of multiple regression and multiple correlation.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("salary")</code></pre>


<h3>Format</h3>

<p>A data frame with 62 observations on the following 5 variables.
</p>

<dl>
<dt><code>time</code></dt><dd><p>Time since Ph.D.</p>
</dd>
<dt><code>publications</code></dt><dd><p>Number of publications</p>
</dd>
<dt><code>female</code></dt><dd><p>gender Male=0, Female =1</p>
</dd>
<dt><code>citations</code></dt><dd><p>Number of citations</p>
</dd>
<dt><code>salary</code></dt><dd><p>Salary</p>
</dd>
</dl>



<h3>Details</h3>

<p>Two extended examples multiple regression in CCAW are discussed in Chapter 3.
</p>
<p>These are nice examples of the use of the <code>link{psych::lmCor}</code> and <code>link{psych::partial.r}</code> functions. 
</p>
<p>Note that example data set in Table 3.2.1 (p 67)  is just the first 15 cases of the complete data set used in Table 3.5.1 (page 81) and included in this data set. 
</p>


<h3>Source</h3>

<p>CD accompanying Cohen, Cohen, Aiken and West (2003) (used with the kind permission of Leona Aiken and Steven West)
</p>


<h3>References</h3>

<p>Cohen, J., Cohen, P., West, S. G., &amp; Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences (3rd ed.). Lawrence Erlbaum Associates Publishers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(salary)
psych::describe(salary)
psych::pairs.panels(salary)
#the standardized coefficients
psych::lmCor(salary ~ time + publications, data=salary)
#or the raw coefficients
mod &lt;- psych::lmCor(salary ~ time + publications, data=salary, std=FALSE)
mod 
#show the part correlations 
psych::partial.r(salary ~ time - publications, data=salary, part=TRUE)
psych::partial.r(salary ~ -time + publications, data=salary, part=TRUE)
#show the predicted salaries based upon the model 
mod &lt;- psych::lmCor(salary ~ time + publications+ citations + female,
      data=salary, std=FALSE)
predicted.salary &lt;- psych::predict.psych(mod,salary)
head(predicted.salary)#compare to CCAW p 81
## 

</code></pre>

<hr>
<h2 id='sat.act'>3 Measures of ability: SATV, SATQ, ACT</h2><span id='topic+sat.act'></span>

<h3>Description</h3>

<p>Self reported scores on the SAT Verbal, SAT Quantitative and ACT   were collected as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project.  Age, gender, and education are also reported. The data from 700 subjects are included here as a demonstration set for correlation and analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sat.act)</code></pre>


<h3>Format</h3>

<p>A data frame with 700 observations on the following 6 variables.
</p>

<dl>
<dt><code>gender</code></dt><dd><p>males = 1, females = 2</p>
</dd>
<dt><code>education</code></dt><dd><p>self reported education 1 = high school ... 5 = graduate work</p>
</dd>
<dt><code>age</code></dt><dd><p>age</p>
</dd>
<dt><code>ACT</code></dt><dd><p>ACT composite scores may range from 1 - 36.  National norms have a mean of 20. </p>
</dd>
<dt><code>SATV</code></dt><dd><p>SAT Verbal scores may range from 200 - 800. </p>
</dd>
<dt><code>SATQ</code></dt><dd><p>SAT Quantitative scores may range from 200 - 800</p>
</dd>
</dl>



<h3>Details</h3>

<p>hese items were collected as part of the  SAPA  project  (<a href="https://www.sapa-project.org/">https://www.sapa-project.org/</a>)to develop online measures of ability (Revelle, Wilt and Rosenthal, 2009).  The score means are higher than national norms suggesting both self selection for people taking on line personality and ability tests and a self reporting bias in scores.
</p>
<p>See also the iq.items data set.  
</p>


<h3>Source</h3>

<p><a href="https://personality-project.org/">https://personality-project.org/</a> 
</p>


<h3>References</h3>

<p>Revelle, William, Wilt, Joshua,  and Rosenthal, Allen (2009)  Personality and Cognition: The Personality-Cognition Link. In Gruszka, Alexandra  and Matthews, Gerald   and Szymura, Blazej (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control,
Springer. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sat.act)
psych::describe(sat.act)
psych::pairs.panels(sat.act)
</code></pre>

<hr>
<h2 id='Schutz'>
The Schutz correlation matrix example from Shapiro and ten Berge</h2><span id='topic+Schutz'></span>

<h3>Description</h3>

<p>Shapiro and ten Berge use the Schutz correlation matrix as an example for Minimum Rank Factor Analysis.  The Schutz data set is also a nice example of how normal minres or maximum likelihood will lead to a Heywood case, but minrank factoring will  not. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Schutz")</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:9, 1:9] 1 0.8 0.28 0.29 0.41 0.38 0.44 0.4 0.41 0.8 ...
- attr(*, &quot;dimnames&quot;)=List of 2
..$ :1] &quot;Word meaning&quot;   &quot;Odd Words&quot;    &quot;Boots&quot;      &quot;Hatchets&quot;   ...
..$ : chr [1:9] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; ...
</p>


<h3>Details</h3>

<p>These are 9 cognitive variables of importance mainly because they are used as an example by  Shapiro and ten Berge for their paper on Minimum Rank Factor Analysis. 
</p>
<p>The solution from the <code>fa</code> function with the fm='minrank'  option is very close (but not exactly equal) to their solution. 
</p>
<p>This example is used to show problems with different methods of factoring. Of the various factoring methods, fm = &quot;minres&quot;, &quot;uls&quot;, or &quot;mle&quot; produce a Heywood case.  Minrank, alpha, and pa do not. 
</p>
<p>See the blant data set for another example of differences across methods.
</p>


<h3>Source</h3>

<p>Richard E. Schutz,(1958) Factorial Validity of the Holzinger-Crowdeer Uni-factor tests.  Educational and Psychological Measurement, 48, 873-875.
</p>


<h3>References</h3>

<p>Alexander Shapiro and Jos M.F. ten Berge (2002) Statistical inference of minimum rank factor analysis. Psychometrika, 67. 70-94
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Schutz)
psych::corPlot(Schutz,numbers=TRUE,upper=FALSE)

f4min &lt;- psych::fa(Schutz,4,fm="minrank")  #for an example of minimum rank factor Analysis
#compare to
f4 &lt;- psych::fa(Schutz,4,fm="mle")  #for the maximum likelihood solution which has a Heywood case 

</code></pre>

<hr>
<h2 id='selectBy'>Select a subset of rows (subjects) meeting one or more criteria for columns
</h2><span id='topic+selectBy'></span><span id='topic+splitBy'></span>

<h3>Description</h3>

<p>Select a subset of a data.frame or matrix for columns meeting specific criteria.  Can do logical AND (default) or OR  of the resulting search.  Columns (variables) are specified by name and the conditions to meet include equality, less than, more than or inequality to a specified set of values.  SplitBy creates new dichotomous variables based on the splitting criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectBy(x, by)
splitBy(x, by, new=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectBy_+3A_x">x</code></td>
<td>
<p>A data frame or matrix
</p>
</td></tr>
<tr><td><code id="selectBy_+3A_by">by</code></td>
<td>
<p>A quote delimited string of variables and criteria values.  Multiple variables may be separated by commas (default to AND)  </p>
</td></tr>
<tr><td><code id="selectBy_+3A_new">new</code></td>
<td>
<p>If true, return a new data frame with just the dichotomous variables otherwise concatenate the new variables to the right margin of x</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two relatively trivial functions to help those less familiar with the subset function or how to use [] to select variables. 
</p>


<h3>Value</h3>

<p>The subset of the original data.frame with just the cases that meet the criteria (selectBy) or new variables, recoded 0,1
</p>
<p><code><a href="#topic+selectBy">selectBy</a></code>  is equivalent to subsetting x by an x value:  small &lt;- x[x[by=criterion]]  or the subset function  small &lt;- subset(x, x$variable == value)
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vJoin">vJoin</a></code>  for another data manipulation function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>testand &lt;- selectBy(attitude, 'rating &lt; 70 &amp; complaints &gt; 60')  #AND
dim(testand)
testor &lt;- selectBy(attitude, 'rating &lt; 60 | complaints &gt; 60')  #OR
dim(testor)
test &lt;- splitBy(attitude, 'rating &gt; 70 , complaints &gt; 60')  
psych::headTail(test)
</code></pre>

<hr>
<h2 id='Spengler'>Project Talent data set from Marion Spengler and Rodica Damian
</h2><span id='topic+Spengler'></span><span id='topic+spengler'></span><span id='topic+Damian'></span><span id='topic+Spengler.stat'></span>

<h3>Description</h3>

<p>Project Talent gave 440,000 US high school students a number of personality and ability tests. Of these, the data fror 346,000 were available for followup. Subsequent followups were collected 11 and 50 years later.
Marion Spengler and her colleagues Rodica Damian, and Brent Roberts reported on the stability and change across 50 years of personality and ability. Here is the correlation matrix of 25 of their variables (Spengler) as well as a slightly different set of 19 variables (Damian).
This is a nice example of mediation and regression from a correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Damian")</code></pre>


<h3>Format</h3>

<p>A 25 x 25 correlation matrix of demographic, personality, and ability variables, based upon 346,660 participants.
</p>

<dl>
<dt><code>Race/Ethnicity</code></dt><dd><p>1 = other, 2 = white/caucasian</p>
</dd>
<dt><code>Sex</code></dt><dd><p>1=Male, 2=Female</p>
</dd>
<dt><code>Age</code></dt><dd><p>Cohort =9th grade, 10th grade, 11th grade, 12th grade</p>
</dd>
<dt><code>Parental</code></dt><dd><p>Parental SES based upon 9 questions of home value, family income, etc.</p>
</dd>
<dt><code>IQ</code></dt><dd><p>Standardized composite of Verbal, Spatial and Mathematical</p>
</dd>
<dt><code>Sociability etc.</code></dt><dd><p>10 scales based upon prior work by Damian and Roberts</p>
</dd>
<dt><code>Maturity</code></dt><dd><p>A higher order factor from the prior 10 scales</p>
</dd>
<dt><code>Extraversion</code></dt><dd><p>The second higher order factor</p>
</dd>
<dt><code>Interest</code></dt><dd><p>Self reported interest in school</p>
</dd>
<dt><code>Reading</code></dt><dd><p>Self report reading skills</p>
</dd>
<dt><code>Writing</code></dt><dd><p>Self report writing skills </p>
</dd>
<dt><code>Responsible</code></dt><dd><p>Self reported responsibility scale</p>
</dd>
<dt><code>Ed.11</code></dt><dd><p>Education level at 11 year followup</p>
</dd>
<dt><code>Educ.50</code></dt><dd><p>Education level at 50 year followup</p>
</dd>
<dt><code>OccPres.11</code></dt><dd><p>Occupational Prestige at 11 year followup</p>
</dd>
<dt><code>OccPres.50</code></dt><dd><p>Occupational Prestige at 50 year followup</p>
</dd>
<dt><code>Income.11</code></dt><dd><p>Income at 11 year followup</p>
</dd>
<dt><code>Income.50</code></dt><dd><p>Income at 50 year followup</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data from Project Talent was collected in 1960 on a representative sample of American high school students.  Subsequent follow up 11 and 50 years later are reported by Spengler et al (2018) and others.
</p>


<h3>Source</h3>

<p>Marion Spengler, supplementary material to Damian et al. and Spengler et al.
</p>


<h3>References</h3>

<p>Rodica Ioana Damian and Marion Spengler and Andreea Sutu and Brent W. Roberts, 2019, Sixteen going on sixty-six: A longitudinal study of personality stability and change across 50 years 
Journal of Personality and Social Psychology, 117, (3) 274-695.
</p>
<p>Marian Spengler and Rodica Ioana Damian and Brent W. Roberts (2018), How you behave in school predicts life success above and beyond family background, broad traits, and cognitive ability
Journal of Personality and Social Psychology, 114 (4) 600-636
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Damian)
Spengler.stat #show the basic descriptives of the original data set
psych::lowerMat(Spengler[psych::cs(IQ,Parental,Ed.11,OccPres.50),
                        psych::cs(IQ,Parental,Ed.11,OccPres.50)])
psych::setCor(OccPres.50 ~ IQ + Parental + (Ed.11),data=Spengler)
#we reduce the number of subjects for faster replication in this example
mod &lt;- psych::mediate(OccPres.50 ~ IQ + Parental + (Ed.11),data=Spengler,
       n.iter=50,n.obs=1000) #for speed
summary(mod)

</code></pre>

<hr>
<h2 id='spi'>A sample from the SAPA Personality Inventory including an item dictionary and scoring keys.</h2><span id='topic+spi'></span><span id='topic+spi.dictionary'></span><span id='topic+spi.keys'></span>

<h3>Description</h3>

<p>The SPI (SAPA Personality Inventory) is a set of 135 items primarily selected from International Personality Item Pool (ipip.ori.org). This is an example data set collected using SAPA procedures the sapa-project.org web site.  This data set includes 10 demographic variables as well.  The data set with 4000 observations on 145 variables may be used for examples in scale construction and validation, as well as empirical scale construction to predict multiple criteria.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("spi")
data(spi.dictionary)
data(spi.keys)
</code></pre>


<h3>Format</h3>

<p>A data frame with 4000 observations on the following 145 variables. (The q numbers are the SAPA item numbers).
</p>

<dl>
<dt><code>age</code></dt><dd><p>Age in years from 11 -90</p>
</dd>
<dt><code>sex</code></dt><dd><p>Reported biological sex (coded by X chromosones =&gt; 1=Male, 2 = Female)</p>
</dd>
<dt><code>health</code></dt><dd><p>Self rated health  1-5: poor, fair, good, very good, excellent </p>
</dd>
<dt><code>p1edu</code></dt><dd><p>Parent 1 education</p>
</dd>
<dt><code>p2edu</code></dt><dd><p>Parent 2 education</p>
</dd>
<dt><code>education</code></dt><dd><p>Respondents education: less than 12, HS grad, current univ, 
some univ, associate degree, college degree, in grad/prof, grad/prof degree </p>
</dd>
<dt><code>wellness</code></dt><dd><p>Self rated &quot;wellnes&quot; 1-2</p>
</dd>
<dt><code>exer</code></dt><dd><p>Frequency of exercise: very rarely, &lt; 1/month, &lt; 1/wk,
1 or 2 times/week, 3-5/wk, &gt; 5 times/week</p>
</dd>
<dt><code>smoke</code></dt><dd><p>never, not last year, &lt; 1/month, &lt;1/week, 1-3 days/week,
most days, up to 5 x /day, up to 20 x /day,  &gt; 20x/day</p>
</dd>
<dt><code>ER</code></dt><dd><p>Emergency room visits none, 1x, 2x, 3 or more times</p>
</dd>
<dt><code>q_253</code></dt><dd><p> see the spi.dictionary for these items (q_253</p>
</dd>
<dt><code>q_1328</code></dt><dd><p>see the dictionary for all items q_1328)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Using the data contributed by about 125,000 visitors to the <a href="https://www.SAPA-project.org/">https://www.SAPA-project.org/</a> website, David Condon has developed a hierarchical framework for assessing personality at two levels. The higher level has the familiar five factors that have been studied extensively in personality research since the 1980s &ndash; Conscientiousness, Agreeableness, Neuroticism, Openness, and Extraversion. The lower level has 27 factors that are considerably more narrow. These were derived based on administrations of about 700 public-domain IPIP items to 3 large samples. Condon describes these scales as being &quot;empirically-derived&quot; because relatively little theory was used to select the number of factors in the hierarchy and the items in the scale for each factor (to be clear, he means relatively little personality theory though he relied on quite a lot of sampling and statistical theory). You can read all about the procedures used to develop this framework in  his book/manual. If you would like to reproduce these analyses, you can download the data files from Dataverse (links are also provided in the manual) and compile this script in R (he used knitR). Instructions are provided in the Preface to the manual.
</p>
<p>The content of the spi items may be seen by examining the spi.dictionary.  Included in the dictionary are the item_id number from the SAPA project, the wording of the item, the source of the item, which Big 5 scale the item marks, and which &quot;Little 27&quot; scale the item marks.  
</p>
<p>This small subset of the data is provided for demonstration purposes. 
</p>


<h3>Source</h3>

<p>https://sapa-project.org/research/SPI/SPIdevelopment.pdf.
</p>


<h3>References</h3>

<p>Condon, D. (2017) The SAPA Personality Inventory: An empirically-derived, hierarchically-organized self-report personality assessment model (https://psyarxiv.com/sc4p9/)
</p>
<p>An analysis using the spi data set and various tools from the psych package may be found at
</p>
<p>Revelle, Dworak and Condon, (2021)  Exploring the persome: the power of the item in understanding personality structure. Personality and Individual Differences, 169, 1.  Doi: 10.1016/j.paid.2020.109905.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(spi)
data(spi.dictionary)
psych::bestScales(spi, criteria="health",dictionary=spi.dictionary)

sc &lt;- psych::scoreVeryFast(spi.keys,spi) #much faster scoring for just scores
sc &lt;- psych::scoreOverlap(spi.keys,spi)  #gives the alpha reliabilities and various stats 
      #these are corrected for overlap
psych::corPlot(sc$corrected,numbers=TRUE,cex=.4,xlas=2,min.length=6,
     main="Structure of SPI (Corrected for overlap) disattenuated r above the diagonal)")
</code></pre>

<hr>
<h2 id='usaf'>17 anthropometric measures from the USAF showing a general factor</h2><span id='topic+usaf'></span><span id='topic+USAF'></span>

<h3>Description</h3>

<p>The correlation matrix of 17 anthropometric measures from the United States Air Force survey of 2420 airmen.  The data are taken from the Anthropometry package and included here as a demonstration of a hierarchical factor structure suitable for analysis by the <code>omega</code> or <code>omegaSem</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("USAF")</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:17, 1:17] 1 0.1148 -0.0309 -0.028 -0.0908 ...
- attr(*, &quot;dimnames&quot;)=List of 2
..$ : chr [1:17] &quot;age&quot; &quot;weight&quot; &quot;grip&quot; &quot;height&quot; ...
..$ : chr [1:17] &quot;age&quot; &quot;weight&quot; &quot;grip&quot; &quot;height&quot; ...
</p>


<h3>Details</h3>

<p>The original data were collected by the USAF and reported in Churchill et al, 1977.  They are included as a data file of 2420 participants and 202 variables (the first being an id) in the Anthropometry package.  The list of variable names may be found in Churchill et al, on pages 99-103.
</p>
<p>The three (correlated) factor structure shows a clear height, bulk, and head size structure with an overall general factor (g)  which may be interpreted as body size. 
</p>
<p>The variables included (and their variable numbers in Anthropometry) are:
</p>

<table>
<tr>
 <td style="text-align: left;">
age </td><td style="text-align: left;">  V1</td>
</tr>
<tr>
 <td style="text-align: left;">
weight </td><td style="text-align: left;"> V2 </td>
</tr>
<tr>
 <td style="text-align: left;">
grip strength </td><td style="text-align: left;"> V12 </td>
</tr>
<tr>
 <td style="text-align: left;">
height (stature) </td><td style="text-align: left;"> V13 </td>
</tr>
<tr>
 <td style="text-align: left;">
leg length </td><td style="text-align: left;"> V26  </td>
</tr>
<tr>
 <td style="text-align: left;">
knee height </td><td style="text-align: left;"> V37 </td>
</tr>
<tr>
 <td style="text-align: left;">
upper arm </td><td style="text-align: left;">  V42 </td>
</tr>
<tr>
 <td style="text-align: left;">
thumb tip reach </td><td style="text-align: left;"> V47 </td>
</tr>
<tr>
 <td style="text-align: left;">
in sleeve  </td><td style="text-align: left;"> V49 </td>
</tr>
<tr>
 <td style="text-align: left;">
chest breadth </td><td style="text-align: left;">  V52</td>
</tr>
<tr>
 <td style="text-align: left;">
hip breadth </td><td style="text-align: left;">  V55 </td>
</tr>
<tr>
 <td style="text-align: left;">
waist circumference </td><td style="text-align: left;"> V71 </td>
</tr>
<tr>
 <td style="text-align: left;">
thigh circumference </td><td style="text-align: left;"> V97 </td>
</tr>
<tr>
 <td style="text-align: left;">
scye circumference </td><td style="text-align: left;"> V103</td>
</tr>
<tr>
 <td style="text-align: left;">
head circumference </td><td style="text-align: left;"> V141 </td>
</tr>
<tr>
 <td style="text-align: left;">
bitragion coronal </td><td style="text-align: left;"> V145 </td>
</tr>
<tr>
 <td style="text-align: left;">
head length </td><td style="text-align: left;"> V150 </td>
</tr>
<tr>
 <td style="text-align: left;">
glabella to wall </td><td style="text-align: left;">  V181 </td>
</tr>
<tr>
 <td style="text-align: left;">
external canthus to wall </td><td style="text-align: left;"> V183 </td>
</tr>
<tr>
 <td style="text-align: left;">

</td>
</tr>

</table>

<p>Note that these numbers are equivalant to the numbers in Churchill et al. The numbers in Anthropometry are these + 1.
</p>


<h3>Source</h3>

<p>Guillermo Vinue, Anthropometry: An R Package for Analysis of Anthropometric Data, Journal of Statistical Software, (2017), 77, 6.  data set = USAFsurvey</p>


<h3>References</h3>

<p>Edmund Churchill, Thomas Churchill, Paul Kikta (1977) The AMRL anthropmetric data bank library, volumes I-V.  (Technical report AMRL-TR-77-1)  ) https://apps.dtic.mil/dtic/tr/fulltext/u2/a047314.pdf
</p>
<p>Guillermo Vinue, Anthropometry: An R Package for Analysis of Anthropometric Data, Journal of Statistical Software, (2017), 77, 6.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(USAF)
psych::corPlot(USAF,xlas=3)
psych::omega(USAF[c(4:8,10:19),c(4:8,10:19)])   #just the size variables
</code></pre>

<hr>
<h2 id='Utility'>Useful utility functions for file/directory exploration and manipulation.</h2><span id='topic+fileScan'></span><span id='topic+fileCreate'></span><span id='topic+filesList'></span><span id='topic+filesInfo'></span><span id='topic+Utility'></span>

<h3>Description</h3>

<p>Wrappers for  dirname, file.choose, readLines. file.create, file.path to be called directly for listing directories, creating files, showing the files in a directory, and listing the content of files in a directory. <code><a href="#topic+fileCreate">fileCreate</a></code> gives the functionality of <code><a href="base.html#topic+file.choose">file.choose</a></code>(new=TRUE). <code><a href="#topic+filesList">filesList</a></code> combines file.choose, dirname, and list.files to show the files in a directory, <code><a href="#topic+fileScan">fileScan</a></code> extends this and then returns the first few lines of each readable file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fileScan(f = NULL, nlines = 3, max = NULL, from = 1, filter = NULL)
filesList(f=NULL)
filesInfo(f=NULL,max=NULL)
fileCreate(newName="new.file")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Utility_+3A_f">f</code></td>
<td>
<p>File path to use as base path (will use file.choose() if missing.  If f is a 
directory, will list the files in that directory, if f is a file, will find the 
directory for that file and then list all of those files.)
</p>
</td></tr>
<tr><td><code id="Utility_+3A_nlines">nlines</code></td>
<td>
<p>How many lines to display</p>
</td></tr>
<tr><td><code id="Utility_+3A_max">max</code></td>
<td>
<p>maximum number of files to display</p>
</td></tr>
<tr><td><code id="Utility_+3A_from">from</code></td>
<td>
<p>First file (number) to display</p>
</td></tr>
<tr><td><code id="Utility_+3A_filter">filter</code></td>
<td>
<p>Just display files with &quot;filter&quot; in the name</p>
</td></tr>
<tr><td><code id="Utility_+3A_newname">newName</code></td>
<td>
<p>The name of the file to be created.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Just a collection of simple wrappers to powerful core R functions.  Allows the user more direct control of what directory to list, to create a file, or to display the content of files.  The functions called  include <code><a href="base.html#topic+file.choose">file.choose</a></code>, <code><a href="base.html#topic+file.path">file.path</a></code>, <code><a href="base.html#topic+file.info">file.info</a></code>,<code><a href="base.html#topic+file.create">file.create</a></code>, <code><a href="base.html#topic+dirname">dirname</a></code>, and <code><a href="base.html#topic+dir.exists">dir.exists</a></code>.  All of these are very powerful functions, but not easy to call interactively.
</p>
<p><code><a href="#topic+fileCreate">fileCreate</a></code> will ask to locate a file using file.choose, set the directory to that location, and then prompt to create a file with the new.name.  This is a workaround for file.choose(new=TRUE) which only works for Macs not using R.studio.
</p>
<p><code><a href="#topic+filesInfo">filesInfo</a></code> will interactively search for a file and then list the information (size, date, ownership) of all the files in that directory.
</p>
<p><code><a href="#topic+filesList">filesList</a></code> will interactively search for a file and then list all the files in same directory.
</p>


<h3>Note</h3>

<p>Work arounds for core-R functions for interactive file manipulation </p>


<h3>Author(s)</h3>

<p>William Revelle</p>


<h3>See Also</h3>

<p><code><a href="#topic+read.file">read.file</a></code> to read in data from a file or <code><a href="#topic+read.clipboard">read.clipboard</a></code> from the clipboard.  <code><a href="#topic+dfOrder">dfOrder</a></code> to sort data.frames.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

if(interactive()) {
#all of these require interactive input and thus are not given as examples

fileCreate("my.new.file.txt") 
filesList()   #show the items in the directory where a file is displayed
fileScan() #show the content of the files in a directory 
#or, if you have a file in mind
 f &lt;- file.choose()  #go find it
filesList(f)
fileScan(f)
}

</code></pre>

<hr>
<h2 id='vegetables'> Paired comparison of preferences for 9 vegetables</h2><span id='topic+vegetables'></span><span id='topic+veg'></span>

<h3>Description</h3>

<p>A classic data set for demonstrating Thurstonian scaling is the preference matrix of 9 vegetables from Guilford (1954).  Used by Guiford, Nunnally, and Nunally and Bernstein, this data set allows for examples of basic scaling techniques.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(vegetables)</code></pre>


<h3>Format</h3>

<p>A data frame with 9 choices on the following 9 vegetables. The values reflect the perecentage of times where the column entry was preferred over the row entry.
</p>

<dl>
<dt><code>Turn</code></dt><dd><p>Turnips</p>
</dd>
<dt><code>Cab</code></dt><dd><p>Cabbage</p>
</dd>
<dt><code>Beet</code></dt><dd><p>Beets</p>
</dd>
<dt><code>Asp</code></dt><dd><p>Asparagus</p>
</dd>
<dt><code>Car</code></dt><dd><p>Carrots</p>
</dd>
<dt><code>Spin</code></dt><dd><p>Spinach</p>
</dd>
<dt><code>S.Beans</code></dt><dd><p>String Beans</p>
</dd>
<dt><code>Peas</code></dt><dd><p>Peas</p>
</dd>
<dt><code>Corn</code></dt><dd><p>Corn</p>
</dd>
</dl>



<h3>Details</h3>

<p>Louis L. Thurstone was a pioneer in psychometric theory and measurement of attitudes, interests, and abilities.  Among his many contributions was a systematic analysis of the process of comparative judgment (thurstone, 1927).  He considered the case of asking subjects to successively compare pairs of objects. If the same subject does this repeatedly, or if  subjects act as random replicates of each other, their judgments can be thought of as sampled from a normal distribution of underlying (latent) scale  scores for each object, Thurstone  proposed that the comparison between the value of two objects could be represented as representing the differences of the average value for each object compared to the standard deviation of the differences between objects.  The basic model is that each item has a normal distribution of response strength and that choice represents the stronger of the two response strengths.  A justification for the normality assumption is that each decision represents the sum of many independent  inputs and thus, through the central limit theorem, is normally distributed. 
</p>
<p>Thurstone considered five different sets of assumptions about the equality and independence of the variances for each item (Thurston, 1927). Torgerson expanded this analysis slightly by considering three classes of data collection (with individuals, between individuals and mixes of within and between) crossed with three sets of assumptions (equal covariance of decision process, equal correlations and small differences in variance, equal variances).  
</p>
<p>This vegetable data set is used by Guilford and by Nunnally to demonstrate Thurstonian scaling. 
</p>


<h3>Source</h3>

<p>Guilford, J.P. (1954)  Psychometric Methods. McGraw-Hill, New York.
</p>


<h3>References</h3>

<p>Nunnally, J. C. (1967). Psychometric theory., McGraw-Hill, New York.<br />
</p>
<p>Revelle, W. An introduction to psychometric theory with applications in R. (in preparation), Springer. <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>


<h3>See Also</h3>

  <p><code><a href="psych.html#topic+thurstone">thurstone</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vegetables)
psych::thurstone(veg)
</code></pre>

<hr>
<h2 id='vJoin'>Combine two matrices or data frames into one based upon variable labels
</h2><span id='topic+vJoin'></span><span id='topic+combineMatrices'></span>

<h3>Description</h3>

<p>A typical problem in data analysis is to combine two data sets into one. vJoin will combine two matrices or data.frames into one data.frame.  Unique column names from set 1 and set 2 are combined as are unique rows. Column names can differ, as can row names. Will match on rownames or a unique key vector. Basically an extension of rbind and cbind without the requirement of matching column and row names. combineMatrices solves a similar problem for correlation matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vJoin(x, y, rnames = TRUE, cnames=TRUE, key.name= NULL)
combineMatrices(x,y, r=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vJoin_+3A_x">x</code></td>
<td>
<p>a matrix or data frame with column and row names.</p>
</td></tr>
<tr><td><code id="vJoin_+3A_y">y</code></td>
<td>
<p>a matrix or data frame with column and row names</p>
</td></tr>
<tr><td><code id="vJoin_+3A_rnames">rnames</code></td>
<td>
<p>If TRUE, the default, match on row names, extend to new names.  If FALSE then add the y data following the x data.</p>
</td></tr>
<tr><td><code id="vJoin_+3A_cnames">cnames</code></td>
<td>
<p>If TRUE colnames are NULL then create unique colnames for x and y</p>
</td></tr>
<tr><td><code id="vJoin_+3A_key.name">key.name</code></td>
<td>
<p>if NULL, match on rownames, otherwise, match on the values of the key.name column &ndash; must be unique</p>
</td></tr>
<tr><td><code id="vJoin_+3A_r">r</code></td>
<td>
<p>shoule we add the diagonal of y?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For an X and Y matrices/data.frames with column and row names, combine the two data sets.  Match  on column and row names  if they exist, extend to unique names if they do not match.  Can also match on a column in each set (key.name)
</p>
<p>Matrices by default do not have column or rownames.  They will be created for  x and for y  (depending upon the rnames and cnames options).
</p>
<p>combineMatrices takes a square matrix (x) and combines with a rectangular matrix y to produce a larger xy matrix. </p>


<h3>Value</h3>

<p>xy: a data frame</p>


<h3>Note</h3>

<p>Inspired by the functionality of full_join and the other related dplyr functions.
</p>


<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X1 &lt;- bfi[1:10,1:5]
Y1 &lt;- bfi[6:15,4:10]
xy &lt;- vJoin(X1,Y1) #match on rownames
xy1 &lt;- vJoin(X1,Y1,rnames=FALSE) #add Y1 items after X1 items

x &lt;- matrix(1:30, ncol=5)
y &lt;- matrix(1:40, ncol=8)
vJoin(x,y)
vJoin(x,y,cnames=FALSE)
vJoin(x,y, rnames= FALSE, cnames=FALSE)


R &lt;- cor(sat.act,use="pairwise")
r1 &lt;- R[1:4,1:4]
r2 &lt;- R[1:4,5:6] 
newr &lt;- combineMatrices(r1,r2)

</code></pre>

<hr>
<h2 id='zola'>Correlation matrix of 135 self report and 30 peer report personality items</h2><span id='topic+zola'></span><span id='topic+zola.keys'></span><span id='topic+zola.dictionary'></span>

<h3>Description</h3>

<p>Zola et al.,  (2021) reported the validity of self report personality items from the SAPA personality inventory (SPI) (Condon, 2018) in terms of 30 peer reports on 8 dimensions. Here are the polychoric correlations of these items.  spi items  were collected using SAPA procedures for 158,631 participants (mean n/item = 18,180), 908 of whom received peer ratings.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>data("zola")</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:165, 1:165] 1 -0.242 0.282 0.65 0.223 ...
- attr(*, &quot;dimnames&quot;)=List of 2
..$ : chr [1:165] &quot;q_253&quot; &quot;q_4296&quot; &quot;q_1855&quot; &quot;q_90&quot; ...
..$ : chr [1:165] &quot;q_253&quot; &quot;q_4296&quot; &quot;q_1855&quot; &quot;q_90&quot; ...
</p>


<h3>Details</h3>

<p>The polychoric correlation matrix of the spi and peer report data. To see the item labels, use the <code>lookupFromKeys</code> .
</p>
<p>This data set is a nice example of a multi-trait, multi-method correlation matrix. (see the scoring  example).  Five dimensions of self report show high correlations with the corresonding peer report scales.    
</p>


<h3>Source</h3>

<p>A. Zola, D.M. Condon, and W. Revelle, (2021)
</p>


<h3>References</h3>

<p>A. Zola, D.M. Condon, and W. Revelle, (2021) The Convergence of Self and Informant Reports in a Large Online Sample, Collabra: Psychology, 7, 1. doi:  10.1525/collabra.25983
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(zola)
psych::lookupFromKeys(zola.keys,zola.dictionary)
scores &lt;- psych::scoreOverlap(zola.keys[c(1:5,33:37)],zola) #MTMM of Big 5
scores
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
