<!DOCTYPE html><html lang="en"><head><title>Help for package sfarrow</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sfarrow}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#arrow_to_sf'><p>Helper function to convert 'data.frame' to <code>sf</code></p></a></li>
<li><a href='#create_metadata'><p>Create standardised geo metadata for Parquet files</p></a></li>
<li><a href='#encode_wkb'><p>Convert <code>sfc</code> geometry columns into a WKB binary format</p></a></li>
<li><a href='#read_sf_dataset'><p>Read an Arrow multi-file dataset and create <code>sf</code> object</p></a></li>
<li><a href='#sfarrow'><p><code>sfarrow</code>: An R package for reading/writing simple feature (<code>sf</code>)</p>
objects from/to Arrow parquet/feather files with <code>arrow</code></a></li>
<li><a href='#st_read_feather'><p>Read a Feather file to <code>sf</code> object</p></a></li>
<li><a href='#st_read_parquet'><p>Read a Parquet file to <code>sf</code> object</p></a></li>
<li><a href='#st_write_feather'><p>Write <code>sf</code> object to Feather file</p></a></li>
<li><a href='#st_write_parquet'><p>Write <code>sf</code> object to Parquet file</p></a></li>
<li><a href='#validate_metadata'><p>Basic checking of key geo metadata columns</p></a></li>
<li><a href='#write_sf_dataset'><p>Write <code>sf</code> object to an Arrow multi-file dataset</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Read/Write Simple Feature Objects ('sf') with 'Apache' 'Arrow'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-10-25</td>
</tr>
<tr>
<td>Description:</td>
<td>Support for reading/writing simple feature ('sf') spatial objects from/to 'Parquet' files. 'Parquet' files are an open-source, column-oriented data storage format from Apache (<a href="https://parquet.apache.org/">https://parquet.apache.org/</a>), now popular across programming languages. This implementation converts simple feature list geometries into well-known binary format for use by 'arrow', and coordinate reference system information is maintained in a standard metadata format.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/wcjochem/sfarrow">https://github.com/wcjochem/sfarrow</a>,
<a href="https://wcjochem.github.io/sfarrow/">https://wcjochem.github.io/sfarrow/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/wcjochem/sfarrow/issues">https://github.com/wcjochem/sfarrow/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>sf, arrow, jsonlite, dplyr,</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-10-27 16:15:34 UTC; jochem</td>
</tr>
<tr>
<td>Author:</td>
<td>Chris Jochem <a href="https://orcid.org/0000-0003-2192-5988"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Chris Jochem &lt;w.c.jochem@soton.ac.uk&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-10-27 16:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='arrow_to_sf'>Helper function to convert 'data.frame' to <code>sf</code></h2><span id='topic+arrow_to_sf'></span>

<h3>Description</h3>

<p>Helper function to convert 'data.frame' to <code>sf</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arrow_to_sf(tbl, metadata)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="arrow_to_sf_+3A_tbl">tbl</code></td>
<td>
<p><code>data.frame</code> from reading an Arrow dataset</p>
</td></tr>
<tr><td><code id="arrow_to_sf_+3A_metadata">metadata</code></td>
<td>
<p><code>list</code> of validated geo metadata</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of <code>sf</code> with CRS and geometry columns
</p>

<hr>
<h2 id='create_metadata'>Create standardised geo metadata for Parquet files</h2><span id='topic+create_metadata'></span>

<h3>Description</h3>

<p>Create standardised geo metadata for Parquet files
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_metadata(df)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_metadata_+3A_df">df</code></td>
<td>
<p>object of class <code>sf</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Reference for metadata standard:
<a href="https://github.com/geopandas/geo-arrow-spec">https://github.com/geopandas/geo-arrow-spec</a>. This is compatible with
<code>GeoPandas</code> Parquet files.
</p>


<h3>Value</h3>

<p>JSON formatted list with geo-metadata
</p>

<hr>
<h2 id='encode_wkb'>Convert <code>sfc</code> geometry columns into a WKB binary format</h2><span id='topic+encode_wkb'></span>

<h3>Description</h3>

<p>Convert <code>sfc</code> geometry columns into a WKB binary format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>encode_wkb(df)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="encode_wkb_+3A_df">df</code></td>
<td>
<p><code>sf</code> object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Allows for more than one geometry column in <code>sfc</code> format
</p>


<h3>Value</h3>

<p><code>data.frame</code> with binary geometry column(s)
</p>

<hr>
<h2 id='read_sf_dataset'>Read an Arrow multi-file dataset and create <code>sf</code> object</h2><span id='topic+read_sf_dataset'></span>

<h3>Description</h3>

<p>Read an Arrow multi-file dataset and create <code>sf</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_sf_dataset(dataset, find_geom = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_sf_dataset_+3A_dataset">dataset</code></td>
<td>
<p>a <code>Dataset</code> object created by <code>arrow::open_dataset</code>
or an <code>arrow_dplyr_query</code></p>
</td></tr>
<tr><td><code id="read_sf_dataset_+3A_find_geom">find_geom</code></td>
<td>
<p>logical. Only needed when returning a subset of columns.
Should all available geometry columns be selected and added to to the
dataset query without being named? Default is <code>FALSE</code> to require
geometry column(s) to be selected specifically.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is primarily for use after opening a dataset with
<code>arrow::open_dataset</code>. Users can then query the <code>arrow Dataset</code>
using <code>dplyr</code> methods such as <code><a href="dplyr.html#topic+filter">filter</a></code> or
<code><a href="dplyr.html#topic+select">select</a></code>. Passing the resulting query to this function
will parse the datasets and create an <code>sf</code> object. The function
expects consistent geographic metadata to be stored with the dataset in
order to create <code><a href="sf.html#topic+sf">sf</a></code> objects.
</p>


<h3>Value</h3>

<p>object of class <code><a href="sf.html#topic+sf">sf</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="arrow.html#topic+open_dataset">open_dataset</a></code>, <code><a href="sf.html#topic+st_read">st_read</a></code>, <code><a href="#topic+st_read_parquet">st_read_parquet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># read spatial object
nc &lt;- sf::st_read(system.file("shape/nc.shp", package="sf"), quiet = TRUE)

# create random grouping
nc$group &lt;- sample(1:3, nrow(nc), replace = TRUE)

# use dplyr to group the dataset. %&gt;% also allowed
nc_g &lt;- dplyr::group_by(nc, group)

# write out to parquet datasets
tf &lt;- tempfile()  # create temporary location
on.exit(unlink(tf))
# partitioning determined by dplyr 'group_vars'
write_sf_dataset(nc_g, path = tf)

list.files(tf, recursive = TRUE)

# open parquet files from dataset
ds &lt;- arrow::open_dataset(tf)

# create a query. %&gt;% also allowed
q &lt;- dplyr::filter(ds, group == 1)

# read the dataset (piping syntax also works)
nc_d &lt;- read_sf_dataset(dataset = q)

nc_d
plot(sf::st_geometry(nc_d))

</code></pre>

<hr>
<h2 id='sfarrow'><code>sfarrow</code>: An R package for reading/writing simple feature (<code>sf</code>)
objects from/to Arrow parquet/feather files with <code>arrow</code></h2><span id='topic+sfarrow'></span>

<h3>Description</h3>

<p>Simple features are a popular, standardised way to create spatial vector data
with a list-type geometry column. Parquet files are standard column-oriented
files designed by Apache Arrow (<a href="https://parquet.apache.org/">https://parquet.apache.org/</a>) for fast
read/writes. <code>sfarrow</code> is designed to support the reading and writing of
simple features in <code>sf</code> objects from/to Parquet files (.parquet) and
Feather files (.feather) within <code>R</code>. A key goal of <code>sfarrow</code> is to
support interoperability of spatial data in files between <code>R</code> and
<code>Python</code> through the use of standardised metadata.
</p>


<h3>Metadata</h3>

<p>Coordinate reference and geometry field information for <code>sf</code> objects are
stored in standard metadata tables within the files. The metadata are based
on a standard representation (Version 0.1.0, reference:
<a href="https://github.com/geopandas/geo-arrow-spec">https://github.com/geopandas/geo-arrow-spec</a>). This is compatible with
the format used by the Python library <code>GeoPandas</code> for read/writing
Parquet/Feather files. Note to users: this metadata format is not yet stable
for production uses and may change in the future.
</p>


<h3>Credits</h3>

<p>This work was undertaken by Chris Jochem, a member of the WorldPop Research
Group at the University of Southampton(<a href="https://www.worldpop.org/">https://www.worldpop.org/</a>).
</p>

<hr>
<h2 id='st_read_feather'>Read a Feather file to <code>sf</code> object</h2><span id='topic+st_read_feather'></span>

<h3>Description</h3>

<p>Read a Feather file. Uses standard metadata information to
identify geometry columns and coordinate reference system information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>st_read_feather(dsn, col_select = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="st_read_feather_+3A_dsn">dsn</code></td>
<td>
<p>character file path to a data source</p>
</td></tr>
<tr><td><code id="st_read_feather_+3A_col_select">col_select</code></td>
<td>
<p>A character vector of column names to keep. Default is
<code>NULL</code> which returns all columns</p>
</td></tr>
<tr><td><code id="st_read_feather_+3A_...">...</code></td>
<td>
<p>additional parameters to pass to
<code><a href="arrow.html#topic+FeatherReader">FeatherReader</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Reference for the metadata used:
<a href="https://github.com/geopandas/geo-arrow-spec">https://github.com/geopandas/geo-arrow-spec</a>. These are standard with
the Python <code>GeoPandas</code> library.
</p>


<h3>Value</h3>

<p>object of class <code><a href="sf.html#topic+sf">sf</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="arrow.html#topic+read_feather">read_feather</a></code>, <code><a href="sf.html#topic+st_read">st_read</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load Natural Earth low-res dataset.
# Created in Python with GeoPandas.to_feather()
path &lt;- system.file("extdata", package = "sfarrow")

world &lt;- st_read_feather(file.path(path, "world.feather"))

world
plot(sf::st_geometry(world))

</code></pre>

<hr>
<h2 id='st_read_parquet'>Read a Parquet file to <code>sf</code> object</h2><span id='topic+st_read_parquet'></span>

<h3>Description</h3>

<p>Read a Parquet file. Uses standard metadata information to
identify geometry columns and coordinate reference system information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>st_read_parquet(dsn, col_select = NULL, props = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="st_read_parquet_+3A_dsn">dsn</code></td>
<td>
<p>character file path to a data source</p>
</td></tr>
<tr><td><code id="st_read_parquet_+3A_col_select">col_select</code></td>
<td>
<p>A character vector of column names to keep. Default is
<code>NULL</code> which returns all columns</p>
</td></tr>
<tr><td><code id="st_read_parquet_+3A_props">props</code></td>
<td>
<p>Now deprecated in <code><a href="arrow.html#topic+read_parquet">read_parquet</a></code>.</p>
</td></tr>
<tr><td><code id="st_read_parquet_+3A_...">...</code></td>
<td>
<p>additional parameters to pass to
<code><a href="arrow.html#topic+ParquetFileReader">ParquetFileReader</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Reference for the metadata used:
<a href="https://github.com/geopandas/geo-arrow-spec">https://github.com/geopandas/geo-arrow-spec</a>. These are standard with
the Python <code>GeoPandas</code> library.
</p>


<h3>Value</h3>

<p>object of class <code><a href="sf.html#topic+sf">sf</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="arrow.html#topic+read_parquet">read_parquet</a></code>, <code><a href="sf.html#topic+st_read">st_read</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load Natural Earth low-res dataset.
# Created in Python with GeoPandas.to_parquet()
path &lt;- system.file("extdata", package = "sfarrow")

world &lt;- st_read_parquet(file.path(path, "world.parquet"))

world
plot(sf::st_geometry(world))

</code></pre>

<hr>
<h2 id='st_write_feather'>Write <code>sf</code> object to Feather file</h2><span id='topic+st_write_feather'></span>

<h3>Description</h3>

<p>Convert a simple features spatial object from <code>sf</code> and
write to a Feather file using <code><a href="arrow.html#topic+write_feather">write_feather</a></code>. Geometry
columns (type <code>sfc</code>) are converted to well-known binary (WKB) format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>st_write_feather(obj, dsn, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="st_write_feather_+3A_obj">obj</code></td>
<td>
<p>object of class <code><a href="sf.html#topic+sf">sf</a></code></p>
</td></tr>
<tr><td><code id="st_write_feather_+3A_dsn">dsn</code></td>
<td>
<p>data source name. A path and file name with .parquet extension</p>
</td></tr>
<tr><td><code id="st_write_feather_+3A_...">...</code></td>
<td>
<p>additional options to pass to <code><a href="arrow.html#topic+write_feather">write_feather</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>obj</code> invisibly
</p>


<h3>See Also</h3>

<p><code><a href="arrow.html#topic+write_feather">write_feather</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># read spatial object
nc &lt;- sf::st_read(system.file("shape/nc.shp", package="sf"), quiet = TRUE)

# create temp file
tf &lt;- tempfile(fileext = '.feather')
on.exit(unlink(tf))

# write out object
st_write_feather(obj = nc, dsn = tf)

# In Python, read the new file with geopandas.read_feather(...)
# read back into R
nc_f &lt;- st_read_feather(tf)

</code></pre>

<hr>
<h2 id='st_write_parquet'>Write <code>sf</code> object to Parquet file</h2><span id='topic+st_write_parquet'></span>

<h3>Description</h3>

<p>Convert a simple features spatial object from <code>sf</code> and
write to a Parquet file using <code><a href="arrow.html#topic+write_parquet">write_parquet</a></code>. Geometry
columns (type <code>sfc</code>) are converted to well-known binary (WKB) format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>st_write_parquet(obj, dsn, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="st_write_parquet_+3A_obj">obj</code></td>
<td>
<p>object of class <code><a href="sf.html#topic+sf">sf</a></code></p>
</td></tr>
<tr><td><code id="st_write_parquet_+3A_dsn">dsn</code></td>
<td>
<p>data source name. A path and file name with .parquet extension</p>
</td></tr>
<tr><td><code id="st_write_parquet_+3A_...">...</code></td>
<td>
<p>additional options to pass to <code><a href="arrow.html#topic+write_parquet">write_parquet</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>obj</code> invisibly
</p>


<h3>See Also</h3>

<p><code><a href="arrow.html#topic+write_parquet">write_parquet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># read spatial object
nc &lt;- sf::st_read(system.file("shape/nc.shp", package="sf"), quiet = TRUE)

# create temp file
tf &lt;- tempfile(fileext = '.parquet')
on.exit(unlink(tf))

# write out object
st_write_parquet(obj = nc, dsn = tf)

# In Python, read the new file with geopandas.read_parquet(...)
# read back into R
nc_p &lt;- st_read_parquet(tf)

</code></pre>

<hr>
<h2 id='validate_metadata'>Basic checking of key geo metadata columns</h2><span id='topic+validate_metadata'></span>

<h3>Description</h3>

<p>Basic checking of key geo metadata columns
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_metadata(metadata)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_metadata_+3A_metadata">metadata</code></td>
<td>
<p>list for geo metadata</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. Throws an error and stops execution
</p>

<hr>
<h2 id='write_sf_dataset'>Write <code>sf</code> object to an Arrow multi-file dataset</h2><span id='topic+write_sf_dataset'></span>

<h3>Description</h3>

<p>Write <code>sf</code> object to an Arrow multi-file dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_sf_dataset(
  obj,
  path,
  format = "parquet",
  partitioning = dplyr::group_vars(obj),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="write_sf_dataset_+3A_obj">obj</code></td>
<td>
<p>object of class <code><a href="sf.html#topic+sf">sf</a></code></p>
</td></tr>
<tr><td><code id="write_sf_dataset_+3A_path">path</code></td>
<td>
<p>string path referencing a directory for the output</p>
</td></tr>
<tr><td><code id="write_sf_dataset_+3A_format">format</code></td>
<td>
<p>output file format (&quot;parquet&quot; or &quot;feather&quot;)</p>
</td></tr>
<tr><td><code id="write_sf_dataset_+3A_partitioning">partitioning</code></td>
<td>
<p>character vector of columns in <code>obj</code> for grouping or
the <code>dplyr::group_vars</code></p>
</td></tr>
<tr><td><code id="write_sf_dataset_+3A_...">...</code></td>
<td>
<p>additional arguments and options passed to
<code>arrow::write_dataset</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Translate an <code>sf</code> spatial object to <code>data.frame</code> with WKB
geometry columns and then write to an <code>arrow</code> dataset with
partitioning. Allows for <code>dplyr</code> grouped datasets (using
<code><a href="dplyr.html#topic+group_by">group_by</a></code>) and uses those variables to define
partitions.
</p>


<h3>Value</h3>

<p><code>obj</code> invisibly
</p>


<h3>See Also</h3>

<p><code><a href="arrow.html#topic+write_dataset">write_dataset</a></code>, <code><a href="#topic+st_read_parquet">st_read_parquet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># read spatial object
nc &lt;- sf::st_read(system.file("shape/nc.shp", package="sf"), quiet = TRUE)

# create random grouping
nc$group &lt;- sample(1:3, nrow(nc), replace = TRUE)

# use dplyr to group the dataset. %&gt;% also allowed
nc_g &lt;- dplyr::group_by(nc, group)

# write out to parquet datasets
tf &lt;- tempfile()  # create temporary location
on.exit(unlink(tf))
# partitioning determined by dplyr 'group_vars'
write_sf_dataset(nc_g, path = tf)

list.files(tf, recursive = TRUE)

# open parquet files from dataset
ds &lt;- arrow::open_dataset(tf)

# create a query. %&gt;% also allowed
q &lt;- dplyr::filter(ds, group == 1)

# read the dataset (piping syntax also works)
nc_d &lt;- read_sf_dataset(dataset = q)

nc_d
plot(sf::st_geometry(nc_d))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
