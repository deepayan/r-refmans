<!DOCTYPE html><html><head><title>Help for package Kira</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Kira}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Kira-package'><p>Machine learning and data mining.</p></a></li>
<li><a href='#brute.force'><p>Brute force method for variable selection.</p></a></li>
<li><a href='#elbow'><p>Elbow method to determine the optimal number of clusters.</p></a></li>
<li><a href='#hierarchical'><p>Hierarchical unsupervised classification.</p></a></li>
<li><a href='#kmeans'><p>kmeans unsupervised classification.</p></a></li>
<li><a href='#knn'><p>k-nearest neighbor (kNN) supervised classification method</p></a></li>
<li><a href='#lda'><p>Linear discriminant analysis (LDA).</p></a></li>
<li><a href='#plot_curve'><p>Graphics of the results of the classification process</p></a></li>
<li><a href='#qda'><p>Quadratic discriminant analysis (QDA).</p></a></li>
<li><a href='#regression'><p>Linear regression supervised classification method</p></a></li>
<li><a href='#results'><p>Results of the classification process</p></a></li>
<li><a href='#silhouette'><p>Silhouette method to determine the optimal number of clusters.</p></a></li>
<li><a href='#vote'><p>Performs the supervised classification vote method.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Machine Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-07-04</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, grDevices, MASS, stats</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.2)</td>
</tr>
<tr>
<td>Description:</td>
<td>Machine learning, containing several algorithms for supervised and unsupervised classification, in addition to a function that plots the Receiver Operating Characteristic (ROC) and Precision-Recall (PRC) curve graphs, and also a function that returns several metrics used for model evaluation, the latter can be used in ranking results from other packs.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Author:</td>
<td>Paulo Cesar Ossani
    <a href="https://orcid.org/0000-0002-6617-8085"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Paulo Cesar Ossani &lt;ossanipc@hotmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-07-05 14:11:13 UTC; Ossan</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-07-05 14:30:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='Kira-package'>Machine learning and data mining.</h2><span id='topic+Kira-package'></span>

<h3>Description</h3>

<p>Machine learning, containing several algorithms, in addition to functions that plot the graphs of the Receiver Operating Characteristic (ROC) and Precision-Recall (PRC) curve, and also a function that returns several metrics used to evaluate the models, the latter can be used in the classification results of other packages.</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> Kira</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0.5</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-07-04</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL(&gt;= 3)</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>This package contains:
</p>

<ul>
<li><p>Algorithms for supervised classification: knn, linear (lda) and quadratic (qda) discriminant analysis, linear regression, etc.
</p>
</li>
<li><p>Algorithms for unsupervised classification: hierarchical, kmeans, etc.
</p>
</li>
<li><p>A function that plots the ROC and PRC curve.
</p>
</li>
<li><p>A function that returns a series of metrics from models.
</p>
</li>
<li><p>Functions that determine the ideal number of clusters: elbow and silhouette.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Paulo Cesar Ossani &lt;ossanipc@hotmail.com&gt;
</p>


<h3>References</h3>

<p>Aha, D. W.; Kibler, D. and Albert, M. K. Instance-based learning algorithms. <em>Machine learning.</em> v.6, n.1, p.37-66. 1991.
</p>
<p>Anitha, S.; Metilda, M. A. R. Y. An extensive investigation of outlier detection by cluster validation indices. <em>Ciencia e Tecnica Vitivinicola - A Science and Technology Journal</em>, v. 34, n. 2, p. 22-32, 2019. doi: 10.13140/RG.2.2.26801.63848
</p>
<p>Charnet, R. at al. <em>Analise de modelos de regressao lienar,</em> 2a ed. Campinas: Editora da Unicamp, 2008. 357 p.
</p>
<p>Chicco, D.; Warrens, M. J. and Jurman, G. The matthews correlation coefficient (mcc) is more informative than cohen's kappa and brier score in binary classification assessment. <em>IEEE Access, IEEE</em>, v. 9, p. 78368-78381, 2021.
</p>
<p>Erich, S. Stop using the Elbow criterion for k-means and how to choose the number of clusters instead. <em>ACM SIGKDD Explorations Newsletter.</em> 25 (1): 36-42. arXiv:2212.12189. 2023. doi: 10.1145/3606274.3606278
</p>
<p>Ferreira, D. F. <em>Estatistica Multivariada.</em> 2a ed. revisada e ampliada. Lavras: Editora UFLA, 2011. 676 p.
</p>
<p>Kaufman, L. and Rousseeuw, P. J. <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>, New York: John Wiley &amp; Sons. 1990.
</p>
<p>Kittler, J.; Hatef, M.; Duin, R. P. W. and Matas, J. On combining classifiers. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence.</em> 20(3):226-239. 1998. doi: 10.1109/34.667881
</p>
<p>Martinez, W. L.; Martinez, A. R.; Solka, J. <em>Exploratory data analysis with MATLAB</em>. 2nd ed. New York: Chapman &amp; Hall/CRC, 2010. 499 p.
</p>
<p>Mingoti, S. A. <em>analysis de dados atraves de metodos de estatistica multivariada:</em> uma abordagem aplicada. Belo Horizonte: UFMG, 2005. 297 p.
</p>
<p>Nicoletti, M. do C. O modelo de aprendizado de maquina baseado em exemplares: principais caracteristicas e algoritmos. Sao Carlos: EdUFSCar, 2005. 61 p.
</p>
<p>Onumanyi, A. J.; Molokomme, D. N.; Isaac, S. J. and Abu-Mahfouz, A. M. Autoelbow: An automatic elbow detection method for estimating the number of clusters in a dataset. <em>Applied Sciences 12</em>, 15. 2022. doi: 10.3390/app12157515
</p>
<p>Rencher, A. C. <em>Methods of multivariate analysis.</em> 2th. ed. New York: J.Wiley, 2002. 708 p.
</p>
<p>Rencher, A. C. and Schaalje, G. B. <em>Linear models in statisctic.</em> 2th. ed. New Jersey: John &amp; Sons, 2008. 672 p.
</p>
<p>Rousseeuw P. J. Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis. <em>Journal of Computational and Applied Mathematics</em>, 20:53-65. 1987. doi: 10.1016/0377-0427(87)90125-7
</p>
<p>Sugar, C. A. and James, G. M. Finding the number of clusters in a dataset: An information-theoretic approach. <em>Journal of the American Statistical Association</em>, 98, 463, 750-763. 2003. doi: 10.1198/016214503000000666
</p>
<p>Venabless, W. N. and Ripley, B. D. <em>Modern Applied Statistics with S.</em> Fourth edition. Springer, 2002. 
</p>
<p>Zhang, Y.; Mandziuk, J.; Quek, H. C. and Goh, W. Curvature-based method for determining the number of clusters. <em>Inf. Sci.</em> 415, 414-428, 2017. doi: 10.1016/j.ins.2017.05.024
</p>

<hr>
<h2 id='brute.force'>Brute force method for variable selection.</h2><span id='topic+brute.force'></span>

<h3>Description</h3>

<p>Brute force method used to determine the smallest number of variables in a supervised classification model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>brute.force(func = NA, train, test, class.train,
            class.test,  args = NA, measure = "Rate Hits", 
            output = 10)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brute.force_+3A_func">func</code></td>
<td>
<p>Supervised classification function to be analyzed.</p>
</td></tr>
<tr><td><code id="brute.force_+3A_train">train</code></td>
<td>
<p>Data set of training, without classes.</p>
</td></tr>
<tr><td><code id="brute.force_+3A_test">test</code></td>
<td>
<p>Test data set.</p>
</td></tr>
<tr><td><code id="brute.force_+3A_class.train">class.train</code></td>
<td>
<p>Vector with training data class names.</p>
</td></tr>
<tr><td><code id="brute.force_+3A_class.test">class.test</code></td>
<td>
<p>Vector with test data class names.</p>
</td></tr>
<tr><td><code id="brute.force_+3A_args">args</code></td>
<td>
<p>Argument using in the classifier giving in 'func'.</p>
</td></tr> 
<tr><td><code id="brute.force_+3A_measure">measure</code></td>
<td>
<p>Measure to evaluate the model:
&quot;Rate Hits&quot; (default), &quot;Kappa Index&quot;, &quot;Sensitivity&quot;,
&quot;Specificity&quot;, &quot;Precision&quot;, &quot;FP Rate&quot;, &quot;FN Rate&quot;,
&quot;Negative Predictive Rate&quot;, &quot;F-Score&quot;, &quot;MCC&quot;,
&quot;ROC Are&quot; or &quot;PRC Area&quot;.<br />
If measure = NA returns all metrics ordered by 'Rate Hits'.</p>
</td></tr>
<tr><td><code id="brute.force_+3A_output">output</code></td>
<td>
<p>Number of elements with the best combinations of variables in the matrix 'best.model' (default = 10).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>best.model</code></td>
<td>
<p>Matrix with the names of the best combinations of variables, according to the evaluation measure used: accuracy, precision, recall etc.</p>
</td></tr> 
<tr><td><code>text.model</code></td>
<td>
<p>Structure of the classification model used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

data  &lt;- iris
names &lt;- colnames(data)
colnames(data) &lt;- c(names[1:4],"class")

#### Start - hold out validation method ####
dat.sample = sample(2, nrow(data), replace = TRUE, prob = c(0.7,0.3))
data.train = data[dat.sample == 1,] # training data set
data.test  = data[dat.sample == 2,] # test data set
class.train = as.factor(data.train$class) # class names of the training data set
class.test  = as.factor(data.test$class)  # class names of the test data set
#### End - hold out validation method ####

r &lt;- (ncol(data) - 1)

res &lt;- brute.force(func = "knn", train = data.train[,1:r], 
                   test = data.test[,1:r], class.train = class.train,  
                   class.test = class.test, args = "k = 1, dist = 'EUC'", 
                   measure = "Rate Hits", output = 20)
res$best.model
res$text.model

res &lt;- brute.force(func = "regression", train = data.train[,1:r], 
                   test = data.test[,1:r], class.train = class.train, 
                   class.test = class.test, args = "intercept = TRUE", 
                   measure = "Rate Hits", output = 20)
res$best.model
res$text.model

test_a &lt;- as.integer(rownames(data.test)) # test data index
class  &lt;- data[,c(r+1)] # classes names
res &lt;- brute.force(func = "lda", train = data[,1:r], test = test_a, 
                   class.train = class, class.test = class.test, 
                   args = "type = 'test', method = 'mle'", 
                   measure = "Rate Hits", output = 20)
res$best.model 
res$text.model
</code></pre>

<hr>
<h2 id='elbow'>Elbow method to determine the optimal number of clusters.</h2><span id='topic+elbow'></span>

<h3>Description</h3>

<p>Generates the Elbow graph and returns the ideal number of clusters.</p>


<h3>Usage</h3>

<pre><code class='language-R'>elbow(data, k.max = 10, method = "AutoElbow", plot = TRUE, 
      cut = TRUE, title = NA, xlabel = NA, ylabel = NA, size = 1.1,  
      grid = TRUE, color = TRUE, savptc = FALSE, width = 3236, 
      height = 2000, res = 300, casc = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="elbow_+3A_data">data</code></td>
<td>
<p>Data with x and y coordinates.</p>
</td></tr>
<tr><td><code id="elbow_+3A_k.max">k.max</code></td>
<td>
<p>Maximum number of clusters for comparison (default = 10).</p>
</td></tr> 
<tr><td><code id="elbow_+3A_method">method</code></td>
<td>
<p>Method used to find the ideal number k of clusters: &quot;jump&quot;, &quot;curvature&quot;, &quot;Exp&quot;, &quot;AutoElbow&quot; (default).</p>
</td></tr> 
<tr><td><code id="elbow_+3A_plot">plot</code></td>
<td>
<p>Indicates whether to plot the elbow graph (default = TRUE).</p>
</td></tr>
<tr><td><code id="elbow_+3A_cut">cut</code></td>
<td>
<p>Indicates whether to plot the best cluster indicative line (default = TRUE).</p>
</td></tr>
<tr><td><code id="elbow_+3A_title">title</code></td>
<td>
<p>Title of the graphic, if not set, assumes the default text.</p>
</td></tr>
<tr><td><code id="elbow_+3A_xlabel">xlabel</code></td>
<td>
<p>Names the X axis, if not set, assumes the default text.</p>
</td></tr>
<tr><td><code id="elbow_+3A_ylabel">ylabel</code></td>
<td>
<p>Names the Y axis, if not set, assumes the default text.</p>
</td></tr>
<tr><td><code id="elbow_+3A_size">size</code></td>
<td>
<p>Size of points on the graph and line thickness (default = 1.1).</p>
</td></tr>
<tr><td><code id="elbow_+3A_grid">grid</code></td>
<td>
<p>Put grid on graph (default = TRUE).</p>
</td></tr>
<tr><td><code id="elbow_+3A_color">color</code></td>
<td>
<p>Colored graphic (default = TRUE).</p>
</td></tr>
<tr><td><code id="elbow_+3A_savptc">savptc</code></td>
<td>
<p>Saves the graph image to a file (default = FALSE).</p>
</td></tr> 
<tr><td><code id="elbow_+3A_width">width</code></td>
<td>
<p>Graphic image width when savptc = TRUE (defaul = 3236).</p>
</td></tr>
<tr><td><code id="elbow_+3A_height">height</code></td>
<td>
<p>Graphic image height when savptc = TRUE (default = 2000).</p>
</td></tr>
<tr><td><code id="elbow_+3A_res">res</code></td>
<td>
<p>Nominal resolution in ppi of the graphic image when savptc = TRUE (default = 300).</p>
</td></tr>
<tr><td><code id="elbow_+3A_casc">casc</code></td>
<td>
<p>Cascade effect in the presentation of the graphic (default = TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>k.ideal</code></td>
<td>
<p>Ideal number of clusters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani</p>


<h3>References</h3>

<p>Erich, S. Stop using the Elbow criterion for k-means and how to choose the number of clusters instead. <em>ACM SIGKDD Explorations Newsletter.</em> 25 (1): 36-42. arXiv:2212.12189. 2023. doi: 10.1145/3606274.3606278
</p>
<p>Sugar, C. A. and James, G. M. Finding the number of clusters in a dataset: An information-theoretic approach. <em>Journal of the American Statistical Association</em>, 98, 463, 750-763. 2003. doi: 10.1198/016214503000000666
</p>
<p>Zhang, Y.; Mandziuk, J.; Quek, H. C. and Goh, W. Curvature-based method for determining the number of clusters. <em>Inf. Sci.</em> 415, 414-428, 2017. doi: 10.1016/j.ins.2017.05.024
</p>
<p>Onumanyi, A. J.; Molokomme, D. N.; Isaac, S. J. and Abu-Mahfouz, A. M. Autoelbow: An automatic elbow detection method for estimating the number of clusters in a dataset. <em>Applied Sciences 12</em>, 15. 2022. doi: 10.3390/app12157515
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

res &lt;- elbow(data = iris[,1:4], k.max = 20, method = "AutoElbow", cut = TRUE, 
             plot = TRUE, title = NA, xlabel = NA, ylabel = NA, size = 1.1, 
             grid = TRUE, savptc = FALSE, width = 3236, color = TRUE, 
             height = 2000, res = 300, casc = FALSE)
             
res$k.ideal # number of clusters

</code></pre>

<hr>
<h2 id='hierarchical'>Hierarchical unsupervised classification.</h2><span id='topic+hierarchical'></span>

<h3>Description</h3>

<p>Performs hierarchical unsupervised classification analysis in a data set.</p>


<h3>Usage</h3>

<pre><code class='language-R'>hierarchical(data, titles = NA, analysis = "Obs", cor.abs = FALSE,
         normalize = FALSE, distance = "euclidean", method = "complete", 
         horizontal = FALSE, num.groups = 0, lambda = 2, savptc = FALSE, 
         width = 3236, height = 2000, res = 300, casc = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hierarchical_+3A_data">data</code></td>
<td>
<p>Data to be analyzed.</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_titles">titles</code></td>
<td>
<p>Titles of the graphics, if not set, assumes the default text.</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_analysis">analysis</code></td>
<td>
<p>&quot;Obs&quot; for analysis on observations (default), &quot;Var&quot; for analysis on variables.</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_cor.abs">cor.abs</code></td>
<td>
<p>Matrix of absolute correlation case 'analysis' = &quot;Var&quot; (default = FALSE).</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_normalize">normalize</code></td>
<td>
<p>Normalize the data only for case 'analysis' = &quot;Obs&quot; (default = FALSE).</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_distance">distance</code></td>
<td>
<p>Metric of the distances in case of hierarchical groupings: &quot;euclidean&quot; (default), &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot; or &quot;minkowski&quot;. Case Analysis = &quot;Var&quot; the metric will be the correlation matrix, according to cor.abs.</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_method">method</code></td>
<td>
<p>Method for analyzing hierarchical groupings: &quot;complete&quot; (default), &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot; or &quot;centroid&quot;.</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_horizontal">horizontal</code></td>
<td>
<p>Horizontal dendrogram (default = FALSE).</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_num.groups">num.groups</code></td>
<td>
<p>Number of groups to be formed.</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_lambda">lambda</code></td>
<td>
<p>Value used in the minkowski distance.</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_savptc">savptc</code></td>
<td>
<p>Saves graphics images to files (default = FALSE).</p>
</td></tr> 
<tr><td><code id="hierarchical_+3A_width">width</code></td>
<td>
<p>Graphics images width when savptc = TRUE (defaul = 3236).</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_height">height</code></td>
<td>
<p>Graphics images height when savptc = TRUE (default = 2000).</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_res">res</code></td>
<td>
<p>Nominal resolution in ppi of the graphics images when savptc = TRUE (default = 300).</p>
</td></tr>
<tr><td><code id="hierarchical_+3A_casc">casc</code></td>
<td>
<p>Cascade effect in the presentation of the graphics (default = TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Several graphics.
</p>
<table>
<tr><td><code>tab.res</code></td>
<td>
<p>Table with similarities and distances of the groups formed.</p>
</td></tr>
<tr><td><code>groups</code></td>
<td>
<p>Original data with groups formed.</p>
</td></tr>
<tr><td><code>res.groups</code></td>
<td>
<p>Results of the groups formed.</p>
</td></tr>
<tr><td><code>R.sqt</code></td>
<td>
<p>Result of the R squared.</p>
</td></tr>
<tr><td><code>sum.sqt</code></td>
<td>
<p>Total sum of squares.</p>
</td></tr>
<tr><td><code>mtx.dist</code></td>
<td>
<p>Matrix of the distances.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani
</p>


<h3>References</h3>

<p>Rencher, A. C. <em>Methods of multivariate analysis.</em> 2th. ed. New York: J.Wiley, 2002. 708 p.
</p>
<p>Mingoti, S. A. <em>analysis de dados atraves de metodos de estatistica multivariada:</em> uma abordagem aplicada. Belo Horizonte: UFMG, 2005. 297 p.
</p>
<p>Ferreira, D. F. <em>Estatistica Multivariada.</em> 2a ed. revisada e ampliada. Lavras: Editora UFLA, 2011. 676 p.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

data &lt;- iris

res &lt;- hierarchical(data[,1:4], titles = NA, analysis = "Obs", cor.abs = FALSE, 
            normalize = FALSE, distance = "euclidean", method = "ward.D", 
            horizontal = FALSE, num.groups = 3, savptc = FALSE, width = 3236, 
            height = 2000, res = 300, casc = FALSE)
      
message("R squared: ", res$R.sqt)     
# message("Total sum of squares: ", res$sum.sqt)
message("Groups formed: "); res$groups
# message("Table with similarities and distances:"); res$tab.res
# message("Table with the results of the groups:"); res$res.groups
# message("Distance Matrix:"); res$mtx.dist

#write.table(file=file.path(tempdir(),"GroupData.csv"), res$groups, sep=";",
#            dec=",",row.names = TRUE)
</code></pre>

<hr>
<h2 id='kmeans'>kmeans unsupervised classification.</h2><span id='topic+kmeans'></span>

<h3>Description</h3>

<p>Performs kmeans unsupervised classification analysis in a data set.</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeans(data, normalize = FALSE, num.groups = 2)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmeans_+3A_data">data</code></td>
<td>
<p>Data to be analyzed.</p>
</td></tr>
<tr><td><code id="kmeans_+3A_normalize">normalize</code></td>
<td>
<p>Normalize the data (default = FALSE).</p>
</td></tr>
<tr><td><code id="kmeans_+3A_num.groups">num.groups</code></td>
<td>
<p>Number of groups to be formed (default = 2).</p>
</td></tr>
</table>


<h3>Value</h3>

 
<table>
<tr><td><code>groups</code></td>
<td>
<p>Original data with groups formed.</p>
</td></tr>
<tr><td><code>res.groups</code></td>
<td>
<p>Results of the groups formed.</p>
</td></tr>
<tr><td><code>R.sqt</code></td>
<td>
<p>Result of the R squared.</p>
</td></tr>
<tr><td><code>sum.sqt</code></td>
<td>
<p>Total sum of squares.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani
</p>


<h3>References</h3>

<p>Rencher, A. C. <em>Methods of multivariate analysis.</em> 2th. ed. New York: J.Wiley, 2002. 708 p.
</p>
<p>Mingoti, S. A. <em>analysis de dados atraves de metodos de estatistica multivariada:</em> uma abordagem aplicada. Belo Horizonte: UFMG, 2005. 297 p.
</p>
<p>Ferreira, D. F. <em>Estatistica Multivariada.</em> 2a ed. revisada e ampliada. Lavras: Editora UFLA, 2011. 676 p.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

data &lt;- iris

res &lt;- kmeans(data[,1:4], normalize = FALSE, num.groups = 3)
 
message("R squared: ", res$R.sqt)             
# message("Total sum of squares: ", res$sum.sqt)
message("Groups formed:"); res$groups
# message("Table with the results of the groups:"); res$res.groups

#write.table(file=file.path(tempdir(),"GroupData.csv"), res$groups, sep=";",
#            dec=",",row.names = TRUE)
</code></pre>

<hr>
<h2 id='knn'>k-nearest neighbor (kNN) supervised classification method</h2><span id='topic+knn'></span>

<h3>Description</h3>

<p>Performs the k-nearest neighbor (kNN) supervised classification method.</p>


<h3>Usage</h3>

<pre><code class='language-R'>knn(train, test, class, k = 1, dist = "euclidean", lambda = 3)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knn_+3A_train">train</code></td>
<td>
<p>Data set of training, without classes.</p>
</td></tr>
<tr><td><code id="knn_+3A_test">test</code></td>
<td>
<p>Test data set.</p>
</td></tr>
<tr><td><code id="knn_+3A_class">class</code></td>
<td>
<p>Vector with training data class names</p>
</td></tr>
<tr><td><code id="knn_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors (default = 1).</p>
</td></tr>
<tr><td><code id="knn_+3A_dist">dist</code></td>
<td>
<p>Distances used in the method: &quot;euclidean&quot; (default), &quot;manhattan&quot;, &quot;minkowski&quot;, &quot;canberra&quot;, &quot;maximum&quot; or &quot;chebyshev&quot;.</p>
</td></tr>
<tr><td><code id="knn_+3A_lambda">lambda</code></td>
<td>
<p>Value used in the minkowski distance (default = 3).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>predict</code></td>
<td>
<p>The classified factors of the test set.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani</p>


<h3>References</h3>

<p>Aha, D. W.; Kibler, D. and Albert, M. K. Instance-based learning algorithms. <em>Machine learning.</em> v.6, n.1, p.37-66. 1991.
</p>
<p>Nicoletti, M. do C. O modelo de aprendizado de maquina baseado em exemplares: principais caracteristicas e algoritmos. Sao Carlos: EdUFSCar, 2005. 61 p.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot_curve">plot_curve</a></code> and <code><a href="#topic+results">results</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

data  &lt;- iris
names &lt;- colnames(data)
colnames(data) &lt;- c(names[1:4],"class")

#### Start - hold out validation method ####
dat.sample = sample(2, nrow(data), replace = TRUE, prob = c(0.7,0.3))
data.train = data[dat.sample == 1,] # training data set
data.test  = data[dat.sample == 2,] # test data set
class.train = as.factor(data.train$class) # class names of the training data set
class.test  = as.factor(data.test$class)  # class names of the test data set
#### End - hold out validation method ####


dist = "euclidean" 
# dist = "manhattan"
# dist = "minkowski"
# dist = "canberra"
# dist = "maximum"
# dist = "chebyshev"

k = 1
lambda = 5

r &lt;- (ncol(data) - 1)
res &lt;- knn(train = data.train[,1:r], test = data.test[,1:r], class = class.train, 
           k = 1, dist = dist, lambda = lambda)

resp &lt;- results(orig.class = class.test, predict = res$predict)

message("Mean squared error:"); resp$mse
message("Mean absolute error:"); resp$mae
message("Relative absolute error:"); resp$rae
message("Confusion matrix:"); resp$conf.mtx  
message("Hit rate: ", resp$rate.hits)
message("Error rate: ", resp$rate.error)
message("Number of correct instances: ", resp$num.hits)
message("Number of wrong instances: ", resp$num.error)
message("Kappa coefficient: ", resp$kappa)
message("General results of the classes:"); resp$res.class

</code></pre>

<hr>
<h2 id='lda'>Linear discriminant analysis (LDA).</h2><span id='topic+lda'></span>

<h3>Description</h3>

<p>Perform linear discriminant analysis.</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda(data, test = NA, class = NA, type = "train", 
   method = "moment", prior = NA)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_+3A_data">data</code></td>
<td>
<p>Data to be classified.</p>
</td></tr>
<tr><td><code id="lda_+3A_test">test</code></td>
<td>
<p>Vector with indices that will be used in 'data' as test. For type = &quot;train&quot;, one has test = NA.</p>
</td></tr>
<tr><td><code id="lda_+3A_class">class</code></td>
<td>
<p>Vector with data classes names.</p>
</td></tr>
<tr><td><code id="lda_+3A_type">type</code></td>
<td>
<p>Type of type:<br />
&quot;train&quot; - data training (default), or<br />
&quot;test&quot; - classifies the data of the vector &quot;test&quot;.</p>
</td></tr>
<tr><td><code id="lda_+3A_method">method</code></td>
<td>
<p>Classification method:<br /> 
&quot;mle&quot; to MLEs,<br />
&quot;mve&quot; to use cov.mv, <br />
&quot;moment&quot; (default) for standard mean and variance estimators, or <br />
&quot;t&quot; for robust estimates based on the t distribution.</p>
</td></tr>
<tr><td><code id="lda_+3A_prior">prior</code></td>
<td>
<p>Probabilities of occurrence of classes. If not specified, it will take the proportions of the classes. If specified, probabilities must follow the order of factor levels.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>predict</code></td>
<td>
<p>The classified factors of the set.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani
</p>


<h3>References</h3>

<p>Rencher, A. C. <em>Methods of multivariate analysis.</em> 2th. ed. New York: J.Wiley, 2002. 708 p.
</p>
<p>Venabless, W. N. and Ripley, B. D. <em>Modern Applied Statistics with S.</em> Fourth edition. Springer, 2002. 
</p>
<p>Mingoti, S. A. <em>Analise de dados atraves de metodos de estatistica multivariada:</em> uma abordagem aplicada. Belo Horizonte: UFMG, 2005. 297 p.
</p>
<p>Ferreira, D. F. <em>Estatistica Multivariada.</em> 2a ed. revisada e ampliada. Lavras: Editora UFLA, 2011. 676 p.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot_curve">plot_curve</a></code> and <code><a href="#topic+results">results</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

data  &lt;- iris
names &lt;- colnames(data)
colnames(data) &lt;- c(names[1:4],"class")

#### Start - hold out validation method ####
dat.sample = sample(2, nrow(data), replace = TRUE, prob = c(0.7,0.3))
data.train = data[dat.sample == 1,] # training data set
data.test  = data[dat.sample == 2,] # test data set
class.train = as.factor(data.train$class) # class names of the training data set
class.test  = as.factor(data.test$class)  # class names of the test data set
#### End - hold out validation method ####

r &lt;- (ncol(data) - 1)
class &lt;- data[,c(r+1)] # classes names

## Data training example
res &lt;- lda(data = data[,1:r], test = NA, class = class, 
           type = "train", method = "moment", prior = NA)

resp &lt;- results(orig.class = class, predict = res$predict)

message("Mean squared error:"); resp$mse
message("Mean absolute error:"); resp$mae
message("Relative absolute error:"); resp$rae
message("Confusion matrix:"); resp$conf.mtx  
message("Hit rate: ", resp$rate.hits)
message("Error rate: ", resp$rate.error)
message("Number of correct instances: ", resp$num.hits)
message("Number of wrong instances: ", resp$num.error)
message("Kappa coefficient: ", resp$kappa)
message("General results of the classes:"); resp$res.class 


## Data test example
class.table &lt;- table(class) # table with the number of elements per class
prior &lt;- as.double(class.table/sum(class.table))
test = as.integer(rownames(data.test)) # test data index

res &lt;- lda(data = data[,1:r], test = test, class = class, 
           type = "test", method = "mle", prior = prior)

resp &lt;- results(orig.class = class.test, predict = res$predict)

message("Mean squared error:"); resp$mse
message("Mean absolute error:"); resp$mae
message("Relative absolute error:"); resp$rae
message("Confusion matrix: "); resp$conf.mtx  
message("Hit rate: ", resp$rate.hits)
message("Error rate: ", resp$rate.error)
message("Number of correct instances: ", resp$num.hits)
message("Number of wrong instances: ", resp$num.error)
message("Kappa coefficient: ", resp$kappa)
message("General results of the classes:"); resp$res.class  

</code></pre>

<hr>
<h2 id='plot_curve'>Graphics of the results of the classification process</h2><span id='topic+plot_curve'></span>

<h3>Description</h3>

<p>Return graphics of the results of the classification process.</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_curve(data, type = "ROC", title = NA, xlabel = NA, ylabel = NA,  
           posleg = 3, boxleg = FALSE, axis = TRUE, size = 1.1, grid = TRUE, 
           color = TRUE, classcolor = NA, savptc = FALSE, width = 3236, 
           height = 2000, res = 300, casc = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_curve_+3A_data">data</code></td>
<td>
<p>Data with x and y coordinates.</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_type">type</code></td>
<td>
<p>ROC (default) or PRC graphics type.</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_title">title</code></td>
<td>
<p>Title of the graphic, if not set, assumes the default text.</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_xlabel">xlabel</code></td>
<td>
<p>Names the X axis, if not set, assumes the default text.</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_ylabel">ylabel</code></td>
<td>
<p>Names the Y axis, if not set, assumes the default text.</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_posleg">posleg</code></td>
<td>
<p>0 with no caption,<br />
1 for caption in the left upper corner,<br />
2 for caption in the right upper corner,<br />
3 for caption in the right lower corner (default),<br />
4 for caption in the left lower corner.</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_boxleg">boxleg</code></td>
<td>
<p>Puts the frame in the caption (default = TRUE).</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_axis">axis</code></td>
<td>
<p>Put the diagonal axis on the graph (default = TRUE).</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_size">size</code></td>
<td>
<p>Size of the points in the graphs (default = 1.1).</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_grid">grid</code></td>
<td>
<p>Put grid on graphs (default = TRUE).</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_color">color</code></td>
<td>
<p>Colored graphics (default = TRUE).</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_classcolor">classcolor</code></td>
<td>
<p>Vector with the colors of the classes.</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_savptc">savptc</code></td>
<td>
<p>Saves graphics images to files (default = FALSE).</p>
</td></tr> 
<tr><td><code id="plot_curve_+3A_width">width</code></td>
<td>
<p>Graphics images width when savptc = TRUE (defaul = 3236).</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_height">height</code></td>
<td>
<p>Graphics images height when savptc = TRUE (default = 2000).</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_res">res</code></td>
<td>
<p>Nominal resolution in ppi of the graphics images when savptc = TRUE (default = 300).</p>
</td></tr>
<tr><td><code id="plot_curve_+3A_casc">casc</code></td>
<td>
<p>Cascade effect in the presentation of the graphic (default = TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ROC or PRC curve.</p>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani</p>


<h3>See Also</h3>

<p><code><a href="#topic+results">results</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

data  &lt;- iris
names &lt;- colnames(data)
colnames(data) &lt;- c(names[1:4],"class")

#### Start - hold out validation method ####
dat.sample = sample(2, nrow(data), replace = TRUE, prob = c(0.7,0.3))
data.train = data[dat.sample == 1,] # training data set
data.test  = data[dat.sample == 2,] # test data set
class.train = as.factor(data.train$class) # class names of the training data set
class.test  = as.factor(data.test$class)  # class names of the test data set
#### End - hold out validation method ####


dist = "euclidean" 
# dist = "manhattan"
# dist = "minkowski"
# dist = "canberra"
# dist = "maximum"
# dist = "chebyshev"

k = 1
lambda = 5

r &lt;- (ncol(data) - 1)
res &lt;- knn(train = data.train[,1:r], test = data.test[,1:r], class = class.train, 
           k = 1, dist = dist, lambda = lambda)

resp &lt;- results(orig.class = class.test, predict = res$predict)

message("Mean squared error:"); resp$mse
message("Mean absolute error:"); resp$mae
message("Relative absolute error:"); resp$rae
message("Confusion matrix:"); resp$conf.mtx  
message("Hit rate: ", resp$rate.hits)
message("Error rate: ", resp$rate.error)
message("Number of correct instances: ", resp$num.hits)
message("Number of wrong instances: ", resp$num.error)
message("Kappa coefficient: ", resp$kappa)
# message("Data for the ROC curve in classes:"); resp$roc.curve 
# message("Data for the PRC curve in classes:"); resp$prc.curve
message("General results of the classes:"); resp$res.class

dat &lt;- resp$roc.curve; tp = "roc"; ps = 3
# dat &lt;- resp$prc.curve; tp = "prc"; ps = 4

plot_curve(data = dat, type = tp, title = NA, xlabel = NA, ylabel = NA,  
           posleg = ps, boxleg = FALSE, axis = TRUE, size = 1.1, grid = TRUE, 
           color = TRUE, classcolor = NA, savptc = FALSE,
           width = 3236, height = 2000, res = 300, casc = FALSE)

</code></pre>

<hr>
<h2 id='qda'>Quadratic discriminant analysis (QDA).</h2><span id='topic+qda'></span>

<h3>Description</h3>

<p>Perform quadratic discriminant analysis.</p>


<h3>Usage</h3>

<pre><code class='language-R'>qda(data, test = NA, class = NA, type = "train",
   method = "moment", prior = NA)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qda_+3A_data">data</code></td>
<td>
<p>Data to be classified.</p>
</td></tr>
<tr><td><code id="qda_+3A_test">test</code></td>
<td>
<p>Vector with indices that will be used in 'data' as test. For type = &quot;train&quot;, one has test = NA.</p>
</td></tr>
<tr><td><code id="qda_+3A_class">class</code></td>
<td>
<p>Vector with data classes names.</p>
</td></tr>
<tr><td><code id="qda_+3A_type">type</code></td>
<td>
<p>Type of type:<br />
&quot;train&quot; - data training (default), or<br />
&quot;test&quot; - classifies the data of the vector &quot;test&quot;.</p>
</td></tr>
<tr><td><code id="qda_+3A_method">method</code></td>
<td>
<p>Classification method:<br /> 
&quot;mle&quot; to MLEs,<br />
&quot;mve&quot; to use cov.mv, <br />
&quot;moment&quot; (default) for standard mean and variance estimators, or <br />
&quot;t&quot; for robust estimates based on the t distribution.</p>
</td></tr>
<tr><td><code id="qda_+3A_prior">prior</code></td>
<td>
<p>Probabilities of occurrence of classes. If not specified, it will take the proportions of the classes. If specified, probabilities must follow the order of factor levels.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>predict</code></td>
<td>
<p>The classified factors of the set.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani
</p>


<h3>References</h3>

<p>Rencher, A. C. <em>Methods of multivariate analysis.</em> 2th. ed. New York: J.Wiley, 2002. 708 p.
</p>
<p>Venabless, W. N. and Ripley, B. D. <em>Modern Applied Statistics with S.</em> Fourth edition. Springer, 2002. 
</p>
<p>Mingoti, S. A. <em>Analise de dados atraves de metodos de estatistica multivariada:</em> uma abordagem aplicada. Belo Horizonte: UFMG, 2005. 297 p.
</p>
<p>Ferreira, D. F. <em>Estatistica Multivariada.</em> 2a ed. revisada e ampliada. Lavras: Editora UFLA, 2011. 676 p.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot_curve">plot_curve</a></code> and <code><a href="#topic+results">results</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

data  &lt;- iris
names &lt;- colnames(data)
colnames(data) &lt;- c(names[1:4],"class")

#### Start - hold out validation method ####
dat.sample = sample(2, nrow(data), replace = TRUE, prob = c(0.7,0.3))
data.train = data[dat.sample == 1,] # training data set
data.test  = data[dat.sample == 2,] # test data set
class.train = as.factor(data.train$class) # class names of the training data set
class.test  = as.factor(data.test$class)  # class names of the test data set
#### End - hold out validation method ####

r &lt;- (ncol(data) - 1)
class &lt;- data[,c(r+1)] # classes names

## Data training example
res &lt;- qda(data = data[,1:r], test = NA, class = class, 
           type = "train", method = "moment", prior = NA)

resp &lt;- results(orig.class = class, predict = res$predict)

message("Mean Squared Error:"); resp$mse
message("Mean absolute error:"); resp$mae
message("Relative absolute error:"); resp$rae
message("Confusion matrix: "); resp$conf.mtx  
message("Hit rate: ", resp$rate.hits)
message("Error rate: ", resp$rate.error)
message("Number of correct instances: ", resp$num.hits)
message("Number of wrong instances: ", resp$num.error)
message("Kappa coefficient: ", resp$kappa)
message("General results of the classes:"); resp$res.class  


## Data test example
class.table &lt;- table(class) # table with the number of elements per class
prior &lt;- as.double(class.table/sum(class.table))
test = as.integer(rownames(data.test)) # test data index

res &lt;- qda(data = data[,1:r], test = test, class = class, 
           type = "test", method = "mle", prior = prior)

resp &lt;- results(orig.class = class.test, predic = res$predict)

message("Mean squared error:"); resp$mse
message("Mean absolute error:"); resp$mae
message("Relative absolute error:"); resp$rae
message("Confusion matrix: "); resp$conf.mtx  
message("Hit rate: ", resp$rate.hits)
message("Error rate: ", resp$rate.error)
message("Number of correct instances: ", resp$num.hits)
message("Number of wrong instances: ", resp$num.error)
message("Kappa coefficient: ", resp$kappa)
message("General results of the classes:"); resp$res.class  

</code></pre>

<hr>
<h2 id='regression'>Linear regression supervised classification method</h2><span id='topic+regression'></span>

<h3>Description</h3>

<p>Performs supervised classification using the linear regression method.</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression(train, test, class, intercept = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regression_+3A_train">train</code></td>
<td>
<p>Data set of training, without classes.</p>
</td></tr>
<tr><td><code id="regression_+3A_test">test</code></td>
<td>
<p>Test data set.</p>
</td></tr>
<tr><td><code id="regression_+3A_class">class</code></td>
<td>
<p>Vector with training data class names.</p>
</td></tr>
<tr><td><code id="regression_+3A_intercept">intercept</code></td>
<td>
<p>Consider the intercept in the regression (default = TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>predict</code></td>
<td>
<p>The classified factors of the test set.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani</p>


<h3>References</h3>

<p>Charnet, R. at al. <em>Analise de modelos de regressao lienar,</em> 2a ed. Campinas: Editora da Unicamp, 2008. 357 p.
</p>
<p>Rencher, A. C. and Schaalje, G. B. <em>Linear models in statisctic.</em> 2th. ed. New Jersey: John &amp; Sons, 2008. 672 p.
</p>
<p>Rencher, A. C. <em>Methods of multivariate analysis.</em> 2th. ed. New York: J.Wiley, 2002. 708 p.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot_curve">plot_curve</a></code> and <code><a href="#topic+results">results</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

data  &lt;- iris
names &lt;- colnames(data)
colnames(data) &lt;- c(names[1:4],"class")

#### Start - hold out validation method ####
dat.sample = sample(2, nrow(data), replace = TRUE, prob = c(0.7,0.3))
data.train = data[dat.sample == 1,] # training data set
data.test  = data[dat.sample == 2,] # test data set
class.train = as.factor(data.train$class) # class names of the training data set
class.test  = as.factor(data.test$class)  # class names of the test data set
#### End - hold out validation method ####

r &lt;- (ncol(data) - 1)
res &lt;- regression(train = data.train[,1:r], test = data.test[,1:r], 
                  class = class.train, intercept = TRUE)

resp &lt;- results(orig.class = class.test, predict = res$predict)

message("Mean squared error:"); resp$mse
message("Mean absolute error:"); resp$mae
message("Relative absolute error:"); resp$rae
message("Confusion matrix:"); resp$conf.mtx  
message("Hit rate: ", resp$rate.hits)
message("Error rate: ", resp$rate.error)
message("Number of correct instances: ", resp$num.hits)
message("Number of wrong instances: ", resp$num.error)
message("Kappa coefficient: ", resp$kappa)
message("General results of the classes:"); resp$res.class

</code></pre>

<hr>
<h2 id='results'>Results of the classification process</h2><span id='topic+results'></span>

<h3>Description</h3>

<p>Returns the results of the classification process.</p>


<h3>Usage</h3>

<pre><code class='language-R'>results(orig.class, predict)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="results_+3A_orig.class">orig.class</code></td>
<td>
<p>Data with the original classes.</p>
</td></tr>
<tr><td><code id="results_+3A_predict">predict</code></td>
<td>
<p>Data with classes of results of classifiers.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>mse</code></td>
<td>
<p>Mean squared error.</p>
</td></tr>
<tr><td><code>mae</code></td>
<td>
<p>Mean absolute error.</p>
</td></tr> 
<tr><td><code>rae</code></td>
<td>
<p>Relative absolute error.</p>
</td></tr>
<tr><td><code>conf.mtx</code></td>
<td>
<p>Confusion matrix.</p>
</td></tr>
<tr><td><code>rate.hits</code></td>
<td>
<p>Hit rate.</p>
</td></tr>
<tr><td><code>rate.error</code></td>
<td>
<p>Error rate.</p>
</td></tr>
<tr><td><code>num.hits</code></td>
<td>
<p>Number of correct instances.</p>
</td></tr>
<tr><td><code>num.error</code></td>
<td>
<p>Number of wrong instances.</p>
</td></tr>
<tr><td><code>kappa</code></td>
<td>
<p>Kappa coefficient.</p>
</td></tr>
<tr><td><code>roc.curve</code></td>
<td>
<p>Data for the ROC curve in classes.</p>
</td></tr>
<tr><td><code>prc.curve</code></td>
<td>
<p>Data for the PRC curve in classes.</p>
</td></tr>
<tr><td><code>res.class</code></td>
<td>
<p>General results of the classes: Sensitivity, Specificity, Precision, TP Rate, FP Rate, NP Rate, F-Score, MCC, ROC Area, PRC Area.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani</p>


<h3>References</h3>

<p>Chicco, D.; Warrens, M. J. and Jurman, G. The matthews correlation coefficient (mcc) is more informative than cohen's kappa and brier score in binary classification assessment. <em>IEEE Access, IEEE</em>, v. 9, p. 78368-78381, 2021.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot_curve">plot_curve</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

data  &lt;- iris
names &lt;- colnames(data)
colnames(data) &lt;- c(names[1:4],"class")

#### Start - hold out validation method ####
dat.sample = sample(2, nrow(data), replace = TRUE, prob = c(0.7,0.3))
data.train = data[dat.sample == 1,] # training data set
data.test  = data[dat.sample == 2,] # test data set
class.train = as.factor(data.train$class) # class names of the training data set
class.test  = as.factor(data.test$class)  # class names of the test data set
#### End - hold out validation method ####


dist = "euclidean" 
# dist = "manhattan"
# dist = "minkowski"
# dist = "canberra"
# dist = "maximum"
# dist = "chebyshev"

k = 1
lambda = 5

r &lt;- (ncol(data) - 1)
res &lt;- knn(train = data.train[,1:r], test = data.test[,1:r], class = class.train, 
           k = 1, dist = dist, lambda = lambda)

resp &lt;- results(orig.class = class.test, predict = res$predict)

message("Mean squared error:"); resp$mse
message("Mean absolute error:"); resp$mae
message("Relative absolute error:"); resp$rae
message("Confusion matrix:"); resp$conf.mtx  
message("Hit rate: ", resp$rate.hits)
message("Error rate: ", resp$rate.error)
message("Number of correct instances: ", resp$num.hits)
message("Number of wrong instances: ", resp$num.error)
message("Kappa coefficient: ", resp$kappa)
# message("Data for the ROC curve in classes:"); resp$roc.curve 
# message("Data for the PRC curve in classes:"); resp$prc.curve
message("General results of the classes:"); resp$res.class

dat &lt;- resp$roc.curve; tp = "roc"; ps = 3
# dat &lt;- resp$prc.curve; tp = "prc"; ps = 4

plot_curve(data = dat, type = tp, title = NA, xlabel = NA, ylabel = NA,  
           posleg = ps, boxleg = FALSE, axis = TRUE, size = 1.1, grid = TRUE, 
           color = TRUE, classcolor = NA, savptc = FALSE, width = 3236, 
           height = 2000, res = 300, casc = FALSE)

</code></pre>

<hr>
<h2 id='silhouette'>Silhouette method to determine the optimal number of clusters.</h2><span id='topic+silhouette'></span>

<h3>Description</h3>

<p>Generates the silhouette graph and returns the ideal number of clusters in the k-means method.</p>


<h3>Usage</h3>

<pre><code class='language-R'>silhouette(data, k.cluster = 2:10, plot = TRUE, cut = TRUE,
           title = NA, xlabel = NA, ylabel = NA, size = 1.1, grid = TRUE, 
           color = TRUE, savptc = FALSE, width = 3236, height = 2000,
           res = 300, casc = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="silhouette_+3A_data">data</code></td>
<td>
<p>Data with x and y coordinates.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_k.cluster">k.cluster</code></td>
<td>
<p>Cluster numbers for comparison in the k-means method (default = 2:10).</p>
</td></tr> 
<tr><td><code id="silhouette_+3A_plot">plot</code></td>
<td>
<p>Indicates whether to plot the silhouette graph (default = TRUE).</p>
</td></tr>
<tr><td><code id="silhouette_+3A_cut">cut</code></td>
<td>
<p>Indicates whether to plot the best cluster indicative line (default = TRUE).</p>
</td></tr>
<tr><td><code id="silhouette_+3A_title">title</code></td>
<td>
<p>Title of the graphic, if not set, assumes the default text.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_xlabel">xlabel</code></td>
<td>
<p>Names the X axis, if not set, assumes the default text.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_ylabel">ylabel</code></td>
<td>
<p>Names the Y axis, if not set, assumes the default text.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_size">size</code></td>
<td>
<p>Size of points on the graph and line thickness (default = 1.1).</p>
</td></tr>
<tr><td><code id="silhouette_+3A_grid">grid</code></td>
<td>
<p>Put grid on graph (default = TRUE).</p>
</td></tr>
<tr><td><code id="silhouette_+3A_color">color</code></td>
<td>
<p>Colored graphic (default = TRUE).</p>
</td></tr>
<tr><td><code id="silhouette_+3A_savptc">savptc</code></td>
<td>
<p>Saves the graph image to a file (default = FALSE).</p>
</td></tr> 
<tr><td><code id="silhouette_+3A_width">width</code></td>
<td>
<p>Graphic image width when savptc = TRUE (defaul = 3236).</p>
</td></tr>
<tr><td><code id="silhouette_+3A_height">height</code></td>
<td>
<p>Graphic image height when savptc = TRUE (default = 2000).</p>
</td></tr>
<tr><td><code id="silhouette_+3A_res">res</code></td>
<td>
<p>Nominal resolution in ppi of the graphic image when savptc = TRUE (default = 300).</p>
</td></tr>
<tr><td><code id="silhouette_+3A_casc">casc</code></td>
<td>
<p>Cascade effect in the presentation of the graphic (default = TRUE).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>k.ideal</code></td>
<td>
<p>Ideal number of clusters.</p>
</td></tr>
<tr><td><code>eve.si</code></td>
<td>
<p>Vector with averages of silhouette indices of cluster groups (si). </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani</p>


<h3>References</h3>

<p>Anitha, S.; Metilda, M. A. R. Y. An extensive investigation of outlier detection by cluster validation indices. <em>Ciencia e Tecnica Vitivinicola - A Science and Technology Journal</em>, v. 34, n. 2, p. 22-32, 2019. doi: 10.13140/RG.2.2.26801.63848
</p>
<p>Kaufman, L. and Rousseeuw, P. J. <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>, New York: John Wiley &amp; Sons. 1990.
</p>
<p>Martinez, W. L.; Martinez, A. R.; Solka, J. <em>Exploratory data analysis with MATLAB</em>. 2nd ed. New York: Chapman &amp; Hall/CRC, 2010. 499 p.
</p>
<p>Rousseeuw P. J. Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis. <em>Journal of Computational and Applied Mathematics</em>, 20:53-65. 1987. doi: 10.1016/0377-0427(87)90125-7
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

res &lt;- silhouette(data = iris[,1:4], k.cluster = 2:10, cut = TRUE, 
                  plot = TRUE, title = NA, xlabel = NA, ylabel = NA, 
                  size = 1.1, grid = TRUE, savptc = FALSE, width = 3236, 
                  color = TRUE, height = 2000, res = 300, casc = TRUE)
             
res$k.ideal # number of clusters
res$eve.si  # vector with averages of si indices


res &lt;- silhouette(data = iris[,1:4], k.cluster = 3, cut = TRUE, 
                  plot = TRUE, title = NA, xlabel = NA, ylabel = NA, 
                  size = 1.1, grid = TRUE, savptc = FALSE, width = 3236, 
                  color = TRUE, height = 2000, res = 300, casc = TRUE)
             
res$k.ideal # number of clusters
res$eve.si  # vector with averages of si indices
</code></pre>

<hr>
<h2 id='vote'>Performs the supervised classification vote method.</h2><span id='topic+vote'></span>

<h3>Description</h3>

<p>Performs the supervised classification voting method, using maximum agreement between classifiers.</p>


<h3>Usage</h3>

<pre><code class='language-R'>vote(mtx.algtms = NA)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vote_+3A_mtx.algtms">mtx.algtms</code></td>
<td>
<p>Matrix with the results of the supervised classification algorithms to be analyzed.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>predict</code></td>
<td>
<p>The classified factors of the test set.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Paulo Cesar Ossani</p>


<h3>References</h3>

<p>Kittler, J.; Hatef, M.; Duin, R. P. W. and Matas, J. On combining classifiers. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence.</em> 20(3):226-239. 1998. doi: 10.1109/34.667881
</p>


<h3>See Also</h3>

<p><code><a href="#topic+results">results</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris) # data set

data  &lt;- iris
names &lt;- colnames(data)
colnames(data) &lt;- c(names[1:4],"class")

#### Start - hold out validation method ####
dat.sample = sample(2, nrow(data), replace = TRUE, prob = c(0.7,0.3))
data.train = data[dat.sample == 1,] # training data set
data.test  = data[dat.sample == 2,] # test data set
class.train = as.factor(data.train$class) # class names of the training data set
class.test  = as.factor(data.test$class)  # class names of the test data set
#### End - hold out validation method ####

test  &lt;- as.integer(rownames(data.test)) # test data index
r     &lt;- (ncol(data)-1)
class &lt;- data[,c(r+1)] # classes names 

mod1 &lt;- knn(train = data.train[,1:r], test = data.test[,1:r],
            class = class.train, k = 1, dist = 'EUC')
mod2 &lt;- knn(train = data.train[,1:r], test = data.test[,1:r],
            class = class.train, k = 2, dist = 'EUC')
mod3 &lt;- lda(data = data[,1:r], test = test, class = class,
            type = 'test', method = 'moment', prior = NA)
mod4 &lt;- qda(data = data[,1:r], test = test, class = class,
            type = 'test', method = 'mle', prior = NA)
mod5 &lt;- qda(data = data[,1:r], test = test, class = class,
            type = 'test', method = 'moment', prior = NA)
mod6 &lt;- regression(train = data.train[,1:r], test = data.test[,1:r],
                   class = class.train, intercept = TRUE)

mod &lt;- cbind(as.data.frame(mod1$predict), mod2$predict, mod3$predict, 
             mod4$predict, mod5$predict, mod6$predict)

res &lt;- vote(mtx.algtms = mod)

resp &lt;- results(orig.class = class.test, predict = res$predict)

print("Confusion matrix:"); resp$conf.mtx  
cat("Hit rate:", resp$rate.hits,
    "\nError rate:", resp$rate.error,
    "\nNumber of correct instances:", resp$num.hits,
    "\nNumber of wrong instances:", resp$num.error,
    "\nKappa coefficient:", resp$kappa)
print("General results of the classes:"); resp$res.class 
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
