<!DOCTYPE html><html lang="en"><head><title>Help for package gnn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gnn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#catch'><p>Catching Results, Warnings and Errors Simultaneously</p></a></li>
<li><a href='#ffGNN'><p>Feedforward for Generative Neural Networks</p></a></li>
<li><a href='#find_box'><p>Box Numbers (Multivariate) Points Fall Into</p></a></li>
<li><a href='#fitGNN'><p>Functions and Methods for Training of Generative Neural Networks</p></a></li>
<li><a href='#FNN'><p>Generative Moment Matching Network</p></a></li>
<li><a href='#GNN_basics'><p>Basic Functions and Methods</p></a></li>
<li><a href='#loss'><p>Loss Function</p></a></li>
<li><a href='#plot'><p>Functions for Plotting</p></a></li>
<li><a href='#raw_keras'><p>Convert GNN model Slots to raw or keras Objects</p></a></li>
<li><a href='#rGNN'><p>Sampling from a Generative Neural Network</p></a></li>
<li><a href='#rm_ext'><p>Remove a File Extension</p></a></li>
<li><a href='#rPrior'><p>Sampling from a Prior Distribution</p></a></li>
<li><a href='#save_load_rda'><p>Save and Load .rda Files with Conversion to Raw and Keras Models</p></a></li>
<li><a href='#TensorFlow_available'><p>A Simple Check whether TensorFlow is Available</p></a></li>
<li><a href='#time'><p>Human-Readable Time Measurement</p></a></li>
<li><a href='#trafos_dimreduction'><p>Dimension-Reduction Transformations for Training or Sampling</p></a></li>
<li><a href='#trafos_margins'><p>Data Transformations for Training or Sampling</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>0.0-4</td>
</tr>
<tr>
<td>Title:</td>
<td>Generative Neural Networks</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools to set up, train, store, load, investigate and analyze
             generative neural networks. In particular, functionality for
	     generative moment matching networks is provided.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marius Hofert &lt;mhofert@hku.hk&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>keras, tensorflow, methods, R6, qrng, tools, copula</td>
</tr>
<tr>
<td>Suggests:</td>
<td>nvmix, qrmdata, RColorBrewer</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>TensorFlow (https://www.tensorflow.org/)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-28 08:30:11 UTC</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-28 02:03:11 UTC; mhofert</td>
</tr>
<tr>
<td>Author:</td>
<td>Marius Hofert <a href="https://orcid.org/0000-0001-8009-4665"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Avinash Prasad [aut]</td>
</tr>
</table>
<hr>
<h2 id='catch'>Catching Results, Warnings and Errors Simultaneously</h2><span id='topic+catch'></span>

<h3>Description</h3>

<p>Catches results, warnings and errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>catch(expr)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="catch_+3A_expr">expr</code></td>
<td>
<p>expression to be evaluated, typically a function call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is particularly useful for large(r) simulation studies
to not fail until finished.
</p>


<h3>Value</h3>

<p><code><a href="base.html#topic+list">list</a></code> with components:
</p>
<table role = "presentation">
<tr><td><code>value</code></td>
<td>
<p>value of <code>expr</code> or <code>NULL</code> in case of
an error.</p>
</td></tr>
<tr><td><code>warning</code></td>
<td>
<p>warning message (see <code><a href="base.html#topic+simpleWarning">simpleWarning</a></code> or
<code><a href="base.html#topic+warning">warning</a>()</code>) or <code><a href="base.html#topic+NULL">NULL</a></code> in case of no warning.</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>error message (see <code><a href="base.html#topic+simpleError">simpleError</a></code> or
<code><a href="base.html#topic+stop">stop</a>()</code>) or <code><a href="base.html#topic+NULL">NULL</a></code> in case of no error.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marius Hofert (based on <code>doCallWE()</code> and <code>tryCatch.W.E()</code> in
the <span class="rlang"><b>R</b></span> package <span class="pkg">simsalapar</span>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gnn) # for being standalone

catch(log(2))
catch(log(-1))
catch(log("a"))
</code></pre>

<hr>
<h2 id='ffGNN'>Feedforward for Generative Neural Networks</h2><span id='topic+ffGNN'></span><span id='topic+ffGNN.gnn_GNN'></span>

<h3>Description</h3>

<p>Feedforward method for objects of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gnn_GNN'
ffGNN(x, data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ffGNN_+3A_x">x</code></td>
<td>
<p>object of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code>.</p>
</td></tr>
<tr><td><code id="ffGNN_+3A_data">data</code></td>
<td>
<p><code><a href="base.html#topic+matrix">matrix</a></code> to be fed forward through <code>x</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output <code><a href="base.html#topic+matrix">matrix</a></code> of <code>x</code> when fed with <code>data</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(TensorFlow_available()) { # rather restrictive (due to R-Forge, winbuilder)
library(gnn) # for being standalone

## Define dummy model
d &lt;- 2 # bivariate case
GMMN &lt;- FNN(c(d, 300, d)) # Feedforward NN with MMD loss (a GMMN; random weights)

## Feedforward
n &lt;- 3
set.seed(271)
X &lt;- ffGNN(GMMN, data = matrix(runif(n * d), ncol = d))
stopifnot(dim(X) == c(n, d))

}
</code></pre>

<hr>
<h2 id='find_box'>Box Numbers (Multivariate) Points Fall Into</h2><span id='topic+find_box'></span>

<h3>Description</h3>

<p>Finding the numbers of boxes that given (multivariate) points fall
into (the default is similar to <code><a href="base.html#topic+findInterval">findInterval</a>()</code> but
other methods are provided, too).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_box(x, endpoints = NULL,
         method = c("per.dim", "lexicographic", "nested", "diagonal"),
         rightmost.closed = TRUE, left.open = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="find_box_+3A_x">x</code></td>
<td>
<p><code class="reqn">(n, d)</code>-matrix containing <code class="reqn">n</code>
<code class="reqn">d</code>-dimensional data points, <code class="reqn">n \ge 1</code>,
<code class="reqn">d \ge 1</code>.</p>
</td></tr>
<tr><td><code id="find_box_+3A_endpoints">endpoints</code></td>
<td>
<p><code class="reqn">d</code>-<code><a href="base.html#topic+list">list</a></code> containing
numeric vectors of endpoints of the intervals in each dimension
(each of the <code class="reqn">d</code> elements is an argument <code>vec</code>
as required by <code><a href="base.html#topic+findInterval">findInterval</a>()</code>).</p>
</td></tr>
<tr><td><code id="find_box_+3A_method">method</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string indicating the
method to be used. Available are:
</p>

<dl>
<dt><code>"per.dim"</code></dt><dd><p>the default. Each row <code>x[i,]</code>
of <code>x</code> produces <code class="reqn">d</code> numbers, where the <code class="reqn">j</code>th
indicates in which interval <code>x[i,j]</code> falls. This is
essentially <code><a href="base.html#topic+findInterval">findInterval</a>()</code> applied to the <code class="reqn">d</code>
coordinate samples (the columns of <code>x</code>).
</p>
</dd>
<dt><code>"lexicographic"</code></dt><dd><p>Each row <code>x[i,]</code> produces one
number, indicating in which box <code>x[i,]</code> falls
if all <em>nonempty</em> boxes are numbered consecutively
in lexicographic order.</p>
</dd>
<dt><code>"nested"</code></dt><dd><p>Each row <code>x[i,]</code> produces one
number, indicating in which box <code>x[i,]</code> falls along
the <code class="reqn">d</code>-dimensional diagonal in a nested way,
with a non-nested middle part if the number of interval
endpoints per dimension is even
(note that this method requires all elements of
<code>endpoints</code> to have the same length, so that the
diagonal is well-defined).</p>
</dd>
<dt><code>"diagonal"</code></dt><dd><p>Each row <code>x[i,]</code> produces one
number, indicating in which box <code>x[i,]</code> falls along
the <code class="reqn">d</code>-dimensional diagonal
(note that this method requires all elements of
<code>endpoints</code> to have the same length, so that the
diagonal is well-defined).</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="find_box_+3A_rightmost.closed">rightmost.closed</code></td>
<td>
<p>see <code><a href="base.html#topic+findInterval">findInterval</a>()</code> (note
the different default here).</p>
</td></tr>
<tr><td><code id="find_box_+3A_left.open">left.open</code></td>
<td>
<p>see <code><a href="base.html#topic+findInterval">findInterval</a>()</code> (note
the different default here).</p>
</td></tr>
<tr><td><code id="find_box_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="base.html#topic+findInterval">findInterval</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The box numbers can be used, for example, to color points; see the
examples below.
</p>


<h3>Value</h3>


<dl>
<dt><code>"per.dim"</code></dt><dd><p><code class="reqn">(n,d)</code>-matrix of box numbers per
dimension.</p>
</dd>
<dt><code>"lexicographic"</code>, <code>"nested"</code>, <code>"diagonal"</code></dt><dd>
<p><code class="reqn">n</code>-vector with box numbers.</p>
</dd>
</dl>

<p>Note that, as <code><a href="base.html#topic+findInterval">findInterval</a>()</code>, <code class="reqn">0</code> means &lsquo;in no box&rsquo;.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example data
n &lt;- 1000
d &lt;- 2
set.seed(271)
U &lt;- matrix(runif(n * d), ncol = d) # (n, d)-matrix of data (here: in [0,1]^d)


### 1 Basic example calls ######################################################

## Define endpoints and evaluate for different methods
epts &lt;- seq(0, 1, by = 1/5) # 5 boxes per dimension
find_box(U, endpoints = epts)[1:10,] # default "per.dim" (first 10 points only)
boxes.lexi &lt;- find_box(U, endpoints = epts, method = "lexicographic")
boxes.nest &lt;- find_box(U, endpoints = epts, method = "nested")
boxes.diag &lt;- find_box(U, endpoints = epts, method = "diagonal")

## Special cases
## First row of U (n = 1)
U[1,] # ~= (0.25, 0.14)
stopifnot(find_box(U[1, 1:2], endpoints = epts) == c(2, 1))
stopifnot(find_box(U[1, 1:2], endpoints = epts, method = "lexicographic") == 1)
## Note concerning the last line: It's 1 because all other boxes are empty
stopifnot(find_box(U[1, 1:2], endpoints = epts, method = "nested") == 2)
stopifnot(find_box(U[1, 1:2], endpoints = epts, method = "diagonal") == 0)
## Single number U[1,1] (d = 1)
U[1,1] # ~= 0.25
stopifnot(find_box(U[1,1], endpoints = epts) == 2)
stopifnot(find_box(U[1,1], endpoints = epts, method = "lexicographic") == 1)
stopifnot(find_box(U[1,1], endpoints = epts, method = "nested") == 2)
stopifnot(find_box(U[1,1], endpoints = epts, method = "diagonal") == 2)


### 2 Coloring points in lexicographic ordering ################################

## Define color palette
library(RColorBrewer)
basecols &lt;- c("#000000", brewer.pal(8, name = "Dark2")[c(8,7,3,1,5,4,2,6)])
mypal &lt;- function(n) rep_len(basecols, length.out = n)

## Colors
ncols &lt;- diff(range(boxes.lexi)) + 1 # maximal number of colors needed
palette(mypal(ncols)) # set palette according to maximum number of colors needed


## Boxes of equal size
boxes.lexi &lt;- find_box(U, endpoints = epts, method = "lexicographic")
cols &lt;- if(min(boxes.lexi) == 0) boxes.lexi + 1 else boxes.lexi
plot(U, pch = 20, xlab = expression(U[1]), ylab = expression(U[2]), col = cols)
abline(v = epts, h = epts, col = "gray50") # guides

## Boxes of different sizes and numbers
epts. &lt;- list(seq(0.2, 1, by = 1/5), seq(1/3, 1, by = 1/3))
boxes.lexi &lt;- find_box(U, endpoints = epts., method = "lexicographic")
cols &lt;- if(min(boxes.lexi) == 0) boxes.lexi + 1 else boxes.lexi
plot(U, pch = 20, xlab = expression(U[1]), ylab = expression(U[2]), col = cols)
abline(v = epts.[[1]], h = epts.[[2]], col = "gray50")


### 3 Coloring points along the diagonal in a nested way #######################

## Boxes of equal size (with 'middle' part)
boxes.nest &lt;- find_box(U, endpoints = epts, method = "nested")
cols &lt;- if(min(boxes.nest) == 0) boxes.nest + 1 else boxes.nest # color numbers
plot(U, pch = 20, xlab = expression(U[1]), ylab = expression(U[2]), col = cols)
abline(v = epts, h = epts, col = "gray50") # guides

## Boxes of different sizes (without 'middle' part; have to be the same number of
## boxes per dimension, otherwise there is no obvious 'diagonal')
epts. &lt;- lapply(1:d, function(j) c(0, 0.1, 0.3, 0.6, 1)) # 4 boxes per dimension
boxes.nest &lt;- find_box(U, endpoints = epts., method = "nested")
cols &lt;- if(min(boxes.nest) == 0) boxes.nest + 1 else boxes.nest # color numbers
plot(U, pch = 20, xlab = expression(U[1]), ylab = expression(U[2]), col = cols)
abline(v = epts.[[1]], h = epts.[[2]], col = "gray50") # guides


### 4 Coloring points along the diagonal #######################################

## Boxes of equal size
boxes.diag &lt;- find_box(U, endpoints = epts, method = "diagonal")
cols &lt;- if(min(boxes.diag) == 0) boxes.diag + 1 else boxes.diag # color numbers
plot(U, pch = 20, xlab = expression(U[1]), ylab = expression(U[2]), col = cols)
abline(v = epts, h = epts, col = "gray50") # guides

## Boxes of different sizes (have to be the same number of
## boxes per dimension, otherwise there is no obvious 'diagonal')
epts. &lt;- lapply(1:d, function(j) c(0, 0.05, 0.1, 0.3, 0.6, 1))
boxes.diag &lt;- find_box(U, endpoints = epts., method = "diagonal")
cols &lt;- if(min(boxes.diag) == 0) boxes.diag + 1 else boxes.diag # color numbers
plot(U, pch = 20, xlab = expression(U[1]), ylab = expression(U[2]), col = cols)
abline(v = epts.[[1]], h = epts.[[2]], col = "gray50") # guides
</code></pre>

<hr>
<h2 id='fitGNN'>Functions and Methods for Training of Generative Neural Networks</h2><span id='topic+fitGNN'></span><span id='topic+fitGNN.gnn_GNN'></span><span id='topic+fitGNNonce'></span><span id='topic+fitGNNonce.gnn_GNN'></span><span id='topic+is.trained'></span><span id='topic+is.trained.gnn_GNN'></span><span id='topic+is.trained.list'></span>

<h3>Description</h3>

<p>Functions and methods for training generative neural networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gnn_GNN'
fitGNN(x, data, batch.size = nrow(data), n.epoch = 100,
    prior = NULL, max.n.prior = 5000, verbose = 2, ...)
## S3 method for class 'gnn_GNN'
fitGNNonce(x, data, batch.size = nrow(data), n.epoch = 100,
    prior = NULL, verbose = 2, file = NULL, name = NULL, ...)
## S3 method for class 'gnn_GNN'
is.trained(x)
## S3 method for class 'list'
is.trained(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fitGNN_+3A_x">x</code></td>
<td>


<dl>
<dt>fitGNN(), fitGNNonce(), is.trained.gnn_GNN()</dt><dd><p>object of class
<code>"gnn_GNN"</code> to be trained.</p>
</dd>
<dt>is.trained.gnn_GNN()</dt><dd><p>object of class
<code>"gnn_GNN"</code> to be trained or a list of such.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="fitGNN_+3A_data">data</code></td>
<td>
<p><code class="reqn">(n, d)</code>-matrix containing the <code class="reqn">n</code>
<code class="reqn">d</code>-dimensional observations of the training data.</p>
</td></tr>
<tr><td><code id="fitGNN_+3A_batch.size">batch.size</code></td>
<td>
<p>number of samples used per stochastic gradient step.</p>
</td></tr>
<tr><td><code id="fitGNN_+3A_n.epoch">n.epoch</code></td>
<td>
<p>number of epochs (one epoch equals one pass through
the complete training dataset while updating the GNN's parameters
through stochastic gradient steps).</p>
</td></tr>
<tr><td><code id="fitGNN_+3A_prior">prior</code></td>
<td>
<p><code class="reqn">(n, d)</code>-matrix of prior samples; see also
<code><a href="#topic+rPrior">rPrior</a>()</code>. If <code>prior = NULL</code> a sample of
independent N(0,1) random variates is generated.</p>
</td></tr>
<tr><td><code id="fitGNN_+3A_max.n.prior">max.n.prior</code></td>
<td>
<p>maximum number of prior samples stored in <code>x</code>
after training.</p>
</td></tr>
<tr><td><code id="fitGNN_+3A_verbose">verbose</code></td>
<td>
<p><code><a href="base.html#topic+integer">integer</a></code> verbose level. Choices are:
</p>

<dl>
<dt>0</dt><dd><p>silent (no output).</p>
</dd>
<dt>1</dt><dd><p>progress bar (via <code><a href="utils.html#topic+txtProgressBar">txtProgressBar</a>()</code>).</p>
</dd>
<dt>2</dt><dd><p>output after each block of epochs (block size is
<code>ceiling(n.epoch/10)</code> if <code>n.epoch &lt;= 100</code> and
<code>ceiling(sqrt(n.epoch))</code> if <code>n.epoch &gt; 100</code>).</p>
</dd>
<dt>3</dt><dd><p>output after each expoch.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="fitGNN_+3A_file">file</code></td>
<td>
<p><code>NULL</code> or a <code><a href="base.html#topic+character">character</a></code> string
specifying the file in which the trained GNN(s) is (are)
saved. If <code>file</code> is provided and the specified file exists,
it is loaded and returned via <code><a href="#topic+load_gnn">load_gnn</a>()</code>.</p>
</td></tr>
<tr><td><code id="fitGNN_+3A_name">name</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string giving the name under
which the fitted <code>x</code> is saved (if <code>NULL</code> the fitted
<code>x</code> is saved under the name <code>"x"</code>).</p>
</td></tr>
<tr><td><code id="fitGNN_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code>fit()</code> (which is <code>keras:::fit.keras.engine.training.Model()</code>).</p>
</td></tr>
</table>


<h3>Value</h3>


<dl>
<dt>fitGNN()</dt><dd><p>the trained <code>x</code>.</p>
</dd>
<dt>fitGNNonce()</dt><dd><p>object of class as <code>x</code> with the trained GNN.</p>
</dd>
<dt>is.trained.gnn_GNN()</dt><dd><p><code><a href="base.html#topic+logical">logical</a></code>
indicating whether <code>x</code> is trained.</p>
</dd>
<dt>is.trained.list()</dt><dd><p><code><a href="base.html#topic+logical">logical</a></code> of length
<code>length(x)</code> indicating, for each component, whether
it is trained.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>See Also</h3>

<p><code><a href="#topic+FNN">FNN</a>()</code>, <code><a href="#topic+save_gnn">save_gnn</a>()</code>,
<code><a href="#topic+load_gnn">load_gnn</a>()</code>.
</p>

<hr>
<h2 id='FNN'>Generative Moment Matching Network</h2><span id='topic+FNN'></span>

<h3>Description</h3>

<p>Constructor for a generative feedforward neural network (FNN) model,
an object of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_FNN"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FNN(dim = c(2, 2), activation = c(rep("relu", length(dim) - 2), "sigmoid"),
    batch.norm = FALSE, dropout.rate = 0, loss.fun = "MMD", n.GPU = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="FNN_+3A_dim">dim</code></td>
<td>
<p><code><a href="base.html#topic+integer">integer</a></code> vector of length at least two, giving
the dimensions of the input layer, the hidden layer(s) (if any) and
the output layer (in this order).</p>
</td></tr>
<tr><td><code id="FNN_+3A_activation">activation</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> vector of length
<code>length(dim) - 1</code> specifying the activation functions
for all hidden layers and the output layer (in this order);
note that the input layer does not have an activation function.</p>
</td></tr>
<tr><td><code id="FNN_+3A_loss.fun">loss.fun</code></td>
<td>
<p>loss function specified as <code><a href="base.html#topic+character">character</a></code>
or <code><a href="base.html#topic+function">function</a></code>.</p>
</td></tr>
<tr><td><code id="FNN_+3A_batch.norm">batch.norm</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether batch
normalization layers are to be added after each hidden layer.</p>
</td></tr>
<tr><td><code id="FNN_+3A_dropout.rate">dropout.rate</code></td>
<td>
<p><code><a href="base.html#topic+numeric">numeric</a></code> value in [0,1] specifying
the fraction of input to be dropped; see the rate parameter of
<code>layer_dropout()</code>. Note that only if positive, dropout
layers are added after each hidden layer.</p>
</td></tr>
<tr><td><code id="FNN_+3A_n.gpu">n.GPU</code></td>
<td>
<p>non-negative <code><a href="base.html#topic+integer">integer</a></code> specifying the number of GPUs
available if the GPU version of TensorFlow is installed.
If positive, a (special) multiple GPU model for data
parallelism is instantiated. Note that for multi-layer perceptrons
on a few GPUs, this model does not yet yield any scale-up computational
factor (in fact, currently very slightly negative scale-ups are likely due
to overhead costs).</p>
</td></tr>
<tr><td><code id="FNN_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="#topic+loss">loss</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_FNN"</code> is a subclass of the
<code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code> which in turn is a subclass of
<code>"gnn_Model"</code>.
</p>


<h3>Value</h3>

<p><code>FNN()</code> returns an object of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_FNN"</code>
with components
</p>

<dl>
<dt><code>model</code></dt><dd><p>FNN model (a <span class="pkg">keras</span> object inheriting from
the R6 classes <code>"keras.engine.training.Model"</code>,
<code>"keras.engine.network.Network"</code>,
<code>"keras.engine.base_layer.Layer"</code>
and <code>"python.builtin.object"</code>, or a <code><a href="base.html#topic+raw">raw</a></code>
object).</p>
</dd>
<dt><code>type</code></dt><dd><p><code><a href="base.html#topic+character">character</a></code> string indicating
the type of model.</p>
</dd>
<dt><code>dim</code></dt><dd><p>see above.</p>
</dd>
<dt><code>activation</code></dt><dd><p>see above.</p>
</dd>
<dt><code>batch.norm</code></dt><dd><p>see above.</p>
</dd>
<dt><code>dropout.rate</code></dt><dd><p>see above.</p>
</dd>
<dt><code>n.param</code></dt><dd><p>number of trainable, non-trainable and total
number of parameters.</p>
</dd>
<dt><code>loss.type</code></dt><dd><p>type of loss function (<code><a href="base.html#topic+character">character</a></code>).</p>
</dd>
<dt><code>n.train</code></dt><dd><p>number of training samples (<code><a href="base.html#topic+NA_integer_">NA_integer_</a></code>
unless trained).</p>
</dd>
<dt><code>batch.size</code></dt><dd><p>batch size (<code><a href="base.html#topic+NA_integer_">NA_integer_</a></code> unless trained).</p>
</dd>
<dt><code>n.epoch</code></dt><dd><p>number of epochs (<code><a href="base.html#topic+NA_integer_">NA_integer_</a></code>
unless trained).</p>
</dd>
<dt><code>loss</code></dt><dd><p><code><a href="base.html#topic+numeric">numeric</a>(n.epoch)</code> containing the
loss function values per epoch.</p>
</dd>
<dt><code>time</code></dt><dd><p>object of S3 class <code>"proc_time"</code>
containing the training time (if trained).</p>
</dd>
<dt><code>prior</code></dt><dd><p><code><a href="base.html#topic+matrix">matrix</a></code> containing a (sub-)sample
of the prior (if trained).</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marius Hofert and Avinash Prasad</p>


<h3>References</h3>

<p>Li, Y., Swersky, K. and Zemel, R. (2015).
Generative moment matching networks.
<em>Proceedings of Machine Learning Research</em>, <b>37</b>
(International Conference on Maching Learning), 1718&ndash;1727.
See http://proceedings.mlr.press/v37/li15.pdf (2019-08-24)
</p>
<p>Dziugaite, G. K., Roy, D. M. and Ghahramani, Z. (2015).
Training generative neural networks via maximum mean discrepancy
optimization. <em>AUAI Press</em>, 258&ndash;267.
See http://www.auai.org/uai2015/proceedings/papers/230.pdf (2019-08-24)
</p>
<p>Hofert, M., Prasad, A. and Zhu, M. (2020).
Quasi-random sampling for multivariate distributions via generative
neural networks. <em>Journal of Computational and Graphical
Statistics</em>, <a href="https://doi.org/10.1080/10618600.2020.1868302">doi:10.1080/10618600.2020.1868302</a>.
</p>
<p>Hofert, M., Prasad, A. and Zhu, M. (2020).
Multivariate time-series modeling with generative neural networks.
See <a href="https://arxiv.org/abs/2002.10645">https://arxiv.org/abs/2002.10645</a>.
</p>
<p>Hofert, M. Prasad, A. and Zhu, M. (2020).
Applications of multivariate quasi-random sampling with neural
networks. See <a href="https://arxiv.org/abs/2012.08036">https://arxiv.org/abs/2012.08036</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(TensorFlow_available()) { # rather restrictive (due to R-Forge, winbuilder)
library(gnn) # for being standalone

## Training data
d &lt;- 2 # bivariate case
P &lt;- matrix(0.9, nrow = d, ncol = d); diag(P) &lt;- 1 # correlation matrix
ntrn &lt;- 60000 # training data sample size
set.seed(271)
library(nvmix)
X &lt;- abs(rNorm(ntrn, scale = P)) # componentwise absolute values of N(0,P) sample

## Plot a subsample
m &lt;- 2000 # subsample size for plots
opar &lt;- par(pty = "s")
plot(X[1:m,], xlab = expression(X[1]), ylab = expression(X[2])) # plot |X|
U &lt;- apply(X, 2, rank) / (ntrn + 1) # pseudo-observations of |X|
plot(U[1:m,], xlab = expression(U[1]), ylab = expression(U[2])) # visual check

## Model 1: A basic feedforward neural network (FNN) with MSE loss function
fnn &lt;- FNN(c(d, 300, d), loss.fun = "MSE") # define the FNN
fnn &lt;- fitGNN(fnn, data = U, n.epoch = 40) # train with batch optimization
plot(fnn, kind = "loss") # plot the loss after each epoch

## Model 2: A GMMN (FNN with MMD loss function)
gmmn &lt;- FNN(c(d, 300, d)) # define the GMMN (initialized with random weights)
## For training we need to use a mini-batch optimization (batch size &lt; nrow(U)).
## For a fair comparison (same number of gradient steps) to NN, we use 500
## samples (25% = 4 gradient steps/epoch) for 10 epochs for GMMN.
library(keras) # for callback_early_stopping()
## We monitor the loss function and stop earlier if the loss function
## over the last patience-many epochs has changed by less than min_delta
## in absolute value. Then we keep the weights that led to the smallest
## loss seen throughout training.
gmmn &lt;- fitGNN(gmmn, data = U, batch.size = 500, n.epoch = 10,
               callbacks = callback_early_stopping(monitor = "loss",
                                                   min_delta = 1e-3, patience = 3,
                                                   restore_best_weights = TRUE))
plot(gmmn, kind = "loss") # plot the loss after each epoch
## Note:
## - Obviously, in a real-world application, batch.size and n.epoch
##   should be (much) larger (e.g., batch.size = 5000, n.epoch = 300).
## - Training is not reproducible (due to keras).

## Model 3: A FNN with CvM loss function
fnnCvM &lt;- FNN(c(d, 300, d), loss.fun = "CvM")
fnnCvM &lt;- fitGNN(fnnCvM, data = U, batch.size = 500, n.epoch = 10,
                 callbacks = callback_early_stopping(monitor = "loss",
                                                     min_delta = 1e-3, patience = 3,
                                                     restore_best_weights = TRUE))
plot(fnnCvM, kind = "loss") # plot the loss after each epoch

## Sample from the different models
set.seed(271)
V.fnn &lt;- rGNN(fnn, size = m)
set.seed(271)
V.gmmn &lt;- rGNN(gmmn, size = m)
set.seed(271)
V.fnnCvM &lt;- rGNN(fnnCvM, size = m)

## Joint plot of training subsample with GMMN PRNs. Clearly, the MSE
## cannot be used to learn the distribution correctly.
layout(matrix(1:4, ncol = 2, byrow = TRUE))
plot(U[1:m,], xlab = expression(U[1]), ylab = expression(U[2]), cex = 0.2)
mtext("Training subsample", side = 4, line = 0.4, adj = 0)
plot(V.fnn,    xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with MSE loss", side = 4, line = 0.4, adj = 0)
plot(V.gmmn,  xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with MMD loss", side = 4, line = 0.4, adj = 0)
plot(V.fnnCvM,    xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with CvM loss", side = 4, line = 0.4, adj = 0)

## Joint plot of training subsample with GMMN QRNs
library(qrng) # for sobol()
V.fnn.    &lt;- rGNN(fnn,    size = m, method = "sobol", randomize = "Owen")
V.gmmn.   &lt;- rGNN(gmmn,   size = m, method = "sobol", randomize = "Owen")
V.fnnCvM. &lt;- rGNN(fnnCvM, size = m, method = "sobol", randomize = "Owen")
plot(U[1:m,], xlab = expression(U[1]), ylab = expression(U[2]), cex = 0.2)
mtext("Training subsample", side = 4, line = 0.4, adj = 0)
plot(V.fnn., xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with MSE loss", side = 4, line = 0.4, adj = 0)
plot(V.gmmn., xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with MMD loss", side = 4, line = 0.4, adj = 0)
plot(V.fnnCvM., xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with CvM loss", side = 4, line = 0.4, adj = 0)
layout(1)
par(opar)
}
</code></pre>

<hr>
<h2 id='GNN_basics'>Basic Functions and Methods</h2><span id='topic+print.gnn_GNN'></span><span id='topic+str.gnn_GNN'></span><span id='topic+summary.gnn_GNN'></span><span id='topic+dim.gnn_GNN'></span><span id='topic+is.GNN'></span><span id='topic+is.GNN.gnn_GNN'></span><span id='topic+is.GNN.list'></span>

<h3>Description</h3>

<p>Basic functions and methods for objects of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gnn_GNN'
print(x, ...)
## S3 method for class 'gnn_GNN'
str(object, ...)
## S3 method for class 'gnn_GNN'
summary(object, ...)
## S3 method for class 'gnn_GNN'
dim(x)
## S3 method for class 'gnn_GNN'
is.GNN(x)
## S3 method for class 'list'
is.GNN(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GNN_basics_+3A_x">x</code></td>
<td>


<dl>
<dt>print(), dim()</dt><dd><p>object of <code><a href="methods.html#topic+S3">S3</a></code> class
<code>"gnn_GNN"</code>.</p>
</dd>
<dt>is.GNN()</dt><dd><p>object of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code>
or a list of such.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="GNN_basics_+3A_object">object</code></td>
<td>
<p>object of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code>.</p>
</td></tr>
<tr><td><code id="GNN_basics_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>


<dl>
<dt>print()</dt><dd><p>return value of the <code><a href="base.html#topic+print">print</a>()</code>
method for objects of class <code>"<a href="base.html#topic+list">list</a>"</code>.</p>
</dd>
<dt>str()</dt><dd><p>nothing, as <code><a href="utils.html#topic+str">str</a>()</code> returns
nothing when applied to objects of class <code>"<a href="base.html#topic+list">list</a>"</code>.</p>
</dd>
<dt>summary()</dt><dd><p>return value of the <code><a href="base.html#topic+summary">summary</a>()</code>
method for objects of class <code>"<a href="base.html#topic+list">list</a>"</code>.</p>
</dd>
<dt>dim()</dt><dd><p>slot <code>dim</code> of <code>x</code>, so a vector of
dimensions of input, hidden and output layers.</p>
</dd>
<dt>is.GNN()</dt><dd><p><code><a href="base.html#topic+logical">logical</a></code> of length equal to the
length of <code>x</code> indicating, for each component,
whether it is an object of class <code>"gnn_GNN"</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(TensorFlow_available()) { # rather restrictive (due to R-Forge, winbuilder)
library(gnn) # for being standalone

d &lt;- 2
dim &lt;- c(d, 300, d) # dimensions of the input, hidden and output layers
GMMN &lt;- FNN(dim) # define the GMMN model
stopifnot(is.GNN(GMMN)) # check for being a GNN
GMMN # print() method
str(GMMN) # str() method
summary(GMMN) # summary() method
stopifnot(dim(GMMN) == c(d, 300, d)) # dim() method

}
</code></pre>

<hr>
<h2 id='loss'>Loss Function</h2><span id='topic+loss'></span><span id='topic+MMD'></span><span id='topic+CvM'></span>

<h3>Description</h3>

<p>Implementation of various loss functions to measure
statistical discrepancy between two datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loss(x, y, type = c("MMD", "CvM", "MSE", "BCE"), ...)

MMD(x, y, ...)
CvM(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loss_+3A_x">x</code></td>
<td>
<p>2d-tensor or <code class="reqn">(n, d)</code>-matrix (during training, <code class="reqn">n</code> is
the batch size and <code class="reqn">d</code> is the dimension of the input dataset).</p>
</td></tr>
<tr><td><code id="loss_+3A_y">y</code></td>
<td>
<p>2d-tensor or <code class="reqn">(m, d)</code>-matrix (during training, <code class="reqn">m</code> is
the batch size (and typically equal to <code class="reqn">n</code>) and <code class="reqn">d</code> is the
dimension of the input dataset).</p>
</td></tr>
<tr><td><code id="loss_+3A_type">type</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string indicating the type of
loss used. Currently available are the
(kernel) maximum mean discrepancy (<code>"MMD"</code>, calling <code>MMD()</code>),
the Cramer-von Mises statistc (<code>"CvM"</code>, calling <code>CvM()</code>)
of Rémillard and Scaillet (2009),
the mean squared error (<code>"MSE"</code>)
and the binary cross entropy (<code>"BCE"</code>).</p>
</td></tr>
<tr><td><code id="loss_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying functions,
most notably <code>bandwidth</code> (a number or numeric vector of
bandwidths for the radial basis function kernels) in case
<code>type = "MMD"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>loss()</code> returns a 0d tensor containing the loss.
</p>
<p><code>MMD()</code> and <code>CvM()</code> return a 0d tensor (if <code>x</code>
and <code>y</code> are tensors) or <code><a href="base.html#topic+numeric">numeric</a>(1)</code> (if <code>x</code> or
<code>y</code> are <span class="rlang"><b>R</b></span> matrices).
</p>


<h3>Author(s)</h3>

<p>Marius Hofert and Avinash Prasad</p>


<h3>References</h3>

<p>Kingma, D. P. and Welling, M. (2014).
Stochastic gradient VB and the variational auto-encoder.
<em>Second International Conference on Learning Representations (ICLR)</em>.
See https://keras.rstudio.com/articles/examples/variational_autoencoder.html
</p>
<p>Rémillard, B. and Scaillet, O. (2009).
Testing for equality between two copulas.
<em>Journal of Multivariate Analysis</em>
<b>100</b>, 377&ndash;386.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FNN">FNN</a>()</code> where <code>loss()</code> is used.
</p>

<hr>
<h2 id='plot'>Functions for Plotting</h2><span id='topic+plot'></span><span id='topic+plot.gnn_GNN'></span>

<h3>Description</h3>

<p>Functions for plotting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gnn_GNN'
plot(x, kind = c("scatter", "loss"), max.n.samples = NULL,
    type = NULL, xlab = NULL, ylab = NULL,
    y2lab = NULL, labels = "X", pair = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_+3A_x">x</code></td>
<td>
<p>trained object of class <code>"gnn_GNN"</code>
whose loss function (loss per epoch of training) is to be plotted.</p>
</td></tr>
<tr><td><code id="plot_+3A_kind">kind</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a>()</code> indicating the type
of plot.</p>
</td></tr>
<tr><td><code id="plot_+3A_max.n.samples">max.n.samples</code></td>
<td>
<p>maximal number of samples to be plotted.</p>
</td></tr>
<tr><td><code id="plot_+3A_type">type</code></td>
<td>
<p>line type; see <code><a href="#topic+plot">plot</a>()</code>.</p>
</td></tr>
<tr><td><code id="plot_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label; see <code><a href="#topic+plot">plot</a>()</code>.</p>
</td></tr>
<tr><td><code id="plot_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label; see <code><a href="#topic+plot">plot</a>()</code>.</p>
</td></tr>
<tr><td><code id="plot_+3A_y2lab">y2lab</code></td>
<td>
<p>secondary y-axis label.</p>
</td></tr>
<tr><td><code id="plot_+3A_labels">labels</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a>()</code> vector indicating
the labels to be used; if of length 1, then the base label
to be used.</p>
</td></tr>
<tr><td><code id="plot_+3A_pair">pair</code></td>
<td>
<p><code><a href="base.html#topic+numeric">numeric</a>(2)</code> containing the indices of the
pair to be plotted.</p>
</td></tr>
<tr><td><code id="plot_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="#topic+plot">plot</a>()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot by side-effect.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>See Also</h3>

<p><code><a href="#topic+fitGNN">fitGNN</a>()</code>.
</p>

<hr>
<h2 id='raw_keras'>Convert GNN model Slots to raw or keras Objects</h2><span id='topic+as.keras'></span><span id='topic+as.raw.gnn_GNN'></span><span id='topic+as.keras.gnn_GNN'></span>

<h3>Description</h3>

<p>Keras objects cannot be saved like other R objects.
The methods <code>as.raw()</code> and <code>as.keras()</code> can
be used to convert the <code>model</code> slots of objects
of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code> to
<code>"<a href="base.html#topic+raw">raw</a>"</code> objects (which can be saved)
or <code>"keras.engine.training.Model"</code> objects (which
can be trained).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gnn_GNN'
as.raw(x)
## S3 method for class 'gnn_GNN'
as.keras(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="raw_keras_+3A_x">x</code></td>
<td>
<p>object of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code> with
slot <code>method</code> converted by the respective method if necessary.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>

<hr>
<h2 id='rGNN'>Sampling from a Generative Neural Network</h2><span id='topic+rGNN'></span><span id='topic+rGNN.gnn_GNN'></span>

<h3>Description</h3>

<p>Sampling method for objects of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gnn_GNN'
rGNN(x, size, prior = NULL, pobs = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rGNN_+3A_x">x</code></td>
<td>
<p>object of <code><a href="methods.html#topic+S3">S3</a></code> class <code>"gnn_GNN"</code>.</p>
</td></tr>
<tr><td><code id="rGNN_+3A_size">size</code></td>
<td>
<p>sample size, a positive <code><a href="base.html#topic+integer">integer</a></code>. Ignored
if <code>prior</code> is a <code><a href="base.html#topic+matrix">matrix</a></code>.</p>
</td></tr>
<tr><td><code id="rGNN_+3A_prior">prior</code></td>
<td>
<p>one of
</p>

<dl>
<dt><code>NULL</code></dt><dd><p>the default, generates independent N(0,1)
realizations as prior sample.</p>
</dd>
<dt><code><a href="base.html#topic+matrix">matrix</a></code></dt><dd><p>passes the
given sample through the GNN <code>x</code>. Such a matrix is
internally (if <code>prior = NULL</code>) and typically obtained
via <code><a href="#topic+rPrior">rPrior</a>()</code>.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="rGNN_+3A_pobs">pobs</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether the
pseudo-observations of the generated samples should be returned.</p>
</td></tr>
<tr><td><code id="rGNN_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="#topic+rPrior">rPrior</a>()</code> if <code>prior = NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>(size, dim(x)[1])</code>-<code><a href="base.html#topic+matrix">matrix</a></code> of samples.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(TensorFlow_available()) { # rather restrictive (due to R-Forge, winbuilder)
library(gnn) # for being standalone

## Define dummy model
d &lt;- 2 # bivariate case
GMMN &lt;- FNN(c(d, 300, d)) # Feedforward NN with MMD loss (a GMMN; random weights)

## Sampling
n &lt;- 3
(X1 &lt;- rGNN(GMMN, size = n)) # default (independent N(0,1) samples as prior)
(X2 &lt;- rGNN(GMMN, size = n, # passing additional arguments to rPrior()
            qmargins = qexp, method = "sobol", seed = 271))
(X3 &lt;- rGNN(GMMN, prior = matrix(rexp(n * d), ncol = d))) # providing 'prior'
stopifnot(dim(X1) == c(n, d), dim(X2) == c(n, d), dim(X3) == c(n, d))

}
</code></pre>

<hr>
<h2 id='rm_ext'>Remove a File Extension</h2><span id='topic+rm_ext'></span>

<h3>Description</h3>

<p>Fixes the removal of file extensions of <code>file_path_sans_ext()</code>
in the case where file names contain digits after the last dot
(which is often used to incorporate numeric numbers into file names).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rm_ext(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rm_ext_+3A_x">x</code></td>
<td>
<p>file name(s) with extension(s) to be stripped off.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The file name without its extension (if the file name had an extension).
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gnn) # for being standalone

myfilepath1 &lt;- "/myusername/my_filename_with_dots_0.25_0.50_0.75.rda"
myfilepath2 &lt;- "/myusername/my_filename_with_dots_0.25_0.50_0.75"
myfilepath3 &lt;- "/myusername/my_filename_with_dots_0.25_0.50_0.75."
myfilepath4 &lt;- "/myusername/my_filename_with_dots_0.25_0.50_0.75._"
myfilepath5 &lt;- "/myusername/my_filename_with_dots_0.25_0.50_0.75._*.rda"
library(tools)
file_path_sans_ext(myfilepath2) # fails (only case)

stopifnot(rm_ext(myfilepath1) == file_path_sans_ext(myfilepath1))
stopifnot(rm_ext(myfilepath2) == myfilepath2)
stopifnot(rm_ext(myfilepath3) == file_path_sans_ext(myfilepath3))
stopifnot(rm_ext(myfilepath4) == file_path_sans_ext(myfilepath4))
stopifnot(rm_ext(myfilepath5) == file_path_sans_ext(myfilepath5))
</code></pre>

<hr>
<h2 id='rPrior'>Sampling from a Prior Distribution</h2><span id='topic+rPrior'></span>

<h3>Description</h3>

<p>Sampling from a prior distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rPrior(n, copula, qmargins = qnorm, method = c("pseudo", "sobol"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rPrior_+3A_n">n</code></td>
<td>
<p>sample size, a positive <code><a href="base.html#topic+integer">integer</a></code>.</p>
</td></tr>
<tr><td><code id="rPrior_+3A_copula">copula</code></td>
<td>
<p>object of <code><a href="base.html#topic+S4">S4</a></code> class <code>"Copula"</code> for
which the method <code>rCopula()</code> is available; see the <span class="rlang"><b>R</b></span>
package <span class="pkg">copula</span>.</p>
</td></tr>
<tr><td><code id="rPrior_+3A_qmargins">qmargins</code></td>
<td>
<p>marginal quantile <code><a href="base.html#topic+function">function</a></code> or a <code><a href="base.html#topic+list">list</a></code>
of length <code>dim(x)[1]</code> of such.</p>
</td></tr>
<tr><td><code id="rPrior_+3A_method">method</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string indicating the sampling
method. If <code>"sobol"</code>, then randomization <code>"digital.shift"</code>
is used (pass <code>seed</code> via ... for reproducibilty;
see the <span class="rlang"><b>R</b></span> package <span class="pkg">qrng</span>).</p>
</td></tr>
<tr><td><code id="rPrior_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>method</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>(n, dim(copula))</code>-<code><a href="base.html#topic+matrix">matrix</a></code> of samples.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gnn) # for being standalone

n &lt;- 5
d &lt;- 3
library(copula)
cop &lt;- claytonCopula(2, dim = d)
X1 &lt;- rPrior(n, copula = cop) # Clayton copula and N(0,1) margins
X2 &lt;- rPrior(n, copula = cop, qmargins = qexp) # Exp(1) margins
X3 &lt;- rPrior(n, copula = cop, qmargins = qexp, method = "sobol", seed = 271)
stopifnot(dim(X1) == c(n, d), dim(X2) == c(n, d), dim(X3) == c(n, d))
</code></pre>

<hr>
<h2 id='save_load_rda'>Save and Load .rda Files with Conversion to Raw and Keras Models</h2><span id='topic+save_gnn'></span><span id='topic+load_gnn'></span>

<h3>Description</h3>

<p>Save and load <code>.rda</code> files with conversion to objects of class
<code>raw</code> (for <code>save_gnn()</code>) or <code>"keras.engine.training.Model"</code>
(for <code>load_gnn()</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>save_gnn(..., file, name = NULL)
load_gnn(file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="save_load_rda_+3A_...">...</code></td>
<td>
<p>objects to be saved in <code>file</code> (under the provided
names if <code>name</code> was provided). Those objects which are of class
<code>"gnn_GNN"</code> are converted with <code><a href="base.html#topic+as.raw">as.raw</a>()</code> before they
are saved.</p>
</td></tr>
<tr><td><code id="save_load_rda_+3A_file">file</code></td>
<td>
<p>file name; see the underlying <code><a href="base.html#topic+save">save</a>()</code>
and <code><a href="base.html#topic+load">load</a>()</code>.</p>
</td></tr>
<tr><td><code id="save_load_rda_+3A_name">name</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> (vector) of name(s) under
which the objects in <code>...</code> are to be saved in <code>file</code>. If
<code><a href="base.html#topic+NULL">NULL</a></code>, the names of the objects provided by ... are
taken as <code>name</code>.</p>
</td></tr>
</table>


<h3>Value</h3>


<dl>
<dt>save_gnn()</dt><dd><p>nothing (generates an <code>.rda</code> by side-effect).</p>
</dd>
<dt>load_gnn()</dt><dd><p>the loaded object(s). Those of class <code>"gnn_GNN"</code>
are converted with <code><a href="#topic+as.keras">as.keras</a>()</code> before they are
returned; this also applies to a component of a loaded object of class
<code><a href="base.html#topic+list">list</a></code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>See Also</h3>

<p>See the underlying functions <code><a href="base.html#topic+load">load</a>()</code>
and <code><a href="base.html#topic+save">save</a>()</code> (among others).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(TensorFlow_available()) { # rather restrictive (due to R-Forge, winbuilder)
library(gnn) # for being standalone

file &lt;- tempfile("foo", fileext = ".rda")
GMMN1 &lt;- FNN()
save_gnn(GMMN1, file = file) # converts GMMN via as.raw()
GMMN2 &lt;- load_gnn(file) # converts loaded object via as.keras()
stopifnot(is.GNN(GMMN2), inherits(GMMN2[["model"]], "keras.engine.training.Model"))
rm(GMMN1, GMMN2) # clean-up
stopifnot(file.remove(file))

}
</code></pre>

<hr>
<h2 id='TensorFlow_available'>A Simple Check whether TensorFlow is Available</h2><span id='topic+TensorFlow_available'></span>

<h3>Description</h3>

<p>A simple (and restrictive) check whether TensorFlow is available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TensorFlow_available()
</code></pre>


<h3>Details</h3>

<p>Essentially calls <code>"pip list | grep tensorflow"</code> via
<code><a href="base.html#topic+system">system</a>()</code>. Only available on non-Windows operating
systems; returns <code><a href="base.html#topic+FALSE">FALSE</a></code> on Windows.
</p>


<h3>Value</h3>

<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether TensorFlow was found.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gnn) # for being standalone

TensorFlow_available()
</code></pre>

<hr>
<h2 id='time'>Human-Readable Time Measurement</h2><span id='topic+as.human'></span><span id='topic+human.time'></span><span id='topic+time.gnn_GNN'></span><span id='topic+print.gnn_proc_time'></span>

<h3>Description</h3>

<p>Functions and methods for extracting and printing timings in
human-readable format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.human(x, fmt = "%.2f")
human.time(expr, print = TRUE, ...)
## S3 method for class 'gnn_GNN'
time(x, human = FALSE, ...)
## S3 method for class 'gnn_proc_time'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="time_+3A_x">x</code></td>
<td>


<dl>
<dt>as.human()</dt><dd><p>object of class <code>"proc_time"</code> as
returned by <code><a href="base.html#topic+system.time">system.time</a>()</code>.</p>
</dd>
<dt>time.gnn_GNN()</dt><dd><p>object of class <code>"gnn_GNN"</code>.</p>
</dd>
<dt>print.gnn_proc_time()</dt><dd><p>object of class
<code>"gnn_proc_time"</code> as returned by <code>time()</code>.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="time_+3A_fmt">fmt</code></td>
<td>
<p>format string as required by <code><a href="base.html#topic+sprintf">sprintf</a>()</code>.</p>
</td></tr>
<tr><td><code id="time_+3A_expr">expr</code></td>
<td>
<p>see <code><a href="base.html#topic+system.time">system.time</a>()</code>.</p>
</td></tr>
<tr><td><code id="time_+3A_print">print</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether to print the
result; either way, it is returned (invisibly if <code>print = TRUE</code>).</p>
</td></tr>
<tr><td><code id="time_+3A_human">human</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether the result is to
be returned in human-readable format.</p>
</td></tr>
<tr><td><code id="time_+3A_...">...</code></td>
<td>
<p>for <code>human.time()</code> and <code>time.gnn_GNN()</code> additional
arguments passed to the underlying <code>as.human()</code>; unused for
<code>print.gnn_proc_time()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>


<dl>
<dt>as.human(), human.time()</dt><dd><p>named <code><a href="base.html#topic+character">character</a>(3)</code> providing
user, system and elapsed time in human-readable format.</p>
</dd>
<dt>time.gnn_GNN()</dt><dd><p>object of class <code>"gnn_proc_time"</code>.</p>
</dd>
<dt>print.gnn_proc_time()</dt><dd><p><code>x</code> (invisibly).</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(TensorFlow_available()) { # rather restrictive (due to R-Forge, winbuilder)
library(gnn) # for being standalone

human.time(Sys.sleep(0.1)) # print human-readable time
(proc.obj &lt;- human.time(Sys.sleep(0.1), print = FALSE)) # save the timing (character(3))
fnn &lt;- FNN()
time(fnn) # default print method for objects of class "gnn_proc_time"
time(fnn, human = TRUE) # human-readable print method for such objects
}
</code></pre>

<hr>
<h2 id='trafos_dimreduction'>Dimension-Reduction Transformations for Training or Sampling</h2><span id='topic+PCA_trafo'></span>

<h3>Description</h3>

<p>Dimension-reduction transformations applied to an input data matrix.
Currently on the principal component transformation and its inverse.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PCA_trafo(x, mu, Gamma, inverse = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trafos_dimreduction_+3A_x">x</code></td>
<td>
<p><code class="reqn">(n, d)</code>-matrix of data (typically before training or
after sampling). If <code>inverse = FALSE</code>, then, conceptually,
an <code class="reqn">(n, d)</code>-matrix with <code class="reqn">1\le k \le d</code>,
where <code class="reqn">d</code> is the dimension of the original data whose dimension
was reduced to <code class="reqn">k</code>.</p>
</td></tr>
<tr><td><code id="trafos_dimreduction_+3A_mu">mu</code></td>
<td>
<p>if <code>inverse = TRUE</code>, a <code class="reqn">d</code>-vector
of centers, where <code class="reqn">d</code> is the dimension to transform <code>x</code>
to.</p>
</td></tr>
<tr><td><code id="trafos_dimreduction_+3A_gamma">Gamma</code></td>
<td>
<p>if <code>inverse = TRUE</code>, a <code class="reqn">(d, k)</code>-matrix with
<code class="reqn">k</code> at least as large as <code>ncol(x)</code> containing the
<code class="reqn">k</code> orthonormal eigenvectors of a covariance matrix sorted
in decreasing order of their eigenvalues; in other words,
the columns of <code>Gamma</code> contain principal axes or loadings.
If a matrix with <code class="reqn">k</code> greater than <code>ncol(x)</code> is provided,
only the first <code class="reqn">k</code>-many are considered.</p>
</td></tr>
<tr><td><code id="trafos_dimreduction_+3A_inverse">inverse</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether the inverse
transformation of the principal component transformation is applied.</p>
</td></tr>
<tr><td><code id="trafos_dimreduction_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="stats.html#topic+prcomp">prcomp</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Conceptually, the principal component transformation transforms a
vector <code class="reqn">\bm{X}</code> to a vector <code class="reqn">\bm{Y}</code> where
<code class="reqn">\bm{Y} = \Gamma^T(\bm{X}-\bm{\mu})</code>,
where <code class="reqn">\bm{\mu}</code> is the mean vector of <code class="reqn">\bm{X}</code>
and <code class="reqn">\Gamma</code> is the <code class="reqn">(d, d)</code>-matrix whose
columns contains the orthonormal eigenvectors of <code>cov(X)</code>.
</p>
<p>The corresponding (conceptual) inverse transformation is
<code class="reqn">\bm{X} = \bm{\mu} + \Gamma \bm{Y}</code>.
</p>
<p>See McNeil et al. (2015, Section 6.4.5).
</p>


<h3>Value</h3>

<p>If <code>inverse = TRUE</code>, the transformed data whose rows contain
<code class="reqn">\bm{X} = \bm{\mu} + \Gamma \bm{Y}</code>, where
<code class="reqn">Y</code> is one row of <code>x</code>. See the details below for the
notation.
</p>
<p>If <code>inverse = FALSE</code>, a <code><a href="base.html#topic+list">list</a></code> containing:
</p>

<dl>
<dt><code>PCs</code>:</dt><dd><p><code class="reqn">(n, d)</code>-matrix of principal components.</p>
</dd>
<dt><code>cumvar</code>:</dt><dd><p>cumulative variances; the <code class="reqn">j</code>th entry
provides the fraction of the explained variance of the first
<code class="reqn">j</code> principal components.</p>
</dd>
<dt><code>sd</code>:</dt><dd><p>sample standard deviations of the transformed data.</p>
</dd>
<dt><code>lambda</code>:</dt><dd><p>eigenvalues of <code>cov(x)</code>.</p>
</dd>
<dt><code>mu</code>:</dt><dd><p><code class="reqn">d</code>-vector of centers of <code>x</code> (see also
above) typically provided to <code>PCA_trafo(, inverse = TRUE)</code>.</p>
</dd>
<dt><code>Gamma</code>:</dt><dd><p><code class="reqn">(d, d)</code>-matrix of principal axes (see also
above) typically provided to <code>PCA_trafo(, inverse = TRUE)</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R., and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gnn) # for being standalone

## Generate data
library(copula)
set.seed(271)
X &lt;- qt(rCopula(1000, gumbelCopula(2, dim = 10)), df = 3.5)
pairs(X, gap = 0, pch = ".")

## Principal component transformation
PCA &lt;- PCA_trafo(X)
Y &lt;- PCA$PCs
PCA$cumvar[3] # fraction of variance explained by the first 3 principal components
which.max(PCA$cumvar &gt; 0.9) # number of principal components it takes to explain 90%

## Biplot (plot of the first two principal components = data transformed with
## the first two principal axes)
plot(Y[,1:2])

## Transform back and compare
X. &lt;- PCA_trafo(Y, mu = PCA$mu, Gamma = PCA$Gamma, inverse = TRUE)
stopifnot(all.equal(X., X))

## Note: One typically transforms back with only some of the principal axes
X. &lt;- PCA_trafo(Y[,1:3], mu = PCA$mu, # mu determines the dimension to transform to
                Gamma = PCA$Gamma, # must be of dim. (length(mu), k) for k &gt;= ncol(x)
                inverse = TRUE)
stopifnot(dim(X.) == c(1000, 10))
## Note: We (typically) transform back to the original dimension.
pairs(X., gap = 0, pch = ".") # pairs of back-transformed first three PCs
</code></pre>

<hr>
<h2 id='trafos_margins'>Data Transformations for Training or Sampling</h2><span id='topic+range_trafo'></span><span id='topic+logis_trafo'></span>

<h3>Description</h3>

<p>Transformations applied to each marginal component sample to map given data
to a different range.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>range_trafo(x, lower, upper, inverse = FALSE)
logis_trafo(x, mean = 0, sd = 1, slope = 1, intercept = 0, inverse = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trafos_margins_+3A_x">x</code></td>
<td>
<p><code class="reqn">(n, d)</code>-matrix of data (typically before training or after sampling).</p>
</td></tr>
<tr><td><code id="trafos_margins_+3A_lower">lower</code></td>
<td>
<p>value or <code class="reqn">d</code>-vector typically
containing the smallest value of each column of <code>x</code>.</p>
</td></tr>
<tr><td><code id="trafos_margins_+3A_upper">upper</code></td>
<td>
<p>value or <code class="reqn">d</code>-vector typically
containing the largest value of each column of <code>x</code>.</p>
</td></tr>
<tr><td><code id="trafos_margins_+3A_mean">mean</code></td>
<td>
<p>value or <code class="reqn">d</code>-vector.</p>
</td></tr>
<tr><td><code id="trafos_margins_+3A_sd">sd</code></td>
<td>
<p>value or <code class="reqn">d</code>-vector.</p>
</td></tr>
<tr><td><code id="trafos_margins_+3A_slope">slope</code></td>
<td>
<p>value or <code class="reqn">d</code>-vector of slopes
of the linear transformations applied after applying
<code><a href="stats.html#topic+plogis">plogis</a>()</code> (before applying <code><a href="stats.html#topic+qlogis">qlogis</a>()</code> if
<code>inverse = TRUE</code>).</p>
</td></tr>
<tr><td><code id="trafos_margins_+3A_intercept">intercept</code></td>
<td>
<p>value or <code class="reqn">d</code>-vector of intercepts
of the linear transformations applied after applying
<code><a href="stats.html#topic+plogis">plogis</a>()</code> (before applying <code><a href="stats.html#topic+qlogis">qlogis</a>()</code> if
<code>inverse = TRUE</code>).</p>
</td></tr>
<tr><td><code id="trafos_margins_+3A_inverse">inverse</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether the inverses
of the respective transformations are to be computed (typically
used after generating data from a neural network trained on
data transformed with the respective transformation and
<code>inverse = FALSE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object as <code>x</code> containing the componentwise transformed data.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(gnn) # for being standalone

## Generate data
n &lt;- 100
set.seed(271)
x &lt;- cbind(rnorm(n), (1-runif(n))^(-1/2)-1) # normal and Pareto(2) margins
plot(x)

## Range transformation
ran &lt;- apply(x, 2, range) # column j = range of the jth column of x
x.ran &lt;- range_trafo(x, lower = ran[1,], upper = ran[2,]) # marginally transform to [0,1]
plot(x.ran) # =&gt; now range [0,1] but points a bit clustered around small y-values
x. &lt;- range_trafo(x.ran, lower = ran[1,], upper = ran[2,], inverse = TRUE) # transform back
stopifnot(all.equal(x., x)) # check

## Logistic transformation
x.logis &lt;- logis_trafo(x) # marginally transform to [0,1] via plogis()
plot(x.logis) # =&gt; y-range is [1/2, 1] which can be harder to train
x. &lt;- logis_trafo(x.logis, inverse = TRUE) # transform back
stopifnot(all.equal(x., x)) # check

## Logistic transformation with scaling to all of [0,1] in the second coordinate
x.logis.scale &lt;- logis_trafo(x, slope = 2, intercept = -1)
plot(x.logis.scale) # =&gt; now y-range is scaled to [0,1]
x. &lt;- logis_trafo(x.logis.scale, slope = 2, intercept = -1, inverse = TRUE) # transform back
stopifnot(all.equal(x., x)) # check

## Logistic transformation with sample mean and standard deviation and then
## transforming the range to [0,1] with a range transformation (note that
## slope = 2, intercept = -1 would not help here as the y-range is not [1/2, 1])
mu &lt;- colMeans(x)
sig &lt;- apply(x, 2, sd)
x.logis.fit &lt;- logis_trafo(x, mean = mu, sd = sig) # marginally plogis(, location, scale)
plot(x.logis.fit) # =&gt; y-range is not [1/2, 1] =&gt; use range transformation
ran &lt;- apply(x.logis.fit, 2, range)
x.logis.fit.ran &lt;- range_trafo(x.logis.fit, lower = ran[1,], upper = ran[2,])
plot(x.logis.fit.ran) # =&gt; now y-range is [1/2, 1]
x. &lt;- logis_trafo(range_trafo(x.logis.fit.ran, lower = ran[1,], upper = ran[2,],
                              inverse = TRUE),
                  mean = mu, sd = sig, inverse = TRUE) # transform back
stopifnot(all.equal(x., x)) # check

## Note that for heavy-tailed data, plogis() can fail to stay inside (0,1)
## even with adapting to sample mean and standard deviation. We now present
## a case where we see that using a fitted logistic distribution function
## is *just* good enough to numerically keep the data inside (0,1).
set.seed(271)
x &lt;- cbind(rnorm(n), (1-runif(n))^(-2)-1) # normal and Pareto(1/2) margins
plot(x) # =&gt; heavy-tailed in y-coordinate
## Transforming with standard logistic distribution function
x.logis &lt;- logis_trafo(x)
stopifnot(any(x.logis[,2] == 1))
## =&gt; There is value numerically indistinguishable from 1 to which applying
##    the inverse transform will lead to Inf
stopifnot(any(is.infinite(logis_trafo(x.logis, inverse = TRUE))))
## Now adapt the logistic distribution to share the mean and standard deviation
## with the data
mu &lt;- colMeans(x)
sig &lt;- apply(x, 2, sd)
x.logis.scale &lt;- logis_trafo(x, mean = mu, sd = sig)
stopifnot(all(x.logis.scale[,2] != 1)) # =&gt; no values equal to 1 anymore

## Alternatively, log() the data first, thus working with a log-logistic
## distribution as transformation
lx &lt;- cbind(x[,1], log(x[,2])) # 2nd coordinate only
lmu &lt;- c(mu[1], mean(lx[,2]))
lsig &lt;- c(sig[1], sd(lx[,2]))
x.llogis &lt;- logis_trafo(lx, mean = lmu, sd = lsig)
x. &lt;- logis_trafo(x.llogis, mean = lmu, sd = lsig, inverse = TRUE)
x.. &lt;- cbind(x.[,1], exp(x.[,2])) # undo log()
stopifnot(all.equal(x.., x))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
