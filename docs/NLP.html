<!DOCTYPE html><html><head><title>Help for package NLP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {NLP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#annotate'><p>Annotate text strings</p></a></li>
<li><a href='#AnnotatedPlainTextDocument'><p>Annotated Plain Text Documents</p></a></li>
<li><a href='#Annotation'><p>Annotation objects</p></a></li>
<li><a href='#annotations_in_spans'><p>Annotations contained in character spans</p></a></li>
<li><a href='#Annotator'><p>Annotator (pipeline) objects</p></a></li>
<li><a href='#annotators'><p>Simple annotator generators</p></a></li>
<li><a href='#CoNLLTextDocument'><p>CoNLL-Style Text Documents</p></a></li>
<li><a href='#CoNLLUTextDocument'>
<p>CoNNL-U Text Documents</p></a></li>
<li><a href='#datetime'><p>Parse ISO 8601 Date/Time Strings</p></a></li>
<li><a href='#features'><p>Extract Annotation Features</p></a></li>
<li><a href='#generics'><p>Access or Modify Content or Metadata</p></a></li>
<li><a href='#language'><p>Parse IETF Language Tag</p></a></li>
<li><a href='#ngrams'><p>Compute N-Grams</p></a></li>
<li><a href='#Span'><p>Span objects</p></a></li>
<li><a href='#String'><p>String objects</p></a></li>
<li><a href='#Tagged_Token'><p>Tagged_Token objects</p></a></li>
<li><a href='#TaggedTextDocument'><p>POS-Tagged Word Text Documents</p></a></li>
<li><a href='#tagsets'><p>NLP Tag Sets</p></a></li>
<li><a href='#TextDocument'><p>Text Documents</p></a></li>
<li><a href='#Tokenizer'><p>Tokenizer objects</p></a></li>
<li><a href='#tokenizers'><p>Regexp tokenizers</p></a></li>
<li><a href='#Tree'><p>Tree objects</p></a></li>
<li><a href='#utils'><p>Annotation Utilities</p></a></li>
<li><a href='#viewers'><p>Text Document Viewers</p></a></li>
<li><a href='#WordListDocument'><p>Word List Text Documents</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.2-1</td>
</tr>
<tr>
<td>Title:</td>
<td>Natural Language Processing Infrastructure</td>
</tr>
<tr>
<td>Description:</td>
<td>Basic classes and methods for Natural Language Processing.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>utils</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Enhances:</td>
<td>udpipe, spacyr, cleanNLP</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-10-14 13:54:10 UTC; hornik</td>
</tr>
<tr>
<td>Author:</td>
<td>Kurt Hornik <a href="https://orcid.org/0000-0003-4198-9911"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kurt Hornik &lt;Kurt.Hornik@R-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-10-14 14:06:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='annotate'>Annotate text strings</h2><span id='topic+annotate'></span>

<h3>Description</h3>

<p>Compute annotations by iteratively calling the given annotators with
the given text and current annotations, and merging the newly computed
annotations with the current ones.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>annotate(s, f, a = Annotation())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="annotate_+3A_s">s</code></td>
<td>
<p>a <code><a href="#topic+String">String</a></code> object, or something coercible to this
using <code><a href="#topic+as.String">as.String</a></code> (e.g., a character string with
appropriate encoding information).</p>
</td></tr>
<tr><td><code id="annotate_+3A_f">f</code></td>
<td>
<p>an <code><a href="#topic+Annotator">Annotator</a></code> or
<code><a href="#topic+Annotator_Pipeline">Annotator_Pipeline</a></code> object, or something coercible to
the latter via <code><a href="#topic+as.Annotator_Pipeline">as.Annotator_Pipeline</a>()</code> (such as a list
of annotator objects).</p>
</td></tr>
<tr><td><code id="annotate_+3A_a">a</code></td>
<td>
<p>an <code><a href="#topic+Annotation">Annotation</a></code> object giving the annotations to
start with.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code><a href="#topic+Annotation">Annotation</a></code> object containing the iteratively computed
and merged annotations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple text.
s &lt;- String("  First sentence.  Second sentence.  ")
##           ****5****0****5****0****5****0****5**

## A very trivial sentence tokenizer.
sent_tokenizer &lt;-
function(s) {
    s &lt;- as.String(s)
    m &lt;- gregexpr("[^[:space:]][^.]*\\.", s)[[1L]]
    Span(m, m + attr(m, "match.length") - 1L)
}
## (Could also use Regexp_Tokenizer() with the above regexp pattern.)
## A simple sentence token annotator based on the sentence tokenizer.
sent_token_annotator &lt;- Simple_Sent_Token_Annotator(sent_tokenizer)

## Annotate sentence tokens.
a1 &lt;- annotate(s, sent_token_annotator)
a1

## A very trivial word tokenizer.
word_tokenizer &lt;-
function(s) {
    s &lt;- as.String(s)
    ## Remove the last character (should be a period when using
    ## sentences determined with the trivial sentence tokenizer).
    s &lt;- substring(s, 1L, nchar(s) - 1L)
    ## Split on whitespace separators.
    m &lt;- gregexpr("[^[:space:]]+", s)[[1L]]
    Span(m, m + attr(m, "match.length") - 1L)
}
## A simple word token annotator based on the word tokenizer.
word_token_annotator &lt;- Simple_Word_Token_Annotator(word_tokenizer)

## Annotate word tokens using the already available sentence token
## annotations.
a2 &lt;- annotate(s, word_token_annotator, a1)
a2

## Can also perform sentence and word token annotations in a pipeline:
p &lt;- Annotator_Pipeline(sent_token_annotator, word_token_annotator)
annotate(s, p)
</code></pre>

<hr>
<h2 id='AnnotatedPlainTextDocument'>Annotated Plain Text Documents</h2><span id='topic+AnnotatedPlainTextDocument'></span><span id='topic+annotation'></span>

<h3>Description</h3>

<p>Create annotated plain text documents from plain text and collections
of annotations for this text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AnnotatedPlainTextDocument(s, a, meta = list())
annotation(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AnnotatedPlainTextDocument_+3A_s">s</code></td>
<td>
<p>a <code><a href="#topic+String">String</a></code> object, or something coercible to this
using <code><a href="#topic+as.String">as.String</a>()</code> (e.g., a character string with
appropriate encoding information).</p>
</td></tr>
<tr><td><code id="AnnotatedPlainTextDocument_+3A_a">a</code></td>
<td>
<p>an <code><a href="#topic+Annotation">Annotation</a></code> object with annotations for
<code>x</code>.</p>
</td></tr>
<tr><td><code id="AnnotatedPlainTextDocument_+3A_meta">meta</code></td>
<td>
<p>a named or empty list of document metadata tag-value
pairs.</p>
</td></tr>
<tr><td><code id="AnnotatedPlainTextDocument_+3A_x">x</code></td>
<td>
<p>an object inheriting from class
<code>"AnnotatedPlainTextDocument"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Annotated plain text documents combine plain text with annotations for
the text.
</p>
<p>A typical workflow is to use <code><a href="#topic+annotate">annotate</a>()</code> with suitable
annotator pipelines to obtain the annotations, and then use
<code>AnnotatedPlainTextDocument()</code> to combine these with the text
being annotated.  This yields an object inheriting from
<code>"AnnotatedPlainTextDocument"</code> and <code>"<a href="#topic+TextDocument">TextDocument</a>"</code>,
from which the text and annotations can be obtained using,
respectively, <code><a href="base.html#topic+as.character">as.character</a>()</code> and <code>annotation()</code>.
</p>
<p>There are methods for class <code>"AnnotatedPlainTextDocument"</code> and
generics
<code><a href="#topic+words">words</a>()</code>,
<code><a href="#topic+sents">sents</a>()</code>,
<code><a href="#topic+paras">paras</a>()</code>,
<code><a href="#topic+tagged_words">tagged_words</a>()</code>,
<code><a href="#topic+tagged_sents">tagged_sents</a>()</code>,
<code><a href="#topic+tagged_paras">tagged_paras</a>()</code>,
<code><a href="#topic+chunked_sents">chunked_sents</a>()</code>,
<code><a href="#topic+parsed_sents">parsed_sents</a>()</code> and
<code><a href="#topic+parsed_paras">parsed_paras</a>()</code>
providing structured views of the text in such documents.  These all
require the necessary annotations to be available in the annotation
object used.
</p>
<p>The methods for generics
<code><a href="#topic+tagged_words">tagged_words</a>()</code>,
<code><a href="#topic+tagged_sents">tagged_sents</a>()</code> and
<code><a href="#topic+tagged_paras">tagged_paras</a>()</code>
provide a mechanism for mapping POS tags via the <code>map</code> argument,
see section <b>Details</b> in the help page for
<code><a href="#topic+tagged_words">tagged_words</a>()</code> for more information.
The POS tagset used will be inferred from the <code>POS_tagset</code>
metadata element of the annotation object used.
</p>


<h3>Value</h3>

<p>For <code>AnnotatedPlainTextDocument()</code>, an annotated plain text
document object inheriting from
<code>"AnnotatedPlainTextTextDocument"</code> and
<code>"<a href="#topic+TextDocument">TextDocument</a>"</code>.
</p>
<p>For <code>annotation()</code>, an <code><a href="#topic+Annotation">Annotation</a></code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TextDocument">TextDocument</a></code> for basic information on the text document
infrastructure employed by package <span class="pkg">NLP</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Use a pre-built annotated plain text document obtained by employing an
## annotator pipeline from package 'StanfordCoreNLP', available from the
## repository at &lt;https://datacube.wu.ac.at&gt;, using the following code:
##   require("StanfordCoreNLP")
##   s &lt;- paste("Stanford University is located in California.",
##              "It is a great university.")
##   p &lt;- StanfordCoreNLP_Pipeline(c("pos", "lemma", "parse"))
##   doc &lt;- AnnotatedPlainTextDocument(s, p(s))

doc &lt;- readRDS(system.file("texts", "stanford.rds", package = "NLP"))

doc

## Extract available annotation:
a &lt;- annotation(doc)
a

## Structured views:
sents(doc)
tagged_sents(doc)
tagged_sents(doc, map = Universal_POS_tags_map)
parsed_sents(doc)

## Add (trivial) paragraph annotation:
s &lt;- as.character(doc)
a &lt;- annotate(s, Simple_Para_Token_Annotator(blankline_tokenizer), a)
doc &lt;- AnnotatedPlainTextDocument(s, a)
## Structured view:
paras(doc)
</code></pre>

<hr>
<h2 id='Annotation'>Annotation objects</h2><span id='topic+Annotation'></span><span id='topic+as.Annotation'></span><span id='topic+as.Annotation.Span'></span><span id='topic+is.Annotation'></span><span id='topic++5B.Annotation'></span><span id='topic++5B+5B.Annotation'></span><span id='topic++24+3C-.Annotation'></span><span id='topic+as.data.frame.Annotation'></span><span id='topic+as.list.Annotation'></span><span id='topic+c.Annotation'></span><span id='topic+duplicated.Annotation'></span><span id='topic+format.Annotation'></span><span id='topic+length.Annotation'></span><span id='topic+merge.Annotation'></span><span id='topic+meta.Annotation'></span><span id='topic+meta+3C-.Annotation'></span><span id='topic+names.Annotation'></span><span id='topic+print.Annotation'></span><span id='topic+subset.Annotation'></span><span id='topic+unique.Annotation'></span>

<h3>Description</h3>

<p>Creation and manipulation of annotation objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Annotation(id = NULL, type = NULL, start, end, features = NULL,
           meta = list())
as.Annotation(x, ...)
## S3 method for class 'Span'
as.Annotation(x, id = NULL, type = NULL, ...)
is.Annotation(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Annotation_+3A_id">id</code></td>
<td>
<p>an integer vector giving the annotation ids, or <code>NULL</code>
(default) resulting in missing ids.</p>
</td></tr>
<tr><td><code id="Annotation_+3A_type">type</code></td>
<td>
<p>a character vector giving the annotation types, or
<code>NULL</code> (default) resulting in missing types.</p>
</td></tr>
<tr><td><code id="Annotation_+3A_start">start</code>, <code id="Annotation_+3A_end">end</code></td>
<td>
<p>integer vectors giving the start and end positions
of the character spans the annotations refer to.</p>
</td></tr>
<tr><td><code id="Annotation_+3A_features">features</code></td>
<td>
<p>a list of (named or empty) feature lists, or
<code>NULL</code> (default), resulting in empty feature lists.</p>
</td></tr>
<tr><td><code id="Annotation_+3A_meta">meta</code></td>
<td>
<p>a named or empty list of annotation metadata tag-value
pairs.</p>
</td></tr>
<tr><td><code id="Annotation_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object (an object of class <code>"<a href="#topic+Span">Span</a>"</code> for the
coercion methods for such objects).</p>
</td></tr>
<tr><td><code id="Annotation_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A single annotation (of natural language text) is a quintuple with
&ldquo;slots&rdquo; &lsquo;id&rsquo;, &lsquo;type&rsquo;, &lsquo;start&rsquo;,
&lsquo;end&rsquo;, and &lsquo;features&rsquo;.  These give, respectively, id and
type, the character span the annotation refers to, and a collection of
annotation features (tag/value pairs).
</p>
<p>Annotation objects provide sequences (allowing positional access) of
single annotations, together with metadata about these.  They have
class <code>"Annotation"</code> and, as they contain character spans, also
inherit from class <code>"<a href="#topic+Span">Span</a>"</code>.  Span objects can be coerced
to annotation objects via <code>as.Annotation()</code> which allows to
specify ids and types (using the default values sets these to
missing), and annotation objects can be coerced to span objects using
<code><a href="#topic+as.Span">as.Span</a>()</code>.
</p>
<p>The features of a single annotation are represented as named or empty
lists.
</p>
<p>Subscripting annotation objects via <code>[</code> extracts subsets of
annotations; subscripting via <code>$</code> extracts the sequence of values
of the named slot, i.e., an integer vector for &lsquo;id&rsquo;,
&lsquo;start&rsquo;, and &lsquo;end&rsquo;, a character vector for
&lsquo;type&rsquo;, and a list of named or empty lists for
&lsquo;features&rsquo;.
</p>
<p>There are several additional methods for class <code>"Annotation"</code>:
<code>print()</code> and <code>format()</code> (which both have a <code>values</code>
argument which if <code>FALSE</code> suppresses indicating the feature map
values);
<code>c()</code> combines annotations (or objects coercible to these using
<code>as.Annotation()</code>);
<code>merge()</code> merges annotations by combining the feature lists of
annotations with otherwise identical slots;
<code>subset()</code> allows subsetting by expressions involving the slot
names; and
<code>as.list()</code> and <code>as.data.frame()</code> coerce, respectively, to
lists (of single annotation objects) and data frames (with annotations
and slots corresponding to rows and columns).
</p>
<p><code>Annotation()</code> creates annotation objects from the given sequences
of slot values: those not <code>NULL</code> must all have the same length
(the number of annotations in the object).
</p>
<p><code>as.Annotation()</code> coerces to annotation objects, with a method
for span objects.
</p>
<p><code>is.Annotation()</code> tests whether an object inherits from class
<code>"Annotation"</code>.
</p>


<h3>Value</h3>

<p>For <code>Annotation()</code> and <code>as.Annotation()</code>, an annotation
object (of class <code>"Annotation"</code> also inheriting from class
<code>"Span"</code>).
</p>
<p>For <code>is.Annotation()</code>, a logical.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple text.
s &lt;- String("  First sentence.  Second sentence.  ")
##           ****5****0****5****0****5****0****5**

## Basic sentence and word token annotations for the text.
a1s &lt;- Annotation(1 : 2,
                  rep.int("sentence", 2L),
                  c( 3L, 20L),
                  c(17L, 35L))
a1w &lt;- Annotation(3 : 6,
                  rep.int("word", 4L),
                  c( 3L,  9L, 20L, 27L),
                  c( 7L, 16L, 25L, 34L))

## Use c() to combine these annotations:
a1 &lt;- c(a1s, a1w)
a1
## Subscripting via '[':
a1[3 : 4]
## Subscripting via '$':
a1$type
## Subsetting according to slot values, directly:
a1[a1$type == "word"]
## or using subset():
subset(a1, type == "word")

## We can subscript string objects by annotation objects to extract the
## annotated substrings:
s[subset(a1, type == "word")]
## We can also subscript by lists of annotation objects:
s[annotations_in_spans(subset(a1, type == "word"),
                       subset(a1, type == "sentence"))]

## Suppose we want to add the sentence constituents (the ids of the
## words in the respective sentences) to the features of the sentence
## annotations.  The basic computation is
lapply(annotations_in_spans(a1[a1$type == "word"],
                            a1[a1$type == "sentence"]),
       function(a) a$id)
## For annotations, we need lists of feature lists:
features &lt;-
    lapply(annotations_in_spans(a1[a1$type == "word"],
                                a1[a1$type == "sentence"]),
           function(e) list(constituents = e$id))
## Could add these directly:
a2 &lt;- a1
a2$features[a2$type == "sentence"] &lt;- features
a2
## Note how the print() method summarizes the features.
## We could also write a sentence constituent annotator
## (note that annotators should always have formals 's' and 'a', even
## though for computing the sentence constituents s is not needed):
sent_constituent_annotator &lt;-
Annotator(function(s, a) {
              i &lt;- which(a$type == "sentence")
              features &lt;-
                  lapply(annotations_in_spans(a[a$type == "word"],
                                              a[i]),
                        function(e) list(constituents = e$id))
              Annotation(a$id[i], a$type[i], a$start[i], a$end[i],
                         features)
         })
sent_constituent_annotator(s, a1)
## Can use merge() to merge the annotations:
a2 &lt;- merge(a1, sent_constituent_annotator(s, a1))
a2
## Equivalently, could have used
a2 &lt;- annotate(s, sent_constituent_annotator, a1)
a2
## which merges automatically.
</code></pre>

<hr>
<h2 id='annotations_in_spans'>Annotations contained in character spans</h2><span id='topic+annotations_in_spans'></span>

<h3>Description</h3>

<p>Extract annotations contained in character spans.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>annotations_in_spans(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="annotations_in_spans_+3A_x">x</code></td>
<td>
<p>an <code><a href="#topic+Annotation">Annotation</a></code> object.</p>
</td></tr>
<tr><td><code id="annotations_in_spans_+3A_y">y</code></td>
<td>
<p>a <code><a href="#topic+Span">Span</a></code> object, or something coercible to this
(such as an <code><a href="#topic+Annotation">Annotation</a></code> object).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements the annotations in <code>x</code> with character spans
contained in the respective elements of <code>y</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple text.
s &lt;- String("  First sentence.  Second sentence.  ")
##           ****5****0****5****0****5****0****5**

## Basic sentence and word token annotation for the text.
a &lt;- c(Annotation(1 : 2,
                  rep.int("sentence", 2L),
                  c( 3L, 20L),
                  c(17L, 35L)),
       Annotation(3 : 6,
                  rep.int("word", 4L),
                  c( 3L,  9L, 20L, 27L),
                  c( 7L, 16L, 25L, 34L)))

## Annotation for word tokens according to sentence:
annotations_in_spans(a[a$type == "word"], a[a$type == "sentence"])
</code></pre>

<hr>
<h2 id='Annotator'>Annotator (pipeline) objects</h2><span id='topic+Annotator'></span><span id='topic+Annotator_Pipeline'></span><span id='topic+as.Annotator_Pipeline'></span>

<h3>Description</h3>

<p>Create annotator (pipeline) objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Annotator(f, meta = list(), classes = NULL)
Annotator_Pipeline(..., meta = list())
as.Annotator_Pipeline(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Annotator_+3A_f">f</code></td>
<td>
<p>an annotator function, which must have formals <code>s</code> and
<code>a</code> giving, respectively, the string with the natural language
text to annotate and an annotation object to start from, and return
an annotation object with the computed annotations.</p>
</td></tr>
<tr><td><code id="Annotator_+3A_meta">meta</code></td>
<td>
<p>an empty or named list of annotator (pipeline) metadata
tag-value pairs.</p>
</td></tr>
<tr><td><code id="Annotator_+3A_classes">classes</code></td>
<td>
<p>a character vector or <code>NULL</code> (default) giving
classes to be used for the created annotator object in addition to
<code>"Annotator"</code>.</p>
</td></tr>
<tr><td><code id="Annotator_+3A_...">...</code></td>
<td>
<p>annotator objects.</p>
</td></tr>
<tr><td><code id="Annotator_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Annotator()</code> checks that the given annotator function has the
appropriate formals, and returns an annotator object which inherits
from the given classes and <code>"Annotator"</code>.  There are
<code>print()</code> and <code>format()</code> methods for such objects, which use
the <code>description</code> element of the metadata if available.
</p>
<p><code>Annotator_Pipeline()</code> creates an annotator pipeline object from
the given annotator objects.  Such pipeline objects can be used by
<code><a href="#topic+annotate">annotate</a>()</code> for successively computing and merging
annotations, and can also be obtained by coercion with
<code>as.Annotator_Pipeline()</code>, which currently handles annotator
objects and lists of such (and of course, annotator pipeline
objects).
</p>


<h3>Value</h3>

<p>For <code>Annotator()</code>, an annotator object inheriting from the given
classes and class <code>"Annotator"</code>.
</p>
<p>For <code>Annotator_Pipeline()</code> and <code>as.Annotator_Pipeline()</code>, an
annotator pipeline object inheriting from class
<code>"Annotator_Pipeline"</code>.
</p>


<h3>See Also</h3>

<p><a href="#topic+Simple+20annotator+20generators">Simple annotator generators</a> for creating &ldquo;simple&rdquo;
annotator objects based on function performing simple basic NLP tasks.
</p>
<p>Package <span class="pkg">StanfordCoreNLP</span> available from the repository at
<a href="https://datacube.wu.ac.at">https://datacube.wu.ac.at</a> which provides generators for annotator
pipelines based on the Stanford CoreNLP tools.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Use blankline_tokenizer() for a simple paragraph token annotator:
para_token_annotator &lt;-
Annotator(function(s, a = Annotation()) {
              spans &lt;- blankline_tokenizer(s)
              n &lt;- length(spans)
              ## Need n consecutive ids, starting with the next "free"
              ## one:
              from &lt;- next_id(a$id)
              Annotation(seq(from = from, length.out = n),
                         rep.int("paragraph", n),
                         spans$start,
                         spans$end)
          },
          list(description = 
              "A paragraph token annotator based on blankline_tokenizer()."))
para_token_annotator
## Alternatively, use Simple_Para_Token_Annotator().

## A simple text with two paragraphs:
s &lt;- String(paste("  First sentence.  Second sentence.  ",
                  "  Second paragraph.  ",
                  sep = "\n\n"))
a &lt;- annotate(s, para_token_annotator)
## Annotations for paragraph tokens.
a
## Extract paragraph tokens.
s[a]
</code></pre>

<hr>
<h2 id='annotators'>Simple annotator generators</h2><span id='topic+Simple_Para_Token_Annotator'></span><span id='topic+Simple_Sent_Token_Annotator'></span><span id='topic+Simple_Word_Token_Annotator'></span><span id='topic+Simple_POS_Tag_Annotator'></span><span id='topic+Simple_Entity_Annotator'></span><span id='topic+Simple_Chunk_Annotator'></span><span id='topic+Simple_Stem_Annotator'></span><span id='topic+Simple+20annotator+20generators'></span>

<h3>Description</h3>

<p>Create annotator objects for composite basic NLP tasks based on
functions performing simple basic tasks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Simple_Para_Token_Annotator(f, meta = list(), classes = NULL)
Simple_Sent_Token_Annotator(f, meta = list(), classes = NULL)
Simple_Word_Token_Annotator(f, meta = list(), classes = NULL)
Simple_POS_Tag_Annotator(f, meta = list(), classes = NULL)
Simple_Entity_Annotator(f, meta = list(), classes = NULL)
Simple_Chunk_Annotator(f, meta = list(), classes = NULL)
Simple_Stem_Annotator(f, meta = list(), classes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="annotators_+3A_f">f</code></td>
<td>
<p>a function performing a &ldquo;simple&rdquo; basic NLP task (see
<b>Details</b>).</p>
</td></tr>
<tr><td><code id="annotators_+3A_meta">meta</code></td>
<td>
<p>an empty or named list of annotator (pipeline) metadata
tag-value pairs.</p>
</td></tr>
<tr><td><code id="annotators_+3A_classes">classes</code></td>
<td>
<p>a character vector or <code>NULL</code> (default) giving
classes to be used for the created annotator object in addition to
the default ones (see <b>Details</b>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The purpose of these functions is to facilitate the creation of
annotators for basic NLP tasks as described below.
</p>
<p><code>Simple_Para_Token_Annotator()</code> creates &ldquo;simple&rdquo; paragraph
token annotators.  Argument <code>f</code> should be a paragraph tokenizer,
which takes a string <code>s</code> with the whole text to be processed, and
returns the spans of the paragraphs in <code>s</code>, or an annotation
object with these spans and (possibly) additional features.  The
generated annotator inherits from the default classes
<code>"Simple_Para_Token_Annotator"</code> and <code>"Annotator"</code>.  It uses
the results of the simple paragraph tokenizer to create and return
annotations with unique ids and type &lsquo;paragraph&rsquo;.
</p>
<p><code>Simple_Sent_Token_Annotator()</code> creates &ldquo;simple&rdquo; sentence
token annotators.  Argument <code>f</code> should be a sentence tokenizer,
which takes a string <code>s</code> with the whole text to be processed, and
returns the spans of the sentences in <code>s</code>, or an annotation
object with these spans and (possibly) additional features.  The
generated annotator inherits from the default classes
<code>"Simple_Sent_Token_Annotator"</code> and <code>"Annotator"</code>.  It uses
the results of the simple sentence tokenizer to create and return
annotations with unique ids and type &lsquo;sentence&rsquo;, possibly
combined with sentence constituent features for already available
paragraph annotations.
</p>
<p><code>Simple_Word_Token_Annotator()</code> creates &ldquo;simple&rdquo; word
token annotators.  Argument <code>f</code> should be a simple word
tokenizer, which takes a string <code>s</code> giving a sentence to be
processed, and returns the spans of the word tokens in <code>s</code>, or an 
annotation object with these spans and (possibly) additional features.
The generated annotator inherits from the default classes
<code>"Simple_Word_Token_Annotator"</code> and <code>"Annotator"</code>.
It uses already available sentence token annotations to extract the
sentences and obtains the results of the word tokenizer for these.  It
then adds the sentence character offsets and unique word token ids,
and word token constituents features for the sentences, and returns
the word token annotations combined with the augmented sentence token
annotations.
</p>
<p><code>Simple_POS_Tag_Annotator()</code> creates &ldquo;simple&rdquo; POS tag
annotators.  Argument <code>f</code> should be a simple POS tagger, which
takes a character vector giving the word tokens in a sentence, and
returns either a character vector with the tags, or a list of feature
maps with the tags as &lsquo;POS&rsquo; feature and possibly other
features.  The generated annotator inherits from the default classes
<code>"Simple_POS_Tag_Annotator"</code> and <code>"Annotator"</code>.  It uses
already available sentence and word token annotations to extract the
word tokens for each sentence and obtains the results of the simple
POS tagger for these, and returns annotations for the word tokens with
the features obtained from the POS tagger.
</p>
<p><code>Simple_Entity_Annotator()</code> creates &ldquo;simple&rdquo; entity
annotators.  Argument <code>f</code> should be a simple entity detector
(&ldquo;named entity recognizer&rdquo;) which takes a character vector
giving the word tokens in a sentence, and return an annotation object
with the <em>word</em> token spans, a &lsquo;kind&rsquo; feature giving the
kind of the entity detected, and possibly other features.  The
generated annotator inherits from the default classes
<code>"Simple_Entity_Annotator"</code> and <code>"Annotator"</code>.  It uses
already available sentence and word token annotations to extract the
word tokens for each sentence and obtains the results of the simple
entity detector for these, transforms word token spans to character
spans and adds unique ids, and returns the combined entity
annotations.
</p>
<p><code>Simple_Chunk_Annotator()</code> creates &ldquo;simple&rdquo; chunk
annotators.  Argument <code>f</code> should be a simple chunker, which takes
as arguments character vectors giving the word tokens and the
corresponding POS tags, and returns either a character vector with the
chunk tags, or a list of feature lists with the tags as
&lsquo;chunk_tag&rsquo; feature and possibly other features.  The generated
annotator inherits from the default classes
<code>"Simple_Chunk_Annotator"</code> and <code>"Annotator"</code>.  It uses
already available annotations to extract the word tokens and POS tags
for each sentence and obtains the results of the simple chunker for
these, and returns word token annotations with the chunk features
(only).
</p>
<p><code>Simple_Stem_Annotator()</code> creates &ldquo;simple&rdquo; stem
annotators.  Argument <code>f</code> should be a simple stemmer, which takes
as arguments a character vector giving the word tokens, and returns a
character vector with the corresponding word stems.  The generated
annotator inherits from the default classes
<code>"Simple_Stem_Annotator"</code> and <code>"Annotator"</code>.  It uses
already available annotations to extract the word tokens, and returns
word token annotations with the corresponding stem features (only).
</p>
<p>In all cases, if the underlying simple processing function returns
annotation objects these should not provide their own ids (or use such
in the features), as the generated annotators will necessarily provide
these (the already available annotations are only available at the
annotator level, but not at the simple processing level).
</p>


<h3>Value</h3>

<p>An annotator object inheriting from the given classes and the default
ones.
</p>


<h3>See Also</h3>

<p>Package <span class="pkg">openNLP</span> which provides annotator generators for sentence
and word tokens, POS tags, entities and chunks, using processing
functions based on the respective Apache OpenNLP MaxEnt processing
resources.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple text.
s &lt;- String("  First sentence.  Second sentence.  ")
##           ****5****0****5****0****5****0****5**

## A very trivial sentence tokenizer.
sent_tokenizer &lt;-
function(s) {
    s &lt;- as.String(s)
    m &lt;- gregexpr("[^[:space:]][^.]*\\.", s)[[1L]]
    Span(m, m + attr(m, "match.length") - 1L)
}
## (Could also use Regexp_Tokenizer() with the above regexp pattern.)
sent_tokenizer(s)
## A simple sentence token annotator based on the sentence tokenizer.
sent_token_annotator &lt;- Simple_Sent_Token_Annotator(sent_tokenizer)
sent_token_annotator
a1 &lt;- annotate(s, sent_token_annotator)
a1
## Extract the sentence tokens.
s[a1]

## A very trivial word tokenizer.
word_tokenizer &lt;-
function(s) {
    s &lt;- as.String(s)
    ## Remove the last character (should be a period when using
    ## sentences determined with the trivial sentence tokenizer).
    s &lt;- substring(s, 1L, nchar(s) - 1L)
    ## Split on whitespace separators.
    m &lt;- gregexpr("[^[:space:]]+", s)[[1L]]
    Span(m, m + attr(m, "match.length") - 1L)
}
lapply(s[a1], word_tokenizer)
## A simple word token annotator based on the word tokenizer.
word_token_annotator &lt;- Simple_Word_Token_Annotator(word_tokenizer)
word_token_annotator
a2 &lt;- annotate(s, word_token_annotator, a1)
a2
## Extract the word tokens.
s[subset(a2, type == "word")]

## A simple word token annotator based on wordpunct_tokenizer():
word_token_annotator &lt;-
    Simple_Word_Token_Annotator(wordpunct_tokenizer,
                                list(description =
                                     "Based on wordpunct_tokenizer()."))
word_token_annotator
a2 &lt;- annotate(s, word_token_annotator, a1)
a2
## Extract the word tokens.
s[subset(a2, type == "word")]
</code></pre>

<hr>
<h2 id='CoNLLTextDocument'>CoNLL-Style Text Documents</h2><span id='topic+CoNLLTextDocument'></span>

<h3>Description</h3>

<p>Create text documents from CoNLL-style files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CoNLLTextDocument(con, encoding = "unknown", format = "conll00",
                  meta = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CoNLLTextDocument_+3A_con">con</code></td>
<td>
<p>a connection object or a character string.
See <code><a href="base.html#topic+scan">scan</a>()</code> for details.
</p>
</td></tr>
<tr><td><code id="CoNLLTextDocument_+3A_encoding">encoding</code></td>
<td>
<p>encoding to be assumed for input strings.
See <code><a href="base.html#topic+scan">scan</a>()</code> for details.
</p>
</td></tr>
<tr><td><code id="CoNLLTextDocument_+3A_format">format</code></td>
<td>
<p>a character vector specifying the format.
See <b>Details</b>.
</p>
</td></tr>
<tr><td><code id="CoNLLTextDocument_+3A_meta">meta</code></td>
<td>
<p>a named or empty list of document metadata tag-value
pairs.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>CoNLL-style files use an extended tabular format where empty lines
separate sentences, and non-empty lines consist of whitespace
separated columns giving the word tokens and annotations for these.
Such formats were popularized through their use for the shared tasks
of CoNLL (Conference on Natural Language Learning), the yearly meeting
of the Special Interest Group on Natural Language Learning of the
Association for Computational Linguistics (see
<a href="https://www.signll.org/conll/">https://www.signll.org/conll/</a> for more information about CoNLL).
</p>
<p>The precise format can vary according to corpus, and must be specified
via argument <code>format</code>, as either a character string giving a
pre-defined format, or otherwise a character vector with elements
giving the names of the &lsquo;fields&rsquo; (columns), and names used to
give the field &lsquo;types&rsquo;, with &lsquo;WORD&rsquo;, &lsquo;POS&rsquo; and
&lsquo;CHUNK&rsquo; to be used for, respectively, word tokens, POS tags, and
chunk tags.  For example, </p>
<pre>  c(WORD = "WORD", POS = "POS", CHUNK = "CHUNK")</pre>
<p>would be a format specification appropriate for the CoNLL-2000
chunking task, as also available as the pre-defined <code>"conll00"</code>,
which serves as default format for reasons of back-compatibility.
Other pre-defined formats are <code>"conll01"</code> (for the CoNLL-2001
clause identification task), <code>"conll02"</code> (for the CoNLL-2002
language-independent named entity recognition task), <code>"conllx"</code>
(for the CoNLL-X format used in at least the CoNLL-2006 and CoNLL-2007
multilingual dependency parsing tasks), and <code>"conll09"</code> (for the
CoNLL-2009 shared task on syntactic and semantic dependencies in
multiple languages).
</p>
<p>The lines are read from the given connection and split into fields
using <code><a href="base.html#topic+scan">scan</a>()</code>.  From this, a suitable representation of
the provided information is obtained, and returned as a CoNLL text
document object inheriting from classes <code>"CoNLLTextDocument"</code> and
<code>"<a href="#topic+TextDocument">TextDocument</a>"</code>.
</p>
<p>There are methods for class <code>"CoNLLTextDocument"</code> and generics
<code><a href="#topic+words">words</a>()</code>,
<code><a href="#topic+sents">sents</a>()</code>,
<code><a href="#topic+tagged_words">tagged_words</a>()</code>,
<code><a href="#topic+tagged_sents">tagged_sents</a>()</code>, and
<code><a href="#topic+chunked_sents">chunked_sents</a>()</code>
(as well as <code><a href="base.html#topic+as.character">as.character</a>()</code>),
which should be used to access the text in such text document
objects.
</p>
<p>The methods for generics
<code><a href="#topic+tagged_words">tagged_words</a>()</code> and
<code><a href="#topic+tagged_sents">tagged_sents</a>()</code>
provide a mechanism for mapping POS tags via the <code>map</code> argument,
see section <b>Details</b> in the help page for
<code><a href="#topic+tagged_words">tagged_words</a>()</code> for more information.
The POS tagset used will be inferred from the <code>POS_tagset</code>
metadata element of the CoNLL-style text document.
</p>


<h3>Value</h3>

<p>An object inheriting from <code>"CoNLLTextDocument"</code> and
<code>"<a href="#topic+TextDocument">TextDocument</a>"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TextDocument">TextDocument</a></code> for basic information on the text document
infrastructure employed by package <span class="pkg">NLP</span>.
</p>
<p><a href="https://www.clips.uantwerpen.be/conll2000/chunking/">https://www.clips.uantwerpen.be/conll2000/chunking/</a> for the
CoNLL-2000 chunking task, and training and test data sets which can be
read in using <code>CoNLLTextDocument()</code>.
</p>

<hr>
<h2 id='CoNLLUTextDocument'>
CoNNL-U Text Documents
</h2><span id='topic+CoNLLUTextDocument'></span>

<h3>Description</h3>

<p>Create text documents from CoNNL-U format files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CoNLLUTextDocument(con, meta = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CoNLLUTextDocument_+3A_con">con</code></td>
<td>
<p>a connection object or a character string.
See <code><a href="base.html#topic+scan">scan</a>()</code> for details.
</p>
</td></tr>
<tr><td><code id="CoNLLUTextDocument_+3A_meta">meta</code></td>
<td>
<p>a named or empty list of document metadata tag-value
pairs.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The CoNLL-U format (see
<a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>)
is a CoNLL-style format for annotated texts popularized and employed
by the Universal Dependencies project
(see <a href="https://universaldependencies.org/">https://universaldependencies.org/</a>).
For each &ldquo;word&rdquo; in the text, this provides exactly the 10
fields
<code>ID</code>,
<code>FORM</code> (word form or punctuation symbol),
<code>LEMMA</code> (lemma or stem of word form),
<code>UPOSTAG</code> (universal part-of-speech tag, see
<a href="https://universaldependencies.org/u/pos/index.html">https://universaldependencies.org/u/pos/index.html</a>),
<code>XPOSTAG</code> (language-specific part-of-speech tag, may be
unavailable),
<code>FEATS</code> (list of morphological features),
<code>HEAD</code>,
<code>DEPREL</code>,
<code>DEPS</code>, and
<code>MISC</code>.
</p>
<p>The lines with these fields and optional comments are read from the
given connection and split into fields using <code><a href="base.html#topic+scan">scan</a>()</code>.
This is combined with consecutive sentence ids into a data frame used
for representing the annotation information, and together with the
given metadata returned as a CoNLL-U text document inheriting from
classes <code>"CoNLLUTextDocument"</code> and <code>"<a href="#topic+TextDocument">TextDocument</a>"</code>.
</p>
<p>The complete annotation information data frame can be extracted via
<code>content()</code>.  CoNLL-U v2 requires providing the complete texts of
each sentence (or a reconstruction thereof) in &lsquo;<span class="samp">&#8288;# text =&#8288;</span>&rsquo; comment
lines.  Where consistently provided, these are made available in the
<code>text</code> attribute of the content data frame.
</p>
<p>In addition, there are methods for generics
<code><a href="base.html#topic+as.character">as.character</a>()</code>,
<code><a href="#topic+words">words</a>()</code>,
<code><a href="#topic+sents">sents</a>()</code>,
<code><a href="#topic+tagged_words">tagged_words</a>()</code>, and
<code><a href="#topic+tagged_sents">tagged_sents</a>()</code>
and class <code>"CoNLLUTextDocument"</code>,
which should be used to access the text in such text document
objects.
</p>
<p>The CoNLL-U format allows to represent both words and (multiword)
tokens (see section &lsquo;Words, Tokens and Empty Nodes&rsquo; in the
format documentation), as distinguished by ids being integers or
integer ranges, with the words being annotated further.  One can
use <code>as.character()</code> to extract the <em>tokens</em>; all other
viewers listed above use the <em>words</em>.  Finally, the viewers
incorporating POS tags take a <code>which</code> argument to specify using
the universal or language-specific tags, by giving a substring of
<code>"UPOSTAG"</code> (default) or <code>"XPOSTAG"</code>.
</p>


<h3>Value</h3>

<p>An object inheriting from <code>"CoNLLUTextDocument"</code> and
<code>"<a href="#topic+TextDocument">TextDocument</a>"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TextDocument">TextDocument</a></code> for basic information on the text document
infrastructure employed by package <span class="pkg">NLP</span>.
</p>
<p><a href="https://universaldependencies.org/">https://universaldependencies.org/</a> for access to the Universal
Dependencies treebanks, which provide annotated texts in <em>many</em>
different languages using CoNLL-U format.
</p>

<hr>
<h2 id='datetime'>Parse ISO 8601 Date/Time Strings</h2><span id='topic+parse_ISO_8601_datetime'></span>

<h3>Description</h3>

<p>Extract date/time components from strings following one of the six
formats specified in the NOTE-datetime ISO 8601 profile
(<a href="https://www.w3.org/TR/NOTE-datetime">https://www.w3.org/TR/NOTE-datetime</a>).
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="datetime_+3A_x">x</code></td>
<td>
<p>a character vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For character strings in one of the formats in the profile, the
corresponding date/time components are extracted, with seconds and
decimal fractions of seconds combined.  Other (malformed) strings are
warned about.
</p>
<p>The extracted components for each string are gathered into a named
list with elements of the appropriate type (integer for year to min;
double for sec; character for the time zone designator).  The object
returned is a (suitably classed) list of such named lists.  This
internal representation may change in future versions.
</p>
<p>One can subscript such ISO 8601 date/time objects using <code>[</code> and
extract components using <code>$</code> (where missing components will
result in <code>NA</code>s), and convert them to the standard R date/time
classes using <code><a href="base.html#topic+as.Date">as.Date</a>()</code>, <code><a href="base.html#topic+as.POSIXct">as.POSIXct</a>()</code> and
<code><a href="base.html#topic+as.POSIXlt">as.POSIXlt</a>()</code> (incomplete elements will convert to
suitably missing elements).  In addition, there are <code>print()</code> and
<code>as.data.frame()</code> methods for such objects.
</p>


<h3>Value</h3>

<p>An object inheriting from class <code>"ISO_8601_datetime"</code> with the
extracted date/time components.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Use the examples from &lt;https://www.w3.org/TR/NOTE-datetime&gt;, plus one
## in UTC.
x &lt;- c("1997",
       "1997-07",
       "1997-07-16",
       "1997-07-16T19:20+01:00",
       "1997-07-16T19:20:30+01:00",
       "1997-07-16T19:20:30.45+01:00",
       "1997-07-16T19:20:30.45Z")
y &lt;- parse_ISO_8601_datetime(x)
y
## Conversions: note that "incomplete" elements are converted to
## "missing".
as.Date(y)
as.POSIXlt(y)
## Subscripting and extracting components:
head(y, 3)
y$mon
</code></pre>

<hr>
<h2 id='features'>Extract Annotation Features</h2><span id='topic+features'></span>

<h3>Description</h3>

<p>Conveniently extract features from annotations and annotated plain
text documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>features(x, type = NULL, simplify = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="features_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>"Annotation"</code> or
<code>"AnnotatedPlainTextDocument"</code>.</p>
</td></tr>
<tr><td><code id="features_+3A_type">type</code></td>
<td>
<p>a character vector of annotation types to be used for
selecting annotations, or <code>NULL</code> (default) to use all
annotations.  When selecting, the elements of <code>type</code> will
partially be matched against the annotation types.</p>
</td></tr>
<tr><td><code id="features_+3A_simplify">simplify</code></td>
<td>
<p>a logical indicating whether to simplify feature
values to a vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>features()</code> conveniently gathers all feature tag-value pairs in
the selected annotations into a data frame with variables the values
for all tags found (using a <code>NULL</code> value for tags without a
value).  In general, variables will be <em>lists</em> of extracted
values.  By default, variables where all elements are length one
atomic vectors are simplified into an atomic vector of values.  The
values for specific tags can be extracted by suitably subscripting the
obtained data frame. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Use a pre-built annotated plain text document,
## see ? AnnotatedPlainTextDocument.
doc &lt;- readRDS(system.file("texts", "stanford.rds", package = "NLP"))
## Extract features of all *word* annotations in doc:
x &lt;- features(doc, "word")
## Could also have abbreviated "word" to "w".
x
## Only lemmas:
x$lemma
## Words together with lemmas:
paste(words(doc), x$lemma, sep = "/")
</code></pre>

<hr>
<h2 id='generics'>Access or Modify Content or Metadata</h2><span id='topic+content'></span><span id='topic+content+3C-'></span><span id='topic+meta'></span><span id='topic+meta+3C-'></span>

<h3>Description</h3>

<p>Access or modify the content or metadata of <span class="rlang"><b>R</b></span> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>content(x)
content(x) &lt;- value
meta(x, tag = NULL, ...)
meta(x, tag = NULL, ...) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generics_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="generics_+3A_value">value</code></td>
<td>
<p>a suitable <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="generics_+3A_tag">tag</code></td>
<td>
<p>a character string or <code>NULL</code> (default), indicating to
return the single metadata value for the given tag, or all metadata
tag/value pairs.</p>
</td></tr>
<tr><td><code id="generics_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are generic functions, with no default methods.
</p>
<p>Often, classed <span class="rlang"><b>R</b></span> objects (e.g., those representing text documents in
packages <span class="pkg">NLP</span> and <span class="pkg">tm</span>) contain information that can be
grouped into &ldquo;content&rdquo;, metadata and other components, where
content can be arbitrary, and metadata are collections of tag/value
pairs represented as named or empty lists.  The <code>content()</code> and
<code>meta()</code> getters and setters aim at providing a consistent
high-level interface to the respective information (abstracting from
how classes internally represent the information).
</p>


<h3>Value</h3>

<p>Methods for <code>meta()</code> should return a named or empty list of
tag/value pairs if no tag is given (default), or the value for the
given tag.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TextDocument">TextDocument</a></code> for basic information on the text document
infrastructure employed by package <span class="pkg">NLP</span>.
</p>

<hr>
<h2 id='language'>Parse IETF Language Tag</h2><span id='topic+parse_IETF_language_tag'></span>

<h3>Description</h3>

<p>Extract language, script, region and variant subtags from IETF
language tags.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_IETF_language_tag(x, expand = FALSE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="language_+3A_x">x</code></td>
<td>
<p>a character vector with IETF language tags.</p>
</td></tr>
<tr><td><code id="language_+3A_expand">expand</code></td>
<td>
<p>a logical indicating whether to expand subtags into
their description(s).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internet Engineering Task Force (IETF) language tags are defined by
IETF BCP 47, which is currently composed by the normative RFC 5646
(<a href="https://tools.ietf.org/html/rfc5646">https://tools.ietf.org/html/rfc5646</a>) and RFC 4647
(<a href="https://tools.ietf.org/html/rfc4646">https://tools.ietf.org/html/rfc4646</a>), along with the normative
content of the IANA Language Subtag Registry regulated by these RFCs.
These tags are used in a number of modern computing standards.
</p>
<p>Each language tag is composed of one or more &ldquo;subtags&rdquo;
separated by hyphens.  Normal language tags have the following
subtags:
</p>

<ul>
<li><p> a language subtag (optionally, with language extension
subtags),
</p>
</li>
<li><p> an optional script subtag,
</p>
</li>
<li><p> an optional region subtag,
</p>
</li>
<li><p> optional variant subtags,
</p>
</li>
<li><p> optional extension subtags,
</p>
</li>
<li><p> an optional private use subtag.
</p>
</li></ul>

<p>Language subtags are mainly derived from ISO 639-1 and ISO 639-2,
script subtags from ISO 15924, and region subtags from ISO 3166-1
alpha-2 and UN M.49, see package <span class="pkg">ISOcodes</span> for more information
about these standards.  Variant subtags are not derived from any
standard.  The Language Subtag Registry
(<a href="https://www.iana.org/assignments/language-subtag-registry">https://www.iana.org/assignments/language-subtag-registry</a>),
maintained by the Internet Assigned Numbers Authority (IANA), lists
the current valid public subtags, as well as the so-called
&ldquo;grandfathered&rdquo; language tags.
</p>
<p>See <a href="https://en.wikipedia.org/wiki/IETF_language_tag">https://en.wikipedia.org/wiki/IETF_language_tag</a> for more
information.
</p>


<h3>Value</h3>

<p>If <code>expand</code> is false, a list of character vectors of the form
<code>"<var>type</var>=<var>subtag</var>"</code>, where <var>type</var> gives the type of
the corresponding subtag (one of &lsquo;Language&rsquo;, &lsquo;Extlang&rsquo;,
&lsquo;Script&rsquo;, &lsquo;Region&rsquo;, &lsquo;Variant&rsquo;, or
&lsquo;Extension&rsquo;), or <code>"<var>type</var>=<var>tag</var>"</code> with <var>type</var>
either &lsquo;Privateuse&rsquo; or &lsquo;Grandfathered&rsquo;.
</p>
<p>Otherwise, a list of lists of character vectors obtained by replacing
the subtags by their corresponding descriptions (which may be
multiple) from the IANA registry.  Note that no such descriptions for
Extension and Privateuse subtags are available in the registry; on the
other hand, empty expansions of the other subtags indicate malformed
tags (as these subtags must be available in the registry).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## German as used in Switzerland:
parse_IETF_language_tag("de-CH")
## Serbian written using Latin script as used in Serbia and Montenegro:
parse_IETF_language_tag("sr-Latn-CS")
## Spanish appropriate to the UN Latin American and Caribbean region:
parse_IETF_language_tag("es-419")
## All in one:
parse_IETF_language_tag(c("de-CH", "sr-Latn-CS", "es-419"))
parse_IETF_language_tag(c("de-CH", "sr-Latn-CS", "es-419"),
                        expand = TRUE)
## Two grandfathered tags:
parse_IETF_language_tag(c("i-klingon", "zh-min-nan"),
                        expand = TRUE)
</code></pre>

<hr>
<h2 id='ngrams'>Compute N-Grams</h2><span id='topic+ngrams'></span>

<h3>Description</h3>

<p>Compute the <code class="reqn">n</code>-grams (contiguous sub-sequences of length <code class="reqn">n</code>)
of a given sequence.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="ngrams_+3A_x">x</code></td>
<td>
<p>a sequence (vector).</p>
</td></tr>
<tr><td><code id="ngrams_+3A_n">n</code></td>
<td>
<p>a positive integer giving the length of contiguous
sub-sequences to be computed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the computed sub-sequences.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>s &lt;- "The quick brown fox jumps over the lazy dog"
## Split into words:
w &lt;- strsplit(s, " ", fixed = TRUE)[[1L]]
## Word tri-grams:
ngrams(w, 3L)
## Word tri-grams pasted together:
vapply(ngrams(w, 3L), paste, "", collapse = " ")
</code></pre>

<hr>
<h2 id='Span'>Span objects</h2><span id='topic+Span'></span><span id='topic+as.Span'></span><span id='topic+is.Span'></span><span id='topic++5B.Span'></span><span id='topic++5B+5B.Span'></span><span id='topic++24+3C-.Span'></span><span id='topic+Ops.Span'></span><span id='topic+as.data.frame.Span'></span><span id='topic+as.list.Span'></span><span id='topic+c.Span'></span><span id='topic+duplicated.Span'></span><span id='topic+format.Span'></span><span id='topic+length.Span'></span><span id='topic+names.Span'></span><span id='topic+print.Span'></span><span id='topic+unique.Span'></span>

<h3>Description</h3>

<p>Creation and manipulation of span objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Span(start, end)
as.Span(x)
is.Span(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Span_+3A_start">start</code>, <code id="Span_+3A_end">end</code></td>
<td>
<p>integer vectors giving the start and end positions
of the spans.</p>
</td></tr>
<tr><td><code id="Span_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A single span is a pair with &ldquo;slots&rdquo; &lsquo;start&rsquo; and
&lsquo;end&rsquo;, giving the start and end positions of the span.
</p>
<p>Span objects provide sequences (allowing positional access) of single
spans.  They have class <code>"Span"</code>.  Span objects can be coerced to
annotation objects via <code><a href="#topic+as.Annotation">as.Annotation</a>()</code> (which of course is
only appropriate provided that the spans are character spans of the
natural language text being annotated), and annotation objects can be
coerced to span objects via <code>as.Span()</code> (giving the character spans
of the annotations).
</p>
<p>Subscripting span objects via <code>[</code> extracts subsets of spans;
subscripting via <code>$</code> extracts integer vectors with the sequence
of values of the named slot.
</p>
<p>There are several additional methods for class <code>"Span"</code>:
<code>print()</code> and <code>format()</code>;
<code>c()</code> combines spans (or objects coercible to these using
<code>as.Span()</code>), and
<code>as.list()</code> and <code>as.data.frame()</code> coerce, respectively, to
lists (of single span objects) and data frames (with spans and slots
corresponding to rows and columns).  Finally, one can add a scalar and
a span object (resulting in shifting the start and end positions by
the scalar).
</p>
<p><code>Span()</code> creates span objects from the given sequences of start
and end positions, which must have the same length.
</p>
<p><code>as.Span()</code> coerces to span objects, with a method for annotation
objects.
</p>
<p><code>is.Span()</code> tests whether an object inherits from class
<code>"Span"</code> (and hence returns <code>TRUE</code> for both span and
annotation objects).
</p>


<h3>Value</h3>

<p>For <code>Span()</code> and <code>as.Span()</code>, a span object (of class
<code>"Span"</code>).
</p>
<p>For <code>is.Span()</code>, a logical.
</p>

<hr>
<h2 id='String'>String objects</h2><span id='topic+String'></span><span id='topic+as.String'></span><span id='topic+is.String'></span>

<h3>Description</h3>

<p>Creation and manipulation of string objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>String(x)
as.String(x)
is.String(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="String_+3A_x">x</code></td>
<td>
<p>a character vector with the appropriate encoding information
for <code>String()</code>; an arbitrary <span class="rlang"><b>R</b></span> object otherwise.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>String objects provide character strings encoded in UTF-8 with class
<code>"String"</code>, which currently has a useful <code>[</code> subscript
method: with indices <code>i</code> and <code>j</code> of length one, this gives a
string object with the substring starting at the position given by
<code>i</code> and ending at the position given by <code>j</code>; subscripting
with a single index which is an object inheriting from class
<code>"<a href="#topic+Span">Span</a>"</code> or a list of such objects returns a character
vector of substrings with the respective spans, or a list thereof.
</p>
<p>Additional methods may be added in the future.
</p>
<p><code>String()</code> creates a string object from a given character vector,
taking the first element of the vector and converting it to UTF-8
encoding.
</p>
<p><code>as.String()</code> is a generic function to coerce to a string object.
The default method calls <code>String()</code> on the result of converting
to character and concatenating into a single string with the elements
separated by newlines.
</p>
<p><code>is.String()</code> tests whether an object inherits from class
<code>"String"</code>.
</p>


<h3>Value</h3>

<p>For <code>String()</code> and <code>as.String()</code>, a string object (of class 
<code>"String"</code>).
</p>
<p>For <code>is.String()</code>, a logical.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple text.
s &lt;- String("  First sentence.  Second sentence.  ")
##           ****5****0****5****0****5****0****5**

## Basic sentence and word token annotation for the text.
a &lt;- c(Annotation(1 : 2,
                  rep.int("sentence", 2L),
                  c( 3L, 20L),
                  c(17L, 35L)),
       Annotation(3 : 6,
                  rep.int("word", 4L),
                  c( 3L,  9L, 20L, 27L),
                  c( 7L, 16L, 25L, 34L)))

## All word tokens (by subscripting with an annotation object):
s[a[a$type == "word"]]
## Word tokens according to sentence (by subscripting with a list of
## annotation objects):
s[annotations_in_spans(a[a$type == "word"], a[a$type == "sentence"])]
</code></pre>

<hr>
<h2 id='Tagged_Token'>Tagged_Token objects</h2><span id='topic+Tagged_Token'></span><span id='topic+as.Tagged_Token'></span><span id='topic+is.Tagged_Token'></span><span id='topic++5B.Tagged_Token'></span><span id='topic++5B+5B.Tagged_Token'></span><span id='topic++24+3C-.Tagged_Token'></span><span id='topic+as.data.frame.Tagged_Token'></span><span id='topic+as.list.Tagged_Token'></span><span id='topic+c.Tagged_Token'></span><span id='topic+duplicated.Tagged_Token'></span><span id='topic+format.Tagged_Token'></span><span id='topic+length.Tagged_Token'></span><span id='topic+names.Tagged_Token'></span><span id='topic+print.Tagged_Token'></span><span id='topic+unique.Tagged_Token'></span>

<h3>Description</h3>

<p>Creation and manipulation of tagged token objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Tagged_Token(token, tag)
as.Tagged_Token(x)
is.Tagged_Token(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tagged_Token_+3A_token">token</code>, <code id="Tagged_Token_+3A_tag">tag</code></td>
<td>
<p>character vectors giving tokens and the
corresponding tags.</p>
</td></tr>
<tr><td><code id="Tagged_Token_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A tagged token is a pair with &ldquo;slots&rdquo; &lsquo;token&rsquo; and
&lsquo;tag&rsquo;, giving the token and the corresponding tag.
</p>
<p>Tagged token objects provide sequences (allowing positional access) of
single tagged tokens.  They have class <code>"Tagged_Token"</code>.
</p>
<p>Subscripting tagged token objects via <code>[</code> extracts subsets of
tagged tokens; subscripting via <code>$</code> extracts character vectors
with the sequence of values of the named slot.
</p>
<p>There are several additional methods for class <code>"Tagged_Token"</code>:
<code>print()</code> and <code>format()</code> (which concatenate tokens and tags
separated by &lsquo;<span class="samp">&#8288;/&#8288;</span>&rsquo;);
<code>c()</code> combines tagged token objects (or objects coercible to
these using <code>as.Tagged_Token()</code>), and
<code>as.list()</code> and <code>as.data.frame()</code> coerce, respectively, to
lists (of single tagged token objects) and data frames (with tagged
tokens and slots corresponding to rows and columns).
</p>
<p><code>Tagged_Token()</code> creates tagged token objects from the given
sequences of tokens and tags, which must have the same length.
</p>
<p><code>as.Tagged_Token()</code> coerces to tagged token objects, with a
method for <code><a href="#topic+TextDocument">TextDocument</a></code> objects using
<code><a href="#topic+tagged_words">tagged_words</a>()</code>.
</p>
<p><code>is.Tagged_Token()</code> tests whether an object inherits from class
<code>"Tagged_Token"</code>.
</p>


<h3>Value</h3>

<p>For <code>Tagged_Token()</code> and <code>as.Tagged_Token()</code>, a tagged token
object (of class <code>"Tagged_Token"</code>).
</p>
<p>For <code>is.Tagged_Token()</code>, a logical.
</p>

<hr>
<h2 id='TaggedTextDocument'>POS-Tagged Word Text Documents</h2><span id='topic+TaggedTextDocument'></span>

<h3>Description</h3>

<p>Create text documents from files containing POS-tagged words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TaggedTextDocument(con, encoding = "unknown",
                   word_tokenizer = whitespace_tokenizer,
                   sent_tokenizer = Regexp_Tokenizer("\n", invert = TRUE),
                   para_tokenizer = blankline_tokenizer,
                   sep = "/",
                   meta = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TaggedTextDocument_+3A_con">con</code></td>
<td>
<p>a connection object or a character string.
See <code><a href="base.html#topic+readLines">readLines</a>()</code> for details.
</p>
</td></tr>
<tr><td><code id="TaggedTextDocument_+3A_encoding">encoding</code></td>
<td>
<p>encoding to be assumed for input strings.
See <code><a href="base.html#topic+readLines">readLines</a>()</code> for details.
</p>
</td></tr>
<tr><td><code id="TaggedTextDocument_+3A_word_tokenizer">word_tokenizer</code></td>
<td>
<p>a function for obtaining the word token spans.</p>
</td></tr>
<tr><td><code id="TaggedTextDocument_+3A_sent_tokenizer">sent_tokenizer</code></td>
<td>
<p>a function for obtaining the sentence token
spans.</p>
</td></tr>
<tr><td><code id="TaggedTextDocument_+3A_para_tokenizer">para_tokenizer</code></td>
<td>
<p>a function for obtaining the paragraph token
spans, or <code>NULL</code> in which case no paragraph tokenization is
performed.</p>
</td></tr>
<tr><td><code id="TaggedTextDocument_+3A_sep">sep</code></td>
<td>
<p>the character string separating the word tokens and their
POS tags.</p>
</td></tr>
<tr><td><code id="TaggedTextDocument_+3A_meta">meta</code></td>
<td>
<p>a named or empty list of document metadata tag-value
pairs.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>TaggedTextDocument()</code> creates documents representing natural
language text as suitable collections of POS-tagged words, based on
using <code><a href="base.html#topic+readLines">readLines</a>()</code> to read text lines from connections
providing such collections.
</p>
<p>The text read is split into paragraph, sentence and tagged word tokens
using the span tokenizers specified by arguments
<code>para_tokenizer</code>, <code>sent_tokenizer</code> and
<code>word_tokenizer</code>.  By default, paragraphs are assumed to be
separated by blank lines, sentences by newlines and tagged word tokens
by whitespace.  Finally, word tokens and their POS tags are obtained
by splitting the tagged word tokens according to <code>sep</code>.  From
this, a suitable representation of the provided collection of
POS-tagged words is obtained, and returned as a tagged text document
object inheriting from classes <code>"TaggedTextDocument"</code> and
<code>"<a href="#topic+TextDocument">TextDocument</a>"</code>.
</p>
<p>There are methods for generics
<code><a href="#topic+words">words</a>()</code>,
<code><a href="#topic+sents">sents</a>()</code>,
<code><a href="#topic+paras">paras</a>()</code>,
<code><a href="#topic+tagged_words">tagged_words</a>()</code>,
<code><a href="#topic+tagged_sents">tagged_sents</a>()</code>, and
<code><a href="#topic+tagged_paras">tagged_paras</a>()</code>
(as well as <code><a href="base.html#topic+as.character">as.character</a>()</code>)
and class <code>"TaggedTextDocument"</code>,
which should be used to access the text in such text document
objects.
</p>
<p>The methods for generics
<code><a href="#topic+tagged_words">tagged_words</a>()</code>, 
<code><a href="#topic+tagged_sents">tagged_sents</a>()</code> and
<code><a href="#topic+tagged_paras">tagged_paras</a>()</code>
provide a mechanism for mapping POS tags via the <code>map</code> argument,
see section <b>Details</b> in the help page for
<code><a href="#topic+tagged_words">tagged_words</a>()</code> for more information.
The POS tagset used will be inferred from the <code>POS_tagset</code>
metadata element of the CoNLL-style text document.
</p>


<h3>Value</h3>

<p>A tagged text document object inheriting from
<code>"TaggedTextDocument"</code> and <code>"<a href="#topic+TextDocument">TextDocument</a>"</code>.
</p>


<h3>See Also</h3>

<p><a href="https://www.nltk.org/nltk_data/packages/corpora/brown.zip">https://www.nltk.org/nltk_data/packages/corpora/brown.zip</a>
which provides the W. N. Francis and H. Kucera Brown tagged word
corpus as an archive of files which can be read in using
<code>TaggedTextDocument()</code>.
</p>
<p>Package <span class="pkg">tm.corpus.Brown</span> available from the repository at
<a href="https://datacube.wu.ac.at">https://datacube.wu.ac.at</a> conveniently provides this corpus
as a <span class="pkg">tm</span> <a href="tm.html#topic+VCorpus">VCorpus</a> of tagged text documents.
</p>

<hr>
<h2 id='tagsets'>NLP Tag Sets</h2><span id='topic+Penn_Treebank_POS_tags'></span><span id='topic+Brown_POS_tags'></span><span id='topic+Universal_POS_tags'></span><span id='topic+Universal_POS_tags_map'></span>

<h3>Description</h3>

<p>Tag sets frequently used in Natural Language Processing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Penn_Treebank_POS_tags
Brown_POS_tags
Universal_POS_tags
Universal_POS_tags_map
</code></pre>


<h3>Details</h3>

<p><code>Penn_Treebank_POS_tags</code> and <code>Brown_POS_tags</code> provide,
respectively, the Penn Treebank POS tags
(<a href="https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html">https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html</a>, Table 2)
and the POS tags used for the Brown corpus
(<a href="http://www.hit.uib.no/icame/brown/bcm.html">http://www.hit.uib.no/icame/brown/bcm.html</a>),
both as data frames with the following variables:
</p>

<dl>
<dt>entry</dt><dd><p>a character vector with the POS tags</p>
</dd>
<dt>description</dt><dd><p>a character vector with short descriptions of the
tags</p>
</dd>
<dt>examples</dt><dd><p>a character vector with examples for the tags</p>
</dd>
</dl>

<p><code>Universal_POS_tags</code> provides the universal POS tagset introduced
by Slav Petrov, Dipanjan Das, and Ryan McDonald
(<a href="https://arxiv.org/abs/1104.2086">https://arxiv.org/abs/1104.2086</a>), as a data frame with character
variables <code>entry</code> and <code>description</code>.
</p>
<p><code>Universal_POS_tags_map</code> is a named list of mappings from
language and treebank specific POS tagsets to the universal POS tags,
with elements named &lsquo;<span class="samp">&#8288;en-ptb&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;en-brown&#8288;</span>&rsquo; giving the
mappings, respectively, for the Penn Treebank and Brown POS tags.
</p>


<h3>Source</h3>

<p><a href="https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html">https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html</a>,
<a href="http://www.hit.uib.no/icame/brown/bcm.html">http://www.hit.uib.no/icame/brown/bcm.html</a>,
<a href="https://github.com/slavpetrov/universal-pos-tags">https://github.com/slavpetrov/universal-pos-tags</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Penn Treebank POS tags
dim(Penn_Treebank_POS_tags)
## Inspect first 20 entries:
write.dcf(head(Penn_Treebank_POS_tags, 20L))

## Brown POS tags
dim(Brown_POS_tags)
## Inspect first 20 entries:
write.dcf(head(Brown_POS_tags, 20L))

## Universal POS tags
Universal_POS_tags

## Available mappings to universal POS tags
names(Universal_POS_tags_map)
</code></pre>

<hr>
<h2 id='TextDocument'>Text Documents</h2><span id='topic+TextDocument'></span>

<h3>Description</h3>

<p>Representing and computing on text documents.
</p>


<h3>Details</h3>

<p><em>Text documents</em> are documents containing (natural language)
text.  In packages which employ the infrastructure provided by package
<span class="pkg">NLP</span>, such documents are represented via the virtual S3 class
<code>"TextDocument"</code>: such packages then provide S3 text document
classes extending the virtual base class (such as the
<code><a href="#topic+AnnotatedPlainTextDocument">AnnotatedPlainTextDocument</a></code> objects provided by package
<span class="pkg">NLP</span> itself).
</p>
<p>All extension classes must provide an <code><a href="base.html#topic+as.character">as.character</a>()</code>
method which extracts the natural language text in documents of the
respective classes in a &ldquo;suitable&rdquo; (not necessarily structured)
form, as well as <code><a href="#topic+content">content</a>()</code> and <code><a href="#topic+meta">meta</a>()</code>
methods for accessing the (possibly raw) document content and metadata.
</p>
<p>In addition, the infrastructure features the generic functions
<code><a href="#topic+words">words</a>()</code>, <code><a href="#topic+sents">sents</a>()</code>, etc., for which
extension classes can provide methods giving a structured view of the
text contained in documents of these classes (returning, e.g., a
character vector with the word tokens in these documents, and a list
of such character vectors).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+AnnotatedPlainTextDocument">AnnotatedPlainTextDocument</a></code>,
<code><a href="#topic+CoNLLTextDocument">CoNLLTextDocument</a></code>,
<code><a href="#topic+CoNLLUTextDocument">CoNLLUTextDocument</a></code>,
<code><a href="#topic+TaggedTextDocument">TaggedTextDocument</a></code>, and
<code><a href="#topic+WordListDocument">WordListDocument</a></code>
for the text document classes provided by package <span class="pkg">NLP</span>.
</p>

<hr>
<h2 id='Tokenizer'>Tokenizer objects</h2><span id='topic+Span_Tokenizer'></span><span id='topic+as.Span_Tokenizer'></span><span id='topic+is.Span_Tokenizer'></span><span id='topic+Token_Tokenizer'></span><span id='topic+as.Token_Tokenizer'></span><span id='topic+is.Token_Tokenizer'></span>

<h3>Description</h3>

<p>Create tokenizer objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Span_Tokenizer(f, meta = list())
as.Span_Tokenizer(x, ...)

Token_Tokenizer(f, meta = list())
as.Token_Tokenizer(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tokenizer_+3A_f">f</code></td>
<td>
<p>a tokenizer function taking the string to tokenize as
argument, and returning either the tokens (for
<code>Token_Tokenizer</code>) or their spans (for
<code>Span_Tokenizer</code>).</p>
</td></tr>
<tr><td><code id="Tokenizer_+3A_meta">meta</code></td>
<td>
<p>a named or empty list of tokenizer metadata tag-value
pairs.</p>
</td></tr>
<tr><td><code id="Tokenizer_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="Tokenizer_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tokenization is the process of breaking a text string up into words,
phrases, symbols, or other meaningful elements called tokens.  This
can be accomplished by returning the sequence of tokens, or the
corresponding spans (character start and end positions).
We refer to tokenization resources of the respective kinds as
&ldquo;token tokenizers&rdquo; and &ldquo;span tokenizers&rdquo;.
</p>
<p><code>Span_Tokenizer()</code> and <code>Token_Tokenizer()</code> return tokenizer
objects which are functions with metadata and suitable class
information, which in turn can be used for converting between the two
kinds using <code>as.Span_Tokenizer()</code> or <code>as.Token_Tokenizer()</code>.
It is also possible to coerce annotator (pipeline) objects to
tokenizer objects, provided that the annotators provide suitable
token annotations.  By default, word tokens are used; this can be
controlled via the <code>type</code> argument of the coercion methods (e.g.,
use <code>type = "sentence"</code> to extract sentence tokens).
</p>
<p>There are also <code>print()</code> and <code>format()</code> methods for
tokenizer objects, which use the <code>description</code> element of the
metadata if available.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Regexp_Tokenizer">Regexp_Tokenizer</a>()</code> for creating regexp span tokenizers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple text.
s &lt;- String("  First sentence.  Second sentence.  ")
##           ****5****0****5****0****5****0****5**

## Use a pre-built regexp (span) tokenizer:
wordpunct_tokenizer
wordpunct_tokenizer(s)
## Turn into a token tokenizer:
tt &lt;- as.Token_Tokenizer(wordpunct_tokenizer)
tt
tt(s)
## Of course, in this case we could simply have done
s[wordpunct_tokenizer(s)]
## to obtain the tokens from the spans.
## Conversion also works the other way round: package 'tm' provides
## the following token tokenizer function:
scan_tokenizer &lt;- function(x)
    scan(text = as.character(x), what = "character", quote = "", 
         quiet = TRUE)
## Create a token tokenizer from this:
tt &lt;- Token_Tokenizer(scan_tokenizer)
tt(s)
## Turn into a span tokenizer:
st &lt;- as.Span_Tokenizer(tt)
st(s)
## Checking tokens from spans:
s[st(s)]
</code></pre>

<hr>
<h2 id='tokenizers'>Regexp tokenizers</h2><span id='topic+Regexp_Tokenizer'></span><span id='topic+blankline_tokenizer'></span><span id='topic+whitespace_tokenizer'></span><span id='topic+wordpunct_tokenizer'></span>

<h3>Description</h3>

<p>Tokenizers using regular expressions to match either tokens or
separators between tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Regexp_Tokenizer(pattern, invert = FALSE, ..., meta = list())
blankline_tokenizer(s)
whitespace_tokenizer(s)
wordpunct_tokenizer(s)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenizers_+3A_pattern">pattern</code></td>
<td>
<p>a character string giving the regular expression to use
for matching.</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_invert">invert</code></td>
<td>
<p>a logical indicating whether to match separators between
tokens.</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to <code><a href="base.html#topic+gregexpr">gregexpr</a>()</code>.</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_meta">meta</code></td>
<td>
<p>a named or empty list of tokenizer metadata tag-value
pairs.</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_s">s</code></td>
<td>
<p>a <code><a href="#topic+String">String</a></code> object, or something coercible to this
using <code><a href="#topic+as.String">as.String</a>()</code> (e.g., a character string with
appropriate encoding information).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Regexp_Tokenizer()</code> creates regexp span tokenizers which use the
given <code>pattern</code> and <code>...</code> arguments to match tokens or
separators between tokens via <code><a href="base.html#topic+gregexpr">gregexpr</a>()</code>, and then
transform the results of this into character spans of the tokens
found.
</p>
<p><code>whitespace_tokenizer()</code> tokenizes by treating any sequence of
whitespace characters as a separator.
</p>
<p><code>blankline_tokenizer()</code> tokenizes by treating any sequence of
blank lines as a separator.
</p>
<p><code>wordpunct_tokenizer()</code> tokenizes by matching sequences of
alphabetic characters and sequences of (non-whitespace) non-alphabetic
characters.
</p>


<h3>Value</h3>

<p><code>Regexp_Tokenizer()</code> returns the created regexp span tokenizer.
</p>
<p><code>blankline_tokenizer()</code>, <code>whitespace_tokenizer()</code> and
<code>wordpunct_tokenizer()</code> return the spans of the tokens found in
<code>s</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Span_Tokenizer">Span_Tokenizer</a>()</code> for general information on span
tokenizer objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple text.
s &lt;- String("  First sentence.  Second sentence.  ")
##           ****5****0****5****0****5****0****5**

spans &lt;- whitespace_tokenizer(s)
spans
s[spans]

spans &lt;- wordpunct_tokenizer(s)
spans
s[spans]
</code></pre>

<hr>
<h2 id='Tree'>Tree objects</h2><span id='topic+Tree'></span><span id='topic+format.Tree'></span><span id='topic+print.Tree'></span><span id='topic+Tree_parse'></span><span id='topic+Tree_apply'></span>

<h3>Description</h3>

<p>Creation and manipulation of tree objects.</p>


<h3>Usage</h3>

<pre><code class='language-R'>Tree(value, children = list())
## S3 method for class 'Tree'
format(x, width = 0.9 * getOption("width"), indent = 0,
       brackets = c("(", ")"), ...)
Tree_parse(x, brackets = c("(", ")"))
Tree_apply(x, f, recursive = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tree_+3A_value">value</code></td>
<td>
<p>a (non-tree) node value of the tree.</p>
</td></tr>
<tr><td><code id="Tree_+3A_children">children</code></td>
<td>
<p>a list giving the children of the tree.</p>
</td></tr>
<tr><td><code id="Tree_+3A_x">x</code></td>
<td>
<p>a tree object for the <code>format()</code> method and
<code>Tree_apply()</code>; a character string for <code>Tree_parse()</code>.</p>
</td></tr>
<tr><td><code id="Tree_+3A_width">width</code></td>
<td>
<p>a positive integer giving the target column for a
single-line nested bracketting.</p>
</td></tr>
<tr><td><code id="Tree_+3A_indent">indent</code></td>
<td>
<p>a non-negative integer giving the indentation used for
formatting.</p>
</td></tr>
<tr><td><code id="Tree_+3A_brackets">brackets</code></td>
<td>
<p>a character vector of length two giving the pair of
opening and closing brackets to be employed for formatting or
parsing.</p>
</td></tr>
<tr><td><code id="Tree_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="Tree_+3A_f">f</code></td>
<td>
<p>a function to be applied to the children nodes.</p>
</td></tr>
<tr><td><code id="Tree_+3A_recursive">recursive</code></td>
<td>
<p>a logical indicating whether to apply <code>f</code>
recursively to the children of the children and so forth.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trees give hierarchical groupings of leaves and subtrees, starting
from the root node of the tree.  In natural language processing, the
syntactic structure of sentences is typically represented by parse
trees (e.g., <a href="https://en.wikipedia.org/wiki/Concrete_syntax_tree">https://en.wikipedia.org/wiki/Concrete_syntax_tree</a>)
and displayed using nested brackettings.
</p>
<p>The tree objects in package <span class="pkg">NLP</span> are patterned after the ones in
NLTK (<a href="https://www.nltk.org">https://www.nltk.org</a>), and primarily designed for representing
parse trees.  A tree object consists of the value of the root node and
its children as a list of leaves and subtrees, where the leaves are
elements with arbitrary non-tree values (and not subtrees with no
children).  The value and children can be extracted via <code>$</code>
subscripting using names <code>value</code> and <code>children</code>,
respectively.
</p>
<p>There is a <code>format()</code> method for tree objects: this first tries a
nested bracketting in a single line of the given width, and if this is
not possible, produces a nested indented bracketting.  The
<code>print()</code> method uses the <code>format()</code> method, and hence its
arguments to control the formatting.
</p>
<p><code>Tree_parse()</code> reads nested brackettings into a tree object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- Tree(1, list(2, Tree(3, list(4)), 5))
format(x)
x$value
x$children

p &lt;- Tree("VP",
          list(Tree("V",
                    list("saw")),
               Tree("NP",
                    list("him"))))
p &lt;- Tree("S",
          list(Tree("NP",
                    list("I")),
               p))
p
## Force nested indented bracketting:
print(p, width = 10)

s &lt;- "(S (NP I) (VP (V saw) (NP him)))"
p &lt;- Tree_parse(s)
p

## Extract the leaves by recursively traversing the children and
## recording the non-tree ones:
Tree_leaf_gatherer &lt;-
function()
{
    v &lt;- list()
    list(update =
         function(e) if(!inherits(e, "Tree")) v &lt;&lt;- c(v, list(e)),
         value = function() v,
         reset = function() { v &lt;&lt;- list() })
}
g &lt;- Tree_leaf_gatherer()
y &lt;- Tree_apply(p, g$update, recursive = TRUE)
g$value()
</code></pre>

<hr>
<h2 id='utils'>Annotation Utilities</h2><span id='topic+next_id'></span><span id='topic+single_feature'></span>

<h3>Description</h3>

<p>Utilities for creating annotation objects.</p>


<h3>Usage</h3>

<pre><code class='language-R'>next_id(id)
single_feature(value, tag)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="utils_+3A_id">id</code></td>
<td>
<p>an integer vector of annotation ids.</p>
</td></tr>
<tr><td><code id="utils_+3A_value">value</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="utils_+3A_tag">tag</code></td>
<td>
<p>a character string.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>next_id()</code> obtains the next &ldquo;available&rdquo; id based on the
given annotation ids (one more than the maximal non-missing id).
</p>
<p><code>single_feature()</code> creates a single feature from the given value
and tag (i.e., a named list with the value named by the tag).
</p>

<hr>
<h2 id='viewers'>Text Document Viewers</h2><span id='topic+sents'></span><span id='topic+words'></span><span id='topic+paras'></span><span id='topic+tagged_sents'></span><span id='topic+tagged_paras'></span><span id='topic+tagged_words'></span><span id='topic+chunked_sents'></span><span id='topic+parsed_sents'></span><span id='topic+parsed_paras'></span>

<h3>Description</h3>

<p>Provide suitable &ldquo;views&rdquo; of the text contained in text
documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>words(x, ...)
sents(x, ...)
paras(x, ...)
tagged_words(x, ...)
tagged_sents(x, ...)
tagged_paras(x, ...)
chunked_sents(x, ...)
parsed_sents(x, ...)
parsed_paras(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="viewers_+3A_x">x</code></td>
<td>
<p>a text document object.</p>
</td></tr>
<tr><td><code id="viewers_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Methods for extracting POS tagged word tokens (i.e., for generics
<code>tagged_words()</code>, <code>tagged_sents()</code> and
<code>tagged_paras()</code>) can optionally provide a mechanism for mapping
the POS tags via a <code>map</code> argument.  This can give a function, a
named character vector (with names and elements the tags to map from
and to, respectively), or a named list of such named character
vectors, with names corresponding to POS tagsets (see
<code><a href="#topic+Universal_POS_tags_map">Universal_POS_tags_map</a></code> for an example).  If a list, the
map used will be the element with name matching the POS tagset used
(this information is typically determined from the text document
metadata; see the the help pages for text document extension classes
implementing this mechanism for details).
</p>
<p>In addition to methods for the text document classes provided by
package <span class="pkg">NLP</span> itself, (see <a href="#topic+TextDocument">TextDocument</a>), package <span class="pkg">NLP</span>
also provides word tokens and POS tagged word tokens for the results
of
<code><a href="udpipe.html#topic+udpipe_annotate">udpipe_annotate</a>()</code>
from package <a href="https://CRAN.R-project.org/package=udpipe"><span class="pkg">udpipe</span></a>,
<code><a href="spacyr.html#topic+spacy_parse">spacy_parse</a>()</code>
from package <a href="https://CRAN.R-project.org/package=spacyr"><span class="pkg">spacyr</span></a>,
and
<code><a href="cleanNLP.html#topic+cnlp_annotate">cnlp_annotate</a>()</code>
from package <a href="https://CRAN.R-project.org/package=cleanNLP"><span class="pkg">cleanNLP</span></a>.
</p>


<h3>Value</h3>

<p>For <code>words()</code>, a character vector with the word tokens in the
document.
</p>
<p>For <code>sents()</code>, a list of character vectors with the word tokens
in the sentences.
</p>
<p>For <code>paras()</code>, a list of lists of character vectors with the word
tokens in the sentences, grouped according to the paragraphs.
</p>
<p>For <code>tagged_words()</code>, a character vector with the POS tagged word
tokens in the document (i.e., the word tokens and their POS tags,
separated by &lsquo;<span class="samp">&#8288;/&#8288;</span>&rsquo;).
</p>
<p>For <code>tagged_sents()</code>, a list of character vectors with the POS
tagged word tokens in the sentences.
</p>
<p>For <code>tagged_paras()</code>, a list of lists of character vectors with
the POS tagged word tokens in the sentences, grouped according to the
paragraphs.
</p>
<p>For <code>chunked_sents()</code>, a list of (flat) <code><a href="#topic+Tree">Tree</a></code>
objects giving the chunk trees for the sentences in the document.
</p>
<p>For <code>parsed_sents()</code>, a list of <code><a href="#topic+Tree">Tree</a></code>
objects giving the parse trees for the sentences in the document.
</p>
<p>For <code>parsed_paras()</code>, a list of lists of <code><a href="#topic+Tree">Tree</a></code>
objects giving the parse trees for the sentences in the document,
grouped according to the paragraphs in the document.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TextDocument">TextDocument</a></code> for basic information on the text document
infrastructure employed by package <span class="pkg">NLP</span>.
</p>

<hr>
<h2 id='WordListDocument'>Word List Text Documents</h2><span id='topic+WordListDocument'></span>

<h3>Description</h3>

<p>Create text documents from word lists.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WordListDocument(con, encoding = "unknown", meta = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WordListDocument_+3A_con">con</code></td>
<td>
<p>a connection object or a character string.
See <code><a href="base.html#topic+readLines">readLines</a>()</code> for details.
</p>
</td></tr>
<tr><td><code id="WordListDocument_+3A_encoding">encoding</code></td>
<td>
<p>encoding to be assumed for input strings.
See <code><a href="base.html#topic+readLines">readLines</a>()</code> for details.
</p>
</td></tr>
<tr><td><code id="WordListDocument_+3A_meta">meta</code></td>
<td>
<p>a named or empty list of document metadata tag-value
pairs.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>WordListDocument()</code> uses <code><a href="base.html#topic+readLines">readLines</a>()</code> to read
collections of words from connections for which each line provides one
word, with blank lines ignored, and returns a word list document
object which inherits from classes <code>"WordListDocument"</code> and
<code>"<a href="#topic+TextDocument">TextDocument</a>"</code>.
</p>
<p>The methods for generics <code><a href="#topic+words">words</a>()</code> and
<code><a href="base.html#topic+as.character">as.character</a>()</code> and class <code>"WordListDocument"</code>
can be used to extract the words.
</p>


<h3>Value</h3>

<p>A word list document object inheriting from <code>"WordListDocument"</code>
and <code>"<a href="#topic+TextDocument">TextDocument</a>"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TextDocument">TextDocument</a></code> for basic information on the text document
infrastructure employed by package <span class="pkg">NLP</span>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
