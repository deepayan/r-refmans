<!DOCTYPE html><html><head><title>Help for package DA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {DA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#KLFDA'><p> Kernel Local Fisher Discriminant Analysis (KLFDA)</p></a></li>
<li><a href='#KLFDA_mk'><p> Kernel Local Fisher Discriminant Analysis (KLFDA) with Multinomial kernel</p></a></li>
<li><a href='#KLFDAM'><p>Kernel local Fisher discriminant analysis</p></a></li>
<li><a href='#kmatrixGauss'><p> Estimating Gaussian Kernel matrix</p></a></li>
<li><a href='#LDAKPC'><p>Linear Fisher discriminant analysis of kernel principal components (DAKPC)</p></a></li>
<li><a href='#LFDA'><p> Local Fisher Discriminant Analysis (LFDA)</p></a></li>
<li><a href='#LFDAKPC'><p>Local Fisher Discriminant Analysis of Kernel principle components (LFDAKPC)</p></a></li>
<li><a href='#Mabayes'><p> Membership assignment by weighted Mahalanobis distance and bayes rule</p></a></li>
<li><a href='#predict'><p> Predict method in DA for discriminant analysis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-07-11</td>
</tr>
<tr>
<td>Title:</td>
<td>Discriminant Analysis for Evolutionary Inference</td>
</tr>
<tr>
<td>Description:</td>
<td>Discriminant Analysis (DA) for evolutionary inference (Qin, X. et al, 2020, &lt;<a href="https://doi.org/10.22541%2Fau.159256808.83862168">doi:10.22541/au.159256808.83862168</a>&gt;), especially for population genetic structure and community structure inference. This package incorporates the commonly used linear and non-linear, local and global supervised learning approaches (discriminant analysis), including Linear Discriminant Analysis of Kernel Principal Components (LDAKPC), Local (Fisher) Linear Discriminant Analysis (LFDA), Local (Fisher) Discriminant Analysis of Kernel Principal Components (LFDAKPC) and Kernel Local (Fisher) Discriminant Analysis (KLFDA). These discriminant analyses can be used to do ecological and evolutionary inference, including demography inference, species identification, and population/community structure inference.</td>
</tr>
<tr>
<td>biocViews:</td>
<td>BiomedicalInformatics, ChIPSeq, Clustering, Coverage,
DNAMethylation, DifferentialExpression,
DifferentialMethylation,Software, DifferentialSplicing,
Epigenetics, FunctionalGenomics, GeneExpression,
GeneSetEnrichment, Genetics, ImmunoOncology,
MultipleComparison, Normalization, Pathways, QualityControl,
RNASeq, Regression, SAGE, Sequencing, Software, SystemsBiology,
TimeCourse, Transcription, Transcriptomics</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://xinghuq.github.io/DA/index.html">https://xinghuq.github.io/DA/index.html</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/xinghuq/DA/issues">https://github.com/xinghuq/DA/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>adegenet,lfda,MASS,kernlab,klaR,plotly,rARPACK,grDevices,stats,utils</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr,testthat,rmarkdown</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-07-11 04:35:11 UTC; Qin_st_andrews</td>
</tr>
<tr>
<td>Author:</td>
<td>Xinghu Qin <a href="https://orcid.org/0000-0003-2351-3610"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Xinghu Qin &lt;qinxinghu@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-07-12 08:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='KLFDA'> Kernel Local Fisher Discriminant Analysis (KLFDA)
</h2><span id='topic+KLFDA'></span>

<h3>Description</h3>

<p>Kernel Local Fisher Discriminant Analysis (KLFDA). This function implements the Kernel Local Fisher Discriminant Analysis with an unified Kernel function. Different from KLFDA function, which adopts the Multinomial Kernel as an example, this function empolys the kernel function that allows you to choose various types of kernels. See the kernel function from &quot;kernelMatriax&quot; (kernlab).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KLFDA(x, y, kernel = kernlab::polydot(degree = 1, scale = 1, offset = 1), 
r = 20, tol, prior, CV = FALSE, usekernel = TRUE, 
fL = 0.5, metric = c("weighted", "orthonormalized", "plain"), 
knn = 6, reg = 0.001, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KLFDA_+3A_x">x</code></td>
<td>
<p> The input training data
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_y">y</code></td>
<td>
<p> The training labels
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_kernel">kernel</code></td>
<td>
<p> The kernel function used to calculate kernel matrix. Choose the corresponding kernel you want, see details.
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_r">r</code></td>
<td>
<p> The number of reduced features you want to keep.
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_tol">tol</code></td>
<td>
<p> The tolerance used to reject the uni-variance. This is important when the variance between classes is small, and setting the large tolerance will avoid the data distortion.
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_prior">prior</code></td>
<td>
<p> The weight of each class, or the proportion of each class.
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_cv">CV</code></td>
<td>
<p> Whether to do cross validation.
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_usekernel">usekernel</code></td>
<td>
<p>whether to use kernel classifier, if TRUE, pass to Naive Bayes classifier.
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_fl">fL</code></td>
<td>
<p> If usekernel is TRUE, pass to the kernel function.
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_metric">metric</code></td>
<td>
<p>type of metric in the embedding space (default: 'weighted') 'weighted' - weighted eigenvectors 'orthonormalized' - orthonormalized 'plain' - raw eigenvectors
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_knn">knn</code></td>
<td>
<p> The number of nearest neighbours
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_reg">reg</code></td>
<td>
<p> The regularization parameter
</p>
</td></tr>
<tr><td><code id="KLFDA_+3A_...">...</code></td>
<td>
<p> additional arguments for the classifier
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function empolys three different classifiers, the basic linear classifier, the Mabayes (Bayes rule and the Mahalanobis distance), and Niave Bayes classifier.
The argeument &quot;kernel&quot; in the klfda function is the kernel function used to calculate the kernel matrix. If usekernel is TRUE, the corresponding kernel parameters will pass the the Naive Bayes kernel classifier.
The kernel parameter can be set to any function, of class kernel, which computes the inner product in feature space between two vector arguments. kernlab provides the most popular kernel functions which can be initialized by using the following functions:
</p>
<p>rbfdot Radial Basis kernel function
</p>
<p>polydot Polynomial kernel function
</p>
<p>vanilladot Linear kernel function
</p>
<p>tanhdot Hyperbolic tangent kernel function
</p>
<p>laplacedot Laplacian kernel function
</p>
<p>besseldot Bessel kernel function
</p>
<p>anovadot ANOVA RBF kernel function
</p>
<p>splinedot the Spline kernel
</p>
<p>(see example.)
</p>
<p>kernelFast is mainly used in situations where columns of the kernel matrix are computed per invocation. In these cases, evaluating the norm of each row-entry over and over again would cause significant computational overhead.
</p>


<h3>Value</h3>

<p> The results give the classified classes and the posterior possibility of each class using different classifier. 
</p>
<table>
<tr><td><code>class</code></td>
<td>
<p>The class labels from linear classifier</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>The posterior possibility of each class from linear classifier</p>
</td></tr>
<tr><td><code>bayes_judgement</code></td>
<td>
<p>Discrimintion results using the Mabayes classifier</p>
</td></tr>
<tr><td><code>bayes_assigment</code></td>
<td>
<p>Discrimintion results using the Naive bayes classifier</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>The reduced features</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>qinxinghu@gmail.com
</p>


<h3>References</h3>

<p>Sugiyama, M (2007).Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis. Journal of Machine Learning Research, vol.8, 1027-1061.
</p>
<p>Sugiyama, M (2006). Local Fisher discriminant analysis for supervised dimensionality reduction. In W. W. Cohen and A. Moore (Eds.), Proceedings of 23rd International Conference on Machine Learning (ICML2006), 905-912.
</p>
<p>Original Matlab Implementation: http://www.ms.k.u-tokyo.ac.jp/software.html#LFDA
</p>
<p>Tang, Y., &amp; Li, W. (2019). lfda: Local Fisher Discriminant Analysis inR. Journal of Open Source Software, 4(39), 1572.
</p>
<p>Moore, A. W. (2004). Naive Bayes Classifiers. In School of Computer Science. Carnegie Mellon University.
</p>
<p>Pierre Enel (2020). Kernel Fisher Discriminant Analysis (https://www.github.com/p-enel/MatlabKFDA), GitHub. Retrieved March 30, 2020.
</p>
<p>Karatzoglou, A., Smola, A., Hornik, K., &amp; Zeileis, A. (2004). kernlab-an S4 package for kernel methods in R. Journal of statistical software, 11(9), 1-20.
</p>


<h3>See Also</h3>

<p>predict.KLFDA, KLFDAM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(kernlab)
btest=KLFDA(as.matrix(iris[,1:4]),as.matrix(as.data.frame(iris[,5])),
kernel=kernlab::rbfdot(sigma = 0.1),
r=3,prior=NULL,tol=1e-90,
reg=0.01,metric =  'plain')
pred=predict.KLFDA(btest,testData=as.matrix(iris[1:10,1:4]),prior=NULL)
</code></pre>

<hr>
<h2 id='KLFDA_mk'> Kernel Local Fisher Discriminant Analysis (KLFDA) with Multinomial kernel
</h2><span id='topic+KLFDA_mk'></span>

<h3>Description</h3>

<p>Kernel Local Fisher Discriminant Analysis (KLFDA). This function implements the Kernel Local Fisher Discriminant Analysis with a Multinomial kernel. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KLFDA_mk(X, Y, r, order, regParam, 
usekernel = TRUE, fL = 0.5, 
priors, tol, reg, metric, 
plotFigures = FALSE, verbose, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KLFDA_mk_+3A_x">X</code></td>
<td>
<p>The input training data
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_y">Y</code></td>
<td>
<p>The training labels
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_r">r</code></td>
<td>
<p> The number of reduced features
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_order">order</code></td>
<td>
<p> The order passing to Multinomial Kernel
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_regparam">regParam</code></td>
<td>
<p>The regularization parameter for kernel matrix
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_usekernel">usekernel</code></td>
<td>
<p> Whether to used kernel classifier
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_fl">fL</code></td>
<td>
<p> pass to kernel classifier if usekenel is TRUE
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_priors">priors</code></td>
<td>
<p>The weight of each class
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_tol">tol</code></td>
<td>
<p> The tolerance for rejecting uni-variance
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_reg">reg</code></td>
<td>
<p> The regularization parameter 
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_metric">metric</code></td>
<td>
<p>	Type of metric in the embedding space (default: 'weighted') 'weighted' - weighted eigenvectors 'orthonormalized' - orthonormalized 'plain' - raw eigenvectors
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_plotfigures">plotFigures</code></td>
<td>
<p> whether to plot the reduced features, 3D plot
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_verbose">verbose</code></td>
<td>
<p> silence the processing
</p>
</td></tr>
<tr><td><code id="KLFDA_mk_+3A_...">...</code></td>
<td>
<p> additional arguments for the classifier
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses Multinomial Kernel, users can replace the Multinomial Kernel based on your own purpose. The final discrimination employs three classifiers, the basic linear classifier, the Mabayes (Bayes rule and the Mahalanobis distance), and Niave Bayes classifier.
</p>


<h3>Value</h3>

<table>
<tr><td><code>class</code></td>
<td>
<p>The class labels from linear classifier</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>The posterior possibility of each class from linear classifier</p>
</td></tr>
<tr><td><code>bayes_judgement</code></td>
<td>
<p>Discrimintion results using the Mabayes classifier</p>
</td></tr>
<tr><td><code>bayes_assigment</code></td>
<td>
<p>Discrimintion results using the Naive bayes classifier</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>The reduced features</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>qinxinghu@gmail.com
</p>


<h3>References</h3>

<p>Sugiyama, M (2007). Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis. Journal of Machine Learning Research, vol.8, 1027-1061.
</p>
<p>Sugiyama, M (2006). Local Fisher discriminant analysis for supervised dimensionality reduction. In W. W. Cohen and A. Moore (Eds.), Proceedings of 23rd International Conference on Machine Learning (ICML2006), 905-912.
</p>
<p>Original Matlab Implementation: http://www.ms.k.u-tokyo.ac.jp/software.html#LFDA
</p>
<p>Tang, Y., &amp; Li, W. (2019). lfda: Local Fisher Discriminant Analysis inR. Journal of Open Source Software, 4(39), 1572.
</p>
<p>Moore, A. W. (2004). Naive Bayes Classifiers. In School of Computer Science. Carnegie Mellon University.
</p>
<p>Pierre Enel (2020). Kernel Fisher Discriminant Analysis (https://www.github.com/p-enel/MatlabKFDA), GitHub. Retrieved March 30, 2020.
</p>
<p>Karatzoglou, A., Smola, A., Hornik, K., &amp; Zeileis, A. (2004). kernlab-an S4 package for kernel methods in R. Journal of statistical software, 11(9), 1-20.
</p>


<h3>See Also</h3>

<p>predict.KLFDA_mk, klfda_1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>btest=KLFDA_mk(X=as.matrix(iris[,1:4]),
Y=as.matrix(as.data.frame(iris[,5])),r=3,order=2,regParam=0.25, 
usekernel=TRUE,fL=0.5,
priors=NULL,tol=1e-90,reg=0.01,metric =  'plain',plotFigures=FALSE,
verbose=TRUE)
#pred=predict.KLFDA_mk(btest,as.matrix(iris[1:10,1:4]))
</code></pre>

<hr>
<h2 id='KLFDAM'>Kernel local Fisher discriminant analysis
</h2><span id='topic+KLFDAM'></span>

<h3>Description</h3>

<p>This function performs Kernel Local Fisher Discriminant Analysis. The function provided here allows users to carry out the KLFDA using a pairwise matrix. We used the gaussan matrix as example. Users can compute different kernel matrix or distance matrix as the input for this function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KLFDAM(kdata, y, r,
metric = c("weighted", "orthonormalized", "plain"),
tol=1e-5,knn = 6, reg = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KLFDAM_+3A_kdata">kdata</code></td>
<td>
<p>The input dataset (kernel matrix). The input data can be a genotype matrix, dataframe, species occurence matrix, or principal components. The dataset have to convert to a kernel matrix before feed into this function.
</p>
</td></tr>
<tr><td><code id="KLFDAM_+3A_y">y</code></td>
<td>
<p> The group lables
</p>
</td></tr>
<tr><td><code id="KLFDAM_+3A_r">r</code></td>
<td>
<p> Number of reduced features
</p>
</td></tr>
<tr><td><code id="KLFDAM_+3A_metric">metric</code></td>
<td>
<p>Type of metric in the embedding space (default: 'weighted') 'weighted' - weighted eigenvectors 'orthonormalized' - orthonormalized 'plain' - raw eigenvectors
</p>
</td></tr>
<tr><td><code id="KLFDAM_+3A_knn">knn</code></td>
<td>
<p>The number of nearest neighbours
</p>
</td></tr>
<tr><td><code id="KLFDAM_+3A_tol">tol</code></td>
<td>
<p>Tolerance to avoid singular values
</p>
</td></tr>
<tr><td><code id="KLFDAM_+3A_reg">reg</code></td>
<td>
<p>The regularization parameter
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Kernel Local Fisher Discriminant Analysis for any kernel matrix. It was proposed in Sugiyama, M (2006, 2007) as a non-linear improvement for discriminant analysis. This function is adopted from Tang et al. 2019.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Z</code></td>
<td>
<p>The reduced features</p>
</td></tr>
<tr><td><code>Tr</code></td>
<td>
<p>The transformation matrix</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tang, Y., &amp; Li, W. (2019). lfda: Local Fisher Discriminant Analysis inR. Journal of Open Source Software, 4(39), 1572.
</p>
<p>Sugiyama, M (2007). Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis. Journal of Machine Learning Research, vol.8, 1027-1061.
</p>
<p>Sugiyama, M (2006). Local Fisher discriminant analysis for supervised dimensionality reduction. In W. W. Cohen and A. Moore (Eds.), Proceedings of 23rd International Conference on Machine Learning (ICML2006), 905-912.
</p>


<h3>See Also</h3>

<p>KLFDA
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kmat &lt;- kmatrixGauss(iris[, -5],sigma=1)
zklfda=KLFDAM(kmat, iris[, 5], r=3,metric = "plain",tol=1e-5 )
print(zklfda$Z)
</code></pre>

<hr>
<h2 id='kmatrixGauss'> Estimating Gaussian Kernel matrix
</h2><span id='topic+kmatrixGauss'></span>

<h3>Description</h3>

<p>This function estimates Gaussian kernel computation for klfda, which maps the original data space to non-linear and higher dimensions. See the deatils of kmatrixGauss from lfda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmatrixGauss(x, sigma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmatrixGauss_+3A_x">x</code></td>
<td>
<p> Input data matrix or dataframe
</p>
</td></tr>
<tr><td><code id="kmatrixGauss_+3A_sigma">sigma</code></td>
<td>
<p> The Gaussian kernel parameter
</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Return a n*n matrix
</p>


<h3>Value</h3>

<p>Return a n*n matrix
</p>


<h3>References</h3>

<p>Tang, Y., &amp; Li, W. (2019). lfda: Local Fisher Discriminant Analysis inR. Journal of Open Source Software, 4(39), 1572.
</p>

<hr>
<h2 id='LDAKPC'>Linear Fisher discriminant analysis of kernel principal components (DAKPC)
</h2><span id='topic+LDAKPC'></span>

<h3>Description</h3>

<p> Linear Fisher discriminant analysis of kernel principal components (DAKPC). This function empolies the LDA and kpca. This function is called Kernel Fisher Discriminant Analysis (KFDA) in other package (kfda). &quot;KFDA&quot; is the misleading name and &quot;KFDA&quot; has crucial error in package kfda. This function rectifies the current existing error for kfda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LDAKPC(x, y, n.pc, usekernel = FALSE, 
fL = 0, kernel.name = "rbfdot", 
kpar = list(0.001), kernel = "gaussian", 
threshold = 1e-05, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LDAKPC_+3A_x">x</code></td>
<td>
<p> Input traing data
</p>
</td></tr>
<tr><td><code id="LDAKPC_+3A_y">y</code></td>
<td>
<p> Input labels
</p>
</td></tr>
<tr><td><code id="LDAKPC_+3A_n.pc">n.pc</code></td>
<td>
<p> number of pcs that will be kept in analysis
</p>
</td></tr>
<tr><td><code id="LDAKPC_+3A_usekernel">usekernel</code></td>
<td>
<p> Whether to use kernel function, if TRUE, it will pass to the kernel.names
</p>
</td></tr>
<tr><td><code id="LDAKPC_+3A_fl">fL</code></td>
<td>
<p> if using kernel, pass to kernel function
</p>
</td></tr>
<tr><td><code id="LDAKPC_+3A_kernel.name">kernel.name</code></td>
<td>
<p> if usekernel is TURE, this will take the kernel name and use the parameters set as you defined
</p>
</td></tr>
<tr><td><code id="LDAKPC_+3A_kpar">kpar</code></td>
<td>
	
<p>the list of hyper-parameters (kernel parameters). This is a list which contains the parameters to be used with the kernel function. Valid parameters for existing kernels are :
</p>
<p>sigma inverse kernel width for the Radial Basis kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
<p>degree, scale, offset for the Polynomial kernel &quot;polydot&quot;
</p>
<p>scale, offset for the Hyperbolic tangent kernel function &quot;tanhdot&quot;
</p>
<p>sigma, order, degree for the Bessel kernel &quot;besseldot&quot;.
</p>
<p>sigma, degree for the ANOVA kernel &quot;anovadot&quot;.
</p>
<p>Hyper-parameters for user defined kernels can be passed through the kpar parameter as well.
</p>
</td></tr>
<tr><td><code id="LDAKPC_+3A_kernel">kernel</code></td>
<td>
<p> kernel name if all the above are not used
</p>
</td></tr>
<tr><td><code id="LDAKPC_+3A_threshold">threshold</code></td>
<td>
	
<p>the threshold for kpc: value of the eigenvalue under which principal components are ignored (only valid when features = 0). (default : 0.0001)
</p>
</td></tr>
<tr><td><code id="LDAKPC_+3A_...">...</code></td>
<td>
<p> additional arguments for the classifier
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>kpca</code></td>
<td>
<p>Results of kernel principal component analysis. Kernel Principal Components Analysis is a nonlinear form of principal component analysis</p>
</td></tr>
<tr><td><code>kpc</code></td>
<td>
<p>Kernel principal components. The scores of the components</p>
</td></tr>
<tr><td><code>LDAKPC</code></td>
<td>
<p>Linear discriminant anslysis of kernel principal components</p>
</td></tr>
<tr><td><code>LDs</code></td>
<td>
<p>The discriminant function. The scores of the components</p>
</td></tr>
<tr><td><code>label</code></td>
<td>
<p>The corresponding class of the data</p>
</td></tr>
<tr><td><code>n.pc</code></td>
<td>
<p>Number of Pcs kept in analysis</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>qinxinghu@gmail.com
</p>


<h3>References</h3>

<p>Karatzoglou, A., Smola, A., Hornik, K., &amp; Zeileis, A. (2004). kernlab-an S4 package for kernel methods in R. Journal of statistical software, 11(9), 1-20.
</p>
<p>Mika, S., Ratsch, G., Weston, J., Scholkopf, B., &amp; Mullers, K. R. (1999, August). Fisher discriminant analysis with kernels. In Neural networks for signal processing IX: Proceedings of the 1999 IEEE signal processing society workshop (cat. no. 98th8468) (pp. 41-48). Ieee.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
train=LDAKPC(iris[,1:4],y=iris[,5],n.pc=3,kernel.name = "rbfdot")
pred=predict.LDAKPC(train,testData = iris[1:10,1:4])
</code></pre>

<hr>
<h2 id='LFDA'> Local Fisher Discriminant Analysis (LFDA)
</h2><span id='topic+LFDA'></span>

<h3>Description</h3>

<p> This function implements local Fisher discriminant analysis. It gives the discriminant function with the posterior possibility of each class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LFDA(x, y, r, prior = proportions,
CV = FALSE, usekernel = TRUE, fL = 0, 
tol, kernel = "gaussian", 
metric = c("orthonormalized", "plain", "weighted"), 
knn = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LFDA_+3A_x">x</code></td>
<td>
<p> Input training data
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_y">y</code></td>
<td>
<p> Training labels
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_r">r</code></td>
<td>
<p> Number of reduced features that will be kept
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_prior">prior</code></td>
<td>
<p> Prior possibility of each class
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_cv">CV</code></td>
<td>
<p> Whether to do cross validation
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_usekernel">usekernel</code></td>
<td>
<p> Whether to use the kernel discrimination in native bayes classifier
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_fl">fL</code></td>
<td>
<p> Feed to native bayes classifier. Factor for Laplace correction, default factor is 0, i.e. no correction.
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_tol">tol</code></td>
<td>
<p> The tolerance used in Mabayes discrimination, see Mabayes
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_kernel">kernel</code></td>
<td>
<p> If usekernel is TRUE, specifying the kernel names, see NaiveBaye.
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_metric">metric</code></td>
<td>
<p> The type of metric in the embedding space (no default), e.g., 'weighted',  weighted eigenvectors; 'orthonormalized' , orthonormalized; 'plain',  raw eigenvectors.
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_knn">knn</code></td>
<td>
<p> Number of nearest neighbors
</p>
</td></tr>
<tr><td><code id="LFDA_+3A_...">...</code></td>
<td>
<p> additional arguments for the classifier
</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>The results give the classified classes and the posterior possibility of each class using different classifier.
</p>


<h3>Value</h3>

<table>
<tr><td><code>class</code></td>
<td>
<p>The class labels</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>The posterior possibility of each class</p>
</td></tr>
<tr><td><code>bayes_judgement</code></td>
<td>
<p>Discrimintion results using the Mabayes classifier</p>
</td></tr>
<tr><td><code>bayes_assigment</code></td>
<td>
<p>Discrimintion results using the Naive bayes classifier</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>The reduced features</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>qinxinghu@gmail.com
</p>


<h3>References</h3>

<p>Sugiyama, M (2007). Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis. Journal of Machine Learning Research, vol.8, 1027-1061.
</p>
<p>Sugiyama, M (2006). Local Fisher discriminant analysis for supervised dimensionality reduction. In W. W. Cohen and A. Moore (Eds.), Proceedings of 23rd International Conference on Machine Learning (ICML2006), 905-912.
</p>
<p>Tang, Y., &amp; Li, W. (2019). lfda: Local Fisher Discriminant Analysis inR. Journal of Open Source Software, 4(39), 1572.
</p>
<p>Moore, A. W. (2004). Naive Bayes Classifiers. In School of Computer Science. Carnegie Mellon University.
</p>
<p>Pierre Enel (2020). Kernel Fisher Discriminant Analysis (https://www.github.com/p-enel/MatlabKFDA), GitHub. Retrieved March 30, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>LFDAtest=LFDA(iris[,1:4],y=iris[,5],r=3, 
CV=FALSE,usekernel = TRUE, fL = 0,
kernel="gaussian",metric = "plain",knn = 6,tol = 1)
LFDApred=predict.LFDA(LFDAtest,iris[1:10,1:4],prior=NULL)
</code></pre>

<hr>
<h2 id='LFDAKPC'>Local Fisher Discriminant Analysis of Kernel principle components (LFDAKPC)
</h2><span id='topic+LFDAKPC'></span>

<h3>Description</h3>

<p>Local Fisher Discriminant Analysis of Kernel principle components
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LFDAKPC(x, y, n.pc, 
usekernel = FALSE, fL = 0, 
kernel.name = "rbfdot", 
kpar = list(0.001), kernel = "gaussian", 
threshold = 1e-05, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LFDAKPC_+3A_x">x</code></td>
<td>
<p> Input traing data
</p>
</td></tr>
<tr><td><code id="LFDAKPC_+3A_y">y</code></td>
<td>
<p> Input labels
</p>
</td></tr>
<tr><td><code id="LFDAKPC_+3A_n.pc">n.pc</code></td>
<td>
<p> number of pcs that will be kept in analysis
</p>
</td></tr>
<tr><td><code id="LFDAKPC_+3A_usekernel">usekernel</code></td>
<td>
<p> Whether to use kernel function, if TRUE, it will pass to the kernel.names
</p>
</td></tr>
<tr><td><code id="LFDAKPC_+3A_fl">fL</code></td>
<td>
<p> if using kernel, pass to kernel function
</p>
</td></tr>
<tr><td><code id="LFDAKPC_+3A_kernel.name">kernel.name</code></td>
<td>
<p> if usekernel is TURE, this will take the kernel name and use the parameters set as you defined
</p>
</td></tr>
<tr><td><code id="LFDAKPC_+3A_kpar">kpar</code></td>
<td>
	
<p>the list of hyper-parameters (kernel parameters). This is a list which contains the parameters to be used with the kernel function. Valid parameters for existing kernels are :
</p>
<p>sigma inverse kernel width for the Radial Basis kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
<p>degree, scale, offset for the Polynomial kernel &quot;polydot&quot;
</p>
<p>scale, offset for the Hyperbolic tangent kernel function &quot;tanhdot&quot;
</p>
<p>sigma, order, degree for the Bessel kernel &quot;besseldot&quot;.
</p>
<p>sigma, degree for the ANOVA kernel &quot;anovadot&quot;.
</p>
<p>Hyper-parameters for user defined kernels can be passed through the kpar parameter as well.
</p>
</td></tr>
<tr><td><code id="LFDAKPC_+3A_kernel">kernel</code></td>
<td>
<p> kernel name if all the above are not used
</p>
</td></tr>
<tr><td><code id="LFDAKPC_+3A_threshold">threshold</code></td>
<td>
	
<p>the threshold for kpc: value of the eigenvalue under which principal components are ignored (only valid when features = 0). (default : 0.0001)
</p>
</td></tr>
<tr><td><code id="LFDAKPC_+3A_...">...</code></td>
<td>
<p> additional arguments for the classifier
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>kpca</code></td>
<td>
<p>Results of kernel principal component analysis. Kernel Principal Components Analysis is a nonlinear form of principal component analysis</p>
</td></tr>
<tr><td><code>kpc</code></td>
<td>
<p>Kernel principal components. The scores of the components</p>
</td></tr>
<tr><td><code>LFDAKPC</code></td>
<td>
<p>LOcal linear discriminant anslysis of kernel principal components</p>
</td></tr>
<tr><td><code>LDs</code></td>
<td>
<p>The discriminant function. The scores of the components</p>
</td></tr>
<tr><td><code>label</code></td>
<td>
<p>The corresponding class of the data</p>
</td></tr>
<tr><td><code>n.pc</code></td>
<td>
<p>Number of Pcs kept in analysis</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>qinxinghu@gmail.com
</p>


<h3>References</h3>

<p>Sugiyama, M (2007). Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis. Journal of Machine Learning Research, vol.8, 1027-1061.
</p>
<p>Sugiyama, M (2006). Local Fisher discriminant analysis for supervised dimensionality reduction. In W. W. Cohen and A. Moore (Eds.), Proceedings of 23rd International Conference on Machine Learning (ICML2006), 905-912.
</p>
<p>Tang, Y., &amp; Li, W. (2019). lfda: Local Fisher Discriminant Analysis inR. Journal of Open Source Software, 4(39), 1572.
</p>
<p>Karatzoglou, A., Smola, A., Hornik, K., &amp; Zeileis, A. (2004). kernlab-an S4 package for kernel methods in R. Journal of statistical software, 11(9), 1-20.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
train=LFDAKPC(iris[,1:4],y=iris[,5],tol=1,n.pc=3,kernel.name = "rbfdot")
pred=predict.LFDAKPC(train,prior=NULL,testData = iris[1:10,1:4])
</code></pre>

<hr>
<h2 id='Mabayes'> Membership assignment by weighted Mahalanobis distance and bayes rule
</h2><span id='topic+Mabayes'></span>

<h3>Description</h3>

<p> The function gives the discrimintion of the potential classes based on Bayes rule and the Mahalanobis distance. This function adopts the function from  Bingpei Wu, 2012, WMDB 1.0 with some corrections of the judement rule.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Mabayes(TrnX, TrnG, p = rep(1, length(levels(TrnG))), TstX = NULL, var.equal = FALSE, tol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Mabayes_+3A_trnx">TrnX</code></td>
<td>
<p> Training data
</p>
</td></tr>
<tr><td><code id="Mabayes_+3A_trng">TrnG</code></td>
<td>
<p> Training label
</p>
</td></tr>
<tr><td><code id="Mabayes_+3A_p">p</code></td>
<td>
<p> prior or proportion of each class
</p>
</td></tr>
<tr><td><code id="Mabayes_+3A_tstx">TstX</code></td>
<td>
<p> Test data
</p>
</td></tr>
<tr><td><code id="Mabayes_+3A_var.equal">var.equal</code></td>
<td>
<p> whether the variance or the weight is equal between classes
</p>
</td></tr>
<tr><td><code id="Mabayes_+3A_tol">tol</code></td>
<td>
<p> The threshold or tolerance value for the covariance and distance
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>posterior and class</code></td>
<td>
<p>The posterior possibility and class labels</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>qinxinghu@gmail.com
</p>


<h3>References</h3>

<p>Bingpei Wu, 2012, WMDB 1.0: Discriminant Analysis Methods by Weight Mahalanobis Distance and bayes.
</p>
<p>Ito, Y., Srinivasan, C., Izumi, H. (2006, September). Discriminant analysis by a neural network with Mahalanobis distance. In International Conference on Artificial Neural Networks (pp. 350-360). Springer, Berlin, Heidelberg.
</p>
<p>Wolfel, M., Ekenel, H. K. (2005, September). Feature weighted Mahalanobis distance: improved robustness for Gaussian classifiers. In 2005 13th European signal processing conference (pp. 1-4). IEEE.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
train=Mabayes(iris[,1:4],iris[,5],TstX= iris[1:10,1:4],tol = 1)

</code></pre>

<hr>
<h2 id='predict'> Predict method in DA for discriminant analysis
</h2><span id='topic+predict'></span><span id='topic+predict.KLFDA'></span><span id='topic+predict.KLFDA_mk'></span><span id='topic+predict.LDAKPC'></span><span id='topic+predict.LFDA'></span><span id='topic+predict.LFDAKPC'></span>

<h3>Description</h3>

<p> Predict method for DA. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'KLFDA_mk'
predict(object,prior,testData, ...)
## S3 method for class 'KLFDA'
predict(object,prior,testData, ...)
## S3 method for class 'LDAKPC'
predict(object,prior,testData, ...)
## S3 method for class 'LFDA'
predict(object,prior,testData, ...)
## S3 method for class 'LFDAKPC'
predict(object,prior,testData, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p>One of the trained object from discriminant analysis
</p>
</td></tr>
<tr><td><code id="predict_+3A_prior">prior</code></td>
<td>
<p> The weights of the groups.
</p>
</td></tr>
<tr><td><code id="predict_+3A_testdata">testData</code></td>
<td>
<p>The test data or new data 
</p>
</td></tr>
<tr><td><code id="predict_+3A_...">...</code></td>
<td>
<p>Arguments passed to the classifiers
</p>
</td></tr>
</table>


<h3>Value</h3>

<p> The predict function will output the predicted points and their predicted possibility
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
