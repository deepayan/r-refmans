<!DOCTYPE html><html><head><title>Help for package ssc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ssc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#coBC'><p>CoBC method</p></a></li>
<li><a href='#coBCCombine'><p>Combining the hypothesis</p></a></li>
<li><a href='#coBCG'><p>CoBC generic method</p></a></li>
<li><a href='#coffee'><p>Time series data set</p></a></li>
<li><a href='#democratic'><p>Democratic method</p></a></li>
<li><a href='#democraticCombine'><p>Combining the hypothesis of the classifiers</p></a></li>
<li><a href='#democraticG'><p>Democratic generic method</p></a></li>
<li><a href='#oneNN'><p>1-NN supervised classifier builder</p></a></li>
<li><a href='#predict.coBC'><p>Predictions of the coBC method</p></a></li>
<li><a href='#predict.democratic'><p>Predictions of the Democratic method</p></a></li>
<li><a href='#predict.OneNN'><p>Model Predictions</p></a></li>
<li><a href='#predict.selfTraining'><p>Predictions of the Self-training method</p></a></li>
<li><a href='#predict.setred'><p>Predictions of the SETRED method</p></a></li>
<li><a href='#predict.snnrce'><p>Predictions of the SNNRCE method</p></a></li>
<li><a href='#predict.triTraining'><p>Predictions of the Tri-training method</p></a></li>
<li><a href='#selfTraining'><p>Self-training method</p></a></li>
<li><a href='#selfTrainingG'><p>Self-training generic method</p></a></li>
<li><a href='#setred'><p>SETRED method</p></a></li>
<li><a href='#setredG'><p>SETRED generic method</p></a></li>
<li><a href='#snnrce'><p>SNNRCE method</p></a></li>
<li><a href='#triTraining'><p>Tri-training method</p></a></li>
<li><a href='#triTrainingCombine'><p>Combining the hypothesis</p></a></li>
<li><a href='#triTrainingG'><p>Tri-training generic method</p></a></li>
<li><a href='#wine'><p>Wine recognition data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Semi-Supervised Classification Methods</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1-0</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides a collection of self-labeled techniques for semi-supervised 
    classification. In semi-supervised classification, both labeled and unlabeled
    data are used to train a classifier. This learning paradigm has obtained promising
    results, specifically in the presence of a reduced set of labeled examples. 
    This package implements a collection of self-labeled techniques to construct a
    classification model. This family of techniques enlarges the original labeled set 
	using the most confident predictions to classify unlabeled data. The techniques 
	implemented can be applied to classification problems in several domains by the 
	specification of a supervised base classifier. At low ratios of labeled data, it 
	can be shown to perform better than classical supervised classifiers.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.3)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, proxy</td>
</tr>
<tr>
<td>Suggests:</td>
<td>caret, e1071, C50, kernlab, testthat, timeDate, stringi,
R.rsp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mabelc/SSC">https://github.com/mabelc/SSC</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mabelc/SSC/issues">https://github.com/mabelc/SSC/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-12-15 20:56:23 UTC; bergmeir</td>
</tr>
<tr>
<td>Author:</td>
<td>Mabel González <a href="https://orcid.org/0000-0003-0152-444X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Osmani Rosado-Falcón
    <a href="https://orcid.org/0000-0002-2639-3354"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  José Daniel Rodríguez
    <a href="https://orcid.org/0000-0002-8489-4106"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Christoph Bergmeir
    <a href="https://orcid.org/0000-0002-3665-9021"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ths, cre],
  Isaac Triguero <a href="https://orcid.org/0000-0002-0150-0651"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  José Manuel Benítez
    <a href="https://orcid.org/0000-0002-2346-0793"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ths]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Christoph Bergmeir &lt;c.bergmeir@decsai.ugr.es&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-12-15 21:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='coBC'>CoBC method</h2><span id='topic+coBC'></span>

<h3>Description</h3>

<p>Co-Training by Committee (CoBC) is a semi-supervised learning algorithm 
with a co-training style. This algorithm trains <code>N</code> classifiers with the learning 
scheme defined in the <code>learner</code> argument using a reduced set of labeled examples. For 
each iteration, an unlabeled 
example is labeled for a classifier if the most confident classifications assigned by the 
other <code>N-1</code> classifiers agree on the labeling proposed. The unlabeled examples 
candidates are selected randomly from a pool of size <code>u</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coBC(x, y, x.inst = TRUE, learner, learner.pars = NULL,
  pred = "predict", pred.pars = NULL, N = 3, perc.full = 0.7,
  u = 100, max.iter = 50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coBC_+3A_x">x</code></td>
<td>
<p>An object that can be coerced to a matrix. This object has two possible 
interpretations according to the value set in the <code>x.inst</code> argument:
a matrix with the training instances where each row represents a single instance
or a precomputed (distance or kernel) matrix between the training examples.</p>
</td></tr>
<tr><td><code id="coBC_+3A_y">y</code></td>
<td>
<p>A vector with the labels of the training instances. In this vector 
the unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="coBC_+3A_x.inst">x.inst</code></td>
<td>
<p>A boolean value that indicates if <code>x</code> is or not an instance matrix.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="coBC_+3A_learner">learner</code></td>
<td>
<p>either a function or a string naming the function for 
training a supervised base classifier, using a set of instances
(or optionally a distance matrix) and it's corresponding classes.</p>
</td></tr>
<tr><td><code id="coBC_+3A_learner.pars">learner.pars</code></td>
<td>
<p>A list with additional parameters for the
<code>learner</code> function if necessary.
Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="coBC_+3A_pred">pred</code></td>
<td>
<p>either a function or a string naming the function for
predicting the probabilities per classes,
using the base classifiers trained with the <code>learner</code> function.
Default is <code>"predict"</code>.</p>
</td></tr>
<tr><td><code id="coBC_+3A_pred.pars">pred.pars</code></td>
<td>
<p>A list with additional parameters for the
<code>pred</code> function if necessary.
Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="coBC_+3A_n">N</code></td>
<td>
<p>The number of classifiers used as committee members. All these classifiers 
are trained using the <code>gen.learner</code> function. Default is 3.</p>
</td></tr>
<tr><td><code id="coBC_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage 
of new labeled examples reaches this value the self-labeling process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="coBC_+3A_u">u</code></td>
<td>
<p>Number of unlabeled instances in the pool. Default is 100.</p>
</td></tr>
<tr><td><code id="coBC_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to execute in the self-labeling process. 
Default is 50.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method trains an ensemble of diverse classifiers. To promote the initial diversity 
the classifiers are trained from the reduced set of labeled examples by Bagging.
The stopping criterion is defined through the fulfillment of one of the following
criteria: the algorithm reaches the number of iterations defined in the <code>max.iter</code>
parameter or the portion of unlabeled set, defined in the <code>perc.full</code> parameter,
is moved to the enlarged labeled set of the classifiers.
</p>


<h3>Value</h3>

<p>A list object of class &quot;coBC&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final <code>N</code> base classifiers trained using the enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of <code>N</code> vectors of indexes related to the training instances 
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the <code>N</code> models. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>pred</dt><dd><p>The function provided in the <code>pred</code> argument.</p>
</dd>
<dt>pred.pars</dt><dd><p>The list provided in the <code>pred.pars</code> argument.</p>
</dd>
<dt>x.inst</dt><dd><p>The value provided in the <code>x.inst</code> argument.</p>
</dd>
</dl>



<h3>References</h3>

<p>Avrim Blum and Tom Mitchell.<br />
<em>Combining labeled and unlabeled data with co-training.</em><br />
In Eleventh Annual Conference on Computational Learning Theory, COLT’ 98, pages 92-100, New York, NY, USA, 1998. ACM.
ISBN 1-58113-057-0. doi: 10.1145/279943.279962.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN as base classifier.
set.seed(1)
m1 &lt;- coBC(x = xtrain, y = ytrain, 
           learner = caret::knn3, 
           learner.pars = list(k = 1),
           pred = "predict")
pred1 &lt;- predict(m1, xitest)
table(pred1, yitest)

## Example: Training from a distance matrix with 1-NN as base classifier.
dtrain &lt;- proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE)
set.seed(1)
m2 &lt;- coBC(x = dtrain, y = ytrain, x.inst = FALSE,
           learner = ssc::oneNN, 
           pred = "predict",
           pred.pars = list(distance.weighting = "none"))
ditest &lt;- proxy::dist(x = xitest, y = xtrain[m2$instances.index,],
                      method = "euclidean", by_rows = TRUE)
pred2 &lt;- predict(m2, ditest)
table(pred2, yitest)

## Example: Training from a set of instances with SVM as base classifier.
learner &lt;- e1071::svm
learner.pars &lt;- list(type = "C-classification", kernel="radial", 
                     probability = TRUE, scale = TRUE)
pred &lt;- function(m, x){
  r &lt;- predict(m, x, probability = TRUE)
  prob &lt;- attr(r, "probabilities")
  prob
}
set.seed(1)
m3 &lt;- coBC(x = xtrain, y = ytrain, 
           learner = learner, 
           learner.pars = learner.pars, 
           pred = pred)
pred3 &lt;- predict(m3, xitest)
table(pred3, yitest)

## Example: Training from a set of instances with Naive-Bayes as base classifier.
set.seed(1)
m4 &lt;- coBC(x = xtrain, y = ytrain, 
           learner = function(x, y) e1071::naiveBayes(x, y), 
           pred = "predict",
           pred.pars = list(type = "raw"))
pred4 &lt;- predict(m4, xitest)
table(pred4, yitest)

## Example: Training from a set of instances with C5.0 as base classifier.
set.seed(1)
m5 &lt;- coBC(x = xtrain, y = ytrain, 
           learner = C50::C5.0, 
           pred = "predict",
           pred.pars = list(type = "prob"))

pred5 &lt;- predict(m5, xitest)
table(pred5, yitest)


</code></pre>

<hr>
<h2 id='coBCCombine'>Combining the hypothesis</h2><span id='topic+coBCCombine'></span>

<h3>Description</h3>

<p>This function combines the probabilities predicted by the committee of 
classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coBCCombine(h.prob, classes)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coBCCombine_+3A_h.prob">h.prob</code></td>
<td>
<p>A list of probability matrices.</p>
</td></tr>
<tr><td><code id="coBCCombine_+3A_classes">classes</code></td>
<td>
<p>The classes in the same order that appear 
in the columns of each matrix in <code>h.prob</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A probability matrix
</p>

<hr>
<h2 id='coBCG'>CoBC generic method</h2><span id='topic+coBCG'></span>

<h3>Description</h3>

<p>CoBC is a semi-supervised learning algorithm with a co-training 
style. This algorithm trains <code>N</code> classifiers with the learning scheme defined in 
<code>gen.learner</code> using a reduced set of labeled examples. For each iteration, an unlabeled
example is labeled for a classifier if the most confident classifications assigned by the 
other <code>N-1</code> classifiers agree on the labeling proposed. The unlabeled examples 
candidates are selected randomly from a pool of size <code>u</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coBCG(y, gen.learner, gen.pred, N = 3, perc.full = 0.7, u = 100,
  max.iter = 50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coBCG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the 
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_gen.learner">gen.learner</code></td>
<td>
<p>A function for training <code>N</code> supervised base classifiers.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_gen.pred">gen.pred</code></td>
<td>
<p>A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_n">N</code></td>
<td>
<p>The number of classifiers used as committee members. All these classifiers 
are trained using the <code>gen.learner</code> function. Default is 3.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage 
of new labeled examples reaches this value the self-labeling process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_u">u</code></td>
<td>
<p>Number of unlabeled instances in the pool. Default is 100.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to execute in the self-labeling process. 
Default is 50.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>coBCG can be helpful in those cases where the method selected as 
base classifier needs a <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general coBC method,
please see <code><a href="#topic+coBC">coBC</a></code> function. Essentially, <code>coBC</code>
function is a wrapper of <code>coBCG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class &quot;coBCG&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final <code>N</code> base classifiers trained using the enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of <code>N</code> vectors of indexes related to the training instances 
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the <code>N</code> models. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
gen.learner1 &lt;- function(indexes, cls)
  caret::knn3(x = xtrain[indexes, ], y = cls, k = 1)
gen.pred1 &lt;- function(model, indexes)
  predict(model, xtrain[indexes, ]) 

set.seed(1)
md1 &lt;- coBCG(y = ytrain, gen.learner1, gen.pred1)

# Predict probabilities per instances using each model
h.prob &lt;- lapply(
  X = md1$model, 
  FUN = function(m) predict(m, xitest)
)
# Combine the predictions
cls1 &lt;- coBCCombine(h.prob, md1$classes)
table(cls1, yitest)

## Example: Training from a distance matrix with 1-NN (oneNN) as base classifier.
dtrain &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
gen.learner2 &lt;- function(indexes, cls) {
  m &lt;- ssc::oneNN(y = cls)
  attr(m, "tra.idxs") &lt;- indexes
  m
}

gen.pred2 &lt;- function(model, indexes)  {
  tra.idxs &lt;- attr(model, "tra.idxs")
  d &lt;- dtrain[indexes, tra.idxs]
  prob &lt;- predict(model, d, distance.weighting = "none")
  prob
}

set.seed(1)
md2 &lt;- coBCG(y = ytrain, gen.learner2, gen.pred2)

# Predict probabilities per instances using each model
ditest &lt;- proxy::dist(x = xitest, y = xtrain[md2$instances.index,],
                      method = "euclidean", by_rows = TRUE)

h.prob &lt;- list()
ninstances &lt;- nrow(dtrain)
for(i in 1:length(md2$model)){
  m &lt;- md2$model[[i]]
  D &lt;- ditest[, md2$model.index.map[[i]]]
  h.prob[[i]] &lt;- predict(m, D)
}
# Combine the predictions
cls2 &lt;- coBCCombine(h.prob, md2$classes)
table(cls2, yitest)

</code></pre>

<hr>
<h2 id='coffee'>Time series data set</h2><span id='topic+coffee'></span>

<h3>Description</h3>

<p>A dataset containing 56 times series z-normalized. Time series length is 286.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(coffee)
</code></pre>


<h3>Format</h3>

<p>A data frame with 56 rows and 287 variables including the class.</p>

<hr>
<h2 id='democratic'>Democratic method</h2><span id='topic+democratic'></span>

<h3>Description</h3>

<p>Democratic Co-Learning is a semi-supervised learning algorithm with a 
co-training style. This algorithm trains N classifiers with different learning schemes 
defined in list <code>gen.learners</code>. During the iterative process, the multiple classifiers
with different inductive biases label data for each other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>democratic(x, y, x.inst = TRUE, learners, learners.pars = NULL,
  preds = rep("predict", length(learners)), preds.pars = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="democratic_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix. This object has two possible 
interpretations according to the value set in the <code>x.inst</code> argument:
a matrix with the training instances where each row represents a single instance
or a precomputed (distance or kernel) matrix between the training examples.</p>
</td></tr>
<tr><td><code id="democratic_+3A_y">y</code></td>
<td>
<p>A vector with the labels of the training instances. In this vector 
the unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="democratic_+3A_x.inst">x.inst</code></td>
<td>
<p>A boolean value that indicates if <code>x</code> is or not an instance matrix.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="democratic_+3A_learners">learners</code></td>
<td>
<p>A list of functions or strings naming the functions for 
training the different supervised base classifiers.</p>
</td></tr>
<tr><td><code id="democratic_+3A_learners.pars">learners.pars</code></td>
<td>
<p>A list with the set of additional parameters for each
learner functions if necessary.
Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="democratic_+3A_preds">preds</code></td>
<td>
<p>A list of functions or strings naming the functions for
predicting the probabilities per classes,
using the base classifiers trained with the functions defined in <code>learners</code>.
Default is <code>"predict"</code> function for each learner in <code>learners</code>.</p>
</td></tr>
<tr><td><code id="democratic_+3A_preds.pars">preds.pars</code></td>
<td>
<p>A list with the set of additional parameters for each
function in <code>preds</code> if necessary.
Default is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method trains an ensemble of diverse classifiers. To promote the initial diversity 
the classifiers must represent different learning schemes.
When x.inst is <code>FALSE</code> all <code>learners</code> defined must be able to learn a classifier
from the precomputed matrix in <code>x</code>.
The iteration process of the algorithm ends when no changes occurs in 
any model during a complete iteration.
The generation of the final hypothesis is 
produced via a weigthed majority voting.
</p>


<h3>Value</h3>

<p>A list object of class &quot;democratic&quot; containing:
</p>

<dl>
<dt>W</dt><dd><p>A vector with the confidence-weighted vote assigned to each classifier.</p>
</dd>
<dt>model</dt><dd><p>A list with the final N base classifiers trained using the 
enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of N vectors of indexes related to the training instances 
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the N <code>models</code>. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>preds</dt><dd><p>The functions provided in the <code>preds</code> argument.</p>
</dd>
<dt>preds.pars</dt><dd><p>The set of lists provided in the <code>preds.pars</code> argument.</p>
</dd>
<dt>x.inst</dt><dd><p>The value provided in the <code>x.inst</code> argument.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 
# 1-NN and C-svc (SVM) as base classifiers.
# knn3 learner
library(caret)
knn &lt;- knn3             # learner function
knn.pars &lt;- list(k = 1) # parameters for learner function
knn.prob &lt;- predict     # function to predict probabilities
knn.prob.pars &lt;- NULL   # parameters for prediction function

# ksvm learner
library(kernlab)
svm &lt;- ksvm             # learner function
svm.pars &lt;- list(       # parameters for learner function
  type = "C-svc",  C = 1, 
  kernel = "rbfdot", kpar = list(sigma = 0.048),
  prob.model = TRUE,
  scaled = FALSE
)
svm.prob &lt;- predict     # function to predict probabilities
svm.prob.pars &lt;- list(  # parameters for prediction function
  type = "probabilities"
)

# train a model
m &lt;- democratic(x = xtrain, y = ytrain, 
                learners = list(knn, svm), 
                learners.pars = list(knn.pars, svm.pars), 
                preds = list(knn.prob, svm.prob), 
                preds.pars = list(knn.prob.pars, svm.prob.pars))
# predict classes
m.pred &lt;- predict(m, xitest)
table(m.pred, yitest)


## End(Not run)

</code></pre>

<hr>
<h2 id='democraticCombine'>Combining the hypothesis of the classifiers</h2><span id='topic+democraticCombine'></span>

<h3>Description</h3>

<p>This function combines the probabilities predicted by the set of 
classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>democraticCombine(pred, W, classes)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="democraticCombine_+3A_pred">pred</code></td>
<td>
<p>A list with the prediction for each classifier.</p>
</td></tr>
<tr><td><code id="democraticCombine_+3A_w">W</code></td>
<td>
<p>A vector with the confidence-weighted vote assigned to each classifier 
during the training process.</p>
</td></tr>
<tr><td><code id="democraticCombine_+3A_classes">classes</code></td>
<td>
<p>the classes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The classification proposed.
</p>

<hr>
<h2 id='democraticG'>Democratic generic method</h2><span id='topic+democraticG'></span>

<h3>Description</h3>

<p>Democratic is a semi-supervised learning algorithm with a co-training 
style. This algorithm trains N classifiers with different learning schemes defined in 
list <code>gen.learners</code>. During the iterative process, the multiple classifiers with
different inductive biases label data for each other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>democraticG(y, gen.learners, gen.preds)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="democraticG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the 
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="democraticG_+3A_gen.learners">gen.learners</code></td>
<td>
<p>A list of functions for training N different supervised base classifiers.
Each function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="democraticG_+3A_gen.preds">gen.preds</code></td>
<td>
<p>A list of functions for predicting the probabilities per classes.
Each function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>democraticG can be helpful in those cases where the method selected as 
base classifier needs a <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general democratic method,
please see <code><a href="#topic+democratic">democratic</a></code> function. Essentially, <code>democratic</code>
function is a wrapper of <code>democraticG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class &quot;democraticG&quot; containing:
</p>

<dl>
<dt>W</dt><dd><p>A vector with the confidence-weighted vote assigned to each classifier.</p>
</dd>
<dt>model</dt><dd><p>A list with the final N base classifiers trained using the 
enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of N vectors of indexes related to the training instances 
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the N <code>models</code>. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
</dl>



<h3>References</h3>

<p>Yan Zhou and Sally Goldman.<br />
<em>Democratic co-learning.</em><br />
In IEEE 16th International Conference on Tools with Artificial Intelligence (ICTAI),
pages 594-602. IEEE, Nov 2004. doi: 10.1109/ICTAI.2004.48.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
# this is a long running example

library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example A: 
# Training from a set of instances with 
# 1-NN and C-svc (SVM) as base classifiers.

### Define knn base classifier using knn3 from caret package
library(caret)
# learner function
knn &lt;- function(indexes, cls) {
  knn3(x = xtrain[indexes, ], y = cls, k = 1)
}
# function to predict probabilities
knn.prob &lt;- function(model, indexes) {
  predict(model, xtrain[indexes, ])
}

### Define svm base classifier using ksvm from kernlab package
library(kernlab)
library(proxy)
# learner function
svm &lt;- function(indexes, cls) {
  rbf &lt;- function(x, y) {
    sigma &lt;- 0.048
    d &lt;- dist(x, y, method = "Euclidean", by_rows = FALSE)
    exp(-sigma *  d * d)
  }
  class(rbf) &lt;- "kernel"
  ksvm(x = xtrain[indexes, ], y = cls, scaled = FALSE,
       type = "C-svc",  C = 1,
       kernel = rbf, prob.model = TRUE)
}
# function to predict probabilities
svm.prob &lt;- function(model, indexes) {
  predict(model, xtrain[indexes, ], type = "probabilities")
}

### Train
m1 &lt;- democraticG(y = ytrain, 
                  gen.learners = list(knn, svm),
                  gen.preds = list(knn.prob, svm.prob))
### Predict
# predict labels using each classifier
m1.pred1 &lt;- predict(m1$model[[1]], xitest, type = "class")
m1.pred2 &lt;- predict(m1$model[[2]], xitest)
# combine predictions
m1.pred &lt;- list(m1.pred1, m1.pred2)
cls1 &lt;- democraticCombine(m1.pred, m1$W, m1$classes)
table(cls1, yitest)

## Example B: 
# Training from a distance matrix and a kernel matrix with 
# 1-NN and C-svc (SVM) as base classifiers.

### Define knn2 base classifier using oneNN from ssc package
library(ssc)
# Compute distance matrix D
# D is used in knn2.prob
D &lt;- as.matrix(dist(x = xtrain, method = "euclidean", by_rows = TRUE))
# learner function
knn2 &lt;- function(indexes, cls) {
  model &lt;- oneNN(y = cls)
  attr(model, "tra.idxs") &lt;- indexes
  model
}
# function to predict probabilities
knn2.prob &lt;- function(model, indexes)  {
  tra.idxs &lt;- attr(model, "tra.idxs")
  predict(model, D[indexes, tra.idxs], distance.weighting = "none")
}

### Define svm2 base classifier using ksvm from kernlab package
library(kernlab)

# Compute kernel matrix K
# K is used in svm2 and svm2.prob functions
sigma &lt;- 0.048
K &lt;- exp(- sigma * D * D)

# learner function
svm2 &lt;- function(indexes, cls) {
  model &lt;- ksvm(K[indexes, indexes], y = cls, 
                type = "C-svc", C = 1,
                kernel = "matrix", 
                prob.model = TRUE)
  attr(model, "tra.idxs") &lt;- indexes
  model
}
# function to predict probabilities
svm2.prob &lt;- function(model, indexes)  {
  tra.idxs &lt;- attr(model, "tra.idxs")
  sv.idxs &lt;- tra.idxs[SVindex(model)]
  predict(model, 
          as.kernelMatrix(K[indexes, sv.idxs]),
          type = "probabilities") 
}


## End(Not run)

</code></pre>

<hr>
<h2 id='oneNN'>1-NN supervised classifier builder</h2><span id='topic+oneNN'></span>

<h3>Description</h3>

<p>Build a model using the given data to be able
to predict the label or the probabilities of other instances,
according to 1-NN algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oneNN(x = NULL, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oneNN_+3A_x">x</code></td>
<td>
<p>This argument is not used, the reason why he gets is to fulfill an agreement</p>
</td></tr>
<tr><td><code id="oneNN_+3A_y">y</code></td>
<td>
<p>a vector with the labels of training instances</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A model with the data needed to use 1-NN
</p>

<hr>
<h2 id='predict.coBC'>Predictions of the coBC method</h2><span id='topic+predict.coBC'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>coBC</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coBC'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.coBC_+3A_object">object</code></td>
<td>
<p>coBC model built with the <code><a href="#topic+coBC">coBC</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.coBC_+3A_x">x</code></td>
<td>
<p>An object that can be coerced to a matrix.
Depending on how the model was built, <code>x</code> is interpreted as a matrix 
with the distances between the unseen instances and the selected training instances, 
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.coBC_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+coBC">coBC</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.democratic'>Predictions of the Democratic method</h2><span id='topic+predict.democratic'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>democratic</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'democratic'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.democratic_+3A_object">object</code></td>
<td>
<p>Democratic model built with the <code><a href="#topic+democratic">democratic</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.democratic_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix 
with the distances between the unseen instances and the selected training instances, 
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.democratic_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+democratic">democratic</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.OneNN'>Model Predictions</h2><span id='topic+predict.OneNN'></span>

<h3>Description</h3>

<p>This function predicts the class label of instances or its probability of
pertaining to each class based on the distance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'OneNN'
predict(object, dists, type = "prob", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.OneNN_+3A_object">object</code></td>
<td>
<p>A model of class OneNN built with <code><a href="#topic+oneNN">oneNN</a></code></p>
</td></tr>
<tr><td><code id="predict.OneNN_+3A_dists">dists</code></td>
<td>
<p>A matrix of distances between the instances to classify (by rows) and
the instances used to train the model (by column)</p>
</td></tr>
<tr><td><code id="predict.OneNN_+3A_type">type</code></td>
<td>
<p>A string that can take two values: <code>"class"</code> for computing the class of
the instances or <code>"prob"</code> for computing the probabilities of belonging to each class.</p>
</td></tr>
<tr><td><code id="predict.OneNN_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>type</code> is equal to <code>"class"</code> a vector of length equal to the rows number
of matrix <code>dists</code>, containing the predicted labels. If <code>type</code> is equal
to <code>"prob"</code> it returns a matrix which has <code>nrow(dists)</code> rows and a column for every
class, where each cell represents the probability that the instance belongs to the class,
according to 1NN.
</p>

<hr>
<h2 id='predict.selfTraining'>Predictions of the Self-training method</h2><span id='topic+predict.selfTraining'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>selfTraining</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'selfTraining'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.selfTraining_+3A_object">object</code></td>
<td>
<p>Self-training model built with the <code><a href="#topic+selfTraining">selfTraining</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.selfTraining_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix 
with the distances between the unseen instances and the selected training instances, 
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.selfTraining_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+selfTraining">selfTraining</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.setred'>Predictions of the SETRED method</h2><span id='topic+predict.setred'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>setred</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'setred'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.setred_+3A_object">object</code></td>
<td>
<p>SETRED model built with the <code><a href="#topic+setred">setred</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.setred_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix 
with the distances between the unseen instances and the selected training instances, 
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.setred_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+setred">setred</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.snnrce'>Predictions of the SNNRCE method</h2><span id='topic+predict.snnrce'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>snnrce</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'snnrce'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.snnrce_+3A_object">object</code></td>
<td>
<p>SNNRCE model built with the <code><a href="#topic+snnrce">snnrce</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.snnrce_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix 
with the distances between the unseen instances and the selected training instances, 
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.snnrce_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+snnrce">snnrce</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.triTraining'>Predictions of the Tri-training method</h2><span id='topic+predict.triTraining'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>triTraining</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'triTraining'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.triTraining_+3A_object">object</code></td>
<td>
<p>Tri-training model built with the <code><a href="#topic+triTraining">triTraining</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.triTraining_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix 
with the distances between the unseen instances and the selected training instances, 
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.triTraining_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+triTraining">triTraining</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='selfTraining'>Self-training method</h2><span id='topic+selfTraining'></span>

<h3>Description</h3>

<p>Self-training is a simple and effective semi-supervised
learning classification method. The self-training classifier is initially
trained with a reduced set of labeled examples. Then it is iteratively retrained
with its own most confident predictions over the unlabeled examples. 
Self-training follows a wrapper methodology using a base supervised 
classifier to establish the possible class of unlabeled instances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selfTraining(x, y, x.inst = TRUE, learner, learner.pars = NULL,
  pred = "predict", pred.pars = NULL, max.iter = 50,
  perc.full = 0.7, thr.conf = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selfTraining_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix. This object has two possible 
interpretations according to the value set in the <code>x.inst</code> argument:
a matrix with the training instances where each row represents a single instance
or a precomputed (distance or kernel) matrix between the training examples.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_y">y</code></td>
<td>
<p>A vector with the labels of the training instances. In this vector 
the unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_x.inst">x.inst</code></td>
<td>
<p>A boolean value that indicates if <code>x</code> is or not an instance matrix.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_learner">learner</code></td>
<td>
<p>either a function or a string naming the function for 
training a supervised base classifier, using a set of instances
(or optionally a distance matrix) and it's corresponding classes.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_learner.pars">learner.pars</code></td>
<td>
<p>A list with additional parameters for the
<code>learner</code> function if necessary.
Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_pred">pred</code></td>
<td>
<p>either a function or a string naming the function for
predicting the probabilities per classes,
using the base classifier trained with the <code>learner</code> function.
Default is <code>"predict"</code>.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_pred.pars">pred.pars</code></td>
<td>
<p>A list with additional parameters for the
<code>pred</code> function if necessary.
Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_max.iter">max.iter</code></td>
<td>
<p>maximum number of iterations to execute the self-labeling process. 
Default is 50.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage 
of new labeled examples reaches this value the self-training process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_thr.conf">thr.conf</code></td>
<td>
<p>A number between 0 and 1 that indicates the confidence threshold.
At each iteration, only the newly labelled examples with a confidence greater than 
this value (<code>thr.conf</code>) are added to the training set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For predicting the most accurate instances per iteration, <code>selfTraining</code>
uses the predictions obtained with the learner specified. To train a model 
using the <code>learner</code> function, it is required a set of instances 
(or a precomputed matrix between the instances if <code>x.inst</code> parameter is <code>FALSE</code>)
in conjunction with the corresponding classes. 
Additionals parameters are provided to the <code>learner</code> function via the 
<code>learner.pars</code> argument. The model obtained is a supervised classifier
ready to predict new instances through the <code>pred</code> function. 
Using a similar idea, the additional parameters to the <code>pred</code> function
are provided using the <code>pred.pars</code> argument. The <code>pred</code> function returns 
the probabilities per class for each new instance. The value of the 
<code>thr.conf</code> argument controls the confidence of instances selected 
to enlarge the labeled set for the next iteration.
</p>
<p>The stopping criterion is defined through the fulfillment of one of the following
criteria: the algorithm reaches the number of iterations defined in the <code>max.iter</code>
parameter or the portion of the unlabeled set, defined in the <code>perc.full</code> parameter,
is moved to the labeled set. In some cases, the process stops and no instances 
are added to the original labeled set. In this case, the user must assign a more 
flexible value to the <code>thr.conf</code> parameter.
</p>


<h3>Value</h3>

<p>A list object of class &quot;selfTraining&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of the training instances used to 
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to <code>x</code> argument.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>pred</dt><dd><p>The function provided in the <code>pred</code> argument.</p>
</dd>
<dt>pred.pars</dt><dd><p>The list provided in the <code>pred.pars</code> argument.</p>
</dd>
</dl>



<h3>References</h3>

<p>David Yarowsky.<br />
<em>Unsupervised word sense disambiguation rivaling supervised methods.</em><br />
In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,
pages 189-196. Association for Computational Linguistics, 1995.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN as base classifier.
m1 &lt;- selfTraining(x = xtrain, y = ytrain, 
                   learner = caret::knn3, 
                   learner.pars = list(k = 1),
                   pred = "predict")
pred1 &lt;- predict(m1, xitest)
table(pred1, yitest)

## Example: Training from a distance matrix with 1-NN as base classifier.
dtrain &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
m2 &lt;- selfTraining(x = dtrain, y = ytrain, x.inst = FALSE,
                   learner = ssc::oneNN, 
                   pred = "predict",
                   pred.pars = list(distance.weighting = "none"))
ditest &lt;- proxy::dist(x = xitest, y = xtrain[m2$instances.index,],
                      method = "euclidean", by_rows = TRUE)
pred2 &lt;- predict(m2, ditest)
table(pred2, yitest)

## Example: Training from a set of instances with SVM as base classifier.
learner &lt;- e1071::svm
learner.pars &lt;- list(type = "C-classification", kernel="radial", 
                     probability = TRUE, scale = TRUE)
pred &lt;- function(m, x){
  r &lt;- predict(m, x, probability = TRUE)
  prob &lt;- attr(r, "probabilities")
  prob
}
m3 &lt;- selfTraining(x = xtrain, y = ytrain, 
                   learner = learner, 
                   learner.pars = learner.pars, 
                   pred = pred)
pred3 &lt;- predict(m3, xitest)
table(pred3, yitest)

## Example: Training from a set of instances with Naive-Bayes as base classifier.
m4 &lt;- selfTraining(x = xtrain, y = ytrain, 
                   learner = function(x, y) e1071::naiveBayes(x, y), 
                   pred = "predict",
                   pred.pars = list(type = "raw"))
pred4 &lt;- predict(m4, xitest)
table(pred4, yitest)

## Example: Training from a set of instances with C5.0 as base classifier.
m5 &lt;- selfTraining(x = xtrain, y = ytrain, 
                   learner = C50::C5.0, 
                   pred = "predict",
                   pred.pars = list(type = "prob"))
pred5 &lt;- predict(m5, xitest)
table(pred5, yitest)


</code></pre>

<hr>
<h2 id='selfTrainingG'>Self-training generic method</h2><span id='topic+selfTrainingG'></span>

<h3>Description</h3>

<p>Self-training is a simple and effective semi-supervised
learning classification method. The self-training classifier is initially
trained with a reduced set of labeled examples. Then it is iteratively retrained
with its own most confident predictions over the unlabeled examples. 
Self-training follows a wrapper methodology using one base supervised 
classifier to establish the possible class of unlabeled instances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selfTrainingG(y, gen.learner, gen.pred, max.iter = 50, perc.full = 0.7,
  thr.conf = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selfTrainingG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the 
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="selfTrainingG_+3A_gen.learner">gen.learner</code></td>
<td>
<p>A function for training a supervised base classifier.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="selfTrainingG_+3A_gen.pred">gen.pred</code></td>
<td>
<p>A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
<tr><td><code id="selfTrainingG_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to execute the self-labeling process. 
Default is 50.</p>
</td></tr>
<tr><td><code id="selfTrainingG_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage 
of new labeled examples reaches this value the self-training process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="selfTrainingG_+3A_thr.conf">thr.conf</code></td>
<td>
<p>A number between 0 and 1 that indicates the confidence theshold.
At each iteration, only the newly labelled examples with a confidence greater than 
this value (<code>thr.conf</code>) are added to the training set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SelfTrainingG can be helpful in those cases where the method selected as 
base classifier needs <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general self-training method,
please see the <code><a href="#topic+selfTraining">selfTraining</a></code> function. Essentially, the <code>selfTraining</code>
function is a wrapper of the <code>selfTrainingG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class &quot;selfTrainingG&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of the training instances used to 
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to the <code>y</code> argument.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
gen.learner &lt;- function(indexes, cls)
  caret::knn3(x = xtrain[indexes, ], y = cls, k = 1)
gen.pred &lt;- function(model, indexes)
  predict(model, xtrain[indexes, ]) 

md1 &lt;- selfTrainingG(y = ytrain, gen.learner, gen.pred)

cls1 &lt;- predict(md1$model, xitest, type = "class")
table(cls1, yitest)

## Example: Training from a distance matrix with 1-NN (oneNN) as base classifier.
dtrain &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
gen.learner &lt;- function(indexes, cls) {
  m &lt;- ssc::oneNN(y = cls)
  attr(m, "tra.idxs") &lt;- indexes
  m
}

gen.pred &lt;- function(model, indexes)  {
  tra.idxs &lt;- attr(model, "tra.idxs")
  d &lt;- dtrain[indexes, tra.idxs]
  prob &lt;- predict(model, d, distance.weighting = "none")
  prob
}

md2 &lt;- selfTrainingG(y = ytrain, gen.learner, gen.pred)
ditest &lt;- proxy::dist(x = xitest, y = xtrain[md2$instances.index,],
                      method = "euclidean", by_rows = TRUE)
cls2 &lt;- predict(md2$model, ditest, type = "class")
table(cls2, yitest)
</code></pre>

<hr>
<h2 id='setred'>SETRED method</h2><span id='topic+setred'></span>

<h3>Description</h3>

<p>SETRED (SElf-TRaining with EDiting) is a variant of the self-training 
classification method (as implemented in the function <code><a href="#topic+selfTraining">selfTraining</a></code>) with a different addition mechanism. 
The SETRED classifier is initially trained with a 
reduced set of labeled examples. Then, it is iteratively retrained with its own most 
confident predictions over the unlabeled examples. SETRED uses an amending scheme 
to avoid the introduction of noisy examples into the enlarged labeled set. For each 
iteration, the mislabeled examples are identified using the local information provided 
by the neighborhood graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setred(x, y, x.inst = TRUE, dist = "Euclidean", learner,
  learner.pars = NULL, pred = "predict", pred.pars = NULL,
  theta = 0.1, max.iter = 50, perc.full = 0.7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setred_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix. This object has two possible 
interpretations according to the value set in the <code>x.inst</code> argument:
a matrix with the training instances where each row represents a single instance
or a precomputed (distance or kernel) matrix between the training examples.</p>
</td></tr>
<tr><td><code id="setred_+3A_y">y</code></td>
<td>
<p>A vector with the labels of the training instances. In this vector 
the unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="setred_+3A_x.inst">x.inst</code></td>
<td>
<p>A boolean value that indicates if <code>x</code> is or not an instance matrix.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="setred_+3A_dist">dist</code></td>
<td>
<p>A distance function or the name of a distance available
in the <code>proxy</code> package to compute 
the distance matrix in the case that <code>x.inst</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="setred_+3A_learner">learner</code></td>
<td>
<p>either a function or a string naming the function for 
training a supervised base classifier, using a set of instances
(or optionally a distance matrix) and it's corresponding classes.</p>
</td></tr>
<tr><td><code id="setred_+3A_learner.pars">learner.pars</code></td>
<td>
<p>A list with additional parameters for the
<code>learner</code> function if necessary.
Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="setred_+3A_pred">pred</code></td>
<td>
<p>either a function or a string naming the function for
predicting the probabilities per classes,
using the base classifier trained with the <code>learner</code> function.
Default is <code>"predict"</code>.</p>
</td></tr>
<tr><td><code id="setred_+3A_pred.pars">pred.pars</code></td>
<td>
<p>A list with additional parameters for the
<code>pred</code> function if necessary.
Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="setred_+3A_theta">theta</code></td>
<td>
<p>Rejection threshold to test the critical region. Default is 0.1.</p>
</td></tr>
<tr><td><code id="setred_+3A_max.iter">max.iter</code></td>
<td>
<p>maximum number of iterations to execute the self-labeling process. 
Default is 50.</p>
</td></tr>
<tr><td><code id="setred_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage 
of new labeled examples reaches this value the self-training process is stopped.
Default is 0.7.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SETRED initiates the self-labeling process by training a model from the original 
labeled set. In each iteration, the <code>learner</code> function detects unlabeled 
examples for which it makes the most confident prediction and labels those examples 
according to the <code>pred</code> function. The identification of mislabeled examples is 
performed using a neighborhood graph created from the distance matrix.
When <code>x.inst</code> is <code>TRUE</code> this distance matrix is computed using
the <code>dist</code> function. On the other hand, when <code>x.inst</code> is <code>FALSE</code>
the matrix provided with <code>x</code> is used both to train a classifier and to create
the neighborhood graph.
Most examples possess the same label in a neighborhood. So if an example locates 
in a neighborhood with too many neighbors from different classes, this example should 
be considered problematic. The value of the <code>theta</code> argument controls the confidence 
of the candidates selected to enlarge the labeled set. The lower this value is, the more 
restrictive is the selection of the examples that are considered good.
For more information about the self-labeled process and the rest of the parameters, please 
see <code><a href="#topic+selfTraining">selfTraining</a></code>.
</p>


<h3>Value</h3>

<p>A list object of class &quot;setred&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of the training instances used to 
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to <code>x</code> argument.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>pred</dt><dd><p>The function provided in the <code>pred</code> argument.</p>
</dd>
<dt>pred.pars</dt><dd><p>The list provided in the <code>pred.pars</code> argument.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ming Li and ZhiHua Zhou.<br />
<em>Setred: Self-training with editing.</em><br />
In Advances in Knowledge Discovery and Data Mining, volume 3518 of Lecture Notes in
Computer Science, pages 611-621. Springer Berlin Heidelberg, 2005.
ISBN 978-3-540-26076-9. doi: 10.1007/11430919 71.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN as base classifier.
m1 &lt;- setred(x = xtrain, y = ytrain, dist = "euclidean", 
            learner = caret::knn3, 
            learner.pars = list(k = 1),
            pred = "predict")
pred1 &lt;- predict(m1, xitest)
table(pred1, yitest)

## Example: Training from a distance matrix with 1-NN as base classifier.
# Compute distances between training instances
library(proxy)
D &lt;- dist(x = xtrain, method = "euclidean", by_rows = TRUE)

m2 &lt;- setred(x = D, y = ytrain, x.inst = FALSE,
            learner = ssc::oneNN, 
            pred = "predict",
            pred.pars = list(distance.weighting = "none"))
ditest &lt;- proxy::dist(x = xitest, y = xtrain[m2$instances.index,],
                      method = "euclidean", by_rows = TRUE)
pred2 &lt;- predict(m2, ditest)
table(pred2, yitest)

## Example: Training from a set of instances with SVM as base classifier.
learner &lt;- e1071::svm
learner.pars &lt;- list(type = "C-classification", kernel="radial", 
                     probability = TRUE, scale = TRUE)
pred &lt;- function(m, x){
  r &lt;- predict(m, x, probability = TRUE)
  prob &lt;- attr(r, "probabilities")
  prob
}
m3 &lt;- setred(x = xtrain, y = ytrain, dist = "euclidean", 
             learner = learner, 
             learner.pars = learner.pars, 
             pred = pred)
pred3 &lt;- predict(m3, xitest)
table(pred3, yitest)

## Example: Training from a set of instances with Naive-Bayes as base classifier.
m4 &lt;- setred(x = xtrain, y = ytrain, dist = "euclidean",
             learner = function(x, y) e1071::naiveBayes(x, y), 
             pred = "predict",
             pred.pars = list(type = "raw"))
pred4 &lt;- predict(m4, xitest)
table(pred4, yitest)

## Example: Training from a set of instances with C5.0 as base classifier.
m5 &lt;- setred(x = xtrain, y = ytrain, dist = "euclidean",
             learner = C50::C5.0, 
             pred = "predict",
             pred.pars = list(type = "prob"))
pred5 &lt;- predict(m5, xitest)
table(pred5, yitest)


</code></pre>

<hr>
<h2 id='setredG'>SETRED generic method</h2><span id='topic+setredG'></span>

<h3>Description</h3>

<p>SETRED is a variant of the self-training classification method 
(<code><a href="#topic+selfTraining">selfTraining</a></code>) with a different addition mechanism. 
The SETRED classifier is initially trained with a 
reduced set of labeled examples. Then it is iteratively retrained with its own most 
confident predictions over the unlabeled examples. SETRED uses an amending scheme 
to avoid the introduction of noisy examples into the enlarged labeled set. For each 
iteration, the mislabeled examples are identified using the local information provided 
by the neighborhood graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setredG(y, D, gen.learner, gen.pred, theta = 0.1, max.iter = 50,
  perc.full = 0.7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setredG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the 
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="setredG_+3A_d">D</code></td>
<td>
<p>A distance matrix between all the training instances. This matrix is used to 
construct the neighborhood graph.</p>
</td></tr>
<tr><td><code id="setredG_+3A_gen.learner">gen.learner</code></td>
<td>
<p>A function for training a supervised base classifier.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="setredG_+3A_gen.pred">gen.pred</code></td>
<td>
<p>A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
<tr><td><code id="setredG_+3A_theta">theta</code></td>
<td>
<p>Rejection threshold to test the critical region. Default is 0.1.</p>
</td></tr>
<tr><td><code id="setredG_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to execute the self-labeling process. 
Default is 50.</p>
</td></tr>
<tr><td><code id="setredG_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage 
of new labeled examples reaches this value the self-training process is stopped.
Default is 0.7.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SetredG can be helpful in those cases where the method selected as 
base classifier needs a <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general setred method,
please see <code><a href="#topic+setred">setred</a></code> function. Essentially, <code>setred</code>
function is a wrapper of <code>setredG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class &quot;setredG&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of the training instances used to 
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to the <code>y</code> argument.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

# Compute distances between training instances
D &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))

## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
gen.learner &lt;- function(indexes, cls)
  caret::knn3(x = xtrain[indexes, ], y = cls, k = 1)
gen.pred &lt;- function(model, indexes)
  predict(model, xtrain[indexes, ]) 

md1 &lt;- setredG(y = ytrain, D, gen.learner, gen.pred)

cls1 &lt;- predict(md1$model, xitest, type = "class")
table(cls1, yitest)

## Example: Training from a distance matrix with 1-NN (oneNN) as base classifier
gen.learner &lt;- function(indexes, cls) {
  m &lt;- ssc::oneNN(y = cls)
  attr(m, "tra.idxs") &lt;- indexes
  m
}

gen.pred &lt;- function(model, indexes)  {
  tra.idxs &lt;- attr(model, "tra.idxs")
  d &lt;- D[indexes, tra.idxs]
  prob &lt;- predict(model, d, distance.weighting = "none")
  prob
}

md2 &lt;- setredG(y = ytrain, D, gen.learner, gen.pred)
ditest &lt;- proxy::dist(x = xitest, y = xtrain[md2$instances.index,],
                      method = "euclidean", by_rows = TRUE)
cls2 &lt;- predict(md2$model, ditest, type = "class")
table(cls2, yitest)
</code></pre>

<hr>
<h2 id='snnrce'>SNNRCE method</h2><span id='topic+snnrce'></span>

<h3>Description</h3>

<p>SNNRCE (Self-training Nearest Neighbor Rule using Cut Edges) is a variant 
of the self-training classification method (<code><a href="#topic+selfTraining">selfTraining</a></code>) with a different 
addition mechanism and a fixed learning scheme (1-NN). SNNRCE uses an amending scheme 
to avoid the introduction of noisy examples into the enlarged labeled set.
The mislabeled examples are identified using the local information provided 
by the neighborhood graph. A statistical test using cut edge weight is used to modify 
the labels of the missclassified examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>snnrce(x, y, x.inst = TRUE, dist = "Euclidean", alpha = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="snnrce_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix. This object has two possible 
interpretations according to the value set in the <code>x.inst</code> argument:
a matrix with the training instances where each row represents a single instance
or a precomputed distance matrix between the training examples.</p>
</td></tr>
<tr><td><code id="snnrce_+3A_y">y</code></td>
<td>
<p>A vector with the labels of the training instances. In this vector 
the unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="snnrce_+3A_x.inst">x.inst</code></td>
<td>
<p>A boolean value that indicates if <code>x</code> is or not an instance matrix.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="snnrce_+3A_dist">dist</code></td>
<td>
<p>A distance function available in the <code>proxy</code> package to compute 
the distance matrix in the case that <code>x.inst</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="snnrce_+3A_alpha">alpha</code></td>
<td>
<p>Rejection threshold to test the critical region. Default is 0.1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SNNRCE initiates the self-labeling process by training a 1-NN from the original 
labeled set. This method attempts to reduce the noise in examples by labeling those instances 
with no cut edges in the initial stages of self-labeling learning. 
These highly confident examples are added into the training set. 
The remaining examples follow the standard self-training process until a minimum number 
of examples will be labeled for each class. A statistical test using cut edge weight is used 
to modify the labels of the missclassified examples The value of the <code>alpha</code> argument 
defines the critical region where the candidates examples are tested. The higher this value 
is, the more relaxed it is the selection of the examples that are considered mislabeled.
</p>


<h3>Value</h3>

<p>A list object of class &quot;snnrce&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of the training instances used to 
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to <code>x</code> argument.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>x.inst</dt><dd><p>The value provided in the <code>x.inst</code> argument.</p>
</dd>
<dt>dist</dt><dd><p>The value provided in the <code>dist</code> argument when x.inst is <code>TRUE</code>.</p>
</dd>
<dt>xtrain</dt><dd><p>A matrix with the subset of training instances referenced by the indexes 
<code>instances.index</code> when x.inst is <code>TRUE</code>.</p>
</dd>
</dl>



<h3>References</h3>

<p>Yu Wang, Xiaoyan Xu, Haifeng Zhao, and Zhongsheng Hua.<br />
<em>Semisupervised learning based on nearest neighbor rule and cut edges.</em><br />
Knowledge-Based Systems, 23(6):547-554, 2010. ISSN 0950-7051. doi: http://dx.doi.org/10.1016/j.knosys.2010.03.012.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN as base classifier.
m1 &lt;- snnrce(x = xtrain, y = ytrain,  dist = "Euclidean")
pred1 &lt;- predict(m1, xitest)
table(pred1, yitest)

## Example: Training from a distance matrix with 1-NN as base classifier.
dtrain &lt;- proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE)
m2 &lt;- snnrce(x = dtrain, y = ytrain, x.inst = FALSE)
ditest &lt;- proxy::dist(x = xitest, y = xtrain[m2$instances.index,],
                      method = "euclidean", by_rows = TRUE)
pred2 &lt;- predict(m2, ditest)
table(pred2, yitest)

</code></pre>

<hr>
<h2 id='triTraining'>Tri-training method</h2><span id='topic+triTraining'></span>

<h3>Description</h3>

<p>Tri-training is a semi-supervised learning algorithm with a co-training 
style. This algorithm trains three classifiers with the same learning scheme from a 
reduced set of labeled examples. For each iteration, an unlabeled example is labeled 
for a classifier if the other two classifiers agree on the labeling proposed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>triTraining(x, y, x.inst = TRUE, learner, learner.pars = NULL,
  pred = "predict", pred.pars = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="triTraining_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix. This object has two possible 
interpretations according to the value set in the <code>x.inst</code> argument:
a matrix with the training instances where each row represents a single instance
or a precomputed (distance or kernel) matrix between the training examples.</p>
</td></tr>
<tr><td><code id="triTraining_+3A_y">y</code></td>
<td>
<p>A vector with the labels of the training instances. In this vector 
the unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="triTraining_+3A_x.inst">x.inst</code></td>
<td>
<p>A boolean value that indicates if <code>x</code> is or not an instance matrix.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="triTraining_+3A_learner">learner</code></td>
<td>
<p>either a function or a string naming the function for 
training a supervised base classifier, using a set of instances
(or optionally a distance matrix) and it's corresponding classes.</p>
</td></tr>
<tr><td><code id="triTraining_+3A_learner.pars">learner.pars</code></td>
<td>
<p>A list with additional parameters for the
<code>learner</code> function if necessary.
Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="triTraining_+3A_pred">pred</code></td>
<td>
<p>either a function or a string naming the function for
predicting the probabilities per classes,
using the base classifiers trained with the <code>learner</code> function.
Default is <code>"predict"</code>.</p>
</td></tr>
<tr><td><code id="triTraining_+3A_pred.pars">pred.pars</code></td>
<td>
<p>A list with additional parameters for the
<code>pred</code> function if necessary.
Default is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tri-training initiates the self-labeling process by training three models from the 
original labeled set, using the <code>learner</code> function specified. 
In each iteration, the algorithm detects unlabeled examples on which two classifiers 
agree with the classification and includes these instances in the enlarged set of the 
third classifier under certain conditions. The generation of the final hypothesis is 
produced via the majority voting. The iteration process ends when no changes occur in 
any model during a complete iteration.
</p>


<h3>Value</h3>

<p>A list object of class &quot;triTraining&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final three base classifiers trained using the enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of three vectors of indexes related to the training instances 
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the three models. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>pred</dt><dd><p>The function provided in the <code>pred</code> argument.</p>
</dd>
<dt>pred.pars</dt><dd><p>The list provided in the <code>pred.pars</code> argument.</p>
</dd>
<dt>x.inst</dt><dd><p>The value provided in the <code>x.inst</code> argument.</p>
</dd>
</dl>



<h3>References</h3>

<p>ZhiHua Zhou and Ming Li.<br />
<em>Tri-training: exploiting unlabeled data using three classifiers.</em><br />
IEEE Transactions on Knowledge and Data Engineering, 17(11):1529-1541, Nov 2005. ISSN 1041-4347. doi: 10.1109/TKDE.2005. 186.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN as base classifier.
set.seed(1)
m1 &lt;- triTraining(x = xtrain, y = ytrain, 
                  learner = caret::knn3, 
                  learner.pars = list(k = 1),
                  pred = "predict")
pred1 &lt;- predict(m1, xitest)
table(pred1, yitest)

## Example: Training from a distance matrix with 1-NN as base classifier.
dtrain &lt;- proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE)
set.seed(1)
m2 &lt;- triTraining(x = dtrain, y = ytrain, x.inst = FALSE,
                  learner = ssc::oneNN, 
                  pred = "predict",
                  pred.pars = list(distance.weighting = "none"))
ditest &lt;- proxy::dist(x = xitest, y = xtrain[m2$instances.index,],
                      method = "euclidean", by_rows = TRUE)
pred2 &lt;- predict(m2, ditest)
table(pred2, yitest)

## Example: Training from a set of instances with SVM as base classifier.
learner &lt;- e1071::svm
learner.pars &lt;- list(type = "C-classification", kernel="radial", 
                     probability = TRUE, scale = TRUE)
pred &lt;- function(m, x){
  r &lt;- predict(m, x, probability = TRUE)
  prob &lt;- attr(r, "probabilities")
  prob
}
set.seed(1)
m3 &lt;- triTraining(x = xtrain, y = ytrain, 
                  learner = learner, 
                  learner.pars = learner.pars, 
                  pred = pred)
pred3 &lt;- predict(m3, xitest)
table(pred3, yitest)

## Example: Training from a set of instances with Naive-Bayes as base classifier.
set.seed(1)
m4 &lt;- triTraining(x = xtrain, y = ytrain, 
                  learner = function(x, y) e1071::naiveBayes(x, y), 
                  pred.pars = list(type = "raw"))
pred4 &lt;- predict(m4, xitest)
table(pred4, yitest)

## Example: Training from a set of instances with C5.0 as base classifier.
set.seed(1)
m5 &lt;- triTraining(x = xtrain, y = ytrain, 
                  learner = C50::C5.0, 
                  pred.pars = list(type = "prob"))
pred5 &lt;- predict(m5, xitest)
table(pred5, yitest)


</code></pre>

<hr>
<h2 id='triTrainingCombine'>Combining the hypothesis</h2><span id='topic+triTrainingCombine'></span>

<h3>Description</h3>

<p>This function combines the predictions obtained 
by the set of classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>triTrainingCombine(pred)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="triTrainingCombine_+3A_pred">pred</code></td>
<td>
<p>A list with the predictions of each classifiers</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of classes
</p>

<hr>
<h2 id='triTrainingG'>Tri-training generic method</h2><span id='topic+triTrainingG'></span>

<h3>Description</h3>

<p>Tri-training is a semi-supervised learning algorithm with a co-training 
style. This algorithm trains three classifiers with the same learning scheme from a 
reduced set of labeled examples. For each iteration, an unlabeled example is labeled 
for a classifier if the other two classifiers agree on the labeling proposed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>triTrainingG(y, gen.learner, gen.pred)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="triTrainingG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the 
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="triTrainingG_+3A_gen.learner">gen.learner</code></td>
<td>
<p>A function for training three supervised base classifiers.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="triTrainingG_+3A_gen.pred">gen.pred</code></td>
<td>
<p>A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>TriTrainingG can be helpful in those cases where the method selected as 
base classifier needs a <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general triTraining method,
please see the <code><a href="#topic+triTraining">triTraining</a></code> function. Essentially, the <code>triTraining</code>
function is a wrapper of the <code>triTrainingG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class &quot;triTrainingG&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final three base classifiers trained using the enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of three vectors of indexes related to the training instances 
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the three models. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
gen.learner &lt;- function(indexes, cls)
  caret::knn3(x = xtrain[indexes, ], y = cls, k = 1)
gen.pred &lt;- function(model, indexes)
  predict(model, xtrain[indexes, ]) 

# Train
set.seed(1)
md1 &lt;- triTrainingG(y = ytrain, gen.learner, gen.pred)

# Predict testing instances using the three classifiers
pred &lt;- lapply(
  X = md1$model, 
  FUN = function(m) predict(m, xitest, type = "class")
)
# Combine the predictions
cls1 &lt;- triTrainingCombine(pred)
table(cls1, yitest)

## Example: Training from a distance matrix with 1-NN (oneNN) as base classifier.
dtrain &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
gen.learner &lt;- function(indexes, cls) {
  m &lt;- ssc::oneNN(y = cls)
  attr(m, "tra.idxs") &lt;- indexes
  m
}

gen.pred &lt;- function(model, indexes)  {
  tra.idxs &lt;- attr(model, "tra.idxs")
  d &lt;- dtrain[indexes, tra.idxs]
  prob &lt;- predict(model, d, distance.weighting = "none")
  prob
}

# Train
set.seed(1)
md2 &lt;- triTrainingG(y = ytrain, gen.learner, gen.pred)

# Predict
ditest &lt;- proxy::dist(x = xitest, y = xtrain[md2$instances.index,],
                      method = "euclidean", by_rows = TRUE)

# Predict testing instances using the three classifiers
pred &lt;- mapply(
  FUN = function(m, indexes){
    D &lt;- ditest[, indexes]
    predict(m, D, type = "class")
  },
  m = md2$model,
  indexes = md2$model.index.map,
  SIMPLIFY = FALSE
)
# Combine the predictions
cls2 &lt;- triTrainingCombine(pred)
table(cls2, yitest)
</code></pre>

<hr>
<h2 id='wine'>Wine recognition data</h2><span id='topic+wine'></span>

<h3>Description</h3>

<p>This dataset is the result of a chemical analysis of wine grown in the same
region in Italy but derived from three different cultivars. The analysis determined
the quantities of 13 constituents found in each of the three types of wines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(wine)
</code></pre>


<h3>Format</h3>

<p>A data frame with 178 rows and 14 variables including the class.</p>


<h3>Details</h3>

<p>The dataset is taken from the UCI data repository, to which it was donated 
by Riccardo Leardi, University of Genova. The attributes are as follows:
</p>

<ul>
<li><p> Alcohol
</p>
</li>
<li><p> Malic acid
</p>
</li>
<li><p> Ash
</p>
</li>
<li><p> Alcalinity of ash
</p>
</li>
<li><p> Magnesium
</p>
</li>
<li><p> Total phenols
</p>
</li>
<li><p> Flavanoids
</p>
</li>
<li><p> Nonflavanoid phenols
</p>
</li>
<li><p> Proanthocyanins
</p>
</li>
<li><p> Color intensity
</p>
</li>
<li><p> Hue
</p>
</li>
<li><p> OD280/OD315 of diluted wines
</p>
</li>
<li><p> Proline
</p>
</li>
<li><p> Wine (class)
</p>
</li></ul>



<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/Wine">https://archive.ics.uci.edu/ml/datasets/Wine</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
