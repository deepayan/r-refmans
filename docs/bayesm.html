<!DOCTYPE html><html><head><title>Help for package bayesm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bayesm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bank'><p>Bank Card Conjoint Data</p></a></li>
<li><a href='#breg'><p>Posterior Draws from a Univariate Regression with Unit Error Variance</p></a></li>
<li><a href='#camera'><p>Conjoint Survey Data for Digital Cameras</p></a></li>
<li><a href='#cgetC'><p>Obtain A List of Cut-offs for Scale Usage Problems</p></a></li>
<li><a href='#cheese'><p>Sliced Cheese Data</p></a></li>
<li><a href='#clusterMix'><p>Cluster Observations Based on Indicator MCMC Draws</p></a></li>
<li><a href='#condMom'><p>Computes Conditional Mean/Var of One Element of MVN given All Others</p></a></li>
<li><a href='#createX'><p>Create X Matrix for Use in Multinomial Logit and Probit Routines</p></a></li>
<li><a href='#customerSat'><p>Customer Satisfaction Data</p></a></li>
<li><a href='#detailing'><p>Physician Detailing Data</p></a></li>
<li><a href='#eMixMargDen'><p>Compute Marginal Densities of A Normal Mixture Averaged over MCMC Draws</p></a></li>
<li><a href='#ghkvec'><p>Compute GHK approximation to Multivariate Normal Integrals</p></a></li>
<li><a href='#llmnl'><p>Evaluate Log Likelihood for Multinomial Logit Model</p></a></li>
<li><a href='#llmnp'><p>Evaluate Log Likelihood for Multinomial Probit Model</p></a></li>
<li><a href='#llnhlogit'><p>Evaluate Log Likelihood for non-homothetic Logit Model</p></a></li>
<li><a href='#lndIChisq'><p>Compute Log of Inverted Chi-Squared Density</p></a></li>
<li><a href='#lndIWishart'><p>Compute Log of Inverted Wishart Density</p></a></li>
<li><a href='#lndMvn'><p> Compute Log of Multivariate Normal Density</p></a></li>
<li><a href='#lndMvst'><p>Compute Log of Multivariate Student-t Density</p></a></li>
<li><a href='#logMargDenNR'><p>Compute Log Marginal Density Using Newton-Raftery Approx</p></a></li>
<li><a href='#margarine'><p>Household Panel Data on Margarine Purchases</p></a></li>
<li><a href='#mixDen'><p>Compute Marginal Density for Multivariate Normal Mixture</p></a></li>
<li><a href='#mixDenBi'><p>Compute Bivariate Marginal Density for a Normal Mixture</p></a></li>
<li><a href='#mnlHess'><p> Computes &ndash;Expected Hessian for Multinomial Logit</p></a></li>
<li><a href='#mnpProb'><p>Compute MNP Probabilities</p></a></li>
<li><a href='#momMix'><p>Compute Posterior Expectation of Normal Mixture Model Moments</p></a></li>
<li><a href='#nmat'><p>Convert Covariance Matrix to a Correlation Matrix</p></a></li>
<li><a href='#numEff'><p>Compute Numerical Standard Error and Relative Numerical Efficiency</p></a></li>
<li><a href='#orangeJuice'><p>Store-level Panel Data on Orange Juice Sales</p></a></li>
<li><a href='#plot.bayesm.hcoef'><p>Plot Method for Hierarchical Model Coefs</p></a></li>
<li><a href='#plot.bayesm.mat'><p>Plot Method for Arrays of MCMC Draws</p></a></li>
<li><a href='#plot.bayesm.nmix'><p>Plot Method for MCMC Draws of Normal Mixtures</p></a></li>
<li><a href='#rbayesBLP'><p>Bayesian Analysis of Random Coefficient Logit Models Using Aggregate Data</p></a></li>
<li><a href='#rbiNormGibbs'><p>Illustrate Bivariate Normal Gibbs Sampler</p></a></li>
<li><a href='#rbprobitGibbs'><p>Gibbs Sampler (Albert and Chib) for Binary Probit</p></a></li>
<li><a href='#rdirichlet'><p>Draw From Dirichlet Distribution</p></a></li>
<li><a href='#rDPGibbs'><p> Density Estimation with Dirichlet Process Prior and Normal Base</p></a></li>
<li><a href='#rhierBinLogit'><p>MCMC Algorithm for Hierarchical Binary Logit</p></a></li>
<li><a href='#rhierLinearMixture'><p>Gibbs Sampler for Hierarchical Linear Model with Mixture-of-Normals Heterogeneity</p></a></li>
<li><a href='#rhierLinearModel'><p>Gibbs Sampler for Hierarchical Linear Model with Normal Heterogeneity</p></a></li>
<li><a href='#rhierMnlDP'><p>MCMC Algorithm for Hierarchical Multinomial Logit with Dirichlet Process Prior Heterogeneity</p></a></li>
<li><a href='#rhierMnlRwMixture'><p>MCMC Algorithm for Hierarchical Multinomial Logit with Mixture-of-Normals Heterogeneity</p></a></li>
<li><a href='#rhierNegbinRw'><p>MCMC Algorithm for Hierarchical Negative Binomial Regression</p></a></li>
<li><a href='#rivDP'><p>Linear &quot;IV&quot; Model with DP Process Prior for Errors</p></a></li>
<li><a href='#rivGibbs'><p>Gibbs Sampler for Linear &quot;IV&quot; Model</p></a></li>
<li><a href='#rmixGibbs'><p> Gibbs Sampler for Normal Mixtures w/o Error Checking</p></a></li>
<li><a href='#rmixture'><p>Draw from Mixture of Normals</p></a></li>
<li><a href='#rmnlIndepMetrop'><p>MCMC Algorithm for Multinomial Logit Model</p></a></li>
<li><a href='#rmnpGibbs'><p>Gibbs Sampler for Multinomial Probit</p></a></li>
<li><a href='#rmultireg'><p>Draw from the Posterior of a Multivariate Regression</p></a></li>
<li><a href='#rmvpGibbs'><p>Gibbs Sampler for Multivariate Probit</p></a></li>
<li><a href='#rmvst'><p> Draw from Multivariate Student-t</p></a></li>
<li><a href='#rnegbinRw'><p>MCMC Algorithm for Negative Binomial Regression</p></a></li>
<li><a href='#rnmixGibbs'><p>Gibbs Sampler for Normal Mixtures</p></a></li>
<li><a href='#rordprobitGibbs'><p>Gibbs Sampler for Ordered Probit</p></a></li>
<li><a href='#rscaleUsage'><p>MCMC Algorithm for Multivariate Ordinal Data with Scale Usage Heterogeneity</p></a></li>
<li><a href='#rsurGibbs'><p>Gibbs Sampler for Seemingly Unrelated Regressions (SUR)</p></a></li>
<li><a href='#rtrun'><p>Draw from Truncated Univariate Normal</p></a></li>
<li><a href='#runireg'><p>IID Sampler for Univariate Regression</p></a></li>
<li><a href='#runiregGibbs'><p>Gibbs Sampler for Univariate Regression</p></a></li>
<li><a href='#rwishart'><p> Draw from Wishart and Inverted Wishart Distribution</p></a></li>
<li><a href='#Scotch'><p>Survey Data on Brands of Scotch Consumed</p></a></li>
<li><a href='#simnhlogit'><p>Simulate from Non-homothetic Logit Model</p></a></li>
<li><a href='#summary.bayesm.mat'><p>Summarize Mcmc Parameter Draws</p></a></li>
<li><a href='#summary.bayesm.nmix'><p>Summarize Draws of Normal Mixture Components</p></a></li>
<li><a href='#summary.bayesm.var'><p>Summarize Draws of Var-Cov Matrices</p></a></li>
<li><a href='#tuna'><p>Canned Tuna Sales Data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>3.1-6</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Inference for Marketing/Micro-Econometrics</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-09-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Peter Rossi &lt;perossichi@gmail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Peter Rossi &lt;perossichi@gmail.com&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.0), utils, stats, graphics, grDevices</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Description:</td>
<td>Covers many important models used
  in marketing and micro-econometrics applications. 
  The package includes:
  Bayes Regression (univariate or multivariate dep var),
  Bayes Seemingly Unrelated Regression (SUR),
  Binary and Ordinal Probit,
  Multinomial Logit (MNL) and Multinomial Probit (MNP),
  Multivariate Probit,
  Negative Binomial (Poisson) Regression,
  Multivariate Mixtures of Normals (including clustering),
  Dirichlet Process Prior Density Estimation with normal base,
  Hierarchical Linear Models with normal prior and covariates,
  Hierarchical Linear Models with a mixture of normals prior and covariates,
  Hierarchical Multinomial Logits with a mixture of normals prior
     and covariates,
  Hierarchical Multinomial Logits with a Dirichlet Process prior and covariates,
  Hierarchical Negative Binomial Regression Models,
  Bayesian analysis of choice-based conjoint data,
  Bayesian treatment of linear instrumental variables models,
  Analysis of Multivariate Ordinal survey data with scale
     usage heterogeneity (as in Rossi et al, JASA (01)),
  Bayesian Analysis of Aggregate Random Coefficient Logit Models as in BLP (see
  Jiang, Manchanda, Rossi 2009)
  For further reference, consult our book, Bayesian Statistics and
  Marketing by Rossi, Allenby and McCulloch (Wiley first edition 2005 and second forthcoming) and Bayesian Non- and Semi-Parametric
  Methods and Applications (Princeton U Press 2014).</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-22 19:26:59 UTC; perossichi</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-23 23:20:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='bank'>Bank Card Conjoint Data</h2><span id='topic+bank'></span>

<h3>Description</h3>

<p>A panel dataset from a conjoint experiment in which two partial profiles of credit cards were presented to 946 respondents from a regional bank wanting to offer credit cards to customers outside of its normal operating region. Each respondent was presented with between 13 and 17 paired comparisons. The bank and attribute levels are disguised to protect the proprietary interests of the cooperating firm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bank)</code></pre>


<h3>Format</h3>

<p>The <code>bank</code> object is a list containing two data frames. The first, <code>choiceAtt</code>, provides choice attributes for the partial credit card profiles. The second, <code>demo</code>, provides demographic information on the respondents.
</p>


<h3>Details</h3>

<p>In the <code>choiceAtt</code> data frame:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$id           </code> </td><td style="text-align: left;"> respondent id </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$choice       </code> </td><td style="text-align: left;"> profile chosen </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Med_FInt     </code> </td><td style="text-align: left;"> medium fixed interest rate </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Low_FInt     </code> </td><td style="text-align: left;"> low fixed interest rate</td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Med_VInt     </code> </td><td style="text-align: left;"> variable interest rate</td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Rewrd_2      </code> </td><td style="text-align: left;"> reward level 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Rewrd_3      </code> </td><td style="text-align: left;"> reward level 3 </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Rewrd_4      </code> </td><td style="text-align: left;"> reward level 4 </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Med_Fee      </code> </td><td style="text-align: left;"> medium annual fee level </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Low_Fee      </code> </td><td style="text-align: left;"> low annual fee level </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Bank_B       </code> </td><td style="text-align: left;"> bank offering the credit card </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Out_State    </code> </td><td style="text-align: left;"> location of the bank offering the credit card </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Med_Rebate   </code> </td><td style="text-align: left;"> medium rebate level </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$High_Rebate  </code> </td><td style="text-align: left;"> high rebate level </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$High_CredLine</code> </td><td style="text-align: left;"> high credit line level </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Long_Grace   </code> </td><td style="text-align: left;"> grace period 
  </td>
</tr>

</table>

<p>The profiles are coded as the difference in attribute levels. Thus, that a &quot;-1&quot; means the profile coded as a choice of &quot;0&quot; has the attribute. A value of 0 means that the attribute was not present in the comparison.
</p>
<p>In the <code>demo</code> data frame:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$id    </code> </td><td style="text-align: left;"> respondent id </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$age   </code> </td><td style="text-align: left;"> respondent age in years </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$income</code> </td><td style="text-align: left;"> respondent income category </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$gender</code> </td><td style="text-align: left;"> female=1 
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Allenby, Gregg and James Ginter (1995), &quot;Using Extremes to Design Products and Segment Markets,&quot; <em>Journal of Marketing Research</em>, 392&ndash;403.</p>


<h3>References</h3>

<p>Appendix A, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)
cat(" table of Binary Dep Var", fill=TRUE)
print(table(bank$choiceAtt[,2]))
cat(" table of Attribute Variables", fill=TRUE)
mat = apply(as.matrix(bank$choiceAtt[,3:16]), 2, table)
print(mat)
cat(" means of Demographic Variables", fill=TRUE)
mat=apply(as.matrix(bank$demo[,2:3]), 2, mean)
print(mat)

## example of processing for use with rhierBinLogit
if(0) {

  choiceAtt = bank$choiceAtt
  Z = bank$demo
  
  ## center demo data so that mean of random-effects
  ## distribution can be interpreted as the average respondent
  Z[,1] = rep(1,nrow(Z))
  Z[,2] = Z[,2] - mean(Z[,2])
  Z[,3] = Z[,3] - mean(Z[,3])
  Z[,4] = Z[,4] - mean(Z[,4])
  Z = as.matrix(Z)
  
  hh = levels(factor(choiceAtt$id))
  nhh = length(hh)
  lgtdata = NULL
  for (i in 1:nhh) {
    y = choiceAtt[choiceAtt[,1]==hh[i], 2]
    nobs = length(y)
    X = as.matrix(choiceAtt[choiceAtt[,1]==hh[i], c(3:16)])
    lgtdata[[i]] = list(y=y, X=X)
  }
  cat("Finished Reading data", fill=TRUE)
  
  Data = list(lgtdata=lgtdata, Z=Z)
  Mcmc = list(R=10000, sbeta=0.2, keep=20)
  
  set.seed(66)
  out = rhierBinLogit(Data=Data, Mcmc=Mcmc)
  
  begin = 5000/20
  summary(out$Deltadraw, burnin=begin)
  summary(out$Vbetadraw, burnin=begin)
  
  ## plotting examples
  if(0){
    
    ## plot grand means of random effects distribution (first row of Delta)
    index = 4*c(0:13)+1
    matplot(out$Deltadraw[,index], type="l", xlab="Iterations/20", ylab="",
            main="Average Respondent Part-Worths")
    
    ## plot hierarchical coefs
    plot(out$betadraw)
    
    ## plot log-likelihood
    plot(out$llike, type="l", xlab="Iterations/20", ylab="", 
         main="Log Likelihood")
  }
}
</code></pre>

<hr>
<h2 id='breg'>Posterior Draws from a Univariate Regression with Unit Error Variance</h2><span id='topic+breg'></span>

<h3>Description</h3>

<p><code>breg</code> makes one draw from the posterior of a univariate regression
(scalar dependent variable) given the error variance = 1.0.
A natural conjugate (normal) prior is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>breg(y, X, betabar, A)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="breg_+3A_y">y</code></td>
<td>
 <p><code class="reqn">n x 1</code> vector of values of dep variable </p>
</td></tr>
<tr><td><code id="breg_+3A_x">X</code></td>
<td>
 <p><code class="reqn">n x k</code> design matrix</p>
</td></tr>
<tr><td><code id="breg_+3A_betabar">betabar</code></td>
<td>
 <p><code class="reqn">k x 1</code> vector for the prior mean of the regression coefficients </p>
</td></tr>
<tr><td><code id="breg_+3A_a">A</code></td>
<td>
 <p><code class="reqn">k x k</code> prior precision matrix </p>
</td></tr>
</table>


<h3>Details</h3>

<p>model: <code class="reqn">y = X'\beta + e</code> with   <code class="reqn">e</code> <code class="reqn">\sim</code> <code class="reqn">N(0,1)</code>.  <br />
prior: <code class="reqn">\beta</code> <code class="reqn">\sim</code> <code class="reqn">N(betabar, A^{-1})</code>.
</p>


<h3>Value</h3>

<p><code class="reqn">k x 1</code> vector containing a draw from the posterior distribution</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check theinput arguments for proper dimensions and type. In particular, <code>X</code> must be a matrix. If you have a vector for <code>X</code>, coerce itinto a matrix with one column.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=1000} else {R=10}

## simulate data
set.seed(66)
n = 100
X = cbind(rep(1,n), runif(n)); beta = c(1,2)
y = X %*% beta + rnorm(n)

## set prior
betabar = c(0,0)
A = diag(c(0.05, 0.05))

## make draws from posterior
betadraw = matrix(double(R*2), ncol = 2)
for (rep in 1:R) {betadraw[rep,] = breg(y,X,betabar,A)}

## summarize draws
mat = apply(betadraw, 2, quantile, probs=c(0.01, 0.05, 0.50, 0.95, 0.99))
mat = rbind(beta,mat); rownames(mat)[1] = "beta"
print(mat)
</code></pre>

<hr>
<h2 id='camera'>Conjoint Survey Data for Digital Cameras</h2><span id='topic+camera'></span>

<h3>Description</h3>

<p>Panel dataset from a conjoint survey for digital cameras with 332 respondents. Data exclude respondents that always answered none, always picked the same brand, always selected the highest priced offering, or who appeared to be answering randomly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(camera)</code></pre>


<h3>Format</h3>

<p>A list of lists. Each inner list corresponds to one survey respondent and contains a numeric vector (<code>y</code>) of choice indicators and a numeric matrix (<code>X</code>) of covariates. Each respondent participated in 16 choice scenarios each including 4 camera options (and an outside option) for a total of 80 rows per respondent.
</p>


<h3>Details</h3>

<p>The covariates included in each <code>X</code> matrix are:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$canon     </code> </td><td style="text-align: left;"> an indicator for brand Canon </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$sony      </code> </td><td style="text-align: left;"> an indicator for brand Sony </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$nikon     </code> </td><td style="text-align: left;"> an indicator for brand Nikon </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$panasonic </code> </td><td style="text-align: left;"> an indicator for brand Panasonic </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$pixels    </code> </td><td style="text-align: left;"> an indicator for a higher pixel count </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$zoom      </code> </td><td style="text-align: left;"> an indicator for a higher level of zoom </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$video     </code> </td><td style="text-align: left;"> an indicator for the ability to capture video </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$swivel    </code> </td><td style="text-align: left;"> an indicator for a swivel video display </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$wifi      </code> </td><td style="text-align: left;"> an indicator for wifi capability </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$price     </code> </td><td style="text-align: left;"> in hundreds of U.S. dollars 
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Allenby, Greg, Jeff Brazell, John Howell, and Peter Rossi (2014), &quot;Economic Valuation of Product Features,&quot; <em>Quantitative Marketing and Economics</em> 12, 421&ndash;456.
</p>
<p>Allenby, Greg, Jeff Brazell, John Howell, and Peter Rossi (2014), &quot;Valuation of Patented Product Features,&quot; <em>Journal of Law and Economics</em> 57, 629&ndash;663.
</p>


<h3>References</h3>

<p>For analysis of a similar dataset, see Case Study 4, <em>Bayesian Statistics and Marketing</em> Rossi, Allenby, and McCulloch.
</p>

<hr>
<h2 id='cgetC'>Obtain A List of Cut-offs for Scale Usage Problems</h2><span id='topic+cgetC'></span>

<h3>Description</h3>

<p><code>cgetC</code> obtains a list of censoring points, or cut-offs, used in the ordinal multivariate probit model of Rossi et al (2001). This approach uses a quadratic parameterization of the cut-offs. The model is useful for modeling correlated ordinal data on a scale from <code class="reqn">1</code> to <code class="reqn">k</code> with different scale usage patterns. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cgetC(e, k)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cgetC_+3A_e">e</code></td>
<td>
<p> quadratic parameter (<code class="reqn">0 &lt;</code> <code>e</code> <code class="reqn">&lt; 1</code>) </p>
</td></tr>
<tr><td><code id="cgetC_+3A_k">k</code></td>
<td>
<p> items are on a scale from <code class="reqn">1,\ldots, k</code> </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of <code class="reqn">k+1</code> cut-offs.</p>


<h3>Warning</h3>

<p>This is a utility function which implements <strong>no</strong> error-checking.</p>


<h3>Author(s)</h3>

<p>Rob McCulloch and Peter Rossi, Anderson School, UCLA. <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>Rossi et al (2001), &ldquo;Overcoming Scale Usage Heterogeneity,&rdquo; <em>JASA</em> 96, 20&ndash;31.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rscaleUsage">rscaleUsage</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>cgetC(0.1, 10)
</code></pre>

<hr>
<h2 id='cheese'>Sliced Cheese Data</h2><span id='topic+cheese'></span>

<h3>Description</h3>

<p>Panel data with sales volume for a package of Borden Sliced Cheese
as well as a measure of display activity and price.  Weekly data aggregated
to the &quot;key&quot; account or retailer/market level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cheese)</code></pre>


<h3>Format</h3>

<p>A data frame with 5555 observations on the following 4 variables:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$RETAILER</code> </td><td style="text-align: left;"> a list of 88 retailers </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$VOLUME  </code> </td><td style="text-align: left;"> unit sales </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$DISP    </code> </td><td style="text-align: left;"> percent ACV on display (a measure of advertising display activity) </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$PRICE   </code> </td><td style="text-align: left;"> in U.S. dollars 
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Boatwright, Peter, Robert McCulloch, and Peter Rossi (1999), &quot;Account-Level Modeling for Trade Promotion,&quot; <em>Journal of the American Statistical Association</em> 94, 1063&ndash;1073.</p>


<h3>References</h3>

<p>Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cheese)
cat(" Quantiles of the Variables ",fill=TRUE)
mat = apply(as.matrix(cheese[,2:4]), 2, quantile)
print(mat)


## example of processing for use with rhierLinearModel
if(0) {
  retailer = levels(cheese$RETAILER)
  nreg = length(retailer)
  nvar = 3
  regdata = NULL
  for (reg in 1:nreg) {
    y = log(cheese$VOLUME[cheese$RETAILER==retailer[reg]])
    iota = c(rep(1,length(y)))
    X = cbind(iota, cheese$DISP[cheese$RETAILER==retailer[reg]],
      log(cheese$PRICE[cheese$RETAILER==retailer[reg]]))
    regdata[[reg]] = list(y=y, X=X)
  }
  Z = matrix(c(rep(1,nreg)), ncol=1)
  nz = ncol(Z)
  
  
  ## run each individual regression and store results
  lscoef = matrix(double(nreg*nvar), ncol=nvar)
  for (reg in 1:nreg) {
    coef = lsfit(regdata[[reg]]$X, regdata[[reg]]$y, intercept=FALSE)$coef
    if (var(regdata[[reg]]$X[,2])==0) {
      lscoef[reg,1]=coef[1] 
      lscoef[reg,3]=coef[2]
    }
    else {lscoef[reg,]=coef}
  }
  
  R = 2000
  Data = list(regdata=regdata, Z=Z)
  Mcmc = list(R=R, keep=1)
  
  set.seed(66)
  out = rhierLinearModel(Data=Data, Mcmc=Mcmc)
  
  cat("Summary of Delta Draws", fill=TRUE)
  summary(out$Deltadraw)
  cat("Summary of Vbeta Draws", fill=TRUE)
  summary(out$Vbetadraw)
  
  # plot hier coefs
  if(0) {plot(out$betadraw)}
}
</code></pre>

<hr>
<h2 id='clusterMix'>Cluster Observations Based on Indicator MCMC Draws</h2><span id='topic+clusterMix'></span>

<h3>Description</h3>

<p><code>clusterMix</code> uses MCMC draws of indicator variables from a normal component mixture model to cluster observations based on a similarity matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusterMix(zdraw, cutoff=0.9, SILENT=FALSE, nprint=BayesmConstant.nprint)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusterMix_+3A_zdraw">zdraw</code></td>
<td>
 <p><code class="reqn">R x nobs</code> array of draws of indicators</p>
</td></tr>
<tr><td><code id="clusterMix_+3A_cutoff">cutoff</code></td>
<td>
<p> cutoff probability for similarity  (def: <code>0.9</code>)</p>
</td></tr>
<tr><td><code id="clusterMix_+3A_silent">SILENT</code></td>
<td>
<p> logical flag for silent operation (def: <code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="clusterMix_+3A_nprint">nprint</code></td>
<td>
<p> print every nprint'th draw (def: <code>100</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Define a similarity matrix, <code class="reqn">Sim</code> with <code>Sim[i,j]=1</code> if observations <code class="reqn">i</code> and <code class="reqn">j</code> are in same component. Compute the posterior mean of Sim over indicator draws.
</p>
<p>Clustering is achieved by two means:
</p>
<p>Method A:
Find the indicator draw whose similarity matrix minimizes <code class="reqn">loss(E[Sim]-Sim(z))</code>,  
where loss is absolute deviation.
</p>
<p>Method B:
Define a Similarity matrix by setting any element of <code class="reqn">E[Sim] = 1</code> if <code class="reqn">E[Sim] &gt; cutoff</code>.
Compute the clustering scheme associated with this &quot;windsorized&quot; Similarity matrix.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>clustera:</code></td>
<td>
<p>indicator function for clustering based on method A above</p>
</td></tr>
<tr><td><code>clusterb:</code></td>
<td>
<p>indicator function for clustering based on method B above</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p> For further discussion, see <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch Chapter 3.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rnmixGibbs">rnmixGibbs</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {

  ## simulate data from mixture of normals
  n = 500
  pvec = c(.5,.5)
  mu1 = c(2,2)
  mu2 = c(-2,-2)
  Sigma1 = matrix(c(1,0.5,0.5,1), ncol=2)
  Sigma2 = matrix(c(1,0.5,0.5,1), ncol=2)
  comps = NULL
  comps[[1]] = list(mu1, backsolve(chol(Sigma1),diag(2)))
  comps[[2]] = list(mu2, backsolve(chol(Sigma2),diag(2)))
  dm = rmixture(n, pvec, comps)
  
  ## run MCMC on normal mixture
  Data = list(y=dm$x)
  ncomp = 2
  Prior = list(ncomp=ncomp, a=c(rep(100,ncomp)))
  R = 2000
  Mcmc = list(R=R, keep=1)
  out = rnmixGibbs(Data=Data, Prior=Prior, Mcmc=Mcmc)
  
  ## find clusters
  begin = 500
  end = R
  outclusterMix = clusterMix(out$nmix$zdraw[begin:end,])
  
  
  ## check on clustering versus "truth"
  ## note: there could be switched labels
  table(outclusterMix$clustera, dm$z)
  table(outclusterMix$clusterb, dm$z)
}
</code></pre>

<hr>
<h2 id='condMom'>Computes Conditional Mean/Var of One Element of MVN given All Others</h2><span id='topic+condMom'></span>

<h3>Description</h3>

<p><code>condMom</code> compute moments of conditional distribution of the <code class="reqn">i</code>th element of a multivariate normal given all others.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>condMom(x, mu, sigi, i)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="condMom_+3A_x">x</code></td>
<td>
<p> vector of values to condition on; <code class="reqn">i</code>th element not used </p>
</td></tr>
<tr><td><code id="condMom_+3A_mu">mu</code></td>
<td>
<p> mean vector with <code>length(x)</code> = <code class="reqn">n</code> </p>
</td></tr>
<tr><td><code id="condMom_+3A_sigi">sigi</code></td>
<td>
<p> inverse of covariance matrix; dimension <code class="reqn">n x n</code> </p>
</td></tr>
<tr><td><code id="condMom_+3A_i">i</code></td>
<td>
<p> conditional distribution of <code class="reqn">i</code>th element </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">x</code> <code class="reqn">\sim</code> <code class="reqn">MVN(mu, sigi^{-1})</code>.
</p>
<p><code>condMom</code> computes moments of <code class="reqn">x_i</code> given <code class="reqn">x_{-i}</code>.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>cmean</code></td>
<td>
<p> conditional mean </p>
</td></tr>
<tr><td><code>cvar</code></td>
<td>
<p> conditional variance</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p> For further discussion, see <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>Examples</h3>

<pre><code class='language-R'>sig  = matrix(c(1, 0.5, 0.5, 0.5, 1, 0.5, 0.5, 0.5, 1), ncol=3)
sigi = chol2inv(chol(sig))
mu   = c(1,2,3)
x    = c(1,1,1)

condMom(x, mu, sigi, 2)
</code></pre>

<hr>
<h2 id='createX'>Create X Matrix for Use in Multinomial Logit and Probit Routines</h2><span id='topic+createX'></span>

<h3>Description</h3>

<p><code>createX</code> makes up an X matrix in the form expected by Multinomial
Logit (<code><a href="#topic+rmnlIndepMetrop">rmnlIndepMetrop</a></code> and <code><a href="#topic+rhierMnlRwMixture">rhierMnlRwMixture</a></code>)
and Probit (<code><a href="#topic+rmnpGibbs">rmnpGibbs</a></code> and <code><a href="#topic+rmvpGibbs">rmvpGibbs</a></code>) routines.  
Requires an array of alternative-specific variables and/or an
array of &quot;demographics&quot; (or variables constant across alternatives) which
may vary across choice occasions. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createX(p, na, nd, Xa, Xd, INT = TRUE, DIFF = FALSE, base=p)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createX_+3A_p">p</code></td>
<td>
<p> integer number of choice alternatives </p>
</td></tr>
<tr><td><code id="createX_+3A_na">na</code></td>
<td>
<p> integer number of alternative-specific vars in <code>Xa</code>  </p>
</td></tr>
<tr><td><code id="createX_+3A_nd">nd</code></td>
<td>
<p> integer number of non-alternative specific vars </p>
</td></tr>
<tr><td><code id="createX_+3A_xa">Xa</code></td>
<td>
 <p><code class="reqn">n x p*na</code> matrix of alternative-specific vars </p>
</td></tr>
<tr><td><code id="createX_+3A_xd">Xd</code></td>
<td>
 <p><code class="reqn">n x nd</code> matrix of non-alternative specific vars </p>
</td></tr>
<tr><td><code id="createX_+3A_int">INT</code></td>
<td>
<p> logical flag for inclusion of intercepts </p>
</td></tr>
<tr><td><code id="createX_+3A_diff">DIFF</code></td>
<td>
<p> logical flag for differencing wrt to base alternative </p>
</td></tr>
<tr><td><code id="createX_+3A_base">base</code></td>
<td>
<p> integer index of base choice alternative </p>
</td></tr>
</table>
<p>Note: <code>na</code>, <code>nd</code>, <code>Xa</code>, <code>Xd</code> can be <code>NULL</code> to indicate lack of <code>Xa</code> or <code>Xd</code> variables.
</p>


<h3>Value</h3>

<p><code>X</code> matrix of dimension <code class="reqn">n*(p-DIFF) x [(INT+nd)*(p-1) + na]</code>.</p>


<h3>Note</h3>

<p><code><a href="#topic+rmnpGibbs">rmnpGibbs</a></code> assumes that the <code>base</code> alternative is the default.</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p> For further discussion, see <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

<p><code><a href="#topic+rmnlIndepMetrop">rmnlIndepMetrop</a></code>, <code><a href="#topic+rmnpGibbs">rmnpGibbs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>na=2; nd=1; p=3
vec = c(1, 1.5, 0.5, 2, 3, 1, 3, 4.5, 1.5)
Xa = matrix(vec, byrow=TRUE, ncol=3)
Xa = cbind(Xa,-Xa)
Xd = matrix(c(-1,-2,-3), ncol=1)
createX(p=p, na=na, nd=nd, Xa=Xa, Xd=Xd)
createX(p=p, na=na, nd=nd, Xa=Xa, Xd=Xd, base=1)
createX(p=p, na=na, nd=nd, Xa=Xa, Xd=Xd, DIFF=TRUE)
createX(p=p, na=na, nd=nd, Xa=Xa, Xd=Xd, DIFF=TRUE, base=2)
createX(p=p, na=na, nd=NULL, Xa=Xa, Xd=NULL)
createX(p=p, na=NULL, nd=nd, Xa=NULL, Xd=Xd)
</code></pre>

<hr>
<h2 id='customerSat'>Customer Satisfaction Data</h2><span id='topic+customerSat'></span>

<h3>Description</h3>

<p>Responses to a satisfaction survey for a Yellow Pages advertising product. All responses are on a 10 point scale from 1 to 10 (1 is &quot;Poor&quot; and 10 is &quot;Excellent&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(customerSat)</code></pre>


<h3>Format</h3>

<p>A data frame with 1811 observations on the following 10 variables:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$q1 </code> </td><td style="text-align: left;"> Overall Satisfaction </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$q2 </code> </td><td style="text-align: left;"> Setting Competitive Prices </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$q3 </code> </td><td style="text-align: left;"> Holding Price Increase to a Minimum </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$q4 </code> </td><td style="text-align: left;"> Appropriate Pricing given Volume </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$q5 </code> </td><td style="text-align: left;"> Demonstrating Effectiveness of Purchase </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$q6 </code> </td><td style="text-align: left;"> Reach a Large Number of Customers </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$q7 </code> </td><td style="text-align: left;"> Reach of Advertising </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$q8 </code> </td><td style="text-align: left;"> Long-term Exposure </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$q9 </code> </td><td style="text-align: left;"> Distribution </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$q10</code> </td><td style="text-align: left;"> Distribution to Right Geographic Areas 
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Rossi, Peter, Zvi Gilula, and Greg Allenby (2001), &quot;Overcoming Scale Usage Heterogeneity,&quot; <em>Journal of the American Statistical Association</em> 96, 20&ndash;31.</p>


<h3>References</h3>

<p>Case Study 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(customerSat)
apply(as.matrix(customerSat),2,table)
## see also examples for 'rscaleUsage'
</code></pre>

<hr>
<h2 id='detailing'>Physician Detailing Data</h2><span id='topic+detailing'></span>

<h3>Description</h3>

<p>Monthly data on physician detailing (sales calls). 23 months of data for each of 1000 physicians; includes physician covariates. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(detailing)</code></pre>


<h3>Format</h3>

<p>The <code>detailing</code> object is a list containing two data frames, <code>counts</code> and <code>demo</code>.
</p>


<h3>Details</h3>

<p>In the <code>counts</code> data frame:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$id            </code> </td><td style="text-align: left;"> identifies the physician </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$scrips        </code> </td><td style="text-align: left;"> the number of new presectiptions ordered by the physician for the drug detailed </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$detailing     </code> </td><td style="text-align: left;"> the number of sales called made to each physician per month </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$lagged_scripts</code> </td><td style="text-align: left;"> scrips value for prior month 
  </td>
</tr>

</table>

<p>In the <code>demo</code> data frame:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$$id         </code> </td><td style="text-align: left;"> identifies the physician </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$generalphys </code> </td><td style="text-align: left;"> dummy for if doctor is a "general practitioner" </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$specialist  </code> </td><td style="text-align: left;"> dummy for if the physician is a specialist in the theraputic class for which the drug is intended </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$mean_samples</code> </td><td style="text-align: left;"> the mean number of free drug samples given the doctor over the sample period 
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Manchanda, Puneet, Pradeep Chintagunta, and Peter Rossi (2004), &quot;Response Modeling with Non-Random Marketing Mix Variables,&quot; <em>Journal of Marketing Research</em> 41, 467&ndash;478.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(detailing)

cat(" table of Counts Dep Var", fill=TRUE)
print(table(detailing$counts[,2]))

cat(" means of Demographic Variables",fill=TRUE)
mat = apply(as.matrix(detailing$demo[,2:4]), 2, mean)
print(mat)


## example of processing for use with 'rhierNegbinRw'
if(0) {
  data(detailing)
  counts = detailing$counts
  Z = detailing$demo
  
  # Construct the Z matrix
  Z[,1] = 1
  Z[,2] = Z[,2] - mean(Z[,2])
  Z[,3] = Z[,3] - mean(Z[,3])
  Z[,4] = Z[,4] - mean(Z[,4])
  Z = as.matrix(Z)
  id = levels(factor(counts$id))
  nreg = length(id)
  nobs = nrow(counts$id)
  
  regdata = NULL
  for (i in 1:nreg) {
    X = counts[counts[,1] == id[i], c(3:4)]
    X = cbind(rep(1, nrow(X)), X)
    y = counts[counts[,1] == id[i], 2]
    X = as.matrix(X)
    regdata[[i]] = list(X=X, y=y)
  }
  rm(detailing, counts)              
  cat("Finished reading data", fill=TRUE)
  fsh()
  Data = list(regdata=regdata, Z=Z)
  
  nvar = ncol(X)            # Number of X variables
  nz = ncol(Z)              # Number of Z variables
  deltabar = matrix(rep(0,nvar*nz), nrow=nz)
  Vdelta = 0.01*diag(nz)
  nu = nvar+3
  V = 0.01*diag(nvar)
  a = 0.5
  b = 0.1
  Prior = list(deltabar=deltabar, Vdelta=Vdelta, nu=nu, V=V, a=a, b=b)
  
  R = 10000
  keep = 1
  s_beta = 2.93/sqrt(nvar)
  s_alpha = 2.93
  c = 2
  Mcmc = list(R=R, keep=keep, s_beta=s_beta, s_alpha=s_alpha, c=c)
  
  out = rhierNegbinRw(Data, Prior, Mcmc)
  
  ## Unit level mean beta parameters
  Mbeta = matrix(rep(0,nreg*nvar), nrow=nreg)
  ndraws = length(out$alphadraw)
  for (i in 1:nreg) { Mbeta[i,] = rowSums(out$Betadraw[i,,])/ndraws }
  
  cat(" Deltadraws ", fill=TRUE)
  summary(out$Deltadraw)
  cat(" Vbetadraws ", fill=TRUE)
  summary(out$Vbetadraw)
  cat(" alphadraws ", fill=TRUE)
  summary(out$alphadraw)
  
  ## plotting examples
  if(0){
    plot(out$betadraw)
    plot(out$alphadraw)
    plot(out$Deltadraw)
  }
}

</code></pre>

<hr>
<h2 id='eMixMargDen'>Compute Marginal Densities of A Normal Mixture Averaged over MCMC Draws</h2><span id='topic+eMixMargDen'></span>

<h3>Description</h3>

<p><code>eMixMargDen</code> assumes that a multivariate mixture of normals has been fitted
via MCMC (using <code>rnmixGibbs</code>). For each MCMC draw, <code>eMixMargDen</code> computes 
the marginal densities for each component in the multivariate mixture on a user-supplied
grid and then averages over the MCMC draws. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eMixMargDen(grid, probdraw, compdraw)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eMixMargDen_+3A_grid">grid</code></td>
<td>
<p> array of grid points, <code>grid[,i]</code> are ordinates for <code>i</code>th dimension of the density </p>
</td></tr>
<tr><td><code id="eMixMargDen_+3A_probdraw">probdraw</code></td>
<td>
<p> array where each row contains a draw of probabilities of the mixture component </p>
</td></tr>
<tr><td><code id="eMixMargDen_+3A_compdraw">compdraw</code></td>
<td>
<p> list of lists of draws of mixture component moments </p>
</td></tr>
</table>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
    <code>length(compdraw)        </code>  </td><td style="text-align: left;"> is the number of MCMC draws. </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>compdraw[[i]]           </code>  </td><td style="text-align: left;"> is a list draws of mu and of the inverse Cholesky root for each of mixture components. </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>compdraw[[i]][[j]]      </code>  </td><td style="text-align: left;"> is <code>j</code>th component. </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>compdraw[[i]][[j]]$mu   </code>  </td><td style="text-align: left;"> is mean vector. </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>compdraw[[i]][[j]]$rooti</code> </td><td style="text-align: left;"> is the UL decomp of <code class="reqn">\Sigma^{-1}</code>.
  </td>
</tr>

</table>



<h3>Value</h3>

<p>An array of the same dimension as <code>grid</code> with density values.</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type. To avoid errors, call with output from <code><a href="#topic+rnmixGibbs">rnmixGibbs</a></code>.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p> For further discussion, see <em>Bayesian Statistics and Marketing</em>by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

<p><code><a href="#topic+rnmixGibbs">rnmixGibbs</a></code></p>

<hr>
<h2 id='ghkvec'>Compute GHK approximation to Multivariate Normal Integrals</h2><span id='topic+ghkvec'></span>

<h3>Description</h3>

<p><code>ghkvec</code> computes the GHK approximation to the integral of a multivariate normal density over a half plane defined by a set of truncation points.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ghkvec(L, trunpt, above, r, HALTON=TRUE, pn)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ghkvec_+3A_l">L</code></td>
<td>
<p> lower triangular Cholesky root of covariance matrix </p>
</td></tr>
<tr><td><code id="ghkvec_+3A_trunpt">trunpt</code></td>
<td>
<p> vector of truncation points</p>
</td></tr>
<tr><td><code id="ghkvec_+3A_above">above</code></td>
<td>
<p> vector of indicators for truncation above(1) or below(0) on an element by element basis </p>
</td></tr>
<tr><td><code id="ghkvec_+3A_r">r</code></td>
<td>
<p> number of draws to use in GHK </p>
</td></tr>
<tr><td><code id="ghkvec_+3A_halton">HALTON</code></td>
<td>
<p> if <code>TRUE</code>, uses Halton sequence. If <code>FALSE</code>, uses <code>R::runif</code> random number generator (def: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="ghkvec_+3A_pn">pn</code></td>
<td>
<p> prime number used for Halton sequence (def: the smallest prime numbers, i.e. 2, 3, 5, ...)</p>
</td></tr>  
</table>


<h3>Value</h3>

<p>Approximation to integral</p>


<h3>Note</h3>

<p><code>ghkvec</code> can accept a vector of truncations and compute more than one integral. That is, <code>length(trunpt)/length(above)</code> number of different integrals, each with the same variance and mean 0 but different truncation points. See 'examples' below for an example with two integrals at different truncation points. The <code>above</code>
argument specifies truncation from above (1) or below (0) on an element by element basis. Only one vector of above is allowed but multiple truncation points are allowed. 
</p>
<p>The user can choose between two random number generators for the numerical integration: psuedo-random numbers by <code>R::runif</code> or quasi-random numbers by a Halton sequence. Generally, the quasi-random (Halton) sequence is more uniformly distributed within domain, so it shows lower error and improved convergence than the psuedo-random (<code>runif</code>) sequence (Morokoff and Caflisch, 1995). 
</p>
<p>For the prime numbers generating Halton sequence, we suggest to use the first smallest prime numbers. Halton (1960) and Kocis and Whiten (1997) prove that their discrepancy measures (how uniformly the sample points are distributed) have the upper bounds, which decrease as the generating prime number decreases. 
</p>
<p>Note: For a high dimensional integration (10 or more dimension), we suggest use of the psuedo-random number generator (<code>R::runif</code>) because, according to Kocis and Whiten (1997), Halton sequences may be highly correlated when the dimension is 10 or more.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.<br />
Keunwoo Kim, Anderson School, UCLA, <a href="mailto:keunwoo.kim@gmail.com">keunwoo.kim@gmail.com</a>.
</p>


<h3>References</h3>

 
<p>For further discussion, see <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch,  Chapter 2.
</p>
<p>For Halton sequence, see Halton (1960, Numerische Mathematik), Morokoff and Caflisch (1995, Journal of Computational Physics), and Kocis and Whiten (1997, ACM Transactions on Mathematical Software).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Sigma = matrix(c(1, 0.5, 0.5, 1), ncol=2)
L = t(chol(Sigma))
trunpt = c(0,0,1,1)
above = c(1,1) 
# here we have a two dimensional integral with two different truncation points
#    (0,0) and (1,1)
# however, there is only one vector of "above" indicators for each integral
#     above=c(1,1) is applied to both integrals.  

# drawn by Halton sequence
ghkvec(L, trunpt, above, r=100)

# use prime number 11 and 13
ghkvec(L, trunpt, above, r=100, HALTON=TRUE, pn=c(11,13))

# drawn by R::runif
ghkvec(L, trunpt, above, r=100, HALTON=FALSE)
</code></pre>

<hr>
<h2 id='llmnl'>Evaluate Log Likelihood for Multinomial Logit Model</h2><span id='topic+llmnl'></span>

<h3>Description</h3>

<p><code>llmnl</code> evaluates log-likelihood for the multinomial logit model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>llmnl(beta, y, X)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="llmnl_+3A_beta">beta</code></td>
<td>
 <p><code class="reqn">k x 1</code> coefficient vector </p>
</td></tr>
<tr><td><code id="llmnl_+3A_y">y</code></td>
<td>
 <p><code class="reqn">n x 1</code> vector of obs on y (1,..., p) </p>
</td></tr>
<tr><td><code id="llmnl_+3A_x">X</code></td>
<td>
 <p><code class="reqn">n*p x k</code> design matrix (use <code>createX</code> to create <code class="reqn">X</code>) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">\mu_i = X_i beta</code>, then <code class="reqn">Pr(y_i=j) = exp(\mu_{i,j}) / \sum_k exp(\mu_{i,k})</code>.<br />
<code class="reqn">X_i</code> is the submatrix of <code class="reqn">X</code> corresponding to the
<code class="reqn">i</code>th observation.  <code class="reqn">X</code> has <code class="reqn">n*p</code> rows.  
</p>
<p>Use <code><a href="#topic+createX">createX</a></code> to create <code class="reqn">X</code>. 
</p>


<h3>Value</h3>

<p>Value of log-likelihood (sum of log prob of observed multinomial outcomes).</p>


<h3>Warning</h3>

<p> This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+createX">createX</a></code>, <code><a href="#topic+rmnlIndepMetrop">rmnlIndepMetrop</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: ll=llmnl(beta,y,X)
</code></pre>

<hr>
<h2 id='llmnp'>Evaluate Log Likelihood for Multinomial Probit Model</h2><span id='topic+llmnp'></span>

<h3>Description</h3>

<p><code>llmnp</code> evaluates the log-likelihood for the multinomial probit model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llmnp(beta, Sigma, X, y, r)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="llmnp_+3A_beta">beta</code></td>
<td>
<p> k x 1 vector of coefficients </p>
</td></tr>
<tr><td><code id="llmnp_+3A_sigma">Sigma</code></td>
<td>
<p> (p-1) x (p-1) covariance matrix of errors </p>
</td></tr>
<tr><td><code id="llmnp_+3A_x">X</code></td>
<td>
<p> n*(p-1) x k array where X is from differenced system </p>
</td></tr>
<tr><td><code id="llmnp_+3A_y">y</code></td>
<td>
<p> vector of n indicators of multinomial response (1, ..., p) </p>
</td></tr>
<tr><td><code id="llmnp_+3A_r">r</code></td>
<td>
<p> number of draws used in GHK </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">X</code> is <code class="reqn">(p-1)*n x k</code> matrix.  Use <code><a href="#topic+createX">createX</a></code> with <code>DIFF=TRUE</code> to create <code class="reqn">X</code>. <br />
</p>
<p>Model for each obs:  <code class="reqn">w = Xbeta + e</code> with <code class="reqn">e</code> <code class="reqn">\sim</code> <code class="reqn">N(0,Sigma)</code>. 
</p>
<p>Censoring mechanism: <br />
if <code class="reqn">y=j (j&lt;p),  w_j &gt; max(w_{-j})</code> and <code class="reqn">w_j &gt;0</code>    <br />
if <code class="reqn">y=p,  w &lt; 0</code>                   <br />
</p>
<p>To use GHK, we must transform so that these are rectangular regions
e.g. if <code class="reqn">y=1,  w_1 &gt; 0</code> and <code class="reqn">w_1 - w_{-1} &gt; 0</code>. 
</p>
<p>Define <code class="reqn">A_j</code> such that if <code class="reqn">j=1,\ldots,p-1</code> then <code class="reqn">A_jw = A_jmu + A_je &gt; 0</code> is equivalent to <code class="reqn">y=j</code>. Thus, if <code class="reqn">y=j</code>, we have <code class="reqn">A_je &gt; -A_jmu</code>.  Lower truncation is <code class="reqn">-A_jmu</code> and <code class="reqn">cov = A_jSigmat(A_j)</code>. For <code class="reqn">j=p</code>, <code class="reqn">e &lt; - mu</code>.
</p>


<h3>Value</h3>

<p>Value of log-likelihood (sum of log prob of observed multinomial outcomes)</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p> Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p> For further discussion, see Chapters 2 and 4, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+createX">createX</a></code>, <code><a href="#topic+rmnpGibbs">rmnpGibbs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: ll=llmnp(beta,Sigma,X,y,r)
</code></pre>

<hr>
<h2 id='llnhlogit'>Evaluate Log Likelihood for non-homothetic Logit Model</h2><span id='topic+llnhlogit'></span>

<h3>Description</h3>

<p><code>llnhlogit</code> evaluates log-likelihood for the Non-homothetic Logit model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llnhlogit(theta, choice, lnprices, Xexpend)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="llnhlogit_+3A_theta">theta</code></td>
<td>
<p> parameter vector (see details section) </p>
</td></tr>
<tr><td><code id="llnhlogit_+3A_choice">choice</code></td>
<td>
 <p><code class="reqn">n x 1</code> vector of choice (1,...,p) </p>
</td></tr>
<tr><td><code id="llnhlogit_+3A_lnprices">lnprices</code></td>
<td>
 <p><code class="reqn">n x p</code>  array of log-prices</p>
</td></tr>
<tr><td><code id="llnhlogit_+3A_xexpend">Xexpend</code></td>
<td>
 <p><code class="reqn">n x d</code> array of vars predicting expenditure </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Non-homothetic logit model, <code class="reqn">Pr(i) = exp(tau v_i) / sum_j exp(tau v_j)</code> <br />
</p>
<p><code class="reqn">v_i = alpha_i - e^{kappaStar_i}u^i - lnp_i</code> <br />
tau is the scale parameter of extreme value error distribution.<br />
<code class="reqn">u^i</code> solves <code class="reqn">u^i = psi_i(u^i)E/p_i</code>.<br />
<code class="reqn">ln(psi_i(U)) = alpha_i - e^{kappaStar_i}U</code>. <br />
<code class="reqn">ln(E) = gamma'Xexpend</code>.<br />
</p>
<p>Structure of theta vector: <br />
alpha: <code class="reqn">p x 1</code> vector of utility intercepts.<br />
kappaStar: <code class="reqn">p x 1</code> vector of utility rotation parms expressed on natural log scale. <br />
gamma: <code class="reqn">k x 1</code> &ndash; expenditure variable coefs.<br />
tau: <code class="reqn">1 x 1</code> &ndash; logit scale parameter.<br />
</p>


<h3>Value</h3>

<p>Value of log-likelihood (sum of log prob of observed multinomial outcomes).</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 4, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

<p><code><a href="#topic+simnhlogit">simnhlogit</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>N=1000; p=3; k=1
theta = c(rep(1,p), seq(from=-1,to=1,length=p), rep(2,k), 0.5)
lnprices = matrix(runif(N*p), ncol=p)
Xexpend = matrix(runif(N*k), ncol=k)
simdata = simnhlogit(theta, lnprices, Xexpend)

## evaluate likelihood at true theta
llstar = llnhlogit(theta, simdata$y, simdata$lnprices, simdata$Xexpend)
</code></pre>

<hr>
<h2 id='lndIChisq'>Compute Log of Inverted Chi-Squared Density</h2><span id='topic+lndIChisq'></span>

<h3>Description</h3>

<p><code>lndIChisq</code> computes the log of an Inverted Chi-Squared Density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lndIChisq(nu, ssq, X)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lndIChisq_+3A_nu">nu</code></td>
<td>
<p> d.f. parameter </p>
</td></tr>
<tr><td><code id="lndIChisq_+3A_ssq">ssq</code></td>
<td>
<p> scale parameter </p>
</td></tr>
<tr><td><code id="lndIChisq_+3A_x">X</code></td>
<td>
<p> ordinate for density evaluation (this must be a matrix)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">Z = nu*ssq / \chi^2_{nu}</code> with <code class="reqn">Z</code> <code class="reqn">\sim</code> Inverted Chi-Squared.  <br />
<code>lndIChisq</code> computes the complete log-density, including normalizing constants.
</p>


<h3>Value</h3>

<p>Log density value</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 2, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="stats.html#topic+dchisq">dchisq</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>lndIChisq(3, 1, matrix(2))
</code></pre>

<hr>
<h2 id='lndIWishart'>Compute Log of Inverted Wishart Density</h2><span id='topic+lndIWishart'></span>

<h3>Description</h3>

<p><code>lndIWishart</code> computes the log of an Inverted Wishart density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lndIWishart(nu, V, IW)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lndIWishart_+3A_nu">nu</code></td>
<td>
<p> d.f. parameter </p>
</td></tr>
<tr><td><code id="lndIWishart_+3A_v">V</code></td>
<td>
<p> &quot;location&quot; parameter  </p>
</td></tr>
<tr><td><code id="lndIWishart_+3A_iw">IW</code></td>
<td>
<p> ordinate for density evaluation </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">Z</code> <code class="reqn">\sim</code> Inverted Wishart(nu,V). <br />
In this parameterization, <code class="reqn">E[Z] = 1/(nu-k-1) V</code>, where <code class="reqn">V</code> is a <code class="reqn">k x k</code> matrix <br />
<code>lndIWishart</code> computes the complete log-density, including normalizing constants.
</p>


<h3>Value</h3>

<p>Log density value</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 2, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+rwishart">rwishart</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>lndIWishart(5, diag(3), diag(3)+0.5)
</code></pre>

<hr>
<h2 id='lndMvn'> Compute Log of Multivariate Normal Density </h2><span id='topic+lndMvn'></span>

<h3>Description</h3>

<p><code>lndMvn</code> computes the log of a Multivariate Normal Density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lndMvn(x, mu, rooti)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lndMvn_+3A_x">x</code></td>
<td>
<p> density ordinate </p>
</td></tr>
<tr><td><code id="lndMvn_+3A_mu">mu</code></td>
<td>
<p> mu vector </p>
</td></tr>
<tr><td><code id="lndMvn_+3A_rooti">rooti</code></td>
<td>
<p> inv of upper triangular Cholesky root of <code class="reqn">\Sigma</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">z</code> <code class="reqn">\sim</code> <code class="reqn">N(mu,\Sigma)</code>
</p>


<h3>Value</h3>

<p>Log density value</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 2, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+lndMvst">lndMvst</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>Sigma = matrix(c(1, 0.5, 0.5, 1), ncol=2)
lndMvn(x=c(rep(0,2)), mu=c(rep(0,2)), rooti=backsolve(chol(Sigma),diag(2)))
</code></pre>

<hr>
<h2 id='lndMvst'>Compute Log of Multivariate Student-t Density</h2><span id='topic+lndMvst'></span>

<h3>Description</h3>

<p><code>lndMvst</code> computes the log of a Multivariate Student-t Density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lndMvst(x, nu, mu, rooti, NORMC)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lndMvst_+3A_x">x</code></td>
<td>
<p> density ordinate </p>
</td></tr>
<tr><td><code id="lndMvst_+3A_nu">nu</code></td>
<td>
<p> d.f. parameter </p>
</td></tr>
<tr><td><code id="lndMvst_+3A_mu">mu</code></td>
<td>
<p> mu vector </p>
</td></tr>
<tr><td><code id="lndMvst_+3A_rooti">rooti</code></td>
<td>
<p> inv of Cholesky root of <code class="reqn">\Sigma</code> </p>
</td></tr>
<tr><td><code id="lndMvst_+3A_normc">NORMC</code></td>
<td>
<p> include normalizing constant (def: <code>FALSE</code>) </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">z</code> <code class="reqn">\sim</code> <code class="reqn">MVst(mu, nu, \Sigma)</code>
</p>


<h3>Value</h3>

<p>Log density value</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 2, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+lndMvn">lndMvn</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>Sigma = matrix(c(1, 0.5, 0.5, 1), ncol=2)
lndMvst(x=c(rep(0,2)), nu=4,mu=c(rep(0,2)), rooti=backsolve(chol(Sigma),diag(2)))
</code></pre>

<hr>
<h2 id='logMargDenNR'>Compute Log Marginal Density Using Newton-Raftery Approx</h2><span id='topic+logMargDenNR'></span>

<h3>Description</h3>

<p><code>logMargDenNR</code> computes log marginal density using the Newton-Raftery approximation. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logMargDenNR(ll)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logMargDenNR_+3A_ll">ll</code></td>
<td>
<p> vector of log-likelihoods evaluated at <code>length(ll)</code> MCMC draws </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Approximation to log marginal density value.</p>


<h3>Warning</h3>

<p>This approximation can be influenced by outliers in the vector of log-likelihoods; use with <strong>care</strong>.  This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p> Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 6, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>

<hr>
<h2 id='margarine'>Household Panel Data on Margarine Purchases</h2><span id='topic+margarine'></span>

<h3>Description</h3>

<p>Panel data on purchases of margarine by 516 households. Demographic variables are included.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(margarine)</code></pre>


<h3>Format</h3>

<p>The <code>detailing</code> object is a list containing two data frames, <code>choicePrice</code> and <code>demos</code>.
</p>


<h3>Details</h3>

<p>In the <code>choicePrice</code> data frame:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$hhid  </code> </td><td style="text-align: left;"> household ID </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$choice</code> </td><td style="text-align: left;"> multinomial indicator of one of the 10 products 
  </td>
</tr>

</table>

<p>The products are indicated by brand and type. 
</p>
<p>Brands:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$Pk  </code> </td><td style="text-align: left;"> Parkay </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$BB  </code> </td><td style="text-align: left;"> BlueBonnett </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Fl  </code> </td><td style="text-align: left;"> Fleischmanns </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Hse </code> </td><td style="text-align: left;"> house </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Gen </code> </td><td style="text-align: left;"> generic </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Imp </code> </td><td style="text-align: left;"> Imperial </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$SS  </code> </td><td style="text-align: left;"> Shed Spread 
  </td>
</tr>

</table>

<p>Product type:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$_Stk</code> </td><td style="text-align: left;"> stick </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$_Tub</code> </td><td style="text-align: left;"> tub 
  </td>
</tr>

</table>
  
<p>In the <code>demos</code> data frame:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$Fs3_4    </code> </td><td style="text-align: left;"> dummy for family size 3-4 </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$Fs5      </code> </td><td style="text-align: left;"> dummy for family size &gt;= 5 </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$college  </code> </td><td style="text-align: left;"> dummy for education status </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$whtcollar</code> </td><td style="text-align: left;"> dummy for job status </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$retired  </code> </td><td style="text-align: left;"> dummy for retirement status 
  </td>
</tr>

</table>

<p>All prices are in U.S. dollars.
</p>


<h3>Source</h3>

<p>Allenby, Greg and Peter Rossi (1991), &quot;Quality Perceptions and Asymmetric Switching Between Brands,&quot; <em>Marketing Science</em> 10, 185&ndash;205.
</p>


<h3>References</h3>

<p>Chapter 5, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(margarine)
cat(" Table of Choice Variable ", fill=TRUE)
print(table(margarine$choicePrice[,2]))

cat(" Means of Prices", fill=TRUE)
mat=apply(as.matrix(margarine$choicePrice[,3:12]), 2, mean)
print(mat)

cat(" Quantiles of Demographic Variables", fill=TRUE)
mat=apply(as.matrix(margarine$demos[,2:8]), 2, quantile)
print(mat)


## example of processing for use with 'rhierMnlRwMixture'
if(0) {
  select = c(1:5,7)  ## select brands
  chPr = as.matrix(margarine$choicePrice)
  
  ## make sure to log prices
  chPr = cbind(chPr[,1], chPr[,2], log(chPr[,2+select]))
  demos = as.matrix(margarine$demos[,c(1,2,5)])
  
  ## remove obs for other alts
  chPr = chPr[chPr[,2] &lt;= 7,]
  chPr = chPr[chPr[,2] != 6,]
  
  ## recode choice
  chPr[chPr[,2] == 7,2] = 6
  
  hhidl = levels(as.factor(chPr[,1]))
  lgtdata = NULL
  nlgt = length(hhidl)
  p = length(select)  ## number of choice alts
  
  ind = 1
  for (i in 1:nlgt) {
    nobs = sum(chPr[,1]==hhidl[i])
    if(nobs &gt;=5) {
      data = chPr[chPr[,1]==hhidl[i],]
      y = data[,2]
      names(y) = NULL
      X = createX(p=p, na=1, Xa=data[,3:8], nd=NULL, Xd=NULL, INT=TRUE, base=1)
      lgtdata[[ind]] = list(y=y, X=X, hhid=hhidl[i])
      ind = ind+1
    }
  }
  nlgt = length(lgtdata)
  
  ## now extract demos corresponding to hhs in lgtdata
  Z = NULL
  nlgt = length(lgtdata)
  for(i in 1:nlgt){
     Z = rbind(Z, demos[demos[,1]==lgtdata[[i]]$hhid, 2:3])
  }
  
  ## take log of income and family size and demean
  Z = log(Z)
  Z[,1] = Z[,1] - mean(Z[,1])
  Z[,2] = Z[,2] - mean(Z[,2])
  
  keep = 5
  R = 20000
  mcmc1 = list(keep=keep, R=R)
  
  out = rhierMnlRwMixture(Data=list(p=p,lgtdata=lgtdata, Z=Z),
                          Prior=list(ncomp=1), Mcmc=mcmc1)
  
  summary(out$Deltadraw)
  summary(out$nmix)
  
  ## plotting examples
  if(0){
    plot(out$nmix)
    plot(out$Deltadraw)
  }
}
</code></pre>

<hr>
<h2 id='mixDen'>Compute Marginal Density for Multivariate Normal Mixture</h2><span id='topic+mixDen'></span>

<h3>Description</h3>

<p><code>mixDen</code> computes the marginal density for each dimension of a normal mixture at each of the points on a user-specifed grid.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixDen(x, pvec, comps)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixDen_+3A_x">x</code></td>
<td>
<p> array where <code class="reqn">i</code>th column gives grid points for <code class="reqn">i</code>th variable </p>
</td></tr>
<tr><td><code id="mixDen_+3A_pvec">pvec</code></td>
<td>
<p> vector of mixture component probabilites </p>
</td></tr>
<tr><td><code id="mixDen_+3A_comps">comps</code></td>
<td>
<p> list of lists of components for normal mixture </p>
</td></tr>
</table>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
    <code>length(comps)   </code> </td><td style="text-align: left;"> is the number of mixture components </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>comps[[j]]      </code> </td><td style="text-align: left;"> is a list of parameters of the <code class="reqn">j</code>th component </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>comps[[j]]$mu   </code> </td><td style="text-align: left;"> is mean vector </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>comps[[j]]$rooti</code> </td><td style="text-align: left;"> is the UL decomp of <code class="reqn">\Sigma^{-1}</code>
  </td>
</tr>

</table>



<h3>Value</h3>

<p>An array of the same dimension as grid with density values</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+rnmixGibbs">rnmixGibbs</a></code> </p>

<hr>
<h2 id='mixDenBi'>Compute Bivariate Marginal Density for a Normal Mixture</h2><span id='topic+mixDenBi'></span>

<h3>Description</h3>

<p><code>mixDenBi</code> computes the implied bivariate marginal density from a mixture of normals with specified mixture probabilities and component parameters. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixDenBi(i, j, xi, xj, pvec, comps)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixDenBi_+3A_i">i</code></td>
<td>
<p> index of first variable </p>
</td></tr>
<tr><td><code id="mixDenBi_+3A_j">j</code></td>
<td>
<p> index of second variable </p>
</td></tr>
<tr><td><code id="mixDenBi_+3A_xi">xi</code></td>
<td>
<p> grid of values of first variable </p>
</td></tr>
<tr><td><code id="mixDenBi_+3A_xj">xj</code></td>
<td>
<p> grid of values of second variable </p>
</td></tr>
<tr><td><code id="mixDenBi_+3A_pvec">pvec</code></td>
<td>
<p> normal mixture probabilities </p>
</td></tr>
<tr><td><code id="mixDenBi_+3A_comps">comps</code></td>
<td>
<p> list of lists of components </p>
</td></tr>
</table>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
    <code>length(comps)   </code> </td><td style="text-align: left;"> is the number of mixture components </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>comps[[j]]      </code> </td><td style="text-align: left;"> is a list of parameters of the <code class="reqn">j</code>th component </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>comps[[j]]$mu   </code> </td><td style="text-align: left;"> is mean vector </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>comps[[j]]$rooti</code> </td><td style="text-align: left;"> is the UL decomp of <code class="reqn">\Sigma^{-1}</code>
  </td>
</tr>

</table>



<h3>Value</h3>

<p>An array (<code>length(xi)=length(xj) x 2</code>) with density value</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p> Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+rnmixGibbs">rnmixGibbs</a></code>, <code><a href="#topic+mixDen">mixDen</a></code>  </p>

<hr>
<h2 id='mnlHess'> Computes &ndash;Expected Hessian for Multinomial Logit</h2><span id='topic+mnlHess'></span>

<h3>Description</h3>

<p><code>mnlHess</code> computes expected Hessian (<code class="reqn">E[H]</code>) for Multinomial Logit Model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mnlHess(beta, y, X)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mnlHess_+3A_beta">beta</code></td>
<td>
 <p><code class="reqn">k x 1</code> vector of coefficients </p>
</td></tr>
<tr><td><code id="mnlHess_+3A_y">y</code></td>
<td>
 <p><code class="reqn">n x 1</code> vector of choices, (<code class="reqn">1,\ldots,p</code>) </p>
</td></tr>
<tr><td><code id="mnlHess_+3A_x">X</code></td>
<td>
 <p><code class="reqn">n*p x k</code> Design matrix </p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+llmnl">llmnl</a></code> for information on structure of X array.  Use <code><a href="#topic+createX">createX</a></code> to make X.
</p>


<h3>Value</h3>

<p><code class="reqn">k x k</code> matrix</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+llmnl">llmnl</a></code>, <code><a href="#topic+createX">createX</a></code>, <code><a href="#topic+rmnlIndepMetrop">rmnlIndepMetrop</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: mnlHess(beta, y, X)
</code></pre>

<hr>
<h2 id='mnpProb'>Compute MNP Probabilities</h2><span id='topic+mnpProb'></span>

<h3>Description</h3>

<p><code>mnpProb</code> computes MNP probabilities for a given <code class="reqn">X</code> matrix corresponding to one observation. This function can be used with output from <code>rmnpGibbs</code> to simulate the posterior distribution of market shares or fitted probabilties.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mnpProb(beta, Sigma, X, r)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mnpProb_+3A_beta">beta</code></td>
<td>
<p> MNP coefficients </p>
</td></tr>
<tr><td><code id="mnpProb_+3A_sigma">Sigma</code></td>
<td>
<p> Covariance matrix of latents </p>
</td></tr>
<tr><td><code id="mnpProb_+3A_x">X</code></td>
<td>
 <p><code class="reqn">X</code> array for one observation &ndash; use <code>createX</code> to make </p>
</td></tr>
<tr><td><code id="mnpProb_+3A_r">r</code></td>
<td>
<p> number of draws used in GHK (def: 100)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+rmnpGibbs">rmnpGibbs</a></code> for definition of the model and the interpretation of the beta and Sigma parameters. Uses the GHK method to compute choice probabilities. To simulate a distribution of probabilities, loop over the beta and Sigma draws from <code>rmnpGibbs</code> output.
</p>


<h3>Value</h3>

<p><code class="reqn">p x 1</code> vector of choice probabilites</p>


<h3>Author(s)</h3>

<p> Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p> For further discussion, see Chapters 2 and 4, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rmnpGibbs">rmnpGibbs</a></code>, <code><a href="#topic+createX">createX</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## example of computing MNP probabilites
## here Xa has the prices of each of the 3 alternatives

Xa    = matrix(c(1,.5,1.5), nrow=1)
X     = createX(p=3, na=1, nd=NULL, Xa=Xa, Xd=NULL, DIFF=TRUE)
beta  = c(1,-1,-2)  ## beta contains two intercepts and the price coefficient
Sigma = matrix(c(1, 0.5, 0.5, 1), ncol=2)

mnpProb(beta, Sigma, X)
</code></pre>

<hr>
<h2 id='momMix'>Compute Posterior Expectation of Normal Mixture Model Moments</h2><span id='topic+momMix'></span>

<h3>Description</h3>

<p><code>momMix</code> averages the moments of a normal mixture model over MCMC draws.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>momMix(probdraw, compdraw)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="momMix_+3A_probdraw">probdraw</code></td>
<td>
 <p><code class="reqn">R x ncomp</code> list of draws of mixture probs </p>
</td></tr>
<tr><td><code id="momMix_+3A_compdraw">compdraw</code></td>
<td>
<p> list of length <code class="reqn">R</code> of draws of mixture component moments </p>
</td></tr>
</table>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
    <code>R                       </code> </td><td style="text-align: left;"> is the number of MCMC draws in argument list above. </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ncomp                   </code> </td><td style="text-align: left;"> is the number of mixture components fitted.</td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>compdraw                </code> </td><td style="text-align: left;"> is a list of lists of lists with mixture components.  </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>compdraw[[i]]           </code> </td><td style="text-align: left;"> is <code class="reqn">i</code>th draw. </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>compdraw[[i]][[j]][[1]] </code> </td><td style="text-align: left;"> is the mean parameter vector for the <code class="reqn">j</code>th component, <code class="reqn">i</code>th MCMC draw. </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>compdraw[[i]][[j]][[2]] </code> </td><td style="text-align: left;"> is the UL decomposition of <code class="reqn">\Sigma^{-1}</code> for the <code class="reqn">j</code>th component, <code class="reqn">i</code>th MCMC draw
  </td>
</tr>

</table>



<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>mu</code></td>
<td>
<p>posterior expectation of mean</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>posterior expectation of covariance matrix</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>posterior expectation of vector of standard deviations</p>
</td></tr>
<tr><td><code>corr</code></td>
<td>
<p>posterior expectation of correlation matrix</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 5, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+rmixGibbs">rmixGibbs</a></code></p>

<hr>
<h2 id='nmat'>Convert Covariance Matrix to a Correlation Matrix</h2><span id='topic+nmat'></span>

<h3>Description</h3>

<p><code>nmat</code> converts a covariance matrix (stored as a vector, col by col) to a correlation matrix (also stored as a vector).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nmat(vec)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nmat_+3A_vec">vec</code></td>
<td>
 <p><code class="reqn">k x k</code> Cov matrix stored as a <code class="reqn">k*k x 1</code> vector (col by col) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This routine is often used with apply to convert an <code class="reqn">R x (k*k)</code> array of covariance MCMC draws to correlations. As in <code>corrdraws = apply(vardraws, 1, nmat)</code>.
</p>


<h3>Value</h3>

<p><code class="reqn">k*k x 1</code> vector with correlation matrix</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(66)
X = matrix(rnorm(200,4), ncol=2)
Varmat = var(X)
nmat(as.vector(Varmat))
</code></pre>

<hr>
<h2 id='numEff'>Compute Numerical Standard Error and Relative Numerical Efficiency</h2><span id='topic+numEff'></span>

<h3>Description</h3>

<p><code>numEff</code> computes the numerical standard error for the mean of a vector of draws as well as the relative numerical efficiency (ratio of variance of mean of this time series process relative to iid sequence).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>numEff(x, m = as.integer(min(length(x),(100/sqrt(5000))*sqrt(length(x)))))</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="numEff_+3A_x">x</code></td>
<td>
 <p><code class="reqn">R x 1</code> vector of draws </p>
</td></tr>
<tr><td><code id="numEff_+3A_m">m</code></td>
<td>
<p> number of lags for autocorrelations </p>
</td></tr>
</table>


<h3>Details</h3>

<p>default for number of lags is chosen so that if <code class="reqn">R=5000</code>, <code class="reqn">m=100</code> and increases as the <code class="reqn">sqrt(R)</code>.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>stderr</code></td>
<td>
<p>standard error of the mean of <code class="reqn">x</code></p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p> variance ratio (relative numerical efficiency) </p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>numEff(rnorm(1000), m=20)
numEff(rnorm(1000))
</code></pre>

<hr>
<h2 id='orangeJuice'>Store-level Panel Data on Orange Juice Sales</h2><span id='topic+orangeJuice'></span>

<h3>Description</h3>

<p>Weekly sales of refrigerated orange juice at 83 stores. Contains demographic information on those stores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(orangeJuice)</code></pre>


<h3>Format</h3>

<p>The <code>orangeJuice</code> object is a list containing two data frames, <code>yx</code> and <code>storedemo</code>.
</p>


<h3>Details</h3>

<p>In the <code>yx</code> data frame: 
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$store    </code> </td><td style="text-align: left;"> store number </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$brand    </code> </td><td style="text-align: left;"> brand indicator </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$week     </code> </td><td style="text-align: left;"> week number </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$logmove  </code> </td><td style="text-align: left;"> log of the number of units sold </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$constant </code> </td><td style="text-align: left;"> a vector of 1s </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$price#   </code> </td><td style="text-align: left;"> price of brand # </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$deal     </code> </td><td style="text-align: left;"> in-store coupon activity </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$feature  </code> </td><td style="text-align: left;"> feature advertisement </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$profit   </code> </td><td style="text-align: left;"> profit 
  </td>
</tr>

</table>

<p>The price variables correspond to the following brands:
</p>

<table>
<tr>
 <td style="text-align: left;">
     1  </td><td style="text-align: left;"> Tropicana Premium 64 oz </td>
</tr>
<tr>
 <td style="text-align: left;">
     2  </td><td style="text-align: left;"> Tropicana Premium 96 oz </td>
</tr>
<tr>
 <td style="text-align: left;">
     3  </td><td style="text-align: left;"> Florida's Natural 64 oz </td>
</tr>
<tr>
 <td style="text-align: left;">
     4  </td><td style="text-align: left;"> Tropicana 64 oz </td>
</tr>
<tr>
 <td style="text-align: left;">
     5  </td><td style="text-align: left;"> Minute Maid 64 oz </td>
</tr>
<tr>
 <td style="text-align: left;">
     6  </td><td style="text-align: left;"> Minute Maid 96 oz </td>
</tr>
<tr>
 <td style="text-align: left;">
     7  </td><td style="text-align: left;"> Citrus Hill 64 oz </td>
</tr>
<tr>
 <td style="text-align: left;">
     8  </td><td style="text-align: left;"> Tree Fresh 64 oz </td>
</tr>
<tr>
 <td style="text-align: left;">
     9  </td><td style="text-align: left;"> Florida Gold 64 oz </td>
</tr>
<tr>
 <td style="text-align: left;">
     10 </td><td style="text-align: left;"> Dominicks 64 oz </td>
</tr>
<tr>
 <td style="text-align: left;">
     11 </td><td style="text-align: left;"> Dominicks 128 oz 
  </td>
</tr>

</table>

<p>In the <code>storedemo</code> data frame:
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$STORE    </code> </td><td style="text-align: left;"> store number </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$AGE60    </code> </td><td style="text-align: left;"> percentage of the population that is aged 60 or older </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$EDUC     </code> </td><td style="text-align: left;"> percentage of the population that has a college degree </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$ETHNIC   </code> </td><td style="text-align: left;"> percent of the population that is black or Hispanic </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$INCOME   </code> </td><td style="text-align: left;"> median income </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$HHLARGE  </code> </td><td style="text-align: left;"> percentage of households with 5 or more persons </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$WORKWOM  </code> </td><td style="text-align: left;"> percentage of women with full-time jobs </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$HVAL150  </code> </td><td style="text-align: left;"> percentage of households worth more than $150,000 </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$SSTRDIST </code> </td><td style="text-align: left;"> distance to the nearest warehouse store </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$SSTRVOL  </code> </td><td style="text-align: left;"> ratio of sales of this store to the nearest warehouse store </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$CPDIST5  </code> </td><td style="text-align: left;"> average distance in miles to the nearest 5 supermarkets </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$CPWVOL5  </code> </td><td style="text-align: left;"> ratio of sales of this store to the average of the nearest five stores 
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Alan L. Montgomery (1997), &quot;Creating Micro-Marketing Pricing Strategies Using Supermarket Scanner Data,&quot; <em>Marketing Science</em> 16(4) 315&ndash;337.</p>


<h3>References</h3>

<p>Chapter 5, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load data
data(orangeJuice)

## print some quantiles of yx data  
cat("Quantiles of the Variables in yx data",fill=TRUE)
mat = apply(as.matrix(orangeJuice$yx), 2, quantile)
print(mat)

## print some quantiles of storedemo data
cat("Quantiles of the Variables in storedemo data",fill=TRUE)
mat = apply(as.matrix(orangeJuice$storedemo), 2, quantile)
print(mat)


## processing for use with rhierLinearModel
if(0) {
  
  ## select brand 1 for analysis
  brand1 = orangeJuice$yx[(orangeJuice$yx$brand==1),]
  
  store = sort(unique(brand1$store))
  nreg = length(store)
  nvar = 14
  
  regdata=NULL
  for (reg in 1:nreg) {
    y = brand1$logmove[brand1$store==store[reg]]
    iota = c(rep(1,length(y)))
    X = cbind(iota,log(brand1$price1[brand1$store==store[reg]]),
                   log(brand1$price2[brand1$store==store[reg]]),
                   log(brand1$price3[brand1$store==store[reg]]),
                   log(brand1$price4[brand1$store==store[reg]]),
                   log(brand1$price5[brand1$store==store[reg]]),
                   log(brand1$price6[brand1$store==store[reg]]),
                   log(brand1$price7[brand1$store==store[reg]]),
                   log(brand1$price8[brand1$store==store[reg]]),
                   log(brand1$price9[brand1$store==store[reg]]),
                   log(brand1$price10[brand1$store==store[reg]]),
                   log(brand1$price11[brand1$store==store[reg]]),
                   brand1$deal[brand1$store==store[reg]],
                   brand1$feat[brand1$store==store[reg]] )
    regdata[[reg]] = list(y=y, X=X)
    }
  
  ## storedemo is standardized to zero mean.
  Z = as.matrix(orangeJuice$storedemo[,2:12]) 
  dmean = apply(Z, 2, mean)
  for (s in 1:nreg) {Z[s,] = Z[s,] - dmean}
  iotaz = c(rep(1,nrow(Z)))
  Z = cbind(iotaz, Z)
  nz = ncol(Z)
  
  Data = list(regdata=regdata, Z=Z)
  Mcmc = list(R=R, keep=1)
  
  out = rhierLinearModel(Data=Data, Mcmc=Mcmc)
  
  summary(out$Deltadraw)
  summary(out$Vbetadraw)
  
  ## plotting examples
  if(0){ plot(out$betadraw) }
}
</code></pre>

<hr>
<h2 id='plot.bayesm.hcoef'>Plot Method for Hierarchical Model Coefs</h2><span id='topic+plot.bayesm.hcoef'></span>

<h3>Description</h3>

<p><code>plot.bayesm.hcoef</code> is an S3 method to plot 3 dim arrays of hierarchical coefficients. Arrays are of class <code>bayesm.hcoef</code> with dimensions: cross-sectional unit <code class="reqn">x</code> coef <code class="reqn">x</code> MCMC draw. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bayesm.hcoef'
plot(x,names,burnin,...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.bayesm.hcoef_+3A_x">x</code></td>
<td>
<p> An object of S3 class, <code>bayesm.hcoef</code> </p>
</td></tr>
<tr><td><code id="plot.bayesm.hcoef_+3A_names">names</code></td>
<td>
<p> a list of names for the variables in the hierarchical model</p>
</td></tr>
<tr><td><code id="plot.bayesm.hcoef_+3A_burnin">burnin</code></td>
<td>
<p> no draws to burnin (def: <code class="reqn">0.1*R</code>)</p>
</td></tr>
<tr><td><code id="plot.bayesm.hcoef_+3A_...">...</code></td>
<td>
<p> standard graphics parameters </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typically, <code>plot.bayesm.hcoef</code> will be invoked by a call to the generic plot function as in <code>plot(object)</code> where object is of class <code>bayesm.hcoef</code>. All of the <code>bayesm</code> hierarchical routines return draws of hierarchical coefficients in this class (see example below).  One can also simply invoke <code>plot.bayesm.hcoef</code> on any valid 3-dim array as in <code>plot.bayesm.hcoef(betadraws)</code>.
<br />
<br />
<code>plot.bayesm.hcoef</code> is also exported for use as a standard function, as in <code>plot.bayesm.hcoef(array)</code>.
</p>


<h3>Author(s)</h3>

<p> Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rhierMnlRwMixture">rhierMnlRwMixture</a></code>,<code><a href="#topic+rhierLinearModel">rhierLinearModel</a></code>,
<code><a href="#topic+rhierLinearMixture">rhierLinearMixture</a></code>,<code><a href="#topic+rhierNegbinRw">rhierNegbinRw</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: out=rhierLinearModel(Data,Prior,Mcmc); plot(out$betadraws)
</code></pre>

<hr>
<h2 id='plot.bayesm.mat'>Plot Method for Arrays of MCMC Draws</h2><span id='topic+plot.bayesm.mat'></span>

<h3>Description</h3>

<p><code>plot.bayesm.mat</code> is an S3 method to plot arrays of MCMC draws. The columns in the array correspond to parameters and the rows to MCMC draws.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bayesm.mat'
plot(x,names,burnin,tvalues,TRACEPLOT,DEN,INT,CHECK_NDRAWS, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.bayesm.mat_+3A_x">x</code></td>
<td>
<p> An object of either S3 class, <code>bayesm.mat</code>, or S3 class, <code>mcmc</code> </p>
</td></tr>
<tr><td><code id="plot.bayesm.mat_+3A_names">names</code></td>
<td>
<p> optional character vector of names for coefficients</p>
</td></tr>
<tr><td><code id="plot.bayesm.mat_+3A_burnin">burnin</code></td>
<td>
<p> number of draws to discard for burn-in (def: <code class="reqn">0.1*nrow(X))</code></p>
</td></tr>
<tr><td><code id="plot.bayesm.mat_+3A_tvalues">tvalues</code></td>
<td>
<p> vector of true values</p>
</td></tr>
<tr><td><code id="plot.bayesm.mat_+3A_traceplot">TRACEPLOT</code></td>
<td>
<p> logical, <code>TRUE</code> provide sequence plots of draws and acfs (def: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="plot.bayesm.mat_+3A_den">DEN</code></td>
<td>
<p> logical, <code>TRUE</code> use density scale on histograms (def: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="plot.bayesm.mat_+3A_int">INT</code></td>
<td>
<p> logical, <code>TRUE</code> put various intervals and points on graph (def: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="plot.bayesm.mat_+3A_check_ndraws">CHECK_NDRAWS</code></td>
<td>
<p> logical, <code>TRUE</code> check that there are at least 100 draws (def: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="plot.bayesm.mat_+3A_...">...</code></td>
<td>
<p> standard graphics parameters </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typically, <code>plot.bayesm.mat</code> will be invoked by a call to the generic plot function as in <code>plot(object)</code> where object is of class <code>bayesm.mat</code>. All of the <code>bayesm</code> MCMC routines return draws in this class (see example below). One can also simply invoke <code>plot.bayesm.mat</code> on any valid 2-dim array as in <code>plot.bayesm.mat(betadraws)</code>. <br />
<br />
<code>plot.bayesm.mat</code> paints (by default) on the histogram: <br />
<br />
green &quot;[]&quot; delimiting 95% Bayesian Credibility Interval <br />
yellow &quot;()&quot; showing +/- 2 numerical standard errors <br />
red &quot;|&quot; showing posterior mean 
<br />
<br />
<code>plot.bayesm.mat</code> is also exported for use as a standard function, as in <code>plot.bayesm.mat(matrix)</code>
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: out=runiregGibbs(Data,Prior,Mcmc); plot(out$betadraw)
</code></pre>

<hr>
<h2 id='plot.bayesm.nmix'>Plot Method for MCMC Draws of Normal Mixtures</h2><span id='topic+plot.bayesm.nmix'></span>

<h3>Description</h3>

<p><code>plot.bayesm.nmix</code> is an S3 method to plot aspects of the fitted density from a list of MCMC draws of normal mixture components. Plots of marginal univariate and bivariate densities are produced.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bayesm.nmix'
plot(x, names, burnin, Grid, bi.sel, nstd, marg, Data, ngrid, ndraw, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.bayesm.nmix_+3A_x">x</code></td>
<td>
<p> An object of S3 class <code>bayesm.nmix</code></p>
</td></tr>
<tr><td><code id="plot.bayesm.nmix_+3A_names">names</code></td>
<td>
<p> optional character vector of names for each of the dimensions</p>
</td></tr>
<tr><td><code id="plot.bayesm.nmix_+3A_burnin">burnin</code></td>
<td>
<p> number of draws to discard for burn-in (def: <code class="reqn">0.1*nrow(X)</code>)</p>
</td></tr>
<tr><td><code id="plot.bayesm.nmix_+3A_grid">Grid</code></td>
<td>
<p> matrix of grid points for densities, def: mean +/- nstd std deviations (if Data no supplied), range of Data if supplied)</p>
</td></tr>
<tr><td><code id="plot.bayesm.nmix_+3A_bi.sel">bi.sel</code></td>
<td>
<p> list of vectors, each giving pairs for bivariate distributions (def: <code>list(c(1,2))</code>)</p>
</td></tr>
<tr><td><code id="plot.bayesm.nmix_+3A_nstd">nstd</code></td>
<td>
<p> number of standard deviations for default Grid (def: 2)</p>
</td></tr>
<tr><td><code id="plot.bayesm.nmix_+3A_marg">marg</code></td>
<td>
<p> logical, if TRUE display marginals (def: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="plot.bayesm.nmix_+3A_data">Data</code></td>
<td>
<p> matrix of data points, used to paint histograms on marginals and for grid</p>
</td></tr>
<tr><td><code id="plot.bayesm.nmix_+3A_ngrid">ngrid</code></td>
<td>
<p> number of grid points for density estimates (def: 50)</p>
</td></tr>
<tr><td><code id="plot.bayesm.nmix_+3A_ndraw">ndraw</code></td>
<td>
<p> number of draws to average Mcmc estimates over (def: 200)</p>
</td></tr>
<tr><td><code id="plot.bayesm.nmix_+3A_...">...</code></td>
<td>
<p> standard graphics parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typically, <code>plot.bayesm.nmix</code> will be invoked by a call to the generic plot function as in <code>plot(object)</code> where object is of class bayesm.nmix.  These objects are lists of three components. The first component is an array of draws of mixture component probabilties.  The second component is not used. The third is a lists of lists of lists with draws of each of the normal components.<br />
<br />
<code>plot.bayesm.nmix</code> can also be used as a standard function, as in <code>plot.bayesm.nmix(list)</code>.
</p>


<h3>Author(s)</h3>

<p> Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rnmixGibbs">rnmixGibbs</a></code>, <code><a href="#topic+rhierMnlRwMixture">rhierMnlRwMixture</a></code>, <code><a href="#topic+rhierLinearMixture">rhierLinearMixture</a></code>, <code><a href="#topic+rDPGibbs">rDPGibbs</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## not run
# out = rnmixGibbs(Data, Prior, Mcmc)

## plot bivariate distributions for dimension 1,2; 3,4; and 1,3
# plot(out,bi.sel=list(c(1,2),c(3,4),c(1,3)))
</code></pre>

<hr>
<h2 id='rbayesBLP'>Bayesian Analysis of Random Coefficient Logit Models Using Aggregate Data</h2><span id='topic+rbayesBLP'></span>

<h3>Description</h3>

<p><code>rbayesBLP</code> implements a hybrid MCMC algorithm for aggregate level sales data in a market with differentiated products. bayesm version 3.1-0 and prior verions contain an error when using instruments with this function; this will be fixed in a future version.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbayesBLP(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbayesBLP_+3A_data">Data</code></td>
<td>
<p>list(X, share, J, Z)</p>
</td></tr>
<tr><td><code id="rbayesBLP_+3A_prior">Prior</code></td>
<td>
<p>list(sigmasqR, theta_hat, A, deltabar, Ad, nu0, s0_sq, VOmega)</p>
</td></tr>
<tr><td><code id="rbayesBLP_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, H, initial_theta_bar, initial_r, initial_tau_sq, initial_Omega, initial_delta, s, cand_cov, tol)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>thetabardraw</code></td>
<td>
<p><code class="reqn">K x R/keep</code> matrix of random coefficient mean draws</p>
</td></tr>
<tr><td><code>Sigmadraw</code></td>
<td>
<p><code class="reqn">K*K x R/keep</code> matrix of random coefficient variance draws</p>
</td></tr>
<tr><td><code>rdraw</code></td>
<td>
<p><code class="reqn">K*K x R/keep</code> matrix of <code class="reqn">r</code> draws (same information as in <code>Sigmadraw</code>)</p>
</td></tr>
<tr><td><code>tausqdraw</code></td>
<td>
<p><code class="reqn">R/keep x 1</code> vector of aggregate demand shock variance draws</p>
</td></tr>
<tr><td><code>Omegadraw</code></td>
<td>
<p><code class="reqn">2*2 x R/keep</code> matrix of correlated endogenous shock variance draws</p>
</td></tr>
<tr><td><code>deltadraw</code></td>
<td>
<p><code class="reqn">I x R/keep</code> matrix of endogenous structural equation coefficient draws</p>
</td></tr>
<tr><td><code>acceptrate</code></td>
<td>
<p>scalor of acceptance rate of Metropolis-Hasting</p>
</td></tr>
<tr><td><code>s</code></td>
<td>
<p>scale parameter used for Metropolis-Hasting</p>
</td></tr>
<tr><td><code>cand_cov</code></td>
<td>
<p>var-cov matrix used for Metropolis-Hasting</p>
</td></tr>
</table>


<h3>Argument Details</h3>

<p><em><code>Data  = list(X, share, J, Z)</code></em> [<code>Z</code> optional]
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>J:      </code> </td><td style="text-align: left;"> number of alternatives, excluding an outside option </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>X:      </code> </td><td style="text-align: left;"> <code class="reqn">J*T x K</code> matrix (no outside option, which is normalized to 0). </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> If IV is used, the last column of <code>X</code> is the endogeneous variable. </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>share:  </code> </td><td style="text-align: left;"> <code class="reqn">J*T</code> vector (no outside option). </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> Note that both the <code>share</code> vector and the <code>X</code> matrix are organized by the <code class="reqn">jt</code> index. </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <code class="reqn">j</code> varies faster than <code class="reqn">t</code>, i.e. <code class="reqn">(j=1,t=1), (j=2,t=1), ..., (j=J,T=1), ..., (j=J,t=T)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Z:      </code> </td><td style="text-align: left;"> <code class="reqn">J*T x I</code> matrix of instrumental variables (optional)
    </td>
</tr>

</table>

<p><em><code>Prior = list(sigmasqR, theta_hat, A, deltabar, Ad, nu0, s0_sq, VOmega)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>sigmasqR:   </code> </td><td style="text-align: left;"> <code class="reqn">K*(K+1)/2</code> vector for <code class="reqn">r</code> prior variance (def: diffuse prior for <code class="reqn">\Sigma</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>theta_hat:  </code> </td><td style="text-align: left;"> <code class="reqn">K</code> vector for <code class="reqn">\theta_bar</code> prior mean (def: 0 vector) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>A:          </code> </td><td style="text-align: left;"> <code class="reqn">K x K</code> matrix for <code class="reqn">\theta_bar</code> prior precision (def: <code>0.01*diag(K)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>deltabar:   </code> </td><td style="text-align: left;"> <code class="reqn">I</code> vector for <code class="reqn">\delta</code> prior mean (def: 0 vector) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Ad:         </code> </td><td style="text-align: left;"> <code class="reqn">I x I</code> matrix for <code class="reqn">\delta</code> prior precision (def: <code>0.01*diag(I)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu0:        </code> </td><td style="text-align: left;"> d.f. parameter for <code class="reqn">\tau_sq</code> and <code class="reqn">\Omega</code> prior (def: K+1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>s0_sq:      </code> </td><td style="text-align: left;"> scale parameter for <code class="reqn">\tau_sq</code> prior (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>VOmega:     </code> </td><td style="text-align: left;"> <code class="reqn">2 x 2</code> matrix parameter for <code class="reqn">\Omega</code> prior (def: <code>matrix(c(1,0.5,0.5,1),2,2)</code>)
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, H, initial_theta_bar, initial_r, initial_tau_sq, initial_Omega, initial_delta, s, cand_cov, tol)</code> [only <code>R</code> and <code>H</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:                  </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:               </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:             </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>H:                  </code> </td><td style="text-align: left;"> number of random draws used for Monte-Carlo integration </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>initial_theta_bar:  </code> </td><td style="text-align: left;"> initial value of <code class="reqn">\theta_bar</code> (def: 0 vector) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>initial_r:          </code> </td><td style="text-align: left;"> initial value of <code class="reqn">r</code> (def: 0 vector) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>initial_tau_sq:     </code> </td><td style="text-align: left;"> initial value of <code class="reqn">\tau_sq</code> (def: 0.1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>initial_Omega:      </code> </td><td style="text-align: left;"> initial value of <code class="reqn">\Omega</code> (def: <code>diag(2)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>initial_delta:      </code> </td><td style="text-align: left;"> initial value of <code class="reqn">\delta</code> (def: 0 vector) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>s:                  </code> </td><td style="text-align: left;"> scale parameter of Metropolis-Hasting increment (def: automatically tuned) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>cand_cov:           </code> </td><td style="text-align: left;"> var-cov matrix of Metropolis-Hasting increment (def: automatically tuned) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>tol:                </code> </td><td style="text-align: left;"> convergence tolerance for the contraction mapping (def: 1e-6)
    </td>
</tr>

</table>



<h3>Model Details</h3>



<h4>Model and Priors (without IV):</h4>

 
<p><code class="reqn">u_ijt = X_jt \theta_i + \eta_jt + e_ijt</code><br />
<code class="reqn">e_ijt</code> <code class="reqn">\sim</code> type I Extreme Value (logit)<br />
<code class="reqn">\theta_i</code> <code class="reqn">\sim</code>  <code class="reqn">N(\theta_bar, \Sigma)</code><br />
<code class="reqn">\eta_jt</code> <code class="reqn">\sim</code> <code class="reqn">N(0, \tau_sq)</code><br /> 
</p>
<p>This structure implies a logit model for each consumer (<code class="reqn">\theta</code>). 
Aggregate shares (<code>share</code>) are produced by integrating this consumer level 
logit model over the assumed normal distribution of <code class="reqn">\theta</code>. 
</p>
<p><code class="reqn">r</code> <code class="reqn">\sim</code> <code class="reqn">N(0, diag(sigmasqR))</code><br />
<code class="reqn">\theta_bar</code> <code class="reqn">\sim</code> <code class="reqn">N(\theta_hat, A^-1)</code><br />
<code class="reqn">\tau_sq</code> <code class="reqn">\sim</code> <code class="reqn">nu0*s0_sq / \chi^2 (nu0)</code><br />
</p>
<p>Note: we observe the aggregate level market share, not individual level choices.<br />
</p>
<p>Note: <code class="reqn">r</code> is the vector of nonzero elements of cholesky root of <code class="reqn">\Sigma</code>. 
Instead of <code class="reqn">\Sigma</code> we draw <code class="reqn">r</code>, which is one-to-one correspondence with the positive-definite <code class="reqn">\Sigma</code>.
</p>



<h4>Model and Priors (with IV):</h4>

 
<p><code class="reqn">u_ijt = X_jt \theta_i + \eta_jt + e_ijt</code><br />
<code class="reqn">e_ijt</code> <code class="reqn">\sim</code> type I Extreme Value (logit)<br />
<code class="reqn">\theta_i</code> <code class="reqn">\sim</code>  <code class="reqn">N(\theta_bar, \Sigma)</code><br />
</p>
<p><code class="reqn">X_jt = [X_exo_jt, X_endo_jt]</code><br />
<code class="reqn">X_endo_jt = Z_jt \delta_jt + \zeta_jt</code><br />
<code class="reqn">vec(\zeta_jt, \eta_jt)</code> <code class="reqn">\sim</code> <code class="reqn">N(0, \Omega)</code><br />
</p>
<p><code class="reqn">r</code> <code class="reqn">\sim</code> <code class="reqn">N(0, diag(sigmasqR))</code><br />
<code class="reqn">\theta_bar</code> <code class="reqn">\sim</code> <code class="reqn">N(\theta_hat, A^-1)</code><br />
<code class="reqn">\delta</code> <code class="reqn">\sim</code> <code class="reqn">N(deltabar, Ad^-1)</code><br />
<code class="reqn">\Omega</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu0, VOmega)</code><br />
</p>



<h3>MCMC and Tuning Details:</h3>



<h4>MCMC Algorithm:</h4>

<p>Step 1 (<code class="reqn">\Sigma</code>):<br />
Given <code class="reqn">\theta_bar</code> and <code class="reqn">\tau_sq</code>, draw <code class="reqn">r</code> via Metropolis-Hasting.<br />
Covert the drawn <code class="reqn">r</code> to <code class="reqn">\Sigma</code>.<br />
</p>
<p>Note: if user does not specify the Metropolis-Hasting increment parameters 
(<code>s</code> and <code>cand_cov</code>), <code>rbayesBLP</code> automatically tunes the parameters.
</p>
<p>Step 2 without IV (<code class="reqn">\theta_bar</code>, <code class="reqn">\tau_sq</code>):<br />
Given <code class="reqn">\Sigma</code>, draw <code class="reqn">\theta_bar</code> and <code class="reqn">\tau_sq</code> via Gibbs sampler.<br />
</p>
<p>Step 2 with IV (<code class="reqn">\theta_bar</code>, <code class="reqn">\delta</code>, <code class="reqn">\Omega</code>):<br />
Given <code class="reqn">\Sigma</code>, draw <code class="reqn">\theta_bar</code>, <code class="reqn">\delta</code>, and <code class="reqn">\Omega</code> via IV Gibbs sampler.<br />
</p>



<h4>Tuning Metropolis-Hastings algorithm:</h4>

<p>r_cand = r_old + s*N(0,cand_cov)<br />
Fix the candidate covariance matrix as cand_cov0 = diag(rep(0.1, K), rep(1, K*(K-1)/2)).<br />
Start from s0 = 2.38/sqrt(dim(r))<br />
</p>
<p>Repeat{<br />
Run 500 MCMC chain.<br />   
If acceptance rate &lt; 30% =&gt; update s1 = s0/5.<br />
If acceptance rate &gt; 50% =&gt; update s1 = s0*3.<br />
(Store r draws if acceptance rate is 20~80%.)<br />
s0 = s1<br />
} until acceptance rate is 30~50%
</p>
<p>Scale matrix C = s1*sqrt(cand_cov0)<br />
Correlation matrix R = Corr(r draws)<br />
Use C*R*C as s^2*cand_cov.
</p>



<h3>Author(s)</h3>

<p>Peter Rossi and K. Kim, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see <em>Bayesian Analysis of Random Coefficient Logit Models Using Aggregate Data</em> by Jiang, Manchanda, and Rossi, <em>Journal of Econometrics</em>, 2009. <br /> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {

## Simulate aggregate level data
simulData &lt;- function(para, others, Hbatch) {
  # Hbatch does the integration for computing market shares
  #      in batches of size Hbatch

  ## parameters
  theta_bar &lt;- para$theta_bar
  Sigma &lt;- para$Sigma
  tau_sq &lt;- para$tau_sq

  T &lt;- others$T	
  J &lt;- others$J	
  p &lt;- others$p	
  H &lt;- others$H	
  K &lt;- J + p	

  ## build X	
  X &lt;- matrix(runif(T*J*p), T*J, p)
  inter &lt;- NULL
  for (t in 1:T) { inter &lt;- rbind(inter, diag(J)) }
  X &lt;- cbind(inter, X)

  ## draw eta ~ N(0, tau_sq)	
  eta &lt;- rnorm(T*J)*sqrt(tau_sq)
  X &lt;- cbind(X, eta)

  share &lt;- rep(0, J*T)
  for (HH in 1:(H/Hbatch)){
    ## draw theta ~ N(theta_bar, Sigma)
    cho &lt;- chol(Sigma)
    theta &lt;- matrix(rnorm(K*Hbatch), nrow=K, ncol=Hbatch)
    theta &lt;- t(cho)%*%theta + theta_bar

    ## utility
    V &lt;- X%*%rbind(theta, 1)
    expV &lt;- exp(V)
    expSum &lt;- matrix(colSums(matrix(expV, J, T*Hbatch)), T, Hbatch)
    expSum &lt;- expSum %x% matrix(1, J, 1)
    choiceProb &lt;- expV / (1 + expSum)
    share &lt;- share +  rowSums(choiceProb) / H
  }

  ## the last K+1'th column is eta, which is unobservable.
  X &lt;- X[,c(1:K)]	
  return (list(X=X, share=share))
}

## true parameter
theta_bar_true &lt;- c(-2, -3, -4, -5)
Sigma_true &lt;- rbind(c(3,2,1.5,1), c(2,4,-1,1.5), c(1.5,-1,4,-0.5), c(1,1.5,-0.5,3))
cho &lt;- chol(Sigma_true)
r_true &lt;- c(log(diag(cho)), cho[1,2:4], cho[2,3:4], cho[3,4]) 
tau_sq_true &lt;- 1

## simulate data
set.seed(66)
T &lt;- 300
J &lt;- 3
p &lt;- 1
K &lt;- 4
H &lt;- 1000000
Hbatch &lt;- 5000

dat &lt;- simulData(para=list(theta_bar=theta_bar_true, Sigma=Sigma_true, tau_sq=tau_sq_true),
                 others=list(T=T, J=J, p=p, H=H), Hbatch)
X &lt;- dat$X
share &lt;- dat$share

## Mcmc run
R &lt;- 2000
H &lt;- 50
Data1 &lt;- list(X=X, share=share, J=J)
Mcmc1 &lt;- list(R=R, H=H, nprint=0)
set.seed(66)
out &lt;- rbayesBLP(Data=Data1, Mcmc=Mcmc1)

## acceptance rate
out$acceptrate

## summary of draws
summary(out$thetabardraw)
summary(out$Sigmadraw)
summary(out$tausqdraw)

### plotting draws
plot(out$thetabardraw)
plot(out$Sigmadraw)
plot(out$tausqdraw)
}
</code></pre>

<hr>
<h2 id='rbiNormGibbs'>Illustrate Bivariate Normal Gibbs Sampler</h2><span id='topic+rbiNormGibbs'></span>

<h3>Description</h3>

<p><code>rbiNormGibbs</code> implements a Gibbs Sampler for the bivariate normal distribution. Intermediate moves are plotted and the output is contrasted with the iid sampler. This function is designed for illustrative/teaching purposes.</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbiNormGibbs(initx=2, inity=-2, rho, burnin=100, R=500)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbiNormGibbs_+3A_initx">initx</code></td>
<td>
<p> initial value of parameter on x axis (def: 2)</p>
</td></tr>
<tr><td><code id="rbiNormGibbs_+3A_inity">inity</code></td>
<td>
<p> initial value of parameter on y axis (def: -2)</p>
</td></tr>
<tr><td><code id="rbiNormGibbs_+3A_rho">rho</code></td>
<td>
<p> correlation for bivariate normals</p>
</td></tr>
<tr><td><code id="rbiNormGibbs_+3A_burnin">burnin</code></td>
<td>
<p> burn-in number of draws (def: 100)</p>
</td></tr>
<tr><td><code id="rbiNormGibbs_+3A_r">R</code></td>
<td>
<p> number of MCMC draws (def: 500)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">(\theta_1, \theta_2) ~ N( (0,0), \Sigma</code>) with <code class="reqn">\Sigma</code> = <code>matrix(c(1,rho,rho,1),ncol=2)</code>
</p>


<h3>Value</h3>

<p><code class="reqn">R x 2</code> matrix of draws
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapters 2 and 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: out=rbiNormGibbs(rho=0.95)
</code></pre>

<hr>
<h2 id='rbprobitGibbs'>Gibbs Sampler (Albert and Chib) for Binary Probit</h2><span id='topic+rbprobitGibbs'></span>

<h3>Description</h3>

<p><code>rbprobitGibbs</code> implements the Albert and Chib Gibbs Sampler for the binary probit model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbprobitGibbs(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbprobitGibbs_+3A_data">Data</code></td>
<td>
<p>list(y, X)</p>
</td></tr>
<tr><td><code id="rbprobitGibbs_+3A_prior">Prior</code></td>
<td>
<p>list(betabar, A)</p>
</td></tr>
<tr><td><code id="rbprobitGibbs_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">z = X\beta + e</code> with <code class="reqn">e</code> <code class="reqn">\sim</code> <code class="reqn">N(0, I)</code> <br />
<code class="reqn">y = 1</code> if <code class="reqn">z &gt; 0</code> 
</p>
<p><code class="reqn">\beta</code> <code class="reqn">\sim</code> <code class="reqn">N(betabar, A^{-1})</code>
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(y, X)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>y:       </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of 0/1 outcomes </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>X:       </code> </td><td style="text-align: left;"> <code class="reqn">n x k</code> design matrix
    </td>
</tr>

</table>

<p><em><code>Prior = list(betabar, A)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>betabar: </code> </td><td style="text-align: left;"> <code class="reqn">k x 1</code> prior mean (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>A:       </code> </td><td style="text-align: left;"> <code class="reqn">k x k</code> prior precision matrix (def: 0.01*I)
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:       </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:    </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:  </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print)
    </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">R/keep x k</code> matrix of betadraws</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rmnpGibbs">rmnpGibbs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

## function to simulate from binary probit including x variable
simbprobit = function(X, beta) {
  y = ifelse((X%*%beta + rnorm(nrow(X)))&lt;0, 0, 1)
  list(X=X, y=y, beta=beta)
}

nobs = 200
X = cbind(rep(1,nobs), runif(nobs), runif(nobs))
beta = c(0,1,-1)
nvar = ncol(X)
simout = simbprobit(X, beta)

Data1 = list(X=simout$X, y=simout$y)
Mcmc1 = list(R=R, keep=1)

out = rbprobitGibbs(Data=Data1, Mcmc=Mcmc1)
summary(out$betadraw, tvalues=beta)

## plotting example
if(0){plot(out$betadraw, tvalues=beta)}
</code></pre>

<hr>
<h2 id='rdirichlet'>Draw From Dirichlet Distribution</h2><span id='topic+rdirichlet'></span>

<h3>Description</h3>

<p><code>rdirichlet</code> draws from Dirichlet
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rdirichlet(alpha)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rdirichlet_+3A_alpha">alpha</code></td>
<td>
<p>vector of Dirichlet parms (must be &gt; 0)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of draws from Dirichlet</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 2, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(66)
rdirichlet(c(rep(3,5)))
</code></pre>

<hr>
<h2 id='rDPGibbs'> Density Estimation with Dirichlet Process Prior and Normal Base </h2><span id='topic+rDPGibbs'></span>

<h3>Description</h3>

<p><code>rDPGibbs</code> implements a Gibbs Sampler to draw from the posterior for a normal mixture problem with a Dirichlet Process prior. A natural conjugate base prior is used along with priors on the hyper parameters of this distribution. One interpretation of this model is as a normal mixture with a random number of components that can grow with the sample size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rDPGibbs(Prior, Data, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rDPGibbs_+3A_data">Data</code></td>
<td>
<p>list(y)</p>
</td></tr>
<tr><td><code id="rDPGibbs_+3A_prior">Prior</code></td>
<td>
<p>list(Prioralpha, lambda_hyper)</p>
</td></tr>
<tr><td><code id="rDPGibbs_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, maxuniq, SCALE, gridsize)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">y_i</code> <code class="reqn">\sim</code> <code class="reqn">N(\mu_i, \Sigma_i)</code>
</p>
<p><code class="reqn">\theta_i=(\mu_i,\Sigma_i)</code> <code class="reqn">\sim</code> <code class="reqn">DP(G_0(\lambda),alpha)</code><br />
</p>
<p><code class="reqn">G_0(\lambda):</code><br />
<code class="reqn">\mu_i | \Sigma_i</code> <code class="reqn">\sim</code> <code class="reqn">N(0,\Sigma_i (x) a^{-1})</code><br />
<code class="reqn">\Sigma_i</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu,nu*v*I)</code>
</p>
<p><code class="reqn">\lambda(a,nu,v):</code><br />
<code class="reqn">a</code> <code class="reqn">\sim</code> uniform on grid[alim[1], alimb[2]]<br />
<code class="reqn">nu</code> <code class="reqn">\sim</code> uniform on grid[dim(data)-1 + exp(nulim[1]), dim(data)-1 + exp(nulim[2])]<br />
<code class="reqn">v</code> <code class="reqn">\sim</code> uniform on grid[vlim[1], vlim[2]]
</p>
<p><code class="reqn">alpha</code> <code class="reqn">\sim</code> <code class="reqn">(1-(\alpha-alphamin)/(alphamax-alphamin))^{power}</code> <br />
<code class="reqn">alpha</code> = alphamin then expected number of components = <code>Istarmin</code> <br />
<code class="reqn">alpha</code> = alphamax then expected number of components = <code>Istarmax</code> 
</p>
<p>We parameterize the prior on <code class="reqn">\Sigma_i</code> such that <code class="reqn">mode(\Sigma)= nu/(nu+2) vI</code>. The support of nu enforces valid IW density; <code class="reqn">nulim[1] &gt; 0</code>
</p>
<p>We use the structure for <code>nmix</code> that is compatible with the <code>bayesm</code> routines for finite mixtures of normals. This allows us to use the same summary and plotting methods.  
</p>
<p>The default choices of <code>alim</code>, <code>nulim</code>, and <code>vlim</code> determine the location and approximate size of candidate &quot;atoms&quot; or possible normal components. The defaults are sensible given that we scale the data.  Without scaling, you want to insure that <code>alim</code> is set for a wide enough range of values (remember a is a precision parameter) and the <code>v</code> is big enough to propose <code>Sigma</code> matrices wide enough to cover the data range.  
</p>
<p>A careful analyst should look at the posterior distribution of <code>a</code>, <code>nu</code>, <code>v</code> to make sure that the support is set correctly in <code>alim</code>, <code>nulim</code>, <code>vlim</code>.  In other words, if we see the posterior bunched up at one end of these support ranges, we should widen the range and rerun.  
</p>
<p>If you want to force the procedure to use many small atoms, then set <code>nulim</code> to consider only large values and set <code>vlim</code> to consider only small scaling constants. Set <code>Istarmax</code> to a large number.  This will create a very &quot;lumpy&quot; density estimate somewhat like the classical Kernel density estimates. Of course, this is not advised if you have a prior belief that densities are relatively smooth.
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(y)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>y: </code> </td><td style="text-align: left;"> <code class="reqn">n x k</code> matrix of observations on <code class="reqn">k</code> dimensional data
    </td>
</tr>

</table>

<p><em><code>Prior = list(Prioralpha, lambda_hyper)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>Prioralpha:  </code> </td><td style="text-align: left;"> <code>list(Istarmin, Istarmax, power)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$Istarmin:  </code> </td><td style="text-align: left;"> is expected number of components at lower bound of support of alpha (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$Istarmax:  </code> </td><td style="text-align: left;"> is expected number of components at upper bound of support of alpha (def: <code>min(50, 0.1*nrow(y))</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$power:     </code> </td><td style="text-align: left;"> is the power parameter for alpha prior (def: 0.8) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>lambda_hyper:</code> </td><td style="text-align: left;"> <code>list(alim, nulim, vlim)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$alim:      </code> </td><td style="text-align: left;"> defines support of a distribution (def: <code>c(0.01, 10)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$nulim:     </code> </td><td style="text-align: left;"> defines support of nu distribution (def: <code>c(0.01, 3)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$vlim:      </code> </td><td style="text-align: left;"> defines support of v distribution (def: <code>c(0.1, 4)</code>)
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, maxuniq, SCALE, gridsize)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
   <code>R:        </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>keep:     </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>nprint:   </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>maxuniq:  </code> </td><td style="text-align: left;"> storage constraint on the number of unique components (def: 200) </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>SCALE:    </code> </td><td style="text-align: left;"> should data be scaled by mean,std deviation before posterior draws (def: <code>TRUE</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>gridsize: </code> </td><td style="text-align: left;"> number of discrete points for hyperparameter priors (def: 20)
    </td>
</tr>

</table>




<h4><code>nmix</code> Details</h4>

<p><code>nmix</code> is a list with 3 components. Several functions in the <code>bayesm</code> package that involve a Dirichlet Process or mixture-of-normals return <code>nmix</code>. Across these functions, a common structure is used for <code>nmix</code> in order to utilize generic summary and plotting functions. 
</p>

<table>
<tr>
 <td style="text-align: left;">
  <code>probdraw:</code> </td><td style="text-align: left;"> <code class="reqn">ncomp x R/keep</code> matrix that reports the probability that each draw came from a particular component </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>zdraw:   </code> </td><td style="text-align: left;"> <code class="reqn">R/keep x nobs</code> matrix that indicates which component each draw is assigned to </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>compdraw:</code> </td><td style="text-align: left;"> A list of <code class="reqn">R/keep</code> lists of <code class="reqn">ncomp</code> lists. Each of the inner-most lists has 2 elemens: a vector of draws for <code>mu</code> and a matrix of draws for the Cholesky root of <code>Sigma</code>.
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>nmix</code></td>
<td>
<p> a list containing: <code>probdraw</code>, <code>zdraw</code>, <code>compdraw</code> (see &ldquo;<code>nmix</code> Details&rdquo; section)</p>
</td></tr>
<tr><td><code>alphadraw</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of alpha draws</p>
</td></tr>
<tr><td><code>nudraw</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of nu draws</p>
</td></tr>
<tr><td><code>adraw</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of a draws</p>
</td></tr>
<tr><td><code>vdraw</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of v draws</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rnmixGibbs">rnmixGibbs</a></code>, <code><a href="#topic+rmixture">rmixture</a></code>, <code><a href="#topic+rmixGibbs">rmixGibbs</a></code>, 
<code><a href="#topic+eMixMargDen">eMixMargDen</a></code>, <code><a href="#topic+momMix">momMix</a></code>, <code><a href="#topic+mixDen">mixDen</a></code>, <code><a href="#topic+mixDenBi">mixDenBi</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

## simulate univariate data from Chi-Sq

N = 200
chisqdf = 8
y1 = as.matrix(rchisq(N,df=chisqdf))

## set arguments for rDPGibbs

Data1 = list(y=y1)
Prioralpha = list(Istarmin=1, Istarmax=10, power=0.8)
Prior1 = list(Prioralpha=Prioralpha)
Mcmc = list(R=R, keep=1, maxuniq=200)

out1 = rDPGibbs(Prior=Prior1, Data=Data1, Mcmc=Mcmc)

if(0){
  ## plotting examples
  rgi = c(0,20)
  grid = matrix(seq(from=rgi[1],to=rgi[2],length.out=50), ncol=1)
  deltax = (rgi[2]-rgi[1]) / nrow(grid)
  plot(out1$nmix, Grid=grid, Data=y1)
  
  ## plot true density with historgram
  plot(range(grid[,1]), 1.5*range(dchisq(grid[,1],df=chisqdf)),
       type="n", xlab=paste("Chisq ; ",N," obs",sep=""), ylab="")
  hist(y1, xlim=rgi, freq=FALSE, col="yellow", breaks=20, add=TRUE)
  lines(grid[,1], dchisq(grid[,1],df=chisqdf) / (sum(dchisq(grid[,1],df=chisqdf))*deltax),
        col="blue", lwd=2)
}

## simulate bivariate data from the  "Banana" distribution (Meng and Barnard) 

banana = function(A, B, C1, C2, N, keep=10, init=10) { 
  R = init*keep + N*keep
  x1 = x2 = 0
  bimat = matrix(double(2*N), ncol=2)
  for (r in 1:R) { 
    x1 = rnorm(1,mean=(B*x2+C1) / (A*(x2^2)+1), sd=sqrt(1/(A*(x2^2)+1)))
    x2 = rnorm(1,mean=(B*x2+C2) / (A*(x1^2)+1), sd=sqrt(1/(A*(x1^2)+1)))
    if (r&gt;init*keep &amp;&amp; r%%keep==0) {
      mkeep = r/keep
      bimat[mkeep-init,] = c(x1,x2)
    } 
  }
  return(bimat)
}

set.seed(66)
nvar2 = 2
A = 0.5
B = 0
C1 = C2 = 3
y2 = banana(A=A, B=B, C1=C1, C2=C2, 1000)

Data2 = list(y=y2)
Prioralpha = list(Istarmin=1, Istarmax=10, power=0.8)
Prior2 = list(Prioralpha=Prioralpha)
Mcmc = list(R=R, keep=1, maxuniq=200)

out2 = rDPGibbs(Prior=Prior2, Data=Data2, Mcmc=Mcmc)


if(0){
  ## plotting examples
  
  rx1 = range(y2[,1])
  rx2 = range(y2[,2])
  x1 = seq(from=rx1[1], to=rx1[2], length.out=50)
  x2 = seq(from=rx2[1], to=rx2[2], length.out=50)
  grid = cbind(x1,x2)
  plot(out2$nmix, Grid=grid, Data=y2)
  
  ## plot true bivariate density
  tden = matrix(double(50*50), ncol=50)
  for (i in 1:50) { 
  for (j in 1:50) {
        tden[i,j] = exp(-0.5*(A*(x1[i]^2)*(x2[j]^2) + 
                    (x1[i]^2) + (x2[j]^2) - 2*B*x1[i]*x2[j] - 
                    2*C1*x1[i] - 2*C2*x2[j]))
  }}
  tden = tden / sum(tden)
  image(x1, x2, tden, col=terrain.colors(100), xlab="", ylab="")
  contour(x1, x2, tden, add=TRUE, drawlabels=FALSE)
  title("True Density")
}
</code></pre>

<hr>
<h2 id='rhierBinLogit'>MCMC Algorithm for Hierarchical Binary Logit</h2><span id='topic+rhierBinLogit'></span>

<h3>Description</h3>

<p><b>This function has been deprecated. Please use <code>rhierMnlRwMixture</code> instead.</b>
</p>
<p><code>rhierBinLogit</code> implements an MCMC algorithm for hierarchical binary logits with a normal heterogeneity distribution. This is a hybrid sampler with a RW Metropolis step for unit-level logit parameters. 
</p>
<p><code>rhierBinLogit</code> is designed for use on choice-based conjoint data with partial profiles. The Design matrix is based on differences of characteristics between two alternatives. See Appendix A of <em>Bayesian Statistics and Marketing</em> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rhierBinLogit(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rhierBinLogit_+3A_data">Data</code></td>
<td>
<p>list(lgtdata, Z)</p>
</td></tr>
<tr><td><code id="rhierBinLogit_+3A_prior">Prior</code></td>
<td>
<p>list(Deltabar, ADelta, nu, V)</p>
</td></tr>
<tr><td><code id="rhierBinLogit_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, sbeta)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">y_{hi} = 1</code> with <code class="reqn">Pr = exp(x_{hi}'\beta_h) / (1+exp(x_{hi}'\beta_h)</code> and <code class="reqn">\beta_h</code> is <code class="reqn">nvar x 1</code> <br />
<code class="reqn">h = 1, \ldots, length(lgtdata)</code> units (or &quot;respondents&quot; for survey data)
</p>
<p><code class="reqn">\beta_h</code> = ZDelta[h,] + <code class="reqn">u_h</code> <br />
Note: here ZDelta refers to <code>Z%*%Delta</code> with ZDelta[h,] the <code class="reqn">h</code>th row of this product<br />
Delta is an <code class="reqn">nz x nvar</code> array
</p>
<p><code class="reqn">u_h</code> <code class="reqn">\sim</code> <code class="reqn">N(0, V_{beta})</code>.  <br />
</p>
<p><code class="reqn">delta = vec(Delta)</code> <code class="reqn">\sim</code> <code class="reqn">N(vec(Deltabar), V_{beta}(x) ADelta^{-1})</code><br />
<code class="reqn">V_{beta}</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu, V)</code>
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(lgtdata, Z)</code> [<code>Z</code> optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>lgtdata:       </code> </td><td style="text-align: left;"> list of lists with each cross-section unit MNL data </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>lgtdata[[h]]$y:</code> </td><td style="text-align: left;"> <code class="reqn">n_h x 1</code> vector of binary outcomes (0,1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>lgtdata[[h]]$X:</code> </td><td style="text-align: left;"> <code class="reqn">n_h x nvar</code> design matrix for h'th unit </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Z:             </code> </td><td style="text-align: left;"> <code class="reqn">nreg x nz</code> mat of unit chars (def: vector of ones)
  </td>
</tr>

</table>

<p><em><code>Prior = list(Deltabar, ADelta, nu, V)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>Deltabar:</code> </td><td style="text-align: left;"> <code class="reqn">nz x nvar</code> matrix of prior means (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ADelta:  </code> </td><td style="text-align: left;"> prior precision matrix (def: 0.01I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:      </code> </td><td style="text-align: left;"> d.f. parameter for IW prior on normal component Sigma (def: nvar+3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>V:       </code> </td><td style="text-align: left;"> pds location parm for IW prior on normal component Sigma (def: nuI)
  </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, sbeta)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:       </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:    </code> </td><td style="text-align: left;"> MCMC thinning parm -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>sbeta:   </code> </td><td style="text-align: left;"> scaling parm for RW Metropolis (def: 0.2)
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>Deltadraw</code></td>
<td>
 <p><code class="reqn">R/keep x nz*nvar</code> matrix of draws of Delta</p>
</td></tr>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">nlgt x nvar x R/keep</code> array of draws of betas</p>
</td></tr>
<tr><td><code>Vbetadraw</code></td>
<td>
 <p><code class="reqn">R/keep x nvar*nvar</code> matrix of draws of Vbeta</p>
</td></tr>
<tr><td><code>llike</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of log-like values</p>
</td></tr>
<tr><td><code>reject</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of reject rates over nlgt units</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Some experimentation with the Metropolis scaling paramter (<code>sbeta</code>) may be required.</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 5, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rhierMnlRwMixture">rhierMnlRwMixture</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=10000} else {R=10}
set.seed(66)

nvar = 5              ## number of coefficients
nlgt = 1000           ## number of cross-sectional units
nobs = 10             ## number of observations per unit
nz = 2                ## number of regressors in mixing distribution

Z = matrix(c(rep(1,nlgt),runif(nlgt,min=-1,max=1)), nrow=nlgt, ncol=nz)
Delta = matrix(c(-2, -1, 0, 1, 2, -1, 1, -0.5, 0.5, 0), nrow=nz, ncol=nvar)
iota = matrix(1, nrow=nvar, ncol=1)
Vbeta = diag(nvar) + 0.5*iota%*%t(iota)

lgtdata=NULL
for (i in 1:nlgt) { 
  beta = t(Delta)%*%Z[i,] + as.vector(t(chol(Vbeta))%*%rnorm(nvar))
  X = matrix(runif(nobs*nvar), nrow=nobs, ncol=nvar)
  prob = exp(X%*%beta) / (1+exp(X%*%beta)) 
  unif = runif(nobs, 0, 1)
  y = ifelse(unif&lt;prob, 1, 0)
  lgtdata[[i]] = list(y=y, X=X, beta=beta)
}

Data1 = list(lgtdata=lgtdata, Z=Z)
Mcmc1 = list(R=R)

out = rhierBinLogit(Data=Data1, Mcmc=Mcmc1)

cat("Summary of Delta draws", fill=TRUE)
summary(out$Deltadraw, tvalues=as.vector(Delta))

cat("Summary of Vbeta draws", fill=TRUE)
summary(out$Vbetadraw, tvalues=as.vector(Vbeta[upper.tri(Vbeta,diag=TRUE)]))

if(0){
## plotting examples
plot(out$Deltadraw,tvalues=as.vector(Delta))
plot(out$betadraw)
plot(out$Vbetadraw,tvalues=as.vector(Vbeta[upper.tri(Vbeta,diag=TRUE)]))
}

</code></pre>

<hr>
<h2 id='rhierLinearMixture'>Gibbs Sampler for Hierarchical Linear Model with Mixture-of-Normals Heterogeneity</h2><span id='topic+rhierLinearMixture'></span>

<h3>Description</h3>

<p><code>rhierLinearMixture</code> implements a Gibbs Sampler for hierarchical linear models with a mixture-of-normals prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rhierLinearMixture(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rhierLinearMixture_+3A_data">Data</code></td>
<td>
<p>list(regdata, Z)</p>
</td></tr>
<tr><td><code id="rhierLinearMixture_+3A_prior">Prior</code></td>
<td>
<p>list(deltabar, Ad, mubar, Amu, nu, V, nu.e, ssq, ncomp)</p>
</td></tr>
<tr><td><code id="rhierLinearMixture_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code>nreg</code> regression equations with <code>nvar</code> as the number of <code class="reqn">X</code> vars in each equation <br />
<code class="reqn">y_i = X_i\beta_i + e_i</code> with <code class="reqn">e_i</code> <code class="reqn">\sim</code> <code class="reqn">N(0, \tau_i)</code>  
</p>
<p><code class="reqn">\tau_i</code> <code class="reqn">\sim</code> <code class="reqn">nu.e*ssq_i/\chi^2_{nu.e}</code> where  <code class="reqn">\tau_i</code> is the variance of <code class="reqn">e_i</code><br />
<code class="reqn">B = Z\Delta + U</code> or <code class="reqn">\beta_i = \Delta' Z[i,]' + u_i</code> <br />
<code class="reqn">\Delta</code> is an <code class="reqn">nz x nvar</code> matrix <br />
</p>
<p><code class="reqn">Z</code> should <em>not</em> include an intercept and should be centered for ease of interpretation. 
The mean of each of the <code>nreg</code> <code class="reqn">\beta</code>s is the mean of the normal mixture. 
Use <code>summary()</code> to compute this mean from the <code>compdraw</code> output.
</p>
<p><code class="reqn">u_i</code> <code class="reqn">\sim</code> <code class="reqn">N(\mu_{ind}, \Sigma_{ind})</code><br />
<code class="reqn">ind</code> <code class="reqn">\sim</code> <code class="reqn">multinomial(pvec)</code> <br />
</p>
<p><code class="reqn">pvec</code> <code class="reqn">\sim</code> <code class="reqn">dirichlet(a)</code><br />
<code class="reqn">delta = vec(\Delta)</code> <code class="reqn">\sim</code> <code class="reqn">N(deltabar, A_d^{-1})</code><br />
<code class="reqn">\mu_j</code> <code class="reqn">\sim</code> <code class="reqn">N(mubar, \Sigma_j(x) Amu^{-1})</code><br />
<code class="reqn">\Sigma_j</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu, V)</code> <br />
</p>
<p>Be careful in assessing the prior parameter <code>Amu</code>: 0.01 can be too small for some applications. 
See chapter 5 of Rossi et al for full discussion.<br />
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(regdata, Z)</code> [<code>Z</code> optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>regdata:        </code> </td><td style="text-align: left;"> list of lists with <code class="reqn">X</code> and <code class="reqn">y</code> matrices for each of <code>nreg=length(regdata)</code> regressions </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>regdata[[i]]$X: </code> </td><td style="text-align: left;"> <code class="reqn">n_i x nvar</code> design matrix for equation <code class="reqn">i</code>  </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>regdata[[i]]$y: </code> </td><td style="text-align: left;"> <code class="reqn">n_i x 1</code> vector of observations for equation <code class="reqn">i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Z:              </code> </td><td style="text-align: left;"> <code class="reqn">nreg x nz</code> matrix of unit characteristics (def: vector of ones)
    </td>
</tr>

</table>

<p><em><code>Prior = list(deltabar, Ad, mubar, Amu, nu, V, nu.e, ssq, ncomp)</code> [all but <code>ncomp</code> are optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>deltabar:       </code> </td><td style="text-align: left;"> <code class="reqn">nz x nvar</code> vector of prior means (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Ad:             </code> </td><td style="text-align: left;"> prior precision matrix for vec(Delta) (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>mubar:          </code> </td><td style="text-align: left;"> <code class="reqn">nvar x 1</code> prior mean vector for normal component mean (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Amu:            </code> </td><td style="text-align: left;"> prior precision for normal component mean (def: 0.01) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:             </code> </td><td style="text-align: left;"> d.f. parameter for IW prior on normal component Sigma (def: nvar+3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>V:              </code> </td><td style="text-align: left;"> PDS location parameter for IW prior on normal component Sigma (def: nu*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu.e:           </code> </td><td style="text-align: left;"> d.f. parameter for regression error variance prior (def: 3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ssq:            </code> </td><td style="text-align: left;"> scale parameter for regression error variance prior (def: <code>var(y_i)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>a:              </code> </td><td style="text-align: left;"> Dirichlet prior parameter (def: 5) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ncomp:          </code> </td><td style="text-align: left;"> number of components used in normal mixture
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:              </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:           </code> </td><td style="text-align: left;"> MCMC thinning parm -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:         </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print)
    </td>
</tr>

</table>




<h4><code>nmix</code> Details</h4>

<p><code>nmix</code> is a list with 3 components. Several functions in the <code>bayesm</code> package that involve a Dirichlet Process or mixture-of-normals return <code>nmix</code>. Across these functions, a common structure is used for <code>nmix</code> in order to utilize generic summary and plotting functions. 
</p>

<table>
<tr>
 <td style="text-align: left;">
  <code>probdraw:</code> </td><td style="text-align: left;"> <code class="reqn">ncomp x R/keep</code> matrix that reports the probability that each draw came from a particular component </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>zdraw:   </code> </td><td style="text-align: left;"> <code class="reqn">R/keep x nobs</code> matrix that indicates which component each draw is assigned to (here, null) </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>compdraw:</code> </td><td style="text-align: left;"> A list of <code class="reqn">R/keep</code> lists of <code class="reqn">ncomp</code> lists. Each of the inner-most lists has 2 elemens: a vector of draws for <code>mu</code> and a matrix of draws for the Cholesky root of <code>Sigma</code>.
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>taudraw</code></td>
<td>
<p><code class="reqn">R/keep x nreg</code> matrix of error variance draws</p>
</td></tr>
<tr><td><code>betadraw</code></td>
<td>
<p><code class="reqn">nreg x nvar x R/keep</code> array of individual regression coef draws</p>
</td></tr>
<tr><td><code>Deltadraw</code></td>
<td>
<p><code class="reqn">R/keep x nz*nvar</code> matrix of Deltadraws</p>
</td></tr>
<tr><td><code>nmix</code></td>
<td>
<p>a list containing: <code>probdraw</code>, <code>zdraw</code>, <code>compdraw</code> (see &ldquo;<code>nmix</code> Details&rdquo; section)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 5, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rhierLinearModel">rhierLinearModel</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

nreg = 300
nobs = 500
nvar = 3
nz = 2

Z = matrix(runif(nreg*nz), ncol=nz) 
Z = t(t(Z) - apply(Z,2,mean))
Delta = matrix(c(1,-1,2,0,1,0), ncol=nz)
tau0 = 0.1
iota = c(rep(1,nobs))

## create arguments for rmixture

tcomps = NULL
a = matrix(c(1,0,0,0.5773503,1.1547005,0,-0.4082483,0.4082483,1.2247449), ncol=3)
tcomps[[1]] = list(mu=c(0,-1,-2),   rooti=a) 
tcomps[[2]] = list(mu=c(0,-1,-2)*2, rooti=a)
tcomps[[3]] = list(mu=c(0,-1,-2)*4, rooti=a)
tpvec = c(0.4, 0.2, 0.4)

## simulated data with Z
regdata = NULL
betas = matrix(double(nreg*nvar), ncol=nvar)
tind = double(nreg)

for (reg in 1:nreg) {
  tempout = rmixture(1,tpvec,tcomps)
  betas[reg,] = Delta%*%Z[reg,] + as.vector(tempout$x)
  tind[reg] = tempout$z
  X = cbind(iota, matrix(runif(nobs*(nvar-1)),ncol=(nvar-1)))
  tau = tau0*runif(1,min=0.5,max=1)
  y = X%*%betas[reg,] + sqrt(tau)*rnorm(nobs)
  regdata[[reg]] = list(y=y, X=X, beta=betas[reg,], tau=tau)
}

## run rhierLinearMixture

Data1 = list(regdata=regdata, Z=Z)
Prior1 = list(ncomp=3)
Mcmc1 = list(R=R, keep=1)

out1 = rhierLinearMixture(Data=Data1, Prior=Prior1, Mcmc=Mcmc1)

cat("Summary of Delta draws", fill=TRUE)
summary(out1$Deltadraw, tvalues=as.vector(Delta))

cat("Summary of Normal Mixture Distribution", fill=TRUE)
summary(out1$nmix)

## plotting examples
if(0){
  plot(out1$betadraw)
  plot(out1$nmix)
  plot(out1$Deltadraw)
}
</code></pre>

<hr>
<h2 id='rhierLinearModel'>Gibbs Sampler for Hierarchical Linear Model with Normal Heterogeneity</h2><span id='topic+rhierLinearModel'></span>

<h3>Description</h3>

<p><code>rhierLinearModel</code> implements a Gibbs Sampler for hierarchical linear models with a normal prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rhierLinearModel(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rhierLinearModel_+3A_data">Data</code></td>
<td>
<p>list(regdata, Z)</p>
</td></tr>
<tr><td><code id="rhierLinearModel_+3A_prior">Prior</code></td>
<td>
<p>list(Deltabar, A, nu.e, ssq, nu, V)</p>
</td></tr>
<tr><td><code id="rhierLinearModel_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code>nreg</code> regression equations with <code class="reqn">nvar</code> <code class="reqn">X</code> variables in each equation <br />
<code class="reqn">y_i = X_i\beta_i + e_i</code> with <code class="reqn">e_i</code> <code class="reqn">\sim</code> <code class="reqn">N(0, \tau_i)</code> 
</p>
<p><code class="reqn">\tau_i</code> <code class="reqn">\sim</code> nu.e*<code class="reqn">ssq_i/\chi^2_{nu.e}</code> where <code class="reqn">\tau_i</code> is the variance of <code class="reqn">e_i</code><br />
<code class="reqn">\beta_i</code> <code class="reqn">\sim</code> N(Z<code class="reqn">\Delta</code>[i,], <code class="reqn">V_{\beta}</code>) <br />
Note:  Z<code class="reqn">\Delta</code> is the matrix <code class="reqn">Z * \Delta</code> and [i,] refers to <code class="reqn">i</code>th row of this product<br />
</p>
<p><code class="reqn">vec(\Delta)</code> given <code class="reqn">V_{\beta}</code> <code class="reqn">\sim</code> <code class="reqn">N(vec(Deltabar), V_{\beta}(x) A^{-1})</code><br />
<code class="reqn">V_{\beta}</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu,V)</code> <br />
<code class="reqn">Delta, Deltabar</code> are <code class="reqn">nz x nvar</code>; <code class="reqn">A</code> is <code class="reqn">nz x nz</code>; <code class="reqn">V_{\beta}</code> is <code class="reqn">nvar x nvar</code>.
</p>
<p>Note: if you don't have any <code class="reqn">Z</code> variables, omit <code class="reqn">Z</code> in the <code>Data</code> argument and 
a vector of ones will be inserted; the matrix <code class="reqn">\Delta</code> will be <code class="reqn">1 x nvar</code> and should 
be interpreted as the mean of all unit <code class="reqn">\beta</code>s.
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(regdata, Z)</code> [<code>Z</code> optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>regdata:        </code> </td><td style="text-align: left;"> list of lists with <code class="reqn">X</code> and <code class="reqn">y</code> matrices for each of <code>nreg=length(regdata)</code> regressions </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>regdata[[i]]$X: </code> </td><td style="text-align: left;"> <code class="reqn">n_i x nvar</code> design matrix for equation <code class="reqn">i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>regdata[[i]]$y: </code> </td><td style="text-align: left;"> <code class="reqn">n_i x 1</code> vector of observations for equation <code class="reqn">i</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Z:              </code> </td><td style="text-align: left;"> <code class="reqn">nreg x nz</code> matrix of unit characteristics (def: vector of ones)
    </td>
</tr>

</table>

<p><em><code>Prior = list(Deltabar, A, nu.e, ssq, nu, V)</code>  [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>Deltabar:       </code> </td><td style="text-align: left;"> <code class="reqn">nz x nvar</code> matrix of prior means (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>A:              </code> </td><td style="text-align: left;"> <code class="reqn">nz x nz</code> matrix for prior precision (def: 0.01I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu.e:           </code> </td><td style="text-align: left;"> d.f. parameter for regression error variance prior (def: 3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ssq:            </code> </td><td style="text-align: left;"> scale parameter for regression error var prior (def: <code>var(y_i)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:             </code> </td><td style="text-align: left;"> d.f. parameter for Vbeta prior (def: nvar+3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>V:              </code> </td><td style="text-align: left;"> Scale location matrix for Vbeta prior (def: nu*I)
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:              </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:           </code> </td><td style="text-align: left;"> MCMC thinning parm -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:         </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print)
    </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>betadraw</code></td>
<td>
<p><code class="reqn">nreg x nvar x R/keep</code> array of individual regression coef draws</p>
</td></tr>
<tr><td><code>taudraw</code></td>
<td>
<p><code class="reqn">R/keep x nreg</code> matrix of error variance draws</p>
</td></tr>
<tr><td><code>Deltadraw</code></td>
<td>
<p><code class="reqn">R/keep x nz*nvar</code> matrix of Deltadraws</p>
</td></tr>
<tr><td><code>Vbetadraw</code></td>
<td>
<p><code class="reqn">R/keep x nvar*nvar</code> matrix of Vbeta draws</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rhierLinearMixture">rhierLinearMixture</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

nreg = 100
nobs = 100
nvar = 3
Vbeta = matrix(c(1, 0.5, 0, 0.5, 2, 0.7, 0, 0.7, 1), ncol=3)

Z = cbind(c(rep(1,nreg)), 3*runif(nreg))
Z[,2] = Z[,2] - mean(Z[,2])
nz = ncol(Z)
Delta = matrix(c(1,-1,2,0,1,0), ncol=2)
Delta = t(Delta) # first row of Delta is means of betas
Beta = matrix(rnorm(nreg*nvar),nrow=nreg)%*%chol(Vbeta) + Z%*%Delta

tau = 0.1
iota = c(rep(1,nobs))
regdata = NULL
for (reg in 1:nreg) { 
  X = cbind(iota, matrix(runif(nobs*(nvar-1)),ncol=(nvar-1)))
	y = X%*%Beta[reg,] + sqrt(tau)*rnorm(nobs)
	regdata[[reg]] = list(y=y, X=X) 
}

Data1 = list(regdata=regdata, Z=Z)
Mcmc1 = list(R=R, keep=1)

out = rhierLinearModel(Data=Data1, Mcmc=Mcmc1)

cat("Summary of Delta draws", fill=TRUE)
summary(out$Deltadraw, tvalues=as.vector(Delta))

cat("Summary of Vbeta draws", fill=TRUE)
summary(out$Vbetadraw, tvalues=as.vector(Vbeta[upper.tri(Vbeta,diag=TRUE)]))

## plotting examples
if(0){
  plot(out$betadraw)
  plot(out$Deltadraw)
}
</code></pre>

<hr>
<h2 id='rhierMnlDP'>MCMC Algorithm for Hierarchical Multinomial Logit with Dirichlet Process Prior Heterogeneity</h2><span id='topic+rhierMnlDP'></span>

<h3>Description</h3>

<p><code>rhierMnlDP</code> is a MCMC algorithm for a hierarchical multinomial logit with a Dirichlet Process prior for the distribution of heteorogeneity.  A base normal model is used so that the DP can be interpreted as allowing for a mixture of normals with as many components as there are panel units. This is a hybrid Gibbs Sampler with a RW Metropolis step for the MNL coefficients for each panel unit.  This procedure can be interpreted as a Bayesian semi-parameteric method in the sense that the DP prior can accomodate heterogeniety of an unknown form.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rhierMnlDP(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rhierMnlDP_+3A_data">Data</code></td>
<td>
<p>list(lgtdata, Z, p)</p>
</td></tr>
<tr><td><code id="rhierMnlDP_+3A_prior">Prior</code></td>
<td>
<p>list(deltabar, Ad, Prioralpha, lambda_hyper)</p>
</td></tr>
<tr><td><code id="rhierMnlDP_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, s, w, maxunique, gridsize)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">y_i</code> <code class="reqn">\sim</code> <code class="reqn">MNL(X_i, \beta_i)</code> with <code class="reqn">i = 1, \ldots, length(lgtdata)</code> and where <code class="reqn">\theta_i</code> is <code class="reqn">nvar x 1</code>
</p>
<p><code class="reqn">\beta_i = Z\Delta</code>[i,] + <code class="reqn">u_i</code> <br />
Note:  Z<code class="reqn">\Delta</code> is the matrix <code class="reqn">Z * \Delta</code>; [i,] refers to <code class="reqn">i</code>th row of this product <br />
Delta is an <code class="reqn">nz x nvar</code> matrix 
</p>
<p><code class="reqn">\beta_i</code> <code class="reqn">\sim</code> <code class="reqn">N(\mu_i, \Sigma_i)</code>
</p>
<p><code class="reqn">\theta_i = (\mu_i, \Sigma_i)</code> <code class="reqn">\sim</code> <code class="reqn">DP(G_0(\lambda), alpha)</code><br />
</p>
<p><code class="reqn">G_0(\lambda):</code><br />
<code class="reqn">\mu_i | \Sigma_i</code> <code class="reqn">\sim</code> <code class="reqn">N(0, \Sigma_i (x) a^{-1})</code><br />
<code class="reqn">\Sigma_i</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu, nu*v*I)</code><br />
<code class="reqn">delta = vec(\Delta)</code> <code class="reqn">\sim</code> <code class="reqn">N(deltabar, A_d^{-1})</code><br />
</p>
<p><code class="reqn">\lambda(a, nu, v):</code><br />
<code class="reqn">a</code> <code class="reqn">\sim</code> uniform[alim[1], alimb[2]]<br />
<code class="reqn">nu</code> <code class="reqn">\sim</code>  dim(data)-1 + exp(z) <br />
<code class="reqn">z</code> <code class="reqn">\sim</code>  uniform[dim(data)-1+nulim[1], nulim[2]]<br />
<code class="reqn">v</code> <code class="reqn">\sim</code> uniform[vlim[1], vlim[2]]
</p>
<p><code class="reqn">alpha</code> <code class="reqn">\sim</code> <code class="reqn">(1-(alpha-alphamin) / (alphamax-alphamin))^{power}</code> <br />
alpha = alphamin then expected number of components = <code>Istarmin</code> <br />
alpha = alphamax then expected number of components = <code>Istarmax</code>
</p>
<p><code class="reqn">Z</code> should NOT include an intercept and is centered for ease of interpretation. The mean of each of the <code>nlgt</code> <code class="reqn">\beta</code>s is the mean of the normal mixture.  Use <code>summary()</code> to compute this mean from the <code>compdraw</code> output.
</p>
<p>We parameterize the prior on <code class="reqn">\Sigma_i</code> such that <code class="reqn">mode(\Sigma) = nu/(nu+2) vI</code>. The support of nu enforces a non-degenerate IW density; <code class="reqn">nulim[1] &gt; 0</code>.
</p>
<p>The default choices of alim, nulim, and vlim determine the location and approximate size of candidate &quot;atoms&quot; or possible normal components. The defaults are sensible given a reasonable scaling of the X variables. You want to insure that alim is set for a wide enough range of values (remember a is a precision parameter) and the v is big enough to propose Sigma matrices wide enough to cover the data range.  
</p>
<p>A careful analyst should look at the posterior distribution of a, nu, v to make sure that the support is set correctly in alim, nulim, vlim.  In other words, if we see the posterior bunched up at one end of these support ranges, we should widen the range and rerun.  
</p>
<p>If you want to force the procedure to use many small atoms, then set nulim to consider only large values and set vlim to consider only small scaling constants.  Set alphamax to a large number.  This will create a very &quot;lumpy&quot; density estimate somewhat like the classical Kernel density estimates. Of course, this is not advised if you have a prior belief that densities are relatively smooth.
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(lgtdata, Z, p)</code> [<code>Z</code> optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>lgtdata:        </code> </td><td style="text-align: left;"> list of lists with each cross-section unit MNL data </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>lgtdata[[i]]$y: </code> </td><td style="text-align: left;"> <code class="reqn">n_i x 1</code> vector of multinomial outcomes (1, ..., m) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>lgtdata[[i]]$X: </code> </td><td style="text-align: left;"> <code class="reqn">n_i x nvar</code> design matrix for <code class="reqn">i</code>th unit </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Z:              </code> </td><td style="text-align: left;"> <code class="reqn">nreg x nz</code> matrix of unit characteristics (def: vector of ones) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>p:              </code> </td><td style="text-align: left;"> number of choice alternatives
    </td>
</tr>

</table>

<p><em><code>Prior = list(deltabar, Ad, Prioralpha, lambda_hyper)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>deltabar:       </code> </td><td style="text-align: left;"> <code class="reqn">nz*nvar x 1</code> vector of prior means (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Ad:             </code> </td><td style="text-align: left;"> prior precision matrix for vec(D) (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Prioralpha:     </code> </td><td style="text-align: left;"> <code>list(Istarmin, Istarmax, power)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$Istarmin:    </code> </td><td style="text-align: left;"> expected number of components at lower bound of support of alpha def(1) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$Istarmax:    </code> </td><td style="text-align: left;"> expected number of components at upper bound of support of alpha (def: min(50, 0.1*nlgt)) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$power:       </code> </td><td style="text-align: left;"> power parameter for alpha prior (def: 0.8)  </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>lambda_hyper:   </code> </td><td style="text-align: left;"> <code>list(alim, nulim, vlim)</code>  </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$alim:        </code> </td><td style="text-align: left;"> defines support of a distribution (def: <code>c(0.01, 2)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$nulim:       </code> </td><td style="text-align: left;"> defines support of nu distribution (def: <code>c(0.001, 3)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>$vlim:        </code> </td><td style="text-align: left;"> defines support of v distribution (def: <code>c(0.1, 4)</code>) 
      </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, s, w, maxunique, gridsize)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:              </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:           </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:         </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>s:              </code> </td><td style="text-align: left;"> scaling parameter for RW Metropolis (def: 2.93/<code>sqrt(nvar)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>w:              </code> </td><td style="text-align: left;"> fractional likelihood weighting parameter (def: 0.1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>maxuniq:        </code> </td><td style="text-align: left;"> storage constraint on the number of unique components (def: 200) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>gridsize:       </code> </td><td style="text-align: left;"> number of discrete points for hyperparameter priors, (def: 20)
    </td>
</tr>

</table>




<h4><code>nmix</code> Details</h4>

<p><code>nmix</code> is a list with 3 components. Several functions in the <code>bayesm</code> package that involve a Dirichlet Process or mixture-of-normals return <code>nmix</code>. Across these functions, a common structure is used for <code>nmix</code> in order to utilize generic summary and plotting functions. 
</p>

<table>
<tr>
 <td style="text-align: left;">
  <code>probdraw:</code> </td><td style="text-align: left;"> <code class="reqn">ncomp x R/keep</code> matrix that reports the probability that each draw came from a particular component (here, a one-column matrix of 1s) </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>zdraw:   </code> </td><td style="text-align: left;"> <code class="reqn">R/keep x nobs</code> matrix that indicates which component each draw is assigned to (here, null) </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>compdraw:</code> </td><td style="text-align: left;"> A list of <code class="reqn">R/keep</code> lists of <code class="reqn">ncomp</code> lists. Each of the inner-most lists has 2 elemens: a vector of draws for <code>mu</code> and a matrix of draws for the Cholesky root of <code>Sigma</code>.
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>Deltadraw</code></td>
<td>
 <p><code class="reqn">R/keep x nz*nvar</code> matrix of draws of Delta, first row is initial value</p>
</td></tr>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">nlgt x nvar x R/keep</code> array of draws of betas</p>
</td></tr>
<tr><td><code>nmix</code></td>
<td>
<p> a list containing: <code>probdraw</code>, <code>zdraw</code>, <code>compdraw</code> (see &ldquo;<code>nmix</code> Details&rdquo; section)</p>
</td></tr>
<tr><td><code>adraw</code></td>
<td>
 <p><code class="reqn">R/keep</code> draws of hyperparm a</p>
</td></tr>
<tr><td><code>vdraw</code></td>
<td>
 <p><code class="reqn">R/keep</code> draws of hyperparm v</p>
</td></tr>
<tr><td><code>nudraw</code></td>
<td>
 <p><code class="reqn">R/keep</code> draws of hyperparm nu</p>
</td></tr>
<tr><td><code>Istardraw</code></td>
<td>
 <p><code class="reqn">R/keep</code> draws of number of unique components</p>
</td></tr>
<tr><td><code>alphadraw</code></td>
<td>
 <p><code class="reqn">R/keep</code> draws of number of DP tightness parameter</p>
</td></tr>
<tr><td><code>loglike</code></td>
<td>
 <p><code class="reqn">R/keep</code> draws of log-likelihood</p>
</td></tr>
</table>


<h3>Note</h3>

<p>As is well known, Bayesian density estimation involves computing the predictive distribution of a &quot;new&quot; unit parameter, <code class="reqn">\theta_{n+1}</code> (here &quot;n&quot;=nlgt). This is done by averaging the normal base distribution over draws from the distribution of <code class="reqn">\theta_{n+1}</code> given <code class="reqn">\theta_1</code>, ..., <code class="reqn">\theta_n</code>, alpha, lambda, data. To facilitate this, we store those draws from the predictive distribution of <code class="reqn">\theta_{n+1}</code> in a list structure compatible with other <code>bayesm</code> routines that implement a finite mixture of normals.
</p>
<p>Large <code>R</code> values may be required (&gt;20,000).
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 5, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rhierMnlRwMixture">rhierMnlRwMixture</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=20000} else {R=10}
set.seed(66)

p = 3                                # num of choice alterns
ncoef = 3  
nlgt = 300                           # num of cross sectional units
nz = 2
Z = matrix(runif(nz*nlgt),ncol=nz)
Z = t(t(Z)-apply(Z,2,mean))          # demean Z
ncomp = 3                            # no of mixture components
Delta=matrix(c(1,0,1,0,1,2),ncol=2)

comps = NULL
comps[[1]] = list(mu=c(0,-1,-2),   rooti=diag(rep(2,3)))
comps[[2]] = list(mu=c(0,-1,-2)*2, rooti=diag(rep(2,3)))
comps[[3]] = list(mu=c(0,-1,-2)*4, rooti=diag(rep(2,3)))
pvec=c(0.4, 0.2, 0.4)

##  simulate from MNL model conditional on X matrix
simmnlwX = function(n,X,beta) {
  k = length(beta)
  Xbeta = X%*%beta
  j = nrow(Xbeta) / n
  Xbeta = matrix(Xbeta, byrow=TRUE, ncol=j)
  Prob = exp(Xbeta)
  iota = c(rep(1,j))
  denom = Prob%*%iota
  Prob = Prob / as.vector(denom)
  y = vector("double", n)
  ind = 1:j
  for (i in 1:n) {
  yvec = rmultinom(1, 1, Prob[i,])
  y[i] = ind%*%yvec}
  return(list(y=y, X=X, beta=beta, prob=Prob))
}

## simulate data with a mixture of 3 normals
simlgtdata = NULL
ni = rep(50,300)
for (i in 1:nlgt) {
  betai = Delta%*%Z[i,] + as.vector(rmixture(1,pvec,comps)$x)
   Xa = matrix(runif(ni[i]*p,min=-1.5,max=0), ncol=p)
   X = createX(p, na=1, nd=NULL, Xa=Xa, Xd=NULL, base=1)
   outa = simmnlwX(ni[i], X, betai)
   simlgtdata[[i]] = list(y=outa$y, X=X, beta=betai)
}

## plot betas
if(0){
  bmat = matrix(0, nlgt, ncoef)
  for(i in 1:nlgt) { bmat[i,] = simlgtdata[[i]]$beta }
  par(mfrow = c(ncoef,1))
  for(i in 1:ncoef) { hist(bmat[,i], breaks=30, col="magenta") }
}

## set Data and Mcmc lists
keep = 5
Mcmc1 = list(R=R, keep=keep)
Data1 = list(p=p, lgtdata=simlgtdata, Z=Z)

out = rhierMnlDP(Data=Data1, Mcmc=Mcmc1)

cat("Summary of Delta draws", fill=TRUE)
summary(out$Deltadraw, tvalues=as.vector(Delta))

## plotting examples
if(0) {
  plot(out$betadraw)
  plot(out$nmix)
}
</code></pre>

<hr>
<h2 id='rhierMnlRwMixture'>MCMC Algorithm for Hierarchical Multinomial Logit with Mixture-of-Normals Heterogeneity</h2><span id='topic+rhierMnlRwMixture'></span>

<h3>Description</h3>

<p><code>rhierMnlRwMixture</code> is a MCMC algorithm for a hierarchical multinomial logit with a mixture of normals heterogeneity distribution. This is a hybrid Gibbs Sampler with a RW Metropolis step for the MNL coefficients for each panel unit. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rhierMnlRwMixture(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rhierMnlRwMixture_+3A_data">Data</code></td>
<td>
<p>list(lgtdata, Z, p)</p>
</td></tr>
<tr><td><code id="rhierMnlRwMixture_+3A_prior">Prior</code></td>
<td>
<p>list(a, deltabar, Ad, mubar, Amu, nu, V, a, ncomp, SignRes)</p>
</td></tr>
<tr><td><code id="rhierMnlRwMixture_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, s, w)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">y_i</code> <code class="reqn">\sim</code> <code class="reqn">MNL(X_i,\beta_i)</code> with <code class="reqn">i = 1, \ldots,</code> length(lgtdata) 
and where <code class="reqn">\beta_i</code> is <code class="reqn">nvar x 1</code>
</p>
<p><code class="reqn">\beta_i</code> = <code class="reqn">Z\Delta</code>[i,] + <code class="reqn">u_i</code> <br />
Note:  Z<code class="reqn">\Delta</code> is the matrix Z * <code class="reqn">\Delta</code> and [i,] refers to <code class="reqn">i</code>th row of this product <br />
Delta is an <code class="reqn">nz x nvar</code> array
</p>
<p><code class="reqn">u_i</code> <code class="reqn">\sim</code> <code class="reqn">N(\mu_{ind},\Sigma_{ind})</code> with <code class="reqn">ind</code> <code class="reqn">\sim</code> multinomial(pvec)
</p>
<p><code class="reqn">pvec</code>                <code class="reqn">\sim</code> dirichlet(a) <br />
<code class="reqn">delta = vec(\Delta)</code> <code class="reqn">\sim</code> <code class="reqn">N(deltabar, A_d^{-1})</code> <br />
<code class="reqn">\mu_j</code>               <code class="reqn">\sim</code> <code class="reqn">N(mubar, \Sigma_j (x) Amu^{-1})</code> <br />
<code class="reqn">\Sigma_j</code>            <code class="reqn">\sim</code> <code class="reqn">IW(nu, V)</code>
</p>
<p>Note: <code class="reqn">Z</code> should NOT include an intercept and is centered for ease of interpretation. 
The mean of each of the <code>nlgt</code> <code class="reqn">\beta</code>s is the mean of the normal mixture. 
Use <code>summary()</code> to compute this mean from the <code>compdraw</code> output.<br />
</p>
<p>Be careful in assessing prior parameter <code>Amu</code>: 0.01 is too small for many applications. 
See chapter 5 of Rossi et al for full discussion.
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(lgtdata, Z, p)</code> [<code>Z</code> optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>lgtdata:        </code> </td><td style="text-align: left;"> list of <code>nlgt=length(lgtdata)</code> lists with each cross-section unit MNL data </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>lgtdata[[i]]$y: </code> </td><td style="text-align: left;"> <code class="reqn">n_i x 1</code> vector of multinomial outcomes (1, ..., p) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>lgtdata[[i]]$X: </code> </td><td style="text-align: left;"> <code class="reqn">n_i*p x nvar</code> design matrix for <code class="reqn">i</code>th unit </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Z:              </code> </td><td style="text-align: left;"> <code class="reqn">nreg x nz</code> matrix of unit chars (def: vector of ones) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>p:              </code> </td><td style="text-align: left;"> number of choice alternatives 
    </td>
</tr>

</table>

<p><em><code>Prior = list(a, deltabar, Ad, mubar, Amu, nu, V, a, ncomp, SignRes)</code> [all but <code>ncomp</code> are optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>a:              </code> </td><td style="text-align: left;"> <code class="reqn">ncomp x 1</code> vector of Dirichlet prior parameters (def: <code>rep(5,ncomp)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>deltabar:       </code> </td><td style="text-align: left;"> <code class="reqn">nz*nvar x 1</code> vector of prior means (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Ad:             </code> </td><td style="text-align: left;"> prior precision matrix for vec(D) (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>mubar:          </code> </td><td style="text-align: left;"> <code class="reqn">nvar x 1</code> prior mean vector for normal component mean (def: 0 if unrestricted; 2 if restricted) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Amu:            </code> </td><td style="text-align: left;"> prior precision for normal component mean (def: 0.01 if unrestricted; 0.1 if restricted) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:             </code> </td><td style="text-align: left;"> d.f. parameter for IW prior on normal component Sigma (def: nvar+3 if unrestricted; nvar+15 if restricted) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>V:              </code> </td><td style="text-align: left;"> PDS location parameter for IW prior on normal component Sigma (def: nu*I if unrestricted; nu*D if restricted with d_pp = 4 if unrestricted and d_pp = 0.01 if restricted) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ncomp:          </code> </td><td style="text-align: left;"> number of components used in normal mixture </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>SignRes:        </code> </td><td style="text-align: left;"> <code class="reqn">nvar x 1</code> vector of sign restrictions on the coefficient estimates (def: <code>rep(0,nvar)</code>)
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, s, w)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:              </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:           </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:         </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>s:              </code> </td><td style="text-align: left;"> scaling parameter for RW Metropolis (def: 2.93/<code>sqrt(nvar)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>w:              </code> </td><td style="text-align: left;"> fractional likelihood weighting parameter (def: 0.1)
    </td>
</tr>

</table>




<h4>Sign Restrictions</h4>

<p>If <code class="reqn">\beta_ik</code> has a sign restriction: <code class="reqn">\beta_ik = SignRes[k] * exp(\beta*_ik)</code>
</p>
<p>To use sign restrictions on the coefficients, <code>SignRes</code> must be an <code class="reqn">nvar x 1</code> vector containing values of either 0, -1, or 1. The value 0 means there is no sign restriction, -1 ensures that the coefficient is <em>negative</em>, and 1 ensures that the coefficient is <em>positive</em>. For example, if <code>SignRes = c(0,1,-1)</code>, the first coefficient is unconstrained, the second will be positive, and the third will be negative.
</p>
<p>The sign restriction is implemented such that if the the <code class="reqn">k</code>'th <code class="reqn">\beta</code> has a non-zero sign restriction (i.e., it is constrained), we have <code class="reqn">\beta_k = SignRes[k] * exp(\beta*_k)</code>.
</p>
<p>The sign restrictions (if used) will be reflected in the <code>betadraw</code> output. However, the unconstrained mixture components are available in <code>nmix</code>. <b>Important:</b> Note that draws from <code>nmix</code> are distributed according to the mixture of normals but <b>not</b> the coefficients in <code>betadraw</code>.
</p>
<p>Care should be taken when selecting priors on any sign restricted coefficients. See related vignette for additional information.
</p>



<h4><code>nmix</code> Details</h4>

<p><code>nmix</code> is a list with 3 components. Several functions in the <code>bayesm</code> package that involve a Dirichlet Process or mixture-of-normals return <code>nmix</code>. Across these functions, a common structure is used for <code>nmix</code> in order to utilize generic summary and plotting functions. 
</p>

<table>
<tr>
 <td style="text-align: left;">
  <code>probdraw:</code> </td><td style="text-align: left;"> <code class="reqn">ncomp x R/keep</code> matrix that reports the probability that each draw came from a particular component </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>zdraw:   </code> </td><td style="text-align: left;"> <code class="reqn">R/keep x nobs</code> matrix that indicates which component each draw is assigned to (here, null) </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>compdraw:</code> </td><td style="text-align: left;"> A list of <code class="reqn">R/keep</code> lists of <code class="reqn">ncomp</code> lists. Each of the inner-most lists has 2 elemens: a vector of draws for <code>mu</code> and a matrix of draws for the Cholesky root of <code>Sigma</code>.
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>Deltadraw</code></td>
<td>
 <p><code class="reqn">R/keep x nz*nvar</code> matrix of draws of Delta, first row is initial value</p>
</td></tr>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">nlgt x nvar x R/keep</code> array of beta draws</p>
</td></tr>
<tr><td><code>nmix</code></td>
<td>
<p> a list containing: <code>probdraw</code>, <code>zdraw</code>, <code>compdraw</code> (see &ldquo;<code>nmix</code> Details&rdquo; section)</p>
</td></tr>
<tr><td><code>loglike</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of log-likelihood for each kept draw</p>
</td></tr>
<tr><td><code>SignRes</code></td>
<td>
 <p><code class="reqn">nvar x 1</code> vector of sign restrictions</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Note: as of version 2.0-2 of <code>bayesm</code>, the fractional weight parameter has been changed to a weight between 0 and 1. 
<code class="reqn">w</code> is the fractional weight on the normalized pooled likelihood. This differs from what is in Rossi et al chapter 5, i.e.
</p>
<p><code class="reqn">like_i^{(1-w)} x like_pooled^{((n_i/N)*w)}</code>
</p>
<p>Large <code>R</code> values may be required (&gt;20,000).
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 5, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rmnlIndepMetrop">rmnlIndepMetrop</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=10000} else {R=10}
set.seed(66)

p = 3                                # num of choice alterns
ncoef = 3  
nlgt = 300                           # num of cross sectional units
nz = 2
Z = matrix(runif(nz*nlgt),ncol=nz)
Z = t(t(Z) - apply(Z,2,mean))        # demean Z
ncomp = 3                            # num of mixture components
Delta = matrix(c(1,0,1,0,1,2),ncol=2)

comps=NULL
comps[[1]] = list(mu=c(0,-1,-2),   rooti=diag(rep(1,3)))
comps[[2]] = list(mu=c(0,-1,-2)*2, rooti=diag(rep(1,3)))
comps[[3]] = list(mu=c(0,-1,-2)*4, rooti=diag(rep(1,3)))
pvec = c(0.4, 0.2, 0.4)

##  simulate from MNL model conditional on X matrix
simmnlwX= function(n,X,beta) {
  k = length(beta)
  Xbeta = X%*%beta
  j = nrow(Xbeta) / n
  Xbeta = matrix(Xbeta, byrow=TRUE, ncol=j)
  Prob = exp(Xbeta)
  iota = c(rep(1,j))
  denom = Prob%*%iota
  Prob = Prob/as.vector(denom)
  y = vector("double",n)
  ind = 1:j
  for (i in 1:n) { 
    yvec = rmultinom(1, 1, Prob[i,])
    y[i] = ind%*%yvec
  }
  return(list(y=y, X=X, beta=beta, prob=Prob))
}

## simulate data
simlgtdata = NULL
ni = rep(50, 300)
for (i in 1:nlgt) {
  betai = Delta%*%Z[i,] + as.vector(rmixture(1,pvec,comps)$x)
   Xa = matrix(runif(ni[i]*p,min=-1.5,max=0), ncol=p)
   X = createX(p, na=1, nd=NULL, Xa=Xa, Xd=NULL, base=1)
   outa = simmnlwX(ni[i], X, betai)
   simlgtdata[[i]] = list(y=outa$y, X=X, beta=betai)
}

## plot betas
if(0){
  bmat = matrix(0, nlgt, ncoef)
  for(i in 1:nlgt) {bmat[i,] = simlgtdata[[i]]$beta}
  par(mfrow = c(ncoef,1))
  for(i in 1:ncoef) { hist(bmat[,i], breaks=30, col="magenta") }
}

## set parms for priors and Z
Prior1 = list(ncomp=5)
keep = 5
Mcmc1 = list(R=R, keep=keep)
Data1 = list(p=p, lgtdata=simlgtdata, Z=Z)

## fit model without sign constraints
out1 = rhierMnlRwMixture(Data=Data1, Prior=Prior1, Mcmc=Mcmc1)

cat("Summary of Delta draws", fill=TRUE)
summary(out1$Deltadraw, tvalues=as.vector(Delta))

cat("Summary of Normal Mixture Distribution", fill=TRUE)
summary(out1$nmix)

## plotting examples
if(0) {
  plot(out1$betadraw)
  plot(out1$nmix)
}

## fit model with constraint that beta_i2 &lt; 0 forall i
Prior2 = list(ncomp=5, SignRes=c(0,-1,0))
out2 = rhierMnlRwMixture(Data=Data1, Prior=Prior2, Mcmc=Mcmc1)

cat("Summary of Delta draws", fill=TRUE)
summary(out2$Deltadraw, tvalues=as.vector(Delta))

cat("Summary of Normal Mixture Distribution", fill=TRUE)
summary(out2$nmix)

## plotting examples
if(0) {
  plot(out2$betadraw)
  plot(out2$nmix)
}
</code></pre>

<hr>
<h2 id='rhierNegbinRw'>MCMC Algorithm for Hierarchical Negative Binomial Regression</h2><span id='topic+rhierNegbinRw'></span>

<h3>Description</h3>

<p><code>rhierNegbinRw</code> implements an MCMC algorithm for the hierarchical Negative Binomial (NBD) regression model. Metropolis steps for each unit-level set of regression parameters are automatically tuned by optimization. Over-dispersion parameter (alpha) is common across units.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rhierNegbinRw(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rhierNegbinRw_+3A_data">Data</code></td>
<td>
<p>list(regdata, Z)</p>
</td></tr>
<tr><td><code id="rhierNegbinRw_+3A_prior">Prior</code></td>
<td>
<p>list(Deltabar, Adelta, nu, V, a, b)</p>
</td></tr>
<tr><td><code id="rhierNegbinRw_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, s_beta, s_alpha, alpha, c, Vbeta0, Delta0)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">y_i</code> <code class="reqn">\sim</code> NBD(mean=<code class="reqn">\lambda</code>, over-dispersion=alpha) <br />
<code class="reqn">\lambda = exp(X_i\beta_i)</code>
</p>
<p><code class="reqn">\beta_i</code> <code class="reqn">\sim</code> <code class="reqn">N(\Delta'z_i,Vbeta)</code>
</p>
<p><code class="reqn">vec(\Delta|Vbeta)</code> <code class="reqn">\sim</code> <code class="reqn">N(vec(Deltabar), Vbeta (x) Adelta)</code> <br />
<code class="reqn">Vbeta</code>             <code class="reqn">\sim</code> <code class="reqn">IW(nu, V)</code> <br />
<code class="reqn">alpha</code>             <code class="reqn">\sim</code> <code class="reqn">Gamma(a, b)</code> (unless <code>Mcmc$alpha</code> specified) <br />
Note: prior mean of <code class="reqn">alpha = a/b</code>, variance <code class="reqn">= a/(b^2)</code>
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(regdata, Z)</code> [<code>Z</code> optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>regdata:        </code> </td><td style="text-align: left;"> list of lists with data on each of <code>nreg</code> units </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>regdata[[i]]$X: </code> </td><td style="text-align: left;"> <code class="reqn">nobs_i x nvar</code> matrix of <code class="reqn">X</code> variables </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>regdata[[i]]$y: </code> </td><td style="text-align: left;"> <code class="reqn">nobs_i x 1</code> vector of count responses </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Z:              </code> </td><td style="text-align: left;"> <code class="reqn">nreg x nz</code> matrix of unit characteristics (def: vector of ones)
    </td>
</tr>

</table>

<p><em><code>Prior = list(Deltabar, Adelta, nu, V, a, b)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>Deltabar:       </code> </td><td style="text-align: left;"> <code class="reqn">nz x nvar</code> prior mean matrix (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Adelta:         </code> </td><td style="text-align: left;"> <code class="reqn">nz x nz</code> PDS prior precision matrix (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:             </code> </td><td style="text-align: left;"> d.f. parameter for Inverted Wishart prior (def: nvar+3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>V:              </code> </td><td style="text-align: left;"> location matrix of Inverted Wishart prior (def: nu*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>a:              </code> </td><td style="text-align: left;"> Gamma prior parameter (def: 0.5) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>b:              </code> </td><td style="text-align: left;"> Gamma prior parameter (def: 0.1)
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, s_beta, s_alpha, alpha, c, Vbeta0, Delta0)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:              </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:           </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:         </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>s_beta:         </code> </td><td style="text-align: left;"> scaling for beta | alpha RW inc cov (def: 2.93/<code>sqrt(nvar)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>s_alpha:        </code> </td><td style="text-align: left;"> scaling for alpha | beta RW inc cov (def: 2.93) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>alpha:          </code> </td><td style="text-align: left;"> over-dispersion parameter (def: alpha ~ Gamma(a,b)) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>c:              </code> </td><td style="text-align: left;"> fractional likelihood weighting parm (def: 2) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Vbeta0:         </code> </td><td style="text-align: left;"> starting value for Vbeta (def: I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Delta0:         </code> </td><td style="text-align: left;"> starting value for Delta (def: 0)
    </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing: 
</p>
<table>
<tr><td><code>llike</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of values of log-likelihood</p>
</td></tr>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">nreg x nvar x R/keep</code> array of beta draws</p>
</td></tr>
<tr><td><code>alphadraw</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of alpha draws</p>
</td></tr>
<tr><td><code>acceptrbeta</code></td>
<td>
<p> acceptance rate of the beta draws</p>
</td></tr>
<tr><td><code>acceptralpha</code></td>
<td>
<p> acceptance rate of the alpha draws</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The NBD regression encompasses Poisson regression in the sense that as alpha goes to infinity the NBD distribution tends to the Poisson.
</p>
<p>For &quot;small&quot; values of alpha, the dependent variable can be extremely variable so that a large number of observations may be required to obtain precise inferences. 
</p>
<p>For ease of interpretation, we recommend demeaning <code class="reqn">Z</code> variables.
</p>


<h3>Author(s)</h3>

<p>Sridhar Narayanan (Stanford GSB) and Peter Rossi (Anderson School, UCLA), <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 5, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rnegbinRw">rnegbinRw</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

# Simulate from the Negative Binomial Regression
simnegbin = function(X, beta, alpha) {
  lambda = exp(X%*%beta)
  y = NULL
  for (j in 1:length(lambda)) {y = c(y, rnbinom(1, mu=lambda[j], size=alpha)) }
  return(y)
  }

nreg = 100        # Number of cross sectional units
T = 50            # Number of observations per unit
nobs = nreg*T
nvar = 2          # Number of X variables
nz = 2            # Number of Z variables
              
## Construct the Z matrix
Z = cbind(rep(1,nreg), rnorm(nreg,mean=1,sd=0.125))

Delta = cbind(c(4,2), c(0.1,-1))
alpha = 5
Vbeta = rbind(c(2,1), c(1,2))

## Construct the regdata (containing X)
simnegbindata = NULL
for (i in 1:nreg) {
    betai = as.vector(Z[i,]%*%Delta) + chol(Vbeta)%*%rnorm(nvar)
    X = cbind(rep(1,T),rnorm(T,mean=2,sd=0.25))
    simnegbindata[[i]] = list(y=simnegbin(X,betai,alpha), X=X, beta=betai)
}

Beta = NULL
for (i in 1:nreg) {Beta = rbind(Beta,matrix(simnegbindata[[i]]$beta,nrow=1))}
Data1 = list(regdata=simnegbindata, Z=Z)
Mcmc1 = list(R=R)

out = rhierNegbinRw(Data=Data1, Mcmc=Mcmc1)

cat("Summary of Delta draws", fill=TRUE)
summary(out$Deltadraw, tvalues=as.vector(Delta))

cat("Summary of Vbeta draws", fill=TRUE)
summary(out$Vbetadraw, tvalues=as.vector(Vbeta[upper.tri(Vbeta,diag=TRUE)]))

cat("Summary of alpha draws", fill=TRUE)
summary(out$alpha, tvalues=alpha)

## plotting examples
if(0){
  plot(out$betadraw)
  plot(out$alpha,tvalues=alpha)
  plot(out$Deltadraw,tvalues=as.vector(Delta))
}
</code></pre>

<hr>
<h2 id='rivDP'>Linear &quot;IV&quot; Model with DP Process Prior for Errors</h2><span id='topic+rivDP'></span>

<h3>Description</h3>

<p><code>rivDP</code> is a Gibbs Sampler for a linear structural equation with an arbitrary number of instruments. <code>rivDP</code> uses a mixture-of-normals for the structural and reduced form equations implemented with a Dirichlet Process prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rivDP(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rivDP_+3A_data">Data</code></td>
<td>
<p>list(y, x, w, z)</p>
</td></tr>
<tr><td><code id="rivDP_+3A_prior">Prior</code></td>
<td>
<p>list(md, Ad, mbg, Abg, lambda, Prioralpha, lambda_hyper)</p>
</td></tr>
<tr><td><code id="rivDP_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, maxuniq, SCALE, gridsize)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">x = z'\delta + e1</code> <br />
<code class="reqn">y = \beta*x + w'\gamma + e2</code> <br />
<code class="reqn">e1,e2</code> <code class="reqn">\sim</code> <code class="reqn">N(\theta_{i})</code> where <code class="reqn">\theta_{i}</code> represents <code class="reqn">\mu_{i}, \Sigma_{i}</code>
</p>
<p>Note: Error terms have non-zero means. 
DO NOT include intercepts in the <code class="reqn">z</code> or <code class="reqn">w</code> matrices.  
This is different from <code>rivGibbs</code> which requires intercepts to be included explicitly.
</p>
<p><code class="reqn">\delta</code> <code class="reqn">\sim</code> <code class="reqn">N(md, Ad^{-1})</code> <br />
<code class="reqn">vec(\beta, \gamma)</code> <code class="reqn">\sim</code> <code class="reqn">N(mbg, Abg^{-1})</code> <br />
<code class="reqn">\theta_{i}</code> <code class="reqn">\sim</code> <code class="reqn">G</code> <br />
<code class="reqn">G</code> <code class="reqn">\sim</code> <code class="reqn">DP(alpha, G_0)</code> 
</p>
<p><code class="reqn">alpha</code> <code class="reqn">\sim</code> <code class="reqn">(1-(alpha-alpha_{min})/(alpha_{max}-alpha{min}))^{power}</code> <br /> 
where <code class="reqn">alpha_{min}</code> and <code class="reqn">alpha_{max}</code> are set using the arguments in the reference below.  
It is highly recommended that you use the default values for the hyperparameters of the prior on alpha.
</p>
<p><code class="reqn">G_0</code> is the natural conjugate prior for <code class="reqn">(\mu,\Sigma)</code>: 
<code class="reqn">\Sigma</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu, vI)</code> and  <code class="reqn">\mu|\Sigma</code> <code class="reqn">\sim</code> <code class="reqn">N(0, \Sigma(x) a^{-1})</code> <br />
These parameters are collected together in the list <code class="reqn">\lambda</code>.  
It is highly recommended that you use the default settings for these hyper-parameters.<br />
</p>
<p><code class="reqn">\lambda(a, nu, v):</code><br />
<code class="reqn">a</code>  <code class="reqn">\sim</code> uniform[alim[1], alimb[2]]<br />
<code class="reqn">nu</code> <code class="reqn">\sim</code> dim(data)-1 + exp(z) <br />
<code class="reqn">z</code>  <code class="reqn">\sim</code> uniform[dim(data)-1+nulim[1], nulim[2]]<br />
<code class="reqn">v</code>  <code class="reqn">\sim</code> uniform[vlim[1], vlim[2]]
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(y, x, w, z)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>y: </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of obs on LHS variable in structural equation </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>x: </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of obs on "endogenous" variable in structural equation </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>w: </code> </td><td style="text-align: left;"> <code class="reqn">n x j</code> matrix of obs on "exogenous" variables in the structural equation </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>z: </code> </td><td style="text-align: left;"> <code class="reqn">n x p</code> matrix of obs on instruments
  </td>
</tr>

</table>

<p><em><code>Prior = list(md, Ad, mbg, Abg, lambda, Prioralpha, lambda_hyper)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>md:  </code> </td><td style="text-align: left;"> <code class="reqn">p</code>-length prior mean of delta (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Ad:  </code> </td><td style="text-align: left;"> <code class="reqn">p x p</code> PDS prior precision matrix for prior on delta (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>mbg: </code> </td><td style="text-align: left;"> <code class="reqn">(j+1)</code>-length prior mean vector for prior on beta,gamma (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Abg: </code> </td><td style="text-align: left;"> <code class="reqn">(j+1)x(j+1)</code> PDS prior precision matrix for prior on beta,gamma (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Prioralpha:</code> </td><td style="text-align: left;"> <code>list(Istarmin, Istarmax, power)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>$Istarmin: </code> </td><td style="text-align: left;">  is expected number of components at lower bound of support of alpha (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>$Istarmax: </code> </td><td style="text-align: left;">  is expected number of components at upper bound of support of alpha (def: <code>floor(0.1*length(y))</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>$power:    </code> </td><td style="text-align: left;">  is the power parameter for alpha prior (def: 0.8)  </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>lambda_hyper:</code> </td><td style="text-align: left;"> <code>list(alim, nulim, vlim)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>$alim:     </code> </td><td style="text-align: left;">  defines support of a distribution (def: <code>c(0.01, 10)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>$nulim:    </code> </td><td style="text-align: left;">  defines support of nu distribution (def: <code>c(0.01, 3)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>$vlim:     </code> </td><td style="text-align: left;">  defines support of v distribution (def: <code>c(0.1, 4)</code>) 
  </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, maxuniq, SCALE, gridsize)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:        </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:     </code> </td><td style="text-align: left;"> MCMC thinning parameter: keep every keepth draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:   </code> </td><td style="text-align: left;"> print the estimated time remaining for every nprint'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>maxuniq:  </code> </td><td style="text-align: left;"> storage constraint on the number of unique components (def: 200) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>SCALE:    </code> </td><td style="text-align: left;"> scale data (def: <code>TRUE</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>gridsize: </code> </td><td style="text-align: left;"> gridsize parameter for alpha draws (def: 20)
  </td>
</tr>

</table>




<h4><code>nmix</code> Details</h4>

<p><code>nmix</code> is a list with 3 components. Several functions in the <code>bayesm</code> package that involve a Dirichlet Process or mixture-of-normals return <code>nmix</code>. Across these functions, a common structure is used for <code>nmix</code> in order to utilize generic summary and plotting functions. 
</p>

<table>
<tr>
 <td style="text-align: left;">
  <code>probdraw:</code> </td><td style="text-align: left;"> <code class="reqn">ncomp x R/keep</code> matrix that reports the probability that each draw came from a particular component (here, a one-column matrix of 1s) </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>zdraw:   </code> </td><td style="text-align: left;"> <code class="reqn">R/keep x nobs</code> matrix that indicates which component each draw is assigned to (here, null) </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>compdraw:</code> </td><td style="text-align: left;"> A list of <code class="reqn">R/keep</code> lists of <code class="reqn">ncomp</code> lists. Each of the inner-most lists has 2 elemens: a vector of draws for <code>mu</code> and a matrix of draws for the Cholesky root of <code>Sigma</code>.
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>deltadraw</code></td>
<td>
 <p><code class="reqn">R/keep x p</code> array of delta draws</p>
</td></tr>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of beta draws</p>
</td></tr>
<tr><td><code>alphadraw</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of draws of Dirichlet Process tightness parameter</p>
</td></tr>
<tr><td><code>Istardraw</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of draws of the number of unique normal components</p>
</td></tr>
<tr><td><code>gammadraw</code></td>
<td>
 <p><code class="reqn">R/keep x j</code> array of gamma draws</p>
</td></tr>
<tr><td><code>nmix</code></td>
<td>
<p> a list containing: <code>probdraw</code>, <code>zdraw</code>, <code>compdraw</code> (see &ldquo;<code>nmix</code> Details&rdquo; section)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see &quot;A Semi-Parametric Bayesian Approach to the Instrumental Variable Problem,&quot; by Conley, Hansen, McCulloch and Rossi, <em>Journal of Econometrics</em> (2008). 
</p>
<p>See also, Chapter 4, <em>Bayesian Non- and Semi-parametric Methods and Applications</em> by Peter Rossi.
</p>


<h3>See Also</h3>

<p><code>rivGibbs</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

## simulate scaled log-normal errors and run
k = 10
delta = 1.5
Sigma = matrix(c(1, 0.6, 0.6, 1), ncol=2)
N = 1000
tbeta = 4
scalefactor = 0.6
root = chol(scalefactor*Sigma)
mu = c(1,1)

## compute interquartile ranges
ninterq = qnorm(0.75) - qnorm(0.25)
error = matrix(rnorm(100000*2), ncol=2)%*%root
error = t(t(error)+mu)
Err = t(t(exp(error))-exp(mu+0.5*scalefactor*diag(Sigma)))
lnNinterq = quantile(Err[,1], prob=0.75) - quantile(Err[,1], prob=0.25)

## simulate data
error = matrix(rnorm(N*2), ncol=2)%*%root
error = t(t(error)+mu)
Err = t(t(exp(error))-exp(mu+0.5*scalefactor*diag(Sigma)))

## scale appropriately  
Err[,1] = Err[,1]*ninterq/lnNinterq
Err[,2] = Err[,2]*ninterq/lnNinterq
z = matrix(runif(k*N), ncol=k)
x = z%*%(delta*c(rep(1,k))) + Err[,1]
y = x*tbeta + Err[,2]

## specify data input and mcmc parameters
Data = list(); 
Data$z = z
Data$x = x
Data$y = y

Mcmc = list()
Mcmc$maxuniq = 100
Mcmc$R = R
end = Mcmc$R

out = rivDP(Data=Data, Mcmc=Mcmc)

cat("Summary of Beta draws", fill=TRUE)
summary(out$betadraw, tvalues=tbeta)

## plotting examples
if(0){
  plot(out$betadraw, tvalues=tbeta)
  plot(out$nmix)  # plot "fitted" density of the errors
}
</code></pre>

<hr>
<h2 id='rivGibbs'>Gibbs Sampler for Linear &quot;IV&quot; Model</h2><span id='topic+rivGibbs'></span>

<h3>Description</h3>

<p><code>rivGibbs</code> is a Gibbs Sampler for a linear structural equation with an arbitrary number of instruments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rivGibbs(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rivGibbs_+3A_data">Data</code></td>
<td>
<p>list(y, x, w, z)</p>
</td></tr>
<tr><td><code id="rivGibbs_+3A_prior">Prior</code></td>
<td>
<p>list(md, Ad, mbg, Abg, nu, V)</p>
</td></tr>
<tr><td><code id="rivGibbs_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">x = z'\delta + e1</code> <br />
<code class="reqn">y = \beta*x + w'\gamma + e2</code> <br />
<code class="reqn">e1,e2</code> <code class="reqn">\sim</code> <code class="reqn">N(0, \Sigma)</code> 
</p>
<p>Note: if intercepts are desired in either equation, include vector of ones in <code class="reqn">z</code> or <code class="reqn">w</code>
</p>
<p><code class="reqn">\delta</code>            <code class="reqn">\sim</code> <code class="reqn">N(md, Ad^{-1})</code> <br />
<code class="reqn">vec(\beta,\gamma)</code> <code class="reqn">\sim</code> <code class="reqn">N(mbg, Abg^{-1})</code> <br />
<code class="reqn">\Sigma</code>            <code class="reqn">\sim</code> <code class="reqn">IW(nu, V)</code>
</p>



<h4>Argument Details</h4>

<p><em><code>Data = list(y, x, w, z)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>y:      </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of obs on LHS variable in structural equation </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>x:      </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of obs on "endogenous" variable in structural equation </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>w:      </code> </td><td style="text-align: left;"> <code class="reqn">n x j</code> matrix of obs on "exogenous" variables in the structural equation </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>z:      </code> </td><td style="text-align: left;"> <code class="reqn">n x p</code> matrix of obs on instruments
  </td>
</tr>

</table>

<p><em><code>Prior = list(md, Ad, mbg, Abg, nu, V)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>md:     </code> </td><td style="text-align: left;"> <code class="reqn">p</code>-length prior mean of delta (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Ad:     </code> </td><td style="text-align: left;"> <code class="reqn">p x p</code> PDS prior precision matrix for prior on delta (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>mbg:    </code> </td><td style="text-align: left;"> <code class="reqn">(j+1)</code>-length prior mean vector for prior on beta,gamma (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Abg:    </code> </td><td style="text-align: left;"> <code class="reqn">(j+1)x(j+1)</code> PDS prior precision matrix for prior on beta,gamma (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:     </code> </td><td style="text-align: left;"> d.f. parameter for Inverted Wishart prior on Sigma (def: 5) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>V:      </code> </td><td style="text-align: left;"> <code class="reqn">2 x 2</code> location matrix for Inverted Wishart prior on Sigma (def: nu*I)
  </td>
</tr>

</table>

<p><em><code>Mcmc = list(R, keep, nprint)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:      </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:   </code> </td><td style="text-align: left;"> MCMC thinning parameter: keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint: </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print)
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>deltadraw</code></td>
<td>
 <p><code class="reqn">R/keep x p</code> matrix of delta draws</p>
</td></tr>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of beta draws</p>
</td></tr>
<tr><td><code>gammadraw</code></td>
<td>
 <p><code class="reqn">R/keep x j</code> matrix of gamma draws</p>
</td></tr>
<tr><td><code>Sigmadraw</code></td>
<td>
 <p><code class="reqn">R/keep x 4</code> matrix of Sigma draws &ndash; each row is the vector form of Sigma</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Rob McCulloch and Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p> For further discussion, see Chapter 5, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

simIV = function(delta, beta, Sigma, n, z, w, gamma) {
  eps = matrix(rnorm(2*n),ncol=2) %*% chol(Sigma)
  x = z%*%delta + eps[,1]
  y = beta*x + eps[,2] + w%*%gamma
  list(x=as.vector(x), y=as.vector(y)) 
  }
  
n = 200
p=1 # number of instruments
z = cbind(rep(1,n), matrix(runif(n*p),ncol=p))
w = matrix(1,n,1)
rho = 0.8
Sigma = matrix(c(1,rho,rho,1), ncol=2)
delta = c(1,4)
beta = 0.5
gamma = c(1)
simiv = simIV(delta, beta, Sigma, n, z, w, gamma)

Data1 = list(); Data1$z = z; Data1$w=w; Data1$x=simiv$x; Data1$y=simiv$y
Mcmc1=list(); Mcmc1$R = R; Mcmc1$keep=1

out = rivGibbs(Data=Data1, Mcmc=Mcmc1)

cat("Summary of Beta draws", fill=TRUE)
summary(out$betadraw, tvalues=beta)

cat("Summary of Sigma draws", fill=TRUE)
summary(out$Sigmadraw, tvalues=as.vector(Sigma[upper.tri(Sigma,diag=TRUE)]))

## plotting examples
if(0){plot(out$betadraw)}
</code></pre>

<hr>
<h2 id='rmixGibbs'> Gibbs Sampler for Normal Mixtures w/o Error Checking</h2><span id='topic+rmixGibbs'></span>

<h3>Description</h3>

<p><code>rmixGibbs</code> makes one draw using the Gibbs Sampler for a mixture of multivariate normals. <code>rmixGibbs</code> is not designed to be called directly. Instead, use <code>rnmixGibbs</code> wrapper function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmixGibbs(y, Bbar, A, nu, V, a, p, z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmixGibbs_+3A_y">y</code></td>
<td>
<p> data array where rows are obs </p>
</td></tr>
<tr><td><code id="rmixGibbs_+3A_bbar">Bbar</code></td>
<td>
<p> prior mean for mean vector of each norm comp </p>
</td></tr>
<tr><td><code id="rmixGibbs_+3A_a">A</code></td>
<td>
<p> prior precision parameter</p>
</td></tr>
<tr><td><code id="rmixGibbs_+3A_nu">nu</code></td>
<td>
<p> prior d.f. parm </p>
</td></tr>
<tr><td><code id="rmixGibbs_+3A_v">V</code></td>
<td>
<p> prior location matrix for covariance prior </p>
</td></tr>
<tr><td><code id="rmixGibbs_+3A_a">a</code></td>
<td>
<p> Dirichlet prior parms </p>
</td></tr>
<tr><td><code id="rmixGibbs_+3A_p">p</code></td>
<td>
<p> prior prob of each mixture component </p>
</td></tr>
<tr><td><code id="rmixGibbs_+3A_z">z</code></td>
<td>
<p> component identities for each observation &ndash; &quot;indicators&quot; </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing:
</p>
<table>
<tr><td><code>p</code></td>
<td>
<p> draw of mixture probabilities</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p> draw of indicators of each component</p>
</td></tr>
<tr><td><code>comps</code></td>
<td>
<p> new draw of normal component parameters </p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Rob McCulloch (Arizona State University) and Peter Rossi (Anderson School, UCLA), <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 5 <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rnmixGibbs">rnmixGibbs</a></code> </p>

<hr>
<h2 id='rmixture'>Draw from Mixture of Normals</h2><span id='topic+rmixture'></span>

<h3>Description</h3>

<p><code>rmixture</code> simulates iid draws from a Multivariate Mixture of Normals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmixture(n, pvec, comps)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmixture_+3A_n">n</code></td>
<td>
<p> number of observations </p>
</td></tr>
<tr><td><code id="rmixture_+3A_pvec">pvec</code></td>
<td>
 <p><code class="reqn">ncomp x 1</code> vector of prior probabilities for each mixture component </p>
</td></tr>
<tr><td><code id="rmixture_+3A_comps">comps</code></td>
<td>
<p> list of mixture component parameters </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>comps</code> is a list of length <code>ncomp</code> with <code>ncomp = length(pvec)</code>. <br />
<code>comps[[j]][[1]]</code> is mean vector for the <code class="reqn">j</code>th component. <br />
<code>comps[[j]][[2]]</code> is the inverse of the cholesky root of <code class="reqn">\Sigma</code> for <code class="reqn">j</code>th component
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>x:</code></td>
<td>
<p> an <code class="reqn">n x</code> <code>length(comps[[1]][[1]])</code> array of iid draws </p>
</td></tr>
<tr><td><code>z:</code></td>
<td>
<p> an <code class="reqn">n x 1</code> vector of indicators of which component each draw is taken from </p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rnmixGibbs">rnmixGibbs</a></code> </p>

<hr>
<h2 id='rmnlIndepMetrop'>MCMC Algorithm for Multinomial Logit Model</h2><span id='topic+rmnlIndepMetrop'></span>

<h3>Description</h3>

<p><code>rmnlIndepMetrop</code> implements Independence Metropolis algorithm for the multinomial logit (MNL) model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmnlIndepMetrop(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmnlIndepMetrop_+3A_data">Data</code></td>
<td>
<p>list(y, X, p)</p>
</td></tr>
<tr><td><code id="rmnlIndepMetrop_+3A_prior">Prior</code></td>
<td>
<p>list(A, betabar)</p>
</td></tr>
<tr><td><code id="rmnlIndepMetrop_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, nu)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p>y <code class="reqn">\sim</code> MNL(X, <code class="reqn">\beta</code>) <br />
<code class="reqn">Pr(y=j) = exp(x_j'\beta)/\sum_k{e^{x_k'\beta}}</code>
</p>
<p><code class="reqn">\beta</code> <code class="reqn">\sim</code> <code class="reqn">N(betabar, A^{-1})</code>
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(y, X, p)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>y:       </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of multinomial outcomes (1, ..., p) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>X:       </code> </td><td style="text-align: left;"> <code class="reqn">n*p x k</code> matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>p:       </code> </td><td style="text-align: left;"> number of alternatives
  </td>
</tr>

</table>

<p><em><code>Prior = list(A, betabar)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>A:       </code> </td><td style="text-align: left;"> <code class="reqn">k x k</code> prior precision matrix (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>betabar: </code> </td><td style="text-align: left;"> <code class="reqn">k x 1</code> prior mean (def: 0)
  </td>
</tr>

</table>

<p><em><code>Mcmc = list(R, keep, nprint, nu)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:       </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:    </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:  </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:      </code> </td><td style="text-align: left;"> d.f. parameter for independent t density (def: 6)
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing: 
</p>
<table>
<tr><td><code>betadraw</code></td>
<td>
<p><code class="reqn">R/keep x k</code> matrix of beta draws</p>
</td></tr>
<tr><td><code>loglike</code></td>
<td>
<p><code class="reqn">R/keep x 1</code> vector of log-likelihood values evaluated at each draw</p>
</td></tr>
<tr><td><code>acceptr</code></td>
<td>
<p>acceptance rate of Metropolis draws</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rhierMnlRwMixture">rhierMnlRwMixture</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

simmnl = function(p, n, beta) {
  ## note: create X array with 2 alt.spec vars
    k = length(beta)
    X1 = matrix(runif(n*p,min=-1,max=1), ncol=p)
    X2 = matrix(runif(n*p,min=-1,max=1), ncol=p)
    X = createX(p, na=2, nd=NULL, Xd=NULL, Xa=cbind(X1,X2), base=1)
    Xbeta = X%*%beta 
  ## now do probs
    p = nrow(Xbeta) / n
    Xbeta = matrix(Xbeta, byrow=TRUE, ncol=p)
    Prob = exp(Xbeta)
    iota = c(rep(1,p))
    denom = Prob%*%iota
    Prob = Prob / as.vector(denom)
  ## draw y
    y = vector("double",n)
    ind = 1:p
    for (i in 1:n) { 
      yvec = rmultinom(1, 1, Prob[i,])
      y[i] = ind%*%yvec 
    }
  return(list(y=y, X=X, beta=beta, prob=Prob))
}

n = 200
p = 3
beta = c(1, -1, 1.5, 0.5)

simout = simmnl(p,n,beta)

Data1 = list(y=simout$y, X=simout$X, p=p)
Mcmc1 = list(R=R, keep=1)

out = rmnlIndepMetrop(Data=Data1, Mcmc=Mcmc1)

cat("Summary of beta draws", fill=TRUE)
summary(out$betadraw, tvalues=beta)

## plotting examples
if(0){plot(out$betadraw)}
</code></pre>

<hr>
<h2 id='rmnpGibbs'>Gibbs Sampler for Multinomial Probit</h2><span id='topic+rmnpGibbs'></span>

<h3>Description</h3>

<p><code>rmnpGibbs</code> implements the McCulloch/Rossi Gibbs Sampler for the multinomial probit model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmnpGibbs(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmnpGibbs_+3A_data">Data</code></td>
<td>
<p>list(y, X, p)</p>
</td></tr>
<tr><td><code id="rmnpGibbs_+3A_prior">Prior</code></td>
<td>
<p>list(betabar, A, nu, V)</p>
</td></tr>
<tr><td><code id="rmnpGibbs_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, beta0, sigma0)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">w_i = X_i\beta + e</code> with <code class="reqn">e</code> <code class="reqn">\sim</code> <code class="reqn">N(0, \Sigma)</code>. 
Note: <code class="reqn">w_i</code> and <code class="reqn">e</code> are <code class="reqn">(p-1) x 1</code>.<br />
<code class="reqn">y_i = j</code> if <code class="reqn">w_{ij} &gt; max(0,w_{i,-j})</code> for <code class="reqn">j=1, \ldots, p-1</code> and 
where <code class="reqn">w_{i,-j}</code> means elements of <code class="reqn">w_i</code> other than the <code class="reqn">j</code>th. <br />
<code class="reqn">y_i = p</code>,  if all <code class="reqn">w_i &lt; 0</code>
</p>
<p><code class="reqn">\beta</code> is not identified. However, <code class="reqn">\beta</code>/sqrt(<code class="reqn">\sigma_{11}</code>) and 
<code class="reqn">\Sigma</code>/<code class="reqn">\sigma_{11}</code> are identified.  See reference or example below for details.
</p>
<p><code class="reqn">\beta</code>  <code class="reqn">\sim</code> <code class="reqn">N(betabar,A^{-1})</code> <br />
<code class="reqn">\Sigma</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu,V)</code> <br />
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(y, X, p)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>y:       </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of multinomial outcomes (1, ..., p) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>X:       </code> </td><td style="text-align: left;"> <code class="reqn">n*(p-1) x k</code> design matrix. To make <code class="reqn">X</code> matrix use <code><a href="#topic+createX">createX</a></code> with <code>DIFF=TRUE</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>p:       </code> </td><td style="text-align: left;"> number of alternatives
    </td>
</tr>

</table>

<p><em><code>Prior = list(betabar, A, nu, V)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>betabar: </code> </td><td style="text-align: left;"> <code class="reqn">k x 1</code> prior mean (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>A:       </code> </td><td style="text-align: left;"> <code class="reqn">k x k</code> prior precision matrix (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:      </code> </td><td style="text-align: left;"> d.f. parameter for Inverted Wishart prior (def: (p-1)+3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>V:       </code> </td><td style="text-align: left;"> PDS location parameter for Inverted Wishart prior (def: nu*I) 
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, beta0, sigma0)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:       </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:    </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:  </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>beta0:   </code> </td><td style="text-align: left;"> initial value for beta (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>sigma0:  </code> </td><td style="text-align: left;"> initial value for sigma (def: I)
    </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing: 
</p>
<table>
<tr><td><code>betadraw</code></td>
<td>
<p><code class="reqn">R/keep x k</code> matrix of betadraws</p>
</td></tr>
<tr><td><code>sigmadraw</code></td>
<td>
<p><code class="reqn">R/keep x (p-1)*(p-1)</code> matrix of sigma draws &ndash; each row is the vector form of sigma</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 4, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rmvpGibbs">rmvpGibbs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

simmnp = function(X, p, n, beta, sigma) {
  indmax = function(x) {which(max(x)==x)}
  Xbeta = X%*%beta
  w = as.vector(crossprod(chol(sigma),matrix(rnorm((p-1)*n),ncol=n))) + Xbeta
  w = matrix(w, ncol=(p-1), byrow=TRUE)
  maxw = apply(w, 1, max)
  y = apply(w, 1, indmax)
  y = ifelse(maxw &lt; 0, p, y)
  return(list(y=y, X=X, beta=beta, sigma=sigma))
}

p = 3
n = 500
beta = c(-1,1,1,2)
Sigma = matrix(c(1, 0.5, 0.5, 1), ncol=2)
k = length(beta)
X1 = matrix(runif(n*p,min=0,max=2),ncol=p)
X2 = matrix(runif(n*p,min=0,max=2),ncol=p)
X = createX(p, na=2, nd=NULL, Xa=cbind(X1,X2), Xd=NULL, DIFF=TRUE, base=p)

simout = simmnp(X,p,500,beta,Sigma)

Data1 = list(p=p, y=simout$y, X=simout$X)
Mcmc1 = list(R=R, keep=1)

out = rmnpGibbs(Data=Data1, Mcmc=Mcmc1)

cat(" Summary of Betadraws ", fill=TRUE)
betatilde = out$betadraw / sqrt(out$sigmadraw[,1])
attributes(betatilde)$class = "bayesm.mat"
summary(betatilde, tvalues=beta)

cat(" Summary of Sigmadraws ", fill=TRUE)
sigmadraw = out$sigmadraw / out$sigmadraw[,1]
attributes(sigmadraw)$class = "bayesm.var"
summary(sigmadraw, tvalues=as.vector(Sigma[upper.tri(Sigma,diag=TRUE)]))

## plotting examples
if(0){plot(betatilde,tvalues=beta)}
</code></pre>

<hr>
<h2 id='rmultireg'>Draw from the Posterior of a Multivariate Regression</h2><span id='topic+rmultireg'></span>

<h3>Description</h3>

<p><code> rmultireg</code> draws from the posterior of a Multivariate Regression model with a natural conjugate prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmultireg(Y, X, Bbar, A, nu, V)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmultireg_+3A_y">Y</code></td>
<td>
 <p><code class="reqn">n x m</code> matrix of observations on m dep vars </p>
</td></tr>
<tr><td><code id="rmultireg_+3A_x">X</code></td>
<td>
 <p><code class="reqn">n x k</code> matrix of observations on indep vars (supply intercept) </p>
</td></tr>
<tr><td><code id="rmultireg_+3A_bbar">Bbar</code></td>
<td>
 <p><code class="reqn">k x m</code> matrix of prior mean of regression coefficients </p>
</td></tr>
<tr><td><code id="rmultireg_+3A_a">A</code></td>
<td>
 <p><code class="reqn">k x k</code> Prior precision matrix </p>
</td></tr>
<tr><td><code id="rmultireg_+3A_nu">nu</code></td>
<td>
<p> d.f. parameter for Sigma </p>
</td></tr>
<tr><td><code id="rmultireg_+3A_v">V</code></td>
<td>
 <p><code class="reqn">m x m</code> pdf location parameter for prior on Sigma </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Model: <br />
<code class="reqn">Y = XB + U</code> with <code class="reqn">cov(u_i) = \Sigma</code> <br />
<code class="reqn">B</code> is <code class="reqn">k x m</code> matrix of coefficients; <code class="reqn">\Sigma</code> is <code class="reqn">m x m</code> covariance matrix.
</p>
<p>Priors: <br />
<code class="reqn">\beta</code> | <code class="reqn">\Sigma</code> <code class="reqn">\sim</code> <code class="reqn">N(betabar, \Sigma(x) A^{-1})</code> <br />
<code class="reqn">betabar = vec(Bbar)</code>;  <code class="reqn">\beta = vec(B)</code> <br />
<code class="reqn">\Sigma</code> <code class="reqn">\sim</code> IW(nu, V) 
</p>


<h3>Value</h3>

<p>A list of the components of a draw from the posterior
</p>
<table>
<tr><td><code>B</code></td>
<td>
<p> draw of regression coefficient matrix </p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p> draw of Sigma </p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 2, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

n =200
m = 2
X = cbind(rep(1,n),runif(n))
k = ncol(X)
B = matrix(c(1,2,-1,3), ncol=m)
Sigma = matrix(c(1, 0.5, 0.5, 1), ncol=m)
RSigma = chol(Sigma)
Y = X%*%B + matrix(rnorm(m*n),ncol=m)%*%RSigma

betabar = rep(0,k*m)
Bbar = matrix(betabar, ncol=m)
A = diag(rep(0.01,k))
nu = 3
V = nu*diag(m)

betadraw = matrix(double(R*k*m), ncol=k*m)
Sigmadraw = matrix(double(R*m*m), ncol=m*m)

for (rep in 1:R) {
  out = rmultireg(Y, X, Bbar, A, nu, V)
  betadraw[rep,] = out$B
  Sigmadraw[rep,] = out$Sigma
  }

cat(" Betadraws ", fill=TRUE)
mat = apply(betadraw, 2, quantile, probs=c(0.01, 0.05, 0.5, 0.95, 0.99))
mat = rbind(as.vector(B),mat)
rownames(mat)[1] = "beta"
print(mat)

cat(" Sigma draws", fill=TRUE)
mat = apply(Sigmadraw, 2 ,quantile, probs=c(0.01, 0.05, 0.5, 0.95, 0.99))
mat = rbind(as.vector(Sigma),mat); rownames(mat)[1]="Sigma"
print(mat)
</code></pre>

<hr>
<h2 id='rmvpGibbs'>Gibbs Sampler for Multivariate Probit</h2><span id='topic+rmvpGibbs'></span>

<h3>Description</h3>

<p><code>rmvpGibbs</code> implements the Edwards/Allenby Gibbs Sampler for the multivariate probit model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmvpGibbs(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmvpGibbs_+3A_data">Data</code></td>
<td>
<p>list(y, X, p)</p>
</td></tr>
<tr><td><code id="rmvpGibbs_+3A_prior">Prior</code></td>
<td>
<p>list(betabar, A, nu, V)</p>
</td></tr>
<tr><td><code id="rmvpGibbs_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, beta0 ,sigma0)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">w_i = X_i\beta + e</code> with <code class="reqn">e</code> <code class="reqn">\sim</code> N(0,<code class="reqn">\Sigma</code>). Note: <code class="reqn">w_i</code> is <code class="reqn">p x 1</code>. <br />
<code class="reqn">y_{ij} = 1</code> if <code class="reqn">w_{ij} &gt; 0</code>, else <code class="reqn">y_i = 0</code>.  <code class="reqn">j = 1, \ldots, p</code> <br />
</p>
<p>beta and Sigma are not identifed. Correlation matrix and the betas divided by the appropriate standard deviation are. 
See reference or example below for details.
</p>
<p><code class="reqn">\beta</code>  <code class="reqn">\sim</code> <code class="reqn">N(betabar, A^{-1})</code> <br />
<code class="reqn">\Sigma</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu, V)</code> <br />
</p>
<p>To make <code class="reqn">X</code> matrix use <code>createX</code>
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(y, X, p)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>X:       </code> </td><td style="text-align: left;"> <code class="reqn">n*p x k</code> Design Matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>y:       </code> </td><td style="text-align: left;"> <code class="reqn">n*p x 1</code> vector of 0/1 outcomes </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>p:       </code> </td><td style="text-align: left;"> dimension of multivariate probit
    </td>
</tr>

</table>

<p><em><code>Prior = list(betabar, A, nu, V)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>betabar: </code> </td><td style="text-align: left;"> <code class="reqn">k x 1</code> prior mean (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>A:       </code> </td><td style="text-align: left;"> <code class="reqn">k x k</code> prior precision matrix (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:      </code> </td><td style="text-align: left;"> d.f. parameter for Inverted Wishart prior (def: (p-1)+3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>V:       </code> </td><td style="text-align: left;"> PDS location parameter for Inverted Wishart prior (def: nu*I) 
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, beta0 ,sigma0)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:       </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:    </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:  </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>beta0:   </code> </td><td style="text-align: left;"> initial value for beta </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>sigma0:  </code> </td><td style="text-align: left;"> initial value for sigma
    </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing: 
</p>
<table>
<tr><td><code>betadraw</code></td>
<td>
<p><code class="reqn">R/keep x k</code> matrix of betadraws</p>
</td></tr>
<tr><td><code>sigmadraw</code></td>
<td>
<p><code class="reqn">R/keep x p*p</code> matrix of sigma draws &ndash; each row is the vector form of sigma</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 4, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rmnpGibbs">rmnpGibbs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

simmvp = function(X, p, n, beta, sigma) {
  w = as.vector(crossprod(chol(sigma),matrix(rnorm(p*n),ncol=n))) + X%*%beta
  y = ifelse(w&lt;0, 0, 1)
  return(list(y=y, X=X, beta=beta, sigma=sigma))
}

p = 3
n = 500
beta = c(-2,0,2)
Sigma = matrix(c(1, 0.5, 0.5, 0.5, 1, 0.5, 0.5, 0.5, 1), ncol=3)
k = length(beta)
I2 = diag(rep(1,p))
xadd = rbind(I2)
for(i in 2:n) { xadd=rbind(xadd,I2) }
X = xadd

simout = simmvp(X,p,500,beta,Sigma)

Data1 = list(p=p, y=simout$y, X=simout$X)
Mcmc1 = list(R=R, keep=1)

out = rmvpGibbs(Data=Data1, Mcmc=Mcmc1)

ind = seq(from=0, by=p, length=k)
inda = 1:3
ind = ind + inda
cat(" Betadraws ", fill=TRUE)
betatilde = out$betadraw / sqrt(out$sigmadraw[,ind])
attributes(betatilde)$class = "bayesm.mat"
summary(betatilde, tvalues=beta/sqrt(diag(Sigma)))

rdraw = matrix(double((R)*p*p), ncol=p*p)
rdraw = t(apply(out$sigmadraw, 1, nmat))
attributes(rdraw)$class = "bayesm.var"
tvalue = nmat(as.vector(Sigma))
dim(tvalue) = c(p,p)
tvalue = as.vector(tvalue[upper.tri(tvalue,diag=TRUE)])
cat(" Draws of Correlation Matrix ", fill=TRUE)
summary(rdraw, tvalues=tvalue)

## plotting examples
if(0){plot(betatilde, tvalues=beta/sqrt(diag(Sigma)))}
</code></pre>

<hr>
<h2 id='rmvst'> Draw from Multivariate Student-t </h2><span id='topic+rmvst'></span>

<h3>Description</h3>

<p><code>rmvst</code> draws from a multivariate student-t distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmvst(nu, mu, root)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmvst_+3A_nu">nu</code></td>
<td>
<p> d.f. parameter </p>
</td></tr>
<tr><td><code id="rmvst_+3A_mu">mu</code></td>
<td>
<p> mean vector </p>
</td></tr>
<tr><td><code id="rmvst_+3A_root">root</code></td>
<td>
<p> Upper Tri Cholesky Root of Sigma </p>
</td></tr>
</table>


<h3>Value</h3>

<p>length(mu) draw vector</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see <em>Bayesian Statistics and Marketing</em>  by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+lndMvst">lndMvst</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(66)
rmvst(nu=5, mu=c(rep(0,2)), root=chol(matrix(c(2,1,1,2), ncol=2)))
</code></pre>

<hr>
<h2 id='rnegbinRw'>MCMC Algorithm for Negative Binomial Regression</h2><span id='topic+rnegbinRw'></span>

<h3>Description</h3>

<p><code>rnegbinRw</code> implements a Random Walk Metropolis Algorithm for the Negative Binomial (NBD) regression model where <code class="reqn">\beta|\alpha</code> and <code class="reqn">\alpha|\beta</code> are drawn with two different random walks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnegbinRw(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rnegbinRw_+3A_data">Data</code></td>
<td>
<p>list(y, X)</p>
</td></tr>
<tr><td><code id="rnegbinRw_+3A_prior">Prior</code></td>
<td>
<p>list(betabar, A, a, b)</p>
</td></tr>
<tr><td><code id="rnegbinRw_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, s_beta, s_alpha, beta0, alpha)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">y</code> <code class="reqn">\sim</code> <code class="reqn">NBD(mean=\lambda, over-dispersion=alpha)</code>  <br />
<code class="reqn">\lambda = exp(x'\beta)</code>
</p>
<p><code class="reqn">\beta</code> <code class="reqn">\sim</code> <code class="reqn">N(betabar, A^{-1})</code> <br />
<code class="reqn">alpha</code> <code class="reqn">\sim</code> <code class="reqn">Gamma(a, b)</code> (unless <code>Mcmc$alpha</code> specified) <br />
Note: prior mean of <code class="reqn">alpha = a/b</code>, <code class="reqn">variance = a/(b^2)</code>
</p>



<h4>Argument Details</h4>

<p><em><code>Data = list(y, X)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>y:       </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of counts (<code class="reqn">0,1,2,\ldots</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>X:       </code> </td><td style="text-align: left;"> <code class="reqn">n x k</code> design matrix
    </td>
</tr>

</table>

<p><em><code>Prior = list(betabar, A, a, b)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>betabar: </code> </td><td style="text-align: left;"> <code class="reqn">k x 1</code> prior mean (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>A:       </code> </td><td style="text-align: left;"> <code class="reqn">k x k</code> PDS prior precision matrix (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>a:       </code> </td><td style="text-align: left;"> Gamma prior parameter (not used if <code>Mcmc$alpha</code> specified) (def: 0.5) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>b:       </code> </td><td style="text-align: left;"> Gamma prior parameter (not used if <code>Mcmc$alpha</code> specified) (def: 0.1)
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, s_beta, s_alpha, beta0, alpha)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:       </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:    </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:  </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>s_beta:  </code> </td><td style="text-align: left;"> scaling for beta | alpha RW inc cov matrix (def: 2.93/<code>sqrt(k)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>s_alpha: </code> </td><td style="text-align: left;"> scaling for alpha | beta RW inc cov matrix (def: 2.93) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>alpha:   </code> </td><td style="text-align: left;"> over-dispersion parameter (def: alpha ~ Gamma(a,b))
    </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing: 
</p>
<table>
<tr><td><code>betadraw</code></td>
<td>
<p><code class="reqn">R/keep x k</code> matrix of beta draws</p>
</td></tr>
<tr><td><code>alphadraw</code></td>
<td>
<p><code class="reqn">R/keep x 1</code> vector of alpha draws</p>
</td></tr>
<tr><td><code>llike</code></td>
<td>
<p><code class="reqn">R/keep x 1</code> vector of log-likelihood values evaluated at each draw</p>
</td></tr>
<tr><td><code>acceptrbeta</code></td>
<td>
<p>acceptance rate of the beta draws</p>
</td></tr>
<tr><td><code>acceptralpha</code></td>
<td>
<p>acceptance rate of the alpha draws</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The NBD regression encompasses Poisson regression in the sense that as alpha goes to infinity the NBD distribution tends toward the Poisson. For &quot;small&quot; values of alpha, the dependent variable can be extremely variable so that a large number of observations may be required to obtain precise inferences.
</p>


<h3>Author(s)</h3>

<p>Sridhar Narayanan (Stanford GSB) and Peter Rossi (Anderson School, UCLA), <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rhierNegbinRw">rhierNegbinRw</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0)  {R=1000} else {R=10}
set.seed(66)

simnegbin = function(X, beta, alpha) {
  # Simulate from the Negative Binomial Regression
  lambda = exp(X%*%beta)
  y = NULL
  for (j in 1:length(lambda)) { y = c(y, rnbinom(1, mu=lambda[j], size=alpha)) }
  return(y)
}

nobs = 500
nvar = 2 # Number of X variables
alpha = 5
Vbeta = diag(nvar)*0.01

# Construct the regdata (containing X)
simnegbindata = NULL
beta = c(0.6, 0.2)
X = cbind(rep(1,nobs), rnorm(nobs,mean=2,sd=0.5))
simnegbindata = list(y=simnegbin(X,beta,alpha), X=X, beta=beta)

Data1 = simnegbindata
Mcmc1 = list(R=R)

out = rnegbinRw(Data=Data1, Mcmc=list(R=R))

cat("Summary of alpha/beta draw", fill=TRUE)
summary(out$alphadraw, tvalues=alpha)
summary(out$betadraw, tvalues=beta)

## plotting examples
if(0){plot(out$betadraw)}
</code></pre>

<hr>
<h2 id='rnmixGibbs'>Gibbs Sampler for Normal Mixtures</h2><span id='topic+rnmixGibbs'></span>

<h3>Description</h3>

<p><code>rnmixGibbs</code> implements a Gibbs Sampler for normal mixtures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnmixGibbs(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rnmixGibbs_+3A_data">Data</code></td>
<td>
<p>list(y)</p>
</td></tr>
<tr><td><code id="rnmixGibbs_+3A_prior">Prior</code></td>
<td>
<p>list(Mubar, A, nu, V, a, ncomp)</p>
</td></tr>
<tr><td><code id="rnmixGibbs_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, Loglike)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">y_i</code> <code class="reqn">\sim</code> <code class="reqn">N(\mu_{ind_i}, \Sigma_{ind_i})</code> <br />
ind <code class="reqn">\sim</code> iid multinomial(p) with <code class="reqn">p</code> an <code class="reqn">ncomp x 1</code> vector of probs
</p>
<p><code class="reqn">\mu_j</code>    <code class="reqn">\sim</code> <code class="reqn">N(mubar, \Sigma_j (x) A^{-1})</code> with <code class="reqn">mubar=vec(Mubar)</code> <br />
<code class="reqn">\Sigma_j</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu, V)</code> <br />
Note: this is the natural conjugate prior &ndash; a special case of multivariate regression <br />
</p>
<p><code class="reqn">p</code> <code class="reqn">\sim</code> Dirchlet(a)
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(y)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>y:       </code> </td><td style="text-align: left;"> <code class="reqn">n x k</code> matrix of data (rows are obs)
    </td>
</tr>

</table>

<p><em><code>Prior = list(Mubar, A, nu, V, a, ncomp)</code> [only <code>ncomp</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>Mubar:   </code> </td><td style="text-align: left;"> <code class="reqn">1 x k</code> vector with prior mean of normal component means (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>A:       </code> </td><td style="text-align: left;"> <code class="reqn">1 x 1</code> precision parameter for prior on mean of normal component (def: 0.01) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:      </code> </td><td style="text-align: left;"> d.f. parameter for prior on Sigma (normal component cov matrix) (def: k+3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>V:       </code> </td><td style="text-align: left;"> <code class="reqn">k x k</code> location matrix of IW prior on Sigma (def: nu*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>a:       </code> </td><td style="text-align: left;"> <code class="reqn">ncomp x 1</code> vector of Dirichlet prior parameters (def: <code>rep(5,ncomp)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ncomp:   </code> </td><td style="text-align: left;"> number of normal components to be included
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, Loglike)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:       </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:    </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:  </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>LogLike: </code> </td><td style="text-align: left;"> logical flag for whether to compute the log-likelihood (def: <code>FALSE</code>)
    </td>
</tr>

</table>




<h4><code>nmix</code> Details</h4>

<p><code>nmix</code> is a list with 3 components. Several functions in the <code>bayesm</code> package that involve a Dirichlet Process or mixture-of-normals return <code>nmix</code>. Across these functions, a common structure is used for <code>nmix</code> in order to utilize generic summary and plotting functions. 
</p>

<table>
<tr>
 <td style="text-align: left;">
  <code>probdraw:</code> </td><td style="text-align: left;"> <code class="reqn">ncomp x R/keep</code> matrix that reports the probability that each draw came from a particular component </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>zdraw:   </code> </td><td style="text-align: left;"> <code class="reqn">R/keep x nobs</code> matrix that indicates which component each draw is assigned to </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>compdraw:</code> </td><td style="text-align: left;"> A list of <code class="reqn">R/keep</code> lists of <code class="reqn">ncomp</code> lists. Each of the inner-most lists has 2 elemens: a vector of draws for <code>mu</code> and a matrix of draws for the Cholesky root of <code>Sigma</code>.
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>ll</code></td>
<td>
 <p><code class="reqn">R/keep x 1</code> vector of log-likelihood values</p>
</td></tr>
<tr><td><code>nmix</code></td>
<td>
<p> a list containing: <code>probdraw</code>, <code>zdraw</code>, <code>compdraw</code> (see &ldquo;<code>nmix</code> Details&rdquo; section)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>In this model, the component normal parameters are not-identified due to label-switching. However, the fitted mixture of normals density is identified as it is invariant to label-switching. See chapter 5 of Rossi et al below for details. 
</p>
<p>Use <code>eMixMargDen</code> or <code>momMix</code> to compute posterior expectation or distribution of various identified parameters.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rmixture">rmixture</a></code>, <code><a href="#topic+rmixGibbs">rmixGibbs</a></code> ,<code><a href="#topic+eMixMargDen">eMixMargDen</a></code>, <code><a href="#topic+momMix">momMix</a></code>,
<code><a href="#topic+mixDen">mixDen</a></code>, <code><a href="#topic+mixDenBi">mixDenBi</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

dim = 5
k = 3   # dimension of simulated data and number of "true" components
sigma = matrix(rep(0.5,dim^2), nrow=dim)
diag(sigma) = 1
sigfac = c(1,1,1)
mufac = c(1,2,3)
compsmv = list()
for(i in 1:k) compsmv[[i]] = list(mu=mufac[i]*1:dim, sigma=sigfac[i]*sigma)

# change to "rooti" scale
comps = list() 
for(i in 1:k) comps[[i]] = list(mu=compsmv[[i]][[1]], rooti=solve(chol(compsmv[[i]][[2]])))
pvec = (1:k) / sum(1:k)

nobs = 500
dm = rmixture(nobs, pvec, comps)

Data1 = list(y=dm$x)
ncomp = 9
Prior1 = list(ncomp=ncomp)
Mcmc1 = list(R=R, keep=1)

out = rnmixGibbs(Data=Data1, Prior=Prior1, Mcmc=Mcmc1)

cat("Summary of Normal Mixture Distribution", fill=TRUE)
summary(out$nmix)

tmom = momMix(matrix(pvec,nrow=1), list(comps))
mat = rbind(tmom$mu, tmom$sd)
cat(" True Mean/Std Dev", fill=TRUE)
print(mat)

## plotting examples
if(0){plot(out$nmix,Data=dm$x)}
</code></pre>

<hr>
<h2 id='rordprobitGibbs'>Gibbs Sampler for Ordered Probit</h2><span id='topic+rordprobitGibbs'></span>

<h3>Description</h3>

<p><code>rordprobitGibbs</code> implements a Gibbs Sampler for the ordered probit model with a RW Metropolis step for the cut-offs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rordprobitGibbs(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rordprobitGibbs_+3A_data">Data</code></td>
<td>
<p>list(y, X, k)</p>
</td></tr>
<tr><td><code id="rordprobitGibbs_+3A_prior">Prior</code></td>
<td>
<p>list(betabar, A, dstarbar, Ad)</p>
</td></tr>
<tr><td><code id="rordprobitGibbs_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, s)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">z = X\beta + e</code> with <code class="reqn">e</code> <code class="reqn">\sim</code> <code class="reqn">N(0, I)</code><br /> 
<code class="reqn">y = k</code> if c[k] <code class="reqn">\le z &lt;</code> c[k+1] with <code class="reqn">k = 1,\ldots,K</code> <br />
cutoffs = {c[1], <code class="reqn">\ldots</code>, c[k+1]} 
</p>
<p><code class="reqn">\beta</code> <code class="reqn">\sim</code> <code class="reqn">N(betabar, A^{-1})</code> <br />
<code class="reqn">dstar</code> <code class="reqn">\sim</code> <code class="reqn">N(dstarbar, Ad^{-1})</code>
</p>
<p>Be careful in assessing prior parameter <code>Ad</code>:  0.1 is too small for many applications.
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(y, X, k)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>y:        </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of observations, (<code class="reqn">1, \ldots, k</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>X:        </code> </td><td style="text-align: left;"> <code class="reqn">n x p</code> Design Matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>k:        </code> </td><td style="text-align: left;"> the largest possible value of y
    </td>
</tr>

</table>

<p><em><code>Prior = list(betabar, A, dstarbar, Ad)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>betabar:  </code> </td><td style="text-align: left;"> <code class="reqn">p x 1</code> prior mean (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>A:        </code> </td><td style="text-align: left;"> <code class="reqn">p x p</code> prior precision matrix (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>dstarbar: </code> </td><td style="text-align: left;"> <code class="reqn">ndstar x 1</code> prior mean,  where <code class="reqn">ndstar=k-2</code> (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Ad:       </code> </td><td style="text-align: left;"> <code class="reqn">ndstar x ndstar</code> prior precision matrix (def: I)
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, s)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:        </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:     </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:   </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>s:        </code> </td><td style="text-align: left;"> scaling parameter for RW Metropolis (def: 2.93/<code>sqrt(p)</code>)
    </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing: 
</p>
<table>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">R/keep x p</code> matrix of betadraws</p>
</td></tr>
<tr><td><code>cutdraw</code></td>
<td>
 <p><code class="reqn">R/keep x (k-1)</code> matrix of cutdraws</p>
</td></tr>
<tr><td><code>dstardraw</code></td>
<td>
 <p><code class="reqn">R/keep x (k-2)</code> matrix of dstardraws</p>
</td></tr>
<tr><td><code>accept</code></td>
<td>
<p> acceptance rate of Metropolis draws for cut-offs</p>
</td></tr>
</table>


<h3>Note</h3>

 
<p>set c[1] = -100 and c[k+1] = 100. c[2] is set to 0 for identification. <br />
</p>
<p>The relationship between cut-offs and dstar is: <br />
c[3] = exp(dstar[1]),  <br />
c[4] = c[3] + exp(dstar[2]), ...,  <br />
c[k] = c[k-1] + exp(dstar[k-2]) 
</p>


<h3>Author(s)</h3>

<p> Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p><em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rbprobitGibbs">rbprobitGibbs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

## simulate data for ordered probit model

simordprobit=function(X, betas, cutoff){
  z = X%*%betas + rnorm(nobs)   
  y = cut(z, br = cutoff, right=TRUE, include.lowest = TRUE, labels = FALSE)  
  return(list(y = y, X = X, k=(length(cutoff)-1), betas= betas, cutoff=cutoff ))
}

nobs = 300 
X = cbind(rep(1,nobs),runif(nobs, min=0, max=5),runif(nobs,min=0, max=5))
k = 5
betas = c(0.5, 1, -0.5)       
cutoff = c(-100, 0, 1.0, 1.8, 3.2, 100)
simout = simordprobit(X, betas, cutoff) 
  
Data=list(X=simout$X, y=simout$y, k=k)

## set Mcmc for ordered probit model
   
Mcmc = list(R=R)   
out = rordprobitGibbs(Data=Data, Mcmc=Mcmc)

cat(" ", fill=TRUE)
cat("acceptance rate= ", accept=out$accept, fill=TRUE)
 
## outputs of betadraw and cut-off draws
  
cat(" Summary of betadraws", fill=TRUE)
summary(out$betadraw, tvalues=betas)
cat(" Summary of cut-off draws", fill=TRUE) 
summary(out$cutdraw, tvalues=cutoff[2:k])

## plotting examples
if(0){plot(out$cutdraw)}
</code></pre>

<hr>
<h2 id='rscaleUsage'>MCMC Algorithm for Multivariate Ordinal Data with Scale Usage Heterogeneity</h2><span id='topic+rscaleUsage'></span>

<h3>Description</h3>

<p><code>rscaleUsage</code> implements an MCMC algorithm for multivariate ordinal data with scale usage heterogeniety.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rscaleUsage(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rscaleUsage_+3A_data">Data</code></td>
<td>
<p>list(x, k)</p>
</td></tr>
<tr><td><code id="rscaleUsage_+3A_prior">Prior</code></td>
<td>
<p>list(nu, V, mubar, Am, gs, Lambdanu, LambdaV)</p>
</td></tr>
<tr><td><code id="rscaleUsage_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint, ndghk, e, y, mu, Sigma, sigma, tau, Lambda)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">n</code> = <code>nrow(x)</code> individuals respond to <code class="reqn">p</code> = <code>ncol(x)</code> questions; 
all questions are on a scale <code class="reqn">1, \ldots, k</code> for respondent <code class="reqn">i</code> and question <code class="reqn">j</code>, <br />
</p>
<p><code class="reqn">x_{ij} = d</code> if <code class="reqn">c_{d-1} \le y_{ij} \le c_d</code> where <code class="reqn">d = 1, \ldots, k</code> and <code class="reqn">c_d = a + bd + ed^2</code> <br />
</p>
<p><code class="reqn">y_i = mu + tau_i*iota + sigma_i*z_i</code> with <code class="reqn">z_i</code> <code class="reqn">\sim</code> <code class="reqn">N(0, Sigma)</code>
</p>
<p><code class="reqn">(tau_i, ln(sigma_i))</code> <code class="reqn">\sim</code> <code class="reqn">N(\phi, Lamda)</code><br />
<code class="reqn">\phi = (0, lambda_{22})</code> <br />
<code class="reqn">mu</code> <code class="reqn">\sim</code> <code class="reqn">N(mubar, Am^{-1})</code><br />
<code class="reqn">Sigma</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu, V)</code> <br />
<code class="reqn">Lambda</code> <code class="reqn">\sim</code> <code class="reqn">IW(Lambdanu, LambdaV)</code> <br />
<code class="reqn">e</code> <code class="reqn">\sim</code> unif on a grid 
</p>
<p>It is highly recommended that the user choose the default prior settings. 
If you wish to change prior settings and/or the grids used, please carefully read the case study listed in the reference below.
</p>



<h4>Argument Details</h4>

<p><em><code>Data  = list(x, k)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>x: </code> </td><td style="text-align: left;"> <code class="reqn">n x p</code> matrix of discrete responses </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>k: </code> </td><td style="text-align: left;"> number of discrete rating scale options
    </td>
</tr>

</table>

<p><em><code>Prior = list(nu, V, mubar, Am, gs, Lambdanu, LambdaV)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>nu:       </code> </td><td style="text-align: left;"> d.f. parameter for Sigma prior (def: p + 3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>V:        </code> </td><td style="text-align: left;"> scale location matrix for Sigma prior (def: nu*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>mubar:    </code> </td><td style="text-align: left;"> <code class="reqn">p x 1</code> vector of prior means (def: <code>rep(k/2,p)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Am:       </code> </td><td style="text-align: left;"> <code class="reqn">p x p</code> prior precision matrix (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>gs:       </code> </td><td style="text-align: left;"> grid size for sigma (def: 100) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Lambdanu: </code> </td><td style="text-align: left;"> d.f. parameter for Lambda prior (def: 20) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>LambdaV:  </code> </td><td style="text-align: left;"> scale location matrix for Lambda prior (def: (Lambdanu - 3)*Lambda)
    </td>
</tr>

</table>

<p><em><code>Mcmc  = list(R, keep, nprint, ndghk, e, y, mu, Sigma, sigma, tau, Lambda)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>R:      </code> </td><td style="text-align: left;"> number of MCMC draws (def: 1000) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:   </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint: </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ndghk:  </code> </td><td style="text-align: left;"> number of draws for a GHK integration (def: 100) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>e:      </code> </td><td style="text-align: left;"> initial value (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>y:      </code> </td><td style="text-align: left;"> initial values (def: x) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>mu:     </code> </td><td style="text-align: left;"> initial values (def: <code>apply(y,2,mean)</code>, a p-length vector) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Sigma:  </code> </td><td style="text-align: left;"> initial value (def: <code>var(y)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>sigma:  </code> </td><td style="text-align: left;"> initial values (def: <code>rep(1,n)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>tau:    </code> </td><td style="text-align: left;"> initial values (def: <code>rep(0,n)</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Lambda: </code> </td><td style="text-align: left;"> initial values (def: <code>matrix(c(4,0,0,.5),ncol=2)</code>)
    </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>Sigmadraw</code></td>
<td>
<p><code class="reqn">R/keep x p*p</code> matrix of Sigma draws &ndash; each row is the vector form of Sigma</p>
</td></tr>
<tr><td><code>mudraw</code></td>
<td>
<p><code class="reqn">R/keep x p</code> matrix of mu draws</p>
</td></tr>
<tr><td><code>taudraw</code></td>
<td>
<p><code class="reqn">R/keep x n</code> matrix of tau draws</p>
</td></tr>
<tr><td><code>sigmadraw</code></td>
<td>
<p><code class="reqn">R/keep x n</code> matrix of sigma draws</p>
</td></tr>
<tr><td><code>Lambdadraw</code></td>
<td>
<p><code class="reqn">R/keep x 4</code> matrix of Lamda draws</p>
</td></tr>
<tr><td><code>edraw</code></td>
<td>
<p><code class="reqn">R/keep x 1</code> vector of e draws</p>
</td></tr>
</table>


<h3>Warning</h3>

<p><code class="reqn">tau_i</code>, <code class="reqn">sigma_i</code> are identified from the scale usage patterns in the <code class="reqn">p</code> questions asked per respondent (# cols of <code class="reqn">x</code>).  Do not attempt to use this on datasets with only a small number of total questions.
</p>


<h3>Author(s)</h3>

<p>Rob McCulloch (Arizona State University) and Peter Rossi (Anderson School, UCLA), <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p> For further discussion, see Case Study 3 on Overcoming Scale Usage Heterogeneity, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=1000} else {R=5} 
set.seed(66)

data(customerSat)
surveydat = list(k=10, x=as.matrix(customerSat))

out = rscaleUsage(Data=surveydat, Mcmc=list(R=R))
summary(out$mudraw)
</code></pre>

<hr>
<h2 id='rsurGibbs'>Gibbs Sampler for Seemingly Unrelated Regressions (SUR)</h2><span id='topic+rsurGibbs'></span>

<h3>Description</h3>

<p><code>rsurGibbs</code> implements a Gibbs Sampler to draw from the posterior of the Seemingly Unrelated Regression (SUR) Model of Zellner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rsurGibbs(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rsurGibbs_+3A_data">Data</code></td>
<td>
<p>list(regdata)</p>
</td></tr>
<tr><td><code id="rsurGibbs_+3A_prior">Prior</code></td>
<td>
<p>list(betabar, A, nu, V)</p>
</td></tr>
<tr><td><code id="rsurGibbs_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">y_i = X_i\beta_i + e_i</code> with  <code class="reqn">i=1,\ldots,m</code> for <code class="reqn">m</code> regressions <br />
(<code class="reqn">e(k,1), \ldots, e(k,m)</code>)' <code class="reqn">\sim</code> <code class="reqn">N(0, \Sigma)</code> with <code class="reqn">k=1, \ldots, n</code> 
</p>
<p>Can be written as a stacked model: <br />
<code class="reqn">y = X\beta + e</code> where <code class="reqn">y</code> is a <code class="reqn">nobs*m</code> vector and <code class="reqn">p</code> = <code>length(beta)</code> = <code>sum(length(beta_i))</code>
</p>
<p>Note: must have the same number of observations (<code class="reqn">n</code>) in each equation but can have a different number of <code class="reqn">X</code> variables (<code class="reqn">p_i</code>) for each equation where <code class="reqn">p = \sum p_i</code>.
</p>
<p><code class="reqn">\beta</code> <code class="reqn">\sim</code> <code class="reqn">N(betabar, A^{-1})</code> <br />
<code class="reqn">\Sigma</code> <code class="reqn">\sim</code> <code class="reqn">IW(nu,V)</code>
</p>



<h4>Argument Details</h4>

<p><em><code>Data = list(regdata)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
      <code>regdata: </code> </td><td style="text-align: left;"> list of lists, <code>regdata[[i]] = list(y=y_i, X=X_i)</code>, where <code>y_i</code> is <code class="reqn">n x 1</code> and <code>X_i</code> is <code class="reqn">n x p_i</code>
    </td>
</tr>

</table>

<p><em><code>Prior = list(betabar, A, nu, V)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
      <code>betabar: </code> </td><td style="text-align: left;"> <code class="reqn">p x 1</code> prior mean (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>A:       </code> </td><td style="text-align: left;"> <code class="reqn">p x p</code> prior precision matrix (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>nu:      </code> </td><td style="text-align: left;"> d.f. parameter for Inverted Wishart prior (def: m+3) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>V:       </code> </td><td style="text-align: left;"> <code class="reqn">m x m</code> scale parameter for Inverted Wishart prior (def: nu*I)
    </td>
</tr>

</table>

<p><em><code>Mcmc = list(R, keep)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
      <code>R:       </code></td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>keep:    </code></td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>nprint:  </code></td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print)
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing: 
</p>
<table>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">R x p</code> matrix of betadraws</p>
</td></tr>
<tr><td><code>Sigmadraw</code></td>
<td>
 <p><code class="reqn">R x (m*m)</code> array of Sigma draws</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+rmultireg">rmultireg</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=1000} else {R=10}
set.seed(66)

## simulate data from SUR
beta1 = c(1,2)
beta2 = c(1,-1,-2)
nobs = 100
nreg = 2
iota = c(rep(1, nobs))
X1 = cbind(iota, runif(nobs))
X2 = cbind(iota, runif(nobs), runif(nobs))
Sigma = matrix(c(0.5, 0.2, 0.2, 0.5), ncol=2)
U = chol(Sigma)
E = matrix(rnorm(2*nobs),ncol=2)%*%U
y1 = X1%*%beta1 + E[,1]
y2 = X2%*%beta2 + E[,2]

## run Gibbs Sampler
regdata = NULL
regdata[[1]] = list(y=y1, X=X1)
regdata[[2]] = list(y=y2, X=X2)

out = rsurGibbs(Data=list(regdata=regdata), Mcmc=list(R=R))

cat("Summary of beta draws", fill=TRUE)
summary(out$betadraw, tvalues=c(beta1,beta2))

cat("Summary of Sigmadraws", fill=TRUE)
summary(out$Sigmadraw, tvalues=as.vector(Sigma[upper.tri(Sigma,diag=TRUE)]))

## plotting examples
if(0){plot(out$betadraw, tvalues=c(beta1,beta2))}
</code></pre>

<hr>
<h2 id='rtrun'>Draw from Truncated Univariate Normal</h2><span id='topic+rtrun'></span>

<h3>Description</h3>

<p><code>rtrun</code> draws from a truncated univariate normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rtrun(mu, sigma, a, b)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rtrun_+3A_mu">mu</code></td>
<td>
<p> mean </p>
</td></tr>
<tr><td><code id="rtrun_+3A_sigma">sigma</code></td>
<td>
<p> standard deviation </p>
</td></tr>
<tr><td><code id="rtrun_+3A_a">a</code></td>
<td>
<p> lower bound </p>
</td></tr>
<tr><td><code id="rtrun_+3A_b">b</code></td>
<td>
<p> upper bound </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that due to the vectorization of the <code>rnorm</code> and <code>qnorm</code> commands in R, all arguments can be vectors of equal length. This makes the inverse CDF method the most efficient to use in R.
</p>


<h3>Value</h3>

<p>Draw (possibly a vector)</p>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>
<p>**Note also that <code>rtrun</code> is currently affected by the numerical accuracy of the inverse CDF function when trunctation points are far out in the tails of the distribution, where &ldquo;far out&rdquo; means <code class="reqn">|a - \mu| / \sigma &gt; 6</code> and/or <code class="reqn">|b - \mu| / \sigma &gt; 6</code>. A fix will be implemented in a future version of <code>bayesm</code>.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 2, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(66)
rtrun(mu=c(rep(0,10)), sigma=c(rep(1,10)), a=c(rep(0,10)), b=c(rep(2,10)))
</code></pre>

<hr>
<h2 id='runireg'>IID Sampler for Univariate Regression</h2><span id='topic+runireg'></span>

<h3>Description</h3>

<p><code>runireg</code> implements an iid sampler to draw from the posterior of a univariate regression with a conjugate prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runireg(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="runireg_+3A_data">Data</code></td>
<td>
<p>list(y, X)</p>
</td></tr>
<tr><td><code id="runireg_+3A_prior">Prior</code></td>
<td>
<p>list(betabar, A, nu, ssq)</p>
</td></tr>
<tr><td><code id="runireg_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(R, keep, nprint)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">y = X\beta + e</code> with <code class="reqn">e</code> <code class="reqn">\sim</code> <code class="reqn">N(0, \sigma^2)</code>
</p>
<p><code class="reqn">\beta</code> <code class="reqn">\sim</code> <code class="reqn">N(betabar, \sigma^2*A^{-1})</code> <br />
<code class="reqn">\sigma^2</code> <code class="reqn">\sim</code> <code class="reqn">(nu*ssq)/\chi^2_{nu}</code>
</p>



<h4>Argument Details</h4>

<p><em><code>Data = list(y, X)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
      <code>y:       </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of observations </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>X:       </code> </td><td style="text-align: left;"> <code class="reqn">n x k</code> design matrix
    </td>
</tr>

</table>

<p><em><code>Prior = list(betabar, A, nu, ssq)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
      <code>betabar: </code> </td><td style="text-align: left;"> <code class="reqn">k x 1</code> prior mean (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>A:       </code> </td><td style="text-align: left;"> <code class="reqn">k x k</code> prior precision matrix (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>nu:      </code> </td><td style="text-align: left;"> d.f. parameter for Inverted Chi-square prior (def: 3) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>ssq:     </code> </td><td style="text-align: left;"> scale parameter for Inverted Chi-square prior (def: <code>var(y)</code>)
    </td>
</tr>

</table>

<p><em><code>Mcmc = list(R, keep, nprint)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
      <code>R:       </code> </td><td style="text-align: left;"> number of draws </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>keep:    </code> </td><td style="text-align: left;"> thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>nprint:  </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print)
    </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing: 
</p>
<table>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">R x k</code> matrix of betadraws </p>
</td></tr>
<tr><td><code>sigmasqdraw</code></td>
<td>
 <p><code class="reqn">R x 1</code> vector of sigma-sq draws</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 2, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+runiregGibbs">runiregGibbs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=2000} else {R=10}
set.seed(66)

n = 200
X = cbind(rep(1,n), runif(n))
beta = c(1,2)
sigsq = 0.25
y = X%*%beta + rnorm(n,sd=sqrt(sigsq))

out = runireg(Data=list(y=y,X=X), Mcmc=list(R=R))

cat("Summary of beta and Sigmasq draws", fill=TRUE)
summary(out$betadraw, tvalues=beta)
summary(out$sigmasqdraw, tvalues=sigsq)

## plotting examples
if(0){plot(out$betadraw)}
</code></pre>

<hr>
<h2 id='runiregGibbs'>Gibbs Sampler for Univariate Regression</h2><span id='topic+runiregGibbs'></span>

<h3>Description</h3>

<p><code>runiregGibbs</code> implements a Gibbs Sampler to draw from posterior of a univariate regression with a conditionally conjugate prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runiregGibbs(Data, Prior, Mcmc)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="runiregGibbs_+3A_data">Data</code></td>
<td>
<p>list(y, X)</p>
</td></tr>
<tr><td><code id="runiregGibbs_+3A_prior">Prior</code></td>
<td>
<p>list(betabar, A, nu, ssq)</p>
</td></tr>
<tr><td><code id="runiregGibbs_+3A_mcmc">Mcmc</code></td>
<td>
<p>list(sigmasq, R, keep, nprint)</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model and Priors</h4>

<p><code class="reqn">y = X\beta + e</code> with <code class="reqn">e</code> <code class="reqn">\sim</code> <code class="reqn">N(0, \sigma^2)</code>
</p>
<p><code class="reqn">\beta</code> <code class="reqn">\sim</code> <code class="reqn">N(betabar, A^{-1})</code><br />
<code class="reqn">\sigma^2</code> <code class="reqn">\sim</code> <code class="reqn">(nu*ssq)/\chi^2_{nu}</code>
</p>



<h4>Argument Details</h4>

<p><em><code>Data = list(y, X)</code></em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>y:       </code> </td><td style="text-align: left;"> <code class="reqn">n x 1</code> vector of observations </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>X:       </code> </td><td style="text-align: left;"> <code class="reqn">n x k</code> design matrix
  </td>
</tr>

</table>

<p><em><code>Prior = list(betabar, A, nu, ssq)</code> [optional]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>betabar: </code> </td><td style="text-align: left;"> <code class="reqn">k x 1</code> prior mean (def: 0) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>A:       </code> </td><td style="text-align: left;"> <code class="reqn">k x k</code> prior precision matrix (def: 0.01*I) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nu:      </code> </td><td style="text-align: left;"> d.f. parameter for Inverted Chi-square prior (def: 3) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>ssq:     </code> </td><td style="text-align: left;"> scale parameter for Inverted Chi-square prior (def: <code>var(y)</code>)
  </td>
</tr>

</table>

<p><em><code>Mcmc = list(sigmasq, R, keep, nprint)</code> [only <code>R</code> required]</em>
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>sigmasq: </code> </td><td style="text-align: left;"> value for <code class="reqn">\sigma^2</code> for first Gibbs sampler draw of <code class="reqn">\beta</code>|<code class="reqn">\sigma^2</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>R:       </code> </td><td style="text-align: left;"> number of MCMC draws </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>keep:    </code> </td><td style="text-align: left;"> MCMC thinning parameter -- keep every <code>keep</code>th draw (def: 1) </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>nprint:  </code> </td><td style="text-align: left;"> print the estimated time remaining for every <code>nprint</code>'th draw (def: 100, set to 0 for no print)
  </td>
</tr>

</table>




<h3>Value</h3>

<p>A list containing: 
</p>
<table>
<tr><td><code>betadraw</code></td>
<td>
 <p><code class="reqn">R x k</code> matrix of betadraws </p>
</td></tr>
<tr><td><code>sigmasqdraw</code></td>
<td>
 <p><code class="reqn">R x 1</code> vector of sigma-sq draws</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 3, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+runireg">runireg</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(nchar(Sys.getenv("LONG_TEST")) != 0) {R=1000} else {R=10}
set.seed(66)

n = 200
X = cbind(rep(1,n), runif(n))
beta = c(1,2)
sigsq = 0.25
y = X%*%beta + rnorm(n,sd=sqrt(sigsq))

out = runiregGibbs(Data=list(y=y, X=X), Mcmc=list(R=R))

cat("Summary of beta and Sigmasq draws", fill=TRUE)
summary(out$betadraw, tvalues=beta)
summary(out$sigmasqdraw, tvalues=sigsq)

## plotting examples
if(0){plot(out$betadraw)}
</code></pre>

<hr>
<h2 id='rwishart'> Draw from Wishart and Inverted Wishart Distribution  </h2><span id='topic+rwishart'></span>

<h3>Description</h3>

<p><code>rwishart</code> draws from the Wishart and Inverted Wishart distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rwishart(nu, V)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rwishart_+3A_nu">nu</code></td>
<td>
<p> d.f. parameter</p>
</td></tr>
<tr><td><code id="rwishart_+3A_v">V</code></td>
<td>
<p> pds location matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the parameterization used here, <code class="reqn">W</code> <code class="reqn">\sim</code> <code class="reqn">W(nu,V)</code> with <code class="reqn">E[W]=nuV</code>.  <br />
</p>
<p>If you want to use an Inverted Wishart prior, you <em>must invert the location matrix</em> 
before calling <code>rwishart</code>, e.g. <br />
<code class="reqn">\Sigma</code> <code class="reqn">\sim</code> IW(nu ,V);  <code class="reqn">\Sigma^{-1}</code> <code class="reqn">\sim</code> <code class="reqn">W(nu, V^{-1})</code>.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>W:</code></td>
<td>
<p> Wishart draw </p>
</td></tr>
<tr><td><code>IW:</code></td>
<td>
<p>Inverted Wishart draw</p>
</td></tr>
<tr><td><code>C:</code></td>
<td>
<p> Upper tri root of W</p>
</td></tr>
<tr><td><code>CI:</code></td>
<td>
<p> inv(C), <code class="reqn">W^{-1}</code> = CICI'</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 2, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(66)
rwishart(5,diag(3))$IW
</code></pre>

<hr>
<h2 id='Scotch'>Survey Data on Brands of Scotch Consumed</h2><span id='topic+Scotch'></span>

<h3>Description</h3>

<p>Data from Simmons Survey. Brands used in last year for those respondents who report consuming scotch. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Scotch)</code></pre>


<h3>Format</h3>

<p>A data frame with 2218 observations on 21 brand variables. <br />
All variables are numeric vectors that are coded 1 if consumed in last year, 0 if not.
</p>


<h3>Source</h3>

<p>Edwards, Yancy and Greg Allenby (2003), &quot;Multivariate Analysis of Multiple Response Data,&quot; <em>Journal of Marketing Research</em>  40, 321&ndash;334.</p>


<h3>References</h3>

<p>Chapter 4, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Scotch)
cat(" Frequencies of Brands", fill=TRUE)
mat = apply(as.matrix(Scotch), 2, mean)
print(mat)


## use Scotch data to run Multivariate Probit Model
if(0) {
  y = as.matrix(Scotch)
  p = ncol(y)
  n = nrow(y)
  dimnames(y) = NULL
  y = as.vector(t(y))
  y = as.integer(y)
  I_p = diag(p)
  X = rep(I_p,n)
  X = matrix(X, nrow=p)
  X = t(X)
  
  R = 2000
  Data = list(p=p, X=X, y=y)
  Mcmc = list(R=R)
  
  set.seed(66)
  out = rmvpGibbs(Data=Data, Mcmc=Mcmc)
  
  ind = (0:(p-1))*p + (1:p)
  cat(" Betadraws ", fill=TRUE)
  mat = apply(out$betadraw/sqrt(out$sigmadraw[,ind]), 2 , quantile, 
        probs=c(0.01, 0.05, 0.5, 0.95, 0.99))
  attributes(mat)$class = "bayesm.mat"
  summary(mat)
  
  rdraw = matrix(double((R)*p*p), ncol=p*p)
  rdraw = t(apply(out$sigmadraw, 1, nmat))
  attributes(rdraw)$class = "bayesm.var"
  cat(" Draws of Correlation Matrix ", fill=TRUE)
  summary(rdraw)
}
</code></pre>

<hr>
<h2 id='simnhlogit'>Simulate from Non-homothetic Logit Model</h2><span id='topic+simnhlogit'></span>

<h3>Description</h3>

<p><code>simnhlogit</code> simulates from the non-homothetic logit model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>simnhlogit(theta, lnprices, Xexpend)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simnhlogit_+3A_theta">theta</code></td>
<td>
<p> coefficient vector </p>
</td></tr>
<tr><td><code id="simnhlogit_+3A_lnprices">lnprices</code></td>
<td>
 <p><code class="reqn">n x p</code> array of prices </p>
</td></tr>
<tr><td><code id="simnhlogit_+3A_xexpend">Xexpend</code></td>
<td>
 <p><code class="reqn">n x k</code> array of values of expenditure variables</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details on parameterization, see <code>llnhlogit</code>.</p>


<h3>Value</h3>

<p>A list containing: 
</p>
<table>
<tr><td><code>y</code></td>
<td>
<p><code class="reqn">n x 1</code> vector of multinomial outcomes (<code class="reqn">1,\ldots,p</code>)</p>
</td></tr>
<tr><td><code>Xexpend</code></td>
<td>
<p> expenditure variables</p>
</td></tr>
<tr><td><code>lnprices</code></td>
<td>
<p> price array </p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p> coefficients</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
 <p><code class="reqn">n x p</code> array of choice probabilities</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This routine is a utility routine that does <strong>not</strong> check the input arguments for proper dimensions and type.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>References</h3>

<p>For further discussion, see Chapter 4, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+llnhlogit">llnhlogit</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>N = 1000
p = 3
k = 1

theta = c(rep(1,p), seq(from=-1,to=1,length=p), rep(2,k), 0.5)
lnprices = matrix(runif(N*p), ncol=p)
Xexpend  = matrix(runif(N*k), ncol=k)

simdata = simnhlogit(theta, lnprices, Xexpend)
</code></pre>

<hr>
<h2 id='summary.bayesm.mat'>Summarize Mcmc Parameter Draws </h2><span id='topic+summary.bayesm.mat'></span>

<h3>Description</h3>

<p><code>summary.bayesm.mat</code> is an S3 method to summarize marginal distributions given an array of draws
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bayesm.mat'
summary(object, names, burnin = trunc(0.1 * nrow(X)), 
  tvalues, QUANTILES = TRUE, TRAILER = TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.bayesm.mat_+3A_object">object</code></td>
<td>
 <p><code>object</code> (hereafter <code>X</code>) is an array of draws, usually an object of class <code>bayesm.mat</code></p>
</td></tr>
<tr><td><code id="summary.bayesm.mat_+3A_names">names</code></td>
<td>
<p> optional character vector of names for the columns of <code>X</code></p>
</td></tr>
<tr><td><code id="summary.bayesm.mat_+3A_burnin">burnin</code></td>
<td>
<p> number of draws to burn-in (def: <code class="reqn">0.1*nrow(X)</code>)</p>
</td></tr>
<tr><td><code id="summary.bayesm.mat_+3A_tvalues">tvalues</code></td>
<td>
<p> optional vector of &quot;true&quot; values for use in simulation examples </p>
</td></tr>
<tr><td><code id="summary.bayesm.mat_+3A_quantiles">QUANTILES</code></td>
<td>
<p> logical for should quantiles be displayed (def: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="summary.bayesm.mat_+3A_trailer">TRAILER</code></td>
<td>
<p> logical for should a trailer be displayed (def: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="summary.bayesm.mat_+3A_...">...</code></td>
<td>
<p> optional arguments for generic function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typically, <code>summary.bayesm.nmix</code> will be invoked by a call to the generic summary function as in <code>summary(object)</code> where object is of class <code>bayesm.mat</code>. Mean, Std Dev, Numerical Standard error (of estimate of posterior mean), relative numerical efficiency (see <code>numEff</code>), and effective sample size are displayed.  If <code>QUANTILES=TRUE</code>, quantiles of marginal distirbutions in the columns of <code class="reqn">X</code> are displayed.<br />
</p>
<p><code>summary.bayesm.mat</code> is also exported for direct use as a standard function, as in <code>summary.bayesm.mat(matrix)</code>.<br />
</p>
<p><code>summary.bayesm.mat(matrix)</code> returns (invisibly) the array of the various summary statistics for further use.  To assess this array use<code>stats=summary(Drawmat)</code>.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+summary.bayesm.var">summary.bayesm.var</a></code>, <code><a href="#topic+summary.bayesm.nmix">summary.bayesm.nmix</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: out=rmnpGibbs(Data,Prior,Mcmc); summary(out$betadraw)
</code></pre>

<hr>
<h2 id='summary.bayesm.nmix'>Summarize Draws of Normal Mixture Components</h2><span id='topic+summary.bayesm.nmix'></span>

<h3>Description</h3>

<p><code>summary.bayesm.nmix</code> is an S3 method to display summaries of the distribution implied
by draws of Normal Mixture Components.  Posterior means and variance-covariance matrices are
displayed.<br />
</p>
<p>Note: 1st and 2nd moments may not be very interpretable for mixtures of normals. This summary function can take a minute or so. The current implementation is not efficient.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bayesm.nmix'
summary(object, names, burnin=trunc(0.1*nrow(probdraw)), ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.bayesm.nmix_+3A_object">object</code></td>
<td>
<p> an object of class <code>bayesm.nmix</code>, a list of lists of draws</p>
</td></tr>
<tr><td><code id="summary.bayesm.nmix_+3A_names">names</code></td>
<td>
<p> optional character vector of names fo reach dimension of the density</p>
</td></tr>
<tr><td><code id="summary.bayesm.nmix_+3A_burnin">burnin</code></td>
<td>
<p> number of draws to burn-in (def: <code class="reqn">0.1*nrow(probdraw)</code>)</p>
</td></tr>
<tr><td><code id="summary.bayesm.nmix_+3A_...">...</code></td>
<td>
<p> parms to send to summary</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>An object of class <code>bayesm.nmix</code> is a list of three components: 
</p>

<dl>
<dt>probdraw    </dt><dd><p> a matrix of <code class="reqn">R/keep</code> rows by dim of normal mix of mixture prob draws</p>
</dd>
<dt>second comp </dt><dd><p> not used</p>
</dd>
<dt>compdraw    </dt><dd><p> list of list of lists with draws of mixture comp parms</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+summary.bayesm.mat">summary.bayesm.mat</a></code>, <code><a href="#topic+summary.bayesm.var">summary.bayesm.var</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: out=rnmix(Data,Prior,Mcmc); summary(out)
</code></pre>

<hr>
<h2 id='summary.bayesm.var'>Summarize Draws of Var-Cov Matrices</h2><span id='topic+summary.bayesm.var'></span>

<h3>Description</h3>

<p><code>summary.bayesm.var</code> is an S3 method to summarize marginal distributions given an array of draws
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bayesm.var'
summary(object, names, burnin = trunc(0.1 * nrow(Vard)), tvalues, QUANTILES = FALSE , ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.bayesm.var_+3A_object">object</code></td>
<td>
 <p><code>object</code> (herafter, <code>Vard</code>) is an array of draws of a covariance matrix </p>
</td></tr>
<tr><td><code id="summary.bayesm.var_+3A_names">names</code></td>
<td>
<p> optional character vector of names for the columns of <code>Vard</code></p>
</td></tr>
<tr><td><code id="summary.bayesm.var_+3A_burnin">burnin</code></td>
<td>
<p> number of draws to burn-in (def: <code class="reqn">0.1*nrow(Vard)</code>)</p>
</td></tr>
<tr><td><code id="summary.bayesm.var_+3A_tvalues">tvalues</code></td>
<td>
<p> optional vector of &quot;true&quot; values for use in simulation examples </p>
</td></tr>
<tr><td><code id="summary.bayesm.var_+3A_quantiles">QUANTILES</code></td>
<td>
<p> logical for should quantiles be displayed (def: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="summary.bayesm.var_+3A_...">...</code></td>
<td>
<p> optional arguments for generic function </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typically, <code>summary.bayesm.var</code> will be invoked by a call to the generic summary function as in <code>summary(object)</code> where <code>object</code> is of class <code>bayesm.var</code>. Mean, Std Dev, Numerical Standard error (of estimate of posterior mean), relative numerical efficiency (see <code>numEff</code>), and effective sample size are displayed.  If <code>QUANTILES=TRUE</code>, quantiles of marginal distirbutions in the columns of <code>Vard</code> are displayed.  <br />
</p>
<p><code>Vard</code> is an array of draws of a covariance matrix stored as vectors.  Each row is a different draw. <br />
</p>
<p>The posterior mean of the vector of standard deviations and the correlation matrix are also displayed
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+summary.bayesm.mat">summary.bayesm.mat</a></code>, <code><a href="#topic+summary.bayesm.nmix">summary.bayesm.nmix</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: out=rmnpGibbs(Data,Prior,Mcmc); summary(out$sigmadraw)
</code></pre>

<hr>
<h2 id='tuna'>Canned Tuna Sales Data</h2><span id='topic+tuna'></span>

<h3>Description</h3>

<p>Volume of canned tuna sales as well as a measure of display activity, log price, and log wholesale price. Weekly data aggregated to the chain level. This data is extracted from the Dominick's Finer Foods database maintained by the Kilts Center for Marketing at the University of Chicago's Booth School of Business. Brands are seven of the top 10 UPCs in the canned tuna product category.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(tuna)</code></pre>


<h3>Format</h3>

<p>A data frame with 338 observations on 30 variables.
</p>

<table>
<tr>
 <td style="text-align: left;">
    ...<code>$WEEK     </code> </td><td style="text-align: left;"> a numeric vector </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$MOVE#    </code> </td><td style="text-align: left;"> unit sales of brand # </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$NSALE#   </code> </td><td style="text-align: left;"> a measure of display activity of brand # </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$LPRICE#  </code> </td><td style="text-align: left;"> log of price of brand # </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$LWHPRIC# </code> </td><td style="text-align: left;"> log of wholesale price of brand # </td>
</tr>
<tr>
 <td style="text-align: left;">
    ...<code>$FULLCUST </code> </td><td style="text-align: left;"> total customers visits 
  </td>
</tr>

</table>

<p>The brands are:
</p>

<table>
<tr>
 <td style="text-align: left;">
    1. </td><td style="text-align: left;"> Star Kist 6 oz. </td>
</tr>
<tr>
 <td style="text-align: left;">
    2. </td><td style="text-align: left;"> Chicken of the Sea 6 oz. </td>
</tr>
<tr>
 <td style="text-align: left;">
    3. </td><td style="text-align: left;"> Bumble Bee Solid 6.12 oz. </td>
</tr>
<tr>
 <td style="text-align: left;">
    4. </td><td style="text-align: left;"> Bumble Bee Chunk 6.12 oz. </td>
</tr>
<tr>
 <td style="text-align: left;">
    5. </td><td style="text-align: left;"> Geisha 6 oz. </td>
</tr>
<tr>
 <td style="text-align: left;">
    6. </td><td style="text-align: left;"> Bumble Bee Large Cans. </td>
</tr>
<tr>
 <td style="text-align: left;">
    7. </td><td style="text-align: left;"> HH Chunk Lite 6.5 oz. 
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Chevalier, Judith, Anil Kashyap, and Peter Rossi (2003), &quot;Why Don't Prices Rise During Periods of Peak Demand? Evidence from Scanner Data,&quot; <em>The American Economic Review</em> , 93(1), 15&ndash;37.</p>


<h3>References</h3>

<p>Chapter 7, <em>Bayesian Statistics and Marketing</em> by Rossi, Allenby, and McCulloch.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(tuna)
cat(" Quantiles of sales", fill=TRUE)
mat = apply(as.matrix(tuna[,2:5]), 2, quantile)
print(mat)


## example of processing for use with rivGibbs
if(0) {
  data(tuna)                          
  t = dim(tuna)[1]    
  customers = tuna[,30]                 
  sales = tuna[,2:8]                                                        
  lnprice = tuna[,16:22]      
  lnwhPrice = tuna[,23:29]      
  share = sales/mean(customers)
  shareout = as.vector(1-rowSums(share))
  lnprob = log(share/shareout)  

  ## create w matrix
  I1 = as.matrix(rep(1,t))
  I0 = as.matrix(rep(0,t))
  intercept = rep(I1,4)
  brand1 = rbind(I1, I0, I0, I0)
  brand2 = rbind(I0, I1, I0, I0)
  brand3 = rbind(I0, I0, I1, I0)
  w = cbind(intercept, brand1, brand2, brand3)  
  
  ## choose brand 1 to 4
  y = as.vector(as.matrix(lnprob[,1:4]))
  X = as.vector(as.matrix(lnprice[,1:4]))
  lnwhPrice = as.vector(as.matrix(lnwhPrice[1:4]))
  z = cbind(w, lnwhPrice)
                        
  Data = list(z=z, w=w, x=X, y=y)
  Mcmc = list(R=R, keep=1)
  
  set.seed(66)
  out = rivGibbs(Data=Data, Mcmc=Mcmc)

  cat(" betadraws ", fill=TRUE)
  summary(out$betadraw)

  ## plotting examples
  if(0){plot(out$betadraw)}
}
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
