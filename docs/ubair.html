<!DOCTYPE html><html lang="en"><head><title>Help for package ubair</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ubair}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calc_performance_metrics'><p>Calculates performance metrics of a business-as-usual model</p></a></li>
<li><a href='#calc_summary_statistics'><p>Calculates summary statistics for predictions and true values</p></a></li>
<li><a href='#clean_data'><p>Clean and Optionally Aggregate Environmental Data</p></a></li>
<li><a href='#copy_default_params'><p>Copy Default Parameters File</p></a></li>
<li><a href='#detrend'><p>Removes trend from data</p></a></li>
<li><a href='#estimate_effect_size'><p>Estimates size of the external effect</p></a></li>
<li><a href='#get_meteo_available'><p>Get Available Meteorological Components</p></a></li>
<li><a href='#load_params'><p>Load Parameters from YAML File</p></a></li>
<li><a href='#load_uba_data_from_dir'><p>Load UBA Data from Directory</p></a></li>
<li><a href='#mock_env_data'><p>Mock Environmental Data</p></a></li>
<li><a href='#plot_counterfactual'><p>Prepare Plot Data and Plot Counterfactuals</p></a></li>
<li><a href='#plot_station_measurements'><p>Descriptive plot of daily time series data</p></a></li>
<li><a href='#prepare_data_for_modelling'><p>Prepare Data for Training a model</p></a></li>
<li><a href='#rescale_predictions'><p>Rescale predictions to original scale.</p></a></li>
<li><a href='#retrend_predictions'><p>Restors the trend in the prediction</p></a></li>
<li><a href='#run_counterfactual'><p>Full counterfactual simulation run</p></a></li>
<li><a href='#run_dynamic_regression'><p>Run the dynamic regression model</p></a></li>
<li><a href='#run_fnn'><p>Train a Feedforward Neural Network (FNN) in a Counterfactual Scenario.</p></a></li>
<li><a href='#run_lightgbm'><p>Run gradient boosting model with lightgbm</p></a></li>
<li><a href='#run_rf'><p>Run random forest model with ranger</p></a></li>
<li><a href='#sample_data_DESN025'><p>Environmental Data for Modelling from station DESN025 in Leipzig-Mitte.</p></a></li>
<li><a href='#scale_data'><p>Standardize Training and Application Data</p></a></li>
<li><a href='#split_data_counterfactual'><p>Split Data into Training and Application Datasets</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Effects of External Conditions on Air Quality</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Analyzes the impact of external conditions on air quality using counterfactual approaches, featuring methods for data preparation, modeling, and visualization.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://gitlab.opencode.de/uba-ki-lab/ubair">https://gitlab.opencode.de/uba-ki-lab/ubair</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.4.0),</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0), deepnet, fastshap, treeshap, shapviz,
knitr, rmarkdown</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Imports:</td>
<td>rlang, data.table, dplyr, ggplot2, forecast, lubridate, tidyr,
yaml, ranger, lightgbm</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LazyDataCompression:</td>
<td>xz</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Note:</td>
<td>Note: The included dataset is licensed under "DL-DE-BY-2.0." See
the dataset documentation for details.</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-23 08:48:02 UTC; vossi</td>
</tr>
<tr>
<td>Author:</td>
<td>Raphael Franke [aut],
  Imke Voss [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Imke Voss &lt;imke.voss@uba.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-27 18:30:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='calc_performance_metrics'>Calculates performance metrics of a business-as-usual model</h2><span id='topic+calc_performance_metrics'></span>

<h3>Description</h3>

<p>Model agnostic function to calculate a number of common performance
metrics on the reference time window.
Uses the true data <code>value</code> and the predictions <code>prediction</code> for this calculation.
The coverage is calculated from the columns <code>value</code>, <code>prediction_lower</code> and
<code>prediction_upper</code>.
Removes dates in the effect and buffer range as the model is not expected to
be performing correctly for these times. The incorrectness is precisely
what we are using for estimating the effect.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_performance_metrics(predictions, date_effect_start = NULL, buffer = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_performance_metrics_+3A_predictions">predictions</code></td>
<td>
<p>data.table or data.frame with the following columns
</p>

<dl>
<dt>date</dt><dd><p>Date of the observation. Needs to be comparable to
date_effect_start element.</p>
</dd>
<dt>value</dt><dd><p>True observed value of the station</p>
</dd>
<dt>prediction</dt><dd><p>Predicted model output for the same time and station
as value</p>
</dd>
<dt>prediction_lower</dt><dd><p>Lower end of the prediction interval</p>
</dd>
<dt>prediction_upper</dt><dd><p>Upper end of the prediction interval</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="calc_performance_metrics_+3A_date_effect_start">date_effect_start</code></td>
<td>
<p>A date. Start date of the
effect that is to be evaluated. The data from this point onwards is disregarded
for calculating model performance</p>
</td></tr>
<tr><td><code id="calc_performance_metrics_+3A_buffer">buffer</code></td>
<td>
<p>Integer. An additional buffer window before date_effect_start to account
for uncertainty in the effect start point. Disregards additional buffer data
points for model evaluation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named vector with performance metrics of the model
</p>

<hr>
<h2 id='calc_summary_statistics'>Calculates summary statistics for predictions and true values</h2><span id='topic+calc_summary_statistics'></span>

<h3>Description</h3>

<p>Helps with analyzing predictions by comparing them with the true values on
a number of relevant summary statistics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_summary_statistics(predictions, date_effect_start = NULL, buffer = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_summary_statistics_+3A_predictions">predictions</code></td>
<td>
<p>Data.table or data.frame with the following columns
</p>

<dl>
<dt>date</dt><dd><p>Date of the observation. Needs to be comparable to
date_effect_start element.</p>
</dd>
<dt>value</dt><dd><p>True observed value of the station</p>
</dd>
<dt>prediction</dt><dd><p>Predicted model output for the same time and station
as value</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="calc_summary_statistics_+3A_date_effect_start">date_effect_start</code></td>
<td>
<p>A date. Start date of the
effect that is to be evaluated. The data from this point onwards is disregarded
for calculating model performance</p>
</td></tr>
<tr><td><code id="calc_summary_statistics_+3A_buffer">buffer</code></td>
<td>
<p>Integer. An additional buffer window before date_effect_start to account
for uncertainty in the effect start point. Disregards additional buffer data
points for model evaluation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of summary statistics with columns true and prediction
</p>

<hr>
<h2 id='clean_data'>Clean and Optionally Aggregate Environmental Data</h2><span id='topic+clean_data'></span>

<h3>Description</h3>

<p>Cleans a data table of environmental measurements by filtering for a specific
station, removing duplicates, and optionally aggregating the data on a daily
basis using the mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean_data(env_data, station, aggregate_daily = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="clean_data_+3A_env_data">env_data</code></td>
<td>
<p>A data table in long format.
Must include columns:
</p>

<dl>
<dt>Station</dt><dd><p>Station identifier for the data.</p>
</dd>
<dt>Komponente</dt><dd><p>Measured environmental component e.g. temperature, NO2.</p>
</dd>
<dt>Wert</dt><dd><p>Measured value.</p>
</dd>
<dt>date</dt><dd><p>Timestamp as Date-Time object (<code style="white-space: pre;">&#8288;YYYY-MM-DD HH:MM:SS&#8288;</code> format).</p>
</dd>
<dt>Komponente_txt</dt><dd><p>Textual description of the component.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="clean_data_+3A_station">station</code></td>
<td>
<p>Character. Name of the station to filter by.</p>
</td></tr>
<tr><td><code id="clean_data_+3A_aggregate_daily">aggregate_daily</code></td>
<td>
<p>Logical. If <code>TRUE</code>, aggregates data to daily mean values. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Duplicate rows (by <code>date</code>, <code>Komponente</code>, and <code>Station</code>) are removed. A warning is issued
if duplicates are found.
</p>


<h3>Value</h3>

<p>A <code>data.table</code>:
</p>

<ul>
<li><p> If <code>aggregate_daily = TRUE</code>: Contains columns for station, component, day, year,
and the daily mean value of the measurements.
</p>
</li>
<li><p> If <code>aggregate_daily = FALSE</code>: Contains cleaned data with duplicates removed.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Example data
env_data &lt;- data.table::data.table(
  Station = c("DENW094", "DENW094", "DENW006", "DENW094"),
  Komponente = c("NO2", "O3", "NO2", "NO2"),
  Wert = c(45, 30, 50, 40),
  date = as.POSIXct(c(
    "2023-01-01 08:00:00", "2023-01-01 09:00:00",
    "2023-01-01 08:00:00", "2023-01-02 08:00:00"
  )),
  Komponente_txt = c(
    "Nitrogen Dioxide", "Ozone", "Nitrogen Dioxide", "Nitrogen Dioxide"
  )
)

# Clean data for StationA without aggregation
cleaned_data &lt;- clean_data(env_data, station = "DENW094", aggregate_daily = FALSE)
print(cleaned_data)
</code></pre>

<hr>
<h2 id='copy_default_params'>Copy Default Parameters File</h2><span id='topic+copy_default_params'></span>

<h3>Description</h3>

<p>Copies the default <code>params.yaml</code> file, included with the package, to a
specified destination directory. This is useful for initializing parameter
files for custom edits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>copy_default_params(dest_dir)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="copy_default_params_+3A_dest_dir">dest_dir</code></td>
<td>
<p>Character. The path to the directory where the <code>params.yaml</code>
file will be copied.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>params.yaml</code> file contains default model parameters for various
configurations such as LightGBM, dynamic regression, and others. See the
<code><a href="#topic+load_params">load_params()</a></code>&lsquo; documentation for an example of the file&rsquo;s structure.
</p>


<h3>Value</h3>

<p>Nothing is returned. A message is displayed upon successful copying.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>copy_default_params(tempdir())
</code></pre>

<hr>
<h2 id='detrend'>Removes trend from data</h2><span id='topic+detrend'></span>

<h3>Description</h3>

<p>Takes a list of train and application data as prepared by
<code><a href="#topic+split_data_counterfactual">split_data_counterfactual()</a></code>
and removes a polynomial, exponential or cubic spline spline trend function.
Trend is obtained only from train data. Use as part of preprocessing before
training a model based on decision trees, i.e. random forest and lightgbm.
For the other methods it may be helpful but they are generally able to
deal with trends themselves. Therefore we recommend to try out different
versions and guide decisisions using the model evaluation metrics from
<code><a href="#topic+calc_performance_metrics">calc_performance_metrics()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>detrend(split_data, mode = "linear", num_splines = 5, log_transform = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="detrend_+3A_split_data">split_data</code></td>
<td>
<p>List of two named dataframes called train and apply</p>
</td></tr>
<tr><td><code id="detrend_+3A_mode">mode</code></td>
<td>
<p>String which defines type of trend is present. Options are
&quot;linear&quot;, &quot;quadratic&quot;, &quot;exponential&quot;, &quot;spline&quot;, &quot;none&quot;.
&quot;none&quot; returns original data</p>
</td></tr>
<tr><td><code id="detrend_+3A_num_splines">num_splines</code></td>
<td>
<p>Defines the number of cubic splines if <code>mode="spline"</code>.
Choose num_splines=1 for cubic polynomial trend. If <code>mode!="spline"</code>, this
parameter is ignored</p>
</td></tr>
<tr><td><code id="detrend_+3A_log_transform">log_transform</code></td>
<td>
<p>If <code>TRUE</code>, use a log-transformation before detrending
to ensure positivity of all predictions in the rest of the pipeline.
A exp transformation is necessary during retrending to return to the solution
space. Use only in combination with <code>log_transform</code> parameter in
<code><a href="#topic+retrend_predictions">retrend_predictions()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Apply <code><a href="#topic+retrend_predictions">retrend_predictions()</a></code> to predictions to return to the
original data units.
</p>


<h3>Value</h3>

<p>List of 3 elements. 2 dataframes: detrended train, apply and the
trend function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mock_env_data)
split_data &lt;- list(
  train = mock_env_data[1:80, ],
  apply = mock_env_data[81:100, ]
)
detrended_list &lt;- detrend(split_data, mode = "linear")
detrended_train &lt;- detrended_list$train
detrended_apply &lt;- detrended_list$apply
trend &lt;- detrended_list$model
</code></pre>

<hr>
<h2 id='estimate_effect_size'>Estimates size of the external effect</h2><span id='topic+estimate_effect_size'></span>

<h3>Description</h3>

<p>Calculates an estimate for the absolute and relative effect size of the
external effect. The absolute effect is the difference between the model
bias in the reference time and the effect time windows. The relative effect
is the absolute effect divided by the mean true value in the reference
window.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_effect_size(df, date_effect_start, buffer = 0, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estimate_effect_size_+3A_df">df</code></td>
<td>
<p>Data.table or data.frame with the following columns
</p>

<dl>
<dt>date</dt><dd><p>Date of the observation. Needs to be comparable to
date_effect_start element.</p>
</dd>
<dt>value</dt><dd><p>True observed value of the station</p>
</dd>
<dt>prediction</dt><dd><p>Predicted model output for the same time and station
as value</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="estimate_effect_size_+3A_date_effect_start">date_effect_start</code></td>
<td>
<p>A date. Start date of the
effect that is to be evaluated. The data from this point onward is disregarded
for calculating model performance.</p>
</td></tr>
<tr><td><code id="estimate_effect_size_+3A_buffer">buffer</code></td>
<td>
<p>Integer. An additional buffer window before date_effect_start to account
for uncertainty in the effect start point. Disregards additional buffer data
points for model evaluation</p>
</td></tr>
<tr><td><code id="estimate_effect_size_+3A_verbose">verbose</code></td>
<td>
<p>Prints an explanation of the results if TRUE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: Since the bias is of the model is an average over predictions and true
values, it is important, that the effect window is specified correctly.
Imagine a scenario like a fire which strongly affects the outcome for one
hour and is gone the next hour. If we use a two week effect window, the
estimated effect will be 14*24=336 times smaller compared to using a 1-hour
effect window. Generally, we advise against studying very short effects (single
hour or single day). The variability of results will be too large to learn
anything meaningful.
</p>


<h3>Value</h3>

<p>A list with two numbers: Absolute and relative estimated effect size.
</p>

<hr>
<h2 id='get_meteo_available'>Get Available Meteorological Components</h2><span id='topic+get_meteo_available'></span>

<h3>Description</h3>

<p>Identifies unique meteorological components from the provided environmental data,
filtering only those that match the predefined UBA naming conventions. These components
include &quot;GLO&quot;, &quot;LDR&quot;, &quot;RFE&quot;, &quot;TMP&quot;, &quot;WIG&quot;, &quot;WIR&quot;, &quot;WIND_U&quot;, and &quot;WIND_V&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_meteo_available(env_data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_meteo_available_+3A_env_data">env_data</code></td>
<td>
<p>Data table containing environmental data.
Must contain column &quot;Komponente&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of available meteorological components.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example environmental data
env_data &lt;- data.table::data.table(
  Komponente = c("TMP", "NO2", "GLO", "WIR"),
  Wert = c(25, 40, 300, 50),
  date = as.POSIXct(c(
    "2023-01-01 08:00:00", "2023-01-01 09:00:00",
    "2023-01-01 10:00:00", "2023-01-01 11:00:00"
  ))
)
# Get available meteorological components
meteo_components &lt;- get_meteo_available(env_data)
print(meteo_components)
</code></pre>

<hr>
<h2 id='load_params'>Load Parameters from YAML File</h2><span id='topic+load_params'></span>

<h3>Description</h3>

<p>Reads a YAML file containing model parameters, including station settings,
variables, and configurations for various models. If no file path is
provided, the function defaults to loading <code>params.yaml</code> from the package's
<code>extdata</code> directory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_params(filepath = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="load_params_+3A_filepath">filepath</code></td>
<td>
<p>Character. Path to the YAML file. If <code>NULL</code>, the function
will attempt to load the default <code>params.yaml</code> provided in the package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The YAML file should define parameters in a structured format, such as:
</p>
<div class="sourceCode yaml"><pre>target: 'NO2'

lightgbm:
  nrounds: 200
  eta: 0.03
  num_leaves: 32

dynamic_regression:
  ntrain: 8760

random_forest:
  num.trees: 300
  max.depth: 10

meteo_variables:
  - GLO
  - TMP
</pre></div>


<h3>Value</h3>

<p>A list containing the parameters loaded from the YAML file.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>params &lt;- load_params()
</code></pre>

<hr>
<h2 id='load_uba_data_from_dir'>Load UBA Data from Directory</h2><span id='topic+load_uba_data_from_dir'></span>

<h3>Description</h3>

<p>This function loads data from CSV files in the specified directory. It supports two formats:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_uba_data_from_dir(data_dir)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="load_uba_data_from_dir_+3A_data_dir">data_dir</code></td>
<td>
<p>Character. Path to the directory containing <code>.csv</code> files.</p>
</td></tr>
</table>


<h3>Details</h3>


<ol>
<li><p> &quot;inv&quot;: Files must contain the following columns:
</p>

<ul>
<li> <p><code>Station</code>, <code>Komponente</code>, <code>Datum</code>, <code>Uhrzeit</code>, <code>Wert</code>.
</p>
</li></ul>

</li>
<li><p> &quot;24Spalten&quot;: Files must contain:
</p>

<ul>
<li> <p><code>Station</code>, <code>Komponente</code>, <code>Datum</code>, and columns <code>Wert01</code>, ..., <code>Wert24</code>.
</p>
</li></ul>

</li></ol>

<p>File names should include &quot;inv&quot; or &quot;24Spalten&quot; to indicate their format. The function scans
recursively for <code>.csv</code> files in subdirectories and combines the data into a single <code>data.table</code>
in long format.
Files that are not in the exected format will be ignored.
</p>


<h3>Value</h3>

<p>A <code>data.table</code> containing the loaded data in long format. Returns an error if no valid
files are found or the resulting dataset is empty.
</p>

<hr>
<h2 id='mock_env_data'>Mock Environmental Data</h2><span id='topic+mock_env_data'></span>

<h3>Description</h3>

<p>A small dataset of environmental variables created for testing and examples. This dataset
includes hourly observations with random values for meteorological and temporal variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mock_env_data
</code></pre>


<h3>Format</h3>

<p>A data frame with 100 rows and 12 variables:
</p>

<dl>
<dt>date</dt><dd><p>POSIXct. Date and time of the observation (hourly increments).</p>
</dd>
<dt>value</dt><dd><p>Numeric. Randomly generated target variable.</p>
</dd>
<dt>GLO</dt><dd><p>Numeric. Global radiation in W/m² (random values between 0 and 1000).</p>
</dd>
<dt>TMP</dt><dd><p>Numeric. Temperature in °C (random values between -10 and 35).</p>
</dd>
<dt>RFE</dt><dd><p>Numeric. Rainfall in mm (random values between 0 and 50).</p>
</dd>
<dt>WIG</dt><dd><p>Numeric. Wind speed in m/s (random values between 0 and 20).</p>
</dd>
<dt>WIR</dt><dd><p>Numeric. Wind direction in degrees (random values between 0 and 360).</p>
</dd>
<dt>LDR</dt><dd><p>Numeric. Longwave downward radiation in W/m² (random values between 0 and 500).</p>
</dd>
<dt>day_julian</dt><dd><p>Integer. Julian day of the year, ranging from 1 to 10.</p>
</dd>
<dt>weekday</dt><dd><p>Integer. Day of the week, ranging from 1 (Monday) to 7 (Sunday).</p>
</dd>
<dt>hour</dt><dd><p>Integer. Hour of the day, ranging from 0 to 23.</p>
</dd>
<dt>date_unix</dt><dd><p>Numeric. UNIX timestamp (seconds since 1970-01-01 00:00:00 UTC).</p>
</dd>
</dl>



<h3>Source</h3>

<p>Generated within the package for example purposes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mock_env_data)
head(mock_env_data)
</code></pre>

<hr>
<h2 id='plot_counterfactual'>Prepare Plot Data and Plot Counterfactuals</h2><span id='topic+plot_counterfactual'></span>

<h3>Description</h3>

<p>Smooths the predictions using a rolling mean, prepares the data for plotting,
and generates the counterfactual plot for the application window. Data before
the red box are reference window, red box is buffer and values after black,
dotted line are effect window.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_counterfactual(
  predictions,
  params,
  window_size = 14,
  date_effect_start = NULL,
  buffer = 0,
  plot_pred_interval = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_counterfactual_+3A_predictions">predictions</code></td>
<td>
<p>The data.table containing the predictions (hourly)</p>
</td></tr>
<tr><td><code id="plot_counterfactual_+3A_params">params</code></td>
<td>
<p>Parameters for plotting, including the target variable.</p>
</td></tr>
<tr><td><code id="plot_counterfactual_+3A_window_size">window_size</code></td>
<td>
<p>The window size for the rolling mean (default is 14 days).</p>
</td></tr>
<tr><td><code id="plot_counterfactual_+3A_date_effect_start">date_effect_start</code></td>
<td>
<p>A date. Start date of the
effect that is to be evaluated. The data from this point onwards is disregarded
for calculating model performance</p>
</td></tr>
<tr><td><code id="plot_counterfactual_+3A_buffer">buffer</code></td>
<td>
<p>Integer. An additional, optional buffer window before
<code>date_effect_start</code> to account for uncertainty in the effect start point.
Disregards additional buffer data points for model evaluation.
Use <code>buffer=0</code> for no buffer.</p>
</td></tr>
<tr><td><code id="plot_counterfactual_+3A_plot_pred_interval">plot_pred_interval</code></td>
<td>
<p>Boolean. If <code>TRUE</code>, shows a grey band of the prediction
interval.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The optional grey ribbon is a prediction interval for the hourly values. The
interpretation for a 90% prediction interval (to be defined in <code>alpha</code> parameter
of <code><a href="#topic+run_counterfactual">run_counterfactual()</a></code>) is that 90% of the true hourly values
(not the rolled means) lie within the grey band. This might be helpful for
getting an idea of the variance of the data and predictions.
</p>


<h3>Value</h3>

<p>A ggplot object with the counterfactual plot. Can be adjusted further,
e.g. set limits for the y-axis for better visualisation.
</p>

<hr>
<h2 id='plot_station_measurements'>Descriptive plot of daily time series data</h2><span id='topic+plot_station_measurements'></span>

<h3>Description</h3>

<p>This function produces descriptive time-series plots with smoothing
for the meteorological and potential target variables that were measured at a station.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_station_measurements(
  env_data,
  variables,
  years = NULL,
  smoothing_factor = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_station_measurements_+3A_env_data">env_data</code></td>
<td>
<p>A data table of measurements of one air quality measurement station.
The data should contain the following columns:
</p>

<dl>
<dt>Station</dt><dd><p>Station identifier where the data was collected.</p>
</dd>
<dt>Komponente</dt><dd><p>The environmental component being measured
(e.g., temperature, NO2).</p>
</dd>
<dt>Wert</dt><dd><p>The measured value of the component.</p>
</dd>
<dt>date</dt><dd><p>The timestamp for the observation,
formatted as a Date-Time object in the format
<code>"YYYY-MM-DD HH:MM:SS"</code> (e.g., &quot;2010-01-01 07:00:00&quot;).</p>
</dd>
<dt>Komponente_txt</dt><dd><p>A textual description or label for the component.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="plot_station_measurements_+3A_variables">variables</code></td>
<td>
<p>list of variables to plot. Must be in <code>env_data$Komponente</code>.
Meteorological variables can be obtained from params.yaml.</p>
</td></tr>
<tr><td><code id="plot_station_measurements_+3A_years">years</code></td>
<td>
<p>Optional. A numeric vector, list, or a range specifying the
years to restrict the plotted data.
You can provide:
</p>

<ul>
<li><p> A single year: <code>years = 2020</code>
</p>
</li>
<li><p> A numeric vector of years: <code>years = c(2019, 2020, 2021)</code>
</p>
</li>
<li><p> A range of years: <code>years = 2019:2021</code>
If not provided, data for all available years will be used.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_station_measurements_+3A_smoothing_factor">smoothing_factor</code></td>
<td>
<p>A number that defines the magnitude of smoothing.
Default is 1. Smaller numbers correspond to less smoothing, larger numbers to more.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot</code> object. This object contains:
</p>

<ul>
<li><p> A time-series line plot for each variable in <code>variables</code>.
</p>
</li>
<li><p> Smoothed lines, with smoothing defined by <code>smoothing_factor</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(data.table)
env_data &lt;- data.table(
  Station = "Station_1",
  Komponente = rep(c("TMP", "NO2"), length.out = 100),
  Wert = rnorm(100, mean = 20, sd = 5),
  date = rep(seq.POSIXt(as.POSIXct("2022-01-01"), , "hour", 50), each = 2),
  year = 2022,
  Komponente_txt = rep(c("Temperature", "NO2"), length.out = 100)
)
plot &lt;- plot_station_measurements(env_data, variables = c("TMP", "NO2"))

</code></pre>

<hr>
<h2 id='prepare_data_for_modelling'>Prepare Data for Training a model</h2><span id='topic+prepare_data_for_modelling'></span>

<h3>Description</h3>

<p>Prepares environmental data by filtering for relevant components,
converting the data to a wide format, and adding temporal features. Should be
called before
<code><a href="#topic+split_data_counterfactual">split_data_counterfactual()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data_for_modelling(env_data, params)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_data_for_modelling_+3A_env_data">env_data</code></td>
<td>
<p>A data table in long format.
Must include the following columns:
</p>

<dl>
<dt>Station</dt><dd><p>Station identifier for the data.</p>
</dd>
<dt>Komponente</dt><dd><p>The environmental component being measured
(e.g., temperature, NO2).</p>
</dd>
<dt>Wert</dt><dd><p>The measured value of the component.</p>
</dd>
<dt>date</dt><dd><p>Timestamp as <code>POSIXct</code> object in <code style="white-space: pre;">&#8288;YYYY-MM-DD HH:MM:SS&#8288;</code> format.</p>
</dd>
<dt>Komponente_txt</dt><dd><p>A textual description of the component.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="prepare_data_for_modelling_+3A_params">params</code></td>
<td>
<p>A list of modelling parameters loaded from <code>params.yaml</code>.
Must include:
</p>

<dl>
<dt>meteo_variables</dt><dd><p>A vector of meteorological variable names.</p>
</dd>
<dt>target</dt><dd><p>The name of the target variable.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.table</code> in wide format, with columns:
<code>date</code>, one column per component, and temporal features
like <code>date_unix</code>, <code>day_julian</code>, <code>weekday</code>, and <code>hour</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>env_data &lt;- data.table::data.table(
  Station = c("StationA", "StationA", "StationA"),
  Komponente = c("NO2", "TMP", "NO2"),
  Wert = c(50, 20, 40),
  date = as.POSIXct(c("2023-01-01 10:00:00", "2023-01-01 11:00:00", "2023-01-02 12:00:00"))
)
params &lt;- list(meteo_variables = c("TMP"), target = "NO2")
prepared_data &lt;- prepare_data_for_modelling(env_data, params)
print(prepared_data)

</code></pre>

<hr>
<h2 id='rescale_predictions'>Rescale predictions to original scale.</h2><span id='topic+rescale_predictions'></span>

<h3>Description</h3>

<p>This function rescales the predicted values (<code>prediction</code>, <code>prediction_lower</code>,
<code>prediction_upper</code>). The scaling is reversed using the means and
standard deviations that were saved from the training data. It is the inverse
function to <code><a href="#topic+scale_data">scale_data()</a></code> and should be used only in combination.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rescale_predictions(scale_result, dt_predictions)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rescale_predictions_+3A_scale_result">scale_result</code></td>
<td>
<p>A list object returned by <code><a href="#topic+scale_data">scale_data()</a></code>,
containing the means and standard deviations used for scaling.</p>
</td></tr>
<tr><td><code id="rescale_predictions_+3A_dt_predictions">dt_predictions</code></td>
<td>
<p>A data frame containing the predictions,
including columns <code>prediction</code>, <code>prediction_lower</code>, <code>prediction_upper</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with the predictions and numeric columns rescaled back
to their original scale.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(mock_env_data)
scale_res &lt;- scale_data(
  train_data = mock_env_data[1:80, ],
  apply_data = mock_env_data[81:100, ]
)
params &lt;- load_params()
res &lt;- run_lightgbm(
  train = scale_res$train, test = scale_res$apply,
  params$lightgbm, alpha = 0.9, calc_shaps = FALSE
)
dt_predictions &lt;- res$dt_predictions
rescaled_predictions &lt;- rescale_predictions(scale_res, dt_predictions)

</code></pre>

<hr>
<h2 id='retrend_predictions'>Restors the trend in the prediction</h2><span id='topic+retrend_predictions'></span>

<h3>Description</h3>

<p>Takes a dataframe of predictions as returned by any of
the 'run_model' functions and restores a trend which was previously
removed via <code><a href="#topic+detrend">detrend()</a></code>. This is necessary for the predictions
and the true values to have the same units. The function is basically
the inverse function to <code><a href="#topic+detrend">detrend()</a></code> and should only be used in
combination with it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>retrend_predictions(dt_predictions, trend, log_transform = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="retrend_predictions_+3A_dt_predictions">dt_predictions</code></td>
<td>
<p>Dataframe of predictions with columns <code>value</code>,
<code>prediction</code>, <code>prediction_lower</code>, <code>prediction_upper</code></p>
</td></tr>
<tr><td><code id="retrend_predictions_+3A_trend">trend</code></td>
<td>
<p>lm object generated by <code><a href="#topic+detrend">detrend()</a></code></p>
</td></tr>
<tr><td><code id="retrend_predictions_+3A_log_transform">log_transform</code></td>
<td>
<p>Returns values to solution space, if they have been
log transformed during detrending. Use only in combination with <code>log_transform</code>
parameter in detrend function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Retrended dataframe with same structure as <code>dt_predictions</code>
which is returned by any of the run_model() functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(mock_env_data)
split_data &lt;- list(
  train = mock_env_data[1:80, ],
  apply = mock_env_data[81:100, ]
)
params &lt;- load_params()
detrended_list &lt;- detrend(split_data,
  mode = "linear"
)
trend &lt;- detrended_list$model
detrended_train &lt;- detrended_list$train
detrended_apply &lt;- detrended_list$apply
result &lt;- run_lightgbm(
  train = detrended_train,
  test = detrended_apply,
  model_params = params$lightgbm,
  alpha = 0.9,
  calc_shaps = FALSE
)
retrended_predictions &lt;- retrend_predictions(result$dt_predictions, trend)

</code></pre>

<hr>
<h2 id='run_counterfactual'>Full counterfactual simulation run</h2><span id='topic+run_counterfactual'></span>

<h3>Description</h3>

<p>Chains detrending, training of a selected model, prediction and retrending together
for ease of use. See documentation of individual functions for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_counterfactual(
  split_data,
  params,
  detrending_function = "none",
  model_type = "rf",
  alpha = 0.9,
  log_transform = FALSE,
  calc_shaps = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="run_counterfactual_+3A_split_data">split_data</code></td>
<td>
<p>List of two named dataframes called train and apply</p>
</td></tr>
<tr><td><code id="run_counterfactual_+3A_params">params</code></td>
<td>
<p>A list of parameters that define the following:
</p>

<dl>
<dt>meteo_variables</dt><dd><p>A character vector specifying the names of the
meteorological variables used as inputs.</p>
</dd>
<dt>model</dt><dd><p>A list of hyperparameters for training the chosen model. Name of this list
and its parameters depend on the chosen models. See <code><a href="#topic+run_dynamic_regression">run_dynamic_regression()</a></code>,
<code><a href="#topic+run_lightgbm">run_lightgbm()</a></code>, <code><a href="#topic+run_rf">run_rf()</a></code> and <code><a href="#topic+run_fnn">run_fnn()</a></code> functions for details</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="run_counterfactual_+3A_detrending_function">detrending_function</code></td>
<td>
<p>String which defines type of trend to remove.
Options are &quot;linear&quot;,&quot;quadratic&quot;, &quot;exponential&quot;, &quot;spline&quot;, &quot;none&quot;. See <code><a href="#topic+detrend">detrend()</a></code>
and <code><a href="#topic+retrend_predictions">retrend_predictions()</a></code> for details.</p>
</td></tr>
<tr><td><code id="run_counterfactual_+3A_model_type">model_type</code></td>
<td>
<p>String to decide which model to use. Current options random
forest &quot;rf&quot;, gradient boosted decision trees &quot;lightgbm&quot;, &quot;dynamic_regression&quot; and feedforward neural network &quot;fnn&quot;</p>
</td></tr>
<tr><td><code id="run_counterfactual_+3A_alpha">alpha</code></td>
<td>
<p>Confidence level of the prediction interval between 0 and 1.</p>
</td></tr>
<tr><td><code id="run_counterfactual_+3A_log_transform">log_transform</code></td>
<td>
<p>If TRUE, uses log transformation during detrending and
retrending. For details see <code><a href="#topic+detrend">detrend()</a></code> documentation</p>
</td></tr>
<tr><td><code id="run_counterfactual_+3A_calc_shaps">calc_shaps</code></td>
<td>
<p>Boolean value. If TRUE, calculate SHAP values for the
method used and format them so they can be visualised with <code><a href="shapviz.html#topic+sv_importance">shapviz:sv_importance()</a></code> and
<code><a href="shapviz.html#topic+sv_dependence">shapviz:sv_dependence()</a></code>.
The SHAP values are generated for a subset (or all, depending on the size of the dataset) of the
test data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame of predictions, model and importance
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mock_env_data)
split_data &lt;- list(
  train = mock_env_data[1:80, ],
  apply = mock_env_data[81:100, ]
)
params &lt;- load_params()
res &lt;- run_counterfactual(split_data, params, detrending_function = "linear")
prediction &lt;- res$retrended_predictions
random_forest_model &lt;- res$model
</code></pre>

<hr>
<h2 id='run_dynamic_regression'>Run the dynamic regression model</h2><span id='topic+run_dynamic_regression'></span>

<h3>Description</h3>

<p>This function trains a dynamic regression model with fourier transformed temporal features
and meteorological variables as external regressors on the
specified training dataset and makes predictions on the test dataset in a
counterfactual scenario. This is referred to as a dynamic regression model in
<a href="https://otexts.com/fpp3/dynamic.html">Forecasting: Principles and Practise, Chapter 10 - Dynamic regression models</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_dynamic_regression(train, test, params, alpha, calc_shaps)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="run_dynamic_regression_+3A_train">train</code></td>
<td>
<p>Dataframe of train data as returned by the <code><a href="#topic+split_data_counterfactual">split_data_counterfactual()</a></code>
function.</p>
</td></tr>
<tr><td><code id="run_dynamic_regression_+3A_test">test</code></td>
<td>
<p>Dataframe of test data as returned by the <code><a href="#topic+split_data_counterfactual">split_data_counterfactual()</a></code>
function.</p>
</td></tr>
<tr><td><code id="run_dynamic_regression_+3A_params">params</code></td>
<td>
<p>list of hyperparameters to use in dynamic_regression call. Only uses ntrain to specify
the number of data points to use for training. Default is 8760 which results in
1 year of hourly data</p>
</td></tr>
<tr><td><code id="run_dynamic_regression_+3A_alpha">alpha</code></td>
<td>
<p>Confidence level of the prediction interval between 0 and 1.</p>
</td></tr>
<tr><td><code id="run_dynamic_regression_+3A_calc_shaps">calc_shaps</code></td>
<td>
<p>Boolean value. If TRUE, calculate SHAP values for the
method used and format them so they can be visualised with <code><a href="shapviz.html#topic+sv_importance">shapviz:sv_importance()</a></code> and
<code><a href="shapviz.html#topic+sv_dependence">shapviz:sv_dependence()</a></code>.
The SHAP values are generated for a subset (or all, depending on the size of the dataset) of the
test data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: Runs the dynamic regression model for individualised use with own data pipeline.
Otherwise use <code><a href="#topic+run_counterfactual">run_counterfactual()</a></code> to call this function.
</p>


<h3>Value</h3>

<p>Data frame of predictions and model
</p>

<hr>
<h2 id='run_fnn'>Train a Feedforward Neural Network (FNN) in a Counterfactual Scenario.</h2><span id='topic+run_fnn'></span>

<h3>Description</h3>

<p>Trains a feedforward neural network (FNN) model on the
specified training dataset and makes predictions on the test dataset in a
counterfactual scenario. The model uses meteorological variables and
sin/cosine-transformed features. Scales the data before training and rescales
predictions, as the model does not converge with unscaled data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_fnn(train, test, params, calc_shaps)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="run_fnn_+3A_train">train</code></td>
<td>
<p>A data frame or tibble containing the training dataset,
including the target variable (<code>value</code>)
and meteorological variables specified in <code>params$meteo_variables</code>.</p>
</td></tr>
<tr><td><code id="run_fnn_+3A_test">test</code></td>
<td>
<p>A data frame or tibble containing the test dataset on which
predictions will be made,
using the same meteorological variables as in the training dataset.</p>
</td></tr>
<tr><td><code id="run_fnn_+3A_params">params</code></td>
<td>
<p>A list of parameters that define the following:
</p>

<dl>
<dt>meteo_variables</dt><dd><p>A character vector specifying the names of the
meteorological variables used as inputs.</p>
</dd>
<dt>fnn</dt><dd><p>A list of hyperparameters for training the feedforward neural
network, including:
</p>

<ul>
<li> <p><code>activation_fun</code>: The activation function for the hidden
layers (e.g., &quot;sigmoid&quot;, &quot;tanh&quot;).
</p>
</li>
<li> <p><code>momentum</code>: The momentum factor for training.
</p>
</li>
<li> <p><code>learningrate_scale</code>: Factor for adjusting learning rate.
</p>
</li>
<li> <p><code>output_fun</code>: The activation function for the output layer
</p>
</li>
<li> <p><code>batchsize</code>: The size of the batches during training.
</p>
</li>
<li> <p><code>hidden_dropout</code>: Dropout rate for the hidden layers to
prevent overfitting.
</p>
</li>
<li> <p><code>visible_dropout</code>: Dropout rate for the input layer.
</p>
</li>
<li> <p><code>hidden_layers</code>: A vector specifying the number of neurons
in each hidden layer.
</p>
</li>
<li> <p><code>num_epochs</code>: Number of epochs (iterations) for training.
</p>
</li>
<li> <p><code>learning_rate</code>: Initial learning rate.
</p>
</li></ul>

</dd>
</dl>
</td></tr>
<tr><td><code id="run_fnn_+3A_calc_shaps">calc_shaps</code></td>
<td>
<p>Boolean value. If TRUE, calculate SHAP values for the
method used and format them so they can be visualised with
<code><a href="shapviz.html#topic+sv_importance">shapviz:sv_importance()</a></code> and
<code><a href="shapviz.html#topic+sv_dependence">shapviz:sv_dependence()</a></code>.
The SHAP values are generated for a subset (or all, depending on the size of the dataset) of the
test data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides flexibility for users with their own data pipelines
or workflows. For a simplified pipeline, consider using
<code><a href="#topic+run_counterfactual">run_counterfactual()</a></code>.
</p>
<p>Experiment with hyperparameters such as <code>learning_rate</code>,
<code>batchsize</code>, <code>hidden_layers</code>, and <code>num_epochs</code> to improve
performance.
</p>
<p>Warning: Using many or large hidden layers in combination with a high number
of epochs can lead to long training times.
</p>


<h3>Value</h3>

<p>A list with three elements:
</p>

<dl>
<dt><code>dt_predictions</code></dt><dd><p>A data frame containing the test data along
with the predicted values:
</p>

<dl>
<dt><code>prediction</code></dt><dd><p>The predicted values from the FNN model.</p>
</dd>
<dt><code>prediction_lower</code></dt><dd><p>The same predicted values, as no
quantile model is available yet for FNN.</p>
</dd>
<dt><code>prediction_upper</code></dt><dd><p>The same predicted values, as no
quantile model is available yet for FNN.</p>
</dd>
</dl>

</dd>
<dt><code>model</code></dt><dd><p>The trained FNN model object from the
<code>deepnet::nn.train()</code> function.</p>
</dd>
<dt><code>importance</code></dt><dd><p>SHAP importance values (if
<code>calc_shaps = TRUE</code>). Otherwise, <code>NULL</code>.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data(mock_env_data)
params &lt;- load_params()
res &lt;- run_fnn(
  train = mock_env_data[1:80, ],
  test = mock_env_data[81:100, ], params,
  calc_shaps = FALSE
)
</code></pre>

<hr>
<h2 id='run_lightgbm'>Run gradient boosting model with lightgbm</h2><span id='topic+run_lightgbm'></span>

<h3>Description</h3>

<p>This function trains a gradient boosting model (lightgbm) on the
specified training dataset and makes predictions on the test dataset in a
counterfactual scenario. The model uses meteorological variables and temporal features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_lightgbm(train, test, model_params, alpha, calc_shaps)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="run_lightgbm_+3A_train">train</code></td>
<td>
<p>Dataframe of train data as returned by the <code><a href="#topic+split_data_counterfactual">split_data_counterfactual()</a></code>
function.</p>
</td></tr>
<tr><td><code id="run_lightgbm_+3A_test">test</code></td>
<td>
<p>Dataframe of test data as returned by the <code><a href="#topic+split_data_counterfactual">split_data_counterfactual()</a></code>
function.</p>
</td></tr>
<tr><td><code id="run_lightgbm_+3A_model_params">model_params</code></td>
<td>
<p>list of hyperparameters to use in lgb.train call.
See <code><a href="lightgbm.html#topic+lgb.train">lightgbm:lgb.train()</a></code> params argument for details.</p>
</td></tr>
<tr><td><code id="run_lightgbm_+3A_alpha">alpha</code></td>
<td>
<p>Confidence level of the prediction interval between 0 and 1.</p>
</td></tr>
<tr><td><code id="run_lightgbm_+3A_calc_shaps">calc_shaps</code></td>
<td>
<p>Boolean value. If TRUE, calculate SHAP values for the
method used and format them so they can be visualised with <code><a href="shapviz.html#topic+sv_importance">shapviz:sv_importance()</a></code> and
<code><a href="shapviz.html#topic+sv_dependence">shapviz:sv_dependence()</a></code>.
The SHAP values are generated for a subset (or all, depending on the size of the dataset) of the
test data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: Runs the gradient boosting model for individualised use with own data pipeline.
Otherwise use <code><a href="#topic+run_counterfactual">run_counterfactual()</a></code>  to call this function.
</p>


<h3>Value</h3>

<p>List with data frame of predictions and model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(mock_env_data)
split_data &lt;- list(
  train = mock_env_data[1:80, ],
  apply = mock_env_data[81:100, ]
)
params &lt;- load_params()
variables &lt;- c("day_julian", "weekday", "hour", params$meteo_variables)
res &lt;- run_lightgbm(
  train = mock_env_data[1:80, ],
  test = mock_env_data[81:100, ], params$lightgbm, alpha = 0.9,
  calc_shaps = FALSE
)
prediction &lt;- res$dt_predictions
model &lt;- res$model

</code></pre>

<hr>
<h2 id='run_rf'>Run random forest model with ranger</h2><span id='topic+run_rf'></span>

<h3>Description</h3>

<p>This function trains a random forest model (ranger) on the
specified training dataset and makes predictions on the test dataset in a
counterfactual scenario. The model uses meteorological variables and temporal features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_rf(train, test, model_params, alpha, calc_shaps)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="run_rf_+3A_train">train</code></td>
<td>
<p>Dataframe of train data as returned by the <code><a href="#topic+split_data_counterfactual">split_data_counterfactual()</a></code>
function.</p>
</td></tr>
<tr><td><code id="run_rf_+3A_test">test</code></td>
<td>
<p>Dataframe of test data as returned by the <code><a href="#topic+split_data_counterfactual">split_data_counterfactual()</a></code>
function.</p>
</td></tr>
<tr><td><code id="run_rf_+3A_model_params">model_params</code></td>
<td>
<p>list of hyperparameters to use in ranger call. See <code><a href="ranger.html#topic+ranger">ranger:ranger()</a></code> for options.</p>
</td></tr>
<tr><td><code id="run_rf_+3A_alpha">alpha</code></td>
<td>
<p>Confidence level of the prediction interval between 0 and 1.</p>
</td></tr>
<tr><td><code id="run_rf_+3A_calc_shaps">calc_shaps</code></td>
<td>
<p>Boolean value. If TRUE, calculate SHAP values for the
method used and format them so they can be visualised with <code><a href="shapviz.html#topic+sv_importance">shapviz:sv_importance()</a></code> and
<code><a href="shapviz.html#topic+sv_dependence">shapviz:sv_dependence()</a></code>.
The SHAP values are generated for a subset (or all, depending on the size of the dataset) of the
test data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: Runs the random forest model for individualised use with own data pipeline.
Otherwise use <code><a href="#topic+run_counterfactual">run_counterfactual()</a></code>  to call this function.
</p>


<h3>Value</h3>

<p>List with data frame of predictions and model
</p>

<hr>
<h2 id='sample_data_DESN025'>Environmental Data for Modelling from station DESN025 in Leipzig-Mitte.</h2><span id='topic+sample_data_DESN025'></span>

<h3>Description</h3>

<p>This dataset contains environmental measurements from the Leipzig Mitte
station provided by the Sächsisches Landesamt für Umwelt,
Landwirtschaft und Geologie (LfULG). Alterations in the data: Codes for
incorrect values have been removed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_data_DESN025
</code></pre>


<h3>Format</h3>

<p>A data table with the following columns:
</p>

<dl>
<dt>Station</dt><dd><p>Station identifier where the data was collected.</p>
</dd>
<dt>Komponente</dt><dd><p>The environmental component being measured
(e.g., temperature, NO2).</p>
</dd>
<dt>Wert</dt><dd><p>The measured value of the component.</p>
</dd>
<dt>date</dt><dd><p>The timestamp for the observation, formatted as a Date-Time
object in the format
<code>"YYYY-MM-DD HH:MM:SS"</code> (e.g., &quot;2010-01-01 07:00:00&quot;).</p>
</dd>
<dt>Komponente_txt</dt><dd><p>A textual description or label for the component.</p>
</dd>
</dl>

<p>The dataset is structured in a long format and is prepared for further
transformation into a wide format for modelling.
</p>


<h3>Details</h3>

<p>The dataset is licensed under the &quot;Data Licence Germany – attribution – version 2.0 (DL-DE-BY-2.0)&quot;.
(1) Any use will be permitted provided it fulfils the requirements of this &quot;Data licence Germany – attribution – Version 2.0&quot;.
</p>
<p>The data and meta-data provided may, for commercial and non-commercial use, in particular
</p>

<ul>
<li><p> be copied, printed, presented, altered, processed and transmitted to third parties;
</p>
</li>
<li><p> be merged with own data and with the data of others and be combined to form new and independent datasets;
</p>
</li>
<li><p> be integrated in internal and external business processes, products and applications in public and non-public electronic networks.
</p>
</li></ul>

<p>(2) The user must ensure that the source note contains the following information:
</p>

<ul>
<li><p> the name of the provider,
</p>
</li>
<li><p> the annotation &quot;Data licence Germany – attribution – Version 2.0&quot; or &quot;dl-de/by-2-0&quot; referring to the licence text available at www.govdata.de/dl-de/by-2-0, and
</p>
</li>
<li><p> a reference to the dataset (URI).
</p>
</li></ul>

<p>This applies only if the entity keeping the data provides the pieces of information 1-3 for the source note.
</p>
<p>(3) Changes, editing, new designs or other amendments must be marked as such in the source note.
</p>
<p>For more information on the license, visit <a href="https://www.govdata.de/dl-de/by-2-0">https://www.govdata.de/dl-de/by-2-0</a>.
</p>


<h3>Source</h3>

<p>Sächsisches Landesamt für Umwelt, Landwirtschaft und Geologie (LfULG).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(sample_data_DESN025)
params &lt;- load_params()
dt_prepared &lt;- prepare_data_for_modelling(sample_data_DESN025, params)

</code></pre>

<hr>
<h2 id='scale_data'>Standardize Training and Application Data</h2><span id='topic+scale_data'></span>

<h3>Description</h3>

<p>This function standardizes numeric columns of the <code>train_data</code> and applies
the same scaling (mean and standard deviation) to the corresponding columns
in <code>apply_data</code>. It returns the standardized data along with the scaling
parameters (means and standard deviations). This is particularly important
for neural network approaches as they tend to be numerically unstable and
deteriorate otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scale_data(train_data, apply_data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="scale_data_+3A_train_data">train_data</code></td>
<td>
<p>A data frame containing the training dataset to be
standardized. It must contain numeric columns.</p>
</td></tr>
<tr><td><code id="scale_data_+3A_apply_data">apply_data</code></td>
<td>
<p>A data frame  containing the dataset to which the scaling
from <code>train_data</code> will be applied.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following elements:
</p>
<table role = "presentation">
<tr><td><code>train</code></td>
<td>
<p>The standardized training data.</p>
</td></tr>
<tr><td><code>apply</code></td>
<td>
<p>The <code>apply_data</code> scaled using the means and standard deviations
from the <code>train_data</code>.</p>
</td></tr>
<tr><td><code>means</code></td>
<td>
<p>The means of the numeric columns in <code>train_data</code>.</p>
</td></tr>
<tr><td><code>sds</code></td>
<td>
<p>The standard deviations of the numeric columns in <code>train_data</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(mock_env_data)
detrended_list &lt;- list(
  train = mock_env_data[1:80, ],
  apply = mock_env_data[81:100, ]
)
scale_result &lt;- scale_data(
  train_data = detrended_list$train,
  apply_data = detrended_list$apply
)
scaled_train &lt;- scale_result$train
scaled_apply &lt;- scale_result$apply
</code></pre>

<hr>
<h2 id='split_data_counterfactual'>Split Data into Training and Application Datasets</h2><span id='topic+split_data_counterfactual'></span>

<h3>Description</h3>

<p>Splits prepared data into training and application datasets based on
specified date ranges for a business-as-usual scenario. Data before
<code>application_start</code> and after <code>application_end</code> is used as training data,
while data within the date range is used for application.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_data_counterfactual(dt_prepared, application_start, application_end)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="split_data_counterfactual_+3A_dt_prepared">dt_prepared</code></td>
<td>
<p>The prepared data table.</p>
</td></tr>
<tr><td><code id="split_data_counterfactual_+3A_application_start">application_start</code></td>
<td>
<p>The start date(date object) for the application
period of the business-as-usual simulation. This coincides with the start of
the reference window.
Can be created by e.g. lubridate::ymd(&quot;20191201&quot;)</p>
</td></tr>
<tr><td><code id="split_data_counterfactual_+3A_application_end">application_end</code></td>
<td>
<p>The end date(date object)  for the application period
of the business-as-usual simulation. This coincides with the end of
the effect window.
Can be created by e.g. lubridate::ymd(&quot;20191201&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements:
</p>

<dl>
<dt>train</dt><dd><p>Data outside the application period.</p>
</dd>
<dt>apply</dt><dd><p>Data within the application period.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>dt_prepared &lt;- data.table::data.table(
  date = as.Date(c("2023-01-01", "2023-01-05", "2023-01-10")),
  value = c(50, 60, 70)
)
result &lt;- split_data_counterfactual(
  dt_prepared,
  application_start = as.Date("2023-01-03"),
  application_end = as.Date("2023-01-08")
)
print(result$train)
print(result$apply)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
