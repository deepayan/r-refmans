<!DOCTYPE html><html><head><title>Help for package randomGLM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {randomGLM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#accuracyMeasures'>
<p>Accuracy measures for a 2x2 confusion matrix or for vectors of predicted and observed values.</p></a></li>
<li><a href='#brainCancer'><p>The brain cancer data set</p></a></li>
<li><a href='#mini'><p>An example data set derived from the brain cancer data set</p></a></li>
<li><a href='#predict.randomGLM'><p>Prediction from a random generalized linear model predictor</p></a></li>
<li><a href='#randomGLM'><p>Random generalized linear model predictor</p></a></li>
<li><a href='#thinRandomGLM'><p>Random generalized linear model predictor thinning</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.10-1</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-04-08</td>
</tr>
<tr>
<td>Title:</td>
<td>Random General Linear Model Prediction</td>
</tr>
<tr>
<td>Author:</td>
<td>Lin Song, Peter Langfelder</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Peter Langfelder &lt;peter.langfelder@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0), MASS, foreach, doParallel,</td>
</tr>
<tr>
<td>Imports:</td>
<td>Hmisc, geometry, survival, matrixStats, parallel</td>
</tr>
<tr>
<td>ZipData:</td>
<td>no</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Description:</td>
<td>A bagging predictor based on generalized linear models (GLMs) is implemented. The method is published in 
   Song, Langfelder and Horvath (2013) &lt;<a href="https://doi.org/10.1186%2F1471-2105-14-5">doi:10.1186/1471-2105-14-5</a>&gt;. </td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://horvath.genetics.ucla.edu/rglm/">https://horvath.genetics.ucla.edu/rglm/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-04-08 16:30:58 UTC; plangfelder</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-04-10 23:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='accuracyMeasures'>
Accuracy measures for a 2x2 confusion matrix or for vectors of predicted and observed values.
</h2><span id='topic+accuracyMeasures'></span>

<h3>Description</h3>

<p>The function calculates various prediction accuracy statistics for predictions of binary or quantitative
(continuous) responses. For binary classification, the function calculates 
the error rate, accuracy, sensitivity, specificity, positive predictive value, and
other accuracy measures. For quantitative prediction, the function calculates correlation, R-squared, error
measures, and the C-index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>accuracyMeasures(
  predicted, 
  observed = NULL, 
  type = c("auto", "binary", "quantitative"),
  levels = if (isTRUE(all.equal(dim(predicted), c(2,2)))) colnames(predicted)
    else if (is.factor(predicted))
      sort(unique(c(as.character(predicted), as.character(observed))))
    else sort(unique(c(observed, predicted))),
  negativeLevel = levels[2], 
  positiveLevel = levels[1] )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="accuracyMeasures_+3A_predicted">predicted</code></td>
<td>
<p> either a a 2x2 confusion matrix (table) whose entries contain non-negative
integers, or a vector of predicted values. Predicted values can be binary or quantitative (see <code>type</code>
below). If a 2x2 matrix is given, it must have valid column and row names that specify the levels of the
predicted and observed variables whose counts the matrix is giving (e.g., the
function <code><a href="base.html#topic+table">table</a></code> sets the names appropriately.) If it is a 2x2 table and the table 
contains non-negative real (non-integer) numbers the function outputs a warning.
</p>
</td></tr>
<tr><td><code id="accuracyMeasures_+3A_observed">observed</code></td>
<td>
<p> if <code>predicted</code> is a vector of predicted values, this (<code>observed</code>) must be a
vector of the same length giving the &quot;gold standard&quot; (or observed) values. Ignored if <code>predicted</code> is a 2x2
table. </p>
</td></tr>
<tr><td><code id="accuracyMeasures_+3A_type">type</code></td>
<td>
<p> character string specifying the type of the prediction problem (i.e., values in the 
<code>predicted</code> and <code>observed</code> vectors). The
default <code>"auto"</code> decides type automatically: 
if <code>predicted</code> is a 2x2 table or if the number of unique values in
the concatenation of <code>predicted</code> and <code>observed</code> is 2, the prediction problem (type) is assumed to
be binary, otherwise it is assumed to be quantitative. Inconsistent specification (for example, when
<code>predicted</code> is a 2x2 matrix and <code>type</code> is <code>"quantitative"</code>) trigger errors. </p>
</td></tr> 
<tr><td><code id="accuracyMeasures_+3A_levels">levels</code></td>
<td>
<p> a 2-element vector specifying the two levels of binary variables. Only used if <code>type</code> is
<code>"binary"</code> (or <code>"auto"</code> that results in the binary type). Defaults to either the column names of
the confusion matrix (if the matrix is specified) or to the sorted unique values of <code>observed</code> and
<code>opredicted</code>. </p>
</td></tr>
<tr><td><code id="accuracyMeasures_+3A_negativelevel">negativeLevel</code></td>
<td>
<p> the binary value (level) that corresponds to the negative outcome. Note that the
default is the second of the sorted levels (for example, if levels are 1,2, the default negative level is
2). Only used if <code>type</code> is
<code>"binary"</code> (or <code>"auto"</code> that results in the binary type).</p>
</td></tr>
<tr><td><code id="accuracyMeasures_+3A_positivelevel">positiveLevel</code></td>
<td>
<p> the binary value (level) that corresponds to the positive outcome. Note that the 
default is the second of the sorted levels (for example, if levels are 1,2, the default negative level is
2). Only used if <code>type</code> is
<code>"binary"</code> (or <code>"auto"</code> that results in the binary type).</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>The rows of the 2x2 table tab must correspond to a test (or predicted) outcome and the columns to a true
outcome (&quot;gold standard&quot;). A table that relates a predicted outcome to a true test outcome is also known as
confusion matrix. Warning: To correctly calculate sensitivity and specificity, the positive and negative
outcome must be properly specified so they can be matched to the appropriate rows and columns in the
confusion table. 
</p>
<p>Interchanging the negative and positive levels swaps the estimates of the sensitivity and specificity 
but has no effect on the error rate or
accuracy. Specifically, denote by <code>pos</code> the index of the positive level in the confusion table, and by
<code>neg</code> th eindex of the negative level in the confusion table. 
The function then defines number of true positives=TP=tab[pos, pos], no.false positives
=FP=tab[pos, neg], no.false negatives=FN=tab[neg, pos], no.true negatives=TN=tab[neg, neg]. 
Then Specificity= TN/(FP+TN)
Sensitivity= TP/(TP+FN) NegativePredictiveValue= TN/(FN + TN) PositivePredictiveValue= TP/(TP + FP)
FalsePositiveRate = 1-Specificity FalseNegativeRate = 1-Sensitivity Power = Sensitivity
LikelihoodRatioPositive = Sensitivity / (1-Specificity) LikelihoodRatioNegative =
(1-Sensitivity)/Specificity. The naive error rate is the error rate of a constant (naive) predictor that
assigns the same outcome to all samples. The prediction of the naive predictor equals the most frequenly
observed outcome. Example: Assume you want to predict disease status and 70 percent of the observed samples
have the disease. Then the naive predictor has an error rate of 30 percent (since it only misclassifies 30
percent of the healthy individuals). </p>


<h3>Value</h3>

<p>Data frame with two columns: 
</p>
<table>
<tr><td><code>Measure</code></td>
<td>
<p>this column contais character strings that specify name of the accuracy measure.</p>
</td></tr>
<tr><td><code>Value</code></td>
<td>
<p>this column contains the numeric estimates of the corresponding accuracy measures.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Steve Horvath and Peter Langfelder
</p>


<h3>References</h3>

<p>http://en.wikipedia.org/wiki/Sensitivity_and_specificity 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>m=100
trueOutcome=sample( c(1,2),m,replace=TRUE)
predictedOutcome=trueOutcome
# now we noise half of the entries of the predicted outcome
predictedOutcome[ 1:(m/2)] =sample(predictedOutcome[ 1:(m/2)] )
tab=table(predictedOutcome, trueOutcome) 
accuracyMeasures(tab)

# Same result:
accuracyMeasures(predictedOutcome, trueOutcome)

</code></pre>

<hr>
<h2 id='brainCancer'>The brain cancer data set</h2><span id='topic+brainCancer'></span>

<h3>Description</h3>

<p>2 sets containing the gene expression profiles of 55 and 65 brain cancer patients respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(brainCancer)
</code></pre>


<h3>Format</h3>

<p><code>brainCancer</code> is a list of 2 components: train and test. &quot;train&quot; is a numeric matrix with 55 samples (rows) across 5000 genes (columns). &quot;test&quot; is a numeric matrix with 65 samples (rows) across the same 5000 genes (columns).     
</p>


<h3>Author(s)</h3>

<p>Lin Song, Steve Horvath</p>


<h3>Source</h3>

<p>Horvath S, Zhang B, Carlson M, Lu K, Zhu S, Felciano R, Laurance M, Zhao W, Shu Q, Lee Y, Scheck A,
Liau L, Wu H, Geschwind D, Febbo P, Kornblum H, TF C, Nelson S, Mischel P: Analysis of Oncogenic Signaling
Networks in Glioblastoma Identifies ASPM as a Novel Molecular Target. Proc Natl Acad Sci U S A 2006,
103(46):17402-7.</p>


<h3>References</h3>

<p>Lin Song, Peter Langfelder, Steve Horvath: Random generalized linear model: a highly accurate and interpretable ensemble predictor. BMC Bioinformatics (2013)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brainCancer)
</code></pre>

<hr>
<h2 id='mini'>An example data set derived from the brain cancer data set</h2><span id='topic+mini'></span>

<h3>Description</h3>

<p>This example contains one training set, one test set, a corresponding binary outcome and a corresponding continuous outcome. Outcomes are gene traits derived from the brain cancer data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mini)
</code></pre>


<h3>Format</h3>

<p><code>mini</code> is a list of 6 components: x, xtest, yB, yBtest, yC and yCtest. &quot;x&quot; is a numeric matrix with 55 samples (rows) across 4999 genes (columns). &quot;xtest&quot; is a numeric matrix with 65 samples (rows) across the same 4999 genes (columns). They are subsets of the original <code>brainCancer</code> data. One gene is left out. &quot;yC&quot; and &quot;yCtest&quot; are continuous outcomes equal to the expression values of the left out gene in the training and test set respectively. &quot;yB&quot; and &quot;yBtest&quot; are binary outcomes dichotomized at the median based on &quot;yC&quot; and &quot;yCtest&quot;.      
</p>


<h3>Author(s)</h3>

<p>Lin Song, Steve Horvath</p>


<h3>Source</h3>

<p>Horvath S, Zhang B, Carlson M, Lu K, Zhu S, Felciano R, Laurance M, Zhao W, Shu Q, Lee Y, Scheck A,
Liau L, Wu H, Geschwind D, Febbo P, Kornblum H, TF C, Nelson S, Mischel P: Analysis of Oncogenic Signaling
Networks in Glioblastoma Identifies ASPM as a Novel Molecular Target. Proc Natl Acad Sci U S A 2006,
103(46):17402-7.</p>


<h3>References</h3>

<p>Lin Song, Peter Langfelder, Steve Horvath: Random generalized linear model: a highly accurate and interpretable ensemble predictor. BMC Bioinformatics (2013)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mini)
</code></pre>

<hr>
<h2 id='predict.randomGLM'>Prediction from a random generalized linear model predictor </h2><span id='topic+predict.randomGLM'></span>

<h3>Description</h3>

<p> Implements a predict method on a previously-constructed random  generalized linear model predictor and new data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'randomGLM'
predict(object, newdata, type=c("response", "class"), 
                 thresholdClassProb = object$details$thresholdClassProb, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.randomGLM_+3A_object">object</code></td>
<td>
<p>a <code>randomGLM</code> object such as one returned by <code><a href="#topic+randomGLM">randomGLM</a></code>. </p>
</td></tr>
<tr><td><code id="predict.randomGLM_+3A_newdata">newdata</code></td>
<td>
<p>specification of test data for which to calculate the prediction.</p>
</td></tr>
<tr><td><code id="predict.randomGLM_+3A_type">type</code></td>
<td>
<p>type of prediction required. Type &quot;response&quot; gives the fitted probabilities for classification, the fitted values for regression. Type &quot;class&quot; applies only to classification, and produces the predicted class labels.</p>
</td></tr>
<tr><td><code id="predict.randomGLM_+3A_thresholdclassprob">thresholdClassProb</code></td>
<td>
<p>the threshold of predictive probabilities to arrive at classification. Takes values between 0 and 1. Only used for binary outcomes.</p>
</td></tr>
<tr><td><code id="predict.randomGLM_+3A_...">...</code></td>
<td>
<p>other arguments that may be passed to and from methods. Currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates prediction on new test data. It only works if <code>object</code> contains the regression
models that were used to construct the predictor (see argument <code>keepModels</code> of the function
<code><a href="#topic+randomGLM">randomGLM</a></code>). 
</p>
<p>If the predictor was trained on a multi-class response, the prediction is applied to each of the
representing binary variables (see <code><a href="#topic+randomGLM">randomGLM</a></code> for details).
</p>


<h3>Value</h3>

<p>For continuous prediction, the predicted values. For classification of binary response, predicted class when
<code>type="class"</code>; or a two-column matrix giving the class probabilities if <code>type="response"</code>.
</p>
<p>If the predictor was trained on a multi-class response, the returned value is a matrix of &quot;cbind&quot;-ed results
for the representing individual binary variables (see <code><a href="#topic+randomGLM">randomGLM</a></code> for details).
</p>


<h3>Author(s)</h3>

<p>Lin Song, Steve Horvath and Peter Langfelder.
</p>


<h3>References</h3>

<p>Lin Song, Peter Langfelder, Steve Horvath: Random generalized linear model: a highly accurate and interpretable ensemble predictor. BMC Bioinformatics (2013)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## binary outcome prediction
# data generation
data(iris)
# Restrict data to first 100 observations
iris=iris[1:100,]
# Turn Species into a factor
iris$Species = as.factor(as.character(iris$Species))
# Select a training and a test subset of the 100 observations
set.seed(1)
indx = sample(100, 67, replace=FALSE)
xyTrain = iris[indx,]
xyTest = iris[-indx,]
xTrain = xyTrain[, -5]
yTrain = xyTrain[, 5]

xTest = xyTest[, -5]
yTest = xyTest[, 5]

# predict with a small number of bags 
# - normally nBags should be at least 100.
RGLM = randomGLM(
   xTrain, yTrain, 
   nCandidateCovariates=ncol(xTrain), 
   nBags=30, 
   keepModels = TRUE, nThreads = 1)

predicted = predict(RGLM, newdata = xTest, type="class")
table(predicted, yTest)

## continuous outcome prediction

x=matrix(rnorm(100*20),100,20)
y=rnorm(100)

xTrain = x[1:50,]
yTrain = y[1:50]
xTest = x[51:100,]
yTest = y[51:100]

RGLM = randomGLM(
   xTrain, yTrain, 
   classify=FALSE, 
   nCandidateCovariates=ncol(xTrain), 
   nBags=10, 
   keepModels = TRUE, nThreads = 1)

predicted = predict(RGLM, newdata = xTest)
</code></pre>

<hr>
<h2 id='randomGLM'>Random generalized linear model predictor</h2><span id='topic+randomGLM'></span>

<h3>Description</h3>

<p>Ensemble predictor comprised of individual generalized linear model predictors. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randomGLM(
  # Input data
  x, y, xtest = NULL,
  weights = NULL,

  # Which columns in x are categorical?
  categoricalColumns = NULL,
  maxCategoricalLevels = 2,

  # Include interactions?
  maxInteractionOrder = 1,
  includeSelfinteractions = TRUE,

  # Prediction type: type can be used to set 
  # the prediction type in a simplified way...
  type = c("auto", "linear", "binary", "count", "general", "survival"),

  # classify is retained mostly for backwards compatibility
  classify = switch(type, 
    auto = !is.Surv(y) &amp; (is.factor(y) | length(unique(y)) &lt; 4),
    linear = FALSE,
    binary = TRUE ,
    count = FALSE,
    general = FALSE,
    survival = FALSE),

  # family can be used to fine-tune the underlying regression model
  family = switch(type, 
    auto = NULL,
    linear = gaussian(link="identity"),
    binary = binomial(link=logit),
    count = poisson(link = "log"),
    general = NULL,
    survival = NULL),

  # Multi-level classification options - only apply to classification 
  # with multi-level response
  multiClass.global = TRUE,
  multiClass.pairwise = FALSE,
  multiClass.minObs = 1,
  multiClass.ignoreLevels = NULL,

  # Sampling options
  nBags = 100,
  replace = TRUE,
  sampleBaggingWeights = NULL,
  nObsInBag = if (replace) nrow(x) else as.integer(0.632 * nrow(x)),
  nFeaturesInBag = ceiling(ifelse(ncol(x)&lt;=10, ncol(x),
        ifelse(ncol(x)&lt;=300, (1.0276-0.00276*ncol(x))*ncol(x), ncol(x)/5))),
  minInBagObs = min( max( nrow(x)/2, 5), 2*nrow(x)/3),
  maxBagAttempts = 100*nBags,
  replaceBadBagFeatures = TRUE,

  # Individual ensemble member predictor options
  nCandidateCovariates=50,
  corFncForCandidateCovariates= cor,
  corOptionsForCandidateCovariates = list(method = "pearson", use="p"),
  mandatoryCovariates = NULL,
  interactionsMandatory = FALSE,
  keepModels = is.null(xtest),

  # Miscellaneous options
  thresholdClassProb = 0.5,
  interactionSeparatorForCoefNames = ".times.",
  randomSeed = 12345,
  nThreads = NULL,
  verbose =0 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randomGLM_+3A_x">x</code></td>
<td>
<p>a matrix whose rows correspond to observations (samples) and whose columns correspond to features (also known as covariates or variables). Thus, <code>x</code> contains the training data sets.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_y">y</code></td>
<td>
<p>outcome variable corresponding to the rows of <code>x</code>: at this point, one can either use a binary class outcome (factor variable) or a quantitative outcome (numeric variable). </p>
</td></tr>
<tr><td><code id="randomGLM_+3A_xtest">xtest</code></td>
<td>
<p>an optional matrix of a second data set (referred to as test data set while the data in
<code>x</code> are interpreted as training data). The number of rows can (and typically will) be different from
the number of rows in <code>x</code>.</p>
</td></tr> 
<tr><td><code id="randomGLM_+3A_weights">weights</code></td>
<td>
<p>optional specifications of sample weights for the regression models. Not to be confused
with <code>sampleBaggingWeights</code> below.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_categoricalcolumns">categoricalColumns</code></td>
<td>
<p>optional specifications of columns that are to be treated as categorical. If
not given, columns with at most <code>maxCategoricalLevels</code> (see below) unique values will be considered
categorical.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_maxcategoricallevels">maxCategoricalLevels</code></td>
<td>
<p>columns with no more than this number of unique values will be considered
categorical.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_maxinteractionorder">maxInteractionOrder</code></td>
<td>
<p> integer specifying the maximum interaction level. The default is to have no
interactions; numbers higher than 1 specify interactions up to that order. For example, 3 means quadratic
and cubic interactions will be included. Warning: higher order interactions greatly increase the computation
time. We see no benefit of using maxInteractionOrder&gt;2.</p>
</td></tr> 
<tr><td><code id="randomGLM_+3A_includeselfinteractions">includeSelfinteractions</code></td>
<td>
<p>logical: should self-interactions be included?</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_type">type</code></td>
<td>
<p>character string specifying the type of the response variable. Recognized values are (unique
abbreviations of) 
<code>"auto"</code>, <code>"linear"</code>, <code>"binary"</code>, <code>"count"</code>, <code>"general"</code>, and <code>"survival"</code>. See
Details for what the individual types mean.
</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_classify">classify</code></td>
<td>
<p>logical indicating whether the response is a categorical variable. This argument is
present mainly for backwards compatibility; please use <code>type</code> above to specify the type of the response
variable. If <code>TRUE</code> the response <code>y</code> will be interpreted as a binary variable and
logistic regression will be used. If <code>FALSE</code> the response <code>y</code> will be interpreted as a
quantitative numeric variable and a least squares regression model will be used to arrive at base learners.
Multi-level classification is split into a series of binary classification problems according to the
<code>multiClass...</code> arguments described below.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_family">family</code></td>
<td>
<p>Specification of family (see <code><a href="stats.html#topic+family">family</a></code>) for general linear model fitting
(see <code><a href="stats.html#topic+glm">glm</a></code>). Default values are provided for most of the specific types but can be
overriden here (for example, if a different link function is desired). There is no default value for
<code>type = "general"</code> and the user must specify a valid family. In contrast, this argument must be
<code>NULL</code> when <code>type = "survival"</code>. </p>
</td></tr>
<tr><td><code id="randomGLM_+3A_multiclass.global">multiClass.global</code></td>
<td>
<p>for multi-level classification, this logical argument controls whether binary
variables of the type &quot;level vs. all others&quot; are included in the series of binary variables to which
classification is applied. </p>
</td></tr>
<tr><td><code id="randomGLM_+3A_multiclass.pairwise">multiClass.pairwise</code></td>
<td>
<p>for multi-level classification, this logical argument controls whether binary 
variables of the type &quot;level A vs. level B&quot; are included in the series of binary variables to which 
classification is applied. </p>
</td></tr>
<tr><td><code id="randomGLM_+3A_multiclass.minobs">multiClass.minObs</code></td>
<td>
<p>an integer specifying the minimum number of observations for each level for the
level to be considered when creating &quot;level vs. all&quot; and &quot;level vs. level&quot; binary variables. </p>
</td></tr>
<tr><td><code id="randomGLM_+3A_multiclass.ignorelevels">multiClass.ignoreLevels</code></td>
<td>
<p>optional specifications of the values (levels) of the input response
<code>y</code> that are to be ignored when constructing level vs. all and level vs. level binary responses. Note
that observation with these values will be included in the &quot;all&quot; but will not have their own &quot;level vs. all&quot;
variables.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_nbags">nBags</code></td>
<td>
<p>number of bags (bootstrap samples) for defining the ensemble predictor, i.e. this also
corresponds to the number of individual GLMs.</p>
</td></tr> 
<tr><td><code id="randomGLM_+3A_replace">replace</code></td>
<td>
<p>logical. If <code>TRUE</code> then each bootstrap sample (bag) is defined by sampling with
replacement. Otherwise, sampling is carried out without replacement. We recommend to choose <code>TRUE</code>. </p>
</td></tr> 
<tr><td><code id="randomGLM_+3A_samplebaggingweights">sampleBaggingWeights</code></td>
<td>
<p>weights assigned to each observations (sample) during bootstrap sampling. Default <code>NULL</code> corresponds to equal weights.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_nobsinbag">nObsInBag</code></td>
<td>
<p>number of observations selected for each bag. Typically, a bootstrap sample (bag) has the
same number of observations as in the original data set (i.e. the rows of <code>x</code>).</p>
</td></tr>  
<tr><td><code id="randomGLM_+3A_nfeaturesinbag">nFeaturesInBag</code></td>
<td>
<p>number of features randomly selected for each bag. Features are randomly selected
without replacement. If there are no interaction terms, then this number should be smaller than or equal to
the number of rows of <code>x</code>.</p>
</td></tr>  
<tr><td><code id="randomGLM_+3A_mininbagobs">minInBagObs</code></td>
<td>
<p>minimum number of unique observations that constitute a valid bag. If the sampling
produces a bag with fewer than this number of unique observations, the bag is discarded and re-sampled again
until the number of unique observations is at least <code>minInBagObs</code>. This helps prevent too few unique
observations in a bag which would lead to problems with model selection. </p>
</td></tr>
<tr><td><code id="randomGLM_+3A_maxbagattempts">maxBagAttempts</code></td>
<td>
<p>Maximum number of bagging attempts.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_replacebadbagfeatures">replaceBadBagFeatures</code></td>
<td>
<p>If a feature in a bag contains missing data, should it be replaced?</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_ncandidatecovariates">nCandidateCovariates</code></td>
<td>
<p>Positive integer. The number of features that are being considered for forward
selection in each GLM (and in each bag). For each bag, the covariates are being chosen according their
highest absolute correlation with the outcome. In case of a binary outcome, it is first turned into a binary
numeric variable.  </p>
</td></tr> 
<tr><td><code id="randomGLM_+3A_corfncforcandidatecovariates">corFncForCandidateCovariates</code></td>
<td>
<p>the correlation function used to select candidate covariates. Choices
include <code><a href="stats.html#topic+cor">cor</a></code> or biweight midcorrelation, <code>bicor</code>, implemented in the
package WGCNA. The biweight mid-correlation is a robust alternative to the Pearson correlation.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_coroptionsforcandidatecovariates">corOptionsForCandidateCovariates</code></td>
<td>
<p>list of arguments to the correlation function. Note that robust correlations are sometimes problematic for binary class outcomes. When using the robust
correlation <code>bicor</code>, use the argument <code>"robustY=FALSE"</code>.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_mandatorycovariates">mandatoryCovariates</code></td>
<td>
<p>indices of features that are included as mandatory covariates in each GLM model. The default is no mandatory features. This allows the user to &quot;force&quot; variables into each GLM.</p>
</td></tr> 
<tr><td><code id="randomGLM_+3A_interactionsmandatory">interactionsMandatory</code></td>
<td>
<p>logical: should interactions of mandatory covariates be mandatory as well? Interactions are only included up to the level specified in <code>maxInteractionOrder</code>. </p>
</td></tr>
<tr><td><code id="randomGLM_+3A_keepmodels">keepModels</code></td>
<td>
<p>logical: should the regression models for each bag be kept? The models are necessary for future predictions using the <code>predict</code> function, predict() generic.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_thresholdclassprob">thresholdClassProb</code></td>
<td>
<p>number in the interval [0,1]. Recommended value 0.5. This parameter is only
relevant in case of a binary outcome, i.e. for a logistic regression model. Then this threshold will be
applied to the predictive class probabilities to arrive at binary outcome (class outcome). </p>
</td></tr>
<tr><td><code id="randomGLM_+3A_interactionseparatorforcoefnames">interactionSeparatorForCoefNames</code></td>
<td>
<p>a character string that will be used to separate feature names when
forming names of interaction terms. This is only used when interactions are actually taken into account (see
<code>maxInteractionLevel</code> above) and only affects coefficient names in models and columns names in returned
<code>featuresInForwardRegression</code> (see output value below). We recommend setting it so the interaction
separator does not conflict with any feature name since this may improve interpretability of the results.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_randomseed">randomSeed</code></td>
<td>
<p>NULL or integer. The seed for the random number generator. If NULL, the seed will not be
set. If non-NULL and the random generator has been initialized prior to the function call, the latter's
state is saved and restored upon exit.</p>
</td></tr> 
<tr><td><code id="randomGLM_+3A_nthreads">nThreads</code></td>
<td>
<p>number of threads (worker processes) to perform the calculation. If not given, will be
determined automatically as the number of available cores if the latter is 3 or less, 
and number of cores minus 1 if the number of available cores is 4 or more. Invalid entries (missing value,
zero or negative values etc.) are changed to 1, with a warning.</p>
</td></tr>
<tr><td><code id="randomGLM_+3A_verbose">verbose</code></td>
<td>
<p>value 0 or 1 which determines the level of verbosity. Zero means silent, 1 reports the bag number the function is working on. At this point verbose output only works if <code>nThreads</code>=1</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>randomGLM</code> can be used to predict a variety of different types of outcomes. The outcome
type is specified by the argument <code>type</code> as follows:
</p>
<p>If <code>type = "auto"</code>, the function will attempt to determine the response type automatically. If the response
is not a <code><a href="survival.html#topic+Surv">Surv</a></code> object and is a factor or has 3 or fewer unique values, 
it is assumed to be categorical. If the number of unique values is 2, the function uses logistic regression;
for categorical responses with more 3 or more possible values, see below.
</p>
<p>If <code>type = "linear"</code>, the responses is assumed to be numeric with Gaussian errors, and the function
will use linear regression.
</p>
<p>If <code>type = "binary"</code>, the response is assumed to be categorical (at this point not necessarily binary
but that may change in the future). If the response has 2 levels, logistic regression (binomial family with
the logit link) is used. If the response has more than 2 levels, see below.
</p>
<p>If <code>type = "count"</code>, the response is assumed to represent counts with Poisson-distributed errors, and
Poisson regression (poisson family with the logarithmic link) is used.
</p>
<p>If <code>type = "general"</code>, the function does not make assumption about the response type and the user must
specify an appropriate family (see <code><a href="stats.html#topic+stats">stats</a>{family}</code>).
</p>
<p>If <code>type = "survival"</code>, the function assumes the response is a censored time, that is a
<code><a href="survival.html#topic+Surv">Surv</a></code> object. In this case the argument <code>family</code> must be <code>NULL</code> (the
default) and the function uses Cox proportional hazard regression implemented in function
<code><a href="survival.html#topic+coxph">coxph</a></code>
</p>
<p>The function proceeds along the following steps:
</p>
<p>Step 1 (bagging): <code>nBags</code> bootstrapped data sets are being generated based on random sampling from the
original training data set (<code>x</code>,<code>y</code>). If a bag contains less than <code>minInBagObs</code> unique
observations or it contains all observations, it is discarded and re-sampled again.
</p>
<p>Step 2 (random subspace): For each bag, <code>nFeaturesInBag</code> features are randomly selected (without
replacement) from the columns of <code>x</code>. Optionally, interaction terms between the selected features can
be formed (see the argument <code>maxInteractionOrder</code>). 
</p>
<p>Step 3 (feature ranking): In each bag, features are ranked according to their correlation with the outcome
measure. Next the top <code>nCandidateCovariates</code> are being considered for forward selection in each GLM
(and in each bag).  
</p>
<p>Step 4 (forward selection): Forward variable selection is employed to define a multivariate GLM model of the
outcome in each bag. 
</p>
<p>Step 5 (aggregating the predictions): Prediction from each bag are aggregated. In case, of a quantitative
outcome, the predictions are simply averaged across the bags. 
</p>
<p>Generally, <code>nCandidateCovariates</code>&gt;100 is not recommended, because the forward
selection process is 
time-consuming. If arguments <code>"nBags=1, replace=FALSE, nObsInBag=nrow(x)"</code> are used, 
the function becomes a forward selection GLM predictor without bagging. 
</p>
<p>Classification of multi-level categorical responses is performed indirectly by turning the single
multi-class response into a set of binary variables. The set can include two types of binary variables:
Level vs. all others (this binary variable is 1 when the original response equals the level and zero
otherwise), and level A vs. level B (this binary variable is 0 when the response equals level A, 1 when the
response equals level B, and NA otherwise). 
For example, if the input response <code>y</code> contains observations with values (levels) &quot;A&quot;, &quot;B&quot;, 
&quot;C&quot;, the binary variables
will have names &quot;all.vs.A&quot; (1 means &quot;A&quot;, 0 means all others), &quot;all.vs.B&quot;,
&quot;all.vs.C&quot;, and optionally also &quot;A.vs.B&quot; (0 means &quot;A&quot;, 1 means &quot;B&quot;, NA means neither &quot;A&quot; nor &quot;B&quot;), &quot;A.vs.C&quot;,  
and &quot;B.vs.C&quot;. 
Note that using pairwise level vs. level binary variables be
very time-consuming since the number of such binary variables grows quadratically with the number of levels
in the response. The user has the option to limit which levels of the original response will have their
&quot;own&quot; binary variables, by setting the minimum observations a level must have to qualify for its own binary
variable, and by explicitly enumerating levels that should not have their own binary variables. Note that
such &quot;ignored&quot; levels are still included on the &quot;all&quot; side of &quot;level vs. all&quot; binary variables. 
</p>
<p>At this time the predictor does not attempt to summarize the binary variable classifications into a single
multi-level classification. 
</p>
<p>Training this predictor on data with fewer than 8 observations is not recommended (and the
function will warn about it). Due to the bagging
step, the number of unique observations in each bag is less than the number of observations in the input
data; the low number of unique observations can (and often will) lead to an essentially perfect fit which
makes it impossible to perfrom meaningful stepwise model selection.  
</p>
<p>Feature names: In general, the column names of input <code>x</code> are assumed to be the feature names. If
<code>x</code> has no column names (i.e., <code>colnames(x)</code> is <code>NULL</code>), stadard column names of the form
<code>"F01", "F02", ...</code> are used. If <code>x</code> has non-NULL column names, they are turned into valid and
unique names using the function <code><a href="base.html#topic+make.names">make.names</a></code>. If the function <code><a href="base.html#topic+make.names">make.names</a></code> returns
names that are not the same as the column names of <code>x</code>, the component <code>featureNamesChanged</code> will
be <code>TRUE</code> and the component <code>nameTranslationTable</code> contains the information about input and actual
used feature names. The feature names are used as predictor names in the individual models in each bag.
</p>


<h3>Value</h3>

<p>The function returns an object of class <code>randomGLM</code>. For continuous prediction or two-level
classification, this is a list with the following components:
</p>
<table>
<tr><td><code>predictedOOB</code></td>
<td>
<p>the continuous prediction (if <code>classify</code> is <code>FALSE</code>) 
or predicted classification (if <code>classify</code> is <code>TRUE</code>) of the input data based on out-of-bag
samples.
</p>
</td></tr>
<tr><td><code>predictedOOB.response</code></td>
<td>
<p>In case of a binary outcome, this is the predicted probability of each outcome
specified by <code>y</code> based on out-of-bag samples. In case of a continuous outcome, this is the predicted
value based on out-of-bag samples (i.e., a copy of <code>predictedOOB</code>).</p>
</td></tr>
<tr><td><code>predictedTest.cont</code></td>
<td>
<p>if test set is given, the predicted probability of each outcome specified by
<code>y</code> for test data for binary outcomes. In case of a continuous outcome, this is the test set predicted
value. </p>
</td></tr>
<tr><td><code>predictedTest</code></td>
<td>
<p>if test set is given, the predicted classification for test data. Only for binary
outcomes.</p>
</td></tr> 
<tr><td><code>candidateFeatures</code></td>
<td>
<p>candidate features in each bag. A list with one component per bag. Each component
is a matrix with <code>maxInteractionOrder</code> rows and <code>nCandidateCovariates</code> columns. 
Each column represents one
interaction obtained by multiplying the features indicated by the entries in each column (0 means no
feature, i.e. a lower order interaction). </p>
</td></tr>
<tr><td><code>featuresInForwardRegression</code></td>
<td>
<p>features selected by forward selection in each bag. A list with one
component per bag. Each component
is a matrix with <code>maxInteractionOrder</code> rows.
Each column represents one
interaction obtained by multiplying the features indicated by the entries in each column (0 means no 
feature, i.e. a lower order interaction). The column names contain human-readable names for the terms. </p>
</td></tr>
<tr><td><code>coefOfForwardRegression</code></td>
<td>
<p>coefficients of forward regression. A list with one
component per bag. Each component is a vector giving the coefficients of the model determined by forward
selection in the corresponding bag. The order of the coefficients is the same as the order of the terms in
the corresponding component of <code>featuresInForwardRegression</code>. </p>
</td></tr>
<tr><td><code>interceptOfForwardRegression</code></td>
<td>
<p>a vector with one component per bag giving the intercept of the
regression model in each bag.</p>
</td></tr>
<tr><td><code>bagObsIndx</code></td>
<td>
<p>a matrix with <code>nObsInBag</code> rows and <code>nBags</code> columns, giving the indices of
observations selected for each bag.</p>
</td></tr> 
<tr><td><code>timesSelectedByForwardRegression</code></td>
<td>
<p>a matrix of <code>maxInteractionOrder</code> rows and number of features
columns. Each entry gives the number of times the corresponding feature appeared in a predictor model at the
corresponding order of interactions. Interactions where a single feature enters more than once (e.g., a
quadratic interaction of the feature with itself) are counted once. 
</p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>the regression models for each bag. Predictor features in each bag model are named using
their</p>
</td></tr>
<tr><td><code>featureNamesChanged</code></td>
<td>
<p>logical indicating whether feature names were copied verbatim from column names
of <code>x</code> (<code>FALSE</code>) or whether they had to be changed to make them valid and unique names
(<code>TRUE</code>).</p>
</td></tr>
<tr><td><code>nameTranslationTable</code></td>
<td>
<p> only present if above <code>featureNamesChanged</code> is <code>TRUE</code>. 
A data frame with three columns and one row per input feature (column of input <code>x</code>) giving the  
feature number, original feature name, and modified feature name that is used for model fitting.</p>
</td></tr>
</table>
<p>In addition, the output value contains a copy of several input arguments. These are included to facilitate
prediction using the <code>predict</code> method. These returned values should be considered undocumented and may
change in the future.
</p>
<p>In the multi-level classification classification case, the returned list (still considered a valid
<code>randomGLM</code> object) contains the following components:
</p>
<table>
<tr><td><code>binaryPredictors</code></td>
<td>
<p>a list with one component per binary variable, containing the <code>randomGLM</code> 
predictor trained on that binary variable as the response. The list is named by the corresponding binary
variable. For example, if the input response <code>y</code> contains observations with values (levels) &quot;A&quot;, &quot;B&quot;,
&quot;C&quot;, the binary variables (and components of this list) 
will have names &quot;all.vs.A&quot; (1 means &quot;A&quot;, 0 means all others), &quot;all.vs.B&quot;,
&quot;all.vs.C&quot;, and optionally also &quot;A.vs.B&quot; (0 means &quot;A&quot;, 1 means &quot;B&quot;, NA means neither &quot;A&quot; nor &quot;B&quot;), &quot;A.vs.C&quot;,
and &quot;B.vs.C&quot;.  </p>
</td></tr>
<tr><td><code>predictedOOB</code></td>
<td>
<p>a matrix in which columns correspond to the binary variables and rows to samples,
containing the predicted binary classification for each binary variable. Columns names and meaning of 0 and
1 are described above. </p>
</td></tr>
<tr><td><code>predictedOOB.response</code></td>
<td>
<p>a matrix with two columns per binary variable, giving the class probabilities
for each of the two classes in each binary variables. Column names contain the variable and class names.</p>
</td></tr>
<tr><td><code>levelMatrix</code></td>
<td>
<p>a character matrix with two rows and one column per binary variable, giving the level
corresponding to value 0 (row 1) and level corresponding to value 1 (row 2). This encodes the same
information as the names of the <code>binaryPredictors</code> list but in a more programmer-friendly way.</p>
</td></tr>
</table>
<p>If input <code>xTest</code> is non-NULL, the components <code>predictedTest</code> and <code>predictedTest.response</code>
contain test set predictions analogous to <code>predictedOOB</code> and <code>predictedOOB.response</code>.
</p>


<h3>Author(s)</h3>

<p>Lin Song, Steve Horvath, Peter Langfelder. 
The function makes use of the <code>glm</code> function and other standard R functions. </p>


<h3>References</h3>

<p>Lin Song, Peter Langfelder, Steve Horvath: Random generalized linear model: a highly accurate and interpretable ensemble predictor. BMC Bioinformatics 2013 Jan 16;14:5. doi: 10.1186/1471-2105-14-5. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## binary outcome prediction
# data generation
data(iris)
# Restrict data to first 100 observations
iris=iris[1:100,]
# Turn Species into a factor
iris$Species = as.factor(as.character(iris$Species))
# Select a training and a test subset of the 100 observations
set.seed(1)
indx = sample(100, 67, replace=FALSE)
xyTrain = iris[indx,]
xyTest = iris[-indx,]
xTrain = xyTrain[, -5]
yTrain = xyTrain[, 5]

xTest = xyTest[, -5]
yTest = xyTest[, 5]

# predict with a small number of bags 
# - normally nBags should be at least 100.
RGLM = randomGLM(
   xTrain, yTrain, 
   xTest, 
   nCandidateCovariates=ncol(xTrain), 
   nBags=30, nThreads = 1)

yPredicted = RGLM$predictedTest
table(yPredicted, yTest)


## continuous outcome prediction

x=matrix(rnorm(100*20),100,20)
y=rnorm(100)

xTrain = x[1:50,]
yTrain = y[1:50]
xTest = x[51:100,]
yTest = y[51:100]

RGLM = randomGLM(
  xTrain, yTrain, 
  xTest, classify=FALSE, 
  nCandidateCovariates=ncol(xTrain), 
  nBags=10, 
  keepModels = TRUE, nThreads = 1)
</code></pre>

<hr>
<h2 id='thinRandomGLM'>Random generalized linear model predictor thinning</h2><span id='topic+thinRandomGLM'></span>

<h3>Description</h3>

<p> This function allows the user to define a &quot;thinned&quot; version of a random  generalized linear
model predictor by focusing on those features that occur relatively frequently. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thinRandomGLM(rGLM, threshold)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="thinRandomGLM_+3A_rglm">rGLM</code></td>
<td>
<p>a <code>randomGLM</code> object such as one returned by <code><a href="#topic+randomGLM">randomGLM</a></code>. </p>
</td></tr>
<tr><td><code id="thinRandomGLM_+3A_threshold">threshold</code></td>
<td>
<p> integer specifying the minimum of times a feature was selected across the bags in
<code>rGLM</code> for the feature to be kept. Note that only features selected <code>threshold +1</code> times and more
are retained. For the purposes of this count, appearances in interactions are not
counted. Features that appear 
<code>threshold</code> times or fewer are removed from the underlying regression models when the models are re-fit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function &quot;thins out&quot; (reduces) a previously-constructed random generalized linear model predictor by
removing rarely selected features and refitting each (generalized) linear model (GLM). 
Each GLM (per bag) is refit using only those
features that occur more than <code>threshold</code> times across the <code>nBags</code> number of bags. The
occurrence count excludes interactions (in other words, the threshold will be applied to the first row of
<code>timesSelectedByForwardRegression</code>).
</p>


<h3>Value</h3>

<p>The function returns a valid <code>randomGLM</code> object (see <code><a href="#topic+randomGLM">randomGLM</a></code> for details) that can be
used as input to the predict() method (see <code><a href="#topic+predict.randomGLM">predict.randomGLM</a></code>). The returned object contains a
copy of  the input <code>rGLM</code> in which the following components were modified:
</p>
<table>
<tr><td><code>predictedOOB</code></td>
<td>
<p>the updated continuous prediction (if <code>classify</code> is <code>FALSE</code>) 
or predicted classification (if <code>classify</code> is <code>TRUE</code>) of the input data based on out-of-bag 
samples.
</p>
</td></tr>
<tr><td><code>predictedOOB.response</code></td>
<td>
<p>In case of a binary outcome, the updated predicted probability of each
outcome
specified by <code>y</code> based on out-of-bag samples. In case of a continuous outcome, this is the predicted
value based on out-of-bag samples (i.e., a copy of <code>predictedOOB</code>).</p>
</td></tr>
<tr><td><code>featuresInForwardRegression</code></td>
<td>
<p>features selected by forward selection in each bag. A list with one
component per bag. Each component
is a matrix with <code>maxInteractionOrder</code> rows.
Each column represents one
interaction obtained by multiplying the features indicated by the entries in each column (0 means no
feature, i.e. a lower order interaction).  </p>
</td></tr>
<tr><td><code>coefOfForwardRegression</code></td>
<td>
<p>coefficients of forward regression. A list with one
component per bag. Each component is a vector giving the coefficients of the model determined by forward
selection in the corresponding bag. The order of the coefficients is the same as the order of the terms in
the corresponding component of <code>featuresInForwardRegression</code>. </p>
</td></tr>
<tr><td><code>interceptOfForwardRegression</code></td>
<td>
<p>a vector with one component per bag giving the intercept of the
regression model in each bag.</p>
</td></tr>
<tr><td><code>timesSelectedByForwardRegression</code></td>
<td>
<p>a matrix of <code>maxInteractionOrder</code> rows and number of features
columns. Each entry gives the number of times the corresponding feature appeared in a predictor model at the
corresponding order of interactions. Interactions where a single feature enters more than once (e.g., a
quadratic interaction of the feature with itself) are counted once.
</p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>the &quot;thinned&quot; regression models for each bag. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lin Song, Steve Horvath, Peter Langfelder</p>


<h3>References</h3>

<p>Lin Song, Peter Langfelder, Steve Horvath: Random generalized linear model: a highly accurate and interpretable ensemble predictor. BMC Bioinformatics (2013)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## binary outcome prediction
# data generation
data(iris)
# Restrict data to first 100 observations
iris=iris[1:100,]
# Turn Species into a factor
iris$Species = as.factor(as.character(iris$Species))
# Select a training and a test subset of the 100 observations
set.seed(1)
indx = sample(100, 67, replace=FALSE)
xyTrain = iris[indx,]
xyTest = iris[-indx,]
xTrain = xyTrain[, -5]
yTrain = xyTrain[, 5]

xTest = xyTest[, -5]
yTest = xyTest[, 5]

# predict with a small number of bags - normally nBags should be at least 100.
RGLM = randomGLM(
   xTrain, yTrain, 
   nCandidateCovariates=ncol(xTrain), 
   nBags=30, 
   keepModels = TRUE, nThreads = 1)
table(RGLM$timesSelectedByForwardRegression[1, ])
# 0  7 23 
# 2  1  1 

thinnedRGLM = thinRandomGLM(RGLM, threshold=7)
predicted = predict(thinnedRGLM, newdata = xTest, type="class")
predicted = predict(RGLM, newdata = xTest, type="class")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
