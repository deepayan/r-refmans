<!DOCTYPE html><html lang="en"><head><title>Help for package YEAB</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {YEAB}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#YEAB-package'><p>YEAB: Analyze Data from Analysis of Behavior Experiments</p></a></li>
<li><a href='#ab_range_normalization'><p>Normalization (or rescaling) between arbitrary a and b</p></a></li>
<li><a href='#balci2019'><p>Peak individual trial analysis using moving average</p></a></li>
<li><a href='#berm'><p>Biexponential Refractory Model (BERM)</p></a></li>
<li><a href='#biexponential'><p>Biexponential Model</p></a></li>
<li><a href='#bp_opt'><p>Find the best fit for individual trials using <code>optim</code></p></a></li>
<li><a href='#ceiling_multiple'><p>Find the nearest multiple</p></a></li>
<li><a href='#curv_index_fry'><p>Curvature index using Fry derivation</p></a></li>
<li><a href='#curv_index_int'><p>Curvature index by numerical integration</p></a></li>
<li><a href='#DD_data'><p>Delay Discounting Data</p></a></li>
<li><a href='#dd_example'><p>An example dataset of delays and normalized subjective values</p></a></li>
<li><a href='#entropy_kde2d'><p>Shannon entropy in two dimensions</p></a></li>
<li><a href='#eq_hyp'><p>Hyperbolic function</p></a></li>
<li><a href='#event_extractor'><p>Event extractor</p></a></li>
<li><a href='#exhaustive_lhl'><p>Individual trial analysis for peak procedure data</p></a></li>
<li><a href='#exhaustive_sbp'><p>Single breakpoint algorithm, the exhaustive version as the one used in Guilhardi &amp; Church 2004</p></a></li>
<li><a href='#exp_fit'><p>Exponential fit with nls</p></a></li>
<li><a href='#f_table'><p>Frequency table for binned data</p></a></li>
<li><a href='#fi60_raw_from_med'><p>Raw Fixed Interval Data</p></a></li>
<li><a href='#fleshler_hoffman'><p>Fleshler &amp; Hoffman (1962) progression</p></a></li>
<li><a href='#fwhm'><p>Full Width at Half Maximum</p></a></li>
<li><a href='#gauss_example'><p>Gaussian Example Data</p></a></li>
<li><a href='#gauss_example_1'><p>Gaussian Example 1 Data</p></a></li>
<li><a href='#gauss_example_2'><p>Gaussian Example 2 Data</p></a></li>
<li><a href='#gaussian_fit'><p>Gaussian + ramp fit with LM algorithm</p></a></li>
<li><a href='#gell_like'><p>Gellerman-like series</p></a></li>
<li><a href='#get_bins'><p>A function to binarize a numeric vector with a given resolution</p></a></li>
<li><a href='#hyp_data'><p>Simulated Data for Hyperbolic Discounting</p></a></li>
<li><a href='#hyp_data_list'><p>Hypothetical dataset list for testing purposes</p></a></li>
<li><a href='#hyperbolic_fit'><p>Hyperbolic fit with nls</p></a></li>
<li><a href='#ind_trials_obj_fun'><p>Objective function for finding the best fit for individual trials</p></a></li>
<li><a href='#ind_trials_opt'><p>Find the best fit for individual trials using <code>optim</code></p></a></li>
<li><a href='#KL_div'><p>Computes the Kullback-Leibler divergence based on kernel density estimates</p></a></li>
<li><a href='#mut_info_discrete'><p>Mutual information of continuous variables using discretization</p></a></li>
<li><a href='#mut_info_knn'><p>Mutual Information for Continuous Variables using kNN</p></a></li>
<li><a href='#n_between_intervals'><p>Find maximum value within intervals</p></a></li>
<li><a href='#objective_bp'><p>Objective function for the breakpoint optimization algorithm</p></a></li>
<li><a href='#optimize_biexponential'><p>Optimization Function for the Biexponential Model</p></a></li>
<li><a href='#r_times'><p>Reaction Times from Peak Procedure</p></a></li>
<li><a href='#read_med'><p>Process MED to csv based on standard data structure event.time</p></a></li>
<li><a href='#sample_from_density'><p>Sample from a density estimate</p></a></li>
<li><a href='#trapezoid_auc'><p>Area under the curve (AUC)</p></a></li>
<li><a href='#unit_normalization'><p>Min-max normalization (also feature rescaling)</p></a></li>
<li><a href='#val_in_interval'><p>True value in interval</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Analyze Data from Analysis of Behavior Experiments</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.6</td>
</tr>
<tr>
<td>Description:</td>
<td>Analyze data from behavioral experiments conducted using 'MED-PC' software developed by Med Associates Inc. 
	Includes functions to fit exponential and hyperbolic models for delay discounting tasks, exponential mixtures for 
	inter-response times, and Gaussian plus ramp models for peak procedure data, among others. For more details, refer to Alcala et al. (2023) &lt;<a href="https://doi.org/10.31234%2Fosf.io%2F8aq2j">doi:10.31234/osf.io/8aq2j</a>&gt;.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Imports:</td>
<td>cluster, doParallel, data.table, dplyr, FNN, foreach, ggplot2,
boot, grid, gridExtra, infotheo, ks, magrittr, minpack.lm,
Polychrome, scales, sfsmisc, MASS, KernSmooth, zoo, usethis,
stats, utils</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Author:</td>
<td>Emmanuel Alcala [aut, cre],
  Rodrigo Sosa [aut],
  Victor Reyes [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Emmanuel Alcala &lt;jealcalat@gmail.com&gt;</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-23</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-29 19:10:08 UTC; jealcalat</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-31 11:00:14 UTC</td>
</tr>
</table>
<hr>
<h2 id='YEAB-package'>YEAB: Analyze Data from Analysis of Behavior Experiments</h2><span id='topic+YEAB'></span><span id='topic+YEAB-package'></span>

<h3>Description</h3>

<p>Analyze data from behavioral experiments conducted using 'MED-PC' software developed by Med Associates Inc. Includes functions to fit exponential and hyperbolic models for delay discounting tasks, exponential mixtures for inter-response times, and Gaussian plus ramp models for peak procedure data, among others. For more details, refer to Alcala et al. (2023) <a href="https://doi.org/10.31234/osf.io/8aq2j">doi:10.31234/osf.io/8aq2j</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Emmanuel Alcala <a href="mailto:jealcalat@gmail.com">jealcalat@gmail.com</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Rodrigo Sosa <a href="mailto:rsosas@up.edu.mx">rsosas@up.edu.mx</a>
</p>
</li>
<li><p> Victor Reyes <a href="mailto:zickvic@hotmail.com">zickvic@hotmail.com</a>
</p>
</li></ul>


<hr>
<h2 id='ab_range_normalization'>Normalization (or rescaling) between arbitrary a and b</h2><span id='topic+ab_range_normalization'></span>

<h3>Description</h3>

<p>Normalization (or rescaling) between arbitrary a and b
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ab_range_normalization(x, a, b)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ab_range_normalization_+3A_x">x</code></td>
<td>
<p>numeric</p>
</td></tr>
<tr><td><code id="ab_range_normalization_+3A_a">a</code></td>
<td>
<p>numeric</p>
</td></tr>
<tr><td><code id="ab_range_normalization_+3A_b">b</code></td>
<td>
<p>numeric</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector rescaled in the range <code class="reqn">x' \in [a, b]</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 5:100
a &lt;- 0
b &lt;- 1
x_scaled &lt;- ab_range_normalization(x, a, b)
x_scaled
a &lt;- 100
b &lt;- 1000
x_scaled &lt;- ab_range_normalization(x, a, b)
x_scaled
</code></pre>

<hr>
<h2 id='balci2019'>Peak individual trial analysis using moving average</h2><span id='topic+balci2019'></span>

<h3>Description</h3>

<p>Peak individual trial analysis using moving average
</p>


<h3>Usage</h3>

<pre><code class='language-R'>balci2019(tasa_norm, bins)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="balci2019_+3A_tasa_norm">tasa_norm</code></td>
<td>
<p>numeric, normalized response rate</p>
</td></tr>
<tr><td><code id="balci2019_+3A_bins">bins</code></td>
<td>
<p>numeric</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on Balci et al 2010
</p>


<h3>Value</h3>

<p>a list with
params: a numeric vector with start, stop, spread and argmax (the bin at which response rate is max)
mov_av: the moving average
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("r_times")
# binarize r_times to create response rate at 2 sec bins
bins &lt;- get_bins(r_times, 0, 180, 2)
bin_res &lt;- 6
tasa &lt;- f_table(bins, 0, 180, bin_res)
tasa_norm &lt;- tasa$prop / max(tasa$prop)
bins &lt;- tasa$bins
balci_ind &lt;- balci2019(tasa_norm, bins)

plot(bins, tasa_norm, xlab = "6 sec bins", )
lines(bins, balci_ind$mov_av, col = "blue", lwd = 2)
abline(v = balci_ind$params[c(1, 2, 4)], lwd = c(1, 1, 2), col = c(1, 1, "red4"))
</code></pre>

<hr>
<h2 id='berm'>Biexponential Refractory Model (BERM)</h2><span id='topic+berm'></span><span id='topic+berm_log_likelihood'></span><span id='topic+map_onto'></span><span id='topic+param_conver'></span><span id='topic+optimize_berm'></span>

<h3>Description</h3>

<p>Implements the biexponential refractory model (BERM) using maximum likelihood estimation
to fit parameters for inter-response times (IRTs) within and between bouts.
</p>
<p>The model is defined as:
</p>
<p style="text-align: center;"><code class="reqn">p(IRT = \tau | \tau \ge \delta) = p w e^{-w (\tau - \delta)} + (1 - p) b e^{-b (\tau - \delta)}</code>
</p>

<p>where <code class="reqn">w</code> and <code class="reqn">b</code> are the rates for within and between bouts, <code class="reqn">p</code> is the proportion of responses in bouts,
and <code class="reqn">\delta</code> is the refractory period.
</p>
<p>Calculates the negative log-likelihood for the BERM model.
</p>
<p>Maps an unconstrained <code>d_hat</code> onto the observed minimum inter-response time (<code>d</code>), ensuring
that it aligns with model constraints.
</p>
<p>Converts raw parameters into their constrained forms to enforce model constraints on
parameters such as <code>w</code>, <code>l0</code>, <code>l1</code>, and <code>d</code>.
</p>
<p>Optimizes the log-likelihood function to estimate BERM model parameters based on
observed inter-response times.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>berm(irt, delta)

berm_log_likelihood(params, irt)

map_onto(d, d_hat)

param_conver(params, min_irt, parnames = c("w", "l0", "l1", "d"))

optimize_berm(irt)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="berm_+3A_irt">irt</code></td>
<td>
<p>A numeric vector of inter-response times.</p>
</td></tr>
<tr><td><code id="berm_+3A_delta">delta</code></td>
<td>
<p>A numeric value for the refractory period.</p>
</td></tr>
<tr><td><code id="berm_+3A_params">params</code></td>
<td>
<p>A numeric vector of raw, unconstrained parameters.</p>
</td></tr>
<tr><td><code id="berm_+3A_d">d</code></td>
<td>
<p>Minimum inter-response time.</p>
</td></tr>
<tr><td><code id="berm_+3A_d_hat">d_hat</code></td>
<td>
<p>Transformed parameter to be mapped onto <code>d</code>.</p>
</td></tr>
<tr><td><code id="berm_+3A_min_irt">min_irt</code></td>
<td>
<p>Minimum inter-response time for mapping <code>d</code>.</p>
</td></tr>
<tr><td><code id="berm_+3A_parnames">parnames</code></td>
<td>
<p>Optional vector of parameter names for labeling.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the negative log-likelihood based on biexponential functions
for the BERM model, adjusting parameters using <code>param_conver</code> to meet constraints.
</p>


<h3>Value</h3>

<p>A data frame with estimated parameters <code class="reqn">w</code> (within-bout rate), <code class="reqn">b</code> (between-bout rate), <code class="reqn">p</code>
(proportion of responses in bouts), and <code class="reqn">\delta</code> (adjusted refractory period).
</p>
<p>Negative log-likelihood value used for parameter estimation.
</p>
<p>Adjusted refractory period used in likelihood estimation.
</p>
<p>A named numeric vector of transformed parameters with constraints applied.
</p>
<p>A named vector of optimized parameters for the BERM model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(43)
l1 &lt;- 1 / 0.5
l2 &lt;- 1 / 0.1
p &lt;- 0.4
n &lt;- 200
delta &lt;- 0.03
irt &lt;- c(rexp(round(n * p), l1), rexp(round(n * (1 - p)), l2)) + delta
optimize_berm(irt)

set.seed(43)
l1 &lt;- 1 / 0.5
l2 &lt;- 1 / 0.1
p &lt;- 0.4
n &lt;- 200
delta &lt;- 0.03
irt &lt;- c(rexp(round(n * p), l1), rexp(round(n * (1 - p)), l2)) + delta
optimize_berm(irt)

</code></pre>

<hr>
<h2 id='biexponential'>Biexponential Model</h2><span id='topic+biexponential'></span>

<h3>Description</h3>

<p>Implements a simpler biexponential model without the refractory period parameter, <code class="reqn">\delta</code>.
</p>
<p>The simpler model is defined as:
</p>
<p style="text-align: center;"><code class="reqn">p(IRT = \tau) = p w e^{-w \tau} + (1 - p) b e^{-b \tau}</code>
</p>

<p>where <code class="reqn">w</code> and <code class="reqn">b</code> represent the within- and between-bout response rates, and <code class="reqn">p</code> is the proportion of responses in bouts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>biexponential(irt)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="biexponential_+3A_irt">irt</code></td>
<td>
<p>A numeric vector representing inter-response times.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with estimated parameters <code class="reqn">w</code> (proportion of responses in bouts), <code class="reqn">l0</code> (within-bout mean IRT),
and <code class="reqn">l1</code> (between-bout mean IRT).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(43)
l1 &lt;- 1 / 0.5
l2 &lt;- 1 / 0.1
p &lt;- 0.4
n &lt;- 200
irt &lt;- c(rexp(round(n * p), l1), rexp(round(n * (1 - p)), l2))
biexponential(irt)

</code></pre>

<hr>
<h2 id='bp_opt'>Find the best fit for individual trials using <code>optim</code></h2><span id='topic+bp_opt'></span>

<h3>Description</h3>

<p>Find the best fit for individual trials by minimizing the
negative sum of areas between the response rate and the target rate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bp_opt(r_times, trial_duration, optim_method = "Brent")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bp_opt_+3A_r_times">r_times</code></td>
<td>
<p>Vector of response times</p>
</td></tr>
<tr><td><code id="bp_opt_+3A_trial_duration">trial_duration</code></td>
<td>
<p>Duration of the trial</p>
</td></tr>
<tr><td><code id="bp_opt_+3A_optim_method">optim_method</code></td>
<td>
<p>character, the optimization method to use</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with the following columns:
</p>

<ul>
<li> <p><code>bp</code>: The breakpoint
</p>
</li>
<li> <p><code>r1</code>: The response rate before the breakpoint
</p>
</li>
<li> <p><code>r2</code>: The response rate after the breakpoint
</p>
</li>
<li> <p><code>d1</code>: The duration of the first state
</p>
</li>
<li> <p><code>d2</code>: The duration of the second state
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data("r_times")
r_times &lt;- r_times[r_times &lt; 60]
bp_from_opt &lt;- bp_opt(r_times, 60)
plot(r_times, seq_along(r_times),
  xlim = c(0, max(r_times)),
  main = "Cummulative Record",
  xlab = "Time (s)",
  ylab = "Cum Resp",
  col = 2, type = "s"
)
abline(v = bp_from_opt$bp)
</code></pre>

<hr>
<h2 id='ceiling_multiple'>Find the nearest multiple</h2><span id='topic+ceiling_multiple'></span>

<h3>Description</h3>

<p>Find the nearest multiple
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ceiling_multiple(x, multiple)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ceiling_multiple_+3A_x">x</code></td>
<td>
<p>numeric, the value for which we want to finde a multiple</p>
</td></tr>
<tr><td><code id="ceiling_multiple_+3A_multiple">multiple</code></td>
<td>
<p>numeric, the multiple</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the nearest multiple
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ceiling_multiple(8, 10) # returns 10
ceiling_multiple(12, 10) # returns 20
ceiling_multiple(21, 11) # returns 22
</code></pre>

<hr>
<h2 id='curv_index_fry'>Curvature index using Fry derivation</h2><span id='topic+curv_index_fry'></span>

<h3>Description</h3>

<p>Curvature index using Fry derivation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>curv_index_fry(cr, time_in, fi_val, n = 4)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="curv_index_fry_+3A_cr">cr</code></td>
<td>
<p>A numeric vector of cumulative response</p>
</td></tr>
<tr><td><code id="curv_index_fry_+3A_time_in">time_in</code></td>
<td>
<p>numeric, time (or the x axis in a cumulative response plot)</p>
</td></tr>
<tr><td><code id="curv_index_fry_+3A_fi_val">fi_val</code></td>
<td>
<p>the FI value</p>
</td></tr>
<tr><td><code id="curv_index_fry_+3A_n">n</code></td>
<td>
<p>numeric, the number of subintervals</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The curvature index as exposed by Fry
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("r_times")
r_times &lt;- r_times[r_times &lt; 60]
cr &lt;- seq_along(r_times)

plot(r_times, cr, type = "s", xlim = c(min(r_times), max(r_times)))
segments(
  x0 = min(r_times), y0 = min(cr),
  x1 = max(r_times), y1 = max(cr)
)
segments(
  x0 = min(r_times) + (max(r_times) - min(r_times)) / 2, y0 = min(cr),
  x1 = max(r_times), y1 = max(cr),
  col = "red"
)
curv_index_fry(cr, r_times, 60, 4)
</code></pre>

<hr>
<h2 id='curv_index_int'>Curvature index by numerical integration</h2><span id='topic+curv_index_int'></span>

<h3>Description</h3>

<p>Curvature index by numerical integration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>curv_index_int(cr, time_in)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="curv_index_int_+3A_cr">cr</code></td>
<td>
<p>numeric, cumulative response</p>
</td></tr>
<tr><td><code id="curv_index_int_+3A_time_in">time_in</code></td>
<td>
<p>numeric, time (or the x axis in a cumulative response plot)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric value that is the proportion of a rect triangle area minus
the area under the curve
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("r_times")
r_times &lt;- r_times[r_times &lt; 60]
cr &lt;- seq_along(r_times)

plot(r_times, cr, type = "s")
curv_index_int(cr, r_times)
segments(
  x0 = min(r_times), y0 = min(cr),
  x1 = max(r_times), y1 = max(cr)
)
segments(
  x0 = min(r_times) + (max(r_times) - min(r_times)) / 2, y0 = min(cr),
  x1 = max(r_times), y1 = max(cr),
  col = "red"
)
</code></pre>

<hr>
<h2 id='DD_data'>Delay Discounting Data</h2><span id='topic+DD_data'></span>

<h3>Description</h3>

<p>Delay Discounting Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DD_data
</code></pre>


<h3>Format</h3>

<p>A data frame with 6 rows and 2 columns:
</p>

<dl>
<dt>norm_sv</dt><dd><p>Normalized subjective values (numeric).</p>
</dd>
<dt>Delay</dt><dd><p>Delays (in seconds) for rewards (numeric).</p>
</dd>
</dl>



<h3>Details</h3>

<p>A dataset containing normalized subjective values (SV) and delays used in a delay discounting task.
</p>
<p>This dataset represents results from a delay discounting experiment. It demonstrates how subjective values decay with increasing delays.
</p>


<h3>Source</h3>

<p>Generated for a delay discounting analysis.
</p>

<hr>
<h2 id='dd_example'>An example dataset of delays and normalized subjective values</h2><span id='topic+dd_example'></span>

<h3>Description</h3>

<p>A dataset containing delay durations and their respective normalized subjective values (norm_sv).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dd_example
</code></pre>


<h3>Format</h3>

<p>A data frame with 6 rows and 2 variables:
</p>

<dl>
<dt>delay</dt><dd><p>The delay duration (in seconds).</p>
</dd>
<dt>norm_sv</dt><dd><p>The normalized subjective value corresponding to the delay.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data(dd_example)
head(dd_example)
</code></pre>

<hr>
<h2 id='entropy_kde2d'>Shannon entropy in two dimensions</h2><span id='topic+entropy_kde2d'></span>

<h3>Description</h3>

<p>Shannon entropy in two dimensions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy_kde2d(x, y, n_grid = 150)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="entropy_kde2d_+3A_x">x</code></td>
<td>
<p>numeric, random vector</p>
</td></tr>
<tr><td><code id="entropy_kde2d_+3A_y">y</code></td>
<td>
<p>numeric, random vector</p>
</td></tr>
<tr><td><code id="entropy_kde2d_+3A_n_grid">n_grid</code></td>
<td>
<p>numeric, number of grid cells to evaluate density</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric value of the entropy in 2D
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
# Generate a 2D normal distribution with a correlation of 0.6
n &lt;- 1000
mean &lt;- c(0, 0)
sd_x &lt;- 1
sd_y &lt;- 5
correlation &lt;- 0.6
sigma &lt;- matrix(
  c(
    sd_x^2,
    correlation * sd_x * sd_y,
    correlation * sd_x * sd_y,
    sd_y^2
  ),
  ncol = 2
)
library(MASS)
simulated_data &lt;- mvrnorm(n, mu = mean, Sigma = sigma)
x &lt;- simulated_data[, 1]
y &lt;- simulated_data[, 2]

cov_matr &lt;- cov(cbind(x, y))
sigmas &lt;- diag(cov_matr)
det_sig &lt;- prod(sigmas)
# According to https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Differential_entropy:

normal_entropy &lt;- function(k, pi, det_sig) {
  # The left part is a constant;
  (k / 2) * (1 + log(2 * pi)) + (1 / 2) * log(det_sig)
}

entropia &lt;- normal_entropy(k = 2, pi = pi, det_sig)
print(entropia) # Should return a value close to 4.3997

result &lt;- entropy_kde2d(x, y, n_grid = 50)
print(result) # Should return a value close to 4.2177

</code></pre>

<hr>
<h2 id='eq_hyp'>Hyperbolic function</h2><span id='topic+eq_hyp'></span>

<h3>Description</h3>

<p>An hyperbolic function to simulate delay discounting data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eq_hyp(k, delay)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="eq_hyp_+3A_k">k</code></td>
<td>
<p>numeric constant, the delay discounting parameter</p>
</td></tr>
<tr><td><code id="eq_hyp_+3A_delay">delay</code></td>
<td>
<p>vector of delays</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of subjective values between 0 and 1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>delay &lt;- seq(0, 10, len = 100)
k &lt;- 0.2
sv &lt;- eq_hyp(k, delay)
plot(delay, sv,
  xlab = "delay",
  ylab = "Sv",
  type = "l"
)
</code></pre>

<hr>
<h2 id='event_extractor'>Event extractor</h2><span id='topic+event_extractor'></span>

<h3>Description</h3>

<p>A function to slice data based on start and stop events. This function
should be used after read_med.r, which outputs a csv of 2 columns: time and
events (in that order). Its use is exemplified at the end of the function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>event_extractor(data_df, ev0, ev1, evname)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="event_extractor_+3A_data_df">data_df</code></td>
<td>
<p>data frame with events ev0 and ev1 (e.g., start of trial and reinforcement delivery)</p>
</td></tr>
<tr><td><code id="event_extractor_+3A_ev0">ev0</code></td>
<td>
<p>event ID start (where the event we want to extract begins)</p>
</td></tr>
<tr><td><code id="event_extractor_+3A_ev1">ev1</code></td>
<td>
<p>event ID stop. This event won't be returned, so keep in mind that</p>
</td></tr>
<tr><td><code id="event_extractor_+3A_evname">evname</code></td>
<td>
<p>a string for the event name, for identification purposes. For example
if the event we want to extract is component 1 in a multiple-2 schedule,
this can be eventname = &quot;c1&quot;, so when we extract the second
component we can row-combine both in a unique dataframe.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Works by trials
</p>


<h3>Value</h3>

<p>data frame with nrows x 4 columns of time, events, cum_id and evname
</p>


<h3>Examples</h3>

<pre><code class='language-R'># If we have a component starting with 5 and ending with 3, 
# say a Fixed Interval 15s and a dataframe of events from the read_med() function,
# we can extract the data of component "FI15" following the next steps:
# 0 - From the output of read_med.R function, load the extracted data and assign it to df
# 1 - source the event_extractor.R function
# 2 - use it with the appropiate arguments as follows

# read raw data from MED
data("fi60_raw_from_med")
# see first 10 lines
head(fi60_raw_from_med, 10)
# create a temporary file to avoid non-staged installation warning
temp_file &lt;- tempfile(fileext = ".txt")
# write the data to the temporary file
writeLines(fi60_raw_from_med, temp_file)
# Process the file using read_med
example_processed &lt;- read_med(
  fname = temp_file, save_file = FALSE,
  col_r = "C:", out = TRUE,
  col_names = c("time", "event"), num_col = 6, time_dot_event = TRUE
)

# Extract specific events (FI15 in this case)
extracted_FI15 &lt;- event_extractor(
  data_df = example_processed,
  ev0 = 5, ev1 = 3,
  evname = "FI15"
)

# Display the first rows of the extracted data
head(extracted_FI15, 30)

</code></pre>

<hr>
<h2 id='exhaustive_lhl'>Individual trial analysis for peak procedure data</h2><span id='topic+exhaustive_lhl'></span>

<h3>Description</h3>

<p>Individual trial analysis for peak procedure data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exhaustive_lhl(r_times, trial_duration)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="exhaustive_lhl_+3A_r_times">r_times</code></td>
<td>
<p>numeric, the times that a response was emitted in a trial</p>
</td></tr>
<tr><td><code id="exhaustive_lhl_+3A_trial_duration">trial_duration</code></td>
<td>
<p>numeric, the peak trial duration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame of start, stop, spread, middle time (mid) and the response
rate at each state (r1 for low, r2 for high and r3 for the second low rate state)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("r_times")
trial_duration &lt;- max(r_times) |&gt; ceiling() # 180
bps &lt;- exhaustive_lhl(r_times, trial_duration)
plot(
  density(
    r_times,
    adjust = 0.8,
    from = 0,
    to = 180
  ),
  main = "",
  ylab = expression(italic(p(t[R]))),
  xlab = "time in peak trial"
)
abline(v = 60, lty = 2)
bps &lt;- exhaustive_lhl(r_times, 180)
abline(v = c(bps$start, bps$stop), col = 2, lty = 2, lwd = 2)
# compare it with fwhm
den &lt;- density(r_times, from = 0, to = trial_duration)
fval &lt;- fwhm(den$x, den$y)
x1 &lt;- fval$peak - fval$fwhm / 2
x2 &lt;- fval$peak + fval$fwhm / 2
plot(den)
abline(v = c(x1, fval$peak, x2), col = c("blue", 1, "blue"))
</code></pre>

<hr>
<h2 id='exhaustive_sbp'>Single breakpoint algorithm, the exhaustive version as the one used in Guilhardi &amp; Church 2004</h2><span id='topic+exhaustive_sbp'></span>

<h3>Description</h3>

<p>Single breakpoint algorithm, the exhaustive version as the one used in Guilhardi &amp; Church 2004
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exhaustive_sbp(r_times, trial_duration)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="exhaustive_sbp_+3A_r_times">r_times</code></td>
<td>
<p>numeric, the times at which a response was emitted in a trial</p>
</td></tr>
<tr><td><code id="exhaustive_sbp_+3A_trial_duration">trial_duration</code></td>
<td>
<p>numeric, the duration of the IF interval</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This algorithm performs an extensive search of every combination (t1, t2)
where t1 starts in the first response through (length(r_times) - 1)
</p>


<h3>Value</h3>

<p>A data frame of 3 columns
<code>bp</code> a numeric value which corresponds to the time at which a break point was detected
<code>r1</code> a numeric value of the response rate <em>before</em> the breakpoint
<code>r2</code> a numeric value of the responser rate <em>after</em> the breakpoint
<code>d1</code> a numeric value of the duration of the first state
<code>d2</code> a numeric value of the duration of the second state
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("r_times")
r_times &lt;- r_times[r_times &lt; 60]
single_bp &lt;- exhaustive_sbp(r_times, 60)

plot(r_times, seq_along(r_times),
  xlim = c(0, max(r_times)),
  main = "Cummulative Record",
  xlab = "Time (s)",
  ylab = "Cum Resp",
  col = 2, type = "s"
)
abline(v = single_bp$bp)

bp_from_opt &lt;- bp_opt(r_times, 60)
abline(v = bp_from_opt$bp, col = 3)
</code></pre>

<hr>
<h2 id='exp_fit'>Exponential fit with nls</h2><span id='topic+exp_fit'></span>

<h3>Description</h3>

<p>This function performs an exponential fit using non-linear least squares (nls).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exp_fit(value, delay, initial_guess, max_iter = 1e+05, scale_offset = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="exp_fit_+3A_value">value</code></td>
<td>
<p>A numeric vector of the subjective values (indifference points).</p>
</td></tr>
<tr><td><code id="exp_fit_+3A_delay">delay</code></td>
<td>
<p>A numeric vector of the delays used in the experiment.</p>
</td></tr>
<tr><td><code id="exp_fit_+3A_initial_guess">initial_guess</code></td>
<td>
<p>A numeric value providing an initial estimate for the parameter <code>k</code>.</p>
</td></tr>
<tr><td><code id="exp_fit_+3A_max_iter">max_iter</code></td>
<td>
<p>An integer specifying the maximum number of iterations for the nls fitting algorithm.
Default is 1e5.</p>
</td></tr>
<tr><td><code id="exp_fit_+3A_scale_offset">scale_offset</code></td>
<td>
<p>A numeric value for the scaling offset used in nls fitting control. Default is 0.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>nls</code> containing the fitted model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See the examples of hyp_fit
</code></pre>

<hr>
<h2 id='f_table'>Frequency table for binned data</h2><span id='topic+f_table'></span>

<h3>Description</h3>

<p>Creates a frequency table from a vector of bins from, for example,
get_bins(). It includes zero-frequency bins. If the bins came from the
responding times, this creates a data.frame of response rate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_table(x, min_x, max_x, bin_res)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_table_+3A_x">x</code></td>
<td>
<p>numeric, a vector of binned data</p>
</td></tr>
<tr><td><code id="f_table_+3A_min_x">min_x</code></td>
<td>
<p>numeric, the minimal value of x</p>
</td></tr>
<tr><td><code id="f_table_+3A_max_x">max_x</code></td>
<td>
<p>numeric, the maximal value of x</p>
</td></tr>
<tr><td><code id="f_table_+3A_bin_res">bin_res</code></td>
<td>
<p>numeric, the bin resolution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("r_times")
bin_res &lt;- 2
min_x &lt;- 0
max_x &lt;- 180
x &lt;- get_bins(r_times, min_x, max_x, bin_res)
xt &lt;- f_table(x, min_x, max_x, bin_res)
plot(xt$bins, xt$prop)
</code></pre>

<hr>
<h2 id='fi60_raw_from_med'>Raw Fixed Interval Data</h2><span id='topic+fi60_raw_from_med'></span>

<h3>Description</h3>

<p>Raw Fixed Interval Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fi60_raw_from_med
</code></pre>


<h3>Format</h3>

<p>A character vector containing lines of data from the file.
</p>


<h3>Details</h3>

<p>A dataset containing raw data from a fixed interval (FI) experiment.
</p>
<p>This dataset is obtained from a raw file generated during an FI experiment. It provides raw, unprocessed behavioral data.
</p>


<h3>Source</h3>

<p>The raw data was read from the file: <code>inst/extdata/fi60_raw.txt</code>.
</p>

<hr>
<h2 id='fleshler_hoffman'>Fleshler &amp; Hoffman (1962) progression</h2><span id='topic+fleshler_hoffman'></span>

<h3>Description</h3>

<p>This function calculates the values of intervals approximately for
an exponential distribution, but avoiding extremely large values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fleshler_hoffman(N, VI)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fleshler_hoffman_+3A_n">N</code></td>
<td>
<p>The total number of intervals.</p>
</td></tr>
<tr><td><code id="fleshler_hoffman_+3A_vi">VI</code></td>
<td>
<p>The value of the Variable Interval</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the values of intervals approximately for
a exponential distribution, but avoiding extremely large values which can
produce extinction.
It uses the formula derived from the Fleshler &amp; Hoffman article, where the
first factor of the equation is -log(1 - p)^(-1), representing the expected
value or mean of the intervals. This value is also the inverse of a Poisson
process, 1/lambda. Since we want the expected value or mean to be the value
of the IV, we replace that constant with VI.
The function handles the case when n = N, where the value becomes undefined
(log(0)), by using L'Hopital's rule to find the limit of the function as n
approaches N. The resulting values are then multiplied by the IV and the
logarithm of N to obtain the final calculated values.
</p>


<h3>Value</h3>

<p>A vector of calculated values for the intervals.
</p>


<h3>References</h3>

<p>Fleshler, M., &amp; Hoffman, H. S. (1962). A progression for generating variable-interval schedules.
Journal of the Experimental Analysis of Behavior, 5(4), 529-530.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Calculate intervals for N = 10, and IV = 30
N &lt;- 15
iv &lt;- 90
intervals &lt;- round(fleshler_hoffman(N,iv), 3)
# Plot the intervals and the exponential distribution corresponding to the
# same mean (IV)
hist(intervals, freq = FALSE)
curve(dexp(x, rate = 1/iv), add = TRUE, col = 'red')
legend('topright', legend = c('F&amp;H', 'Exponential'), lty = 1, col = c('black', 'red'))
</code></pre>

<hr>
<h2 id='fwhm'>Full Width at Half Maximum</h2><span id='topic+fwhm'></span>

<h3>Description</h3>

<p>Full Width at Half Maximum
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fwhm(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fwhm_+3A_x">x</code></td>
<td>
<p>numeric, a vector of values from a distribution (density)</p>
</td></tr>
<tr><td><code id="fwhm_+3A_y">y</code></td>
<td>
<p>numeric, a vector of probabilities</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function allows to compute the spread of a symmetric function
even when it is not normally distributed. It first finds the x at which y is max,
then
x1 and x2 can be recovered using x1=peak-fwhm/2, x2=peak+fwhm/2
</p>


<h3>Value</h3>

<p>a list with the fwhm and the x at which the max ocurred
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(170)
rx &lt;- rnorm(100)
den &lt;- density(rx)
fval &lt;- fwhm(den$x, den$y)
x1 &lt;- fval$peak - fval$fwhm / 2
x2 &lt;- fval$peak + fval$fwhm / 2
plot(den)
abline(v = c(x1, fval$peak, x2), col = c(1, 2, 1))

</code></pre>

<hr>
<h2 id='gauss_example'>Gaussian Example Data</h2><span id='topic+gauss_example'></span>

<h3>Description</h3>

<p>This dataset contains a series of bins and corresponding response averages from an experimental task.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gauss_example
</code></pre>


<h3>Format</h3>

<p>A data frame with 91 rows and 2 columns:
</p>

<dl>
<dt>Bin</dt><dd><p>Numeric. The bin number.</p>
</dd>
<dt>Response_Average</dt><dd><p>Numeric. The average response in each bin.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Generated as part of a synthetic example for the task.
</p>

<hr>
<h2 id='gauss_example_1'>Gaussian Example 1 Data</h2><span id='topic+gauss_example_1'></span>

<h3>Description</h3>

<p>This dataset contains a series of bins and corresponding response averages from another experimental task, similar to the first example.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gauss_example_1
</code></pre>


<h3>Format</h3>

<p>A data frame with 91 rows and 2 columns:
</p>

<dl>
<dt>Bin</dt><dd><p>Numeric. The bin number.</p>
</dd>
<dt>Response_Average</dt><dd><p>Numeric. The average response in each bin.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Generated as part of a synthetic example for the task.
</p>

<hr>
<h2 id='gauss_example_2'>Gaussian Example 2 Data</h2><span id='topic+gauss_example_2'></span>

<h3>Description</h3>

<p>This dataset contains a series of bins and corresponding response averages, this time with a slightly different distribution and experimental task design.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gauss_example_2
</code></pre>


<h3>Format</h3>

<p>A data frame with 91 rows and 2 columns:
</p>

<dl>
<dt>Bin</dt><dd><p>Numeric. The bin number.</p>
</dd>
<dt>Response_Average</dt><dd><p>Numeric. The average response in each bin.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Generated as part of a synthetic example for the task.
</p>

<hr>
<h2 id='gaussian_fit'>Gaussian + ramp fit with LM algorithm</h2><span id='topic+gaussian_fit'></span>

<h3>Description</h3>

<p>Gaussian + ramp fit with LM algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_fit(
  responses,
  time_vec,
  par = list(a = 0.1, d = 0.1, t0 = 18, b = 10, c = 1),
  max.iter = 500
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gaussian_fit_+3A_responses">responses</code></td>
<td>
<p>numeric, vector of response or response rate</p>
</td></tr>
<tr><td><code id="gaussian_fit_+3A_time_vec">time_vec</code></td>
<td>
<p>numeric, time bins</p>
</td></tr>
<tr><td><code id="gaussian_fit_+3A_par">par</code></td>
<td>
<p>a list of parameters for the gaussian + linear;
see Buhusi, C. V., Perera, D., &amp; Meck, W. H. (2005) for an explanation</p>
</td></tr>
<tr><td><code id="gaussian_fit_+3A_max.iter">max.iter</code></td>
<td>
<p>numeric, max number of iterations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ver Buhusi, C. V., Perera, D., &amp; Meck, W. H. (2005). Memory for timing visual and auditory signals in albino and pigmented rats.
</p>
<p>This algorithm uses the nonlinear least squares nls.lm (Levenberg–Marquardt) from the minpack.lm package
</p>


<h3>Value</h3>

<p>a numeric vector of coefficients, the same as the par argument
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Function to create synthetic data
g_plus_lin &lt;- function(par, time) {
  par$a * exp(-0.5 * ((time - par$t0) / par$b)**2) + par$c * (time - par$t0) + par$d
}
# real params
pars &lt;- list(a = 20, t0 = 20, b = 10, c = 0.2, d = 1)
# time vector for simulation
ti &lt;- seq(0, 60, 0.1)
# time vector for sampling with 2 sec of resolution
ti_data &lt;- seq(0, 60, 2)
# r(t) real
y_curva &lt;- g_plus_lin(par = pars, ti)
# r(t) sampled with noise
y_data &lt;- g_plus_lin(par = pars, ti_data) + rnorm(length(ti_data), 0, sd = 2)
# param estimation
par_est &lt;- gaussian_fit(responses = y_data, t = ti_data, par = pars, max.iter = 10500)
par_est
# fitted curve
y_hat &lt;- g_plus_lin(par_est |&gt; as.list(), ti)
# plot results

plot(ti,
  y_curva,
  type = "l",
  col = "blue",
  lwd = 2,
  ylim = c(0, max(y_curva, y_data)),
  xlab = "Time in trial",
  ylab = "R(t)",
)
points(ti_data, y_data, pch = 21, bg = "red", cex = 1.2)
lines(
  ti,
  y_hat,
  col = "green2",
  lwd = 2
)
legend(
  "topright",
  legend = c("real", "real + noise", "ajuste nls.lm"),
  lty = c(1, 0, 1),
  pch = c(NA, 21),
  pt.bg = c(NA, "red"),
  col = c("blue", 1, "green2"),
  pt.cex = 0.9,
  cex = 0.6
)
</code></pre>

<hr>
<h2 id='gell_like'>Gellerman-like series</h2><span id='topic+gell_like'></span>

<h3>Description</h3>

<p>Gellerman-like series
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gell_like(n)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gell_like_+3A_n">n</code></td>
<td>
<p>numeric, a vector of 0 and 1 (see Details)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm implements a Gellerman-like series based on
Herrera, D., &amp; Treviño, M. (2015). http://doi.org/10.1371/journal.pone.0136084
The algorithm samples from a binomial distribution and imposes two restrictions
</p>

<ol>
<li><p> no more than 3 consecutive values of 0s or 1s.
</p>
</li>
<li><p> the number of trials 0 or 1 must be the same for a given n.
</p>
</li></ol>



<h3>Value</h3>

<p>a numeric vector of randomly distributed 0s and 1s
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(165)
gell_like(8) # 0 0 1 1 1 0 1 0
</code></pre>

<hr>
<h2 id='get_bins'>A function to binarize a numeric vector with a given resolution</h2><span id='topic+get_bins'></span>

<h3>Description</h3>

<p>A function to binarize a numeric vector with a given resolution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_bins(x, x_min, x_max, res)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_bins_+3A_x">x</code></td>
<td>
<p>numeric, the vector to be binarized</p>
</td></tr>
<tr><td><code id="get_bins_+3A_x_min">x_min</code></td>
<td>
<p>numeric, the min value of a vector to create the bins (e.g., 0)</p>
</td></tr>
<tr><td><code id="get_bins_+3A_x_max">x_max</code></td>
<td>
<p>numeric, the maximum value of the vector x to binarize</p>
</td></tr>
<tr><td><code id="get_bins_+3A_res">res</code></td>
<td>
<p>numeric, the resolution; if x is time, res can be 1 s</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the vector of bins for which x is in
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:20
get_bins(x, 0, 20, 5)
# Returns
# [1]  5  5  5  5  5 10 10 10 10 10 15 15 15 15 15 20 20 20 20 20
# set.seed(10)
x &lt;- runif(20, 0, 10)
get_bins(x, 0, 10, 0.5)
# Returns
# 1] 5.5 3.5 4.5 7.0 1.0 2.5 3.0 3.0 6.5 4.5 7.0 6.0 1.5 6.0 4.0 4.5 1.0 3.0 4.0 8.5
</code></pre>

<hr>
<h2 id='hyp_data'>Simulated Data for Hyperbolic Discounting</h2><span id='topic+hyp_data'></span>

<h3>Description</h3>

<p>Simulated Data for Hyperbolic Discounting
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hyp_data
</code></pre>


<h3>Format</h3>

<p>A list with 3 elements:
</p>

<dl>
<dt>sv</dt><dd><p>A numeric vector of normalized subjective values with added noise.</p>
</dd>
<dt>delay</dt><dd><p>A numeric vector of delays (in seconds).</p>
</dd>
<dt>real_k</dt><dd><p>The real value of the discounting parameter.</p>
</dd>
</dl>



<h3>Details</h3>

<p>A list of simulated data for fitting hyperbolic discounting models.
</p>
<p>This dataset was generated to simulate the behavior of a hyperbolic discounting function.
It is commonly used in behavioral economics and psychology to study delay discounting behaviors.
</p>


<h3>Source</h3>

<p>Generated using a custom simulation function.
</p>

<hr>
<h2 id='hyp_data_list'>Hypothetical dataset list for testing purposes</h2><span id='topic+hyp_data_list'></span>

<h3>Description</h3>

<p>This list contains three components: sv, delay, and real_k, representing subjective values, delay durations, and the real discount rate respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hyp_data_list
</code></pre>


<h3>Format</h3>

<p>A list with three components:
</p>

<dl>
<dt>sv</dt><dd><p>A numeric vector of subjective values.</p>
</dd>
<dt>delay</dt><dd><p>A numeric vector of delay durations (in arbitrary units).</p>
</dd>
<dt>real_k</dt><dd><p>A numeric value representing the real discount rate.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Hypothetical data generated for demonstration.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hyp_data_list)
str(hyp_data_list)
</code></pre>

<hr>
<h2 id='hyperbolic_fit'>Hyperbolic fit with nls</h2><span id='topic+hyperbolic_fit'></span>

<h3>Description</h3>

<p>Hyperbolic fit with nls
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hyperbolic_fit(value, delay, initial_guess, max_iter = 1e+05, scale_offset = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hyperbolic_fit_+3A_value">value</code></td>
<td>
<p>A numeric vector of the subjective values (indifference points)</p>
</td></tr>
<tr><td><code id="hyperbolic_fit_+3A_delay">delay</code></td>
<td>
<p>A numeric vector of the delays used</p>
</td></tr>
<tr><td><code id="hyperbolic_fit_+3A_initial_guess">initial_guess</code></td>
<td>
<p>A numeric value providing an initial start for k</p>
</td></tr>
<tr><td><code id="hyperbolic_fit_+3A_max_iter">max_iter</code></td>
<td>
<p>Positive integer with maximum number of iterations</p>
</td></tr>
<tr><td><code id="hyperbolic_fit_+3A_scale_offset">scale_offset</code></td>
<td>
<p>A constant to be added if the residuals are close to 0.
This is to avoid division by 0, which is know to cause problems of convergence.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class nls
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulated data with k = 0.5
data("hyp_data")
delay &lt;- hyp_data$delay
sv &lt;- hyp_data$sv
real_k &lt;- hyp_data$real_k
model_hyp &lt;- hyperbolic_fit(sv, delay, initial_guess = 0.01)
summary(model_hyp)
k_est &lt;- coef(model_hyp)
k_est
# plot real and estimated sv
delay_real &lt;- seq(0, max(delay), len = 100)
# first, simulate how the data should look with the real k
real_sv &lt;- eq_hyp(real_k, delay_real)
# simulate estimated fitting line
est_sv &lt;- eq_hyp(k_est, delay_real)

plot(
  delay, sv,
  pch = 21,
  col = 1,
  bg = 8,
  xlab = "Delay",
  ylab = "Subjective value"
)
lines(
  delay_real,
  est_sv,
  col = "red",
  lwd = 2
)
# real data
lines(
  delay_real,
  real_sv,
  type = "l",
  col = "blue",
  lwd = 2
)
legend(
  "topright",
  legend = c("data", "real", "fit"),
  text.col = "white",
  pch = c(21, NA, NA),
  col = c(1, NA, NA),
  pt.bg = c(8, NA, NA),
  bty = "n"
)
legend(
  "topright",
  legend = c("data", "real", "fit"),
  pch = c(NA, NA, NA),
  lty = c(NA, 1, 1),
  col = c(NA, "blue", "red"),
  bty = "n"
)

# Now an example with real data
data("DD_data")
# first, fit a linear model
lineal_m &lt;- lm(norm_sv ~ Delay, data = DD_data)
# hyperbolic model
hyp_m &lt;- hyperbolic_fit(DD_data$norm_sv, delay = DD_data$Delay, 0.1)
# exponential model
exp_m &lt;- exp_fit(DD_data$norm_sv, delay = DD_data$Delay, 0.1)
AIC(lineal_m, hyp_m, exp_m)
# compare visually
k_hyp &lt;- coef(hyp_m)
k_exp &lt;- coef(exp_m)
k_lin &lt;- coef(lineal_m)
delay_vec &lt;- seq(0, max(DD_data$Delay), len = 200)
plot(
  DD_data$Delay,
  DD_data$norm_sv,
  ylim = c(0, 1),
  pch = 21,
  ylab = "SV",
  xlab = "Delay",
  bg = "orange",
  col = "black"
)
lines(
  delay_vec,
  eq_hyp(k = k_hyp, delay_vec),
  col = "green4",
  lwd = 2
)
lines(
  delay_vec,
  exp(-k_exp * delay_vec),
  col = "steelblue",
  lwd = 2
)
abline(lineal_m, lty = 2, lwd = 2)

legend(
  "topright",
  legend = c("data", "exp fit", "hyp fit", "linear fit"),
  text.col = "white",
  pch = c(21, NA, NA, NA),
  col = c(1, NA, NA, NA),
  pt.bg = c("orange", NA, NA, NA),
  bty = "n"
)
legend(
  "topright",
  legend = c("data", "exp fit", "hyp fit", "linear fit"),
  pch = c(NA, NA, NA, NA),
  lty = c(NA, 1, 1, 2),
  col = c(NA, "steelblue", "green4", 1),
  bty = "n"
)
# plot AIC values
aic_val &lt;- AIC(lineal_m, hyp_m, exp_m) |&gt; round(2)
leg &lt;- sprintf(paste(rownames(aic_val), "= %s", sep = " "), aic_val$AIC)
legend(
  "bottomleft",
  title = "AIC\n(the smaller, the better)",
  legend = leg,
  bty = "n"
)
</code></pre>

<hr>
<h2 id='ind_trials_obj_fun'>Objective function for finding the best fit for individual trials</h2><span id='topic+ind_trials_obj_fun'></span>

<h3>Description</h3>

<p>This function is used by <code>optim</code> to find the best fit for
individual trials by minimizing the sum of areas between the response rate
and the target rate. Do not call this function directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ind_trials_obj_fun(params, r_times, trial_duration)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ind_trials_obj_fun_+3A_params">params</code></td>
<td>
<p>A vector of parameters to be optimized.</p>
</td></tr>
<tr><td><code id="ind_trials_obj_fun_+3A_r_times">r_times</code></td>
<td>
<p>Vector of response times</p>
</td></tr>
<tr><td><code id="ind_trials_obj_fun_+3A_trial_duration">trial_duration</code></td>
<td>
<p>Duration of the trial</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric value representing the sum of areas between the response
rate and the target rate.
</p>

<hr>
<h2 id='ind_trials_opt'>Find the best fit for individual trials using <code>optim</code></h2><span id='topic+ind_trials_opt'></span>

<h3>Description</h3>

<p>Find the best fit for individual trials by minimizing the
sum of areas between the response rate and the target rate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ind_trials_opt(r_times, trial_duration, optim_method = "Nelder-Mead")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ind_trials_opt_+3A_r_times">r_times</code></td>
<td>
<p>Vector of response times</p>
</td></tr>
<tr><td><code id="ind_trials_opt_+3A_trial_duration">trial_duration</code></td>
<td>
<p>Duration of the trial</p>
</td></tr>
<tr><td><code id="ind_trials_opt_+3A_optim_method">optim_method</code></td>
<td>
<p>Optimization method. See <code>optim</code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with the following columns:
</p>

<ul>
<li> <p><code>start</code>: The start time of the peak
</p>
</li>
<li> <p><code>stop</code>: The stop time of the peak
</p>
</li>
<li> <p><code>spread</code>: The spread of the peak (stop - start)
</p>
</li>
<li> <p><code>middle</code>: The middle of the peak (mean of start and stop)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>response_times &lt;- c(
  28.1, 40.7, 44.2, 44.4, 44.7, 45, 45.4, 47.9, 48.1, 48.3,
  48.6, 48.8, 49.8, 50.2, 50.7, 51.2, 51.4, 51.7, 51.9, 52.7, 53, 53.5, 53.7,
  53.9, 54.1, 54.3, 54.9, 55.3, 55.5, 55.7, 55.8, 57.2, 57.4, 57.7, 58.3,
  58.5, 58.7, 60.4, 60.6, 60.7, 61.1, 61.6, 61.8, 62.6, 62.8, 63.1, 63.3,
  63.5, 63.8, 64.4, 64.8, 64.9, 65.1, 66.1, 66.4, 67, 68.7, 68.9, 69.5, 69.6,
  70.1, 70.9, 71, 71.3, 71.6, 71.8, 73.9, 74.1, 74.4, 74.6, 75.2, 76.4,
  76.6, 77.4, 77.6, 77.8, 78.2, 79.3, 79.9, 80.5, 80.7, 81.3, 82.2, 82.4,
  82.6, 82.9, 83, 83.1, 83.7, 84.4, 84.4, 84.8, 85, 85.6, 86.6, 87, 87.1,
  87.3, 87.4, 87.8, 88.1, 88.2, 89.4, 99.1, 99.3, 99.6, 99.8, 100.2,
  133.1, 133.1, 133.6, 134.9, 135.2, 135.3, 135.4, 135.7, 136.5, 173.8,
  174.1, 174.3, 174.7, 175.9, 176.3, 176.6, 177.4, 177.5, 177.7, 178.1,
  178.2, 178.4, 178.5, 178.8, 179.4
)
# Replace with your own initial guess
initial_guess &lt;- c(min(response_times), mean(response_times))
trial_duration &lt;- max(response_times)
result &lt;- ind_trials_opt(response_times, trial_duration)
plot(
  density(
    response_times,
    adjust = 0.8,
    from = 0,
    to = trial_duration
  ),
  main = "Density plot of response times",
  xlab = "Response time (ms)",
  ylab = expression(italic(p(t[R]))),
)
abline(v = 60, lty = 2)
abline(v = result$start, col = "red")
abline(v = result$stop, col = "red")
abline(v = result$middle, col = "red")
</code></pre>

<hr>
<h2 id='KL_div'>Computes the Kullback-Leibler divergence based on kernel density estimates</h2><span id='topic+KL_div'></span>

<h3>Description</h3>

<p>Computes the Kullback-Leibler divergence based on kernel density estimates
of two samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KL_div(x, y, from_a, to_b)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KL_div_+3A_x">x</code></td>
<td>
<p>numeric, the values from a sample p</p>
</td></tr>
<tr><td><code id="KL_div_+3A_y">y</code></td>
<td>
<p>numeric, the values from a sample q</p>
</td></tr>
<tr><td><code id="KL_div_+3A_from_a">from_a</code></td>
<td>
<p>numeric, the lower limit of the integration</p>
</td></tr>
<tr><td><code id="KL_div_+3A_to_b">to_b</code></td>
<td>
<p>numeric, the upper limit of the integration</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Kullback-Leibler divergence is defined as
</p>
<p style="text-align: center;"><code class="reqn">D_{KL}(P||Q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx</code>
</p>



<h3>Value</h3>

<p>a numeric value that is the kl divergence
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
p &lt;- rnorm(100)
q &lt;- rnorm(100)
KL_div(p, q, -Inf, Inf) # 0.07579204
q &lt;- rnorm(100, 10, 4)
KL_div(p, q, -Inf, Inf) # 7.769912
</code></pre>

<hr>
<h2 id='mut_info_discrete'>Mutual information of continuous variables using discretization</h2><span id='topic+mut_info_discrete'></span>

<h3>Description</h3>

<p>Mutual information of continuous variables using discretization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mut_info_discrete(x, y, method = "emp")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mut_info_discrete_+3A_x">x</code></td>
<td>
<p>A numeric vector</p>
</td></tr>
<tr><td><code id="mut_info_discrete_+3A_y">y</code></td>
<td>
<p>A numeric vector or equal or unequal size as x</p>
</td></tr>
<tr><td><code id="mut_info_discrete_+3A_method">method</code></td>
<td>
<p>The method to estimate entropy; available methods are &quot;emp&quot;, &quot;mm&quot;,
&quot;shrink&quot;, &quot;sg&quot; (default:&quot;emp&quot;). See details</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is based on the infotheo package. It uses equalfreq discretization by default.
x and y need not be of equal size.
</p>


<h3>Value</h3>

<p>A numeric value representing the mutual information between x and y
</p>


<h3>References</h3>

<p>Meyer, P. E. (2008). Information-Theoretic Variable Selection and Network Inference from Microarray Data.
PhD thesis of the Universite Libre de Bruxelles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
x &lt;- rnorm(1000)
y &lt;- rnorm(1000)
plot(x, y)
# close to 0 if they are independent
mut_info_discrete(x, y)
y &lt;- 100 * x + rnorm(length(x), 0, 12)
plot(x, y)
# far from 0 if they are not independent
mut_info_discrete(x, y)
# simulate a sine function with noise
set.seed(123)
x &lt;- seq(0, 5, 0.1)
y &lt;- 5 * sin(x * pi)
y_with_noise &lt;- y + rnorm(length(x), 0, 1)
plot(x, y_with_noise)
lines(x, y, col = 2)
# add a regression line
abline(lm(y ~ x))
# compute correlation coefficient; for nonlinear functions is close to 0
cor(x, y_with_noise)
# mutual information can detect nonlinear dependencies
mut_info_discrete(x, y_with_noise)
</code></pre>

<hr>
<h2 id='mut_info_knn'>Mutual Information for Continuous Variables using kNN</h2><span id='topic+mut_info_knn'></span>

<h3>Description</h3>

<p>Mutual Information for Continuous Variables using kNN
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mut_info_knn(x, y, k = 5, direct = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mut_info_knn_+3A_x">x</code></td>
<td>
<p>Numeric vector.</p>
</td></tr>
<tr><td><code id="mut_info_knn_+3A_y">y</code></td>
<td>
<p>Numeric vector.</p>
</td></tr>
<tr><td><code id="mut_info_knn_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to use; default is 5.</p>
</td></tr>
<tr><td><code id="mut_info_knn_+3A_direct">direct</code></td>
<td>
<p>Logical; if <code>TRUE</code>, mutual information is calculated
using the k-nearest neighbors (kNN) estimator; if <code>FALSE</code>, it is computed
via entropy estimates as <code class="reqn">I(X;Y) = H(X) + H(Y) - H(X,Y)</code>. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric; an estimate of the mutual information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
x &lt;- rnorm(1000)
y &lt;- rnorm(1000)
# Close to 0 if they are independent
mut_info_knn(x, y, k = 5)
y &lt;- 100 * x + rnorm(length(x), 0, 12)
# Far from 0 if they are not independent
mut_info_knn(x, y, k = 5)
</code></pre>

<hr>
<h2 id='n_between_intervals'>Find maximum value within intervals</h2><span id='topic+n_between_intervals'></span>

<h3>Description</h3>

<p>This function searches for the maximum value within a distribution
(represented by vector x) that falls within a series of intervals
specified by the vector intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>n_between_intervals(x, intervals, time_in)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="n_between_intervals_+3A_x">x</code></td>
<td>
<p>A numeric vector representing the distribution from which
to find the maximum value within intervals.</p>
</td></tr>
<tr><td><code id="n_between_intervals_+3A_intervals">intervals</code></td>
<td>
<p>A numeric vector specifying the intervals within
which to search for the maximum value.</p>
</td></tr>
<tr><td><code id="n_between_intervals_+3A_time_in">time_in</code></td>
<td>
<p>A numeric vector representing the corresponding time points
for the values in the vector x, which is used to determine whether
the values fall within the specified intervals.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector containing the maximum value within each
interval specified by 'intervals'. If no values fall within an interval,
returns 0 for that interval.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create a vector of data with a logarithmically increasing distribution
log_data &lt;- round(exp(seq(log(1), log(100), length.out = 100)))

# Define intervals to cover the range 1-100
intervals &lt;- seq(1, 100, by = 20)

# Create a corresponding time vector
time_in &lt;- seq(1, 100, length.out = 100)

# Find maximum value within intervals
n_between_intervals(log_data, intervals, time_in)
</code></pre>

<hr>
<h2 id='objective_bp'>Objective function for the breakpoint optimization algorithm</h2><span id='topic+objective_bp'></span>

<h3>Description</h3>

<p>Objective function for the breakpoint optimization algorithm
for fixed interval trials. This function is used by <code>optim</code> to find the
optimal breakpoint. Do not call this function directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objective_bp(param, r_times, trial_duration)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="objective_bp_+3A_param">param</code></td>
<td>
<p>A numeric value of the breakpoint</p>
</td></tr>
<tr><td><code id="objective_bp_+3A_r_times">r_times</code></td>
<td>
<p>A numeric vector of response times</p>
</td></tr>
<tr><td><code id="objective_bp_+3A_trial_duration">trial_duration</code></td>
<td>
<p>A numeric value of the trial duration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric value representing the sum of areas between the response
rate and the target rate.
</p>

<hr>
<h2 id='optimize_biexponential'>Optimization Function for the Biexponential Model</h2><span id='topic+optimize_biexponential'></span><span id='topic+biexponential_log_likelihood'></span>

<h3>Description</h3>

<p>Optimizes the log-likelihood function to estimate biexponential model parameters based on observed inter-response times.
</p>
<p>Calculates the negative log-likelihood for the simpler biexponential model, which does not include the
refractory period parameter, <code class="reqn">\delta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimize_biexponential(irt)

biexponential_log_likelihood(params, irt)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optimize_biexponential_+3A_irt">irt</code></td>
<td>
<p>A numeric vector representing inter-response times.</p>
</td></tr>
<tr><td><code id="optimize_biexponential_+3A_params">params</code></td>
<td>
<p>A numeric vector of initial parameter estimates for optimization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the negative log-likelihood based on biexponential functions
for the simpler biexponential model, adjusting parameters using transformations to meet constraints.
</p>


<h3>Value</h3>

<p>A named vector of optimized parameters for the biexponential model.
</p>
<p>Negative log-likelihood value used for parameter estimation.
</p>

<hr>
<h2 id='r_times'>Reaction Times from Peak Procedure</h2><span id='topic+r_times'></span>

<h3>Description</h3>

<p>Reaction Times from Peak Procedure
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r_times
</code></pre>


<h3>Format</h3>

<p>A numeric vector with 132 elements representing reaction times.
</p>


<h3>Details</h3>

<p>A dataset containing reaction times (in seconds) from an experiment using the peak procedure.
</p>
<p>These times are derived from a peak procedure experiment, typically used in behavioral experiments to measure timing abilities in subjects.
</p>


<h3>Source</h3>

<p>Generated during a behavioral analysis experiment.
</p>

<hr>
<h2 id='read_med'>Process MED to csv based on standard data structure event.time</h2><span id='topic+read_med'></span>

<h3>Description</h3>

<p>Process MED to csv based on standard data structure event.time
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_med(
  fname,
  save_file = FALSE,
  path_save = NULL,
  col_r = "C:",
  col_names = c("time", "event"),
  out = TRUE,
  num_col = 6,
  time_dot_event = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_med_+3A_fname">fname</code></td>
<td>
<p>chr, the name of a MED file to read; can include the directory</p>
</td></tr>
<tr><td><code id="read_med_+3A_save_file">save_file</code></td>
<td>
<p>logical, save csv on the disk? TRUE or FALSE (default)</p>
</td></tr>
<tr><td><code id="read_med_+3A_path_save">path_save</code></td>
<td>
<p>chr, directory to save csv files if save_file is TRUE;</p>
</td></tr>
<tr><td><code id="read_med_+3A_col_r">col_r</code></td>
<td>
<p>chr, MED array to read (may be an event.time variable; see Details)</p>
</td></tr>
<tr><td><code id="read_med_+3A_col_names">col_names</code></td>
<td>
<p>chr, a vector of column names</p>
</td></tr>
<tr><td><code id="read_med_+3A_out">out</code></td>
<td>
<p>logical, if true returns the data.frame of n x 2</p>
</td></tr>
<tr><td><code id="read_med_+3A_num_col">num_col</code></td>
<td>
<p>int, corresponds to DISKCOLUMNS of MED</p>
</td></tr>
<tr><td><code id="read_med_+3A_time_dot_event">time_dot_event</code></td>
<td>
<p>logical, if true, assumes that array to process has a time.event format</p>
</td></tr>
<tr><td><code id="read_med_+3A_...">...</code></td>
<td>
<p>other arguments passed to <code><a href="utils.html#topic+read.table">read.table</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default behavior of this function has time_dot_event = TRUE,
which means that the raw MED can be should be in time.event convention.
For example, if a response is coded as 23, the time is in 1/100 seconds and a response
occurred at 2 minutes, the event is saved in, say, column C as 6000.23. This will be
processed as
time event
6000  23
</p>
<p>However, if time_dot_event = FALSE, the output will be a data.frame with one column
values. For example
values
6000.23
</p>


<h3>Value</h3>

<p>if out is true, returns a data.frame; if save_file is TRUE, writes the data.frame in csv format at path_save
</p>


<h3>Examples</h3>

<pre><code class='language-R'># read raw data from MED
data("fi60_raw_from_med")
# see first 10 lines
head(fi60_raw_from_med, 10)
# create a temporary file to avoid non-staged installation warning
temp_file &lt;- tempfile(fileext = ".txt")
# write the data to the temporary file
writeLines(fi60_raw_from_med, temp_file)
# Use the temporary file for processing
fi60_processed &lt;- read_med(fname = temp_file, save_file = FALSE,
  col_r = "C:", out = TRUE,
  col_names = c("time", "event"), num_col = 6, time_dot_event = TRUE)
head(fi60_processed)
# __________________________________________________________________________
## To use in bulk
# 1) Generate a list of filenames of raw MED data
# 2) Loop over the list with the function, using each element
#    of the list as the fname argument.
# __________________________________________________________________________
# Suppose all raw MED files start with 2020, and you are in the working directory
# If all the raw MED files are in the wd, we can directly get the filenames
# with unspecified path
# filenames &lt;- list.files(pattern = "^2020")
# The above line will look in the wd for all the files starting with "2020"
# and it will save it as a vector of strings in "filenames".
# With that vector, make a for loop like the following:
# __________________________________________________________________________
# If you want to work immediately with the processed data, first create an empty
# dataframe to store the data file per file
# df_working = data.frame()
# __________________________________________________________________________
# for (f in filenames) {
#   df_tmp &lt;- read_med(fname = f,
#                     path_save = "data/processed/", # put here your path to save the csv
#                     col_r = 'C:', # if the time.event vector is saved in variable C
#                     out = TRUE ) # If you want to store processed data in df_tmp,
# otherwise write out = FALSE
# now append at rows the new data.frame
#   df_working = rbind(df_working, df_tmp)
# }
# Thats all.

</code></pre>

<hr>
<h2 id='sample_from_density'>Sample from a density estimate</h2><span id='topic+sample_from_density'></span>

<h3>Description</h3>

<p>Sample from a density estimate
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_from_density(x, n)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sample_from_density_+3A_x">x</code></td>
<td>
<p>A numeric variable from a (un)known distribution</p>
</td></tr>
<tr><td><code id="sample_from_density_+3A_n">n</code></td>
<td>
<p>Number of samples to return</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sample with distribution close to x
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- rnorm(1000)
y &lt;- sample_from_density(x, 1000)

hist(x,
  breaks = 30,
  freq = FALSE,
  col = "grey",
  main = "Original data"
)

</code></pre>

<hr>
<h2 id='trapezoid_auc'>Area under the curve (AUC)</h2><span id='topic+trapezoid_auc'></span>

<h3>Description</h3>

<p>Calculate the area under the curve (AUC) using the trapezoid
method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trapezoid_auc(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trapezoid_auc_+3A_x">x</code></td>
<td>
<p>A numeric vector of x values</p>
</td></tr>
<tr><td><code id="trapezoid_auc_+3A_y">y</code></td>
<td>
<p>A numeric vector of y values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric value of the area under the curve
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x_values &lt;- c(0, 1, 2, 3, 4) # Delay times
y_values &lt;- c(1, 0.8, 0.6, 0.4, 0.2) # Discounted values
auc_result &lt;- trapezoid_auc(x_values, y_values)
print(paste("Area Under Curve: ", auc_result))
</code></pre>

<hr>
<h2 id='unit_normalization'>Min-max normalization (also feature rescaling)</h2><span id='topic+unit_normalization'></span>

<h3>Description</h3>

<p>Min-max normalization (also feature rescaling)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unit_normalization(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="unit_normalization_+3A_x">x</code></td>
<td>
<p>numeric, vector of values to rescale</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector rescaled in the range <code class="reqn">x' \in [0, 1]</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 5:100
x_scaled &lt;- unit_normalization(x)
x_scaled
</code></pre>

<hr>
<h2 id='val_in_interval'>True value in interval</h2><span id='topic+val_in_interval'></span>

<h3>Description</h3>

<p>True value in interval
</p>


<h3>Usage</h3>

<pre><code class='language-R'>val_in_interval(df, lowLim, upLim, true.val)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="val_in_interval_+3A_df">df</code></td>
<td>
<p>A data frame containing the intervals to be evaluated. Each row should
correspond to an interval with lower and upper limits.</p>
</td></tr>
<tr><td><code id="val_in_interval_+3A_lowlim">lowLim</code></td>
<td>
<p>the column index or name in the data frame <code>df</code>
corresponding to the lower limit of the interval.</p>
</td></tr>
<tr><td><code id="val_in_interval_+3A_uplim">upLim</code></td>
<td>
<p>the column index or name in the data frame <code>df</code>
corresponding to the upper limit of the interval.</p>
</td></tr>
<tr><td><code id="val_in_interval_+3A_true.val">true.val</code></td>
<td>
<p>the true value to be checked if it falls within
the interval defined by <code>lowLim</code> and <code>upLim</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of n elements with an integer value
for each interval: 0 if the value is below the interval,
1 if it is inside the interval (with a rightmost open limit),
and 2 if it is above the interval.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example data frame with intervals
df &lt;- data.frame(lower = c(1, 5, 10), upper = c(3, 8, 15))

# Check if the value 6 is within any of the intervals
val_in_interval(df, "lower", "upper", 6)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
