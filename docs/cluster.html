<!DOCTYPE html><html><head><title>Help for package cluster</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cluster}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#agnes'><p>Agglomerative Nesting (Hierarchical Clustering)</p></a></li>
<li><a href='#agnes.object'><p>Agglomerative Nesting (AGNES) Object</p></a></li>
<li><a href='#agriculture'><p>European Union Agricultural Workforces</p></a></li>
<li><a href='#animals'><p>Attributes of Animals</p></a></li>
<li><a href='#bannerplot'><p>Plot Banner (of Hierarchical Clustering)</p></a></li>
<li><a href='#chorSub'><p>Subset of C-horizon of Kola Data</p></a></li>
<li><a href='#clara'><p>Clustering Large Applications</p></a></li>
<li><a href='#clara.object'><p>Clustering Large Applications (CLARA) Object</p></a></li>
<li><a href='#clusGap'><p>Gap Statistic for Estimating the Number of Clusters</p></a></li>
<li><a href='#clusplot'><p>Bivariate Cluster Plot (of a Partitioning Object)</p></a></li>
<li><a href='#clusplot.default'><p>Bivariate Cluster Plot (clusplot) Default Method</p></a></li>
<li><a href='#cluster-internal'><p>Internal cluster functions</p></a></li>
<li><a href='#coef.hclust'><p>Agglomerative / Divisive Coefficient for 'hclust' Objects</p></a></li>
<li><a href='#daisy'><p>Dissimilarity Matrix Calculation</p></a></li>
<li><a href='#diana'><p>DIvisive ANAlysis Clustering</p></a></li>
<li><a href='#dissimilarity.object'><p>Dissimilarity Matrix Object</p></a></li>
<li><a href='#ellipsoidhull'><p>Compute the Ellipsoid Hull or Spanning Ellipsoid of a Point Set</p></a></li>
<li><a href='#fanny'><p>Fuzzy Analysis Clustering</p></a></li>
<li><a href='#fanny.object'><p>Fuzzy Analysis (FANNY) Object</p></a></li>
<li><a href='#flower'><p>Flower Characteristics</p></a></li>
<li><a href='#lower.to.upper.tri.inds'><p>Permute Indices for Triangular Matrices</p></a></li>
<li><a href='#medoids'><p>Compute <code>pam</code>-consistent Medoids from Clustering</p></a></li>
<li><a href='#mona'><p>MONothetic Analysis Clustering of Binary Variables</p></a></li>
<li><a href='#mona.object'><p>Monothetic Analysis (MONA) Object</p></a></li>
<li><a href='#pam'><p>Partitioning Around Medoids</p></a></li>
<li><a href='#pam.object'><p>Partitioning Around Medoids (PAM) Object</p></a></li>
<li><a href='#partition.object'><p>Partitioning Object</p></a></li>
<li><a href='#plantTraits'><p>Plant Species Traits Data</p></a></li>
<li><a href='#plot.agnes'><p>Plots of an Agglomerative Hierarchical Clustering</p></a></li>
<li><a href='#plot.diana'><p>Plots of a Divisive Hierarchical Clustering</p></a></li>
<li><a href='#plot.mona'><p>Banner of Monothetic Divisive Hierarchical Clusterings</p></a></li>
<li><a href='#plot.partition'><p>Plot of a Partition of the Data Set</p></a></li>
<li><a href='#pltree'><p>Plot Clustering Tree of a Hierarchical Clustering</p></a></li>
<li><a href='#pluton'><p>Isotopic Composition Plutonium Batches</p></a></li>
<li><a href='#predict.ellipsoid'><p>Predict Method for Ellipsoid Objects</p></a></li>
<li><a href='#print.agnes'><p>Print Method for AGNES Objects</p></a></li>
<li><a href='#print.clara'><p>Print Method for CLARA Objects</p></a></li>
<li><a href='#print.diana'><p>Print Method for DIANA Objects</p></a></li>
<li><a href='#print.dissimilarity'><p>Print and Summary Methods for Dissimilarity Objects</p></a></li>
<li><a href='#print.fanny'><p>Print and Summary Methods for FANNY Objects</p></a></li>
<li><a href='#print.mona'><p>Print Method for MONA Objects</p></a></li>
<li><a href='#print.pam'><p>Print Method for PAM Objects</p></a></li>
<li><a href='#ruspini'><p>Ruspini Data</p></a></li>
<li><a href='#silhouette'><p>Compute or Extract Silhouette Information from Clustering</p></a></li>
<li><a href='#sizeDiss'><p>Sample Size of Dissimilarity Like Object</p></a></li>
<li><a href='#summary.agnes'><p>Summary Method for 'agnes' Objects</p></a></li>
<li><a href='#summary.clara'><p>Summary Method for 'clara' Objects</p></a></li>
<li><a href='#summary.diana'><p>Summary Method for 'diana' Objects</p></a></li>
<li><a href='#summary.mona'><p>Summary Method for 'mona' Objects</p></a></li>
<li><a href='#summary.pam'><p>Summary Method for PAM Objects</p></a></li>
<li><a href='#twins.object'><p>Hierarchical Clustering Object</p></a></li>
<li><a href='#volume.ellipsoid'><p>Compute the Volume (of an Ellipsoid)</p></a></li>
<li><a href='#votes.repub'><p>Votes for Republican Candidate in Presidential Elections</p></a></li>
<li><a href='#xclara'><p>Bivariate Data Set with 3 Clusters</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>2.1.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-30</td>
</tr>
<tr>
<td>Priority:</td>
<td>recommended</td>
</tr>
<tr>
<td>Title:</td>
<td>"Finding Groups in Data": Cluster Analysis Extended Rousseeuw et
al.</td>
</tr>
<tr>
<td>Description:</td>
<td>Methods for Cluster analysis.  Much extended the original from
	Peter Rousseeuw, Anja Struyf and Mia Hubert,
	based on Kaufman and Rousseeuw (1990) "Finding Groups in Data".</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Martin Maechler &lt;maechler@stat.math.ethz.ch&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, grDevices, stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, Matrix</td>
</tr>
<tr>
<td>SuggestsNote:</td>
<td>MASS: two examples using cov.rob() and mvrnorm(); Matrix
tools for testing</td>
</tr>
<tr>
<td>Enhances:</td>
<td>mvoutlier, fpc, ellipse, sfsmisc</td>
</tr>
<tr>
<td>EnhancesNote:</td>
<td>xref-ed in man/*.Rd</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>BuildResaveData:</td>
<td>no</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://svn.r-project.org/R-packages/trunk/cluster/">https://svn.r-project.org/R-packages/trunk/cluster/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-30 17:52:28 UTC; maechler</td>
</tr>
<tr>
<td>Author:</td>
<td>Martin Maechler <a href="https://orcid.org/0000-0002-8685-9910"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Peter Rousseeuw <a href="https://orcid.org/0000-0002-3807-5353"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut] (Fortran original),
  Anja Struyf [aut] (S original),
  Mia Hubert <a href="https://orcid.org/0000-0001-6398-4850"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut]
    (S original),
  Kurt Hornik <a href="https://orcid.org/0000-0003-4198-9911"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [trl,
    ctb] (port to R; maintenance(1999-2000)),
  Matthias Studer [ctb],
  Pierre Roudier [ctb],
  Juan Gonzalez [ctb],
  Kamil Kozlowski [ctb],
  Erich Schubert <a href="https://orcid.org/0000-0001-9143-4880"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb] (fastpam options for pam()),
  Keefe Murphy [ctb] (volume.ellipsoid({d &gt;= 3}))</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-01 22:10:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='agnes'>Agglomerative Nesting (Hierarchical Clustering)</h2><span id='topic+agnes'></span>

<h3>Description</h3>

<p>Computes agglomerative hierarchical clustering of the dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>agnes(x, diss = inherits(x, "dist"), metric = "euclidean",
      stand = FALSE, method = "average", par.method,
      keep.diss = n &lt; 100, keep.data = !diss, trace.lev = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="agnes_+3A_x">x</code></td>
<td>

<p>data matrix or data frame, or dissimilarity matrix, depending on the
value of the <code>diss</code> argument.
</p>
<p>In case of a matrix or data frame, each row corresponds to an observation,
and each column corresponds to a variable. All variables must be numeric.
Missing values (NAs) are allowed.
</p>
<p>In case of a dissimilarity matrix, <code>x</code> is typically the output of
<code><a href="#topic+daisy">daisy</a></code> or <code><a href="stats.html#topic+dist">dist</a></code>.
Also a vector with length n*(n-1)/2 is allowed (where n is the number
of observations), and will be interpreted in the same way as the
output of the above-mentioned functions. Missing values (NAs) are not
allowed.
</p>
</td></tr>
<tr><td><code id="agnes_+3A_diss">diss</code></td>
<td>

<p>logical flag: if TRUE (default for <code>dist</code> or
<code>dissimilarity</code> objects), then <code>x</code> is assumed to be a
dissimilarity matrix.  If FALSE, then <code>x</code> is treated as
a matrix of observations by variables.
</p>
</td></tr>
<tr><td><code id="agnes_+3A_metric">metric</code></td>
<td>

<p>character string specifying the metric to be used for calculating
dissimilarities between observations.
The currently available options are <code>"euclidean"</code> and <code>"manhattan"</code>.
Euclidean distances are root sum-of-squares of differences, and
manhattan distances are the sum of absolute differences.
If <code>x</code> is already a dissimilarity matrix, then this argument will
be ignored.
</p>
</td></tr>
<tr><td><code id="agnes_+3A_stand">stand</code></td>
<td>

<p>logical flag: if TRUE, then the measurements in <code>x</code> are
standardized before calculating the dissimilarities. Measurements
are standardized for each variable (column), by subtracting the
variable's mean value and dividing by the variable's mean absolute
deviation.  If <code>x</code> is already a dissimilarity matrix, then this
argument will be ignored.
</p>
</td></tr>
<tr><td><code id="agnes_+3A_method">method</code></td>
<td>

<p>character string defining the clustering method.  The six methods
implemented are
<code>"average"</code> ([unweighted pair-]group [arithMetic] average method, aka &lsquo;UPGMA&rsquo;),
<code>"single"</code> (single linkage), <code>"complete"</code> (complete linkage),
<code>"ward"</code> (Ward's method),
<code>"weighted"</code> (weighted average linkage, aka &lsquo;WPGMA&rsquo;), its generalization
<code>"flexible"</code> which uses (a constant version of)
the Lance-Williams formula and the <code>par.method</code> argument, and
<code>"gaverage"</code> a generalized <code>"average"</code> aka &ldquo;flexible
UPGMA&rdquo; method also using the Lance-Williams formula and <code>par.method</code>.
</p>
<p>The default is <code>"average"</code>.
</p>
</td></tr>
<tr><td><code id="agnes_+3A_par.method">par.method</code></td>
<td>

<p>If <code>method</code> is <code>"flexible"</code> or <code>"gaverage"</code>, a numeric
vector of length 1, 3, or 4, (with a default for <code>"gaverage"</code>), see in
the details section.
</p>
</td></tr>
<tr><td><code id="agnes_+3A_keep.diss">keep.diss</code>, <code id="agnes_+3A_keep.data">keep.data</code></td>
<td>
<p>logicals indicating if the dissimilarities
and/or input data <code>x</code> should be kept in the result.  Setting
these to <code>FALSE</code> can give much smaller results and hence even save
memory allocation <em>time</em>.</p>
</td></tr>
<tr><td><code id="agnes_+3A_trace.lev">trace.lev</code></td>
<td>
<p>integer specifying a trace level for printing
diagnostics during the algorithm.  Default <code>0</code> does not print
anything; higher values print increasingly more.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>agnes</code> is fully described in chapter 5 of Kaufman and Rousseeuw (1990).
Compared to other agglomerative clustering methods such as <code>hclust</code>,
<code>agnes</code> has the following features: (a) it yields the
agglomerative coefficient (see <code><a href="#topic+agnes.object">agnes.object</a></code>)
which measures the amount of clustering structure found; and (b)
apart from the usual tree it also provides the banner, a novel
graphical display (see <code><a href="#topic+plot.agnes">plot.agnes</a></code>).
</p>
<p>The <code>agnes</code>-algorithm constructs a hierarchy of clusterings.<br />
At first, each observation is a small cluster by itself.  Clusters are
merged until only one large cluster remains which contains all the
observations.  At each stage the two <em>nearest</em> clusters are combined
to form one larger cluster.
</p>
<p>For <code>method="average"</code>, the distance between two clusters is the
average of the dissimilarities between the points in one cluster and the
points in the other cluster.
<br />
In <code>method="single"</code>, we use the smallest dissimilarity between a
point in the first cluster and a point in the second cluster (nearest
neighbor method).
<br />
When <code>method="complete"</code>, we use the largest dissimilarity
between a point in the first cluster and a point in the second cluster
(furthest neighbor method).
</p>
<p>The <code>method = "flexible"</code> allows (and requires) more details:
The Lance-Williams formula specifies how dissimilarities are
computed when clusters are agglomerated (equation (32) in K&amp;R(1990),
p.237).  If clusters <code class="reqn">C_1</code> and <code class="reqn">C_2</code> are agglomerated into a
new cluster, the dissimilarity between their union and another
cluster <code class="reqn">Q</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">
    D(C_1 \cup C_2, Q) = \alpha_1 * D(C_1, Q) + \alpha_2 * D(C_2, Q) +
                         \beta * D(C_1,C_2) + \gamma * |D(C_1, Q) - D(C_2, Q)|,
  </code>
</p>

<p>where the four coefficients <code class="reqn">(\alpha_1, \alpha_2, \beta, \gamma)</code>
are specified by the vector <code>par.method</code>, either directly as vector of
length 4, or (more conveniently) if <code>par.method</code> is of length 1,
say <code class="reqn">= \alpha</code>, <code>par.method</code> is extended to
give the &ldquo;Flexible Strategy&rdquo; (K&amp;R(1990), p.236 f) with
Lance-Williams coefficients <code class="reqn">(\alpha_1 = \alpha_2 = \alpha, \beta =
    1 - 2\alpha, \gamma=0)</code>.<br />
Also, if <code>length(par.method) == 3</code>, <code class="reqn">\gamma = 0</code> is set.
</p>
<p><b>Care</b> and expertise is probably needed when using <code>method = "flexible"</code>
particularly for the case when <code>par.method</code> is specified of
longer length than one.  Since <span class="pkg">cluster</span> version 2.0, choices
leading to invalid <code>merge</code> structures now signal an error (from
the C code already).
The <em>weighted average</em> (<code>method="weighted"</code>) is the same as
<code>method="flexible", par.method = 0.5</code>.  Further,
<code>method= "single"</code>  is equivalent to <code>method="flexible", par.method = c(.5,.5,0,-.5)</code>, and
<code>method="complete"</code> is equivalent to <code>method="flexible", par.method = c(.5,.5,0,+.5)</code>.
</p>
<p>The <code>method = "gaverage"</code> is a generalization of <code>"average"</code>, aka
&ldquo;flexible UPGMA&rdquo; method, and is (a generalization of the approach)
detailed in Belbin et al. (1992).  As <code>"flexible"</code>, it uses the
Lance-Williams formula above for dissimilarity updating, but with
<code class="reqn">\alpha_1</code> and <code class="reqn">\alpha_2</code> not constant, but <em>proportional</em> to
the <em>sizes</em> <code class="reqn">n_1</code> and <code class="reqn">n_2</code> of the clusters <code class="reqn">C_1</code> and
<code class="reqn">C_2</code> respectively, i.e,
</p>
<p style="text-align: center;"><code class="reqn">\alpha_j = \alpha'_j \frac{n_1}{n_1+n_2},</code>
</p>

<p>where <code class="reqn">\alpha'_1</code>, <code class="reqn">\alpha'_2</code> are determined from <code>par.method</code>,
either directly as <code class="reqn">(\alpha_1, \alpha_2, \beta, \gamma)</code> or
<code class="reqn">(\alpha_1, \alpha_2, \beta)</code> with <code class="reqn">\gamma = 0</code>, or (less flexibly,
but more conveniently) as follows:
</p>
<p>Belbin et al proposed &ldquo;flexible beta&rdquo;, i.e. the user would only
specify <code class="reqn">\beta</code> (as <code>par.method</code>), sensibly in
</p>
<p style="text-align: center;"><code class="reqn">-1 \leq \beta &lt; 1,</code>
</p>

<p>and <code class="reqn">\beta</code> determines <code class="reqn">\alpha'_1</code> and <code class="reqn">\alpha'_2</code> as
</p>
<p style="text-align: center;"><code class="reqn">\alpha'_j = 1 - \beta,</code>
</p>
<p> and <code class="reqn">\gamma = 0</code>.
</p>
<p>This <code class="reqn">\beta</code> may be specified by <code>par.method</code> (as length 1 vector),
and if <code>par.method</code> is not specified, a default value of -0.1 is used,
as Belbin et al recommend taking a <code class="reqn">\beta</code> value around -0.1 as a general
agglomerative hierarchical clustering strategy.
</p>
<p>Note that <code>method = "gaverage", par.method = 0</code> (or <code>par.method =
  c(1,1,0,0)</code>) is equivalent to the <code>agnes()</code> default method <code>"average"</code>.
</p>


<h3>Value</h3>

<p>an object of class <code>"agnes"</code> (which extends <code>"twins"</code>)
representing the clustering.  See <code><a href="#topic+agnes.object">agnes.object</a></code> for
details, and methods applicable.
</p>


<h3>BACKGROUND</h3>

<p>Cluster analysis divides a dataset into groups (clusters) of
observations that are similar to each other.
</p>

<dl>
<dt>Hierarchical methods</dt><dd><p>like
<code>agnes</code>, <code><a href="#topic+diana">diana</a></code>, and <code><a href="#topic+mona">mona</a></code>
construct a hierarchy of clusterings, with the number of clusters
ranging from one to the number of observations.</p>
</dd>
<dt>Partitioning methods</dt><dd><p>like
<code><a href="#topic+pam">pam</a></code>, <code><a href="#topic+clara">clara</a></code>, and <code><a href="#topic+fanny">fanny</a></code>
require that the number of clusters be given by the user.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Method <code>"gaverage"</code> has been contributed by Pierre Roudier, Landcare
Research, New Zealand.
</p>


<h3>References</h3>

<p>Kaufman, L. and Rousseeuw, P.J. (1990). (=: &ldquo;K&amp;R(1990)&rdquo;)
<em>Finding Groups in Data: An Introduction to Cluster Analysis</em>.
Wiley, New York.
</p>
<p>Anja Struyf, Mia Hubert and Peter J. Rousseeuw (1996)
Clustering in an Object-Oriented Environment.
<em>Journal of Statistical Software</em> <b>1</b>.
<a href="https://doi.org/10.18637/jss.v001.i04">doi:10.18637/jss.v001.i04</a>
</p>
<p>Struyf, A., Hubert, M. and Rousseeuw, P.J. (1997). Integrating
Robust Clustering Techniques in S-PLUS,
<em>Computational Statistics and Data Analysis</em>, <b>26</b>, 17&ndash;37.
</p>
<p>Lance, G.N., and W.T. Williams (1966).
A General Theory of Classifactory Sorting Strategies, I. Hierarchical
Systems.
<em>Computer J.</em> <b>9</b>, 373&ndash;380.
</p>
<p>Belbin, L., Faith, D.P. and Milligan, G.W. (1992). A Comparison of
Two Approaches to Beta-Flexible Clustering.
<em>Multivariate Behavioral Research</em>, <b>27</b>, 417&ndash;433.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+agnes.object">agnes.object</a></code>, <code><a href="#topic+daisy">daisy</a></code>, <code><a href="#topic+diana">diana</a></code>,
<code><a href="stats.html#topic+dist">dist</a></code>, <code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="#topic+plot.agnes">plot.agnes</a></code>,
<code><a href="#topic+twins.object">twins.object</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(votes.repub)
agn1 &lt;- agnes(votes.repub, metric = "manhattan", stand = TRUE)
agn1
plot(agn1)

op &lt;- par(mfrow=c(2,2))
agn2 &lt;- agnes(daisy(votes.repub), diss = TRUE, method = "complete")
plot(agn2)
## alpha = 0.625 ==&gt; beta = -1/4  is "recommended" by some
agnS &lt;- agnes(votes.repub, method = "flexible", par.meth = 0.625)
plot(agnS)
par(op)

## "show" equivalence of three "flexible" special cases
d.vr &lt;- daisy(votes.repub)
a.wgt  &lt;- agnes(d.vr, method = "weighted")
a.sing &lt;- agnes(d.vr, method = "single")
a.comp &lt;- agnes(d.vr, method = "complete")
iC &lt;- -(6:7) # not using 'call' and 'method' for comparisons
stopifnot(
  all.equal(a.wgt [iC], agnes(d.vr, method="flexible", par.method = 0.5)[iC])   ,
  all.equal(a.sing[iC], agnes(d.vr, method="flex", par.method= c(.5,.5,0, -.5))[iC]),
  all.equal(a.comp[iC], agnes(d.vr, method="flex", par.method= c(.5,.5,0, +.5))[iC]))

## Exploring the dendrogram structure
(d2 &lt;- as.dendrogram(agn2)) # two main branches
d2[[1]] # the first branch
d2[[2]] # the 2nd one  { 8 + 42  = 50 }
d2[[1]][[1]]# first sub-branch of branch 1 .. and shorter form
identical(d2[[c(1,1)]],
          d2[[1]][[1]])
## a "textual picture" of the dendrogram :
str(d2)

data(agriculture)

## Plot similar to Figure 7 in ref
## Not run: plot(agnes(agriculture), ask = TRUE)


data(animals)
aa.a  &lt;- agnes(animals) # default method = "average"
aa.ga &lt;- agnes(animals, method = "gaverage")
op &lt;- par(mfcol=1:2, mgp=c(1.5, 0.6, 0), mar=c(.1+ c(4,3,2,1)),
          cex.main=0.8)
plot(aa.a,  which.plot = 2)
plot(aa.ga, which.plot = 2)
par(op)


## Show how "gaverage" is a "generalized average":
aa.ga.0 &lt;- agnes(animals, method = "gaverage", par.method = 0)
stopifnot(all.equal(aa.ga.0[iC], aa.a[iC]))
</code></pre>

<hr>
<h2 id='agnes.object'>Agglomerative Nesting (AGNES) Object</h2><span id='topic+agnes.object'></span>

<h3>Description</h3>

<p>The objects of class <code>"agnes"</code>
represent an agglomerative hierarchical clustering of a dataset.
</p>


<h3>Value</h3>

<p>A legitimate <code>agnes</code> object is a list with the following components:
</p>
<table>
<tr><td><code>order</code></td>
<td>

<p>a vector giving a permutation of the original observations to allow
for plotting, in the sense that the branches of a clustering tree
will not cross.</p>
</td></tr>
<tr><td><code>order.lab</code></td>
<td>

<p>a vector similar to <code>order</code>, but containing observation labels
instead of observation numbers. This component is only available if
the original observations were labelled.
</p>
</td></tr>
<tr><td><code>height</code></td>
<td>

<p>a vector with the distances between merging clusters at the successive
stages.
</p>
</td></tr>
<tr><td><code>ac</code></td>
<td>

<p>the agglomerative coefficient, measuring the clustering structure of the
dataset.
</p>
<p>For each observation i, denote by m(i) its dissimilarity to the
first cluster it is merged with, divided by the dissimilarity of the
merger in the final step of the algorithm.  The <code>ac</code> is the
average of all 1 - m(i). It can also be seen as the average width
(or the percentage filled) of the banner plot.  Because <code>ac</code>
grows with the number of observations, this measure should not
be used to compare datasets of very different sizes.
</p>
</td></tr>
<tr><td><code>merge</code></td>
<td>

<p>an (n-1) by 2 matrix, where n is the number of observations.  Row i
of <code>merge</code> describes the merging of clusters at step i of the
clustering.  If a number j in the row is negative, then the single
observation |j| is merged at this stage.  If j is positive, then the
merger is with the cluster formed at stage j of the algorithm.
</p>
</td></tr>
<tr><td><code>diss</code></td>
<td>

<p>an object of class <code>"dissimilarity"</code> (see
<code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code>), representing the total
dissimilarity matrix of the dataset.
</p>
</td></tr>
<tr><td><code>data</code></td>
<td>

<p>a matrix containing the original or standardized measurements, depending
on the <code>stand</code> option of the function <code>agnes</code>. If a
dissimilarity matrix was given as input structure, then this
component is not available.
</p>
</td></tr>
</table>


<h3>GENERATION</h3>

<p>This class of objects is returned from <code><a href="#topic+agnes">agnes</a></code>.
</p>


<h3>METHODS</h3>

<p>The <code>"agnes"</code> class has methods for the following generic functions:
<code>print</code>, <code>summary</code>, <code>plot</code>, and
<code><a href="stats.html#topic+as.dendrogram">as.dendrogram</a></code>.
</p>
<p>In addition, <code><a href="stats.html#topic+cutree">cutree</a>(x, *)</code> can be used to &ldquo;cut&rdquo;
the dendrogram in order to produce cluster assignments.
</p>


<h3>INHERITANCE</h3>

<p>The class <code>"agnes"</code> inherits from <code>"twins"</code>.
Therefore, the generic functions <code><a href="#topic+pltree">pltree</a></code> and
<code><a href="stats.html#topic+as.hclust">as.hclust</a></code> are available for <code>agnes</code> objects.
After applying <code>as.hclust()</code>, all <em>its</em> methods are
available, of course.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code>, <code><a href="#topic+diana">diana</a></code>,
<code><a href="stats.html#topic+as.hclust">as.hclust</a></code>, <code><a href="stats.html#topic+hclust">hclust</a></code>,
<code><a href="#topic+plot.agnes">plot.agnes</a></code>, <code><a href="#topic+twins.object">twins.object</a></code>.
</p>
<p><code><a href="stats.html#topic+cutree">cutree</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agriculture)
ag.ag &lt;- agnes(agriculture)
class(ag.ag)
pltree(ag.ag) # the dendrogram

## cut the dendrogram -&gt; get cluster assignments:
(ck3 &lt;- cutree(ag.ag, k = 3))
(ch6 &lt;- cutree(as.hclust(ag.ag), h = 6))
stopifnot(identical(unname(ch6), ck3))
</code></pre>

<hr>
<h2 id='agriculture'>European Union Agricultural Workforces</h2><span id='topic+agriculture'></span>

<h3>Description</h3>

<p>Gross National Product (GNP) per capita and percentage of the
population working in agriculture for each country belonging to the
European Union in 1993.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(agriculture)</code></pre>


<h3>Format</h3>

<p>A data frame with 12 observations on 2 variables:
</p>

<table>
<tr>
 <td style="text-align: right;">
    [ , 1] </td><td style="text-align: left;"> <code>x</code> </td><td style="text-align: left;"> numeric </td><td style="text-align: left;"> per capita GNP </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , 2] </td><td style="text-align: left;"> <code>y</code> </td><td style="text-align: left;"> numeric </td><td style="text-align: left;"> percentage in agriculture
  </td>
</tr>

</table>

<p>The row names of the data frame indicate the countries.
</p>


<h3>Details</h3>

<p>The data seem to show two clusters, the &ldquo;more agricultural&rdquo; one
consisting of Greece, Portugal, Spain, and Ireland.
</p>


<h3>Source</h3>

<p>Eurostat (European Statistical Agency, 1994):
<em>Cijfers en feiten: Een statistisch portret van de Europese Unie</em>.
</p>


<h3>References</h3>

<p>see those in <code><a href="#topic+agnes">agnes</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code>, <code><a href="#topic+daisy">daisy</a></code>, <code><a href="#topic+diana">diana</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agriculture)

## Compute the dissimilarities using Euclidean metric and without
## standardization
daisy(agriculture, metric = "euclidean", stand = FALSE)

## 2nd plot is similar to Figure 3 in Struyf et al (1996)
plot(pam(agriculture, 2))

## Plot similar to Figure 7 in Struyf et al (1996)
## Not run: plot(agnes(agriculture), ask = TRUE)


## Plot similar to Figure 8 in Struyf et al (1996)
## Not run: plot(diana(agriculture), ask = TRUE)

</code></pre>

<hr>
<h2 id='animals'>Attributes of Animals</h2><span id='topic+animals'></span>

<h3>Description</h3>

<p>This data set considers 6 binary attributes for 20 animals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(animals)</code></pre>


<h3>Format</h3>

<p>A data frame with 20 observations on 6 variables:
</p>

<table>
<tr>
 <td style="text-align: right;">
    [ , 1] </td><td style="text-align: left;"> war </td><td style="text-align: left;"> warm-blooded </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , 2] </td><td style="text-align: left;"> fly </td><td style="text-align: left;"> can fly </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , 3] </td><td style="text-align: left;"> ver </td><td style="text-align: left;"> vertebrate </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , 4] </td><td style="text-align: left;"> end </td><td style="text-align: left;"> endangered </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , 5] </td><td style="text-align: left;"> gro </td><td style="text-align: left;"> live in groups </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , 6] </td><td style="text-align: left;"> hai </td><td style="text-align: left;"> have hair </td>
</tr>
<tr>
 <td style="text-align: right;">
  </td>
</tr>

</table>

<p>All variables are encoded as 1 = 'no', 2 = 'yes'.
</p>


<h3>Details</h3>

<p>This dataset is useful for illustrating monothetic (only a single
variable is used for each split) hierarchical clustering.
</p>


<h3>Source</h3>

<p>Leonard Kaufman and Peter J. Rousseeuw (1990):
<em>Finding Groups in Data</em>
(pp 297ff).
New York: Wiley.
</p>


<h3>References</h3>

<p>see Struyf, Hubert &amp; Rousseeuw (1996),  in <code><a href="#topic+agnes">agnes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(animals)
apply(animals,2, table) # simple overview

ma &lt;- mona(animals)
ma
## Plot similar to Figure 10 in Struyf et al (1996)
plot(ma)
</code></pre>

<hr>
<h2 id='bannerplot'>Plot Banner (of Hierarchical Clustering)</h2><span id='topic+bannerplot'></span>

<h3>Description</h3>

<p>Draws a &ldquo;banner&rdquo;, i.e. basically a horizontal <code><a href="graphics.html#topic+barplot">barplot</a></code>
visualizing the (agglomerative or divisive) hierarchical clustering or
an other binary dendrogram structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bannerplot(x, w = rev(x$height), fromLeft = TRUE,
           main=NULL, sub=NULL, xlab = "Height",  adj = 0,
           col = c(2, 0), border = 0, axes = TRUE, frame.plot = axes,
           rev.xax = !fromLeft, xax.pretty = TRUE,
           labels = NULL, nmax.lab = 35, max.strlen = 5,
           yax.do = axes &amp;&amp; length(x$order) &lt;= nmax.lab,
           yaxRight = fromLeft, y.mar = 2.4 + max.strlen/2.5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bannerplot_+3A_x">x</code></td>
<td>
<p>a list with components <code>order</code>, <code>order.lab</code> and
<code>height</code> when <code>w</code>, the next argument is not specified.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_w">w</code></td>
<td>
<p>non-negative numeric vector of bar widths.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_fromleft">fromLeft</code></td>
<td>
<p>logical, indicating if the banner is from the left or not.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_main">main</code>, <code id="bannerplot_+3A_sub">sub</code></td>
<td>
<p>main and sub titles, see <code><a href="graphics.html#topic+title">title</a></code>.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_xlab">xlab</code></td>
<td>
<p>x axis label (with &lsquo;correct&rsquo; default e.g. for
<code>plot.agnes</code>).</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_adj">adj</code></td>
<td>
<p>passed to <code><a href="graphics.html#topic+title">title</a>(main,sub)</code> for string adjustment.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_col">col</code></td>
<td>
<p>vector of length 2, for two horizontal segments.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_border">border</code></td>
<td>
<p>color for bar border; now defaults to background (no border).</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_axes">axes</code></td>
<td>
<p>logical indicating if axes (and labels) should be drawn at all.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_frame.plot">frame.plot</code></td>
<td>
<p>logical indicating the banner should be framed;
mainly used when <code>border = 0</code> (as per default).</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_rev.xax">rev.xax</code></td>
<td>
<p>logical indicating if the x axis should be reversed (as
in <code>plot.diana</code>).</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_xax.pretty">xax.pretty</code></td>
<td>
<p>logical or integer indicating if
<code><a href="base.html#topic+pretty">pretty</a>()</code> should be used for the x axis.
<code>xax.pretty = FALSE</code> is mainly for back compatibility.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_labels">labels</code></td>
<td>
<p>labels to use on y-axis; the default is constructed from
<code>x</code>.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_nmax.lab">nmax.lab</code></td>
<td>
<p>integer indicating the number of labels which is
considered too large for single-name labelling the banner plot.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_max.strlen">max.strlen</code></td>
<td>
<p>positive integer giving the length to which
strings are truncated in banner plot labeling.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_yax.do">yax.do</code></td>
<td>
<p>logical indicating if a y axis and banner labels should
be drawn.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_yaxright">yaxRight</code></td>
<td>
<p>logical indicating if the y axis is on the right or left.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_y.mar">y.mar</code></td>
<td>
<p>positive number specifying the margin width to use when
banners are labeled (along a y-axis).  The default adapts to the
string width and optimally would also dependend on the font.</p>
</td></tr>
<tr><td><code id="bannerplot_+3A_...">...</code></td>
<td>
<p>graphical parameters (see <code><a href="graphics.html#topic+par">par</a></code>) may also
be supplied as arguments to this function.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This is mainly a utility called from <code><a href="#topic+plot.agnes">plot.agnes</a></code>,
<code><a href="#topic+plot.diana">plot.diana</a></code> and <code><a href="#topic+plot.mona">plot.mona</a></code>.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler (from original code of Kaufman and Rousseeuw).</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agriculture)
bannerplot(agnes(agriculture), main = "Bannerplot")
</code></pre>

<hr>
<h2 id='chorSub'>Subset of C-horizon of Kola Data</h2><span id='topic+chorSub'></span>

<h3>Description</h3>

<p>This is a small rounded subset of the C-horizon data
<code><a href="mvoutlier.html#topic+chorizon">chorizon</a></code> from package <span class="pkg">mvoutlier</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(chorSub)</code></pre>


<h3>Format</h3>

<p>A data frame with 61 observations on 10 variables.  The variables
contain scaled concentrations of chemical elements.
</p>


<h3>Details</h3>

<p>This data set was produced from <code>chorizon</code> via these statements:
</p>
<pre>
    data(chorizon, package = "mvoutlier")
    chorSub &lt;- round(100*scale(chorizon[,101:110]))[190:250,]
    storage.mode(chorSub) &lt;- "integer"
    colnames(chorSub) &lt;- gsub("_.*", '', colnames(chorSub))
  </pre>


<h3>Source</h3>

<p>Kola Project (1993-1998)
</p>


<h3>See Also</h3>

<p><code><a href="mvoutlier.html#topic+chorizon">chorizon</a></code> in package <span class="pkg">mvoutlier</span> and other
Kola data in the same package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(chorSub)
summary(chorSub)
pairs(chorSub, gap= .1)# some outliers
</code></pre>

<hr>
<h2 id='clara'>Clustering Large Applications</h2><span id='topic+clara'></span>

<h3>Description</h3>

<p>Computes a <code>"clara"</code> object, a <code><a href="base.html#topic+list">list</a></code> representing a
clustering of the data into <code>k</code> clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clara(x, k, metric = c("euclidean", "manhattan", "jaccard"),
      stand = FALSE, cluster.only = FALSE, samples = 5,
      sampsize = min(n, 40 + 2 * k), trace = 0, medoids.x = TRUE,
      keep.data = medoids.x, rngR = FALSE, pamLike = FALSE, correct.d = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clara_+3A_x">x</code></td>
<td>

<p>data matrix or data frame, each row corresponds to an observation,
and each column corresponds to a variable.  All variables must be
numeric (or logical).
Missing values (NAs) are allowed.</p>
</td></tr>
<tr><td><code id="clara_+3A_k">k</code></td>
<td>
<p>integer, the number of clusters.
It is required that <code class="reqn">0 &lt; k &lt; n</code> where <code class="reqn">n</code> is the number of
observations (i.e., n = <code>nrow(x)</code>).</p>
</td></tr>
<tr><td><code id="clara_+3A_metric">metric</code></td>
<td>

<p>character string specifying the metric to be used for calculating
dissimilarities between observations.
The currently available options are &quot;euclidean&quot;, &quot;manhattan&quot;, 
&quot;jaccard&quot;. 
</p>
<p>Euclidean distances are root sum-of-squares of differences, and
manhattan distances are the sum of absolute differences.
</p>
</td></tr>
<tr><td><code id="clara_+3A_stand">stand</code></td>
<td>
<p>logical, indicating if the measurements in <code>x</code> are
standardized before calculating the dissimilarities.  Measurements
are standardized for each variable (column), by subtracting the
variable's mean value and dividing by the variable's mean absolute
deviation.
</p>
</td></tr>
<tr><td><code id="clara_+3A_cluster.only">cluster.only</code></td>
<td>
<p>logical; if true, only the clustering will be
computed and returned, see details.</p>
</td></tr>
<tr><td><code id="clara_+3A_samples">samples</code></td>
<td>
<p>integer, say <code class="reqn">N</code>, the number of samples to be drawn from the
dataset.  The default, <code>N = 5</code>, is rather small for historical (and
now back compatibility) reasons and we <em>recommend to set
<code>samples</code> an order of magnitude larger</em>.
</p>
</td></tr>
<tr><td><code id="clara_+3A_sampsize">sampsize</code></td>
<td>
<p>integer, say <code class="reqn">j</code>, the number of observations in each
sample.  <code>sampsize</code> should be higher than the number of clusters
(<code>k</code>) and at most the number of observations (<code class="reqn">n =</code>
<code>nrow(x)</code>).  While computational effort is proportional to <code class="reqn">j^2</code>,
see note below, it may still be advisable to set
<code class="reqn">j = </code><code>sampsize</code> to a <em>larger</em> value than the (historical) default.</p>
</td></tr>
<tr><td><code id="clara_+3A_trace">trace</code></td>
<td>
<p>integer indicating a <em>trace level</em> for diagnostic
output during the algorithm.</p>
</td></tr>
<tr><td><code id="clara_+3A_medoids.x">medoids.x</code></td>
<td>
<p>logical indicating if the medoids should be
returned, identically to some rows of the input data <code>x</code>.  If
<code>FALSE</code>, <code>keep.data</code> must be false as well, and the medoid
indices, i.e., row numbers of the medoids will still be returned
(<code>i.med</code> component), and the algorithm saves space by needing
one copy less of <code>x</code>.</p>
</td></tr>
<tr><td><code id="clara_+3A_keep.data">keep.data</code></td>
<td>
<p>logical indicating if the (<em>scaled</em> if
<code>stand</code> is true) data should be kept in the result.
Setting this to <code>FALSE</code> saves memory (and hence time), but
disables <code><a href="#topic+clusplot">clusplot</a>()</code>ing of the result.  Use
<code>medoids.x = FALSE</code> to save even more memory.</p>
</td></tr>
<tr><td><code id="clara_+3A_rngr">rngR</code></td>
<td>
<p>logical indicating if <span class="rlang"><b>R</b></span>'s random number generator should
be used instead of the primitive clara()-builtin one.  If true, this
also means that each call to <code>clara()</code> returns a different result
&ndash; though only slightly different in good situations.</p>
</td></tr>
<tr><td><code id="clara_+3A_pamlike">pamLike</code></td>
<td>
<p>logical indicating if the &ldquo;swap&rdquo; phase (see
<code><a href="#topic+pam">pam</a></code>, in C code) should use the same algorithm as
<code><a href="#topic+pam">pam</a>()</code>.  Note that from Kaufman and Rousseeuw's
description this <em>should</em> have been true always, but as the
original Fortran code and the subsequent port to C has always
contained a small one-letter change (a typo according to Martin Maechler)
with respect to PAM, the default, <code>pamLike = FALSE</code> has been chosen to
remain back compatible rather than &ldquo;PAM compatible&rdquo;.</p>
</td></tr>
<tr><td><code id="clara_+3A_correct.d">correct.d</code></td>
<td>
<p>logical or integer indicating that&mdash;only in the case
of <code>NA</code>s present in <code>x</code>&mdash;the correct distance computation
should be used instead of the wrong formula which has been present
in the original Fortran code and been in use up to early 2016.
</p>
<p>Because the new correct formula is not back compatible, for the time
being, a warning is signalled in this case, unless the user explicitly
specifies <code>correct.d</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>clara</code> (for &quot;euclidean&quot; and &quot;manhattan&quot;) is fully described in
chapter 3 of Kaufman and Rousseeuw (1990).
Compared to other partitioning methods such as <code>pam</code>, it can deal with
much larger datasets.  Internally, this is achieved by considering
sub-datasets of fixed size (<code>sampsize</code>) such that the time and
storage requirements become linear in <code class="reqn">n</code> rather than quadratic.
</p>
<p>Each sub-dataset is partitioned into <code>k</code> clusters using the same
algorithm as in <code><a href="#topic+pam">pam</a></code>.<br />
Once <code>k</code> representative objects have been selected from the
sub-dataset, each observation of the entire dataset is assigned
to the nearest medoid.
</p>
<p>The mean (equivalent to the sum) of the dissimilarities of the
observations to their closest medoid is used as a measure of the
quality of the clustering.  The sub-dataset for which the mean (or
sum) is minimal, is retained.  A further analysis is carried out on
the final partition.
</p>
<p>Each sub-dataset is forced to contain the medoids obtained from the
best sub-dataset until then.  Randomly drawn observations are added to
this set until <code>sampsize</code> has been reached.
</p>
<p>When <code>cluster.only</code> is true, the result is simply a (possibly
named) integer vector specifying the clustering, i.e.,<br />
<code>clara(x,k, cluster.only=TRUE)</code> is the same as <br />
<code>clara(x,k)$clustering</code> but computed more efficiently.
</p>


<h3>Value</h3>

<p>If <code>cluster.only</code> is false (as by default),
an object of class <code>"clara"</code> representing the clustering.  See
<code><a href="#topic+clara.object">clara.object</a></code> for details.
</p>
<p>If <code>cluster.only</code> is true, the result is the &quot;clustering&quot;, an
integer vector of length <code class="reqn">n</code> with entries from <code>1:k</code>.
</p>


<h3>Note</h3>

<p>By default, the random sampling is implemented with a <em>very</em>
simple scheme (with period <code class="reqn">2^{16} = 65536</code>) inside the Fortran
code, independently of <span class="rlang"><b>R</b></span>'s random number generation, and as a matter
of fact, deterministically.  Alternatively, we recommend setting
<code>rngR = TRUE</code> which uses <span class="rlang"><b>R</b></span>'s random number generators.  Then,
<code>clara()</code> results are made reproducible typically by using
<code><a href="base.html#topic+set.seed">set.seed</a>()</code> before calling <code>clara</code>.
</p>
<p>The storage requirement of <code>clara</code> computation (for small
<code>k</code>) is about
<code class="reqn">O(n \times p) + O(j^2)</code> where
<code class="reqn">j = \code{sampsize}</code>, and <code class="reqn">(n,p) = \code{dim(x)}</code>.
The CPU computing time (again assuming small <code>k</code>) is about
<code class="reqn">O(n \times p \times j^2 \times N)</code>, where
<code class="reqn">N = \code{samples}</code>.
</p>
<p>For &ldquo;small&rdquo; datasets, the function <code><a href="#topic+pam">pam</a></code> can be used
directly.  What can be considered <em>small</em>, is really a function
of available computing power, both memory (RAM) and speed.
Originally (1990), &ldquo;small&rdquo; meant less than 100 observations;
in 1997, the authors said <em>&ldquo;small (say with fewer than 200
observations)&rdquo;</em>; as of 2006, you can use <code><a href="#topic+pam">pam</a></code> with
several thousand observations.
</p>


<h3>Author(s)</h3>

<p>Kaufman and Rousseeuw (see <code><a href="#topic+agnes">agnes</a></code>), originally.
Metric <code>"jaccard"</code>: Kamil Kozlowski (<code>@ownedoutcomes.com</code>)
and Kamil Jadeszko.
All arguments from <code>trace</code> on, and most <span class="rlang"><b>R</b></span> documentation and all
tests by Martin Maechler.
</p>



<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code> for background and references;
<code><a href="#topic+clara.object">clara.object</a></code>, <code><a href="#topic+pam">pam</a></code>,
<code><a href="#topic+partition.object">partition.object</a></code>, <code><a href="#topic+plot.partition">plot.partition</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 500 objects, divided into 2 clusters.
x &lt;- rbind(cbind(rnorm(200,0,8), rnorm(200,0,8)),
           cbind(rnorm(300,50,8), rnorm(300,50,8)))
clarax &lt;- clara(x, 2, samples=50)
clarax
clarax$clusinfo
## using pamLike=TRUE  gives the same (apart from the 'call'):
all.equal(clarax[-8],
          clara(x, 2, samples=50, pamLike = TRUE)[-8])
plot(clarax)

## cluster.only = TRUE -- save some memory/time :
clclus &lt;- clara(x, 2, samples=50, cluster.only = TRUE)
stopifnot(identical(clclus, clarax$clustering))


## 'xclara' is an artificial data set with 3 clusters of 1000 bivariate
## objects each.
data(xclara)
(clx3 &lt;- clara(xclara, 3))
## "better" number of samples
cl.3 &lt;- clara(xclara, 3, samples=100)
## but that did not change the result here:
stopifnot(cl.3$clustering == clx3$clustering)
## Plot similar to Figure 5 in Struyf et al (1996)
## Not run: plot(clx3, ask = TRUE)


## Try 100 times *different* random samples -- for reliability:
nSim &lt;- 100
nCl &lt;- 3 # = no.classes
set.seed(421)# (reproducibility)
cl &lt;- matrix(NA,nrow(xclara), nSim)
for(i in 1:nSim)
   cl[,i] &lt;- clara(xclara, nCl, medoids.x = FALSE, rngR = TRUE)$cluster
tcl &lt;- apply(cl,1, tabulate, nbins = nCl)
## those that are not always in same cluster (5 out of 3000 for this seed):
(iDoubt &lt;- which(apply(tcl,2, function(n) all(n &lt; nSim))))
if(length(iDoubt)) { # (not for all seeds)
  tabD &lt;- tcl[,iDoubt, drop=FALSE]
  dimnames(tabD) &lt;- list(cluster = paste(1:nCl), obs = format(iDoubt))
  t(tabD) # how many times in which clusters
}
</code></pre>

<hr>
<h2 id='clara.object'>Clustering Large Applications (CLARA) Object</h2><span id='topic+clara.object'></span>

<h3>Description</h3>

<p>The objects of class <code>"clara"</code> represent a partitioning of a large
dataset into clusters and are typically returned from <code><a href="#topic+clara">clara</a></code>.
</p>


<h3>Value</h3>

<p>A legitimate <code>clara</code> object is a list with the following components:
</p>
<table>
<tr><td><code>sample</code></td>
<td>

<p>labels or case numbers of the observations in the best sample, that is,
the sample used by the <code>clara</code> algorithm for the final partition.</p>
</td></tr>
<tr><td><code>medoids</code></td>
<td>
<p>the medoids or representative objects of the clusters.
It is a matrix with in each row the coordinates of one medoid.
Possibly <code>NULL</code>, namely when the object resulted from
<code>clara(*, medoids.x=FALSE)</code>. Use the following <code>i.med</code> in
that case.</p>
</td></tr>
<tr><td><code>i.med</code></td>
<td>

<p>the <em>indices</em> of the <code>medoids</code> above: <code>medoids &lt;- x[i.med,]</code>
where <code>x</code> is the original data matrix in <code>clara(x,*)</code>.</p>
</td></tr>
<tr><td><code>clustering</code></td>
<td>
<p>the clustering vector, see <code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>the objective function for the final clustering of
the entire dataset.</p>
</td></tr>
<tr><td><code>clusinfo</code></td>
<td>

<p>matrix, each row gives numerical information for one cluster. These
are the cardinality of the cluster (number of observations), the
maximal and average dissimilarity between the observations in the
cluster and the cluster's medoid.  
The last column is the maximal
dissimilarity between the observations in the cluster and the
cluster's medoid, divided by the minimal dissimilarity between the
cluster's medoid and the medoid of any other cluster. If this ratio
is small, the cluster is well-separated from the other clusters.
</p>
</td></tr>
<tr><td><code>diss</code></td>
<td>
<p>dissimilarity (maybe NULL), see <code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>silinfo</code></td>
<td>
<p>list with silhouette width information for the best sample, see
<code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>generating call, see <code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>matrix, possibibly standardized, or NULL, see
<code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
</table>


<h3>Methods, Inheritance</h3>

<p>The <code>"clara"</code> class has methods for the following generic functions:
<code>print</code>, <code>summary</code>.
</p>
<p>The class <code>"clara"</code> inherits from <code>"partition"</code>.
Therefore, the generic functions <code>plot</code> and <code>clusplot</code> can
be used on a <code>clara</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clara">clara</a></code>, <code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code>,
<code><a href="#topic+partition.object">partition.object</a></code>, <code><a href="#topic+plot.partition">plot.partition</a></code>.
</p>

<hr>
<h2 id='clusGap'>Gap Statistic for Estimating the Number of Clusters</h2><span id='topic+clusGap'></span><span id='topic+maxSE'></span><span id='topic+print.clusGap'></span><span id='topic+plot.clusGap'></span>

<h3>Description</h3>

<p><code>clusGap()</code> calculates a goodness of clustering measure, the
&ldquo;gap&rdquo; statistic.  For each number of clusters <code class="reqn">k</code>, it
compares <code class="reqn">\log(W(k))</code> with
<code class="reqn">E^*[\log(W(k))]</code> where the latter is defined via
bootstrapping, i.e., simulating from a reference (<code class="reqn">H_0</code>)
distribution, a uniform distribution on the hypercube determined by
the ranges of <code>x</code>, after first centering, and then
<code><a href="base.html#topic+svd">svd</a></code> (aka &lsquo;PCA&rsquo;)-rotating them when (as by
default) <code>spaceH0 = "scaledPCA"</code>.
</p>
<p><code>maxSE(f, SE.f)</code> determines the location of the <b>maximum</b>
of <code>f</code>, taking a &ldquo;1-SE rule&rdquo; into account for the
<code>*SE*</code> methods.  The default method <code>"firstSEmax"</code> looks for
the smallest <code class="reqn">k</code> such that its value <code class="reqn">f(k)</code> is not more than 1
standard error away from the first local maximum.
This is similar but not the same as <code>"Tibs2001SEmax"</code>, Tibshirani
et al's recommendation of determining the number of clusters from the
gap statistics and their standard deviations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusGap(x, FUNcluster, K.max, B = 100, d.power = 1,
        spaceH0 = c("scaledPCA", "original"),
        verbose = interactive(), ...)

maxSE(f, SE.f,
      method = c("firstSEmax", "Tibs2001SEmax", "globalSEmax",
                 "firstmax", "globalmax"),
      SE.factor = 1)

## S3 method for class 'clusGap'
print(x, method = "firstSEmax", SE.factor = 1, ...)

## S3 method for class 'clusGap'
plot(x, type = "b", xlab = "k", ylab = expression(Gap[k]),
     main = NULL, do.arrows = TRUE,
     arrowArgs = list(col="red3", length=1/16, angle=90, code=3), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusGap_+3A_x">x</code></td>
<td>
<p>numeric matrix or <code><a href="base.html#topic+data.frame">data.frame</a></code>.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_funcluster">FUNcluster</code></td>
<td>
<p>a <code><a href="base.html#topic+function">function</a></code> which accepts as first
argument a (data) matrix like <code>x</code>, second argument, say
<code class="reqn">k, k\geq 2</code>, the number of clusters desired,
and returns a <code><a href="base.html#topic+list">list</a></code> with a component named (or shortened to)
<code>cluster</code> which is a vector of length <code>n = nrow(x)</code> of
integers in <code>1:k</code> determining the clustering or grouping of the
<code>n</code> observations.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_k.max">K.max</code></td>
<td>
<p>the maximum number of clusters to consider, must be at
least two.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_b">B</code></td>
<td>
<p>integer, number of Monte Carlo (&ldquo;bootstrap&rdquo;) samples.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_d.power">d.power</code></td>
<td>
<p>a positive integer specifying the power <code class="reqn">p</code> which
is applied to the euclidean distances (<code><a href="stats.html#topic+dist">dist</a></code>) before
they are summed up to give <code class="reqn">W(k)</code>.  The default, <code>d.power = 1</code>,
corresponds to the &ldquo;historical&rdquo; <span class="rlang"><b>R</b></span> implementation, whereas
<code>d.power = 2</code> corresponds to what Tibshirani et al had
proposed.  This was found by Juan Gonzalez, in 2016-02.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_spaceh0">spaceH0</code></td>
<td>
<p>a <code><a href="base.html#topic+character">character</a></code> string specifying the
space of the <code class="reqn">H_0</code> distribution (of <em>no</em> cluster).  Both
<code>"scaledPCA"</code> and <code>"original"</code> use a uniform distribution
in a hyper cube and had been mentioned in the reference;
<code>"original"</code> been added after a proposal (including code) by
Juan Gonzalez.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_verbose">verbose</code></td>
<td>
<p>integer or logical, determining if &ldquo;progress&rdquo;
output should be printed.  The default prints one bit per bootstrap
sample.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_...">...</code></td>
<td>
<p>(for <code>clusGap()</code>:) optionally further arguments for
<code>FUNcluster()</code>, see <code>kmeans</code> example below.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_f">f</code></td>
<td>
<p>numeric vector of &lsquo;function values&rsquo;, of length
<code class="reqn">K</code>, whose (&ldquo;1 SE respected&rdquo;) maximum we want.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_se.f">SE.f</code></td>
<td>
<p>numeric vector of length <code class="reqn">K</code> of standard errors of <code>f</code>.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_method">method</code></td>
<td>
<p>character string indicating how the &ldquo;optimal&rdquo;
number of clusters, <code class="reqn">\hat k</code>, is computed from the gap
statistics (and their standard deviations), or more generally how
the location <code class="reqn">\hat k</code> of the maximum of <code class="reqn">f_k</code>
should be determined.
</p>


<dl>
<dt><code>"globalmax"</code>:</dt><dd><p>simply corresponds to the global maximum,
i.e., is <code>which.max(f)</code></p>
</dd>
<dt><code>"firstmax"</code>:</dt><dd><p>gives the location of the first <em>local</em>
maximum.</p>
</dd>
<dt><code>"Tibs2001SEmax"</code>:</dt><dd><p>uses the criterion, Tibshirani et
al (2001) proposed: &ldquo;the smallest <code class="reqn">k</code> such that <code class="reqn">f(k)
	    \ge f(k+1) - s_{k+1}</code>&rdquo;.  Note that this chooses <code class="reqn">k = 1</code>
when all standard deviations are larger than the differences
<code class="reqn">f(k+1) - f(k)</code>.</p>
</dd>
<dt><code>"firstSEmax"</code>:</dt><dd><p>location of the first <code class="reqn">f()</code> value
which is not smaller than the first <em>local</em> maximum minus
<code>SE.factor * SE.f[]</code>, i.e, within an &ldquo;f S.E.&rdquo; range
of that maximum (see also <code>SE.factor</code>).
</p>
<p>This, the default, has been proposed by Martin Maechler in 2012,
when adding <code>clusGap()</code> to the <span class="pkg">cluster</span> package, after
having seen the <code>"globalSEmax"</code> proposal (in code) and read
the <code>"Tibs2001SEmax"</code> proposal.</p>
</dd>
<dt><code>"globalSEmax"</code>:</dt><dd><p>(used in Dudoit and Fridlyand (2002),
supposedly following Tibshirani's proposition):
location of the first <code class="reqn">f()</code> value which is not smaller than
the <em>global</em> maximum minus <code>SE.factor * SE.f[]</code>, i.e,
within an &ldquo;f S.E.&rdquo; range of that maximum (see also
<code>SE.factor</code>).</p>
</dd>
</dl>

<p>See the examples for a comparison in a simple case.
</p>
</td></tr>
<tr><td><code id="clusGap_+3A_se.factor">SE.factor</code></td>
<td>
<p>[When <code>method</code> contains <code>"SE"</code>] Determining
the optimal number of clusters, Tibshirani et al. proposed the
&ldquo;1 S.E.&rdquo;-rule.  Using an <code>SE.factor</code> <code class="reqn">f</code>, the
&ldquo;f S.E.&rdquo;-rule is used, more generally.</p>
</td></tr>

<tr><td><code id="clusGap_+3A_type">type</code>, <code id="clusGap_+3A_xlab">xlab</code>, <code id="clusGap_+3A_ylab">ylab</code>, <code id="clusGap_+3A_main">main</code></td>
<td>
<p>arguments with the same meaning as in
<code><a href="graphics.html#topic+plot.default">plot.default</a>()</code>, with different default.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_do.arrows">do.arrows</code></td>
<td>
<p>logical indicating if (1 SE -)&ldquo;error bars&rdquo;
should be drawn, via <code><a href="graphics.html#topic+arrows">arrows</a>()</code>.</p>
</td></tr>
<tr><td><code id="clusGap_+3A_arrowargs">arrowArgs</code></td>
<td>
<p>a list of arguments passed to <code><a href="graphics.html#topic+arrows">arrows</a>()</code>;
the default, notably <code>angle</code> and <code>code</code>, provide a style
matching usual error bars.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main result <code>&lt;res&gt;$Tab[,"gap"]</code> of course is from
bootstrapping aka Monte Carlo simulation and hence random, or
equivalently, depending on the initial random seed (see
<code><a href="base.html#topic+set.seed">set.seed</a>()</code>).
On the other hand, in our experience, using <code>B = 500</code> gives
quite precise results such that the gap plot is basically unchanged
after an another run.
</p>


<h3>Value</h3>

<p><code>clusGap(..)</code> returns an object of S3 class <code>"clusGap"</code>,
basically a list with components
</p>
<table>
<tr><td><code>Tab</code></td>
<td>
<p>a matrix with <code>K.max</code> rows and 4 columns, named
&quot;logW&quot;, &quot;E.logW&quot;, &quot;gap&quot;, and &quot;SE.sim&quot;,
where <code>gap = E.logW - logW</code>, and <code>SE.sim</code> corresponds to
the standard error of <code>gap</code>, <code>SE.sim[k]=</code><code class="reqn">s_k</code>,
where <code class="reqn">s_k := \sqrt{1 + 1/B} sd^*(gap_j)</code>, and <code class="reqn">sd^*()</code> is the standard deviation of the
simulated (&ldquo;bootstrapped&rdquo;) gap values.
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the <code>clusGap(..)</code> <code><a href="base.html#topic+call">call</a></code>.</p>
</td></tr>
<tr><td><code>spaceH0</code></td>
<td>
<p>the <code>spaceH0</code> argument (<code><a href="base.html#topic+match.arg">match.arg</a>()</code>ed).</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>number of observations, i.e., <code>nrow(x)</code>.</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>input <code>B</code></p>
</td></tr>
<tr><td><code>FUNcluster</code></td>
<td>
<p>input function <code>FUNcluster</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>This function is originally based on the functions <code>gap</code> of
former (Bioconductor) package <span class="pkg">SAGx</span> by Per Broberg,
<code>gapStat()</code> from former package <span class="pkg">SLmisc</span> by Matthias Kohl
and ideas from <code>gap()</code> and its methods of package <span class="pkg">lga</span> by
Justin Harrington.
</p>
<p>The current implementation is by Martin Maechler.
</p>
<p>The implementation of <code>spaceH0 = "original"</code> is based on code
proposed by Juan Gonzalez.
</p>


<h3>References</h3>

<p>Tibshirani, R., Walther, G. and Hastie, T. (2001).
Estimating the number of data clusters via the Gap statistic.
<em>Journal of the Royal Statistical Society B</em>, <b>63</b>, 411&ndash;423.
</p>
<p>Tibshirani, R., Walther, G. and Hastie, T. (2000).
Estimating the number of clusters in a dataset via the Gap statistic.
Technical Report. Stanford.
</p>
<p>Dudoit, S. and Fridlyand, J. (2002)
A prediction-based resampling method for estimating the number of clusters in a
dataset. <em>Genome Biology</em> <b>3</b>(7).
<a href="https://doi.org/10.1186/gb-2002-3-7-research0036">doi:10.1186/gb-2002-3-7-research0036</a>
</p>
<p>Per Broberg (2006). SAGx: Statistical Analysis of the GeneChip.
R package version 1.9.7.



<a href="https://bioconductor.org/packages/3.12/bioc/html/SAGx.html">https://bioconductor.org/packages/3.12/bioc/html/SAGx.html</a>
Deprecated and removed from Bioc ca. 2022
</p>


<h3>See Also</h3>

<p><code><a href="#topic+silhouette">silhouette</a></code> for a much simpler less sophisticated
goodness of clustering measure.
</p>
<p><code><a href="fpc.html#topic+cluster.stats">cluster.stats</a>()</code> in package <span class="pkg">fpc</span> for
alternative measures.
</p>



<h3>Examples</h3>

<pre><code class='language-R'>### --- maxSE() methods -------------------------------------------
(mets &lt;- eval(formals(maxSE)$method))
fk &lt;- c(2,3,5,4,7,8,5,4)
sk &lt;- c(1,1,2,1,1,3,1,1)/2
## use plot.clusGap():
plot(structure(class="clusGap", list(Tab = cbind(gap=fk, SE.sim=sk))))
## Note that 'firstmax' and 'globalmax' are always at 3 and 6 :
sapply(c(1/4, 1,2,4), function(SEf)
        sapply(mets, function(M) maxSE(fk, sk, method = M, SE.factor = SEf)))

### --- clusGap() -------------------------------------------------
## ridiculously nicely separated clusters in 3 D :
x &lt;- rbind(matrix(rnorm(150,           sd = 0.1), ncol = 3),
           matrix(rnorm(150, mean = 1, sd = 0.1), ncol = 3),
           matrix(rnorm(150, mean = 2, sd = 0.1), ncol = 3),
           matrix(rnorm(150, mean = 3, sd = 0.1), ncol = 3))

## Slightly faster way to use pam (see below)
pam1 &lt;- function(x,k) list(cluster = pam(x,k, cluster.only=TRUE))

## We do not recommend using hier.clustering here, but if you want,
## there is  factoextra::hcut () or a cheap version of it
hclusCut &lt;- function(x, k, d.meth = "euclidean", ...)
   list(cluster = cutree(hclust(dist(x, method=d.meth), ...), k=k))

## You can manually set it before running this :    doExtras &lt;- TRUE  # or  FALSE
if(!(exists("doExtras") &amp;&amp; is.logical(doExtras)))
  doExtras &lt;- cluster:::doExtras()

if(doExtras) {
  ## Note we use  B = 60 in the following examples to keep them "speedy".
  ## ---- rather keep the default B = 500 for your analysis!

  ## note we can  pass 'nstart = 20' to kmeans() :
  gskmn &lt;- clusGap(x, FUN = kmeans, nstart = 20, K.max = 8, B = 60)
  gskmn #-&gt; its print() method
  plot(gskmn, main = "clusGap(., FUN = kmeans, n.start=20, B= 60)")
  set.seed(12); system.time(
    gsPam0 &lt;- clusGap(x, FUN = pam, K.max = 8, B = 60)
  )
  set.seed(12); system.time(
    gsPam1 &lt;- clusGap(x, FUN = pam1, K.max = 8, B = 60)
  )
  ## and show that it gives the "same":
  not.eq &lt;- c("call", "FUNcluster"); n &lt;- names(gsPam0)
  eq &lt;- n[!(n %in% not.eq)]
  stopifnot(identical(gsPam1[eq], gsPam0[eq]))
  print(gsPam1, method="globalSEmax")
  print(gsPam1, method="globalmax")

  print(gsHc &lt;- clusGap(x, FUN = hclusCut, K.max = 8, B = 60))

}# end {doExtras}

gs.pam.RU &lt;- clusGap(ruspini, FUN = pam1, K.max = 8, B = 60)
gs.pam.RU
plot(gs.pam.RU, main = "Gap statistic for the 'ruspini' data")
mtext("k = 4 is best .. and  k = 5  pretty close")

## This takes a minute..
## No clustering ==&gt; k = 1 ("one cluster") should be optimal:
Z &lt;- matrix(rnorm(256*3), 256,3)
gsP.Z &lt;- clusGap(Z, FUN = pam1, K.max = 8, B = 200)
plot(gsP.Z, main = "clusGap(&lt;iid_rnorm_p=3&gt;)  ==&gt; k = 1  cluster is optimal")
gsP.Z

</code></pre>

<hr>
<h2 id='clusplot'>Bivariate Cluster Plot (of a Partitioning Object)</h2><span id='topic+clusplot'></span><span id='topic+clusplot.partition'></span>

<h3>Description</h3>

<p>Draws a 2-dimensional &ldquo;clusplot&rdquo; (clustering plot) on the
current graphics device.
The generic function has a default and a <code>partition</code> method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusplot(x, ...)

## S3 method for class 'partition'
clusplot(x, main = NULL, dist = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusplot_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object, here, specifically an object of class
<code>"partition"</code>, e.g. created by one of the functions
<code><a href="#topic+pam">pam</a></code>, <code><a href="#topic+clara">clara</a></code>, or <code><a href="#topic+fanny">fanny</a></code>.</p>
</td></tr>
<tr><td><code id="clusplot_+3A_main">main</code></td>
<td>
<p>title for the plot; when <code>NULL</code> (by default), a title
is constructed, using <code>x$call</code>.</p>
</td></tr>
<tr><td><code id="clusplot_+3A_dist">dist</code></td>
<td>
<p>when <code>x</code> does not have a <code>diss</code> nor a
<code>data</code> component, e.g., for <code><a href="#topic+pam">pam</a>(dist(*),
      keep.diss=FALSE)</code>, <code>dist</code> must specify the dissimilarity for the
clusplot.</p>
</td></tr>
<tr><td><code id="clusplot_+3A_...">...</code></td>
<td>
<p>optional arguments passed to methods, notably the
<code><a href="#topic+clusplot.default">clusplot.default</a></code> method (except for the <code>diss</code>
one) may also be supplied to this function.  Many graphical parameters
(see <code><a href="graphics.html#topic+par">par</a></code>) may also be supplied as arguments here.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>clusplot.partition()</code> method relies on <code><a href="#topic+clusplot.default">clusplot.default</a></code>.
</p>
<p>If the clustering algorithms <code>pam</code>, <code>fanny</code> and <code>clara</code>
are applied to a data matrix of observations-by-variables then a
clusplot of the resulting clustering can always be drawn.  When the
data matrix contains missing values and the clustering is performed
with <code><a href="#topic+pam">pam</a></code> or <code><a href="#topic+fanny">fanny</a></code>, the dissimilarity
matrix will be given as input to <code>clusplot</code>.  When the clustering
algorithm <code><a href="#topic+clara">clara</a></code> was applied to a data matrix with <code><a href="base.html#topic+NA">NA</a></code>s
then <code>clusplot()</code> will replace the missing values as described in
<code><a href="#topic+clusplot.default">clusplot.default</a></code>, because a dissimilarity matrix is not
available.
</p>


<h3>Value</h3>

<p>For the <code>partition</code> (and <code>default</code>) method: An invisible
list with components <code>Distances</code> and <code>Shading</code>, as for
<code><a href="#topic+clusplot.default">clusplot.default</a></code>, see there.
</p>


<h3>Side Effects</h3>

<p>a 2-dimensional clusplot is created on the current graphics device.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusplot.default">clusplot.default</a></code> for references;
<code><a href="#topic+partition.object">partition.object</a></code>, <code><a href="#topic+pam">pam</a></code>,
<code><a href="#topic+pam.object">pam.object</a></code>, <code><a href="#topic+clara">clara</a></code>,
<code><a href="#topic+clara.object">clara.object</a></code>, <code><a href="#topic+fanny">fanny</a></code>,
<code><a href="#topic+fanny.object">fanny.object</a></code>, <code><a href="graphics.html#topic+par">par</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## For more, see ?clusplot.default

## generate 25 objects, divided into 2 clusters.
x &lt;- rbind(cbind(rnorm(10,0,0.5), rnorm(10,0,0.5)),
           cbind(rnorm(15,5,0.5), rnorm(15,5,0.5)))
clusplot(pam(x, 2))
## add noise, and try again :
x4 &lt;- cbind(x, rnorm(25), rnorm(25))
clusplot(pam(x4, 2))
</code></pre>

<hr>
<h2 id='clusplot.default'>Bivariate Cluster Plot (clusplot) Default Method</h2><span id='topic+clusplot.default'></span>

<h3>Description</h3>

<p>Creates a bivariate plot visualizing a partition (clustering) of the data. All
observation are represented by points in the plot, using principal
components or multidimensional scaling. Around each cluster an ellipse
is drawn.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
clusplot(x, clus, diss = FALSE,
          s.x.2d = mkCheckX(x, diss), stand = FALSE,
          lines = 2, shade = FALSE, color = FALSE,
          labels= 0, plotchar = TRUE,
          col.p = "dark green", col.txt = col.p,
          col.clus = if(color) c(2, 4, 6, 3) else 5, cex = 1, cex.txt = cex,
          span = TRUE,
          add = FALSE,
          xlim = NULL, ylim = NULL,
          main = paste("CLUSPLOT(", deparse1(substitute(x)),")"),
          sub = paste("These two components explain",
             round(100 * var.dec, digits = 2), "% of the point variability."),
          xlab = "Component 1", ylab = "Component 2",
          verbose = getOption("verbose"),
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusplot.default_+3A_x">x</code></td>
<td>
<p>matrix or data frame, or dissimilarity matrix, depending on
the value of the <code>diss</code> argument.
</p>
<p>In case of a matrix (alike), each row corresponds to an observation,
and each column corresponds to a variable.  All variables must be
numeric.  Missing values (<code><a href="base.html#topic+NA">NA</a></code>s) are allowed.  They are
replaced by the median of the corresponding variable.  When some
variables or some observations contain only missing values, the
function stops with a warning message.
</p>
<p>In case of a dissimilarity matrix, <code>x</code> is the output of
<code><a href="#topic+daisy">daisy</a></code> or <code><a href="stats.html#topic+dist">dist</a></code> or a symmetric matrix.  Also,
a vector of length <code class="reqn">n*(n-1)/2</code> is allowed (where <code class="reqn">n</code> is the
number of observations), and will be interpreted in the same way as
the output of the above-mentioned functions.  Missing values (NAs)
are not allowed.
</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_clus">clus</code></td>
<td>

<p>a vector of length n representing a clustering of <code>x</code>.  For
each observation the vector lists the number or name of the cluster
to which it has been assigned. <code>clus</code> is often the clustering
component of the output of <code><a href="#topic+pam">pam</a></code>, <code><a href="#topic+fanny">fanny</a></code> or
<code><a href="#topic+clara">clara</a></code>.</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_diss">diss</code></td>
<td>

<p>logical indicating if <code>x</code> will be considered as a dissimilarity
matrix or a matrix of observations by variables (see <code>x</code>
arugment above).</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_s.x.2d">s.x.2d</code></td>
<td>
<p>a <code><a href="base.html#topic+list">list</a></code> with components named <code>x</code> (a <code class="reqn">n
      \times 2</code> matrix; typically something like principal components of
original data), <code>labs</code> and <code>var.dec</code>.</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_stand">stand</code></td>
<td>

<p>logical flag: if true, then the representations of the n observations in the
2-dimensional plot are standardized.
</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_lines">lines</code></td>
<td>

<p>integer out of <code>0, 1, 2</code>, used to obtain an idea of the
distances between ellipses.  The distance between two ellipses E1
and E2 is measured along the line connecting the centers <code class="reqn">m1</code>
and <code class="reqn">m2</code> of the two ellipses.
</p>
<p>In case E1 and E2 overlap on the line through <code class="reqn">m1</code> and <code class="reqn">m2</code>,
no line is drawn.  Otherwise, the result depends on the value of
<code>lines</code>: If
</p>

<dl>
<dt>lines = 0,</dt><dd><p>no distance lines will appear on the plot;</p>
</dd>
<dt>lines = 1,</dt><dd><p>the line segment between <code class="reqn">m1</code> and <code class="reqn">m2</code> is drawn;</p>
</dd>
<dt>lines = 2,</dt><dd><p>a line segment between the boundaries of E1 and
E2 is drawn (along the line connecting <code class="reqn">m1</code> and <code class="reqn">m2</code>).</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="clusplot.default_+3A_shade">shade</code></td>
<td>

<p>logical flag: if TRUE, then the ellipses are shaded in relation to their
density. The density is the number of points in the cluster divided by the
area of the ellipse.
</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_color">color</code></td>
<td>

<p>logical flag: if TRUE, then the ellipses are colored with respect to their
density. With increasing density, the colors are light blue, light
green, red and purple.  To see these colors on the graphics device, an
appropriate color scheme should be selected (we recommend a white
background).</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_labels">labels</code></td>
<td>

<p>integer code, currently one of 0,1,2,3,4 and 5.  If
</p>

<dl>
<dt>labels= 0,</dt><dd><p>no labels are placed in the plot;</p>
</dd>
<dt>labels= 1,</dt><dd><p>points and ellipses can be identified in the plot (see
<code><a href="graphics.html#topic+identify">identify</a></code>);</p>
</dd>
<dt>labels= 2,</dt><dd><p>all points and ellipses are labelled in the plot;</p>
</dd>
<dt>labels= 3,</dt><dd><p>only the points are labelled in the plot;</p>
</dd>
<dt>labels= 4,</dt><dd><p>only the ellipses are labelled in the plot.</p>
</dd>
<dt>labels= 5,</dt><dd><p>the ellipses are labelled in the plot, and
points can be identified.</p>
</dd>
</dl>

<p>The levels of the vector <code>clus</code> are taken as labels for the
clusters.  The labels
of the points are the rownames of <code>x</code> if <code>x</code> is matrix like.
Otherwise (<code>diss = TRUE</code>), <code>x</code> is a vector, point labels
can be attached to <code>x</code> as a &quot;Labels&quot; attribute
(<code>attr(x,"Labels")</code>), as is done for the output of
<code><a href="#topic+daisy">daisy</a></code>.
</p>
<p>A possible <code><a href="base.html#topic+names">names</a></code> attribute of <code>clus</code> will not
be taken into account.
</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_plotchar">plotchar</code></td>
<td>

<p>logical flag: if TRUE, then the plotting symbols differ for points belonging
to different clusters.
</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_span">span</code></td>
<td>

<p>logical flag: if TRUE, then each cluster is represented by the ellipse with
smallest area containing all its points. (This is a special case of the
minimum volume ellipsoid.)<br />
If FALSE, the ellipse is based on the mean and covariance matrix of the
same points.  While this is faster to compute, it often yields a much
larger ellipse.
</p>
<p>There are also some special cases:  When a cluster consists of only
one point, a tiny circle is drawn around it.  When the points of a
cluster fall on a straight line, <code>span=FALSE</code> draws a narrow
ellipse around it and <code>span=TRUE</code> gives the exact line segment.
</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_add">add</code></td>
<td>
<p>logical indicating if ellipses (and labels if <code>labels</code>
is true) should be <em>added</em> to an already existing plot.  If
false, neither a <code><a href="graphics.html#topic+title">title</a></code> or sub title, see <code>sub</code>,
is written.</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_col.p">col.p</code></td>
<td>
<p>color code(s) used for the observation points.</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_col.txt">col.txt</code></td>
<td>
<p>color code(s) used for the labels (if <code>labels &gt;= 2</code>).</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_col.clus">col.clus</code></td>
<td>
<p>color code for the ellipses (and their labels);
only one if color is false (as per default).</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_cex">cex</code>, <code id="clusplot.default_+3A_cex.txt">cex.txt</code></td>
<td>
<p>character <b>ex</b>pansion (size), for the point
symbols and point labels, respectively.</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_xlim">xlim</code>, <code id="clusplot.default_+3A_ylim">ylim</code></td>
<td>
<p>numeric vectors of length 2, giving the x- and y-
ranges as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_main">main</code></td>
<td>
<p>main title for the plot; by default, one is constructed.</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_sub">sub</code></td>
<td>
<p>sub title for the plot; by default, one is constructed.</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_xlab">xlab</code>, <code id="clusplot.default_+3A_ylab">ylab</code></td>
<td>
<p>x- and y- axis labels for the plot, with defaults.</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_verbose">verbose</code></td>
<td>
<p>a logical indicating, if there should be extra
diagnostic output; mainly for &lsquo;debugging&rsquo;.</p>
</td></tr>
<tr><td><code id="clusplot.default_+3A_...">...</code></td>
<td>
<p>Further graphical parameters may also be supplied, see
<code><a href="graphics.html#topic+par">par</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>clusplot</code> uses function calls
<code><a href="stats.html#topic+princomp">princomp</a>(*, cor = (ncol(x) &gt; 2))</code> or
<code><a href="stats.html#topic+cmdscale">cmdscale</a>(*, add=TRUE)</code>, respectively, depending on
<code>diss</code> being false or true.  These functions are data reduction
techniques to represent the data in a bivariate plot.
</p>
<p>Ellipses are then drawn to indicate the clusters.  The further layout of the
plot is determined by the optional arguments.
</p>


<h3>Value</h3>

<p>An invisible list with components:
</p>
<table>
<tr><td><code>Distances</code></td>
<td>

<p>When <code>lines</code> is 1 or 2 we optain a k by k matrix (k is the number of
clusters).  The element in <code>[i,j]</code> is the distance between ellipse
i and ellipse j.<br />
If <code>lines = 0</code>, then the value of this component is <code>NA</code>.
</p>
</td></tr>
<tr><td><code>Shading</code></td>
<td>

<p>A vector of length k (where k is the number of clusters), containing the
amount of shading per cluster. Let y be a vector where element i is the
ratio between the number of points in cluster i and the area of ellipse i.
When the cluster i is a line segment, y[i] and the density of the cluster are
set to <code>NA</code>. Let z be the sum of all the elements of y without the NAs.
Then we put shading = y/z *37 + 3 .
</p>
</td></tr>
</table>


<h3>Side Effects</h3>

<p>a visual display of the clustering is plotted on the current graphics device.
</p>


<h3>Note</h3>

<p>When we have 4 or fewer clusters, then the <code>color=TRUE</code> gives
every cluster a different color.  When there are more than 4 clusters,
clusplot uses the function <code><a href="#topic+pam">pam</a></code> to cluster the
densities into 4 groups such that ellipses with nearly the same
density get the same color.  <code>col.clus</code> specifies the colors used.
</p>
<p>The <code>col.p</code> and <code>col.txt</code> arguments, added for <span class="rlang"><b>R</b></span>,
are recycled to have length the number of observations.
If <code>col.p</code> has more than one value, using <code>color = TRUE</code> can
be confusing because of a mix of point and ellipse colors.
</p>


<h3>References</h3>

<p>Pison, G., Struyf, A. and Rousseeuw, P.J. (1999)
Displaying a Clustering with CLUSPLOT,
<em>Computational Statistics and Data Analysis</em>, <b>30</b>, 381&ndash;392.<br />


</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990).
<em>Finding Groups in Data: An Introduction to Cluster Analysis.</em>
Wiley, New York.
</p>
<p>Struyf, A., Hubert, M. and Rousseeuw, P.J. (1997).
Integrating Robust Clustering Techniques in S-PLUS,
<em>Computational Statistics and Data Analysis</em>, <b>26</b>, 17-37.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+princomp">princomp</a></code>, <code><a href="stats.html#topic+cmdscale">cmdscale</a></code>, <code><a href="#topic+pam">pam</a></code>,
<code><a href="#topic+clara">clara</a></code>, <code><a href="#topic+daisy">daisy</a></code>, <code><a href="graphics.html#topic+par">par</a></code>,
<code><a href="graphics.html#topic+identify">identify</a></code>, <code><a href="MASS.html#topic+cov.mve">cov.mve</a></code>,
<code><a href="#topic+clusplot.partition">clusplot.partition</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## plotting votes.diss(dissimilarity) in a bivariate plot and
## partitioning into 2 clusters
data(votes.repub)
votes.diss &lt;- daisy(votes.repub)
pamv &lt;- pam(votes.diss, 2, diss = TRUE)
clusplot(pamv, shade = TRUE)
## is the same as
votes.clus &lt;- pamv$clustering
clusplot(votes.diss, votes.clus, diss = TRUE, shade = TRUE)
## Now look at components 3 and 2 instead of 1 and 2:
str(cMDS &lt;- cmdscale(votes.diss, k=3, add=TRUE))
clusplot(pamv, s.x.2d = list(x=cMDS$points[, c(3,2)],
                             labs=rownames(votes.repub), var.dec=NA),
         shade = TRUE, col.p = votes.clus,
         sub="", xlab = "Component 3", ylab = "Component 2")

clusplot(pamv, col.p = votes.clus, labels = 4)# color points and label ellipses
# "simple" cheap ellipses: larger than minimum volume:
# here they are *added* to the previous plot:
clusplot(pamv, span = FALSE, add = TRUE, col.clus = "midnightblue")

## Setting a small *label* size:
clusplot(votes.diss, votes.clus, diss = TRUE, labels = 3, cex.txt = 0.6)

if(dev.interactive()) { #  uses identify() *interactively* :
  clusplot(votes.diss, votes.clus, diss = TRUE, shade = TRUE, labels = 1)
  clusplot(votes.diss, votes.clus, diss = TRUE, labels = 5)# ident. only points
}

## plotting iris (data frame) in a 2-dimensional plot and partitioning
## into 3 clusters.
data(iris)
iris.x &lt;- iris[, 1:4]
cl3 &lt;- pam(iris.x, 3)$clustering
op &lt;- par(mfrow= c(2,2))
clusplot(iris.x, cl3, color = TRUE)
U &lt;- par("usr")
## zoom in :
rect(0,-1, 2,1, border = "orange", lwd=2)
clusplot(iris.x, cl3, color = TRUE, xlim = c(0,2), ylim = c(-1,1))
box(col="orange",lwd=2); mtext("sub region", font = 4, cex = 2)
##  or zoom out :
clusplot(iris.x, cl3, color = TRUE, xlim = c(-4,4), ylim = c(-4,4))
mtext("'super' region", font = 4, cex = 2)
rect(U[1],U[3], U[2],U[4], lwd=2, lty = 3)

# reset graphics
par(op)
</code></pre>

<hr>
<h2 id='cluster-internal'>Internal cluster functions</h2><span id='topic+meanabsdev'></span>

<h3>Description</h3>

<p>Internal cluster functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meanabsdev(y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster-internal_+3A_y">y</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are not to be called by the user.
</p>
<p>A more <span class="rlang"><b>R</b></span>-like version of <code>meanabsdev()</code> would default to
<code>na.rm=FALSE</code>, as e.g., <code><a href="base.html#topic+mean">mean</a>()</code> or <code><a href="stats.html#topic+median">median</a>()</code>:
</p>
<pre>
  meanabsdev &lt;- function(y, na.rm=FALSE)
       mean(abs(y - mean(y, na.rm=na.rm)), na.rm=na.rm)
</pre>

<hr>
<h2 id='coef.hclust'>Agglomerative / Divisive Coefficient for 'hclust' Objects</h2><span id='topic+coefHier'></span><span id='topic+coef.hclust'></span><span id='topic+coef.twins'></span>

<h3>Description</h3>

<p>Computes the &ldquo;agglomerative coefficient&rdquo; (aka &ldquo;divisive
coefficient&rdquo; for <code><a href="#topic+diana">diana</a></code>), measuring the
clustering structure of the dataset.
</p>
<p>For each observation i, denote by <code class="reqn">m(i)</code> its dissimilarity to the
first cluster it is merged with, divided by the dissimilarity of the
merger in the final step of the algorithm.  The agglomerative
coefficient is the average of all <code class="reqn">1 - m(i)</code>.  It can also be seen
as the average width (or the percentage filled) of the banner plot.
</p>
<p><code>coefHier()</code> directly interfaces to the underlying C code, and
&ldquo;proves&rdquo; that <em>only</em> <code>object$heights</code> is needed to
compute the coefficient.
</p>
<p>Because it grows with the number of observations, this measure should not
be used to compare datasets of very different sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coefHier(object)
coef.hclust(object, ...)
## S3 method for class 'hclust'
coef(object, ...)
## S3 method for class 'twins'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.hclust_+3A_object">object</code></td>
<td>
<p>an object of class <code>"hclust"</code> or <code>"twins"</code>,
i.e., typically the result of
<code><a href="stats.html#topic+hclust">hclust</a>(.)</code>,<code><a href="#topic+agnes">agnes</a>(.)</code>, or <code><a href="#topic+diana">diana</a>(.)</code>.
</p>
<p>Since <code>coef.hclust</code> only uses <code>object$heights</code>, and
<code>object$merge</code>, <code>object</code> can be any list-like object with
appropriate <code>merge</code> and <code>heights</code> components.
</p>
<p>For <code>coefHier</code>, even only <code>object$heights</code> is needed.
</p>
</td></tr>
<tr><td><code id="coef.hclust_+3A_...">...</code></td>
<td>
<p>currently unused potential further arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a number specifying the <em>agglomerative</em> (or <em>divisive</em> for
<code>diana</code> objects) coefficient as defined by Kaufman and Rousseeuw,
see <code><a href="#topic+agnes.object">agnes.object</a> $ ac</code> or <code><a href="#topic+diana.object">diana.object</a> $ dc</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agriculture)
aa &lt;- agnes(agriculture)
coef(aa) # really just extracts aa$ac
coef(as.hclust(aa))# recomputes
coefHier(aa)       # ditto

</code></pre>

<hr>
<h2 id='daisy'>Dissimilarity Matrix Calculation</h2><span id='topic+daisy'></span>

<h3>Description</h3>

<p>Compute all the pairwise dissimilarities (distances) between observations
in the data set.  The original variables may be of mixed types.  In
that case, or whenever <code>metric = "gower"</code> is set, a
generalization of Gower's formula is used, see &lsquo;Details&rsquo;
below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>daisy(x, metric = c("euclidean", "manhattan", "gower"),
      stand = FALSE, type = list(), weights = rep.int(1, p),
      warnBin = warnType, warnAsym = warnType, warnConst = warnType,
      warnType = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="daisy_+3A_x">x</code></td>
<td>

<p>numeric matrix or data frame, of dimension <code class="reqn">n\times p</code>,
say.  Dissimilarities will be computed
between the rows of <code>x</code>.  Columns of mode <code>numeric</code>
(i.e. all columns when <code>x</code> is a matrix) will be recognized as
interval scaled variables, columns of class <code>factor</code> will be
recognized as nominal variables, and columns of class <code>ordered</code>
will be recognized as ordinal variables.  Other variable types
should be specified with the <code>type</code> argument.  Missing values
(<code><a href="base.html#topic+NA">NA</a></code>s) are allowed.
</p>
</td></tr>
<tr><td><code id="daisy_+3A_metric">metric</code></td>
<td>

<p>character string specifying the metric to be used.
The currently available options are <code>"euclidean"</code> (the default),
<code>"manhattan"</code> and <code>"gower"</code>.<br />
Euclidean distances are root sum-of-squares of differences, and
manhattan distances are the sum of absolute differences.
</p>
<p>&ldquo;Gower's distance&rdquo; is chosen by metric <code>"gower"</code>
or automatically if some columns of <code>x</code> are not numeric. Also
known as Gower's coefficient (1971),
expressed as a dissimilarity, this implies that a particular
standardisation will be applied to each variable, and the
&ldquo;distance&rdquo; between two units is the sum of all the
variable-specific distances, see the details section.
</p>
</td></tr>
<tr><td><code id="daisy_+3A_stand">stand</code></td>
<td>
<p>logical flag: if TRUE, then the measurements in <code>x</code>
are standardized before calculating the
dissimilarities.  Measurements are standardized for each variable
(column), by subtracting the variable's mean value and dividing by
the variable's mean absolute deviation.
</p>
<p>If not all columns of <code>x</code> are numeric, <code>stand</code> will
be ignored and Gower's standardization (based on the
<code><a href="base.html#topic+range">range</a></code>) will be applied in any case, see argument
<code>metric</code>, above, and the details section.
</p>
</td></tr>
<tr><td><code id="daisy_+3A_type">type</code></td>
<td>
<p>list for specifying some (or all) of the types of the
variables (columns) in <code>x</code>.  The list may contain the following
components:
</p>

<dl>
<dt><code>"asymm"</code>  </dt><dd><p><b>A</b>symmetric binary variable, aka
<code>"A"</code> in result <code>Types</code>, see <code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code>.</p>
</dd>
<dt><code>"symm"</code>   </dt><dd><p><b>S</b>ymmetric  binary variable, aka <code>"S"</code>.</p>
</dd>
<dt><code>"factor"</code> </dt><dd><p><b>N</b>ominal &ndash; the default for <code><a href="base.html#topic+factor">factor</a></code>
variables, aka <code>"N"</code>.  When the factor has 2 levels, this is
equivalent to <code>type = "S"</code> for a (symmetric) binary variable.</p>
</dd>
<dt><code>"ordered"</code></dt><dd><p><b>O</b>rdinal &ndash; the default for <code><a href="base.html#topic+ordered">ordered</a></code>
(factor) variables, aka <code>"O"</code>, see <code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code>.</p>
</dd>
<dt><code>"logratio"</code></dt><dd><p>ratio scaled numeric variables that are to
be logarithmically transformed (<code><a href="base.html#topic+log10">log10</a></code>) and then
treated as numeric (<code>"I"</code>): must be <em>positive</em> numeric variable.</p>
</dd>
<dt><code>"ordratio"</code></dt><dd><p>&ldquo;ra<b>T</b>io&rdquo;-like
variable to be treated as <code><a href="base.html#topic+ordered">ordered</a></code> (using the factor
codes <code>unclass(<a href="base.html#topic+as.ordered">as.ordered</a>(x[,j]))</code>), aka <code>"T"</code>.</p>
</dd>
<dt><code>"numeric"</code>/<code>"integer"</code></dt><dd><p><b>I</b>nterval
scaled &ndash; the <b>default</b> for all numeric (incl <code>integer</code>)
columns of <code>x</code>, aka <code>"I"</code> in result <code>Types</code>, see
<code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code>.</p>
</dd>
</dl>

<p>Each component is a (character or numeric) vector, containing either
the names or the numbers of the corresponding columns of <code>x</code>.
</p>
<p>Variables not mentioned in <code>type</code> are interpreted as usual, see
argument <code>x</code>, and also &lsquo;default&rsquo; above.  Consequently,
the default <code>type = list()</code> may often be sufficient.
</p>
</td></tr>
<tr><td><code id="daisy_+3A_weights">weights</code></td>
<td>
<p>an optional numeric vector of length <code class="reqn">p</code>(=<code>ncol(x)</code>); to
be used in &ldquo;case 2&rdquo; (mixed variables, or <code>metric = "gower"</code>),
specifying a weight for each variable (<code>x[,k]</code>) instead of
<code class="reqn">1</code> in Gower's original formula.</p>
</td></tr>
<tr><td><code id="daisy_+3A_warnbin">warnBin</code>, <code id="daisy_+3A_warnasym">warnAsym</code>, <code id="daisy_+3A_warnconst">warnConst</code></td>
<td>
<p>logicals indicating if the
corresponding type checking warnings should be signalled (when found).</p>
</td></tr>
<tr><td><code id="daisy_+3A_warntype">warnType</code></td>
<td>
<p>logical indicating if <em>all</em> the type checking
warnings should be active or not.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The original version of <code>daisy</code> is fully described in chapter 1
of Kaufman and Rousseeuw (1990).
Compared to <code><a href="stats.html#topic+dist">dist</a></code> whose input must be numeric
variables, the main feature of <code>daisy</code> is its ability to handle
other variable types as well (e.g. nominal, ordinal, (a)symmetric
binary) even when different types occur in the same data set.
</p>
<p>The handling of nominal, ordinal, and (a)symmetric binary data is
achieved by using the general dissimilarity coefficient of Gower
(1971).  If <code>x</code> contains any columns of these
data-types, both arguments <code>metric</code> and <code>stand</code> will be
ignored and Gower's coefficient will be used as the metric.  This can
also be activated for purely numeric data by <code>metric = "gower"</code>.
With that, each variable (column) is first standardized by dividing
each entry by the range of the corresponding variable, after
subtracting the minimum value; consequently the rescaled variable has
range <code class="reqn">[0,1]</code>, exactly.

</p>
<p>Note that setting the type to <code>symm</code> (symmetric binary) gives the
same dissimilarities as using <em>nominal</em> (which is chosen for
non-ordered factors) only when no missing values are present, and more
efficiently.
</p>
<p>Note that <code>daisy</code> signals a warning when 2-valued numerical
variables do not have an explicit <code>type</code> specified, because the
reference authors recommend to consider using <code>"asymm"</code>; the
warning may be silenced by <code>warnBin = FALSE</code>.
</p>
<p>In the <code>daisy</code> algorithm, missing values in a row of x are not
included in the dissimilarities involving that row.  There are two
main cases,
</p>

<ol>
<li><p> If all variables are interval scaled (and <code>metric</code> is
<em>not</em> <code>"gower"</code>), the metric is &quot;euclidean&quot;, and
<code class="reqn">n_g</code> is the number of columns in which
neither row i and j have NAs, then the dissimilarity d(i,j) returned is
<code class="reqn">\sqrt{p/n_g}</code> (<code class="reqn">p=</code>ncol(x)) times the
Euclidean distance between the two vectors of length <code class="reqn">n_g</code>
shortened to exclude NAs.  The rule is similar for the &quot;manhattan&quot;
metric, except that the coefficient is <code class="reqn">p/n_g</code>.  If <code class="reqn">n_g = 0</code>,
the dissimilarity is NA.
</p>
</li>
<li><p> When some variables have a type other than interval scaled, or
if <code>metric = "gower"</code> is specified, the
dissimilarity between two rows is the weighted mean of the contributions of
each variable.  Specifically,
</p>
<p style="text-align: center;"><code class="reqn">d_{ij} = d(i,j) = \frac{\sum_{k=1}^p w_k \delta_{ij}^{(k)} d_{ij}^{(k)}}{
	  \sum_{k=1}^p w_k \delta_{ij}^{(k)}}.
      </code>
</p>

<p>In other words, <code class="reqn">d_{ij}</code> is a weighted mean of
<code class="reqn">d_{ij}^{(k)}</code> with weights <code class="reqn">w_k \delta_{ij}^{(k)}</code>,
where <code class="reqn">w_k</code><code>= weigths[k]</code>,
<code class="reqn">\delta_{ij}^{(k)}</code> is 0 or 1, and
<code class="reqn">d_{ij}^{(k)}</code>, the k-th variable contribution to the
total distance, is a distance between <code>x[i,k]</code> and <code>x[j,k]</code>,
see below.
</p>
<p>The 0-1 weight <code class="reqn">\delta_{ij}^{(k)}</code> becomes zero
when the variable <code>x[,k]</code> is missing in either or both rows
(i and j), or when the variable is asymmetric binary and both
values are zero.  In all other situations it is 1.
</p>
<p>The contribution <code class="reqn">d_{ij}^{(k)}</code> of a nominal or binary variable to the total
dissimilarity is 0 if both values are equal, 1 otherwise.
The contribution of other variables is the absolute difference of
both values, divided by the total range of that variable.  Note
that &ldquo;standard scoring&rdquo; is applied to ordinal variables,
i.e., they are replaced by their integer codes <code>1:K</code>.  Note
that this is not the same as using their ranks (since there
typically are ties).


</p>
<p>As the individual contributions <code class="reqn">d_{ij}^{(k)}</code> are in
<code class="reqn">[0,1]</code>, the dissimilarity <code class="reqn">d_{ij}</code> will remain in
this range.
If all weights <code class="reqn">w_k \delta_{ij}^{(k)}</code> are zero,
the dissimilarity is set to <code><a href="base.html#topic+NA">NA</a></code>.
</p>
</li></ol>



<h3>Value</h3>

<p>an object of class <code>"dissimilarity"</code> containing the
dissimilarities among the rows of <code>x</code>.  This is typically the
input for the functions <code>pam</code>, <code>fanny</code>, <code>agnes</code> or
<code>diana</code>.  For more details, see <code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code>.
</p>


<h3>Background</h3>

<p>Dissimilarities are used as inputs to cluster analysis and
multidimensional scaling.  The choice of metric may have a
large impact.
</p>


<h3>Author(s)</h3>

<p>Anja Struyf, Mia Hubert, and Peter and Rousseeuw, for the original
version.
<br />
Martin Maechler improved the <code><a href="base.html#topic+NA">NA</a></code> handling and
<code>type</code> specification checking, and extended functionality to
<code>metric = "gower"</code> and the optional <code>weights</code> argument.
</p>


<h3>References</h3>

<p>Gower, J. C. (1971)
A general coefficient of similarity and some of its properties,
<em>Biometrics</em> <b>27</b>, 857&ndash;874.
</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990)
<em>Finding Groups in Data: An Introduction to Cluster Analysis</em>.
Wiley, New York.
</p>
<p>Struyf, A., Hubert, M. and Rousseeuw, P.J. (1997)
Integrating Robust Clustering Techniques in S-PLUS,
<em>Computational Statistics and Data Analysis</em> <b>26</b>, 17&ndash;37.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>,
<code><a href="#topic+pam">pam</a></code>, <code><a href="#topic+fanny">fanny</a></code>, <code><a href="#topic+clara">clara</a></code>,
<code><a href="#topic+agnes">agnes</a></code>, <code><a href="#topic+diana">diana</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(agriculture)
## Example 1 in ref:
##  Dissimilarities using Euclidean metric and without standardization
d.agr &lt;- daisy(agriculture, metric = "euclidean", stand = FALSE)
d.agr
as.matrix(d.agr)[,"DK"] # via as.matrix.dist(.)
## compare with
as.matrix(daisy(agriculture, metric = "gower"))

## Example 2 in reference, extended  ---  different ways of "mixed" / "gower":

example(flower) # -&gt; data(flower) *and* provide 'flowerN'

summary(d0    &lt;- daisy(flower))  # -&gt; the first 3 {0,1} treated as *N*ominal
summary(dS123 &lt;- daisy(flower,  type = list(symm = 1:3))) # first 3 treated as *S*ymmetric
stopifnot(dS123 == d0) # i.e.,  *S*ymmetric &lt;==&gt; *N*ominal {for 2-level factor}
summary(dNS123&lt;- daisy(flowerN, type = list(symm = 1:3)))
stopifnot(dS123 == d0)
## by default, however ...
summary(dA123 &lt;- daisy(flowerN)) # .. all 3 logicals treated *A*symmetric binary (w/ warning)
summary(dA3  &lt;- daisy(flower, type = list(asymm = 3)))
summary(dA13 &lt;- daisy(flower, type = list(asymm = c(1, 3), ordratio = 7)))
## Mixing variable *names* and column numbers (failed in the past):
summary(dfl3 &lt;- daisy(flower, type = list(asymm = c("V1", "V3"), symm= 2,
                                          ordratio= 7, logratio= 8)))

## If we'd treat the first 3 as simple {0,1}
Nflow &lt;- flower
Nflow[,1:3] &lt;- lapply(flower[,1:3], \(f) as.integer(as.character(f)))
summary(dN &lt;- daisy(Nflow)) # w/ warning: treated binary .. 1:3 as interval
## Still, using Euclidean/Manhattan distance for {0-1} *is* identical to treating them as "N" :
stopifnot(dN == d0)
stopifnot(dN == daisy(Nflow, type = list(symm = 1:3))) # or as "S"
</code></pre>

<hr>
<h2 id='diana'>DIvisive ANAlysis Clustering</h2><span id='topic+diana'></span><span id='topic+diana.object'></span>

<h3>Description</h3>

<p>Computes a divisive hierarchical clustering of the dataset
returning an object of class <code>diana</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diana(x, diss = inherits(x, "dist"), metric = "euclidean", stand = FALSE,
      stop.at.k = FALSE,
      keep.diss = n &lt; 100, keep.data = !diss, trace.lev = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diana_+3A_x">x</code></td>
<td>

<p>data matrix or data frame, or dissimilarity matrix or object,
depending on the value of the <code>diss</code> argument.
</p>
<p>In case of a matrix or data frame, each row corresponds to an observation,
and each column corresponds to a variable.  All variables must be numeric.
Missing values (<code><a href="base.html#topic+NA">NA</a></code>s) <em>are</em> allowed.
</p>
<p>In case of a dissimilarity matrix, <code>x</code> is typically the output
of <code><a href="#topic+daisy">daisy</a></code> or <code><a href="stats.html#topic+dist">dist</a></code>.  Also a vector of
length n*(n-1)/2 is allowed (where n is the number of observations),
and will be interpreted in the same way as the output of the
above-mentioned functions. Missing values (NAs) are <em>not</em> allowed.
</p>
</td></tr>
<tr><td><code id="diana_+3A_diss">diss</code></td>
<td>

<p>logical flag: if TRUE (default for <code>dist</code> or
<code>dissimilarity</code> objects), then <code>x</code> will be considered as a
dissimilarity matrix.  If FALSE, then <code>x</code> will be considered as
a matrix of observations by variables.
</p>
</td></tr>
<tr><td><code id="diana_+3A_metric">metric</code></td>
<td>

<p>character string specifying the metric to be used for calculating
dissimilarities between observations.<br />
The currently available options are &quot;euclidean&quot; and
&quot;manhattan&quot;.  Euclidean distances are root sum-of-squares of
differences, and manhattan distances are the sum of absolute
differences.  If <code>x</code> is already a dissimilarity matrix, then
this argument will be ignored.
</p>
</td></tr>
<tr><td><code id="diana_+3A_stand">stand</code></td>
<td>
<p>logical; if true, the measurements in <code>x</code> are
standardized before calculating the dissimilarities.  Measurements
are standardized for each variable (column), by subtracting the
variable's mean value and dividing by the variable's mean absolute
deviation.  If <code>x</code> is already a dissimilarity matrix, then this
argument will be ignored.</p>
</td></tr>
<tr><td><code id="diana_+3A_stop.at.k">stop.at.k</code></td>
<td>
<p>logical or integer, <code>FALSE</code> by default.
Otherwise must be integer, say <code class="reqn">k</code>, in <code class="reqn">\{1,2,..,n\}</code>,
specifying that the <code>diana</code> algorithm should stop early.

Non-default NOT YET IMPLEMENTED.</p>
</td></tr>
<tr><td><code id="diana_+3A_keep.diss">keep.diss</code>, <code id="diana_+3A_keep.data">keep.data</code></td>
<td>
<p>logicals indicating if the dissimilarities
and/or input data <code>x</code> should be kept in the result.  Setting
these to <code>FALSE</code> can give much smaller results and hence even save
memory allocation <em>time</em>.</p>
</td></tr>
<tr><td><code id="diana_+3A_trace.lev">trace.lev</code></td>
<td>
<p>integer specifying a trace level for printing
diagnostics during the algorithm.  Default <code>0</code> does not print
anything; higher values print increasingly more.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>diana</code> is fully described in chapter 6 of Kaufman and Rousseeuw (1990).
It is probably unique in computing a divisive hierarchy, whereas most
other software for hierarchical clustering is agglomerative.
Moreover, <code>diana</code> provides (a) the divisive coefficient
(see <code>diana.object</code>) which measures the amount of clustering structure
found; and (b) the banner, a novel graphical display
(see <code>plot.diana</code>).
</p>
<p>The <code>diana</code>-algorithm constructs a hierarchy of clusterings,
starting with one large
cluster containing all n observations. Clusters are divided until each cluster
contains only a single observation.<br />
At each stage, the cluster with the largest diameter is selected.
(The diameter of a cluster is the largest dissimilarity between any
two of its observations.)<br />
To divide the selected cluster, the algorithm first looks for its most
disparate observation (i.e., which has the largest average dissimilarity to the
other observations of the selected cluster). This observation initiates the
&quot;splinter group&quot;. In subsequent steps, the algorithm reassigns observations
that are closer to the &quot;splinter group&quot; than to the &quot;old party&quot;. The result
is a division of the selected cluster into two new clusters.
</p>


<h3>Value</h3>

<p>an object of class <code>"diana"</code> representing the clustering;
this class has methods for the following generic functions:
<code>print</code>, <code>summary</code>, <code>plot</code>.
</p>
<p>Further, the class <code>"diana"</code> inherits from
<code>"twins"</code>.  Therefore, the generic function <code><a href="#topic+pltree">pltree</a></code> can be
used on a <code>diana</code> object, and <code><a href="stats.html#topic+as.hclust">as.hclust</a></code> and
<code><a href="stats.html#topic+as.dendrogram">as.dendrogram</a></code> methods are available.
</p>
<p>A legitimate <code>diana</code> object is a list with the following components:
</p>
<table>
<tr><td><code>order</code></td>
<td>

<p>a vector giving a permutation of the original observations to allow for
plotting, in the sense that the branches of a clustering tree will
not cross.
</p>
</td></tr>
<tr><td><code>order.lab</code></td>
<td>

<p>a vector similar to <code>order</code>, but containing observation labels
instead of observation numbers.  This component is only available if
the original observations were labelled.
</p>
</td></tr>
<tr><td><code>height</code></td>
<td>
<p>a vector with the diameters of the clusters prior to splitting.
</p>
</td></tr>
<tr><td><code>dc</code></td>
<td>

<p>the divisive coefficient, measuring the clustering structure of the
dataset.  For each observation i, denote by <code class="reqn">d(i)</code> the diameter of the
last cluster to which it belongs (before being split off as a single
observation), divided by the diameter of the whole dataset.  The
<code>dc</code> is the average of all <code class="reqn">1 - d(i)</code>.  It can also be seen
as the average width (or the percentage filled) of the banner plot.
Because <code>dc</code> grows with the number of observations, this
measure should not be used to compare datasets of very different
sizes.
</p>
</td></tr>
<tr><td><code>merge</code></td>
<td>

<p>an (n-1) by 2 matrix, where n is the number of
observations. Row i of <code>merge</code> describes the split at step n-i of
the clustering. If a number <code class="reqn">j</code> in row r is negative, then the single
observation <code class="reqn">|j|</code> is split off at stage n-r. If j is positive, then the
cluster that will be splitted at stage n-j (described by row j), is
split off at stage n-r.
</p>
</td></tr>
<tr><td><code>diss</code></td>
<td>

<p>an object of class <code>"dissimilarity"</code>, representing the total
dissimilarity matrix of the dataset.
</p>
</td></tr>
<tr><td><code>data</code></td>
<td>

<p>a matrix containing the original or standardized measurements, depending
on the <code>stand</code> option of the function <code>agnes</code>.  If a
dissimilarity matrix was given as input structure, then this component
is not available.
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code> also for background and references;
<code><a href="stats.html#topic+cutree">cutree</a></code> (and <code><a href="stats.html#topic+as.hclust">as.hclust</a></code>) for grouping
extraction; <code><a href="#topic+daisy">daisy</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>,
<code><a href="#topic+plot.diana">plot.diana</a></code>, <code><a href="#topic+twins.object">twins.object</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(votes.repub)
dv &lt;- diana(votes.repub, metric = "manhattan", stand = TRUE)
print(dv)
plot(dv)

## Cut into 2 groups:
dv2 &lt;- cutree(as.hclust(dv), k = 2)
table(dv2) # 8 and 42 group members
rownames(votes.repub)[dv2 == 1]

## For two groups, does the metric matter ?
dv0 &lt;- diana(votes.repub, stand = TRUE) # default: Euclidean
dv.2 &lt;- cutree(as.hclust(dv0), k = 2)
table(dv2 == dv.2)## identical group assignments

str(as.dendrogram(dv0)) # {via as.dendrogram.twins() method}

data(agriculture)
## Plot similar to Figure 8 in ref
## Not run: plot(diana(agriculture), ask = TRUE)

</code></pre>

<hr>
<h2 id='dissimilarity.object'>Dissimilarity Matrix Object</h2><span id='topic+dissimilarity.object'></span>

<h3>Description</h3>

<p>Objects of class <code>"dissimilarity"</code> representing the dissimilarity
matrix of a dataset.
</p>


<h3>Value</h3>

<p>The dissimilarity matrix is symmetric, and hence its lower triangle
(column wise) is represented as a vector to save storage space.
If the object, is called <code>do</code>, and <code>n</code> the number of
observations, i.e., <code>n &lt;- attr(do, "Size")</code>, then
for <code class="reqn">i &lt; j &lt;= n</code>, the dissimilarity between (row) i and j is
<code>do[n*(i-1) - i*(i-1)/2 + j-i]</code>.
The length of the vector is <code class="reqn">n*(n-1)/2</code>, i.e., of order <code class="reqn">n^2</code>.
</p>
<p><code>"dissimilarity"</code> objects also inherit from class
<code><a href="stats.html#topic+dist">dist</a></code> and can use <code>dist</code> methods, in
particular, <code><a href="base.html#topic+as.matrix">as.matrix</a></code>, such that <code class="reqn">d_{ij}</code>
from above is just <code>as.matrix(do)[i,j]</code>.
</p>
<p>The object has the following attributes:
</p>
<table>
<tr><td><code>Size</code></td>
<td>
<p>the number of observations in the dataset.</p>
</td></tr>
<tr><td><code>Metric</code></td>
<td>
<p>the metric used for calculating the
dissimilarities.  Possible values are &quot;euclidean&quot;, &quot;manhattan&quot;,
&quot;mixed&quot; (if variables of different types were present in the
dataset), and &quot;unspecified&quot;.</p>
</td></tr>
<tr><td><code>Labels</code></td>
<td>
<p>optionally, contains the labels, if any, of the
observations of the dataset.</p>
</td></tr>
<tr><td><code>NA.message</code></td>
<td>
<p>optionally, if a dissimilarity could not be
computed, because of too many missing values for some observations
of the dataset.</p>
</td></tr>
<tr><td><code>Types</code></td>
<td>
<p>when a mixed metric was used, the types for each
variable as one-letter codes, see also <code>type</code> in <code><a href="#topic+daisy">daisy</a>()</code>:

</p>

<dl>
<dt><code>A</code>: </dt><dd><p>Asymmetric binary</p>
</dd>
<dt><code>S</code>: </dt><dd><p>Symmetric  binary</p>
</dd>
<dt><code>N</code>: </dt><dd><p>Nominal (factor)</p>
</dd>
<dt><code>O</code>: </dt><dd><p>Ordinal (ordered factor)</p>
</dd>
<dt><code>I</code>: </dt><dd><p>Interval scaled, possibly after log transform
<code>"logratio"</code> (numeric)</p>
</dd>
<dt><code>T</code>: </dt><dd><p>ra<b>T</b>io treated as <code><a href="base.html#topic+ordered">ordered</a></code></p>
</dd>
</dl>
</td></tr>
</table>


<h3>GENERATION</h3>

<p><code><a href="#topic+daisy">daisy</a></code> returns this class of objects.
Also the functions <code>pam</code>, <code>clara</code>, <code>fanny</code>,
<code>agnes</code>, and <code>diana</code> return a <code>dissimilarity</code> object,
as one component of their return objects.
</p>


<h3>METHODS</h3>

<p>The <code>"dissimilarity"</code> class has methods for the following generic
functions: <code>print</code>, <code>summary</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+daisy">daisy</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>,
<code><a href="#topic+pam">pam</a></code>, <code><a href="#topic+clara">clara</a></code>, <code><a href="#topic+fanny">fanny</a></code>,
<code><a href="#topic+agnes">agnes</a></code>, <code><a href="#topic+diana">diana</a></code>.
</p>

<hr>
<h2 id='ellipsoidhull'>Compute the Ellipsoid Hull or Spanning Ellipsoid of a Point Set</h2><span id='topic+ellipsoidhull'></span><span id='topic+print.ellipsoid'></span>

<h3>Description</h3>

<p>Compute the &ldquo;ellipsoid hull&rdquo; or &ldquo;spanning ellipsoid&rdquo;, i.e. the
ellipsoid of minimal volume (&lsquo;area&rsquo; in 2D) such that all given points
lie just inside or on the boundary of the ellipsoid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ellipsoidhull(x, tol=0.01, maxit=5000,
              ret.wt = FALSE, ret.sqdist = FALSE, ret.pr = FALSE)
## S3 method for class 'ellipsoid'
print(x, digits = max(1, getOption("digits") - 2), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ellipsoidhull_+3A_x">x</code></td>
<td>
<p>the <code class="reqn">n</code> <code class="reqn">p</code>-dimensional points  asnumeric
<code class="reqn">n\times p</code> matrix.</p>
</td></tr>
<tr><td><code id="ellipsoidhull_+3A_tol">tol</code></td>
<td>
<p>convergence tolerance for Titterington's algorithm.
Setting this to much smaller values may drastically increase the number of
iterations needed, and you may want to increas <code>maxit</code> as well.</p>
</td></tr>
<tr><td><code id="ellipsoidhull_+3A_maxit">maxit</code></td>
<td>
<p>integer giving the maximal number of iteration steps for
the algorithm.</p>
</td></tr>
<tr><td><code id="ellipsoidhull_+3A_ret.wt">ret.wt</code>, <code id="ellipsoidhull_+3A_ret.sqdist">ret.sqdist</code>, <code id="ellipsoidhull_+3A_ret.pr">ret.pr</code></td>
<td>
<p>logicals indicating if additional
information should be returned, <code>ret.wt</code> specifying the
<em>weights</em>, <code>ret.sqdist</code> the <em><b>sq</b>uared
<b>dist</b>ances</em> and <code>ret.pr</code> the final <b>pr</b>obabilities
in the algorithms.</p>
</td></tr>
<tr><td><code id="ellipsoidhull_+3A_digits">digits</code>, <code id="ellipsoidhull_+3A_...">...</code></td>
<td>
<p>the usual arguments to <code><a href="base.html#topic+print">print</a></code> methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The &ldquo;spanning ellipsoid&rdquo; algorithm is said to stem from
Titterington(1976), in Pison et al (1999) who use it for
<code><a href="#topic+clusplot.default">clusplot.default</a></code>.<br />
The problem can be seen as a special case of the &ldquo;Min.Vol.&rdquo;
ellipsoid of which a more more flexible and general implementation is
<code><a href="MASS.html#topic+cov.mve">cov.mve</a></code> in the <code>MASS</code> package.
</p>


<h3>Value</h3>

<p>an object of class <code>"ellipsoid"</code>, basically a <code><a href="base.html#topic+list">list</a></code>
with several components, comprising at least
</p>
<table>
<tr><td><code>cov</code></td>
<td>
<p><code class="reqn">p\times p</code> <em>covariance</em> matrix description
the ellipsoid.</p>
</td></tr>
<tr><td><code>loc</code></td>
<td>
<p><code class="reqn">p</code>-dimensional location of the ellipsoid center.</p>
</td></tr>
<tr><td><code>d2</code></td>
<td>
<p>average squared radius.  Further, <code class="reqn">d2 = t^2</code>, where
<code class="reqn">t</code> is &ldquo;the value of a t-statistic on the ellipse
boundary&rdquo; (from <code><a href="ellipse.html#topic+ellipse">ellipse</a></code> in the
<a href="https://CRAN.R-project.org/package=ellipse"><span class="pkg">ellipse</span></a> package), and hence, more usefully,
<code>d2 = qchisq(alpha, df = p)</code>, where <code>alpha</code> is the
confidence level for p-variate normally distributed data with
location and covariance <code>loc</code> and <code>cov</code> to lie inside the
ellipsoid.</p>
</td></tr>
<tr><td><code>wt</code></td>
<td>
<p>the vector of weights iff <code>ret.wt</code> was true.</p>
</td></tr>
<tr><td><code>sqdist</code></td>
<td>
<p>the vector of squared distances iff <code>ret.sqdist</code> was true.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>the vector of algorithm probabilities iff <code>ret.pr</code> was true.</p>
</td></tr>
<tr><td><code>it</code></td>
<td>
<p>number of iterations used.</p>
</td></tr>
<tr><td><code>tol</code>, <code>maxit</code></td>
<td>
<p>just the input argument, see above.</p>
</td></tr>
<tr><td><code>eps</code></td>
<td>
<p>the achieved tolerance which is the maximal squared radius
minus <code class="reqn">p</code>.</p>
</td></tr>
<tr><td><code>ierr</code></td>
<td>
<p>error code as from the algorithm; <code>0</code> means <em>ok</em>.</p>
</td></tr>
<tr><td><code>conv</code></td>
<td>
<p>logical indicating if the converged.  This is defined as
<code>it &lt; maxit &amp;&amp; ierr == 0</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Martin Maechler did the present class implementation; Rousseeuw
et al did the underlying original code.</p>


<h3>References</h3>

<p>Pison, G., Struyf, A. and Rousseeuw, P.J. (1999)
Displaying a Clustering with CLUSPLOT,
<em>Computational Statistics and Data Analysis</em>, <b>30</b>, 381&ndash;392.<br />


</p>
<p>D.M. Titterington (1976)
Algorithms for computing D-optimal design on finite design spaces.  In
<em>Proc.\ of the 1976 Conf.\ on Information Science and Systems</em>,
213&ndash;216; John Hopkins University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.ellipsoid">predict.ellipsoid</a></code> which is also the
<code><a href="stats.html#topic+predict">predict</a></code> method for <code>ellipsoid</code> objects.
<code><a href="#topic+volume.ellipsoid">volume.ellipsoid</a></code> for an example of &lsquo;manual&rsquo;
<code>ellipsoid</code> object construction;<br />
further <code><a href="ellipse.html#topic+ellipse">ellipse</a></code> from package <a href="https://CRAN.R-project.org/package=ellipse"><span class="pkg">ellipse</span></a>
and <code><a href="sfsmisc.html#topic+ellipsePoints">ellipsePoints</a></code> from package <a href="https://CRAN.R-project.org/package=sfsmisc"><span class="pkg">sfsmisc</span></a>.
</p>
<p><code><a href="grDevices.html#topic+chull">chull</a></code> for the convex hull,
<code><a href="#topic+clusplot">clusplot</a></code> which makes use of this; <code><a href="MASS.html#topic+cov.mve">cov.mve</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(100)
xy &lt;- unname(cbind(x, rnorm(100) + 2*x + 10))
exy. &lt;- ellipsoidhull(xy)
exy. # &gt;&gt; calling print.ellipsoid()

plot(xy, main = "ellipsoidhull(&lt;Gauss data&gt;) -- 'spanning points'")
lines(predict(exy.), col="blue")
points(rbind(exy.$loc), col = "red", cex = 3, pch = 13)

exy &lt;- ellipsoidhull(xy, tol = 1e-7, ret.wt = TRUE, ret.sq = TRUE)
str(exy) # had small 'tol', hence many iterations
(ii &lt;- which(zapsmall(exy $ wt) &gt; 1e-6))
## --&gt; only about 4 to 6  "spanning ellipsoid" points
round(exy$wt[ii],3); sum(exy$wt[ii]) # weights summing to 1
points(xy[ii,], pch = 21, cex = 2,
       col="blue", bg = adjustcolor("blue",0.25))
</code></pre>

<hr>
<h2 id='fanny'>Fuzzy Analysis Clustering</h2><span id='topic+fanny'></span>

<h3>Description</h3>

<p>Computes a fuzzy clustering of the data into <code>k</code> clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fanny(x, k, diss = inherits(x, "dist"), memb.exp = 2,
      metric = c("euclidean", "manhattan", "SqEuclidean"),
      stand = FALSE, iniMem.p = NULL, cluster.only = FALSE,
      keep.diss = !diss &amp;&amp; !cluster.only &amp;&amp; n &lt; 100,
      keep.data = !diss &amp;&amp; !cluster.only,
      maxit = 500, tol = 1e-15, trace.lev = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fanny_+3A_x">x</code></td>
<td>

<p>data matrix or data frame, or dissimilarity matrix, depending on the
value of the <code>diss</code> argument.
</p>
<p>In case of a matrix or data frame, each row corresponds to an observation,
and each column corresponds to a variable. All variables must be numeric.
Missing values (NAs) are allowed.
</p>
<p>In case of a dissimilarity matrix, <code>x</code> is typically the output
of <code><a href="#topic+daisy">daisy</a></code> or <code><a href="stats.html#topic+dist">dist</a></code>.  Also a vector of
length n*(n-1)/2 is allowed (where n is the number of observations),
and will be interpreted in the same way as the output of the
above-mentioned functions.  Missing values (NAs) are not allowed.
</p>
</td></tr>
<tr><td><code id="fanny_+3A_k">k</code></td>
<td>
<p>integer giving the desired number of clusters.  It is
required that <code class="reqn">0 &lt; k &lt; n/2</code> where <code class="reqn">n</code> is the number of
observations.</p>
</td></tr>
<tr><td><code id="fanny_+3A_diss">diss</code></td>
<td>

<p>logical flag: if TRUE (default for <code>dist</code> or
<code>dissimilarity</code> objects), then <code>x</code> is assumed to be a
dissimilarity matrix.  If FALSE, then <code>x</code> is treated as
a matrix of observations by variables.
</p>
</td></tr>
<tr><td><code id="fanny_+3A_memb.exp">memb.exp</code></td>
<td>
<p>number <code class="reqn">r</code> strictly larger than 1 specifying the
<em>membership exponent</em> used in the fit criterion; see the
&lsquo;Details&rsquo; below. Default: <code>2</code> which used to be hardwired
inside FANNY.</p>
</td></tr>
<tr><td><code id="fanny_+3A_metric">metric</code></td>
<td>
<p>character string specifying the metric to be used for
calculating dissimilarities between observations.  Options are
<code>"euclidean"</code> (default), <code>"manhattan"</code>, and
<code>"SqEuclidean"</code>.  Euclidean distances are root sum-of-squares
of differences, and manhattan distances are the sum of absolute
differences, and <code>"SqEuclidean"</code>, the <em>squared</em> euclidean
distances are sum-of-squares of differences.  Using this last option is
equivalent (but somewhat slower) to computing so called &ldquo;fuzzy C-means&rdquo;.
<br />
If <code>x</code> is already a dissimilarity matrix, then this argument will
be ignored.
</p>
</td></tr>
<tr><td><code id="fanny_+3A_stand">stand</code></td>
<td>
<p>logical; if true, the measurements in <code>x</code> are
standardized before calculating the dissimilarities.  Measurements
are standardized for each variable (column), by subtracting the
variable's mean value and dividing by the variable's mean absolute
deviation.  If <code>x</code> is already a dissimilarity matrix, then this
argument will be ignored.</p>
</td></tr>
<tr><td><code id="fanny_+3A_inimem.p">iniMem.p</code></td>
<td>
<p>numeric <code class="reqn">n \times k</code> matrix or <code>NULL</code>
(by default); can be used to specify a starting <code>membership</code>
matrix, i.e., a matrix of non-negative numbers, each row summing to
one.
</p>
</td></tr> 
<tr><td><code id="fanny_+3A_cluster.only">cluster.only</code></td>
<td>
<p>logical; if true, no silhouette information will be
computed and returned, see details.</p>
</td></tr>
<tr><td><code id="fanny_+3A_keep.diss">keep.diss</code>, <code id="fanny_+3A_keep.data">keep.data</code></td>
<td>
<p>logicals indicating if the dissimilarities
and/or input data <code>x</code> should be kept in the result.  Setting
these to <code>FALSE</code> can give smaller results and hence also save
memory allocation <em>time</em>.</p>
</td></tr>
<tr><td><code id="fanny_+3A_maxit">maxit</code>, <code id="fanny_+3A_tol">tol</code></td>
<td>
<p>maximal number of iterations and default tolerance
for convergence (relative convergence of the fit criterion) for the
FANNY algorithm.  The defaults <code>maxit = 500</code> and <code>tol =
      1e-15</code> used to be hardwired inside the algorithm.</p>
</td></tr>
<tr><td><code id="fanny_+3A_trace.lev">trace.lev</code></td>
<td>
<p>integer specifying a trace level for printing
diagnostics during the C-internal algorithm.
Default <code>0</code> does not print anything; higher values print
increasingly more.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In a fuzzy clustering, each observation is &ldquo;spread out&rdquo; over
the various clusters.  Denote by <code class="reqn">u_{iv}</code> the membership
of observation <code class="reqn">i</code> to cluster <code class="reqn">v</code>.
</p>
<p>The memberships are nonnegative, and for a fixed observation i they sum to 1.
The particular method <code>fanny</code> stems from chapter 4 of
Kaufman and Rousseeuw (1990) (see the references in
<code><a href="#topic+daisy">daisy</a></code>) and has been extended by Martin Maechler to allow
user specified <code>memb.exp</code>, <code>iniMem.p</code>, <code>maxit</code>,
<code>tol</code>, etc.
</p>
<p>Fanny aims to minimize the objective function
</p>
<p style="text-align: center;"><code class="reqn">\sum_{v=1}^k
    \frac{\sum_{i=1}^n\sum_{j=1}^n u_{iv}^r u_{jv}^r d(i,j)}{
      2 \sum_{j=1}^n u_{jv}^r}</code>
</p>

<p>where <code class="reqn">n</code> is the number of observations, <code class="reqn">k</code> is the number of
clusters, <code class="reqn">r</code> is the membership exponent <code>memb.exp</code> and
<code class="reqn">d(i,j)</code> is the dissimilarity between observations <code class="reqn">i</code> and <code class="reqn">j</code>.
<br /> Note that <code class="reqn">r \to 1</code> gives increasingly crisper
clusterings whereas <code class="reqn">r \to \infty</code> leads to complete
fuzzyness.  K&amp;R(1990), p.191 note that values too close to 1 can lead
to slow convergence.  Further note that even the default, <code class="reqn">r = 2</code>
can lead to complete fuzzyness, i.e., memberships <code class="reqn">u_{iv} \equiv
    1/k</code>.  In that case a warning is signalled and the
user is advised to chose a smaller <code>memb.exp</code> (<code class="reqn">=r</code>).
</p>
<p>Compared to other fuzzy clustering methods, <code>fanny</code> has the following
features: (a) it also accepts a dissimilarity matrix; (b) it is
more robust to the <code>spherical cluster</code> assumption; (c) it provides
a novel graphical display, the silhouette plot (see
<code><a href="#topic+plot.partition">plot.partition</a></code>).
</p>


<h3>Value</h3>

<p>an object of class <code>"fanny"</code> representing the clustering.
See <code><a href="#topic+fanny.object">fanny.object</a></code> for details.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code> for background and references;
<code><a href="#topic+fanny.object">fanny.object</a></code>, <code><a href="#topic+partition.object">partition.object</a></code>,
<code><a href="#topic+plot.partition">plot.partition</a></code>, <code><a href="#topic+daisy">daisy</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 10+15 objects in two clusters, plus 3 objects lying
## between those clusters.
x &lt;- rbind(cbind(rnorm(10, 0, 0.5), rnorm(10, 0, 0.5)),
           cbind(rnorm(15, 5, 0.5), rnorm(15, 5, 0.5)),
           cbind(rnorm( 3,3.2,0.5), rnorm( 3,3.2,0.5)))
fannyx &lt;- fanny(x, 2)
## Note that observations 26:28 are "fuzzy" (closer to # 2):
fannyx
summary(fannyx)
plot(fannyx)

(fan.x.15 &lt;- fanny(x, 2, memb.exp = 1.5)) # 'crispier' for obs. 26:28
(fanny(x, 2, memb.exp = 3))               # more fuzzy in general

data(ruspini)
f4 &lt;- fanny(ruspini, 4)
stopifnot(rle(f4$clustering)$lengths == c(20,23,17,15))
plot(f4, which = 1)
## Plot similar to Figure 6 in Stryuf et al (1996)
plot(fanny(ruspini, 5))
</code></pre>

<hr>
<h2 id='fanny.object'>Fuzzy Analysis (FANNY) Object</h2><span id='topic+fanny.object'></span>

<h3>Description</h3>

<p>The objects of class <code>"fanny"</code> represent a fuzzy clustering of a
dataset.
</p>


<h3>Value</h3>

<p>A legitimate <code>fanny</code> object is a list with the following components:
</p>
<table>
<tr><td><code>membership</code></td>
<td>

<p>matrix containing the memberships for each pair consisting of an
observation and a cluster.
</p>
</td></tr>
<tr><td><code>memb.exp</code></td>
<td>
<p>the membership exponent used in the fitting criterion.</p>
</td></tr>
<tr><td><code>coeff</code></td>
<td>

<p>Dunn's partition coefficient <code class="reqn">F(k)</code> of the clustering, where
<code class="reqn">k</code> is the number of clusters. <code class="reqn">F(k)</code> is the sum of all
<em>squared</em> membership coefficients, divided by the number of
observations.  Its value is between <code class="reqn">1/k</code> and 1.
</p>
<p>The normalized form of the coefficient is also given.  It is defined
as <code class="reqn">(F(k) - 1/k) / (1 - 1/k)</code>, and ranges between 0 and 1.
A low value of Dunn's coefficient indicates a very fuzzy clustering,
whereas a value close to 1 indicates a near-crisp clustering.
</p>
</td></tr>
<tr><td><code>clustering</code></td>
<td>

<p>the clustering vector of the nearest crisp clustering, see
<code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>k.crisp</code></td>
<td>
<p>integer (<code class="reqn">\le k</code>) giving the number of <em>crisp</em>
clusters; can be less than <code class="reqn">k</code>, where it's recommended to
decrease <code>memb.exp</code>.</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>

<p>named vector containing the minimal value of the objective function
reached by the FANNY algorithm and the relative convergence
tolerance <code>tol</code> used.
</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>

<p>named vector with <code>iterations</code>, the number of iterations needed
and <code>converged</code> indicating if the algorithm converged (in
<code>maxit</code> iterations within convergence tolerance <code>tol</code>).
</p>
</td></tr>
<tr><td><code>diss</code></td>
<td>

<p>an object of class <code>"dissimilarity"</code>, see
<code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>generating call, see <code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>silinfo</code></td>
<td>

<p>list with silhouette information of the nearest crisp clustering, see
<code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>matrix, possibibly standardized, or NULL, see
<code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
</table>


<h3>GENERATION</h3>

<p>These objects are returned from <code><a href="#topic+fanny">fanny</a></code>.
</p>


<h3>METHODS</h3>

<p>The <code>"fanny"</code> class has methods for the following generic functions:
<code>print</code>, <code>summary</code>.
</p>


<h3>INHERITANCE</h3>

<p>The class <code>"fanny"</code> inherits from <code>"partition"</code>.
Therefore, the generic functions <code>plot</code> and <code>clusplot</code> can
be used on a <code>fanny</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fanny">fanny</a></code>, <code><a href="#topic+print.fanny">print.fanny</a></code>,
<code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code>,
<code><a href="#topic+partition.object">partition.object</a></code>, <code><a href="#topic+plot.partition">plot.partition</a></code>.
</p>

<hr>
<h2 id='flower'>Flower Characteristics</h2><span id='topic+flower'></span>

<h3>Description</h3>

<p>8 characteristics for 18 popular flowers.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(flower)</code></pre>


<h3>Format</h3>

<p>A data frame with 18 observations on 8 variables:
</p>

<table>
<tr>
 <td style="text-align: right;">
    [ , "V1"] </td><td style="text-align: left;"> factor  </td><td style="text-align: left;"> winters </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , "V2"] </td><td style="text-align: left;"> factor  </td><td style="text-align: left;"> shadow </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , "V3"] </td><td style="text-align: left;"> factor  </td><td style="text-align: left;"> tubers </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , "V4"] </td><td style="text-align: left;"> factor  </td><td style="text-align: left;"> color </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , "V5"] </td><td style="text-align: left;"> ordered </td><td style="text-align: left;"> soil </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , "V6"] </td><td style="text-align: left;"> ordered </td><td style="text-align: left;"> preference </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , "V7"] </td><td style="text-align: left;"> numeric </td><td style="text-align: left;"> height </td>
</tr>
<tr>
 <td style="text-align: right;">
    [ , "V8"] </td><td style="text-align: left;"> numeric </td><td style="text-align: left;"> distance
  </td>
</tr>

</table>


<dl>
<dt>V1</dt><dd><p>winters, is binary and indicates whether the plant may be left
in the garden when it freezes.</p>
</dd>
<dt>V2</dt><dd><p>shadow, is binary and shows whether the plant needs to stand
in the shadow.</p>
</dd>
<dt>V3</dt><dd><p>tubers, is asymmetric binary and distinguishes between plants
with tubers and plants that grow in any other way.</p>
</dd>
<dt>V4</dt><dd><p>color, is nominal and specifies the flower's color (1 = white,
2 = yellow, 3 = pink, 4 = red, 5 = blue).</p>
</dd>
<dt>V5</dt><dd><p>soil, is ordinal and indicates whether the plant grows in dry
(1), normal (2), or wet (3) soil.</p>
</dd>
<dt>V6</dt><dd><p>preference, is ordinal and gives someone's preference ranking
going from 1 to 18.</p>
</dd>
<dt>V7</dt><dd><p>height, is interval scaled, the plant's height in centimeters.</p>
</dd>
<dt>V8</dt><dd><p>distance, is interval scaled, the distance in centimeters that
should be left between the plants.</p>
</dd>
</dl>



<h3>References</h3>

<p>Struyf, Hubert and Rousseeuw (1996), see <code><a href="#topic+agnes">agnes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(flower)
str(flower) # factors, ordered, numeric

## "Nicer" version (less numeric more self explainable) of 'flower':
flowerN &lt;- flower
colnames(flowerN) &lt;- c("winters", "shadow", "tubers", "color",
                       "soil", "preference", "height", "distance")
for(j in 1:3) flowerN[,j] &lt;- (flowerN[,j] == "1")
levels(flowerN$color) &lt;- c("1" = "white", "2" = "yellow", "3" = "pink",
                           "4" = "red", "5" = "blue")[levels(flowerN$color)]
levels(flowerN$soil)  &lt;- c("1" = "dry", "2" = "normal", "3" = "wet")[levels(flowerN$soil)]
flowerN

## ==&gt; example(daisy)  on how it is used
</code></pre>

<hr>
<h2 id='lower.to.upper.tri.inds'>Permute Indices for Triangular Matrices</h2><span id='topic+lower.to.upper.tri.inds'></span><span id='topic+upper.to.lower.tri.inds'></span>

<h3>Description</h3>

<p>Compute index vectors for extracting or reordering of lower or upper
triangular matrices that are stored as contiguous vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lower.to.upper.tri.inds(n)
upper.to.lower.tri.inds(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lower.to.upper.tri.inds_+3A_n">n</code></td>
<td>
<p>integer larger than 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>integer vector containing a permutation of <code>1:N</code> where
<code class="reqn">N = n(n-1)/2</code>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+upper.tri">upper.tri</a></code>, <code><a href="base.html#topic+lower.tri">lower.tri</a></code> with a related
purpose.</p>


<h3>Examples</h3>

<pre><code class='language-R'>m5 &lt;- matrix(NA,5,5)
m &lt;- m5; m[lower.tri(m)] &lt;- upper.to.lower.tri.inds(5); m
m &lt;- m5; m[upper.tri(m)] &lt;- lower.to.upper.tri.inds(5); m

stopifnot(lower.to.upper.tri.inds(2) == 1,
          lower.to.upper.tri.inds(3) == 1:3,
          upper.to.lower.tri.inds(3) == 1:3,
     sort(upper.to.lower.tri.inds(5)) == 1:10,
     sort(lower.to.upper.tri.inds(6)) == 1:15)
</code></pre>

<hr>
<h2 id='medoids'>Compute <code>pam</code>-consistent Medoids from Clustering</h2><span id='topic+medoids'></span>

<h3>Description</h3>

<p>Given a data matrix or dissimilarity <code>x</code> for say <code class="reqn">n</code>
observational units and a clustering,
compute the <code><a href="#topic+pam">pam</a>()</code>-consistent medoids.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medoids(x, clustering, diss = inherits(x, "dist"), USE.NAMES = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="medoids_+3A_x">x</code></td>
<td>
<p>Either a data matrix or data frame, or dissimilarity matrix or
object, see also <code><a href="#topic+pam">pam</a></code>.</p>
</td></tr>
<tr><td><code id="medoids_+3A_clustering">clustering</code></td>
<td>
<p>an integer vector of length <code class="reqn">n</code>, the number of
observations, giving for each observation the number ('id') of the
cluster to which it belongs.  In other words, <code>clustering</code> has
values from <code>1:k</code> where <code>k</code> is the number of clusters, see
also <code><a href="#topic+partition.object">partition.object</a></code> and <code><a href="stats.html#topic+cutree">cutree</a>()</code>, for
examples where such clustering vectors are computed.</p>
</td></tr>
<tr><td><code id="medoids_+3A_diss">diss</code></td>
<td>
<p>see also <code><a href="#topic+pam">pam</a></code>.</p>
</td></tr>
<tr><td><code id="medoids_+3A_use.names">USE.NAMES</code></td>
<td>
<p>a logical, typical false, passed to the
<code><a href="base.html#topic+vapply">vapply</a>()</code> call computing the medoids.</p>
</td></tr>
<tr><td><code id="medoids_+3A_...">...</code></td>
<td>
<p>optional further argument passed to <code><a href="#topic+pam">pam</a>(xj, k=1, ...)</code>,
notably <code>metric</code>, or <code>variant="f_5"</code> to use a faster algorithm, or
<code>trace.lev = k</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of length
</p>


<h3>Author(s)</h3>

<p>Martin Maechler, after being asked how <code><a href="#topic+pam">pam</a>()</code> could be used
instead of <code><a href="stats.html#topic+kmeans">kmeans</a>()</code>, starting from a previous clustering.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pam">pam</a></code>, <code><a href="stats.html#topic+kmeans">kmeans</a></code>.
Further, <code><a href="stats.html#topic+cutree">cutree</a>()</code> and <code><a href="#topic+agnes">agnes</a></code> (or <code><a href="stats.html#topic+hclust">hclust</a></code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## From example(agnes):
data(votes.repub)
agn1 &lt;- agnes(votes.repub, metric = "manhattan", stand = TRUE)
agn2 &lt;- agnes(daisy(votes.repub), diss = TRUE, method = "complete")
agnS &lt;- agnes(votes.repub, method = "flexible", par.meth = 0.625)

for(k in 2:11) {
  print(table(cl.k &lt;- cutree(agnS, k=k)))
  stopifnot(length(cl.k) == nrow(votes.repub), 1 &lt;= cl.k, cl.k &lt;= k, table(cl.k) &gt;= 2)
  m.k &lt;- medoids(votes.repub, cl.k)
  cat("k =", k,"; sort(medoids) = "); dput(sort(m.k), control={})
}

</code></pre>

<hr>
<h2 id='mona'>MONothetic Analysis Clustering of Binary Variables</h2><span id='topic+mona'></span>

<h3>Description</h3>

<p>Returns a list representing a divisive hierarchical clustering of
a dataset with binary variables only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mona(x, trace.lev = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mona_+3A_x">x</code></td>
<td>

<p>data matrix or data frame in which each row corresponds to an
observation, and each column corresponds to a variable.  All
variables must be binary.  A limited number of missing values (<code>NA</code>s)
is allowed.  Every observation must have at least one value different
from <code><a href="base.html#topic+NA">NA</a></code>.  No variable should have half of its values
missing.  There must be at least one variable which has no missing
values.  A variable with all its non-missing values identical is
not allowed.</p>
</td></tr>
<tr><td><code id="mona_+3A_trace.lev">trace.lev</code></td>
<td>
<p>logical or integer indicating if (and how much) the
algorithm should produce progress output.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mona</code> is fully described in chapter 7 of Kaufman and Rousseeuw (1990).
It is &ldquo;monothetic&rdquo; in the sense that each division is based on a
single (well-chosen) variable, whereas most other hierarchical methods
(including <code>agnes</code> and <code>diana</code>) are &ldquo;polythetic&rdquo;, i.e. they use
all variables together.
</p>
<p>The <code>mona</code>-algorithm constructs a hierarchy of clusterings,
starting with one large cluster.  Clusters are divided until all
observations in the same cluster have identical values for all variables.
<br />
At each stage, all clusters are divided according to the values of one
variable. A cluster is divided into one cluster with all observations having
value 1 for that variable, and another cluster with all observations having
value 0 for that variable.
</p>
<p>The variable used for splitting a cluster is the variable with the maximal
total association to the other variables, according to the observations in the
cluster to be splitted. The association between variables f and g
is given by a(f,g)*d(f,g) - b(f,g)*c(f,g), where a(f,g), b(f,g), c(f,g),
and d(f,g) are the numbers in the contingency table of f and g.
[That is, a(f,g) (resp. d(f,g)) is the number of observations for which f and g
both have value 0 (resp. value 1); b(f,g) (resp. c(f,g)) is the number of
observations for which f has value 0 (resp. 1) and g has value 1 (resp. 0).]
The total association of a variable f is the sum of its associations to all
variables.
</p>


<h3>Value</h3>

<p>an object of class <code>"mona"</code> representing the clustering.
See <code><a href="#topic+mona.object">mona.object</a></code> for details.
</p>


<h3>Missing Values (<code><a href="base.html#topic+NA">NA</a></code>s)</h3>

<p>The mona-algorithm requires &ldquo;pure&rdquo; 0-1 values.  However,
<code>mona(x)</code> allows <code>x</code> to contain (not too many)
<code><a href="base.html#topic+NA">NA</a></code>s.  In a preliminary step, these are &ldquo;imputed&rdquo;,
i.e., all missing values are filled in.  To do this, the same measure
of association between variables is used as in the algorithm.  When variable
f has missing values, the variable g with the largest absolute association
to f is looked up. When the association between f and g is positive,
any missing value of f is replaced by the value of g for the same
observation. If the association between f and g is negative, then any missing
value of f is replaced by the value of 1-g for the same
observation.
</p>


<h3>Note</h3>

<p>In <span class="pkg">cluster</span> versions before 2.0.6, the algorithm entered an
infinite loop in the boundary case of one variable, i.e.,
<code>ncol(x) == 1</code>, which currently signals an error (because the
algorithm now in C, haes not correctly taken account of this special case).

</p>


<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code> for background and references;
<code><a href="#topic+mona.object">mona.object</a></code>, <code><a href="#topic+plot.mona">plot.mona</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(animals)
ma &lt;- mona(animals)
ma
## Plot similar to Figure 10 in Struyf et al (1996)
plot(ma)

## One place to see if/how error messages are *translated* (to 'de' / 'pl'):
ani.NA   &lt;- animals; ani.NA[4,] &lt;- NA
aniNA    &lt;- within(animals, { end[2:9] &lt;- NA })
aniN2    &lt;- animals; aniN2[cbind(1:6, c(3, 1, 4:6, 2))] &lt;- NA
ani.non2 &lt;- within(animals, end[7] &lt;- 3 )
ani.idNA &lt;- within(animals, end[!is.na(end)] &lt;- 1 )
try( mona(ani.NA)   ) ## error: .. object with all values missing
try( mona(aniNA)    ) ## error: .. more than half missing values
try( mona(aniN2)    ) ## error: all have at least one missing
try( mona(ani.non2) ) ## error: all must be binary
try( mona(ani.idNA) ) ## error:  ditto
</code></pre>

<hr>
<h2 id='mona.object'>Monothetic Analysis (MONA) Object</h2><span id='topic+mona.object'></span>

<h3>Description</h3>

<p>The objects of class <code>"mona"</code> represent the divisive
hierarchical clustering of a dataset with only binary variables
(measurements).   This class of objects is returned from
<code><a href="#topic+mona">mona</a></code>.
</p>


<h3>Value</h3>

<p>A legitimate <code>mona</code> object is a list with the following components:
</p>
<table>
<tr><td><code>data</code></td>
<td>

<p>matrix with the same dimensions as the original data matrix,
but with factors coded as 0 and 1, and all missing values replaced.
</p>
</td></tr>
<tr><td><code>order</code></td>
<td>

<p>a vector giving a permutation of the original observations to allow
for plotting, in the sense that the branches of a clustering tree
will not cross.
</p>
</td></tr>
<tr><td><code>order.lab</code></td>
<td>

<p>a vector similar to <code>order</code>, but containing observation labels
instead of observation numbers. This component is only available if
the original observations were labelled.
</p>
</td></tr>
<tr><td><code>variable</code></td>
<td>

<p>vector of length n-1 where n is the number of observations,
specifying the variables used to separate the observations of <code>order</code>.
</p>
</td></tr>
<tr><td><code>step</code></td>
<td>

<p>vector of length n-1 where n is the number of observations,
specifying the separation steps at which the observations of
<code>order</code> are separated.
</p>
</td></tr>
</table>


<h3>METHODS</h3>

<p>The <code>"mona"</code> class has methods for the following generic functions:
<code>print</code>, <code>summary</code>, <code>plot</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mona">mona</a></code> for examples etc, <code><a href="#topic+plot.mona">plot.mona</a></code>.
</p>

<hr>
<h2 id='pam'>Partitioning Around Medoids</h2><span id='topic+pam'></span>

<h3>Description</h3>

<p>Partitioning (clustering) of the data into <code>k</code> clusters &ldquo;around
medoids&rdquo;, a more robust version of K-means.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pam(x, k, diss = inherits(x, "dist"),
    metric = c("euclidean", "manhattan"), 
    medoids = if(is.numeric(nstart)) "random",
    nstart = if(variant == "faster") 1 else NA,
    stand = FALSE, cluster.only = FALSE,
    do.swap = TRUE,
    keep.diss = !diss &amp;&amp; !cluster.only &amp;&amp; n &lt; 100,
    keep.data = !diss &amp;&amp; !cluster.only,
    variant = c("original", "o_1", "o_2", "f_3", "f_4", "f_5", "faster"),
    pamonce = FALSE, trace.lev = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pam_+3A_x">x</code></td>
<td>

<p>data matrix or data frame, or dissimilarity matrix or object,
depending on the value of the <code>diss</code> argument.
</p>
<p>In case of a matrix or data frame, each row corresponds to an
observation, and each column corresponds to a variable.  All
variables must be numeric (or logical).  Missing values (<code><a href="base.html#topic+NA">NA</a></code>s)
<em>are</em> allowed&mdash;as long as every pair of observations has at
least one case not missing.
</p>
<p>In case of a dissimilarity matrix, <code>x</code> is typically the output
of <code><a href="#topic+daisy">daisy</a></code> or <code><a href="stats.html#topic+dist">dist</a></code>.  Also a vector of
length n*(n-1)/2 is allowed (where n is the number of observations),
and will be interpreted in the same way as the output of the
above-mentioned functions. Missing values (<code><a href="base.html#topic+NA">NA</a></code>s) are
<em>not</em> allowed.
</p>
</td></tr>
<tr><td><code id="pam_+3A_k">k</code></td>
<td>
<p>positive integer specifying the number of clusters, less than
the number of observations.</p>
</td></tr>
<tr><td><code id="pam_+3A_diss">diss</code></td>
<td>

<p>logical flag: if TRUE (default for <code>dist</code> or
<code>dissimilarity</code> objects), then <code>x</code> will be considered as a
dissimilarity matrix.  If FALSE, then <code>x</code> will be considered as
a matrix of observations by variables.
</p>
</td></tr>
<tr><td><code id="pam_+3A_metric">metric</code></td>
<td>

<p>character string specifying the metric to be used for calculating
dissimilarities between observations.<br />
The currently available options are &quot;euclidean&quot; and
&quot;manhattan&quot;.  Euclidean distances are root sum-of-squares of
differences, and manhattan distances are the sum of absolute
differences.  If <code>x</code> is already a dissimilarity matrix, then
this argument will be ignored.
</p>
</td></tr>
<tr><td><code id="pam_+3A_medoids">medoids</code></td>
<td>
<p>NULL (default) or length-<code>k</code> vector of integer
indices (in <code>1:n</code>) specifying initial medoids instead of using
the &lsquo;<em>build</em>&rsquo; algorithm.</p>
</td></tr>
<tr><td><code id="pam_+3A_nstart">nstart</code></td>
<td>
<p>used only when <code>medoids = "random"</code>: specifies the
<em>number</em> of random &ldquo;starts&rdquo;;  this argument corresponds to
the one of <code><a href="stats.html#topic+kmeans">kmeans</a>()</code> (from <span class="rlang"><b>R</b></span>'s package <span class="pkg">stats</span>).</p>
</td></tr>
<tr><td><code id="pam_+3A_stand">stand</code></td>
<td>
<p>logical; if true, the measurements in <code>x</code> are
standardized before calculating the dissimilarities.  Measurements
are standardized for each variable (column), by subtracting the
variable's mean value and dividing by the variable's mean absolute
deviation.  If <code>x</code> is already a dissimilarity matrix, then this
argument will be ignored.</p>
</td></tr>
<tr><td><code id="pam_+3A_cluster.only">cluster.only</code></td>
<td>
<p>logical; if true, only the clustering will be
computed and returned, see details.</p>
</td></tr>
<tr><td><code id="pam_+3A_do.swap">do.swap</code></td>
<td>
<p>logical indicating if the <b>swap</b> phase should
happen. The default, <code>TRUE</code>, correspond to the
original algorithm.  On the other hand, the <b>swap</b> phase is
much more computer intensive than the <b>build</b> one for large
<code class="reqn">n</code>, so can be skipped by <code>do.swap = FALSE</code>.</p>
</td></tr>
<tr><td><code id="pam_+3A_keep.diss">keep.diss</code>, <code id="pam_+3A_keep.data">keep.data</code></td>
<td>
<p>logicals indicating if the dissimilarities
and/or input data <code>x</code> should be kept in the result.  Setting
these to <code>FALSE</code> can give much smaller results and hence even save
memory allocation <em>time</em>.</p>
</td></tr>
<tr><td><code id="pam_+3A_pamonce">pamonce</code></td>
<td>
<p>logical or integer in <code>0:6</code> specifying algorithmic
short cuts as proposed by Reynolds et al. (2006), and
Schubert and Rousseeuw (2019, 2021) see below.</p>
</td></tr>
<tr><td><code id="pam_+3A_variant">variant</code></td>
<td>
<p>a <code><a href="base.html#topic+character">character</a></code> string denoting the variant of
PAM algorithm to use; a more self-documenting version of <code>pamonce</code>
which should be used preferably; note that <code>"faster"</code> not only
uses <code>pamonce = 6</code> but also <code>nstart = 1</code> and hence
<code>medoids = "random"</code> by default.</p>
</td></tr>
<tr><td><code id="pam_+3A_trace.lev">trace.lev</code></td>
<td>
<p>integer specifying a trace level for printing
diagnostics during the build and swap phase of the algorithm.
Default <code>0</code> does not print anything; higher values print
increasingly more.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic <code>pam</code> algorithm is fully described in chapter 2 of
Kaufman and Rousseeuw(1990).  Compared to the k-means approach in <code>kmeans</code>, the
function <code>pam</code> has the following features: (a) it also accepts a
dissimilarity matrix; (b) it is more robust because it minimizes a sum
of dissimilarities instead of a sum of squared euclidean distances;
(c) it provides a novel graphical display, the silhouette plot (see
<code>plot.partition</code>) (d) it allows to select the number of clusters
using <code>mean(<a href="#topic+silhouette">silhouette</a>(pr)[, "sil_width"])</code> on the result
<code>pr &lt;- pam(..)</code>, or directly its component
<code>pr$silinfo$avg.width</code>, see also <code><a href="#topic+pam.object">pam.object</a></code>.
</p>
<p>When <code>cluster.only</code> is true, the result is simply a (possibly
named) integer vector specifying the clustering, i.e.,<br />
<code>pam(x,k, cluster.only=TRUE)</code> is the same as <br />
<code>pam(x,k)$clustering</code> but computed more efficiently.
</p>
<p>The <code>pam</code>-algorithm is based on the search for <code>k</code>
representative objects or medoids among the observations of the
dataset.  These observations should represent the structure of the
data.  After finding a set of <code>k</code> medoids, <code>k</code> clusters are
constructed by assigning each observation to the nearest medoid.  The
goal is to find <code>k</code> representative objects which minimize the sum
of the dissimilarities of the observations to their closest
representative object.
<br />
By default, when <code>medoids</code> are not specified, the algorithm first
looks for a good initial set of medoids (this is called the
<b>build</b> phase).  Then it finds a local minimum for the
objective function, that is, a solution such that there is no single
switch of an observation with a medoid (i.e. a &lsquo;swap&rsquo;) that will
decrease the objective (this is called the <b>swap</b> phase).
</p>
<p>When the <code>medoids</code> are specified (or randomly generated), their order does <em>not</em>
matter; in general, the algorithms have been designed to not depend on
the order of the observations.
</p>
<p>The <code>pamonce</code> option, new in cluster 1.14.2 (Jan. 2012), has been
proposed by Matthias Studer, University of Geneva, based on the
findings by Reynolds et al. (2006) and was extended by Erich Schubert,
TU Dortmund, with the FastPAM optimizations.
</p>
<p>The default <code>FALSE</code> (or integer <code>0</code>) corresponds to the
original &ldquo;swap&rdquo; algorithm, whereas <code>pamonce = 1</code> (or
<code>TRUE</code>), corresponds to the first proposal .... 
and <code>pamonce = 2</code> additionally implements the second proposal as
well. 
</p>
<p>The key ideas of &lsquo;FastPAM&rsquo; (Schubert and Rousseeuw, 2019) are implemented
except for the linear approximate build as follows:
</p>

<dl>
<dt><code>pamonce = 3</code>:</dt><dd>
<p>reduces the runtime by a factor of O(k) by exploiting
that points cannot be closest to all current medoids at the same time.</p>
</dd>
<dt><code>pamonce = 4</code>:</dt><dd><p> additionally allows executing multiple swaps
per iteration, usually reducing the number of iterations.</p>
</dd>
<dt><code>pamonce = 5</code>:</dt><dd><p> adds minor optimizations copied from the
<code>pamonce = 2</code> approach, and is expected to be the fastest of the
&lsquo;FastPam&rsquo; variants included.</p>
</dd>
</dl>

<p>&lsquo;FasterPAM&rsquo; (Schubert and Rousseeuw, 2021) is implemented via
</p>

<dl>
<dt><code>pamonce = 6</code>:</dt><dd><p>execute each swap which improves results
immediately, and hence typically multiple swaps per iteration;
this swapping algorithm runs in <code class="reqn">O(n^2)</code> rather than
<code class="reqn">O(n(n-k)k)</code> time which is much faster for all but small <code class="reqn">k</code>.</p>
</dd>
</dl>

<p>In addition, &lsquo;FasterPAM&rsquo; uses <em>random</em> initialization of the
medoids (instead of the &lsquo;<em>build</em>&rsquo; phase) to avoid the
<code class="reqn">O(n^2 k)</code> initialization cost of the build algorithm.  In particular
for large k, this yields a much faster algorithm, while preserving a
similar result quality.
</p>
<p>One may decide to use <em>repeated</em> random initialization by setting
<code>nstart &gt; 1</code>.
</p>


<h3>Value</h3>

<p>an object of class <code>"pam"</code> representing the clustering.  See
<code>?<a href="#topic+pam.object">pam.object</a></code> for details.
</p>


<h3>Note</h3>

<p>For large datasets, <code>pam</code> may need too much memory or too much
computation time since both are <code class="reqn">O(n^2)</code>.  Then,
<code><a href="#topic+clara">clara</a>()</code> is preferable, see its documentation.
</p>
<p>There is hard limit currently, <code class="reqn">n \le 65536</code>, at
<code class="reqn">2^{16}</code> because for larger <code class="reqn">n</code>, <code class="reqn">n(n-1)/2</code> is larger than
the maximal integer (<code><a href="base.html#topic+.Machine">.Machine</a>$integer.max</code> = <code class="reqn">2^{31} - 1</code>).
</p>


<h3>Author(s)</h3>

<p>Kaufman and Rousseeuw's orginal Fortran code was translated to C
and augmented in several ways, e.g. to allow <code>cluster.only=TRUE</code>
or <code>do.swap=FALSE</code>, by Martin Maechler.
<br />
Matthias Studer, Univ.Geneva provided the <code>pamonce</code> (<code>1</code> and <code>2</code>)
implementation.
<br />
Erich Schubert, TU Dortmund contributed the <code>pamonce</code> (<code>3</code> to <code>6</code>)
implementation.
</p>


<h3>References</h3>

<p>Reynolds, A., Richards, G., de la Iglesia, B. and Rayward-Smith, V. (1992)
Clustering rules: A comparison of partitioning and hierarchical
clustering algorithms;
<em>Journal of Mathematical Modelling and Algorithms</em> <b>5</b>,
475&ndash;504. <a href="https://doi.org/10.1007/s10852-005-9022-1">doi:10.1007/s10852-005-9022-1</a>.
</p>
<p>Erich Schubert and Peter J. Rousseeuw (2019)
Faster k-Medoids Clustering:
Improving the PAM, CLARA, and CLARANS Algorithms;
SISAP 2020, 171&ndash;187. <a href="https://doi.org/10.1007/978-3-030-32047-8_16">doi:10.1007/978-3-030-32047-8_16</a>.
</p>
<p>Erich Schubert and Peter J. Rousseeuw (2021)
Fast and Eager k-Medoids Clustering:
O(k) Runtime Improvement of the PAM, CLARA, and CLARANS Algorithms;
Preprint, to appear in Information Systems (<a href="https://arxiv.org/abs/2008.05171">https://arxiv.org/abs/2008.05171</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code> for background and references;
<code><a href="#topic+pam.object">pam.object</a></code>, <code><a href="#topic+clara">clara</a></code>, <code><a href="#topic+daisy">daisy</a></code>,
<code><a href="#topic+partition.object">partition.object</a></code>, <code><a href="#topic+plot.partition">plot.partition</a></code>,
<code><a href="stats.html#topic+dist">dist</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 25 objects, divided into 2 clusters.
x &lt;- rbind(cbind(rnorm(10,0,0.5), rnorm(10,0,0.5)),
           cbind(rnorm(15,5,0.5), rnorm(15,5,0.5)))
pamx &lt;- pam(x, 2)
pamx # Medoids: '7' and '25' ...
summary(pamx)
plot(pamx)
## use obs. 1 &amp; 16 as starting medoids -- same result (typically)
(p2m &lt;- pam(x, 2, medoids = c(1,16)))
## no _build_ *and* no _swap_ phase: just cluster all obs. around (1, 16):
p2.s &lt;- pam(x, 2, medoids = c(1,16), do.swap = FALSE)
p2.s

p3m &lt;- pam(x, 3, trace = 2)
## rather stupid initial medoids:
(p3m. &lt;- pam(x, 3, medoids = 3:1, trace = 1))


pam(daisy(x, metric = "manhattan"), 2, diss = TRUE)

data(ruspini)
## Plot similar to Figure 4 in Stryuf et al (1996)
## Not run: plot(pam(ruspini, 4), ask = TRUE)

</code></pre>

<hr>
<h2 id='pam.object'>Partitioning Around Medoids (PAM) Object</h2><span id='topic+pam.object'></span>

<h3>Description</h3>

<p>The objects of class <code>"pam"</code> represent a partitioning of a
dataset into clusters.
</p>


<h3>Value</h3>

<p>A legitimate <code>pam</code> object is a <code><a href="base.html#topic+list">list</a></code> with the following components:
</p>
<table>
<tr><td><code>medoids</code></td>
<td>

<p>the medoids or representative objects of the
clusters.  If a dissimilarity matrix was given as input to
<code>pam</code>, then a vector of numbers or labels of observations is
given, else <code>medoids</code> is a <code><a href="base.html#topic+matrix">matrix</a></code> with in each
row the coordinates of one medoid.</p>
</td></tr>
<tr><td><code>id.med</code></td>
<td>
<p>integer vector of <em>indices</em> giving the medoid
observation numbers.</p>
</td></tr>
<tr><td><code>clustering</code></td>
<td>
<p>the clustering vector, see <code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>the objective function after the first and second
step of the <code>pam</code> algorithm.</p>
</td></tr>
<tr><td><code>isolation</code></td>
<td>

<p>vector with length equal to the number of clusters, specifying which
clusters are isolated clusters (L- or L*-clusters) and which clusters are
not isolated.<br />
A cluster is an L*-cluster iff its diameter is smaller than its
separation.  A cluster is an L-cluster iff for each observation i
the maximal dissimilarity between i and any other observation of the
cluster is smaller than the minimal dissimilarity between i and any
observation of another cluster.  Clearly each L*-cluster is also an
L-cluster.
</p>
</td></tr>
<tr><td><code>clusinfo</code></td>
<td>

<p>matrix, each row gives numerical information for one cluster. These
are the cardinality of the cluster (number of observations), the
maximal and average dissimilarity between the observations in the
cluster and the cluster's medoid, 
the diameter of the cluster
(maximal dissimilarity between two observations of the cluster), and
the separation of the cluster (minimal dissimilarity between an
observation of the cluster and an observation of another cluster).
</p>
</td></tr>
<tr><td><code>silinfo</code></td>
<td>
<p>list with silhouette width information, see
<code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>diss</code></td>
<td>
<p>dissimilarity (maybe NULL), see <code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>generating call, see <code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>(possibibly standardized) see <code><a href="#topic+partition.object">partition.object</a></code>.</p>
</td></tr>
</table>


<h3>GENERATION</h3>

<p>These objects are returned from <code><a href="#topic+pam">pam</a></code>.</p>


<h3>METHODS</h3>

<p>The <code>"pam"</code> class has methods for the following generic functions:
<code>print</code>, <code>summary</code>.
</p>


<h3>INHERITANCE</h3>

<p>The class <code>"pam"</code> inherits from <code>"partition"</code>.
Therefore, the generic functions <code>plot</code> and <code>clusplot</code> can
be used on a <code>pam</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pam">pam</a></code>, <code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code>,
<code><a href="#topic+partition.object">partition.object</a></code>, <code><a href="#topic+plot.partition">plot.partition</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Use the silhouette widths for assessing the best number of clusters,
## following a one-dimensional example from Christian Hennig :
##
x &lt;- c(rnorm(50), rnorm(50,mean=5), rnorm(30,mean=15))
asw &lt;- numeric(20)
## Note that "k=1" won't work!
for (k in 2:20)
  asw[k] &lt;- pam(x, k) $ silinfo $ avg.width
k.best &lt;- which.max(asw)
cat("silhouette-optimal number of clusters:", k.best, "\n")

plot(1:20, asw, type= "h", main = "pam() clustering assessment",
     xlab= "k  (# clusters)", ylab = "average silhouette width")
axis(1, k.best, paste("best",k.best,sep="\n"), col = "red", col.axis = "red")
</code></pre>

<hr>
<h2 id='partition.object'>Partitioning Object</h2><span id='topic+partition'></span><span id='topic+partition.object'></span>

<h3>Description</h3>

<p>The objects of class <code>"partition"</code> represent a partitioning of a
dataset into clusters.
</p>


<h3>Value</h3>

<p>a <code>"partition"</code> object is a list with the following
(and typically more) components:
</p>
<table>
<tr><td><code>clustering</code></td>
<td>

<p>the clustering vector.  An integer vector of length <code class="reqn">n</code>, the number of
observations, giving for each observation the number ('id') of the
cluster to which it belongs.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched <code><a href="base.html#topic+call">call</a></code> generating the object.</p>
</td></tr>
<tr><td><code>silinfo</code></td>
<td>

<p>a list with all <em>silhouette</em> information, only available when
the number of clusters is non-trivial, i.e., <code class="reqn">1 &lt; k &lt; n</code> and
then has the following components, see <code><a href="#topic+silhouette">silhouette</a></code>
</p>

<dl>
<dt>widths</dt><dd><p>an (n x 3) matrix, as returned by
<code><a href="#topic+silhouette">silhouette</a>()</code>, with for each observation i the
cluster to which i belongs, as well as the neighbor cluster of i
(the cluster, not containing i, for which the average
dissimilarity between its observations and i is minimal), and
the silhouette width <code class="reqn">s(i)</code> of the observation.
</p>
</dd>
<dt>clus.avg.widths</dt><dd><p>the average silhouette width per cluster.</p>
</dd>
<dt>avg.width</dt><dd><p>the average silhouette width for the dataset, i.e.,
simply the average of <code class="reqn">s(i)</code> over all observations <code class="reqn">i</code>.</p>
</dd>
</dl>

<p>This information is also needed to construct a <em>silhouette plot</em> of
the clustering, see <code><a href="#topic+plot.partition">plot.partition</a></code>.
</p>
<p>Note that <code>avg.width</code> can be maximized over different
clusterings (e.g. with varying number of clusters) to choose an
<em>optimal</em> clustering.
</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>value of criterion maximized during the
partitioning algorithm, may more than one entry for different stages.</p>
</td></tr>
<tr><td><code>diss</code></td>
<td>

<p>an object of class <code>"dissimilarity"</code>, representing the total
dissimilarity matrix of the dataset (or relevant subset, e.g. for
<code>clara</code>).
</p>
</td></tr>
<tr><td><code>data</code></td>
<td>

<p>a matrix containing the original or standardized data.  This might
be missing to save memory or when a dissimilarity matrix was given
as input structure to the clustering method.
</p>
</td></tr>
</table>


<h3>GENERATION</h3>

<p>These objects are returned from <code>pam</code>, <code>clara</code> or <code>fanny</code>.
</p>


<h3>METHODS</h3>

<p>The <code>"partition"</code> class has a method for the following generic functions:
<code>plot</code>, <code>clusplot</code>.
</p>


<h3>INHERITANCE</h3>

<p>The following classes inherit from class <code>"partition"</code> :
<code>"pam"</code>, <code>"clara"</code> and <code>"fanny"</code>.
</p>
<p>See <code><a href="#topic+pam.object">pam.object</a></code>, <code><a href="#topic+clara.object">clara.object</a></code> and
<code><a href="#topic+fanny.object">fanny.object</a></code> for details.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pam">pam</a></code>, <code><a href="#topic+clara">clara</a></code>, <code><a href="#topic+fanny">fanny</a></code>.
</p>

<hr>
<h2 id='plantTraits'>Plant Species Traits Data</h2><span id='topic+plantTraits'></span>

<h3>Description</h3>

<p>This dataset constitutes a description of 136 plant species
according to biological attributes (morphological or reproductive)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(plantTraits)
</code></pre>


<h3>Format</h3>

<p>A data frame with 136 observations on the following 31 variables.
</p>

<dl>
<dt><code>pdias</code></dt><dd><p>Diaspore mass (mg)</p>
</dd>
<dt><code>longindex</code></dt><dd><p>Seed bank longevity</p>
</dd>
<dt><code>durflow</code></dt><dd><p>Flowering duration</p>
</dd>
<dt><code>height</code></dt><dd><p>Plant height, an ordered factor with levels
<code>1</code> &lt; <code>2</code> &lt; ... &lt; <code>8</code>.</p>
</dd>
<dt><code>begflow</code></dt><dd><p>Time of first flowering, an ordered factor with levels <code>1</code> &lt; <code>2</code> &lt; <code>3</code> &lt; <code>4</code> &lt; <code>5</code> &lt; <code>6</code> &lt; <code>7</code> &lt; <code>8</code> &lt; <code>9</code></p>
</dd>
<dt><code>mycor</code></dt><dd><p>Mycorrhizas, an ordered factor with levels <code>0</code>never &lt; <code>1</code> sometimes&lt; <code>2</code>always</p>
</dd>
<dt><code>vegaer</code></dt><dd><p>aerial vegetative propagation, an ordered
factor with levels <code>0</code>never &lt; <code>1</code> present but limited&lt; <code>2</code>important.</p>
</dd>
<dt><code>vegsout</code></dt><dd><p>underground vegetative propagation, an ordered
factor with 3 levels identical to <code>vegaer</code> above.</p>
</dd>
<dt><code>autopoll</code></dt><dd><p>selfing pollination, an ordered factor with
levels <code>0</code>never &lt; <code>1</code>rare &lt; <code>2</code> often&lt; the rule<code>3</code></p>
</dd>
<dt><code>insects</code></dt><dd><p>insect pollination, an ordered factor with 5 levels <code>0</code> &lt; ... &lt; <code>4</code>.</p>
</dd>
<dt><code>wind</code></dt><dd><p>wind pollination, an ordered factor with 5 levels <code>0</code> &lt; ... &lt; <code>4</code>.</p>
</dd>
<dt><code>lign</code></dt><dd><p>a binary factor with levels <code>0:1</code>,
indicating if plant is woody.</p>
</dd>
<dt><code>piq</code></dt><dd><p>a binary factor indicating if plant is thorny.</p>
</dd>
<dt><code>ros</code></dt><dd><p>a binary factor indicating if plant is rosette.</p>
</dd>
<dt><code>semiros</code></dt><dd><p>semi-rosette plant, a binary factor (<code>0</code>:
no; <code>1</code>: yes).</p>
</dd>
<dt><code>leafy</code></dt><dd><p>leafy plant, a binary factor.</p>
</dd>
<dt><code>suman</code></dt><dd><p>summer annual, a binary factor.</p>
</dd>
<dt><code>winan</code></dt><dd><p>winter annual, a binary factor.</p>
</dd>
<dt><code>monocarp</code></dt><dd><p>monocarpic perennial, a binary factor.</p>
</dd>
<dt><code>polycarp</code></dt><dd><p>polycarpic perennial, a binary factor.</p>
</dd>
<dt><code>seasaes</code></dt><dd><p>seasonal aestival leaves, a binary factor.</p>
</dd>
<dt><code>seashiv</code></dt><dd><p>seasonal hibernal leaves, a binary factor.</p>
</dd>
<dt><code>seasver</code></dt><dd><p>seasonal vernal leaves, a binary factor.</p>
</dd>
<dt><code>everalw</code></dt><dd><p>leaves always evergreen, a binary factor.</p>
</dd>
<dt><code>everparti</code></dt><dd><p>leaves partially evergreen, a binary factor.</p>
</dd>
<dt><code>elaio</code></dt><dd><p>fruits with an elaiosome (dispersed by ants), a binary factor.</p>
</dd>
<dt><code>endozoo</code></dt><dd><p>endozoochorous fruits, a binary factor.</p>
</dd>
<dt><code>epizoo</code></dt><dd><p>epizoochorous fruits, a binary factor.</p>
</dd>
<dt><code>aquat</code></dt><dd><p>aquatic dispersal fruits, a binary factor.</p>
</dd>
<dt><code>windgl</code></dt><dd><p>wind dispersed fruits,  a binary factor.</p>
</dd>
<dt><code>unsp</code></dt><dd><p>unspecialized mechanism of seed dispersal, a binary factor.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Most of factor attributes are not disjunctive. For example, a plant can be usually
pollinated by insects but sometimes self-pollination can occured.
</p>


<h3>Source</h3>

<p>Vallet, Jeanne (2005)
<em>Structuration de communauts vgtales et analyse comparative de
traits biologiques le long d'un gradient d'urbanisation</em>.
Mmoire de Master 2 'Ecologie-Biodiversit-Evolution';
Universit Paris Sud XI, 30p.+ annexes (in french)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(plantTraits)

## Calculation of a dissimilarity matrix
library(cluster)
dai.b &lt;- daisy(plantTraits,
               type = list(ordratio = 4:11, symm = 12:13, asymm = 14:31))

## Hierarchical classification
agn.trts &lt;- agnes(dai.b, method="ward")
plot(agn.trts, which.plots = 2, cex= 0.6)
plot(agn.trts, which.plots = 1)
cutree6 &lt;- cutree(agn.trts, k=6)
cutree6

## Principal Coordinate Analysis
cmdsdai.b &lt;- cmdscale(dai.b, k=6)
plot(cmdsdai.b[, 1:2], asp = 1, col = cutree6)
</code></pre>

<hr>
<h2 id='plot.agnes'>Plots of an Agglomerative Hierarchical Clustering</h2><span id='topic+plot.agnes'></span>

<h3>Description</h3>

<p>Creates plots for visualizing an <code>agnes</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'agnes'
plot(x, ask = FALSE, which.plots = NULL, main = NULL,
           sub = paste("Agglomerative Coefficient = ",round(x$ac, digits = 2)),
           adj = 0, nmax.lab = 35, max.strlen = 5, xax.pretty = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.agnes_+3A_x">x</code></td>
<td>
<p>an object of class <code>"agnes"</code>, typically created by
<code><a href="#topic+agnes">agnes</a>(.)</code>.</p>
</td></tr>
<tr><td><code id="plot.agnes_+3A_ask">ask</code></td>
<td>
<p>logical; if true and <code>which.plots</code> is <code>NULL</code>,
<code>plot.agnes</code> operates in interactive mode, via <code><a href="utils.html#topic+menu">menu</a></code>.</p>
</td></tr>
<tr><td><code id="plot.agnes_+3A_which.plots">which.plots</code></td>
<td>
<p>integer vector or NULL (default), the latter
producing both plots.  Otherwise, <code>which.plots</code>
must contain integers of <code>1</code> for a <em>banner</em> plot or <code>2</code> for a
dendrogram or &ldquo;clustering tree&rdquo;.</p>
</td></tr>
<tr><td><code id="plot.agnes_+3A_main">main</code>, <code id="plot.agnes_+3A_sub">sub</code></td>
<td>
<p>main and sub title for the plot, with convenient
defaults.  See documentation for these arguments in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="plot.agnes_+3A_adj">adj</code></td>
<td>
<p>for label adjustment in <code><a href="#topic+bannerplot">bannerplot</a>()</code>.</p>
</td></tr>
<tr><td><code id="plot.agnes_+3A_nmax.lab">nmax.lab</code></td>
<td>
<p>integer indicating the number of labels which is
considered too large for single-name labelling the banner plot.</p>
</td></tr>
<tr><td><code id="plot.agnes_+3A_max.strlen">max.strlen</code></td>
<td>
<p>positive integer giving the length to which
strings are truncated in banner plot labeling.</p>
</td></tr>
<tr><td><code id="plot.agnes_+3A_xax.pretty">xax.pretty</code></td>
<td>
<p>logical or integer indicating if
<code><a href="base.html#topic+pretty">pretty</a>(*, n = xax.pretty)</code> should be used for the x axis.
<code>xax.pretty = FALSE</code> is for back compatibility.</p>
</td></tr>
<tr><td><code id="plot.agnes_+3A_...">...</code></td>
<td>
<p>graphical parameters (see <code><a href="graphics.html#topic+par">par</a></code>) may also
be supplied and are passed to <code><a href="#topic+bannerplot">bannerplot</a>()</code> or
<code>pltree()</code> (see <code><a href="#topic+pltree.twins">pltree.twins</a></code>), respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>ask = TRUE</code>, rather than producing each plot sequentially,
<code>plot.agnes</code> displays a menu listing all the plots that can be produced.
If the menu is not desired but a pause between plots is still wanted
one must set <code>par(ask= TRUE)</code> before invoking the plot command.
</p>
<p>The banner displays the hierarchy of clusters, and is equivalent to a tree.
See Rousseeuw (1986) or chapter 5 of Kaufman and Rousseeuw (1990).
The banner plots distances at which observations and clusters are merged.
The observations are listed in the order found by the <code>agnes</code> algorithm,
and the numbers in the <code>height</code> vector are represented as bars
between the observations.
</p>
<p>The leaves of the clustering tree are the original observations.  Two
branches come together at the distance between the two clusters being merged.
</p>
<p>For more customization of the plots, rather call
<code><a href="#topic+bannerplot">bannerplot</a></code> and <code>pltree()</code>, i.e., its method
<code><a href="#topic+pltree.twins">pltree.twins</a></code>, respectively.
</p>
<p>directly with
corresponding arguments, e.g., <code>xlab</code> or <code>ylab</code>.
</p>


<h3>Side Effects</h3>

<p>Appropriate plots are produced on the current graphics device. This can
be one or both of the following choices:
<br /> Banner
<br /> Clustering tree
</p>


<h3>Note</h3>

<p>In the banner plot, observation labels are only printed when the
number of observations is limited less than <code>nmax.lab</code> (35, by
default), for readability.  Moreover, observation labels are truncated
to maximally <code>max.strlen</code> (5) characters.
</p>
<p>For the dendrogram, more flexibility than via <code>pltree()</code> is
provided by <code>dg &lt;- <a href="stats.html#topic+as.dendrogram">as.dendrogram</a>(x)</code> and
plotting <code>dg</code> via <code><a href="stats.html#topic+plot.dendrogram">plot.dendrogram</a></code>.
</p>


<h3>References</h3>

<p>Kaufman, L. and Rousseeuw, P.J. (1990)
<em>Finding Groups in Data: An Introduction to Cluster Analysis</em>.
Wiley, New York.
</p>
<p>Rousseeuw, P.J. (1986). A visual display for hierarchical classification,
in <em>Data Analysis and Informatics 4</em>; edited by E. Diday,
Y. Escoufier, L. Lebart, J. Pages, Y. Schektman, and R. Tomassone.
North-Holland, Amsterdam, 743&ndash;748.
</p>
<p>Struyf, A., Hubert, M. and Rousseeuw, P.J. (1997)
Integrating Robust Clustering Techniques in S-PLUS,
<em>Computational Statistics and Data Analysis</em>, <b>26</b>, 17&ndash;37.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code> and <code><a href="#topic+agnes.object">agnes.object</a></code>;
<code><a href="#topic+bannerplot">bannerplot</a></code>, <code><a href="#topic+pltree.twins">pltree.twins</a></code>,
and <code><a href="graphics.html#topic+par">par</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Can also pass 'labels' to pltree() and bannerplot():
data(iris)
cS &lt;- as.character(Sp &lt;- iris$Species)
cS[Sp == "setosa"] &lt;- "S"
cS[Sp == "versicolor"] &lt;- "V"
cS[Sp == "virginica"] &lt;- "g"
ai &lt;- agnes(iris[, 1:4])
plot(ai, labels = cS, nmax = 150)# bannerplot labels are mess
</code></pre>

<hr>
<h2 id='plot.diana'>Plots of a Divisive Hierarchical Clustering</h2><span id='topic+plot.diana'></span>

<h3>Description</h3>

<p>Creates plots for visualizing a <code>diana</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'diana'
plot(x, ask = FALSE, which.plots = NULL, main = NULL,
           sub = paste("Divisive Coefficient = ", round(x$dc, digits = 2)),
           adj = 0, nmax.lab = 35, max.strlen = 5, xax.pretty = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.diana_+3A_x">x</code></td>
<td>
<p>an object of class <code>"diana"</code>, typically created by
<code><a href="#topic+diana">diana</a>(.)</code>.</p>
</td></tr>
<tr><td><code id="plot.diana_+3A_ask">ask</code></td>
<td>
<p>logical; if true and <code>which.plots</code> is <code>NULL</code>,
<code>plot.diana</code> operates in interactive mode, via <code><a href="utils.html#topic+menu">menu</a></code>.</p>
</td></tr>
<tr><td><code id="plot.diana_+3A_which.plots">which.plots</code></td>
<td>
<p>integer vector or NULL (default), the latter
producing both plots.  Otherwise, <code>which.plots</code>
must contain integers of <code>1</code> for a <em>banner</em> plot or <code>2</code> for a
dendrogram or &ldquo;clustering tree&rdquo;.</p>
</td></tr>
<tr><td><code id="plot.diana_+3A_main">main</code>, <code id="plot.diana_+3A_sub">sub</code></td>
<td>
<p>main and sub title for the plot, each with a convenient
default.  See documentation for these arguments in
<code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="plot.diana_+3A_adj">adj</code></td>
<td>
<p>for label adjustment in <code><a href="#topic+bannerplot">bannerplot</a>()</code>.</p>
</td></tr>
<tr><td><code id="plot.diana_+3A_nmax.lab">nmax.lab</code></td>
<td>
<p>integer indicating the number of labels which is
considered too large for single-name labelling the banner plot.</p>
</td></tr>
<tr><td><code id="plot.diana_+3A_max.strlen">max.strlen</code></td>
<td>
<p>positive integer giving the length to which
strings are truncated in banner plot labeling.</p>
</td></tr>
<tr><td><code id="plot.diana_+3A_xax.pretty">xax.pretty</code></td>
<td>
<p>logical or integer indicating if
<code><a href="base.html#topic+pretty">pretty</a>(*, n = xax.pretty)</code> should be used for the x axis.
<code>xax.pretty = FALSE</code> is for back compatibility.</p>
</td></tr>
<tr><td><code id="plot.diana_+3A_...">...</code></td>
<td>
<p>graphical parameters (see <code><a href="graphics.html#topic+par">par</a></code>) may also
be supplied and are passed to <code><a href="#topic+bannerplot">bannerplot</a>()</code> or
<code><a href="#topic+pltree">pltree</a>()</code>, respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>ask = TRUE</code>, rather than producing each plot sequentially,
<code>plot.diana</code> displays a menu listing all the plots that can be produced.
If the menu is not desired but a pause between plots is still wanted
one must set <code>par(ask= TRUE)</code> before invoking the plot command.
</p>
<p>The banner displays the hierarchy of clusters, and is equivalent to a tree.
See Rousseeuw (1986) or chapter 6 of Kaufman and Rousseeuw (1990).
The banner plots the diameter of each cluster being splitted.
The observations are listed in the order found by the <code>diana</code>
algorithm, and the numbers in the <code>height</code> vector are represented
as bars between the observations.
</p>
<p>The leaves of the clustering tree are the original observations.
A branch splits up at the diameter of the cluster being splitted.
</p>


<h3>Side Effects</h3>

<p>An appropriate plot is produced on the current graphics device. This can
be one or both of the following choices:
<br /> Banner
<br /> Clustering tree
</p>


<h3>Note</h3>

<p>In the banner plot,
observation labels are only printed when the number of observations is
limited less than <code>nmax.lab</code> (35, by default), for readability.
Moreover, observation labels are truncated to maximally
<code>max.strlen</code> (5) characters.
</p>


<h3>References</h3>

<p>see those in <code><a href="#topic+plot.agnes">plot.agnes</a></code>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+diana">diana</a></code>, <code><a href="#topic+diana.object">diana.object</a></code>,
<code><a href="#topic+twins.object">twins.object</a></code>, <code><a href="graphics.html#topic+par">par</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>example(diana)# -&gt; dv &lt;- diana(....)

plot(dv, which = 1, nmax.lab = 100)

## wider labels :
op &lt;- par(mar = par("mar") + c(0, 2, 0,0))
plot(dv, which = 1, nmax.lab = 100, max.strlen = 12)
par(op)
</code></pre>

<hr>
<h2 id='plot.mona'>Banner of Monothetic Divisive Hierarchical Clusterings</h2><span id='topic+plot.mona'></span>

<h3>Description</h3>

<p>Creates the banner of a <code>mona</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mona'
plot(x, main = paste("Banner of ", deparse1(x$call)),
          sub = NULL, xlab = "Separation step",
	  col = c(2,0), axes = TRUE, adj = 0,
          nmax.lab = 35, max.strlen = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.mona_+3A_x">x</code></td>
<td>
<p>an object of class <code>"mona"</code>, typically created by
<code><a href="#topic+mona">mona</a>(.)</code>.</p>
</td></tr>
<tr><td><code id="plot.mona_+3A_main">main</code>, <code id="plot.mona_+3A_sub">sub</code></td>
<td>
<p>main and sub titles for the plot, with convenient
defaults.  See documentation in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="plot.mona_+3A_xlab">xlab</code></td>
<td>
<p>x axis label, see <code><a href="graphics.html#topic+title">title</a></code>.</p>
</td></tr>
<tr><td><code id="plot.mona_+3A_col">col</code>, <code id="plot.mona_+3A_adj">adj</code></td>
<td>
<p>graphical parameters passed to <code><a href="#topic+bannerplot">bannerplot</a>()</code>.</p>
</td></tr>
<tr><td><code id="plot.mona_+3A_axes">axes</code></td>
<td>
<p>logical, indicating if (labeled) axes should be drawn.</p>
</td></tr>
<tr><td><code id="plot.mona_+3A_nmax.lab">nmax.lab</code></td>
<td>
<p>integer indicating the number of labels which is
considered too large for labeling.</p>
</td></tr>
<tr><td><code id="plot.mona_+3A_max.strlen">max.strlen</code></td>
<td>
<p>positive integer giving the length to which
strings are truncated in labeling.</p>
</td></tr>
<tr><td><code id="plot.mona_+3A_...">...</code></td>
<td>
<p>further graphical arguments are passed to
<code><a href="#topic+bannerplot">bannerplot</a>()</code> and <code><a href="graphics.html#topic+text">text</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots the separation step at which clusters are splitted.  The
observations are given in the order found by the <code>mona</code>
algorithm, the numbers in the <code>step</code> vector are represented as
bars between the observations.
</p>
<p>When a long bar is drawn between two observations,
those observations have the same value for each variable.
See chapter 7 of Kaufman and Rousseeuw (1990).
</p>


<h3>Side Effects</h3>

<p>A banner is plotted on the current graphics device.
</p>


<h3>Note</h3>

<p>In the banner plot,
observation labels are only printed when the number of observations is
limited less than <code>nmax.lab</code> (35, by default), for readability.
Moreover, observation labels are truncated to maximally
<code>max.strlen</code> (5) characters.
</p>


<h3>References</h3>

<p>see those in <code><a href="#topic+plot.agnes">plot.agnes</a></code>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+mona">mona</a></code>, <code><a href="#topic+mona.object">mona.object</a></code>, <code><a href="graphics.html#topic+par">par</a></code>.
</p>

<hr>
<h2 id='plot.partition'>Plot of a Partition of the Data Set</h2><span id='topic+plot.partition'></span>

<h3>Description</h3>

<p>Creates plots for visualizing a <code>partition</code> object.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'partition'
plot(x, ask = FALSE, which.plots = NULL,
     nmax.lab = 40, max.strlen = 5, data = x$data, dist = NULL,
     stand = FALSE, lines = 2,
     shade = FALSE, color = FALSE, labels = 0, plotchar = TRUE,
     span = TRUE, xlim = NULL, ylim = NULL, main = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.partition_+3A_x">x</code></td>
<td>
<p>an object of class <code>"partition"</code>, typically created by the
functions <code><a href="#topic+pam">pam</a></code>, <code><a href="#topic+clara">clara</a></code>, or <code><a href="#topic+fanny">fanny</a></code>.</p>
</td></tr>
<tr><td><code id="plot.partition_+3A_ask">ask</code></td>
<td>
<p>logical; if true and <code>which.plots</code> is <code>NULL</code>,
<code>plot.partition</code> operates in interactive mode, via <code><a href="utils.html#topic+menu">menu</a></code>.</p>
</td></tr>
<tr><td><code id="plot.partition_+3A_which.plots">which.plots</code></td>
<td>
<p>integer vector or NULL (default), the latter
producing both plots.  Otherwise, <code>which.plots</code> must contain
integers of <code>1</code> for a <em>clusplot</em> or <code>2</code> for
<em>silhouette</em>.</p>
</td></tr>
<tr><td><code id="plot.partition_+3A_nmax.lab">nmax.lab</code></td>
<td>
<p>integer indicating the number of labels which is
considered too large for single-name labeling the silhouette plot.</p>
</td></tr>
<tr><td><code id="plot.partition_+3A_max.strlen">max.strlen</code></td>
<td>
<p>positive integer giving the length to which
strings are truncated in silhouette plot labeling.</p>
</td></tr>
<tr><td><code id="plot.partition_+3A_data">data</code></td>
<td>
<p>numeric matrix with the scaled data; per default taken
from the partition object <code>x</code>, but can be specified explicitly.</p>
</td></tr>
<tr><td><code id="plot.partition_+3A_dist">dist</code></td>
<td>
<p>when <code>x</code> does not have a <code>diss</code> component as for
<code><a href="#topic+pam">pam</a>(*, keep.diss=FALSE)</code>, <code>dist</code> must be the
dissimilarity if a clusplot is desired.</p>
</td></tr>
<tr><td><code id="plot.partition_+3A_stand">stand</code>, <code id="plot.partition_+3A_lines">lines</code>, <code id="plot.partition_+3A_shade">shade</code>, <code id="plot.partition_+3A_color">color</code>, <code id="plot.partition_+3A_labels">labels</code>, <code id="plot.partition_+3A_plotchar">plotchar</code>, <code id="plot.partition_+3A_span">span</code>, <code id="plot.partition_+3A_xlim">xlim</code>, <code id="plot.partition_+3A_ylim">ylim</code>, <code id="plot.partition_+3A_main">main</code>, <code id="plot.partition_+3A_...">...</code></td>
<td>

<p>All optional arguments available for the <code><a href="#topic+clusplot.default">clusplot.default</a></code>
function (except for the <code>diss</code> one) and graphical parameters
(see <code><a href="graphics.html#topic+par">par</a></code>) may also be supplied as arguments to this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>ask= TRUE</code>, rather than producing each plot sequentially,
<code>plot.partition</code> displays a menu listing all the plots that can
be produced.
If the menu is not desired but a pause between plots is still wanted,
call <code>par(ask= TRUE)</code> before invoking the plot command.
</p>
<p>The <em>clusplot</em> of a cluster partition consists of a two-dimensional
representation of the observations, in which the clusters are
indicated by ellipses (see <code><a href="#topic+clusplot.partition">clusplot.partition</a></code> for more
details).
</p>
<p>The <em>silhouette plot</em> of a nonhierarchical clustering is fully
described in Rousseeuw (1987) and in chapter 2 of Kaufman and
Rousseeuw (1990).
For each observation i, a bar is drawn, representing its silhouette
width s(i), see <code><a href="#topic+silhouette">silhouette</a></code> for details.
Observations are grouped per cluster, starting with cluster 1 at the
top.  Observations with a large s(i) (almost 1) are very well
clustered, a small s(i) (around 0) means that the observation lies
between two clusters, and observations with a negative s(i) are
probably placed in the wrong cluster.
</p>
<p>A clustering can be performed for several values of <code>k</code> (the number of
clusters).  Finally, choose the value of <code>k</code> with the largest overall
average silhouette width.
</p>


<h3>Side Effects</h3>

<p>An appropriate plot is produced on the current graphics device.  This
can be one or both of the following choices:
<br /> Clusplot
<br /> Silhouette plot
</p>


<h3>Note</h3>

<p>In the silhouette plot, observation labels are only printed when the
number of observations is less than <code>nmax.lab</code> (40, by default),
for readability.  Moreover, observation labels are truncated to
maximally <code>max.strlen</code> (5) characters.  <br />
For more flexibility, use <code>plot(silhouette(x), ...)</code>, see
<code><a href="#topic+plot.silhouette">plot.silhouette</a></code>.
</p>


<h3>References</h3>

<p>Rousseeuw, P.J. (1987)
Silhouettes: A graphical aid to the interpretation and validation of
cluster analysis. <em>J. Comput. Appl. Math.</em>, <b>20</b>, 53&ndash;65.
</p>
<p>Further, the references in <code><a href="#topic+plot.agnes">plot.agnes</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+partition.object">partition.object</a></code>, <code><a href="#topic+clusplot.partition">clusplot.partition</a></code>,
<code><a href="#topic+clusplot.default">clusplot.default</a></code>, <code><a href="#topic+pam">pam</a></code>,
<code><a href="#topic+pam.object">pam.object</a></code>, <code><a href="#topic+clara">clara</a></code>,
<code><a href="#topic+clara.object">clara.object</a></code>, <code><a href="#topic+fanny">fanny</a></code>,
<code><a href="#topic+fanny.object">fanny.object</a></code>, <code><a href="graphics.html#topic+par">par</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 25 objects, divided into 2 clusters.
x &lt;- rbind(cbind(rnorm(10,0,0.5), rnorm(10,0,0.5)),
           cbind(rnorm(15,5,0.5), rnorm(15,5,0.5)))
plot(pam(x, 2))

## Save space not keeping data in clus.object, and still clusplot() it:
data(xclara)
cx &lt;- clara(xclara, 3, keep.data = FALSE)
cx$data # is NULL
plot(cx, data = xclara)
</code></pre>

<hr>
<h2 id='pltree'>Plot Clustering Tree of a Hierarchical Clustering</h2><span id='topic+pltree'></span><span id='topic+pltree.twins'></span>

<h3>Description</h3>

<p><code>pltree()</code> Draws a clustering tree (&ldquo;dendrogram&rdquo;) on the
current graphics device.  We provide the <code>twins</code> method draws the
tree of a <code>twins</code> object, i.e., hierarchical clustering,
typically resulting from <code><a href="#topic+agnes">agnes</a>()</code> or <code><a href="#topic+diana">diana</a>()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pltree(x, ...)
## S3 method for class 'twins'
pltree(x, main = paste("Dendrogram of ", deparse1(x$call)),
             labels = NULL, ylab = "Height", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pltree_+3A_x">x</code></td>
<td>
<p>in general, an <span class="rlang"><b>R</b></span> object for which a <code>pltree</code> method is
defined; specifically, an object of class <code>"twins"</code>, typically
created by either <code><a href="#topic+agnes">agnes</a>()</code> or <code><a href="#topic+diana">diana</a>()</code>.</p>
</td></tr>
<tr><td><code id="pltree_+3A_main">main</code></td>
<td>
<p>main title with a sensible default.</p>
</td></tr>
<tr><td><code id="pltree_+3A_labels">labels</code></td>
<td>
<p>labels to use; the default is constructed from <code>x</code>.</p>
</td></tr>
<tr><td><code id="pltree_+3A_ylab">ylab</code></td>
<td>
<p>label for y-axis.</p>
</td></tr>
<tr><td><code id="pltree_+3A_...">...</code></td>
<td>
<p>graphical parameters (see <code><a href="graphics.html#topic+par">par</a></code>) may also
be supplied as arguments to this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Creates a plot of a clustering tree given a <code>twins</code> object.  The
leaves of the tree are the original observations.  In case of an
agglomerative clustering, two branches come together at the distance
between the two clusters being merged.  For a divisive clustering, a
branch splits up at the diameter of the cluster being splitted.
</p>
<p>Note that currently the method function simply calls
<code>plot(<a href="stats.html#topic+as.hclust">as.hclust</a>(x), ...)</code>, which dispatches to
<code><a href="stats.html#topic+plot.hclust">plot.hclust</a>(..)</code>.  If more flexible plots are needed,
consider <code>xx &lt;- <a href="stats.html#topic+as.dendrogram">as.dendrogram</a>(<a href="stats.html#topic+as.hclust">as.hclust</a>(x))</code> and plotting
<code>xx</code>, see <code><a href="stats.html#topic+plot.dendrogram">plot.dendrogram</a></code>.
</p>


<h3>Value</h3>

<p>a NULL value is returned.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code>, <code><a href="#topic+agnes.object">agnes.object</a></code>, <code><a href="#topic+diana">diana</a></code>,
<code><a href="#topic+diana.object">diana.object</a></code>, <code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="graphics.html#topic+par">par</a></code>,
<code><a href="#topic+plot.agnes">plot.agnes</a></code>, <code><a href="#topic+plot.diana">plot.diana</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(votes.repub)
agn &lt;- agnes(votes.repub)
pltree(agn)

dagn  &lt;- as.dendrogram(as.hclust(agn))
dagn2 &lt;- as.dendrogram(as.hclust(agn), hang = 0.2)
op &lt;- par(mar = par("mar") + c(0,0,0, 2)) # more space to the right
plot(dagn2, horiz = TRUE)
plot(dagn, horiz = TRUE, center = TRUE,
     nodePar = list(lab.cex = 0.6, lab.col = "forest green", pch = NA),
     main = deparse(agn$call))
par(op)
</code></pre>

<hr>
<h2 id='pluton'>Isotopic Composition Plutonium Batches</h2><span id='topic+pluton'></span>

<h3>Description</h3>

<p>The <code>pluton</code> data frame has 45 rows and 4 columns,
containing percentages of isotopic composition of 45 Plutonium
batches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(pluton)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>Pu238</dt><dd><p>the percentages of <code class="reqn">\ ^{238}Pu</code>,
always less than 2 percent.</p>
</dd>
<dt>Pu239</dt><dd><p>the percentages of <code class="reqn">\ ^{239}Pu</code>,
typically between 60 and 80 percent (from neutron capture of Uranium,
<code class="reqn">\ ^{238}U</code>).</p>
</dd>
<dt>Pu240</dt><dd><p>percentage of the plutonium 240 isotope.</p>
</dd>
<dt>Pu241</dt><dd><p>percentage of the plutonium 241 isotope.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Note that the percentage of plutonium~242 can be computed from the
other four percentages, see the examples.
</p>
<p>In the reference below it is explained why it is very desirable to
combine these plutonium patches in three groups of similar size.
</p>


<h3>Source</h3>

<p>Available as &lsquo;<span class="file">pluton.dat</span>&rsquo; from the archive of the University of Antwerpen,




&lsquo;<span class="file">..../datasets/clusplot-examples.tar.gz</span>&rsquo;, no longer available.
</p>


<h3>References</h3>

<p>Rousseeuw, P.J. and Kaufman, L and Trauwaert, E. (1996)
Fuzzy clustering using scatter matrices,
<em>Computational Statistics and Data Analysis</em> <b>23</b>(1), 135&ndash;151.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(pluton)

hist(apply(pluton,1,sum), col = "gray") # between 94% and 100%
pu5 &lt;- pluton
pu5$Pu242 &lt;- 100 - apply(pluton,1,sum) # the remaining isotope.
pairs(pu5)
</code></pre>

<hr>
<h2 id='predict.ellipsoid'>Predict Method for Ellipsoid Objects</h2><span id='topic+predict.ellipsoid'></span><span id='topic+ellipsoidPoints'></span>

<h3>Description</h3>

<p>Compute points on the ellipsoid boundary, mostly for drawing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict.ellipsoid(object, n.out=201, ...)
## S3 method for class 'ellipsoid'
predict(object, n.out=201, ...)
ellipsoidPoints(A, d2, loc, n.half = 201)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ellipsoid_+3A_object">object</code></td>
<td>
<p>an object of class <code>ellipsoid</code>, typically from
<code><a href="#topic+ellipsoidhull">ellipsoidhull</a>()</code>; alternatively any list-like object
with proper components, see details below.</p>
</td></tr>
<tr><td><code id="predict.ellipsoid_+3A_n.out">n.out</code>, <code id="predict.ellipsoid_+3A_n.half">n.half</code></td>
<td>
<p>half the number of points to create.</p>
</td></tr>
<tr><td><code id="predict.ellipsoid_+3A_a">A</code>, <code id="predict.ellipsoid_+3A_d2">d2</code>, <code id="predict.ellipsoid_+3A_loc">loc</code></td>
<td>
<p>arguments of the auxilary <code>ellipsoidPoints</code>,
see below.</p>
</td></tr>
<tr><td><code id="predict.ellipsoid_+3A_...">...</code></td>
<td>
<p>passed to and from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note <code>ellipsoidPoints</code> is the workhorse function of
<code>predict.ellipsoid</code> a standalone function and method for
<code>ellipsoid</code> objects, see <code><a href="#topic+ellipsoidhull">ellipsoidhull</a></code>.
The class of <code>object</code> is not checked; it must solely have valid
components <code>loc</code> (length <code class="reqn">p</code>), the <code class="reqn">p \times p</code>
matrix <code>cov</code> (corresponding to <code>A</code>) and <code>d2</code> for the
center, the shape (&ldquo;covariance&rdquo;) matrix and the squared average
radius (or distance) or <code><a href="stats.html#topic+qchisq">qchisq</a>(*, p)</code> quantile.
</p>
<p>Unfortunately, this is only implemented for <code class="reqn">p = 2</code>, currently;
contributions for <code class="reqn">p \ge 3</code> are <em>very welcome</em>.
</p>


<h3>Value</h3>

<p>a numeric matrix of dimension <code>2*n.out</code> times <code class="reqn">p</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ellipsoidhull">ellipsoidhull</a></code>, <code><a href="#topic+volume.ellipsoid">volume.ellipsoid</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## see also  example(ellipsoidhull)

## Robust vs. L.S. covariance matrix
set.seed(143)
x &lt;- rt(200, df=3)
y &lt;- 3*x + rt(200, df=2)
plot(x,y, main="non-normal data (N=200)")
mtext("with classical and robust cov.matrix ellipsoids")
X &lt;- cbind(x,y)
C.ls &lt;- cov(X) ; m.ls &lt;- colMeans(X)
d2.99 &lt;- qchisq(0.99, df = 2)
lines(ellipsoidPoints(C.ls, d2.99, loc=m.ls), col="green")
if(require(MASS)) {
  Cxy &lt;- cov.rob(cbind(x,y))
  lines(ellipsoidPoints(Cxy$cov, d2 = d2.99, loc=Cxy$center), col="red")
}# MASS
</code></pre>

<hr>
<h2 id='print.agnes'>Print Method for AGNES Objects</h2><span id='topic+print.agnes'></span>

<h3>Description</h3>

<p>Prints the call, agglomerative coefficient, ordering of objects and
distances between merging clusters ('Height') of an <code>agnes</code> object.
</p>
<p>This is a method for the generic <code><a href="base.html#topic+print">print</a>()</code> function for objects
inheriting from class <code>agnes</code>, see <code><a href="#topic+agnes.object">agnes.object</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'agnes'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.agnes_+3A_x">x</code></td>
<td>
<p>an agnes object.</p>
</td></tr>
<tr><td><code id="print.agnes_+3A_...">...</code></td>
<td>
<p>potential further arguments (required by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+summary.agnes">summary.agnes</a></code> producing more output;
<code><a href="#topic+agnes">agnes</a></code>, <code><a href="#topic+agnes.object">agnes.object</a></code>, <code><a href="base.html#topic+print">print</a></code>,
<code><a href="base.html#topic+print.default">print.default</a></code>.
</p>

<hr>
<h2 id='print.clara'>Print Method for CLARA Objects</h2><span id='topic+print.clara'></span>

<h3>Description</h3>

<p>Prints the best sample, medoids, clustering vector and objective function
of <code>clara</code> object.
</p>
<p>This is a method for the function <code><a href="base.html#topic+print">print</a>()</code> for objects
inheriting from class <code><a href="#topic+clara">clara</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'clara'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.clara_+3A_x">x</code></td>
<td>
<p>a clara object.</p>
</td></tr>
<tr><td><code id="print.clara_+3A_...">...</code></td>
<td>
<p>potential further arguments (require by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+summary.clara">summary.clara</a></code> producing more output;
<code><a href="#topic+clara">clara</a></code>, <code><a href="#topic+clara.object">clara.object</a></code>, <code><a href="base.html#topic+print">print</a></code>,
<code><a href="base.html#topic+print.default">print.default</a></code>.
</p>

<hr>
<h2 id='print.diana'>Print Method for DIANA Objects</h2><span id='topic+print.diana'></span>

<h3>Description</h3>

<p>Prints the ordering of objects, diameters of splitted clusters,
and divisive coefficient of a <code>diana</code> object.
</p>
<p>This is a method for the function <code><a href="base.html#topic+print">print</a>()</code> for objects
inheriting from class <code><a href="#topic+diana">diana</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'diana'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.diana_+3A_x">x</code></td>
<td>
<p>a diana object.</p>
</td></tr>
<tr><td><code id="print.diana_+3A_...">...</code></td>
<td>
<p>potential further arguments (require by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+diana">diana</a></code>, <code><a href="#topic+diana.object">diana.object</a></code>, <code><a href="base.html#topic+print">print</a></code>,
<code><a href="base.html#topic+print.default">print.default</a></code>.
</p>

<hr>
<h2 id='print.dissimilarity'>Print and Summary Methods for Dissimilarity Objects</h2><span id='topic+print.dissimilarity'></span><span id='topic+summary.dissimilarity'></span><span id='topic+print.summary.dissimilarity'></span>

<h3>Description</h3>

<p>Print or summarize the distances and the attributes of a
<code>dissimilarity</code> object.
</p>
<p>These are methods for the functions <code>print()</code> and <code>summary()</code> for
<code>dissimilarity</code> objects.  See <code>print</code>, <code>print.default</code>,
or <code>summary</code> for the general behavior of these.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dissimilarity'
print(x, diag = NULL, upper = NULL,
      digits = getOption("digits"), justify = "none", right = TRUE, ...)
## S3 method for class 'dissimilarity'
summary(object,
        digits = max(3, getOption("digits") - 2), ...)
## S3 method for class 'summary.dissimilarity'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.dissimilarity_+3A_x">x</code>, <code id="print.dissimilarity_+3A_object">object</code></td>
<td>
<p>a <code>dissimilarity</code> object or a
<code>summary.dissimilarity</code> one for <code>print.summary.dissimilarity()</code>.</p>
</td></tr>
<tr><td><code id="print.dissimilarity_+3A_digits">digits</code></td>
<td>
<p>the number of digits to use, see <code><a href="base.html#topic+print.default">print.default</a></code>.</p>
</td></tr>
<tr><td><code id="print.dissimilarity_+3A_diag">diag</code>, <code id="print.dissimilarity_+3A_upper">upper</code>, <code id="print.dissimilarity_+3A_justify">justify</code>, <code id="print.dissimilarity_+3A_right">right</code></td>
<td>
<p>optional arguments specifying how
the triangular dissimilarity matrix is printed; see
<code><a href="stats.html#topic+print.dist">print.dist</a></code>.</p>
</td></tr>
<tr><td><code id="print.dissimilarity_+3A_...">...</code></td>
<td>
<p>potential further arguments (require by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+daisy">daisy</a></code>, <code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code>,
<code><a href="base.html#topic+print">print</a></code>, <code><a href="base.html#topic+print.default">print.default</a></code>, <code><a href="stats.html#topic+print.dist">print.dist</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## See  example(daisy)

 sd &lt;- summary(daisy(matrix(rnorm(100), 20,5)))
 sd # -&gt; print.summary.dissimilarity(.)
 str(sd)
</code></pre>

<hr>
<h2 id='print.fanny'>Print and Summary Methods for FANNY Objects</h2><span id='topic+print.fanny'></span><span id='topic+summary.fanny'></span><span id='topic+print.summary.fanny'></span>

<h3>Description</h3>

<p>Prints the objective function, membership coefficients and clustering vector
of <code>fanny</code> object.
</p>
<p>This is a method for the function <code><a href="base.html#topic+print">print</a>()</code> for objects
inheriting from class <code><a href="#topic+fanny">fanny</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fanny'
print(x, digits = getOption("digits"), ...)
## S3 method for class 'fanny'
summary(object, ...)
## S3 method for class 'summary.fanny'
print(x, digits = getOption("digits"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fanny_+3A_x">x</code>, <code id="print.fanny_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+fanny">fanny</a></code> object.</p>
</td></tr>
<tr><td><code id="print.fanny_+3A_digits">digits</code></td>
<td>
<p>number of significant digits for printing, see
<code><a href="base.html#topic+print.default">print.default</a></code>.</p>
</td></tr>
<tr><td><code id="print.fanny_+3A_...">...</code></td>
<td>
<p>potential further arguments (required by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+fanny">fanny</a></code>, <code><a href="#topic+fanny.object">fanny.object</a></code>, <code><a href="base.html#topic+print">print</a></code>,
<code><a href="base.html#topic+print.default">print.default</a></code>.
</p>

<hr>
<h2 id='print.mona'>Print Method for MONA Objects</h2><span id='topic+print.mona'></span>

<h3>Description</h3>

<p>Prints the ordering of objects, separation steps, and used variables
of a <code>mona</code> object.
</p>
<p>This is a method for the function <code><a href="base.html#topic+print">print</a>()</code> for objects
inheriting from class <code><a href="#topic+mona">mona</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mona'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.mona_+3A_x">x</code></td>
<td>
<p>a mona object.</p>
</td></tr>
<tr><td><code id="print.mona_+3A_...">...</code></td>
<td>
<p>potential further arguments (require by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+mona">mona</a></code>, <code><a href="#topic+mona.object">mona.object</a></code>, <code><a href="base.html#topic+print">print</a></code>,
<code><a href="base.html#topic+print.default">print.default</a></code>.
</p>

<hr>
<h2 id='print.pam'>Print Method for PAM Objects</h2><span id='topic+print.pam'></span>

<h3>Description</h3>

<p>Prints the medoids, clustering vector and objective function
of <code>pam</code> object.
</p>
<p>This is a method for the function <code><a href="base.html#topic+print">print</a>()</code> for objects
inheriting from class <code><a href="#topic+pam">pam</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pam'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.pam_+3A_x">x</code></td>
<td>
<p>a pam object.</p>
</td></tr>
<tr><td><code id="print.pam_+3A_...">...</code></td>
<td>
<p>potential further arguments (require by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+pam">pam</a></code>, <code><a href="#topic+pam.object">pam.object</a></code>, <code><a href="base.html#topic+print">print</a></code>,
<code><a href="base.html#topic+print.default">print.default</a></code>.
</p>

<hr>
<h2 id='ruspini'>Ruspini Data</h2><span id='topic+ruspini'></span>

<h3>Description</h3>

<p>The Ruspini data set, consisting of 75 points in four groups that is
popular for illustrating clustering techniques.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ruspini)</code></pre>


<h3>Format</h3>

<p>A data frame with 75 observations on 2 variables giving the x and y
coordinates of the points, respectively.
</p>


<h3>Source</h3>

<p>E. H. Ruspini (1970)
Numerical methods for fuzzy clustering.
<em>Inform. Sci.</em> <b>2</b>, 319&ndash;350.
</p>


<h3>References</h3>

<p>see those in <code><a href="#topic+agnes">agnes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ruspini)

## Plot similar to Figure 4 in Stryuf et al (1996)
## Not run: plot(pam(ruspini, 4), ask = TRUE)


## Plot similar to Figure 6 in Stryuf et al (1996)
plot(fanny(ruspini, 5))
</code></pre>

<hr>
<h2 id='silhouette'>Compute or Extract Silhouette Information from Clustering</h2><span id='topic+silhouette'></span><span id='topic+silhouette.clara'></span><span id='topic+silhouette.default'></span><span id='topic+silhouette.partition'></span><span id='topic+sortSilhouette'></span><span id='topic+summary.silhouette'></span><span id='topic+print.summary.silhouette'></span><span id='topic+plot.silhouette'></span>

<h3>Description</h3>

<p>Compute silhouette information according to a given clustering in
<code class="reqn">k</code> clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>silhouette(x, ...)
## Default S3 method:
  silhouette(x, dist, dmatrix, ...)
## S3 method for class 'partition'
silhouette(x, ...)
## S3 method for class 'clara'
silhouette(x, full = FALSE, subset = NULL, ...)

sortSilhouette(object, ...)
## S3 method for class 'silhouette'
summary(object, FUN = mean, ...)
## S3 method for class 'silhouette'
plot(x, nmax.lab = 40, max.strlen = 5,
     main = NULL, sub = NULL, xlab = expression("Silhouette width "* s[i]),
     col = "gray",  do.col.sort = length(col) &gt; 1, border = 0,
     cex.names = par("cex.axis"), do.n.k = TRUE, do.clus.stat = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="silhouette_+3A_x">x</code></td>
<td>
<p>an object of appropriate class; for the <code>default</code>
method an integer vector with <code class="reqn">k</code> different integer cluster
codes or a list with such an <code>x$clustering</code>
component.  Note that silhouette statistics are only defined if
<code class="reqn">2 \le k \le n-1</code>.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_dist">dist</code></td>
<td>
<p>a dissimilarity object inheriting from class
<code><a href="stats.html#topic+dist">dist</a></code> or coercible to one.  If not specified,
<code>dmatrix</code> must be.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_dmatrix">dmatrix</code></td>
<td>
<p>a symmetric dissimilarity matrix (<code class="reqn">n \times n</code>),
specified instead of <code>dist</code>, which can be more efficient.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_full">full</code></td>
<td>
<p>logical or number in <code class="reqn">[0,1]</code> specifying if a <em>full</em>
silhouette should be computed for <code><a href="#topic+clara">clara</a></code> object.  When a
number, say <code class="reqn">f</code>, for a random <code><a href="base.html#topic+sample.int">sample.int</a>(n, size = f*n)</code>
of the data the silhouette values are computed.
This requires <code class="reqn">O((f*n)^2)</code> memory, since the full dissimilarity of
the (sub)sample (see <code><a href="#topic+daisy">daisy</a></code>) is needed internally.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_subset">subset</code></td>
<td>
<p>a subset from <code>1:n</code>, specified instead of <code>full</code>
to specify the indices of the observations to be used for the silhouette
computations.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_object">object</code></td>
<td>
<p>an object of class <code>silhouette</code>.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_...">...</code></td>
<td>
<p>further arguments passed to and from methods.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_fun">FUN</code></td>
<td>
<p>function used to summarize silhouette widths.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_nmax.lab">nmax.lab</code></td>
<td>
<p>integer indicating the number of labels which is
considered too large for single-name labeling the silhouette plot.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_max.strlen">max.strlen</code></td>
<td>
<p>positive integer giving the length to which
strings are truncated in silhouette plot labeling.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_main">main</code>, <code id="silhouette_+3A_sub">sub</code>, <code id="silhouette_+3A_xlab">xlab</code></td>
<td>
<p>arguments to <code><a href="graphics.html#topic+title">title</a></code>; have a
sensible non-NULL default here.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_col">col</code>, <code id="silhouette_+3A_border">border</code>, <code id="silhouette_+3A_cex.names">cex.names</code></td>
<td>
<p>arguments passed
<code><a href="graphics.html#topic+barplot">barplot</a>()</code>; note that the default used to be <code>col
      = heat.colors(n), border = par("fg")</code> instead.<br />
<code>col</code> can also be a color vector of length <code class="reqn">k</code> for
clusterwise coloring, see also <code>do.col.sort</code>:
</p>
</td></tr>
<tr><td><code id="silhouette_+3A_do.col.sort">do.col.sort</code></td>
<td>
<p>logical indicating if the colors <code>col</code> should
be sorted &ldquo;along&rdquo; the silhouette; this is useful for casewise or
clusterwise coloring.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_do.n.k">do.n.k</code></td>
<td>
<p>logical indicating if <code class="reqn">n</code> and <code class="reqn">k</code> &ldquo;title text&rdquo;
should be written.</p>
</td></tr>
<tr><td><code id="silhouette_+3A_do.clus.stat">do.clus.stat</code></td>
<td>
<p>logical indicating if cluster size and averages
should be written right to the silhouettes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each observation i, the <em>silhouette width</em> <code class="reqn">s(i)</code> is
defined as follows: <br />
Put a(i) = average dissimilarity between i and all other points of the
cluster to which i belongs (if i is the <em>only</em> observation in
its cluster, <code class="reqn">s(i) := 0</code> without further calculations).
For all <em>other</em> clusters C, put <code class="reqn">d(i,C)</code> = average
dissimilarity of i to all observations of C.  The smallest of these
<code class="reqn">d(i,C)</code> is <code class="reqn">b(i) := \min_C d(i,C)</code>,
and can be seen as the dissimilarity between i and its &ldquo;neighbor&rdquo;
cluster, i.e., the nearest one to which it does <em>not</em> belong.
Finally, </p>
<p style="text-align: center;"><code class="reqn">s(i) := \frac{b(i) - a(i) }{max(a(i), b(i))}.</code>
</p>

<p><code>silhouette.default()</code> is now based on C code donated by Romain
Francois (the R version being still available as <code>cluster:::silhouetteR</code>).
</p>
<p>Observations with a large <code class="reqn">s(i)</code> (almost 1) are very well
clustered, a small <code class="reqn">s(i)</code> (around 0) means that the observation
lies between two clusters, and observations with a negative
<code class="reqn">s(i)</code> are probably placed in the wrong cluster.
</p>


<h3>Value</h3>

<p><code>silhouette()</code> returns an object, <code>sil</code>, of class
<code>silhouette</code> which is an <code class="reqn">n \times 3</code> matrix with
attributes.  For each observation i, <code>sil[i,]</code> contains the
cluster to which i belongs as well as the neighbor cluster of i (the
cluster, not containing i, for which the average dissimilarity between its
observations and i is minimal), and the silhouette width <code class="reqn">s(i)</code> of
the observation.  The <code><a href="base.html#topic+colnames">colnames</a></code> correspondingly are
<code>c("cluster", "neighbor", "sil_width")</code>.
</p>
<p><code>summary(sil)</code> returns an object of class
<code>summary.silhouette</code>, a list with components
</p>

<dl>
<dt><code>si.summary</code>:</dt><dd><p>numerical <code><a href="base.html#topic+summary">summary</a></code> of the
individual silhouette widths <code class="reqn">s(i)</code>.</p>
</dd>
<dt><code>clus.avg.widths</code>:</dt><dd><p>numeric (rank 1) array of clusterwise
<em>means</em> of silhouette widths where <code>mean = FUN</code> is used.</p>
</dd>
<dt><code>avg.width</code>:</dt><dd><p>the total mean <code>FUN(s)</code> where
<code>s</code> are the individual silhouette widths.</p>
</dd>
<dt><code>clus.sizes</code>:</dt><dd><p><code><a href="base.html#topic+table">table</a></code> of the <code class="reqn">k</code> cluster sizes.</p>
</dd>
<dt><code>call</code>:</dt><dd><p>if available, the <code><a href="base.html#topic+call">call</a></code> creating <code>sil</code>.</p>
</dd>
<dt><code>Ordered</code>:</dt><dd><p>logical identical to <code>attr(sil, "Ordered")</code>,
see below.</p>
</dd>
</dl>

<p><code>sortSilhouette(sil)</code> orders the rows of <code>sil</code> as in the
silhouette plot, by cluster (increasingly) and decreasing silhouette
width <code class="reqn">s(i)</code>.
<br />
<code>attr(sil, "Ordered")</code> is a logical indicating if <code>sil</code> <em>is</em>
ordered as by <code>sortSilhouette()</code>.  In that case,
<code>rownames(sil)</code> will contain case labels or numbers, and <br />
<code>attr(sil, "iOrd")</code> the ordering index vector.
</p>


<h3>Note</h3>

<p>While <code>silhouette()</code> is <em>intrinsic</em> to the
<code><a href="#topic+partition">partition</a></code> clusterings, and hence has a (trivial) method
for these, it is straightforward to get silhouettes from hierarchical
clusterings from <code>silhouette.default()</code> with
<code><a href="stats.html#topic+cutree">cutree</a>()</code> and distance as input.
</p>
<p>By default, for <code><a href="#topic+clara">clara</a>()</code> partitions, the silhouette is
just for the best random <em>subset</em> used.  Use <code>full = TRUE</code>
to compute (and later possibly plot) the full silhouette.
</p>


<h3>References</h3>

<p>Rousseeuw, P.J. (1987)
Silhouettes: A graphical aid to the interpretation and validation of
cluster analysis. <em>J. Comput. Appl. Math.</em>, <b>20</b>, 53&ndash;65.
</p>
<p>chapter 2 of Kaufman and Rousseeuw (1990), see
the references in <code><a href="#topic+plot.agnes">plot.agnes</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+partition.object">partition.object</a></code>, <code><a href="#topic+plot.partition">plot.partition</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ruspini)
pr4 &lt;- pam(ruspini, 4)
str(si &lt;- silhouette(pr4))
(ssi &lt;- summary(si))
plot(si) # silhouette plot
plot(si, col = c("red", "green", "blue", "purple"))# with cluster-wise coloring

si2 &lt;- silhouette(pr4$clustering, dist(ruspini, "canberra"))
summary(si2) # has small values: "canberra"'s fault
plot(si2, nmax= 80, cex.names=0.6)

op &lt;- par(mfrow= c(3,2), oma= c(0,0, 3, 0),
          mgp= c(1.6,.8,0), mar= .1+c(4,2,2,2))
for(k in 2:6)
   plot(silhouette(pam(ruspini, k=k)), main = paste("k = ",k), do.n.k=FALSE)
mtext("PAM(Ruspini) as in Kaufman &amp; Rousseeuw, p.101",
      outer = TRUE, font = par("font.main"), cex = par("cex.main")); frame()

## the same with cluster-wise colours:
c6 &lt;- c("tomato", "forest green", "dark blue", "purple2", "goldenrod4", "gray20")
for(k in 2:6)
   plot(silhouette(pam(ruspini, k=k)), main = paste("k = ",k), do.n.k=FALSE,
        col = c6[1:k])
par(op)

## clara(): standard silhouette is just for the best random subset
data(xclara)
set.seed(7)
str(xc1k &lt;- xclara[ sample(nrow(xclara), size = 1000) ,]) # rownames == indices
cl3 &lt;- clara(xc1k, 3)
plot(silhouette(cl3))# only of the "best" subset of 46
## The full silhouette: internally needs large (36 MB) dist object:
sf &lt;- silhouette(cl3, full = TRUE) ## this is the same as
s.full &lt;- silhouette(cl3$clustering, daisy(xc1k))
stopifnot(all.equal(sf, s.full, check.attributes = FALSE, tolerance = 0))
## color dependent on original "3 groups of each 1000": % __FIXME ??__
plot(sf, col = 2+ as.integer(names(cl3$clustering) ) %/% 1000,
     main ="plot(silhouette(clara(.), full = TRUE))")

## Silhouette for a hierarchical clustering:
ar &lt;- agnes(ruspini)
si3 &lt;- silhouette(cutree(ar, k = 5), # k = 4 gave the same as pam() above
    	           daisy(ruspini))
stopifnot(is.data.frame(di3 &lt;- as.data.frame(si3)))
plot(si3, nmax = 80, cex.names = 0.5)
## 2 groups: Agnes() wasn't too good:
si4 &lt;- silhouette(cutree(ar, k = 2), daisy(ruspini))
plot(si4, nmax = 80, cex.names = 0.5)
</code></pre>

<hr>
<h2 id='sizeDiss'>Sample Size of Dissimilarity Like Object</h2><span id='topic+sizeDiss'></span>

<h3>Description</h3>

<p>Returns the number of observations (<em>sample size</em>) corresponding
to a  dissimilarity like object, or equivalently,
the number of rows or columns of a matrix
when only the lower or upper triangular part (without diagonal) is given.
</p>
<p>It is nothing else but the inverse function of <code class="reqn">f(n) = n(n-1)/2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sizeDiss(d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sizeDiss_+3A_d">d</code></td>
<td>
<p>any <span class="rlang"><b>R</b></span> object with length (typically) <code class="reqn">n(n-1)/2</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a number; <code class="reqn">n</code> if <code>length(d) == n(n-1)/2</code>, <code>NA</code> otherwise.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dissimilarity.object">dissimilarity.object</a></code> and also
<code><a href="stats.html#topic+as.dist">as.dist</a></code> for class <code>dissimilarity</code> and
<code>dist</code> objects which have a <code>Size</code> attribute.</p>


<h3>Examples</h3>

<pre><code class='language-R'>sizeDiss(1:10)# 5, since 10 == 5 * (5 - 1) / 2
sizeDiss(1:9) # NA

n &lt;- 1:100
stopifnot(n == sapply( n*(n-1)/2, function(n) sizeDiss(logical(n))))
</code></pre>

<hr>
<h2 id='summary.agnes'>Summary Method for 'agnes' Objects</h2><span id='topic+summary.agnes'></span><span id='topic+print.summary.agnes'></span>

<h3>Description</h3>

<p>Returns (and prints) a summary list for an <code>agnes</code> object.
Printing gives more output than the corresponding
<code><a href="#topic+print.agnes">print.agnes</a></code> method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'agnes'
summary(object, ...)
## S3 method for class 'summary.agnes'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.agnes_+3A_x">x</code>, <code id="summary.agnes_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+agnes">agnes</a></code> object.</p>
</td></tr>
<tr><td><code id="summary.agnes_+3A_...">...</code></td>
<td>
<p>potential further arguments (require by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code>, <code><a href="#topic+agnes.object">agnes.object</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(agriculture)
summary(agnes(agriculture))
</code></pre>

<hr>
<h2 id='summary.clara'>Summary Method for 'clara' Objects</h2><span id='topic+summary.clara'></span><span id='topic+print.summary.clara'></span>

<h3>Description</h3>

<p>Returns (and prints) a summary list for a <code>clara</code> object.
Printing gives more output than the corresponding
<code><a href="#topic+print.clara">print.clara</a></code> method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'clara'
summary(object, ...)
## S3 method for class 'summary.clara'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.clara_+3A_x">x</code>, <code id="summary.clara_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+clara">clara</a></code> object.</p>
</td></tr>
<tr><td><code id="summary.clara_+3A_...">...</code></td>
<td>
<p>potential further arguments (require by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+clara.object">clara.object</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 2000 objects, divided into 5 clusters.
set.seed(47)
x &lt;- rbind(cbind(rnorm(400, 0,4), rnorm(400, 0,4)),
           cbind(rnorm(400,10,8), rnorm(400,40,6)),
           cbind(rnorm(400,30,4), rnorm(400, 0,4)),
           cbind(rnorm(400,40,4), rnorm(400,20,2)),
           cbind(rnorm(400,50,4), rnorm(400,50,4))
)
clx5 &lt;- clara(x, 5)
## Mis'classification' table:
table(rep(1:5, rep(400,5)), clx5$clust) # -&gt; 1 "error"
summary(clx5)

## Graphically:
par(mfrow = c(3,1), mgp = c(1.5, 0.6, 0), mar = par("mar") - c(0,0,2,0))
plot(x, col = rep(2:6, rep(400,5)))
plot(clx5)
</code></pre>

<hr>
<h2 id='summary.diana'>Summary Method for 'diana' Objects</h2><span id='topic+summary.diana'></span><span id='topic+print.summary.diana'></span>

<h3>Description</h3>

<p>Returns (and prints) a summary list for a <code>diana</code> object.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'diana'
summary(object, ...)
## S3 method for class 'summary.diana'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.diana_+3A_x">x</code>, <code id="summary.diana_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+diana">diana</a></code> object.</p>
</td></tr>
<tr><td><code id="summary.diana_+3A_...">...</code></td>
<td>
<p>potential further arguments (require by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+diana">diana</a></code>, <code><a href="#topic+diana.object">diana.object</a></code>.</p>

<hr>
<h2 id='summary.mona'>Summary Method for 'mona' Objects</h2><span id='topic+summary.mona'></span><span id='topic+print.summary.mona'></span>

<h3>Description</h3>

<p>Returns (and prints) a summary list for a <code>mona</code> object.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mona'
summary(object, ...)
## S3 method for class 'summary.mona'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.mona_+3A_x">x</code>, <code id="summary.mona_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+mona">mona</a></code> object.</p>
</td></tr>
<tr><td><code id="summary.mona_+3A_...">...</code></td>
<td>
<p>potential further arguments (require by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+mona">mona</a></code>, <code><a href="#topic+mona.object">mona.object</a></code>.</p>

<hr>
<h2 id='summary.pam'>Summary Method for PAM Objects</h2><span id='topic+summary.pam'></span><span id='topic+print.summary.pam'></span>

<h3>Description</h3>

<p>Summarize a <code><a href="#topic+pam">pam</a></code> object and return an object
of class <code>summary.pam</code>.
There's a <code><a href="base.html#topic+print">print</a></code> method for the latter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pam'
summary(object, ...)
## S3 method for class 'summary.pam'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.pam_+3A_x">x</code>, <code id="summary.pam_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+pam">pam</a></code> object.</p>
</td></tr>
<tr><td><code id="summary.pam_+3A_...">...</code></td>
<td>
<p>potential further arguments (require by generic).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+pam">pam</a></code>, <code><a href="#topic+pam.object">pam.object</a></code>.
</p>

<hr>
<h2 id='twins.object'>Hierarchical Clustering Object</h2><span id='topic+twins.object'></span><span id='topic+twins'></span>

<h3>Description</h3>

<p>The objects of class <code>"twins"</code> represent an agglomerative or
divisive (polythetic) hierarchical clustering of a dataset.
</p>


<h3>Value</h3>

<p>See <code><a href="#topic+agnes.object">agnes.object</a></code> and <code><a href="#topic+diana.object">diana.object</a></code> for details.
</p>


<h3>GENERATION</h3>

<p>This class of objects is returned from <code>agnes</code> or <code>diana</code>.
</p>


<h3>METHODS</h3>

<p>The <code>"twins"</code> class has a method for the following generic function:
<code>pltree</code>.
</p>


<h3>INHERITANCE</h3>

<p>The following classes inherit from class <code>"twins"</code> :
<code>"agnes"</code> and <code>"diana"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+agnes">agnes</a></code>,<code><a href="#topic+diana">diana</a></code>.
</p>

<hr>
<h2 id='volume.ellipsoid'>Compute the Volume (of an Ellipsoid)</h2><span id='topic+volume'></span><span id='topic+volume.ellipsoid'></span>

<h3>Description</h3>

<p>Compute the volume of geometric <span class="rlang"><b>R</b></span> object.
This is a generic function and has a method for <code>ellipsoid</code> objects
(typically resulting from <code><a href="#topic+ellipsoidhull">ellipsoidhull</a>()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>volume(object, ...)
## S3 method for class 'ellipsoid'
volume(object, log = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="volume.ellipsoid_+3A_object">object</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object the volume of which is wanted; for the
<code>ellipsoid</code> method, an object of that class (see
<code><a href="#topic+ellipsoidhull">ellipsoidhull</a></code> or the example below).</p>
</td></tr>
<tr><td><code id="volume.ellipsoid_+3A_log">log</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating if the volume should be
returned in log scale.  Maybe needed in largish dimensions.</p>
</td></tr>
<tr><td><code id="volume.ellipsoid_+3A_...">...</code></td>
<td>
<p>potential further arguments of methods, e.g. <code>log</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a number, the volume <code class="reqn">V</code> (or <code class="reqn">\log(V)</code> if <code>log = TRUE</code>) of
the given <code>object</code>.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler (2002, extracting from former <code><a href="#topic+clusplot">clusplot</a></code> code);
Keefe Murphy (2019) provided code for dimensions <code class="reqn">d &gt; 2</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ellipsoidhull">ellipsoidhull</a></code> for spanning ellipsoid computation.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example(ellipsoidhull) # which defines 'ellipsoid' object &lt;namefoo&gt;

myEl &lt;- structure(list(cov = rbind(c(3,1),1:2), loc = c(0,0), d2 = 10),
                   class = "ellipsoid")
volume(myEl)# i.e. "area" here (d = 2)
myEl # also mentions the "volume"

set.seed(1)
d5 &lt;- matrix(rt(500, df=3), 100,5)
e5 &lt;- ellipsoidhull(d5)
</code></pre>

<hr>
<h2 id='votes.repub'>Votes for Republican Candidate in Presidential Elections</h2><span id='topic+votes.repub'></span>

<h3>Description</h3>

<p>A data frame with the percents of votes given to the republican
candidate in presidential elections from 1856 to 1976.  Rows
represent the 50 states, and columns the 31 elections.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(votes.repub)</code></pre>


<h3>Source</h3>

<p>S. Peterson (1973):
<em>A Statistical History of the American Presidential Elections</em>.
New York: Frederick Ungar Publishing Co.
</p>
<p>Data from 1964 to 1976 is from R. M. Scammon, 
<em>American Votes 12</em>, Congressional Quarterly.
</p>

<hr>
<h2 id='xclara'>Bivariate Data Set with 3 Clusters</h2><span id='topic+xclara'></span>

<h3>Description</h3>

<p>An artificial data set consisting of 3000 points in 3 quite well-separated
clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(xclara)</code></pre>


<h3>Format</h3>

<p>A data frame with 3000 observations on 2 numeric variables (named
<code>V1</code> and <code>V2</code>) giving the
<code class="reqn">x</code> and <code class="reqn">y</code> coordinates of the points, respectively.
</p>


<h3>Note</h3>

<p>Our version of the <code>xclara</code> is slightly more rounded than the one
from <code><a href="utils.html#topic+read.table">read.table</a>("xclara.dat")</code> and the relative
difference measured by <code><a href="Matrix.html#topic+all.equal">all.equal</a></code> is <code>1.15e-7</code> for
<code>V1</code> and <code>1.17e-7</code> for <code>V2</code> which suggests that our
version has been the result of a <code><a href="base.html#topic+options">options</a>(digits = 7)</code>
formatting.
</p>
<p>Previously (before May 2017), it was claimed the three cluster were
each of size 1000, which is clearly wrong.  <code><a href="#topic+pam">pam</a>(*, 3)</code>
gives cluster sizes of 899, 1149, and 952, which apart from seven
&ldquo;outliers&rdquo; (or &ldquo;mislabellings&rdquo;) correspond to
observation indices <code class="reqn">\{1:900\}</code>, <code class="reqn">\{901:2050\}</code>, and
<code class="reqn">\{2051:3000\}</code>, see the example.
</p>


<h3>Source</h3>

<p>Sample data set accompanying the reference below (file
&lsquo;<span class="file">xclara.dat</span>&rsquo; in side &lsquo;<span class="file">clus_examples.tar.gz</span>&rsquo;).
</p>


<h3>References</h3>

<p>Anja Struyf, Mia Hubert &amp; Peter J. Rousseeuw (1996)
Clustering in an Object-Oriented Environment.
<em>Journal of Statistical Software</em> <b>1</b>.
<a href="https://doi.org/10.18637/jss.v001.i04">doi:10.18637/jss.v001.i04</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Visualization: Assuming groups are defined as {1:1000}, {1001:2000}, {2001:3000}
plot(xclara, cex = 3/4, col = rep(1:3, each=1000))
p.ID &lt;- c(78, 1411, 2535) ## PAM's medoid indices  == pam(xclara, 3)$id.med
text(xclara[p.ID,], labels = 1:3, cex=2, col=1:3)

 px &lt;- pam(xclara, 3) ## takes ~2 seconds
 cxcl &lt;- px$clustering ; iCl &lt;- split(seq_along(cxcl), cxcl)
 boxplot(iCl, range = 0.7, horizontal=TRUE,
         main = "Indices of the 3 clusters of  pam(xclara, 3)")

 ## Look more closely now:
 bxCl &lt;- boxplot(iCl, range = 0.7, plot=FALSE)
 ## We see 3 + 2 + 2 = 7  clear "outlier"s  or "wrong group" observations:
 with(bxCl, rbind(out, group))
 ## out   1038 1451 1610   30  327  562  770
 ## group    1    1    1    2    2    3    3
 ## Apart from these, what are the robust ranges of indices? -- Robust range:
 t(iR &lt;- bxCl$stats[c(1,5),])
 ##    1  900
 ##  901 2050
 ## 2051 3000
 gc &lt;- adjustcolor("gray20",1/2)
 abline(v = iR, col = gc, lty=3)
 axis(3, at = c(0, iR[2,]), padj = 1.2, col=gc, col.axis=gc)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
