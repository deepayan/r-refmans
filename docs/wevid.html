<!DOCTYPE html><html><head><title>Help for package wevid</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {wevid}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#plotcumfreqs'><p>Plot the cumulative frequency distributions in cases and in controls</p></a></li>
<li><a href='#plotroc'><p>Plot crude and model-based ROC curves</p></a></li>
<li><a href='#plotWdists'><p>Plot the distribution of the weight of evidence in cases and in controls</p></a></li>
<li><a href='#prop.belowthreshold'><p>Proportions of cases and controls below a threshold of weight of evidence</p></a></li>
<li><a href='#recalibrate.p'><p>Recalibrate posterior probabilities</p></a></li>
<li><a href='#summary-densities'><p>Summary evaluation of predictive performance</p></a></li>
<li><a href='#Wdensities'><p>Compute densities of weights of evidence in cases and controls</p></a></li>
<li><a href='#Wdensities.crude'><p>Calculate the crude smoothed densities of W in cases and in controls</p></a></li>
<li><a href='#weightsofevidence'><p>Calculate weights of evidence in natural log units</p></a></li>
<li><a href='#wevid-package'><p>Quantifying performance of a diagnostic test using the sampling distribution</p>
of the weight of evidence favouring case over noncase status</a></li>
<li><a href='#wevid.datasets'><p>Example datasets</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Quantifying Performance of a Binary Classifier Through Weight of
Evidence</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-09-12</td>
</tr>
<tr>
<td>Description:</td>
<td>The distributions of the weight of evidence (log Bayes factor) favouring case over noncase status in a test dataset (or test folds generated by cross-validation) can be used to quantify the performance of a diagnostic test (McKeigue (2019), &lt;<a href="https://doi.org/10.1177%2F0962280218776989">doi:10.1177/0962280218776989</a>&gt;). The package can be used with any test dataset on which you have observed case-control status and have computed prior and posterior probabilities of case status using a model learned on a training dataset. To quantify how the predictor will behave as a risk stratifier, the quantiles of the distributions of weight of evidence in cases and controls can be calculated and plotted.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www.homepages.ed.ac.uk/pmckeigu/preprints/classify/wevidtutorial.html">http://www.homepages.ed.ac.uk/pmckeigu/preprints/classify/wevidtutorial.html</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2, mclust, pROC (&ge; 1.9), reshape2, zoo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 2.0.0)</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-09-12 15:18:09 UTC; mcolombo</td>
</tr>
<tr>
<td>Author:</td>
<td>Paul McKeigue <a href="https://orcid.org/0000-0002-5217-1034"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Marco Colombo <a href="https://orcid.org/0000-0001-6672-0623"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marco Colombo &lt;mar.colombo13@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-09-12 15:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='plotcumfreqs'>Plot the cumulative frequency distributions in cases and in controls</h2><span id='topic+plotcumfreqs'></span>

<h3>Description</h3>

<p>Plot the cumulative frequency distributions in cases and in controls
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotcumfreqs(densities)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotcumfreqs_+3A_densities">densities</code></td>
<td>
<p>Densities object produced by <code><a href="#topic+Wdensities">Wdensities</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object representing the cumulative frequency distributions of the
smoothed densities of the weights of evidence in cases and in controls.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cleveland)
densities &lt;- with(cleveland, Wdensities(y, posterior.p, prior.p))
plotcumfreqs(densities)

</code></pre>

<hr>
<h2 id='plotroc'>Plot crude and model-based ROC curves</h2><span id='topic+plotroc'></span>

<h3>Description</h3>

<p>While the crude ROC curve can be non-concave and is generally not smooth,
the model-based ROC curve is always concave, as the corresponding densities
have been adjusted to be mathematically consistent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotroc(densities)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotroc_+3A_densities">densities</code></td>
<td>
<p>Densities object produced by <code><a href="#topic+Wdensities">Wdensities</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object representing crude and model-based ROC curves.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cleveland)
densities &lt;- with(cleveland, Wdensities(y, posterior.p, prior.p))
plotroc(densities)

</code></pre>

<hr>
<h2 id='plotWdists'>Plot the distribution of the weight of evidence in cases and in controls</h2><span id='topic+plotWdists'></span>

<h3>Description</h3>

<p>Plot the distribution of the weight of evidence in cases and in controls
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotWdists(densities, distlabels = c("Crude", "Model-based"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotWdists_+3A_densities">densities</code></td>
<td>
<p>Densities object produced by <code><a href="#topic+Wdensities">Wdensities</a></code>.</p>
</td></tr>
<tr><td><code id="plotWdists_+3A_distlabels">distlabels</code></td>
<td>
<p>Character vector of length 2 to be used to label the crude
and the model-based curves (in that order).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object representing the distributions of crude and model-based
weights of evidence in cases and in controls.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cleveland)
densities &lt;- with(cleveland, Wdensities(y, posterior.p, prior.p))
plotWdists(densities)

# Example which requires fitting a mixture distribution
data(fitonly)
densities &lt;- with(fitonly, Wdensities(y, posterior.p, prior.p))

# truncate spike
plotWdists(densities) + ggplot2::scale_y_continuous(limits=c(0, 0.5))

</code></pre>

<hr>
<h2 id='prop.belowthreshold'>Proportions of cases and controls below a threshold of weight of evidence</h2><span id='topic+prop.belowthreshold'></span>

<h3>Description</h3>

<p>Proportions of cases and controls below a threshold of weight of evidence
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prop.belowthreshold(densities, w.threshold)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop.belowthreshold_+3A_densities">densities</code></td>
<td>
<p>Densities object produced by <code><a href="#topic+Wdensities">Wdensities</a></code>.</p>
</td></tr>
<tr><td><code id="prop.belowthreshold_+3A_w.threshold">w.threshold</code></td>
<td>
<p>Threshold value of weight of evidence (natural logs).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector of length 2 listing the proportions of controls and
cases with weight of evidence below the given threshold.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cleveland)
densities &lt;- with(cleveland, Wdensities(y, posterior.p, prior.p))
w.threshold &lt;- log(4) # threshold Bayes factor of 4
prop.belowthreshold(densities, w.threshold)

</code></pre>

<hr>
<h2 id='recalibrate.p'>Recalibrate posterior probabilities</h2><span id='topic+recalibrate.p'></span>

<h3>Description</h3>

<p>Transforms posterior probabilities to logits, fits a logistic regression model
and returns the predictive probabilities from this model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recalibrate.p(y, posterior.p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recalibrate.p_+3A_y">y</code></td>
<td>
<p>Binary outcome label (0 for controls, 1 for cases).</p>
</td></tr>
<tr><td><code id="recalibrate.p_+3A_posterior.p">posterior.p</code></td>
<td>
<p>Vector of posterior probabilities.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Recalibrated posterior probabilities.
</p>

<hr>
<h2 id='summary-densities'>Summary evaluation of predictive performance</h2><span id='topic+summary-densities'></span><span id='topic+summary.Wdensities'></span><span id='topic+mean.Wdensities'></span><span id='topic+auroc.crude'></span><span id='topic+auroc.model'></span><span id='topic+lambda.crude'></span><span id='topic+lambda.model'></span>

<h3>Description</h3>

<p>Summary evaluation of predictive performance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Wdensities'
summary(object, ...)

## S3 method for class 'Wdensities'
mean(x, ...)

auroc.crude(densities)

auroc.model(densities)

lambda.crude(densities)

lambda.model(densities)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary-densities_+3A_object">object</code>, <code id="summary-densities_+3A_x">x</code>, <code id="summary-densities_+3A_densities">densities</code></td>
<td>
<p>Densities object produced by
<code><a href="#topic+Wdensities">Wdensities</a></code>.</p>
</td></tr>
<tr><td><code id="summary-densities_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods. These are
currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>summary</code> returns a data frame that reports the number of cases and
controls, the test log-likelihood, the crude and model-based C-statistic
and expected weight of evidence Lambda.
</p>
<p><code>mean</code> returns a numeric vector listing the mean densities of the weight
of evidence in controls and in cases.
</p>
<p><code>auroc.crude</code> and <code>auroc.model</code> return the area under the ROC curve
according to the crude and the model-based densities of weight of evidence,
respectively.
</p>
<p><code>lambda.crude</code> and <code>lambda.model</code> return the expected weight of
evidence (expected information for discrimination) in bits from the crude
and the model-based densities, respectively.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cleveland)
densities &lt;- with(cleveland, Wdensities(y, posterior.p, prior.p))

summary(densities)
mean(densities)
auroc.model(densities)
lambda.model(densities)

</code></pre>

<hr>
<h2 id='Wdensities'>Compute densities of weights of evidence in cases and controls</h2><span id='topic+Wdensities'></span>

<h3>Description</h3>

<p>The function computes smoothed densities of the weight of evidence in
cases and in controls from the crude probabilities, then adjusts them to
make them mathematically consistent so that p(W_ctrl) = exp(-W) p(W_case).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Wdensities(y, posterior.p, prior.p, range.xseq = c(-25, 25),
  x.stepsize = 0.01, adjust.bw = 1, recalibrate = TRUE,
  debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Wdensities_+3A_y">y</code></td>
<td>
<p>Binary outcome label (0 for controls, 1 for cases).</p>
</td></tr>
<tr><td><code id="Wdensities_+3A_posterior.p">posterior.p</code></td>
<td>
<p>Vector of posterior probabilities generated by using model
to predict on test data.</p>
</td></tr>
<tr><td><code id="Wdensities_+3A_prior.p">prior.p</code></td>
<td>
<p>Vector of prior probabilities.</p>
</td></tr>
<tr><td><code id="Wdensities_+3A_range.xseq">range.xseq</code></td>
<td>
<p>Range of points where the curves should be sampled.</p>
</td></tr>
<tr><td><code id="Wdensities_+3A_x.stepsize">x.stepsize</code></td>
<td>
<p>Distance between each point.</p>
</td></tr>
<tr><td><code id="Wdensities_+3A_adjust.bw">adjust.bw</code></td>
<td>
<p>Bandwidth adjustment for the Gaussian kernel density
estimator. By default it is set to 1 (no adjustment), setting it to
a value smaller/larger than 1 reduces/increases the smoothing of
the kernel. This argument is ignored if more than one mixture component
is identified.</p>
</td></tr>
<tr><td><code id="Wdensities_+3A_recalibrate">recalibrate</code></td>
<td>
<p>If <code>TRUE</code> (the default) the weights of evidence are
calculated after the posterior probabilities have been recalibrated
against <code>y</code> using a logistic regression model.</p>
</td></tr>
<tr><td><code id="Wdensities_+3A_debug">debug</code></td>
<td>
<p>If <code>TRUE</code>, the size of the adjustment is reported.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the sample distributions in cases and controls support a 2-component
mixture model (based on model comparison with BIC) for the densities, this
will be detected and a 2-component mixture model will be fitted before
adjustment.
</p>


<h3>Value</h3>

<p>A densities object that contains the information necessary to compute
summary measures and generate plots.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cleveland)
densities &lt;- with(cleveland, Wdensities(y, posterior.p, prior.p))

# Example which requires fitting a mixture distribution
data(fitonly)
densities &lt;- with(fitonly, Wdensities(y, posterior.p, prior.p))

</code></pre>

<hr>
<h2 id='Wdensities.crude'>Calculate the crude smoothed densities of W in cases and in controls</h2><span id='topic+Wdensities.crude'></span><span id='topic+Wdensities.mix'></span>

<h3>Description</h3>

<p>These functions allow to compute the smoothed densities of the weight of
evidence in cases and in controls from the crude probabilities.
<code>Wdensities.crude</code> is designed for the general case; if the model
probabilities reflect a spike-slab mixture distribution, where a high
proportion of values of the predictor are zero, then <code>Wdensities.mix</code>
will be more appropriate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Wdensities.crude(y, W, xseq, adjust.bw = 1)

Wdensities.mix(y, W, xseq, mixcomponent)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Wdensities.crude_+3A_y">y</code></td>
<td>
<p>Binary outcome label (0 for controls, 1 for cases).</p>
</td></tr>
<tr><td><code id="Wdensities.crude_+3A_w">W</code></td>
<td>
<p>Weight of evidence.</p>
</td></tr>
<tr><td><code id="Wdensities.crude_+3A_xseq">xseq</code></td>
<td>
<p>Sequence of points where the curves should be sampled.</p>
</td></tr>
<tr><td><code id="Wdensities.crude_+3A_adjust.bw">adjust.bw</code></td>
<td>
<p>Bandwidth adjustment.</p>
</td></tr>
<tr><td><code id="Wdensities.crude_+3A_mixcomponent">mixcomponent</code></td>
<td>
<p>integer vector same length as <code>y</code>, coded 1 or 2.
Typically set by <code>Wdensities</code> which calls this function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of crude densities for controls and cases sampled at each point in the
range provided.
</p>

<hr>
<h2 id='weightsofevidence'>Calculate weights of evidence in natural log units</h2><span id='topic+weightsofevidence'></span>

<h3>Description</h3>

<p>Calculate weights of evidence in natural log units
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightsofevidence(posterior.p, prior.p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightsofevidence_+3A_posterior.p">posterior.p</code></td>
<td>
<p>Vector of posterior probabilities generated by using model
to predict on test data.</p>
</td></tr>
<tr><td><code id="weightsofevidence_+3A_prior.p">prior.p</code></td>
<td>
<p>Vector of prior probabilities.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The weight of evidence in nats for each observation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cleveland) # load example dataset
W &lt;- with(cleveland, weightsofevidence(posterior.p, prior.p))

</code></pre>

<hr>
<h2 id='wevid-package'>Quantifying performance of a diagnostic test using the sampling distribution
of the weight of evidence favouring case over noncase status</h2><span id='topic+wevid'></span><span id='topic+wevid-package'></span>

<h3>Description</h3>

<p>The <span class="pkg">wevid</span> package provides functions for quantifying the performance
of a diagnostic test (or any other binary classifier) by calculating and
plotting the distributions in cases and noncases of the weight of evidence
favouring case over noncase status.
</p>


<h3>Details</h3>

<p>The distributions of the weight of evidence (log Bayes factor) favouring
case over noncase status in a test dataset (or test folds generated by
cross-validation) can be used to quantify the performance of a diagnostic
test.
</p>
<p>In comparison with the C-statistic (area under ROC curve), the expected
weight of evidence (expected information for discrimination) has several
advantages as a summary measure of predictive performance. To quantify how
the predictor will behave as a risk stratifier, the quantiles of the
distributions of weight of evidence in cases and controls can be calculated
and plotted.
</p>
<p>This package can be used with any test dataset on which you have observed
case-control status and have computed prior and posterior probabilities of
case status using a model learned on a training dataset.
Therefore, you should have computed on a test dataset (or on test folds used
for cross-validation):
</p>

<ol>
<li><p> The prior probability of case status (this may be just the frequency of
cases in the training data).
</p>
</li>
<li><p> The posterior probability of case status (using the model learned on
the training data to predict on the test data).
</p>
</li>
<li><p> The observed case status (coded as 0 for noncases, 1 for cases).
</p>
</li></ol>

<p>The main function of the package is <code><a href="#topic+Wdensities">Wdensities</a></code> which computes
the crude and model-based densities of weight of evidence in cases and
controls. Once these are computed, they can be plotted with
<code><a href="#topic+plotWdists">plotWdists</a></code> and <code><a href="#topic+plotcumfreqs">plotcumfreqs</a></code>. Summary statistics
can be reported with <code><a href="#topic+summary.Wdensities">summary</a></code>.
</p>


<h3>Author(s)</h3>

<p>Paul McKeigue <a href="mailto:paul.mckeigue@ed.ac.uk">paul.mckeigue@ed.ac.uk</a>
</p>


<h3>References</h3>

<p>Paul McKeigue (2019), Quantifying performance of a diagnostic test as the
expected information for discrimination: Relation to the C-statistic.
<em>Statistical Methods for Medical Research</em>, 28 (6), 1841-1851.
https://doi.org/10.1177/0962280218776989.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="http://www.homepages.ed.ac.uk/pmckeigu/preprints/classify/wevidtutorial.html">http://www.homepages.ed.ac.uk/pmckeigu/preprints/classify/wevidtutorial.html</a>
</p>
</li></ul>


<hr>
<h2 id='wevid.datasets'>Example datasets</h2><span id='topic+wevid.datasets'></span><span id='topic+cleveland'></span><span id='topic+pima'></span><span id='topic+fitonly'></span>

<h3>Description</h3>

<p>Example datasets
</p>
<p>The <span class="pkg">wevid</span> package comes with the following dataset:
</p>

<ul>
<li> <p><code>cleveland</code> is based on cross-validated prediction of coronary
disease in the Cleveland Heart Study (297 observations).
</p>
</li></ul>


<ul>
<li> <p><code>pima</code> is based on cross-validated prediction of diabetes
in Pima Native Americans (768 observations).
</p>
</li></ul>


<ul>
<li> <p><code>fitonly</code> is based on cross-validated prediction of colorectal
cancer from fecal immunochemical test (FIT) only in Michigan (242
observations). As most controls and some cases have have zero values
in the FIT test, to fit densities to the sampled values of weight of
evidence in controls and cases it is necessary to specify spike-slab
mixtures.
</p>
</li></ul>



<h3>Format</h3>

<p>Each dataset consists of a data frame with the following variables:
</p>

<ul>
<li><p> prior.p: Prior probabilities of case status.
</p>
</li>
<li><p> posterior.p: Posterior probabilities of case status.
</p>
</li>
<li><p> y: Case-control status.
</p>
</li></ul>


<h3>Source</h3>

<p><a href="http://www.homepages.ed.ac.uk/pmckeigu/preprints/classify/wevidtutorial.html">http://www.homepages.ed.ac.uk/pmckeigu/preprints/classify/wevidtutorial.html</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
