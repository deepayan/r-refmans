<!DOCTYPE html><html><head><title>Help for package corpustools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {corpustools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#add_multitoken_label'><p>Choose and add multitoken strings based on multitoken categories</p></a></li>
<li><a href='#agg_label'><p>Helper function for aggregate_rsyntax</p></a></li>
<li><a href='#agg_tcorpus'><p>Aggregate the tokens data</p></a></li>
<li><a href='#aggregate_rsyntax'><p>Aggregate rsyntax annotations</p></a></li>
<li><a href='#as.tcorpus'><p>Force an object to be a tCorpus class</p></a></li>
<li><a href='#as.tcorpus.default'><p>Force an object to be a tCorpus class</p></a></li>
<li><a href='#as.tcorpus.tCorpus'><p>Force an object to be a tCorpus class</p></a></li>
<li><a href='#backbone_filter'><p>Extract the backbone of a network.</p></a></li>
<li><a href='#browse_hits'><p>View hits in a browser</p></a></li>
<li><a href='#browse_texts'><p>Create and view a full text browser</p></a></li>
<li><a href='#calc_chi2'><p>Vectorized computation of chi^2 statistic for a 2x2 crosstab containing the values</p>
[a, b]
[c, d]</a></li>
<li><a href='#compare_corpus'><p>Compare tCorpus vocabulary to that of another (reference) tCorpus</p></a></li>
<li><a href='#compare_documents'><p>Calculate the similarity of documents</p></a></li>
<li><a href='#compare_subset'><p>Compare vocabulary of a subset of a tCorpus to the rest of the tCorpus</p></a></li>
<li><a href='#corenlp_tokens'><p>coreNLP example sentences</p></a></li>
<li><a href='#count_tcorpus'><p>Count results of search hits, or of a given feature in tokens</p></a></li>
<li><a href='#create_tcorpus'><p>Create a tCorpus</p></a></li>
<li><a href='#docfreq_filter'><p>Support function for subset method</p></a></li>
<li><a href='#dtm_compare'><p>Compare two document term matrices</p></a></li>
<li><a href='#dtm_wordcloud'><p>Plot a word cloud from a dtm</p></a></li>
<li><a href='#ego_semnet'><p>Create an ego network</p></a></li>
<li><a href='#export_span_annotations'><p>Export span annotations</p></a></li>
<li><a href='#feature_associations'><p>Get common nearby features given a query or query hits</p></a></li>
<li><a href='#feature_stats'><p>Feature statistics</p></a></li>
<li><a href='#fold_rsyntax'><p>Fold rsyntax annotations</p></a></li>
<li><a href='#freq_filter'><p>Support function for subset method</p></a></li>
<li><a href='#get_dtm'><p>Create a document term matrix.</p></a></li>
<li><a href='#get_global_i'><p>Compute global feature positions</p></a></li>
<li><a href='#get_kwic'><p>Get keyword-in-context (KWIC) strings</p></a></li>
<li><a href='#get_stopwords'><p>Get a character vector of stopwords</p></a></li>
<li><a href='#laplace'><p>Laplace (i.e. add constant) smoothing</p></a></li>
<li><a href='#melt_quanteda_dict'><p>Convert a quanteda dictionary to a long data.table format</p></a></li>
<li><a href='#merge_tcorpora'><p>Merge tCorpus objects</p></a></li>
<li><a href='#plot_semnet'><p>Visualize a semnet network</p></a></li>
<li><a href='#plot_words'><p>Plot a wordcloud with words ordered and coloured according to a dimension (x)</p></a></li>
<li><a href='#plot.contextHits'><p>S3 plot for contextHits class</p></a></li>
<li><a href='#plot.featureAssociations'><p>visualize feature associations</p></a></li>
<li><a href='#plot.featureHits'><p>S3 plot for featureHits class</p></a></li>
<li><a href='#plot.vocabularyComparison'><p>visualize vocabularyComparison</p></a></li>
<li><a href='#preprocess_tokens'><p>Preprocess tokens in a character vector</p></a></li>
<li><a href='#print.contextHits'><p>S3 print for contextHits class</p></a></li>
<li><a href='#print.featureHits'><p>S3 print for featureHits class</p></a></li>
<li><a href='#print.tCorpus'><p>S3 print for tCorpus class</p></a></li>
<li><a href='#refresh_tcorpus'><p>Refresh a tCorpus object using the current version of corpustools</p></a></li>
<li><a href='#require_package'><p>Check if package with given version exists</p></a></li>
<li><a href='#search_contexts'><p>Search for documents or sentences using Boolean queries</p></a></li>
<li><a href='#search_dictionary'><p>Dictionary lookup</p></a></li>
<li><a href='#search_features'><p>Find tokens using a Lucene-like search query</p></a></li>
<li><a href='#semnet'><p>Create a semantic network based on the co-occurence of tokens in documents</p></a></li>
<li><a href='#semnet_window'><p>Create a semantic network based on the co-occurence of tokens in token windows</p></a></li>
<li><a href='#set_network_attributes'><p>Set some default network attributes for pretty plotting</p></a></li>
<li><a href='#sgt'><p>Simple Good Turing smoothing</p></a></li>
<li><a href='#show_udpipe_models'><p>Show the names of udpipe models</p></a></li>
<li><a href='#sotu_texts'><p>State of the Union addresses</p></a></li>
<li><a href='#stopwords_list'><p>Basic stopword lists</p></a></li>
<li><a href='#subset_query'><p>Subset tCorpus token data using a query</p></a></li>
<li><a href='#subset.tCorpus'><p>S3 subset for tCorpus class</p></a></li>
<li><a href='#summary.contextHits'><p>S3 summary for contextHits class</p></a></li>
<li><a href='#summary.featureHits'><p>S3 summary for featureHits class</p></a></li>
<li><a href='#summary.tCorpus'><p>Summary of a tCorpus object</p></a></li>
<li><a href='#tc_plot_tree'><p>Visualize a dependency tree</p></a></li>
<li><a href='#tc_sotu_udpipe'><p>A tCorpus with a small sample of sotu paragraphs parsed with udpipe</p></a></li>
<li><a href='#tCorpus'><p>tCorpus: a corpus class for tokenized texts</p></a></li>
<li><a href='#tCorpus_compare'><p>Corpus comparison</p></a></li>
<li><a href='#tCorpus_create'><p>Creating a tCorpus</p></a></li>
<li><a href='#tCorpus_data'><p>Methods and functions for viewing, modifying and subsetting tCorpus data</p></a></li>
<li><a href='#tCorpus_docsim'><p>Document similarity</p></a></li>
<li><a href='#tCorpus_features'><p>Preprocessing, subsetting and analyzing features</p></a></li>
<li><a href='#tCorpus_modify_by_reference'><p>Modify tCorpus by reference</p></a></li>
<li><a href='#tCorpus_querying'><p>Use Boolean queries to analyze the tCorpus</p></a></li>
<li><a href='#tCorpus_semnet'><p>Feature co-occurrence based semantic network analysis</p></a></li>
<li><a href='#tCorpus_topmod'><p>Topic modeling</p></a></li>
<li><a href='#tCorpus$annotate_rsyntax'><p>Annotate tokens based on rsyntax queries</p></a></li>
<li><a href='#tCorpus$code_dictionary'><p>Dictionary lookup</p></a></li>
<li><a href='#tCorpus$code_features'><p>Code features in a tCorpus based on a search string</p></a></li>
<li><a href='#tCorpus$context'><p>Get a context vector</p></a></li>
<li><a href='#tCorpus$deduplicate'><p>Deduplicate documents</p></a></li>
<li><a href='#tCorpus$delete_columns'><p>Delete column from the data and meta data</p></a></li>
<li><a href='#tCorpus$feats_to_columns'><p>Cast the &quot;feats&quot; column in UDpipe tokens to columns</p></a></li>
<li><a href='#tCorpus$feature_subset'><p>Filter features</p></a></li>
<li><a href='#tCorpus$fold_rsyntax'><p>Fold rsyntax annotations</p></a></li>
<li><a href='#tCorpus$get'><p>Access the data from a tCorpus</p></a></li>
<li><a href='#tCorpus$lda_fit'><p>Estimate a LDA topic model</p></a></li>
<li><a href='#tCorpus$merge'><p>Merge the token and meta data.tables of a tCorpus with another data.frame</p></a></li>
<li><a href='#tCorpus$preprocess'><p>Preprocess feature</p></a></li>
<li><a href='#tCorpus$replace_dictionary'><p>Replace tokens with dictionary match</p></a></li>
<li><a href='#tCorpus$search_recode'><p>Recode features in a tCorpus based on a search string</p></a></li>
<li><a href='#tCorpus$set'><p>Modify the token and meta data.tables of a tCorpus</p></a></li>
<li><a href='#tCorpus$set_levels'><p>Change levels of factor columns</p></a></li>
<li><a href='#tCorpus$set_name'><p>Change column names of data and meta data</p></a></li>
<li><a href='#tCorpus$subset'><p>Subset a tCorpus</p></a></li>
<li><a href='#tCorpus$subset_query'><p>Subset tCorpus token data using a query</p></a></li>
<li><a href='#tCorpus$udpipe_clauses'><p>Add columns indicating who did what</p></a></li>
<li><a href='#tCorpus$udpipe_quotes'><p>Add columns indicating who said what</p></a></li>
<li><a href='#tokens_to_tcorpus'><p>Create a tcorpus based on tokens (i.e. preprocessed texts)</p></a></li>
<li><a href='#tokenWindowOccurence'><p>Gives the window in which a term occured in a matrix.</p></a></li>
<li><a href='#top_features'><p>Show top features</p></a></li>
<li><a href='#transform_rsyntax'><p>Apply rsyntax transformations</p></a></li>
<li><a href='#udpipe_clause_tqueries'><p>Get a list of tqueries for extracting who did what</p></a></li>
<li><a href='#udpipe_quote_tqueries'><p>Get a list of tqueries for extracting quotes</p></a></li>
<li><a href='#udpipe_simplify'><p>Simplify tokenIndex created with the udpipe parser</p></a></li>
<li><a href='#udpipe_spanquote_tqueries'><p>Get a list of tqueries for finding candidates for span quotes.</p></a></li>
<li><a href='#udpipe_tcorpus'><p>Create a tCorpus using udpipe</p></a></li>
<li><a href='#untokenize'><p>Reconstruct original texts</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.5.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-05-07</td>
</tr>
<tr>
<td>Title:</td>
<td>Managing, Querying and Analyzing Tokenized Text</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides text analysis in R, focusing on the use of a tokenized text format. In this format, the positions of tokens are maintained, and each token can be annotated (e.g., part-of-speech tags, dependency relations).
    Prominent features include advanced Lucene-like querying for specific tokens or contexts (e.g., documents, sentences),
    similarity statistics for words and documents, exporting to DTM for compatibility with many text analysis packages,
    and the possibility to reconstruct original text from tokens to facilitate interpretation.</td>
</tr>
<tr>
<td>Author:</td>
<td>Kasper Welbers and Wouter van Atteveldt</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kasper Welbers &lt;kasperwelbers@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, wordcloud (&ge; 2.5), stringi, Rcpp (&ge; 0.12.12), R6,
udpipe (&ge; 0.8.3), digest, data.table (&ge; 1.10.4), quanteda (&ge;
1.5.1), igraph, tokenbrowser (&ge; 0.1.5), RNewsflow (&ge; 1.2.1),
Matrix (&ge; 1.2), parallel, pbapply (&ge; 1.4), rsyntax (&ge; 0.1.1)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, tm (&ge; 0.6), topicmodels, knitr, rmarkdown</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppProgress</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/kasperwelbers/corpustools">https://github.com/kasperwelbers/corpustools</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-08 07:50:30 UTC; kasper</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-08 09:50:15 UTC</td>
</tr>
</table>
<hr>
<h2 id='add_multitoken_label'>Choose and add multitoken strings based on multitoken categories</h2><span id='topic+add_multitoken_label'></span>

<h3>Description</h3>

<p>Given a multitoken category (e.g., named entity ids), this function finds the most frequently occuring string in this category and adds it as a label for the category
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_multitoken_label(
  tc,
  colloc_id,
  feature = "token",
  new_feature = sprintf("%s_l", colloc_id),
  pref_subset = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_multitoken_label_+3A_tc">tc</code></td>
<td>
<p>a tcorpus object</p>
</td></tr>
<tr><td><code id="add_multitoken_label_+3A_colloc_id">colloc_id</code></td>
<td>
<p>the data column containing the unique id for multitoken tokens</p>
</td></tr>
<tr><td><code id="add_multitoken_label_+3A_feature">feature</code></td>
<td>
<p>the name of the feature column</p>
</td></tr>
<tr><td><code id="add_multitoken_label_+3A_new_feature">new_feature</code></td>
<td>
<p>the name of the new feature column</p>
</td></tr>
<tr><td><code id="add_multitoken_label_+3A_pref_subset">pref_subset</code></td>
<td>
<p>Optionally, a subset call, to specify a subset that has priority for finding the most frequently occuring string</p>
</td></tr>
</table>

<hr>
<h2 id='agg_label'>Helper function for aggregate_rsyntax</h2><span id='topic+agg_label'></span>

<h3>Description</h3>

<p>This function is used within the <code><a href="#topic+aggregate_rsyntax">aggregate_rsyntax</a></code> function to facilitate aggregating by specific labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>agg_label(label, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="agg_label_+3A_label">label</code></td>
<td>
<p>The rsyntax label. Needs to be an existing value in the annotation column (as specified when calling <code><a href="#topic+aggregate_rsyntax">aggregate_rsyntax</a></code>)</p>
</td></tr>
<tr><td><code id="agg_label_+3A_...">...</code></td>
<td>
<p>Specify the new aggregated columns in name-value pairs. The name is the name of the new column, and the value should be a function over a column in $tokens. 
For example:  subject = paste(token, collapse = ' ')  would create the column 'subject', of which the values are the concatenated tokens. See examples for more.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Not relevant. Should only be used within <code><a href="#topic+aggregate_rsyntax">aggregate_rsyntax</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = tc_sotu_udpipe$copy()
tc$udpipe_clauses()

## count number of tokens in predicate
aggregate_rsyntax(tc, 'clause', txt=FALSE,
                  agg_label('predicate', n = length(token_id)))
</code></pre>

<hr>
<h2 id='agg_tcorpus'>Aggregate the tokens data</h2><span id='topic+agg_tcorpus'></span>

<h3>Description</h3>

<p>This is a wrapper for the data.table aggregate function, for easy aggregation of the tokens data grouped by columns in the tokens or meta data.
The .id argument is an important addition, because token annotation often contain values that span multiple rows.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>agg_tcorpus(tc, ..., by = NULL, .id = NULL, wide = T)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="agg_tcorpus_+3A_tc">tc</code></td>
<td>
<p>A tCorpus</p>
</td></tr>
<tr><td><code id="agg_tcorpus_+3A_...">...</code></td>
<td>
<p>The name of the aggregated column and the function over an existing column are given as a name value pair. For example,
count = length(token) will count the number of tokens in each group, and sentiment = mean(sentiment, na.rm=T)
will calculate the mean score for a column with sentiment scores.</p>
</td></tr>
<tr><td><code id="agg_tcorpus_+3A_by">by</code></td>
<td>
<p>A character vector with column names from the tokens and/or meta data.</p>
</td></tr>
<tr><td><code id="agg_tcorpus_+3A_.id">.id</code></td>
<td>
<p>If an id column is given, only rows for which this id is not NA are used, and only one row for each id is used.
This prevents double counting of values in annotations that span multiple rows. For example, a sentiment dictionary can match the tokens &quot;not good&quot;, in which case
the sentiment score (-1) will be assigned to both tokens. These annotations should have an _id column that indicates the unique matches.</p>
</td></tr>
<tr><td><code id="agg_tcorpus_+3A_wide">wide</code></td>
<td>
<p>Should results be in wide or long format?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tc = create_tcorpus(sotu_texts, doc_col='id')

library(quanteda)
dict = data_dictionary_LSD2015
dict = melt_quanteda_dict(dict)
dict$sentiment = ifelse(dict$code %in% c('positive','neg_negative'), 1, -1)
tc$code_dictionary(dict)

agg_tcorpus(tc, N = length(sentiment), sent = mean(sentiment), .id='code_id')
agg_tcorpus(tc, sent = mean(sentiment), .id='code_id', by='president')
agg_tcorpus(tc, sent = mean(sentiment), .id='code_id', by=c('president', 'token'))

</code></pre>

<hr>
<h2 id='aggregate_rsyntax'>Aggregate rsyntax annotations</h2><span id='topic+aggregate_rsyntax'></span>

<h3>Description</h3>

<p>A method for aggregating rsyntax annotations. The intended purpose is to compute aggregate values for a given label in an annotation column.
</p>
<p>For example, you used annotate_rsyntax to add a column with subject-predicate labels, and now you want to concatenate the tokens with these labels.
With annotate_rsyntax you would first aggregate the subject tokens, then aggregate the predicate tokens. By default (txt = T) the column with concatenated tokens are added.
</p>
<p>You can specify any aggregation function using any column in tc$tokens. So say you want to perform a sentiment analysis on the quotes of politicians. You first used annotate_rsyntax to create an annotation column 'quote',
that has the labels 'source', 'verb', and 'quote'. You also used code_dictionary to add a column with unique politician ID's and a column with sentiment scores.
Now you can aggregate the source tokens  to get a single unique ID, and aggregate the quote tokens to get a single sentiment score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregate_rsyntax(
  tc,
  annotation,
  ...,
  by_col = NULL,
  txt = F,
  labels = NULL,
  rm_na = T
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregate_rsyntax_+3A_tc">tc</code></td>
<td>
<p>a tCorpus</p>
</td></tr>
<tr><td><code id="aggregate_rsyntax_+3A_annotation">annotation</code></td>
<td>
<p>The name of the rsyntax annotation column</p>
</td></tr>
<tr><td><code id="aggregate_rsyntax_+3A_...">...</code></td>
<td>
<p>To aggregate columns for specific</p>
</td></tr>
<tr><td><code id="aggregate_rsyntax_+3A_by_col">by_col</code></td>
<td>
<p>A character vector with other column names in tc$tokens to aggregate by.</p>
</td></tr>
<tr><td><code id="aggregate_rsyntax_+3A_txt">txt</code></td>
<td>
<p>If TRUE, add columns with concatenated tokens for each label. Can also be a character vector specifying for which specific labels to create this column</p>
</td></tr>
<tr><td><code id="aggregate_rsyntax_+3A_labels">labels</code></td>
<td>
<p>Instead of using all labels, a character vector of labels can be given</p>
</td></tr>
<tr><td><code id="aggregate_rsyntax_+3A_rm_na">rm_na</code></td>
<td>
<p>If TRUE, remove rows with only NA values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
tc = tc_sotu_udpipe$copy()
tc$udpipe_clauses()

subject_verb_predicate = aggregate_rsyntax(tc, 'clause', txt=TRUE)
head(subject_verb_predicate)

## We can also add specific aggregation functions

## count number of tokens in predicate
aggregate_rsyntax(tc, 'clause',
                  agg_label('predicate', n = length(token_id)))
                  
## same, but with txt for only the subject label
aggregate_rsyntax(tc, 'clause', txt='subject',
                  agg_label('predicate', n = length(token_id)))

                                
## example application: sentiment scores for specific subjects

# first use queries to code subjects
tc$code_features(column = 'who',
                 query  = c('I#  I~s &lt;this president&gt;', 
                            'we# we americans &lt;american people&gt;'))

# then use dictionary to get sentiment scores
dict = melt_quanteda_dict(quanteda::data_dictionary_LSD2015)
dict$sentiment = ifelse(dict$code %in% c('negative','neg_positive'), -1, 1)
tc$code_dictionary(dict)

sent = aggregate_rsyntax(tc, 'clause', txt='predicate',
                  agg_label('subject', subject = na.omit(who)[1]),
                  agg_label('predicate', sentiment = mean(sentiment, na.rm=TRUE)))
head(sent)
sent[,list(sentiment=mean(sentiment, na.rm=TRUE), n=.N), by='subject']

## End(Not run)
</code></pre>

<hr>
<h2 id='as.tcorpus'>Force an object to be a tCorpus class</h2><span id='topic+as.tcorpus'></span>

<h3>Description</h3>

<p>Force an object to be a tCorpus class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.tcorpus(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.tcorpus_+3A_x">x</code></td>
<td>
<p>the object to be forced</p>
</td></tr>
<tr><td><code id="as.tcorpus_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>

<hr>
<h2 id='as.tcorpus.default'>Force an object to be a tCorpus class</h2><span id='topic+as.tcorpus.default'></span>

<h3>Description</h3>

<p>Force an object to be a tCorpus class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
as.tcorpus(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.tcorpus.default_+3A_x">x</code></td>
<td>
<p>the object to be forced</p>
</td></tr>
<tr><td><code id="as.tcorpus.default_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x = c('First text','Second text')
as.tcorpus(x) ## x is not a tCorpus object

## End(Not run)
</code></pre>

<hr>
<h2 id='as.tcorpus.tCorpus'>Force an object to be a tCorpus class</h2><span id='topic+as.tcorpus.tCorpus'></span>

<h3>Description</h3>

<p>Force an object to be a tCorpus class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tCorpus'
as.tcorpus(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.tcorpus.tCorpus_+3A_x">x</code></td>
<td>
<p>the object to be forced</p>
</td></tr>
<tr><td><code id="as.tcorpus.tCorpus_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c('First text', 'Second text'))
as.tcorpus(tc)
</code></pre>

<hr>
<h2 id='backbone_filter'>Extract the backbone of a network.</h2><span id='topic+backbone_filter'></span>

<h3>Description</h3>

<p>Based on the following paper: Serrano, M. A., Boguna, M., &amp; Vespignani, A. (2009). Extracting the multiscale backbone of complex weighted networks. Proceedings of the National Academy of Sciences, 106(16), 6483-6488.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backbone_filter(
  g,
  alpha = 0.05,
  direction = "none",
  delete_isolates = T,
  max_vertices = NULL,
  use_original_alpha = T,
  k_is_n = F
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="backbone_filter_+3A_g">g</code></td>
<td>
<p>A graph in the 'Igraph' format.</p>
</td></tr>
<tr><td><code id="backbone_filter_+3A_alpha">alpha</code></td>
<td>
<p>The threshold for the alpha. Can be interpreted similar to a p value (see paper for clarrification).</p>
</td></tr>
<tr><td><code id="backbone_filter_+3A_direction">direction</code></td>
<td>
<p>direction = 'none' can be used for both directed and undirected networks, and is (supposed to be) the disparity filter proposed in Serrano et al. (2009) is used. By setting to 'in' or 'out', the alpha is only calculated for out or in edges. This is an experimental use of the backbone extraction (so beware!) but it seems a logical application.</p>
</td></tr>
<tr><td><code id="backbone_filter_+3A_delete_isolates">delete_isolates</code></td>
<td>
<p>If TRUE, vertices with degree 0 (i.e. no edges) are deleted.</p>
</td></tr>
<tr><td><code id="backbone_filter_+3A_max_vertices">max_vertices</code></td>
<td>
<p>Optional. Set a maximum number of vertices for the network to be produced. The alpha is then automatically lowered to the point that only the given number of vertices remains connected (degree &gt; 0). This can be usefull if the purpose is to make an interpretation friendly network. See e.g., http://jcom.sissa.it/archive/14/01/JCOM_1401_2015_A01</p>
</td></tr>
<tr><td><code id="backbone_filter_+3A_use_original_alpha">use_original_alpha</code></td>
<td>
<p>if max_vertices is not NULL, this determines whether the lower alpha for selecting the top vertices is also used as a threshold for the edges, or whether the original value given in the alpha parameter is used.</p>
</td></tr>
<tr><td><code id="backbone_filter_+3A_k_is_n">k_is_n</code></td>
<td>
<p>the disparity filter method for backbone extraction uses the number of existing edges (k) for each node, which can be arbitraty if there are many very weak ties, which is often the case in a co-occurence network. By setting k_is_n to TRUE, it is 'assumed' that all nodes are connected, which makes sense from a language model perspective (i.e. probability for co-occurence is never zero)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A graph in the Igraph format
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tc = create_tcorpus(sotu_texts, doc_column = 'id')
tc$preprocess('token','feature', remove_stopwords = TRUE, use_stemming = TRUE, min_docfreq = 10)

g = semnet_window(tc, 'feature', window.size = 10)
igraph::vcount(g)
igraph::ecount(g)
gb = backbone_filter(g, max_vertices = 100)
igraph::vcount(gb)
igraph::ecount(gb)
plot_semnet(gb)

</code></pre>

<hr>
<h2 id='browse_hits'>View hits in a browser</h2><span id='topic+browse_hits'></span>

<h3>Description</h3>

<p>Creates a static HTML file to view the query hits in the tcorpus in full text mode.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>browse_hits(
  tc,
  hits,
  token_col = "token",
  n = 500,
  select = c("first", "random"),
  header = "",
  subheader = NULL,
  meta_cols = NULL,
  seed = NA,
  view = T,
  filename = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="browse_hits_+3A_tc">tc</code></td>
<td>
<p>a tCorpus</p>
</td></tr>
<tr><td><code id="browse_hits_+3A_hits">hits</code></td>
<td>
<p>a featureHits object, as returned by <a href="#topic+search_features">search_features</a></p>
</td></tr>
<tr><td><code id="browse_hits_+3A_token_col">token_col</code></td>
<td>
<p>The name of the column in tc$tokens that contain the token text</p>
</td></tr>
<tr><td><code id="browse_hits_+3A_n">n</code></td>
<td>
<p>If doc_ids is NULL, Only n of the results are printed (to prevent accidentally making huge browsers).</p>
</td></tr>
<tr><td><code id="browse_hits_+3A_select">select</code></td>
<td>
<p>If n is smaller than the number of documents in tc, select determines how the n documents are selected</p>
</td></tr>
<tr><td><code id="browse_hits_+3A_header">header</code></td>
<td>
<p>Optionally, a title presented at the top of the browser</p>
</td></tr>
<tr><td><code id="browse_hits_+3A_subheader">subheader</code></td>
<td>
<p>Optionally, overwrite the subheader. By default the subheader reports the number of documents</p>
</td></tr>
<tr><td><code id="browse_hits_+3A_meta_cols">meta_cols</code></td>
<td>
<p>A character vector with names of columns in tc$meta, used to only show the selected columns</p>
</td></tr>
<tr><td><code id="browse_hits_+3A_seed">seed</code></td>
<td>
<p>If select is &quot;random&quot;, seed can be used to set a random seed</p>
</td></tr>
<tr><td><code id="browse_hits_+3A_view">view</code></td>
<td>
<p>If TRUE (default), view the browser in the Viewer window (turn off if this is not supported)</p>
</td></tr>
<tr><td><code id="browse_hits_+3A_filename">filename</code></td>
<td>
<p>Optionally, save the browser at a specified location</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The url for the file location is returned (invisibly)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tc = create_tcorpus(sotu_texts, doc_column='id')
hits = search_features(tc, c("Terrorism# terroris*", "War# war*"))
browse_hits(tc, hits)

</code></pre>

<hr>
<h2 id='browse_texts'>Create and view a full text browser</h2><span id='topic+browse_texts'></span>

<h3>Description</h3>

<p>Creates a static HTML file to view the texts in the tcorpus in full text mode.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>browse_texts(
  tc,
  doc_ids = NULL,
  token_col = "token",
  n = 500,
  select = c("first", "random"),
  header = "",
  subheader = NULL,
  highlight = NULL,
  scale = NULL,
  category = NULL,
  rsyntax = NULL,
  value = NULL,
  meta_cols = NULL,
  seed = NA,
  nav = NULL,
  top_nav = NULL,
  thres_nav = 1,
  view = T,
  highlight_col = "yellow",
  scale_col = c("red", "blue", "green"),
  filename = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="browse_texts_+3A_tc">tc</code></td>
<td>
<p>a tCorpus</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_doc_ids">doc_ids</code></td>
<td>
<p>A vector with document ids to view</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_token_col">token_col</code></td>
<td>
<p>The name of the column in tc$tokens that contain the token text</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_n">n</code></td>
<td>
<p>Only n of the results are printed (to prevent accidentally making huge browsers).</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_select">select</code></td>
<td>
<p>If n is smaller than the number of documents in tc, select determines how the n documents are selected</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_header">header</code></td>
<td>
<p>Optionally, a title presented at the top of the browser</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_subheader">subheader</code></td>
<td>
<p>Optionally, overwrite the subheader. By default the subheader reports the number of documents</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_highlight">highlight</code></td>
<td>
<p>Highlighe mode: provide the name of a numeric column in tc$tokens with values between 0 and 1, used to highlight tokens.
Can also be a character vector, in which case al non-NA values are highlighted</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_scale">scale</code></td>
<td>
<p>Scale mode: provide the name of a numeric column in tc$tokens with values between -1 and 1, used to color tokens on a scale (set colors with scale_col)</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_category">category</code></td>
<td>
<p>Category mode: provide the name of a character or factor column in tc$tokens. Each unique value will have its own color, and navigation for categories will be added (nav cannot be used with this option)</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_rsyntax">rsyntax</code></td>
<td>
<p>rsyntax mode: provide the name of an rsyntax annotation column (see <code><a href="#topic+annotate_rsyntax">annotate_rsyntax</a></code>)</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_value">value</code></td>
<td>
<p>rsyntax mode argument: if rsyntax mode is used, value can be a character vector with values in the rsyntax annotation column. 
If used, only these values are fully colored, and the other (non NA) values only have border colors.</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_meta_cols">meta_cols</code></td>
<td>
<p>A character vector with names of columns in tc$meta, used to only show the selected columns</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_seed">seed</code></td>
<td>
<p>If select is &quot;random&quot;, seed can be used to set a random seed. After sampling the seed is re-initialized with set.seed(NULL).</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_nav">nav</code></td>
<td>
<p>Optionally, a column in tc$meta to add navigation (only supports simple filtering on unique values).
This is not possible if category is used.</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_top_nav">top_nav</code></td>
<td>
<p>A number. If navigation based on token annotations is used, filters will only apply to top x values with highest token occurence in a document</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_thres_nav">thres_nav</code></td>
<td>
<p>Like top_nav, but specifying a threshold for the minimum number of tokens.</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_view">view</code></td>
<td>
<p>If TRUE (default), view the browser in the Viewer window (turn off if this is not supported)</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_highlight_col">highlight_col</code></td>
<td>
<p>If highlight is used, the color for highlighting</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_scale_col">scale_col</code></td>
<td>
<p>If scale is used, a vector with 2 or more colors used to create a color ramp. That is, -1 is first color, +1 is last color, if three colors are given 0 matches the middle color, and colors in between are interpolated.</p>
</td></tr>
<tr><td><code id="browse_texts_+3A_filename">filename</code></td>
<td>
<p>Optionally, save the browser at a specified location</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The url for the file location is returned (invisibly)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tc = create_tcorpus(sotu_texts, doc_column='id')

queries = c('War# war soldier* weapon*',
            'Economy# econom* market* tax*')
tc$code_features(queries)

browse_texts(tc, category='code')

</code></pre>

<hr>
<h2 id='calc_chi2'>Vectorized computation of chi^2 statistic for a 2x2 crosstab containing the values
[a, b]
[c, d]</h2><span id='topic+calc_chi2'></span>

<h3>Description</h3>

<p>Vectorized computation of chi^2 statistic for a 2x2 crosstab containing the values
[a, b]
[c, d]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_chi2(a, b, c, d, correct = T, cochrans_criteria = F)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_chi2_+3A_a">a</code></td>
<td>
<p>topleft value of the table</p>
</td></tr>
<tr><td><code id="calc_chi2_+3A_b">b</code></td>
<td>
<p>topright value</p>
</td></tr>
<tr><td><code id="calc_chi2_+3A_c">c</code></td>
<td>
<p>bottomleft value</p>
</td></tr>
<tr><td><code id="calc_chi2_+3A_d">d</code></td>
<td>
<p>bottomright value</p>
</td></tr>
<tr><td><code id="calc_chi2_+3A_correct">correct</code></td>
<td>
<p>if TRUE, use yates correction. Can be a vector of length a (i.e. the number of tables)</p>
</td></tr>
<tr><td><code id="calc_chi2_+3A_cochrans_criteria">cochrans_criteria</code></td>
<td>
<p>if TRUE, check if cochrans_criteria indicate that a correction should be used. This overrides the correct parameter</p>
</td></tr>
</table>

<hr>
<h2 id='compare_corpus'>Compare tCorpus vocabulary to that of another (reference) tCorpus</h2><span id='topic+compare_corpus'></span>

<h3>Description</h3>

<p>Compare tCorpus vocabulary to that of another (reference) tCorpus
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_corpus(
  tc,
  tc_y,
  feature,
  smooth = 0.1,
  min_ratio = NULL,
  min_chi2 = NULL,
  is_subset = F,
  yates_cor = c("auto", "yes", "no"),
  what = c("freq", "docfreq", "cooccurrence")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_corpus_+3A_tc">tc</code></td>
<td>
<p>a <code><a href="#topic+tCorpus">tCorpus</a></code></p>
</td></tr>
<tr><td><code id="compare_corpus_+3A_tc_y">tc_y</code></td>
<td>
<p>the reference tCorpus</p>
</td></tr>
<tr><td><code id="compare_corpus_+3A_feature">feature</code></td>
<td>
<p>the column name of the feature that is to be compared</p>
</td></tr>
<tr><td><code id="compare_corpus_+3A_smooth">smooth</code></td>
<td>
<p>Laplace smoothing is used for the calculation of the probabilities. Here you can set the added (pseuocount) value.</p>
</td></tr>
<tr><td><code id="compare_corpus_+3A_min_ratio">min_ratio</code></td>
<td>
<p>threshold for the ratio value, which is the ratio of the relative frequency of a term in dtm.x and dtm.y</p>
</td></tr>
<tr><td><code id="compare_corpus_+3A_min_chi2">min_chi2</code></td>
<td>
<p>threshold for the chi^2 value</p>
</td></tr>
<tr><td><code id="compare_corpus_+3A_is_subset">is_subset</code></td>
<td>
<p>Specify whether tc is a subset of tc_y. In this case, the term frequencies of tc will be subtracted from the term frequencies in tc_y</p>
</td></tr>
<tr><td><code id="compare_corpus_+3A_yates_cor">yates_cor</code></td>
<td>
<p>mode for using yates correctsion in the chi^2 calculation. Can be turned on (&quot;yes&quot;) or off (&quot;no&quot;), or set to &quot;auto&quot;, in which case cochrans rule is used to determine whether yates' correction is used.</p>
</td></tr>
<tr><td><code id="compare_corpus_+3A_what">what</code></td>
<td>
<p>choose whether to compare the frequency (&quot;freq&quot;) of terms, or the document frequency (&quot;docfreq&quot;). This also affects how chi^2 is calculated, comparing either freq relative to vocabulary size or docfreq relative to corpus size (N)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vocabularyComparison object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tc = create_tcorpus(sotu_texts, doc_column = 'id')

tc$preprocess('token', 'feature', remove_stopwords = TRUE, use_stemming = TRUE)

obama = tc$subset_meta(president == 'Barack Obama', copy=TRUE)
bush = tc$subset_meta(president == 'George W. Bush', copy=TRUE)

comp = compare_corpus(tc, bush, 'feature')
comp = comp[order(-comp$chi),]
head(comp)
plot(comp)

</code></pre>

<hr>
<h2 id='compare_documents'>Calculate the similarity of documents</h2><span id='topic+compare_documents'></span>

<h3>Description</h3>

<p>Calculate the similarity of documents
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_documents(
  tc,
  feature = "token",
  date_col = NULL,
  meta_cols = NULL,
  hour_window = c(24),
  measure = c("cosine", "overlap_pct"),
  min_similarity = 0,
  weight = c("norm_tfidf", "tfidf", "termfreq", "docfreq"),
  ngrams = NA,
  from_subset = NULL,
  to_subset = NULL,
  return_igraph = T,
  verbose = T
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_documents_+3A_tc">tc</code></td>
<td>
<p>A <a href="#topic+tCorpus">tCorpus</a></p>
</td></tr>
<tr><td><code id="compare_documents_+3A_feature">feature</code></td>
<td>
<p>the column name of the feature that is to be used for the comparison.</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_date_col">date_col</code></td>
<td>
<p>a date with time in POSIXct. If given together with hour_window, only documents within the given hour_window will be compared.</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_meta_cols">meta_cols</code></td>
<td>
<p>a character vector with columns in the meta data / docvars. If given, only documents for which these values are identical are compared</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_hour_window">hour_window</code></td>
<td>
<p>A vector of length 1 or 2. If length is 1, the same value is used for the left and right side of the window. If length is 2, the first and second value determine the left and right side. For example, the value 12 will compare each document to all documents between the previous and next 12 hours, and c(-10, 36) will compare each document to all documents between the previous 10 and the next 36 hours.</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_measure">measure</code></td>
<td>
<p>the similarity measure. Currently supports cosine similarity (symmetric) and overlap_pct (asymmetric)</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_min_similarity">min_similarity</code></td>
<td>
<p>A threshold for the similarity score</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_weight">weight</code></td>
<td>
<p>a weighting scheme for the document-term matrix. Default is term-frequency inverse document frequency with normalized rows (document length).</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_ngrams">ngrams</code></td>
<td>
<p>an integer. If given, ngrams of this length are used</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_from_subset">from_subset</code></td>
<td>
<p>An expression to select a subset. If given, only this subset will be compared to other documents</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_to_subset">to_subset</code></td>
<td>
<p>An expression to select a subset. If given, documents are only compared to this subset</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_return_igraph">return_igraph</code></td>
<td>
<p>If TRUE, return as an igraph network. Otherwise, return as a list with the edgelist and meta data.</p>
</td></tr>
<tr><td><code id="compare_documents_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, report progress</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An igraph graph in which nodes are documents and edges represent similarity scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = data.frame(text = c('a b c d e',
                        'e f g h i j k',
                        'a b c'),
               date = as.POSIXct(c('2010-01-01','2010-01-01','2012-01-01')))
tc = create_tcorpus(d)

g = compare_documents(tc)
igraph::get.data.frame(g)

g = compare_documents(tc, measure = 'overlap_pct')
igraph::get.data.frame(g)

g = compare_documents(tc, date_col = 'date', hour_window = c(0,36))
igraph::get.data.frame(g)
</code></pre>

<hr>
<h2 id='compare_subset'>Compare vocabulary of a subset of a tCorpus to the rest of the tCorpus</h2><span id='topic+compare_subset'></span>

<h3>Description</h3>

<p>Compare vocabulary of a subset of a tCorpus to the rest of the tCorpus
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_subset(
  tc,
  feature,
  subset_x = NULL,
  subset_meta_x = NULL,
  query_x = NULL,
  query_feature = "token",
  smooth = 0.1,
  min_ratio = NULL,
  min_chi2 = NULL,
  yates_cor = c("auto", "yes", "no"),
  what = c("freq", "docfreq", "cooccurrence")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_subset_+3A_tc">tc</code></td>
<td>
<p>a <code><a href="#topic+tCorpus">tCorpus</a></code></p>
</td></tr>
<tr><td><code id="compare_subset_+3A_feature">feature</code></td>
<td>
<p>the column name of the feature that is to be compared</p>
</td></tr>
<tr><td><code id="compare_subset_+3A_subset_x">subset_x</code></td>
<td>
<p>an expression to subset the tCorpus. The vocabulary of the subset will be compared to the rest of the tCorpus</p>
</td></tr>
<tr><td><code id="compare_subset_+3A_subset_meta_x">subset_meta_x</code></td>
<td>
<p>like subset_x, but using using the meta data</p>
</td></tr>
<tr><td><code id="compare_subset_+3A_query_x">query_x</code></td>
<td>
<p>like subset_x, but using a query search to select documents (see <a href="#topic+search_contexts">search_contexts</a>)</p>
</td></tr>
<tr><td><code id="compare_subset_+3A_query_feature">query_feature</code></td>
<td>
<p>if query_x is used, the column name of the feature used in the query search.</p>
</td></tr>
<tr><td><code id="compare_subset_+3A_smooth">smooth</code></td>
<td>
<p>Laplace smoothing is used for the calculation of the probabilities. Here you can set the added (pseuocount) value.</p>
</td></tr>
<tr><td><code id="compare_subset_+3A_min_ratio">min_ratio</code></td>
<td>
<p>threshold for the ratio value, which is the ratio of the relative frequency of a term in dtm.x and dtm.y</p>
</td></tr>
<tr><td><code id="compare_subset_+3A_min_chi2">min_chi2</code></td>
<td>
<p>threshold for the chi^2 value</p>
</td></tr>
<tr><td><code id="compare_subset_+3A_yates_cor">yates_cor</code></td>
<td>
<p>mode for using yates correctsion in the chi^2 calculation. Can be turned on (&quot;yes&quot;) or off (&quot;no&quot;), or set to &quot;auto&quot;, in which case cochrans rule is used to determine whether yates' correction is used.</p>
</td></tr>
<tr><td><code id="compare_subset_+3A_what">what</code></td>
<td>
<p>choose whether to compare the frequency (&quot;freq&quot;) of terms, or the document frequency (&quot;docfreq&quot;). This also affects how chi^2 is calculated, comparing either freq relative to vocabulary size or docfreq relative to corpus size (N)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vocabularyComparison object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tc = create_tcorpus(sotu_texts, doc_column = 'id')

tc$preprocess('token', 'feature', remove_stopwords = TRUE, use_stemming = TRUE)

comp = compare_subset(tc, 'feature', subset_meta_x = president == 'Barack Obama')
comp = comp[order(-comp$chi),]
head(comp)
plot(comp)

comp = compare_subset(tc, 'feature', query_x = 'terroris*')
comp = comp[order(-comp$chi),]
head(comp, 10)

</code></pre>

<hr>
<h2 id='corenlp_tokens'>coreNLP example sentences</h2><span id='topic+corenlp_tokens'></span>

<h3>Description</h3>

<p>coreNLP example sentences
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(corenlp_tokens)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>

<hr>
<h2 id='count_tcorpus'>Count results of search hits, or of a given feature in tokens</h2><span id='topic+count_tcorpus'></span>

<h3>Description</h3>

<p>Count results of search hits, or of a given feature in tokens
</p>


<h3>Usage</h3>

<pre><code class='language-R'>count_tcorpus(
  tc,
  meta_cols = NULL,
  hits = NULL,
  feature = NULL,
  count = c("documents", "tokens", "hits"),
  wide = T
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="count_tcorpus_+3A_tc">tc</code></td>
<td>
<p>A tCorpus</p>
</td></tr>
<tr><td><code id="count_tcorpus_+3A_meta_cols">meta_cols</code></td>
<td>
<p>The columns in the meta data by which the results should be grouped</p>
</td></tr>
<tr><td><code id="count_tcorpus_+3A_hits">hits</code></td>
<td>
<p>featureHits or contextHits (output of <code><a href="#topic+search_features">search_features</a></code>, <code><a href="#topic+search_dictionary">search_dictionary</a></code> or <code><a href="#topic+search_contexts">search_contexts</a></code>)</p>
</td></tr>
<tr><td><code id="count_tcorpus_+3A_feature">feature</code></td>
<td>
<p>Instead of hits, a specific feature column can be selected.</p>
</td></tr>
<tr><td><code id="count_tcorpus_+3A_count">count</code></td>
<td>
<p>How should the results be counted? Number of documents, tokens, or unique hits. The difference between tokens and hits is that hits can encompass multiple tokens (e.g., &quot;Bob Smith&quot; is 1 hit and 2 tokens).</p>
</td></tr>
<tr><td><code id="count_tcorpus_+3A_wide">wide</code></td>
<td>
<p>Should results be in wide or long format?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data table
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tc = create_tcorpus(sotu_texts, doc_col='id')
hits = search_features(tc, c("US# &lt;united states&gt;", "Economy# econom*"))
count_tcorpus(tc, hits=hits)
count_tcorpus(tc, hits=hits, meta_cols='president')
count_tcorpus(tc, hits=hits, meta_cols='president', wide=FALSE)

</code></pre>

<hr>
<h2 id='create_tcorpus'>Create a tCorpus</h2><span id='topic+create_tcorpus'></span><span id='topic+create_tcorpus.character'></span><span id='topic+create_tcorpus.data.frame'></span><span id='topic+create_tcorpus.factor'></span><span id='topic+create_tcorpus.corpus'></span>

<h3>Description</h3>

<p>Create a <a href="#topic+tCorpus">tCorpus</a> from raw text input. Input can be a character (or factor) vector, data.frame or quanteda corpus.
If a data.frame is given, all columns other than the document id and text columns are included as meta data. If a quanteda
corpus is given, the ids and texts are already specified, and the docvars will be included in the tCorpus as meta data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_tcorpus(x, ...)

## S3 method for class 'character'
create_tcorpus(
  x,
  doc_id = 1:length(x),
  meta = NULL,
  udpipe_model = NULL,
  split_sentences = F,
  max_sentences = NULL,
  max_tokens = NULL,
  udpipe_model_path = getwd(),
  udpipe_cache = 3,
  udpipe_cores = NULL,
  udpipe_batchsize = 50,
  use_parser = F,
  remember_spaces = F,
  verbose = T,
  ...
)

## S3 method for class 'data.frame'
create_tcorpus(
  x,
  text_columns = "text",
  doc_column = "doc_id",
  udpipe_model = NULL,
  split_sentences = F,
  max_sentences = NULL,
  max_tokens = NULL,
  udpipe_model_path = getwd(),
  udpipe_cache = 3,
  udpipe_cores = NULL,
  udpipe_batchsize = 50,
  use_parser = F,
  remember_spaces = F,
  verbose = T,
  ...
)

## S3 method for class 'factor'
create_tcorpus(x, ...)

## S3 method for class 'corpus'
create_tcorpus(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_tcorpus_+3A_x">x</code></td>
<td>
<p>main input. can be a character (or factor) vector where each value is a full text, or a data.frame that has a column that contains full texts.
If x (or a text_column in x) has leading or trailing whitespace, this is cut off (and you'll get a warning about it).</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_...">...</code></td>
<td>
<p>Arguments passed to create_tcorpus.character</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_doc_id">doc_id</code></td>
<td>
<p>if x is a character/factor vector, doc_id can be used to specify document ids. This has to be a vector of the same length as x</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_meta">meta</code></td>
<td>
<p>A data.frame with document meta information (e.g., date, source). The rows of the data.frame need to match the values of x</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_udpipe_model">udpipe_model</code></td>
<td>
<p>Optionally, the name of a Universal Dependencies language model (e.g., &quot;english-ewt&quot;, &quot;dutch-alpino&quot;), to use the udpipe package
(<code><a href="udpipe.html#topic+udpipe_annotate">udpipe_annotate</a></code>) for natural language processing. You can use <code><a href="#topic+show_udpipe_models">show_udpipe_models</a></code> to get
an overview of the available models. For more information about udpipe and performance benchmarks of the UD models, see the
GitHub page of the <a href="https://github.com/bnosac/udpipe">udpipe package</a>.</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_split_sentences">split_sentences</code></td>
<td>
<p>Logical. If TRUE, the sentence number of tokens is also computed. (only if udpipe_model is not used)</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_max_sentences">max_sentences</code></td>
<td>
<p>An integer. Limits the number of sentences per document to the specified number. If set when split_sentences == FALSE, split_sentences will be set to TRUE.</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_max_tokens">max_tokens</code></td>
<td>
<p>An integer. Limits the number of tokens per document to the specified number</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_udpipe_model_path">udpipe_model_path</code></td>
<td>
<p>If udpipe_model is used, this path wil be used to look for the model, and if the model doesn't yet exist it will be downloaded to this location. Defaults to working directory</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_udpipe_cache">udpipe_cache</code></td>
<td>
<p>The number of persistent caches to keep for inputs of udpipe. The caches store tokens in batches.
This way, if a lot of data has to be parsed, or if R crashes, udpipe can continue from the latest batch instead of start over.
The caches are stored in the corpustools_data folder (in udpipe_model_path). Only the most recent [udpipe_caches] caches will be stored.</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_udpipe_cores">udpipe_cores</code></td>
<td>
<p>If udpipe_model is used, this sets the number of parallel cores. If not specified, will use the same number of cores as used by data.table (or limited to OMP_THREAD_LIMIT).</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_udpipe_batchsize">udpipe_batchsize</code></td>
<td>
<p>In order to report progress and cache results, texts are parsed with udpipe in batches of 50.
The price is that there will be some overhead for each batch, so for very large jobs it can be faster to increase the batchsize.
If the number of texts divided by the number of parallel cores is lower than the batchsize, the texts are evenly distributed over cores.</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_use_parser">use_parser</code></td>
<td>
<p>If TRUE, use dependency parser (only if udpipe_model is used)</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_remember_spaces">remember_spaces</code></td>
<td>
<p>If TRUE, a column with spaces after each token and column with the start and end positions of tokens are included. Can turn it of for a bit more speed and less memory use, but some features won't work.</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, report progress. Only if x is large enough to require multiple sequential batches</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_text_columns">text_columns</code></td>
<td>
<p>if x is a data.frame, this specifies the column(s) that contains text. If multiple columns are used, they are pasted together separated by a double line break.
If remember_spaces is true, a &quot;field&quot; column is also added that show the column name for each token, and the start/end positions are local within these fields</p>
</td></tr>
<tr><td><code id="create_tcorpus_+3A_doc_column">doc_column</code></td>
<td>
<p>If x is a data.frame, this specifies the column with the document ids.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, texts will only be tokenized, and basic preprocessing techniques (lowercasing, stemming) can be applied with the
<code><a href="#topic+preprocess">preprocess</a></code> method. Alternatively, the udpipe package can be used to apply more advanced NLP preprocessing, by
using the udpipe_model argument.
</p>
<p>For certain advanced features you need to set remember_spaces to true. We are often used to forgetting all about spaces when
we do bag-of-word type stuff, and that's sad. With remember_spaces, the exact position of each token is remembered, including 
what type of space follows the token (like a space or a line break), and what text field the token came from (if multiple text_columns are specified in create_tcorpus.data.frame)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## ...
tc = create_tcorpus(c('Text one first sentence. Text one second sentence', 'Text two'))
tc$tokens

tc = create_tcorpus(c('Text one first sentence. Text one second sentence', 'Text two'),
                    split_sentences = TRUE)
tc$tokens

## with meta (easier to S3 method for data.frame)
meta = data.frame(doc_id = c(1,2), source = c('a','b'))
tc = create_tcorpus(c('Text one first sentence. Text one second sentence', 'Text two'),
                    split_sentences = TRUE,
                    doc_id = c(1,2),
                    meta = meta)
tc
d = data.frame(text = c('Text one first sentence. Text one second sentence.',
               'Text two', 'Text three'),
               date = c('2010-01-01','2010-01-01','2012-01-01'),
               source = c('A','B','B'))

tc = create_tcorpus(d, split_sentences = TRUE)
tc
tc$tokens

## use multiple text columns
d$headline = c('Head one', 'Head two', 'Head three')
## use custom doc_id
d$doc_id = c('#1', '#2', '#3')

tc = create_tcorpus(d, text_columns = c('headline','text'), doc_column = 'doc_id',
                    split_sentences = TRUE)
tc
tc$tokens
## It makes little sense to have full texts as factors, but it tends to happen.
## The create_tcorpus S3 method for factors is essentially identical to the
##  method for a character vector.
text = factor(c('Text one first sentence', 'Text one second sentence'))
tc = create_tcorpus(text)
tc$tokens

library(quanteda)
create_tcorpus(data_corpus_inaugural)
</code></pre>

<hr>
<h2 id='docfreq_filter'>Support function for subset method</h2><span id='topic+docfreq_filter'></span>

<h3>Description</h3>

<p>Support function to enable subsetting by document frequency stats of a given feature.
Should only be used within the tCorpus subset method, or any tCorpus method that supports a subset argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>docfreq_filter(
  x,
  min = -Inf,
  max = Inf,
  top = NULL,
  bottom = NULL,
  doc_id = parent.frame()$doc_id
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="docfreq_filter_+3A_x">x</code></td>
<td>
<p>the name of the feature column. Can be given as a call or a string.</p>
</td></tr>
<tr><td><code id="docfreq_filter_+3A_min">min</code></td>
<td>
<p>A number, setting the minimum document frequency value</p>
</td></tr>
<tr><td><code id="docfreq_filter_+3A_max">max</code></td>
<td>
<p>A number, setting the maximum document frequency value</p>
</td></tr>
<tr><td><code id="docfreq_filter_+3A_top">top</code></td>
<td>
<p>A number. If given, only the top x features with the highest document frequency are TRUE</p>
</td></tr>
<tr><td><code id="docfreq_filter_+3A_bottom">bottom</code></td>
<td>
<p>A number. If given, only the bottom x features with the highest document frequency are TRUE</p>
</td></tr>
<tr><td><code id="docfreq_filter_+3A_doc_id">doc_id</code></td>
<td>
<p>Added for reference, but should not be used. Automatically takes doc_id from tCorpus if the docfreq_filter function is used within the subset method.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c('a a a b b', 'a a c c'))

tc$tokens
tc$subset(subset = docfreq_filter(token, min=2))
tc$tokens
</code></pre>

<hr>
<h2 id='dtm_compare'>Compare two document term matrices</h2><span id='topic+dtm_compare'></span>

<h3>Description</h3>

<p>Compare two document term matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_compare(
  dtm.x,
  dtm.y = NULL,
  smooth = 0.1,
  min_ratio = NULL,
  min_chi2 = NULL,
  select_rows = NULL,
  yates_cor = c("auto", "yes", "no"),
  x_is_subset = F,
  what = c("freq", "docfreq", "cooccurrence")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dtm_compare_+3A_dtm.x">dtm.x</code></td>
<td>
<p>the main document-term matrix</p>
</td></tr>
<tr><td><code id="dtm_compare_+3A_dtm.y">dtm.y</code></td>
<td>
<p>the 'reference' document-term matrix</p>
</td></tr>
<tr><td><code id="dtm_compare_+3A_smooth">smooth</code></td>
<td>
<p>Laplace smoothing is used for the calculation of the probabilities. Here you can set the added (pseuocount) value.</p>
</td></tr>
<tr><td><code id="dtm_compare_+3A_min_ratio">min_ratio</code></td>
<td>
<p>threshold for the ratio value, which is the ratio of the relative frequency of a term in dtm.x and dtm.y</p>
</td></tr>
<tr><td><code id="dtm_compare_+3A_min_chi2">min_chi2</code></td>
<td>
<p>threshold for the chi^2 value</p>
</td></tr>
<tr><td><code id="dtm_compare_+3A_select_rows">select_rows</code></td>
<td>
<p>Alternative to using dtm.y. Has to be a vector with rownames, by which</p>
</td></tr>
<tr><td><code id="dtm_compare_+3A_yates_cor">yates_cor</code></td>
<td>
<p>mode for using yates correctsion in the chi^2 calculation. Can be turned on (&quot;yes&quot;) or off (&quot;no&quot;), or set to &quot;auto&quot;, in which case cochrans rule is used to determine whether yates' correction is used.</p>
</td></tr>
<tr><td><code id="dtm_compare_+3A_x_is_subset">x_is_subset</code></td>
<td>
<p>Specify whether dtm.x is a subset of dtm.y. In this case, the term frequencies of dtm.x will be subtracted from the term frequencies in dtm.y</p>
</td></tr>
<tr><td><code id="dtm_compare_+3A_what">what</code></td>
<td>
<p>choose whether to compare the frequency (&quot;freq&quot;) of terms, or the document frequency (&quot;docfreq&quot;). This also affects how chi^2 is calculated, comparing either freq relative to vocabulary size or docfreq relative to corpus size (N)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with rows corresponding to the terms in dtm and the statistics in the columns
</p>

<hr>
<h2 id='dtm_wordcloud'>Plot a word cloud from a dtm</h2><span id='topic+dtm_wordcloud'></span>

<h3>Description</h3>

<p>Compute the term frequencies for the dtm and plot a word cloud with the top n topics
You can either supply a document-term matrix or provide terms and freqs directly
(in which case this is an alias for wordcloud::wordcloud with sensible defaults)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_wordcloud(
  dtm = NULL,
  nterms = 100,
  freq.fun = NULL,
  terms = NULL,
  freqs = NULL,
  scale = c(4, 0.5),
  min.freq = 1,
  rot.per = 0.15,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dtm_wordcloud_+3A_dtm">dtm</code></td>
<td>
<p>the document-term matrix</p>
</td></tr>
<tr><td><code id="dtm_wordcloud_+3A_nterms">nterms</code></td>
<td>
<p>the amount of words to plot (default 100)</p>
</td></tr>
<tr><td><code id="dtm_wordcloud_+3A_freq.fun">freq.fun</code></td>
<td>
<p>if given, will be applied to the frequenies (e.g. sqrt)</p>
</td></tr>
<tr><td><code id="dtm_wordcloud_+3A_terms">terms</code></td>
<td>
<p>the terms to plot, ignored if dtm is given</p>
</td></tr>
<tr><td><code id="dtm_wordcloud_+3A_freqs">freqs</code></td>
<td>
<p>the frequencies to plot, ignored if dtm is given</p>
</td></tr>
<tr><td><code id="dtm_wordcloud_+3A_scale">scale</code></td>
<td>
<p>the scale to plot (see wordcloud::wordcloud)</p>
</td></tr>
<tr><td><code id="dtm_wordcloud_+3A_min.freq">min.freq</code></td>
<td>
<p>the minimum frquency to include (see wordcloud::wordcloud)</p>
</td></tr>
<tr><td><code id="dtm_wordcloud_+3A_rot.per">rot.per</code></td>
<td>
<p>the percentage of vertical words (see wordcloud::wordcloud)</p>
</td></tr>
<tr><td><code id="dtm_wordcloud_+3A_...">...</code></td>
<td>
<p>other arguments passed to wordcloud::wordcloud</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## create DTM
tc = create_tcorpus(sotu_texts[1:100,], doc_column = 'id')
tc$preprocess('token', 'feature', remove_stopwords = TRUE)
dtm = get_dtm(tc, 'feature')


dtm_wordcloud(dtm, nterms = 20)

## or without a DTM
dtm_wordcloud(terms = c('in','the','cloud'), freqs = c(2,5,10))

</code></pre>

<hr>
<h2 id='ego_semnet'>Create an ego network</h2><span id='topic+ego_semnet'></span>

<h3>Description</h3>

<p>Create an ego network from an igraph object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ego_semnet(
  g,
  vertex_names,
  depth = 1,
  only_filter_vertices = T,
  weight_attr = "weight",
  min_weight = NULL,
  top_edges = NULL,
  max_edges_level = NULL,
  directed = c("out", "in")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ego_semnet_+3A_g">g</code></td>
<td>
<p>an igraph object</p>
</td></tr>
<tr><td><code id="ego_semnet_+3A_vertex_names">vertex_names</code></td>
<td>
<p>a character string with the names of the ego vertices/nodes</p>
</td></tr>
<tr><td><code id="ego_semnet_+3A_depth">depth</code></td>
<td>
<p>the number of degrees from the ego vertices/nodes that are included. 1 means that only the direct neighbours are included</p>
</td></tr>
<tr><td><code id="ego_semnet_+3A_only_filter_vertices">only_filter_vertices</code></td>
<td>
<p>if True, the algorithm will only filter out vertices/nodes that are not in the ego network. If False (default) then it also filters out the edges.</p>
</td></tr>
<tr><td><code id="ego_semnet_+3A_weight_attr">weight_attr</code></td>
<td>
<p>the name of the edge attribute. if NA, no weight is used, and min_weight and top_edges are ignored</p>
</td></tr>
<tr><td><code id="ego_semnet_+3A_min_weight">min_weight</code></td>
<td>
<p>a number indicating the minimum weight</p>
</td></tr>
<tr><td><code id="ego_semnet_+3A_top_edges">top_edges</code></td>
<td>
<p>for each vertex within the given depth, only keep the top n edges with the strongest edge weight. Can also be a vector of the same length as the depth value, in which case a different value is used at each level: first value for level 1, second value for level 2, etc.</p>
</td></tr>
<tr><td><code id="ego_semnet_+3A_max_edges_level">max_edges_level</code></td>
<td>
<p>the maximum number of edges to be added at each level of depth.</p>
</td></tr>
<tr><td><code id="ego_semnet_+3A_directed">directed</code></td>
<td>
<p>if the network is directed, specify whether 'out' degrees or 'in' degrees are used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is similar to the ego function in igraph, but with some notable differences. Firstly, if multiple vertex_names are given, the ego network for both is given in 1 network (whereas igraph creates a list of networks). Secondly, the min_weight and top_edges parameters can be used to focus on the strongest edges.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c('a b c', 'd e f', 'a d'))
g = semnet(tc, 'token')

igraph::get.data.frame(g)
plot_semnet(g)
## only keep nodes directly connected to given node
g_ego = ego_semnet(g, 'e')
igraph::get.data.frame(g_ego)
plot_semnet(g_ego)

## only keep edges directly connected to given node
g_ego = ego_semnet(g, 'e', only_filter_vertices = FALSE)
igraph::get.data.frame(g_ego)
plot_semnet(g_ego)

## only keep nodes connected to given node with a specified degree (i.e. distance)
g_ego = ego_semnet(g, 'e', depth = 2)
igraph::get.data.frame(g_ego)
plot_semnet(g_ego)
</code></pre>

<hr>
<h2 id='export_span_annotations'>Export span annotations</h2><span id='topic+export_span_annotations'></span>

<h3>Description</h3>

<p>Export columns from a tCorpus as span annotations (annotations over a span of text).
The annotations are returned as a data.table where each row is an annotation, with columns:
doc_id, variable, value, field, offset, length and text. The key purpose is that these span annotations
are linked to exact character positions in the text. This also means that this function can 
only be used if position information is available (i.e. if remember_spaces=T was used when creating the tCorpus)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_span_annotations(tc, variables)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_span_annotations_+3A_tc">tc</code></td>
<td>
<p>A tCorpus, created with <code><a href="#topic+create_tcorpus">create_tcorpus</a></code>, where remember_spaces must have been set to TRUE</p>
</td></tr>
<tr><td><code id="export_span_annotations_+3A_variables">variables</code></td>
<td>
<p>A character vector with variables (columns in tc$tokens) to export</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that if there are spans with gaps in them (e.g. based on proximity queries), they are split into different annotations.
Thus some information can be lost.
</p>


<h3>Value</h3>

<p>A data.table where each row is a span annotation, with columns: doc_id, variable, value, field, offset, length, text
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(sotu_texts, c('president','text'), doc_column='id', remember_spaces=TRUE)
tc$code_features(c('war# war peace', 'us being# &lt;(i we) (am are)&gt;'))
export_span_annotations(tc, 'code')
</code></pre>

<hr>
<h2 id='feature_associations'>Get common nearby features given a query or query hits</h2><span id='topic+feature_associations'></span>

<h3>Description</h3>

<p>Get common nearby features given a query or query hits
</p>


<h3>Usage</h3>

<pre><code class='language-R'>feature_associations(
  tc,
  feature,
  query = NULL,
  hits = NULL,
  query_feature = "token",
  window = 15,
  n = 25,
  min_freq = 1,
  sort_by = c("chi2", "ratio", "freq"),
  subset = NULL,
  subset_meta = NULL,
  include_self = F
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feature_associations_+3A_tc">tc</code></td>
<td>
<p>a <a href="#topic+tCorpus">tCorpus</a></p>
</td></tr>
<tr><td><code id="feature_associations_+3A_feature">feature</code></td>
<td>
<p>The name of the feature column in $tokens</p>
</td></tr>
<tr><td><code id="feature_associations_+3A_query">query</code></td>
<td>
<p>A character string that is a query. See <a href="#topic+search_features">search_features</a> for documentation of the query language.</p>
</td></tr>
<tr><td><code id="feature_associations_+3A_hits">hits</code></td>
<td>
<p>Alternatively, instead of giving a query, the results of <a href="#topic+search_features">search_features</a> can be used.</p>
</td></tr>
<tr><td><code id="feature_associations_+3A_query_feature">query_feature</code></td>
<td>
<p>If query is used, the column in $tokens on which the query is performed. By default uses 'token'</p>
</td></tr>
<tr><td><code id="feature_associations_+3A_window">window</code></td>
<td>
<p>The size of the word window (i.e. the number of words next to the feature)</p>
</td></tr>
<tr><td><code id="feature_associations_+3A_n">n</code></td>
<td>
<p>the top n of associated features</p>
</td></tr>
<tr><td><code id="feature_associations_+3A_min_freq">min_freq</code></td>
<td>
<p>Optionally, ignore features that occur less than min_freq times</p>
</td></tr>
<tr><td><code id="feature_associations_+3A_sort_by">sort_by</code></td>
<td>
<p>The value by which to sort the features</p>
</td></tr>
<tr><td><code id="feature_associations_+3A_subset">subset</code></td>
<td>
<p>A call (or character string of a call) as one would normally pass to subset.tCorpus. If given, the keyword has to occur within the subset. This is for instance usefull to only look in named entity POS tags when searching for people or organization. Note that the condition does not have to occur within the subset.</p>
</td></tr>
<tr><td><code id="feature_associations_+3A_subset_meta">subset_meta</code></td>
<td>
<p>A call (or character string of a call) as one would normally pass to the subset_meta parameter of subset.tCorpus. If given, the keyword has to occur within the subset documents. This is for instance usefull to make queries date dependent. For example, in a longitudinal analysis of politicians, it is often required to take changing functions and/or party affiliations into account. This can be accomplished by using subset_meta = &quot;date &gt; xxx &amp; date &lt; xxx&quot; (given that the appropriate date column exists in the meta data).</p>
</td></tr>
<tr><td><code id="feature_associations_+3A_include_self">include_self</code></td>
<td>
<p>If True, include the feature itself in the output</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tc = create_tcorpus(sotu_texts, doc_column = 'id')
tc$preprocess()

## directly from query
topf = feature_associations(tc, 'feature', 'war')
head(topf, 20) ## frequent words close to "war"

## adjust window size
topf = feature_associations(tc, 'feature', 'war', window = 5)
head(topf, 20) ## frequent words very close (five tokens) to "war"

## you can also first perform search_features, to get hits for (complex) queries
hits = search_features(tc, '"war terror"~10')
topf = feature_associations(tc, 'feature', hits = hits)
head(topf, 20) ## frequent words close to the combination of "war" and "terror" within 10 words

</code></pre>

<hr>
<h2 id='feature_stats'>Feature statistics</h2><span id='topic+feature_stats'></span>

<h3>Description</h3>

<p>Compute a number of useful statistics for features: term frequency, idf, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>feature_stats(tc, feature, context_level = c("document", "sentence"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feature_stats_+3A_tc">tc</code></td>
<td>
<p>a tCorpus</p>
</td></tr>
<tr><td><code id="feature_stats_+3A_feature">feature</code></td>
<td>
<p>The name of the feature column</p>
</td></tr>
<tr><td><code id="feature_stats_+3A_context_level">context_level</code></td>
<td>
<p>Should results be returned at document or sentence level</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c('Text one first sentence. Text one second sentence', 'Text two'),
                    split_sentences = TRUE)


fs = feature_stats(tc, 'token')
head(fs)
fs = feature_stats(tc, 'token', context_level = 'sentence')
head(fs)

</code></pre>

<hr>
<h2 id='fold_rsyntax'>Fold rsyntax annotations</h2><span id='topic+fold_rsyntax'></span>

<h3>Description</h3>

<p>If a tCorpus has rsyntax annotations (see <code><a href="#topic+annotate_rsyntax">annotate_rsyntax</a></code>), it can be convenient to aggregate tokens that have a certain semantic label.
For example, if you have a query for labeling &quot;source&quot; and &quot;quote&quot;, you can add an aggegated value for the sources (such as a unique ID) as a column, and then remove the quote tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fold_rsyntax(tc, annotation, by_label, ..., txt = F, rm_by = T)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fold_rsyntax_+3A_tc">tc</code></td>
<td>
<p>A tCorpus</p>
</td></tr>
<tr><td><code id="fold_rsyntax_+3A_annotation">annotation</code></td>
<td>
<p>The name of an rsyntax annotation column</p>
</td></tr>
<tr><td><code id="fold_rsyntax_+3A_by_label">by_label</code></td>
<td>
<p>The labels in this column for which you want to aggregate the tokens</p>
</td></tr>
<tr><td><code id="fold_rsyntax_+3A_...">...</code></td>
<td>
<p>Specify the new aggregated columns in name-value pairs. The name is the name of the new column, and the value should be a function over a column in $tokens. 
For example:  subject = paste(token, collapse = ' ')  would create the column 'subject', of which the values are the concatenated tokens. See examples for more.</p>
</td></tr>
<tr><td><code id="fold_rsyntax_+3A_txt">txt</code></td>
<td>
<p>If TRUE, add _txt column with concatenated tokens for by_label.</p>
</td></tr>
<tr><td><code id="fold_rsyntax_+3A_rm_by">rm_by</code></td>
<td>
<p>If TRUE (default), remove the column(s) specified in by_label</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a transformed tCorpus
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = tc_sotu_udpipe$copy()
tc$udpipe_clauses()

fold_rsyntax(tc, 'clause', by_label = 'subject', subject = paste(token, collapse=' '))
</code></pre>

<hr>
<h2 id='freq_filter'>Support function for subset method</h2><span id='topic+freq_filter'></span>

<h3>Description</h3>

<p>Support function to enable subsetting by frequency stats of a given feature.
Should only be used within the tCorpus subset method, or any tCorpus method that supports a subset argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freq_filter(x, min = -Inf, max = Inf, top = NULL, bottom = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="freq_filter_+3A_x">x</code></td>
<td>
<p>the name of the feature column. Can be given as a call or a string.</p>
</td></tr>
<tr><td><code id="freq_filter_+3A_min">min</code></td>
<td>
<p>A number, setting the minimum frequency value</p>
</td></tr>
<tr><td><code id="freq_filter_+3A_max">max</code></td>
<td>
<p>A number, setting the maximum frequency value</p>
</td></tr>
<tr><td><code id="freq_filter_+3A_top">top</code></td>
<td>
<p>A number. If given, only the top x features with the highest frequency are TRUE</p>
</td></tr>
<tr><td><code id="freq_filter_+3A_bottom">bottom</code></td>
<td>
<p>A number. If given, only the bottom x features with the highest frequency are TRUE</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c('a a a b b'))

tc$tokens
tc$subset(subset = freq_filter(token, min=3))
tc$tokens
</code></pre>

<hr>
<h2 id='get_dtm'>Create a document term matrix.</h2><span id='topic+get_dtm'></span><span id='topic+get_dfm'></span>

<h3>Description</h3>

<p>Create a document term matrix. The default output is a sparse matrix (Matrix, TsparseMatrix). Alternatively, the dtm style from the tm and quanteda package can be used.
</p>
<p>The dfm function is shorthand for using quanteda's dfm (document feature matrix) class. The meta data in the tcorpus is then automatically added as docvars in the dfm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_dtm(
  tc,
  feature,
  context_level = c("document", "sentence"),
  weight = c("termfreq", "docfreq", "tfidf", "norm_tfidf"),
  drop_empty_terms = T,
  form = c("Matrix", "tm_dtm", "quanteda_dfm"),
  subset_tokens = NULL,
  subset_meta = NULL,
  context = NULL,
  context_labels = T,
  feature_labels = T,
  ngrams = NA,
  ngram_before_subset = F
)

get_dfm(
  tc,
  feature,
  context_level = c("document", "sentence"),
  weight = c("termfreq", "docfreq", "tfidf", "norm_tfidf"),
  drop_empty_terms = T,
  subset_tokens = NULL,
  subset_meta = NULL,
  context = NULL,
  context_labels = T,
  feature_labels = T,
  ngrams = NA,
  ngram_before_subset = F
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_dtm_+3A_tc">tc</code></td>
<td>
<p>a <code><a href="#topic+tCorpus">tCorpus</a></code></p>
</td></tr>
<tr><td><code id="get_dtm_+3A_feature">feature</code></td>
<td>
<p>The name of the feature column</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_context_level">context_level</code></td>
<td>
<p>Select whether the rows of the dtm should represent &quot;documents&quot; or &quot;sentences&quot;.</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_weight">weight</code></td>
<td>
<p>Select the weighting scheme for the DTM. Currently supports term frequency (termfreq), document frequency (docfreq), term frequency inverse document frequency (tfidf) and tfidf with normalized document vectors.</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_drop_empty_terms">drop_empty_terms</code></td>
<td>
<p>If True, tokens that do not occur (i.e. column where sum is 0) are ignored.</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_form">form</code></td>
<td>
<p>The output format. Default is a sparse matrix in the dgTMatrix class from the Matrix package. Alternatives are tm_dtm for a DocumentTermMatrix in the tm package format or quanteda_dfm for the document feature matrix from the quanteda package.</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_subset_tokens">subset_tokens</code></td>
<td>
<p>A subset call to select which rows to use in the DTM</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_subset_meta">subset_meta</code></td>
<td>
<p>A subset call for the meta data, to select which documents to use in the DTM</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_context">context</code></td>
<td>
<p>Instead of using the document or sentence context, an custom context can be specified. Has to be a vector of the same length as the number of tokens, that serves as the index column. Each unique value will be a row in the DTM.</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_context_labels">context_labels</code></td>
<td>
<p>If False, the DTM will not be given rownames</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_feature_labels">feature_labels</code></td>
<td>
<p>If False, the DTM will not be given column names</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_ngrams">ngrams</code></td>
<td>
<p>Optionally, use ngrams instead of individual tokens. This is more memory efficient than first creating an ngram feature in the tCorpus.</p>
</td></tr>
<tr><td><code id="get_dtm_+3A_ngram_before_subset">ngram_before_subset</code></td>
<td>
<p>If a subset is used, ngrams can be made before the subset, in which case an ngram can contain tokens that have been filtered out after the subset. Alternatively, if ngrams are made after the subset, ngrams will span over the gaps of tokens that are filtered out.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A document term matrix, in the format specified in the form argument
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c("First text first sentence. First text first sentence.",
                   "Second text first sentence"), doc_column = 'id', split_sentences = TRUE)

## Perform additional preprocessing on the 'token' column, and save as the 'feature' column
tc$preprocess('token', 'feature', remove_stopwords = TRUE, use_stemming = TRUE)
tc$tokens

## default: regular sparse matrix, using the Matrix package
m = get_dtm(tc, 'feature')
class(m)
m

## alternatively, create quanteda ('quanteda_dfm') or tm ('tm_dtm') class for DTM

m = get_dtm(tc, 'feature', form = 'quanteda_dfm')
class(m)
m


## create DTM with sentences as rows (instead of documents)
m = get_dtm(tc, 'feature', context_level = 'sentence')
nrow(m)

## use weighting
m = get_dtm(tc, 'feature', weight = 'norm_tfidf')
</code></pre>

<hr>
<h2 id='get_global_i'>Compute global feature positions</h2><span id='topic+get_global_i'></span>

<h3>Description</h3>

<p>Features are given global ids, with an added distance (max_window_size) between contexts (e.g., documents, sentences).
This way, the distance of features can be calculated across multiple contexts using a single vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_global_i(
  tc,
  context_level = c("document", "sentence"),
  max_window_size = 200
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_global_i_+3A_tc">tc</code></td>
<td>
<p>tCorpus object</p>
</td></tr>
<tr><td><code id="get_global_i_+3A_context_level">context_level</code></td>
<td>
<p>either 'document' or 'sentence'</p>
</td></tr>
<tr><td><code id="get_global_i_+3A_max_window_size">max_window_size</code></td>
<td>
<p>Determines the size of the gap between documents. Called max_window_size because this gap determines what the maximum window size is for non-overlapping windows between documents</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tCorpus object
</p>

<hr>
<h2 id='get_kwic'>Get keyword-in-context (KWIC) strings</h2><span id='topic+get_kwic'></span>

<h3>Description</h3>

<p>Create a data.frame with keyword-in-context strings for given indices (i), search results (hits) or search strings (keyword).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_kwic(
  tc,
  hits = NULL,
  i = NULL,
  query = NULL,
  code = "",
  ntokens = 10,
  n = NA,
  nsample = NA,
  output_feature = "token",
  query_feature = "token",
  context_level = c("document", "sentence"),
  kw_tag = c("&lt;", "&gt;"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_kwic_+3A_tc">tc</code></td>
<td>
<p>a tCorpus</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_hits">hits</code></td>
<td>
<p>results of feature search. see <a href="#topic+search_features">search_features</a>.</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_i">i</code></td>
<td>
<p>instead of the hits argument, you can give the indices of features directly.</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_query">query</code></td>
<td>
<p>instead of using the hits or i arguments, a search string can be given directly. Note that this simply a convenient shorthand for first creating a hits object with <a href="#topic+search_features">search_features</a>. If a query is given, then the ... argument is used to pass other arguments to <a href="#topic+search_features">search_features</a>.</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_code">code</code></td>
<td>
<p>if 'i' or 'query' is used, the code argument can be used to add a code label. Should be a vector of the same length that gives the code for each i or query, or a vector of length 1 for a single label.</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_ntokens">ntokens</code></td>
<td>
<p>an integers specifying the size of the context, i.e. the number of tokens left and right of the keyword.</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_n">n</code></td>
<td>
<p>a number, specifying the total number of hits</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_nsample">nsample</code></td>
<td>
<p>like n, but with a random sample of hits. If multiple codes are used, the sample is drawn for each code individually.</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_output_feature">output_feature</code></td>
<td>
<p>the feature column that is used to make the KWIC.</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_query_feature">query_feature</code></td>
<td>
<p>If query is used, the feature column that is used to perform the query</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_context_level">context_level</code></td>
<td>
<p>Select the maxium context (document or sentence).</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_kw_tag">kw_tag</code></td>
<td>
<p>a character vector of length 2, that gives the symbols before (first value) and after (second value) the keyword in the KWIC string. Can for instance be used to prepare KWIC with format tags for highlighting.</p>
</td></tr>
<tr><td><code id="get_kwic_+3A_...">...</code></td>
<td>
<p>See <a href="#topic+search_features">search_features</a> for the query parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is mainly for viewing results in the R console. If you want to create a subset corpus based on the context
of query results, you can use <a href="#topic+subset_query">subset_query</a> with the window argument. Also, the <a href="#topic+browse_hits">browse_hits</a> function
is a good alternative for viewing query hits in full text.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = tokens_to_tcorpus(corenlp_tokens, sentence_col = 'sentence', token_id_col = 'id')

## look directly for a term (or complex query)
get_kwic(tc, query = 'love*')

## or, first perform a feature search, and then get the KWIC for the results
hits = search_features(tc, '(john OR mark) AND mary AND love*', context_level = 'sentence')
get_kwic(tc, hits=hits, context_level = 'sentence')
</code></pre>

<hr>
<h2 id='get_stopwords'>Get a character vector of stopwords</h2><span id='topic+get_stopwords'></span>

<h3>Description</h3>

<p>Get a character vector of stopwords
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_stopwords(lang)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_stopwords_+3A_lang">lang</code></td>
<td>
<p>The language. Current options are: &quot;danish&quot;, &quot;dutch&quot;, &quot;english&quot;, &quot;finnish&quot;, &quot;french&quot;, &quot;german&quot;, &quot;hungarian&quot;, &quot;italian&quot;, &quot;norwegian&quot;, &quot;portuguese&quot;, &quot;romanian&quot;, &quot;russian&quot;, &quot;spanish&quot; and &quot;swedish&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector containing stopwords
</p>


<h3>Examples</h3>

<pre><code class='language-R'>en_stop = get_stopwords('english')
nl_stop = get_stopwords('dutch')
ge_stop = get_stopwords('german')

head(en_stop)
head(nl_stop)
head(ge_stop)
</code></pre>

<hr>
<h2 id='laplace'>Laplace (i.e. add constant) smoothing</h2><span id='topic+laplace'></span>

<h3>Description</h3>

<p>Laplace (i.e. add constant) smoothing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>laplace(freq, add = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="laplace_+3A_freq">freq</code></td>
<td>
<p>A numeric vector of term frequencies (integers).</p>
</td></tr>
<tr><td><code id="laplace_+3A_add">add</code></td>
<td>
<p>The added value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector with the smoothed term proportions
</p>


<h3>Examples</h3>

<pre><code class='language-R'>laplace(c(0,0,1,1,1,2,2,2,3,3,4,7,10))
</code></pre>

<hr>
<h2 id='melt_quanteda_dict'>Convert a quanteda dictionary to a long data.table format</h2><span id='topic+melt_quanteda_dict'></span>

<h3>Description</h3>

<p>This is used internally in the tCorpus dictionary search functions, but can be used manually
for more control. For example, adding numeric scores for sentiment dictionaries, and specifying which label/code to use in search_dictionary().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>melt_quanteda_dict(dict, column = "code", .index = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="melt_quanteda_dict_+3A_dict">dict</code></td>
<td>
<p>The quanteda dictionary</p>
</td></tr>
<tr><td><code id="melt_quanteda_dict_+3A_column">column</code></td>
<td>
<p>The name of the column with the label/code. If dictionary contains multiple levels,
additional columns are added with the suffix _l[i], where [i] is the level.</p>
</td></tr>
<tr><td><code id="melt_quanteda_dict_+3A_.index">.index</code></td>
<td>
<p>Do not use (used for recursive melting)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
d = quanteda::data_dictionary_LSD2015
melt_quanteda_dict(d)

</code></pre>

<hr>
<h2 id='merge_tcorpora'>Merge tCorpus objects</h2><span id='topic+merge_tcorpora'></span>

<h3>Description</h3>

<p>Create one tcorpus based on multiple tcorpus objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_tcorpora(
  ...,
  keep_data = c("intersect", "all"),
  keep_meta = c("intersect", "all"),
  if_duplicate = c("stop", "rename", "drop"),
  duplicate_tag = "#D"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge_tcorpora_+3A_...">...</code></td>
<td>
<p>tCorpus objects, or a list with tcorpus objects</p>
</td></tr>
<tr><td><code id="merge_tcorpora_+3A_keep_data">keep_data</code></td>
<td>
<p>if 'intersect', then only the token data columns that occur in all tCorpurs objects are kept</p>
</td></tr>
<tr><td><code id="merge_tcorpora_+3A_keep_meta">keep_meta</code></td>
<td>
<p>if 'intersect', then only the document meta columns that occur in all tCorpurs objects are kept</p>
</td></tr>
<tr><td><code id="merge_tcorpora_+3A_if_duplicate">if_duplicate</code></td>
<td>
<p>determine behaviour if there are duplicate doc_ids across tcorpora. By default, this yields an error, but you can set it to &quot;rename&quot; to change the names of duplicates (which makes sense of only the doc_ids are duplicate, but not the actual content), or &quot;drop&quot; to ignore duplicates, keeping only the first unique occurence.</p>
</td></tr>
<tr><td><code id="merge_tcorpora_+3A_duplicate_tag">duplicate_tag</code></td>
<td>
<p>a character string. if if_duplicates is &quot;rename&quot;, this tag is added to the document id. (this is repeated till no duplicates remain)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tCorpus object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc1 = create_tcorpus(sotu_texts[1:10,], doc_column = 'id')
tc2 = create_tcorpus(sotu_texts[11:20,], doc_column = 'id')
tc = merge_tcorpora(tc1, tc2)
tc$n_meta

#### duplicate handling ####
tc1 = create_tcorpus(sotu_texts[1:10,], doc_column = 'id')
tc2 = create_tcorpus(sotu_texts[6:15,], doc_column = 'id')


## with "rename", has 20 documents of which 5 duplicates
tc = merge_tcorpora(tc1,tc2, if_duplicate = 'rename')
tc$n_meta
sum(grepl('#D', tc$meta$doc_id))

## with "drop", has 15 documents without duplicates
tc = merge_tcorpora(tc1,tc2, if_duplicate = 'drop')
tc$n_meta
mean(grepl('#D', tc$meta$doc_id))
</code></pre>

<hr>
<h2 id='plot_semnet'>Visualize a semnet network</h2><span id='topic+plot_semnet'></span>

<h3>Description</h3>

<p>plot_semnet is a wrapper for the plot.igraph() function optimized for plotting a semantic network of the &quot;semnet&quot; class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_semnet(
  g,
  weight_attr = "weight",
  min_weight = NA,
  delete_isolates = F,
  vertexsize_attr = "freq",
  vertexsize_coef = 1,
  vertexcolor_attr = NA,
  edgewidth_coef = 1,
  max_backbone_alpha = NA,
  labelsize_coef = 1,
  labelspace_coef = 1.1,
  reduce_labeloverlap = F,
  redo_layout = F,
  return_graph = T,
  vertex.label.dist = 0.25,
  layout_fun = igraph::layout_with_fr,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_semnet_+3A_g">g</code></td>
<td>
<p>A network in the igraph format. Specifically designed for the output of coOccurenceNetwork() and windowedCoOccurenceNetwork()</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_weight_attr">weight_attr</code></td>
<td>
<p>The name of the weight attribute. Default is 'weight'</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_min_weight">min_weight</code></td>
<td>
<p>The minimum weight. All edges with a lower weight are dropped</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_delete_isolates">delete_isolates</code></td>
<td>
<p>If TRUE, isolate vertices (also after applying min_weight) are dropped</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_vertexsize_attr">vertexsize_attr</code></td>
<td>
<p>a character string indicating a vertex attribute that represents size. Default is 'freq', which is created in the coOccurenceNetwork functions to indicate the number of times a token occured.</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_vertexsize_coef">vertexsize_coef</code></td>
<td>
<p>a coefficient for changing the vertex size.</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_vertexcolor_attr">vertexcolor_attr</code></td>
<td>
<p>a character string indicating a vertex attribute that represents color. The attribute can also be a numeric value (e.g., a cluster membership) in which case colors are assigned to numbers. If no (valid) color attribute is given, vertex color are based on undirected fastgreedy.community() clustering.</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_edgewidth_coef">edgewidth_coef</code></td>
<td>
<p>a coefficient for changing the edge width</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_max_backbone_alpha">max_backbone_alpha</code></td>
<td>
<p>If g has an edge attribute named alpha (added if backbone extraction is used), this specifies the maximum alpha value.</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_labelsize_coef">labelsize_coef</code></td>
<td>
<p>a coefficient for increasing or decreasing the size of the vertexlabel.</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_labelspace_coef">labelspace_coef</code></td>
<td>
<p>a coefficient that roughly determines the minimal distance between vertex labels, based on the size of labels. Only used if reduce_labeloverlap is TRUE.</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_reduce_labeloverlap">reduce_labeloverlap</code></td>
<td>
<p>if TRUE, an algorithm is used to reduce overlap as best as possible.</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_redo_layout">redo_layout</code></td>
<td>
<p>If TRUE, a new layout will be calculated using layout_with_fr(). If g does not have a layout attribute (g$layout), a new layout is automatically calculated.</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_return_graph">return_graph</code></td>
<td>
<p>if TRUE, plot_semnet() also returns the graph object with the attributes and layout as shown in the plot.</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_vertex.label.dist">vertex.label.dist</code></td>
<td>
<p>The distance of the label to the center of the vertex</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_layout_fun">layout_fun</code></td>
<td>
<p>The igraph layout function that is used.</p>
</td></tr>
<tr><td><code id="plot_semnet_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on to plot.igraph()</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Before plotting the network, the set_network_attributes() function is used to set pretty defaults for plotting. Optionally, reduce_labeloverlap can be used to prevent labeloverlap (as much as possible).
</p>


<h3>Value</h3>

<p>Plots a network, and returns the network object if return_graph is TRUE.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(sotu_texts, doc_column = 'id')
tc$preprocess('token','feature', remove_stopwords = TRUE, use_stemming = TRUE, min_docfreq=10)

g = semnet_window(tc, 'feature', window.size = 10)
g = backbone_filter(g, max_vertices = 100)
plot_semnet(g)

</code></pre>

<hr>
<h2 id='plot_words'>Plot a wordcloud with words ordered and coloured according to a dimension (x)</h2><span id='topic+plot_words'></span>

<h3>Description</h3>

<p>Plot a wordcloud with words ordered and coloured according to a dimension (x)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_words(
  x,
  y = NULL,
  words,
  wordfreq = rep(1, length(x)),
  xlab = "",
  ylab = "",
  yaxt = "n",
  scale = 1,
  random.y = T,
  xlim = NULL,
  ylim = NULL,
  col = c("darkred", "navyblue"),
  fixed_col = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_words_+3A_x">x</code></td>
<td>
<p>The (approximate) x positions of the words</p>
</td></tr>
<tr><td><code id="plot_words_+3A_y">y</code></td>
<td>
<p>The (approximate) y positions of the words</p>
</td></tr>
<tr><td><code id="plot_words_+3A_words">words</code></td>
<td>
<p>A character vector with the words to plot</p>
</td></tr>
<tr><td><code id="plot_words_+3A_wordfreq">wordfreq</code></td>
<td>
<p>The frequency of the words, defaulting to 1</p>
</td></tr>
<tr><td><code id="plot_words_+3A_xlab">xlab</code></td>
<td>
<p>Label of the x axis</p>
</td></tr>
<tr><td><code id="plot_words_+3A_ylab">ylab</code></td>
<td>
<p>Label of the y axis</p>
</td></tr>
<tr><td><code id="plot_words_+3A_yaxt">yaxt</code></td>
<td>
<p>see <code>par</code> documentation</p>
</td></tr>
<tr><td><code id="plot_words_+3A_scale">scale</code></td>
<td>
<p>Maximum size to scale the wordsize</p>
</td></tr>
<tr><td><code id="plot_words_+3A_random.y">random.y</code></td>
<td>
<p>if TRUE, the y position of words is random, otherwise it represents the word frequency.</p>
</td></tr>
<tr><td><code id="plot_words_+3A_xlim">xlim</code></td>
<td>
<p>Starting value of x axis</p>
</td></tr>
<tr><td><code id="plot_words_+3A_ylim">ylim</code></td>
<td>
<p>Starting value of y axis</p>
</td></tr>
<tr><td><code id="plot_words_+3A_col">col</code></td>
<td>
<p>A vector of colors that is passed to colorRamp to interpolate colors over x axis</p>
</td></tr>
<tr><td><code id="plot_words_+3A_fixed_col">fixed_col</code></td>
<td>
<p>Optionally, a vector of the exact colors given to words.</p>
</td></tr>
<tr><td><code id="plot_words_+3A_...">...</code></td>
<td>
<p>additional parameters passed to the plot function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nothing
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x = c(-10, -5, 3, 5)
y = c(0, 2, 5, 10)
words = c('words', 'where', 'you', 'like')


plot_words(x,y,words, c(1,2,3,4))

</code></pre>

<hr>
<h2 id='plot.contextHits'>S3 plot for contextHits class</h2><span id='topic+plot.contextHits'></span>

<h3>Description</h3>

<p>S3 plot for contextHits class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'contextHits'
plot(x, min_weight = 0, backbone_alpha = NA, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.contextHits_+3A_x">x</code></td>
<td>
<p>a contextHits object, as returned by <a href="#topic+search_contexts">search_contexts</a></p>
</td></tr>
<tr><td><code id="plot.contextHits_+3A_min_weight">min_weight</code></td>
<td>
<p>Optionally, the minimum weight for an edge in the network</p>
</td></tr>
<tr><td><code id="plot.contextHits_+3A_backbone_alpha">backbone_alpha</code></td>
<td>
<p>Optionally, the alpha threshold for backbone extraction (similar to a p-value, and lower is more strict)</p>
</td></tr>
<tr><td><code id="plot.contextHits_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
tc = create_tcorpus(sotu_texts, doc_column='id')
hits = search_contexts(tc, c('War# war* OR army OR bomb*','Terrorism# terroris*',
                              'Economy# econom* OR bank*','Education# educat* OR school*'))

plot(hits)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.featureAssociations'>visualize feature associations</h2><span id='topic+plot.featureAssociations'></span>

<h3>Description</h3>

<p>visualize feature associations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'featureAssociations'
plot(x, n = 25, size = c("chi2", "freq", "ratio"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.featureAssociations_+3A_x">x</code></td>
<td>
<p>a featureAssociations object, created with the <a href="#topic+feature_associations">feature_associations</a> function</p>
</td></tr>
<tr><td><code id="plot.featureAssociations_+3A_n">n</code></td>
<td>
<p>the number of words in the plot</p>
</td></tr>
<tr><td><code id="plot.featureAssociations_+3A_size">size</code></td>
<td>
<p>use &quot;freq&quot;, &quot;chi2&quot; or &quot;ratio&quot; for determining the size of words</p>
</td></tr>
<tr><td><code id="plot.featureAssociations_+3A_...">...</code></td>
<td>
<p>additional arguments passed to dtm_wordcloud</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## as example, compare SOTU paragraphs about taxes to rest
tc = create_tcorpus(sotu_texts[1:100,], doc_column = 'id')
comp = compare_subset(tc, 'token', query_x = 'tax*')


plot(comp, balance=TRUE)
plot(comp, mode = 'ratio_x')
plot(comp, mode = 'ratio_y')

</code></pre>

<hr>
<h2 id='plot.featureHits'>S3 plot for featureHits class</h2><span id='topic+plot.featureHits'></span>

<h3>Description</h3>

<p>S3 plot for featureHits class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'featureHits'
plot(x, min_weight = 0, backbone_alpha = NA, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.featureHits_+3A_x">x</code></td>
<td>
<p>a featureHits object, as returned by <a href="#topic+search_features">search_features</a></p>
</td></tr>
<tr><td><code id="plot.featureHits_+3A_min_weight">min_weight</code></td>
<td>
<p>Optionally, the minimum weight for an edge in the network</p>
</td></tr>
<tr><td><code id="plot.featureHits_+3A_backbone_alpha">backbone_alpha</code></td>
<td>
<p>Optionally, the alpha threshold for backbone extraction (similar to a p-value, and lower is more strict)</p>
</td></tr>
<tr><td><code id="plot.featureHits_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
tc = create_tcorpus(sotu_texts, doc_column='id')
hits = search_features(tc, c('War# war* OR army OR bomb*','Terrorism# terroris*',
                              'Economy# econom* OR bank*','Education# educat* OR school*'))
plot(hits)

</code></pre>

<hr>
<h2 id='plot.vocabularyComparison'>visualize vocabularyComparison</h2><span id='topic+plot.vocabularyComparison'></span>

<h3>Description</h3>

<p>visualize vocabularyComparison
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vocabularyComparison'
plot(
  x,
  n = 25,
  mode = c("both", "ratio_x", "ratio_y"),
  balance = T,
  size = c("chi2", "freq", "ratio"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.vocabularyComparison_+3A_x">x</code></td>
<td>
<p>a vocabularyComparison object, created with the <a href="#topic+compare_corpus">compare_corpus</a> or <a href="#topic+compare_subset">compare_subset</a> method</p>
</td></tr>
<tr><td><code id="plot.vocabularyComparison_+3A_n">n</code></td>
<td>
<p>the number of words in the plot</p>
</td></tr>
<tr><td><code id="plot.vocabularyComparison_+3A_mode">mode</code></td>
<td>
<p>use &quot;both&quot; to plot both overrepresented and underrepresented words using the plot_words function.
Whether a term is under- or overrepresented is indicated on the x-axis, which shows the log ratios (negative is underrepresented, positive is overrepresented).
Use &quot;ratio_x&quot; or &quot;ratio_y&quot; to only plot overrepresented or underrepresented words using dtm_wordcloud</p>
</td></tr>
<tr><td><code id="plot.vocabularyComparison_+3A_balance">balance</code></td>
<td>
<p>if TRUE, get an equal amount of terms on the left (underrepresented) and right (overrepresented) side. If FALSE, the top chi words are used, regardless of ratio.</p>
</td></tr>
<tr><td><code id="plot.vocabularyComparison_+3A_size">size</code></td>
<td>
<p>use &quot;freq&quot;, &quot;chi2&quot; or &quot;ratio&quot; for determining the size of words</p>
</td></tr>
<tr><td><code id="plot.vocabularyComparison_+3A_...">...</code></td>
<td>
<p>additional arguments passed to plot_words (&quot;both&quot; mode) or dtm_wordcloud (ratio modes)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## as example, compare SOTU paragraphs about taxes to rest
tc = create_tcorpus(sotu_texts[1:100,], doc_column = 'id')
comp = compare_subset(tc, 'token', query_x = 'tax*')


plot(comp, balance=TRUE)
plot(comp, mode = 'ratio_x')
plot(comp, mode = 'ratio_y')

</code></pre>

<hr>
<h2 id='preprocess_tokens'>Preprocess tokens in a character vector</h2><span id='topic+preprocess_tokens'></span>

<h3>Description</h3>

<p>Preprocess tokens in a character vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess_tokens(
  x,
  context = NULL,
  language = "english",
  use_stemming = F,
  lowercase = T,
  ngrams = 1,
  replace_whitespace = F,
  as_ascii = F,
  remove_punctuation = T,
  remove_stopwords = F,
  remove_numbers = F,
  min_freq = NULL,
  min_docfreq = NULL,
  max_freq = NULL,
  max_docfreq = NULL,
  min_char = NULL,
  max_char = NULL,
  ngram_skip_empty = T
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_tokens_+3A_x">x</code></td>
<td>
<p>A character or factor vector in which each element is a token (i.e. a tokenized text)</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_context">context</code></td>
<td>
<p>Optionally, a character vector of the same length as x, specifying the context of token (e.g., document, sentence). Has to be given if ngram &gt; 1</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_language">language</code></td>
<td>
<p>The language used for stemming and removing stopwords</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_use_stemming">use_stemming</code></td>
<td>
<p>Logical, use stemming. (Make sure the specify the right language!)</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_lowercase">lowercase</code></td>
<td>
<p>Logical, make token lowercase</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_ngrams">ngrams</code></td>
<td>
<p>A number, specifying the number of tokens per ngram. Default is unigrams (1).</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_replace_whitespace">replace_whitespace</code></td>
<td>
<p>Logical. If TRUE, all whitespace is replaced by underscores</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_as_ascii">as_ascii</code></td>
<td>
<p>Logical. If TRUE, tokens will be forced to ascii</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_remove_punctuation">remove_punctuation</code></td>
<td>
<p>Logical. if TRUE, punctuation is removed</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_remove_stopwords">remove_stopwords</code></td>
<td>
<p>Logical. If TRUE, stopwords are removed (Make sure to specify the right language!)</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>remove features that are only numbers</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_min_freq">min_freq</code></td>
<td>
<p>an integer, specifying minimum token frequency.</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_min_docfreq">min_docfreq</code></td>
<td>
<p>an integer, specifying minimum document frequency.</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_max_freq">max_freq</code></td>
<td>
<p>an integer, specifying minimum token frequency.</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_max_docfreq">max_docfreq</code></td>
<td>
<p>an integer, specifying minimum document frequency.</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_min_char">min_char</code></td>
<td>
<p>an integer, specifying minimum number of characters in a term</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_max_char">max_char</code></td>
<td>
<p>an integer, specifying maximum number of characters in a term</p>
</td></tr>
<tr><td><code id="preprocess_tokens_+3A_ngram_skip_empty">ngram_skip_empty</code></td>
<td>
<p>if ngrams are used, determines whether empty (filtered out) terms are skipped (i.e. c(&quot;this&quot;, NA, &quot;test&quot;), becomes &quot;this_test&quot;) or</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a factor vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tokens = c('I', 'am', 'a', 'SHORT', 'example', 'sentence', '!')

## default is lowercase without punctuation
preprocess_tokens(tokens)

## optionally, delete stopwords, perform stemming, and make ngrams
preprocess_tokens(tokens, remove_stopwords = TRUE, use_stemming = TRUE)
preprocess_tokens(tokens, context = NA, ngrams = 3)
</code></pre>

<hr>
<h2 id='print.contextHits'>S3 print for contextHits class</h2><span id='topic+print.contextHits'></span>

<h3>Description</h3>

<p>S3 print for contextHits class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'contextHits'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.contextHits_+3A_x">x</code></td>
<td>
<p>a contextHits object, as returned by <a href="#topic+search_contexts">search_contexts</a></p>
</td></tr>
<tr><td><code id="print.contextHits_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>text = c('A B C', 'D E F. G H I', 'A D', 'GGG')
tc = create_tcorpus(text, doc_id = c('a','b','c','d'), split_sentences = TRUE)
hits = search_contexts(tc, c('query label# A AND B', 'second query# (A AND Q) OR ("D E") OR I'))

hits
</code></pre>

<hr>
<h2 id='print.featureHits'>S3 print for featureHits class</h2><span id='topic+print.featureHits'></span>

<h3>Description</h3>

<p>S3 print for featureHits class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'featureHits'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.featureHits_+3A_x">x</code></td>
<td>
<p>a featureHits object, as returned by <a href="#topic+search_features">search_features</a></p>
</td></tr>
<tr><td><code id="print.featureHits_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>text = c('A B C', 'D E F. G H I', 'A D', 'GGG')
tc = create_tcorpus(text, doc_id = c('a','b','c','d'), split_sentences = TRUE)
hits = search_features(tc, c('query label# A AND B', 'second query# (A AND Q) OR ("D E") OR I'))

hits
</code></pre>

<hr>
<h2 id='print.tCorpus'>S3 print for tCorpus class</h2><span id='topic+print.tCorpus'></span>

<h3>Description</h3>

<p>S3 print for tCorpus class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tCorpus'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.tCorpus_+3A_x">x</code></td>
<td>
<p>a tCorpus object</p>
</td></tr>
<tr><td><code id="print.tCorpus_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c('First text', 'Second text'))
print(tc)
</code></pre>

<hr>
<h2 id='refresh_tcorpus'>Refresh a tCorpus object using the current version of corpustools</h2><span id='topic+refresh_tcorpus'></span>

<h3>Description</h3>

<p>As an R6 class, tCorpus contains its methods within the class object (i.e. itself). Therefore, if you use a new version of corpustools with an older tCorpus object (e.g., stored as a .rds. file), then the methods are not automatically updated. You can then use refresh_tcorpus() to reinitialize the tCorpus object with the current version of corpustools.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>refresh_tcorpus(tc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="refresh_tcorpus_+3A_tc">tc</code></td>
<td>
<p>a tCorpus object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tCorpus object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c('First text', 'Second text'))
refresh_tcorpus(tc)
</code></pre>

<hr>
<h2 id='require_package'>Check if package with given version exists</h2><span id='topic+require_package'></span>

<h3>Description</h3>

<p>Check if package with given version exists
</p>


<h3>Usage</h3>

<pre><code class='language-R'>require_package(package, min_version = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="require_package_+3A_package">package</code></td>
<td>
<p>The name of the package</p>
</td></tr>
<tr><td><code id="require_package_+3A_min_version">min_version</code></td>
<td>
<p>The minimum version</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An error if package does not exist
</p>

<hr>
<h2 id='search_contexts'>Search for documents or sentences using Boolean queries</h2><span id='topic+search_contexts'></span>

<h3>Description</h3>

<p>Search for documents or sentences using Boolean queries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>search_contexts(
  tc,
  query,
  code = NULL,
  feature = "token",
  context_level = c("document", "sentence"),
  not = F,
  verbose = F,
  as_ascii = F
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="search_contexts_+3A_tc">tc</code></td>
<td>
<p>a <code><a href="#topic+tCorpus">tCorpus</a></code></p>
</td></tr>
<tr><td><code id="search_contexts_+3A_query">query</code></td>
<td>
<p>A character string that is a query. See details for available query operators and modifiers. Can be multiple queries (as a vector), in which case it is recommended to also specifiy the code argument, to label results.</p>
</td></tr>
<tr><td><code id="search_contexts_+3A_code">code</code></td>
<td>
<p>If given, used as a label for the results of the query. Especially usefull if multiple queries are used.</p>
</td></tr>
<tr><td><code id="search_contexts_+3A_feature">feature</code></td>
<td>
<p>The name of the feature column</p>
</td></tr>
<tr><td><code id="search_contexts_+3A_context_level">context_level</code></td>
<td>
<p>Select whether the queries should occur within while &quot;documents&quot; or specific &quot;sentences&quot;. Returns results at the specified level.</p>
</td></tr>
<tr><td><code id="search_contexts_+3A_not">not</code></td>
<td>
<p>If TRUE, perform a NOT search. Return the articles/sentences for which the query is not found.</p>
</td></tr>
<tr><td><code id="search_contexts_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, progress messages will be printed</p>
</td></tr>
<tr><td><code id="search_contexts_+3A_as_ascii">as_ascii</code></td>
<td>
<p>if TRUE, perform search in ascii.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Brief summary of the query language
</p>
<p>The following operators and modifiers are supported:
</p>

<ul>
<li><p>The standaard Boolean operators: AND, OR and NOT. As a shorthand, an empty space can be used as an OR statement, so that &quot;this that those&quot; means &quot;this OR that OR those&quot;. NOT statements stricly mean AND NOT, so should only be used between terms. If you want to find <em>everything except</em> certain terms, you can use * (wildcard for <em>anything</em>) like this: &quot;* NOT (this that those)&quot;.
</p>
</li>
<li><p>For complex queries parentheses can (and should) be used. e.g. '(spam AND eggs) NOT (fish and (chips OR albatros))
</p>
</li>
<li><p>Wildcards ? and *. The questionmark can be used to match 1 unknown character or no character at all, e.g. &quot;?at&quot; would find &quot;cat&quot;, &quot;hat&quot; and &quot;at&quot;. The asterisk can be used to match any number of unknown characters. Both the asterisk and questionmark can be used at the start, end and within a term.
</p>
</li>
<li><p>Multitoken strings, or exact strings, can be specified using quotes. e.g. &quot;united states&quot;
</p>
</li>
<li><p>tokens within a given token distance can be found using quotes plus tilde and a number specifiying the token distance. e.g. &quot;climate chang*&quot;~10
</p>
</li>
<li><p>Alternatively, angle brackets (&lt;&gt;) can be used instead of quotes, which also enables nesting exact strings in proximity/window search
</p>
</li>
<li><p>Queries are not case sensitive, but can be made so by adding the ~s flag. e.g. COP~s only finds &quot;COP&quot; in uppercase. The ~s flag can also be used on quotes to make all terms within quotes case sensitive, and this can be combined with the token proximity flag. e.g. &quot;Marco Polo&quot;~s10
</p>
</li></ul>



<h3>Value</h3>

<p>A contextHits object, which is a list with $hits (data.frame with locations) and $queries (copy of queries for provenance)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text = c('A B C', 'D E F. G H I', 'A D', 'GGG')
tc = create_tcorpus(text, doc_id = c('a','b','c','d'), split_sentences = TRUE)
tc$tokens

hits = search_contexts(tc, c('query label# A AND B', 'second query# (A AND Q) OR ("D E") OR I'))
hits          ## print shows number of hits
hits$hits     ## hits is a list, with hits$hits being a data.frame with specific contexts
summary(hits) ## summary gives hits per query

## sentence level
hits = search_contexts(tc, c('query label# A AND B', 'second query# (A AND Q) OR ("D E") OR I'),
                          context_level = 'sentence')
hits$hits     ## hits is a list, with hits$hits being a data.frame with specific contexts



## query language examples

## single term
search_contexts(tc, 'A')$hits

search_contexts(tc, 'G*')$hits    ## wildcard *
search_contexts(tc, '*G')$hits    ## wildcard *
search_contexts(tc, 'G*G')$hits   ## wildcard *

search_contexts(tc, 'G?G')$hits   ## wildcard ?
search_contexts(tc, 'G?')$hits    ## wildcard ? (no hits)

## boolean
search_contexts(tc, 'A AND B')$hits
search_contexts(tc, 'A AND D')$hits
search_contexts(tc, 'A AND (B OR D)')$hits

search_contexts(tc, 'A NOT B')$hits
search_contexts(tc, 'A NOT (B OR D)')$hits


## sequence search (adjacent words)
search_contexts(tc, '"A B"')$hits
search_contexts(tc, '"A C"')$hits ## no hit, because not adjacent

search_contexts(tc, '"A (B OR D)"')$hits ## can contain nested OR
## cannot contain nested AND or NOT!!

search_contexts(tc, '&lt;A B&gt;')$hits ## can also use &lt;&gt; instead of "".

## proximity search (using ~ flag)
search_contexts(tc, '"A C"~5')$hits ## A AND C within a 5 word window
search_contexts(tc, '"A C"~1')$hits ## no hit, because A and C more than 1 word apart

search_contexts(tc, '"A (B OR D)"~5')$hits ## can contain nested OR
search_contexts(tc, '"A &lt;B C&gt;"~5')$hits    ## can contain nested sequence (must use &lt;&gt;)
search_contexts(tc, '&lt;A &lt;B C&gt;&gt;~5')$hits    ## (&lt;&gt; is always OK, but cannot nest quotes in quotes)
## cannot contain nested AND or NOT!!


## case sensitive search
search_contexts(tc, 'g')$hits     ## normally case insensitive
search_contexts(tc, 'g~s')$hits   ## use ~s flag to make term case sensitive

search_contexts(tc, '(a OR g)~s')$hits   ## use ~s flag on everything between parentheses
search_contexts(tc, '(a OR G)~s')$hits   ## use ~s flag on everything between parentheses

search_contexts(tc, '"a b"~s')$hits   ## use ~s flag on everything between quotes
search_contexts(tc, '"A B"~s')$hits   ## use ~s flag on everything between quotes


</code></pre>

<hr>
<h2 id='search_dictionary'>Dictionary lookup</h2><span id='topic+search_dictionary'></span>

<h3>Description</h3>

<p>Similar to search_features, but for fast matching of large dictionaries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>search_dictionary(
  tc,
  dict,
  token_col = "token",
  string_col = "string",
  code_col = "code",
  sep = " ",
  mode = c("unique_hits", "features"),
  case_sensitive = F,
  use_wildcards = T,
  ascii = F,
  verbose = F
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="search_dictionary_+3A_tc">tc</code></td>
<td>
<p>A tCorpus</p>
</td></tr>
<tr><td><code id="search_dictionary_+3A_dict">dict</code></td>
<td>
<p>A dictionary. Can be either a data.frame or a quanteda dictionary. If a data.frame is given, it has to
have a column named &quot;string&quot;  (or use string_col argument) that contains the dictionary terms, and a column &quot;code&quot; (or use code_col argument) that contains the
label/code represented by this string. Each row has a single string, that can be
a single word or a sequence of words seperated by a whitespace (e.g., &quot;not bad&quot;), and can have the common ? and * wildcards.
If a quanteda dictionary is given, it is automatically converted to this type of data.frame with the
<code><a href="#topic+melt_quanteda_dict">melt_quanteda_dict</a></code> function. This can be done manually for more control over labels.</p>
</td></tr>
<tr><td><code id="search_dictionary_+3A_token_col">token_col</code></td>
<td>
<p>The feature in tc that contains the token text.</p>
</td></tr>
<tr><td><code id="search_dictionary_+3A_string_col">string_col</code></td>
<td>
<p>If dict is a data.frame, the name of the column in dict with the dictionary lookup string. Default is &quot;string&quot;</p>
</td></tr>
<tr><td><code id="search_dictionary_+3A_code_col">code_col</code></td>
<td>
<p>The name of the column in dict with the dictionary code/label. Default is &quot;code&quot;.
If dict is a quanteda dictionary with multiple levels, &quot;code_l2&quot;, &quot;code_l3&quot;, etc. can be used to select levels..</p>
</td></tr>
<tr><td><code id="search_dictionary_+3A_sep">sep</code></td>
<td>
<p>A regular expression for separating multi-word lookup strings (default is &quot; &quot;, which is what quanteda dictionaries use).
For example, if the dictionary contains &quot;Barack Obama&quot;, sep should be &quot; &quot; so that it matches the consequtive tokens &quot;Barack&quot; and &quot;Obama&quot;.
In some dictionaries, however, it might say &quot;Barack+Obama&quot;, so in that case sep = '\\+' should be used.</p>
</td></tr>
<tr><td><code id="search_dictionary_+3A_mode">mode</code></td>
<td>
<p>There are two modes: &quot;unique_hits&quot; and &quot;features&quot;. The &quot;unique_hits&quot; mode prioritizes finding unique matches, which is recommended for counting how often a dictionary term occurs.
If a term matches multiple dictionary terms (which should only happen for nested multi-word terms, such as &quot;bad&quot; and &quot;not bad&quot;), the longest term is always used. 
The features mode does not delete duplicates.</p>
</td></tr>
<tr><td><code id="search_dictionary_+3A_case_sensitive">case_sensitive</code></td>
<td>
<p>logical, should lookup be case sensitive?</p>
</td></tr>
<tr><td><code id="search_dictionary_+3A_use_wildcards">use_wildcards</code></td>
<td>
<p>Use the wildcards * (any number including none of any character) and ? (one or none of any character). If FALSE, exact string matching is used</p>
</td></tr>
<tr><td><code id="search_dictionary_+3A_ascii">ascii</code></td>
<td>
<p>If true, convert text to ascii before matching</p>
</td></tr>
<tr><td><code id="search_dictionary_+3A_verbose">verbose</code></td>
<td>
<p>If true, report progress</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with the id value (taken from dict$id) for each row in tc$tokens
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dict = data.frame(string = c('this is', 'for a', 'not big enough'), code=c('a','c','b'))
tc = create_tcorpus(c('this is a test','This town is not big enough for a test'))
search_dictionary(tc, dict)$hits
</code></pre>

<hr>
<h2 id='search_features'>Find tokens using a Lucene-like search query</h2><span id='topic+search_features'></span>

<h3>Description</h3>

<p>Search tokens in a tokenlist using Lucene-like queries. For a detailed explanation of the query language, see the details below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>search_features(
  tc,
  query,
  code = NULL,
  feature = "token",
  mode = c("unique_hits", "features"),
  context_level = c("document", "sentence"),
  keep_longest = TRUE,
  as_ascii = F,
  verbose = F
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="search_features_+3A_tc">tc</code></td>
<td>
<p>a <code><a href="#topic+tCorpus">tCorpus</a></code></p>
</td></tr>
<tr><td><code id="search_features_+3A_query">query</code></td>
<td>
<p>A character string that is a query. See details for available query operators and modifiers. Can be multiple queries (as a vector), in which case it is recommended to also specifiy the code argument, to label results.</p>
</td></tr>
<tr><td><code id="search_features_+3A_code">code</code></td>
<td>
<p>The code given to the tokens that match the query (usefull when looking for multiple queries). Can also put code label in query with # (see details)</p>
</td></tr>
<tr><td><code id="search_features_+3A_feature">feature</code></td>
<td>
<p>The name of the feature column within which to search.</p>
</td></tr>
<tr><td><code id="search_features_+3A_mode">mode</code></td>
<td>
<p>There are two modes: &quot;unique_hits&quot; and &quot;features&quot;. The &quot;unique_hits&quot; mode prioritizes finding full and unique matches., which is recommended for counting how often a query occurs. However, this also means that some tokens for which the query is satisfied might not assigned a hit_id. The &quot;features&quot; mode, instead, prioritizes finding all tokens, which is recommended for coding coding features (the code_features and search_recode methods always use features mode).</p>
</td></tr>
<tr><td><code id="search_features_+3A_context_level">context_level</code></td>
<td>
<p>Select whether the queries should occur within while &quot;documents&quot; or specific &quot;sentences&quot;.</p>
</td></tr>
<tr><td><code id="search_features_+3A_keep_longest">keep_longest</code></td>
<td>
<p>If TRUE, then overlapping in case of overlapping queries strings in unique_hits mode, the query with the most separate terms is kept. For example, in the text &quot;mr. Bob Smith&quot;, the query [smith OR &quot;bob smith&quot;] would match &quot;Bob&quot; and &quot;Smith&quot;. If keep_longest is FALSE, the match that is used is determined by the order in the query itself. The same query would then match only &quot;Smith&quot;.</p>
</td></tr>
<tr><td><code id="search_features_+3A_as_ascii">as_ascii</code></td>
<td>
<p>if TRUE, perform search in ascii.</p>
</td></tr>
<tr><td><code id="search_features_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, progress messages will be printed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Brief summary of the query language
</p>
<p>The following operators and modifiers are supported:
</p>

<ul>
<li><p>The standaard Boolean operators: AND, OR and NOT. As a shorthand, an empty space can be used as an OR statement, so that &quot;this that those&quot; means &quot;this OR that OR those&quot;. NOT statements stricly mean AND NOT, so should only be used between terms. If you want to find <em>everything except</em> certain terms, you can use * (wildcard for <em>anything</em>) like this: &quot;* NOT (this that those)&quot;.
</p>
</li>
<li><p>For complex queries parentheses can (and should) be used. e.g. '(spam AND eggs) NOT (fish and (chips OR albatros))
</p>
</li>
<li><p>Wildcards ? and *. The questionmark can be used to match 1 unknown character or no character at all, e.g. &quot;?at&quot; would find &quot;cat&quot;, &quot;hat&quot; and &quot;at&quot;. The asterisk can be used to match any number of unknown characters. Both the asterisk and questionmark can be used at the start, end and within a term.
</p>
</li>
<li><p>Multitoken strings, or exact strings, can be specified using quotes. e.g. &quot;united states&quot;
</p>
</li>
<li><p>tokens within a given token distance can be found using quotes plus tilde and a number specifiying the token distance. e.g. &quot;climate chang*&quot;~10
</p>
</li>
<li><p>Alternatively, angle brackets (&lt;&gt;) can be used instead of quotes, which also enables nesting exact strings in proximity/window search
</p>
</li>
<li><p>Queries are not case sensitive, but can be made so by adding the ~s flag. e.g. COP~s only finds &quot;COP&quot; in uppercase. The ~s flag can also be used on parentheses or quotes to make all terms within case sensitive, and this can be combined with the token proximity flag. e.g. &quot;Marco Polo&quot;~s10
</p>
</li>
<li><p>The ~g (ghost) flag can be used to mark a term (or all terms within parentheses/quotes) as a ghost term. This has two effects. Firstly, features that match the query term will not be in the results. This is usefull if a certain term is important for getting reliable search results, but not conceptually relevant. Secondly, ghost terms can be used multiple times, in different query hits (only relevant in unique_hits mode). For example, in the text &quot;A B C&quot;, the query 'A~g AND (B C)' will return both B and C as separate hit, whereas 'A AND (B C)' will return A and B as a single hit.
</p>
</li>
<li><p>A code label can be included at the beginning of a query, followed by a # to start the query (label# query). Note that to search for a hashtag symbol, you need to escape it with \ (double \ in R character vector)
</p>
</li>
<li><p>Aside from the feature column (specified with the feature argument) a query can include any column in the token data. To manually select a column, use 'columnname: ' at the start of a query or nested query (i.e. between parentheses or quotes). See examples for clarification.
</p>
</li></ul>



<h3>Value</h3>

<p>A featureHits object, which is a list with $hits (data.frame with locations) and $queries (copy of queries for provenance)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text = c('A B C', 'D E F. G H I', 'A D', 'GGG')
tc = create_tcorpus(text, doc_id = c('a','b','c','d'), split_sentences = TRUE)
tc$tokens ## (example uses letters instead of words for simple query examples)

hits = search_features(tc, c('query label# A AND B', 'second query# (A AND Q) OR ("D E") OR I'))
hits          ## print shows number of hits
hits$hits     ## hits is a list, with hits$hits being a data.frame with specific features
summary(hits) ## summary gives hits per query

## sentence level
hits = search_features(tc, c('query label# A AND B', 'second query# (A AND Q) OR ("D E") OR I'),
                          context_level = 'sentence')
hits$hits     ## hits is a list, with hits$hits being a data.frame with specific features




## query language examples

## single term
search_features(tc, 'A')$hits

search_features(tc, 'G*')$hits    ## wildcard *
search_features(tc, '*G')$hits    ## wildcard *
search_features(tc, 'G*G')$hits   ## wildcard *

search_features(tc, 'G?G')$hits   ## wildcard ?
search_features(tc, 'G?')$hits    ## wildcard ? (no hits)

## boolean
search_features(tc, 'A AND B')$hits
search_features(tc, 'A AND D')$hits
search_features(tc, 'A AND (B OR D)')$hits

search_features(tc, 'A NOT B')$hits
search_features(tc, 'A NOT (B OR D)')$hits


## sequence search (adjacent words)
search_features(tc, '"A B"')$hits
search_features(tc, '"A C"')$hits ## no hit, because not adjacent

search_features(tc, '"A (B OR D)"')$hits ## can contain nested OR
## cannot contain nested AND or NOT!!

search_features(tc, '&lt;A B&gt;')$hits ## can also use &lt;&gt; instead of "".

## proximity search (using ~ flag)
search_features(tc, '"A C"~5')$hits ## A AND C within a 5 word window
search_features(tc, '"A C"~1')$hits ## no hit, because A and C more than 1 word apart

search_features(tc, '"A (B OR D)"~5')$hits ## can contain nested OR
search_features(tc, '"A &lt;B C&gt;"~5')$hits    ## can contain nested sequence (must use &lt;&gt;)
search_features(tc, '&lt;A &lt;B C&gt;&gt;~5')$hits    ## &lt;&gt; is always OK, but cannot nest "" in ""
## cannot contain nested AND or NOT!!

## case sensitive search (~s flag)
search_features(tc, 'g')$hits     ## normally case insensitive
search_features(tc, 'g~s')$hits   ## use ~s flag to make term case sensitive

search_features(tc, '(a OR g)~s')$hits   ## use ~s flag on everything between parentheses
search_features(tc, '(a OR G)~s')$hits

search_features(tc, '"a b"~s')$hits   ## use ~s flag on everything between quotes
search_features(tc, '"A B"~s')$hits   ## use ~s flag on everything between quotes

## ghost terms (~g flag)
search_features(tc, 'A AND B~g')$hits    ## ghost term (~g) has to occur, but is not returned
search_features(tc, 'A AND Q~g')$hits    ## no hi

# (can also be used on parentheses/quotes/anglebrackets for all nested terms)


## "unique_hits" versus "features" mode
tc = create_tcorpus('A A B')

search_features(tc, 'A AND B')$hits ## in "unique_hits" (default), only match full queries
# (B is not repeated to find a second match of A AND B)

search_features(tc, 'A AND B', mode = 'features')$hits ## in "features", match any match
# (note that hit_id in features mode is irrelevant)

# ghost terms (used for conditions) can be repeated
search_features(tc, 'A AND B~g')$hits


</code></pre>

<hr>
<h2 id='semnet'>Create a semantic network based on the co-occurence of tokens in documents</h2><span id='topic+semnet'></span>

<h3>Description</h3>

<p>This function calculates the co-occurence of features and returns a network/graph in the igraph format, where nodes are tokens and edges represent the similarity/adjacency of tokens. Co-occurence is calcuated based on how often two tokens occured within the same document (e.g., news article, chapter, paragraph, sentence). The semnet_window() function can be used to calculate co-occurrence of tokens within a given token distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>semnet(
  tc,
  feature = "token",
  measure = c("con_prob", "con_prob_weighted", "cosine", "count_directed",
    "count_undirected", "chi2"),
  context_level = c("document", "sentence"),
  backbone = F,
  n.batches = NA
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="semnet_+3A_tc">tc</code></td>
<td>
<p>a tCorpus or a featureHits object (i.e. the result of search_features)</p>
</td></tr>
<tr><td><code id="semnet_+3A_feature">feature</code></td>
<td>
<p>The name of the feature column</p>
</td></tr>
<tr><td><code id="semnet_+3A_measure">measure</code></td>
<td>
<p>The similarity measure. Currently supports: &quot;con_prob&quot; (conditional probability), &quot;con_prob_weighted&quot;, &quot;cosine&quot; similarity, &quot;count_directed&quot; (i.e number of cooccurrences) and &quot;count_undirected&quot; (same as count_directed, but returned as an undirected network, chi2 (chi-square score))</p>
</td></tr>
<tr><td><code id="semnet_+3A_context_level">context_level</code></td>
<td>
<p>Determine whether features need to co-occurr within &quot;documents&quot; or &quot;sentences&quot;</p>
</td></tr>
<tr><td><code id="semnet_+3A_backbone">backbone</code></td>
<td>
<p>If True, add an edge attribute for the backbone alpha</p>
</td></tr>
<tr><td><code id="semnet_+3A_n.batches">n.batches</code></td>
<td>
<p>If a number, perform the calculation in batches</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an Igraph graph in which nodes are features and edges are similarity scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text = c('A B C', 'D E F. G H I', 'A D', 'GGG')
tc = create_tcorpus(text, doc_id = c('a','b','c','d'), split_sentences = TRUE)

g = semnet(tc, 'token')
g
igraph::get.data.frame(g)
plot_semnet(g)
</code></pre>

<hr>
<h2 id='semnet_window'>Create a semantic network based on the co-occurence of tokens in token windows</h2><span id='topic+semnet_window'></span>

<h3>Description</h3>

<p>This function calculates the co-occurence of features and returns a network/graph
in the igraph format, where nodes are tokens and edges represent the similarity/adjacency of tokens.
Co-occurence is calcuated based on how often two tokens co-occurr within a given token distance.
</p>
<p>If a featureHits object is given as input, then for for query hits that have multiple positions (i.e. terms
connected with AND statements or word proximity) the raw count score is biased. For the count_* measures
therefore only the first position of the query hit is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>semnet_window(
  tc,
  feature = "token",
  measure = c("con_prob", "cosine", "count_directed", "count_undirected", "chi2"),
  context_level = c("document", "sentence"),
  window.size = 10,
  direction = "&lt;&gt;",
  backbone = F,
  n.batches = 5,
  matrix_mode = c("positionXwindow", "windowXwindow")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="semnet_window_+3A_tc">tc</code></td>
<td>
<p>a tCorpus or a featureHits object (i.e. the result of search_features)</p>
</td></tr>
<tr><td><code id="semnet_window_+3A_feature">feature</code></td>
<td>
<p>The name of the feature column</p>
</td></tr>
<tr><td><code id="semnet_window_+3A_measure">measure</code></td>
<td>
<p>The similarity measure. Currently supports: &quot;con_prob&quot; (conditional probability),
&quot;cosine&quot; similarity, &quot;count_directed&quot; (i.e number of cooccurrences) and &quot;count_undirected&quot;
(same as count_directed, but returned as an undirected network, chi2 (chi-square score))</p>
</td></tr>
<tr><td><code id="semnet_window_+3A_context_level">context_level</code></td>
<td>
<p>Determine whether features need to co-occurr within &quot;documents&quot; or &quot;sentences&quot;</p>
</td></tr>
<tr><td><code id="semnet_window_+3A_window.size">window.size</code></td>
<td>
<p>The token distance within which features are considered to co-occurr</p>
</td></tr>
<tr><td><code id="semnet_window_+3A_direction">direction</code></td>
<td>
<p>Determine whether co-occurrence is assymmetricsl (&quot;&lt;&gt;&quot;) or takes the order of tokens
into account. If direction is '&lt;', then the from/x feature needs to occur before the
to/y feature. If direction is '&gt;', then after.</p>
</td></tr>
<tr><td><code id="semnet_window_+3A_backbone">backbone</code></td>
<td>
<p>If True, add an edge attribute for the backbone alpha</p>
</td></tr>
<tr><td><code id="semnet_window_+3A_n.batches">n.batches</code></td>
<td>
<p>To limit memory use the calculation is divided into batches. This parameter controls
the number of batches.</p>
</td></tr>
<tr><td><code id="semnet_window_+3A_matrix_mode">matrix_mode</code></td>
<td>
<p>There are two approaches for calculating window co-occurrence (see details). By
default we use positionXmatrix, but matrixXmatrix is optional because it might
be favourable for some uses, and might make more sense for cosine similarity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two approaches for calculating window co-occurrence.
One is to measure how often a feature occurs within a given token window, which
can be calculating by calculating the inner product of a matrix that contains the
exact position of features and a matrix that contains the occurrence window.
We refer to this as the &quot;positionXwindow&quot; mode. Alternatively, we can measure how
much the windows of features overlap, for which take the inner product of two window
matrices, which we call the &quot;windowXwindow&quot; mode. The positionXwindow approach has the advantage
of being easy to interpret (e.g. how likely is feature &quot;Y&quot; to occurr within 10
tokens from feature &quot;X&quot;?). The windowXwindow mode, on the other hand, has the interesting
feature that similarity is stronger if tokens co-occurr more closely together
(since then their windows overlap more), but this only works well for similarity measures that
normalize the similarity (e.g., cosine). Currently, we only use the positionXwindow mode,
but windowXwindow could be interesting to use as well, and for cosine it might actually make more
sense.
</p>


<h3>Value</h3>

<p>an Igraph graph in which nodes are features and edges are similarity scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text = c('A B C', 'D E F. G H I', 'A D', 'GGG')
tc = create_tcorpus(text, doc_id = c('a','b','c','d'), split_sentences = TRUE)

g = semnet_window(tc, 'token', window.size = 1)
g
igraph::get.data.frame(g)
plot_semnet(g)
</code></pre>

<hr>
<h2 id='set_network_attributes'>Set some default network attributes for pretty plotting</h2><span id='topic+set_network_attributes'></span>

<h3>Description</h3>

<p>The purpose of this function is to create some default network attribute options to plot networks in a nice and insightfull way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_network_attributes(
  g,
  size_attribute = "freq",
  color_attribute = NA,
  redo_layout = F,
  edgewidth_coef = 1,
  layout_fun = igraph::layout_with_fr
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_network_attributes_+3A_g">g</code></td>
<td>
<p>A graph in the Igraph format.</p>
</td></tr>
<tr><td><code id="set_network_attributes_+3A_size_attribute">size_attribute</code></td>
<td>
<p>the name of the vertex attribute to be used to set the size of nodes</p>
</td></tr>
<tr><td><code id="set_network_attributes_+3A_color_attribute">color_attribute</code></td>
<td>
<p>the name of the attribute that is used to select the color</p>
</td></tr>
<tr><td><code id="set_network_attributes_+3A_redo_layout">redo_layout</code></td>
<td>
<p>if TRUE, force new layout if layout already exists as a graph attribute</p>
</td></tr>
<tr><td><code id="set_network_attributes_+3A_edgewidth_coef">edgewidth_coef</code></td>
<td>
<p>A coefficient for changing the edge width</p>
</td></tr>
<tr><td><code id="set_network_attributes_+3A_layout_fun">layout_fun</code></td>
<td>
<p>THe igraph layout function used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a network in the Igraph format
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c('A B C', 'B C', 'B D'))
g = semnet(tc, 'token')

igraph::get.edge.attribute(g)
igraph::get.vertex.attribute(g)
plot(g)
g = set_network_attributes(g, size_attribute = 'freq')
igraph::get.edge.attribute(g)
igraph::get.vertex.attribute(g)
plot(g)
</code></pre>

<hr>
<h2 id='sgt'>Simple Good Turing smoothing</h2><span id='topic+sgt'></span>

<h3>Description</h3>

<p>Implementation of the Simple Good Turing smoothing proposed in: Gale, W. A., and Sampson, G. (1995). Good turing frequency estimation without tears. Journal of Quantitative Linguistics, 2(3), 217-237.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sgt(freq)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sgt_+3A_freq">freq</code></td>
<td>
<p>A numeric vector of frequencies (integers).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector with the smoothed term proportions
</p>

<hr>
<h2 id='show_udpipe_models'>Show the names of udpipe models</h2><span id='topic+show_udpipe_models'></span>

<h3>Description</h3>

<p>Returns a data.table with the language, treebank and udpipe_model name.
Uses the default model repository provided by the udpipe package (<code><a href="udpipe.html#topic+udpipe_download_model">udpipe_download_model</a></code>).
For more information about udpipe and performance benchmarks of the UD models, see the
GitHub page of the <a href="https://github.com/bnosac/udpipe">udpipe package</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show_udpipe_models()
</code></pre>


<h3>Value</h3>

<p>a data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>show_udpipe_models()
</code></pre>

<hr>
<h2 id='sotu_texts'>State of the Union addresses</h2><span id='topic+sotu_texts'></span>

<h3>Description</h3>

<p>State of the Union addresses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sotu_texts)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>

<hr>
<h2 id='stopwords_list'>Basic stopword lists</h2><span id='topic+stopwords_list'></span>

<h3>Description</h3>

<p>Basic stopword lists
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(stopwords_list)
</code></pre>


<h3>Format</h3>

<p>A named list, with names matching the languages used by SnowballC
</p>

<hr>
<h2 id='subset_query'>Subset tCorpus token data using a query</h2><span id='topic+subset_query'></span>

<h3>Description</h3>

<p>A convenience function that searches for contexts (documents, sentences), and uses the results to <a href="#topic+subset">subset</a> the tCorpus token data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subset_query(
  tc,
  query,
  feature = "token",
  context_level = c("document", "sentence"),
  not = F,
  as_ascii = F,
  window = NA
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subset_query_+3A_tc">tc</code></td>
<td>
<p>A <code><a href="#topic+tCorpus">tCorpus</a></code></p>
</td></tr>
<tr><td><code id="subset_query_+3A_query">query</code></td>
<td>
<p>A character string that is a query. See <a href="#topic+search_contexts">search_contexts</a> for query syntax.</p>
</td></tr>
<tr><td><code id="subset_query_+3A_feature">feature</code></td>
<td>
<p>The name of the feature columns on which the query is used.</p>
</td></tr>
<tr><td><code id="subset_query_+3A_context_level">context_level</code></td>
<td>
<p>Select whether the query and subset are performed at the document or sentence level.</p>
</td></tr>
<tr><td><code id="subset_query_+3A_not">not</code></td>
<td>
<p>If TRUE, perform a NOT search. Return the articles/sentences for which the query is not found.</p>
</td></tr>
<tr><td><code id="subset_query_+3A_as_ascii">as_ascii</code></td>
<td>
<p>if TRUE, perform search in ascii.</p>
</td></tr>
<tr><td><code id="subset_query_+3A_window">window</code></td>
<td>
<p>If used, uses a word distance as the context (overrides context_level)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the documentation for <a href="#topic+search_contexts">search_contexts</a> for an explanation of the query language.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text = c('A B C', 'D E F. G H I', 'A D', 'GGG')
tc = create_tcorpus(text, doc_id = c('a','b','c','d'), split_sentences = TRUE)

## subset by reference
tc2 = subset_query(tc, 'A')
tc2$meta

</code></pre>

<hr>
<h2 id='subset.tCorpus'>S3 subset for tCorpus class</h2><span id='topic+subset.tCorpus'></span>

<h3>Description</h3>

<p>S3 subset for tCorpus class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tCorpus'
subset(x, subset = NULL, subset_meta = NULL, window = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subset.tCorpus_+3A_x">x</code></td>
<td>
<p>a tCorpus object</p>
</td></tr>
<tr><td><code id="subset.tCorpus_+3A_subset">subset</code></td>
<td>
<p>logical expression indicating rows to keep in the tokens data.</p>
</td></tr>
<tr><td><code id="subset.tCorpus_+3A_subset_meta">subset_meta</code></td>
<td>
<p>logical expression indicating rows to keep in the document meta data.</p>
</td></tr>
<tr><td><code id="subset.tCorpus_+3A_window">window</code></td>
<td>
<p>If not NULL, an integer specifiying the window to be used to return the subset. For instance, if the subset contains token 10 in a document and window is 5, the subset will contain token 5 to 15. Naturally, this does not apply to subset_meta.</p>
</td></tr>
<tr><td><code id="subset.tCorpus_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## create tcorpus of 5 bush and obama docs
tc = create_tcorpus(sotu_texts[c(1:5,801:805),], doc_col='id')

## subset to keep only tokens where token_id &lt;= 20 (i.e.first 20 tokens)
tcs1 = subset(tc, token_id &lt; 20)
tcs1

## subset to keep only documents where president is Barack Obama
tcs2 = subset(tc, subset_meta = president == 'Barack Obama')
tcs2
</code></pre>

<hr>
<h2 id='summary.contextHits'>S3 summary for contextHits class</h2><span id='topic+summary.contextHits'></span>

<h3>Description</h3>

<p>S3 summary for contextHits class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'contextHits'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.contextHits_+3A_object">object</code></td>
<td>
<p>a contextHits object, as returned by <a href="#topic+search_contexts">search_contexts</a></p>
</td></tr>
<tr><td><code id="summary.contextHits_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>text = c('A B C', 'D E F. G H I', 'A D', 'GGG')
tc = create_tcorpus(text, doc_id = c('a','b','c','d'), split_sentences = TRUE)
hits = search_contexts(tc, c('query label# A AND B', 'second query# (A AND Q) OR ("D E") OR I'))

summary(hits)
</code></pre>

<hr>
<h2 id='summary.featureHits'>S3 summary for featureHits class</h2><span id='topic+summary.featureHits'></span>

<h3>Description</h3>

<p>S3 summary for featureHits class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'featureHits'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.featureHits_+3A_object">object</code></td>
<td>
<p>a featureHits object, as returned by <a href="#topic+search_features">search_features</a></p>
</td></tr>
<tr><td><code id="summary.featureHits_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>text = c('A B C', 'D E F. G H I', 'A D', 'GGG')
tc = create_tcorpus(text, doc_id = c('a','b','c','d'), split_sentences = TRUE)
hits = search_features(tc, c('query label# A AND B', 'second query# (A AND Q) OR ("D E") OR I'))

summary(hits)
</code></pre>

<hr>
<h2 id='summary.tCorpus'>Summary of a tCorpus object</h2><span id='topic+summary.tCorpus'></span>

<h3>Description</h3>

<p>Summary of a tCorpus object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tCorpus'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.tCorpus_+3A_object">object</code></td>
<td>
<p>A tCorpus object</p>
</td></tr>
<tr><td><code id="summary.tCorpus_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c('First text', 'Second text'))
summary(tc)
</code></pre>

<hr>
<h2 id='tc_plot_tree'>Visualize a dependency tree</h2><span id='topic+tc_plot_tree'></span>

<h3>Description</h3>

<p>A wrapper for the <code><a href="rsyntax.html#topic+plot_tree">plot_tree</a></code> function, that can be used directly on a tCorpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tc_plot_tree(
  tc,
  ...,
  annotation = NULL,
  sentence_i = 1,
  doc_id = NULL,
  pdf_file = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tc_plot_tree_+3A_tc">tc</code></td>
<td>
<p>a tCorpus</p>
</td></tr>
<tr><td><code id="tc_plot_tree_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="rsyntax.html#topic+plot_tree">plot_tree</a></code>. Most importantly, this is used to select which specific columns to display on the bottom rows.
For instance, tc_plot_tree(tc, token, lemma, POS) shows only these three columns.</p>
</td></tr>
<tr><td><code id="tc_plot_tree_+3A_annotation">annotation</code></td>
<td>
<p>Optionally, the name of a column with an rsyntax annotation.</p>
</td></tr>
<tr><td><code id="tc_plot_tree_+3A_sentence_i">sentence_i</code></td>
<td>
<p>By default, plot_tree uses the first sentence (sentence_i = 1) in the data. sentence_i can be changed to select other sentences by position (the i-th unique sentence in the data). Note that sentence_i does not refer to the values in the sentence column (for this use the sentence argument together with doc_id)</p>
</td></tr>
<tr><td><code id="tc_plot_tree_+3A_doc_id">doc_id</code></td>
<td>
<p>Optionally, the document id can be specified. If so, sentence_i refers to the i-th sentence within the given document.</p>
</td></tr>
<tr><td><code id="tc_plot_tree_+3A_pdf_file">pdf_file</code></td>
<td>
<p>Directly save the plot as a pdf file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plots a dependency tree.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (interactive()) 
  tc_plot_tree(tc_sotu_udpipe, token, lemma, POS)
</code></pre>

<hr>
<h2 id='tc_sotu_udpipe'>A tCorpus with a small sample of sotu paragraphs parsed with udpipe</h2><span id='topic+tc_sotu_udpipe'></span>

<h3>Description</h3>

<p>A tCorpus with a small sample of sotu paragraphs parsed with udpipe
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(tc_sotu_udpipe)
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>

<hr>
<h2 id='tCorpus'>tCorpus: a corpus class for tokenized texts</h2><span id='topic+tCorpus'></span><span id='topic+tcorpus'></span>

<h3>Description</h3>

<p>The tCorpus is a class for managing tokenized texts, stored as a data.frame in which each row represents a token, and columns contain the positions and features of these tokens.
</p>


<h3>Methods and Functions</h3>

<p>The corpustools package uses both functions and methods for working with the tCorpus.
</p>
<p>Methods are used for all operations that modify the tCorpus itself, such as subsetting or adding columns.
This allows the data to be <a href="#topic+tCorpus_modify_by_reference">modified by reference</a>.
Methods are accessed using the dollar sign after the tCorpus object. For example, if the tCorpus is named tc, the subset method can be called as tc$subset(...)
</p>
<p>Functions are used for all operations that return a certain output, such as search results or a semantic network.
These are used in the common R style that you know and love. For example, if the tCorpus is named tc, a semantic network can be created with semnet(tc, ...)
</p>


<h3>Overview of methods and functions</h3>

<p>The primary goal of the tCorpus is to facilitate various corpus analysis techniques. The documentation for currently implemented techniques can be reached through the following links.
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus_create">Create a tCorpus</a> </td><td style="text-align: left;"> Functions for creating a tCorpus object </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus_data">Manage tCorpus data</a> </td><td style="text-align: left;"> Methods for viewing, modifying and subsetting tCorpus data </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus_features">Features</a> </td><td style="text-align: left;"> Preprocessing, subsetting and analyzing features </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus_querying">Using search strings</a> </td><td style="text-align: left;"> Use Boolean queries to analyze the tCorpus </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus_semnet">Co-occurrence networks</a>  </td><td style="text-align: left;"> Feature co-occurrence based semantic network analysis </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus_compare">Corpus comparison</a> </td><td style="text-align: left;"> Compare corpora </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus_topmod">Topic modeling</a>  </td><td style="text-align: left;"> Create and visualize topic models </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus_docsim">Document similarity</a> </td><td style="text-align: left;"> Calculate document similarity </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<hr>
<h2 id='tCorpus_compare'>Corpus comparison</h2><span id='topic+tCorpus_compare'></span>

<h3>Description</h3>

<p><a href="#topic+tCorpus">(back to overview)</a>
</p>


<h3>Details</h3>

<p><strong>Compare vocabulary of two corpora</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+compare_corpus">compare_corpus()</a> </td><td style="text-align: left;"> Compare vocabulary of one tCorpus to another </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+compare_subset">compare_subset()</a> </td><td style="text-align: left;"> Compare subset of a tCorpus to the rest of the tCorpus
</td>
</tr>

</table>


<hr>
<h2 id='tCorpus_create'>Creating a tCorpus</h2><span id='topic+tCorpus_create'></span>

<h3>Description</h3>

<p><a href="#topic+tCorpus">(back to overview)</a>
</p>


<h3>Details</h3>

<p><strong>Create a tCorpus</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+create_tcorpus">create_tcorpus()</a> </td><td style="text-align: left;"> Create a tCorpus from raw text input </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tokens_to_tcorpus">tokens_to_tcorpus()</a> </td><td style="text-align: left;"> Create a tCorpus from a data.frame of already tokenized texts
</td>
</tr>

</table>


<hr>
<h2 id='tCorpus_data'>Methods and functions for viewing, modifying and subsetting tCorpus data</h2><span id='topic+tCorpus_data'></span>

<h3>Description</h3>

<p><a href="#topic+tCorpus">(back to overview)</a>
</p>


<h3>Details</h3>

<p><strong>Get data</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24get">$get()</a> </td><td style="text-align: left;"> Get (by default deep copy) token data, with the possibility to select columns and subset.
                                   Instead of copying you can also access the token data with tc$tokens </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24get">$get_meta()</a> </td><td style="text-align: left;"> Get meta data, with the possibility to select columns and subset. Like tokens, you can also
                                  access meta data with tc$meta  </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+get_dtm">get_dtm()</a> </td><td style="text-align: left;"> Create a document term matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+get_dfm">get_dfm()</a> </td><td style="text-align: left;"> Create a document term matrix, using the Quanteda dfm format </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24context">$context()</a> </td><td style="text-align: left;"> Get a context vector. Currently supports documents or globally unique sentences.
</td>
</tr>

</table>

<p><strong>Modify</strong>
</p>
<p>The token and meta data can be modified with the set* and delete* methods. All modifications are performed by reference.
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24set">$set()</a> </td><td style="text-align: left;"> Modify the token data by setting the values of one (existing or new) column. </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24set">$set_meta()</a> </td><td style="text-align: left;"> The set method for the document meta data </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24set_levels">$set_levels()</a> </td><td style="text-align: left;"> Change the levels of factor columns. </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24set_meta_levels">$set_meta_levels()</a> </td><td style="text-align: left;"> Change the levels of factor columns in the meta data </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24set_name">$set_name()</a> </td><td style="text-align: left;"> Modify column names of token data. </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24set_meta_name">$set_meta_name()</a> </td><td style="text-align: left;"> Delete columns in the meta data </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24delete_columns">$delete_columns()</a> </td><td style="text-align: left;"> Delete columns.  </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24delete_meta_columns">$delete_meta_columns()</a> </td><td style="text-align: left;"> Delete columns in the meta data
</td>
</tr>

</table>

<p>Modifying is restricted in certain ways to ensure that the data always meets the assumptions required for tCorpus methods.
tCorpus automatically tests whether assumptions are violated, so you don't have to think about this yourself.
The most important limitations are that you cannot subset or append the data.
For subsetting, you can use the <a href="#topic+tCorpus+24subset">tCorpus$subset</a> method, and to add data to a tcorpus you can use the <a href="#topic+merge_tcorpora">merge_tcorpora</a> function.
</p>
<p><strong>Subsetting, merging/adding</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+subset">subset()</a> </td><td style="text-align: left;"> Modify the token and/or meta data using the <a href="#topic+subset">subset</a> function. A subset expression can be specified for both the token data (subset) and the document meta data (subset_meta). </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+subset_query">subset_query()</a> </td><td style="text-align: left;">  Subset the tCorpus based on a query, as used in <a href="#topic+search_contexts">search_contexts</a> </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24subset">$subset()</a> </td><td style="text-align: left;"> Like subset, but as an R6 method that changes the tCorpus by reference </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24subset_query">$subset_query()</a> </td><td style="text-align: left;"> Like subset_query, but as an R6 method that changes the tCorpus by reference
</td>
</tr>

</table>

<p><strong>Fields</strong>
</p>
<p>For the sake of convenience, the number of rows and column names of the data and meta data.tables can be accessed directly.
</p>

<table>
<tr>
 <td style="text-align: left;">
  $n </td><td style="text-align: left;"> The number of tokens (i.e. rows in the data) </td>
</tr>
<tr>
 <td style="text-align: left;">
  $n_meta </td><td style="text-align: left;"> The number of documents (i.e. rows in the document meta data) </td>
</tr>
<tr>
 <td style="text-align: left;">
  $names </td><td style="text-align: left;"> The names of the token data columns </td>
</tr>
<tr>
 <td style="text-align: left;">
  $names_meta </td><td style="text-align: left;"> The names of the document meta data columns
</td>
</tr>

</table>


<hr>
<h2 id='tCorpus_docsim'>Document similarity</h2><span id='topic+tCorpus_docsim'></span>

<h3>Description</h3>

<p><a href="#topic+tCorpus">(back to overview)</a>
</p>


<h3>Details</h3>

<p><strong>Compare documents, and perform similarity based deduplication</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+compare_documents">compare_documents()</a> </td><td style="text-align: left;"> Compare documents </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24deduplicate">$deduplicate()</a> </td><td style="text-align: left;"> Remove duplicate documents
</td>
</tr>

</table>


<hr>
<h2 id='tCorpus_features'>Preprocessing, subsetting and analyzing features</h2><span id='topic+tCorpus_features'></span>

<h3>Description</h3>

<p><a href="#topic+tCorpus">(back to overview)</a>
</p>


<h3>Details</h3>

<p><strong>Pre-process features</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24preprocess">$preprocess()</a> </td><td style="text-align: left;"> Create or modify a feature by preprocessing an existing feature </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24feature_subset">$feature_subset()</a> </td><td style="text-align: left;"> Similar to using subset, but instead of deleting rows it only sets rows for a specified feature to NA.
</td>
</tr>

</table>

<p><strong>Inspect features</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+feature_stats">feature_stats()</a> </td><td style="text-align: left;"> Create a data.frame with feature statistics </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+top_features">top_features()</a> </td><td style="text-align: left;"> Show top features, optionally grouped by a given factor
</td>
</tr>

</table>


<hr>
<h2 id='tCorpus_modify_by_reference'>Modify tCorpus by reference</h2><span id='topic+tCorpus_modify_by_reference'></span>

<h3>Description</h3>

<p><a href="#topic+tCorpus">(back to overview)</a>
</p>


<h3>Details</h3>

<p>If any tCorpus method is used that changes the corpus (e.g., set, subset),
the change is made by reference. This is convenient when working with a large
corpus, because it means that the corpus does not have to be copied when changes are made,
which is slower and less memory efficient.
</p>
<p>To illustrate, for a tCorpus object named 'tc', the subset method can be called like this:
</p>
<p><strong>tc$subset(doc_id %in% selection)</strong>
</p>
<p>The 'tc' object itself is now modified, and does not have to be assigned to a name, as would be the more
common R philosophy. Like this:
</p>
<p><strong>tc = tc$subset(doc_id %in% selection)</strong>
</p>
<p>The results of both lines of code are the same. The assignment in the second approach is not necessary,
but doesn't harm either because tc$subset returns the modified corpus invisibly (see ?invisible if that sounds spooky).
</p>
<p>Be aware, however, that the following does not work!!
</p>
<p><strong>tc2 = tc$subset(doc_id %in% selection)</strong>
</p>
<p>In this case, tc2 does contain the subsetted corpus, but tc itself will also be subsetted!!
</p>
<p>Using the R6 method for subset forces this approach on you, because it is faster and more memory efficient.
If you do want to make a copy, there are several solutions.
</p>
<p>Firstly, for some methods we provide identical functions. For example, instead of the $subset() R6 method,
we can use the subset() function.
</p>
<p><strong>tc2 = subset(tc, doc_id %in% selection)</strong>
</p>
<p>We promise that only the R6 methods (called as tc$method()) will change the data by reference.
</p>
<p>A second option is that R6 methods where copying is often usefull have copy parameter
Modifying by reference only happens in the R6 methods
</p>
<p><strong>tc2 = tc$subset(doc_id %in% selection, copy=TRUE)</strong>
</p>
<p>Finally, you can always make a deep copy of the entire tCorpus before modifying it, using the $copy() method.
</p>
<p><strong>tc2 = tc$copy()</strong>
</p>

<hr>
<h2 id='tCorpus_querying'>Use Boolean queries to analyze the tCorpus</h2><span id='topic+tCorpus_querying'></span>

<h3>Description</h3>

<p><a href="#topic+tCorpus">(back to overview)</a>
</p>


<h3>Details</h3>

<p><strong>Feature-level queries</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+search_features">search_features())</a> </td><td style="text-align: left;"> Search for features based on keywords and conditions </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24code_features">$code_features())</a> </td><td style="text-align: left;">  Add a column to the token data based on feature search results </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24search_recode">$search_recode()</a> </td><td style="text-align: left;"> Use the search_features query syntax to recode features </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+feature_associations">feature_associations()</a> </td><td style="text-align: left;"> Given a query, get words that often co-occur nearby </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+get_kwic">kwic()</a> </td><td style="text-align: left;"> Get keyword-in-context (kwic) strings </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+browse_hits">browse_hits()</a> </td><td style="text-align: left;"> Create full-text browsers with highlighted search hits
</td>
</tr>

</table>

<p><strong>Context-level queries</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+search_contexts">search_contexts()</a> </td><td style="text-align: left;"> Search for documents or sentences using Lucene-like queries </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24subset_query">$subset_query()</a> </td><td style="text-align: left;"> use the search_contexts query syntax to subset the tCorpus
</td>
</tr>

</table>


<hr>
<h2 id='tCorpus_semnet'>Feature co-occurrence based semantic network analysis</h2><span id='topic+tCorpus_semnet'></span>

<h3>Description</h3>

<p><a href="#topic+tCorpus">(back to overview)</a>
</p>


<h3>Details</h3>

<p><strong>Create networks</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+semnet">semnet)</a> </td><td style="text-align: left;"> Feature co-occurrence within contexts (documents, sentences) </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+semnet_window">semnet_window()</a> </td><td style="text-align: left;"> Feature co-occurrence within a specified token distance
</td>
</tr>

</table>

<p><strong>Support functions for analyzing and visualizing the semantic network</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+ego_semnet">ego_semnet()</a> </td><td style="text-align: left;"> Create an ego network from an Igraph network </td>
</tr>
<tr>
 <td style="text-align: left;">
  <a href="#topic+plot_semnet">plot_semnet()</a> </td><td style="text-align: left;"> Convenience function for visualizing an Igraph network, specialized for semantic networks
</td>
</tr>

</table>


<hr>
<h2 id='tCorpus_topmod'>Topic modeling</h2><span id='topic+tCorpus_topmod'></span>

<h3>Description</h3>

<p><a href="#topic+tCorpus">(back to overview)</a>
</p>


<h3>Details</h3>

<p><strong>Train a topic model</strong>
</p>

<table>
<tr>
 <td style="text-align: left;">
  <a href="#topic+tCorpus+24lda_fit">$lda_fit()</a> </td><td style="text-align: left;"> Latent Dirichlet Allocation
</td>
</tr>

</table>


<hr>
<h2 id='tCorpus+24annotate_rsyntax'>Annotate tokens based on rsyntax queries</h2><span id='topic+tCorpus+24annotate_rsyntax'></span><span id='topic+annotate_rsyntax'></span>

<h3>Description</h3>

<p>Apply queries to extract syntax patterns, and add the results as three columns to a tokenlist.
The first column contains the ids for each hit. The second column contains the annotation label. The third column contains the fill level (which you probably won't use, but is important for some features).
Only nodes that are given a name in the tquery (using the label parameter) will be added as annotation.
</p>
<p>Note that while queries only find 1 node for each labeled component of a pattern (e.g., quote queries have 1 node for &quot;source&quot; and 1 node for &quot;quote&quot;), 
all children of these nodes can be annotated by settting fill to TRUE. If a child has multiple ancestors, only the most direct ancestors are used (see documentation for the fill argument).
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>
annotate_rsyntax(column, ..., block = NULL, fill = TRUE, 
                 overwrite = FALSE, block_fill = FALSE, copy = TRUE, 
                 verbose = FALSE)</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24annotate_rsyntax_+3A_column">column</code></td>
<td>
<p>The name of the column in which the annotations are added. The unique ids are added as column_id</p>
</td></tr>
<tr><td><code id="tCorpus+2B24annotate_rsyntax_+3A_...">...</code></td>
<td>
<p>One or multiple tqueries, or a list of queries, as created with <code><a href="rsyntax.html#topic+tquery">tquery</a></code>. Queries can be given a named by using a named argument, which will be used in the annotation_id to keep track of which query was used.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24annotate_rsyntax_+3A_block">block</code></td>
<td>
<p>Optionally, specify ids (doc_id - sentence - token_id triples) that are blocked from querying and filling (ignoring the id and recursive searches through the id).</p>
</td></tr>
<tr><td><code id="tCorpus+2B24annotate_rsyntax_+3A_fill">fill</code></td>
<td>
<p>Logical. If TRUE (default) also assign the fill nodes (as specified in the tquery). Otherwise these are ignored</p>
</td></tr>
<tr><td><code id="tCorpus+2B24annotate_rsyntax_+3A_overwrite">overwrite</code></td>
<td>
<p>Applies if column already exists. If TRUE, existing column will be overwritten. If FALSE, the existing annotations in the column will be blocked, and new annotations will be added. This is identical to using multiple queries.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24annotate_rsyntax_+3A_block_fill">block_fill</code></td>
<td>
<p>If TRUE (and overwrite is FALSE), the existing fill nodes will also be blocked. In other words, the new annotations will only be added if the</p>
</td></tr>
<tr><td><code id="tCorpus+2B24annotate_rsyntax_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, report progress (only usefull if multiple queries are given)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(rsyntax)

## spacy tokens for: Mary loves John, and Mary was loved by John
tokens = tokens_spacy[tokens_spacy$doc_id == 'text3',]
tc = tokens_to_tcorpus(tokens)

## two simple example tqueries
passive = tquery(pos = "VERB*", label = "predicate",
                 children(relation = c("agent"), label = "subject"))
active =  tquery(pos = "VERB*", label = "predicate",
                 children(relation = c("nsubj", "nsubjpass"), label = "subject"))

tc$annotate_rsyntax("clause", pas=passive, act=active)
tc$tokens

if (interactive()) {
plot_tree(tc$tokens, annotation='clause')
}
if (interactive()) {
syntax_reader(tc$tokens, annotation = 'clause', value='subject')
}
</code></pre>

<hr>
<h2 id='tCorpus+24code_dictionary'>Dictionary lookup</h2><span id='topic+tCorpus+24code_dictionary'></span><span id='topic+code_dictionary'></span>

<h3>Description</h3>

<p>Add a column to the token data that contains a code (the query label) for tokens that match the dictionary
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>code_dictionary(...)</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24code_dictionary_+3A_dict">dict</code></td>
<td>
<p>A dictionary. Can be either a data.frame or a quanteda dictionary. If a data.frame is given, it has to
have a column named &quot;string&quot; (or use string_col argument) that contains the dictionary terms. All other columns are added to the
tCorpus $tokens data. Each row has a single string, that can be
a single word or a sequence of words seperated by a whitespace (e.g., &quot;not bad&quot;), and can have the common ? and * wildcards.
If a quanteda dictionary is given, it is automatically converted to this type of data.frame with the
<code><a href="#topic+melt_quanteda_dict">melt_quanteda_dict</a></code> function. This can be done manually for more control over labels.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_dictionary_+3A_token_col">token_col</code></td>
<td>
<p>The feature in tc that contains the token text.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_dictionary_+3A_string_col">string_col</code></td>
<td>
<p>If dict is a data.frame, the name of the column in dict that contains the dictionary lookup string</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_dictionary_+3A_sep">sep</code></td>
<td>
<p>A regular expression for separating multi-word lookup strings (default is &quot; &quot;, which is what quanteda dictionaries use).
For example, if the dictionary contains &quot;Barack Obama&quot;, sep should be &quot; &quot; so that it matches the consequtive tokens &quot;Barack&quot; and &quot;Obama&quot;.
In some dictionaries, however, it might say &quot;Barack+Obama&quot;, so in that case sep = '\\+' should be used.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_dictionary_+3A_case_sensitive">case_sensitive</code></td>
<td>
<p>logical, should lookup be case sensitive?</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_dictionary_+3A_column">column</code></td>
<td>
<p>The name of the column added to $tokens. [column]_id contains the unique id of the match.
If a quanteda dictionary is given, the label for the match is in the column named [column].
If a dictionary has multiple levels, these are added as [column]_l[level].</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_dictionary_+3A_use_wildcards">use_wildcards</code></td>
<td>
<p>Use the wildcards * (any number including none of any character) and ? (one or none of any character). If FALSE, exact string matching is used.
(&quot;:-)&quot; versus &quot;:&quot; &quot;-&quot; &quot;)&quot;). This is only behind the scenes for the dictionary lookup, and will not affect tokenization in the corpus.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_dictionary_+3A_ascii">ascii</code></td>
<td>
<p>If true, convert text to ascii before matching</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_dictionary_+3A_verbose">verbose</code></td>
<td>
<p>If true, report progress</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the tCorpus
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dict = data.frame(string = c('good','bad','ugl*','nice','not pret*', ':)', ':('), 
                  sentiment=c(1,-1,-1,1,-1,1,-1))
tc = create_tcorpus(c('The good, the bad and the ugly, is nice :) but not pretty :('))
tc$code_dictionary(dict)
tc$tokens
</code></pre>

<hr>
<h2 id='tCorpus+24code_features'>Code features in a tCorpus based on a search string</h2><span id='topic+tCorpus+24code_features'></span><span id='topic+code_features'></span>

<h3>Description</h3>

<p>like <code><a href="#topic+search_features">search_features</a></code>, but instead of return hits only adds a column to the token data that contains a code (the query label) for tokens that match the query.
Note that only one code can be assigned to each token, so if there are overlapping results for different queries, the code for the last query
will be used. This means that the order of queries (in the query argument) matters.
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>code_features(query, code=NULL, feature='token', column='code', ...)</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24code_features_+3A_query">query</code></td>
<td>
<p>A character string that is a query. See <a href="#topic+search_features">search_features</a> for documentation of the query language.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_features_+3A_code">code</code></td>
<td>
<p>The code given to the tokens that match the query (usefull when looking for multiple queries). Can also put code label in query with # (see details)</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_features_+3A_feature">feature</code></td>
<td>
<p>The name of the feature column within which to search.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_features_+3A_column">column</code></td>
<td>
<p>The name of the column that is added to the data</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_features_+3A_add_column">add_column</code></td>
<td>
<p>list of name-value pairs, used to add additional columns. The name will become the column name, and the value should be a vector of the same length as the query vector.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_features_+3A_context_level">context_level</code></td>
<td>
<p>Select whether the queries should occur within while &quot;documents&quot; or specific &quot;sentences&quot;.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_features_+3A_as_ascii">as_ascii</code></td>
<td>
<p>if TRUE, perform search in ascii.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_features_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, progress messages will be printed</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_features_+3A_overwrite">overwrite</code></td>
<td>
<p>If TRUE (default) and column already exists, overwrite previous results.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24code_features_+3A_...">...</code></td>
<td>
<p>alternative way to specify name-value pairs for adding additional columns</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus('Anna and Bob are secretive')

tc$code_features(c("actors# anna bob", "associations# secretive"))
tc$tokens
</code></pre>

<hr>
<h2 id='tCorpus+24context'>Get a context vector</h2><span id='topic+tCorpus+24context'></span><span id='topic+context'></span>

<h3>Description</h3>

<p>Depending on the purpose, the context of an analysis can be the document level or sentence level. the tCorpus$context() method offers a convenient way to get the context id of tokens for different settings.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24context_+3A_context_level">context_level</code></td>
<td>
<p>Select whether the context is document or sentence level</p>
</td></tr>
<tr><td><code id="tCorpus+2B24context_+3A_with_labels">with_labels</code></td>
<td>
<p>Return context as only ids (numeric, starting at 1) or with labels (factor)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>data(context_level = c('document','sentence'), with_labels = T)</pre>


<h3>Examples</h3>

<pre><code class='language-R'>tc &lt;- create_tcorpus(c('Text one first sentence. Text one second sentence', 'Text two'),
                     split_sentences = TRUE)

doc &lt;- tc$context() ## default context is doc_id (document level)
doc

sent &lt;- tc$context('sentence') ## can specify sentence level
sent
</code></pre>

<hr>
<h2 id='tCorpus+24deduplicate'>Deduplicate documents</h2><span id='topic+tCorpus+24deduplicate'></span><span id='topic+deduplicate'></span>

<h3>Description</h3>

<p>Deduplicate documents based on similarity scores. Can be used to filter out identical documents, but also similar documents.
</p>
<p>Note that deduplication occurs by reference (<a href="#topic+tCorpus_modify_by_reference">tCorpus_modify_by_reference</a>) unless copy is set to TRUE.
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>deduplicate(feature='token', date_col=NULL, meta_cols=NULL, hour_window=NULL, min_docfreq=2, max_docfreq_pct=0.5, measure=c('cosine','overlap_pct'), similarity=1, keep=c('first','last', 'random'), weight=c('norm_tfidf', 'tfidf', 'termfreq','docfreq'), ngrams=NA, print_duplicates=F, copy=F)</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_feature">feature</code></td>
<td>
<p>the column name of the feature that is to be used for the comparison.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_date_col">date_col</code></td>
<td>
<p>The column name for a column with a date vector (in POSIXct). If given together with hour_window, only documents within the given hour_window will be compared.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_meta_cols">meta_cols</code></td>
<td>
<p>a vector with names for columns in the meta data. If given, documents are only considered duplicates if the values of these columns are identical (in addition to having a high similarity score)</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_hour_window">hour_window</code></td>
<td>
<p>A vector of length 1 or 2. If length is 1, the same value is used for the left and right side of the window. If length is 2, the first and second value determine the left and right side. For example, the value 12 will compare each document to all documents between the previous and next 12 hours, and c(-10, 36) will compare each document to all documents between the previous 10 and the next 36 hours.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_min_docfreq">min_docfreq</code></td>
<td>
<p>a minimum document frequency for features. This is mostly to lighten computational load. Default is 2, because terms that occur once cannot overlap across documents</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_max_docfreq_pct">max_docfreq_pct</code></td>
<td>
<p>a maximum document frequency percentage for features. High frequency terms contain little information for identifying duplicates. Default is 0.5 (i.e. terms that occur in more than 50 percent of documents are ignored),</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_lowercase">lowercase</code></td>
<td>
<p>If True, make feature lowercase</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_measure">measure</code></td>
<td>
<p>the similarity measure. Currently supports cosine similarity (symmetric) and overlap_pct (asymmetric)</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_similarity">similarity</code></td>
<td>
<p>the similarity threshold used to determine whether two documents are duplicates. Default is 1, meaning 100 percent identical.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_keep">keep</code></td>
<td>
<p>select either 'first', 'last' or 'random'. Determines which document of duplicates to delete. If a date is given, 'first' and 'last' specify whether the earliest or latest document is kept.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_weight">weight</code></td>
<td>
<p>a weighting scheme for the document-term matrix. Default is term-frequency inverse document frequency with normalized rows (document length).</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_ngrams">ngrams</code></td>
<td>
<p>an integer. If given, ngrams of this length are used</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_print_deduplicates">print_deduplicates</code></td>
<td>
<p>if TRUE, print ids of duplicates that are deleted</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_verbose">verbose</code></td>
<td>
<p>if TRUE, report progress</p>
</td></tr>
<tr><td><code id="tCorpus+2B24deduplicate_+3A_copy">copy</code></td>
<td>
<p>If TRUE, the method returns a new tCorpus object instead of deduplicating the current one by reference.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>d = data.frame(text = c('a b c d e',
                        'e f g h i j k',
                        'a b c'),
               date = as.POSIXct(c('2010-01-01','2010-01-01','2012-01-01')))
tc = create_tcorpus(d)

tc$meta
dedup = tc$deduplicate(feature='token', date_col = 'date', similarity = 0.8, copy=TRUE)
dedup$meta

dedup = tc$deduplicate(feature='token', date_col = 'date', similarity = 0.8, keep = 'last',
                       copy=TRUE)
dedup$meta
</code></pre>

<hr>
<h2 id='tCorpus+24delete_columns'>Delete column from the data and meta data</h2><span id='topic+tCorpus+24delete_columns'></span><span id='topic+tCorpus+24delete_meta_columns'></span><span id='topic+delete_columns'></span><span id='topic+delete_meta_columns'></span>

<h3>Description</h3>

<p><strong>Usage:</strong>
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24delete_columns_+3A_cnames">cnames</code></td>
<td>
<p>the names of the columns to delete</p>
</td></tr>
</table>


<h3>Details</h3>

<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>delete_columns(cnames)</pre>
<pre>delete_meta_columns(cnames)</pre>


<h3>Examples</h3>

<pre><code class='language-R'>d = data.frame(text = c('Text one','Text two','Text three'),
               date = c('2010-01-01','2010-01-01','2012-01-01'))
tc = create_tcorpus(d)

tc$tokens
tc$delete_columns('token')
tc$tokens

tc$meta
tc$delete_meta_columns('date')
tc$meta
</code></pre>

<hr>
<h2 id='tCorpus+24feats_to_columns'>Cast the &quot;feats&quot; column in UDpipe tokens to columns</h2><span id='topic+tCorpus+24feats_to_columns'></span><span id='topic+feats_to_columms'></span>

<h3>Description</h3>

<p>If the UDpipe parser is used in <code><a href="#topic+create_tcorpus">create_tcorpus</a></code>, the 'feats' column contains strings with features
(e.g, Number=Sing|PronType=Dem). To work with these nested features it is more convenient to cast them to columns.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24feats_to_columns_+3A_keep">keep</code></td>
<td>
<p>Optionally, the names of features to keep</p>
</td></tr>
<tr><td><code id="tCorpus+2B24feats_to_columns_+3A_drop">drop</code></td>
<td>
<p>Optionally, the names of features to drop</p>
</td></tr>
<tr><td><code id="tCorpus+2B24feats_to_columns_+3A_rm_column">rm_column</code></td>
<td>
<p>If TRUE (default), remove the original column</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>
feats_to_columns(keep=NULL, drop=NULL, rm_column=TRUE)
</pre>


<h3>Examples</h3>

<pre><code class='language-R'>if (interactive()) {
tc = create_tcorpus('This is a test Bobby.', udpipe_model='english-ewt')
tc$feats_to_columns()
tc$tokens

tc = create_tcorpus('This is a test Bobby.', udpipe_model='english-ewt')
tc$feats_to_columns(keep = c('Gender','Tense','Person'))
tc$tokens
}
</code></pre>

<hr>
<h2 id='tCorpus+24feature_subset'>Filter features</h2><span id='topic+tCorpus+24feature_subset'></span><span id='topic+feature_subset'></span>

<h3>Description</h3>

<p>Similar to using <a href="#topic+tCorpus+24subset">tCorpus$subset</a>, but instead of deleting rows it only sets rows for a specified feature to NA. This can be very convenient, because it enables only a selection of features to be used in an analysis (e.g. a topic model) but maintaining the context of the full article, so that results can be viewed in this context (e.g. a topic browser).
</p>
<p>Just as in subset, it is easy to use objects and functions in the filter, including the special functions for using term frequency statistics (see documentation for <a href="#topic+tCorpus+24subset">tCorpus$subset</a>).
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>feature_subset(column, new_column, subset)</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24feature_subset_+3A_column">column</code></td>
<td>
<p>the column containing the feature to be used as the input</p>
</td></tr>
<tr><td><code id="tCorpus+2B24feature_subset_+3A_subset">subset</code></td>
<td>
<p>logical expression indicating rows to keep in the tokens data. i.e. rows for which the logical expression is FALSE will be set to NA.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24feature_subset_+3A_new_column">new_column</code></td>
<td>
<p>the column to save the filtered feature. Can be a new column or overwrite an existing one.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24feature_subset_+3A_min_freq">min_freq</code></td>
<td>
<p>an integer, specifying minimum token frequency.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24feature_subset_+3A_min_docfreq">min_docfreq</code></td>
<td>
<p>an integer, specifying minimum document frequency.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24feature_subset_+3A_max_freq">max_freq</code></td>
<td>
<p>an integer, specifying minimum token frequency.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24feature_subset_+3A_max_docfreq">max_docfreq</code></td>
<td>
<p>an integer, specifying minimum document frequency.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24feature_subset_+3A_min_char">min_char</code></td>
<td>
<p>an integer, specifying minimum characters in a token</p>
</td></tr>
<tr><td><code id="tCorpus+2B24feature_subset_+3A_max_char">max_char</code></td>
<td>
<p>an integer, specifying maximum characters in a token</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus('a a a a b b b c c')

tc$feature_subset('token', 'tokens_subset1', subset = token_id &lt; 5)
tc$feature_subset('token', 'tokens_subset2', subset = freq_filter(token, min = 3))

tc$tokens
</code></pre>

<hr>
<h2 id='tCorpus+24fold_rsyntax'>Fold rsyntax annotations</h2><span id='topic+tCorpus+24fold_rsyntax'></span>

<h3>Description</h3>

<p>If a tCorpus has rsyntax annotations (see <code><a href="#topic+annotate_rsyntax">annotate_rsyntax</a></code>), it can be convenient to aggregate tokens that have a certain semantic label.
For example, if you have a query for labeling &quot;source&quot; and &quot;quote&quot;, you can add an aggegated value for the sources (such as a unique ID) as a column, and then remove the quote tokens.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24fold_rsyntax_+3A_annotation">annotation</code></td>
<td>
<p>The name of an rsyntax annotation column</p>
</td></tr>
<tr><td><code id="tCorpus+2B24fold_rsyntax_+3A_by_label">by_label</code></td>
<td>
<p>The labels in this column for which you want to aggregate the tokens</p>
</td></tr>
<tr><td><code id="tCorpus+2B24fold_rsyntax_+3A_...">...</code></td>
<td>
<p>Specify the new aggregated columns in name-value pairs. The name is the name of the new column, and the value should be a function over a column in $tokens. 
For example:  subject = paste(token, collapse = ' ')  would create the column 'subject', of which the values are the concatenated tokens. See examples for more.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24fold_rsyntax_+3A_txt">txt</code></td>
<td>
<p>If TRUE, add _txt column with concatenated tokens for by_label</p>
</td></tr>
<tr><td><code id="tCorpus+2B24fold_rsyntax_+3A_rm_by">rm_by</code></td>
<td>
<p>If TRUE (default), remove the column(s) specified in by_label</p>
</td></tr>
<tr><td><code id="tCorpus+2B24fold_rsyntax_+3A_copy">copy</code></td>
<td>
<p>If TRUE, return a copy of the transformed tCorpus, instead of transforming the tCorpus by reference</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>
fold_rsyntax(annotation, by_label, ..., 
             to_label=NULL, rm_by=T, copy=F)</pre>


<h3>Examples</h3>

<pre><code class='language-R'>tc = tc_sotu_udpipe$copy()
tc$udpipe_clauses()

tc$fold_rsyntax('clause', by_label = 'subject', subject = paste(token, collapse=' '))
tc$tokens
</code></pre>

<hr>
<h2 id='tCorpus+24get'>Access the data from a tCorpus</h2><span id='topic+tCorpus+24get'></span><span id='topic+tCorpus+24get_meta'></span><span id='topic+get'></span><span id='topic+get_meta'></span>

<h3>Description</h3>

<p>Get (a copy of) the token and meta data. For quick access recommend using tc$tokens and tc$meta to get the tokens and meta data.tables, which does not copy the data.
However, you should then make sure to not change the data.tables by reference, or you might break the tCorpus.
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 active method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>get(columns=NULL, keep_df=F, as.df=F, subset=NULL, doc_id=NULL, token_id=NULL, safe_copy=T)</pre>
<pre>get_meta(columns=NULL, keep_df=F, as.df=F, subset=NULL, doc_id=NULL, safe_copy=T)</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24get_+3A_columns">columns</code></td>
<td>
<p>character vector with the names of the columns</p>
</td></tr>
<tr><td><code id="tCorpus+2B24get_+3A_keep_df">keep_df</code></td>
<td>
<p>if True, the output will be a data.table (or data.frame) even if it only contains 1 columns</p>
</td></tr>
<tr><td><code id="tCorpus+2B24get_+3A_as.df">as.df</code></td>
<td>
<p>if True, the output will be a regular data.frame instead of a data.table</p>
</td></tr>
<tr><td><code id="tCorpus+2B24get_+3A_subset">subset</code></td>
<td>
<p>Optionally, only get a subset of rows (see <a href="#topic+tCorpus+24subset">tCorpus$subset</a> method)</p>
</td></tr>
<tr><td><code id="tCorpus+2B24get_+3A_doc_id">doc_id</code></td>
<td>
<p>A vector with document ids to select rows. Faster than subset, because it uses binary search. Cannot be used in combination with subset. If duplicate doc_ids are given, duplicate rows are returned.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24get_+3A_token_id">token_id</code></td>
<td>
<p>A vector with token indices. Can only be used in pairs with doc_id. For example, if doc_id = c(1,1,1,2,2) and token_id = c(1,2,3,1,2), then the first three tokens of doc 1 and the first 2 tokens of doc 2 are returned. This is mainly usefull for fast (binary search) retrieval of specific tokens.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24get_+3A_safe_copy">safe_copy</code></td>
<td>
<p>for advanced use. The get methods always return a copy of the data, even if the full data is returned (i.e. use get without parameters). This is to prevent accidental changes within tCorpus data (which can break it) if the returned data is modified by reference (see data.table documentation). If safe_copy is set to FALSE and get is called without parameters&mdash;tc$get(safe_copy=F))&mdash;then no copy is made, which is much faster and more memory efficient. Use this if you need speed and efficiency, but make sure not to change the output data.table by reference.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>d = data.frame(text = c('Text one first sentence. Text one second sentence', 'Text two'),
               medium = c('A','B'),
               date = c('2010-01-01','2010-02-01'),
               doc_id = c('D1','D2'))
tc = create_tcorpus(d, split_sentences = TRUE)

## get token data
tc$tokens                     ## full data.table
tc$get(c('doc_id','token'))  ## data.table with selected columns
head(tc$get('doc_id'))       ## single column as vector
head(tc$get(as.df = TRUE))      ## return as regular data.frame

## get subset
tc$get(subset = token_id %in% 1:2)

## subset on keys using (fast) binary search
tc$get(doc_id = 'D1')              ## for doc_id
tc$get(doc_id = 'D1', token_id = 5) ## for doc_id / token pairs


##### use get for meta data with get_meta
tc$meta

## option to repeat meta data to match tokens
tc$get_meta(per_token = TRUE) ## (note that first doc is repeated, and rows match tc$n)

</code></pre>

<hr>
<h2 id='tCorpus+24lda_fit'>Estimate a LDA topic model</h2><span id='topic+tCorpus+24lda_fit'></span><span id='topic+lda_fit'></span>

<h3>Description</h3>

<p>Estimate an LDA topic model using the LDA function from the topicmodels package.
The parameters other than dtm are simply passed to the sampler but provide a workable default.
See the description of that function for more information
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>
lda_fit(feature, create_feature=NULL, K=50, num.iterations=500, alpha=50/K,
     eta=.01, burnin=250, context_level=c('document','sentence'), ...)
</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24lda_fit_+3A_feature">feature</code></td>
<td>
<p>the name of the feature columns</p>
</td></tr>
<tr><td><code id="tCorpus+2B24lda_fit_+3A_create_feature">create_feature</code></td>
<td>
<p>optionally, add a feature column that indicates the topic to which a feature was assigned (in the last iteration). Has to be a character string, that will be the name of the new feature column</p>
</td></tr>
<tr><td><code id="tCorpus+2B24lda_fit_+3A_k">K</code></td>
<td>
<p>the number of clusters</p>
</td></tr>
<tr><td><code id="tCorpus+2B24lda_fit_+3A_num.iterations">num.iterations</code></td>
<td>
<p>the number of iterations</p>
</td></tr>
<tr><td><code id="tCorpus+2B24lda_fit_+3A_method">method</code></td>
<td>
<p>set method. see documentation for LDA function of the topicmodels package</p>
</td></tr>
<tr><td><code id="tCorpus+2B24lda_fit_+3A_alpha">alpha</code></td>
<td>
<p>the alpha parameter</p>
</td></tr>
<tr><td><code id="tCorpus+2B24lda_fit_+3A_eta">eta</code></td>
<td>
<p>the eta parameter#'</p>
</td></tr>
<tr><td><code id="tCorpus+2B24lda_fit_+3A_burnin">burnin</code></td>
<td>
<p>The number of burnin iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A fitted LDA model, and optionally a new column in the tcorpus (added by reference)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (interactive()) {
  tc = create_tcorpus(sotu_texts, doc_column = 'id')
  tc$preprocess('token', 'feature', remove_stopwords = TRUE, use_stemming = TRUE, min_freq=10)
  set.seed(1)
  m = tc$lda_fit('feature', create_feature = 'lda', K = 5, alpha = 0.1)
  m
  topicmodels::terms(m, 10)
  tc$tokens
}

</code></pre>

<hr>
<h2 id='tCorpus+24merge'>Merge the token and meta data.tables of a tCorpus with another data.frame</h2><span id='topic+tCorpus+24merge'></span><span id='topic+merge'></span><span id='topic+merge_meta'></span>

<h3>Description</h3>

<p>Add columns to token/meta by merging with a data.frame df. Only possible for unique matches (i.e. the columns specified in by are unique in df)
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24merge_+3A_df">df</code></td>
<td>
<p>A data.frame (can be regular, data.table or tibble)</p>
</td></tr>
<tr><td><code id="tCorpus+2B24merge_+3A_by">by</code></td>
<td>
<p>The columns to match on. Must exist in both tokens/meta and df. If the columns in tokens/meta and df have different names, use by.x and by.y</p>
</td></tr>
<tr><td><code id="tCorpus+2B24merge_+3A_by.x">by.x</code></td>
<td>
<p>The names of the columns used in tokens/meta</p>
</td></tr>
<tr><td><code id="tCorpus+2B24merge_+3A_by.y">by.y</code></td>
<td>
<p>The names of the columns used in df</p>
</td></tr>
<tr><td><code id="tCorpus+2B24merge_+3A_columns">columns</code></td>
<td>
<p>Optionally, specify which specific columns from df to merge to tokens</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>merge(df, by, by.x, by.y)</pre>
<pre>merge_meta(df, by, by.x, by.y)</pre>


<h3>Examples</h3>

<pre><code class='language-R'>d = data.frame(text = c('This is an example. Best example ever.', 'oh my god', 'so good'),
               id = c('a','b','c'),
               source  =c('aa','bb','cc'))
tc = create_tcorpus(d, doc_col='id', split_sentences = TRUE)

df = data.frame(doc_id=c('a','b'), test=c('A','B'))
tc$merge(df, by='doc_id')
tc$tokens

df = data.frame(doc_id=c('a','b'), sentence=1, test2=c('A','B'))
tc$merge(df, by=c('doc_id', 'sentence'))
tc$tokens

df = data.frame(doc_id=c('a','b'), sentence=1, token_id=c(3,4), test3=c('A','B'))
tc$merge(df, by=c('doc_id', 'sentence', 'token_id'))
tc$tokens

meta = data.frame(doc_id=c('a','b'), test=c('A','B'))
tc$merge_meta(meta, by='doc_id')
tc$meta

meta = data.frame(source=c('aa'), test2=c('A'))
tc$merge_meta(meta, by='source')
tc$meta
</code></pre>

<hr>
<h2 id='tCorpus+24preprocess'>Preprocess feature</h2><span id='topic+tCorpus+24preprocess'></span><span id='topic+preprocess'></span>

<h3>Description</h3>

<p><strong>Usage:</strong>
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24preprocess_+3A_column">column</code></td>
<td>
<p>the column containing the feature to be used as the input</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_new_column">new_column</code></td>
<td>
<p>the column to save the preprocessed feature. Can be a new column or overwrite an existing one.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_lowercase">lowercase</code></td>
<td>
<p>make feature lowercase</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_ngrams">ngrams</code></td>
<td>
<p>create ngrams. The ngrams match the rows in the token data, with the feature in the row being the last token of the ngram. For example, given the features &quot;this is an example&quot;, the third feature (&quot;an&quot;) will have the trigram &quot;this_is_an&quot;. Ngrams at the beginning of a context will have empty spaces. Thus, in the previous example, the second feature (&quot;is&quot;) will have the trigram &quot;_is_an&quot;.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_ngram_context">ngram_context</code></td>
<td>
<p>Ngrams will not be created across contexts, which can be documents or sentences. For example, if the context_level is sentences, then the last token of sentence 1 will not form an ngram with the first token of sentence 2.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_as_ascii">as_ascii</code></td>
<td>
<p>convert characters to ascii. This is particularly usefull for dealing with special characters.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_remove_punctuation">remove_punctuation</code></td>
<td>
<p>remove (i.e. make NA) any features that are <em>only</em> punctuation (e.g., dots, comma's)</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_remove_stopwords">remove_stopwords</code></td>
<td>
<p>remove (i.e. make NA) stopwords. (!) Make sure to set the language argument correctly.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>remove features that are only numbers</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_use_stemming">use_stemming</code></td>
<td>
<p>reduce features (tokens) to their stem</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_language">language</code></td>
<td>
<p>The language used for stopwords and stemming</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_min_freq">min_freq</code></td>
<td>
<p>an integer, specifying minimum token frequency.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_min_docfreq">min_docfreq</code></td>
<td>
<p>an integer, specifying minimum document frequency.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_max_freq">max_freq</code></td>
<td>
<p>an integer, specifying minimum token frequency.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_max_docfreq">max_docfreq</code></td>
<td>
<p>an integer, specifying minimum document frequency.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_min_char">min_char</code></td>
<td>
<p>an integer, specifying minimum number of characters in a term</p>
</td></tr>
<tr><td><code id="tCorpus+2B24preprocess_+3A_max_char">max_char</code></td>
<td>
<p>an integer, specifying maximum number of characters in a term</p>
</td></tr>
</table>


<h3>Details</h3>

<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>
preprocess(column='token', new_column='feature', lowercase=T, ngrams=1,
           ngram_context=c('document', 'sentence'), as_ascii=F, remove_punctuation=T,
           remove_stopwords=F, remove_numbers=F, use_stemming=F, language='english',
           min_freq=NULL, min_docfreq=NULL, max_freq=NULL, max_docfreq=NULL, min_char=NULL, max_char=NULL)
           </pre>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus('I am a SHORT example sentence! That I am!')

## default is lowercase without punctuation
tc$preprocess('token', 'preprocessed_1')

## delete stopwords and perform stemming
tc$preprocess('token', 'preprocessed_2', remove_stopwords = TRUE, use_stemming = TRUE)

## filter on minimum frequency
tc$preprocess('token', 'preprocessed_3', min_freq=2)

## make ngrams
tc$preprocess('token', 'preprocessed_4', ngrams = 3)

tc$tokens
</code></pre>

<hr>
<h2 id='tCorpus+24replace_dictionary'>Replace tokens with dictionary match</h2><span id='topic+tCorpus+24replace_dictionary'></span><span id='topic+replace_dictionary'></span>

<h3>Description</h3>

<p>Uses <code><a href="#topic+search_dictionary">search_dictionary</a></code>, and replaces tokens that match the dictionary lookup term with the dictionary code.
Multi-token matches (e.g., &quot;Barack Obama&quot;) will become single tokens. Multiple lookup terms per code can be used to deal with
alternatives such as &quot;Barack Obama&quot;, &quot;president Obama&quot; and &quot;Obama&quot;.
</p>
<p>This method can also be use to concatenate ASCII symbols into emoticons, given a dictionary of emoticons.
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>replace_dictionary(...)</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_dict">dict</code></td>
<td>
<p>A dictionary. Can be either a data.frame or a quanteda dictionary. If a data.frame is given, it has to
have a column named &quot;string&quot;  (or use string_col argument) that contains the dictionary terms, and a column &quot;code&quot; (or use code_col argument) that contains the
label/code represented by this string. Each row has a single string, that can be
a single word or a sequence of words seperated by a whitespace (e.g., &quot;not bad&quot;), and can have the common ? and * wildcards.
If a quanteda dictionary is given, it is automatically converted to this type of data.frame with the
<code><a href="#topic+melt_quanteda_dict">melt_quanteda_dict</a></code> function. This can be done manually for more control over labels.
Finally, you can also just pass a character vector. All multi word strings (like emoticons) will then be
collapsed into single tokens.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_token_col">token_col</code></td>
<td>
<p>The feature in tc that contains the token text.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_string_col">string_col</code></td>
<td>
<p>If dict is a data.frame, the name of the column in dict with the dictionary lookup string. Default is &quot;string&quot;</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_code_col">code_col</code></td>
<td>
<p>The name of the column in dict with the dictionary code/label. Default is &quot;code&quot;.
If dict is a quanteda dictionary with multiple levels, &quot;code_l2&quot;, &quot;code_l3&quot;, etc. can be used to select levels.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_replace_cols">replace_cols</code></td>
<td>
<p>The names of the columns in tc$tokens that will be replaced by the dictionary code. Default is the column on which the dictionary is applied,
but in some cases it might make sense to replace multiple columns (like token and lemma)</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_sep">sep</code></td>
<td>
<p>A regular expression for separating multi-word lookup strings (default is &quot; &quot;, which is what quanteda dictionaries use).
For example, if the dictionary contains &quot;Barack Obama&quot;, sep should be &quot; &quot; so that it matches the consequtive tokens &quot;Barack&quot; and &quot;Obama&quot;.
In some dictionaries, however, it might say &quot;Barack+Obama&quot;, so in that case sep = '\+' should be used.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_code_from_features">code_from_features</code></td>
<td>
<p>If TRUE, instead of replacing features with the matched code columnm, use the most frequent occuring string in the features.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_code_sep">code_sep</code></td>
<td>
<p>If code_from_features is TRUE, the separator for pasting features together. Default is an underscore, which is recommended because it has special
features in corpustools. Most importantly, if a query or dictionary search is performed, multi-word tokens concatenated with an underscore are treated
as separate consecutive words. So, &quot;Bob_Smith&quot; would still match a lookup for the two consequtive words &quot;bob smith&quot;</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_decrement_ids">decrement_ids</code></td>
<td>
<p>If TRUE (default), decrement token ids after concatenating multi-token matches. So, if the tokens c(&quot;:&quot;, &quot;)&quot;, &quot;yay&quot;) have token_id c(1,2,3),
then after concatenating ASCII emoticons, the tokens will be c(&quot;:)&quot;, &quot;yay&quot;) with token_id c(1,2)</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_case_sensitive">case_sensitive</code></td>
<td>
<p>logical, should lookup be case sensitive?</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_use_wildcards">use_wildcards</code></td>
<td>
<p>Use the wildcards * (any number including none of any character) and ? (one or none of any character). If FALSE, exact string matching is used</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_ascii">ascii</code></td>
<td>
<p>If true, convert text to ascii before matching</p>
</td></tr>
<tr><td><code id="tCorpus+2B24replace_dictionary_+3A_verbose">verbose</code></td>
<td>
<p>If true, report progress</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with the id value (taken from dict$id) for each row in tc$tokens
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus('happy :) sad :( happy 8-)')
tc$tokens   ## tokenization has broken up emoticons (as it should)

# corpustools dictionary lookup automatically normalizes tokenization of 
# tokens and dictionary strings. The dictionary string ":)" would match both
# the single token ":)" and two consequtive tokens c(":", ")"). This 
# makes it easy and foolproof to look for emoticons like this:
emoticon_dict = data.frame(
   code   = c('happy_emo','happy_emo', 'sad_emo'), 
   string = c(':)',             '8-)',      ':(')) 
   
tc$replace_dictionary(emoticon_dict)
tc$tokens

# If a string is passed to replace dictionary, it will collapse multi-word
# strings. .
tc = create_tcorpus('happy :) sad :( Barack Obama')
tc$tokens
tc$replace_dictionary(c(':)', '8-)', 'Barack Obama'))
tc$tokens

</code></pre>

<hr>
<h2 id='tCorpus+24search_recode'>Recode features in a tCorpus based on a search string</h2><span id='topic+tCorpus+24search_recode'></span><span id='topic+search_recode'></span>

<h3>Description</h3>

<p>Search features (see <code><a href="#topic+search_features">search_features</a></code>) and replace features with a new value
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>
search_recode(feature, new_value, keyword, condition = NA, condition_once = FALSE)
</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24search_recode_+3A_feature">feature</code></td>
<td>
<p>The feature in which to search</p>
</td></tr>
<tr><td><code id="tCorpus+2B24search_recode_+3A_new_value">new_value</code></td>
<td>
<p>the character string with which all features that are found are replaced</p>
</td></tr>
<tr><td><code id="tCorpus+2B24search_recode_+3A_query">query</code></td>
<td>
<p>See <code><a href="#topic+search_features">search_features</a></code> for the query parameters</p>
</td></tr>
<tr><td><code id="tCorpus+2B24search_recode_+3A_...">...</code></td>
<td>
<p>Additional search_features parameters. See <code><a href="#topic+search_features">search_features</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='tCorpus+24set'>Modify the token and meta data.tables of a tCorpus</h2><span id='topic+tCorpus+24set'></span><span id='topic+tCorpus+24set_meta'></span><span id='topic+set'></span><span id='topic+set_meta'></span>

<h3>Description</h3>

<p>Modify the token/meta data.table by setting the values of one (existing or new) column. The subset argument can be used to modify only subsets of columns, and can be a logical vector (select TRUE rows), numeric vector (indices of TRUE rows) or logical expression (e.g. pos == 'noun'). If a new column is made whie using a subset, then the rows outside of the selection are set to NA.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24set_+3A_column">column</code></td>
<td>
<p>Name of a new column (to create) or existing column (to transform)</p>
</td></tr>
<tr><td><code id="tCorpus+2B24set_+3A_value">value</code></td>
<td>
<p>An expression to be evaluated within the token/meta data, or a vector of the same length as the number of rows in the data. Note that if a subset is used, the length of value should be the same as the length of the subset (the TRUE cases of the subset expression) or a single value.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24set_+3A_subset">subset</code></td>
<td>
<p>logical expression indicating rows to keep in the tokens data or meta data</p>
</td></tr>
<tr><td><code id="tCorpus+2B24set_+3A_subset_value">subset_value</code></td>
<td>
<p>If subset is used, should value also be subsetted? Default is TRUE, which is what you want if
the value has the same length as the full data.table (which is the case if a column in tokens is used).
However, if the vector of values is already of the length of the subset, subset_value should be FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>set(column, value, subset)</pre>
<pre>set_meta(column, value, subset)</pre>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(sotu_texts[1:5,], doc_column = 'id')

tc$tokens  ## show original

## create new column
i &lt;- 1:tc$n
tc$set(column = 'i', i)
## create new column based on existing column(s)
tc$set(column = 'token_upper', toupper(token))
## use subset to modify existing column
tc$set('token', paste0('***', token, '***'), subset = token_id == 1)
## use subset to create new column with NA's
tc$set('second_token', token, subset = token_id == 2)

tc$tokens  ## show after set


##### use set for meta data with set_meta
tc$set_meta('party_pres', paste(party, president, sep=': '))
tc$meta
</code></pre>

<hr>
<h2 id='tCorpus+24set_levels'>Change levels of factor columns</h2><span id='topic+tCorpus+24set_levels'></span><span id='topic+tCorpus+24set_meta_levels'></span><span id='topic+set_levels'></span><span id='topic+set_meta_levels'></span>

<h3>Description</h3>

<p>For factor columns, the levels can be changed directly (and by reference). This is particularly usefull for fast preprocessing (e.g., making tokens lowercase, )
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24set_levels_+3A_column">column</code></td>
<td>
<p>the name of the column</p>
</td></tr>
<tr><td><code id="tCorpus+2B24set_levels_+3A_levels">levels</code></td>
<td>
<p>The new levels</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>set_levels(column, levels)</pre>
<pre>set_meta_levels(column, levels)</pre>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(c('Text one first sentence. Text one second sentence', 'Text two'))

## change factor levels of a column in the token data
unique_tokens &lt;- tc$get_levels('token')
tc$set_levels('token', toupper(unique_tokens))
tc$tokens
</code></pre>

<hr>
<h2 id='tCorpus+24set_name'>Change column names of data and meta data</h2><span id='topic+tCorpus+24set_name'></span><span id='topic+tCorpus+24set_meta_name'></span><span id='topic+set_name'></span><span id='topic+set_meta_name'></span>

<h3>Description</h3>

<p><strong>Usage:</strong>
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24set_name_+3A_oldname">oldname</code></td>
<td>
<p>the current/old column name</p>
</td></tr>
<tr><td><code id="tCorpus+2B24set_name_+3A_newname">newname</code></td>
<td>
<p>the new column name</p>
</td></tr>
</table>


<h3>Details</h3>

<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>set_name(oldname, newname)</pre>
<pre>set_meta_name(oldname, newname)</pre>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(sotu_texts[1:5,], doc_column = 'id')

## change column name in token data
tc$names ## original column names
tc$set_name(oldname = 'token', newname = 'word')
tc$tokens

## change column name in meta data
tc$meta_names ## original column names
tc$set_meta_name(oldname = 'party', newname = 'clan')
tc$set_meta_name(oldname = 'president', newname = 'clan leader')
tc$meta
</code></pre>

<hr>
<h2 id='tCorpus+24subset'>Subset a tCorpus</h2><span id='topic+tCorpus+24subset'></span><span id='topic+tCorpus+24subset_meta'></span><span id='topic+subset'></span><span id='topic+subset_meta'></span>

<h3>Description</h3>

<p>Returns the subset of a tCorpus. The selection can be made separately (and simultaneously) for the token data (using subset) and the meta data (using subset_meta). The subset arguments work according to the <a href="data.table.html#topic+subset.data.table">subset.data.table</a> function.
</p>
<p>There are two flavours. You can either use subset(tc, ...) or tc$subset(...). The difference is that the second approach changes the tCorpus by reference. 
In other words, tc$subset() will delete the rows from the tCorpus, instead of creating a new tCorpus.
Modifying the tCorpus by reference is more efficient (which becomes important if the tCorpus is large), but the more classic subset(tc, ...) approach is often more obvious.
</p>
<p>Subset can also be used to select rows based on token/feature frequences. This is a common step in corpus analysis, where it often makes sense to ignore very rare and/or very frequent tokens.
To do so, there are several special functions that can be used within a subset call.
The freq_filter() and docfreq_filter() can be used to filter terms based on term frequency and document frequency, respectively. (see examples)
</p>
<p>The subset_meta() method is an alternative for using subset(subset_meta = ...), that is added for consistency with the other _meta methods.
</p>
<p>Note that you can also use the <a href="#topic+tCorpus+24feature_subset">tCorpus$feature_subset</a> method if you want to filter out low/high frequency tokens, but do not want to delete the rows in the tCorpus.
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>
subset(tc, subset = NULL, subset_meta = NULL, 
       window = NULL)
tc$subset(subset = NULL, subset_meta = NULL,
          window = NULL, copy = F)
tc$subset_meta(subset = NULL, copy = F)
</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24subset_+3A_subset">subset</code></td>
<td>
<p>logical expression indicating rows to keep in the tokens data.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24subset_+3A_subset_meta">subset_meta</code></td>
<td>
<p>logical expression indicating rows to keep in the document meta data.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24subset_+3A_window">window</code></td>
<td>
<p>If not NULL, an integer specifiying the window to be used to return the subset. For instance, if the subset contains token 10 in a document and window is 5, the subset will contain token 5 to 15. Naturally, this does not apply to subset_meta.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24subset_+3A_copy">copy</code></td>
<td>
<p>If TRUE, the method returns a new tCorpus object instead of subsetting the current one. This is added for convenience when analyzing a subset of the data. e.g., tc_nyt = tc$subset_meta(medium == &quot;New_York_Times&quot;, copy=T)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(sotu_texts[1:5,], doc_column = 'id')
tc$n ## original number of tokens

## select only first 20 tokens per document
tc2 = subset(tc, token_id &lt; 20)
tc2$n

## Note that the original is untouched
tc$n

## Now we subset by reference. This doesn't make a copy, but changes tc itself
tc$subset(token_id &lt; 20)
tc$n 

## you can filter on term frequency and document frequency with the freq_filter() and
## docfreq_filter() functions
tc = create_tcorpus(sotu_texts[c(1:5,800:805),], doc_column = 'id')
tc$subset( freq_filter(token, min = 2, max = 4) )
tc$tokens

###### subset can be used for meta data by using the subset_meta argument, or the subset_meta method
tc$n_meta
tc$meta
tc$subset(subset_meta = president == 'Barack Obama')
tc$n_meta
</code></pre>

<hr>
<h2 id='tCorpus+24subset_query'>Subset tCorpus token data using a query</h2><span id='topic+tCorpus+24subset_query'></span>

<h3>Description</h3>

<p>A convenience function that searches for contexts (documents, sentences), and uses the results to <a href="#topic+subset">subset</a> the tCorpus token data.
</p>
<p>See the documentation for <a href="#topic+search_contexts">search_contexts</a> for an explanation of the query language.
</p>
<p><strong>Usage:</strong>
</p>
<p>## R6 method for class tCorpus. Use as tc$method (where tc is a tCorpus object).
</p>
<pre>subset_query(query, feature = 'token', context_level = c('document','sentence','window'))</pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24subset_query_+3A_query">query</code></td>
<td>
<p>A character string that is a query. See <a href="#topic+search_contexts">search_contexts</a> for query syntax.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24subset_query_+3A_feature">feature</code></td>
<td>
<p>The name of the feature columns on which the query is used.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24subset_query_+3A_context_level">context_level</code></td>
<td>
<p>Select whether the query and subset are performed at the document or sentence level.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24subset_query_+3A_window">window</code></td>
<td>
<p>If used, uses a word distance as the context (overrides context_level)</p>
</td></tr>
<tr><td><code id="tCorpus+2B24subset_query_+3A_as_ascii">as_ascii</code></td>
<td>
<p>if TRUE, perform search in ascii.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24subset_query_+3A_not">not</code></td>
<td>
<p>If TRUE, perform a NOT search. Return the articles/sentences for which the query is not found.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24subset_query_+3A_copy">copy</code></td>
<td>
<p>If TRUE, return modified copy of data instead of subsetting the input tcorpus by reference.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>text = c('A B C', 'D E F. G H I', 'A D', 'GGG')
tc = create_tcorpus(text, doc_id = c('a','b','c','d'), split_sentences = TRUE)

## subset by reference
tc$subset_query('A')
tc$meta

## using copy mechanic
class(tc$tokens$doc_id)
tc2 = tc$subset_query('A AND D', copy=TRUE)

tc2$get_meta()

tc$meta ## (unchanged)
</code></pre>

<hr>
<h2 id='tCorpus+24udpipe_clauses'>Add columns indicating who did what</h2><span id='topic+tCorpus+24udpipe_clauses'></span><span id='topic+udpipe_clauses'></span>

<h3>Description</h3>

<p>An off-the-shelf application of rsyntax for extracting subject-verb clauses. Designed for working with a tCorpus created with <code><a href="#topic+udpipe_tcorpus">udpipe_tcorpus</a></code>.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24udpipe_clauses_+3A_column">column</code></td>
<td>
<p>The name of the column in $tokens to store the results.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24udpipe_clauses_+3A_tqueries">tqueries</code></td>
<td>
<p>A list of tQueries. By default uses the off-the-shelf tqueries in <code><a href="#topic+udpipe_clause_tqueries">udpipe_clause_tqueries</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tCorpus
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tc = tc_sotu_udpipe$copy()
tc$udpipe_clauses()
if (interactive()) {
  tc_plot_tree(tc, token, lemma, POS, annotation='clause')
  browse_texts(tc, rsyntax='clause', value='subject')
}

</code></pre>

<hr>
<h2 id='tCorpus+24udpipe_quotes'>Add columns indicating who said what</h2><span id='topic+tCorpus+24udpipe_quotes'></span><span id='topic+udpipe_quotes'></span>

<h3>Description</h3>

<p>An off-the-shelf application of rsyntax for extracting quotes. Designed for working with a tCorpus created with <code><a href="#topic+udpipe_tcorpus">udpipe_tcorpus</a></code>.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="tCorpus+2B24udpipe_quotes_+3A_tqueries">tqueries</code></td>
<td>
<p>A list of tqueries. By default uses the off-the-shelf tqueries in <code><a href="#topic+udpipe_quote_tqueries">udpipe_quote_tqueries</a></code>.</p>
</td></tr>
<tr><td><code id="tCorpus+2B24udpipe_quotes_+3A_span_tqueries">span_tqueries</code></td>
<td>
<p>Additional tqueries for finding candidates for 'span quotes' (i.e. quotes that span multiple sentences, indicated by quotation marks). 
By default uses the off-the-shelf tqueries in <code><a href="#topic+udpipe_spanquote_tqueries">udpipe_spanquote_tqueries</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default tqueries are provided for detecting source-quote relations within sentences (<code><a href="#topic+udpipe_quote_tqueries">udpipe_quote_tqueries</a></code>), and for 
detecting source candidates for text between quotation marks that can span across multiple sentences (<code><a href="#topic+udpipe_spanquote_tqueries">udpipe_spanquote_tqueries</a></code>).
These have mainly been developed and tested for the english-ewt udpipe model.
</p>
<p>There are two ways to customize this function. One is to specify a custom character vector of verb lemma. This vector should then be passed as an argument
to the two functions for generarting the default tqueries. The second (more advanced) way is to provide a custom list of tqueries. 
(Note that the udpipe_quote_tqueries and udpipe_spanquote_tqueries functions simply create lists of queries. You can create new lists, or add tqueries to these lists).
!! If you create custom tqueries, make sure that the labels for the quote and source tokens are 'source' and 'quote'. For the spanquote_tqueries,
the label for the source candidate should be 'source'.
</p>


<h3>Value</h3>

<p>the columns 'quote', 'quote_id', and 'quote_verbatim' are added to tokens
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
txt = 'Bob said that he likes Mary. John did not like that: 
       "how dare he!". "It is I, John, who likes Mary!!"'
tc = udpipe_tcorpus(txt, model = 'english-ewt')
tc$udpipe_quotes()

if (interactive()) {
  tc_plot_tree(tc, token, lemma, POS, annotation='quote')
  browse_texts(tc, rsyntax='quote', value='source')
}

## you can provide your own lists of tqueries, or use the two 
## query generating functions to customize the specific 'verb lemma'
## (i.e. the lemma for verbs that indicate speech)

custom_verb_lemma = c('say','state')   ## this should be longer
quote_tqueries =      udpipe_quote_tqueries(custom_verb_lemma)
span_quote_tqueries = udpipe_spanquote_tqueries(custom_verb_lemma)

## note that these use simply lists with tqueries, so you can also
## create your own list or customize these lists

quote_tqueries
span_quote_tqueries

if (interactive()) {
tc$udpipe_quotes(tqueries = quote_tqueries, span_tqueries = span_quote_tqueries)
tc_plot_tree(tc, token, lemma, POS, annotation='quote')
browse_texts(tc, rsyntax='quote', value='source')
}

## End(Not run)
</code></pre>

<hr>
<h2 id='tokens_to_tcorpus'>Create a tcorpus based on tokens (i.e. preprocessed texts)</h2><span id='topic+tokens_to_tcorpus'></span>

<h3>Description</h3>

<p>Create a tcorpus based on tokens (i.e. preprocessed texts)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens_to_tcorpus(
  tokens,
  doc_col = "doc_id",
  token_id_col = "token_id",
  token_col = NULL,
  sentence_col = NULL,
  parent_col = NULL,
  meta = NULL,
  meta_cols = NULL,
  feature_cols = NULL,
  sent_is_local = T,
  token_is_local = T,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokens_to_tcorpus_+3A_tokens">tokens</code></td>
<td>
<p>A data.frame in which rows represent tokens, and columns indicate (at least) the document in which the token occured (doc_col) and the position of the token in that document or globally (token_id_col)</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_doc_col">doc_col</code></td>
<td>
<p>The name of the column that contains the document ids/names</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_token_id_col">token_id_col</code></td>
<td>
<p>The name of the column that contains the positions of tokens. If NULL, it is assumed that the data.frame is ordered by the order of tokens and does not contain gaps (e.g., filtered out tokens)</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_token_col">token_col</code></td>
<td>
<p>Optionally, the name of the column that contains the token text. This column will then be renamed to &quot;token&quot; in the tcorpus, which is the default name
for many functions (e.g., querying, printing text)</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_sentence_col">sentence_col</code></td>
<td>
<p>Optionally, the name of the column that indicates the sentences in which tokens occured. This can be necessary if tokens are not local at the document level (see token_is_local argument),
and sentence information can be used in several tcorpus functions.</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_parent_col">parent_col</code></td>
<td>
<p>Optionally, the name of the column that contains the id of the parent (if a dependency parser was used). If token_is_local = FALSE, then the token_ids will be transormed,
so parent ids need to be changed as well. Default is 'parent', but if this column is not present the parent is ignored.</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_meta">meta</code></td>
<td>
<p>Optionally, a data.frame with document meta data. Needs to contain a column with the document ids (with the same name)</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_meta_cols">meta_cols</code></td>
<td>
<p>Alternatively, if there are document meta columns in the tokens data.table, meta_cols can be used to recognized them. Note that these values have to be unique within documents.</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_feature_cols">feature_cols</code></td>
<td>
<p>Optionally, specify which columns to include in the tcorpus. If NULL, all column are included (except the specified columns for documents, sentences and positions)</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_sent_is_local">sent_is_local</code></td>
<td>
<p>Sentences in the tCorpus are assumed to be locally unique within documents. If sent_is_local is FALSE, then sentences are transformed to be locally unique. However,  it is then assumed that the first sentence in a document is sentence 1, which might not be the case if tokens (input) is a subset.</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_token_is_local">token_is_local</code></td>
<td>
<p>Same as sent_is_local, but for token_id. !! if the data has a parent column, make sure to specify parent_col, so that the parent ids are also transformed</p>
</td></tr>
<tr><td><code id="tokens_to_tcorpus_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>head(corenlp_tokens)

tc = tokens_to_tcorpus(corenlp_tokens, doc_col = 'doc_id',
                       sentence_col = 'sentence', token_id_col = 'id')
tc

meta = data.frame(doc_id = 1, medium = 'A', date = '2010-01-01')
tc = tokens_to_tcorpus(corenlp_tokens, doc_col = 'doc_id',
                       sentence_col = 'sentence', token_id_col = 'id', meta=meta)
tc
</code></pre>

<hr>
<h2 id='tokenWindowOccurence'>Gives the window in which a term occured in a matrix.</h2><span id='topic+tokenWindowOccurence'></span>

<h3>Description</h3>

<p>This function returns the occurence of tokens (position.matrix) and the window of occurence (window.matrix). This format enables the co-occurence of tokens within sliding windows (i.e. token distance) to be calculated by multiplying position.matrix with window.matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenWindowOccurence(
  tc,
  feature,
  context_level = c("document", "sentence"),
  window.size = 10,
  direction = "&lt;&gt;",
  distance_as_value = F,
  batch_rows = NULL,
  drop_empty_terms = T
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenWindowOccurence_+3A_tc">tc</code></td>
<td>
<p>a tCorpus object</p>
</td></tr>
<tr><td><code id="tokenWindowOccurence_+3A_feature">feature</code></td>
<td>
<p>The name of the feature column</p>
</td></tr>
<tr><td><code id="tokenWindowOccurence_+3A_context_level">context_level</code></td>
<td>
<p>Select whether to use &quot;document&quot; or &quot;sentence&quot; as context boundaries</p>
</td></tr>
<tr><td><code id="tokenWindowOccurence_+3A_window.size">window.size</code></td>
<td>
<p>The distance within which tokens should occur from each other to be counted as a co-occurence.</p>
</td></tr>
<tr><td><code id="tokenWindowOccurence_+3A_direction">direction</code></td>
<td>
<p>a string indicating whether only the left ('&lt;') or right ('&gt;') side of the window, or both ('&lt;&gt;'), should be used.</p>
</td></tr>
<tr><td><code id="tokenWindowOccurence_+3A_distance_as_value">distance_as_value</code></td>
<td>
<p>If True, the values of the matrix will represent the shorts distance to the occurence of a feature</p>
</td></tr>
<tr><td><code id="tokenWindowOccurence_+3A_batch_rows">batch_rows</code></td>
<td>
<p>Used in functions that call this function in batches</p>
</td></tr>
<tr><td><code id="tokenWindowOccurence_+3A_drop_empty_terms">drop_empty_terms</code></td>
<td>
<p>If TRUE, emtpy terms (with zero occurence) will be dropped</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two matrices. position.mat gives the specific position of a term, and window.mat gives the window in which each token occured. The rows represent the position of a term, and matches the input of this function (position, term and context). The columns represents terms.
</p>

<hr>
<h2 id='top_features'>Show top features</h2><span id='topic+top_features'></span>

<h3>Description</h3>

<p>Show top features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>top_features(
  tc,
  feature,
  n = 10,
  group_by = NULL,
  group_by_meta = NULL,
  rank_by = c("freq", "chi2"),
  dropNA = T,
  return_long = F
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="top_features_+3A_tc">tc</code></td>
<td>
<p>a tCorpus</p>
</td></tr>
<tr><td><code id="top_features_+3A_feature">feature</code></td>
<td>
<p>The name of the feature</p>
</td></tr>
<tr><td><code id="top_features_+3A_n">n</code></td>
<td>
<p>Return the top n features</p>
</td></tr>
<tr><td><code id="top_features_+3A_group_by">group_by</code></td>
<td>
<p>A column in the token data to group the top features by. For example, if token data contains part-of-speech tags (pos), then grouping by pos will show the top n feature per part-of-speech tag.</p>
</td></tr>
<tr><td><code id="top_features_+3A_group_by_meta">group_by_meta</code></td>
<td>
<p>A column in the meta data to group the top features by.</p>
</td></tr>
<tr><td><code id="top_features_+3A_rank_by">rank_by</code></td>
<td>
<p>The method for ranking the terms. Currently supports frequency (default) and the 'Chi2' value for the
relative frequency of a term in a topic compared to the overall corpus.
If return_long is used, the Chi2 score is also returned, but note that there are negative Chi2 scores.
This is used to indicate that the relative frequency of a feature in a group was lower than
the relative frequency in the corpus  (i.e. under-represented).</p>
</td></tr>
<tr><td><code id="top_features_+3A_dropna">dropNA</code></td>
<td>
<p>if TRUE, drop NA features</p>
</td></tr>
<tr><td><code id="top_features_+3A_return_long">return_long</code></td>
<td>
<p>if TRUE, results will be returned in a long format that contains more information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = tokens_to_tcorpus(corenlp_tokens, token_id_col = 'id')

top_features(tc, 'lemma')
top_features(tc, 'lemma', group_by = 'NER', group_by_meta='doc_id')
</code></pre>

<hr>
<h2 id='transform_rsyntax'>Apply rsyntax transformations</h2><span id='topic+transform_rsyntax'></span>

<h3>Description</h3>

<p>This is an experimental function for applying rsyntax transformations directly on a tcorpus,
to create a new tcorpus with the transformed tokens. The argument f should be self defined function
that wraps rsyntax transformations. Or more generally, a function that takes a tokens data.frame (or data.table) as input, and returns a tokens data.frame (or data.table). 
For examples, see corpustools:::ud_relcl, or corpustools::udpipe_simplify for a function that wraps multiple transformations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transform_rsyntax(tc, f, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transform_rsyntax_+3A_tc">tc</code></td>
<td>
<p>a tCorpus</p>
</td></tr>
<tr><td><code id="transform_rsyntax_+3A_f">f</code></td>
<td>
<p>functions that perform rsyntax tree transformations</p>
</td></tr>
<tr><td><code id="transform_rsyntax_+3A_...">...</code></td>
<td>
<p>arguments passed to f</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tCorpus after applying the transformations
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (interactive()) {
tc = tc_sotu_udpipe$copy()
tc2 = transform_rsyntax(tc, udpipe_simplify)

browse_texts(tc2)
   rsyntax::plot_tree(tc$tokens, token, lemma, POS, sentence_i=20)
   rsyntax::plot_tree(tc2$tokens, token, lemma, POS, sentence_i=20)
}
</code></pre>

<hr>
<h2 id='udpipe_clause_tqueries'>Get a list of tqueries for extracting who did what</h2><span id='topic+udpipe_clause_tqueries'></span>

<h3>Description</h3>

<p>An off-the-shelf list of tqueries for extracting subject-verb clauses. Designed for working with a tCorpus created with <code><a href="#topic+udpipe_tcorpus">udpipe_tcorpus</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_clause_tqueries(verbs = NULL, exclude_verbs = verb_lemma("quote"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="udpipe_clause_tqueries_+3A_verbs">verbs</code></td>
<td>
<p>A character vector for specific verbs to use. By default uses all verbs (except for those specified in exclude_verbs)</p>
</td></tr>
<tr><td><code id="udpipe_clause_tqueries_+3A_exclude_verbs">exclude_verbs</code></td>
<td>
<p>A character vector for specific verbs NOT to use. By default uses the verbs that indicate speech (that are used for extracting who said what, in <code><a href="#topic+udpipe_quote_tqueries">udpipe_quote_tqueries</a></code>)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>udpipe_clause_tqueries()
</code></pre>

<hr>
<h2 id='udpipe_quote_tqueries'>Get a list of tqueries for extracting quotes</h2><span id='topic+udpipe_quote_tqueries'></span>

<h3>Description</h3>

<p>An off-the-shelf list of tqueries for extracting quotes. Designed for working with a tCorpus created with <code><a href="#topic+udpipe_tcorpus">udpipe_tcorpus</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_quote_tqueries(say_verbs = verb_lemma("quote"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="udpipe_quote_tqueries_+3A_say_verbs">say_verbs</code></td>
<td>
<p>A character vector of verb lemma that indicate speech (e.g., say, state). A default list is included in verb_lemma('quote'), but certain lemma might be more accurate/appropriate depending on the corpus.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>udpipe_quote_tqueries()
</code></pre>

<hr>
<h2 id='udpipe_simplify'>Simplify tokenIndex created with the udpipe parser</h2><span id='topic+udpipe_simplify'></span>

<h3>Description</h3>

<p>This is an off-the-shelf implementation of several rsyntax transformation for 
simplifying text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_simplify(
  tokens,
  split_conj = T,
  rm_punct = F,
  new_sentences = F,
  rm_mark = F
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="udpipe_simplify_+3A_tokens">tokens</code></td>
<td>
<p>A tokenIndex, based on output from the ud parser.</p>
</td></tr>
<tr><td><code id="udpipe_simplify_+3A_split_conj">split_conj</code></td>
<td>
<p>If TRUE, split conjunctions into separate sentences</p>
</td></tr>
<tr><td><code id="udpipe_simplify_+3A_rm_punct">rm_punct</code></td>
<td>
<p>If TRUE, remove punctuation afterwards</p>
</td></tr>
<tr><td><code id="udpipe_simplify_+3A_new_sentences">new_sentences</code></td>
<td>
<p>If TRUE, assign new sentence and token_id after splitting</p>
</td></tr>
<tr><td><code id="udpipe_simplify_+3A_rm_mark">rm_mark</code></td>
<td>
<p>If TRUE, remove children with a mark relation if this is used in the simplification.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tokenIndex
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (interactive()) {
tc = tc_sotu_udpipe$copy()
tc2 = transform_rsyntax(tc, udpipe_simplify)

browse_texts(tc2)
   rsyntax::plot_tree(tc_sotu_udpipe$tokens, token, lemma, POS, sentence_i=20)
   rsyntax::plot_tree(tc2$tokens, token, lemma, POS, sentence_i=20)
}
</code></pre>

<hr>
<h2 id='udpipe_spanquote_tqueries'>Get a list of tqueries for finding candidates for span quotes.</h2><span id='topic+udpipe_spanquote_tqueries'></span>

<h3>Description</h3>

<p>Quote extraction with tqueries is limited to quotes within sentences. When (verbatim) quotes span multiple sentences (which we call span quotes here), they are often indicated
with quotation marks. While it is relatively easy to identify these quotes, it is less straightforward to identify the sources of these quotes.
A good approach is to first apply tqueries for finding quotes within sentences, because a source mentioned just before (we use 2 sentences) a span quote is often also the
source of this span quote. For cases where there is no previous source, we can apply simple queries for finding source candidates. Thats what the
tqueries created with the current function are for.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_spanquote_tqueries(say_verbs = verb_lemma("quote"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="udpipe_spanquote_tqueries_+3A_say_verbs">say_verbs</code></td>
<td>
<p>A character vector of verb lemma that indicate speech (e.g., say, state). A default list is included in verb_lemma('quote'), but certain lemma might be more accurate/appropriate depending on the corpus.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This procedure is supported in rsyntax with the <code><a href="rsyntax.html#topic+add_span_quotes">add_span_quotes</a></code> function. In corpustools this function is implemented within
the <code><a href="#topic+udpipe_quotes">udpipe_quotes</a></code> method. The current function provides the default tqueries for the span quotes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>udpipe_spanquote_tqueries()
</code></pre>

<hr>
<h2 id='udpipe_tcorpus'>Create a tCorpus using udpipe</h2><span id='topic+udpipe_tcorpus'></span><span id='topic+udpipe_tcorpus.character'></span><span id='topic+udpipe_tcorpus.data.frame'></span><span id='topic+udpipe_tcorpus.factor'></span><span id='topic+udpipe_tcorpus.corpus'></span>

<h3>Description</h3>

<p>This is simply shorthand for using create_tcorpus with the udpipe_ arguments and certain specific settings.
This is the way to create a tCorpus if you want to use the syntax analysis functionalities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_tcorpus(x, ...)

## S3 method for class 'character'
udpipe_tcorpus(
  x,
  model = "english-ewt",
  doc_id = 1:length(x),
  meta = NULL,
  max_sentences = NULL,
  model_path = getwd(),
  cache = 3,
  cores = NULL,
  batchsize = 50,
  use_parser = T,
  start_end = F,
  verbose = T,
  ...
)

## S3 method for class 'data.frame'
udpipe_tcorpus(
  x,
  model = "english-ewt",
  text_columns = "text",
  doc_column = "doc_id",
  max_sentences = NULL,
  model_path = getwd(),
  cache = 3,
  cores = 1,
  batchsize = 50,
  use_parser = T,
  start_end = F,
  verbose = T,
  ...
)

## S3 method for class 'factor'
udpipe_tcorpus(x, ...)

## S3 method for class 'corpus'
udpipe_tcorpus(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="udpipe_tcorpus_+3A_x">x</code></td>
<td>
<p>main input. can be a character (or factor) vector where each value is a full text, or a data.frame that has a column that contains full texts.</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_...">...</code></td>
<td>
<p>Arguments passed to create_tcorpus.character</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_model">model</code></td>
<td>
<p>The name of a Universal Dependencies language model (e.g., &quot;english-ewt&quot;, &quot;dutch-alpino&quot;), to use the udpipe package
(<code><a href="udpipe.html#topic+udpipe_annotate">udpipe_annotate</a></code>). If you don't know the model name, just type the language and you'll get a suggestion. Otherwise, use <code><a href="#topic+show_udpipe_models">show_udpipe_models</a></code> to get
an overview of the available models. For more information about udpipe and performance benchmarks of the UD models, see the
GitHub page of the <a href="https://github.com/bnosac/udpipe">udpipe package</a>.</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_doc_id">doc_id</code></td>
<td>
<p>if x is a character/factor vector, doc_id can be used to specify document ids. This has to be a vector of the same length as x</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_meta">meta</code></td>
<td>
<p>A data.frame with document meta information (e.g., date, source). The rows of the data.frame need to match the values of x</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_max_sentences">max_sentences</code></td>
<td>
<p>An integer. Limits the number of sentences per document to the specified number.</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_model_path">model_path</code></td>
<td>
<p>If udpipe_model is used, this path wil be used to look for the model, and if the model doesn't yet exist it will be downloaded to this location. Defaults to working directory</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_cache">cache</code></td>
<td>
<p>The number of persistent caches to keep for inputs of udpipe. The caches store tokens in batches.
This way, if a lot of data has to be parsed, or if R crashes, udpipe can continue from the latest batch instead of start over.
The caches are stored in the corpustools_data folder (in udpipe_model_path). Only the most recent [udpipe_caches] caches will be stored.</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_cores">cores</code></td>
<td>
<p>If udpipe_model is used, this sets the number of parallel cores. If not specified, will use the same number of cores as used by data.table (or limited to OMP_THREAD_LIMIT)</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_batchsize">batchsize</code></td>
<td>
<p>In order to report progress and cache results, texts are parsed with udpipe in batches of 50.
The price is that there will be some overhead for each batch, so for very large jobs it can be faster to increase the batchsize.
If the number of texts divided by the number of parallel cores is lower than the batchsize, the texts are evenly distributed over cores.</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_use_parser">use_parser</code></td>
<td>
<p>If TRUE, use dependency parser (only if udpipe_model is used)</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_start_end">start_end</code></td>
<td>
<p>If TRUE, include start and end positions of tokens</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, report progress. Only if x is large enough to require multiple sequential batches</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_text_columns">text_columns</code></td>
<td>
<p>if x is a data.frame, this specifies the column(s) that contains text. The texts are paste together in the order specified here.</p>
</td></tr>
<tr><td><code id="udpipe_tcorpus_+3A_doc_column">doc_column</code></td>
<td>
<p>If x is a data.frame, this specifies the column with the document ids.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## ...
if (interactive()) {
tc = udpipe_tcorpus(c('Text one first sentence. Text one second sentence', 'Text two'), 
                     model = 'english-ewt')
tc$tokens
}
if (interactive()) {
tc = udpipe_tcorpus(sotu_texts[1:5,], doc_column='id', model = 'english-ewt')
tc$tokens
}
## It makes little sense to have full texts as factors, but it tends to happen.
## The create_tcorpus S3 method for factors is essentially identical to the
##  method for a character vector.

text = factor(c('Text one first sentence', 'Text one second sentence'))
if (interactive()) {
tc = udpipe_tcorpus(text, 'english-ewt-')
tc$tokens
}
# library(quanteda)
# udpipe_tcorpus(data_corpus_inaugural, 'english-ewt')
</code></pre>

<hr>
<h2 id='untokenize'>Reconstruct original texts</h2><span id='topic+untokenize'></span>

<h3>Description</h3>

<p>If the tCorpus was created with remember_spaces = T, you can rebuild the original texts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>untokenize(tc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="untokenize_+3A_tc">tc</code></td>
<td>
<p>A tCorpus, created with <code><a href="#topic+create_tcorpus">create_tcorpus</a></code>, with remember_spaces = TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with the text fields and meta fields as columns.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tc = create_tcorpus(sotu_texts, doc_column='id')
untokenize(tc)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
