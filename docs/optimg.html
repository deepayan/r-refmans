<!DOCTYPE html><html lang="en"><head><title>Help for package optimg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {optimg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#optimg'><p>General-Purpose Gradient-Based Optimization</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>General-Purpose Gradient-Based Optimization</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-10-07</td>
</tr>
<tr>
<td>Author:</td>
<td>Vithor Rosa Franco &lt;vithorfranco@gmail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Vithor Rosa Franco &lt;vithorfranco@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides general purpose tools for helping users to implement steepest
             gradient descent methods for function optimization; for details see
             Ruder (2016) &lt;<a href="https://doi.org/10.48550/arXiv.1609.04747">doi:10.48550/arXiv.1609.04747</a>&gt;. Currently, the Steepest 2-Groups
             Gradient Descent and the Adaptive Moment Estimation (Adam) are the
             methods implemented. Other methods will be implemented in the future.</td>
</tr>
<tr>
<td>Imports:</td>
<td>ucminf (&ge; 1.1-4)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/vthorrf/optimg">https://github.com/vthorrf/optimg</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-10-07 12:11:12 UTC; Avell</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-10-07 16:30:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='optimg'>General-Purpose Gradient-Based Optimization</h2><span id='topic+optimg'></span>

<h3>Description</h3>

<p>General-purpose optimization based on gradient descent algorithms. It is greatly inspired on the stats::optim function, aiming at increased usability and adaptability for optim's users.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimg(par, fn, gr=NULL, ..., method=c("STGD","ADAM"),
       Interval=1e-6, maxit=100, tol=1e-8, full=F, verbose=F)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optimg_+3A_par">par</code></td>
<td>
<p>Initial values for the parameters to be optimized over.</p>
</td></tr>
<tr><td><code id="optimg_+3A_fn">fn</code></td>
<td>
<p>A function to be minimized. It should return a scalar result.</p>
</td></tr>
<tr><td><code id="optimg_+3A_gr">gr</code></td>
<td>
<p>A function to return the gradient. Should include &quot;Interval&quot; as an argument, even if not used. If it is NULL, a finite-difference approximation will be used.</p>
</td></tr>
<tr><td><code id="optimg_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to fn and gr.</p>
</td></tr>
<tr><td><code id="optimg_+3A_method">method</code></td>
<td>
<p>The method to be used. See ‘Details’.</p>
</td></tr>
<tr><td><code id="optimg_+3A_interval">Interval</code></td>
<td>
<p>The epsilon difference to be used by gr.</p>
</td></tr>
<tr><td><code id="optimg_+3A_maxit">maxit</code></td>
<td>
<p>The maximum number of iterations. Defaults to 100.</p>
</td></tr>
<tr><td><code id="optimg_+3A_tol">tol</code></td>
<td>
<p>Relative convergence tolerance. The algorithm stops if it is unable to reduce the value by a factor of reltol * (abs(val) + reltol) at a step. Defaults to 1e-8.</p>
</td></tr>
<tr><td><code id="optimg_+3A_full">full</code></td>
<td>
<p>Boolean argument for returning all results, or only the basics (see Value). Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="optimg_+3A_verbose">verbose</code></td>
<td>
<p>Boolean argument for printing update status. Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As with the optim function, ... must be matched exactly. Also as with optim, optimg performs minimization. All the methods implemented are the &quot;steepest&quot; variation of the original methods. This means that tuning parameters are optimized at each step automatically. This makes the algorithms slower, but also more effective, especially for complex models.
</p>
<p>The default method (unless the length of par is equal to 1; in which case, the default is &quot;ADAM&quot;) is an implementation of the Steepest 2-Group Gradient Descent (&quot;STGD&quot;) algorithm. This algorithm is a variation of the Steepest Gradient Descent method which optimizes different step sizes for two groups of gradients: those within a standard deviation (below or above), and those beyond a standard deviation (below or above).
</p>
<p>Method &quot;ADAM&quot; is the Adaptive Moment Estimation method. This method computes adaptive learning rates for each parameter. Adam stores both exponentially decaying average of past squared gradients, as well as measures of momentum.
</p>


<h3>Value</h3>

<p>If full = FALSE, a list with components:
</p>
<table role = "presentation">
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of fn corresponding to par.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>A scalar giving the number of iterations before convergence is reached.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer code. 0 indicates successful completion. 1 indicates that the iteration limit maxit had been reached.</p>
</td></tr>
</table>
<p>If full = TRUE, apart from the above, components regarding the changes of par, gradient, value, and auxiliary parameters will also be returned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Fit a simple linear regression with RMSE as cost function
require(optimg)

# Predictor
x &lt;- seq(-3,3,len=100)
# Criterion
y &lt;- rnorm(100, 2 + {1.2*x}, 1)

# RMSE cost function
fn &lt;- function(par, X) {
  mu &lt;- par[1] + {par[2] * X}
  rmse &lt;- sqrt(mean({y-mu}^2))
  return(rmse)
}

# Compare optimization methods
optim(c(0,0),fn,X=x,method="Nelder-Mead")
optim(c(0,0),fn,X=x,method="BFGS")
optim(c(0,0),fn,X=x,method="CG")
optim(c(0,0),fn,X=x,method="L-BFGS-B")
optimg(c(0,0),fn,X=x,method="ADAM")
optimg(c(0,0),fn,X=x,method="STGD")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
