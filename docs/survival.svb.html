<!DOCTYPE html><html><head><title>Help for package survival.svb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {survival.svb}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#elbo'><p>Compute the evidence lower bound (ELBO)</p></a></li>
<li><a href='#svb.fit'><p>Fit sparse variational Bayesian proportional hazards models.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fit High-Dimensional Proportional Hazards Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0-2</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-01-17</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Komodromos</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Komodromos &lt;mk1019@ic.ac.uk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of methodology designed to perform: (i) variable 
    selection, (ii) effect estimation, and (iii) uncertainty quantification, 
    for high-dimensional survival data. Our method uses a spike-and-slab prior 
    with Laplace slab and Dirac spike and approximates the corresponding 
    posterior using variational inference, a popular method in machine learning 
    for scalable conditional inference. Although approximate, the variational 
    posterior provides excellent point estimates and good control of the false 
    discovery rate. For more information see Komodromos et al. (2021) 
    &lt;<a href="https://doi.org/10.48550/arXiv.2112.10270">doi:10.48550/arXiv.2112.10270</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.6), glmnet, survival</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mkomod/survival.svb">https://github.com/mkomod/survival.svb</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mkomod/survival.svb/issues">https://github.com/mkomod/survival.svb/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-01-17 09:41:08 UTC; michael</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-01-17 10:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='elbo'>Compute the evidence lower bound (ELBO)</h2><span id='topic+elbo'></span>

<h3>Description</h3>

<p>Compute the evidence lower bound (ELBO)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elbo(Y, delta, X, fit, nrep = 10000, center = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="elbo_+3A_y">Y</code></td>
<td>
<p>Failure times.</p>
</td></tr>
<tr><td><code id="elbo_+3A_delta">delta</code></td>
<td>
<p>Censoring indicator, 0: censored, 1: uncensored.</p>
</td></tr>
<tr><td><code id="elbo_+3A_x">X</code></td>
<td>
<p>Design matrix.</p>
</td></tr>
<tr><td><code id="elbo_+3A_fit">fit</code></td>
<td>
<p>Fit model.</p>
</td></tr>
<tr><td><code id="elbo_+3A_nrep">nrep</code></td>
<td>
<p>Number of Monte Carlo samples.</p>
</td></tr>
<tr><td><code id="elbo_+3A_center">center</code></td>
<td>
<p>Should the design matrix be centered.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list containing: <br />
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p>The mean of the ELBO.</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>The standard-deviation of the ELBO.</p>
</td></tr>
<tr><td><code>expected.likelihood</code></td>
<td>
<p>The expectation of the likelihood
under the variational posterior.</p>
</td></tr>
<tr><td><code>kl</code></td>
<td>
<p>The KL between the variational posterior and prior.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The evidence lower bound (ELBO) is a popular goodness of fit measure
used in variational inference. Under the variational posterior the
ELBO is given as
</p>
<p style="text-align: center;"><code class="reqn">ELBO = E_{\tilde{\Pi}}[\log L_p(\beta; Y, X, \delta)] - KL(\tilde{\Pi} \| \Pi)</code>
</p>

<p>where <code class="reqn">\tilde{\Pi}</code> is the variational posterior, <code class="reqn">\Pi</code> is the prior,
<code class="reqn">L_p(\beta; Y, X, \delta)</code> is Cox's partial likelihood. Intuitively,
within the ELBO we incur a trade-off between how well we fit to the data
(through the expectation of the log-partial-likelihood) and how close we
are to our prior (in terms of KL divergence). Ideally we want the ELBO to be 
as small as possible.
</p>

<hr>
<h2 id='svb.fit'>Fit sparse variational Bayesian proportional hazards models.</h2><span id='topic+svb.fit'></span>

<h3>Description</h3>

<p>Fit sparse variational Bayesian proportional hazards models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svb.fit(
  Y,
  delta,
  X,
  lambda = 1,
  a0 = 1,
  b0 = ncol(X),
  mu.init = NULL,
  s.init = rep(0.05, ncol(X)),
  g.init = rep(0.5, ncol(X)),
  maxiter = 1000,
  tol = 0.001,
  alpha = 1,
  center = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svb.fit_+3A_y">Y</code></td>
<td>
<p>Failure times.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_delta">delta</code></td>
<td>
<p>Censoring indicator, 0: censored, 1: uncensored.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_x">X</code></td>
<td>
<p>Design matrix.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_lambda">lambda</code></td>
<td>
<p>Penalisation parameter, default: <code>lambda=1</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_a0">a0</code></td>
<td>
<p>Beta distribution parameter, default: <code>a0=1</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_b0">b0</code></td>
<td>
<p>Beta distribution parameter, default: <code>b0=ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_mu.init">mu.init</code></td>
<td>
<p>Initial value for the mean of the Gaussian component of 
the variational family (<code class="reqn">\mu</code>), default taken from LASSO fit.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_s.init">s.init</code></td>
<td>
<p>Initial value for the standard deviations of the Gaussian 
component of the variational family (<code class="reqn">s</code>), default: 
<code>rep(0.05, ncol(X))</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_g.init">g.init</code></td>
<td>
<p>Initial value for the inclusion probability (<code class="reqn">\gamma</code>), 
default: <code>rep(0.5, ncol(X))</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations, default: <code>1000</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance, default: <code>0.001</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_alpha">alpha</code></td>
<td>
<p>The elastic-net mixing parameter used for initialising <code>mu.init</code>. 
When <code>alpha=1</code> the lasso penalty is used and <code>alpha=0</code> the ridge 
penalty, values between 0 and 1 give a mixture of the two penalties, default:
<code>1</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_center">center</code></td>
<td>
<p>Center X prior to fitting, increases numerical stability, 
default: <code>TRUE</code></p>
</td></tr>
<tr><td><code id="svb.fit_+3A_verbose">verbose</code></td>
<td>
<p>Print additional information: default: <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list containing: <br />
</p>
<table>
<tr><td><code>beta_hat</code></td>
<td>
<p>Point estimate for the coefficients <code class="reqn">\beta</code>, taken as 
the mean under the variational approximation.
<code class="reqn">\hat{\beta}_j = E_{\tilde{\Pi}} [ \beta_j ] = \gamma_j \mu_j</code>.</p>
</td></tr>
<tr><td><code>inclusion_prob</code></td>
<td>
<p>Posterior inclusion probabilities. Used to describe
the posterior probability a coefficient is non-zero.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>Final value for the means of the Gaussian component of the variational 
family <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code>s</code></td>
<td>
<p>Final value for the standard deviation of the Gaussian component of 
the variational family <code class="reqn">s</code>.</p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>Final value for the inclusion probability (<code class="reqn">\gamma</code>).</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Value of lambda used.</p>
</td></tr>
<tr><td><code>a0</code></td>
<td>
<p>Value of <code class="reqn">\alpha_0</code> used.</p>
</td></tr>
<tr><td><code>b0</code></td>
<td>
<p>Value of <code class="reqn">\beta_0</code> used.</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>Describes whether the algorithm converged.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Rather than compute the posterior using MCMC, we turn to approximating it
using variational inference. Within variational inference we re-cast
Bayesian inference as an optimisation problem, where we minimize the 
Kullback-Leibler (KL) divergence between a family of tractable distributions 
and the posterior, <code class="reqn">\Pi</code>. <br /> <br /> In our case we use a mean-field variational 
family,
</p>
<p style="text-align: center;"><code class="reqn">Q = \{ \prod_{j=1}^p \gamma_j N(\mu_j, s_j^2) + (1 - \gamma_j) \delta_0 \}</code>
</p>

<p>where <code class="reqn">\mu_j</code> is the mean and <code class="reqn">s_j</code> the std. dev for the Gaussian 
component, <code class="reqn">\gamma_j</code> the inclusion probabilities, <code class="reqn">\delta_0</code> a Dirac mass 
at zero and <code class="reqn">p</code> the number of coefficients.<br /> <br /> The components of the
variational family (<code class="reqn">\mu, s, \gamma</code>) are then optimised by minimizing the 
Kullback-Leibler divergence between the variational family and the posterior,
</p>
<p style="text-align: center;"><code class="reqn">\tilde{\Pi} = \arg \min KL(Q \| \Pi).</code>
</p>
<p> We use co-ordinate ascent
variational inference (CAVI) to solve the above optimisation problem. <br /> <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 125                        # number of sample
p &lt;- 250                        # number of features
s &lt;- 5                          # number of non-zero coefficients
censoring_lvl &lt;- 0.25           # degree of censoring


# generate some test data
set.seed(1)
b &lt;- sample(c(runif(s, -2, 2), rep(0, p-s)))
X &lt;- matrix(rnorm(n * p), nrow=n)
Y &lt;- log(1 - runif(n)) / -exp(X %*% b)
delta  &lt;- runif(n) &gt; censoring_lvl   		# 0: censored, 1: uncensored
Y[!delta] &lt;- Y[!delta] * runif(sum(!delta))	# rescale censored data


# fit the model
f &lt;- survival.svb::svb.fit(Y, delta, X, mu.init=rep(0, p))


## Larger Example
n &lt;- 250                        # number of sample
p &lt;- 1000                       # number of features
s &lt;- 10                         # number of non-zero coefficients
censoring_lvl &lt;- 0.4            # degree of censoring


# generate some test data
set.seed(1)
b &lt;- sample(c(runif(s, -2, 2), rep(0, p-s)))
X &lt;- matrix(rnorm(n * p), nrow=n)
Y &lt;- log(1 - runif(n)) / -exp(X %*% b)
delta  &lt;- runif(n) &gt; censoring_lvl   		# 0: censored, 1: uncensored
Y[!delta] &lt;- Y[!delta] * runif(sum(!delta))	# rescale censored data


# fit the model
f &lt;- survival.svb::svb.fit(Y, delta, X)


# plot the results
plot(b, xlab=expression(beta), main="Coefficient value", pch=8, ylim=c(-2,2))
points(f$beta_hat, pch=20, col=2)
legend("topleft", legend=c(expression(beta), expression(hat(beta))),
       pch=c(8, 20), col=c(1, 2))
plot(f$inclusion_prob, main="Inclusion Probabilities", ylab=expression(gamma))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
