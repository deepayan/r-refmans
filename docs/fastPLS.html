<!DOCTYPE html><html lang="en"><head><title>Help for package fastPLS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fastPLS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#fastcor'><p>Fast Correlation Analysis</p></a></li>
<li><a href='#grid-internal'><p>Internal Grid Functions</p></a></li>
<li><a href='#optim.pls.cv'><p>Cross-Validation with PLS-DA.</p></a></li>
<li><a href='#pls'><p>Partial Least Squares.</p></a></li>
<li><a href='#pls.double.cv'><p>Cross-Validation with PLS-DA.</p></a></li>
<li><a href='#predict.fastPLS'><p>Prediction Partial Least Squares regression.</p></a></li>
<li><a href='#transformy'><p>Conversion Classification Vector to Matrix</p></a></li>
<li><a href='#ViP'><p>Variable Importance in the Projection.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Fast Implementation of Partial Least Square</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-12-09</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation in 'Rcpp' / 'RcppArmadillo' of Partial Least Square algorithms. This package includes other functions to perform the double cross-validation and a fast correlation.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.0), Matrix</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.17), methods</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, Matrix</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-12-10 09:54:19 UTC; user</td>
</tr>
<tr>
<td>Author:</td>
<td>Stefano Cacciatore
    <a href="https://orcid.org/0000-0001-7052-7156"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, trl,
    cre],
  Dupe Ojo <a href="https://orcid.org/0000-0002-5301-8592"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Leonardo Tenori <a href="https://orcid.org/0000-0001-6438-059X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Alessia Vignoli <a href="https://orcid.org/0000-0003-4038-6596"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Stefano Cacciatore &lt;stefano.cacciatore@icgeb.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-12-11 15:30:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='fastcor'>Fast Correlation Analysis</h2><span id='topic+fastcor'></span>

<h3>Description</h3>

<p>This function perform a fast calculation of the Spearman's correlation coefficient.</p>


<h3>Usage</h3>

<pre><code class='language-R'>fastcor (a, b=NULL, byrow=TRUE, diag=TRUE)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fastcor_+3A_a">a</code></td>
<td>
<p>a matrix of training set cases.</p>
</td></tr>
<tr><td><code id="fastcor_+3A_b">b</code></td>
<td>
<p>a matrix of training set cases.</p>
</td></tr>
<tr><td><code id="fastcor_+3A_byrow">byrow</code></td>
<td>
<p>if byrow == T rows are correlated (much faster) else columns</p>
</td></tr>
<tr><td><code id="fastcor_+3A_diag">diag</code></td>
<td>
<p>if diag == T only the diagonal of the cor matrix is returned (much faster).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output matrix of correlation coefficient.
</p>


<h3>Author(s)</h3>

<p>Stefano Cacciatore, Leonardo Tenori, Dupe Ojo, Alessia Vignoli</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim.pls.cv">optim.pls.cv</a></code>,<code><a href="#topic+pls.double.cv">pls.double.cv</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
data=as.matrix(iris[,-5])
fastcor(data)



</code></pre>

<hr>
<h2 id='grid-internal'>Internal Grid Functions</h2><span id='topic+grid-internal'></span>

<h3>Description</h3>

<p>Internal Grid functions
</p>


<h3>Details</h3>

<p>These are not to be called by the user (or in some cases are just
waiting for proper documentation to be written :).
</p>


<h3>Value</h3>

<p>The return values of these function are used for internal usage.
</p>

<hr>
<h2 id='optim.pls.cv'>Cross-Validation with PLS-DA.</h2><span id='topic+optim.pls.cv'></span>

<h3>Description</h3>

<p>This function performs a 10-fold cross validation on a given data set using Partial Least Squares (PLS) model. To assess the prediction ability of the model, a 10-fold cross-validation is conducted by generating splits with a ratio 1:9 of the data set. This is achieved by removing 10% of samples prior to any step of the statistical analysis, including PLS component selection and scaling. Best number of component for PLS was carried out by means of 10-fold cross-validation on the remaining 90% selecting the best Q2y value. Permutation testing was undertaken to estimate the classification/regression performance of predictors.</p>


<h3>Usage</h3>

<pre><code class='language-R'>
optim.pls.cv (Xdata,
              Ydata, 
              ncomp, 
              constrain=NULL,
              scaling = c("centering", "autoscaling","none"),
              method = c("plssvd", "simpls"),
              svd.method = c("irlba", "dc"),
              kfold=10)              
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optim.pls.cv_+3A_xdata">Xdata</code></td>
<td>
<p>a matrix of independent variables or predictors.</p>
</td></tr>
<tr><td><code id="optim.pls.cv_+3A_ydata">Ydata</code></td>
<td>
<p>the responses. If Ydata is a numeric vector, a regression analysis will be performed. If Ydata is factor, a classification analysis will be performed. </p>
</td></tr>
<tr><td><code id="optim.pls.cv_+3A_ncomp">ncomp</code></td>
<td>
<p>the number of latent components to be used for classification.</p>
</td></tr>
<tr><td><code id="optim.pls.cv_+3A_constrain">constrain</code></td>
<td>
<p>a vector of <code>nrow(data)</code> elements. Sample sharing a specific  identifier or characteristics will be grouped together either in the training set or in the test set of cross-validation.</p>
</td></tr>  
<tr><td><code id="optim.pls.cv_+3A_scaling">scaling</code></td>
<td>
<p>the scaling method to be used. Choices are &quot;<code>centering</code>&quot;, &quot;<code>autoscaling</code>&quot;, or &quot;<code>none</code>&quot; (by default = &quot;<code>centering</code>&quot;). A partial string sufficient to uniquely identify the choice is permitted.</p>
</td></tr>
<tr><td><code id="optim.pls.cv_+3A_method">method</code></td>
<td>
<p>the algorithm to be used to perform the PLS. Choices are &quot;<code>plssvd</code>&quot; or &quot;<code>simpls</code>&quot; (by default = &quot;<code>plssvd</code>&quot;). A partial string sufficient to uniquely identify the choice is permitted.</p>
</td></tr>
<tr><td><code id="optim.pls.cv_+3A_svd.method">svd.method</code></td>
<td>
<p>the SVD method to be used to perform the PLS. Choices are &quot;<code>irlba</code>&quot; or &quot;<code>dc</code>&quot; (by default = &quot;<code>irlba</code>&quot;). A partial string sufficient to uniquely identify the choice is permitted.</p>
</td></tr>
<tr><td><code id="optim.pls.cv_+3A_kfold">kfold</code></td>
<td>
<p>number of cross-validations loops.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output of the result is a list with the following components:
</p>
<table role = "presentation">
<tr><td><code>B</code></td>
<td>
<p>the (p x m x length(ncomp)) array containing the regression coefficients. Each row corresponds to a predictor variable and each column to a response variable. The third dimension of the matrix B corresponds to the number of PLS components used to compute the regression coefficients. If ncomp has length 1, B is just a (p x m) matrix.</p>
</td></tr>
<tr><td><code>Ypred</code></td>
<td>
<p>the vector containing the predicted values of the response variables obtained by cross-validation.</p>
</td></tr>
<tr><td><code>Yfit</code></td>
<td>
<p>the vector containing the fitted values of the response variables.</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>the (p x max(ncomp)) matrix containing the X-loadings.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>the (m x max(ncomp)) matrix containing the Y-loadings.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the (ntrain x max(ncomp)) matrix containing the X-scores (latent components)</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>the (p x max(ncomp)) matrix containing the weights used to construct the latent components.</p>
</td></tr>
<tr><td><code>Q2Y</code></td>
<td>
<p>predicting power of model.</p>
</td></tr>
<tr><td><code>R2Y</code></td>
<td>
<p>proportion of variance in Y.</p>
</td></tr>
<tr><td><code>R2X</code></td>
<td>
<p>vector containg the explained variance of X by each PLS component.</p>
</td></tr>
<tr><td><code>txtQ2Y</code></td>
<td>
<p>a summary of the Q2y values.</p>
</td></tr>
<tr><td><code>txtR2Y</code></td>
<td>
<p>a summary of the R2y values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dupe Ojo, Alessia Vignoli, Stefano Cacciatore, Leonardo Tenori</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls">pls</a></code>,<code><a href="#topic+pls.double.cv">pls.double.cv</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
data=iris[,-5]
labels=iris[,5]
pp=optim.pls.cv(data,labels,2:4)
pp$optim_comp


</code></pre>

<hr>
<h2 id='pls'>Partial Least Squares.</h2><span id='topic+pls'></span>

<h3>Description</h3>

<p>Partial Least Squares (PLS) classification and regression for test set from training set.</p>


<h3>Usage</h3>

<pre><code class='language-R'>           
pls (Xtrain, 
     Ytrain, 
     Xtest = NULL, 
     Ytest = NULL, 
     ncomp=min(5,c(ncol(Xtrain),nrow(Xtrain))),
     scaling = c("centering", "autoscaling","none"), 
     method = c("plssvd", "simpls"),
     svd.method = c("irlba", "dc"),
     fit = FALSE,
     proj = FALSE, 
     perm.test = FALSE, 
     times = 100)           
           
           

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pls_+3A_xtrain">Xtrain</code></td>
<td>
<p>a matrix of training set cases.</p>
</td></tr>
<tr><td><code id="pls_+3A_ytrain">Ytrain</code></td>
<td>
<p>a classification vector.</p>
</td></tr>
<tr><td><code id="pls_+3A_xtest">Xtest</code></td>
<td>
<p>a matrix of test set cases.</p>
</td></tr>
<tr><td><code id="pls_+3A_ytest">Ytest</code></td>
<td>
<p>a classification vector.</p>
</td></tr>
<tr><td><code id="pls_+3A_ncomp">ncomp</code></td>
<td>
<p>the number of components to consider.</p>
</td></tr>
<tr><td><code id="pls_+3A_scaling">scaling</code></td>
<td>
<p>the scaling method to be used. Choices are &quot;<code>centering</code>&quot;, &quot;<code>autoscaling</code>&quot;, or &quot;<code>none</code>&quot; (by default = &quot;<code>centering</code>&quot;). A partial string sufficient to uniquely identify the choice is permitted.</p>
</td></tr>
<tr><td><code id="pls_+3A_method">method</code></td>
<td>
<p>the algorithm to be used to perform the PLS. Choices are &quot;<code>plssvd</code>&quot; or &quot;<code>simpls</code>&quot; (by default = &quot;<code>plssvd</code>&quot;). A partial string sufficient to uniquely identify the choice is permitted.</p>
</td></tr>
<tr><td><code id="pls_+3A_svd.method">svd.method</code></td>
<td>
<p>the SVD method to be used to perform the PLS. Choices are &quot;<code>irlba</code>&quot; or &quot;<code>dc</code>&quot; (by default = &quot;<code>irlba</code>&quot;). A partial string sufficient to uniquely identify the choice is permitted.</p>
</td></tr>
<tr><td><code id="pls_+3A_fit">fit</code></td>
<td>
<p>a boolean value to perform the fit.</p>
</td></tr>
<tr><td><code id="pls_+3A_proj">proj</code></td>
<td>
<p>a boolean value to perform the fit.</p>
</td></tr>
<tr><td><code id="pls_+3A_perm.test">perm.test</code></td>
<td>
<p>a classification vector.</p>
</td></tr>
<tr><td><code id="pls_+3A_times">times</code></td>
<td>
<p>a classification vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table role = "presentation">
<tr><td><code>B</code></td>
<td>
<p>the (p x m x length(ncomp)) matrix containing the regression coefficients. Each row corresponds to a predictor variable and each column to a response variable. The third dimension of the matrix B corresponds to the number of PLS components used to compute the regression coefficients. If ncomp has length 1, B is just a (p x m) matrix.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>the (m x max(ncomp)) matrix containing the Y-loadings.</p>
</td></tr>
<tr><td><code>Ttrain</code></td>
<td>
<p>the (ntrain x max(ncomp)) matrix containing the X-scores (latent components)</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>the (p x max(ncomp)) matrix containing the weights used to construct the latent components.</p>
</td></tr>
<tr><td><code>mX</code></td>
<td>
<p>mean X.</p>
</td></tr>
<tr><td><code>vX</code></td>
<td>
<p>variance X.</p>
</td></tr>
<tr><td><code>mY</code></td>
<td>
<p>mean Y.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>matrix for the independent variable X. This indicates how the original data relates to the latent components.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>number of predictor variables</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p>number of components used in the PLS model</p>
</td></tr>
<tr><td><code>Yfit</code></td>
<td>
<p>the prediction values based on the PLS model</p>
</td></tr>
<tr><td><code>R2Y</code></td>
<td>
<p>proportion of variance in Y</p>
</td></tr>
<tr><td><code>classification</code></td>
<td>
<p>a boolgean output is given indicating if the response variable is a classification</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>level of response variable Y</p>
</td></tr>
<tr><td><code>Ypred</code></td>
<td>
<p>the (ntest x m x length(ncomp)) containing the predicted values of the response variables for the observations from Xtest. The third dimension of the matrix Ypred corresponds to the number of PLS components used to compute the regression coefficients.</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>the (p x max(ncomp)) matrix containing the X-loadings.</p>
</td></tr>
<tr><td><code>Ttest</code></td>
<td>
<p>...</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dupe Ojo, Alessia Vignoli, Stefano Cacciatore, Leonardo Tenori</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim.pls.cv">optim.pls.cv</a></code>,<code><a href="#topic+pls.double.cv">pls.double.cv</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
data=iris[,-5]
labels=iris[,5]
ss=sample(150,15)
ncomponent=3

z=pls(data[-ss,], labels[-ss], data[ss,], ncomp=ncomponent) 

</code></pre>

<hr>
<h2 id='pls.double.cv'>Cross-Validation with PLS-DA.</h2><span id='topic+pls.double.cv'></span>

<h3>Description</h3>

<p>This function performs a 10-fold cross validation on a given data set using Partial Least Squares (PLS) model. To assess the prediction ability of the model, a 10-fold cross-validation is conducted by generating splits with a ratio 1:9 of the data set, that is by removing 10% of samples prior to any step of the statistical analysis, including PLS component selection and scaling. Best number of component for PLS was carried out by means of 10-fold cross-validation on the remaining 90% selecting the best Q2y value. Permutation testing was undertaken to estimate the classification/regression performance of predictors.</p>


<h3>Usage</h3>

<pre><code class='language-R'>
pls.double.cv (Xdata,
               Ydata,
               ncomp=min(5,c(ncol(Xdata),nrow(Xdata))),
               constrain=1:nrow(Xdata),
               scaling = c("centering", "autoscaling","none"), 
               method = c("plssvd", "simpls"),
              svd.method = c("irlba", "dc"),
               perm.test=FALSE,
               times=100,
               runn=10,
               kfold_inner=10, 
               kfold_outer=10)           
              
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pls.double.cv_+3A_xdata">Xdata</code></td>
<td>
<p>a matrix.</p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_ydata">Ydata</code></td>
<td>
<p>the responses. If Ydata is a numeric vector, a regression analysis will be performed. If Ydata is factor, a classification analysis will be performed. </p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_ncomp">ncomp</code></td>
<td>
<p>the number of latent components to be used for classification.</p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_constrain">constrain</code></td>
<td>
<p>a vector of <code>nrow(data)</code> elements. Sample with the same identifying constrain will be split in the training set or in the test set of cross-validation together.</p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_scaling">scaling</code></td>
<td>
<p>the scaling method to be used. Choices are &quot;<code>centering</code>&quot;, &quot;<code>autoscaling</code>&quot;, or &quot;<code>none</code>&quot; (by default = &quot;<code>centering</code>&quot;). A partial string sufficient to uniquely identify the choice is permitted.</p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_method">method</code></td>
<td>
<p>the algorithm to be used to perform the PLS. Choices are &quot;<code>plssvd</code>&quot; or &quot;<code>simpls</code>&quot; (by default = &quot;<code>plssvd</code>&quot;). A partial string sufficient to uniquely identify the choice is permitted.</p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_svd.method">svd.method</code></td>
<td>
<p>the SVD method to be used to perform the PLS. Choices are &quot;<code>irlba</code>&quot; or &quot;<code>dc</code>&quot; (by default = &quot;<code>irlba</code>&quot;). A partial string sufficient to uniquely identify the choice is permitted.</p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_perm.test">perm.test</code></td>
<td>
<p>a classification vector.</p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_times">times</code></td>
<td>
<p>number of cross-validations with permutated samples</p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_runn">runn</code></td>
<td>
<p>number of cross-validations loops.</p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_kfold_inner">kfold_inner</code></td>
<td>
<p>if perform the optmization of the number of components.</p>
</td></tr>
<tr><td><code id="pls.double.cv_+3A_kfold_outer">kfold_outer</code></td>
<td>
<p>if perform the optmization of the number of components.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table role = "presentation">
<tr><td><code>B</code></td>
<td>
<p>the (p x m x length(ncomp)) array containing the regression coefficients. Each row corresponds to a predictor variable and each column to a response variable. The third dimension of the matrix B corresponds to the number of PLS components used to compute the regression coefficients. If ncomp has length 1, B is just a (p x m) matrix.</p>
</td></tr>
<tr><td><code>Ypred</code></td>
<td>
<p>the vector containing the predicted values of the response variables obtained by cross-validation.</p>
</td></tr>
<tr><td><code>Yfit</code></td>
<td>
<p>the vector containing the fitted values of the response variables.</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>the (p x max(ncomp)) matrix containing the X-loadings.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>the (m x max(ncomp)) matrix containing the Y-loadings.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the (ntrain x max(ncomp)) matrix containing the X-scores (latent components)</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>the (p x max(ncomp)) matrix containing the weights used to construct the latent components.</p>
</td></tr>
<tr><td><code>Q2Y</code></td>
<td>
<p>predictive power of the model.</p>
</td></tr>
<tr><td><code>R2Y</code></td>
<td>
<p>proportion of variance in Y.</p>
</td></tr>
<tr><td><code>R2X</code></td>
<td>
<p>vector containg the explained variance of X by each PLS component.</p>
</td></tr>
<tr><td><code>txtQ2Y</code></td>
<td>
<p>a summary of the Q2y values.</p>
</td></tr>
<tr><td><code>txtR2Y</code></td>
<td>
<p>a summary of the R2y values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dupe Ojo, Alessia Vignoli, Stefano Cacciatore, Leonardo Tenori</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim.pls.cv">optim.pls.cv</a></code>,<code><a href="#topic+pls">pls</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
data=iris[,-5]
labels=iris[,5]
pp=pls.double.cv(data,labels,2:4)


</code></pre>

<hr>
<h2 id='predict.fastPLS'>Prediction Partial Least Squares regression.</h2><span id='topic+predict.fastPLS'></span>

<h3>Description</h3>

<p>Partial Least Squares (PLS) regression for test set from training set.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fastPLS'
predict(object, newdata, Ytest=NULL, proj=FALSE, ...) 

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.fastPLS_+3A_object">object</code></td>
<td>
<p>a matrix of training set cases.</p>
</td></tr>
<tr><td><code id="predict.fastPLS_+3A_newdata">newdata</code></td>
<td>
<p>a matrix of predictor variables X for the test set.</p>
</td></tr>
<tr><td><code id="predict.fastPLS_+3A_ytest">Ytest</code></td>
<td>
<p>a vector of the response variable Y from Xtest.</p>
</td></tr>
<tr><td><code id="predict.fastPLS_+3A_proj">proj</code></td>
<td>
<p>projection of the test set.</p>
</td></tr>
<tr><td><code id="predict.fastPLS_+3A_...">...</code></td>
<td>
<p>further arguments. Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table role = "presentation">
<tr><td><code>Ypred</code></td>
<td>
<p>the (ntest x m x length(ncomp)) containing the predicted values of the response variables for the observations from Xtest. The third dimension of the matrix Ypred corresponds to the number of PLS components used to compute the regression coefficients.</p>
</td></tr>
<tr><td><code>Q2Y</code></td>
<td>
<p>predictive power of model</p>
</td></tr>
<tr><td><code>Ttest</code></td>
<td>
<p>the (ntrain x max(ncomp)) matrix containing the X-scores (latent components)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dupe Ojo, Alessia Vignoli, Stefano Cacciatore, Leonardo Tenori</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim.pls.cv">optim.pls.cv</a></code>,<code><a href="#topic+pls.double.cv">pls.double.cv</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
data=iris[,-5]
labels=iris[,5]
ss=sample(150,15)
ncomponent=3

z=pls(data[-ss,], labels[-ss],  ncomp=ncomponent) 
predict(z,data[ss,],FALSE)


</code></pre>

<hr>
<h2 id='transformy'>Conversion Classification Vector to Matrix</h2><span id='topic+transformy'></span>

<h3>Description</h3>

<p>This function converts a classification vector into a classification matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>transformy(y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="transformy_+3A_y">y</code></td>
<td>
<p>a vector or factor.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function converts a classification vector into a classification matrix. Different groups are compared amongst each other.
</p>


<h3>Value</h3>

<p>A matrix.
</p>


<h3>Author(s)</h3>

<p>Dupe Ojo, Alessia Vignoli, Stefano Cacciatore, Leonardo Tenori</p>


<h3>Examples</h3>

<pre><code class='language-R'>y=c(1,1,1,1,2,2,2,3,3)
print(y)
z=transformy(y)
print(z)
</code></pre>

<hr>
<h2 id='ViP'>Variable Importance in the Projection.</h2><span id='topic+ViP'></span>

<h3>Description</h3>

<p>Variable Importance in the Projection (VIP) is a score that measures how important a variable is in a Partial Least Squares (PLS) model. VIP scores are used to identify which variables are most important in a model and are often used for variable selection. </p>


<h3>Usage</h3>

<pre><code class='language-R'>ViP (model) 

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ViP_+3A_model">model</code></td>
<td>
<p>a object returning from the pls function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table role = "presentation">
<tr><td><code>B</code></td>
<td>
<p>the (p x m x length(ncomp)) matrix containing the regression coefficients. Each row corresponds to a predictor variable and each column to a response variable. The third dimension of the matrix B corresponds to the number of PLS components used to compute the regression coefficients. If ncomp has length 1, B is just a (p x m) matrix.</p>
</td></tr>
<tr><td><code>Ypred</code></td>
<td>
<p>the (ntest x m x length(ncomp)) containing the predicted values of the response variables for the observations from Xtest. The third dimension of the matrix Ypred corresponds to the number of PLS components used to compute the regression coefficients.</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>the (p x max(ncomp)) matrix containing the X-loadings.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>the (m x max(ncomp)) matrix containing the Y-loadings.</p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p>the (ntrain x max(ncomp)) matrix containing the X-scores (latent components)</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>the (p x max(ncomp)) matrix containing the weights used to construct the latent components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dupe Ojo, Alessia Vignoli, Stefano Cacciatore, Leonardo Tenori</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim.pls.cv">optim.pls.cv</a></code>,<code><a href="#topic+pls.double.cv">pls.double.cv</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
data=as.matrix(iris[,-5])
labels=iris[,5]
pp=pls(data,labels,ncomp = 2)
ViP(pp)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
