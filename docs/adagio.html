<!DOCTYPE html><html><head><title>Help for package adagio</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {adagio}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#assignment'>
<p>Linear Sum Assignment Problem</p></a></li>
<li><a href='#bpp_approx'>
<p>Approximate Bin Packing</p></a></li>
<li><a href='#change_making'>
<p>Change Making Problem</p></a></li>
<li><a href='#CMAES'>
<p>Covariance Matrix Adaptation Evolution Strategy</p></a></li>
<li><a href='#fminviz,flineviz'>
<p>Visualize Function Minimum</p></a></li>
<li><a href='#hamiltonian'>
<p>Finds a Hamiltonian path or cycle</p></a></li>
<li><a href='#Historize'>
<p>Historize function</p></a></li>
<li><a href='#hookejeeves'>
<p>Hooke-Jeeves Minimization Method</p></a></li>
<li><a href='#knapsack'>
<p>0-1 Knapsack Problem</p></a></li>
<li><a href='#maxempty'>
<p>Maximally Empty Rectangle Problem</p></a></li>
<li><a href='#maxquad'>
<p>The MAXQUAD Test Function</p></a></li>
<li><a href='#maxsub'>
<p>Maximal Sum Subarray</p></a></li>
<li><a href='#mknapsack'>
<p>Multiple 0-1 Knapsack Problem</p></a></li>
<li><a href='#neldermead'>
<p>Nelder-Mead Minimization Method</p></a></li>
<li><a href='#occurs'>
<p>Finding Subsequences</p></a></li>
<li><a href='#setcover'>
<p>Set cover problem</p></a></li>
<li><a href='#SIAM test functions'>
<p>Trefethen and Wagon Test Functions</p></a></li>
<li><a href='#simpleDE'>
<p>Simple Differential Evolution Algorithm</p></a></li>
<li><a href='#simpleEA'>
<p>Simple Evolutionary Algorithm</p></a></li>
<li><a href='#subsetsum'>
<p>Subset Sum Problem</p></a></li>
<li><a href='#Testfunctions'>
<p>Optimization Test Functions</p></a></li>
<li><a href='#transfinite'>
<p>Boxed Region Transformation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Discrete and Global Optimization Routines</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-26</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hans W. Borchers &lt;hwborchers@googlemail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, stats, lpSolve (&ge; 5.6.15)</td>
</tr>
<tr>
<td>Description:</td>
<td>
    The R package 'adagio' will provide methods and algorithms for
    (discrete) optimization, e.g. knapsack and subset sum procedures,
	derivative-free Nelder-Mead and Hooke-Jeeves minimization, and
	some (evolutionary) global optimization functions.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-26 08:37:53 UTC; hwb</td>
</tr>
<tr>
<td>Author:</td>
<td>Hans W. Borchers [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-26 13:30:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='assignment'>
Linear Sum Assignment Problem
</h2><span id='topic+assignment'></span>

<h3>Description</h3>

<p>Linear (sum) assignment problem, or LSAP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>assignment(cmat, dir = "min")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="assignment_+3A_cmat">cmat</code></td>
<td>
<p>quadratic (numeric) matrix, the cost matrix.</p>
</td></tr>
<tr><td><code id="assignment_+3A_dir">dir</code></td>
<td>
<p>direction, can be &quot;min&quot; or &quot;max&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Solves the linear (sum) assignment problem for quadratic matrices.
Uses the <code>lp.assign</code> function from the <code>lpSolve</code> package,
that is it solves LSAP as a mixed integer linear programming problem.
</p>


<h3>Value</h3>

<p>List with components <code>perm</code>, the permutation that defines the
minimum solution, <code>min</code>, the minimum value, and <code>err</code> is
always <code>0</code>, i.e. not used at the moment.
</p>


<h3>Note</h3>

<p>Slower than the Hungarian algorithm in package <code>clue</code>.
</p>


<h3>References</h3>

<p>Burkard, R., M. Dell'Amico, and S. Martello (2009). Assignment Problems.
Society for Industrial and Applied Mathematics (SIAM).
</p>
<p>Martello, S., and P. Toth (1990). Knapsack Problems: Algorithms and
Computer Implementations. John Wiley &amp; Sons, Ltd.
</p>


<h3>See Also</h3>

<p><code>clue::solve_LSAP</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##  Example similar to clue::solve_LSAP
set.seed(8237)
x &lt;- matrix(sample(1:100), nrow = 10)
y &lt;- assignment(x)
# show permutation and check minimum sum
y$perm                          #   7  6 10  5  8  2  1  4  9  3
y$min                           # 173
z &lt;- cbind(1:10, y$perm)
x[z]                            #  16  9 49  6 17 14  1 44 10  7
y$min == sum(x[z])              # TRUE

## Not run: 
##  Example: minimize sum of distances of complex points
n &lt;- 100
x &lt;- rt(n, df=3) + 1i * rt(n, df=3)
y &lt;- runif(n) + 1i * runif(n)
cmat &lt;- round(outer(x, y, FUN = function(x,y) Mod(x - y)), 2)
system.time(T1 &lt;- assignment(cmat))     # elapsed: 0.003
T1$min / 100                            # 145.75

## Hungarian algorithm in package 'clue'
library("clue")
system.time(T2 &lt;- solve_LSAP(cmat))     # elapsed: 0.014
sum(cmat[cbind(1:n, T2)])               # 145.75

## End(Not run)
</code></pre>

<hr>
<h2 id='bpp_approx'>
Approximate Bin Packing
</h2><span id='topic+bpp_approx'></span>

<h3>Description</h3>

<p>Solves the Bin Packing problem approximately.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bpp_approx(S, cap, method = c("firstfit", "bestfit", "worstfit"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bpp_approx_+3A_s">S</code></td>
<td>
<p>vector of weights (or sizes) of items.</p>
</td></tr>
<tr><td><code id="bpp_approx_+3A_cap">cap</code></td>
<td>
<p>same capacity for all the bins.</p>
</td></tr>
<tr><td><code id="bpp_approx_+3A_method">method</code></td>
<td>
<p>which approximate method to use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Solves approximately the Bin Packing problem for numeric weights and bins,
all having the same volume.
</p>
<p>Possible methods are &quot;firstfit&quot;, &quot;bestfit&quot;, and &quot;worstfit&quot;. &quot;firstfit&quot; tries
to place each item as early as possible, &quot;bestfit&quot; such that the remaining
space in the bin is as small as possible, and &quot;worstfit&quot; such that the
remaining space is as big as possible.
</p>
<p>Best results are achieved with the &quot;bestfit&quot; method. &quot;firstfit&quot; may be a
reasonable alternative. For smaller and medium-sized data the approximate
results will come quite close to the exact solution, see the examples.
</p>
<p>In general, the results are much better if the items in <code>S</code> are sorted
decreasingly. If they are not, an immediate warning is issued.
</p>


<h3>Value</h3>

<p>A list of the following components:
</p>
<table>
<tr><td><code>nbins</code></td>
<td>
<p>minimum number of bins.</p>
</td></tr>
<tr><td><code>xbins</code></td>
<td>
<p>index of the bin each item is assigned to.</p>
</td></tr>
<tr><td><code>sbins</code></td>
<td>
<p>sum of item sizes in each bin.</p>
</td></tr>
<tr><td><code>filled</code></td>
<td>
<p>total volume filled in the bins (as percentage).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The Bin Packing problem can be solved as a Linear Program. The formulation
is a bit tricky, and it turned out 'lpSolve' does not solve medium-sized
problems in acceptable time. (Tests with 'Rglpk' will follow.)
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>Silvano Martello. &quot;Bin packing problems&quot;. In: 23rd Belgian Mathematical 
Optimization Workshop, La-Roche-en-Ardennes 2019.
</p>


<h3>See Also</h3>

<p>Function <code>binpacking</code> in package 'knapsack' (on R-Forge).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## (1)
S &lt;- c(50, 3, 48, 53, 53, 4, 3, 41, 23, 20, 52, 49)
cap &lt;- 100
bpp_approx(S, cap, method = "bestfit")
## exact    -- $nbins 4, filled 99.75 %
## firstfit -- $nbins 6, filled 66.5  %
## bestfit  -- $nbins 5, filled 79.8  %
## ! when decreasingly sorted, 'bestfit' with nbins = 4

## (2)
S &lt;- c(100,99,89,88,87,75,67,65,65,57,57,49,47,31,27,18,13,9,8,1)
cap &lt;- 100
bpp_approx(S, cap, method = "firstfit")
# firstfit: 12 bins; exact: 12 bins

## Not run: 
## (3)
S &lt;-  c(99,99,96,96,92,92,91,88,87,86,
        85,76,74,72,69,67,67,62,61,56,
        52,51,49,46,44,42,40,40,33,33,
        30,30,29,28,28,27,25,24,23,22,
        21,20,17,14,13,11,10, 7, 7, 3)
cap &lt;- 100
bpp_approx(S, cap)
# exact: 25; firstfit: 25; bestfit: 25 nbins

## (4)
# 20 no.s in 1..100, capacity 100
set.seed(7013)
S &lt;- sample(1:100, 20, replace = TRUE)
cap &lt;- 100
bpp_approx(sort(S, decreasing = TRUE), cap, method = "bestfit")
# exact: 12 bins; firstfit and bestfit: 13; worstfit: 14 bins

## End(Not run)
</code></pre>

<hr>
<h2 id='change_making'>
Change Making Problem
</h2><span id='topic+change_making'></span>

<h3>Description</h3>

<p>Solves the Change Making problem as an integer linear program.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  change_making(items, value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="change_making_+3A_items">items</code></td>
<td>
<p>vector of integer numbers greater than zero.</p>
</td></tr>
<tr><td><code id="change_making_+3A_value">value</code></td>
<td>
<p>integer number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Change Making problem attempts to find a minimal combination of
items that sum up to a given value. If the items are distinct positive
integers, the solution is unique.
</p>
<p>If the problem is infeasible, i.e. there is no such combination, the
returned <code>count</code> is 0.
</p>
<p>The problem is treated as an Integer Linear Program (ILP) and solved
with the <code>lp</code> solver in <code>lpSolve</code>.
</p>


<h3>Value</h3>

<p>Returns a list with components <code>count</code>, the number of items used to
sum up to the value, and <code>solution</code>, the number of items used
per item.
</p>


<h3>References</h3>

<p>See the Wikipedia article on the &quot;change making problem&quot;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+setcover">setcover</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>items = c(2, 5, 10, 50, 100)
value = 999
change_making(items, value)

## Not run: 
solutions &lt;- numeric(20)
for (m in 1:20) {
    sol &lt;- change_making(items, m)
    solutions[m] &lt;- sol$count
}
solutions
#&gt;  [1] 0 1 0 2 1 3 2 4 3 1 4 2 5 3 2 4 3 5 4 2

## End(Not run)
</code></pre>

<hr>
<h2 id='CMAES'>
Covariance Matrix Adaptation Evolution Strategy
</h2><span id='topic+pureCMAES'></span>

<h3>Description</h3>

<p>The CMA-ES (Covariance Matrix Adaptation Evolution Strategy) is an 
evolutionary algorithm for difficult non-linear non-convex optimization 
problems in continuous domain.
The CMA-ES is typically applied to unconstrained or bounded constraint 
optimization problems, and search space dimensions between three and fifty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pureCMAES(par, fun, lower = NULL, upper = NULL, sigma = 0.5,
                    stopfitness = -Inf, stopeval = 1000*length(par)^2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CMAES_+3A_par">par</code></td>
<td>
<p>objective variables initial point.</p>
</td></tr>
<tr><td><code id="CMAES_+3A_fun">fun</code></td>
<td>
<p>objective/target/fitness function.</p>
</td></tr>
<tr><td><code id="CMAES_+3A_lower">lower</code>, <code id="CMAES_+3A_upper">upper</code></td>
<td>
<p>lower and upper bounds for the parameters.</p>
</td></tr>
<tr><td><code id="CMAES_+3A_sigma">sigma</code></td>
<td>
<p>coordinate wise standard deviation (step size).</p>
</td></tr>
<tr><td><code id="CMAES_+3A_stopfitness">stopfitness</code></td>
<td>
<p>stop if fitness &lt; stopfitness (minimization).</p>
</td></tr>
<tr><td><code id="CMAES_+3A_stopeval">stopeval</code></td>
<td>
<p>stop after stopeval number of function evaluations</p>
</td></tr>
<tr><td><code id="CMAES_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The CMA-ES implements a stochastic variable-metric method.
In the very particular case of a convex-quadratic objective function 
the covariance matrix adapts to the inverse of the Hessian matrix, up
to a scalar factor and small random fluctuations. The update equations 
for mean and covariance matrix maximize a likelihood while resembling 
an expectation-maximization algorithm.
</p>


<h3>Value</h3>

<p>Returns a list with components <code>xmin</code> and <code>fmin</code>.
</p>
<p>Be patient; for difficult problems or high dimensions the function may run 
for several minutes; avoid problem dimensions of 30 and more!
</p>


<h3>Note</h3>

<p>There are other implementations of Hansen's CMAES in package &lsquo;cmaes&rsquo; 
(simplified form) and in package &lsquo;parma&rsquo; as cmaes() (extended form).
</p>


<h3>Author(s)</h3>

<p>Copyright (c) 2003-2010 Nikolas Hansen for Matlab code PURECMAES;
converted to R by Hans W Borchers.
(Hansen's homepage: www.cmap.polytechnique.fr/~nikolaus.hansen/)
</p>


<h3>References</h3>

<p>Hansen, N. (2011). The CMA Evolution Strategy: A Tutorial.<br />
<a href="https://arxiv.org/abs/1604.00772">https://arxiv.org/abs/1604.00772</a>
</p>
<p>Hansen, N., D.V. Arnold, and A. Auger (2013). Evolution Strategies. 
J. Kacprzyk and W. Pedrycz (Eds.). Handbook of Computational Intelligence,
Springer-Verlag, 2015.
</p>


<h3>See Also</h3>

<p><code>cmaes::cmaes</code>, <code>parma::cmaes</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
##  Polynomial minimax approximation of data points
##  (see the Remez algorithm)
n &lt;- 10; m &lt;- 101           # polynomial of degree 10; no. of data points
xi &lt;- seq(-1, 1, length = m)
yi &lt;- 1 / (1 + (5*xi)^2)    # Runge's function

pval &lt;- function(p, x)      # Horner scheme
    outer(x, (length(p) - 1):0, "^") %*% p

pfit &lt;- function(x, y, n)   # polynomial fitting of degree n
    qr.solve(outer(x, seq(n, 0), "^"), y)

fn1 &lt;- function(p)           # objective function
    max(abs(pval(p, xi) - yi))

pf &lt;- pfit(xi, yi, 10)      # start with a least-squares fitting
sol1 &lt;- pureCMAES(pf, fn1, rep(-200, 11), rep(200, 11))
zapsmall(sol1$xmin)
# [1]  -50.24826    0.00000  135.85352    0.00000 -134.20107    0.00000
# [7]   59.19315    0.00000  -11.55888    0.00000    0.93453

print(sol1$fmin, digits = 10)
# [1] 0.06546780411

##  Polynomial fitting in the L1 norm
##  (or use LP or IRLS approaches)
fn2 &lt;- function(p)
    sum(abs(pval(p, xi) - yi))

sol2 &lt;- pureCMAES(pf, fn2, rep(-100, 11), rep(100, 11))
zapsmall(sol2$xmin)
# [1] -21.93238   0.00000  62.91083   0.00000 -67.84847   0.00000 
# [7]  34.14398   0.00000  -8.11899   0.00000   0.84533

print(sol2$fmin, digits = 10)
# [1] 3.061810639

## End(Not run)
</code></pre>

<hr>
<h2 id='fminviz+2Cflineviz'>
Visualize Function Minimum
</h2><span id='topic+fminviz'></span><span id='topic+flineviz'></span>

<h3>Description</h3>

<p>Visualizes multivariate functions around a point or along a line between 
two points in <code>R^n</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fminviz(fn, x0, nlines = 2*length(x0),
        npoints = 51, scaled = 1.0)

flineviz(fn, x1, x2, npoints = 51, scaled = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fminviz+2B2Cflineviz_+3A_fn">fn</code></td>
<td>
<p>multivariate function to be visualized.</p>
</td></tr>
<tr><td><code id="fminviz+2B2Cflineviz_+3A_x0">x0</code>, <code id="fminviz+2B2Cflineviz_+3A_x1">x1</code>, <code id="fminviz+2B2Cflineviz_+3A_x2">x2</code></td>
<td>
<p>points in n-dimensional space.</p>
</td></tr>
<tr><td><code id="fminviz+2B2Cflineviz_+3A_nlines">nlines</code></td>
<td>
<p>number of lines to plot.</p>
</td></tr>
<tr><td><code id="fminviz+2B2Cflineviz_+3A_npoints">npoints</code></td>
<td>
<p>number of points used to plot a line.</p>
</td></tr>
<tr><td><code id="fminviz+2B2Cflineviz_+3A_scaled">scaled</code></td>
<td>
<p>scale factor to extend the line(s).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fminviz</code> vizualizes the behavior of a multivariate function <code>fn</code> 
around a point <code>x0</code>. It randomly selects <code>nlines</code> lines through 
<code>x0</code> in <code>R^n</code> and draws the curves of the function along these 
lines in one graph.
</p>
<p>Curves that have at least one point below <code>fn(x0)</code> are drawn in red, 
all others in blue. The scale on the x-axis is the Euclidean distance in 
<code>R^n</code>. The <code>scale</code> factor can change it.
</p>
<p><code>flineviz</code> vizualizes the behavior of a multivariate function <code>fn</code> 
along the straight line between the points <code>x1</code> and <code>x2</code>. Points
<code>x1</code> and <code>x2</code> are also plotted.
</p>


<h3>Value</h3>

<p>Plots a line graph and returns <code>NULL</code> (invisibly).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  f1 &lt;- function(x) x[1]^2 - x[2]^2
  fminviz(f1, c(0, 0), nlines = 10)

  f2 &lt;- function(x) (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2
  flineviz(f2, c(0, 0), c(1, 1))
  
## End(Not run)
</code></pre>

<hr>
<h2 id='hamiltonian'>
Finds a Hamiltonian path or cycle
</h2><span id='topic+hamiltonian'></span>

<h3>Description</h3>

<p>A Hamiltionian path or cycle (a.k.a. Hamiltonian circuit) is a path through
a graph that visits each vertex exactly once, resp. a closed path through
the graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hamiltonian(edges, start = 1, cycle = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hamiltonian_+3A_edges">edges</code></td>
<td>
<p>an edge list describing an undirected graph.</p>
</td></tr>
<tr><td><code id="hamiltonian_+3A_start">start</code></td>
<td>
<p>vertex number to start the path or cycle.</p>
</td></tr>
<tr><td><code id="hamiltonian_+3A_cycle">cycle</code></td>
<td>
<p>Boolean, should a path or a full cycle be found.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>hamiltonian()</code> applies a backtracking algorithm that is relatively
efficient for graphs of up to 30&ndash;40 vertices. The edge list is first
transformed to a list where the <code>i</code>-th component contains the list
of all vertices connected to vertex <code>i</code>.
</p>
<p>The edge list must be of the form <code>c(v1, v2, v3, v2, ...)</code> meaning
that there are edges <code>v1 --&gt; v2, v3 --&gt; v4</code>, etc., connecting these
vertices. Therefore, an edge list has an even number of entries.
</p>
<p>If the function returns <code>NULL</code>, there is no Hamiltonian path or
cycle. The function does not check if the graph is connected or not. And
if <code>cycle = TRUE</code> is used, then there also exists an edge from the
last to the first entry in the resulting path.
</p>
<p>Ifa Hamiltonian cycle exists in the graph it will be found whatever the
starting vertex was. For a Hamiltonian path this is different and a 
successful search may very well depend on the start.
</p>


<h3>Value</h3>

<p>Returns a vector containing vertex number of a valid path or cycle,
or <code>NULL</code> if no path or cycle has been found (i.e., does not exist);
If a cycle was requested, there exists an edge from the last to the first
vertex in this list of edges.
</p>


<h3>Note</h3>

<p>See the <code>igraph</code> package for more information about handling graphs and
defining them through edge lists or other constructs.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>Papadimitriou, Ch. H., and K. Steiglitz (1998). Optimization Problems:
Algorithms and Complexity. Prentice-Hall/Dover Publications.
</p>


<h3>See Also</h3>

<p>Package <code>igraph</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Dodekaeder graph
D20_edges &lt;- c(
     1,  2,  1,  5,  1,  6,  2,  3,  2,  8,  3,  4,  3, 10,  4,  5,  4, 12,
     5, 14,  6,  7,  6, 15,  7,  8,  7, 16,  8,  9,  9, 10,  9, 17, 10, 11,
    11, 12, 11, 18, 12, 13, 13, 14, 13, 19, 14, 15, 15, 20, 16, 17, 16, 20, 
    17, 18, 18, 19, 19, 20)
hamiltonian(D20_edges, cycle = TRUE)
# [1]  1  2  3  4  5 14 13 12 11 10  9  8  7 16 17 18 19 20 15  6
hamiltonian(D20_edges, cycle = FALSE)
# [1]  1  2  3  4  5 14 13 12 11 10  9  8  7  6 15 20 16 17 18 19

## Herschel graph
# The Herschel graph the smallest non-Hamiltonian polyhedral graph.
H11_edges &lt;- c(
     1,  2,  1,  8,  1,  9,  1, 10,  2,  3,  2, 11,  3,  4,  3,  9,  4,  5,
     4, 11,  5,  6,  5,  9,  5, 10,  6,  7,  6, 11,  7,  8,  7, 10,  8, 11)
hamiltonian(H11_edges, cycle = FALSE)
# NULL

## Not run: 
## Example: Graph constructed from squares
N &lt;- 45  # 23, 32, 45
Q &lt;- (2:trunc(sqrt(2*N-1)))^2
sq_edges &lt;- c()
for (i in 1:(N-1)) {
    for (j in (i+1):N) {
        if ((i+j) 
            sq_edges &lt;- c(sq_edges, i, j)
    }
}

require(igraph)
sq_graph &lt;- make_graph(sq_edges, directed=FALSE)
plot(sq_graph)

if (N == 23) {
    # does not find a path with start=1 ...
    hamiltonian(sq_edges, start=18, cycle=FALSE)
    # hamiltonian(sq_edges)                     # NULL
} else if (N == 32) {
    # the first of these graphs that is Hamiltonian ...
    # hamiltonian(sq_edges, cycle=FALSE)
    hamiltonian(sq_edges)
} else if (N == 45) {
    # takes much too long ...
    # hamiltonian(sq_edges, cycle=FALSE)
    hamiltonian(sq_edges)
}
## End(Not run)
</code></pre>

<hr>
<h2 id='Historize'>
Historize function
</h2><span id='topic+Historize'></span>

<h3>Description</h3>

<p>Provides storage for function calls.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Historize(fun, len = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Historize_+3A_fun">fun</code></td>
<td>
<p>Function of one or several variables.</p>
</td></tr>
<tr><td><code id="Historize_+3A_len">len</code></td>
<td>
<p>If &gt; 0, input values will be stored, too.</p>
</td></tr>
<tr><td><code id="Historize_+3A_...">...</code></td>
<td>
<p>Additional parameters to be handed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Historize() adds storage to the function. If <code>len=0</code> then only
function values will be stored in a vector, if <code>len&gt;0</code> then
variables will be stored in a vector, and function values will be stored
in a matrix, one line for every function call.
</p>
<p>If <code>Fn = Historize(...)</code> is the 'historized' function, then the
storage can be retrieved with <code>Fn()</code>, and can be cleared with
<code>Fn(NULL)</code>.
</p>
<p>Filling the storage will take extra time and can slow down the
function calls. Especially also storing the variables used in the
call (with <code>len&gt;0</code>) will make it considerably slower.
</p>
<p>Functions can have multivariate output; the user is asked to take
care of handling the output vector or matrix correctly. The function
may even require additional parameters.
</p>


<h3>Value</h3>

<p>Returns a list, $input the input variables as matrix, $H2 the function
values as vector, $nvars the number of input variables of the function,
and $ncalls the number or recorded function calls.
</p>


<h3>Note</h3>

<p>Can also be applied to functions that output a vector (same length
for every call).
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>See Also</h3>

<p><code>trace</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f &lt;- function(x) sum(x^2)
F &lt;- Historize(f, len = 1)
c( F(c(1,1)), F(c(1,2)), F(c(2,1)), F(c(2,2)) )
#&gt; [1] 2 5 5 8
F()
#&gt; $input
#&gt;      [,1] [,2]
#&gt; [1,]    1    1
#&gt; [2,]    1    2
#&gt; [3,]    2    1
#&gt; [4,]    2    2
#&gt; 
#&gt; $values
#&gt; [1] 2 5 5 8
#&gt; 
#&gt; $nvars
#&gt; [1] 2
#&gt; 
#&gt; $ncalls
#&gt; [1] 4
F(NULL)     # reset memory


## Rastrigin under Differential Evolution

Fn &lt;- Historize(fnRastrigin)
dm &lt;- 10
lb &lt;- rep(-5.2, dm); ub &lt;- -lb

sol &lt;- simpleDE(Fn, lower = lb, upper = ub)
fvalues &lt;- Fn()$values
fvals &lt;- cummin(fvalues)

## Not run: 
plot(fvalues, type = 'p', col = 7, pch = '.', cex = 2,
     main = "Simple DE optimization", xlab = '', ylab = '')
lines(fvals, col = 2, lwd = 2)
legend(length(fvalues), max(fvalues),
       c("Intermediate values", "cummulated min"),
       xjust = 1, col = c(7, 2), lwd = 2)
grid()
## End(Not run)
</code></pre>

<hr>
<h2 id='hookejeeves'>
Hooke-Jeeves Minimization Method
</h2><span id='topic+hookejeeves'></span>

<h3>Description</h3>

<p>An implementation of the Hooke-Jeeves algorithm for derivative-free
optimization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hookejeeves(x0, f, lb = NULL, ub = NULL,
            tol = 1e-08,
            target = Inf, maxfeval = Inf, info = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hookejeeves_+3A_x0">x0</code></td>
<td>
<p>starting vector.</p>
</td></tr>
<tr><td><code id="hookejeeves_+3A_f">f</code></td>
<td>
<p>nonlinear function to be minimized.</p>
</td></tr>
<tr><td><code id="hookejeeves_+3A_lb">lb</code>, <code id="hookejeeves_+3A_ub">ub</code></td>
<td>
<p>lower and upper bounds.</p>
</td></tr>
<tr><td><code id="hookejeeves_+3A_tol">tol</code></td>
<td>
<p>relative tolerance, to be used as stopping rule.</p>
</td></tr>
<tr><td><code id="hookejeeves_+3A_target">target</code></td>
<td>
<p>iteration stops when this value is reached.</p>
</td></tr>
<tr><td><code id="hookejeeves_+3A_maxfeval">maxfeval</code></td>
<td>
<p>maximum number of allowed function evaluations.</p>
</td></tr>
<tr><td><code id="hookejeeves_+3A_info">info</code></td>
<td>
<p>logical, whether to print information during the main loop.</p>
</td></tr>
<tr><td><code id="hookejeeves_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method computes a new point using the values of <code>f</code> at suitable
points along the orthogonal coordinate directions around the last point.
</p>


<h3>Value</h3>

<p>List with following components:
</p>
<table>
<tr><td><code>xmin</code></td>
<td>
<p>minimum solution found so far.</p>
</td></tr>
<tr><td><code>fmin</code></td>
<td>
<p>value of <code>f</code> at minimum.</p>
</td></tr>
<tr><td><code>fcalls</code></td>
<td>
<p>number of function evaluations.</p>
</td></tr>
<tr><td><code>niter</code></td>
<td>
<p>number of iterations performed.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Hooke-Jeeves is notorious for its number of function calls.
Memoization is often suggested as a remedy.
</p>
<p>For a similar implementation of Hooke-Jeeves see the &lsquo;dfoptim&rsquo; package.
</p>


<h3>References</h3>

<p>C.T. Kelley (1999), Iterative Methods for Optimization, SIAM.
</p>
<p>Quarteroni, Sacco, and Saleri (2007), Numerical Mathematics,
Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neldermead">neldermead</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##  Rosenbrock function
rosenbrock &lt;- function(x) {
    n &lt;- length(x)
    x1 &lt;- x[2:n]
    x2 &lt;- x[1:(n-1)]
    sum(100*(x1-x2^2)^2 + (1-x2)^2)
}

hookejeeves(c(0,0,0,0), rosenbrock)
# $xmin
# [1] 1.000000 1.000001 1.000002 1.000004
# $fmin
# [1] 4.774847e-12
# $fcalls
# [1] 2499
# $niter
#[1] 26

hookejeeves(rep(0,4), lb=rep(-1,4), ub=0.5, rosenbrock)
# $xmin
# [1] 0.50000000 0.26221320 0.07797602 0.00608027
# $fmin
# [1] 1.667875
# $fcalls
# [1] 571
# $niter
# [1] 26
</code></pre>

<hr>
<h2 id='knapsack'>
0-1 Knapsack Problem
</h2><span id='topic+knapsack'></span>

<h3>Description</h3>

<p>Solves the 0-1 (binary) single knapsack problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knapsack(w, p, cap)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knapsack_+3A_w">w</code></td>
<td>
<p>integer vector of weights.</p>
</td></tr>
<tr><td><code id="knapsack_+3A_p">p</code></td>
<td>
<p>integer vector of profits.</p>
</td></tr>
<tr><td><code id="knapsack_+3A_cap">cap</code></td>
<td>
<p>maximal capacity of the knapsack, integer too.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>knapsack</code> solves the 0-1, or: binary, single knapsack problem by
using the dynamic programming approach. The problem can be formulated as:
</p>
<p>Maximize <code>sum(x*p)</code> such that <code>sum(x*w) &lt;= cap</code>, where <code>x</code>
is a vector with <code>x[i] == 0 or 1</code>.
</p>
<p>Knapsack procedures can even solve subset sum problems,
see the examples 3 and 3' below.
</p>


<h3>Value</h3>

<p>A list with components <code>capacity</code>, <code>profit</code>, and <code>indices</code>.
</p>


<h3>Author(s)</h3>

<p>HwB  email: &lt;hwborchers@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Papadimitriou, C. H., and K. Steiglitz (1998). Combinatorial Optimization:
Algorithms and Complexity. Dover Publications 1982, 1998.
</p>
<p>Horowitz, E., and S. Sahni (1978). Fundamentals of Computer Algorithms.
Computer Science Press, Rockville, ML.
</p>


<h3>See Also</h3>

<p><code>knapsack::knapsack</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1
p &lt;- c(15, 100, 90, 60, 40, 15, 10,  1)
w &lt;- c( 2,  20, 20, 30, 40, 30, 60, 10)
cap &lt;- 102
(is &lt;- knapsack(w, p, cap))
# [1] 1 2 3 4 6 , capacity 102 and total profit 280

## Example 2
p &lt;- c(70, 20, 39, 37, 7, 5, 10)
w &lt;- c(31, 10, 20, 19, 4, 3,  6)
cap &lt;- 50
(is &lt;- knapsack(w, p, cap))
# [1] 1 4 , capacity 50 and total profit 107

## Not run: 
##  Example 3: subset sum
p &lt;- seq(2, 44, by = 2)^2
w &lt;- p
is &lt;- knapsack(w, p, 2012)
p[is$indices]  # 16  36  64 144 196 256 324 400 576

##  Example 3': maximize number of items
#   w &lt;- seq(2, 44, by = 2)^2
#   p &lt;- numeric(22) + 1
#  is &lt;- knapsack(w, p, 2012)

## Example 4 from Rosetta Code:
w = c(  9, 13, 153,  50, 15, 68, 27, 39, 23, 52, 11,
       32, 24,  48,  73, 42, 43, 22,  7, 18,  4, 30)
p = c(150, 35, 200, 160, 60, 45, 60, 40, 30, 10, 70,
       30, 15,  10,  40, 70, 75, 80, 20, 12, 50, 10)
cap = 400
system.time(is &lt;- knapsack(w, p, cap))  # 0.001 sec

## End(Not run)
</code></pre>

<hr>
<h2 id='maxempty'>
Maximally Empty Rectangle Problem
</h2><span id='topic+maxempty'></span>

<h3>Description</h3>

<p>Find the largest/maximal empty rectangle, i.e. with largest area, not 
containing given points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxempty(x, y, ax = c(0, 1), ay = c(0, 1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxempty_+3A_x">x</code>, <code id="maxempty_+3A_y">y</code></td>
<td>
<p>coordinates of points to be avoided.</p>
</td></tr>
<tr><td><code id="maxempty_+3A_ax">ax</code>, <code id="maxempty_+3A_ay">ay</code></td>
<td>
<p>left and right resp. lower and upper constraints.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Find the largest or maximal empty two-dimensional rectangle in a rectangular
area. The edges of this rectangle have to be parallel to the edges of the
enclosing rectangle (and parallel to the coordinate axes). &lsquo;Empty&rsquo; means
that none of the points given are contained in the interior of the found
rectangle.
</p>


<h3>Value</h3>

<p>List with <code>area</code> and <code>rect</code> the rectangle as a vector usable for
the <code>rect</code> graphics function.
</p>


<h3>Note</h3>

<p>The algorithm has a run-time of <code>O(n^2)</code> while there are run-times of
<code>O(n*log(n))</code> reported in the literature, utilizing a more complex
data structure. I don't know of any comparable algorithms for the largest 
empty circle problem.
</p>


<h3>Author(s)</h3>

<p>HwB  email: &lt;hwborchers@googlemail.com&gt;
</p>


<h3>References</h3>

<p>B. Chazelle, R. L. Drysdale, and D. T. Lee (1986). Computing the Largest 
Empty Rectangle. SIAM Journal of Computing, Vol. 15(1), pp. 300&ndash;315.
</p>
<p>A. Naamad, D. T. Lee, and W.-L. Hsu (1984). On the Maximum Empty Rectangle 
Problem. Discrete Applied Mathematics, Vol. 8, pp. 267&ndash;277.
</p>


<h3>See Also</h3>

<p><code>Hmisc::largest.empty</code> with a Fortran implementation of this code.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 100; set.seed(8237)
x &lt;- runif(N); y &lt;- runif(N)
R &lt;- maxempty(x, y, c(0,1), c(0,1))
R
# $area
# [1] 0.08238793
# $rect
# [1] 0.7023670 0.1797339 0.8175771 0.8948442

## Not run: 
plot(x, y, pch="+", xlim=c(0,1), ylim=c(0,1), col="darkgray",
      main = "Maximally empty rectangle")
rect(0, 0, 1, 1, border = "red", lwd = 1, lty = "dashed")
do.call(rect, as.list(R$rect))
grid()
## End(Not run)
</code></pre>

<hr>
<h2 id='maxquad'>
The MAXQUAD Test Function
</h2><span id='topic+maxquad'></span>

<h3>Description</h3>

<p>Lemarechal's MAXQUAD optimization test function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxquad(n, m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxquad_+3A_n">n</code></td>
<td>
<p>number of variables of the generated test function.</p>
</td></tr>
<tr><td><code id="maxquad_+3A_m">m</code></td>
<td>
<p>number of functions to compete for the maximum.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>MAXQUAD actually is a family of minimax functions, parametrized by the
number <code>n</code> of variables and the number <code>m</code> of functions whose
maximum it is.
</p>


<h3>Value</h3>

<p>Returns a list with components <code>fn</code> the generated test function of
<code>n</code> variables, and <code>gr</code> the corresponding (analytical) gradient 
function.
</p>


<h3>References</h3>

<p>Kuntsevich, A., and F. Kappel (1997). SolvOpt &ndash; The Solver for Local 
Nonlinear Optimization Problems.
Manual Version 1.1, Institute of Mathematics, University of Graz.
</p>
<p>Lemarechal, C., and R. Mifflin, Eds. (1978). Nonsmooth Optimization.
Pergamon Press, Oxford.
</p>
<p>Shor, N. Z. (1985). Minimization Methods for Non-differentiable Functions.
Series in Computational Mathematics, Springer-Verlag, Berlin.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test function of 5 variables, defined as maximum of 5 smooth functions
maxq &lt;- maxquad(5, 5)
fnMaxquad &lt;- maxq$fn
grMaxquad &lt;- maxq$gr
# shor
</code></pre>

<hr>
<h2 id='maxsub'>
Maximal Sum Subarray
</h2><span id='topic+maxsub'></span><span id='topic+maxsub2d'></span>

<h3>Description</h3>

<p>Find a subarray with maximal positive sum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxsub(x, inds = TRUE)

maxsub2d(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxsub_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="maxsub_+3A_a">A</code></td>
<td>
<p>numeric matrix</p>
</td></tr>
<tr><td><code id="maxsub_+3A_inds">inds</code></td>
<td>
<p>logical; shall the indices be returned?</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>maxsub</code> finds a contiguous subarray whose sum is maximally positive.
This is sometimes called Kadane's algorithm.
<code>maxsub</code> will use a very fast version with a running time of
<code>O(n)</code> where <code>n</code> is the length of the input vector <code>x</code>.
</p>
<p><code>maxsub2d</code> finds a (contiguous) submatrix whose sum of elements is
maximally positive. The approach taken here is to apply the one-dimensional
routine to summed arrays between all rows of <code>A</code>. This has a run-time
of <code>O(n^3)</code>, though a run-time of <code>O(n^2 log n)</code> seems possible
see the reference below.
<code>maxsub2d</code> can solve a 100-by-100 matrix in a few seconds &ndash;
but beware of bigger ones.
</p>


<h3>Value</h3>

<p>Either just a maximal sum, or a list this sum as component <code>sum</code> plus
the start and end indices as a vector <code>inds</code>.
</p>


<h3>Note</h3>

<p>In special cases, the matrix <code>A</code> may be sparse or (as in the example
section) only have one nonzero element in each row and column. Expectation 
is that there may exists a more efficient (say <code>O(n^2)</code>) algorithm in 
these special cases.
</p>


<h3>Author(s)</h3>

<p>HwB  &lt;hwborchers@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Bentley, Jon (1986). &ldquo;Programming Pearls&rdquo;, Column 7.
Addison-Wesley Publ. Co., Reading, MA.
</p>
<p>T. Takaoka (2002). Efficient Algorithms for the Maximum Subarray Problem by
Distance Matrix Multiplication. The Australasian Theory Symposion, CATS 2002.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##  Find a maximal sum subvector
set.seed(8237)
x &lt;- rnorm(1e6)
system.time(res &lt;- maxsub(x, inds = TRUE))
res

##  Standard example: Find a maximal sum submatrix
A &lt;- matrix(c(0,-2,-7,0, 9,2,-6,2, -4,1,-4,1, -1,8,0,2),
            nrow = 4, ncol = 4, byrow =TRUE)
maxsub2d(A)
# $sum:  15
# $inds: 2 4 1 2 , i.e., rows = 2..4, columns = 1..2

## Not run: 
##  Application to points in the unit square:
set.seed(723)
N &lt;- 50; w &lt;- rnorm(N)
x &lt;- runif(N); y &lt;- runif(N)
clr &lt;- ifelse (w &gt;= 0, "blue", "red")
plot(x, y, pch = 20, col = clr, xlim = c(0, 1), ylim = c(0, 1))

xs &lt;- unique(sort(x)); ns &lt;- length(xs)
X  &lt;- c(0, ((xs[1:(ns-1)] + xs[2:ns])/2), 1)
ys &lt;- unique(sort(y)); ms &lt;- length(ys)
Y  &lt;- c(0, ((ys[1:(ns-1)] + ys[2:ns])/2), 1)
abline(v = X, col = "gray")
abline(h = Y, col = "gray")

A &lt;- matrix(0, N, N)
xi &lt;- findInterval(x, X); yi &lt;- findInterval(y, Y)
for (i in 1:N) A[yi[i], xi[i]] &lt;- w[i]

msr &lt;- maxsub2d(A)
rect(X[msr$inds[3]], Y[msr$inds[1]], X[msr$inds[4]+1], Y[msr$inds[2]+1])

## End(Not run)
</code></pre>

<hr>
<h2 id='mknapsack'>
Multiple 0-1 Knapsack Problem
</h2><span id='topic+mknapsack'></span>

<h3>Description</h3>

<p>Solves the 0-1 (binary) multiple knapsack problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mknapsack(w, p, cap)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mknapsack_+3A_w">w</code></td>
<td>
<p>vector of (positive) weights.</p>
</td></tr>
<tr><td><code id="mknapsack_+3A_p">p</code></td>
<td>
<p>vector of (positive) profits.</p>
</td></tr>
<tr><td><code id="mknapsack_+3A_cap">cap</code></td>
<td>
<p>vector of capacities of different knapsacks.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Solves the 0-1 multiple knapsack problem for a set of profits and weights.<br />
A multiple 0-1 knapsack problem can be formulated as:
</p>
<p><code>
  maximize  vstar = p(1)*(x(1,1) + ... + x(m,1)) + ...
                        ... + p(n)*(x(1,n) + ... + x(m,n))
  subject to
      w(1)*x(i,1) + ... + w(n)*x(i,n) &lt;= cap(i)   for  i=1,...,m
      x(1,j) + ... + x(m,j) &lt;= 1   for  j=1,...,n
      x(i,j) = 0 or 1   for  i=1,...,m ,  j=1,...,n ,
  </code>
</p>
<p>The multiple knapsack problem is reformulated as a linear program and
solved with the help of package <code>lpSolve</code>.
</p>
<p>This function can be used for the single knapsack problem as well,
but the 'dynamic programming' version in the <code>knapsack</code> function
is faster (but: allows only integer values).
</p>
<p>The solution found is most often not unique and may not be the most compact
one. In the future, we will attempt to 'compactify' through backtracking.
The number of backtracks will be returned in list element <code>bs</code>.
</p>


<h3>Value</h3>

<p>A list with components, <code>ksack</code> the knapsack numbers the items are
assigned to, <code>value</code> the total value/profit of the solution found, and 
<code>bs</code> the number of backtracks used.
</p>


<h3>Note</h3>

<p>Contrary to earlier versions, the sequence of profits and weights has been
interchanged: first the weights, then profits.
</p>
<p>The compiled version was transferred to the <code>knapsack</code> package on
R-Forge (see project 'optimist').
</p>


<h3>References</h3>

<p>Kellerer, H., U. Pferschy, and D. Pisinger (2004). Knapsack Problems.
Springer-Verlag, Berlin Heidelberg.
</p>
<p>Martello, S., and P. Toth (1990). Knapsack Problems: Algorithms and
Computer Implementations. John Wiley &amp; Sons, Ltd.
</p>


<h3>See Also</h3>

<p>Other packages implementing knapsack routines.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example 1: single knapsack
w &lt;- c( 2,  20, 20, 30, 40, 30, 60, 10)
p &lt;- c(15, 100, 90, 60, 40, 15, 10,  1)
cap &lt;- 102
(is &lt;- mknapsack(w, p, cap))
which(is$ksack == 1)
# [1] 1 2 3 4 6 , capacity 102 and total profit 280

## Example 2: multiple knapsack
w &lt;- c( 40,  60, 30, 40, 20, 5)
p &lt;- c(110, 150, 70, 80, 30, 5)
cap &lt;- c(85, 65)
is &lt;- mknapsack(w, p, cap)
# kps 1: 1,4;  kps 2: 2,6;  value: 345

## Example 3: multiple knapsack
p &lt;- c(78, 35, 89, 36, 94, 75, 74, 79, 80, 16)
w &lt;- c(18,  9, 23, 20, 59, 61, 70, 75, 76, 30)
cap &lt;- c(103, 156)
is &lt;- mknapsack(w, p, cap)
# kps 1: 3,4,5;  kps 2: 1,6,9; value: 452 

## Not run: 
# How to Cut Your Planks with R
# R-bloggers, Rasmus Baath, 2016-06-12
#
# This is application of multiple knapsacks to cutting planks into pieces.

planks_we_have &lt;- c(120, 137, 220, 420, 480)
planks_we_want &lt;- c(19, 19, 19, 19, 79, 79, 79, 103, 103,
                    103, 135, 135, 135, 135, 160)
s &lt;- mknapsack(planks_we_want, planks_we_want + 1, planks_we_have)
s$ksack
##  [1] 5 5 5 5 3 5 5 4 1 5 4 5 3 2 4

# Solution w/o backtracking
# bin 1 :  103                          | Rest:  17
# bin 2 :  135                          | Rest:   2
# bin 3 :   79 +  135                   | Rest:   6
# bin 4 :  103 +  135 + 160             | Rest:  22
# bin 5 : 4*19 + 2*79 + 103 + 135       | Rest:   8
#
# Solution with reversing the bins (bigger ones first)
# bin 1 :  103                          | Rest:   4
# bin 2 :  2*19 +    79                 | Rest:  20
# bin 3 :   79  +   135                 | Rest:   6
# bin 4 : 2*19  +    79 + 135 + 160     | Rest:   8
# bin 5 : 2*103 + 2*135                 | Rest:  17
#
# Solution with backtracking (compactification)
# sol = c(1, 4, 4, 1, 1, 3, 4, 5, 5, 5, 5, 4, 2, 3, 4)
# bin 1 : 2*19 +   79                   | Rest:   3
# bin 2 :  135                          | Rest:   2
# bin 3 :   79 +  135                   | Rest:   6
# bin 4 : 2*19 +   79 + 135 + 160       | Rest:   8
# bin 5 : 3*103 + 135                   | Rest:  36

## End(Not run)
</code></pre>

<hr>
<h2 id='neldermead'>
Nelder-Mead Minimization Method
</h2><span id='topic+neldermead'></span><span id='topic+neldermeadb'></span>

<h3>Description</h3>

<p>An implementation of the Nelder-Mead algorithm for derivative-free
optimization / function minimization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neldermead( fn, x0, ..., adapt = TRUE,
            tol = 1e-10, maxfeval = 10000, 
			step = rep(1.0, length(x0)))

neldermeadb(fn, x0, ..., lower, upper, adapt = TRUE,
            tol = 1e-10, maxfeval = 10000,
            step = rep(1, length(x0)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neldermead_+3A_fn">fn</code></td>
<td>
<p>nonlinear function to be minimized.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_x0">x0</code></td>
<td>
<p>starting point for the iteration.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_adapt">adapt</code></td>
<td>
<p>logical; adapt to parameter dimension.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_tol">tol</code></td>
<td>
<p>terminating limit for the variance of function values;
can be made *very* small, like <code>tol=1e-50</code>.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_maxfeval">maxfeval</code></td>
<td>
<p>maximum number of function evaluations.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_step">step</code></td>
<td>
<p>size and shape of initial simplex; relative magnitudes of
its elements should reflect the units of the variables.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to the function.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_lower">lower</code>, <code id="neldermead_+3A_upper">upper</code></td>
<td>
<p>lower and upper bounds.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Also called a &lsquo;simplex&rsquo; method for finding the local minimum of a function
of several variables. The method is a pattern search that compares function
values at the vertices of the simplex. The process generates a sequence of
simplices with ever reducing sizes.
</p>
<p>The simplex function minimisation procedure due to Nelder and Mead (1965),
as implemented by O'Neill (1971), with subsequent comments by Chambers and 
Ertel 1974, Benyon 1976, and Hill 1978. For another elaborate implementation 
of Nelder-Mead in R based on Matlab code by Kelley see package &lsquo;dfoptim&rsquo;.
</p>
<p><code>eldermead</code> can be used up to 20 dimensions (then &lsquo;tol&rsquo; and &lsquo;maxfeval&rsquo;
need to be increased). With <code>adapt=TRUE</code> it applies adaptive
coefficients for the simplicial search, depending on the problem dimension
&ndash; see Fuchang and Lixing (2012). This approach especially reduces the
number of function calls.
</p>
<p>With upper and/or lower bounds, <code>neldermeadb</code> applies <code>transfinite</code>
to define the function on all of R^n and to retransform the solution to the 
bounded domain. Of course, if the optimum is near to the boundary, results 
will not be as accurate as when the minimum is in the interior.
</p>


<h3>Value</h3>

<p>List with following components:
</p>
<table>
<tr><td><code>xmin</code></td>
<td>
<p>minimum solution found.</p>
</td></tr>
<tr><td><code>fmin</code></td>
<td>
<p>value of <code>f</code> at minimum.</p>
</td></tr>
<tr><td><code>fcount</code></td>
<td>
<p>number of iterations performed.</p>
</td></tr>
<tr><td><code>restarts</code></td>
<td>
<p>number of restarts.</p>
</td></tr>
<tr><td><code>errmess</code></td>
<td>
<p>error message</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Original FORTRAN77 version by R O'Neill; MATLAB version by John Burkardt
under LGPL license. Re-implemented in R by Hans W. Borchers.
</p>


<h3>References</h3>

<p>Nelder, J., and R. Mead (1965). A simplex method for function minimization.
Computer Journal, Volume 7, pp. 308-313.
</p>
<p>O'Neill, R. (1971). Algorithm AS 47: Function Minimization Using a Simplex 
Procedure. Applied Statistics, Volume 20(3), pp. 338-345.
</p>
<p>J. C. Lagarias et al. (1998). Convergence properties of the Nelder-Mead
simplex method in low dimensions. SIAM Journal for Optimization, Vol. 9,
No. 1, pp 112-147.
</p>
<p>Fuchang Gao and Lixing Han (2012). Implementing the Nelder-Mead simplex
algorithm with adaptive parameters. Computational Optimization and
Applications, Vol. 51, No. 1, pp. 259-277.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hookejeeves">hookejeeves</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##  Classical tests as in the article by Nelder and Mead
# Rosenbrock's parabolic valley
rpv &lt;- function(x) 100*(x[2] - x[1]^2)^2 + (1 - x[1])^2
x0 &lt;- c(-2, 1)
neldermead(rpv, x0)                     #  1 1

# Fletcher and Powell's helic valley
fphv &lt;- function(x)
    100*(x[3] - 10*atan2(x[2], x[1])/(2*pi))^2 + 
        (sqrt(x[1]^2 + x[2]^2) - 1)^2 +x[3]^2
x0 &lt;- c(-1, 0, 0)
neldermead(fphv, x0)                    #  1 0 0

# Powell's Singular Function (PSF)
psf &lt;- function(x)  (x[1] + 10*x[2])^2 + 5*(x[3] - x[4])^2 + 
                    (x[2] - 2*x[3])^4 + 10*(x[1] - x[4])^4
x0 &lt;- c(3, -1, 0, 1)
neldermead(psf, x0)         #  0 0 0 0, needs maximum number of function calls

# Bounded version of Nelder-Mead
lower &lt;- c(-Inf, 0,   0)
upper &lt;- c( Inf, 0.5, 1)
x0 &lt;- c(0, 0.1, 0.1)
neldermeadb(fnRosenbrock, c(0, 0.1, 0.1), lower = lower, upper = upper)
# $xmin = c(0.7085595, 0.5000000, 0.2500000)
# $fmin = 0.3353605

## Not run: 
# Can run Rosenbrock's function in 30 dimensions in one and a half minutes:
neldermead(fnRosenbrock, rep(0, 30), tol=1e-20, maxfeval=10^7)
# $xmin
#  [1]  0.9999998 1.0000004 1.0000000 1.0000001 1.0000000 1.0000001
#  [7]  1.0000002 1.0000001 0.9999997 0.9999999 0.9999997 1.0000000
# [13]  0.9999999 0.9999994 0.9999998 0.9999999 0.9999999 0.9999999
# [19]  0.9999999 1.0000001 0.9999998 1.0000000 1.0000003 0.9999999
# [25]  1.0000000 0.9999996 0.9999995 0.9999990 0.9999973 0.9999947
# $fmin
# [1] 5.617352e-10
# $fcount
# [1] 1426085
# elapsed time is 96.008000 seconds 
## End(Not run)
</code></pre>

<hr>
<h2 id='occurs'>
Finding Subsequences
</h2><span id='topic+occurs'></span><span id='topic+count'></span>

<h3>Description</h3>

<p>Counts items, or finds subsequences of (integer) sequences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  count(x, sorted = TRUE)

  occurs(subseq, series)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="occurs_+3A_x">x</code></td>
<td>
<p>array of items, i.e. numbers or characters.</p>
</td></tr>
<tr><td><code id="occurs_+3A_sorted">sorted</code></td>
<td>
<p>logical; default is to sort items beforehand.</p>
</td></tr>
<tr><td><code id="occurs_+3A_subseq">subseq</code></td>
<td>
<p>vector of integers.</p>
</td></tr>
<tr><td><code id="occurs_+3A_series">series</code></td>
<td>
<p>vector of integers.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>count</code> counts the items, similar to <code>table</code>, but as fast and
a more tractable output. If <code>sorted</code> then the total number per item
will be counted, else per repetition.
</p>
<p>If <code>m</code> and <code>n</code> are the lengths of <code>s</code> and <code>S</code> resp.,
then <code>occurs(s, S)</code> determines all positions <code>i</code> such that
<code>s == S[i, ..., i+m-1]</code>.
</p>
<p>The code is vectorized and relatively fast. It is intended to complement
this with an implementation of Rabin-Karp, and possibly Knuth-Morris-Pratt
and Boyer-Moore algorithms.
</p>


<h3>Value</h3>

<p><code>count</code> returns a list with components <code>v</code> the items and
<code>e</code> the number of times it apears in the array.
<code>occurs</code> returns a vector of indices, the positions where the
subsequence appears in the series.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##  Examples
patrn &lt;- c(1,2,3,4)
exmpl &lt;- c(3,3,4,2,3,1,2,3,4,8,8,23,1,2,3,4,4,34,4,3,2,1,1,2,3,4)
occurs(patrn, exmpl)
## [1]  6 13 23

## Not run: 
set.seed(2437)
p &lt;- sample(1:20, 1000000, replace=TRUE)
system.time(i &lt;- occurs(c(1,2,3,4,5), p))  #=&gt;  [1] 799536
##  user  system elapsed 
## 0.017   0.000   0.017 [sec]

system.time(c &lt;- count(p))
##  user  system elapsed 
## 0.075   0.000   0.076 
print(c)
## $v
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
## $e
##  [1] 49904 50216 49913 50154 49967 50045 49747 49883 49851 49893
## [11] 50193 50024 49946 49828 50319 50279 50019 49990 49839 49990

## End(Not run)
</code></pre>

<hr>
<h2 id='setcover'>
Set cover problem
</h2><span id='topic+setcover'></span>

<h3>Description</h3>

<p>Solves the Set Cover problem as an integer linear program.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  setcover(Sets, weights)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setcover_+3A_sets">Sets</code></td>
<td>
<p>matrix of 0s and 1s, each line defining a subset.</p>
</td></tr>
<tr><td><code id="setcover_+3A_weights">weights</code></td>
<td>
<p>numerical weights for each subset.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Set Cover problems attempts to find in subsets (of a 'universe')
a minimal set of subsets that still covers the whole set.
</p>
<p>Each line of the matrix <code>Sets</code> defines a characteristic function of
a subset. It is required that each element of the universe is contained
in at least one of these subsets.
</p>
<p>The problem is treated as an Integer Linear Program (ILP) and solved
with the <code>lp</code> solver in <code>lpSolve</code>.
</p>


<h3>Value</h3>

<p>Returns a list with components <code>sets</code>, giving the indices of subsets,
and <code>objective</code>, the sum of weights of subsets present in the solution.
</p>


<h3>References</h3>

<p>See the Wikipedia article on the &quot;set cover problem&quot;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+knapsack">knapsack</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Define 12 subsets of universe {1, ..., 10}.
set.seed(7*11*13)
A &lt;- matrix(sample(c(0,1), prob = c(0.8,0.2), size = 120, replace =TRUE),
            nrow = 12, ncol = 10)
sol &lt;- setcover(Sets = A, weights = rep(1, 12))
sol
## $sets
## [1]  1  2  9 12
## $no.sets
##[1] 4

# all universe elements are covered:
colSums(A[sol$sets, ])
## [1] 1 1 2 1 1 1 2 1 1 2
</code></pre>

<hr>
<h2 id='SIAM+20test+20functions'>
Trefethen and Wagon Test Functions
</h2><span id='topic+fnTrefethen'></span><span id='topic+fnWagon'></span>

<h3>Description</h3>

<p>Test functions for global optimization posed for the SIAM 100-digit
challenge in 2002 by Nick Trefethen, Oxford University, UK.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fnTrefethen(p2)
fnWagon(p3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SIAM+2B20test+2B20functions_+3A_p2">p2</code></td>
<td>
<p>Numerical vector of length 2.</p>
</td></tr>
<tr><td><code id="SIAM+2B20test+2B20functions_+3A_p3">p3</code></td>
<td>
<p>Numerical vector of length 3.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are highly nonlinear and oscillating functions in two and three
dimensions with thousands of local mimima inside the unit square resp.
cube (i.e., [-1, 1] x [-1, 1] or [-1, 1] x [-1, 1] x [-1, 1]).
</p>


<h3>Value</h3>

<p>Function value is a single real number.
</p>


<h3>Author(s)</h3>

<p>HwB  &lt;hwborchers@googlemail.com&gt;
</p>


<h3>References</h3>

<p>F. Bornemann, D. Laurie, S. Wagon, and J. Waldvogel (2004). The
SIAM 100-Digit Challenge: A Study in High-Accuracy Numerical Computing.
Society for Industrial and Applied Mathematics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  x &lt;- 2*runif(5) - 1
  fnTrefethen(x)
  fnWagon(x)

  ## Not run: 
  T &lt;- matrix(NA, nrow=1001, ncol=1001)
  for (i in 1:1001) {
    for (j in 1:1001) {
        T[i, j] &lt;- fnTrefethen(c(x[i], y[j]))
    }
  }
  image(x, y, T)
  contour(x, y, T, add=TRUE)
  
## End(Not run)
</code></pre>

<hr>
<h2 id='simpleDE'>
Simple Differential Evolution Algorithm
</h2><span id='topic+simpleDE'></span>

<h3>Description</h3>

<p>Simple Differential Evolution for Minimization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simpleDE(fun, lower, upper, N = 64, nmax = 256, r = 0.4, 
            confined = TRUE, log = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simpleDE_+3A_fun">fun</code></td>
<td>
<p>the objective function to be minimized.</p>
</td></tr>
<tr><td><code id="simpleDE_+3A_lower">lower</code></td>
<td>
<p>vector of lower bounds for all coordinates.</p>
</td></tr>
<tr><td><code id="simpleDE_+3A_upper">upper</code></td>
<td>
<p>vector of upper bounds for all coordinates.</p>
</td></tr>
<tr><td><code id="simpleDE_+3A_n">N</code></td>
<td>
<p>population size.</p>
</td></tr>
<tr><td><code id="simpleDE_+3A_nmax">nmax</code></td>
<td>
<p>bound on the number of generations.</p>
</td></tr>
<tr><td><code id="simpleDE_+3A_r">r</code></td>
<td>
<p>amplification factor.</p>
</td></tr>
<tr><td><code id="simpleDE_+3A_confined">confined</code></td>
<td>
<p>logical; stay confined within bounds.</p>
</td></tr>
<tr><td><code id="simpleDE_+3A_log">log</code></td>
<td>
<p>logical; shall a trace be printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Evolutionary search to minimize a function: For points in the current
generation, children are formed by taking a linear combination of parents,
i.e., each member of the next generation has the form
</p>
<p style="text-align: center;"><code class="reqn">p_1 + r(p_2 - p_3)</code>
</p>

<p>where the <code class="reqn">p_i</code> are members of the current generation and <code class="reqn">r</code> is an
amplification factor.
</p>


<h3>Value</h3>

<p>List with the following components:
</p>
<table>
<tr><td><code>fmin</code></td>
<td>
<p>function value at the minimum found.</p>
</td></tr>
<tr><td><code>xmin</code></td>
<td>
<p>numeric vector representing the minimum.</p>
</td></tr>
<tr><td><code>nfeval</code></td>
<td>
<p>number of function calls.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Original Mathematica version by Dirk Laurie in the SIAM textbook.
Translated to R by Hans W Borchers.
</p>


<h3>Author(s)</h3>

<p>HwB &lt;hwborchers@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Dirk Laurie. &ldquo;A Complex Optimization&quot;. Chapter 5 In: F. Bornemann,
D. Laurie, S. Wagon, and J. Waldvogel (Eds.). The SIAM 100-Digit Challenge.
Society of Industrial and Applied Mathematics, 2004.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+simpleEA">simpleEA</a></code>, <code>DEoptim</code> in the &lsquo;DEoptim&rsquo; package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  simpleDE(fnTrefethen, lower = c(-1,-1), upper = c(1,1))
  # $fmin
  # [1] -3.306869
  # $xmin
  # [1] -0.02440308  0.21061243  # this is the true global optimum!
</code></pre>

<hr>
<h2 id='simpleEA'>
Simple Evolutionary Algorithm
</h2><span id='topic+simpleEA'></span>

<h3>Description</h3>

<p>Simple Evolutionary Algorithm for Minimization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simpleEA(fn, lower, upper, N = 100, ..., con = 0.1, new = 0.05,
         tol = 1e-10, eps = 1e-07, scl = 1/2, confined = FALSE, log = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simpleEA_+3A_fn">fn</code></td>
<td>
<p>the objective function to be minimized.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_lower">lower</code></td>
<td>
<p>vector of lower bounds for all coordinates.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_upper">upper</code></td>
<td>
<p>vector of upper bounds for all coordinates.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_n">N</code></td>
<td>
<p>number of children per parent.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed to the function.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_con">con</code></td>
<td>
<p>percentage of individuals concentrating to the best parents.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_new">new</code></td>
<td>
<p>percentage of new individuals not focussed on existing parents.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_tol">tol</code></td>
<td>
<p>tolerance; if in the last three loops no better individuals
were found up to this tolerance, stop.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_eps">eps</code></td>
<td>
<p>grid size bound to be reached.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_scl">scl</code></td>
<td>
<p>scaling factor for shrinking the grid.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_confined">confined</code></td>
<td>
<p>logical; shall the set of individuals be strictly
respect the boundary? Default: FALSE.</p>
</td></tr>
<tr><td><code id="simpleEA_+3A_log">log</code></td>
<td>
<p>logical, should best solution found be printed per step.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Evolutionary search to minimize a function: For each point in the current
generation, <em>n</em> random points are introduced and the <em>n</em> best
results of each generation (and its parents) are used to form the next
generation.
</p>
<p>The scale shrinks the generation of new points as the algorithm proceeds.
It is possible for some children to lie outside the given rectangle, and
therefore the final result may lie outside the unit rectangle well.
(TO DO: Make this an option.)
</p>


<h3>Value</h3>

<p>List with the following components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>numeric vector representing the minimum found.</p>
</td></tr>
<tr><td><code>val</code></td>
<td>
<p>function value at the minimum found.</p>
</td></tr>
<tr><td><code>fun.calls</code></td>
<td>
<p>number of function calls made.</p>
</td></tr>
<tr><td><code>rel.scl</code></td>
<td>
<p>last scaling factor indicating grid size in last step.</p>
</td></tr>
<tr><td><code>rel.tol</code></td>
<td>
<p>relative tolerance within the last three minima found.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Original Mathematica Version by Stan Wagon in the SIAM textbook.
Translated to R by Hans W Borchers.
</p>


<h3>Author(s)</h3>

<p>HwB &lt;hwborchers@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Stan Wagon. &ldquo;Think Globally, Act Locally&quot;. Chapter 4 In: F. Bornemann,
D. Laurie, S. Wagon, and J. Waldvogel (Eds.). The SIAM 100-Digit Challenge.
Society of Industrial and Applied Mathematics, 2004.
</p>


<h3>See Also</h3>

<p><code>DEoptim</code> in the &lsquo;DEoptim&rsquo; package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  simpleEA(fnTrefethen, lower=c(-1,-1), upper=c(1,1), log=FALSE)
  # $par
  # [1] -0.02440310  0.21061243  # this is the true global optimum!
  # $val
  # [1] -3.306869
</code></pre>

<hr>
<h2 id='subsetsum'>
Subset Sum Problem
</h2><span id='topic+subsetsum'></span><span id='topic+sss_test'></span>

<h3>Description</h3>

<p>Subset sum routine for positive integers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subsetsum(S, t, method = "greedy")

sss_test(S, t)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subsetsum_+3A_s">S</code></td>
<td>
<p>vector of positive integers.</p>
</td></tr>
<tr><td><code id="subsetsum_+3A_t">t</code></td>
<td>
<p>target value, bigger than all items in <code>S</code>.</p>
</td></tr>
<tr><td><code id="subsetsum_+3A_method">method</code></td>
<td>
<p>can be &ldquo;greedy&rdquo; or &ldquo;dynamic&rdquo;, where &ldquo;dynamic&rdquo; stands
for the dynamic programming approach.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>subsetsum</code> is searching for a set of elements in <code>S</code> that
sum up to <code>t</code> by continuously adding more elements of <code>S</code>.
</p>
<p>It is not required that <code>S</code> is decreasingly sorted. But for reasons
of efficiency and smaller execution times it is urgently recommended to
sort the item set in decreasing order. See the examples to find out how
to handle your data.
</p>
<p>The first components will be preferred, i.e., if <code>S</code> is decreasing,
the sum with larger elements will be found, if increasing, the sum with
smaller elements. Because of timing considerations, the default is to
sort decreasingly before processing.
</p>
<p>The dynamic method may be faster for large sets, but will also require
much more memory if the target value is large. 
</p>
<p><code>sss_test</code> will find the biggest number below or equal to <code>t</code>
that can be expressed as a sum of items in <code>S</code>. It will not return
any indices. It can be quite fast, though it preprocesses the set <code>S</code>
to be sorted decreasingly, too. 
</p>


<h3>Value</h3>

<p>List with the target value, if reached, and vector of indices of elements
in <code>S</code> that sum up to <code>t</code>.
</p>
<p>If no solution is found, the dynamic method will return indices for the
largest value below the target, the greedy method witll return NULL.
</p>
<p><code>sss_test</code> will simply return maximum sum value found.
</p>


<h3>Note</h3>

<p>A compiled version &ndash; and much faster, in Fortran &ndash; can be found in
package 'knapsack' (R-Forge, project 'optimist') as <code>subsetsum</code>.
A recursive version, returning *all* solutions, is much too slow in R,
but is possible in Julia and can be asked from the author.
</p>


<h3>Author(s)</h3>

<p>HwB  email: &lt;hwborchers@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Horowitz, E., and S. Sahni (1978). Fundamentals of Computer Algorithms.
Computer Science Press, Rockville, ML.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxsub">maxsub</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>t &lt;- 5842
S &lt;- c(267, 493, 869, 961, 1000, 1153, 1246, 1598, 1766, 1922)

# S is not decreasingly sorted, so ...
o  &lt;- order(S, decreasing = TRUE)
So &lt;- S[o]                          # So is decreasingly sorted

sol &lt;- subsetsum(So, t)             # $inds:  2 4 6 7 8  w.r.t.  So
is  &lt;- o[sol$inds]                  # is:     9 7 5 4 3  w.r.t.  S
sum(S[is])                          # 5842

## Not run: 
amount &lt;- 4748652
products &lt;- 
c(30500,30500,30500,30500,42000,42000,42000,42000,
  42000,42000,42000,42000,42000,42000,71040,90900,
  76950,35100,71190,53730,456000,70740,70740,533600,
  83800,59500,27465,28000,28000,28000,28000,28000,
  26140,49600,77000,123289,27000,27000,27000,27000,
  27000,27000,80000,33000,33000,55000,77382,48048,
  51186,40000,35000,21716,63051,15025,15025,15025,
  15025,800000,1110000,59700,25908,829350,1198000,1031655)

# prepare set
prods &lt;- products[products &lt;= amount]  # no elements &gt; amount
prods &lt;- sort(prods, decreasing=TRUE)  # decreasing order

# now find one solution
system.time(is &lt;- subsetsum(prods, amount))
#  user  system elapsed 
# 0.030   0.000   0.029 

prods[is]
#  [1]   70740   70740   71190   76950   77382   80000   83800
#  [8]   90900  456000  533600  829350 1110000 1198000

sum(prods[is]) == amount
# [1] TRUE

# Timings:
#             unsorted   decr.sorted
# "greedy"      22.930         0.030    (therefore the default settings)
# "dynamic"      2.515         0.860    (overhead for smaller sets)
# sss_test       8.450         0.040    (no indices returned)

## End(Not run)
</code></pre>

<hr>
<h2 id='Testfunctions'>
Optimization Test Functions
</h2><span id='topic+fnRosenbrock'></span><span id='topic+grRosenbrock'></span><span id='topic+fnRastrigin'></span><span id='topic+grRastrigin'></span><span id='topic+fnNesterov'></span><span id='topic+grNesterov'></span><span id='topic+fnNesterov1'></span><span id='topic+fnNesterov2'></span><span id='topic+fnHald'></span><span id='topic+grHald'></span><span id='topic+fnShor'></span><span id='topic+grShor'></span>

<h3>Description</h3>

<p>Simple and often used test function defined in higher dimensions and with
analytical gradients, especially suited for performance tests. Analytical 
gradients, where existing, are provided with the <code>gr</code> prefix.
The dimension is determined by the length of the input vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fnRosenbrock(x)
grRosenbrock(x)
fnRastrigin(x)
grRastrigin(x)
fnNesterov(x)
grNesterov(x)
fnNesterov1(x)
fnNesterov2(x)
fnHald(x)
grHald(x)
fnShor(x)
grShor(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Testfunctions_+3A_x">x</code></td>
<td>
<p>numeric vector of a certain length.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Rosenbrock</b> &ndash; Rosenbrock's famous valley function from 1960. It can
also be regarded as a least-squares problem:
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^{n-1} (1-x_i)^2 + 100 (x_{i+1}-x_i^2)^2</code>
</p>


<table>
<tr>
 <td style="text-align: left;">
  No. of Vars.: </td><td style="text-align: left;"> n &gt;= 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Bounds: </td><td style="text-align: left;"> -5.12 &lt;= xi &lt;= 5.12 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Local minima: </td><td style="text-align: left;"> at f(-1, 1, ..., 1) for n &gt;= 4 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Minimum: </td><td style="text-align: left;"> 0.0 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Solution: </td><td style="text-align: left;"> xi = 1, i = 1:n </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p><b>Nesterov</b> &ndash; Nesterov's smooth adaptation of Rosenbrock, based on the 
idea of Chebyshev polynomials. This function is even more difficult to 
optimize than Rosenbrock's:
</p>
<p style="text-align: center;"><code class="reqn">(1 - x_1)^2 / 4 + \sum_{i=1}^{n-1} (1 + x_{i+1} - 2 x_i^2)^2</code>
</p>

<p>Two nonsmooth Nesterov functions are available: Nesterov2 and Nesterov1,
defined as
</p>
<p style="text-align: center;"><code class="reqn">(1 - x_1)^2 / 4 + \sum_{i=1}^{n-1} |1 + x_{i+1} - 2 x_i^2|</code>
</p>

<p style="text-align: center;"><code class="reqn">|1 - x_1| / 4 + \sum_{i=1}^{n-1} (|1 + x_{i+1} - 2 |x_i||</code>
</p>


<table>
<tr>
 <td style="text-align: left;">
  No. of Vars.: </td><td style="text-align: left;"> n &gt;= 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Bounds: </td><td style="text-align: left;"> -5.12 &lt;= xi &lt;= 5.12 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Local minima: ?</td>
</tr>
<tr>
 <td style="text-align: left;">
  Minimum: </td><td style="text-align: left;"> 0.0 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Solution: </td><td style="text-align: left;"> xi = 1, i = 1:n </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p><b>Nesterov1</b> and <b>Nesterov2</b> &ndash; Simlar to <code>Nesterov</code>, except the terms added are taken with absolute value, which makes this function nonsmooth and painful for gradient-based optimization routines; no gradient provided.<br /> (Nesterov2 uses absolute instead of quadratic terms.)
</p>
<p><b>Rastrigin</b> &ndash; Rastrigin's function is a famous, non-convex example from 1989 for global optimization. It is a typical example of a multimodal function with many local minima:
</p>
<p style="text-align: center;"><code class="reqn">10 n + \sum_1^n (x_i^2 - 10 \cos(2 \pi x_i))</code>
</p>


<table>
<tr>
 <td style="text-align: left;">
  No. of Vars.: </td><td style="text-align: left;"> n &gt;= 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Bounds: </td><td style="text-align: left;"> -5.12 &lt;= xi &lt;= 5.12 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Local minima: </td><td style="text-align: left;"> many </td>
</tr>
<tr>
 <td style="text-align: left;">
  Minimum: </td><td style="text-align: left;"> 0.0 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Solution: </td><td style="text-align: left;"> xi = 0, i = 1:n </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p><b>Hald</b> &ndash; Hald's function is a typical example of a non-smooth test
function, from Hald and Madsen in 1981.
</p>
<p style="text-align: center;"><code class="reqn">\max_{1 \le i \le n} | \frac{x_1 + x_2 t_i}{1 + x_3 t_i + x_4 t_i^2 + x_5 t_i^3} - \exp(t_i)|</code>
</p>

<p>where <code class="reqn">n = 21</code> and <code class="reqn">t_i = -1 + (i - 1)/10</code> for <code class="reqn">1 \le i \le 21</code>.
</p>

<table>
<tr>
 <td style="text-align: left;">
  No. of Vars.: </td><td style="text-align: left;"> n =5 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Bounds: </td><td style="text-align: left;"> -1 &lt;= xi &lt;= 1 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Local minima: </td><td style="text-align: left;"> ? </td>
</tr>
<tr>
 <td style="text-align: left;">
  Minimum: </td><td style="text-align: left;"> 0.0001223713 </td>
</tr>
<tr>
 <td style="text-align: left;">
  Solution: </td><td style="text-align: left;"> (0.99987763,  0.25358844, -0.74660757,  0.24520150, -0.03749029) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p><b>Shor</b> &ndash; Shor's function is another typical example of a non-smooth test
function, a benchmark for Shor's R-algorithm.
</p>


<h3>Value</h3>

<p>Returns the values of the test function resp. its gradient at that point.
If an analytical gradient is not available, a function computing the gradient 
numerically will be provided.
</p>


<h3>References</h3>

<p>Search the Internet.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- runif(5)
fnHald(x); grHald(x)

# Compare analytical and numerical gradient
shor_gr &lt;- function(x) adagio:::ns.grad(fnShor, x)    # internal gradient
grShor(x); shor_gr(x) 
</code></pre>

<hr>
<h2 id='transfinite'>
Boxed Region Transformation
</h2><span id='topic+transfinite'></span>

<h3>Description</h3>

<p>Transformation of a box/bound constrained region to an unconstrained one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transfinite(lower, upper, n = length(lower))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transfinite_+3A_lower">lower</code>, <code id="transfinite_+3A_upper">upper</code></td>
<td>
<p>lower and upper box/bound constraints.</p>
</td></tr>
<tr><td><code id="transfinite_+3A_n">n</code></td>
<td>
<p>length of upper, lower if both are scalars, to which they get
repeated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Transforms a constraint region in n-dimensional space bijectively to the
unconstrained <code class="reqn">R^n</code> space, applying a <code>atanh</code> resp. <code>exp</code>
transformation to each single variable that is bound constraint.
</p>
<p>It provides two functions, <code>h: B = []x...x[] --&gt; R^n</code> and its inverse
<code>hinv</code>. These functions can, for example, be used to add box/bound
constraints to a constrained optimization problem that is to be solved with
a (nonlinear) solver not allowing constraints.
</p>


<h3>Value</h3>

<p>Returns to functions as components <code>h</code> and <code>hinv</code> of a list. 
</p>


<h3>Note</h3>

<p>Based on an idea of Ravi Varadhan, intrinsically used in his implementation
of Nelder-Mead in the &lsquo;dfoptim&rsquo; package.
</p>
<p>For positivity constraints, <code>x&gt;=0</code>, this approach is considered to be
numerically more stable than <code>x--&gt;exp(x)</code> or <code>x--&gt;x^2</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lower &lt;- c(-Inf, 0,   0)
upper &lt;- c( Inf, 0.5, 1)
Tf &lt;- transfinite(lower, upper)
h &lt;- Tf$h; hinv &lt;- Tf$hinv

## Not run: 
##  Solve Rosenbrock with one variable restricted
rosen &lt;- function(x) {
    n &lt;- length(x)
    x1 &lt;- x[2:n]; x2 &lt;- x[1:(n-1)]
    sum(100*(x1-x2^2)^2 + (1-x2)^2)
}
f  &lt;- function(x) rosen(hinv(x))    # f must be defined on all of R^n
x0 &lt;- c(0.1, 0.1, 0.1)              # starting point not on the boundary!
nm &lt;- nelder_mead(h(x0), f)         # unconstraint Nelder-Mead
hinv(nm$xmin); nm$fmin              # box/bound constraint solution
# [1] 0.7085596 0.5000000 0.2500004
# [1] 0.3353605

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
