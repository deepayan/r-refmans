<!DOCTYPE html><html lang="en"><head><title>Help for package TransTGGM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {TransTGGM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#example.data'><p>Some example data</p></a></li>
<li><a href='#tensor.GGM.trans'><p>Transfer learning for tensor graphical models.</p></a></li>
<li><a href='#Theta.est'><p>Fast sparse precision matrix estimation.</p></a></li>
<li><a href='#Theta.tuning'><p>Fast sparse precision matrix estimation.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Transfer Learning for Tensor Graphical Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Tensor Gaussian graphical models (GGMs) have important applications in numerous areas, which can interpret conditional independence structures within tensor data. Yet, the available tensor data in one single study is often limited due to high acquisition costs. 
             Although relevant studies can provide additional data, it remains an open question how to pool such heterogeneous data. This package implements a transfer learning framework for tensor GGMs, which takes full advantage of informative auxiliary domains even when non-informative auxiliary domains are present, benefiting from the carefully designed data-adaptive weights. 
             Reference: 
             Ren, M., Zhen Y., and Wang J. (2022). "Transfer learning for tensor graphical models" &lt;<a href="https://doi.org/10.48550/arXiv.2211.09391">doi:10.48550/arXiv.2211.09391</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, Matrix, rTensor, Tlasso, glasso, doParallel,expm</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyDataCompression:</td>
<td>xz</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-22 11:08:00 UTC; 10259</td>
</tr>
<tr>
<td>Author:</td>
<td>Mingyang Ren <a href="https://orcid.org/0000-0002-8061-9940"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Yaoming Zhen [aut],
  Junhui Wang [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mingyang Ren &lt;renmingyang17@mails.ucas.ac.cn&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-11-23 11:30:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='example.data'>Some example data</h2><span id='topic+example.data'></span>

<h3>Description</h3>

<p>Some example data
</p>


<h3>Format</h3>

<p>A list including:
t.data: Tensor data in the target domain with n=50 and (p1,p2,p3)=(10,10,10).
A.data: Tensor data in auxiliary domains with K=5 and nk=100.
t.Omega.true.list: A list, the true precision matrices of all modes in the target domain.
</p>


<h3>Source</h3>

<p>Simulated data
</p>

<hr>
<h2 id='tensor.GGM.trans'>Transfer learning for tensor graphical models.</h2><span id='topic+tensor.GGM.trans'></span>

<h3>Description</h3>

<p>The main function for Transfer learning for tensor graphical models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensor.GGM.trans(t.data, A.data, A.lambda, A.orac = NULL, c=0.6,
                        t.lambda.int.trans=NULL, t.lambda.int.aggr=NULL,
                        theta.algm="cd", cov.select="inverse",
                        cov.select.agg.size = "inverse",
                        cov.select.agg.diff = "tensor.prod",
                        symmetric = TRUE, init.method="Tlasso",
                        init.method.aux="Tlasso", mode.set = NULL,
                        init.iter.Tlasso=2, cn.lam2=seq(0.1,2,length.out =10),
                        c.lam.Tlasso=20, c.lam.sepa=20, adjust.BIC=FALSE,
                        normalize = TRUE, inti.the=TRUE, sel.ind="fit")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tensor.GGM.trans_+3A_t.data">t.data</code></td>
<td>
<p>The tensor data in the target domain, a p1 * p2 * ... * pM * n array, where n is the sample size and pm is dimension of the m-th tensor mode. M should be larger than 2.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_a.data">A.data</code></td>
<td>
<p>The tensor data in auxiliary domains, a list with K elements, each of which is a p1 * p2 * ... * pM * nk array, where nk is the sample size of the k-th auxiliary domain.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_a.lambda">A.lambda</code></td>
<td>
<p>The tuning parameters used for initialization in auxiliary domains, a list with K elements, each of which is a M-dimensional vector corresponding to M modes.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_a.orac">A.orac</code></td>
<td>
<p>The set of informative auxiliary domains, and the default setting is NULL, which means that no set is specified.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_c">c</code></td>
<td>
<p>The c of subjects in the target domain are used for initialization of the transfer learning, and the remaining 1-c of subjects are used for the model selection step. The default setting is 0.8.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_t.lambda.int.trans">t.lambda.int.trans</code></td>
<td>
<p>The tuning parameters used for initialization in the target domain (based on c subjects used for transfer learning), that is, the tuning lambda for Tlasso (PAMI, 2020) &amp; Separable method (JCGS, 2022)</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_t.lambda.int.aggr">t.lambda.int.aggr</code></td>
<td>
<p>The tuning parameters used for initialization in the target domain (based on 1-c subjects used for the model selection step).</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_theta.algm">theta.algm</code></td>
<td>
<p>The optimization algorithm used to solve <code class="reqn">\widehat{\Omega}</code> in step 2(b), which can be selected as &quot;admm&quot; (ADMM algorithm) or &quot;cd&quot; (coordinate descent).</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_cov.select">cov.select</code></td>
<td>
<p>Methods used to calculate covariance matrices for initialization in both target and auxiliary domains, which can be selected as &quot;tensor.prod&quot; (tensor product based on tensor subject and the initial estimate of the precision matrix, TPAMI, 2020) and &quot;inverse&quot; (direct inversion of the initial estimate of the precision matrix)</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_cov.select.agg.size">cov.select.agg.size</code></td>
<td>
<p>Methods used to calculate covariance matrices for model selection step in the target domain.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_cov.select.agg.diff">cov.select.agg.diff</code></td>
<td>
<p>Methods used to calculate covariance matrices for model selection step in the target domain.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_symmetric">symmetric</code></td>
<td>
<p>Whether to symmetrize the final estimated precision matrices, and the default is True.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_init.method">init.method</code></td>
<td>
<p>The initialization method for tensor precision matrices in the target domain, which can be selected as &quot;Tlasso&quot; (PAMI, 2020) &amp; &quot;sepa&quot; (Separable method, JCGS, 2022). Note that the &quot;sepa&quot; method has not been included in the current version of this R package to circumvent code ownership issues.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_init.method.aux">init.method.aux</code></td>
<td>
<p>The initialization method for tensor precision matrices in auxiliary domains.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_mode.set">mode.set</code></td>
<td>
<p>Whether to estimate only the specified mode, and the default setting is NULL, which means estimating all mode.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_init.iter.tlasso">init.iter.Tlasso</code></td>
<td>
<p>The number of maximal iteration when using Tlasso for initialization, default is 2.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_cn.lam2">cn.lam2</code></td>
<td>
<p>The coefficient set in tuning parameters used to solve <code class="reqn">\widehat{\Omega}</code> in step 2(b), default is seq(0.1,1,length.out =10).</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_c.lam.tlasso">c.lam.Tlasso</code></td>
<td>
<p>The coefficient in tuning parameters for initialization (when using Tlasso): <code class="reqn">c.lam.Tlasso * \sqrt( pm * \log(pm)/( n*p1*...*pM ))</code>, default is 20 suggested in (PAMI, 2020).</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_c.lam.sepa">c.lam.sepa</code></td>
<td>
<p>The coefficient in tuning parameters for initialization (when using sepa): <code class="reqn">c.lam.sepa * \sqrt( pm * \log(pm)/( n*p1*...*pM ))</code>.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_adjust.bic">adjust.BIC</code></td>
<td>
<p>Whether to use the adjusted BIC to select lambda2, the default setting is F.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_normalize">normalize</code></td>
<td>
<p>The normalization method of precision matrix. When using Tlasso, <code class="reqn">\Omega_{11} = 1</code> if normalize = F and <code class="reqn">\| \Omega_{11} \|_{F} = 1</code> if normalize = T. Default value is T.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_inti.the">inti.the</code></td>
<td>
<p>T: the initial values in Step 2(b) is Omega0.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_sel.ind">sel.ind</code></td>
<td>
<p>The approach to model selection, which can be selected from c(&quot;fit&quot;, &quot;predict&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including:
Omega.list: the final estimation result of the target precision matrices after the model selection of transfer learning-based estimation and initial estimation (in which the initial covariance matrices of auxiliary domains is weighted by sample sizes);
Omega.sym.list: the symmetrized final estimation result in Omega.list;
Omega.list.diff: the final estimation result of the target precision matrices after the model selection of transfer learning-based estimation and initial estimation (in which the initial covariance matrices of auxiliary domains is weighted by the differences with the target domain);
Omega.sym.list.diff: the symmetrized final estimation result in Omega.list.diff;
res.trans.list: transfer learning-based estimation results.
</p>


<h3>Author(s)</h3>

<p>Mingyang Ren, Yaoming Zhen, Junhui Wang. Maintainer: Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>.
</p>


<h3>References</h3>

<p>Ren, M., Zhen Y., and Wang J. (2022). Transfer learning for tensor graphical models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(TransTGGM)
library(Tlasso)
data(example.data)
t.data = example.data$t.data
A.data = example.data$A.data
t.Omega.true.list = example.data$t.Omega.true.list
normalize = TRUE

K = length(A.data)
p.vec = dim(t.data)
M = length(p.vec) - 1
n = p.vec[M+1]
p.vec = p.vec[1:M]
tla.lambda = 20*sqrt( p.vec*log(p.vec) / ( n * prod(p.vec) ))
A.lambda = list()
for (k in 1:K) {
  A.lambda[[k]] = 20*sqrt( log(p.vec) / ( dim(A.data[[k]])[M+1] * prod(p.vec) ))
}

res.final = tensor.GGM.trans(t.data, A.data, A.lambda, normalize = normalize)
Tlasso.Omega.list = Tlasso.fit(t.data, lambda.vec = tla.lambda,
                    norm.type = 1+as.numeric(normalize))

i.Omega = as.data.frame(t(unlist(est.analysis(res.final$Omega.list, t.Omega.true.list))))
i.Omega.diff = t(unlist(est.analysis(res.final$Omega.list.diff, t.Omega.true.list)))
i.Omega.diff = as.data.frame(i.Omega.diff)
i.Tlasso = as.data.frame(t(unlist(est.analysis(Tlasso.Omega.list, t.Omega.true.list))))
i.Omega.diff     # proposed.v
i.Omega          # proposed
i.Tlasso         # Tlasso





</code></pre>

<hr>
<h2 id='Theta.est'>Fast sparse precision matrix estimation.</h2><span id='topic+Theta.est'></span>

<h3>Description</h3>

<p>The fast sparse precision matrix estimation in step 2(b).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Theta.est(S.hat.A, delta.hat, lam2=0.1, Omega.hat0=NULL,
                 n=100, max_iter=10, eps=1e-3, method = "cd")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Theta.est_+3A_s.hat.a">S.hat.A</code></td>
<td>
<p>The sample covariance matrix.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_delta.hat">delta.hat</code></td>
<td>
<p>The divergence matrix estimated in step 2(a). If the precision matrix is estimated in the common case (Liu and Luo, 2015, JMVA), it can be set to zero matrix.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_lam2">lam2</code></td>
<td>
<p>A float value, a tuning parameter.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_omega.hat0">Omega.hat0</code></td>
<td>
<p>The initial values of the precision matrix, which can be unspecified.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_n">n</code></td>
<td>
<p>The sample size.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_max_iter">max_iter</code></td>
<td>
<p>Int, maximum number of cycles of the algorithm.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_eps">eps</code></td>
<td>
<p>A float value, algorithm termination threshold.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_method">method</code></td>
<td>
<p>The optimization algorithm, which can be selected as &quot;admm&quot; (ADMM algorithm) or &quot;cd&quot; (coordinate descent).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including:
Theta.hat.m: the optimal precision matrix;
BIC.summary: the summary of BICs;
Theta.hat.list.m: the precision matrices corresponding to a sequence of tuning parameters.
</p>


<h3>Author(s)</h3>

<p>Mingyang Ren, Yaoming Zhen, Junhui Wang. Maintainer: Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>.
</p>


<h3>References</h3>

<p>Ren, M., Zhen Y., and Wang J. (2022). Transfer learning for tensor graphical models.
Liu, W. and Luo X. (2015). Fast and adaptive sparse precision matrix estimation in high dimensions, Journal of Multivariate Analysis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p = 20
n = 200
omega = diag(rep(1,p))
for (i in 1:p) {
  for (j in 1:p) {
    omega[i,j] = 0.3^(abs(i-j))*(abs(i-j) &lt; 2)
  }
}
Sigma = solve(omega)
X = MASS::mvrnorm(n, rep(0,p), Sigma)
S.hat.A = cov(X)
delta.hat = diag(rep(1,p)) - diag(rep(1,p))
omega.hat = Theta.est(S.hat.A, delta.hat, lam2=0.2)



</code></pre>

<hr>
<h2 id='Theta.tuning'>Fast sparse precision matrix estimation.</h2><span id='topic+Theta.tuning'></span>

<h3>Description</h3>

<p>The fast sparse precision matrix estimation in step 2(b).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Theta.tuning(lambda2, S.hat.A, delta.hat, Omega.hat0, n.A,
                    theta.algm="cd", adjust.BIC=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Theta.tuning_+3A_lambda2">lambda2</code></td>
<td>
<p>A vector, a sequence of tuning parameters.</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_s.hat.a">S.hat.A</code></td>
<td>
<p>The sample covariance matrix.</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_delta.hat">delta.hat</code></td>
<td>
<p>The divergence matrix estimated in step 2(a). If the precision matrix is estimated in the common case (Liu and Luo, 2015, JMVA), it can be set to zero matrix.</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_omega.hat0">Omega.hat0</code></td>
<td>
<p>The initial values of the precision matrix.</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_n.a">n.A</code></td>
<td>
<p>The sample size.</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_theta.algm">theta.algm</code></td>
<td>
<p>The optimization algorithm used to solve <code class="reqn">\widehat{\Omega}</code> in step 2(b), which can be selected as &quot;admm&quot; (ADMM algorithm) or &quot;cd&quot; (coordinate descent).</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_adjust.bic">adjust.BIC</code></td>
<td>
<p>Whether to use the adjusted BIC to select lambda2, the default setting is F.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including:
Theta.hat.m: the optimal precision matrix;
BIC.summary: the summary of BICs;
Theta.hat.list.m: the precision matrices corresponding to a sequence of tuning parameters.
</p>


<h3>Author(s)</h3>

<p>Mingyang Ren, Yaoming Zhen, Junhui Wang. Maintainer: Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>.
</p>


<h3>References</h3>

<p>Ren, M., Zhen Y., and Wang J. (2022). Transfer learning for tensor graphical models.
Liu, W. and Luo X. (2015). Fast and adaptive sparse precision matrix estimation in high dimensions, Journal of Multivariate Analysis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p = 20
n = 200
omega = diag(rep(1,p))
for (i in 1:p) {
  for (j in 1:p) {
    omega[i,j] = 0.3^(abs(i-j))*(abs(i-j) &lt; 2)
  }
}
Sigma = solve(omega)
X = MASS::mvrnorm(n, rep(0,p), Sigma)
S.hat.A = cov(X)
delta.hat = diag(rep(1,p)) - diag(rep(1,p))
lambda2 = seq(0.1,0.5,length.out =10)
res = Theta.tuning(lambda2, S.hat.A, delta.hat, n.A=n)
omega.hat = res$Theta.hat.m



</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
