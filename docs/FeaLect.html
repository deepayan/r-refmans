<!DOCTYPE html><html><head><title>Help for package FeaLect</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FeaLect}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#compute.balanced'><p>Balances between negative and positive samples by oversampling.</p></a></li>
<li><a href='#compute.logistic.score'><p>Fits a logistic regression model using the linear scores</p></a></li>
<li><a href='#doctor.validate'><p>	Validates a model using validating samples.</p></a></li>
<li><a href='#FeaLect'>
<p>Computes the scores of the features.</p></a></li>
<li><a href='#FeaLect-package'>
<p>Scores Features for Feature Selection</p></a></li>
<li><a href='#ignore.redundant'><p>Refines a feature matrix</p></a></li>
<li><a href='#input.check.FeaLect'><p>	Checks the inputs to Fealect() function.</p></a></li>
<li><a href='#mcl_sll'><p>MCL and SLL lymphoma subtypes</p></a></li>
<li><a href='#random.subset'><p>Selects a random subset of the  input.</p></a></li>
<li><a href='#train.doctor'>
<p>Fits various models based on a combination on penalized linear models and logistic regression.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Scores Features for Feature Selection</td>
</tr>
<tr>
<td>Version:</td>
<td>1.20</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-02-25</td>
</tr>
<tr>
<td>Author:</td>
<td>Habil Zare</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Habil Zare &lt;zare@u.washington.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>lars, rms</td>
</tr>
<tr>
<td>Description:</td>
<td>For each feature, a score is computed that can be useful
        for feature selection. Several random subsets are sampled from
        the input data and for each random subset, various linear
        models are fitted using lars method. A score is assigned to
        each feature based on the tendency of LASSO in including that
        feature in the models.Finally, the average score and the models
        are returned as the output. The features with relatively low
        scores are recommended to be ignored because they can lead to
        overfitting of the model to the training data. Moreover, for
        each random subset, the best set of features in terms of global
        error is returned. They are useful for applying Bolasso, the
        alternative feature selection method that recommends the
        intersection of features subsets.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-02-25 17:30:06 UTC</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-02-25 16:22:57 UTC; habil</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
</table>
<hr>
<h2 id='compute.balanced'>Balances between negative and positive samples by oversampling.</h2><span id='topic+compute.balanced'></span>

<h3>Description</h3>

<p>If negative samples are less than positive ones, more copies of the negative cases are added and vice versa.</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute.balanced(F_, L_)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute.balanced_+3A_f_">F_</code></td>
<td>

<p>The feature matrix, each column is a feature.</p>
</td></tr>
<tr><td><code id="compute.balanced_+3A_l_">L_</code></td>
<td>

<p>The vector of labels named according to the rows of F.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>	Considerably unbalanced classes may be probabilistic for fitting some models.</p>


<h3>Value</h3>

<p>Returns a list of: 
</p>
<table>
<tr><td><code>F_</code></td>
<td>

<p>The feature matrix, each column is a feature.</p>
</td></tr>
<tr><td><code>L_</code></td>
<td>

<p>The vector of labels named according to the rows of F.</p>
</td></tr>	
</table>


<h3>Author(s)</h3>

<p>Habil Zare</p>


<h3>References</h3>

<p>&quot;Statistical Analysis of Overfitting Features&quot;, manuscript in preparation.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeaLect">FeaLect</a></code>, <code><a href="#topic+train.doctor">train.doctor</a></code>, <code><a href="#topic+doctor.validate">doctor.validate</a></code>, 
<code><a href="#topic+random.subset">random.subset</a></code>, <code><a href="#topic+compute.balanced">compute.balanced</a></code>,<code><a href="#topic+compute.logistic.score">compute.logistic.score</a></code>, 
<code><a href="#topic+ignore.redundant">ignore.redundant</a></code>, <code><a href="#topic+input.check.FeaLect">input.check.FeaLect</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FeaLect)
data(mcl_sll)
F &lt;- as.matrix(mcl_sll[ ,-1])	# The Feature matrix
L &lt;- as.numeric(mcl_sll[ ,1])	# The labels
names(L) &lt;- rownames(F)
message(L)

balanced &lt;- compute.balanced(F_=F, L_=L)
message(balanced$L_)

</code></pre>

<hr>
<h2 id='compute.logistic.score'>Fits a logistic regression model using the linear scores</h2><span id='topic+compute.logistic.score'></span>

<h3>Description</h3>

<p> A logistic regression model is fitted to the linear scores using lrm() function 
and the logistic scores are computed using the formula: 1/(1+exp(-(a+bX))) where a and b are the logistic coefficients.</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute.logistic.score(F_, L_, considered.features, training.samples, validating.samples,
			   linear.scores, report.fitting.failure = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute.logistic.score_+3A_f_">F_</code></td>
<td>

<p>The feature matrix, each column is a feature.</p>
</td></tr>
<tr><td><code id="compute.logistic.score_+3A_l_">L_</code></td>
<td>

<p>The vector of labels named according to the rows of F.</p>
</td></tr>
<tr><td><code id="compute.logistic.score_+3A_training.samples">training.samples</code></td>
<td>

<p>The names of rows of F that should be considered as training samples.</p>
</td></tr>
<tr><td><code id="compute.logistic.score_+3A_validating.samples">validating.samples</code></td>
<td>

<p>The names of rows of F that should be considered as validating samples.</p>
</td></tr>
<tr><td><code id="compute.logistic.score_+3A_considered.features">considered.features</code></td>
<td>

<p>The names of columns of F that determine the features of interest.</p>
</td></tr>
<tr><td><code id="compute.logistic.score_+3A_linear.scores">linear.scores</code></td>
<td>

<p>A vector that contains for each training or validating sample, a linear score predicted by the linear method.</p>
</td></tr>
<tr><td><code id="compute.logistic.score_+3A_report.fitting.failure">report.fitting.failure</code></td>
<td>

<p>If TRUE, any failure in fitting the linear of logistic models will be printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The logistic regression will be fitted to all training and validating samples.</p>


<h3>Value</h3>

<p>Returns a list of: 
</p>
<table>
<tr><td><code>logistic.scores</code></td>
<td>
<p>A vector of predicted logistic values for all samples.</p>
</td></tr>
<tr><td><code>logistic.cofs</code></td>
<td>
<p>The coefficients that are computed by logistic regression.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Logistic regression is also done on top of fitting the linear models.</p>


<h3>Author(s)</h3>

<p>Habil Zare</p>


<h3>References</h3>

<p>&quot;Statistical Analysis of Overfitting Features&quot;, manuscript in preparation.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeaLect">FeaLect</a></code>, <code><a href="#topic+train.doctor">train.doctor</a></code>, <code><a href="#topic+doctor.validate">doctor.validate</a></code>, 
<code><a href="#topic+random.subset">random.subset</a></code>, <code><a href="#topic+compute.balanced">compute.balanced</a></code>,<code><a href="#topic+compute.logistic.score">compute.logistic.score</a></code>, 
<code><a href="#topic+ignore.redundant">ignore.redundant</a></code>, <code><a href="#topic+input.check.FeaLect">input.check.FeaLect</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FeaLect)
data(mcl_sll)
F &lt;- as.matrix(mcl_sll[ ,-1])	# The Feature matrix
L &lt;- as.numeric(mcl_sll[ ,1])	# The labels
names(L) &lt;- rownames(F)
all.samples &lt;- rownames(F); ts &lt;- all.samples[5:10]; vs &lt;- all.samples[c(1,22)]
L &lt;- L[c(ts,vs)]
L

asymptotic.scores &lt;- c(1,0.9,0.8,0.2,0.1,0.1,0.7,0.2)

compute.logistic.score(F_=F, L_=L, training.samples=ts, validating.samples=vs, 
			     considered.features=colnames(F),linear.scores= asymptotic.scores)

</code></pre>

<hr>
<h2 id='doctor.validate'>	Validates a model using validating samples.</h2><span id='topic+doctor.validate'></span>

<h3>Description</h3>

<p>A model fitted on the training samples, can be validated on a separate validating set. The recall, precision, and accuracy of the model are computed. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>doctor.validate(true.labels, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="doctor.validate_+3A_true.labels">true.labels</code></td>
<td>

<p>A vector of 0 and 1.
</p>
</td></tr>
<tr><td><code id="doctor.validate_+3A_predictions">predictions</code></td>
<td>

<p>A vector of 0 and 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>F-measure is equal to:  2 times precision times recall / (precision+recall).
</p>


<h3>Value</h3>

<p>F-measure, precision, and recall are calculated. Also, the mis-labeled cases are reported.
</p>


<h3>Author(s)</h3>

<p>Habil Zare</p>


<h3>References</h3>

<p>&quot;Statistical Analysis of Overfitting Features&quot;, manuscript in preparation.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeaLect">FeaLect</a></code>, <code><a href="#topic+train.doctor">train.doctor</a></code>, <code><a href="#topic+doctor.validate">doctor.validate</a></code>, 
<code><a href="#topic+random.subset">random.subset</a></code>, <code><a href="#topic+compute.balanced">compute.balanced</a></code>,<code><a href="#topic+compute.logistic.score">compute.logistic.score</a></code>, 
<code><a href="#topic+ignore.redundant">ignore.redundant</a></code>, <code><a href="#topic+input.check.FeaLect">input.check.FeaLect</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>tls &lt;- c(1,1,1,0,0)
ps &lt;- c(1,1,0,1,0)
names(tls) &lt;- 1:5; names(ps) &lt;- 1:5

doctor.validate(true.labels=tls, predictions=ps)

</code></pre>

<hr>
<h2 id='FeaLect'>
Computes the scores of the features. 
</h2><span id='topic+FeaLect'></span>

<h3>Description</h3>

<p>Several random subsets are sampled from the input data and for each random subset, various linear models are fitted using lars method. 
A score is assigned to each feature based on the tendency of LASSO in including that feature in the models.
Finally, the average score and the models are returned as the output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FeaLect(F, L, maximum.features.num = dim(F)[2], total.num.of.models, gamma = 3/4, 
	   persistence = 1000, talk = FALSE, minimum.class.size = 2, 
	   report.fitting.failure = FALSE, return_linear.models = TRUE, balance = TRUE,
	   replace = TRUE, plot.scores = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FeaLect_+3A_f">F</code></td>
<td>

<p>The feature matrix, each column is a feature.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_l">L</code></td>
<td>

<p>The vector of labels named according to the rows of F.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_maximum.features.num">maximum.features.num</code></td>
<td>
 
<p>Upto this number of features are allowed to contribute to each linear model.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_total.num.of.models">total.num.of.models</code></td>
<td>

<p>The total number of models that are fitted.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_gamma">gamma</code></td>
<td>

<p>A value in range 0-1 that determines the relative size of sample subsets.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_persistence">persistence</code></td>
<td>

<p>Maximum number of tries for randomly choosing.samples,
If we try this many times and the obtained labels are all the same,
we give up (maybe the whole labels are the same) with the error message: &quot; Not enough variation in the labels...&quot;.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_talk">talk</code></td>
<td>

<p>If TRUE, some messages are printed during the computations.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_minimum.class.size">minimum.class.size</code></td>
<td>

<p>The size of both positive and negative classes should be greater than this threshold after sampling.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_report.fitting.failure">report.fitting.failure</code></td>
<td>

<p>If TRUE, any failure in fitting the linear of logistic models will be printed.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_return_linear.models">return_linear.models</code></td>
<td>

<p>The models are memory intensive, so for if they more than 1000, we may decide to ignore them to prevent memory outage.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_balance">balance</code></td>
<td>

<p>If TRUE, the cases will be balanced for the same number of positive vs. negatives by oversampling before fitting the linear model.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_replace">replace</code></td>
<td>

<p>If TRUE, the subsets are sampled with replacement.</p>
</td></tr>
<tr><td><code id="FeaLect_+3A_plot.scores">plot.scores</code></td>
<td>

<p>If TRUE, the scores are plotted in logarithmic scale after each iteration.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the reference for more details.</p>


<h3>Value</h3>

<p>Returns a list of: 
</p>
<table>
<tr><td><code>log.scores</code></td>
<td>
<p>A vector containing the logarithm of final scores.</p>
</td></tr>
<tr><td><code>feature.matrix</code></td>
<td>
<p>The input feature matrix.</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>
<p>The input labels</p>
</td></tr>
<tr><td><code>total.num.of.models</code></td>
<td>
<p>The total number of models that are fitted.</p>
</td></tr>
<tr><td><code>maximum.features.num</code></td>
<td>
<p>Upto this number of features are allowed to contribute to each linear model.</p>
</td></tr>
<tr><td><code>feature.scores.history</code></td>
<td>
<p>The matrix of history of feature scores where column i contains the scores after i runs.</p>
</td></tr>
<tr><td><code>num.of.features.score</code></td>
<td>
<p>A vector, entry i contains the number of times that i has been the best number of features.</p>
</td></tr>
<tr><td><code>best.feature.num</code></td>
<td>
<p> The i'th value of this vector is the best number of features for the i'th model.</p>
</td></tr>
<tr><td><code>mislabeling.record</code></td>
<td>
<p> A vector that keeps track of the frequency of mislabelling for each cases.</p>
</td></tr>
<tr><td><code>doctors</code></td>
<td>
<p>List of all models which are created by train.doctor() function.</p>
</td></tr>
<tr><td><code>best.features.intersection</code></td>
<td>
<p> Best features are computed for each sampling and their intersection is reported as this vector of features names</p>
</td></tr>
<tr><td><code>features.with.best.global.error</code></td>
<td>
<p> A list containing the sets of features. The set i was the best for i'th sampling.</p>
</td></tr>
<tr><td><code>time.taken</code></td>
<td>
<p>Total time used for executing this function.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Logistic regression is also done on top of fitting the linear models.</p>


<h3>Author(s)</h3>

<p>Habil Zare</p>


<h3>References</h3>

<p>&quot;Statistical Analysis of Overfitting Features&quot;, manuscript in preparation.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeaLect">FeaLect</a></code>, <code><a href="#topic+train.doctor">train.doctor</a></code>, <code><a href="#topic+doctor.validate">doctor.validate</a></code>, 
<code><a href="#topic+random.subset">random.subset</a></code>, <code><a href="#topic+compute.balanced">compute.balanced</a></code>,<code><a href="#topic+compute.logistic.score">compute.logistic.score</a></code>, 
<code><a href="#topic+ignore.redundant">ignore.redundant</a></code>, <code><a href="#topic+input.check.FeaLect">input.check.FeaLect</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FeaLect)
data(mcl_sll)
F &lt;- as.matrix(mcl_sll[ ,-1])	# The Feature matrix
L &lt;- as.numeric(mcl_sll[ ,1])	# The labels
names(L) &lt;- rownames(F)
message(dim(F)[1], " samples and ",dim(F)[2], " features.")

## For this data, total.num.of.models is suggested to be at least 100.
FeaLect.result &lt;-FeaLect(F=F,L=L,maximum.features.num=10,total.num.of.models=20,talk=TRUE)	

</code></pre>

<hr>
<h2 id='FeaLect-package'>
Scores Features for Feature Selection
</h2><span id='topic+FeaLect-package'></span>

<h3>Description</h3>


<p>Suppose you have a feature matrix with 200 features and only 20 samples and your goal is to build a classifier. 
You can run the FeaLect() function to compute the scores for your features. Only the relatively high score 
features (say the top 20) are recommended for further analysis. In this way, one can prevent overfitting by
reducing the number of features significantly. 
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> FeaLect</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Scores Features for Feature Selection</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.20</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2020-02-25</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Habil Zare</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Habil Zare &lt;zare@u.washington.edu&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> lars, rms</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> For each feature, a score is computed that can be useful
        for feature selection. Several random subsets are sampled from
        the input data and for each random subset, various linear
        models are fitted using lars method. A score is assigned to
        each feature based on the tendency of LASSO in including that
        feature in the models.Finally, the average score and the models
        are returned as the output. The features with relatively low
        scores are recommended to be ignored because they can lead to
        overfitting of the model to the training data. Moreover, for
        each random subset, the best set of features in terms of global
        error is returned. They are useful for applying Bolasso, the
        alternative feature selection method that recommends the
        intersection of features subsets.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
Repository: </td><td style="text-align: left;"> CRAN</td>
</tr>
<tr>
 <td style="text-align: left;">
Date/Publication: </td><td style="text-align: left;"> 2018-06-01 13:13:46 UTC</td>
</tr>
<tr>
 <td style="text-align: left;">
Packaged: </td><td style="text-align: left;"> 2018-06-01 00:07:37 UTC; habil</td>
</tr>
<tr>
 <td style="text-align: left;">
NeedsCompilation: </td><td style="text-align: left;"> no</td>
</tr>
<tr>
 <td style="text-align: left;">
RoxygenNote: </td><td style="text-align: left;"> 6.0.1</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
FeaLect                 Computes the scores of the features.
FeaLect-package         Scores Features for Feature Selection
compute.balanced        Balances between negative and positive samples
                        by oversampling.
compute.logistic.score
                        Fits a logistic regression model using the
                        linear scores
doctor.validate         Validates a model using validating samples.
ignore.redundant        Refines a feature matrix
input.check.FeaLect     Checks the inputs to Fealect() function.
mcl_sll                 MCL and SLL lymphoma subtypes
random.subset           Selects a random subset of the input.
train.doctor            Fits various models based on a combination on
                        penalized linear models and logistic
                        regression.
</pre>


<h3>Author(s)</h3>

<p>Habil Zare
</p>
<p>Maintainer: Habil Zare &lt;zare@u.washington.edu&gt;
</p>


<h3>References</h3>

<p>Zare, Habil, et al. &quot;Scoring relevancy of features based on combinatorial analysis of Lasso with application to lymphoma diagnosis.&quot; <em>BMC genomics</em>. Vol. 14. No. 1. BioMed Central, 2013.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeaLect">FeaLect</a></code>, <code><a href="#topic+train.doctor">train.doctor</a></code>, <code><a href="#topic+doctor.validate">doctor.validate</a></code>, 
<code><a href="#topic+random.subset">random.subset</a></code>, <code><a href="#topic+compute.balanced">compute.balanced</a></code>,<code><a href="#topic+compute.logistic.score">compute.logistic.score</a></code>, 
<code><a href="#topic+ignore.redundant">ignore.redundant</a></code>, <code><a href="#topic+input.check.FeaLect">input.check.FeaLect</a></code>,
<code><a href="lars.html#topic+lars">lars</a>-package</code>, and <code>SparseLearner-package</code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FeaLect)
data(mcl_sll)
F &lt;- as.matrix(mcl_sll[ ,-1])	# The Feature matrix
L &lt;- as.numeric(mcl_sll[ ,1])	# The labels
names(L) &lt;- rownames(F)
message(dim(F)[1], " samples and ",dim(F)[2], " features.")

## For this data, total.num.of.models is suggested to be at least 100.
FeaLect.result.1 &lt;-FeaLect(F=F,L=L,maximum.features.num=10,total.num.of.models=20,talk=TRUE)
</code></pre>

<hr>
<h2 id='ignore.redundant'>Refines a feature matrix</h2><span id='topic+ignore.redundant'></span>

<h3>Description</h3>

<p>If the value a feature is the same for all points (e.g. =0), it can be ignored.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ignore.redundant(F, num.of.values = 1)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ignore.redundant_+3A_f">F</code></td>
<td>

<p>The feature matrix, each column is a feature.</p>
</td></tr>
<tr><td><code id="ignore.redundant_+3A_num.of.values">num.of.values</code></td>
<td>

<p>A feature should have more than this threshold non-zero values not to be ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The refined feature matrix.</p>


<h3>Author(s)</h3>

<p>Habil Zare</p>


<h3>References</h3>

<p>&quot;Statistical Analysis of Overfitting Features&quot;, manuscript in preparation.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeaLect">FeaLect</a></code>, <code><a href="#topic+train.doctor">train.doctor</a></code>, <code><a href="#topic+doctor.validate">doctor.validate</a></code>, 
<code><a href="#topic+random.subset">random.subset</a></code>, <code><a href="#topic+compute.balanced">compute.balanced</a></code>,<code><a href="#topic+compute.logistic.score">compute.logistic.score</a></code>, 
<code><a href="#topic+ignore.redundant">ignore.redundant</a></code>, <code><a href="#topic+input.check.FeaLect">input.check.FeaLect</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FeaLect)
data(mcl_sll)
F &lt;- as.matrix(mcl_sll[ ,-1])	# The Feature matrix
#F &lt;- cbind(F, rep(1, times=dim(F)[1]))
message(dim(F)[1], " samples and ",dim(F)[2], " features.")

G &lt;- ignore.redundant(F)
message("for ",dim(G)[1], " samples, ",dim(G)[2], " features are left.")

</code></pre>

<hr>
<h2 id='input.check.FeaLect'>	Checks the inputs to Fealect() function.</h2><span id='topic+input.check.FeaLect'></span>

<h3>Description</h3>

<p>We should have: F as a matrix, L as a vector, and length of L be equal to number of rows of F.
They should have names accordingly. </p>


<h3>Usage</h3>

<pre><code class='language-R'>input.check.FeaLect(F_, L_, maximum.features.num, gamma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="input.check.FeaLect_+3A_f_">F_</code></td>
<td>

<p>The feature matrix, each column is a feature.</p>
</td></tr>
<tr><td><code id="input.check.FeaLect_+3A_l_">L_</code></td>
<td>

<p>The vector of labels named according to the rows of F.</p>
</td></tr>	
<tr><td><code id="input.check.FeaLect_+3A_maximum.features.num">maximum.features.num</code></td>
<td>
 
<p>Upto this number of features are allowed to contribute to each linear model.</p>
</td></tr>
<tr><td><code id="input.check.FeaLect_+3A_gamma">gamma</code></td>
<td>

<p>A value in range 0-1 that determines the relative size of sample subsets.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the input is not appropriate, error or warning message will be produced.
</p>


<h3>Value</h3>

<p>Returns a list of: 
</p>
<table>
<tr><td><code>F_</code></td>
<td>

<p>The feature matrix, each column is a feature.</p>
</td></tr>
<tr><td><code>L_</code></td>
<td>

<p>The vector of labels named according to the rows of F.</p>
</td></tr>	
<tr><td><code>maximum.features.num</code></td>
<td>
 
<p>Upto this number of features are allowed to contribute to each linear model.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Habil Zare</p>


<h3>References</h3>

<p>&quot;Statistical Analysis of Overfitting Features&quot;, manuscript in preparation.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeaLect">FeaLect</a></code>, <code><a href="#topic+train.doctor">train.doctor</a></code>, <code><a href="#topic+doctor.validate">doctor.validate</a></code>, 
<code><a href="#topic+random.subset">random.subset</a></code>, <code><a href="#topic+compute.balanced">compute.balanced</a></code>,<code><a href="#topic+compute.logistic.score">compute.logistic.score</a></code>, 
<code><a href="#topic+ignore.redundant">ignore.redundant</a></code>, <code><a href="#topic+input.check.FeaLect">input.check.FeaLect</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FeaLect)
data(mcl_sll)
F &lt;- as.matrix(mcl_sll[ ,-1])	# The Feature matrix
L &lt;- as.numeric(mcl_sll[ ,1])	# The labels
names(L) &lt;- rownames(F)

checked &lt;- input.check.FeaLect(F_=F, L_=L, maximum.features.num=10, gamma=3/4)

</code></pre>

<hr>
<h2 id='mcl_sll'>MCL and SLL lymphoma subtypes</h2><span id='topic+mcl_sll'></span>

<h3>Description</h3>

<p>A total of 237 features are identified for 22 lymphoma patients.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mcl_sll)</code></pre>


<h3>Format</h3>

<p> A matrix. Each of the 237 columns represents a features except the first column which contains the label vector.
Each of the 22 rows represents a patients.</p>


<h3>Details</h3>

<p>7 cases diagnosed with Mantel Cell Lymphoma (MCL)  and 15 cases with Small Lymphocytic Lymphoma (SLL).
The presented features are computed based on flow cytometry data
The fist column contains the label vector which has value 1 for MCL cases and 0 for SLL cases.</p>


<h3>Source</h3>

<p>British Columbia Cancer Agency</p>


<h3>References</h3>

<p>&quot;Statistical Analysis of Overfitting Features&quot;, manuscript in preparation.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeaLect">FeaLect</a></code>, <code><a href="#topic+train.doctor">train.doctor</a></code>, <code><a href="#topic+doctor.validate">doctor.validate</a></code>, 
<code><a href="#topic+random.subset">random.subset</a></code>, <code><a href="#topic+compute.balanced">compute.balanced</a></code>,<code><a href="#topic+compute.logistic.score">compute.logistic.score</a></code>, 
<code><a href="#topic+ignore.redundant">ignore.redundant</a></code>, <code><a href="#topic+input.check.FeaLect">input.check.FeaLect</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FeaLect)
data(mcl_sll)
F &lt;- as.matrix(mcl_sll[ ,-1])	# The Feature matrix
L &lt;- as.numeric(mcl_sll[ ,1])	# The labels
names(L) &lt;- rownames(F)
message(dim(F)[1], " samples and ",dim(F)[2], " features.")
L

</code></pre>

<hr>
<h2 id='random.subset'>Selects a random subset of the  input.</h2><span id='topic+random.subset'></span>

<h3>Description</h3>

<p>If a subset of samples are selected randomly, the navigate of positive classes might be too sparse or even empty. 
This function will repeat sampling until the classes are appropriate in this sense.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random.subset(F_, L_, gamma, persistence = 1000, minimum.class.size=2, replace)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="random.subset_+3A_f_">F_</code></td>
<td>

<p>The feature matrix, each column is a feature.</p>
</td></tr>
<tr><td><code id="random.subset_+3A_l_">L_</code></td>
<td>

<p>The vector of labels named according to the rows of F.</p>
</td></tr>
<tr><td><code id="random.subset_+3A_gamma">gamma</code></td>
<td>

<p>A value in range 0-1 that determines the relative size of sample subsets.</p>
</td></tr>
<tr><td><code id="random.subset_+3A_persistence">persistence</code></td>
<td>

<p>Maximum number of tries for randomly choosing.samples,
If we try this many times and the obtained labels are all the same,
we give up (maybe the whole labels are the same) with the error message: &quot; Not enough variation in the labels...&quot;.</p>
</td></tr>
<tr><td><code id="random.subset_+3A_minimum.class.size">minimum.class.size</code></td>
<td>

<p>A lower bound on the number of samples in each class.</p>
</td></tr>
<tr><td><code id="random.subset_+3A_replace">replace</code></td>
<td>

<p>If TRUE, sampling is done by replacement.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function also returns a refined feature matrix by ignoring too sparse features after sampling.</p>


<h3>Value</h3>

<p>Returns a list of: 
</p>
<table>
<tr><td><code>X_</code></td>
<td>

<p>The sampled feature matrix, each column is a feature after ignoring the redundant ones.</p>
</td></tr>
<tr><td><code>Y_</code></td>
<td>

<p>The vector of labels named according to the rows of X_.</p>
</td></tr>	
<tr><td><code>remainder.samples</code></td>
<td>
 
<p>The names of the rows of F_ which do not appear in X_, later on can be used for validation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Habil Zare</p>


<h3>References</h3>

<p>&quot;Statistical Analysis of Overfitting Features&quot;, manuscript in preparation.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeaLect">FeaLect</a></code>, <code><a href="#topic+train.doctor">train.doctor</a></code>, <code><a href="#topic+doctor.validate">doctor.validate</a></code>, 
<code><a href="#topic+random.subset">random.subset</a></code>, <code><a href="#topic+compute.balanced">compute.balanced</a></code>,<code><a href="#topic+compute.logistic.score">compute.logistic.score</a></code>, 
<code><a href="#topic+ignore.redundant">ignore.redundant</a></code>, <code><a href="#topic+input.check.FeaLect">input.check.FeaLect</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FeaLect)
data(mcl_sll)
F &lt;- as.matrix(mcl_sll[ ,-1])	# The Feature matrix
L &lt;- as.numeric(mcl_sll[ ,1])	# The labels
names(L) &lt;- rownames(F)
message(dim(F)[1], " samples and ",dim(F)[2], " features.")

XY &lt;- random.subset(F_=F, L_=L, gamma=3/4,replace=TRUE)
XY$remainder.samples

</code></pre>

<hr>
<h2 id='train.doctor'>
Fits various models based on a combination on penalized linear models and logistic regression.</h2><span id='topic+train.doctor'></span>

<h3>Description</h3>

<p>Various linear models are fitted to the training samples using lars method. 
The models differ in the number of features and each is validated by validating samples. 
A score is also assigned to each feature based on the tendency of LASSO in including that feature in the models.</p>


<h3>Usage</h3>

<pre><code class='language-R'>train.doctor(F_, L_, training.samples, validating.samples, considered.features, 
		 maximum.features.num, balance = TRUE, return_linear.models = TRUE, 
		 report.fitting.failure = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train.doctor_+3A_f_">F_</code></td>
<td>

<p>The feature matrix, each column is a feature.</p>
</td></tr>
<tr><td><code id="train.doctor_+3A_l_">L_</code></td>
<td>

<p>The vector of labels named according to the rows of F.</p>
</td></tr>
<tr><td><code id="train.doctor_+3A_training.samples">training.samples</code></td>
<td>

<p>The names of rows of F that should be considered as training samples.</p>
</td></tr>
<tr><td><code id="train.doctor_+3A_validating.samples">validating.samples</code></td>
<td>

<p>The names of rows of F that should be considered as validating samples.</p>
</td></tr>
<tr><td><code id="train.doctor_+3A_considered.features">considered.features</code></td>
<td>

<p>The names of columns of F that determine the features of interest.</p>
</td></tr>
<tr><td><code id="train.doctor_+3A_maximum.features.num">maximum.features.num</code></td>
<td>

<p>Upto this number of features are allowed to contribute to each linear model.</p>
</td></tr>
<tr><td><code id="train.doctor_+3A_balance">balance</code></td>
<td>

<p>If TRUE, the cases will be balanced for the same number of positive vs. negatives by oversampling before fitting the linear model.</p>
</td></tr>
<tr><td><code id="train.doctor_+3A_return_linear.models">return_linear.models</code></td>
<td>

<p>The models are memory intensive, so for if they more than 1000, we may decide to ignore them to prevent memory outage.</p>
</td></tr>
<tr><td><code id="train.doctor_+3A_report.fitting.failure">report.fitting.failure</code></td>
<td>

<p>If TRUE, any failure in fitting the linear of logistic models will be printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the reference for more details.</p>


<h3>Value</h3>

<p>Returns a list of: 
</p>
<table>
<tr><td><code>linear.models</code></td>
<td>
<p>The result of model fitting computed by lars().</p>
</td></tr>
<tr><td><code>best.number.of.features</code></td>
<td>
<p>According to best accuracy.</p>
</td></tr>
<tr><td><code>probabilities</code></td>
<td>
<p>The best computed logistic score.</p>
</td></tr>
<tr><td><code>accuracy</code></td>
<td>
<p>The best F-measure.</p>
</td></tr>
<tr><td><code>best.logistic.cof</code></td>
<td>
<p>According to best accuracy.</p>
</td></tr>
<tr><td><code>contribution.to.feature.scores</code></td>
<td>
<p>This vector should be added to the total feature scores.</p>
</td></tr>
<tr><td><code>contribution.to.feature.scores.frequency</code></td>
<td>
<p> This vector should be added to the total frequency of features.</p>
</td></tr>
<tr><td><code>training.samples</code></td>
<td>
<p>Input, the names of rows of F that should be considered as training samples.</p>
</td></tr>
<tr><td><code>validating.samples</code></td>
<td>
<p>Input, the names of rows of F that should be considered as validating samples.</p>
</td></tr>
<tr><td><code>precision</code></td>
<td>
<p>Ratio of number of true positives to predicted positives.</p>
</td></tr>
<tr><td><code>recall</code></td>
<td>
<p>Ratio of number of true positives to real positives.</p>
</td></tr>	
<tr><td><code>selected.features.sequence</code></td>
<td>
<p>A list of sets of features which are selected in different models.</p>
</td></tr>
<tr><td><code>global.errors</code></td>
<td>
<p>A vector of global error of the linear fits.</p>
</td></tr>
<tr><td><code>features.with.best.global.error</code></td>
<td>
<p>A vector of names of good features in terms of global error of linear fits.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Logistic regression is also done on top of fitting the linear models.</p>


<h3>Author(s)</h3>

<p>Habil Zare</p>


<h3>References</h3>

<p>&quot;Statistical Analysis of Overfitting Features&quot;, manuscript in preparation.</p>


<h3>See Also</h3>

<p><code><a href="#topic+FeaLect">FeaLect</a></code>, <code><a href="#topic+train.doctor">train.doctor</a></code>, <code><a href="#topic+doctor.validate">doctor.validate</a></code>, 
<code><a href="#topic+random.subset">random.subset</a></code>, <code><a href="#topic+compute.balanced">compute.balanced</a></code>,<code><a href="#topic+compute.logistic.score">compute.logistic.score</a></code>, 
<code><a href="#topic+ignore.redundant">ignore.redundant</a></code>, <code><a href="#topic+input.check.FeaLect">input.check.FeaLect</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FeaLect)
data(mcl_sll)
F &lt;- as.matrix(mcl_sll[ ,-1])	# The Feature matrix
L &lt;- as.numeric(mcl_sll[ ,1])	# The labels
names(L) &lt;- rownames(F)
message(dim(F)[1], " samples and ",dim(F)[2], " features.")

all.samples &lt;- rownames(F); ts &lt;- all.samples[5:10]; vs &lt;- all.samples[c(1,22)]

doctor &lt;- train.doctor(F_=F, L_=L, training.samples=ts, validating.samples=vs,
       considered.features=colnames(F), maximum.features.num=10)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
