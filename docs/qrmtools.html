<!DOCTYPE html><html><head><title>Help for package qrmtools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {qrmtools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#alloc'><p>Computing allocations</p></a></li>
<li><a href='#Black_Scholes'><p>Black&ndash;Scholes formula and the Greeks</p></a></li>
<li><a href='#Brownian'><p>Brownian and Related Motions</p></a></li>
<li><a href='#catch'><p>Catching Results, Warnings and Errors Simultaneously</p></a></li>
<li><a href='#fit_ARMA_GARCH'><p>Fitting ARMA-GARCH Processes</p></a></li>
<li><a href='#fit_GARCH_11'><p>Fast(er) and Numerically More Robust Fitting of GARCH(1,1) Processes</p></a></li>
<li><a href='#fit_GEV'><p>Parameter Estimators of the Generalized Extreme Value Distribution</p></a></li>
<li><a href='#fit_GPD'><p>Parameter Estimators of the Generalized Pareto Distribution</p></a></li>
<li><a href='#get_data'><p>Tools for Getting and Working with Data</p></a></li>
<li><a href='#GEV'><p>Generalized Extreme Value Distribution</p></a></li>
<li><a href='#GEV_shape_plot'><p>Fitted GEV Shape as a Function of the Threshold</p></a></li>
<li><a href='#GPD'><p>(Generalized) Pareto Distribution</p></a></li>
<li><a href='#GPD_shape_plot'><p>Fitted GPD Shape as a Function of the Threshold</p></a></li>
<li><a href='#GPDtail'><p>GPD-Based Tail Distribution (POT method)</p></a></li>
<li><a href='#hierarchical_matrix'><p>Construction of Hierarchical Matrices</p></a></li>
<li><a href='#Hill'><p>Hill Estimator and Plot</p></a></li>
<li><a href='#matrix_density_plota'><p>Density Plot of the Values from a Lower Triangular Matrix</p></a></li>
<li><a href='#matrix_plot'><p>Graphical Tool for Visualizing Matrices</p></a></li>
<li><a href='#mean_excess'><p>Mean Excess</p></a></li>
<li><a href='#NA_plot'><p>Graphical Tool for Visualizing NAs in a Data Set</p></a></li>
<li><a href='#pp_qq_plot'><p>P-P and Q-Q Plots</p></a></li>
<li><a href='#returns'><p>Computing Returns and Inverse Transformation</p></a></li>
<li><a href='#risk_measures'><p>Risk Measures</p></a></li>
<li><a href='#step_plot'><p>Plot of Step Functions, Empirical Distribution and Quantile Functions</p></a></li>
<li><a href='#tail_plot'><p>Plot of an Empirical Surival Function with Smith Estimator</p></a></li>
<li><a href='#tests'><p>Formal Tests of Multivariate Normality</p></a></li>
<li><a href='#VaR_ES_bounds_analytical'><p>&ldquo;Analytical&rdquo; Best and Worst Value-at-Risk for Given Marginals</p></a></li>
<li><a href='#VaR_ES_bounds_rearrange'><p>Worst and Best Value-at-Risk and Best Expected Shortfall</p>
for Given Marginals via Rearrangements</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.0-17</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools for Quantitative Risk Management</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions and data sets for reproducing selected results from
  the book "Quantitative Risk Management: Concepts, Techniques and Tools".
  Furthermore, new developments and auxiliary functions for Quantitative
  Risk Management practice.</td>
</tr>
<tr>
<td>Author:</td>
<td>Marius Hofert [aut, cre],
  Kurt Hornik [aut],
  Alexander J. McNeil [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marius Hofert &lt;mhofert@hku.hk&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, lattice, quantmod, Quandl, zoo, xts, methods,
grDevices, stats, rugarch, utils, ADGofTest</td>
</tr>
<tr>
<td>Suggests:</td>
<td>combinat, copula, qrng, sfsmisc, RColorBrewer, sn, knitr,
rmarkdown</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a> | file LICENCE</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-04 16:20:06 UTC</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-04 10:22:44 UTC; mhofert</td>
</tr>
</table>
<hr>
<h2 id='alloc'>Computing allocations</h2><span id='topic+alloc_ellip'></span><span id='topic+conditioning'></span><span id='topic+alloc_np'></span>

<h3>Description</h3>

<p>Computing (capital) allocations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## For elliptical distributions under certain assumptions
alloc_ellip(total, loc, scale)

## Nonparametrically
conditioning(x, level, risk.measure = "VaR_np", ...)
alloc_np(x, level, risk.measure = "VaR_np", include.conditional = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alloc_+3A_total">total</code></td>
<td>
<p>total to be allocated (typically the risk measure of the
sum of the underlying loss random variables).</p>
</td></tr>
<tr><td><code id="alloc_+3A_loc">loc</code></td>
<td>
<p>location vector of the elliptical distribution of the loss
random vector.</p>
</td></tr>
<tr><td><code id="alloc_+3A_scale">scale</code></td>
<td>
<p>scale (covariance) matrix of the elliptical distribution of
the loss random vector.</p>
</td></tr>
<tr><td><code id="alloc_+3A_x">x</code></td>
<td>
<p><code class="reqn">(n, d)</code>-matrix containing <code class="reqn">n</code> iid
<code class="reqn">d</code>-dimensional losses.</p>
</td></tr>
<tr><td><code id="alloc_+3A_level">level</code></td>
<td>
<p>either one or two confidence level(s) for
<code>risk.measure</code>; in the former case the upper bound on the
conditioning region is determined by confidence level 1.</p>
</td></tr>
<tr><td><code id="alloc_+3A_risk.measure">risk.measure</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string or
<code><a href="base.html#topic+function">function</a></code> specifying the risk measure to be computed
on the row sums of <code>x</code> based on the given level(s) in order
to determine the conditioning region.</p>
</td></tr>
<tr><td><code id="alloc_+3A_include.conditional">include.conditional</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether
the computed sub-sample of <code>x</code> is to be returned, too.</p>
</td></tr>
<tr><td><code id="alloc_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>risk.measure</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The result of <code>alloc_ellip()</code> for <code>loc = 0</code> can be found in
McNeil et al. (2015, Corollary 8.43). Otherwise, McNeil et al. (2015,
Theorem 8.28 (1)) can be used to derive the result.
</p>


<h3>Value</h3>

<p><code class="reqn">d</code>-vector of allocated amounts (the allocation) according to the
Euler principle under the assumption that the underlying loss random
vector follows a <code class="reqn">d</code>-dimensional elliptical distribution with
location vector <code>loc</code> (<code class="reqn">\bm{mu}</code> in the reference) and
scale matrix <code>scale</code> (<code class="reqn">\Sigma</code> in the reference, a
covariance matrix) and that the risk measure is law-invariant,
positive-homogeneous and translation invariant.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R. and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Ellipitical case ###########################################################

## Construct a covariance matrix
sig &lt;- 1:3 # standard deviations
library(copula) # for p2P() here
P &lt;- p2P(c(-0.5, 0.3, 0.5)) # (3, 3) correlation matrix
Sigma &lt;- P * sig %*% t(sig) # corresponding covariance matrix
stopifnot(all.equal(cov2cor(Sigma), P)) # sanity check

## Compute the allocation of 1.2 for a joint loss L ~ E_3(0, Sigma, psi)
AC &lt;- alloc_ellip(1.2, loc = 0, scale = Sigma) # allocated amounts
stopifnot(all.equal(sum(AC), 1.2)) # sanity check
## Be careful to check whether the aforementioned assumptions hold.


### Nonparametrically ##########################################################

## Generate data
set.seed(271)
X &lt;- qt(rCopula(1e5, copula = gumbelCopula(2, dim = 5)), df = 3.5)

## Estimate an allocation via MC based on a sub-sample whose row sums have a
## nonparametric VaR with confidence level in ...
alloc_np(X, level = 0.9) # ... (0.9, 1]
CA  &lt;- alloc_np(X, level = c(0.9, 0.95)) # ... in (0.9, 0.95]
CA. &lt;- alloc_np(X, level = c(0.9, 0.95), risk.measure = VaR_np) # providing a function
stopifnot(identical(CA, CA.))
</code></pre>

<hr>
<h2 id='Black_Scholes'>Black&ndash;Scholes formula and the Greeks</h2><span id='topic+Black_Scholes'></span><span id='topic+Black_Scholes_Greeks'></span>

<h3>Description</h3>

<p>Compute the Black&ndash;Scholes formula and the Greeks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Black_Scholes(t, S, r, sigma, K, T, type = c("call", "put"))
Black_Scholes_Greeks(t, S, r, sigma, K, T, type = c("call", "put"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Black_Scholes_+3A_t">t</code></td>
<td>
<p>initial or current time <code class="reqn">t</code> (in years).</p>
</td></tr>
<tr><td><code id="Black_Scholes_+3A_s">S</code></td>
<td>
<p>stock price at time <code class="reqn">t</code>.</p>
</td></tr>
<tr><td><code id="Black_Scholes_+3A_r">r</code></td>
<td>
<p>risk-free annual interest rate.</p>
</td></tr>
<tr><td><code id="Black_Scholes_+3A_sigma">sigma</code></td>
<td>
<p>annual volatility (standard deviation).</p>
</td></tr>
<tr><td><code id="Black_Scholes_+3A_k">K</code></td>
<td>
<p>strike.</p>
</td></tr>
<tr><td><code id="Black_Scholes_+3A_t">T</code></td>
<td>
<p>maturity (in years).</p>
</td></tr>
<tr><td><code id="Black_Scholes_+3A_type">type</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string indicating whether
a call (the default) or a put option is considered.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note again that <code>t</code> is time in years. In the context of McNeil et
al. (2015, Chapter 9), this is <code class="reqn">\tau_t = t/250</code>.
</p>


<h3>Value</h3>

<p><code>Black_Scholes()</code> returns the value of a European-style call or put
option (depending on the chosen <code>type</code>) on a non-dividend paying stock.
</p>
<p><code>Black_Scholes_Greeks()</code> returns the first-order derivatives
delta, theta, rho, vega and the second-order derivatives gamma, vanna
and vomma (depending on the chosen <code>type</code>) in this order.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R., and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>

<hr>
<h2 id='Brownian'>Brownian and Related Motions</h2><span id='topic+rBrownian'></span><span id='topic+deBrowning'></span>

<h3>Description</h3>

<p>Simulate paths of dependent Brownian motions, geometric
Brownian motions and Brownian bridges based on given increment
copula samples. And extract copula increments from paths of
dependent Brownian motions and geometric Brownian motions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rBrownian(N, t, d = 1, U = matrix(runif(N * n * d), ncol = d),
          drift = 0, vola = 1, type = c("BM", "GBM", "BB"), init = 1)
deBrowning(x, t, drift = 0, vola = 1, type = c("BM", "GBM"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Brownian_+3A_n">N</code></td>
<td>
<p>number <code class="reqn">N</code> of paths to simulate (positive integer).</p>
</td></tr>
<tr><td><code id="Brownian_+3A_x">x</code></td>
<td>
<p><code class="reqn">n+1</code>-vector containing one path of the specified
stochastic process or <code class="reqn">(n+1, d)</code>-matrix containing one path of
the specified <code class="reqn">d</code> stochastic processes or <code class="reqn">(N, n+1, d)</code>-array
containing <code class="reqn">N</code> paths of the specified <code class="reqn">d</code> stochastic processes.</p>
</td></tr>
<tr><td><code id="Brownian_+3A_t">t</code></td>
<td>
<p><code class="reqn">n+1</code>-vector of the form
<code class="reqn">(t_0,\dots,t_n)</code>
with <code class="reqn">0 = t_0 &lt; \dots &lt; t_n</code>
containing the time points where the stochastic processes are considered.</p>
</td></tr>
<tr><td><code id="Brownian_+3A_d">d</code></td>
<td>
<p>number <code class="reqn">d</code> of stochastic processes to simulate (positive
integer).</p>
</td></tr>
<tr><td><code id="Brownian_+3A_u">U</code></td>
<td>
<p><code class="reqn">(N\cdot n, d)</code>-<code><a href="base.html#topic+matrix">matrix</a></code> of copula
realizations to be converted to the joint increments of
the stochastic processes.</p>
</td></tr>
<tr><td><code id="Brownian_+3A_drift">drift</code></td>
<td>
<p><code class="reqn">d</code>-vector or number
(then recycled to a <code class="reqn">d</code>-vector) of drifts
(typically denoted by <code class="reqn">\mu</code>). Note that risk-neutral
drifts are <code class="reqn">r - \sigma^2/2</code>, where <code class="reqn">r</code>
is the risk-free interest rate and <code class="reqn">\sigma</code>
the volatility.</p>
</td></tr>
<tr><td><code id="Brownian_+3A_vola">vola</code></td>
<td>
<p><code class="reqn">d</code>-vector or number (then recycled to a <code class="reqn">d</code>-vector)
of volatilities (typically denoted by <code class="reqn">\sigma</code>).</p>
</td></tr>
<tr><td><code id="Brownian_+3A_type">type</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string indicating whether
a Brownian motion (<code>"BM"</code>), geometric Brownian motion
(<code>"GBM"</code>) or Brownian bridge (<code>"BB"</code>) is to be considered.</p>
</td></tr>
<tr><td><code id="Brownian_+3A_init">init</code></td>
<td>
<p><code class="reqn">d</code>-vector or number (then recycled to a
<code class="reqn">d</code>-vector) of initial values (typically stock prices at time 0)
for <code>type = "GBM"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>rBrownian()</code> returns an <code class="reqn">(N, n+1, d)</code>-array containing
the <code class="reqn">N</code> paths of the specified <code class="reqn">d</code>
stochastic processes simulated at the <code class="reqn">n+1</code> time points
(<code class="reqn">t_0 = 0, t_1,\dots, t_n</code>).
</p>
<p><code>deBrowning()</code> returns an <code class="reqn">(N, n, d)</code>-array containing
the <code class="reqn">N</code> paths of the copula increments of the <code class="reqn">d</code> stochastic
processes over the <code class="reqn">n+1</code> time points
(<code class="reqn">t_0 = 0, t_1,\dots, t_n</code>).
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Setup
d &lt;- 3 # dimension
library(copula)
tcop &lt;- tCopula(iTau(tCopula(), tau = 0.5), dim = d, df = 4) # t_4 copula
vola &lt;- seq(0.05, 0.20, length.out = d) # volatilities sigma
r &lt;- 0.01 # risk-free interest rate
drift &lt;- r - vola^2/2 # marginal drifts
init &lt;- seq(10, 100, length.out = d) # initial stock prices
N &lt;- 100 # number of replications
n &lt;- 25 # number of time intervals
t &lt;- 0:n/n # time points 0 = t_0 &lt; ... &lt; t_n

## Simulate N paths of a cross-sectionally dependent d-dimensional
## (geometric) Brownian motion ((G)BM) over n time steps
set.seed(271)
U &lt;- rCopula(N * n, copula = tcop) # for dependent increments
X &lt;- rBrownian(N, t = t, d = d, U = U, drift = drift, vola = vola) # BM
S &lt;- rBrownian(N, t = t, d = d, U = U, drift = drift, vola = vola,
               type = "GBM", init = init) # GBM
stopifnot(dim(X) == c(N, n+1, d), dim(S) == c(N, n+1, d))

## DeBrowning
Z.X &lt;- deBrowning(X, t = t, drift = drift, vola = vola) # BM
Z.S &lt;- deBrowning(S, t = t, drift = drift, vola = vola, type = "GBM") # GBM
stopifnot(dim(Z.X) == c(N, n, d), dim(Z.S) == c(N, n, d))
## Note that for BMs, one loses one observation as X_{t_0} = 0 (or some other
## fixed value, so there is no random increment there that can be deBrowned.


    ## If we map the increments back to their copula sample, do we indeed
    ## see the copula samples again?
    U.Z.X &lt;- pnorm(Z.X) # map to copula sample
    U.Z.S &lt;- pnorm(Z.S) # map to copula sample
    stopifnot(all.equal(U.Z.X, U.Z.S)) # sanity check
    ## Visual check
    pairs(U.Z.X[,1,], gap = 0) # check at the first time point of the BM
    pairs(U.Z.X[,n,], gap = 0) # check at the last time point of the BM
    pairs(U.Z.S[,1,], gap = 0) # check at the first time point of the GBM
    pairs(U.Z.S[,n,], gap = 0) # check at the last time point of the GBM
    ## Numerical check
    ## First convert the (N * n, d)-matrix U to an (N, n, d)-array but in
    ## the right way (array(U, dim = c(N, n, d)) would use the U's in the
    ## wrong order)
    U. &lt;- aperm(array(U, dim = c(n, N, d)), perm = c(2,1,3))
    ## Now compare
    stopifnot(all.equal(U.Z.X, U., check.attributes = FALSE))
    stopifnot(all.equal(U.Z.S, U., check.attributes = FALSE))



    ## Generate dependent GBM sample paths with quasi-random numbers
    library(qrng)
    set.seed(271)
    U.. &lt;- cCopula(to_array(sobol(N, d = d * n, randomize = "digital.shift"), f = n),
                   copula = tcop, inverse = TRUE)
    S. &lt;- rBrownian(N, t = t, d = d, U = U.., drift = drift, vola = vola,
                    type = "GBM", init = init)
    pairs(S [,2,], gap = 0) # pseudo-samples at t_1
    pairs(S.[,2,], gap = 0) # quasi-samples at t_1
    pairs(S [,n+1,], gap = 0) # pseudo-samples at t_n
    pairs(S.[,n+1,], gap = 0) # quasi-samples at t_n



    ## Generate paths from a Brownian bridge
    B &lt;- rBrownian(N, t = t, type = "BB")
    plot(NA, xlim = 0:1, ylim = range(B),
         xlab = "Time t", ylab = expression("Brownian bridge path"~(B[t])))
    for(i in 1:N)
        lines(t, B[i,,], col = adjustcolor("black", alpha.f = 25/N))

</code></pre>

<hr>
<h2 id='catch'>Catching Results, Warnings and Errors Simultaneously</h2><span id='topic+catch'></span>

<h3>Description</h3>

<p>Catches results, warnings and errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>catch(expr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="catch_+3A_expr">expr</code></td>
<td>
<p>expression to be evaluated, typically a function call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is particularly useful for large(r) simulation studies
to not fail until finished.
</p>


<h3>Value</h3>

<p><code><a href="base.html#topic+list">list</a></code> with components:
</p>
<table>
<tr><td><code>value</code></td>
<td>
<p>value of <code>expr</code> or <code>NULL</code> in case of
an error.</p>
</td></tr>
<tr><td><code>warning</code></td>
<td>
<p>warning message (see <code><a href="base.html#topic+simpleWarning">simpleWarning</a></code> or
<code><a href="base.html#topic+warning">warning</a>()</code>) or <code><a href="base.html#topic+NULL">NULL</a></code> in case of no warning.</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>error message (see <code><a href="base.html#topic+simpleError">simpleError</a></code> or
<code><a href="base.html#topic+stop">stop</a>()</code>) or <code><a href="base.html#topic+NULL">NULL</a></code> in case of no error.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marius Hofert (based on <code>doCallWE()</code> and <code>tryCatch.W.E()</code> in
the <span class="rlang"><b>R</b></span> package <span class="pkg">simsalapar</span>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>catch(log(2))
catch(log(-1))
catch(log("a"))
</code></pre>

<hr>
<h2 id='fit_ARMA_GARCH'>Fitting ARMA-GARCH Processes</h2><span id='topic+fit_ARMA_GARCH'></span>

<h3>Description</h3>

<p>Fail-safe componentwise fitting of univariate ARMA-GARCH processes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_ARMA_GARCH(x, ugarchspec.list = ugarchspec(), solver = "hybrid",
               verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_ARMA_GARCH_+3A_x">x</code></td>
<td>
<p><code><a href="base.html#topic+matrix">matrix</a></code>-like data structure, possibly an
<code>xts</code> object.</p>
</td></tr>
<tr><td><code id="fit_ARMA_GARCH_+3A_ugarchspec.list">ugarchspec.list</code></td>
<td>
<p>object of class <code>uGARCHspec</code> (as returned by
<code>ugarchspec()</code>) or a list of such. In case of a list, its
length has to be equal to the number of columns of
<code>x</code>. <code>ugarchspec.list</code> provides the ARMA-GARCH
specifications for each of the time series (columns of <code>x</code>).</p>
</td></tr>
<tr><td><code id="fit_ARMA_GARCH_+3A_solver">solver</code></td>
<td>
<p>string indicating the solver used; see <code>?ugarchfit</code>.</p>
</td></tr>
<tr><td><code id="fit_ARMA_GARCH_+3A_verbose">verbose</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether verbose
output is given.</p>
</td></tr>
<tr><td><code id="fit_ARMA_GARCH_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="rugarch.html#topic+ugarchfit">ugarchfit</a>()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>x</code> consists of one column only (e.g. a vector),
<code>ARMA_GARCH()</code> returns the fitted object; otherwise it returns
a list of such.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit_GARCH_11">fit_GARCH_11</a>()</code> for fast(er) and numerically more
robust fitting of GARCH(1,1) processes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(rugarch)
library(copula)

## Read the data, build -log-returns
data(SMI.12) # Swiss Market Index data
stocks &lt;- c("CSGN", "BAER", "UBSN", "SREN", "ZURN") # components we work with
x &lt;- SMI.12[, stocks]
X &lt;- -returns(x)
n &lt;- nrow(X)
d &lt;- ncol(X)

## Fit ARMA-GARCH models to the -log-returns
## Note: - Our choice here is purely for demonstration purposes.
##         The models are not necessarily adequate
##       - The sample size n is *too* small here for properly capturing GARCH effects.
##         Again, this is only for demonstration purposes here.
uspec &lt;- c(rep(list(ugarchspec(distribution.model = "std")), d-2), # ARMA(1,1)-GARCH(1,1)
           list(ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(2,2)),
                           distribution.model = "std")),
           list(ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(2,1)),
                           mean.model = list(armaOrder = c(1,2), include.mean = TRUE),
                           distribution.model = "std")))
system.time(fitAG &lt;- fit_ARMA_GARCH(X, ugarchspec.list = uspec))
str(fitAG, max.level = 1) # list with components fit, warning, error
## Now access the list to check

## Not run: 
## Pick out the standardized residuals, plot them and fit a t copula to them
## Note: ugarchsim() needs the residuals to be standardized; working with
##       standardize = FALSE still requires to simulate them from the
##       respective standardized marginal distribution functions.
Z &lt;- sapply(fitAG$fit, residuals, standardize = TRUE)
U &lt;- pobs(Z)
pairs(U, gap = 0)
system.time(fitC &lt;- fitCopula(tCopula(dim = d, dispstr = "un"), data = U,
                              method = "mpl"))

## Simulate (standardized) Z
set.seed(271)
U. &lt;- rCopula(n, fitC@copula) # simulate from the fitted copula
nu &lt;- sapply(1:d, function(j) fitAG$fit[[j]]@fit$coef["shape"]) # extract (fitted) d.o.f. nu
Z. &lt;- sapply(1:d, function(j) sqrt((nu[j]-2)/nu[j]) * qt(U.[,j], df = nu[j])) # Z

## Simulate from fitted model
X. &lt;- sapply(1:d, function(j)
    fitted(ugarchsim(fitAG$fit[[j]], n.sim = n, m.sim = 1, startMethod = "sample",
                     rseed = 271, custom.dist = list(name = "sample",
                                                     distfit = Z.[,j, drop = FALSE]))))

## Plots original vs simulated -log-returns
opar &lt;- par(no.readonly = TRUE)
layout(matrix(1:(2*d), ncol = d)) # layout
ran &lt;- range(X, X.)
for(j in 1:d) {
    plot(X[,j],  type = "l", ylim = ran, ylab = paste(stocks[j], "-log-returns"))
    plot(X.[,j], type = "l", ylim = ran, ylab = "Simulated -log-returns")
}
    par(opar)

## End(Not run)
</code></pre>

<hr>
<h2 id='fit_GARCH_11'>Fast(er) and Numerically More Robust Fitting of GARCH(1,1) Processes</h2><span id='topic+fit_GARCH_11'></span><span id='topic+tail_index_GARCH_11'></span>

<h3>Description</h3>

<p>Fast(er) and numerically more robust fitting of GARCH(1,1) processes
according to Zumbach (2000).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_GARCH_11(x, init = NULL, sig2 = mean(x^2), delta = 1,
             distr = c("norm", "st"), control = list(), ...)
tail_index_GARCH_11(innovations, alpha1, beta1,
                    interval = c(1e-6, 1e2), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_GARCH_11_+3A_x">x</code></td>
<td>
<p>vector of length <code class="reqn">n</code> containing the data (typically
log-returns) to be fitted a GARCH(1,1) to.</p>
</td></tr>
<tr><td><code id="fit_GARCH_11_+3A_init">init</code></td>
<td>
<p>vector of length 2 giving the initial values for the
likelihood fitting. Note that these are initial values for
<code class="reqn">z_{corr}</code> and <code class="reqn">z_{ema}</code>
as in Zumbach (2000).</p>
</td></tr>
<tr><td><code id="fit_GARCH_11_+3A_sig2">sig2</code></td>
<td>
<p>annualized variance (third parameter of the
reparameterization according to Zumbach (2000)).</p>
</td></tr>
<tr><td><code id="fit_GARCH_11_+3A_delta">delta</code></td>
<td>
<p>unit of time (defaults to 1 meaning daily data;
for yearly data, use 250).</p>
</td></tr>
<tr><td><code id="fit_GARCH_11_+3A_distr">distr</code></td>
<td>
<p>character string specifying the innovation distribution
(<code>"norm"</code> for N(0,1) or <code>"st"</code> for a standardized <code class="reqn">t</code>
distribution).</p>
</td></tr>
<tr><td><code id="fit_GARCH_11_+3A_control">control</code></td>
<td>
<p>see <code>?<a href="stats.html#topic+optim">optim</a>()</code>.</p>
</td></tr>
<tr><td><code id="fit_GARCH_11_+3A_innovations">innovations</code></td>
<td>
<p>random variates from the innovation distribution;
for example, obtained via <code><a href="stats.html#topic+rnorm">rnorm</a>()</code> or
<code><a href="stats.html#topic+rt">rt</a>(, df = nu) * sqrt((nu-2)/nu)</code> where <code>nu</code> are
the d.o.f. of the <code class="reqn">t</code> distribution.</p>
</td></tr>
<tr><td><code id="fit_GARCH_11_+3A_alpha1">alpha1</code></td>
<td>
<p>nonnegative GARCH(1,1) coefficient <code class="reqn">alpha[1]</code>
satisfying <code class="reqn">alpha[1] + beta[1] &lt; 1</code>.</p>
</td></tr>
<tr><td><code id="fit_GARCH_11_+3A_beta1">beta1</code></td>
<td>
<p>nonnegative GARCH(1,1) coefficient <code class="reqn">beta[1]</code>
satisfying <code class="reqn">alpha[1] + beta[1] &lt; 1</code>.</p>
</td></tr>
<tr><td><code id="fit_GARCH_11_+3A_interval">interval</code></td>
<td>
<p>initial interval for computing the tail index;
passed to the underlying <code><a href="stats.html#topic+uniroot">uniroot</a>()</code>.</p>
</td></tr>
<tr><td><code id="fit_GARCH_11_+3A_...">...</code></td>
<td>


<dl>
<dt><code>fit_GARCH_11()</code>:</dt><dd><p>additional arguments passed to the
underlying <code><a href="stats.html#topic+optim">optim</a>()</code>.</p>
</dd>
<dt><code>tail_index_GARCH_11()</code>:</dt><dd><p>additional arguments passed
to the underlying <code><a href="stats.html#topic+uniroot">uniroot</a>()</code>.</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Value</h3>


<dl>
<dt><code>fit_GARCH_11()</code>:<br /></dt><dd>

<dl>
<dt>coef:</dt><dd><p>estimated coefficients <code class="reqn">\alpha_0</code>,
<code class="reqn">\alpha_1</code>, <code class="reqn">\beta_1</code> and, if
<code>distr = "st"</code> the estimated degrees of freedom.</p>
</dd>
<dt>logLik:</dt><dd><p>maximized log-likelihood.</p>
</dd>
<dt>counts:</dt><dd><p>number of calls to the objective function; see
<code>?<a href="stats.html#topic+optim">optim</a></code>.</p>
</dd>
<dt>convergence:</dt><dd><p>convergence code ('0' indicates successful
completion); see <code>?<a href="stats.html#topic+optim">optim</a></code>.</p>
</dd>
<dt>message:</dt><dd><p>see <code>?<a href="stats.html#topic+optim">optim</a></code>.</p>
</dd>
<dt>sig.t:</dt><dd><p>vector of length <code class="reqn">n</code> giving the conditional
volatility.</p>
</dd>
<dt>Z.t:</dt><dd><p>vector of length <code class="reqn">n</code> giving the standardized
residuals.</p>
</dd>
</dl>
</dd>
<dt><code>tail_index_GARCH_11()</code>:</dt><dd>
<p>The tail index <code class="reqn">alpha</code> estimated by Monte Carlo via
McNeil et al. (2015, p. 576), so the <code class="reqn">alpha</code> which solves
</p>
<p style="text-align: center;"><code class="reqn">E({(\alpha_1Z^2 + \beta_1)}^{\alpha/2}) = 1</code>
</p>
<p>,
where <code class="reqn">Z</code> are the <code>innovations</code>. If no solution
is found (e.g. if the objective function does not have
different sign at the endpoints of <code>interval</code>),
<code><a href="base.html#topic+NA">NA</a></code> is returned.
</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>Zumbach, G. (2000). The pitfalls in fitting GARCH (1,1) processes.
<em>Advances in Quantitative Asset Management</em> <b>1</b>, 179&ndash;200.
</p>
<p>McNeil, A. J., Frey, R. and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit_ARMA_GARCH">fit_ARMA_GARCH</a>()</code> based on <span class="pkg">rugarch</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example 1: N(0,1) innovations ##############################################

## Generate data from a GARCH(1,1) with N(0,1) innovations
library(rugarch)
uspec &lt;- ugarchspec(variance.model = list(model = "sGARCH",
                                          garchOrder = c(1, 1)),
                    distribution.model = "norm",
                    mean.model = list(armaOrder = c(0, 0)),
                    fixed.pars = list(mu = 0,
                                      omega = 0.1, # alpha_0
                                      alpha1 = 0.2, # alpha_1
                                      beta1 = 0.3)) # beta_1
X &lt;- ugarchpath(uspec, n.sim = 1e4, rseed = 271) # sample (set.seed() fails!)
X.t &lt;- as.numeric(X@path$seriesSim) # actual path (X_t)

## Fitting via ugarchfit()
uspec. &lt;- ugarchspec(variance.model = list(model = "sGARCH",
                                           garchOrder = c(1, 1)),
                     distribution.model = "norm",
                     mean.model = list(armaOrder = c(0, 0)))
fit &lt;- ugarchfit(uspec., data = X.t)
coef(fit) # fitted mu, alpha_0, alpha_1, beta_1
Z &lt;- fit@fit$z # standardized residuals
stopifnot(all.equal(mean(Z), 0, tol = 1e-2),
          all.equal(var(Z),  1, tol = 1e-3))

## Fitting via fit_GARCH_11()
fit. &lt;- fit_GARCH_11(X.t)
fit.$coef # fitted alpha_0, alpha_1, beta_1
Z. &lt;- fit.$Z.t # standardized residuals
stopifnot(all.equal(mean(Z.), 0, tol = 5e-3),
          all.equal(var(Z.),  1, tol = 1e-3))

## Compare
stopifnot(all.equal(fit.$coef, coef(fit)[c("omega", "alpha1", "beta1")],
                    tol = 5e-3, check.attributes = FALSE)) # fitted coefficients
summary(Z. - Z) # standardized residuals


### Example 2: t_nu(0, (nu-2)/nu) innovations ##################################

## Generate data from a GARCH(1,1) with t_nu(0, (nu-2)/nu) innovations
uspec &lt;- ugarchspec(variance.model = list(model = "sGARCH",
                                          garchOrder = c(1, 1)),
                    distribution.model = "std",
                    mean.model = list(armaOrder = c(0, 0)),
                    fixed.pars = list(mu = 0,
                                      omega = 0.1, # alpha_0
                                      alpha1 = 0.2, # alpha_1
                                      beta1 = 0.3, # beta_1
                                      shape = 4)) # nu
X &lt;- ugarchpath(uspec, n.sim = 1e4, rseed = 271) # sample (set.seed() fails!)
X.t &lt;- as.numeric(X@path$seriesSim) # actual path (X_t)

## Fitting via ugarchfit()
uspec. &lt;- ugarchspec(variance.model = list(model = "sGARCH",
                                           garchOrder = c(1, 1)),
                     distribution.model = "std",
                     mean.model = list(armaOrder = c(0, 0)))
fit &lt;- ugarchfit(uspec., data = X.t)
coef(fit) # fitted mu, alpha_0, alpha_1, beta_1, nu
Z &lt;- fit@fit$z # standardized residuals
stopifnot(all.equal(mean(Z), 0, tol = 1e-2),
          all.equal(var(Z),  1, tol = 5e-2))

## Fitting via fit_GARCH_11()
fit. &lt;- fit_GARCH_11(X.t, distr = "st")
c(fit.$coef, fit.$df) # fitted alpha_0, alpha_1, beta_1, nu
Z. &lt;- fit.$Z.t # standardized residuals
stopifnot(all.equal(mean(Z.), 0, tol = 2e-2),
          all.equal(var(Z.),  1, tol = 2e-2))

## Compare
fit.coef &lt;- coef(fit)[c("omega", "alpha1", "beta1", "shape")]
fit..coef &lt;- c(fit.$coef, fit.$df)
stopifnot(all.equal(fit.coef, fit..coef, tol = 7e-2, check.attributes = FALSE))
summary(Z. - Z) # standardized residuals
</code></pre>

<hr>
<h2 id='fit_GEV'>Parameter Estimators of the Generalized Extreme Value Distribution</h2><span id='topic+fit_GEV_quantile'></span><span id='topic+fit_GEV_PWM'></span><span id='topic+logLik_GEV'></span><span id='topic+fit_GEV_MLE'></span>

<h3>Description</h3>

<p>Quantile matching estimator, probability weighted moments estimator,
log-likelihood and maximum-likelihood estimator for the parameters of
the generalized extreme value distribution (GEV).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_GEV_quantile(x, p = c(0.25, 0.5, 0.75), cutoff = 3)
fit_GEV_PWM(x)

logLik_GEV(param, x)
fit_GEV_MLE(x, init = c("shape0", "PWM", "quantile"),
            estimate.cov = TRUE, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_GEV_+3A_x">x</code></td>
<td>
<p>numeric vector of data. In the block maxima method, these are the
block maxima.</p>
</td></tr>
<tr><td><code id="fit_GEV_+3A_p">p</code></td>
<td>
<p><code>numeric(3)</code> specifying the probabilities whose quantiles are
matched.</p>
</td></tr>
<tr><td><code id="fit_GEV_+3A_cutoff">cutoff</code></td>
<td>
<p>positive <code class="reqn">z</code> after which <code class="reqn">\exp(-z)</code>
is truncated to 0.</p>
</td></tr>
<tr><td><code id="fit_GEV_+3A_param">param</code></td>
<td>
<p><code>numeric(3)</code> containing the value of the shape
<code class="reqn">\xi</code> (a real), location <code class="reqn">\mu</code> (a real) and scale
<code class="reqn">\sigma</code> (positive real) parameters of the GEV
distribution in this order.</p>
</td></tr>
<tr><td><code id="fit_GEV_+3A_init">init</code></td>
<td>
<p><code>character</code> string specifying the method for
computing initial values. Can also be <code>numeric(3)</code>
for directly providing <code class="reqn">\xi</code>, <code class="reqn">\mu</code>, <code class="reqn">\sigma</code>.</p>
</td></tr>
<tr><td><code id="fit_GEV_+3A_estimate.cov">estimate.cov</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether the
asymptotic covariance matrix of the parameter estimators is to be
estimated (inverse of observed Fisher information (negative Hessian
of log-likelihood evaluated at MLE)) and standard errors for the
estimators of <code class="reqn">\xi</code>, <code class="reqn">\mu</code>, <code class="reqn">\sigma</code>
returned, too.</p>
</td></tr>
<tr><td><code id="fit_GEV_+3A_control">control</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code>; passed to the underlying
<code><a href="stats.html#topic+optim">optim</a>()</code>.</p>
</td></tr>
<tr><td><code id="fit_GEV_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="stats.html#topic+optim">optim</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fit_GEV_quantile()</code> matches the empirical <code>p</code>-quantiles.
</p>
<p><code>fit_GEV_PWM()</code> computes the probability weighted moments (PWM)
estimator of Hosking et al. (1985); see also Landwehr and Wallis (1979).
</p>
<p><code>fit_GEV_MLE()</code> uses, as default, the case <code class="reqn">\xi = 0</code>
for computing initial values; this is actually a small positive value
since Nelder&ndash;Mead could fail otherwise. For the other available
methods for computing initial values, <code class="reqn">\sigma</code> (obtained
from the case <code class="reqn">\xi = 0</code>) is doubled in order to guarantee
a finite log-likelihood at the initial values. After several
experiments (see the source code), one can safely say that finding
initial values for fitting GEVs via MLE is non-trivial; see also the
block maxima method script about the Black Monday event on
<a href="https://qrmtutorial.org">https://qrmtutorial.org</a>.
</p>
<p>Caution: See Coles (2001, p. 55) for how to interpret <code class="reqn">\xi\le
    -0.5</code>; in particular, the standard asymptotic properties
of the MLE do not apply.
</p>


<h3>Value</h3>

<p><code>fit_GEV_quantile()</code> and <code>fit_GEV_PWM()</code> return a
<code>numeric(3)</code> giving the parameter estimates for the GEV
distribution.
</p>
<p><code>logLik_GEV()</code> computes the log-likelihood of the GEV
distribution (<code>-Inf</code> if not admissible).
</p>
<p><code>fit_GEV_MLE()</code> returns the return object of <code><a href="stats.html#topic+optim">optim</a>()</code>
(by default, the return value <code>value</code> is the log-likelihood) and,
appended, the estimated asymptotic covariance matrix and
standard errors of the parameter estimators, if <code>estimate.cov</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R. and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>
<p>Hosking, J. R. M., Wallis, J. R. and Wood, E. F. (1985).
Estimation of the Generalized Extreme-Value Distribution by the
Method of Probability-Weighted Moments. <em>Technometrics</em>
<b>27</b>(3), 251&ndash;261.
</p>
<p>Landwehr, J. M. and Wallis, J. R. (1979).
Probability Weighted Moments Compared With Some Traditional
Techniques in Estimating Gumbel Parameters and Quantiles.
<em>Water Resourches Research</em> <b>15</b>(5), 1055&ndash;1064.
</p>
<p>Coles, S. (2001).
<em>An Introduction to Statistical Modeling of Extreme Values</em>.
Springer-Verlag.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Simulate some data
xi &lt;- 0.5
mu &lt;- -2
sig &lt;- 3
n &lt;- 1000
set.seed(271)
X &lt;- rGEV(n, shape = xi, loc = mu, scale = sig)

## Fitting via matching quantiles
(fit.q &lt;- fit_GEV_quantile(X))
stopifnot(all.equal(fit.q[["shape"]], xi,  tol = 0.12),
          all.equal(fit.q[["loc"]],   mu,  tol = 0.12),
          all.equal(fit.q[["scale"]], sig, tol = 0.005))

## Fitting via PWMs
(fit.PWM &lt;- fit_GEV_PWM(X))
stopifnot(all.equal(fit.PWM[["shape"]], xi,  tol = 0.16),
          all.equal(fit.PWM[["loc"]],   mu,  tol = 0.15),
          all.equal(fit.PWM[["scale"]], sig, tol = 0.08))

## Fitting via MLE
(fit.MLE &lt;- fit_GEV_MLE(X))
(est &lt;- fit.MLE$par) # estimates of xi, mu, sigma
stopifnot(all.equal(est[["shape"]], xi,  tol = 0.07),
          all.equal(est[["loc"]],   mu,  tol = 0.12),
          all.equal(est[["scale"]], sig, tol = 0.06))
fit.MLE$SE # estimated asymp. variances of MLEs = std. errors of MLEs

## Plot the log-likelihood in the shape parameter xi for fixed
## location mu and scale sigma (fixed as generated)
xi. &lt;- seq(-0.1, 0.8, length.out = 65)
logLik &lt;- sapply(xi., function(xi..) logLik_GEV(c(xi.., mu, sig), x = X))
plot(xi., logLik, type = "l", xlab = expression(xi),
     ylab = expression("GEV distribution log-likelihood for fixed"~mu~"and"~sigma))
## =&gt; Numerically quite challenging (for this seed!)

## Plot the profile likelihood for these xi's
## Note: As initial values for the nuisance parameters mu, sigma, we
##       use their values in the case xi = 0 (for all fixed xi = xi.,
##       in particular those xi != 0). Furthermore, for the given data X
##       and xi = xi., we make sure the initial value for sigma is so large
##       that the density is not 0 and thus the log-likelihood is finite.
pLL &lt;- sapply(xi., function(xi..) {
    scale.init &lt;- sqrt(6 * var(X)) / pi
    loc.init &lt;- mean(X) - scale.init * 0.5772157
    while(!is.finite(logLik_GEV(c(xi.., loc.init, scale.init), x = X)) &amp;&amp;
          is.finite(scale.init)) scale.init &lt;- scale.init * 2
    optim(c(loc.init, scale.init), fn = function(nuis)
                logLik_GEV(c(xi.., nuis), x = X),
    		        control = list(fnscale = -1))$value
})
plot(xi., pLL, type = "l", xlab = expression(xi),
     ylab = "GEV distribution profile log-likelihood")
</code></pre>

<hr>
<h2 id='fit_GPD'>Parameter Estimators of the Generalized Pareto Distribution</h2><span id='topic+fit_GPD_MOM'></span><span id='topic+fit_GPD_PWM'></span><span id='topic+logLik_GPD'></span><span id='topic+fit_GPD_MLE'></span>

<h3>Description</h3>

<p>Method-of-moments estimator, probability weighted moments estimator,
log-likelihood and maximum-likelihood estimator for the parameters of
the generalized Pareto distribution (GPD).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_GPD_MOM(x)
fit_GPD_PWM(x)

logLik_GPD(param, x)
fit_GPD_MLE(x, init = c("PWM", "MOM", "shape0"),
            estimate.cov = TRUE, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_GPD_+3A_x">x</code></td>
<td>
<p>numeric vector of data. In the peaks-over-threshold method,
these are the excesses (exceedances minus threshold).</p>
</td></tr>
<tr><td><code id="fit_GPD_+3A_param">param</code></td>
<td>
<p><code>numeric(2)</code> containing the value of the
shape <code class="reqn">\xi</code> (a real) and scale <code class="reqn">\beta</code>
(positive real) parameters of the GPD in this order.</p>
</td></tr>
<tr><td><code id="fit_GPD_+3A_init">init</code></td>
<td>
<p><code>character</code> string specifying the method for
computing initial values. Can also be <code>numeric(2)</code>
for directly providing <code class="reqn">\xi</code> and <code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code id="fit_GPD_+3A_estimate.cov">estimate.cov</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether the
asymptotic covariance matrix of the parameter estimators is to be
estimated (inverse of observed Fisher information (negative Hessian
of log-likelihood evaluated at MLE)) and standard errors for the
estimators of <code class="reqn">\xi</code> and <code class="reqn">\beta</code>
returned, too.</p>
</td></tr>
<tr><td><code id="fit_GPD_+3A_control">control</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code>; passed to the underlying
<code><a href="stats.html#topic+optim">optim</a>()</code>.</p>
</td></tr>
<tr><td><code id="fit_GPD_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="stats.html#topic+optim">optim</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fit_GPD_MOM()</code> computes the method-of-moments (MOM) estimator.
</p>
<p><code>fit_GPD_PWM()</code> computes the probability weighted moments (PWM)
estimator of Hosking and Wallis (1987); see also Landwehr et al. (1979).
</p>
<p><code>fit_GPD_MLE()</code> uses, as default, <code>fit_GPD_PWM()</code> for
computing initial values. The former requires the data <code>x</code>
to be non-negative and adjusts <code class="reqn">\beta</code> if <code class="reqn">\xi</code> is
negative, so that the log-likelihood at the initial value should be
finite.
</p>


<h3>Value</h3>

<p><code>fit_GEV_MOM()</code> and <code>fit_GEV_PWM()</code> return a
<code>numeric(3)</code> giving the parameter estimates for the GPD.
</p>
<p><code>logLik_GPD()</code> computes the log-likelihood of the GPD
(<code>-Inf</code> if not admissible).
</p>
<p><code>fit_GPD_MLE()</code> returns the return object of <code><a href="stats.html#topic+optim">optim</a>()</code>
and, appended, the estimated asymptotic covariance matrix and
standard errors of the parameter estimators, if <code>estimate.cov</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R. and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>
<p>Hosking, J. R. M. and Wallis, J. R. (1987).
Parameter and Quantile Estimation for the Generalized Pareto Distribution.
<em>Technometrics</em> <b>29</b>(3), 339&ndash;349.
</p>
<p>Landwehr, J. M., Matalas, N. C. and Wallis, J. R. (1979).
Estimation of Parameters and Quantiles of Wakeby Distributions.
<em>Water Resourches Research</em> <b>15</b>(6), 1361&ndash;1379.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Simulate some data
xi &lt;- 0.5
beta &lt;- 3
n &lt;- 1000
set.seed(271)
X &lt;- rGPD(n, shape = xi, scale = beta)

## Fitting via matching moments
(fit.MOM &lt;- fit_GPD_MOM(X))
stopifnot(all.equal(fit.MOM[["shape"]], xi,   tol = 0.52),
          all.equal(fit.MOM[["scale"]], beta, tol = 0.24))

## Fitting via PWMs
(fit.PWM &lt;- fit_GPD_PWM(X))
stopifnot(all.equal(fit.PWM[["shape"]], xi,   tol = 0.2),
          all.equal(fit.PWM[["scale"]], beta, tol = 0.12))

## Fitting via MLE
(fit.MLE &lt;- fit_GPD_MLE(X))
(est &lt;- fit.MLE$par) # estimates of xi, mu, sigma
stopifnot(all.equal(est[["shape"]], xi,   tol = 0.12),
          all.equal(est[["scale"]], beta, tol = 0.11))
fit.MLE$SE # estimated asymp. variances of MLEs = std. errors of MLEs

## Plot the log-likelihood in the shape parameter xi for fixed
## scale beta (fixed as generated)
xi. &lt;- seq(-0.1, 0.8, length.out = 65)
logLik &lt;- sapply(xi., function(xi..) logLik_GPD(c(xi.., beta), x = X))
plot(xi., logLik, type = "l", xlab = expression(xi),
     ylab = expression("GPD log-likelihood for fixed"~beta))

## Plot the profile likelihood for these xi's
## (with an initial interval for the nuisance parameter beta such that
##  logLik_GPD() is finite)
pLL &lt;- sapply(xi., function(xi..) {
    ## Choose beta interval for optimize()
    int &lt;- if(xi.. &gt;= 0) {
               ## Method-of-Moment estimator
               mu.hat &lt;- mean(X)
               sig2.hat &lt;- var(X)
               shape.hat &lt;- (1-mu.hat^2/sig2.hat)/2
               scale.hat &lt;- mu.hat*(1-shape.hat)
               ## log-likelihood always fine for xi.. &gt;= 0 for all beta
               c(1e-8, 2 * scale.hat)
           } else { # xi.. &lt; 0
               ## Make sure logLik_GPD() is finite at endpoints of int
               mx &lt;- max(X)
               -xi.. * mx * c(1.01, 100) # -xi * max(X) * scaling
               ## Note: for shapes xi.. closer to 0, the upper scaling factor
               ##       needs to be chosen sufficiently large in order
               ##       for optimize() to find an optimum (not just the
               ##       upper end point). Try it with '2' instead of '100'.
           }
    ## Optimization
    optimize(function(nuis) logLik_GPD(c(xi.., nuis), x = X),
             interval = int, maximum = TRUE)$maximum
})
plot(xi., pLL, type = "l", xlab = expression(xi),
     ylab = "GPD profile log-likelihood")
</code></pre>

<hr>
<h2 id='get_data'>Tools for Getting and Working with Data</h2><span id='topic+get_data'></span>

<h3>Description</h3>

<p>Download (and possibly) merge data from freely available databases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_data(x, from = NULL, to = NULL,
         src = c("yahoo", "quandl", "oanda", "FRED", "google"),
         FUN = NULL, verbose = TRUE, warn = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_data_+3A_x">x</code></td>
<td>
<p>vector of ticker symbols (e.g. <code>"^GSPC"</code> if
<code>src = "yahoo"</code> or <code>"EUR/USD"</code> if <code>src = "oanda"</code>).</p>
</td></tr>
<tr><td><code id="get_data_+3A_from">from</code></td>
<td>
<p>start date as a <code>Date</code> object or character string
(in international date format <code>"yyyy-mm-dd"</code>); if <code>NULL</code>,
the earliest date with available data is picked.</p>
</td></tr>
<tr><td><code id="get_data_+3A_to">to</code></td>
<td>
<p>end date as a <code>Date</code> object or character string
(in international date format <code>"yyyy-mm-dd"</code>); if <code>NULL</code>,
the last date with available data is picked.</p>
</td></tr>
<tr><td><code id="get_data_+3A_src">src</code></td>
<td>
<p>character string specifying the data source
(e.g. <code>"yahoo"</code> for stocks or <code>"oanda"</code> for FX data);
see <code><a href="quantmod.html#topic+getSymbols">getSymbols</a>()</code> and <code><a href="Quandl.html#topic+Quandl">Quandl</a>()</code>.</p>
</td></tr>
<tr><td><code id="get_data_+3A_fun">FUN</code></td>
<td>
<p><code><a href="base.html#topic+function">function</a></code> to be applied to the data before
being returned. This can be
</p>

<dl>
<dt>the identity:</dt><dd><p>if the data could not be retrieved (and is
thus replaced by <code><a href="base.html#topic+NA">NA</a></code>);</p>
</dd>
<dt>the given <code>FUN</code>:</dt><dd><p>if <code>FUN</code> has been provided;</p>
</dd>
<dt>a useful default:</dt><dd><p>if <code>FUN = NULL</code>; the default uses
the adjusted close price <code><a href="quantmod.html#topic+Ad">Ad</a>()</code> if
<code>src = "yahoo"</code>,
the close price <code><a href="quantmod.html#topic+Cl">Cl</a>()</code> if
<code>src = "google"</code> and the identity otherwise.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="get_data_+3A_verbose">verbose</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether progress
monitoring should be done.</p>
</td></tr>
<tr><td><code id="get_data_+3A_warn">warn</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether a warning
is given showing the error message when fetching <code>x</code> fails.</p>
</td></tr>
<tr><td><code id="get_data_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="quantmod.html#topic+getSymbols">getSymbols</a>()</code> from <span class="pkg">quantmod</span> or
<code><a href="Quandl.html#topic+Quandl">Quandl</a>()</code> from <span class="pkg">Quandl</span> (if <code>src = "quandl"</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>FUN</code> is typically one of <span class="pkg">quantmod</span>'s <code><a href="quantmod.html#topic+Op">Op</a></code>,
<code><a href="quantmod.html#topic+Hi">Hi</a></code>, <code><a href="quantmod.html#topic+Lo">Lo</a></code>, <code><a href="quantmod.html#topic+Cl">Cl</a></code>, <code><a href="quantmod.html#topic+Vo">Vo</a></code>,
<code><a href="quantmod.html#topic+Ad">Ad</a></code> or one of the combined functions <code><a href="quantmod.html#topic+OpCl">OpCl</a></code>,
<code><a href="quantmod.html#topic+ClCl">ClCl</a></code>, <code><a href="quantmod.html#topic+HiCl">HiCl</a></code>, <code><a href="quantmod.html#topic+LoCl">LoCl</a></code>, <code><a href="quantmod.html#topic+LoHi">LoHi</a></code>,
<code><a href="quantmod.html#topic+OpHi">OpHi</a></code>, <code><a href="quantmod.html#topic+OpLo">OpLo</a></code>, <code><a href="quantmod.html#topic+OpOp">OpOp</a></code>.
</p>


<h3>Value</h3>

<p><code>xts</code> object containing the data with column name(s)
adjusted to be the ticker symbol (in case lengths match; otherwise the
column names are not adjusted); <code><a href="base.html#topic+NA">NA</a></code> if
data is not available.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Note: This needs a working internet connection
## Get stock and volatility data (for all available trading days)
dat &lt;- get_data(c("^GSPC", "^VIX")) # note: this needs a working internet connection
## Plot them (Alternative: plot.xts() from xtsExtra)
library(zoo)
plot.zoo(dat, screens = 1, main = "", xlab = "Trading day", ylab = "Value")

## End(Not run)
</code></pre>

<hr>
<h2 id='GEV'>Generalized Extreme Value Distribution</h2><span id='topic+dGEV'></span><span id='topic+pGEV'></span><span id='topic+qGEV'></span><span id='topic+rGEV'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random variate
generation for the generalized extreme value distribution (GEV).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dGEV(x, shape, loc = 0, scale = 1, log = FALSE)
pGEV(q, shape, loc = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
qGEV(p, shape, loc = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
rGEV(n, shape, loc = 0, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GEV_+3A_x">x</code>, <code id="GEV_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="GEV_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="GEV_+3A_n">n</code></td>
<td>
<p>number of observations.</p>
</td></tr>
<tr><td><code id="GEV_+3A_shape">shape</code></td>
<td>
<p>GEV shape parameter <code class="reqn">\xi</code>, a real.</p>
</td></tr>
<tr><td><code id="GEV_+3A_loc">loc</code></td>
<td>
<p>GEV location parameter <code class="reqn">\mu</code>, a real.</p>
</td></tr>
<tr><td><code id="GEV_+3A_scale">scale</code></td>
<td>
<p>GEV scale parameter <code class="reqn">\sigma</code>, a positive real.</p>
</td></tr>
<tr><td><code id="GEV_+3A_lower.tail">lower.tail</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code>; if <code>TRUE</code> (default)
probabilities are <code class="reqn">P(X \le x)</code> otherwise, <code class="reqn">P(X &gt; x)</code>.</p>
</td></tr>
<tr><td><code id="GEV_+3A_log">log</code>, <code id="GEV_+3A_log.p">log.p</code></td>
<td>
<p>logical; if <code>TRUE</code>, probabilities <code>p</code> are
given as <code>log(p)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution function of the generalized extreme value
distribution is given by
</p>
<p style="text-align: center;"><code class="reqn">F(x) = \left\{ \begin{array}{ll}
    \exp(-(1-\xi(x-\mu)/\sigma)^{-1/\xi}), &amp; \xi\neq 0,\ 1+\xi(x-\mu)/\sigma&gt;0,\\
    \exp(-e^{-(x-\mu)/\sigma}), &amp; \xi = 0,
  \end{array}\right.</code>
</p>

<p>where <code class="reqn">\sigma&gt;0</code>.
</p>


<h3>Value</h3>

<p><code>dGEV()</code> computes the density, <code>pGEV()</code> the distribution
function, <code>qGEV()</code> the quantile function and <code>rGEV()</code> random
variates of the generalized extreme value distribution.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R., and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Basic sanity checks
plot(pGEV(rGEV(1000, shape = 0.5), shape = 0.5)) # should be U[0,1]
curve(dGEV(x, shape = 0.5), from = -3, to = 5)
</code></pre>

<hr>
<h2 id='GEV_shape_plot'>Fitted GEV Shape as a Function of the Threshold</h2><span id='topic+GEV_shape_plot'></span>

<h3>Description</h3>

<p>Fit GEVs to block maxima and plot the fitted GPD shape as
a function of the block size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GEV_shape_plot(x, blocksize = tail(pretty(seq_len(length(x)/20), n = 64), -1),
               estimate.cov = TRUE, conf.level = 0.95,
               CI.col = adjustcolor(1, alpha.f = 0.2),
               lines.args = list(), xlim = NULL, ylim = NULL,
               xlab = "Block size",  ylab = NULL,
               xlab2 = "Number of blocks", plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GEV_shape_plot_+3A_x">x</code></td>
<td>
<p><code><a href="base.html#topic+vector">vector</a></code> of <code><a href="base.html#topic+numeric">numeric</a></code> data.</p>
</td></tr>
<tr><td><code id="GEV_shape_plot_+3A_blocksize">blocksize</code></td>
<td>
<p><code><a href="base.html#topic+numeric">numeric</a></code> vector of block sizes for which
to fit a GEV to the block maxima.</p>
</td></tr>
<tr><td><code id="GEV_shape_plot_+3A_estimate.cov">estimate.cov</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether
confidence intervals are to be computed.</p>
</td></tr>
<tr><td><code id="GEV_shape_plot_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the confidence intervals if
<code>estimate.cov</code>.</p>
</td></tr>
<tr><td><code id="GEV_shape_plot_+3A_ci.col">CI.col</code></td>
<td>
<p>color of the pointwise asymptotic confidence intervals
(CIs); if <code>NA</code>, no CIs are shown.</p>
</td></tr>
<tr><td><code id="GEV_shape_plot_+3A_lines.args">lines.args</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> of arguments passed to
the underlying <code><a href="graphics.html#topic+lines">lines</a>()</code> for drawing the shape
parameter as a function of the block size.</p>
</td></tr>
<tr><td><code id="GEV_shape_plot_+3A_xlim">xlim</code>, <code id="GEV_shape_plot_+3A_ylim">ylim</code>, <code id="GEV_shape_plot_+3A_xlab">xlab</code>, <code id="GEV_shape_plot_+3A_ylab">ylab</code></td>
<td>
<p>see <code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
<tr><td><code id="GEV_shape_plot_+3A_xlab2">xlab2</code></td>
<td>
<p>label of the secondary x-axis.</p>
</td></tr>
<tr><td><code id="GEV_shape_plot_+3A_plot">plot</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether a plot is produced.</p>
</td></tr>
<tr><td><code id="GEV_shape_plot_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Such plots can be used in the block maxima method for determining
the optimal block size (as the smallest after which the plot
is (roughly) stable).
</p>


<h3>Value</h3>

<p>Invisibly returns a <code><a href="base.html#topic+list">list</a></code> containing the block sizes
considered, the corresponding block maxima and the fitted GEV
distribution objects as returned by the underlying
<code><a href="#topic+fit_GEV_MLE">fit_GEV_MLE</a>()</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(271)
X &lt;- rPar(5e4, shape = 4)
GEV_shape_plot(X)
abline(h = 1/4, lty = 3) # theoretical xi = 1/shape for Pareto
</code></pre>

<hr>
<h2 id='GPD'>(Generalized) Pareto Distribution</h2><span id='topic+dGPD'></span><span id='topic+pGPD'></span><span id='topic+qGPD'></span><span id='topic+rGPD'></span><span id='topic+dPar'></span><span id='topic+pPar'></span><span id='topic+qPar'></span><span id='topic+rPar'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random variate
generation for the (generalized) Pareto distribution (GPD).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dGPD(x, shape, scale, log = FALSE)
pGPD(q, shape, scale, lower.tail = TRUE, log.p = FALSE)
qGPD(p, shape, scale, lower.tail = TRUE, log.p = FALSE)
rGPD(n, shape, scale)

dPar(x, shape, scale = 1, log = FALSE)
pPar(q, shape, scale = 1, lower.tail = TRUE, log.p = FALSE)
qPar(p, shape, scale = 1, lower.tail = TRUE, log.p = FALSE)
rPar(n, shape, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GPD_+3A_x">x</code>, <code id="GPD_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="GPD_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="GPD_+3A_n">n</code></td>
<td>
<p>number of observations.</p>
</td></tr>
<tr><td><code id="GPD_+3A_shape">shape</code></td>
<td>
<p>GPD shape parameter <code class="reqn">\xi</code> (a real number) and
Pareto shape parameter <code class="reqn">\theta</code> (a positive number).</p>
</td></tr>
<tr><td><code id="GPD_+3A_scale">scale</code></td>
<td>
<p>GPD scale parameter <code class="reqn">\beta</code> (a positive number)
and Pareto scale parameter <code class="reqn">\kappa</code> (a positive number).</p>
</td></tr>
<tr><td><code id="GPD_+3A_lower.tail">lower.tail</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code>; if <code>TRUE</code> (default)
probabilities are <code class="reqn">P(X \le x)</code> otherwise, <code class="reqn">P(X &gt; x)</code>.</p>
</td></tr>
<tr><td><code id="GPD_+3A_log">log</code>, <code id="GPD_+3A_log.p">log.p</code></td>
<td>
<p>logical; if <code>TRUE</code>, probabilities <code>p</code> are
given as <code>log(p)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution function of the generalized Pareto distribution is
given by
</p>
<p style="text-align: center;"><code class="reqn">F(x) = \left\{ \begin{array}{ll}
    1-(1+\xi x/\beta)^{-1/\xi}, &amp; \xi \neq 0,\\
    1-\exp(-x/\beta), &amp; \xi = 0,
    \end{array}\right.</code>
</p>

<p>where <code class="reqn">\beta&gt;0</code> and <code class="reqn">x\ge0</code> if <code class="reqn">\xi\ge
  0</code>
and <code class="reqn">x\in[0,-\beta/\xi]</code> if <code class="reqn">\xi&lt;0</code>.
</p>
<p>The distribution function of the Pareto distribution is given by
</p>
<p style="text-align: center;"><code class="reqn">F(x) = 1-(1+x/\kappa)^{-\theta},\ x\ge 0,</code>
</p>
<p> where <code class="reqn">\theta &gt; 0</code>, <code class="reqn">\kappa &gt; 0</code>.
</p>
<p>In contrast to <code>dGPD()</code>, <code>pGPD()</code>, <code>qGPD()</code> and
<code>rGPD()</code>, the functions <code>dPar()</code>, <code>pPar()</code>,
<code>qPar()</code> and <code>rPar()</code> are vectorized in their main
argument and the parameters.
</p>


<h3>Value</h3>

<p><code>dGPD()</code> computes the density, <code>pGPD()</code> the distribution
function, <code>qGPD()</code> the quantile function and <code>rGPD()</code> random
variates of the generalized Pareto distribution.
</p>
<p>Similary for <code>dPar()</code>, <code>pPar()</code>, <code>qPar()</code> and
<code>rPar()</code> for the Pareto distribution.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R., and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Basic sanity checks
curve(dGPD(x, shape = 0.5, scale = 3), from = -1, to = 5)
plot(pGPD(rGPD(1000, shape = 0.5, scale = 3), shape = 0.5, scale = 3)) # should be U[0,1]
</code></pre>

<hr>
<h2 id='GPD_shape_plot'>Fitted GPD Shape as a Function of the Threshold</h2><span id='topic+GPD_shape_plot'></span>

<h3>Description</h3>

<p>Fit GPDs to various thresholds and plot the fitted GPD shape as
a function of the threshold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GPD_shape_plot(x, thresholds = seq(quantile(x, 0.5), quantile(x, 0.99),
                                   length.out = 65),
               estimate.cov = TRUE, conf.level = 0.95,
               CI.col = adjustcolor(1, alpha.f = 0.2),
               lines.args = list(), xlim = NULL, ylim = NULL,
               xlab = "Threshold", ylab = NULL,
               xlab2 = "Excesses", plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GPD_shape_plot_+3A_x">x</code></td>
<td>
<p><code><a href="base.html#topic+vector">vector</a></code> of <code><a href="base.html#topic+numeric">numeric</a></code> data.</p>
</td></tr>
<tr><td><code id="GPD_shape_plot_+3A_thresholds">thresholds</code></td>
<td>
<p><code><a href="base.html#topic+numeric">numeric</a></code> vector of thresholds for which
to fit a GPD to the excesses.</p>
</td></tr>
<tr><td><code id="GPD_shape_plot_+3A_estimate.cov">estimate.cov</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether
confidence intervals are to be computed.</p>
</td></tr>
<tr><td><code id="GPD_shape_plot_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the confidence intervals if
<code>estimate.cov</code>.</p>
</td></tr>
<tr><td><code id="GPD_shape_plot_+3A_ci.col">CI.col</code></td>
<td>
<p>color of the pointwise asymptotic confidence intervals
(CIs); if <code>NA</code>, no CIs are shown.</p>
</td></tr>
<tr><td><code id="GPD_shape_plot_+3A_lines.args">lines.args</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> of arguments passed to
the underlying <code><a href="graphics.html#topic+lines">lines</a>()</code> for drawing the shape
parameter as a function of the threshold.</p>
</td></tr>
<tr><td><code id="GPD_shape_plot_+3A_xlim">xlim</code>, <code id="GPD_shape_plot_+3A_ylim">ylim</code>, <code id="GPD_shape_plot_+3A_xlab">xlab</code>, <code id="GPD_shape_plot_+3A_ylab">ylab</code></td>
<td>
<p>see <code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
<tr><td><code id="GPD_shape_plot_+3A_xlab2">xlab2</code></td>
<td>
<p>label of the secondary x-axis.</p>
</td></tr>
<tr><td><code id="GPD_shape_plot_+3A_plot">plot</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether a plot is produced.</p>
</td></tr>
<tr><td><code id="GPD_shape_plot_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Such plots can be used in the peaks-over-threshold method for
determining the optimal threshold (as the smallest after which the plot
is (roughly) stable).
</p>


<h3>Value</h3>

<p>Invisibly returns a <code><a href="base.html#topic+list">list</a></code> containing the thresholds
considered, the corresponding excesses and the fitted GPD
objects as returned by the underlying <code><a href="#topic+fit_GPD_MLE">fit_GPD_MLE</a>()</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(271)
X &lt;- rt(1000, df = 3.5)
GPD_shape_plot(X)
</code></pre>

<hr>
<h2 id='GPDtail'>GPD-Based Tail Distribution (POT method)</h2><span id='topic+dGPDtail'></span><span id='topic+pGPDtail'></span><span id='topic+qGPDtail'></span><span id='topic+rGPDtail'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random variate
generation for the GPD-based tail distribution in the POT method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dGPDtail(x, threshold, p.exceed, shape, scale, log = FALSE)
pGPDtail(q, threshold, p.exceed, shape, scale, lower.tail = TRUE, log.p = FALSE)
qGPDtail(p, threshold, p.exceed, shape, scale, lower.tail = TRUE, log.p = FALSE)
rGPDtail(n, threshold, p.exceed, shape, scale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GPDtail_+3A_x">x</code>, <code id="GPDtail_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="GPDtail_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="GPDtail_+3A_n">n</code></td>
<td>
<p>number of observations.</p>
</td></tr>
<tr><td><code id="GPDtail_+3A_threshold">threshold</code></td>
<td>
<p>threshold <code class="reqn">u</code> in the POT method.</p>
</td></tr>
<tr><td><code id="GPDtail_+3A_p.exceed">p.exceed</code></td>
<td>
<p>probability of exceeding the threshold u; for the
Smith estimator, this is <code>mean(x &gt; threshold)</code> for <code>x</code>
being the data.</p>
</td></tr>
<tr><td><code id="GPDtail_+3A_shape">shape</code></td>
<td>
<p>GPD shape parameter <code class="reqn">\xi</code> (a real number).</p>
</td></tr>
<tr><td><code id="GPDtail_+3A_scale">scale</code></td>
<td>
<p>GPD scale parameter <code class="reqn">\beta</code> (a positive number).</p>
</td></tr>
<tr><td><code id="GPDtail_+3A_lower.tail">lower.tail</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code>; if <code>TRUE</code> (default)
probabilities are <code class="reqn">P(X \le x)</code> otherwise, <code class="reqn">P(X &gt; x)</code>.</p>
</td></tr>
<tr><td><code id="GPDtail_+3A_log">log</code>, <code id="GPDtail_+3A_log.p">log.p</code></td>
<td>
<p>logical; if <code>TRUE</code>, probabilities <code>p</code> are
given as <code>log(p)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">u</code> denote the threshold (<code>threshold</code>), <code class="reqn">p_u</code> the exceedance
probability (<code>p.exceed</code>) and <code class="reqn">F_{GPD}</code> the GPD
distribution function. Then the distribution function of the GPD-based tail
distribution is given by
</p>
<p style="text-align: center;"><code class="reqn">F(q) = 1-p_u(1-F_{GPD}(q - u))</code>
</p>
<p>. The quantile function is
</p>
<p style="text-align: center;"><code class="reqn">F^{-1}(p) = u + F_GPD^{-1}(1-(1-p)/p_u)</code>
</p>
<p> and the density is
</p>
<p style="text-align: center;"><code class="reqn">f(x) = p_u f_{GPD}(x - u)</code>
</p>
<p>, where <code class="reqn">f_{GPD}</code> denotes the GPD
density.
</p>
<p>Note that the distribution function has a jumpt of height <code class="reqn">P(X \le
    u)</code> (<code>1-p.exceed</code>) at <code class="reqn">u</code>.
</p>


<h3>Value</h3>

<p><code>dGPDtail()</code> computes the density, <code>pGPDtail()</code> the distribution
function, <code>qGPDtail()</code> the quantile function and <code>rGPDtail()</code> random
variates of the GPD-based tail distribution in the POT method.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R., and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data to work with
set.seed(271)
X &lt;- rt(1000, df = 3.5) # in MDA(H_{1/df}); see MFE (2015, Section 16.1.1)

## Determine thresholds for POT method
mean_excess_plot(X[X &gt; 0])
abline(v = 1.5)
u &lt;- 1.5 # threshold

## Fit GPD to the excesses (per margin)
fit &lt;- fit_GPD_MLE(X[X &gt; u] - u)
fit$par
1/fit$par["shape"] # =&gt; close to df

## Estimate threshold exceedance probabilities
p.exceed &lt;- mean(X &gt; u)

## Define corresponding densities, distribution function and RNG
dF &lt;- function(x) dGPDtail(x, threshold = u, p.exceed = p.exceed,
                           shape = fit$par["shape"], scale = fit$par["scale"])
pF &lt;- function(q) pGPDtail(q, threshold = u, p.exceed = p.exceed,
                           shape = fit$par["shape"], scale = fit$par["scale"])
rF &lt;- function(n) rGPDtail(n, threshold = u, p.exceed = p.exceed,
                           shape = fit$par["shape"], scale = fit$par["scale"])

## Basic check of dF()
curve(dF, from = u - 1, to = u + 5)

## Basic check of pF()
curve(pF, from = u, to = u + 5, ylim = 0:1) # quite flat here
abline(v = u, h = 1-p.exceed, lty = 2) # mass at u is 1-p.exceed (see 'Details')

## Basic check of rF()
set.seed(271)
X. &lt;- rF(1000)
plot(X., ylab = "Losses generated from the fitted GPD-based tail distribution")
stopifnot(all.equal(mean(X. == u), 1-p.exceed, tol = 7e-3)) # confirms the above
## Pick out 'continuous part'
X.. &lt;- X.[X. &gt; u]
plot(pF(X..), ylab = "Probability-transformed tail losses") # should be U[1-p.exceed, 1]
</code></pre>

<hr>
<h2 id='hierarchical_matrix'>Construction of Hierarchical Matrices</h2><span id='topic+hierarchical_matrix'></span>

<h3>Description</h3>

<p>Constructing hierarchical matrices, used, for example, for
hierarchical dependence models, clustering, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hierarchical_matrix(x, diagonal = rep(1, d))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hierarchical_matrix_+3A_x">x</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> of length 2 or 3 containing the
homogeneous <code><a href="base.html#topic+numeric">numeric</a></code>
entry of the current block of the hierarchical matrix, the
<code><a href="base.html#topic+integer">integer</a></code> components belongning to the current block (or
<code>NULL</code>) and, possibly, another (nested) <code><a href="base.html#topic+list">list</a></code> of
the same type.</p>
</td></tr>
<tr><td><code id="hierarchical_matrix_+3A_diagonal">diagonal</code></td>
<td>
<p>diagonal elements of the hierarchical matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the examples for how to use.
</p>


<h3>Value</h3>

<p>A hierarchical <code><a href="base.html#topic+matrix">matrix</a></code> of the structure
as specified in <code>x</code> with off-diagonal entries as specified
in <code>x</code> and diagonal entries as specified in <code>diagonal</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>rho &lt;- c(0.2, 0.3, 0.5, 0.8) # some entries (e.g., correlations)

## Test homogeneous case
x &lt;- list(rho[1], 1:6)
hierarchical_matrix(x)

## Two-level case with one block of size 2
x &lt;- list(rho[1], 1, list(rho[2], 2:3))
hierarchical_matrix(x)

## Two-level case with one block of size 2 and a larger homogeneous block
x &lt;- list(rho[1], 1:3, list(rho[2], 4:5))
hierarchical_matrix(x)

## Test two-level case with three blocks of size 2
x &lt;- list(rho[1], NULL, list(list(rho[2], 1:2),
                             list(rho[3], 3:4),
                             list(rho[4], 5:6)))
hierarchical_matrix(x)

## Test three-level case
x &lt;- list(rho[1], 1:3, list(rho[2], NULL, list(list(rho[3], 4:5),
                                               list(rho[4], 6:8))))
hierarchical_matrix(x)

## Test another three-level case
x &lt;- list(rho[1], c(3, 6, 1), list(rho[2], c(9, 2, 7, 5),
                                   list(rho[3], c(8, 4))))
hierarchical_matrix(x)
</code></pre>

<hr>
<h2 id='Hill'>Hill Estimator and Plot</h2><span id='topic+Hill_estimator'></span><span id='topic+Hill_plot'></span>

<h3>Description</h3>

<p>Compute the Hill estimator and Hill plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Hill_estimator(x, k = c(10, length(x)), conf.level = 0.95)
Hill_plot(x, k = c(10, length(x)), conf.level = 0.95, Hill.estimator = NULL,
          log = "x", xlim = NULL, ylim = NULL,
          xlab = "Order statistics", ylab = "Tail index",
          CI.col = adjustcolor(1, alpha.f = 0.2), lines.args = list(),
          xaxis2 = TRUE, xlab2 = "Empirical probability", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hill_+3A_x">x</code></td>
<td>
<p><code><a href="base.html#topic+vector">vector</a></code> of <code><a href="base.html#topic+numeric">numeric</a></code> data.</p>
</td></tr>
<tr><td><code id="Hill_+3A_k">k</code></td>
<td>
<p><code><a href="base.html#topic+vector">vector</a></code> of length 2, determining the smallest
and largest number of order statistics of <code>x</code> to compute the
Hill estimator for (the smallest needs to be &gt;= 2). If <code>k</code> is
of length 1, <code>k</code> is expanded by <code>length(x)</code>.</p>
</td></tr>
<tr><td><code id="Hill_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the confidence intervals.</p>
</td></tr>
<tr><td><code id="Hill_+3A_hill.estimator">Hill.estimator</code></td>
<td>
<p>object as returned by <code>Hill_estimator()</code>.</p>
</td></tr>
<tr><td><code id="Hill_+3A_log">log</code>, <code id="Hill_+3A_xlim">xlim</code>, <code id="Hill_+3A_ylim">ylim</code>, <code id="Hill_+3A_xlab">xlab</code>, <code id="Hill_+3A_ylab">ylab</code></td>
<td>
<p>see <code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
<tr><td><code id="Hill_+3A_ci.col">CI.col</code></td>
<td>
<p>color of the pointwise asymptotic confidence intervals
(CIs); if <code>NA</code>, no CIs are shown.</p>
</td></tr>
<tr><td><code id="Hill_+3A_lines.args">lines.args</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> of additional arguments
for the underlying <code><a href="graphics.html#topic+lines">lines</a>()</code> call to draw the Hill
estimator.</p>
</td></tr>
<tr><td><code id="Hill_+3A_xaxis2">xaxis2</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether a third
axis is drawn that shows the empirical probabilities
<code>1-(k-1)/length(x)</code> corresponding to <code>k</code>, so the value of
the column <code>k.prob</code> as returned by <code>Hill_estimator()</code>.</p>
</td></tr>
<tr><td><code id="Hill_+3A_xlab2">xlab2</code></td>
<td>
<p>label of the secondary x-axis.</p>
</td></tr>
<tr><td><code id="Hill_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See McNeil et al. (2015, Section 5.2.4, (5.23))
</p>


<h3>Value</h3>


<dl>
<dt><code>Hill_estimator()</code>:</dt><dd><p>A five-column matrix containing the
indices <code>k</code>, their corresponding empirical probabilities
<code>k.prob</code>, the estimated tail indices <code>tail.index</code>,
and the lower and upper CI endpoints <code>CI.low</code> and <code>CI.up</code>.</p>
</dd>
<dt><code>Hill_plot()</code>:</dt><dd><p>Hill plot by side-effect.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R. and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(271)
X &lt;- rt(1000, df = 3.5)
Y &lt;- X[X &gt; 0]
Hill_plot(Y)
Hill_plot(Y, log = "", CI.col = NA)
</code></pre>

<hr>
<h2 id='matrix_density_plota'>Density Plot of the Values from a Lower Triangular Matrix</h2><span id='topic+matrix_density_plot'></span>

<h3>Description</h3>

<p>Density plot of all values in the lower triangular part of a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrix_density_plot(x, xlab = "Entries in the lower triangular matrix",
                    main = "", text = NULL, side = 4, line = 1, adj = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matrix_density_plota_+3A_x">x</code></td>
<td>
<p><code><a href="base.html#topic+matrix">matrix</a></code>-like object.</p>
</td></tr>
<tr><td><code id="matrix_density_plota_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label.</p>
</td></tr>
<tr><td><code id="matrix_density_plota_+3A_main">main</code></td>
<td>
<p>title.</p>
</td></tr>
<tr><td><code id="matrix_density_plota_+3A_text">text</code></td>
<td>
<p>see <code><a href="graphics.html#topic+mtext">mtext</a>()</code>. The <code>text = ""</code>, it is omitted.</p>
</td></tr>
<tr><td><code id="matrix_density_plota_+3A_side">side</code></td>
<td>
<p>see <code><a href="graphics.html#topic+mtext">mtext</a>()</code>.</p>
</td></tr>
<tr><td><code id="matrix_density_plota_+3A_line">line</code></td>
<td>
<p>see <code><a href="graphics.html#topic+mtext">mtext</a>()</code>.</p>
</td></tr>
<tr><td><code id="matrix_density_plota_+3A_adj">adj</code></td>
<td>
<p>see <code><a href="graphics.html#topic+mtext">mtext</a>()</code>.</p>
</td></tr>
<tr><td><code id="matrix_density_plota_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying <code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>matrix_density_plot()</code> is typically used for symmetric matrices
(like correlation matrices, matrices of pairwise Kendall's tau or tail
dependence parameters) to check the distribution of their off-diagonal
entries.
</p>


<h3>Value</h3>

<p><code><a href="base.html#topic+invisible">invisible</a>()</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a random correlation matrix
d &lt;- 50
L &lt;- diag(1:d)
set.seed(271)
L[lower.tri(L)] &lt;- runif(choose(d,2))
Sigma &lt;- L 
P &lt;- cor(Sigma)
## Density of its lower triangular entries
matrix_density_plot(P)
</code></pre>

<hr>
<h2 id='matrix_plot'>Graphical Tool for Visualizing Matrices</h2><span id='topic+matrix_plot'></span>

<h3>Description</h3>

<p>Plot of a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrix_plot(x, ran = range(x, na.rm = TRUE), ylim = rev(c(0.5, nrow(x) + 0.5)),
            xlab = "Column", ylab = "Row",
            scales = list(alternating = c(1,1), tck = c(1,0),
                          x = list(at = pretty(1:ncol(x)), rot = 90),
                          y = list(at = pretty(1:nrow(x)))),
            at = NULL, colorkey = NULL, col = c("royalblue3", "white", "maroon3"),
            col.regions = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matrix_plot_+3A_x">x</code></td>
<td>
<p><code><a href="base.html#topic+matrix">matrix</a></code>-like object.</p>
</td></tr>
<tr><td><code id="matrix_plot_+3A_ran">ran</code></td>
<td>
<p>range (can be used to enforce (-1,1), for example).</p>
</td></tr>
<tr><td><code id="matrix_plot_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits in reverse order (for the rows to appear 'top down').</p>
</td></tr>
<tr><td><code id="matrix_plot_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label.</p>
</td></tr>
<tr><td><code id="matrix_plot_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label.</p>
</td></tr>
<tr><td><code id="matrix_plot_+3A_scales">scales</code></td>
<td>
<p>see <code><a href="lattice.html#topic+levelplot">levelplot</a>()</code>; if <code><a href="base.html#topic+NULL">NULL</a></code>,
labels and ticks are omitted.</p>
</td></tr>
<tr><td><code id="matrix_plot_+3A_at">at</code></td>
<td>
<p>see <code><a href="lattice.html#topic+levelplot">levelplot</a>()</code>. If <code><a href="base.html#topic+NULL">NULL</a></code>, a
useful default is computed based on the given values in <code>x</code>.</p>
</td></tr>
<tr><td><code id="matrix_plot_+3A_colorkey">colorkey</code></td>
<td>
<p>see <code><a href="lattice.html#topic+levelplot">levelplot</a>()</code>. If <code><a href="base.html#topic+NULL">NULL</a></code>, a
useful default is computed based on <code>at</code>.</p>
</td></tr>
<tr><td><code id="matrix_plot_+3A_col">col</code></td>
<td>
<p><code><a href="base.html#topic+vector">vector</a></code> of length two (if all values of
<code>x</code> are non-positive or all are non-negative; note that also a
vector of length three is allowed in this case) or three (if
<code>x</code> contains negative and positive values) providing the color
key's default colors.</p>
</td></tr>
<tr><td><code id="matrix_plot_+3A_col.regions">col.regions</code></td>
<td>
<p>see <code><a href="lattice.html#topic+levelplot">levelplot</a>()</code>. If <code><a href="base.html#topic+NULL">NULL</a></code>, a
useful default is computed based on <code>at</code>.</p>
</td></tr>
<tr><td><code id="matrix_plot_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="lattice.html#topic+levelplot">levelplot</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plot of a matrix.
</p>


<h3>Value</h3>

<p>The plot, a Trellis object.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a random correlation matrix
d &lt;- 50
L &lt;- diag(1:d)
set.seed(271)
L[lower.tri(L)] &lt;- runif(choose(d,2)) # random Cholesky factor
Sigma &lt;- L 
P &lt;- cor(Sigma)

## Default
matrix_plot(P)
matrix_plot(P, ran = c(-1, 1)) # within (-1, 1)
matrix_plot(abs(P)) # if nonnegative
L. &lt;- L
diag(L.) &lt;- NA
matrix_plot(L.) # Cholesky factor without diagonal

## Default if nonpositive
matrix_plot(-abs(P))

## Changing colors
matrix_plot(P, ran = c(-1, 1),
            col.regions = grey(c(seq(0, 1, length.out = 100),
                                 seq(1, 0, length.out = 100))))

## An example with overlaid lines
library(lattice)
my_panel &lt;- function(...) {
    panel.levelplot(...)
    panel.abline(h = c(10, 20), v = c(10, 20), lty = 2)
}
matrix_plot(P, panel = my_panel)
</code></pre>

<hr>
<h2 id='mean_excess'>Mean Excess</h2><span id='topic+mean_excess_np'></span><span id='topic+mean_excess_plot'></span><span id='topic+mean_excess_GPD'></span>

<h3>Description</h3>

<p>Sample mean excess function, mean excess function of a GPD
and sample mean excess plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_excess_np(x, omit = 3)
mean_excess_plot(x, omit = 3,
                 xlab = "Threshold", ylab = "Mean excess over threshold", ...)
mean_excess_GPD(x, shape, scale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean_excess_+3A_x">x</code></td>
<td>


<dl>
<dt><code>mean_excess_GPD()</code>:</dt><dd><p><code><a href="base.html#topic+numeric">numeric</a></code> vector of
evaluation points of the mean excess function of the GPD.</p>
</dd>
<dt>otherwise:</dt><dd><p><code><a href="base.html#topic+numeric">numeric</a></code> vector of data.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="mean_excess_+3A_omit">omit</code></td>
<td>
<p>number <code class="reqn">\ge 1</code> of unique 
last observations to be omitted from the sorted data (as mean excess plot
becomes unreliable for these observations as thresholds).</p>
</td></tr>
<tr><td><code id="mean_excess_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label.</p>
</td></tr>
<tr><td><code id="mean_excess_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label.</p>
</td></tr>
<tr><td><code id="mean_excess_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
<tr><td><code id="mean_excess_+3A_shape">shape</code></td>
<td>
<p>GPD shape parameter <code class="reqn">\xi</code>.</p>
</td></tr>
<tr><td><code id="mean_excess_+3A_scale">scale</code></td>
<td>
<p>GPD scale parameter <code class="reqn">\beta</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mean excess plots can be used in the peaks-over-threshold method for
choosing a threshold. To this end, one chooses the smallest threshold
above which the mean excess plot is roughly linear.
</p>


<h3>Value</h3>

<p><code>mean_excess_np()</code> returns a two-column matrix giving
the sorted data without the <code>omit</code>-largest unique values
(first column) and the corresponding values of the sample mean excess
function (second column). It is mainly used in <code>mean_excess_plot()</code>.
</p>
<p><code>mean_excess_plot()</code> returns <code>invisible()</code>.
</p>
<p><code>mean_excess_GPD()</code> returns the mean excess function of a
generalized Pareto distribution evaluated at <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate losses to work with
set.seed(271)
X &lt;- rt(1000, df = 3.5) # in MDA(H_{1/df}); see MFE (2015, Section 16.1.1)

## (Sample) mean excess plot and threshold choice
mean_excess_plot(X[X &gt; 0]) # we only use positive values here to see 'more'
## =&gt; Any value in [0.8, 2] seems reasonable as threshold at first sight
##    but 0.8 to 1 turns out to be too small for the degrees of
##    freedom implied by the GPD estimator to be close to the true value 3.5.
## =&gt; We go with threshold 1.5 here.
u &lt;- 1.5 # thresholds

## An alternative way
ME &lt;- mean_excess_np(X[X &gt; 0])
plot(ME, xlab = "Threshold", ylab = "Mean excess over threshold")

## Mean excess plot with mean excess function of the fitted GPD
fit &lt;- fit_GPD_MLE(X[X &gt; u] - u)
q &lt;- seq(u, ME[nrow(ME),"x"], length.out = 129)
MEF.GPD &lt;- mean_excess_GPD(q-u, shape = fit$par[["shape"]], scale = fit$par[["scale"]])
mean_excess_plot(X[X &gt; 0]) # mean excess plot for positive losses...
lines(q, MEF.GPD, col = "royalblue", lwd = 1.4) # ... with mean excess function of the fitted GPD
</code></pre>

<hr>
<h2 id='NA_plot'>Graphical Tool for Visualizing NAs in a Data Set</h2><span id='topic+NA_plot'></span>

<h3>Description</h3>

<p>Plot NAs in a data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NA_plot(x, col = c("black", "white"), xlab = "Time", ylab = "Component",
        text = "Black: NA; White: Available data",
        side = 4, line = 1, adj = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NA_plot_+3A_x">x</code></td>
<td>
<p>matrix (ideally an <code>xts</code> object).</p>
</td></tr>
<tr><td><code id="NA_plot_+3A_col">col</code></td>
<td>
<p>bivariate vector containing the colors for missing and
available data, respectively.</p>
</td></tr>
<tr><td><code id="NA_plot_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label.</p>
</td></tr>
<tr><td><code id="NA_plot_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label.</p>
</td></tr>
<tr><td><code id="NA_plot_+3A_text">text</code></td>
<td>
<p>see <code><a href="graphics.html#topic+mtext">mtext</a>()</code>. The <code>text = ""</code>, it is omitted.</p>
</td></tr>
<tr><td><code id="NA_plot_+3A_side">side</code></td>
<td>
<p>see <code><a href="graphics.html#topic+mtext">mtext</a>()</code>.</p>
</td></tr>
<tr><td><code id="NA_plot_+3A_line">line</code></td>
<td>
<p>see <code><a href="graphics.html#topic+mtext">mtext</a>()</code>.</p>
</td></tr>
<tr><td><code id="NA_plot_+3A_adj">adj</code></td>
<td>
<p>see <code><a href="graphics.html#topic+mtext">mtext</a>()</code>.</p>
</td></tr>
<tr><td><code id="NA_plot_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="graphics.html#topic+image">image</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Indicate <code><a href="base.html#topic+NA">NA</a></code>s in a data set.
</p>


<h3>Value</h3>

<p><code><a href="base.html#topic+invisible">invisible</a>()</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data
n &lt;- 1000 # sample size
d &lt;- 100 # dimension
set.seed(271) # set seed
x &lt;- matrix(runif(n*d), ncol = d) # generate data

## Assign missing data
k &lt;- ceiling(d/4) # fraction of columns with some NAs
j &lt;- sample(1:d, size = k) # columns j with NAs
i &lt;- sample(1:n, size = k) # 1:i will be NA in each column j
X &lt;- x
for(k. in seq_len(k)) X[1:i[k.], j[k.]] &lt;- NA # put in NAs

## Plot NAs
NA_plot(X) # indicate NAs
</code></pre>

<hr>
<h2 id='pp_qq_plot'>P-P and Q-Q Plots</h2><span id='topic+pp_plot'></span><span id='topic+qq_plot'></span>

<h3>Description</h3>

<p>Probability-probability plots and quantile-quantile plots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pp_plot(x, FUN, pch = 20, xlab = "Theoretical probabilities",
        ylab = "Sample probabilities", ...)
qq_plot(x, FUN = qnorm, method = c("theoretical", "empirical"),
        pch = 20, do.qqline = TRUE, qqline.args = NULL,
        xlab = "Theoretical quantiles", ylab = "Sample quantiles",
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pp_qq_plot_+3A_x">x</code></td>
<td>
<p>data <code><a href="base.html#topic+vector">vector</a></code>.</p>
</td></tr>
<tr><td><code id="pp_qq_plot_+3A_fun">FUN</code></td>
<td>
<p><code><a href="base.html#topic+function">function</a></code>. For
</p>

<dl>
<dt><code>pp_plot()</code>:</dt><dd><p>The distribution function (vectorized).</p>
</dd>
<dt><code>qq_plot()</code>:</dt><dd><p>The quantile function (vectorized).</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="pp_qq_plot_+3A_pch">pch</code></td>
<td>
<p>plot symbol.</p>
</td></tr>
<tr><td><code id="pp_qq_plot_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label.</p>
</td></tr>
<tr><td><code id="pp_qq_plot_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label.</p>
</td></tr>
<tr><td><code id="pp_qq_plot_+3A_do.qqline">do.qqline</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether a Q-Q line
is plotted.</p>
</td></tr>
<tr><td><code id="pp_qq_plot_+3A_method">method</code></td>
<td>
<p>method used to construct the Q-Q line. If
<code>"theoretical"</code>, the theoretically true line with intercept 0
and slope 1 is displayed; if <code>"empirical"</code>, the intercept
and slope are determined with <code><a href="stats.html#topic+qqline">qqline</a>()</code>. The former
helps deciding whether <code>x</code> comes from the distribution
specified by <code>FUN</code> exactly, the latter whether <code>x</code>
comes from a location-scale transformed distribution specified by
<code>FUN</code>.</p>
</td></tr>
<tr><td><code id="pp_qq_plot_+3A_qqline.args">qqline.args</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> containing additional arguments
passed to the underlying <code><a href="graphics.html#topic+abline">abline</a>()</code> functions. Defaults to
<code>list(a = 0, b = 1)</code> if <code>method = "theoretical"</code> and <code>list()</code>
if <code>method = "empirical"</code>.</p>
</td></tr>
<tr><td><code id="pp_qq_plot_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that Q-Q plots are more widely used than P-P plots
(as they highlight deviations in the tails more clearly).
</p>


<h3>Value</h3>

<p><code><a href="base.html#topic+invisible">invisible</a>()</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data
n &lt;- 1000
mu &lt;- 1
sig &lt;- 3
nu &lt;- 3.5
set.seed(271) # set seed
x &lt;- mu + sig * sqrt((nu-2)/nu) * rt(n, df = nu) # sample from t_nu(mu, sig^2)

## P-P plot
pF &lt;- function(q) pt((q - mu) / (sig * sqrt((nu-2)/nu)), df = nu)
pp_plot(x, FUN = pF)

## Q-Q plot
qF &lt;- function(p) mu + sig * sqrt((nu-2)/nu) * qt(p, df = nu)
qq_plot(x, FUN = qF)

## A comparison with R's qqplot() and qqline()
qqplot(qF(ppoints(length(x))), x) # the same (except labels)
qqline(x, distribution = qF) # slightly different (since *estimated*)

## Difference of the two methods
set.seed(271)
z &lt;- rnorm(1000)
## Standardized data
qq_plot(z, FUN = qnorm) # fine
qq_plot(z, FUN = qnorm, method = "empirical") # fine
## Location-scale transformed data
mu &lt;- 3
sig &lt;- 2
z. &lt;- mu+sig*z
qq_plot(z., FUN = qnorm) # not fine (z. comes from N(mu, sig^2), not N(0,1))
qq_plot(z., FUN = qnorm, method = "empirical") # fine (as intercept and slope are estimated)
</code></pre>

<hr>
<h2 id='returns'>Computing Returns and Inverse Transformation</h2><span id='topic+returns'></span><span id='topic+returns_qrmtools'></span>

<h3>Description</h3>

<p>Compute log-returns, simple returns and basic differences (or the
inverse operations) from given data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>returns(x, method = c("logarithmic", "simple", "diff"), inverse = FALSE,
        start, start.date)
returns_qrmtools(x, method = c("logarithmic", "simple", "diff"),
                 inverse = FALSE, start, start.date)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="returns_+3A_x">x</code></td>
<td>
<p>matrix or vector (possibly a <code>xts</code> object) to be turned
into returns (if <code>inverse = FALSE</code>)
or returns to be turned into the original data (if <code>inverse =
      TRUE</code>).</p>
</td></tr>
<tr><td><code id="returns_+3A_method">method</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string indicating the method to
be used (log-returns (logarithmic changes), simple returns (relative
changes), or basic differences). Note that this can also be a vector
of such methods of length equal to the number of columns of <code>x</code>.</p>
</td></tr>
<tr><td><code id="returns_+3A_inverse">inverse</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether the inverse
transformation (data from given returns) shall be computed (if
<code>TRUE</code>, this requires <code>start</code> to be specified).</p>
</td></tr>
<tr><td><code id="returns_+3A_start">start</code></td>
<td>
<p>if <code>inverse = TRUE</code>, the last available value
of the time series to be constructed from the given returns <code>x</code>.</p>
</td></tr>
<tr><td><code id="returns_+3A_start.date">start.date</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> or <code><a href="base.html#topic+Date">Date</a></code> object
to be used as the date corresponding to the value <code>start</code>;
currently only used for <code><a href="xts.html#topic+xts">xts</a></code> objects.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>inverse = FALSE</code> and <code>x</code> is an <code>xts</code> object, the
returned object is an <code>xts</code>, too.
</p>
<p>Note that the <span class="rlang"><b>R</b></span> package <span class="pkg">timeSeries</span> also contains a function
<code>returns()</code> (and hence the order in which <span class="pkg">timeSeries</span> and
<span class="pkg">qrmtools</span> are loaded matters to get the right <code>returns()</code>).
For this reason, <code>returns_qrmtools()</code> is an alias for
<code>returns()</code> from <span class="pkg">qrmtools</span>.
</p>


<h3>Value</h3>

<p><code><a href="base.html#topic+vector">vector</a></code> or <code><a href="base.html#topic+matrix">matrix</a></code> with the same number of
columns as <code>x</code> just one row less if <code>inverse = FALSE</code>
or one row more if <code>inverse = TRUE</code>.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate two paths of a geometric Brownian motion
S0 &lt;- 10 # current stock price S_0
r &lt;- 0.01 # risk-free annual interest rate
sig &lt;- 0.2 # (constant) annual volatility
T &lt;- 2 # maturity in years
N &lt;- 250 # business days per year
t &lt;- 1:(N*T) # time points to be sampled
npath &lt;- 2 # number of paths
set.seed(271) # for reproducibility
S &lt;- replicate(npath, S0 * exp(cumsum(rnorm(N*T, # sample paths of S_t
                                            mean = (r-sig^2/2)/N,
                                            sd = sqrt((sig^2)/N))))) # (N*T, npath)

## Turn into xts objects
library(xts)
sdate &lt;- as.Date("2000-05-02") # start date
S.  &lt;- as.xts(S, order.by = seq(sdate, length.out = N*T, by = "1 week"))
plot(S.[,1], main = "Stock 1")
plot(S.[,2], main = "Stock 2")


### Log-returns ################################################################

## Based on S[,1]
X &lt;- returns(S[,1]) # build log-returns (one element less than S)
Y &lt;- returns(X, inverse = TRUE, start = S[1,1]) # transform back
stopifnot(all.equal(Y, S[,1]))

## Based on S
X &lt;- returns(S) # build log-returns (one element less than S)
Y &lt;- returns(X, inverse = TRUE, start = S[1,]) # transform back
stopifnot(all.equal(Y, S))

## Based on S.[,1]
X &lt;- returns(S.[,1])
Y &lt;- returns(X, inverse = TRUE, start = S.[1,1], start.date = sdate)
stopifnot(all.equal(Y, S.[,1], check.attributes = FALSE))

## Based on S.
X &lt;- returns(S.)
Y &lt;- returns(X, inverse = TRUE, start = S.[1], start.date = sdate)
stopifnot(all.equal(Y, S., check.attributes = FALSE))

## Sign-adjusted (negative) log-returns
X &lt;- -returns(S) # build -log-returns
Y &lt;- returns(-X, inverse = TRUE, start = S[1,]) # transform back
stopifnot(all.equal(Y, S))


### Simple returns #############################################################

## Simple returns based on S
X &lt;- returns(S, method = "simple")
Y &lt;- returns(X, method = "simple", inverse = TRUE, start = S[1,])
stopifnot(all.equal(Y, S))

## Simple returns based on S.
X &lt;- returns(S., method = "simple")
Y &lt;- returns(X, method = "simple", inverse = TRUE, start = S.[1,],
             start.date = sdate)
stopifnot(all.equal(Y, S., check.attributes = FALSE))

## Sign-adjusted (negative) simple returns
X &lt;- -returns(S, method = "simple")
Y &lt;- returns(-X, method = "simple", inverse = TRUE, start = S[1,])
stopifnot(all.equal(Y, S))


### Basic differences ##########################################################

## Basic differences based on S
X &lt;- returns(S, method = "diff")
Y &lt;- returns(X, method = "diff", inverse = TRUE, start = S[1,])
stopifnot(all.equal(Y, S))

## Basic differences based on S.
X &lt;- returns(S., method = "diff")
Y &lt;- returns(X, method = "diff", inverse = TRUE, start = S.[1,],
             start.date = sdate)
stopifnot(all.equal(Y, S., check.attributes = FALSE))

## Sign-adjusted (negative) basic differences
X &lt;- -returns(S, method = "diff")
Y &lt;- returns(-X, method = "diff", inverse = TRUE, start = S[1,])
stopifnot(all.equal(Y, S))


### Vector-case of 'method' ####################################################

X &lt;- returns(S., method = c("logarithmic", "diff"))
Y &lt;- returns(X, method = c("logarithmic", "diff"), inverse = TRUE, start = S.[1,],
             start.date = sdate)
stopifnot(all.equal(Y, S., check.attributes = FALSE))
</code></pre>

<hr>
<h2 id='risk_measures'>Risk Measures</h2><span id='topic+VaR_np'></span><span id='topic+VaR_t'></span><span id='topic+VaR_t01'></span><span id='topic+VaR_GPD'></span><span id='topic+VaR_Par'></span><span id='topic+VaR_GPDtail'></span><span id='topic+ES_np'></span><span id='topic+ES_t'></span><span id='topic+ES_t01'></span><span id='topic+ES_GPD'></span><span id='topic+ES_Par'></span><span id='topic+ES_GPDtail'></span><span id='topic+RVaR_np'></span><span id='topic+gVaR'></span><span id='topic+gEX'></span>

<h3>Description</h3>

<p>Computing risk measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Value-at-risk
VaR_np(x, level, names = FALSE, type = 1, ...)
VaR_t(level, loc = 0, scale = 1, df = Inf)
VaR_t01(level, df = Inf)
VaR_GPD(level, shape, scale)
VaR_Par(level, shape, scale = 1)
VaR_GPDtail(level, threshold, p.exceed, shape, scale)

## Expected shortfall
ES_np(x, level, method = c("&gt;", "&gt;="), verbose = FALSE, ...)
ES_t(level, loc = 0, scale = 1, df = Inf)
ES_t01(level, df = Inf)
ES_GPD(level, shape, scale)
ES_Par(level, shape, scale = 1)
ES_GPDtail(level, threshold, p.exceed, shape, scale)

## Range value-at-risk
RVaR_np(x, level, ...)

## Multivariate geometric value-at-risk and expectiles
gVaR(x, level, start = colMeans(x),
     method = if(length(level) == 1) "Brent" else "Nelder-Mead", ...)
gEX(x, level, start = colMeans(x),
    method = if(length(level) == 1) "Brent" else "Nelder-Mead", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="risk_measures_+3A_x">x</code></td>
<td>


<dl>
<dt><code>gVaR()</code>, <code>gEX()</code>:</dt><dd><p><code><a href="base.html#topic+matrix">matrix</a></code> of
(rowwise) multivariate losses.</p>
</dd>
<dt><code>VaR_np()</code>, <code>ES_np()</code>, <code>RVaR_np()</code>:</dt><dd><p>if <code>x</code> is a
<code><a href="base.html#topic+matrix">matrix</a></code> then <code><a href="base.html#topic+rowSums">rowSums</a>()</code> is applied
first (so value-at-risk and expected shortfall of the sum
is computed).</p>
</dd>
<dt>otherwise:</dt><dd><p><code><a href="base.html#topic+vector">vector</a></code> of losses.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="risk_measures_+3A_level">level</code></td>
<td>


<dl>
<dt><code>RVaR_np()</code>:</dt><dd><p><code><a href="base.html#topic+vector">vector</a></code> of length 1 or 2
giving the lower and upper confidence level; if of length 1,
it is interpreted as the lower confidence level and the upper
one is taken to be 1.</p>
</dd>
<dt><code>gVaR()</code>, <code>gEX()</code>:</dt><dd><p><code><a href="base.html#topic+vector">vector</a></code> or
<code><a href="base.html#topic+matrix">matrix</a></code> of	(rowwise) confidence levels <code class="reqn">\alpha</code>
(all in <code class="reqn">[0,1]</code>).</p>
</dd>
<dt>otherwise:</dt><dd><p>confidence level <code class="reqn">\alpha\in[0,1]</code>.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="risk_measures_+3A_names">names</code></td>
<td>
<p>see <code>?<a href="stats.html#topic+quantile">quantile</a></code>.</p>
</td></tr>
<tr><td><code id="risk_measures_+3A_type">type</code></td>
<td>
<p>see <code>?<a href="stats.html#topic+quantile">quantile</a></code>.</p>
</td></tr>
<tr><td><code id="risk_measures_+3A_loc">loc</code></td>
<td>
<p>location parameter <code class="reqn">\mu</code>.</p>
</td></tr>
<tr><td><code id="risk_measures_+3A_shape">shape</code></td>
<td>


<dl>
<dt><code>VaR_GPD()</code>, <code>ES_GPD()</code>:</dt><dd><p>GPD shape parameter
<code class="reqn">\xi</code>, a real number.</p>
</dd>
<dt><code>VaR_Par()</code>, <code>ES_Par()</code>:</dt><dd><p>Pareto shape parameter
<code class="reqn">\theta</code>, a positive number.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="risk_measures_+3A_scale">scale</code></td>
<td>


<dl>
<dt><code>VaR_t()</code>, <code>ES_t()</code>:</dt><dd><p><code class="reqn">t</code> scale parameter
<code class="reqn">\sigma</code>, a positive number.</p>
</dd>
<dt><code>VaR_GPD()</code>, <code>ES_GPD()</code>:</dt><dd><p>GPD scale parameter
<code class="reqn">\beta</code>, a positive number.</p>
</dd>
<dt><code>VaR_Par()</code>, <code>ES_Par()</code>:</dt><dd><p>Pareto scale parameter
<code class="reqn">\kappa</code>, a positive number.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="risk_measures_+3A_df">df</code></td>
<td>
<p>degrees of freedom, a positive number; choose <code>df = Inf</code>
for the normal distribution. For the standardized <code class="reqn">t</code>
distributions, <code>df</code> has to be greater than 2.</p>
</td></tr>
<tr><td><code id="risk_measures_+3A_threshold">threshold</code></td>
<td>
<p>threhold <code class="reqn">u</code> (used to estimate the exceedance
probability based on the data <code>x</code>).</p>
</td></tr>
<tr><td><code id="risk_measures_+3A_p.exceed">p.exceed</code></td>
<td>
<p>exceedance probability; typically <code>mean(x &gt; threshold)</code>
for <code>x</code> being the data modeled with the peaks-over-threshold
(POT) method.</p>
</td></tr>
<tr><td><code id="risk_measures_+3A_start">start</code></td>
<td>
<p><code><a href="base.html#topic+vector">vector</a></code> of initial values for the underlying
<code><a href="stats.html#topic+optim">optim</a>()</code>.</p>
</td></tr>
<tr><td><code id="risk_measures_+3A_method">method</code></td>
<td>


<dl>
<dt><code>ES_np()</code>:</dt><dd><p><code><a href="base.html#topic+character">character</a></code> string indicating the method
for computing expected shortfall.</p>
</dd>
<dt><code>gVaR()</code>, <code>gEX()</code>:</dt><dd><p>the optimization method passed to the
underlying <code><a href="stats.html#topic+optim">optim</a>()</code>.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="risk_measures_+3A_verbose">verbose</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether verbose
output is given (in case the mean is computed over (too) few
observations).</p>
</td></tr>
<tr><td><code id="risk_measures_+3A_...">...</code></td>
<td>


<dl>
<dt><code>VaR_np()</code>:</dt><dd><p>additional arguments passed to the
underlying <code><a href="stats.html#topic+quantile">quantile</a>()</code>.</p>
</dd>
<dt><code>ES_np()</code>, <code>RVaR_np()</code>:</dt><dd><p>additional arguments passed to
the underlying <code>VaR_np()</code>.</p>
</dd>
<dt><code>gVaR()</code>, <code>gEX()</code>:</dt><dd><p>additional arguments passed to
the underlying <code><a href="stats.html#topic+optim">optim</a>()</code>.</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

<p>The distribution function of the Pareto distribution is given by
</p>
<p style="text-align: center;"><code class="reqn">F(x) = 1-(\kappa/(\kappa+x))^{\theta},\ x\ge 0,</code>
</p>
<p> where <code class="reqn">\theta &gt; 0</code>, <code class="reqn">\kappa &gt; 0</code>.
</p>


<h3>Value</h3>

<p><code>VaR_np()</code>, <code>ES_np()</code>, <code>RVaR_np()</code> estimate
value-at-risk, expected shortfall and range value-at-risk
non-parametrically. For expected shortfall, if <code>method = "&gt;="</code>
(<code>method = "&gt;"</code>, the default), losses greater than or equal to
(strictly greater than) the nonparametric value-at-risk estimate are
averaged; in the former case, there might be no such loss, in which
case <code>NaN</code> is returned. For range value-at-risk, losses greater
than the nonparametric VaR estimate at level
<code>level[1]</code> and less than or equal to the nonparametric VaR
estimate at level <code>level[2]</code> are averaged.
</p>
<p><code>VaR_t()</code>, <code>ES_t()</code> compute value-at-risk and expected
shortfall for the <code class="reqn">t</code> (or normal) distribution. <code>VaR_t01()</code>,
<code>ES_t01()</code> compute value-at-risk and expected shortfall for the
standardized <code class="reqn">t</code> (or normal) distribution, so scaled <code class="reqn">t</code>
distributions to have mean 0 and variance 1; note that they require
a degrees of freedom parameter greater than 2.
</p>
<p><code>VaR_GPD()</code>, <code>ES_GPD()</code> compute value-at-risk and expected
shortfall for the generalized Pareto distribution (GPD).
</p>
<p><code>VaR_Par()</code>, <code>ES_Par()</code> compute value-at-risk and expected
shortfall for the Pareto distribution.
</p>
<p><code>gVaR()</code>, <code>gEX()</code> compute the multivariate geometric
value-at-risk and expectiles suggested by Chaudhuri (1996) and
Herrmann et al. (2018), respectively.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R. and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>
<p>Chaudhuri, P. (1996).
On a geometric notion of quantiles for multivariate data.
<em>Journal of the American Statistical Assosiation</em> 91(434),
862&ndash;872.
</p>
<p>Herrmann, K., Hofert, M. and Mailhot, M. (2018).
Multivariate geometric expectiles.
<em>Scandinavian Actuarial Journal</em>, 2018(7), 629&ndash;659.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### 1 Univariate measures ######################################################

## Generate some losses and (non-parametrically) estimate VaR_alpha and ES_alpha
set.seed(271)
L &lt;- rlnorm(1000, meanlog = -1, sdlog = 2) # L ~ LN(mu, sig^2)
## Note: - meanlog = mean(log(L)) = mu, sdlog = sd(log(L)) = sig
##       - E(L) = exp(mu + (sig^2)/2), var(L) = (exp(sig^2)-1)*exp(2*mu + sig^2)
##         To obtain a sample with E(L) = a and var(L) = b, use:
##         mu = log(a)-log(1+b/a^2)/2 and sig = sqrt(log(1+b/a^2))
VaR_np(L, level = 0.99)
ES_np(L,  level = 0.99)

## Example 2.16 in McNeil, Frey, Embrechts (2015)
V &lt;- 10000 # value of the portfolio today
sig &lt;- 0.2/sqrt(250) # daily volatility (annualized volatility of 20%)
nu &lt;- 4 # degrees of freedom for the t distribution
alpha &lt;- seq(0.001, 0.999, length.out = 256) # confidence levels
VaRnorm &lt;- VaR_t(alpha, scale = V*sig, df = Inf)
VaRt4 &lt;- VaR_t(alpha, scale = V*sig*sqrt((nu-2)/nu), df = nu)
ESnorm &lt;- ES_t(alpha, scale = V*sig, df = Inf)
ESt4 &lt;- ES_t(alpha, scale = V*sig*sqrt((nu-2)/nu), df = nu)
ran &lt;- range(VaRnorm, VaRt4, ESnorm, ESt4)
plot(alpha, VaRnorm, type = "l", ylim = ran, xlab = expression(alpha), ylab = "")
lines(alpha, VaRt4, col = "royalblue3")
lines(alpha, ESnorm, col = "darkorange2")
lines(alpha, ESt4, col = "maroon3")
legend("bottomright", bty = "n", lty = rep(1,4), col = c("black",
       "royalblue3", "darkorange3", "maroon3"),
       legend = c(expression(VaR[alpha]~~"for normal model"),
                  expression(VaR[alpha]~~"for "*t[4]*" model"),
                  expression(ES[alpha]~~"for normal model"),
                  expression(ES[alpha]~~"for "*t[4]*" model")))


### 2 Multivariate measures ####################################################

## Setup
library(copula)
n &lt;- 1e4 # MC sample size
nu &lt;- 3 # degrees of freedom
th &lt;- iTau(tCopula(df = nu), tau = 0.5) # correlation parameter
cop &lt;- tCopula(param = th, df = nu) # t copula
set.seed(271) # for reproducibility
U &lt;- rCopula(n, cop = cop) # copula sample
theta &lt;- c(2.5, 4) # marginal Pareto parameters
stopifnot(theta &gt; 2) # need finite 2nd moments
X &lt;- sapply(1:2, function(j) qPar(U[,j], shape = theta[j])) # generate X
N &lt;- 17 # number of angles (rather small here because of run time)
phi &lt;- seq(0, 2*pi, length.out = N) # angles
r &lt;- 0.98 # radius
alpha &lt;- r * cbind(alpha1 = cos(phi), alpha2 = sin(phi)) # vector of confidence levels

## Compute geometric value-at-risk
system.time(res &lt;- gVaR(X, level = alpha))
gvar &lt;- t(sapply(seq_len(nrow(alpha)), function(i) {
    x &lt;- res[[i]]
    if(x[["convergence"]] != 0) # 0 = 'converged'
        warning("No convergence for alpha = (", alpha[i,1], ", ", alpha[i,2],
                ") (row ", i, ")")
    x[["par"]]
})) # (N, 2)-matrix

## Compute geometric expectiles
system.time(res &lt;- gEX(X, level = alpha))
gex &lt;- t(sapply(seq_len(nrow(alpha)), function(i) {
    x &lt;- res[[i]]
    if(x[["convergence"]] != 0) # 0 = 'converged'
        warning("No convergence for alpha = (", alpha[i,1], ", ", alpha[i,2],
                ") (row ", i, ")")
    x[["par"]]
})) # (N, 2)-matrix

## Plot geometric VaR and geometric expectiles
plot(gvar, type = "b", xlab = "Component 1 of geometric VaRs and expectiles",
     ylab = "Component 2 of geometric VaRs and expectiles",
     main = "Multivariate geometric VaRs and expectiles")
lines(gex, type = "b", col = "royalblue3")
legend("bottomleft", lty = 1, bty = "n", col = c("black", "royalblue3"),
       legend = c("geom. VaR", "geom. expectile"))
lab &lt;- substitute("MC sample size n ="~n.*","~t[nu.]~"copula with Par("*th1*
                  ") and Par("*th2*") margins",
                  list(n. = n, nu. = nu, th1 = theta[1], th2 = theta[2]))
mtext(lab, side = 4, line = 1, adj = 0)
</code></pre>

<hr>
<h2 id='step_plot'>Plot of Step Functions, Empirical Distribution and Quantile Functions</h2><span id='topic+step_plot'></span><span id='topic+edf_plot'></span><span id='topic+eqf_plot'></span>

<h3>Description</h3>

<p>Plotting step functions, empirical distribution functions and
empirical quantile functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_plot(x, y, y0 = NA, x0 = NA, x1 = NA, method = c("edf", "eqf"), log = "",
          verticals = NA, do.points = NA, add = FALSE,
          col = par("col"), main = "", xlab = "x", ylab = "Function value at x",
          plot.args = NULL, segments.args = NULL, points.args = NULL)
edf_plot(x, y0 = 0, x0 = NA, x1 = NA, log = "",
         verticals = NA, do.points = NA, col = par("col"),
         main = "", xlab = "x", ylab = "Distribution function at x", ...)
eqf_plot(x, y0 = NA, x0 = 0, x1 = 1, log = "",
         verticals = NA, do.points = NA, col = par("col"),
         main = "", xlab = "x", ylab = "Quantile function at x", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_plot_+3A_x">x</code></td>
<td>


<dl>
<dt><code>step_plot()</code>:</dt><dd><p><code><a href="base.html#topic+numeric">numeric</a></code> vector of
x-values.</p>
</dd>
<dt><code>edf_plot()</code>:</dt><dd><p><code><a href="base.html#topic+numeric">numeric</a></code> vector or a
<code><a href="base.html#topic+list">list</a></code> of <code><a href="base.html#topic+numeric">numeric</a></code> vectors; if a list, each
element corresponds to the x-values of an empirical distribution function.</p>
</dd>
<dt><code>eqf_plot()</code>:</dt><dd><p>similar to <code>edf_plot()</code>.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="step_plot_+3A_y">y</code></td>
<td>
<p>y-values corresponding to <code>x</code>.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_y0">y0</code></td>
<td>
<p>y-value of the graph extending to the left of the
first x-value.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_x0">x0</code></td>
<td>
<p>smallest x-value.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_x1">x1</code></td>
<td>
<p>largest x-value.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_method">method</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string indicating the type of
method to be used (<code>"edf"</code> for empricial distribution function
types of plots and <code>"eqf"</code> for empirical quantile function types).</p>
</td></tr>
<tr><td><code id="step_plot_+3A_log">log</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> indicating whether
a logarithmic x-axis is used.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_verticals">verticals</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether to plot
vertical lines (defaults to <code><a href="base.html#topic+TRUE">TRUE</a></code> if and only if
there are 100 or more data points).</p>
</td></tr>
<tr><td><code id="step_plot_+3A_do.points">do.points</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> (vector) indicating whether points are
to be plotted (defaults to <code><a href="base.html#topic+TRUE">TRUE</a></code> if and only if there
are less than 100 data points).</p>
</td></tr>
<tr><td><code id="step_plot_+3A_add">add</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether the current plot
is added to the last one.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_col">col</code></td>
<td>
<p>color (for <code>edf_plot()</code> this can be a
<code><a href="base.html#topic+vector">vector</a></code>).</p>
</td></tr>
<tr><td><code id="step_plot_+3A_main">main</code></td>
<td>
<p>title.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_plot.args">plot.args</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> of additional arguments passed to
the underlying <code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_segments.args">segments.args</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> of additional arguments passed to
the underlying <code><a href="graphics.html#topic+segments">segments</a>()</code>.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_points.args">points.args</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> of additional arguments passed to
the underlying <code><a href="graphics.html#topic+points">points</a>()</code>.</p>
</td></tr>
<tr><td><code id="step_plot_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code>step_plot()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing (plot by side-effect).
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(5, 2, 4, 2, 3, 2, 2, 2, 1, 2) # example data
edf_plot(x) # empirical distribution function (edf)
edf_plot(x, log = "x")
edf_plot(x, verticals = TRUE)
edf_plot(x, do.points = FALSE)
cols &lt;- c("black", "royalblue3")
edf_plot(list(x, x+2), col = cols) # edf with shifted edf
edf_plot(list(x, x+2), col = cols, x0 = 0.5, x1 = 7.5)
edf_plot(list(x, x+2), col = cols, x0 = 0.5, x1 = 7.5, verticals = TRUE)
eqf_plot(x) # empirical quantile function
eqf_plot(x, verticals = TRUE)
</code></pre>

<hr>
<h2 id='tail_plot'>Plot of an Empirical Surival Function with Smith Estimator</h2><span id='topic+tail_plot'></span>

<h3>Description</h3>

<p>Plot an empirical tail survival function, possibly overlaid
with the Smith estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tail_plot(x, threshold, shape = NULL, scale = NULL,
          q = NULL, length.out = 129, lines.args = list(),
          log = "xy", xlim = NULL, ylim = NULL,
          xlab = "x", ylab = "Tail probability at x", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tail_plot_+3A_x">x</code></td>
<td>
<p><code><a href="base.html#topic+numeric">numeric</a></code> vector of data.</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_threshold">threshold</code></td>
<td>
<p><code>numeric(1)</code> giving the threshold <code class="reqn">u</code> above
which the tail (starts and) is to be plotted.</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_shape">shape</code></td>
<td>
<p><code>NULL</code> or the GPD shape parameter <code class="reqn">\xi</code>
(typically obtained via <code><a href="#topic+fit_GPD_MLE">fit_GPD_MLE</a>()</code>).</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_scale">scale</code></td>
<td>
<p><code>NULL</code> or the GPD shape parameter <code class="reqn">\beta</code>
(typically obtained via <code><a href="#topic+fit_GPD_MLE">fit_GPD_MLE</a>()</code>).</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_q">q</code></td>
<td>
<p><code>NULL</code>, <code>numeric(1)</code> or <code><a href="base.html#topic+numeric">numeric</a></code> vector of
evaluationn points of the  Smith estimator (semi-parametric GPD-based tail
estimator in the POT method). If <code>NULL</code>, the evaluation points
are determined internally as an equidistant sequence of length
<code>length.out</code> between the smallest and largest exceedance
(taken equidistant in log-scale if <code>log</code> contains <code>"x"</code>).
If <code>numeric(1)</code>, then the behavior is similar to <code>NULL</code>
with the exception that the plot is extended to the right of the
largest exceedance if <code>q</code> is larger than the largest
exceedance.</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_length.out">length.out</code></td>
<td>
<p>length of <code>q</code>.</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_lines.args">lines.args</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> of arguments passed to the
underlying <code><a href="graphics.html#topic+lines">lines</a>()</code>.</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_log">log</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> indicating whether
logarithmic axes are to be used.</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits.</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits.</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label.</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label.</p>
</td></tr>
<tr><td><code id="tail_plot_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="base.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If both <code>shape</code> and <code>scale</code> are provided, <code>tail_plot()</code>
overlays the empirical tail survival function estimator (evaluated at
the exceedances) with the corresponding GPD. In this case,
<code>tail_plot()</code> invisibly returns a list with two two-column
matrices, one containing the x-values and y-values of the
empirical survival distribution estimator and one containing the
x-values and y-values of the Smith estimator. If <code>shape</code> or
<code>scale</code> are <code>NULL</code>, <code>tail_plot()</code> invisibly returns
a two-column matrix with the x-values and y-values of the empirical
survival distribution estimator.
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate losses to work with
set.seed(271)
X &lt;- rt(1000, df = 3.5) # in MDA(H_{1/df}); see MFE (2015, Section 16.1.1)

## Threshold (see ?dGPDtail, for example)
u &lt;- 1.5 # threshold

## Plots of empirical survival distribution functions (overlaid with Smith estimator)
tail_plot(X, threshold = u, log = "", type = "b") # =&gt; need log-scale
tail_plot(X, threshold = u, type = "s") # as a step function
fit &lt;- fit_GPD_MLE(X[X &gt; u] - u) # fit GPD to excesses (POT method)
tail_plot(X, threshold = u, # without log-scale
          shape = fit$par[["shape"]], scale = fit$par[["scale"]], log = "")
tail_plot(X, threshold = u, # highlights linearity
          shape = fit$par[["shape"]], scale = fit$par[["scale"]])
</code></pre>

<hr>
<h2 id='tests'>Formal Tests of Multivariate Normality</h2><span id='topic+maha2_test'></span><span id='topic+mardia_test'></span>

<h3>Description</h3>

<p>Compute formal tests based on the Mahalanobis distances and
Mahalanobis angles of multivariate normality (including Mardia's
kurtosis test and Mardia's skewness test).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maha2_test(x, type = c("ad.test", "ks.test"), dist = c("chi2", "beta"), ...)
mardia_test(x, type = c("kurtosis", "skewness"), method = c("direct", "chol"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tests_+3A_x">x</code></td>
<td>
<p>(n, d)-matrix of data.</p>
</td></tr>
<tr><td><code id="tests_+3A_type">type</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string indicating the type of test:
</p>

<dl>
<dt><code>"ad.test"</code>:</dt><dd><p>Anderson-Darling test as computed by the
underlying <code><a href="ADGofTest.html#topic+ad.test">ad.test</a>()</code>.</p>
</dd>
<dt><code>"ks.test"</code>:</dt><dd><p>Kolmogorov-Smirnov test as computed by the
underlying <code><a href="stats.html#topic+ks.test">ks.test</a>()</code>.</p>
</dd>
<dt><code>"kurtosis"</code>:</dt><dd><p>Mardia's kurtosis test (based on Mahalanobis
distances).</p>
</dd>
<dt><code>"skewness"</code>:</dt><dd><p>Mardia's skewness test (based on Mahalanobis
angles).</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="tests_+3A_dist">dist</code></td>
<td>
<p>distribution to check against.</p>
</td></tr>
<tr><td><code id="tests_+3A_method">method</code></td>
<td>
<p>method for computing the Mahalanobis angles.</p>
</td></tr>
<tr><td><code id="tests_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
<code><a href="ADGofTest.html#topic+ad.test">ad.test</a>()</code> or <code><a href="stats.html#topic+ks.test">ks.test</a>()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>htest</code> object (for <code>maha2_test</code> the one returned
by the underlying <code><a href="ADGofTest.html#topic+ad.test">ad.test</a>()</code> or <code><a href="stats.html#topic+ks.test">ks.test</a>()</code>).
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(271)
U &lt;- matrix(runif(3 * 200), ncol = 3)
X &lt;- cbind(qexp(U[,1]), qnorm(U[,2:3]))
maha2_test(X) # at the 'edge' of rejecting
maha2_test(X, type = "ks.test") # at the 'edge', too
mardia_test(X) # clearly rejects at 5%
mardia_test(X, type = "skewness") # clearly rejects at 5%
</code></pre>

<hr>
<h2 id='VaR_ES_bounds_analytical'>&ldquo;Analytical&rdquo; Best and Worst Value-at-Risk for Given Marginals</h2><span id='topic+crude_VaR_bounds'></span><span id='topic+VaR_bounds_hom'></span><span id='topic+dual_bound'></span>

<h3>Description</h3>

<p>Compute the best and worst Value-at-Risk (VaR) for given marginal
distributions with an &ldquo;analytical&rdquo; method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## ``Analytical'' methods
crude_VaR_bounds(level, qF, d = NULL, ...)
VaR_bounds_hom(level, d, method = c("Wang", "Wang.Par", "dual"),
               interval = NULL, tol = NULL, ...)
dual_bound(s, d, pF, tol = .Machine$double.eps^0.25, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VaR_ES_bounds_analytical_+3A_level">level</code></td>
<td>
<p>confidence level <code class="reqn">\alpha</code> for VaR and ES
(e.g., 0.99).</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_analytical_+3A_qf">qF</code></td>
<td>
<p><code>d</code>-list containing the marginal quantile functions.
In the homogeneous case, <code>qF</code> can also be a single function.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_analytical_+3A_d">d</code></td>
<td>
<p>dimension (number of risk factors; <code class="reqn">\ge 2</code>). For
<code>crude_VaR_bounds()</code>, <code>d</code> only needs to be given in
the homogeneous case in which <code>qF</code> is a <code><a href="base.html#topic+function">function</a></code>.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_analytical_+3A_method">method</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string. <code>method = "Wang"</code> and
<code>method = "Wang.Par"</code>
apply the approach of McNeil et al. (2015,
Proposition 8.32) for computing best (i.e., smallest) and
worst (i.e., largest) VaR. The
latter method assumes Pareto margins and thus does
not require numerical integration.
<code>method = "dual"</code> applies the dual bound approach as
in Embrechts et al. (2013, Proposition 4)
for computing worst VaR (no value for the best
VaR can be obtained with this approach and thus
<code><a href="base.html#topic+NA">NA</a></code> is returned for the best VaR).</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_analytical_+3A_interval">interval</code></td>
<td>
<p>initial interval (a <code><a href="base.html#topic+numeric">numeric</a>(2)</code>) for
computing worst VaR. If not provided, these are the defaults chosen:
</p>

<dl>
<dt><code>method = "Wang"</code>:</dt><dd><p>initial
interval is <code class="reqn">[0,(1-\alpha)/d]</code>.</p>
</dd>
<dt><code>method = "Wang.Par"</code>:</dt><dd><p>initial
interval is <code class="reqn">[c_l,c_u]</code>, where <code class="reqn">c_l</code> and <code class="reqn">c_u</code>
are chosen as in Hofert et al. (2015).</p>
</dd>
<dt><code>method = "dual"</code>:</dt><dd><p>in this case, no good defaults are known.
Note that the lower endpoint of the initial interval has to be
sufficiently large in order for the the inner root-finding algorithm
to find a root; see Details.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="VaR_ES_bounds_analytical_+3A_tol">tol</code></td>
<td>
<p>tolerance for
<code><a href="stats.html#topic+uniroot">uniroot</a>()</code>
for computing worst VaR. This defaults (for <code>tol = NULL</code>) to
<code class="reqn">2.2204*10^{-16}</code>
for <code>method = "Wang"</code> or <code>method = "Wang.Par"</code> (where a
smaller tolerance is crucial) and to <code><a href="stats.html#topic+uniroot">uniroot</a>()</code>'s
default <code>.Machine$double.eps^0.25</code> otherwise. Note that for
<code>method = "dual"</code>, <code>tol</code> is used for both the outer
and the inner root-finding procedure.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_analytical_+3A_s">s</code></td>
<td>
<p>dual bound evaluation point.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_analytical_+3A_pf">pF</code></td>
<td>
<p>marginal loss distribution function (homogeneous case only).</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_analytical_+3A_...">...</code></td>
<td>


<dl>
<dt><code>crude_VaR_bounds()</code>:</dt><dd><p>ellipsis argument passed
to (all provided) quantile functions.</p>
</dd>
<dt><code>VaR_bounds_hom()</code>:</dt><dd><p>case <code>method = "Wang"</code>
requires the quantile function <code>qF()</code> to be provided
and additional arguments passed via the ellipsis argument are passed
on to


the underlying <code><a href="stats.html#topic+integrate">integrate</a>()</code>. For
<code>method = "Wang.Par"</code>
the ellipsis argument must contain the parameter <code>shape</code>
(the shape parameter <code class="reqn">\theta&gt;0</code> of the Pareto distribution).
For <code>method = "dual"</code>, the ellipsis argument must contain the
distribution function <code>pF()</code> and the initial interval
<code>interval</code> for the outer root finding procedure (not for
<code>d = 2</code>); additional arguments are passed on to the underlying
<code><a href="stats.html#topic+integrate">integrate</a>()</code> for computing the dual bound
<code class="reqn">D(s)</code>.</p>
</dd>
<dt><code>dual_bound()</code>:</dt><dd><p>ellipsis argument is passed to the
underlying <code><a href="stats.html#topic+integrate">integrate</a>()</code>.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>d = 2</code>, <code>VaR_bounds_hom()</code> uses the method of
Embrechts et al. (2013,
Proposition 2). For <code>method = "Wang"</code> and <code>method = "Wang.Par"</code>
the method presented in McNeil et al. (2015, Prop. 8.32) is
implemented; this goes back to Embrechts et al. (2014, Prop. 3.1; note that
the published version of this paper contains typos for both bounds).
This requires
one <code><a href="stats.html#topic+uniroot">uniroot</a>()</code> and, for the generic <code>method = "Wang"</code>,
one <code><a href="stats.html#topic+integrate">integrate</a>()</code>. The critical part for the
generic <code>method = "Wang"</code> is the lower endpoint of the initial
interval for <code><a href="stats.html#topic+uniroot">uniroot</a>()</code>. If the (marginal)
distribution function has finite first moment, this can be taken as
0. However, if it has infinite first moment, the lower endpoint has to
be positive (but must lie below the unknown root). Note that the upper
endpoint <code class="reqn">(1-\alpha)/d</code> also happens to be a
root and thus one needs a proper initional interval containing the
root and being stricticly contained in
<code class="reqn">(0,(1-\alpha)/d</code>.
In the case of Pareto margins, Hofert et al. (2015) have
derived such an initial (which is used by
<code>method = "Wang.Par"</code>).
Also note that the chosen smaller default tolerances for
<code><a href="stats.html#topic+uniroot">uniroot</a>()</code> in case of <code>method = "Wang"</code> and
<code>method = "Wang.Par"</code> are crucial for obtaining reliable
VaR values; see Hofert et al. (2015).
</p>
<p>For <code>method = "dual"</code> for computing worst VaR, the method
presented of Embrechts et al. (2013, Proposition 4) is implemented.
This requires two (nested) <code><a href="stats.html#topic+uniroot">uniroot</a>()</code>, and an
<code><a href="stats.html#topic+integrate">integrate</a>()</code>. For the inner root-finding procedure to
find a root, the lower endpoint of the provided initial
<code>interval</code> has to be &ldquo;sufficiently large&rdquo;.
</p>
<p>Note that these approaches for computing the
VaR bounds in the homogeneous case are numerically non-trivial;
see the source code and <code>vignette("VaR_bounds",
    package = "qrmtools")</code>
for more details. As a
rule of thumb, use <code>method = "Wang"</code> if you have to (i.e., if the
margins are not Pareto) and <code>method = "Wang.Par"</code> if you can (i.e.,
if the margins are Pareto). It is not recommended to use
(the numerically even more challenging) <code>method = "dual"</code>.
</p>


<h3>Value</h3>

<p><code>crude_VaR_bounds()</code> returns crude lower and upper bounds for
VaR at confidence level <code class="reqn">\alpha</code> for any
<code class="reqn">d</code>-dimensional model with marginal quantile functions
specified by <code>qF</code>.
</p>
<p><code>VaR_bounds_hom()</code> returns the best and worst VaR at
confidence level <code class="reqn">\alpha</code> for <code class="reqn">d</code> risks with equal
distribution function specified by the ellipsis <code>...</code>.
</p>
<p><code>dual_bound()</code> returns the value of the dual bound <code class="reqn">D(s)</code> as
given in Embrechts, Puccetti, Rschendorf
(2013, Eq. (12)).
</p>


<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>Embrechts, P., Puccetti, G., Rschendorf, L.,
Wang, R. and Beleraj, A. (2014).  An Academic Response to Basel
3.5. <em>Risks</em> <b>2</b>(1), 25&ndash;48.
</p>
<p>Embrechts, P., Puccetti, G. and Rschendorf, L. (2013).
Model uncertainty and VaR aggregation. <em>Journal of Banking &amp;
Finance</em> <b>37</b>, 2750&ndash;2764.
</p>
<p>McNeil, A. J., Frey, R. and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>
<p>Hofert, M., Memartoluie, A., Saunders, D. and Wirjanto, T. (2017).
Improved Algorithms for Computing Worst
Value-at-Risk. <em>Statistics &amp; Risk Modeling</em>
or, for an earlier version, <a href="https://arxiv.org/abs/1505.02281">https://arxiv.org/abs/1505.02281</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+RA">RA</a>()</code>, <code><a href="#topic+ARA">ARA</a>()</code>, <code><a href="#topic+ABRA">ABRA</a>()</code>
for empirical solutions in the inhomogeneous case.
</p>
<p><code>vignette("VaR_bounds", package = "qrmtools")</code>
for more example calls, numerical challenges
encoutered and a comparison of the different methods for computing
the worst (i.e., largest) Value-at-Risk.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See ?rearrange
</code></pre>

<hr>
<h2 id='VaR_ES_bounds_rearrange'>Worst and Best Value-at-Risk and Best Expected Shortfall
for Given Marginals via Rearrangements</h2><span id='topic+rearrange'></span><span id='topic+block_rearrange'></span><span id='topic+RA'></span><span id='topic+ARA'></span><span id='topic+ABRA'></span>

<h3>Description</h3>

<p>Compute the worst and best Value-at-Risk (VaR) and the best expected
shortfall (ES) for given marginal distributions via rearrangements.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Workhorses
## Column rearrangements
rearrange(X, tol = 0, tol.type = c("relative", "absolute"),
          n.lookback = ncol(X), max.ra = Inf,
          method = c("worst.VaR", "best.VaR", "best.ES"),
	  sample = TRUE, is.sorted = FALSE, trace = FALSE, ...)
## Block rearrangements
block_rearrange(X, tol = 0, tol.type = c("absolute", "relative"),
                n.lookback = ncol(X), max.ra = Inf,
                method = c("worst.VaR", "best.VaR", "best.ES"),
                sample = TRUE, trace = FALSE, ...)

## User interfaces
## Rearrangement Algorithm
RA(level, qF, N, abstol = 0, n.lookback = length(qF), max.ra = Inf,
   method = c("worst.VaR", "best.VaR", "best.ES"), sample = TRUE)
## Adaptive Rearrangement Algorithm
ARA(level, qF, N.exp = seq(8, 19, by = 1), reltol = c(0, 0.01),
    n.lookback = length(qF), max.ra = 10*length(qF),
    method = c("worst.VaR", "best.VaR", "best.ES"),
    sample = TRUE)
## Adaptive Block Rearrangement Algorithm
ABRA(level, qF, N.exp = seq(8, 19, by = 1), absreltol = c(0, 0.01),
     n.lookback = NULL, max.ra = Inf,
     method = c("worst.VaR", "best.VaR", "best.ES"),
     sample = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_x">X</code></td>
<td>
<p>(<code>N</code>, <code>d</code>)-matrix of quantiles (to be
rearranged). If <code>is.sorted</code> it is assumed that the columns of
<code>X</code> are sorted in <em>increasing</em> order.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_tol">tol</code></td>
<td>
<p>(absolute or relative) tolerance to determine (the
individual) convergence.  This should normally be a number
greater than or equal to 0, but <code>rearrange()</code> also allows
for <code>tol = NULL</code> which means that columns are rearranged
until each column is oppositely ordered to the sum of all other
columns.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_tol.type">tol.type</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string indicating the
type of convergence tolerance function to be used (<code>"relative"</code>
for relative tolerance and <code>"absolute"</code> for absolute tolerance).</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_n.lookback">n.lookback</code></td>
<td>
<p>number of rearrangements to look back for deciding
about numerical convergence. Use this option with care.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_max.ra">max.ra</code></td>
<td>
<p>maximal number of (considered) column rearrangements
of the underlying matrix of quantiles (can be set to <code>Inf</code>).</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_method">method</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> string indicating whether bounds
for the worst/best VaR or the best ES should be computed.
These bounds are termed <code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code>
in the literature (and below) and are theoretically not guaranteed
bounds of worst/best VaR or best ES;
however, they are treated as such in practice and are typically in
line with results from <code><a href="#topic+VaR_bounds_hom">VaR_bounds_hom</a>()</code> in the homogeneous
case, for example.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_sample">sample</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether each column of
the two underlying matrices of quantiles (see Step 3 of the Rearrangement
Algorithm in Embrechts et al. (2013))
are randomly permuted before the rearrangements begin. This typically has
quite a positive effect on run time (as most of the time is spent
(oppositely) ordering columns (for <code>rearrange()</code>) or blocks
(for <code>block_rearrange()</code>)).</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_is.sorted">is.sorted</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether the columns of
<code>X</code> are sorted in increasing order.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_trace">trace</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating whether the underlying
matrix is printed after each rearrangement step. See
<code>vignette("VaR_bounds", package = "qrmtools")</code> for how to interpret
the output.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_level">level</code></td>
<td>
<p>confidence level <code class="reqn">\alpha</code> for VaR and ES (e.g., 0.99).</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_qf">qF</code></td>
<td>
<p><code>d</code>-list containing the marginal quantile functions.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_n">N</code></td>
<td>
<p>number of discretization points.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_abstol">abstol</code></td>
<td>
<p>absolute convergence tolerance <code class="reqn">\epsilon</code>
to determine the individual convergence, i.e., the change in the computed
minimal row sums (for <code>method = "worst.VaR"</code>) or maximal
row sums (for <code>method = "best.VaR"</code>) or expected shortfalls (for
<code>method = "best.ES"</code>) for the lower bound <code class="reqn">\underline{s}_N</code>
and the upper bound <code class="reqn">\overline{s}_N</code>. <code>abstol</code> is typically
<code class="reqn">\ge0</code>; it can also be <code><a href="base.html#topic+NULL">NULL</a></code>, see <code>tol</code>
above.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_n.exp">N.exp</code></td>
<td>
<p>exponents of the number of discretization points
(a <code><a href="base.html#topic+vector">vector</a></code>) over which the algorithm iterates to find
the smallest number of discretization points for which the desired
accuracy (specified by <code>abstol</code> and <code>reltol</code>) is attained;
for each number of discretization points, at most <code>max.ra</code>-many
column rearrangements are of the underlying matrix of quantiles
are considered.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_reltol">reltol</code></td>
<td>
<p><code><a href="base.html#topic+vector">vector</a></code> of length two containing the
individual (first component; used to determine convergence of the
minimal row sums (for <code>method = "worst.VaR"</code>) or maximal row
sums (for <code>method = "best.VaR"</code>) or expected shortfalls (for
<code>method = "best.ES"</code>) for <code class="reqn">\underline{s}_N</code> and
<code class="reqn">\overline{s}_N</code>) and the joint (second component; relative
tolerance between the computed <code class="reqn">\underline{s}_N</code> and
<code class="reqn">\overline{s}_N</code> with respect to <code class="reqn">\overline{s}_N</code>) relative
convergence tolerances. <code>reltol</code> can also be of length one
in which case it denotes the joint relative tolerance; the
individual relative tolerance is taken as <code>NULL</code>
(see <code>tol</code> above) in this case.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_absreltol">absreltol</code></td>
<td>
<p><code><a href="base.html#topic+vector">vector</a></code> of length two containing the
individual (first component; used to determine convergence of the
minimal row sums (for <code>method = "worst.VaR"</code>) or maximal row
sums (for <code>method = "best.VaR"</code>) or expected shortfalls (for
<code>method = "best.ES"</code>) for <code class="reqn">\underline{s}_N</code> and
<code class="reqn">\overline{s}_N</code>) absolute and the joint (second component; relative
tolerance between the computed <code class="reqn">\underline{s}_N</code> and
<code class="reqn">\overline{s}_N</code> with respect to <code class="reqn">\overline{s}_N</code>) relative
convergence tolerances. <code>absreltol</code> can also be of length one
in which case it denotes the joint relative tolerance; the
individual absolute tolerance is taken as 0 in this case.</p>
</td></tr>
<tr><td><code id="VaR_ES_bounds_rearrange_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying
optimization function. Currently, this is only used if
<code>method = "best.ES"</code> in which case the required confidence
level <code class="reqn">\alpha</code> must be provided as argument <code>level</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>rearrange()</code> is an auxiliary
function (workhorse). It is called by <code>RA()</code> and <code>ARA()</code>.
After a column rearrangement of <code>X</code>, the tolerance between the
minimal row sum (for the worst VaR) or maximal row sum (for the best
VaR) or expected shortfall (obtained from the row sums; for the best
ES) after this rearrangement and the one of <code class="reqn">n.lookback</code>
rearrangement steps before is computed and convergence determined.
For performance reasons, no input checking is done for
<code>rearrange()</code> and it can change in future versions to (futher)
improve run time. Overall it should only be used by experts.
</p>
<p><code>block_rearrange()</code>, the workhorse underlying <code>ABRA()</code>,
is similar to <code>rearrange()</code> in that it
checks whether convergence has occurred after every rearrangement by
comparing the change to the row sum variance from <code>n.lookback</code>
rearrangement steps back. <code>block_rearrange()</code> differs from
<code>rearrange</code> in the following ways. First, instead of single columns,
whole (randomly chosen) blocks (two at a time) are chosen and
oppositely ordered. Since some of the ideas for improving the speed of
<code>rearrange()</code> do not carry over to <code>block_rearrange()</code>, the
latter should in general not be as fast as the former.
Second, instead of using minimal or maximal row
sums or expected shortfall to determine numerical convergence,
<code>block_rearrange()</code> uses the variance of the vector of row sums
to determine numerical convergence. By default, it targets a variance
of 0 (which is also why the default <code>tol.type</code> is <code>"absolute"</code>).
</p>
<p>For the Rearrangement Algorithm <code>RA()</code>, convergence of
<code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code> is determined if the
minimal row sum (for the worst VaR) or maximal row sum (for the best
VaR) or expected shortfall (obtained from the row sums; for the best ES)
satisfies the specified <code>abstol</code> (so <code class="reqn">\le\epsilon</code>)
after at most <code>max.ra</code>-many column rearrangements. This is different
from Embrechts et al. (2013) who use <code class="reqn">&lt;\epsilon</code> and
only check for convergence after an iteration through all
columns of the underlying matrix of quantiles has been completed.
</p>
<p>For the Adaptive Rearrangement Algorithm <code>ARA()</code>
and the Adaptive Block Rearrangement Algorithm <code>ABRA()</code>,
convergence of <code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code>
is determined if, after at most <code>max.ra</code>-many column
rearrangements, the (the individual relative tolerance)
<code>reltol[1]</code> is satisfied <em>and</em> the
relative (joint) tolerance between both bounds is at most <code>reltol[2]</code>.
</p>
<p>Note that <code>RA()</code>, <code>ARA()</code> and <code>ABRA()</code> need to evalute the
0-quantile (for the lower bound for the best VaR) and
the 1-quantile (for the upper bound for the
worst VaR). As the algorithms, due to performance reasons, can only
handle finite values, the 0-quantile and the 1-quantile need to be
adjusted if infinite. Instead of the 0-quantile,
the <code class="reqn">\alpha/(2N)</code>-quantile is
computed and instead of the 1-quantile the
<code class="reqn">\alpha+(1-\alpha)(1-1/(2N))</code>-quantile
is computed for such margins (if the 0-quantile or the 1-quantile is
finite, no adjustment is made).
</p>
<p><code>rearrange()</code>, <code>block_rearrange()</code>, <code>RA()</code>, <code>ARA()</code>
and <code>ABRA()</code> compute <code class="reqn">\underline{s}_N</code> and
<code class="reqn">\overline{s}_N</code> which are, from a practical
point of view, treated as bounds for the worst (i.e., largest) or the
best (i.e., smallest) VaR or the best (i.e., smallest ES), but which are
not known to be such bounds from a theoretical point of view; see also above.
Calling them &ldquo;bounds&rdquo; for worst/best VaR or best ES is thus
theoretically not correct (unless proven) but &ldquo;practical&rdquo;.
The literature thus speaks of <code class="reqn">(\underline{s}_N, \overline{s}_N)</code> as
the rearrangement gap.
</p>


<h3>Value</h3>

<p><code>rearrange()</code> and <code>block_rearrange()</code> return a
<code><a href="base.html#topic+list">list</a></code> containing
</p>

<dl>
<dt><code>bound</code>:</dt><dd><p>computed <code class="reqn">\underline{s}_N</code>
or <code class="reqn">\overline{s}_N</code>.</p>
</dd>
<dt><code>tol</code>:</dt><dd><p>reached tolerance (i.e., the (absolute or
relative) change of the minimal row sum (for
<code>method = "worst.VaR"</code>) or maximal row sum
(for <code>method = "best.VaR"</code>) or expected shortfall (for
<code>method = "best.ES"</code>) after the last rearrangement).</p>
</dd>
<dt><code>converged</code>:</dt><dd><p><code><a href="base.html#topic+logical">logical</a></code> indicating whether
the desired (absolute or relative) tolerance <code>tol</code> has been
reached.</p>
</dd>
<dt><code>opt.row.sums</code>:</dt><dd><p><code><a href="base.html#topic+vector">vector</a></code> containing the
computed optima (minima for <code>method = "worst.VaR"</code>; maxima
for <code>method = "best.VaR"</code>; expected shortfalls for
<code>method = "best.ES"</code>) for the row sums after each (considered)
rearrangement.</p>
</dd>
<dt><code>X.rearranged</code>:</dt><dd><p>(<code>N</code>, <code>d</code>)-<code><a href="base.html#topic+matrix">matrix</a></code>
containing the rearranged <code>X</code>.</p>
</dd>
<dt><code>X.rearranged.opt.row</code>:</dt><dd><p><code><a href="base.html#topic+vector">vector</a></code> containing
the row of <code>X.rearranged</code> which leads to the final optimal
sum. If there is more than one such row, the columnwise averaged
row is returned.</p>
</dd>
</dl>

<p><code>RA()</code> returns a <code><a href="base.html#topic+list">list</a></code> containing
</p>

<dl>
<dt><code>bounds</code>:</dt><dd><p>bivariate vector containing the computed
<code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code> (the so-called
rearrangement range) which are typically treated as bounds for
worst/best VaR or best ES; see also above.</p>
</dd>
<dt><code>rel.ra.gap</code>:</dt><dd><p>reached relative tolerance (also known as
relative rearrangement gap) between
<code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code> computed with
respect to <code class="reqn">\overline{s}_N</code>.</p>
</dd>
<dt><code>ind.abs.tol</code>:</dt><dd><p>bivariate <code><a href="base.html#topic+vector">vector</a></code> containing
the reached individual absolute tolerances (i.e., the absolute change
of the minimal row sums
(for <code>method = "worst.VaR"</code>) or maximal row sums
(for <code>method = "best.VaR"</code>) or expected shortfalls
(for <code>mehtod = "best.ES"</code>) for computing <code class="reqn">\underline{s}_N</code>
and <code class="reqn">\overline{s}_N</code>;
see also <code>tol</code> returned by <code>rearrange()</code> above).</p>
</dd>
<dt><code>converged</code>:</dt><dd><p>bivariate <code><a href="base.html#topic+logical">logical</a></code> vector
indicating convergence of the computed <code class="reqn">\underline{s}_N</code> and
<code class="reqn">\overline{s}_N</code> (i.e., whether the desired tolerances were
reached).</p>
</dd>
<dt><code>num.ra</code>:</dt><dd><p>bivariate vector containing the number
of column rearrangments of the underlying matrices
of quantiles for
<code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code>.</p>
</dd>
<dt><code>opt.row.sums</code>:</dt><dd><p><code><a href="base.html#topic+list">list</a></code> of length two containing
the computed optima (minima for <code>method = "worst.VaR"</code>; maxima
for <code>method = "best.VaR"</code>; expected shortfalls for
<code>method = "best.ES"</code>) for the row sums after each
(considered) column rearrangement for the computed
<code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code>; see also
<code>rearrange()</code>.</p>
</dd>
<dt><code>X</code>:</dt><dd><p>initially constructed (<code>N</code>, <code>d</code>)-matrices
of quantiles for computing
<code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code>.</p>
</dd>
<dt><code>X.rearranged</code>:</dt><dd><p>rearranged matrices <code>X</code> for
<code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code>.</p>
</dd>
<dt><code>X.rearranged.opt.row</code>:</dt><dd><p>rows corresponding to optimal
row sum (see <code>X.rearranged.opt.row</code> as returned by
<code>rearrange()</code>) for <code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code>.</p>
</dd>
</dl>

<p><code>ARA()</code> and <code>ABRA()</code> return a <code><a href="base.html#topic+list">list</a></code> containing
</p>

<dl>
<dt><code>bounds</code>:</dt><dd><p>see <code>RA()</code>.</p>
</dd>
<dt><code>rel.ra.gap</code>:</dt><dd><p>see <code>RA()</code>.</p>
</dd>
<dt><code>tol</code>:</dt><dd><p>trivariate <code><a href="base.html#topic+vector">vector</a></code> containing
the reached individual (relative for <code>ARA()</code>; absolute for
<code>ABRA()</code>) tolerances and the reached joint
relative tolerance (computed with respect to <code class="reqn">\overline{s}_N</code>).</p>
</dd>
<dt><code>converged</code>:</dt><dd><p>trivariate <code><a href="base.html#topic+logical">logical</a></code>
<code><a href="base.html#topic+vector">vector</a></code> indicating individual convergence of the computed
<code class="reqn">\underline{s}_N</code> (first entry) and <code class="reqn">\overline{s}_N</code>
(second entry) and indicating joint convergence of the two bounds
according to the attained joint relative tolerance (third entry).</p>
</dd>
<dt><code>N.used</code>:</dt><dd><p>actual <code>N</code> used for computing
the (final) <code class="reqn">\underline{s}_N</code> and <code class="reqn">\overline{s}_N</code>.</p>
</dd>
<dt><code>num.ra</code>:</dt><dd><p>see <code>RA()</code>; computed for <code>N.used</code>.</p>
</dd>
<dt><code>opt.row.sums</code>:</dt><dd><p>see <code>RA()</code>; computed for <code>N.used</code>.</p>
</dd>
<dt><code>X</code>:</dt><dd><p>see <code>RA()</code>; computed for <code>N.used</code>.</p>
</dd>
<dt><code>X.rearranged</code>:</dt><dd><p>see <code>RA()</code>; computed for <code>N.used</code>.</p>
</dd>
<dt><code>X.rearranged.opt.row</code>:</dt><dd><p>see <code>RA()</code>; computed for <code>N.used</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>Embrechts, P., Puccetti, G., Rschendorf, L.,
Wang, R. and Beleraj, A. (2014).  An Academic Response to Basel
3.5. <em>Risks</em> <b>2</b>(1), 25&ndash;48.
</p>
<p>Embrechts, P., Puccetti, G. and Rschendorf, L. (2013).
Model uncertainty and VaR aggregation. <em>Journal of Banking &amp;
Finance</em> <b>37</b>, 2750&ndash;2764.
</p>
<p>McNeil, A. J., Frey, R. and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>
<p>Hofert, M., Memartoluie, A., Saunders, D. and Wirjanto, T. (2017).
Improved Algorithms for Computing Worst
Value-at-Risk. <em>Statistics &amp; Risk Modeling</em>
or, for an earlier version, <a href="https://arxiv.org/abs/1505.02281">https://arxiv.org/abs/1505.02281</a>.
</p>
<p>Bernard, C., Rschendorf, L. and Vanduffel,
S. (2013). Value-at-Risk bounds with variance constraints.
See <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2342068">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2342068</a>.
</p>
<p>Bernard, C. and McLeish, D. (2014). Algorithms for Finding Copulas
Minimizing Convex Functions of Sums.
See <a href="https://arxiv.org/abs/1502.02130v3">https://arxiv.org/abs/1502.02130v3</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+VaR_bounds_hom">VaR_bounds_hom</a>()</code> for an &ldquo;analytical&rdquo; approach for
computing best and worst Value-at-Risk in the homogeneous casse.
</p>
<p><code>vignette("VaR_bounds", package = "qrmtools")</code>
for more example calls, numerical challenges
encoutered and a comparison of the different methods for computing
the worst (i.e., largest) Value-at-Risk.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### 1 Reproducing selected examples of McNeil et al. (2015; Table 8.1) #########

## Setup
alpha &lt;- 0.95
d &lt;- 8
theta &lt;- 3
qF &lt;- rep(list(function(p) qPar(p, shape = theta)), d)

## Worst VaR
N &lt;- 5e4
set.seed(271)
system.time(RA.worst.VaR &lt;- RA(alpha, qF = qF, N = N, method = "worst.VaR"))
RA.worst.VaR$bounds
stopifnot(RA.worst.VaR$converged,
          all.equal(RA.worst.VaR$bounds[["low"]],
                    RA.worst.VaR$bounds[["up"]], tol = 1e-4))

## Best VaR
N &lt;- 5e4
set.seed(271)
system.time(RA.best.VaR &lt;- RA(alpha, qF = qF, N = N, method = "best.VaR"))
RA.best.VaR$bounds
stopifnot(RA.best.VaR$converged,
          all.equal(RA.best.VaR$bounds[["low"]],
                    RA.best.VaR$bounds[["up"]], tol = 1e-4))

## Best ES
N &lt;- 5e4 # actually, we need a (much larger) N here (but that's time consuming)
set.seed(271)
system.time(RA.best.ES &lt;- RA(alpha, qF = qF, N = N, method = "best.ES"))
RA.best.ES$bounds
stopifnot(RA.best.ES$converged,
          all.equal(RA.best.ES$bounds[["low"]],
                    RA.best.ES$bounds[["up"]], tol = 5e-1))


### 2 More Pareto examples (d = 2, d = 8; hom./inhom. case; explicit/RA/ARA) ###

alpha &lt;- 0.99 # VaR confidence level
th &lt;- 2 # Pareto parameter theta
qF &lt;- function(p, theta = th) qPar(p, shape = theta) # Pareto quantile function
pF &lt;- function(q, theta = th) pPar(q, shape = theta) # Pareto distribution function


### 2.1 The case d = 2 #########################################################

d &lt;- 2 # dimension

## ``Analytical''
VaRbounds &lt;- VaR_bounds_hom(alpha, d = d, qF = qF) # (best VaR, worst VaR)

## Adaptive Rearrangement Algorithm (ARA)
set.seed(271) # set seed (for reproducibility)
ARAbest  &lt;- ARA(alpha, qF = rep(list(qF), d), method = "best.VaR")
ARAworst &lt;- ARA(alpha, qF = rep(list(qF), d))

## Rearrangement Algorithm (RA) with N as in ARA()
RAbest  &lt;- RA(alpha, qF = rep(list(qF), d), N = ARAbest$N.used, method = "best.VaR")
RAworst &lt;- RA(alpha, qF = rep(list(qF), d), N = ARAworst$N.used)

## Compare
stopifnot(all.equal(c(ARAbest$bounds[1], ARAbest$bounds[2],
                       RAbest$bounds[1],  RAbest$bounds[2]),
                    rep(VaRbounds[1], 4), tolerance = 0.004, check.names = FALSE))
stopifnot(all.equal(c(ARAworst$bounds[1], ARAworst$bounds[2],
                       RAworst$bounds[1],  RAworst$bounds[2]),
                    rep(VaRbounds[2], 4), tolerance = 0.003, check.names = FALSE))


### 2.2 The case d = 8 #########################################################

d &lt;- 8 # dimension

## ``Analytical''
I &lt;- crude_VaR_bounds(alpha, qF = qF, d = d) # crude bound
VaR.W     &lt;- VaR_bounds_hom(alpha, d = d, method = "Wang", qF = qF)
VaR.W.Par &lt;- VaR_bounds_hom(alpha, d = d, method = "Wang.Par", shape = th)
VaR.dual  &lt;- VaR_bounds_hom(alpha, d = d, method = "dual", interval = I, pF = pF)

## Adaptive Rearrangement Algorithm (ARA) (with different relative tolerances)
set.seed(271) # set seed (for reproducibility)
ARAbest  &lt;- ARA(alpha, qF = rep(list(qF), d), reltol = c(0.001, 0.01), method = "best.VaR")
ARAworst &lt;- ARA(alpha, qF = rep(list(qF), d), reltol = c(0.001, 0.01))

## Rearrangement Algorithm (RA) with N as in ARA and abstol (roughly) chosen as in ARA
RAbest  &lt;- RA(alpha, qF = rep(list(qF), d), N = ARAbest$N.used,
              abstol = mean(tail(abs(diff(ARAbest$opt.row.sums$low)), n = 1),
                            tail(abs(diff(ARAbest$opt.row.sums$up)), n = 1)),
              method = "best.VaR")
RAworst &lt;- RA(alpha, qF = rep(list(qF), d), N = ARAworst$N.used,
              abstol = mean(tail(abs(diff(ARAworst$opt.row.sums$low)), n = 1),
                            tail(abs(diff(ARAworst$opt.row.sums$up)), n = 1)))

## Compare
stopifnot(all.equal(c(VaR.W[1], ARAbest$bounds, RAbest$bounds),
                    rep(VaR.W.Par[1],5), tolerance = 0.004, check.names = FALSE))
stopifnot(all.equal(c(VaR.W[2], VaR.dual[2], ARAworst$bounds, RAworst$bounds),
                    rep(VaR.W.Par[2],6), tolerance = 0.003, check.names = FALSE))

## Using (some of) the additional results computed by (A)RA()
xlim &lt;- c(1, max(sapply(RAworst$opt.row.sums, length)))
ylim &lt;- range(RAworst$opt.row.sums)
plot(RAworst$opt.row.sums[[2]], type = "l", xlim = xlim, ylim = ylim,
     xlab = "Number or rearranged columns",
     ylab = paste0("Minimal row sum per rearranged column"),
     main = substitute("Worst VaR minimal row sums ("*alpha==a.*","~d==d.*" and Par("*
                       th.*"))", list(a. = alpha, d. = d, th. = th)))
lines(1:length(RAworst$opt.row.sums[[1]]), RAworst$opt.row.sums[[1]], col = "royalblue3")
legend("bottomright", bty = "n", lty = rep(1,2),
       col = c("black", "royalblue3"), legend = c("upper bound", "lower bound"))
## =&gt; One should use ARA() instead of RA()


### 3 "Reproducing" examples from Embrechts et al. (2013) ######################

### 3.1 "Reproducing" Table 1 (but seed and eps are unknown) ###################

## Left-hand side of Table 1
N &lt;- 50
d &lt;- 3
qPar &lt;- rep(list(qF), d)
p &lt;- alpha + (1-alpha)*(0:(N-1))/N # for 'worst' (= largest) VaR
X &lt;- sapply(qPar, function(qF) qF(p))
cbind(X, rowSums(X))

## Right-hand side of Table 1
set.seed(271)
res &lt;- RA(alpha, qF = qPar, N = N)
row.sum &lt;- rowSums(res$X.rearranged$low)
cbind(res$X.rearranged$low, row.sum)[order(row.sum),]


### 3.2 "Reproducing" Table 3 for alpha = 0.99 #################################

## Note: The seed for obtaining the exact results as in Table 3 is unknown
N &lt;- 2e4 # we use a smaller N here to save run time
eps &lt;- 0.1 # absolute tolerance
xi &lt;- c(1.19, 1.17, 1.01, 1.39, 1.23, 1.22, 0.85, 0.98)
beta &lt;- c(774, 254, 233, 412, 107, 243, 314, 124)
qF.lst &lt;- lapply(1:8, function(j){ function(p) qGPD(p, shape = xi[j], scale = beta[j])})
set.seed(271)
res.best &lt;- RA(0.99, qF = qF.lst, N = N, abstol = eps, method = "best.VaR")
print(format(res.best$bounds, scientific = TRUE), quote = FALSE) # close to first value of 1st row
res.worst &lt;- RA(0.99, qF = qF.lst, N = N, abstol = eps)
print(format(res.worst$bounds, scientific = TRUE), quote = FALSE) # close to last value of 1st row


### 4 Further checks ###########################################################

## Calling the workhorses directly
set.seed(271)
ra &lt;- rearrange(X)
bra &lt;- block_rearrange(X)
stopifnot(ra$converged, bra$converged,
          all.equal(ra$bound, bra$bound, tolerance = 6e-3))

## Checking ABRA against ARA
set.seed(271)
ara  &lt;- ARA (alpha, qF = qPar)
abra &lt;- ABRA(alpha, qF = qPar)
stopifnot(ara$converged, abra$converged,
          all.equal(ara$bound[["low"]], abra$bound[["low"]], tolerance = 2e-3),
          all.equal(ara$bound[["up"]],  abra$bound[["up"]],  tolerance = 6e-3))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
