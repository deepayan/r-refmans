<!DOCTYPE html><html><head><title>Help for package epca</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {epca}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#absmin'><p>Absmin Rotation</p></a></li>
<li><a href='#absmin.criteria'><p>Absmin Criteria</p></a></li>
<li><a href='#cpve'><p>Cumulative Proportion of Variance Explained (CPVE)</p></a></li>
<li><a href='#dist.matrix'><p>Matrix Column Distance</p></a></li>
<li><a href='#distance'><p>Matrix Distance</p></a></li>
<li><a href='#epca-package'><p>Exploratory Principal Component Analysis</p></a></li>
<li><a href='#exp.frac'><p>Calculate fractional exponent/power</p></a></li>
<li><a href='#hard'><p>Hard-thresholding</p></a></li>
<li><a href='#inner'><p>Matrix Inner Product</p></a></li>
<li><a href='#labelCluster'><p>Label Cluster</p></a></li>
<li><a href='#misClustRate'><p>Mis-Classification Rate (MCR)</p></a></li>
<li><a href='#norm.Lp'><p>Element-wise Matrix Norm</p></a></li>
<li><a href='#permColumn'><p>Permute columns of a block membership matrix</p></a></li>
<li><a href='#pitprops'><p>Pitprops correlation data</p></a></li>
<li><a href='#polar'><p>Polar Decomposition</p></a></li>
<li><a href='#print.sca'><p>Print SCA</p></a></li>
<li><a href='#print.sma'><p>Print SMA</p></a></li>
<li><a href='#prs'><p>Polar-Rotate-Shrink</p></a></li>
<li><a href='#pve'><p>Proportion of Variance Explained (PVE)</p></a></li>
<li><a href='#rootmatrix'><p>Find root matrix</p></a></li>
<li><a href='#rotation'><p>Varimax Rotation</p></a></li>
<li><a href='#sca'><p>Sparse Component Analysis</p></a></li>
<li><a href='#shrinkage'><p>Shrinkage</p></a></li>
<li><a href='#sma'><p>Sparse Matrix Approximation</p></a></li>
<li><a href='#soft'><p>Soft-thresholding</p></a></li>
<li><a href='#varimax'><p>Varimax Rotation</p></a></li>
<li><a href='#varimax.criteria'><p>The varimax criterion</p></a></li>
<li><a href='#vgQ.absmin'><p>Gradient of Absmin Criterion</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Exploratory Principal Component Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-07-10</td>
</tr>
<tr>
<td>Description:</td>
<td>
    Exploratory principal component analysis for large-scale dataset, including sparse principal component analysis and sparse matrix approximation.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/fchen365/epca">https://github.com/fchen365/epca</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/fchen365/epca/issues">https://github.com/fchen365/epca/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5),</td>
</tr>
<tr>
<td>Imports:</td>
<td>clue, irlba, Matrix, GPArotation,</td>
</tr>
<tr>
<td>Suggests:</td>
<td>elasticnet, ggcorrplot, tidyverse, rmarkdown, reshape2,
markdown, RSpectra, matlabr, knitr, PMA, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-07-10 05:29:59 UTC; fanchen</td>
</tr>
<tr>
<td>Author:</td>
<td>Fan Chen <a href="https://orcid.org/0000-0003-4508-6023"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Fan Chen &lt;fan.chen@wisc.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-07-10 19:50:11 UTC</td>
</tr>
</table>
<hr>
<h2 id='absmin'>Absmin Rotation</h2><span id='topic+absmin'></span>

<h3>Description</h3>

<p>Given a p x k matrix <code>x</code>,
finds the orthogonal matrix (rotation) that minimizes the <a href="#topic+absmin.criteria">absmin.criteria</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>absmin(x, r0 = diag(ncol(x)), normalize = FALSE, eps = 1e-05, maxit = 1000L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="absmin_+3A_x">x</code></td>
<td>
<p>a <code>matrix</code> or <code>Matrix</code>, initial factor loadings matrix for which the rotation criterian is to be optimized.</p>
</td></tr>
<tr><td><code id="absmin_+3A_r0">r0</code></td>
<td>
<p><code>matrix</code>, initial rotation matrix.</p>
</td></tr>
<tr><td><code id="absmin_+3A_normalize">normalize</code></td>
<td>
<p>logical. Should Kaiser normalization be performed?
If so the rows of <code>x</code> are re-scaled to unit length before
rotation, and scaled back afterwards.</p>
</td></tr>
<tr><td><code id="absmin_+3A_eps">eps</code></td>
<td>
<p>The tolerance for stopping: the relative change in the sum
of singular values.</p>
</td></tr>
<tr><td><code id="absmin_+3A_maxit">maxit</code></td>
<td>
<p><code>integer</code>, maximum number of iteration (default to 1,000).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with three elements:
</p>
<table>
<tr><td><code>rotated</code></td>
<td>
<p>the rotated matrix.</p>
</td></tr>
<tr><td><code>rotmat</code></td>
<td>
<p>the (orthogonal) rotation matrix.</p>
</td></tr>
<tr><td><code>n.iter</code></td>
<td>
<p>the number of iteration taken.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code>GPArotation::GPForth</code>
</p>

<hr>
<h2 id='absmin.criteria'>Absmin Criteria</h2><span id='topic+absmin.criteria'></span>

<h3>Description</h3>

<p>Calculate the absmin criteria.
This is a helper function for <a href="#topic+absmin">absmin</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>absmin.criteria(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="absmin.criteria_+3A_x">x</code></td>
<td>
<p>a <code>matrix</code> or <code>Matrix</code>, initial factor loadings matrix for which the rotation criterian is to be optimized.</p>
</td></tr>
</table>

<hr>
<h2 id='cpve'>Cumulative Proportion of Variance Explained (CPVE)</h2><span id='topic+cpve'></span>

<h3>Description</h3>

<p>Calculate the CPVE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpve(x, v, is.cov = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpve_+3A_x">x</code></td>
<td>
<p><code>matrix</code> or <code>Matrix</code>, the original data matrix or the Gram matrix.</p>
</td></tr>
<tr><td><code id="cpve_+3A_v">v</code></td>
<td>
<p><code>matrix</code> or <code>Matrix</code>, coefficients of linear transformation, e.g., loadings (in PCA).</p>
</td></tr>
<tr><td><code id="cpve_+3A_is.cov">is.cov</code></td>
<td>
<p><code>logical</code>, whether the input matrix is a covariance matrix (or a Gram matrix).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>numeric</code> vector of length <code>ncol(v)</code>, the i-th value is the CPVE of the first i columns in <code>v</code>.
</p>


<h3>See Also</h3>

<p><a href="#topic+pve">pve</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use the "swiss" data
## find two sparse PCs
s.sca &lt;- sca(swiss, 2, gamma = sqrt(ncol(swiss)))
ld &lt;- loadings(s.sca)
cpve(as.matrix(swiss), ld)

</code></pre>

<hr>
<h2 id='dist.matrix'>Matrix Column Distance</h2><span id='topic+dist.matrix'></span>

<h3>Description</h3>

<p>Compute the distance between two matrices.
The distance between two matrices is defined as the sum of distances between column pairs.
This function matches the columns of two matrices, such that the matrix distance
(i.e., the sum of paired column distances) is minimized.
This is accomplished by solving an optimization over column permutation.
Given two matrices, <code>x</code> and <code>y</code>, find permutation p() that minimizes
sum_i similarity(<code style="white-space: pre;">&#8288;x[,p(i)], y[,i]&#8288;</code>),
where the <code>similarity()</code> can be &quot;euclidean&quot; distance, 1 - &quot;cosine&quot;, or &quot;maximum&quot; difference (manhattan distance).
The solution is computed by <code><a href="clue.html#topic+solve_LSAP">clue::solve_LSAP()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist.matrix(x, y, method = "euclidean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.matrix_+3A_x">x</code>, <code id="dist.matrix_+3A_y">y</code></td>
<td>
<p><code>matrix</code> or <code>Matrix</code>, of the same number of rows. The columns of <code>x</code> and <code>y</code> will be scaled to unit length.</p>
</td></tr>
<tr><td><code id="dist.matrix_+3A_method">method</code></td>
<td>
<p>distance measure, &quot;maximum&quot;, &quot;cosine&quot;, or &quot;euclidean&quot; are implemented.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>list</code> of four components:
</p>
<table>
<tr><td><code>dist</code></td>
<td>
<p><code>dist</code>, the distance matrix.</p>
</td></tr>
<tr><td><code>match</code></td>
<td>
<p><code>solve_LSAP</code>, the column matches.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p><code>numeric</code> vector, the distance between pairs of columns.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p><code>character</code>, the distance measure used.</p>
</td></tr>
<tr><td><code>nrow</code></td>
<td>
<p><code>integer</code>, the dimension of the input matrices, i.e., <code>nrow(x)</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="clue.html#topic+solve_LSAP">clue::solve_LSAP</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- diag(4)
y &lt;- x + rnorm(16, sd = 0.05) # add some noise
y = t(t(y) / sqrt(colSums(y ^ 2))) ## normalize the columns
## euclidian distance between column pairs, with minimal matches
dist.matrix(x, y, "euclidean")

</code></pre>

<hr>
<h2 id='distance'>Matrix Distance</h2><span id='topic+distance'></span>

<h3>Description</h3>

<p>Matrix Distance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distance(x, y, method = "euclidean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distance_+3A_x">x</code>, <code id="distance_+3A_y">y</code></td>
<td>
<p><code>matrix</code> or <code>Matrix</code>, of the same number of rows. The columns of <code>x</code> and <code>y</code> will be scaled to unit length.</p>
</td></tr>
<tr><td><code id="distance_+3A_method">method</code></td>
<td>
<p>distance measure, &quot;maximum&quot;, &quot;cosine&quot;, or &quot;euclidean&quot; are implemented.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric</code>, the distance between two matrices.
</p>

<hr>
<h2 id='epca-package'>Exploratory Principal Component Analysis</h2><span id='topic+epca-package'></span>

<h3>Description</h3>

<p><code>epca</code> is for comprehending any data matrix that contains <em>low-rank</em> and <em>sparse</em> underlying signals of interest.
The package currently features two key tools: (1) <code>sca</code> for <strong>s</strong>parse principal <strong>c</strong>omponent <strong>a</strong>nalysis and
(2) <code>sma</code> for <strong>s</strong>parse <strong>m</strong>atrix <strong>a</strong>pproximation, a two-way data analysis for simultaneously row and column dimensionality reductions.
</p>


<h3>References</h3>

<p>Chen, F. and Rohe K. (2020) &quot;A New Basis for Sparse PCA&quot;.
</p>

<hr>
<h2 id='exp.frac'>Calculate fractional exponent/power</h2><span id='topic+exp.frac'></span>

<h3>Description</h3>

<p>Calculate fractional exponent/power, <code>a^(num/den)</code>, where a could be negative.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'frac'
exp(a, num, den)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="exp.frac_+3A_a">a</code></td>
<td>
<p><code>numeric(1)</code>, base (could be negative).</p>
</td></tr>
<tr><td><code id="exp.frac_+3A_num">num</code></td>
<td>
<p>a positive <code>integer</code>, numerator of the exponent.</p>
</td></tr>
<tr><td><code id="exp.frac_+3A_den">den</code></td>
<td>
<p>a positive <code>integer</code>, denominator of the exponent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric</code>, the evaluated a^(num/den)
</p>

<hr>
<h2 id='hard'>Hard-thresholding</h2><span id='topic+hard'></span>

<h3>Description</h3>

<p>Perform hard-thresholding given the cut-off value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hard(x, t)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hard_+3A_x">x</code></td>
<td>
<p>any numerical <code>matrix</code> or <code>vector</code>.</p>
</td></tr>
<tr><td><code id="hard_+3A_t">t</code></td>
<td>
<p><code>numeric</code>, the amount to hard-threshold, i.e., <code class="reqn">sgn(x_{ij}) (|x_{ij}-t|)_+</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='inner'>Matrix Inner Product</h2><span id='topic+inner'></span>

<h3>Description</h3>

<p>Calculate the custom matrix inner product <code>z</code> of two matrices, <code>x</code> and <code>y</code>,
where <code>z[i,j] = FUN(x[,i], y[,j])</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inner(x, y, FUN = "crossprod", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inner_+3A_x">x</code>, <code id="inner_+3A_y">y</code></td>
<td>
<p><code>matrix</code> or <code>Matrix</code>.</p>
</td></tr>
<tr><td><code id="inner_+3A_fun">FUN</code></td>
<td>
<p><code>function</code> or a <code>character(1)</code> name of base function.
The function should take in two vectors as input and output a <code>numeric(1)</code> result.</p>
</td></tr>
<tr><td><code id="inner_+3A_...">...</code></td>
<td>
<p>additional parameters for <code>FUN</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>matrix</code>, inner product of <code>x</code> and <code>y</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(1:6, 2, 3)
y &lt;- matrix(7:12, 2, 3)
## The default is equivalent to `crossprod(x, y)`
inner(x, y)
## We can compute the pair-wise Euclidean distance of columns.
EuclideanDistance = function(x, y) crossprod(x, y)^2
inner(x, y, EuclideanDistance)

</code></pre>

<hr>
<h2 id='labelCluster'>Label Cluster</h2><span id='topic+labelCluster'></span>

<h3>Description</h3>

<p>Assign cluster labels to each row from the membership matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>labelCluster(x, ties.method = "random")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="labelCluster_+3A_x">x</code></td>
<td>
<p><code>matrix</code> with non-negative entries, where <code>x[i,j]</code> is the estimated likelihood (or any equivalent measure) of node i belongs to block j. The higher the more likely.</p>
</td></tr>
<tr><td><code id="labelCluster_+3A_ties.method">ties.method</code></td>
<td>
<p><code>character</code>, how should ties be handled, &quot;random&quot;, &quot;first&quot;, &quot;last&quot; are allowed. See <code><a href="base.html#topic+rank">base::rank()</a></code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>integer</code> vector of the same length as <code>x</code>. Each entry is one of 1, 2, ..., <code>ncol(x)</code>.
</p>

<hr>
<h2 id='misClustRate'>Mis-Classification Rate (MCR)</h2><span id='topic+misClustRate'></span>

<h3>Description</h3>

<p>Compute the empirical MCR, assuming that #cluster = #block,
This calculation allows a permutation on clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>misClustRate(cluster, truth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="misClustRate_+3A_cluster">cluster</code></td>
<td>
<p>vector of <code>integer</code> or <code>factor</code>, estimated cluster membership.</p>
</td></tr>
<tr><td><code id="misClustRate_+3A_truth">truth</code></td>
<td>
<p>a vector of the same length as <code>clusters</code>, the true cluster labels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric</code>, the MCR.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>truth = rep(1:3, each = 30)
cluster = rep(3:1, times = c(25, 32, 33))
misClustRate(cluster, truth)
</code></pre>

<hr>
<h2 id='norm.Lp'>Element-wise Matrix Norm</h2><span id='topic+norm.Lp'></span>

<h3>Description</h3>

<p>Compute element-wise matrix Lp-norm.
This is a helper function to <code><a href="#topic+shrinkage">shrinkage()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>norm.Lp(x, p = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="norm.Lp_+3A_x">x</code></td>
<td>
<p>a <code>matrix</code> or <code>Matrix</code>.</p>
</td></tr>
<tr><td><code id="norm.Lp_+3A_p">p</code></td>
<td>
<p><code>numeric(1)</code>, the p for defining the Lp norm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric(1)</code>, the absolute sum of all elements.
</p>

<hr>
<h2 id='permColumn'>Permute columns of a block membership matrix</h2><span id='topic+permColumn'></span>

<h3>Description</h3>

<p>Perform column permutation of block membership matrix for aesthetic visualization.
That is, the k-th column gives k-th cluster. This is done by ranking the column sums of squares (by default).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permColumn(x, s = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="permColumn_+3A_x">x</code></td>
<td>
<p>a non-negative matrix, nNode x nBlock,</p>
</td></tr>
<tr><td><code id="permColumn_+3A_s">s</code></td>
<td>
<p>integer, order of non-linear</p>
</td></tr>
</table>

<hr>
<h2 id='pitprops'>Pitprops correlation data</h2><span id='topic+pitprops'></span>

<h3>Description</h3>

<p>The <code>pitprops</code> data is a correlation matrix that was calculated from 180 observations.
There are 13 explanatory variables.
Jeffers (1967) tried to interpret the first six PCs.
This is a classical example showing the difficulty of interpreting principal components.
</p>


<h3>References</h3>

<p>Jeffers, J. (1967) &quot;Two case studies in the application of principal component&quot;, <em>Applied Statistics</em>, 16, 225-236.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## NOT TEST
data(pitprops)
ggcorrplot::ggcorrplot(pitprops)


</code></pre>

<hr>
<h2 id='polar'>Polar Decomposition</h2><span id='topic+polar'></span>

<h3>Description</h3>

<p>Perform the polar decomposition of an n x p (n &gt; p) matrix <code>x</code> into two parts: <code>u</code> and <code>h</code>,
where <code>u</code> is an n x p unitary matrix with orthogonal columns (i.e. <code>crossprod(u)</code> is the identity matrix),
and <code>h</code> is a p x p positive-semidefinite Hermitian matrix.
The function returns the <code>u</code> matrix.
This is a helper function of <code><a href="#topic+prs">prs()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polar(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="polar_+3A_x">x</code></td>
<td>
<p>a <code>matrix</code> or <code>Matrix</code>, which is presumed full-rank.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>matrix</code> of the unitary part of the polar decomposition.
</p>


<h3>References</h3>

<p>Chen, F. and Rohe, K. (2020) &quot;A New Basis for Sparse Principal Component Analysis.&quot;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(1:6, nrow = 3)
polar_x &lt;- polar(x)

</code></pre>

<hr>
<h2 id='print.sca'>Print SCA</h2><span id='topic+print.sca'></span>

<h3>Description</h3>

<p>Print SCA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sca'
print(x, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.sca_+3A_x">x</code></td>
<td>
<p>an <code>sca</code> object.</p>
</td></tr>
<tr><td><code id="print.sca_+3A_verbose">verbose</code></td>
<td>
<p><code>logical(1)</code>, whether to print out loadings.</p>
</td></tr>
<tr><td><code id="print.sca_+3A_...">...</code></td>
<td>
<p>additional input to generic <a href="base.html#topic+print">print</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Print an <code>sca</code> object interactively.
</p>

<hr>
<h2 id='print.sma'>Print SMA</h2><span id='topic+print.sma'></span>

<h3>Description</h3>

<p>Print SMA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sma'
print(x, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.sma_+3A_x">x</code></td>
<td>
<p>an <code>sma</code> object.</p>
</td></tr>
<tr><td><code id="print.sma_+3A_verbose">verbose</code></td>
<td>
<p><code>logical(1)</code>, whether to print out loadings.</p>
</td></tr>
<tr><td><code id="print.sma_+3A_...">...</code></td>
<td>
<p>additional input to generic <a href="base.html#topic+print">print</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Print an <code>sma</code> object interactively.
</p>

<hr>
<h2 id='prs'>Polar-Rotate-Shrink</h2><span id='topic+prs'></span>

<h3>Description</h3>

<p>This function is a helper function of <code><a href="#topic+sma">sma()</a></code>.
It performs polar docomposition, orthogonal rotation, and soft-thresholding shrinkage in order.
The three steps together enable sparse estimates of the SMA and SCA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prs(x, z.hat, gamma, rotate, shrink, normalize, order, flip, epsilon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prs_+3A_x">x</code>, <code id="prs_+3A_z.hat">z.hat</code></td>
<td>
<p>the matrix product <code>crossprod(x, z.hat)</code> is the actual Polar-Rotate-Shrink object. <code>x</code> and <code>z.hat</code> are input separatedly because the former is additionally used to compute the proportion of variance explained, in the case when <code>order = TRUE</code>.</p>
</td></tr>
<tr><td><code id="prs_+3A_gamma">gamma</code></td>
<td>
<p><code>numeric</code>, the sparsity parameter.</p>
</td></tr>
<tr><td><code id="prs_+3A_rotate">rotate</code></td>
<td>
<p><code>character(1)</code>, rotation method. Two options are currently available: &quot;varimax&quot; (default) or &quot;absmin&quot; (see details).</p>
</td></tr>
<tr><td><code id="prs_+3A_shrink">shrink</code></td>
<td>
<p><code>character(1)</code>, shrinkage method, either &quot;soft&quot;- (default) or &quot;hard&quot;-thresholding (see details).</p>
</td></tr>
<tr><td><code id="prs_+3A_normalize">normalize</code></td>
<td>
<p><code>logical</code>, whether to rows normalization should be done before and undone afterward the rotation (see details).</p>
</td></tr>
<tr><td><code id="prs_+3A_order">order</code></td>
<td>
<p><code>logical</code>, whether to re-order the columns of the estimates (see Details below).</p>
</td></tr>
<tr><td><code id="prs_+3A_flip">flip</code></td>
<td>
<p><code>logical</code>, whether to flip the signs of the columns of estimates such that all columns are positive-skewed (see details).</p>
</td></tr>
<tr><td><code id="prs_+3A_epsilon">epsilon</code></td>
<td>
<p><code>numeric</code>, tolerance of convergence precision (default to 0.00001).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong><code>rotate</code></strong>: The <code>rotate</code> option specifies the rotation technique to
use. Currently, there are two build-in options—“varimax” and “absmin”.
The “varimax” rotation maximizes the element-wise L4 norm of the rotated
matrix. It is faster and computationally more stable. The “absmin”
rotation minimizes the absolute sum of the rotated matrix. It is sharper
(as it directly minimizes the L1 norm) but slower and computationally
less stable.
</p>
<p><strong><code>shrink</code></strong>: The <code>shrink</code> option specifies the shrinkage operator to
use. Currently, there are two build-in options—“soft”- and
“hard”-thresholding. The “soft”-thresholding universally reduce all
elements and sets the small elements to zeros. The “hard”-thresholding
only sets the small elements to zeros.
</p>
<p><strong><code>normalize</code></strong>: The argument <code>normalize</code> gives an indication of if and
how any normalization should be done before rotation, and then undone
after rotation. If normalize is <code>FALSE</code> (the default) no normalization
is done. If normalize is <code>TRUE</code> then Kaiser normalization is done. (So
squared row entries of normalized <code>x</code> sum to 1.0. This is sometimes
called Horst normalization.) For <code>rotate="absmin"</code>, if <code>normalize</code> is a
vector of length equal to the number of indicators (i.e., the number of
rows of <code>x</code>), then the columns are divided by <code>normalize</code> before
rotation and multiplied by <code>normalize</code> after rotation. Also, If
<code>normalize</code> is a function then it should take <code>x</code> as an argument and
return a vector which is used like the vector above.
</p>
<p><strong><code>order</code></strong>: In PCA (and SVD), the principal components (and the
singular vectors) are ordered. For this, we order the sparse components
(i.e., the columns of <code>z</code> or <code>y</code>) by their explained variance in the
data, which is defined as <code>sum((x %*% y)^2)</code>, where y is a column of the
sparse component. Note: not to be confused with the cumulative
proportion of variance explained by <code>y</code> (and <code>z</code>), particularly when <code>y</code>
(and <code>z</code>) is may not be strictly orthogonal.
</p>
<p><strong><code>flip</code></strong>: The argument <code>flip</code> gives an indication of if and the
columns of estimated sparse component should be flipped. Note that the
estimated (sparse) loadings, i.e., the weights on original variables,
are column-wise invariant to a sign flipping. This is because flipping
of a principal direction does not influence the amount of the explained
variance by the component. If <code>flip=TRUE</code>, then the columns of loadings
will be flip accordingly, such that each column is positive-skewed. This
means that for each column, the sum of cubic elements (i.e., <code>sum(x^3)</code>)
are non-negative.
</p>


<h3>Value</h3>

<p>a <code>matrix</code> of the sparse estimate, of the same dimension as <code>crossprod(x, z.hat)</code>.
</p>


<h3>References</h3>

<p>Chen, F. and Rohe, K. (2020) &quot;A New Basis for Sparse Principal Component Analysis.&quot;
</p>


<h3>See Also</h3>

<p><a href="#topic+sma">sma</a>, <a href="#topic+sca">sca</a>, <a href="#topic+polar">polar</a>, <a href="#topic+rotation">rotation</a>, <a href="#topic+shrinkage">shrinkage</a>
</p>

<hr>
<h2 id='pve'>Proportion of Variance Explained (PVE)</h2><span id='topic+pve'></span>

<h3>Description</h3>

<p>Calculate the Proportion of variance explained by a set of linear transformation, (e.g. eigenvectors).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pve(x, v, is.cov = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pve_+3A_x">x</code></td>
<td>
<p><code>matrix</code> or <code>Matrix</code>, the original data matrix or the Gram matrix.</p>
</td></tr>
<tr><td><code id="pve_+3A_v">v</code></td>
<td>
<p><code>matrix</code> or <code>Matrix</code>, coefficients of linear transformation, e.g., loadings (in PCA).</p>
</td></tr>
<tr><td><code id="pve_+3A_is.cov">is.cov</code></td>
<td>
<p><code>logical</code>, whether the input matrix is a covariance matrix (or a Gram matrix).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>numeric</code> value between 0 and 1, the proportion of total variance in <code>x</code> explained by the PCs whose loadings are in <code>v</code>.
</p>


<h3>References</h3>

<p>Shen, H., &amp; Huang, J. Z. (2008). &quot;Sparse principal component analysis via regularized low rank matrix approximation.&quot; <em>Journal of multivariate analysis</em>, 99(6), 1015-1034.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use the "swiss" data
## find two sparse PCs
s.sca &lt;- sca(swiss, 2, gamma = sqrt(ncol(swiss)))
ld &lt;- loadings(s.sca)
pve(as.matrix(swiss), ld)
</code></pre>

<hr>
<h2 id='rootmatrix'>Find root matrix</h2><span id='topic+rootmatrix'></span>

<h3>Description</h3>

<p>Find the root matrix (<code>x</code>) from the Gram matrix (i.e., <code>crossprod(x)</code>).
This is also useful when the input is a covariance matrix, up to a scaling factor of n-1, where n is the sample size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rootmatrix(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rootmatrix_+3A_x">x</code></td>
<td>
<p>a symmetric <code>matrix</code> (will trigger error if not symmetric).</p>
</td></tr>
</table>

<hr>
<h2 id='rotation'>Varimax Rotation</h2><span id='topic+rotation'></span>

<h3>Description</h3>

<p>Perform varimax rotation.
Flip the signs of columns so that the resulting matrix is positive-skewed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rotation(
  x,
  rotate = c("varimax", "absmin"),
  normalize = FALSE,
  flip = TRUE,
  eps = 1e-06
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rotation_+3A_x">x</code></td>
<td>
<p>a <code>matrix</code> or <code>Matrix</code>.</p>
</td></tr>
<tr><td><code id="rotation_+3A_rotate">rotate</code></td>
<td>
<p><code>character(1)</code>, rotation method. Two options are currently available: &quot;varimax&quot; (default) or &quot;absmin&quot; (see details).</p>
</td></tr>
<tr><td><code id="rotation_+3A_normalize">normalize</code></td>
<td>
<p><code>logical</code>, whether to rows normalization should be done before and undone afterward the rotation (see details).</p>
</td></tr>
<tr><td><code id="rotation_+3A_flip">flip</code></td>
<td>
<p><code>logical</code>, whether to flip the signs of the columns of estimates such that all columns are positive-skewed (see details).</p>
</td></tr>
<tr><td><code id="rotation_+3A_eps">eps</code></td>
<td>
<p><code>numeric</code> precision tolerance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong><code>rotate</code></strong>: The <code>rotate</code> option specifies the rotation technique to
use. Currently, there are two build-in options—“varimax” and “absmin”.
The “varimax” rotation maximizes the element-wise L4 norm of the rotated
matrix. It is faster and computationally more stable. The “absmin”
rotation minimizes the absolute sum of the rotated matrix. It is sharper
(as it directly minimizes the L1 norm) but slower and computationally
less stable.
</p>
<p><strong><code>normalize</code></strong>: The argument <code>normalize</code> gives an indication of if and
how any normalization should be done before rotation, and then undone
after rotation. If normalize is <code>FALSE</code> (the default) no normalization
is done. If normalize is <code>TRUE</code> then Kaiser normalization is done. (So
squared row entries of normalized <code>x</code> sum to 1.0. This is sometimes
called Horst normalization.) For <code>rotate="absmin"</code>, if <code>normalize</code> is a
vector of length equal to the number of indicators (i.e., the number of
rows of <code>x</code>), then the columns are divided by <code>normalize</code> before
rotation and multiplied by <code>normalize</code> after rotation. Also, If
<code>normalize</code> is a function then it should take <code>x</code> as an argument and
return a vector which is used like the vector above.
</p>
<p><strong><code>flip</code></strong>: The argument <code>flip</code> gives an indication of if and the
columns of estimated sparse component should be flipped. Note that the
estimated (sparse) loadings, i.e., the weights on original variables,
are column-wise invariant to a sign flipping. This is because flipping
of a principal direction does not influence the amount of the explained
variance by the component. If <code>flip=TRUE</code>, then the columns of loadings
will be flip accordingly, such that each column is positive-skewed. This
means that for each column, the sum of cubic elements (i.e., <code>sum(x^3)</code>)
are non-negative.
</p>


<h3>Value</h3>

<p>the rotated matrix of the same dimension as <code>x</code>.
</p>


<h3>References</h3>

<p>Chen, F. and Rohe, K. (2020) &quot;A New Basis for Sparse Principal Component Analysis.&quot;
</p>


<h3>See Also</h3>

<p><a href="#topic+prs">prs</a>, <a href="#topic+varimax">varimax</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use the "swiss" data
fa &lt;- factanal( ~., 2, data = swiss, rotation = "none")
rotation(loadings(fa))
</code></pre>

<hr>
<h2 id='sca'>Sparse Component Analysis</h2><span id='topic+sca'></span>

<h3>Description</h3>

<p><code>sca</code> performs sparse principal components analysis on the given numeric data matrix.
Choices of rotation techniques and shrinkage operators are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sca(
  x,
  k = min(5, dim(x)),
  gamma = NULL,
  is.cov = FALSE,
  rotate = c("varimax", "absmin"),
  shrink = c("soft", "hard"),
  center = TRUE,
  scale = FALSE,
  normalize = FALSE,
  order = TRUE,
  flip = TRUE,
  max.iter = 1000,
  epsilon = 1e-05,
  quiet = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sca_+3A_x">x</code></td>
<td>
<p><code>matrix</code> or <code>Matrix</code> to be analyzed.</p>
</td></tr>
<tr><td><code id="sca_+3A_k">k</code></td>
<td>
<p><code>integer</code>, rank of approximation.</p>
</td></tr>
<tr><td><code id="sca_+3A_gamma">gamma</code></td>
<td>
<p><code>numeric(1)</code>, sparsity parameter, default to <code>sqrt(pk)</code>, where n x p is the dimension of <code>x</code>.</p>
</td></tr>
<tr><td><code id="sca_+3A_is.cov">is.cov</code></td>
<td>
<p><code>logical</code>, default to <code>FALSE</code>, whether the <code>x</code> is a covariance matrix (or Gram matrix, i.e., <code>crossprod()</code> of some design matrix). If <code>TRUE</code>, both <code>center</code> and <code>scale</code> will be ignored/skipped.</p>
</td></tr>
<tr><td><code id="sca_+3A_rotate">rotate</code></td>
<td>
<p><code>character(1)</code>, rotation method. Two options are currently available: &quot;varimax&quot; (default) or &quot;absmin&quot; (see details).</p>
</td></tr>
<tr><td><code id="sca_+3A_shrink">shrink</code></td>
<td>
<p><code>character(1)</code>, shrinkage method, either &quot;soft&quot;- (default) or &quot;hard&quot;-thresholding (see details).</p>
</td></tr>
<tr><td><code id="sca_+3A_center">center</code></td>
<td>
<p><code>logical</code>, whether to center columns of <code>x</code> (see <code><a href="base.html#topic+scale">scale()</a></code>).</p>
</td></tr>
<tr><td><code id="sca_+3A_scale">scale</code></td>
<td>
<p><code>logical</code>, whether to scale columns of <code>x</code> (see <code><a href="base.html#topic+scale">scale()</a></code>).</p>
</td></tr>
<tr><td><code id="sca_+3A_normalize">normalize</code></td>
<td>
<p><code>logical</code>, whether to rows normalization should be done before and undone afterward the rotation (see details).</p>
</td></tr>
<tr><td><code id="sca_+3A_order">order</code></td>
<td>
<p><code>logical</code>, whether to re-order the columns of the estimates (see Details below).</p>
</td></tr>
<tr><td><code id="sca_+3A_flip">flip</code></td>
<td>
<p><code>logical</code>, whether to flip the signs of the columns of estimates such that all columns are positive-skewed (see details).</p>
</td></tr>
<tr><td><code id="sca_+3A_max.iter">max.iter</code></td>
<td>
<p><code>integer</code>, maximum number of iteration (default to 1,000).</p>
</td></tr>
<tr><td><code id="sca_+3A_epsilon">epsilon</code></td>
<td>
<p><code>numeric</code>, tolerance of convergence precision (default to 0.00001).</p>
</td></tr>
<tr><td><code id="sca_+3A_quiet">quiet</code></td>
<td>
<p><code>logical</code>, whether to mute the process report (default to <code>TRUE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong><code>rotate</code></strong>: The <code>rotate</code> option specifies the rotation technique to
use. Currently, there are two build-in options—“varimax” and “absmin”.
The “varimax” rotation maximizes the element-wise L4 norm of the rotated
matrix. It is faster and computationally more stable. The “absmin”
rotation minimizes the absolute sum of the rotated matrix. It is sharper
(as it directly minimizes the L1 norm) but slower and computationally
less stable.
</p>
<p><strong><code>shrink</code></strong>: The <code>shrink</code> option specifies the shrinkage operator to
use. Currently, there are two build-in options—“soft”- and
“hard”-thresholding. The “soft”-thresholding universally reduce all
elements and sets the small elements to zeros. The “hard”-thresholding
only sets the small elements to zeros.
</p>
<p><strong><code>normalize</code></strong>: The argument <code>normalize</code> gives an indication of if and
how any normalization should be done before rotation, and then undone
after rotation. If normalize is <code>FALSE</code> (the default) no normalization
is done. If normalize is <code>TRUE</code> then Kaiser normalization is done. (So
squared row entries of normalized <code>x</code> sum to 1.0. This is sometimes
called Horst normalization.) For <code>rotate="absmin"</code>, if <code>normalize</code> is a
vector of length equal to the number of indicators (i.e., the number of
rows of <code>x</code>), then the columns are divided by <code>normalize</code> before
rotation and multiplied by <code>normalize</code> after rotation. Also, If
<code>normalize</code> is a function then it should take <code>x</code> as an argument and
return a vector which is used like the vector above.
</p>
<p><strong><code>order</code></strong>: In PCA (and SVD), the principal components (and the
singular vectors) are ordered. For this, we order the sparse components
(i.e., the columns of <code>z</code> or <code>y</code>) by their explained variance in the
data, which is defined as <code>sum((x %*% y)^2)</code>, where y is a column of the
sparse component. Note: not to be confused with the cumulative
proportion of variance explained by <code>y</code> (and <code>z</code>), particularly when <code>y</code>
(and <code>z</code>) is may not be strictly orthogonal.
</p>
<p><strong><code>flip</code></strong>: The argument <code>flip</code> gives an indication of if and the
columns of estimated sparse component should be flipped. Note that the
estimated (sparse) loadings, i.e., the weights on original variables,
are column-wise invariant to a sign flipping. This is because flipping
of a principal direction does not influence the amount of the explained
variance by the component. If <code>flip=TRUE</code>, then the columns of loadings
will be flip accordingly, such that each column is positive-skewed. This
means that for each column, the sum of cubic elements (i.e., <code>sum(x^3)</code>)
are non-negative.
</p>


<h3>Value</h3>

<p>an <code>sca</code> object that contains:
</p>
<table>
<tr><td><code>loadings</code></td>
<td>
<p><code>matrix</code>, sparse loadings of PCs.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>an n x k <code>matrix</code>, the component scores, calculated using centered (and/or scaled) <code>x</code>. This will only be available when <code>is.cov = FALSE</code>.</p>
</td></tr>
<tr><td><code>cpve</code></td>
<td>
<p>a <code>numeric</code> vector of length <code>k</code>, cumulative proportion of variance in <code>x</code> explained by the top PCs (after center and/or scale).</p>
</td></tr>
<tr><td><code>center</code></td>
<td>
<p><code>logical</code>, this records the <code>center</code> parameter.</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>
<p><code>logical</code>, this records the <code>scale</code> parameter.</p>
</td></tr>
<tr><td><code>n.iter</code></td>
<td>
<p><code>integer</code>, number of iteration taken.</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p><code>integer</code>, sample size, that is, <code>nrow(x)</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chen, F. and Rohe, K. (2020) &quot;A New Basis for Sparse Principal Component Analysis.&quot;
</p>


<h3>See Also</h3>

<p><a href="#topic+sma">sma</a>, <a href="#topic+prs">prs</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## ------ example 1 ------
## simulate a low-rank data matrix with some additive Gaussian noise
n &lt;- 300
p &lt;- 50
k &lt;- 5 ## rank
z &lt;- shrinkage(polar(matrix(runif(n * k), n, k)), sqrt(n))
b &lt;- diag(5) * 3
y &lt;- shrinkage(polar(matrix(runif(p * k), p, k)), sqrt(p))
e &lt;- matrix(rnorm(n * p, sd = .01), n, p)
x &lt;- scale(z %*% b %*% t(y) + e)

## perform sparse PCA
s.sca &lt;- sca(x, k)
s.sca

## ------ example 2 ------
## use the `pitprops` data from the `elasticnet` package
data(pitprops)

## find 6 sparse PCs
s.sca &lt;- sca(pitprops, 6, gamma = 6, is.cov = TRUE)
print(s.sca, verbose = TRUE)

</code></pre>

<hr>
<h2 id='shrinkage'>Shrinkage</h2><span id='topic+shrinkage'></span>

<h3>Description</h3>

<p>Shrink a matrix using soft-thresholding or hard-thresholding.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shrinkage(x, gamma, shrink = c("soft", "hard"), epsilon = 1e-11)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shrinkage_+3A_x">x</code></td>
<td>
<p><code>matrix</code> or <code>Matrix</code>, to be threshold.</p>
</td></tr>
<tr><td><code id="shrinkage_+3A_gamma">gamma</code></td>
<td>
<p><code>numeric</code>, the constraint of Lp norm, i.e. <code class="reqn">\|x\|\le \gamma</code>.</p>
</td></tr>
<tr><td><code id="shrinkage_+3A_shrink">shrink</code></td>
<td>
<p><code>character(1)</code>, shrinkage method, either &quot;soft&quot;- (default) or &quot;hard&quot;-thresholding (see details).</p>
</td></tr>
<tr><td><code id="shrinkage_+3A_epsilon">epsilon</code></td>
<td>
<p><code>numeric</code>, precision tolerance. This should be greater than <code>.Machine$double.eps</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A binary search to find the cut-off value.
</p>
<p><strong><code>shrink</code></strong>: The <code>shrink</code> option specifies the shrinkage operator to
use. Currently, there are two build-in options—“soft”- and
“hard”-thresholding. The “soft”-thresholding universally reduce all
elements and sets the small elements to zeros. The “hard”-thresholding
only sets the small elements to zeros.
</p>


<h3>Value</h3>

<p>a <code>list</code> with two components:
</p>
<table>
<tr><td><code>matrix</code></td>
<td>
<p>matrix, the matrix that results from soft-thresholding</p>
</td></tr>
<tr><td><code>norm</code></td>
<td>
<p>numeric, the norm of the matrix after soft-thresholding. This value is close to constraint if using the second option.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chen, F. and Rohe, K. (2020) &quot;A New Basis for Sparse Principal Component Analysis.&quot;
</p>


<h3>See Also</h3>

<p><a href="#topic+prs">prs</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(1:6, nrow = 3)
shrink_x &lt;- shrinkage(x, 1)

</code></pre>

<hr>
<h2 id='sma'>Sparse Matrix Approximation</h2><span id='topic+sma'></span>

<h3>Description</h3>

<p>Perform the sparse matrix approximation (SMA) of a data matrix <code>x</code> as three multiplicative components: <code>z</code>, <code>b</code>, and <code>t(y)</code>,
where <code>z</code> and <code>y</code> are sparse, and <code>b</code> is low-rank but not necessarily diagonal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sma(
  x,
  k = min(5, dim(x)),
  gamma = NULL,
  rotate = c("varimax", "absmin"),
  shrink = c("soft", "hard"),
  center = FALSE,
  scale = FALSE,
  normalize = FALSE,
  order = FALSE,
  flip = FALSE,
  max.iter = 1000,
  epsilon = 1e-05,
  quiet = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sma_+3A_x">x</code></td>
<td>
<p><code>matrix</code> or <code>Matrix</code> to be analyzed.</p>
</td></tr>
<tr><td><code id="sma_+3A_k">k</code></td>
<td>
<p><code>integer</code>, rank of approximation.</p>
</td></tr>
<tr><td><code id="sma_+3A_gamma">gamma</code></td>
<td>
<p><code>numeric(2)</code>, sparsity parameters. If <code>gamma</code> is <code>numeric(1)</code>, it is used for both left and right sparsity component (i.e, <code>z</code> and <code>y</code>). If absent, the two parameters are set as (default): <code>sqrt(nk)</code> and <code>sqrt(pk)</code> for <code>z</code> and <code>y</code> respectively, where n x p is the dimension of <code>x</code>.</p>
</td></tr>
<tr><td><code id="sma_+3A_rotate">rotate</code></td>
<td>
<p><code>character(1)</code>, rotation method. Two options are currently available: &quot;varimax&quot; (default) or &quot;absmin&quot; (see details).</p>
</td></tr>
<tr><td><code id="sma_+3A_shrink">shrink</code></td>
<td>
<p><code>character(1)</code>, shrinkage method, either &quot;soft&quot;- (default) or &quot;hard&quot;-thresholding (see details).</p>
</td></tr>
<tr><td><code id="sma_+3A_center">center</code></td>
<td>
<p><code>logical</code>, whether to center columns of <code>x</code> (see <code><a href="base.html#topic+scale">scale()</a></code>).</p>
</td></tr>
<tr><td><code id="sma_+3A_scale">scale</code></td>
<td>
<p><code>logical</code>, whether to scale columns of <code>x</code> (see <code><a href="base.html#topic+scale">scale()</a></code>).</p>
</td></tr>
<tr><td><code id="sma_+3A_normalize">normalize</code></td>
<td>
<p><code>logical</code>, whether to rows normalization should be done before and undone afterward the rotation (see details).</p>
</td></tr>
<tr><td><code id="sma_+3A_order">order</code></td>
<td>
<p><code>logical</code>, whether to re-order the columns of the estimates (see Details below).</p>
</td></tr>
<tr><td><code id="sma_+3A_flip">flip</code></td>
<td>
<p><code>logical</code>, whether to flip the signs of the columns of estimates such that all columns are positive-skewed (see details).</p>
</td></tr>
<tr><td><code id="sma_+3A_max.iter">max.iter</code></td>
<td>
<p><code>integer</code>, maximum number of iteration (default to 1,000).</p>
</td></tr>
<tr><td><code id="sma_+3A_epsilon">epsilon</code></td>
<td>
<p><code>numeric</code>, tolerance of convergence precision (default to 0.00001).</p>
</td></tr>
<tr><td><code id="sma_+3A_quiet">quiet</code></td>
<td>
<p><code>logical</code>, whether to mute the process report (default to <code>TRUE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong><code>rotate</code></strong>: The <code>rotate</code> option specifies the rotation technique to
use. Currently, there are two build-in options—“varimax” and “absmin”.
The “varimax” rotation maximizes the element-wise L4 norm of the rotated
matrix. It is faster and computationally more stable. The “absmin”
rotation minimizes the absolute sum of the rotated matrix. It is sharper
(as it directly minimizes the L1 norm) but slower and computationally
less stable.
</p>
<p><strong><code>shrink</code></strong>: The <code>shrink</code> option specifies the shrinkage operator to
use. Currently, there are two build-in options—“soft”- and
“hard”-thresholding. The “soft”-thresholding universally reduce all
elements and sets the small elements to zeros. The “hard”-thresholding
only sets the small elements to zeros.
</p>
<p><strong><code>normalize</code></strong>: The argument <code>normalize</code> gives an indication of if and
how any normalization should be done before rotation, and then undone
after rotation. If normalize is <code>FALSE</code> (the default) no normalization
is done. If normalize is <code>TRUE</code> then Kaiser normalization is done. (So
squared row entries of normalized <code>x</code> sum to 1.0. This is sometimes
called Horst normalization.) For <code>rotate="absmin"</code>, if <code>normalize</code> is a
vector of length equal to the number of indicators (i.e., the number of
rows of <code>x</code>), then the columns are divided by <code>normalize</code> before
rotation and multiplied by <code>normalize</code> after rotation. Also, If
<code>normalize</code> is a function then it should take <code>x</code> as an argument and
return a vector which is used like the vector above.
</p>
<p><strong><code>order</code></strong>: In PCA (and SVD), the principal components (and the
singular vectors) are ordered. For this, we order the sparse components
(i.e., the columns of <code>z</code> or <code>y</code>) by their explained variance in the
data, which is defined as <code>sum((x %*% y)^2)</code>, where y is a column of the
sparse component. Note: not to be confused with the cumulative
proportion of variance explained by <code>y</code> (and <code>z</code>), particularly when <code>y</code>
(and <code>z</code>) is may not be strictly orthogonal.
</p>
<p><strong><code>flip</code></strong>: The argument <code>flip</code> gives an indication of if and the
columns of estimated sparse component should be flipped. Note that the
estimated (sparse) loadings, i.e., the weights on original variables,
are column-wise invariant to a sign flipping. This is because flipping
of a principal direction does not influence the amount of the explained
variance by the component. If <code>flip=TRUE</code>, then the columns of loadings
will be flip accordingly, such that each column is positive-skewed. This
means that for each column, the sum of cubic elements (i.e., <code>sum(x^3)</code>)
are non-negative.
</p>


<h3>Value</h3>

<p>an <code>sma</code> object that contains:
</p>
<table>
<tr><td><code>z</code>, <code>b</code>, <code>t(y)</code></td>
<td>
<p>the three parts in the SMA.
<code>z</code> is a sparse n x k <code>matrix</code> that contains the row components (loadings).
The row names of <code>z</code> inherit the row names of <code>x</code>.
<code>b</code> is a k x k <code>matrix</code> that contains the scores of SMA;
the Frobenius norm of <code>b</code> equals to the total variance explained by the SMA.
<code>y</code> is a sparse n x k <code>matrix</code>that contains the column components (loadings).</p>
</td></tr>
</table>
<p>The row names of <code>y</code> inherit the column names of <code>x</code>.
</p>
<table>
<tr><td><code>score</code></td>
<td>
<p>the total variance explained by the SMA.
This is the optimal objective value obtained.</p>
</td></tr>
<tr><td><code>n.iter</code></td>
<td>
<p><code>integer</code>, the number of iteration taken.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chen, F. and Rohe, K. (2020) &quot;A New Basis for Sparse Principal Component Analysis.&quot;
</p>


<h3>See Also</h3>

<p><a href="#topic+sca">sca</a>, <a href="#topic+prs">prs</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulate a rank-5 data matrix with some additive Gaussian noise
n &lt;- 300
p &lt;- 50
k &lt;- 5 ## rank
z &lt;- shrinkage(polar(matrix(runif(n * k), n, k)), sqrt(n))
b &lt;- diag(5) * 3
y &lt;- shrinkage(polar(matrix(runif(p * k), p, k)), sqrt(p))
e &lt;- matrix(rnorm(n * p, sd = .01), n, p)
x &lt;- scale(z %*% b %*% t(y) + e)

## perform sparse matrix approximation
s.sma &lt;- sma(x, k)
s.sma

</code></pre>

<hr>
<h2 id='soft'>Soft-thresholding</h2><span id='topic+soft'></span>

<h3>Description</h3>

<p>Perform soft-thresholding given the cut-off value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>soft(x, t)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="soft_+3A_x">x</code></td>
<td>
<p>any numerical <code>matrix</code> or <code>vector</code>.</p>
</td></tr>
<tr><td><code id="soft_+3A_t">t</code></td>
<td>
<p><code>numeric</code>, the amount to soft-threshold, i.e., <code class="reqn">sgn(x_{ij}) (|x_{ij}-t|)_+</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='varimax'>Varimax Rotation</h2><span id='topic+varimax'></span>

<h3>Description</h3>

<p>This is a re-implementation of <a href="stats.html#topic+varimax">stats::varimax</a>,
which (1) adds a parameter for the maximum number of iterations,
(2) sets the default <code>normalize</code> parameter to <code>FALSE</code>,
(3) outputs the number of iteration taken, and
(4) returns regular <code>matrix</code> rather than in <code>loadings</code> class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varimax(x, normalize = FALSE, eps = 1e-05, maxit = 1000L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varimax_+3A_x">x</code></td>
<td>
<p>A loadings matrix, with <code class="reqn">p</code> rows and <code class="reqn">k &lt; p</code> columns</p>
</td></tr>
<tr><td><code id="varimax_+3A_normalize">normalize</code></td>
<td>
<p>logical. Should Kaiser normalization be performed?
If so the rows of <code>x</code> are re-scaled to unit length before
rotation, and scaled back afterwards.</p>
</td></tr>
<tr><td><code id="varimax_+3A_eps">eps</code></td>
<td>
<p>The tolerance for stopping: the relative change in the sum
of singular values.</p>
</td></tr>
<tr><td><code id="varimax_+3A_maxit">maxit</code></td>
<td>
<p><code>integer</code>, maximum number of iteration (default to 1,000).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with three elements:
</p>
<table>
<tr><td><code>rotated</code></td>
<td>
<p>the rotated matrix.</p>
</td></tr>
<tr><td><code>rotmat</code></td>
<td>
<p>the (orthogonal) rotation matrix.</p>
</td></tr>
<tr><td><code>n.iter</code></td>
<td>
<p>the number of iterations taken.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="stats.html#topic+varimax">stats::varimax</a>
</p>

<hr>
<h2 id='varimax.criteria'>The varimax criterion</h2><span id='topic+varimax.criteria'></span>

<h3>Description</h3>

<p>Calculate the varimax criterion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varimax.criteria(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varimax.criteria_+3A_x">x</code></td>
<td>
<p>a <code>matrix</code> or <code>Matrix</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>numeric</code> of evaluated varimax criterion.
</p>


<h3>References</h3>

<p><a href="https://en.wikipedia.org/wiki/Varimax_rotation">Varimax rotation (Wikipedia)</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use the "swiss" data
fa &lt;- factanal( ~., 2, data = swiss, rotation = "none")
lds &lt;- loadings(fa)

## compute varimax criterion:
varimax.criteria(lds)

## compute varimax criterion (after the varimax rotation):
rlds &lt;- rotation(lds, rotate = "varimax")
varimax.criteria(rlds)

</code></pre>

<hr>
<h2 id='vgQ.absmin'>Gradient of Absmin Criterion</h2><span id='topic+vgQ.absmin'></span>

<h3>Description</h3>

<p>This is a helper function for <a href="#topic+absmin">absmin</a> and is not to be used directly by users.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vgQ.absmin(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vgQ.absmin_+3A_x">x</code></td>
<td>
<p>a <code>matrix</code> or <code>Matrix</code>, initial factor loadings matrix for which the rotation criterian is to be optimized.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list required by <code>GPArotation::GPForth</code> for the absmin rotation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## NOT RUN
## NOT for users to call.

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
