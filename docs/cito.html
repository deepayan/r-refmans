<!DOCTYPE html><html><head><title>Help for package cito</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cito}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ALE'><p>Accumulated Local Effect Plot (ALE)</p></a></li>
<li><a href='#analyze_training'><p>Visualize training of Neural Network</p></a></li>
<li><a href='#avgPool'><p>Average pooling layer</p></a></li>
<li><a href='#cito'><p>'cito': Building and training neural networks</p></a></li>
<li><a href='#cnn'><p>CNN</p></a></li>
<li><a href='#coef.citocnn'><p>Returns list of parameters the neural network model currently has in use</p></a></li>
<li><a href='#coef.citodnn'><p>Returns list of parameters the neural network model currently has in use</p></a></li>
<li><a href='#conditionalEffects'><p>Calculate average conditional effects</p></a></li>
<li><a href='#config_lr_scheduler'><p>Creation of customized learning rate scheduler objects</p></a></li>
<li><a href='#config_optimizer'><p>Creation of customized optimizer objects</p></a></li>
<li><a href='#config_tuning'><p>Config hyperparameter tuning</p></a></li>
<li><a href='#continue_training'><p>Continues training of a model generated with <code>dnn</code> or <code>cnn</code> for additional epochs.</p></a></li>
<li><a href='#conv'><p>Convolutional layer</p></a></li>
<li><a href='#create_architecture'><p>CNN architecture</p></a></li>
<li><a href='#dnn'><p>DNN</p></a></li>
<li><a href='#e'><p>Embeddings</p></a></li>
<li><a href='#findReTrmClasses'><p>list of specials &ndash; taken from enum.R</p></a></li>
<li><a href='#linear'><p>Linear layer</p></a></li>
<li><a href='#maxPool'><p>Maximum pooling layer</p></a></li>
<li><a href='#PDP'><p>Partial Dependence Plot (PDP)</p></a></li>
<li><a href='#plot.citoarchitecture'><p>Plot the CNN architecture</p></a></li>
<li><a href='#plot.citocnn'><p>Plot the CNN architecture</p></a></li>
<li><a href='#plot.citodnn'><p>Creates graph plot which gives an overview of the network architecture.</p></a></li>
<li><a href='#predict.citocnn'><p>Predict from a fitted cnn model</p></a></li>
<li><a href='#predict.citodnn'><p>Predict from a fitted dnn model</p></a></li>
<li><a href='#print.avgPool'><p>Print pooling layer</p></a></li>
<li><a href='#print.citoarchitecture'><p>Print class citoarchitecture</p></a></li>
<li><a href='#print.citocnn'><p>Print class citocnn</p></a></li>
<li><a href='#print.citodnn'><p>Print class citodnn</p></a></li>
<li><a href='#print.conditionalEffects'><p>Print average conditional effects</p></a></li>
<li><a href='#print.conv'><p>Print conv layer</p></a></li>
<li><a href='#print.linear'><p>Print linear layer</p></a></li>
<li><a href='#print.maxPool'><p>Print pooling layer</p></a></li>
<li><a href='#print.summary.citodnn'><p>Print method for class summary.citodnn</p></a></li>
<li><a href='#print.transfer'><p>Print transfer model</p></a></li>
<li><a href='#residuals.citodnn'><p>Extract Model Residuals</p></a></li>
<li><a href='#simulate_shapes'><p>Data Simulation for CNN</p></a></li>
<li><a href='#summary.citocnn'><p>Summary citocnn</p></a></li>
<li><a href='#summary.citodnn'><p>Summarize Neural Network of class citodnn</p></a></li>
<li><a href='#sumTerms'><p>combine a list of formula terms as a sum</p></a></li>
<li><a href='#transfer'><p>Transfer learning</p></a></li>
<li><a href='#tune'><p>Tune hyperparameter</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Building and Training Neural Networks</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>The 'cito' package provides a user-friendly interface for training and interpreting deep neural networks (DNN). 'cito' simplifies the fitting of DNNs by supporting the familiar formula syntax, hyperparameter tuning under cross-validation, and helps to detect and handle convergence problems.  DNNs can be trained on CPU, GPU and MacOS GPUs. In addition, 'cito' has many downstream functionalities such as various explainable AI (xAI) metrics (e.g. variable importance, partial dependence plots, accumulated local effect plots, and effect estimates) to interpret trained DNNs. 'cito' optionally provides confidence intervals (and p-values) for all xAI metrics and predictions. At the same time, 'cito' is computationally efficient because it is based on the deep learning framework 'torch'. The 'torch' package is native to R, so no Python installation or other API is required for this package.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>Imports:</td>
<td>coro, checkmate, torch, gridExtra, parabar, abind, progress,
cli, torchvision, tibble, lme4</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>spelling, rmarkdown, testthat, plotly, ggraph, igraph, stats,
ggplot2, knitr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/citoverse/cito/issues">https://github.com/citoverse/cito/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://citoverse.github.io/cito/">https://citoverse.github.io/cito/</a></td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-18 20:48:48 UTC; maximilianpichler</td>
</tr>
<tr>
<td>Author:</td>
<td>Christian Amesöder [aut],
  Maximilian Pichler
    <a href="https://orcid.org/0000-0003-2252-8327"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Florian Hartig <a href="https://orcid.org/0000-0002-6255-9059"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Armin Schenk [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Maximilian Pichler &lt;maximilian.pichler@biologie.uni-regensburg.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-18 22:50:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='ALE'>Accumulated Local Effect Plot (ALE)</h2><span id='topic+ALE'></span><span id='topic+ALE.citodnn'></span><span id='topic+ALE.citodnnBootstrap'></span>

<h3>Description</h3>

<p>Performs an ALE for one or more features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ALE(
  model,
  variable = NULL,
  data = NULL,
  K = 10,
  ALE_type = c("equidistant", "quantile"),
  plot = TRUE,
  parallel = FALSE,
  ...
)

## S3 method for class 'citodnn'
ALE(
  model,
  variable = NULL,
  data = NULL,
  K = 10,
  ALE_type = c("equidistant", "quantile"),
  plot = TRUE,
  parallel = FALSE,
  ...
)

## S3 method for class 'citodnnBootstrap'
ALE(
  model,
  variable = NULL,
  data = NULL,
  K = 10,
  ALE_type = c("equidistant", "quantile"),
  plot = TRUE,
  parallel = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ALE_+3A_model">model</code></td>
<td>
<p>a model created by <code><a href="#topic+dnn">dnn</a></code></p>
</td></tr>
<tr><td><code id="ALE_+3A_variable">variable</code></td>
<td>
<p>variable as string for which the PDP should be done</p>
</td></tr>
<tr><td><code id="ALE_+3A_data">data</code></td>
<td>
<p>data on which ALE is performed on, if NULL training data will be used.</p>
</td></tr>
<tr><td><code id="ALE_+3A_k">K</code></td>
<td>
<p>number of neighborhoods original feature space gets divided into</p>
</td></tr>
<tr><td><code id="ALE_+3A_ale_type">ALE_type</code></td>
<td>
<p>method on how the feature space is divided into neighborhoods.</p>
</td></tr>
<tr><td><code id="ALE_+3A_plot">plot</code></td>
<td>
<p>plot ALE or not</p>
</td></tr>
<tr><td><code id="ALE_+3A_parallel">parallel</code></td>
<td>
<p>parallelize over bootstrap models or not</p>
</td></tr>
<tr><td><code id="ALE_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="stats.html#topic+predict">predict</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of plots made with 'ggplot2' consisting of an individual plot for each defined variable.
</p>


<h3>Explanation</h3>

<p>Accumulated Local Effect plots (ALE) quantify how the predictions change when the features change. They are similar to partial dependency plots but are more robust to feature collinearity.
</p>


<h3>Mathematical details</h3>

<p>If the defined variable is a numeric feature, the ALE is performed.
Here, the non centered effect for feature j with k equally distant neighborhoods is defined as:
</p>
<p><code class="reqn"> \hat{\tilde{f}}_{j,ALE}(x)=\sum_{k=1}^{k_j(x)}\frac{1}{n_j(k)}\sum_{i:x_{j}^{(i)}\in{}N_j(k)}\left[\hat{f}(z_{k,j},x^{(i)}_{\setminus{}j})-\hat{f}(z_{k-1,j},x^{(i)}_{\setminus{}j})\right]</code>
</p>
<p>Where <code class="reqn">N_j(k)</code> is the k-th neighborhood and <code class="reqn">n_j(k)</code> is the number of observations in the k-th neighborhood.
</p>
<p>The last part of the equation,
<code class="reqn">\left[\hat{f}(z_{k,j},x^{(i)}_{\setminus{}j})-\hat{f}(z_{k-1,j},x^{(i)}_{\setminus{}j})\right]</code>
represents the difference in model prediction when the value of feature j is exchanged with the upper and lower border of the current neighborhood.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PDP">PDP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

# Build and train  Network
nn.fit&lt;- dnn(Sepal.Length~., data = datasets::iris)

ALE(nn.fit, variable = "Petal.Length")
}

</code></pre>

<hr>
<h2 id='analyze_training'>Visualize training of Neural Network</h2><span id='topic+analyze_training'></span>

<h3>Description</h3>

<p>After training a model with cito, this function helps to analyze the training process and decide on best performing model.
Creates a 'plotly' figure which allows to zoom in and out on training graph
</p>


<h3>Usage</h3>

<pre><code class='language-R'>analyze_training(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="analyze_training_+3A_object">object</code></td>
<td>
<p>a model created by <code><a href="#topic+dnn">dnn</a></code> or <code><a href="#topic+cnn">cnn</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The baseline loss is the most important reference. If the model was not able to achieve a better (lower) loss than the baseline (which is the loss for a intercept only model), the model probably did not converge. Possible reasons include an improper learning rate, too few epochs, or too much regularization. See the <code>?dnn</code> help or the <code>vignette("B-Training_neural_networks")</code>.
</p>


<h3>Value</h3>

<p>a 'plotly' figure
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)
set.seed(222)
validation_set&lt;- sample(c(1:nrow(datasets::iris)),25)

# Build and train  Network
nn.fit&lt;- dnn(Sepal.Length~., data = datasets::iris[-validation_set,],validation = 0.1)

# show zoomable plot of training and validation losses
analyze_training(nn.fit)

# Use model on validation set
predictions &lt;- predict(nn.fit, iris[validation_set,])

# Scatterplot
plot(iris[validation_set,]$Sepal.Length,predictions)
}

</code></pre>

<hr>
<h2 id='avgPool'>Average pooling layer</h2><span id='topic+avgPool'></span>

<h3>Description</h3>

<p>creates a 'avgPool' 'citolayer' object that is used by <code><a href="#topic+create_architecture">create_architecture</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>avgPool(kernel_size = NULL, stride = NULL, padding = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="avgPool_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple) size of the kernel in this layer. Use a tuple if the kernel size isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="avgPool_+3A_stride">stride</code></td>
<td>
<p>(int or tuple) stride of the kernel in this layer. NULL sets the stride equal to the kernel size. Use a tuple if the stride isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="avgPool_+3A_padding">padding</code></td>
<td>
<p>(int or tuple) zero-padding added to both sides of the input. Use a tuple if the padding isn't equal in all dimensions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a 'avgPool' 'citolayer' object that is passed to the <code><a href="#topic+create_architecture">create_architecture</a></code> function.
The parameters that aren't assigned here (and are therefore still NULL) are filled with the default values passed to <code><a href="#topic+create_architecture">create_architecture</a></code>.
</p>


<h3>Value</h3>

<p>S3 object of class <code>"avgPool" "citolayer"</code>
</p>


<h3>Author(s)</h3>

<p>Armin Schenk
</p>


<h3>See Also</h3>

<p><code><a href="#topic+create_architecture">create_architecture</a></code>
</p>

<hr>
<h2 id='cito'>'cito': Building and training neural networks</h2><span id='topic+cito'></span><span id='topic+cito-package'></span>

<h3>Description</h3>

<p>The 'cito' package provides a user-friendly interface for training and interpreting deep neural networks (DNN). 'cito' simplifies the fitting of DNNs by supporting the familiar formula syntax, hyperparameter tuning under cross-validation, and helps to detect and handle convergence problems.  DNNs can be trained on CPU, GPU and MacOS GPUs. In addition, 'cito' has many downstream functionalities such as various explainable AI (xAI) metrics (e.g. variable importance, partial dependence plots, accumulated local effect plots, and effect estimates) to interpret trained DNNs. 'cito' optionally provides confidence intervals (and p-values) for all xAI metrics and predictions. At the same time, 'cito' is computationally efficient because it is based on the deep learning framework 'torch'. The 'torch' package is native to R, so no Python installation or other API is required for this package.
</p>


<h3>Details</h3>

<p>Cito is built around its main function <code><a href="#topic+dnn">dnn</a></code>, which creates and trains a deep neural network. Various tools for analyzing the trained neural network are available.
</p>


<h3>Installation</h3>

<p>in order to install cito please follow these steps:
</p>
<p><code>install.packages("cito")</code>
</p>
<p><code>library(torch)</code>
</p>
<p><code>install_torch(reinstall = TRUE)</code>
</p>
<p><code>library(cito)</code>
</p>


<h3>cito functions and typical workflow</h3>


<ul>
<li> <p><code><a href="#topic+dnn">dnn</a></code>: train deep neural network
</p>
</li>
<li> <p><code><a href="#topic+analyze_training">analyze_training</a></code>: check for convergence by comparing training loss with baseline loss
</p>
</li>
<li> <p><code><a href="#topic+continue_training">continue_training</a></code>: continues training of an existing cito dnn model for additional epochs
</p>
</li>
<li> <p><code><a href="#topic+summary.citodnn">summary.citodnn</a></code>: extract xAI metrics/effects to understand how predictions are made
</p>
</li>
<li> <p><code><a href="#topic+PDP">PDP</a></code>: plot the partial dependency plot for a specific feature
</p>
</li>
<li> <p><code><a href="#topic+ALE">ALE</a></code>: plot the accumulated local effect plot for a specific feature
</p>
</li></ul>

<p>Check out the vignettes for more details on training NN and how a typical workflow with 'cito' could look like.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

# Example workflow in cito

## Build and train  Network
### softmax is used for multi-class responses (e.g., Species)
nn.fit&lt;- dnn(Species~., data = datasets::iris, loss = "softmax")

## The training loss is below the baseline loss but at the end of the
## training the loss was still decreasing, so continue training for another 50
## epochs
nn.fit &lt;- continue_training(nn.fit, epochs = 50L)

# Sturcture of Neural Network
print(nn.fit)

# Plot Neural Network
plot(nn.fit)
## 4 Input nodes (first layer) because of 4 features
## 3 Output nodes (last layer) because of 3 response species (one node for each
## level in the response variable).
## The layers between the input and output layer are called hidden layers (two
## of them)

## We now want to understand how the predictions are made, what are the
## important features? The summary function automatically calculates feature
## importance (the interpretation is similar to an anova) and calculates
## average conditional effects that are similar to linear effects:
summary(nn.fit)

## To visualize the effect (response-feature effect), we can use the ALE and
## PDP functions

# Partial dependencies
PDP(nn.fit, variable = "Petal.Length")

# Accumulated local effect plots
ALE(nn.fit, variable = "Petal.Length")



# Per se, it is difficult to get confidence intervals for our xAI metrics (or
# for the predictions). But we can use bootstrapping to obtain uncertainties
# for all cito outputs:
## Re-fit the neural network with bootstrapping
nn.fit&lt;- dnn(Species~.,
             data = datasets::iris,
             loss = "softmax",
             epochs = 150L,
             verbose = FALSE,
             bootstrap = 20L)
## convergence can be tested via the analyze_training function
analyze_training(nn.fit)

## Summary for xAI metrics (can take some time):
summary(nn.fit)
## Now with standard errors and p-values
## Note: Take the p-values with a grain of salt! We do not know yet if they are
## correct (e.g. if you use regularization, they are likely conservative == too
## large)

## Predictions with bootstrapping:
dim(predict(nn.fit))
## predictions are by default averaged (over the bootstrap samples)



# Hyperparameter tuning (experimental feature)
hidden_values = matrix(c(5, 2,
                         4, 2,
                         10,2,
                         15,2), 4, 2, byrow = TRUE)
## Potential architectures we want to test, first column == number of nodes
print(hidden_values)

nn.fit = dnn(Species~.,
             data = iris,
             epochs = 30L,
             loss = "softmax",
             hidden = tune(values = hidden_values),
             lr = tune(0.00001, 0.1) # tune lr between range 0.00001 and 0.1
             )
## Tuning results:
print(nn.fit$tuning)

# test = Inf means that tuning was cancelled after only one fit (within the CV)


# Advanced: Custom loss functions and additional parameters
## Normal Likelihood with sd parameter:
custom_loss = function(pred, true) {
  logLik = torch::distr_normal(pred,
                               scale = torch::nnf_relu(scale)+
                                 0.001)$log_prob(true)
  return(-logLik$mean())
}

nn.fit&lt;- dnn(Sepal.Length~.,
             data = datasets::iris,
             loss = custom_loss,
             verbose = FALSE,
             custom_parameters = list(scale = 1.0)
)
nn.fit$parameter$scale

## Multivariate normal likelihood with parametrized covariance matrix
## Sigma = L*L^t + D
## Helper function to build covariance matrix
create_cov = function(LU, Diag) {
  return(torch::torch_matmul(LU, LU$t()) + torch::torch_diag(Diag$exp()+0.01))
}

custom_loss_MVN = function(true, pred) {
  Sigma = create_cov(SigmaPar, SigmaDiag)
  logLik = torch::distr_multivariate_normal(pred,
                                            covariance_matrix = Sigma)$
    log_prob(true)
  return(-logLik$mean())
}


nn.fit&lt;- dnn(cbind(Sepal.Length, Sepal.Width, Petal.Length)~.,
             data = datasets::iris,
             lr = 0.01,
             verbose = FALSE,
             loss = custom_loss_MVN,
             custom_parameters =
               list(SigmaDiag =  rep(0, 3),
                    SigmaPar = matrix(rnorm(6, sd = 0.001), 3, 2))
)
as.matrix(create_cov(nn.fit$loss$parameter$SigmaPar,
                     nn.fit$loss$parameter$SigmaDiag))

}

</code></pre>

<hr>
<h2 id='cnn'>CNN</h2><span id='topic+cnn'></span>

<h3>Description</h3>

<p>fits a custom convolutional neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnn(
  X,
  Y = NULL,
  architecture,
  loss = c("mse", "mae", "softmax", "cross-entropy", "gaussian", "binomial", "poisson"),
  optimizer = c("sgd", "adam", "adadelta", "adagrad", "rmsprop", "rprop"),
  lr = 0.01,
  alpha = 0.5,
  lambda = 0,
  validation = 0,
  batchsize = 32L,
  burnin = 10,
  shuffle = TRUE,
  epochs = 100,
  early_stopping = NULL,
  lr_scheduler = NULL,
  custom_parameters = NULL,
  device = c("cpu", "cuda", "mps"),
  plot = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnn_+3A_x">X</code></td>
<td>
<p>predictor: array with dimension 3, 4 or 5 for 1D-, 2D- or 3D-convolutions, respectively. The first dimension are the samples, the second dimension the channels and the third - fifth dimension are the input dimensions</p>
</td></tr>
<tr><td><code id="cnn_+3A_y">Y</code></td>
<td>
<p>response: vector, factor, numerical matrix or logical matrix</p>
</td></tr>
<tr><td><code id="cnn_+3A_architecture">architecture</code></td>
<td>
<p>'citoarchitecture' object created by <code><a href="#topic+create_architecture">create_architecture</a></code></p>
</td></tr>
<tr><td><code id="cnn_+3A_loss">loss</code></td>
<td>
<p>loss after which network should be optimized. Can also be distribution from the stats package or own function, see details</p>
</td></tr>
<tr><td><code id="cnn_+3A_optimizer">optimizer</code></td>
<td>
<p>which optimizer used for training the network, for more adjustments to optimizer see <code><a href="#topic+config_optimizer">config_optimizer</a></code></p>
</td></tr>
<tr><td><code id="cnn_+3A_lr">lr</code></td>
<td>
<p>learning rate given to optimizer</p>
</td></tr>
<tr><td><code id="cnn_+3A_alpha">alpha</code></td>
<td>
<p>add L1/L2 regularization to training  <code class="reqn">(1 - \alpha) * |weights| + \alpha ||weights||^2</code> will get added for each layer. Must be between 0 and 1</p>
</td></tr>
<tr><td><code id="cnn_+3A_lambda">lambda</code></td>
<td>
<p>strength of regularization: lambda penalty, <code class="reqn">\lambda * (L1 + L2)</code> (see alpha)</p>
</td></tr>
<tr><td><code id="cnn_+3A_validation">validation</code></td>
<td>
<p>percentage of data set that should be taken as validation set (chosen randomly)</p>
</td></tr>
<tr><td><code id="cnn_+3A_batchsize">batchsize</code></td>
<td>
<p>number of samples that are used to calculate one learning rate step</p>
</td></tr>
<tr><td><code id="cnn_+3A_burnin">burnin</code></td>
<td>
<p>training is aborted if the trainings loss is not below the baseline loss after burnin epochs</p>
</td></tr>
<tr><td><code id="cnn_+3A_shuffle">shuffle</code></td>
<td>
<p>if TRUE, data in each batch gets reshuffled every epoch</p>
</td></tr>
<tr><td><code id="cnn_+3A_epochs">epochs</code></td>
<td>
<p>epochs the training goes on for</p>
</td></tr>
<tr><td><code id="cnn_+3A_early_stopping">early_stopping</code></td>
<td>
<p>if set to integer, training will stop if loss has gotten higher for defined number of epochs in a row, will use validation loss if available.</p>
</td></tr>
<tr><td><code id="cnn_+3A_lr_scheduler">lr_scheduler</code></td>
<td>
<p>learning rate scheduler created with <code><a href="#topic+config_lr_scheduler">config_lr_scheduler</a></code></p>
</td></tr>
<tr><td><code id="cnn_+3A_custom_parameters">custom_parameters</code></td>
<td>
<p>List of parameters/variables to be optimized. Can be used in a custom loss function. See Vignette for example.</p>
</td></tr>
<tr><td><code id="cnn_+3A_device">device</code></td>
<td>
<p>device on which network should be trained on.</p>
</td></tr>
<tr><td><code id="cnn_+3A_plot">plot</code></td>
<td>
<p>plot training loss</p>
</td></tr>
<tr><td><code id="cnn_+3A_verbose">verbose</code></td>
<td>
<p>print training and validation loss of epochs</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an S3 object of class <code>"citocnn"</code> is returned. It is a list containing everything there is to know about the model and its training process.
The list consists of the following attributes:
</p>
<table>
<tr><td><code>net</code></td>
<td>
<p>An object of class &quot;nn_sequential&quot; &quot;nn_module&quot;, originates from the torch package and represents the core object of this workflow.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The original function call</p>
</td></tr>
<tr><td><code>loss</code></td>
<td>
<p>A list which contains relevant information for the target variable and the used loss function</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>Contains data used for training the model</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>List of weights for each training epoch</p>
</td></tr>
<tr><td><code>use_model_epoch</code></td>
<td>
<p>Integer, which defines which model from which training epoch should be used for prediction.</p>
</td></tr>
<tr><td><code>loaded_model_epoch</code></td>
<td>
<p>Integer, shows which model from which epoch is loaded currently into model$net.</p>
</td></tr>
<tr><td><code>model_properties</code></td>
<td>
<p>A list of properties of the neural network, contains number of input nodes, number of output nodes, size of hidden layers, activation functions, whether bias is included and if dropout layers are included.</p>
</td></tr>
<tr><td><code>training_properties</code></td>
<td>
<p>A list of all training parameters that were used the last time the model was trained. It consists of learning rate, information about an learning rate scheduler, information about the optimizer, number of epochs, whether early stopping was used, if plot was active, lambda and alpha for L1/L2 regularization, batchsize, shuffle, was the data set split into validation and training, which formula was used for training and at which epoch did the training stop.</p>
</td></tr>
<tr><td><code>losses</code></td>
<td>
<p>A data.frame containing training and validation losses of each epoch</p>
</td></tr>
</table>


<h3>Convolutional neural networks:</h3>

<p>Convolutional Neural Networks (CNNs) are a specialized type of neural network designed for processing structured grid data, such as images.
The characterizing parts of the architecture are convolutional layers, pooling layers and fully-connected (linear) layers:
</p>

<ul>
<li><p> Convolutional layers are the core building blocks of CNNs. They consist of filters (also called kernels), which are small, learnable matrices. These filters slide over the input data to perform element-wise multiplication, producing feature maps that capture local patterns and features. Multiple filters are used to detect different features in parallel. They help the network learn hierarchical representations of the input data by capturing low-level features (edges, textures) and gradually combining them (in subsequent convolutional layers) to form higher-level features.
</p>
</li>
<li><p> Pooling layers are used to downsample the spatial dimensions of the feature maps while retaining important information. Max pooling is a common pooling operation, where the maximum value in a local region of the input is retained, reducing the size of the feature maps.
</p>
</li>
<li><p> Fully-connected (linear) layers connect every neuron in one layer to every neuron in the next layer. These layers are found at the end of the network and are responsible for combining high-level features to make final predictions.
</p>
</li></ul>



<h3>Loss functions / Likelihoods</h3>

<p>We support loss functions and likelihoods for different tasks:</p>

<table>
<tr>
 <td style="text-align: left;">
   Name </td><td style="text-align: left;"> Explanation </td><td style="text-align: left;"> Example / Task </td>
</tr>
<tr>
 <td style="text-align: left;">
   mse </td><td style="text-align: left;"> mean squared error </td><td style="text-align: left;"> Regression, predicting continuous values </td>
</tr>
<tr>
 <td style="text-align: left;">
   mae </td><td style="text-align: left;"> mean absolute error </td><td style="text-align: left;"> Regression, predicting continuous values </td>
</tr>
<tr>
 <td style="text-align: left;">
   softmax </td><td style="text-align: left;"> categorical cross entropy </td><td style="text-align: left;"> Multi-class, species classification </td>
</tr>
<tr>
 <td style="text-align: left;">
   cross-entropy </td><td style="text-align: left;"> categorical cross entropy </td><td style="text-align: left;"> Multi-class, species classification </td>
</tr>
<tr>
 <td style="text-align: left;">
   gaussian </td><td style="text-align: left;"> Normal likelihood </td><td style="text-align: left;"> Regression, residual error is also estimated (similar to <code>stats::lm()</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
   binomial </td><td style="text-align: left;"> Binomial likelihood </td><td style="text-align: left;"> Classification/Logistic regression, mortality </td>
</tr>
<tr>
 <td style="text-align: left;">
   Poisson </td><td style="text-align: left;"> Poisson likelihood </td><td style="text-align: left;"> Regression, count data, e.g. species abundances </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Training and convergence of neural networks</h3>

<p>Ensuring convergence can be tricky when training neural networks. Their training is sensitive to a combination of the learning rate (how much the weights are updated in each optimization step), the batch size (a random subset of the data is used in each optimization step), and the number of epochs (number of optimization steps). Typically, the learning rate should be decreased with the size of the neural networks (amount of learnable parameters). We provide a baseline loss (intercept only model) that can give hints about an appropriate learning rate:
</p>
<p><img src="../help/figures/learningrates.jpg" alt="Learning rates" />
</p>
<p>If the training loss of the model doesn't fall below the baseline loss, the learning rate is either too high or too low. If this happens, try higher and lower learning rates.
</p>
<p>A common strategy is to try (manually) a few different learning rates to see if the learning rate is on the right scale.
</p>
<p>See the troubleshooting vignette (<code>vignette("B-Training_neural_networks")</code>) for more help on training and debugging neural networks.
</p>


<h3>Finding the right architecture</h3>

<p>As with the learning rate, there is no definitive guide to choosing the right architecture for the right task. However, there are some general rules/recommendations: In general, wider, and deeper neural networks can improve generalization - but this is a double-edged sword because it also increases the risk of overfitting. So, if you increase the width and depth of the network, you should also add regularization (e.g., by increasing the lambda parameter, which corresponds to the regularization strength). Furthermore, in <a href="https://arxiv.org/abs/2306.10551">Pichler &amp; Hartig, 2023</a>, we investigated the effects of the hyperparameters on the prediction performance as a function of the data size. For example, we found that the <code>selu</code> activation function outperforms <code>relu</code> for small data sizes (&lt;100 observations).
</p>
<p>We recommend starting with moderate sizes (like the defaults), and if the model doesn't generalize/converge, try larger networks along with a regularization that helps minimize the risk of overfitting (see <code>vignette("B-Training_neural_networks")</code> ).
</p>


<h3>Overfitting</h3>

<p>Overfitting means that the model fits the training data well, but generalizes poorly to new observations. We can use the validation argument to detect overfitting. If the validation loss starts to increase again at a certain point, it often means that the models are starting to overfit your training data:
</p>
<p><img src="../help/figures/overfitting.jpg" alt="Overfitting" />
</p>
<p><strong>Solutions</strong>:
</p>

<ul>
<li><p> Re-train with epochs = point where model started to overfit
</p>
</li>
<li><p> Early stopping, stop training when model starts to overfit, can be specified using the <code style="white-space: pre;">&#8288;early_stopping=…&#8288;</code> argument
</p>
</li>
<li><p> Use regularization (dropout or elastic-net, see next section)
</p>
</li></ul>



<h3>Regularization</h3>

<p>Elastic Net regularization combines the strengths of L1 (Lasso) and L2 (Ridge) regularization. It introduces a penalty term that encourages sparse weight values while maintaining overall weight shrinkage. By controlling the sparsity of the learned model, Elastic Net regularization helps avoid overfitting while allowing for meaningful feature selection. We advise using elastic net (e.g. lambda = 0.001 and alpha = 0.2).
</p>
<p>Dropout regularization helps prevent overfitting by randomly disabling a portion of neurons during training. This technique encourages the network to learn more robust and generalized representations, as it prevents individual neurons from relying too heavily on specific input patterns. Dropout has been widely adopted as a simple yet effective regularization method in deep learning.
In the case of 2D and 3D inputs whole feature maps are disabled. Since the torch package doesn't currently support feature map-wise dropout for 1D inputs, instead random neurons in the feature maps are disabled similar to dropout in linear layers.
</p>
<p>By utilizing these regularization methods in your neural network training with the cito package, you can improve generalization performance and enhance the network's ability to handle unseen data. These techniques act as valuable tools in mitigating overfitting and promoting more robust and reliable model performance.
</p>


<h3>Custom Optimizer and Learning Rate Schedulers</h3>

<p>When training a network, you have the flexibility to customize the optimizer settings and learning rate scheduler to optimize the learning process. In the cito package, you can initialize these configurations using the <code><a href="#topic+config_lr_scheduler">config_lr_scheduler</a></code> and <code><a href="#topic+config_optimizer">config_optimizer</a></code> functions.
</p>
<p><code><a href="#topic+config_lr_scheduler">config_lr_scheduler</a></code> allows you to define a specific learning rate scheduler that controls how the learning rate changes over time during training. This is beneficial in scenarios where you want to adaptively adjust the learning rate to improve convergence or avoid getting stuck in local optima.
</p>
<p>Similarly, the <code><a href="#topic+config_optimizer">config_optimizer</a></code> function enables you to specify the optimizer for your network. Different optimizers, such as stochastic gradient descent (SGD), Adam, or RMSprop, offer various strategies for updating the network's weights and biases during training. Choosing the right optimizer can significantly impact the training process and the final performance of your neural network.
</p>


<h3>Training on graphic cards</h3>

<p>If you have an NVIDIA CUDA-enabled device and have installed the CUDA toolkit version 11.3 and cuDNN 8.4, you can take advantage of GPU acceleration for training your neural networks. It is crucial to have these specific versions installed, as other versions may not be compatible.
For detailed installation instructions and more information on utilizing GPUs for training, please refer to the <a href="https://torch.mlverse.org/docs/articles/installation.html">mlverse: 'torch' documentation</a>.
</p>
<p>Note: GPU training is optional, and the package can still be used for training on CPU even without CUDA and cuDNN installations.
</p>


<h3>Author(s)</h3>

<p>Armin Schenk, Maximilian Pichler
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.citocnn">predict.citocnn</a></code>, <code><a href="#topic+plot.citocnn">plot.citocnn</a></code>,  <code><a href="#topic+coef.citocnn">coef.citocnn</a></code>, <code><a href="#topic+print.citocnn">print.citocnn</a></code>, <code><a href="#topic+summary.citocnn">summary.citocnn</a></code>, <code><a href="#topic+continue_training">continue_training</a></code>, <code><a href="#topic+analyze_training">analyze_training</a></code>
</p>

<hr>
<h2 id='coef.citocnn'>Returns list of parameters the neural network model currently has in use</h2><span id='topic+coef.citocnn'></span>

<h3>Description</h3>

<p>Returns list of parameters the neural network model currently has in use
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citocnn'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.citocnn_+3A_object">object</code></td>
<td>
<p>a model created by <code><a href="#topic+cnn">cnn</a></code></p>
</td></tr>
<tr><td><code id="coef.citocnn_+3A_...">...</code></td>
<td>
<p>nothing implemented yet</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of weights of neural network
</p>

<hr>
<h2 id='coef.citodnn'>Returns list of parameters the neural network model currently has in use</h2><span id='topic+coef.citodnn'></span><span id='topic+coef.citodnnBootstrap'></span>

<h3>Description</h3>

<p>Returns list of parameters the neural network model currently has in use
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citodnn'
coef(object, ...)

## S3 method for class 'citodnnBootstrap'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.citodnn_+3A_object">object</code></td>
<td>
<p>a model created by <code><a href="#topic+dnn">dnn</a></code></p>
</td></tr>
<tr><td><code id="coef.citodnn_+3A_...">...</code></td>
<td>
<p>nothing implemented yet</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of weights of neural network
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

set.seed(222)
validation_set&lt;- sample(c(1:nrow(datasets::iris)),25)

# Build and train  Network
nn.fit&lt;- dnn(Sepal.Length~., data = datasets::iris[-validation_set,])

# Sturcture of Neural Network
print(nn.fit)

#analyze weights of Neural Network
coef(nn.fit)
}

</code></pre>

<hr>
<h2 id='conditionalEffects'>Calculate average conditional effects</h2><span id='topic+conditionalEffects'></span><span id='topic+conditionalEffects.citodnn'></span><span id='topic+conditionalEffects.citodnnBootstrap'></span>

<h3>Description</h3>

<p>Average conditional effects calculate the local derivatives for each observation for each feature. They are similar to marginal effects. And the average of these conditional effects is an approximation of linear effects (see Pichler and Hartig, 2023 for more details). You can use this function to either calculate main effects (on the diagonal, take a look at the example) or interaction effects (off-diagonals) between features.
</p>
<p>To obtain uncertainties for these effects, enable the bootstrapping option in the <code>dnn(..)</code> function (see example).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conditionalEffects(
  object,
  interactions = FALSE,
  epsilon = 0.1,
  device = c("cpu", "cuda", "mps"),
  indices = NULL,
  data = NULL,
  type = "response",
  ...
)

## S3 method for class 'citodnn'
conditionalEffects(
  object,
  interactions = FALSE,
  epsilon = 0.1,
  device = c("cpu", "cuda", "mps"),
  indices = NULL,
  data = NULL,
  type = "response",
  ...
)

## S3 method for class 'citodnnBootstrap'
conditionalEffects(
  object,
  interactions = FALSE,
  epsilon = 0.1,
  device = c("cpu", "cuda", "mps"),
  indices = NULL,
  data = NULL,
  type = "response",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conditionalEffects_+3A_object">object</code></td>
<td>
<p>object of class <code>citodnn</code></p>
</td></tr>
<tr><td><code id="conditionalEffects_+3A_interactions">interactions</code></td>
<td>
<p>calculate interactions or not (computationally expensive)</p>
</td></tr>
<tr><td><code id="conditionalEffects_+3A_epsilon">epsilon</code></td>
<td>
<p>difference used to calculate derivatives</p>
</td></tr>
<tr><td><code id="conditionalEffects_+3A_device">device</code></td>
<td>
<p>which device</p>
</td></tr>
<tr><td><code id="conditionalEffects_+3A_indices">indices</code></td>
<td>
<p>of variables for which the ACE are calculated</p>
</td></tr>
<tr><td><code id="conditionalEffects_+3A_data">data</code></td>
<td>
<p>data which is used to calculate the ACE</p>
</td></tr>
<tr><td><code id="conditionalEffects_+3A_type">type</code></td>
<td>
<p>ACE on which scale (response or link)</p>
</td></tr>
<tr><td><code id="conditionalEffects_+3A_...">...</code></td>
<td>
<p>additional arguments that are passed to the predict function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an S3 object of class <code>"conditionalEffects"</code> is returned.
The list consists of the following attributes:
</p>
<table>
<tr><td><code>result</code></td>
<td>
<p>3-dimensional array with the raw results</p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p>Matrix, average conditional effects</p>
</td></tr>
<tr><td><code>abs</code></td>
<td>
<p>Matrix, summed absolute conditional effects</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>Matrix, standard deviation of the conditional effects</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Maximilian Pichler
</p>


<h3>References</h3>

<p>Scholbeck, C. A., Casalicchio, G., Molnar, C., Bischl, B., &amp; Heumann, C. (2022). Marginal effects for non-linear prediction functions. arXiv preprint arXiv:2201.08837.
</p>
<p>Pichler, M., &amp; Hartig, F. (2023). Can predictive models be used for causal inference?. arXiv preprint arXiv:2306.10551.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

# Build and train  Network
nn.fit = dnn(Sepal.Length~., data = datasets::iris)

# Calculate average conditional effects
ACE = conditionalEffects(nn.fit)

## Main effects (categorical features are not supported)
ACE

## With interaction effects:
ACE = conditionalEffects(nn.fit, interactions = TRUE)
## The off diagonal elements are the interaction effects
ACE[[1]]$mean
## ACE is a list, elements correspond to the number of response classes
## Sepal.length == 1 Response so we have only one
## list element in the ACE object

# Re-train NN with bootstrapping to obtain standard errors
nn.fit = dnn(Sepal.Length~., data = datasets::iris, bootstrap = 30L)
## The summary method calculates also the conditional effects, and if
## bootstrapping was used, it will also report standard errors and p-values:
summary(nn.fit)


}

</code></pre>

<hr>
<h2 id='config_lr_scheduler'>Creation of customized learning rate scheduler objects</h2><span id='topic+config_lr_scheduler'></span>

<h3>Description</h3>

<p>Helps create custom learning rate schedulers for <code><a href="#topic+dnn">dnn</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>config_lr_scheduler(
  type = c("lambda", "multiplicative", "reduce_on_plateau", "one_cycle", "step"),
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="config_lr_scheduler_+3A_type">type</code></td>
<td>
<p>String defining which type of scheduler should be used. See Details.</p>
</td></tr>
<tr><td><code id="config_lr_scheduler_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, additional information about scheduler will be printed to console.</p>
</td></tr>
<tr><td><code id="config_lr_scheduler_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to scheduler. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>different learning rate scheduler need different variables, these functions will tell you which variables can be set:
</p>

<ul>
<li><p> lambda: <code><a href="torch.html#topic+lr_lambda">lr_lambda</a></code>
</p>
</li>
<li><p> multiplicative: <code><a href="torch.html#topic+lr_multiplicative">lr_multiplicative</a></code>
</p>
</li>
<li><p> reduce_on_plateau: <code><a href="torch.html#topic+lr_reduce_on_plateau">lr_reduce_on_plateau</a></code>
</p>
</li>
<li><p> one_cycle: <code><a href="torch.html#topic+lr_one_cycle">lr_one_cycle</a></code>
</p>
</li>
<li><p> step: <code><a href="torch.html#topic+lr_step">lr_step</a></code>
</p>
</li></ul>



<h3>Value</h3>

<p>object of class cito_lr_scheduler to give to <code><a href="#topic+dnn">dnn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

# create learning rate scheduler object
scheduler &lt;- config_lr_scheduler(type = "step",
                        step_size = 30,
                        gamma = 0.15,
                        verbose = TRUE)

# Build and train  Network
nn.fit&lt;- dnn(Sepal.Length~., data = datasets::iris, lr_scheduler = scheduler)

}

</code></pre>

<hr>
<h2 id='config_optimizer'>Creation of customized optimizer objects</h2><span id='topic+config_optimizer'></span>

<h3>Description</h3>

<p>Helps you create custom optimizer for <code><a href="#topic+dnn">dnn</a></code>. It is recommended to set learning rate in <code><a href="#topic+dnn">dnn</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>config_optimizer(
  type = c("adam", "adadelta", "adagrad", "rmsprop", "rprop", "sgd"),
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="config_optimizer_+3A_type">type</code></td>
<td>
<p>character string defining which optimizer should be used. See Details.</p>
</td></tr>
<tr><td><code id="config_optimizer_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, additional information about scheduler will be printed to console</p>
</td></tr>
<tr><td><code id="config_optimizer_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to optimizer. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>different optimizer need different variables, this function will tell you how the variables are set.
For more information see the corresponding functions:
</p>

<ul>
<li><p> adam: <code><a href="torch.html#topic+optim_adam">optim_adam</a></code>
</p>
</li>
<li><p> adadelta: <code><a href="torch.html#topic+optim_adadelta">optim_adadelta</a></code>
</p>
</li>
<li><p> adagrad: <code><a href="torch.html#topic+optim_adagrad">optim_adagrad</a></code>
</p>
</li>
<li><p> rmsprop: <code><a href="torch.html#topic+optim_rmsprop">optim_rmsprop</a></code>
</p>
</li>
<li><p> rprop: <code><a href="torch.html#topic+optim_rprop">optim_rprop</a></code>
</p>
</li>
<li><p> sgd: <code><a href="torch.html#topic+optim_sgd">optim_sgd</a></code>
</p>
</li></ul>



<h3>Value</h3>

<p>object of class cito_optim to give to <code><a href="#topic+dnn">dnn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

# create optimizer object
opt &lt;- config_optimizer(type = "adagrad",
                        lr_decay = 1e-04,
                        weight_decay = 0.1,
                        verbose = TRUE)

# Build and train  Network
nn.fit&lt;- dnn(Sepal.Length~., data = datasets::iris, optimizer = opt)

}

</code></pre>

<hr>
<h2 id='config_tuning'>Config hyperparameter tuning</h2><span id='topic+config_tuning'></span>

<h3>Description</h3>

<p>Config hyperparameter tuning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>config_tuning(
  CV = 5,
  steps = 10,
  parallel = FALSE,
  NGPU = 1,
  cancel = TRUE,
  bootstrap_final = NULL,
  bootstrap_parallel = FALSE,
  return_models = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="config_tuning_+3A_cv">CV</code></td>
<td>
<p>numeric, specifies k-folded cross validation</p>
</td></tr>
<tr><td><code id="config_tuning_+3A_steps">steps</code></td>
<td>
<p>numeric, number of random tuning steps</p>
</td></tr>
<tr><td><code id="config_tuning_+3A_parallel">parallel</code></td>
<td>
<p>numeric, number of parallel cores (tuning steps are parallelized)</p>
</td></tr>
<tr><td><code id="config_tuning_+3A_ngpu">NGPU</code></td>
<td>
<p>numeric, set if more than one GPU is available, tuning will be parallelized over CPU cores and GPUs, only works for NCPU &gt; 1</p>
</td></tr>
<tr><td><code id="config_tuning_+3A_cancel">cancel</code></td>
<td>
<p>CV/tuning for specific hyperparameter set if model cannot reduce loss below baseline after burnin or returns NA loss</p>
</td></tr>
<tr><td><code id="config_tuning_+3A_bootstrap_final">bootstrap_final</code></td>
<td>
<p>bootstrap final model, if all models should be boostrapped it must be set globally via the bootstrap argument in the <code><a href="#topic+dnn">dnn()</a></code> function</p>
</td></tr>
<tr><td><code id="config_tuning_+3A_bootstrap_parallel">bootstrap_parallel</code></td>
<td>
<p>should the bootstrapping be parallelized or not</p>
</td></tr>
<tr><td><code id="config_tuning_+3A_return_models">return_models</code></td>
<td>
<p>return individual models</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that hyperparameter tuning can be expensive. We have implemented an option to parallelize hyperparameter tuning, including parallelization over one or more GPUs (the hyperparameter evaluation is parallelized, not the CV). This can be especially useful for small models. For example, if you have 4 GPUs, 20 CPU cores, and 20 steps (random samples from the random search), you could run &lsquo;dnn(..., device=&quot;cuda&quot;,lr = tune(), batchsize=tune(), tuning=config_tuning(parallel=20, NGPU=4)&rsquo;, which will distribute 20 model fits across 4 GPUs, so that each GPU will process 5 models (in parallel).
</p>

<hr>
<h2 id='continue_training'>Continues training of a model generated with <code><a href="#topic+dnn">dnn</a></code> or <code><a href="#topic+cnn">cnn</a></code> for additional epochs.</h2><span id='topic+continue_training'></span><span id='topic+continue_training.citodnn'></span><span id='topic+continue_training.citodnnBootstrap'></span><span id='topic+continue_training.citocnn'></span>

<h3>Description</h3>

<p>If the training/validation loss is still decreasing at the end of the training, it is often a sign that the NN has not yet converged. You can use this function to continue training instead of re-training the entire model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>continue_training(model, ...)

## S3 method for class 'citodnn'
continue_training(
  model,
  epochs = 32,
  data = NULL,
  device = NULL,
  verbose = TRUE,
  changed_params = NULL,
  ...
)

## S3 method for class 'citodnnBootstrap'
continue_training(
  model,
  epochs = 32,
  data = NULL,
  device = NULL,
  verbose = TRUE,
  changed_params = NULL,
  parallel = FALSE,
  ...
)

## S3 method for class 'citocnn'
continue_training(
  model,
  epochs = 32,
  X = NULL,
  Y = NULL,
  device = c("cpu", "cuda", "mps"),
  verbose = TRUE,
  changed_params = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="continue_training_+3A_model">model</code></td>
<td>
<p>a model created by <code><a href="#topic+dnn">dnn</a></code> or <code><a href="#topic+cnn">cnn</a></code></p>
</td></tr>
<tr><td><code id="continue_training_+3A_...">...</code></td>
<td>
<p>class-specific arguments</p>
</td></tr>
<tr><td><code id="continue_training_+3A_epochs">epochs</code></td>
<td>
<p>additional epochs the training should continue for</p>
</td></tr>
<tr><td><code id="continue_training_+3A_data">data</code></td>
<td>
<p>matrix or data.frame. If not provided data from original training will be used</p>
</td></tr>
<tr><td><code id="continue_training_+3A_device">device</code></td>
<td>
<p>can be used to overwrite device used in previous training</p>
</td></tr>
<tr><td><code id="continue_training_+3A_verbose">verbose</code></td>
<td>
<p>print training and validation loss of epochs</p>
</td></tr>
<tr><td><code id="continue_training_+3A_changed_params">changed_params</code></td>
<td>
<p>list of arguments to change compared to original training setup, see <code><a href="#topic+dnn">dnn</a></code> which parameter can be changed</p>
</td></tr>
<tr><td><code id="continue_training_+3A_parallel">parallel</code></td>
<td>
<p>train bootstrapped model in parallel</p>
</td></tr>
<tr><td><code id="continue_training_+3A_x">X</code></td>
<td>
<p>array. If not provided X from original training will be used</p>
</td></tr>
<tr><td><code id="continue_training_+3A_y">Y</code></td>
<td>
<p>vector, factor, numerical matrix or logical matrix. If not provided Y from original training will be used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a model of class citodnn, citodnnBootstrap or citocnn created by <code><a href="#topic+dnn">dnn</a></code> or <code><a href="#topic+cnn">cnn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

set.seed(222)
validation_set&lt;- sample(c(1:nrow(datasets::iris)),25)

# Build and train  Network
nn.fit&lt;- dnn(Sepal.Length~., data = datasets::iris[-validation_set,], epochs = 32)

# continue training for another 32 epochs
nn.fit&lt;- continue_training(nn.fit,epochs = 32)

# Use model on validation set
predictions &lt;- predict(nn.fit, iris[validation_set,])
}

</code></pre>

<hr>
<h2 id='conv'>Convolutional layer</h2><span id='topic+conv'></span>

<h3>Description</h3>

<p>creates a 'conv' 'citolayer' object that is used by <code><a href="#topic+create_architecture">create_architecture</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conv(
  n_kernels = NULL,
  kernel_size = NULL,
  stride = NULL,
  padding = NULL,
  dilation = NULL,
  bias = NULL,
  activation = NULL,
  normalization = NULL,
  dropout = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conv_+3A_n_kernels">n_kernels</code></td>
<td>
<p>(int) amount of kernels in this layer</p>
</td></tr>
<tr><td><code id="conv_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple) size of the kernels in this layer. Use a tuple if the kernel size isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="conv_+3A_stride">stride</code></td>
<td>
<p>(int or tuple) stride of the kernels in this layer. NULL sets the stride equal to the kernel size. Use a tuple if the stride isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="conv_+3A_padding">padding</code></td>
<td>
<p>(int or tuple) zero-padding added to both sides of the input. Use a tuple if the padding isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="conv_+3A_dilation">dilation</code></td>
<td>
<p>(int or tuple) dilation of the kernels in this layer. Use a tuple if the dilation isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="conv_+3A_bias">bias</code></td>
<td>
<p>(boolean) if TRUE, adds a learnable bias to the kernels of this layer</p>
</td></tr>
<tr><td><code id="conv_+3A_activation">activation</code></td>
<td>
<p>(string) activation function that is used after this layer. The following activation functions are supported: &quot;relu&quot;, &quot;leaky_relu&quot;, &quot;tanh&quot;, &quot;elu&quot;, &quot;rrelu&quot;, &quot;prelu&quot;, &quot;softplus&quot;, &quot;celu&quot;, &quot;selu&quot;, &quot;gelu&quot;, &quot;relu6&quot;, &quot;sigmoid&quot;, &quot;softsign&quot;, &quot;hardtanh&quot;, &quot;tanhshrink&quot;, &quot;softshrink&quot;, &quot;hardshrink&quot;, &quot;log_sigmoid&quot;</p>
</td></tr>
<tr><td><code id="conv_+3A_normalization">normalization</code></td>
<td>
<p>(boolean) if TRUE, batch normalization is used after this layer</p>
</td></tr>
<tr><td><code id="conv_+3A_dropout">dropout</code></td>
<td>
<p>(float) dropout rate of this layer. Set to 0 for no dropout</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a 'conv' 'citolayer' object that is passed to the <code><a href="#topic+create_architecture">create_architecture</a></code> function.
The parameters that aren't assigned here (and are therefore still NULL) are filled with the default values passed to <code><a href="#topic+create_architecture">create_architecture</a></code>.
</p>


<h3>Value</h3>

<p>S3 object of class <code>"conv" "citolayer"</code>
</p>


<h3>Author(s)</h3>

<p>Armin Schenk
</p>


<h3>See Also</h3>

<p><code><a href="#topic+create_architecture">create_architecture</a></code>
</p>

<hr>
<h2 id='create_architecture'>CNN architecture</h2><span id='topic+create_architecture'></span>

<h3>Description</h3>

<p>creates a 'citoarchitecture' object that is used by <code><a href="#topic+cnn">cnn</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_architecture(
  ...,
  default_n_neurons = 10,
  default_n_kernels = 10,
  default_kernel_size = list(conv = 3, maxPool = 2, avgPool = 2),
  default_stride = list(conv = 1, maxPool = NULL, avgPool = NULL),
  default_padding = list(conv = 0, maxPool = 0, avgPool = 0),
  default_dilation = list(conv = 1, maxPool = 1),
  default_bias = list(conv = TRUE, linear = TRUE),
  default_activation = list(conv = "relu", linear = "relu"),
  default_normalization = list(conv = FALSE, linear = FALSE),
  default_dropout = list(conv = 0, linear = 0)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_architecture_+3A_...">...</code></td>
<td>
<p>objects of class 'citolayer' created by <code><a href="#topic+linear">linear</a></code>, <code><a href="#topic+conv">conv</a></code>, <code><a href="#topic+maxPool">maxPool</a></code>, <code><a href="#topic+avgPool">avgPool</a></code> or <code><a href="#topic+transfer">transfer</a></code></p>
</td></tr>
<tr><td><code id="create_architecture_+3A_default_n_neurons">default_n_neurons</code></td>
<td>
<p>(int) default value: amount of neurons in a linear layer</p>
</td></tr>
<tr><td><code id="create_architecture_+3A_default_n_kernels">default_n_kernels</code></td>
<td>
<p>(int) default value: amount of kernels in a convolutional layer</p>
</td></tr>
<tr><td><code id="create_architecture_+3A_default_kernel_size">default_kernel_size</code></td>
<td>
<p>(int or tuple) default value: size of the kernels in convolutional and pooling layers. Use a tuple if the kernel size isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="create_architecture_+3A_default_stride">default_stride</code></td>
<td>
<p>(int or tuple) default value: stride of the kernels in convolutional and pooling layers. NULL sets the stride equal to the kernel size. Use a tuple if the stride isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="create_architecture_+3A_default_padding">default_padding</code></td>
<td>
<p>(int or tuple) default value: zero-padding added to both sides of the input. Use a tuple if the padding isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="create_architecture_+3A_default_dilation">default_dilation</code></td>
<td>
<p>(int or tuple) default value: dilation of the kernels in convolutional and maxPooling layers. Use a tuple if the dilation isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="create_architecture_+3A_default_bias">default_bias</code></td>
<td>
<p>(boolean) default value: if TRUE, adds a learnable bias to neurons of linear and kernels of convolutional layers</p>
</td></tr>
<tr><td><code id="create_architecture_+3A_default_activation">default_activation</code></td>
<td>
<p>(string) default value: activation function that is used after linear and convolutional layers. The following activation functions are supported: &quot;relu&quot;, &quot;leaky_relu&quot;, &quot;tanh&quot;, &quot;elu&quot;, &quot;rrelu&quot;, &quot;prelu&quot;, &quot;softplus&quot;, &quot;celu&quot;, &quot;selu&quot;, &quot;gelu&quot;, &quot;relu6&quot;, &quot;sigmoid&quot;, &quot;softsign&quot;, &quot;hardtanh&quot;, &quot;tanhshrink&quot;, &quot;softshrink&quot;, &quot;hardshrink&quot;, &quot;log_sigmoid&quot;</p>
</td></tr>
<tr><td><code id="create_architecture_+3A_default_normalization">default_normalization</code></td>
<td>
<p>(boolean) default value: if TRUE, batch normalization is used after linear and convolutional layers</p>
</td></tr>
<tr><td><code id="create_architecture_+3A_default_dropout">default_dropout</code></td>
<td>
<p>(float) default value: dropout rate of linear and convolutional layers. Set to 0 for no dropout</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a 'citoarchitecture' object that provides the <code><a href="#topic+cnn">cnn</a></code> function with all information about the architecture of the CNN that will be created and trained.
The final architecture consists of the layers in the sequence they were passed to this function.
All parameters of the 'citolayer' objects, that are still NULL because they haven't been specified at the creation of the layer, are filled with the given default parameters for their specific layer type (linear, conv, maxPool, avgPool).
The default values can be changed by either passing a list with the values for specific layer types (in which case the defaults of layer types which aren't in the list remain the same)
or by passing a single value (in which case the defaults for all layer types is set to that value).
</p>


<h3>Value</h3>

<p>S3 object of class <code>"citoarchitecture"</code>
</p>


<h3>Author(s)</h3>

<p>Armin Schenk
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cnn">cnn</a></code>, <code><a href="#topic+linear">linear</a></code>, <code><a href="#topic+conv">conv</a></code>, <code><a href="#topic+maxPool">maxPool</a></code>, <code><a href="#topic+avgPool">avgPool</a></code>, <code><a href="#topic+transfer">transfer</a></code>, <code><a href="#topic+print.citoarchitecture">print.citoarchitecture</a></code>, <code><a href="#topic+plot.citoarchitecture">plot.citoarchitecture</a></code>
</p>

<hr>
<h2 id='dnn'>DNN</h2><span id='topic+dnn'></span>

<h3>Description</h3>

<p>fits a custom deep neural network using the Multilayer Perceptron architecture. <code>dnn()</code> supports the formula syntax and allows to customize the neural network to a maximal degree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnn(
  formula = NULL,
  data = NULL,
  hidden = c(50L, 50L),
  activation = "selu",
  bias = TRUE,
  dropout = 0,
  loss = c("mse", "mae", "softmax", "cross-entropy", "gaussian", "binomial", "poisson",
    "mvp", "nbinom"),
  validation = 0,
  lambda = 0,
  alpha = 0.5,
  optimizer = c("sgd", "adam", "adadelta", "adagrad", "rmsprop", "rprop"),
  lr = 0.01,
  batchsize = NULL,
  burnin = 30,
  baseloss = NULL,
  shuffle = TRUE,
  epochs = 100,
  bootstrap = NULL,
  bootstrap_parallel = FALSE,
  plot = TRUE,
  verbose = TRUE,
  lr_scheduler = NULL,
  custom_parameters = NULL,
  device = c("cpu", "cuda", "mps"),
  early_stopping = FALSE,
  tuning = config_tuning(),
  X = NULL,
  Y = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dnn_+3A_formula">formula</code></td>
<td>
<p>an object of class &quot;<code><a href="stats.html#topic+formula">formula</a></code>&quot;: a description of the model that should be fitted</p>
</td></tr>
<tr><td><code id="dnn_+3A_data">data</code></td>
<td>
<p>matrix or data.frame with features/predictors and response variable</p>
</td></tr>
<tr><td><code id="dnn_+3A_hidden">hidden</code></td>
<td>
<p>hidden units in layers, length of hidden corresponds to number of layers</p>
</td></tr>
<tr><td><code id="dnn_+3A_activation">activation</code></td>
<td>
<p>activation functions, can be of length one, or a vector of different activation functions for each layer</p>
</td></tr>
<tr><td><code id="dnn_+3A_bias">bias</code></td>
<td>
<p>whether use biases in the layers, can be of length one, or a vector (number of hidden layers + 1 (last layer)) of logicals for each layer.</p>
</td></tr>
<tr><td><code id="dnn_+3A_dropout">dropout</code></td>
<td>
<p>dropout rate, probability of a node getting left out during training (see <code><a href="torch.html#topic+nn_dropout">nn_dropout</a></code>)</p>
</td></tr>
<tr><td><code id="dnn_+3A_loss">loss</code></td>
<td>
<p>loss after which network should be optimized. Can also be distribution from the stats package or own function, see details</p>
</td></tr>
<tr><td><code id="dnn_+3A_validation">validation</code></td>
<td>
<p>percentage of data set that should be taken as validation set (chosen randomly)</p>
</td></tr>
<tr><td><code id="dnn_+3A_lambda">lambda</code></td>
<td>
<p>strength of regularization: lambda penalty, <code class="reqn">\lambda * (L1 + L2)</code> (see alpha)</p>
</td></tr>
<tr><td><code id="dnn_+3A_alpha">alpha</code></td>
<td>
<p>add L1/L2 regularization to training  <code class="reqn">(1 - \alpha) * |weights| + \alpha ||weights||^2</code> will get added for each layer. Must be between 0 and 1</p>
</td></tr>
<tr><td><code id="dnn_+3A_optimizer">optimizer</code></td>
<td>
<p>which optimizer used for training the network, for more adjustments to optimizer see <code><a href="#topic+config_optimizer">config_optimizer</a></code></p>
</td></tr>
<tr><td><code id="dnn_+3A_lr">lr</code></td>
<td>
<p>learning rate given to optimizer</p>
</td></tr>
<tr><td><code id="dnn_+3A_batchsize">batchsize</code></td>
<td>
<p>number of samples that are used to calculate one learning rate step, default is 10% of the training data</p>
</td></tr>
<tr><td><code id="dnn_+3A_burnin">burnin</code></td>
<td>
<p>training is aborted if the trainings loss is not below the baseline loss after burnin epochs</p>
</td></tr>
<tr><td><code id="dnn_+3A_baseloss">baseloss</code></td>
<td>
<p>baseloss, if null baseloss corresponds to intercept only models</p>
</td></tr>
<tr><td><code id="dnn_+3A_shuffle">shuffle</code></td>
<td>
<p>if TRUE, data in each batch gets reshuffled every epoch</p>
</td></tr>
<tr><td><code id="dnn_+3A_epochs">epochs</code></td>
<td>
<p>epochs the training goes on for</p>
</td></tr>
<tr><td><code id="dnn_+3A_bootstrap">bootstrap</code></td>
<td>
<p>bootstrap neural network or not, numeric corresponds to number of bootstrap samples</p>
</td></tr>
<tr><td><code id="dnn_+3A_bootstrap_parallel">bootstrap_parallel</code></td>
<td>
<p>parallelize (CPU) bootstrapping</p>
</td></tr>
<tr><td><code id="dnn_+3A_plot">plot</code></td>
<td>
<p>plot training loss</p>
</td></tr>
<tr><td><code id="dnn_+3A_verbose">verbose</code></td>
<td>
<p>print training and validation loss of epochs</p>
</td></tr>
<tr><td><code id="dnn_+3A_lr_scheduler">lr_scheduler</code></td>
<td>
<p>learning rate scheduler created with <code><a href="#topic+config_lr_scheduler">config_lr_scheduler</a></code></p>
</td></tr>
<tr><td><code id="dnn_+3A_custom_parameters">custom_parameters</code></td>
<td>
<p>List of parameters/variables to be optimized. Can be used in a custom loss function. See Vignette for example.</p>
</td></tr>
<tr><td><code id="dnn_+3A_device">device</code></td>
<td>
<p>device on which network should be trained on. mps correspond to M1/M2 GPU devices.</p>
</td></tr>
<tr><td><code id="dnn_+3A_early_stopping">early_stopping</code></td>
<td>
<p>if set to integer, training will stop if loss has gotten higher for defined number of epochs in a row, will use validation loss is available.</p>
</td></tr>
<tr><td><code id="dnn_+3A_tuning">tuning</code></td>
<td>
<p>tuning options created with <code><a href="#topic+config_tuning">config_tuning</a></code></p>
</td></tr>
<tr><td><code id="dnn_+3A_x">X</code></td>
<td>
<p>Feature matrix or data.frame, alternative data interface</p>
</td></tr>
<tr><td><code id="dnn_+3A_y">Y</code></td>
<td>
<p>Response vector, factor, matrix or data.frame, alternative data interface</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an S3 object of class <code>"cito.dnn"</code> is returned. It is a list containing everything there is to know about the model and its training process.
The list consists of the following attributes:
</p>
<table>
<tr><td><code>net</code></td>
<td>
<p>An object of class &quot;nn_sequential&quot; &quot;nn_module&quot;, originates from the torch package and represents the core object of this workflow.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The original function call</p>
</td></tr>
<tr><td><code>loss</code></td>
<td>
<p>A list which contains relevant information for the target variable and the used loss function</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>Contains data used for training the model</p>
</td></tr>
<tr><td><code>weigths</code></td>
<td>
<p>List of weights for each training epoch</p>
</td></tr>
<tr><td><code>use_model_epoch</code></td>
<td>
<p>Integer, which defines which model from which training epoch should be used for prediction. 1 = best model, 2 = last model</p>
</td></tr>
<tr><td><code>loaded_model_epoch</code></td>
<td>
<p>Integer, shows which model from which epoch is loaded currently into model$net.</p>
</td></tr>
<tr><td><code>model_properties</code></td>
<td>
<p>A list of properties of the neural network, contains number of input nodes, number of output nodes, size of hidden layers, activation functions, whether bias is included and if dropout layers are included.</p>
</td></tr>
<tr><td><code>training_properties</code></td>
<td>
<p>A list of all training parameters that were used the last time the model was trained. It consists of learning rate, information about an learning rate scheduler, information about the optimizer, number of epochs, whether early stopping was used, if plot was active, lambda and alpha for L1/L2 regularization, batchsize, shuffle, was the data set split into validation and training, which formula was used for training and at which epoch did the training stop.</p>
</td></tr>
<tr><td><code>losses</code></td>
<td>
<p>A data.frame containing training and validation losses of each epoch</p>
</td></tr>
</table>


<h3>Activation functions</h3>

<p>Supported activation functions:  &quot;relu&quot;, &quot;leaky_relu&quot;, &quot;tanh&quot;, &quot;elu&quot;, &quot;rrelu&quot;, &quot;prelu&quot;, &quot;softplus&quot;, &quot;celu&quot;, &quot;selu&quot;, &quot;gelu&quot;, &quot;relu6&quot;, &quot;sigmoid&quot;, &quot;softsign&quot;, &quot;hardtanh&quot;, &quot;tanhshrink&quot;, &quot;softshrink&quot;, &quot;hardshrink&quot;, &quot;log_sigmoid&quot;
</p>


<h3>Loss functions / Likelihoods</h3>

<p>We support loss functions and likelihoods for different tasks:</p>

<table>
<tr>
 <td style="text-align: left;">
   Name </td><td style="text-align: left;"> Explanation </td><td style="text-align: left;"> Example / Task </td>
</tr>
<tr>
 <td style="text-align: left;">
   mse </td><td style="text-align: left;"> mean squared error </td><td style="text-align: left;"> Regression, predicting continuous values </td>
</tr>
<tr>
 <td style="text-align: left;">
   mae </td><td style="text-align: left;"> mean absolute error </td><td style="text-align: left;"> Regression, predicting continuous values </td>
</tr>
<tr>
 <td style="text-align: left;">
   softmax </td><td style="text-align: left;"> categorical cross entropy </td><td style="text-align: left;"> Multi-class, species classification </td>
</tr>
<tr>
 <td style="text-align: left;">
   cross-entropy </td><td style="text-align: left;"> categorical cross entropy </td><td style="text-align: left;"> Multi-class, species classification </td>
</tr>
<tr>
 <td style="text-align: left;">
   gaussian </td><td style="text-align: left;"> Normal likelihood </td><td style="text-align: left;"> Regression, residual error is also estimated (similar to <code>stats::lm()</code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
   binomial </td><td style="text-align: left;"> Binomial likelihood </td><td style="text-align: left;"> Classification/Logistic regression, mortality </td>
</tr>
<tr>
 <td style="text-align: left;">
   poisson </td><td style="text-align: left;"> Poisson likelihood </td><td style="text-align: left;"> Regression, count data, e.g. species abundances </td>
</tr>
<tr>
 <td style="text-align: left;">
   nbinom </td><td style="text-align: left;"> Negative binomial likelihood </td><td style="text-align: left;"> Regression, count data with dispersion parameter </td>
</tr>
<tr>
 <td style="text-align: left;">
   mvp </td><td style="text-align: left;"> multivariate probit model </td><td style="text-align: left;"> joint species distribution model, multi species (presence absence) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Training and convergence of neural networks</h3>

<p>Ensuring convergence can be tricky when training neural networks. Their training is sensitive to a combination of the learning rate (how much the weights are updated in each optimization step), the batch size (a random subset of the data is used in each optimization step), and the number of epochs (number of optimization steps). Typically, the learning rate should be decreased with the size of the neural networks (depth of the network and width of the hidden layers). We provide a baseline loss (intercept only model) that can give hints about an appropriate learning rate:
</p>
<p><img src="../help/figures/learningrates.jpg" alt="Learning rates" />
</p>
<p>If the training loss of the model doesn't fall below the baseline loss, the learning rate is either too high or too low. If this happens, try higher and lower learning rates.
</p>
<p>A common strategy is to try (manually) a few different learning rates to see if the learning rate is on the right scale.
</p>
<p>See the troubleshooting vignette (<code>vignette("B-Training_neural_networks")</code>) for more help on training and debugging neural networks.
</p>


<h3>Finding the right architecture</h3>

<p>As with the learning rate, there is no definitive guide to choosing the right architecture for the right task. However, there are some general rules/recommendations: In general, wider, and deeper neural networks can improve generalization - but this is a double-edged sword because it also increases the risk of overfitting. So, if you increase the width and depth of the network, you should also add regularization (e.g., by increasing the lambda parameter, which corresponds to the regularization strength). Furthermore, in <a href="https://arxiv.org/abs/2306.10551">Pichler &amp; Hartig, 2023</a>, we investigated the effects of the hyperparameters on the prediction performance as a function of the data size. For example, we found that the <code>selu</code> activation function outperforms <code>relu</code> for small data sizes (&lt;100 observations).
</p>
<p>We recommend starting with moderate sizes (like the defaults), and if the model doesn't generalize/converge, try larger networks along with a regularization that helps minimize the risk of overfitting (see <code>vignette("B-Training_neural_networks")</code> ).
</p>


<h3>Overfitting</h3>

<p>Overfitting means that the model fits the training data well, but generalizes poorly to new observations. We can use the validation argument to detect overfitting. If the validation loss starts to increase again at a certain point, it often means that the models are starting to overfit your training data:
</p>
<p><img src="../help/figures/overfitting.jpg" alt="Overfitting" />
</p>
<p><strong>Solutions</strong>:
</p>

<ul>
<li><p> Re-train with epochs = point where model started to overfit
</p>
</li>
<li><p> Early stopping, stop training when model starts to overfit, can be specified using the <code style="white-space: pre;">&#8288;early_stopping=…&#8288;</code> argument
</p>
</li>
<li><p> Use regularization (dropout or elastic-net, see next section)
</p>
</li></ul>



<h3>Regularization</h3>

<p>Elastic Net regularization combines the strengths of L1 (Lasso) and L2 (Ridge) regularization. It introduces a penalty term that encourages sparse weight values while maintaining overall weight shrinkage. By controlling the sparsity of the learned model, Elastic Net regularization helps avoid overfitting while allowing for meaningful feature selection. We advise using elastic net (e.g. lambda = 0.001 and alpha = 0.2).
</p>
<p>Dropout regularization helps prevent overfitting by randomly disabling a portion of neurons during training. This technique encourages the network to learn more robust and generalized representations, as it prevents individual neurons from relying too heavily on specific input patterns. Dropout has been widely adopted as a simple yet effective regularization method in deep learning.
</p>
<p>By utilizing these regularization methods in your neural network training with the cito package, you can improve generalization performance and enhance the network's ability to handle unseen data. These techniques act as valuable tools in mitigating overfitting and promoting more robust and reliable model performance.
</p>


<h3>Uncertainty</h3>

<p>We can use bootstrapping to generate uncertainties for all outputs. Bootstrapping can be enabled by setting <code>bootstrap = ...</code> to the number of bootstrap samples to be used. Note, however, that the computational cost can be excessive.
</p>
<p>In some cases it may be worthwhile to parallelize bootstrapping, for example if you have a GPU and the neural network is small. Parallelization for bootstrapping can be enabled by setting the <code>bootstrap_parallel = ...</code> argument to the desired number of calls to run in parallel.
</p>


<h3>Custom Optimizer and Learning Rate Schedulers</h3>

<p>When training a network, you have the flexibility to customize the optimizer settings and learning rate scheduler to optimize the learning process. In the cito package, you can initialize these configurations using the <code><a href="#topic+config_lr_scheduler">config_lr_scheduler</a></code> and <code><a href="#topic+config_optimizer">config_optimizer</a></code> functions.
</p>
<p><code><a href="#topic+config_lr_scheduler">config_lr_scheduler</a></code> allows you to define a specific learning rate scheduler that controls how the learning rate changes over time during training. This is beneficial in scenarios where you want to adaptively adjust the learning rate to improve convergence or avoid getting stuck in local optima.
</p>
<p>Similarly, the <code><a href="#topic+config_optimizer">config_optimizer</a></code> function enables you to specify the optimizer for your network. Different optimizers, such as stochastic gradient descent (SGD), Adam, or RMSprop, offer various strategies for updating the network's weights and biases during training. Choosing the right optimizer can significantly impact the training process and the final performance of your neural network.
</p>


<h3>Hyperparameter tuning</h3>

<p>We have implemented experimental support for hyperparameter tuning. We can mark hyperparameters that should be tuned by cito by setting their values to <code>tune()</code>, for example <code style="white-space: pre;">&#8288;dnn (..., lr = tune()&#8288;</code>. <code><a href="#topic+tune">tune()</a></code> is a function that creates a range of random values for the given hyperparameter. You can change the maximum and minimum range of the potential hyperparameters or pass custom values to the <code>tune(values = c(....))</code> function. The following table lists the hyperparameters that can currently be tuned:</p>

<table>
<tr>
 <td style="text-align: left;">
   Hyperparameter </td><td style="text-align: left;"> Example </td><td style="text-align: left;"> Details </td>
</tr>
<tr>
 <td style="text-align: left;">
   hidden </td><td style="text-align: left;"> <code style="white-space: pre;">&#8288;dnn(…,hidden=tune(10, 20, fixed=’depth’))&#8288;</code> </td><td style="text-align: left;"> Depth and width can be both tuned or only one of them, if both of them should be tuned, vectors for lower and upper #' boundaries must be provided (first = number of nodes) </td>
</tr>
<tr>
 <td style="text-align: left;">
   bias </td><td style="text-align: left;"> <code style="white-space: pre;">&#8288;dnn(…, bias=tune())&#8288;</code> </td><td style="text-align: left;"> Should the bias be turned on or off for all hidden layers </td>
</tr>
<tr>
 <td style="text-align: left;">
   lambda </td><td style="text-align: left;"> <code style="white-space: pre;">&#8288;dnn(…, lambda = tune(0.0001, 0.1))&#8288;</code> </td><td style="text-align: left;"> lambda will be tuned within the range (0.0001, 0.1) </td>
</tr>
<tr>
 <td style="text-align: left;">
   alpha </td><td style="text-align: left;"> <code style="white-space: pre;">&#8288;dnn(…, lambda = tune(0.2, 0.4))&#8288;</code> </td><td style="text-align: left;"> alpha will be tuned within the range (0.2, 0.4) </td>
</tr>
<tr>
 <td style="text-align: left;">
   activation </td><td style="text-align: left;"> <code style="white-space: pre;">&#8288;dnn(…, activation = tune())&#8288;</code> </td><td style="text-align: left;"> activation functions of the hidden layers will be tuned </td>
</tr>
<tr>
 <td style="text-align: left;">
   dropout </td><td style="text-align: left;"> <code style="white-space: pre;">&#8288;dnn(…, dropout = tune())&#8288;</code> </td><td style="text-align: left;"> Dropout rate will be tuned (globally for all layers) </td>
</tr>
<tr>
 <td style="text-align: left;">
   lr </td><td style="text-align: left;"> <code style="white-space: pre;">&#8288;dnn(…, lr = tune())&#8288;</code> </td><td style="text-align: left;"> Learning rate will be tuned </td>
</tr>
<tr>
 <td style="text-align: left;">
   batchsize </td><td style="text-align: left;"> <code style="white-space: pre;">&#8288;dnn(…, batchsize = tune())&#8288;</code> </td><td style="text-align: left;"> batch size will be tuned </td>
</tr>
<tr>
 <td style="text-align: left;">
   epochs </td><td style="text-align: left;"> <code style="white-space: pre;">&#8288;dnn(…, batchsize = tune())&#8288;</code> </td><td style="text-align: left;"> batchsize will be tuned </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The hyperparameters are tuned by random search (i.e., random values for the hyperparameters within a specified range) and by cross-validation. The exact tuning regime can be specified with <a href="#topic+config_tuning">config_tuning</a>.
</p>
<p>Note that hyperparameter tuning can be expensive. We have implemented an option to parallelize hyperparameter tuning, including parallelization over one or more GPUs (the hyperparameter evaluation is parallelized, not the CV). This can be especially useful for small models. For example, if you have 4 GPUs, 20 CPU cores, and 20 steps (random samples from the random search), you could run <code style="white-space: pre;">&#8288;dnn(..., device="cuda",lr = tune(), batchsize=tune(), tuning=config_tuning(parallel=20, NGPU=4)&#8288;</code>, which will distribute 20 model fits across 4 GPUs, so that each GPU will process 5 models (in parallel).
</p>
<p>As this is an experimental feature, we welcome feature requests and bug reports on our github site.
</p>
<p>For the custom values, all hyperparameters except for the hidden layers require a vector of values. Hidden layers expect a two-column matrix where the first column is the number of hidden nodes and the second column corresponds to the number of hidden layers.
</p>


<h3>How neural networks work</h3>

<p>In Multilayer Perceptron (MLP) networks, each neuron is connected to every neuron in the previous layer and every neuron in the subsequent layer. The value of each neuron is computed using a weighted sum of the outputs from the previous layer, followed by the application of an activation function. Specifically, the value of a neuron is calculated as the weighted sum of the outputs of the neurons in the previous layer, combined with a bias term. This sum is then passed through an activation function, which introduces non-linearity into the network. The calculated value of each neuron becomes the input for the neurons in the next layer, and the process continues until the output layer is reached. The choice of activation function and the specific weight values determine the network's ability to learn and approximate complex relationships between inputs and outputs.
</p>
<p>Therefore the value of each neuron can be calculated using: <code class="reqn"> a (\sum_j{ w_j * a_j})</code>. Where <code class="reqn">w_j</code> is the weight and <code class="reqn">a_j</code> is the value from neuron j to the current one. a() is the activation function, e.g. <code class="reqn"> relu(x) = max(0,x)</code>
</p>


<h3>Training on graphic cards</h3>

<p>If you have an NVIDIA CUDA-enabled device and have installed the CUDA toolkit version 11.3 and cuDNN 8.4, you can take advantage of GPU acceleration for training your neural networks. It is crucial to have these specific versions installed, as other versions may not be compatible.
For detailed installation instructions and more information on utilizing GPUs for training, please refer to the <a href="https://torch.mlverse.org/docs/articles/installation.html">mlverse: 'torch' documentation</a>.
</p>
<p>Note: GPU training is optional, and the package can still be used for training on CPU even without CUDA and cuDNN installations.
</p>


<h3>Author(s)</h3>

<p>Christian Amesoeder, Maximilian Pichler
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.citodnn">predict.citodnn</a></code>, <code><a href="#topic+plot.citodnn">plot.citodnn</a></code>,  <code><a href="#topic+coef.citodnn">coef.citodnn</a></code>,<code><a href="#topic+print.citodnn">print.citodnn</a></code>, <code><a href="#topic+summary.citodnn">summary.citodnn</a></code>, <code><a href="#topic+continue_training">continue_training</a></code>, <code><a href="#topic+analyze_training">analyze_training</a></code>, <code><a href="#topic+PDP">PDP</a></code>, <code><a href="#topic+ALE">ALE</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

# Example workflow in cito

## Build and train  Network
### softmax is used for multi-class responses (e.g., Species)
nn.fit&lt;- dnn(Species~., data = datasets::iris, loss = "softmax")

## The training loss is below the baseline loss but at the end of the
## training the loss was still decreasing, so continue training for another 50
## epochs
nn.fit &lt;- continue_training(nn.fit, epochs = 50L)

# Sturcture of Neural Network
print(nn.fit)

# Plot Neural Network
plot(nn.fit)
## 4 Input nodes (first layer) because of 4 features
## 3 Output nodes (last layer) because of 3 response species (one node for each
## level in the response variable).
## The layers between the input and output layer are called hidden layers (two
## of them)

## We now want to understand how the predictions are made, what are the
## important features? The summary function automatically calculates feature
## importance (the interpretation is similar to an anova) and calculates
## average conditional effects that are similar to linear effects:
summary(nn.fit)

## To visualize the effect (response-feature effect), we can use the ALE and
## PDP functions

# Partial dependencies
PDP(nn.fit, variable = "Petal.Length")

# Accumulated local effect plots
ALE(nn.fit, variable = "Petal.Length")



# Per se, it is difficult to get confidence intervals for our xAI metrics (or
# for the predictions). But we can use bootstrapping to obtain uncertainties
# for all cito outputs:
## Re-fit the neural network with bootstrapping
nn.fit&lt;- dnn(Species~.,
             data = datasets::iris,
             loss = "softmax",
             epochs = 150L,
             verbose = FALSE,
             bootstrap = 20L)
## convergence can be tested via the analyze_training function
analyze_training(nn.fit)

## Summary for xAI metrics (can take some time):
summary(nn.fit)
## Now with standard errors and p-values
## Note: Take the p-values with a grain of salt! We do not know yet if they are
## correct (e.g. if you use regularization, they are likely conservative == too
## large)

## Predictions with bootstrapping:
dim(predict(nn.fit))
## predictions are by default averaged (over the bootstrap samples)



# Hyperparameter tuning (experimental feature)
hidden_values = matrix(c(5, 2,
                         4, 2,
                         10,2,
                         15,2), 4, 2, byrow = TRUE)
## Potential architectures we want to test, first column == number of nodes
print(hidden_values)

nn.fit = dnn(Species~.,
             data = iris,
             epochs = 30L,
             loss = "softmax",
             hidden = tune(values = hidden_values),
             lr = tune(0.00001, 0.1) # tune lr between range 0.00001 and 0.1
             )
## Tuning results:
print(nn.fit$tuning)

# test = Inf means that tuning was cancelled after only one fit (within the CV)


# Advanced: Custom loss functions and additional parameters
## Normal Likelihood with sd parameter:
custom_loss = function(pred, true) {
  logLik = torch::distr_normal(pred,
                               scale = torch::nnf_relu(scale)+
                                 0.001)$log_prob(true)
  return(-logLik$mean())
}

nn.fit&lt;- dnn(Sepal.Length~.,
             data = datasets::iris,
             loss = custom_loss,
             verbose = FALSE,
             custom_parameters = list(scale = 1.0)
)
nn.fit$parameter$scale

## Multivariate normal likelihood with parametrized covariance matrix
## Sigma = L*L^t + D
## Helper function to build covariance matrix
create_cov = function(LU, Diag) {
  return(torch::torch_matmul(LU, LU$t()) + torch::torch_diag(Diag$exp()+0.01))
}

custom_loss_MVN = function(true, pred) {
  Sigma = create_cov(SigmaPar, SigmaDiag)
  logLik = torch::distr_multivariate_normal(pred,
                                            covariance_matrix = Sigma)$
    log_prob(true)
  return(-logLik$mean())
}


nn.fit&lt;- dnn(cbind(Sepal.Length, Sepal.Width, Petal.Length)~.,
             data = datasets::iris,
             lr = 0.01,
             verbose = FALSE,
             loss = custom_loss_MVN,
             custom_parameters =
               list(SigmaDiag =  rep(0, 3),
                    SigmaPar = matrix(rnorm(6, sd = 0.001), 3, 2))
)
as.matrix(create_cov(nn.fit$loss$parameter$SigmaPar,
                     nn.fit$loss$parameter$SigmaDiag))

}

</code></pre>

<hr>
<h2 id='e'>Embeddings</h2><span id='topic+e'></span>

<h3>Description</h3>

<p>Can be used for categorical variables, a more efficient alternative to one-hot encoding
</p>


<h3>Usage</h3>

<pre><code class='language-R'>e(dim = 1L, weights = NULL, train = TRUE, lambda = 0, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="e_+3A_dim">dim</code></td>
<td>
<p>integer, embedding dimension</p>
</td></tr>
<tr><td><code id="e_+3A_weights">weights</code></td>
<td>
<p>matrix, to use custom embedding matrices</p>
</td></tr>
<tr><td><code id="e_+3A_train">train</code></td>
<td>
<p>logical, should the embeddings be trained or not</p>
</td></tr>
<tr><td><code id="e_+3A_lambda">lambda</code></td>
<td>
<p>regularization strength on the embeddings</p>
</td></tr>
<tr><td><code id="e_+3A_alpha">alpha</code></td>
<td>
<p>mix between L1 and L2 regularization</p>
</td></tr>
</table>

<hr>
<h2 id='findReTrmClasses'>list of specials &ndash; taken from enum.R</h2><span id='topic+findReTrmClasses'></span>

<h3>Description</h3>

<p>list of specials &ndash; taken from enum.R
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findReTrmClasses()
</code></pre>

<hr>
<h2 id='linear'>Linear layer</h2><span id='topic+linear'></span>

<h3>Description</h3>

<p>creates a 'linear' 'citolayer' object that is used by <code><a href="#topic+create_architecture">create_architecture</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linear(
  n_neurons = NULL,
  bias = NULL,
  activation = NULL,
  normalization = NULL,
  dropout = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linear_+3A_n_neurons">n_neurons</code></td>
<td>
<p>(int) amount of hidden neurons in this layer</p>
</td></tr>
<tr><td><code id="linear_+3A_bias">bias</code></td>
<td>
<p>(boolean) if TRUE, adds a learnable bias to the neurons of this layer</p>
</td></tr>
<tr><td><code id="linear_+3A_activation">activation</code></td>
<td>
<p>(string) activation function that is used after this layer. The following activation functions are supported: &quot;relu&quot;, &quot;leaky_relu&quot;, &quot;tanh&quot;, &quot;elu&quot;, &quot;rrelu&quot;, &quot;prelu&quot;, &quot;softplus&quot;, &quot;celu&quot;, &quot;selu&quot;, &quot;gelu&quot;, &quot;relu6&quot;, &quot;sigmoid&quot;, &quot;softsign&quot;, &quot;hardtanh&quot;, &quot;tanhshrink&quot;, &quot;softshrink&quot;, &quot;hardshrink&quot;, &quot;log_sigmoid&quot;</p>
</td></tr>
<tr><td><code id="linear_+3A_normalization">normalization</code></td>
<td>
<p>(boolean) if TRUE, batch normalization is used after this layer</p>
</td></tr>
<tr><td><code id="linear_+3A_dropout">dropout</code></td>
<td>
<p>(float) dropout rate of this layer. Set to 0 for no dropout</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a 'linear' 'citolayer' object that is passed to the <code><a href="#topic+create_architecture">create_architecture</a></code> function.
The parameters that aren't assigned here (and are therefore still NULL) are filled with the default values passed to <code><a href="#topic+create_architecture">create_architecture</a></code>.
</p>


<h3>Value</h3>

<p>S3 object of class <code>"linear" "citolayer"</code>
</p>


<h3>Author(s)</h3>

<p>Armin Schenk
</p>


<h3>See Also</h3>

<p><code><a href="#topic+create_architecture">create_architecture</a></code>
</p>

<hr>
<h2 id='maxPool'>Maximum pooling layer</h2><span id='topic+maxPool'></span>

<h3>Description</h3>

<p>creates a 'maxPool' 'citolayer' object that is used by <code><a href="#topic+create_architecture">create_architecture</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxPool(kernel_size = NULL, stride = NULL, padding = NULL, dilation = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxPool_+3A_kernel_size">kernel_size</code></td>
<td>
<p>(int or tuple) size of the kernel in this layer. Use a tuple if the kernel size isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="maxPool_+3A_stride">stride</code></td>
<td>
<p>(int or tuple) stride of the kernel in this layer. NULL sets the stride equal to the kernel size. Use a tuple if the stride isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="maxPool_+3A_padding">padding</code></td>
<td>
<p>(int or tuple) zero-padding added to both sides of the input. Use a tuple if the padding isn't equal in all dimensions</p>
</td></tr>
<tr><td><code id="maxPool_+3A_dilation">dilation</code></td>
<td>
<p>(int or tuple) dilation of the kernel in this layer. Use a tuple if the dilation isn't equal in all dimensions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a 'maxPool' 'citolayer' object that is passed to the <code><a href="#topic+create_architecture">create_architecture</a></code> function.
The parameters that aren't assigned here (and are therefore still NULL) are filled with the default values passed to <code><a href="#topic+create_architecture">create_architecture</a></code>.
</p>


<h3>Value</h3>

<p>S3 object of class <code>"maxPool" "citolayer"</code>
</p>


<h3>Author(s)</h3>

<p>Armin Schenk
</p>


<h3>See Also</h3>

<p><code><a href="#topic+create_architecture">create_architecture</a></code>
</p>

<hr>
<h2 id='PDP'>Partial Dependence Plot (PDP)</h2><span id='topic+PDP'></span><span id='topic+PDP.citodnn'></span><span id='topic+PDP.citodnnBootstrap'></span>

<h3>Description</h3>

<p>Calculates the Partial Dependency Plot for one feature, either numeric or categorical. Returns it as a plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PDP(
  model,
  variable = NULL,
  data = NULL,
  ice = FALSE,
  resolution.ice = 20,
  plot = TRUE,
  parallel = FALSE,
  ...
)

## S3 method for class 'citodnn'
PDP(
  model,
  variable = NULL,
  data = NULL,
  ice = FALSE,
  resolution.ice = 20,
  plot = TRUE,
  parallel = FALSE,
  ...
)

## S3 method for class 'citodnnBootstrap'
PDP(
  model,
  variable = NULL,
  data = NULL,
  ice = FALSE,
  resolution.ice = 20,
  plot = TRUE,
  parallel = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PDP_+3A_model">model</code></td>
<td>
<p>a model created by <code><a href="#topic+dnn">dnn</a></code></p>
</td></tr>
<tr><td><code id="PDP_+3A_variable">variable</code></td>
<td>
<p>variable as string for which the PDP should be done. If none is supplied it is done for all variables.</p>
</td></tr>
<tr><td><code id="PDP_+3A_data">data</code></td>
<td>
<p>specify new data PDP should be performed . If NULL, PDP is performed on the training data.</p>
</td></tr>
<tr><td><code id="PDP_+3A_ice">ice</code></td>
<td>
<p>Individual Conditional Dependence will be shown if TRUE</p>
</td></tr>
<tr><td><code id="PDP_+3A_resolution.ice">resolution.ice</code></td>
<td>
<p>resolution in which ice will be computed</p>
</td></tr>
<tr><td><code id="PDP_+3A_plot">plot</code></td>
<td>
<p>plot PDP or not</p>
</td></tr>
<tr><td><code id="PDP_+3A_parallel">parallel</code></td>
<td>
<p>parallelize over bootstrap models or not</p>
</td></tr>
<tr><td><code id="PDP_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="stats.html#topic+predict">predict</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of plots made with 'ggplot2' consisting of an individual plot for each defined variable.
</p>


<h3>Description</h3>

<p>Performs a Partial Dependency Plot (PDP) estimation to analyze the relationship between a selected feature and the target variable.
</p>
<p>The PDP function estimates the partial function <code class="reqn">\hat{f}_S</code>:
</p>
<p><code class="reqn">\hat{f}_S(x_S)=\frac{1}{n}\sum_{i=1}^n\hat{f}(x_S,x^{(i)}_{C})</code>
</p>
<p>with a Monte Carlo Estimation:
</p>
<p><code class="reqn">\hat{f}_S(x_S)=\frac{1}{n}\sum_{i=1}^n\hat{f}(x_S,x^{(i)}_{C})</code>
using a Monte Carlo estimation method. It calculates the average prediction of the target variable for different values of the selected feature while keeping other features constant.
</p>
<p>For categorical features, all data instances are used, and each instance is set to one level of the categorical feature. The average prediction per category is then calculated and visualized in a bar plot.
</p>
<p>If the <code>ice</code> parameter is set to <code>TRUE</code>, the Individual Conditional Expectation (ICE) curves are also shown. These curves illustrate how each individual data sample reacts to changes in the feature value. Please note that this option is not available for categorical features. Unlike PDP, the ICE curves are computed using a value grid instead of utilizing every value of every data entry.
</p>
<p>Note: The PDP analysis provides valuable insights into the relationship between a specific feature and the target variable, helping to understand the feature's impact on the model's predictions.
If a categorical feature is analyzed, all data instances are used and set to each level.
Then an average is calculated per category and put out in a bar plot.
</p>
<p>If ice is set to true additional the individual conditional dependence will be shown and the original PDP will be colored yellow.
These lines show, how each individual data sample reacts to changes in the feature. This option is not available for categorical features.
Unlike PDP the ICE curves are computed with a value grid instead of utilizing every value of every data entry.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ALE">ALE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

# Build and train  Network
nn.fit&lt;- dnn(Sepal.Length~., data = datasets::iris)

PDP(nn.fit, variable = "Petal.Length")
}

</code></pre>

<hr>
<h2 id='plot.citoarchitecture'>Plot the CNN architecture</h2><span id='topic+plot.citoarchitecture'></span>

<h3>Description</h3>

<p>Plot the CNN architecture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citoarchitecture'
plot(x, input_shape, output_shape, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.citoarchitecture_+3A_x">x</code></td>
<td>
<p>an object of class citoarchitecture created by <code><a href="#topic+create_architecture">create_architecture</a></code></p>
</td></tr>
<tr><td><code id="plot.citoarchitecture_+3A_input_shape">input_shape</code></td>
<td>
<p>a vector with the dimensions of a single sample (e.g. c(3,28,28))</p>
</td></tr>
<tr><td><code id="plot.citoarchitecture_+3A_output_shape">output_shape</code></td>
<td>
<p>the number of nodes in the output layer</p>
</td></tr>
<tr><td><code id="plot.citoarchitecture_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nothing
</p>

<hr>
<h2 id='plot.citocnn'>Plot the CNN architecture</h2><span id='topic+plot.citocnn'></span>

<h3>Description</h3>

<p>Plot the CNN architecture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citocnn'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.citocnn_+3A_x">x</code></td>
<td>
<p>a model created by <code><a href="#topic+cnn">cnn</a></code></p>
</td></tr>
<tr><td><code id="plot.citocnn_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>original object x
</p>

<hr>
<h2 id='plot.citodnn'>Creates graph plot which gives an overview of the network architecture.</h2><span id='topic+plot.citodnn'></span><span id='topic+plot.citodnnBootstrap'></span>

<h3>Description</h3>

<p>Creates graph plot which gives an overview of the network architecture.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citodnn'
plot(x, node_size = 1, scale_edges = FALSE, ...)

## S3 method for class 'citodnnBootstrap'
plot(x, node_size = 1, scale_edges = FALSE, which_model = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.citodnn_+3A_x">x</code></td>
<td>
<p>a model created by <code><a href="#topic+dnn">dnn</a></code></p>
</td></tr>
<tr><td><code id="plot.citodnn_+3A_node_size">node_size</code></td>
<td>
<p>size of node in plot</p>
</td></tr>
<tr><td><code id="plot.citodnn_+3A_scale_edges">scale_edges</code></td>
<td>
<p>edge weight gets scaled according to other weights (layer specific)</p>
</td></tr>
<tr><td><code id="plot.citodnn_+3A_...">...</code></td>
<td>
<p>no further functionality implemented yet</p>
</td></tr>
<tr><td><code id="plot.citodnn_+3A_which_model">which_model</code></td>
<td>
<p>which model from the ensemble should be plotted</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot made with 'ggraph' + 'igraph' that represents the neural network
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

set.seed(222)
validation_set&lt;- sample(c(1:nrow(datasets::iris)),25)

# Build and train  Network
nn.fit&lt;- dnn(Sepal.Length~., data = datasets::iris[-validation_set,])

plot(nn.fit)
}

</code></pre>

<hr>
<h2 id='predict.citocnn'>Predict from a fitted cnn model</h2><span id='topic+predict.citocnn'></span>

<h3>Description</h3>

<p>Predict from a fitted cnn model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citocnn'
predict(
  object,
  newdata = NULL,
  type = c("link", "response", "class"),
  device = c("cpu", "cuda", "mps"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.citocnn_+3A_object">object</code></td>
<td>
<p>a model created by <code><a href="#topic+cnn">cnn</a></code></p>
</td></tr>
<tr><td><code id="predict.citocnn_+3A_newdata">newdata</code></td>
<td>
<p>new data for predictions</p>
</td></tr>
<tr><td><code id="predict.citocnn_+3A_type">type</code></td>
<td>
<p>which value should be calculated, either raw response, output of link function or predicted class (in case of classification)</p>
</td></tr>
<tr><td><code id="predict.citocnn_+3A_device">device</code></td>
<td>
<p>device on which network should be trained on.</p>
</td></tr>
<tr><td><code id="predict.citocnn_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>prediction matrix
</p>

<hr>
<h2 id='predict.citodnn'>Predict from a fitted dnn model</h2><span id='topic+predict.citodnn'></span><span id='topic+predict.citodnnBootstrap'></span>

<h3>Description</h3>

<p>Predict from a fitted dnn model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citodnn'
predict(
  object,
  newdata = NULL,
  type = c("link", "response", "class"),
  device = c("cpu", "cuda", "mps"),
  reduce = c("mean", "median", "none"),
  ...
)

## S3 method for class 'citodnnBootstrap'
predict(
  object,
  newdata = NULL,
  type = c("link", "response", "class"),
  device = c("cpu", "cuda", "mps"),
  reduce = c("mean", "median", "none"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.citodnn_+3A_object">object</code></td>
<td>
<p>a model created by <code><a href="#topic+dnn">dnn</a></code></p>
</td></tr>
<tr><td><code id="predict.citodnn_+3A_newdata">newdata</code></td>
<td>
<p>new data for predictions</p>
</td></tr>
<tr><td><code id="predict.citodnn_+3A_type">type</code></td>
<td>
<p>type of predictions. The default is on the scale of the linear predictor, &quot;response&quot; is on the scale of the response, and &quot;class&quot; means that class predictions are returned (if it is a classification task)</p>
</td></tr>
<tr><td><code id="predict.citodnn_+3A_device">device</code></td>
<td>
<p>device on which network should be trained on.</p>
</td></tr>
<tr><td><code id="predict.citodnn_+3A_reduce">reduce</code></td>
<td>
<p>predictions from bootstrapped model are by default reduced (mean, optional median or none)</p>
</td></tr>
<tr><td><code id="predict.citodnn_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>prediction matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

set.seed(222)
validation_set&lt;- sample(c(1:nrow(datasets::iris)),25)

# Build and train  Network
nn.fit&lt;- dnn(Sepal.Length~., data = datasets::iris[-validation_set,])

# Use model on validation set
predictions &lt;- predict(nn.fit, iris[validation_set,])
# Scatterplot
plot(iris[validation_set,]$Sepal.Length,predictions)
# MAE
mean(abs(predictions-iris[validation_set,]$Sepal.Length))
}

</code></pre>

<hr>
<h2 id='print.avgPool'>Print pooling layer</h2><span id='topic+print.avgPool'></span>

<h3>Description</h3>

<p>Print pooling layer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'avgPool'
print(x, input_shape, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.avgPool_+3A_x">x</code></td>
<td>
<p>an object of class avgPool</p>
</td></tr>
<tr><td><code id="print.avgPool_+3A_input_shape">input_shape</code></td>
<td>
<p>input shape</p>
</td></tr>
<tr><td><code id="print.avgPool_+3A_...">...</code></td>
<td>
<p>further arguments, not supported yet</p>
</td></tr>
</table>

<hr>
<h2 id='print.citoarchitecture'>Print class citoarchitecture</h2><span id='topic+print.citoarchitecture'></span>

<h3>Description</h3>

<p>Print class citoarchitecture
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citoarchitecture'
print(x, input_shape, output_shape, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.citoarchitecture_+3A_x">x</code></td>
<td>
<p>an object created by <code><a href="#topic+create_architecture">create_architecture</a></code></p>
</td></tr>
<tr><td><code id="print.citoarchitecture_+3A_input_shape">input_shape</code></td>
<td>
<p>a vector with the dimensions of a single sample (e.g. c(3,28,28))</p>
</td></tr>
<tr><td><code id="print.citoarchitecture_+3A_output_shape">output_shape</code></td>
<td>
<p>the number of nodes in the output layer</p>
</td></tr>
<tr><td><code id="print.citoarchitecture_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>original object
</p>

<hr>
<h2 id='print.citocnn'>Print class citocnn</h2><span id='topic+print.citocnn'></span>

<h3>Description</h3>

<p>Print class citocnn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citocnn'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.citocnn_+3A_x">x</code></td>
<td>
<p>a model created by <code><a href="#topic+cnn">cnn</a></code></p>
</td></tr>
<tr><td><code id="print.citocnn_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>original object x
</p>

<hr>
<h2 id='print.citodnn'>Print class citodnn</h2><span id='topic+print.citodnn'></span><span id='topic+print.citodnnBootstrap'></span>

<h3>Description</h3>

<p>Print class citodnn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citodnn'
print(x, ...)

## S3 method for class 'citodnnBootstrap'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.citodnn_+3A_x">x</code></td>
<td>
<p>a model created by <code><a href="#topic+dnn">dnn</a></code></p>
</td></tr>
<tr><td><code id="print.citodnn_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>original object x gets returned
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(torch::torch_is_installed()){
library(cito)

set.seed(222)
validation_set&lt;- sample(c(1:nrow(datasets::iris)),25)

# Build and train  Network
nn.fit&lt;- dnn(Sepal.Length~., data = datasets::iris[-validation_set,])

# Structure of Neural Network
print(nn.fit)
}

</code></pre>

<hr>
<h2 id='print.conditionalEffects'>Print average conditional effects</h2><span id='topic+print.conditionalEffects'></span><span id='topic+print.conditionalEffectsBootstrap'></span>

<h3>Description</h3>

<p>Print average conditional effects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'conditionalEffects'
print(x, ...)

## S3 method for class 'conditionalEffectsBootstrap'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.conditionalEffects_+3A_x">x</code></td>
<td>
<p>print ACE calculated by <code><a href="#topic+conditionalEffects">conditionalEffects</a></code></p>
</td></tr>
<tr><td><code id="print.conditionalEffects_+3A_...">...</code></td>
<td>
<p>optional arguments for compatibility with the generic function, no function implemented</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix with average conditional effects
</p>

<hr>
<h2 id='print.conv'>Print conv layer</h2><span id='topic+print.conv'></span>

<h3>Description</h3>

<p>Print conv layer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'conv'
print(x, input_shape, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.conv_+3A_x">x</code></td>
<td>
<p>an object of class conv</p>
</td></tr>
<tr><td><code id="print.conv_+3A_input_shape">input_shape</code></td>
<td>
<p>input shape</p>
</td></tr>
<tr><td><code id="print.conv_+3A_...">...</code></td>
<td>
<p>further arguments, not supported yet</p>
</td></tr>
</table>

<hr>
<h2 id='print.linear'>Print linear layer</h2><span id='topic+print.linear'></span>

<h3>Description</h3>

<p>Print linear layer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'linear'
print(x, input_shape, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.linear_+3A_x">x</code></td>
<td>
<p>an object of class linear</p>
</td></tr>
<tr><td><code id="print.linear_+3A_input_shape">input_shape</code></td>
<td>
<p>input shape</p>
</td></tr>
<tr><td><code id="print.linear_+3A_...">...</code></td>
<td>
<p>further arguments, not supported yet</p>
</td></tr>
</table>

<hr>
<h2 id='print.maxPool'>Print pooling layer</h2><span id='topic+print.maxPool'></span>

<h3>Description</h3>

<p>Print pooling layer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxPool'
print(x, input_shape, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.maxPool_+3A_x">x</code></td>
<td>
<p>an object of class maxPool</p>
</td></tr>
<tr><td><code id="print.maxPool_+3A_input_shape">input_shape</code></td>
<td>
<p>input shape</p>
</td></tr>
<tr><td><code id="print.maxPool_+3A_...">...</code></td>
<td>
<p>further arguments, not supported yet</p>
</td></tr>
</table>

<hr>
<h2 id='print.summary.citodnn'>Print method for class summary.citodnn</h2><span id='topic+print.summary.citodnn'></span><span id='topic+print.summary.citodnnBootstrap'></span>

<h3>Description</h3>

<p>Print method for class summary.citodnn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.citodnn'
print(x, ...)

## S3 method for class 'summary.citodnnBootstrap'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.citodnn_+3A_x">x</code></td>
<td>
<p>a summary object created by <code><a href="#topic+summary.citodnn">summary.citodnn</a></code></p>
</td></tr>
<tr><td><code id="print.summary.citodnn_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with Matrices for importance, average CE, absolute sum of CE, and standard deviation of the CE
</p>

<hr>
<h2 id='print.transfer'>Print transfer model</h2><span id='topic+print.transfer'></span>

<h3>Description</h3>

<p>Print transfer model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'transfer'
print(x, input_shape, output_shape, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.transfer_+3A_x">x</code></td>
<td>
<p>an object of class transfer</p>
</td></tr>
<tr><td><code id="print.transfer_+3A_input_shape">input_shape</code></td>
<td>
<p>input shape</p>
</td></tr>
<tr><td><code id="print.transfer_+3A_output_shape">output_shape</code></td>
<td>
<p>output shape</p>
</td></tr>
<tr><td><code id="print.transfer_+3A_...">...</code></td>
<td>
<p>further arguments, not supported yet</p>
</td></tr>
</table>

<hr>
<h2 id='residuals.citodnn'>Extract Model Residuals</h2><span id='topic+residuals.citodnn'></span>

<h3>Description</h3>

<p>Returns residuals of training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citodnn'
residuals(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.citodnn_+3A_object">object</code></td>
<td>
<p>a model created by <code><a href="#topic+dnn">dnn</a></code></p>
</td></tr>
<tr><td><code id="residuals.citodnn_+3A_...">...</code></td>
<td>
<p>no additional arguments implemented</p>
</td></tr>
</table>


<h3>Value</h3>

<p>residuals of training set
</p>

<hr>
<h2 id='simulate_shapes'>Data Simulation for CNN</h2><span id='topic+simulate_shapes'></span>

<h3>Description</h3>

<p>generates images of rectangles and ellipsoids
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_shapes(n, size, channels = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_shapes_+3A_n">n</code></td>
<td>
<p>number of images</p>
</td></tr>
<tr><td><code id="simulate_shapes_+3A_size">size</code></td>
<td>
<p>size of the (quadratic) images</p>
</td></tr>
<tr><td><code id="simulate_shapes_+3A_channels">channels</code></td>
<td>
<p>number of channels the generated data has (in each channel a new rectangle/ellipsoid is created)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates simple data to demonstrate the usage of cnn().
The generated images are of centered rectangles and ellipsoids with random widths and heights.
</p>


<h3>Value</h3>

<p>array of dimension (n, 1, size, size)
</p>


<h3>Author(s)</h3>

<p>Armin Schenk
</p>

<hr>
<h2 id='summary.citocnn'>Summary citocnn</h2><span id='topic+summary.citocnn'></span>

<h3>Description</h3>

<p>currently the same as the print.citocnn method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citocnn'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.citocnn_+3A_object">object</code></td>
<td>
<p>a model created by <code><a href="#topic+cnn">cnn</a></code></p>
</td></tr>
<tr><td><code id="summary.citocnn_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>original object x
</p>

<hr>
<h2 id='summary.citodnn'>Summarize Neural Network of class citodnn</h2><span id='topic+summary.citodnn'></span><span id='topic+summary.citodnnBootstrap'></span>

<h3>Description</h3>

<p>Performs a Feature Importance calculation based on Permutations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'citodnn'
summary(object, n_permute = NULL, device = NULL, ...)

## S3 method for class 'citodnnBootstrap'
summary(object, n_permute = NULL, device = NULL, adjust_se = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.citodnn_+3A_object">object</code></td>
<td>
<p>a model of class citodnn created by <code><a href="#topic+dnn">dnn</a></code></p>
</td></tr>
<tr><td><code id="summary.citodnn_+3A_n_permute">n_permute</code></td>
<td>
<p>number of permutations performed. Default is <code class="reqn">3 * \sqrt{n}</code>, where n euqals then number of samples in the training set</p>
</td></tr>
<tr><td><code id="summary.citodnn_+3A_device">device</code></td>
<td>
<p>for calculating variable importance and conditional effects</p>
</td></tr>
<tr><td><code id="summary.citodnn_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
<tr><td><code id="summary.citodnn_+3A_adjust_se">adjust_se</code></td>
<td>
<p>adjust standard errors for importance (standard errors are multiplied with 1/sqrt(3) )</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Performs the feature importance calculation as suggested by  Fisher, Rudin, and Dominici (2018), and the mean and standard deviation of the average conditional Effects as suggested by Pichler &amp; Hartig (2023).
</p>
<p>Feature importances are in their interpretation similar to a ANOVA. Main and interaction effects are absorbed into the features. Also, feature importances are prone to collinearity between features, i.e. if two features are collinear, the importances might be overestimated.
</p>
<p>Average conditional effects (ACE) are similar to marginal effects and approximate linear effects, i.e. their interpretation is similar to effects in a linear regression model.
</p>
<p>The standard deviation of the ACE informs about the non-linearity of the feature effects. Higher values correlate with stronger non-linearities.
</p>
<p>For each feature n permutation get done and original and permuted predictive mean squared error (<code class="reqn">e_{perm}</code> &amp; <code class="reqn">e_{orig}</code>) get evaluated with <code class="reqn"> FI_j= e_{perm}/e_{orig}</code>. Based on Mean Squared Error.
</p>


<h3>Value</h3>

<p>summary.citodnn returns an object of class &quot;summary.citodnn&quot;, a list with components
</p>

<hr>
<h2 id='sumTerms'>combine a list of formula terms as a sum</h2><span id='topic+sumTerms'></span>

<h3>Description</h3>

<p>combine a list of formula terms as a sum
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sumTerms(termList)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sumTerms_+3A_termlist">termList</code></td>
<td>
<p>a list of formula terms</p>
</td></tr>
</table>

<hr>
<h2 id='transfer'>Transfer learning</h2><span id='topic+transfer'></span>

<h3>Description</h3>

<p>creates a 'transfer' 'citolayer' object that is used by <code><a href="#topic+create_architecture">create_architecture</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transfer(
  name = c("alexnet", "inception_v3", "mobilenet_v2", "resnet101", "resnet152",
    "resnet18", "resnet34", "resnet50", "resnext101_32x8d", "resnext50_32x4d", "vgg11",
    "vgg11_bn", "vgg13", "vgg13_bn", "vgg16", "vgg16_bn", "vgg19", "vgg19_bn",
    "wide_resnet101_2", "wide_resnet50_2"),
  pretrained = TRUE,
  freeze = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transfer_+3A_name">name</code></td>
<td>
<p>The name of the pretrained model</p>
</td></tr>
<tr><td><code id="transfer_+3A_pretrained">pretrained</code></td>
<td>
<p>if FALSE, random weights are used instead of the pretrained weights</p>
</td></tr>
<tr><td><code id="transfer_+3A_freeze">freeze</code></td>
<td>
<p>if TRUE, the weights of the pretrained model (except the &quot;classifier&quot; part at the end) aren't changed in the training anymore. Only works if pretrained=TRUE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a 'transfer' 'citolayer' object that is passed to the <code><a href="#topic+create_architecture">create_architecture</a></code> function.
With this object the pretrained models that are available in the 'torchvision' package can be used in cito.
When 'freeze' is set to TRUE, only the weights of the last part of the network (consisting of one or more linear layers) are adjusted in the training.
There mustn't be any other citolayers before the transfer citolayer object when calling <code><a href="#topic+create_architecture">create_architecture</a></code>.
If there are any citolayers after the transfer citolayer, the linear classifier part of the pretrained model is replaced with the specified citolayers.
</p>


<h3>Value</h3>

<p>S3 object of class <code>"transfer" "citolayer"</code>
</p>


<h3>Author(s)</h3>

<p>Armin Schenk
</p>


<h3>See Also</h3>

<p><code><a href="#topic+create_architecture">create_architecture</a></code>
</p>

<hr>
<h2 id='tune'>Tune hyperparameter</h2><span id='topic+tune'></span>

<h3>Description</h3>

<p>Control hyperparameter tuning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune(
  lower = NULL,
  upper = NULL,
  fixed = NULL,
  additional = NULL,
  values = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_+3A_lower">lower</code></td>
<td>
<p>numeric, numeric vector, character, lower boundaries of tuning space</p>
</td></tr>
<tr><td><code id="tune_+3A_upper">upper</code></td>
<td>
<p>numeric, numeric vector, character, upper boundaries of tuning space</p>
</td></tr>
<tr><td><code id="tune_+3A_fixed">fixed</code></td>
<td>
<p>character, used for multi-dimensional hyperparameters such as hidden, which dimensions should be fixed</p>
</td></tr>
<tr><td><code id="tune_+3A_additional">additional</code></td>
<td>
<p>numeric, additional control parameter which sets the value of the fixed argument</p>
</td></tr>
<tr><td><code id="tune_+3A_values">values</code></td>
<td>
<p>custom values from which hyperparameters are sampled, must be a matrix for hidden layers (first column == nodes, second column == number of layers)</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
