<!DOCTYPE html><html><head><title>Help for package nsprcomp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nsprcomp}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#asdev'><p>Additional Explained Standard Deviation</p></a></li>
<li><a href='#cardinality'><p>Cardinality of Column Vectors</p></a></li>
<li><a href='#nscumcomp'><p>Non-Negative and Sparse Cumulative PCA</p></a></li>
<li><a href='#nsprcomp'><p>Non-Negative and Sparse PCA</p></a></li>
<li><a href='#peav'><p>Percentage Explained Additional Variance</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.5.1-2</td>
</tr>
<tr>
<td>Title:</td>
<td>Non-Negative and Sparse PCA</td>
</tr>
<tr>
<td>Description:</td>
<td>Two methods for performing a constrained principal
        component analysis (PCA), where non-negativity and/or sparsity
        constraints are enforced on the principal axes (PAs). The
        function 'nsprcomp' computes one principal component (PC) after
        the other. Each PA is optimized such that the corresponding PC
        has maximum additional variance not explained by the previous
        components. In contrast, the function 'nscumcomp' jointly
        computes all PCs such that the cumulative variance is maximal.
        Both functions have the same interface as the 'prcomp' function
        from the 'stats' package (plus some extra parameters), and both
        return the result of the analysis as an object of class
        'nsprcomp', which inherits from 'prcomp'. See
        <a href="https://sigg-iten.ch/learningbits/2013/05/27/nsprcomp-is-on-cran/">https://sigg-iten.ch/learningbits/2013/05/27/nsprcomp-is-on-cran/</a>
        and Sigg et al. (2008) &lt;<a href="https://doi.org/10.1145%2F1390156.1390277">doi:10.1145/1390156.1390277</a>&gt; for more
        details.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://sigg-iten.ch/research/">https://sigg-iten.ch/research/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/chrsigg/nsprcomp/issues">https://github.com/chrsigg/nsprcomp/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, testthat (&ge; 0.8), roxygen2</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-06-05 07:40:43 UTC; chrsigg</td>
</tr>
<tr>
<td>Author:</td>
<td>Christian Sigg <a href="https://orcid.org/0000-0003-1067-9224"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  R Core team [ctb] (prcomp interface, formula implementation and
    documentation)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Christian Sigg &lt;christian@sigg-iten.ch&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-06-05 11:48:17 UTC</td>
</tr>
</table>
<hr>
<h2 id='asdev'>Additional Explained Standard Deviation</h2><span id='topic+asdev'></span>

<h3>Description</h3>

<p><code>asdev</code> computes the <em>additional</em> standard deviation explained by each 
principal component, taking into account the possible non-orthogonality of
the pseudo-rotation matrix <code class="reqn">\mathbf{W}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>asdev(x, w, center = TRUE, scale. = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asdev_+3A_x">x</code></td>
<td>
<p>a numeric data matrix with the observations as rows</p>
</td></tr>
<tr><td><code id="asdev_+3A_w">w</code></td>
<td>
<p>a numeric data matrix with the principal axes as columns</p>
</td></tr>
<tr><td><code id="asdev_+3A_center">center</code></td>
<td>
<p>a logical value indicating whether the empirical mean of 
<code>x</code> should be subtracted. Alternatively, a vector of length equal to 
the number of columns of <code>x</code> can be supplied. The value is passed to 
<code><a href="base.html#topic+scale">scale</a></code>.</p>
</td></tr>
<tr><td><code id="asdev_+3A_scale.">scale.</code></td>
<td>
<p>a logical value indicating whether the columns of <code>x</code> 
should be scaled to have unit variance before the analysis takes place. The
default is <code>FALSE</code> for consistency with <code>prcomp</code>. Alternatively, 
a vector of length equal to the number of columns of <code>x</code> can be 
supplied.  The value is passed to <code><a href="base.html#topic+scale">scale</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The additional standard deviation of a component is measured after projecting
the corresponding principal axis to the ortho-complement space spanned by the
previous principal axes. This procedure ensures that the variance explained 
by non-orthogonal principal axes is not counted multiple times. If the 
principal axes are pairwise orthogonal (e.g. computed using standard PCA), 
the additional standard deviations are identical to the standard deviations 
of the columns of the scores matrix <code class="reqn">\mathbf{XW}</code>.
</p>
<p><code>asdev</code> is also useful to build a partial PCA model from
<code class="reqn">\mathbf{W}</code>, to be completed with additional components computed
using <code><a href="#topic+nsprcomp">nsprcomp</a></code>.
</p>


<h3>Value</h3>

<p><code>asdev</code> returns a list with class <code>(nsprcomp, prcomp)</code> 
containing the following elements: </p>
<table>
<tr><td><code>sdev</code></td>
<td>
<p>the additional standard 
deviation explained by each component</p>
</td></tr> <tr><td><code>rotation</code></td>
<td>
<p>copied from the 
input argument <code>w</code></p>
</td></tr> <tr><td><code>x</code></td>
<td>
<p>the scores matrix <code class="reqn">\mathbf{XW}</code>, 
containing the principal components as columns (after centering and scaling
if requested)</p>
</td></tr> <tr><td><code>center</code>, <code>scale.</code></td>
<td>
<p>the centering and scaling used</p>
</td></tr> 
<tr><td><code>xp</code></td>
<td>
<p>the deflated data matrix corresponding to <code>x</code></p>
</td></tr> <tr><td><code>q</code></td>
<td>
<p>an 
orthonormal basis for the principal subspace</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The PCA terminology is not consistent across the literature. Given a 
zero mean data matrix <code class="reqn">\mathbf{X}</code> (with observations as rows) and a
basis <code class="reqn">\mathbf{W}</code> of the principal subspace, we define the scores 
matrix as <code class="reqn">\mathbf{Z}=\mathbf{XW}</code> which contains the principal 
components as its columns. The columns of the pseudo-rotation matrix 
<code class="reqn">\mathbf{W}</code> are called the principal axes, and the elements of 
<code class="reqn">\mathbf{W}</code> are called the loadings.
</p>


<h3>References</h3>

<p>Mackey, L. (2009) Deflation Methods for Sparse PCA. In 
<em>Advances in Neural Information Processing Systems</em> (pp. 1017&ndash;1024).
</p>

<hr>
<h2 id='cardinality'>Cardinality of Column Vectors</h2><span id='topic+cardinality'></span>

<h3>Description</h3>

<p>Computes the cardinality (the sum of non-zero elements) of each column of 
the matrix <code class="reqn">\mathbf{W}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cardinality(w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cardinality_+3A_w">w</code></td>
<td>
<p>a numeric matrix, e.g. the rotation matrix of a sparse PCA analysis</p>
</td></tr>
</table>

<hr>
<h2 id='nscumcomp'>Non-Negative and Sparse Cumulative PCA</h2><span id='topic+nscumcomp'></span><span id='topic+nscumcomp.default'></span><span id='topic+nscumcomp.formula'></span>

<h3>Description</h3>

<p>Performs a PCA-like analysis on the given data matrix, where
non-negativity and/or sparsity constraints are enforced on the principal axes
(PAs). In contrast to regular PCA, which greedily maximises the variance of
each principal component (PC), <code>nscumcomp</code> <em>jointly</em> optimizes the
components such that the cumulative variance of all PCs is maximal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nscumcomp(x, ...)

## Default S3 method:
nscumcomp(x, ncomp = min(dim(x)), omega = rep(1, nrow(x)),
  k = d * ncomp, nneg = FALSE, gamma = 0, center = TRUE,
  scale. = FALSE, nrestart = 5, em_tol = 0.001, em_maxiter = 20,
  verbosity = 0, ...)

## S3 method for class 'formula'
nscumcomp(formula, data = NULL, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nscumcomp_+3A_x">x</code></td>
<td>
<p>a numeric matrix or data frame which provides the data for the
analysis.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_ncomp">ncomp</code></td>
<td>
<p>the number of principal components (PCs) to be computed. The 
default is to compute a full basis for <code>x</code>.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_omega">omega</code></td>
<td>
<p>a vector with as many entries as there are data samples, to 
perform weighted PCA (analogous to weighted least-squares regression). The 
default is an equal weighting of all samples.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_k">k</code></td>
<td>
<p>an upper bound on the total number of non-zero loadings of the 
pseudo-rotation matrix <code class="reqn">\mathbf{W}</code>. <code>k</code> is increased if
necessary to ensure at least one non-zero coefficient per principal axis.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_nneg">nneg</code></td>
<td>
<p>a logical value indicating whether the loadings should be 
non-negative, i.e. the PAs should be constrained to the non-negative 
orthant.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_gamma">gamma</code></td>
<td>
<p>a non-negative penalty on the divergence from orthonormality of 
the pseudo-rotation matrix. The default is not to penalize, but a positive 
value is sometimes necessary to avoid PAs collapsing onto each other.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_center">center</code></td>
<td>
<p>a logical value indicating whether the empirical mean of (the 
columns of) <code>x</code> should be subtracted. Alternatively, a vector of 
length equal to the number of columns of <code>x</code> can be supplied. The 
value is passed to <code><a href="base.html#topic+scale">scale</a></code>.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_scale.">scale.</code></td>
<td>
<p>a logical value indicating whether the columns of <code>x</code> 
should be scaled to have unit variance before the analysis takes place. The
default is <code>FALSE</code> for consistency with <code>prcomp</code>. Alternatively, 
a vector of length equal to the number of columns of <code>x</code> can be 
supplied. The value is passed to <code><a href="base.html#topic+scale">scale</a></code>.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_nrestart">nrestart</code></td>
<td>
<p>the number of random restarts for computing the 
pseudo-rotation matrix via expectation-maximization (EM) iterations. The 
solution achieving the minimum of the objective function over all random 
restarts is kept. A value greater than one can help to avoid poor local 
minima.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_em_tol">em_tol</code></td>
<td>
<p>If the relative change of the objective is less than 
<code>em_tol</code> between iterations, the EM procedure is asssumed to have 
converged to a local optimum.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_em_maxiter">em_maxiter</code></td>
<td>
<p>the maximum number of EM iterations to be performed. The EM
procedure is terminated if either the <code>em_tol</code> or the 
<code>em_maxiter</code> criterion is satisfied.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_verbosity">verbosity</code></td>
<td>
<p>an integer specifying the verbosity level. Greater values 
result in more output, the default is to be quiet.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_formula">formula</code></td>
<td>
<p>a formula with no response variable, referring only to numeric variables.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="nscumcomp_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nscumcomp</code> computes all PCs jointly using expectation-maximization (EM)
iterations. The M-step is equivalent to minimizing the objective function
</p>
<p style="text-align: center;"><code class="reqn">\left\Vert \mathbf{X}-\mathbf{Z}\mathbf{W}^{\top}\right\Vert
_{F}^{2}+\gamma\left\Vert \mathbf{W}^{\top}\mathbf{W}-\mathbf{I}\right\Vert
_{F}^{2}</code>
</p>

<p>w.r.t. the pseudo-rotation matrix <code class="reqn">\mathbf{W}</code>, where 
<code class="reqn">\mathbf{Z}=\mathbf{X}\mathbf{W}\left(\mathbf{W}^\top\mathbf{W}\right)^{-1}</code>
is the scores matrix modified to account for the non-orthogonality of 
<code class="reqn">\mathbf{W}</code>, <code class="reqn">\mathbf{I}</code> is the identity matrix and 
<code>gamma</code> is the Lagrange parameter associated with the ortho-normality 
penalty on <code class="reqn">\mathbf{W}</code>. Non-negativity of the loadings is achieved by
enforcing a zero lower bound in the L-BFGS-B algorithm used for the
minimization of the objective, and sparsity is achieved by a subsequent soft
thresholding of <code class="reqn">\mathbf{W}</code>.
</p>


<h3>Value</h3>

<p><code>nscumcomp</code> returns a list with class <code>(nsprcomp, prcomp)</code> 
containing the following elements: </p>
<table>
<tr><td><code>sdev</code></td>
<td>
<p>the additional standard 
deviation explained by each component, see <code><a href="#topic+asdev">asdev</a></code>.</p>
</td></tr> 
<tr><td><code>rotation</code></td>
<td>
<p>the matrix of non-negative and/or sparse loadings, 
containing the principal axes as columns.</p>
</td></tr> <tr><td><code>x</code></td>
<td>
<p>the scores matrix 
<code class="reqn">\mathbf{XW}</code> containing the principal components as columns 
(after centering and scaling if requested)</p>
</td></tr> <tr><td><code>center</code>, <code>scale</code></td>
<td>
<p>the 
centering and scaling used, or <code>FALSE</code></p>
</td></tr> <tr><td><code>xp</code></td>
<td>
<p>the deflated data 
matrix corresponding to <code>x</code></p>
</td></tr> <tr><td><code>q</code></td>
<td>
<p>an orthonormal basis for the 
principal subspace</p>
</td></tr>
</table>
<p>The components are returned in order of decreasing variance for 
convenience.
</p>


<h3>Note</h3>

<p>The PCA terminology is not consistent across the literature. Given a 
zero mean data matrix <code class="reqn">\mathbf{X}</code> (with observations as rows) and a
basis <code class="reqn">\mathbf{W}</code> of the principal subspace, we define the scores 
matrix as <code class="reqn">\mathbf{Z}=\mathbf{XW}</code> which contains the principal 
components as its columns. The columns of the pseudo-rotation matrix 
<code class="reqn">\mathbf{W}</code> are called the principal axes, and the elements of 
<code class="reqn">\mathbf{W}</code> are called the loadings.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+asdev">asdev</a></code>,  <code><a href="#topic+peav">peav</a></code>, <code><a href="#topic+nsprcomp">nsprcomp</a></code>, 
<code><a href="base.html#topic+scale">scale</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE)) withAutoprint({

  set.seed(1)

  # Regular PCA, with tolerance set to return five PCs
  pca &lt;- prcomp(MASS::Boston, tol = 0.35, scale. = TRUE)
  cumsum(pca$sdev[1:5])

  # Sparse cumulative PCA with five components and a total of 20 non-zero loadings.
  # The orthonormality penalty is set to a value which avoids co-linear principal
  # axes. Note that the non-zero loadings are not distributed uniformly over
  # the components.
  scc &lt;- nscumcomp(MASS::Boston, ncomp = 5, k = 20, gamma = 1e4, scale. = TRUE)
  cumsum(scc$sdev)
  cardinality(scc$rotation)

  # Non-negative sparse cumulative PCA
  nscumcomp(MASS::Boston, ncomp = 5, nneg = TRUE, k = 20, gamma = 1e4, scale. = TRUE)
})
</code></pre>

<hr>
<h2 id='nsprcomp'>Non-Negative and Sparse PCA</h2><span id='topic+nsprcomp'></span><span id='topic+nsprcomp.default'></span><span id='topic+nsprcomp.formula'></span>

<h3>Description</h3>

<p>Performs a constrained principal component analysis,
where non-negativity and/or sparsity constraints are enforced on the principal axes.
The result is returned as an object of class <code>nsprcomp</code>, which inherits from
<code>prcomp</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nsprcomp(x, ...)

## Default S3 method:
nsprcomp(x, retx = TRUE, ncomp = min(dim(x)),
  omega = rep(1, nrow(x)), k = ncol(x), nneg = FALSE, center = TRUE,
  scale. = FALSE, tol = NULL, nrestart = 5, em_tol = 0.001,
  em_maxiter = 100, partial_model = NULL, verbosity = 0, ...)

## S3 method for class 'formula'
nsprcomp(formula, data = NULL, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nsprcomp_+3A_x">x</code></td>
<td>
<p>a numeric matrix or data frame which provides the data 
for the principal component analysis.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_retx">retx</code></td>
<td>
<p>a logical value indicating whether the principal components, i.e.
<code>x</code> projected into the principal subspace, should be returned.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_ncomp">ncomp</code></td>
<td>
<p>the number of principal components (PCs) to be computed. With 
the default setting, PCs are computed until <code>x</code> is fully deflated. 
<code>ncomp</code> can be specified implicitly if <code>k</code> is given as a vector.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_omega">omega</code></td>
<td>
<p>a vector with as many entries as there are data samples, to 
perform weighted PCA (analogous to weighted least-squares regression). The 
default is an equal weighting of all samples.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_k">k</code></td>
<td>
<p>either a scalar or a vector of length <code>ncomp</code>, specifying the 
upper bounds on the cardinalities of the principal axes (PAs).</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_nneg">nneg</code></td>
<td>
<p>a logical value indicating whether the loadings should be 
non-negative, i.e. the PAs should be constrained to the non-negative 
orthant.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_center">center</code></td>
<td>
<p>a logical value indicating whether the empirical mean of (the 
columns) of <code>x</code> should be subtracted. Alternatively, a vector of 
length equal to the number of columns of <code>x</code> can be supplied. The 
value is passed to <code><a href="base.html#topic+scale">scale</a></code>.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_scale.">scale.</code></td>
<td>
<p>a logical value indicating whether the columns of <code>x</code> 
should be scaled to have unit variance before the analysis takes place. The
default is <code>FALSE</code> for consistency with <code>prcomp</code>. Alternatively, 
a vector of length equal to the number of columns of <code>x</code> can be 
supplied. The value is passed to <code><a href="base.html#topic+scale">scale</a></code>.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_tol">tol</code></td>
<td>
<p>a threshold indicating the magnitude below which components should
be omitted. Components are omitted if their standard deviations are less 
than or equal to <code>tol</code> times the standard deviation of the first 
component. With the default <code>NULL</code> setting, no components are omitted.
With <code>tol = 0</code> or <code>tol = sqrt(.Machine$double.eps)</code>, essentially 
constant components are omitted.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_nrestart">nrestart</code></td>
<td>
<p>the number of random restarts for computing the principal 
component via expectation-maximization (EM) iterations. The solution 
achieving maximum standard deviation over all random restarts is kept. A 
value greater than one can help to avoid poor local maxima.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_em_tol">em_tol</code></td>
<td>
<p>If the relative change of the objective is less than 
<code>em_tol</code> between iterations, the EM procedure is asssumed to have 
converged to a local optimum.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_em_maxiter">em_maxiter</code></td>
<td>
<p>the maximum number of EM iterations to be performed. The EM
procedure is terminated if either the <code>em_tol</code> or the 
<code>em_maxiter</code> criterion is satisfied.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_partial_model">partial_model</code></td>
<td>
<p><code>NULL</code> or an object of class <code>nsprcomp</code>. The 
computation can be continued from a partial model by providing a 
<code>nsprcomp</code> object (either from a previous run of this function or from
<code><a href="#topic+asdev">asdev</a></code>) and setting <code>ncomp</code> to a value greater than the 
number of components contained in the partial model. See the examples for 
an illustration.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_verbosity">verbosity</code></td>
<td>
<p>an integer specifying the verbosity level. Greater values 
result in more output, the default is to be quiet.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_formula">formula</code></td>
<td>
<p>a formula with no response variable, referring only to numeric variables.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
<code>formula</code>. By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="nsprcomp_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The &lsquo;factory-fresh&rsquo;
default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nsprcomp</code> computes a principal component (PC) using expectation-maximization
iterations, where non-negativity of the loadings is achieved by projecting
the principal axis (PA) into the non-negative orthant, and sparsity of the 
loadings is achieved by soft thresholding (Sigg and Buhmann, 2008).
</p>
<p>Because constrained principal axes no longer correspond to true eigenvectors 
of the covariance matrix and are usually not pairwise orthogonal, special
attention needs to be paid when computing more than a single PC. The
algorithm implements the generalized deflation method proposed by
Mackey (2009) to maximize the additional variance of each
component. Given a basis of the space spanned by the previous PAs, the
variance of the PC is maximized after projecting the current PA to the
ortho-complement space of the basis. This procedure maximizes the
additional variance not explained by previous components, and is
identical to standard PCA if no sparsity or non-negativity constraints
are enforced on the PAs.
</p>
<p>See the references for further details.
</p>


<h3>Value</h3>

<p><code>nsprcomp</code> returns a list with class <code>(nsprcomp, prcomp)</code> 
containing the following elements: </p>
<table>
<tr><td><code>sdev</code></td>
<td>
<p>the additional standard 
deviation explained by each component, see <code><a href="#topic+asdev">asdev</a></code>.</p>
</td></tr> 
<tr><td><code>rotation</code></td>
<td>
<p>the matrix of non-negative and/or sparse loadings, 
containing the principal axes as columns.</p>
</td></tr> <tr><td><code>x</code></td>
<td>
<p>the scores matrix 
<code class="reqn">\mathbf{XW}</code> containing the principal components as columns 
(after centering and scaling if requested). For the formula method, 
<code><a href="stats.html#topic+napredict">napredict</a></code> is applied to handle the treatment of values 
omitted by the <code>na.action</code>.</p>
</td></tr> <tr><td><code>center</code>, <code>scale</code></td>
<td>
<p>the centering and 
scaling used, or <code>FALSE</code></p>
</td></tr> <tr><td><code>xp</code></td>
<td>
<p>the deflated data matrix 
corresponding to <code>x</code></p>
</td></tr> <tr><td><code>q</code></td>
<td>
<p>an orthonormal basis for the principal 
subspace</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The PCA terminology is not consistent across the literature. Given a 
zero mean data matrix <code class="reqn">\mathbf{X}</code> (with observations as rows) and a
basis <code class="reqn">\mathbf{W}</code> of the principal subspace, we define the scores 
matrix as <code class="reqn">\mathbf{Z}=\mathbf{XW}</code> which contains the principal 
components as its columns. The columns of the pseudo-rotation matrix
<code class="reqn">\mathbf{W}</code> are called the principal axes, and the elements of
<code class="reqn">\mathbf{W}</code> are called the loadings.
</p>
<p>Deflating the data matrix accumulates numerical errors over successive PCs.
</p>


<h3>References</h3>

<p>Sigg, C. D. and Buhmann, J. M. (2008) Expectation-Maximization 
for Sparse and Non-Negative PCA. In <em>Proceedings of the 25th 
International Conference on Machine Learning</em> (pp. 960&ndash;967).
</p>
<p>Mackey, L. (2009) Deflation Methods for Sparse PCA. In 
<em>Advances in Neural Information Processing Systems</em> (pp. 1017&ndash;1024).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+asdev">asdev</a></code>,  <code><a href="#topic+peav">peav</a></code>, <code><a href="stats.html#topic+prcomp">prcomp</a></code>, 
<code><a href="base.html#topic+scale">scale</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE)) withAutoprint({
  set.seed(1)

  # Regular PCA, with the tolerance set to return five PCs
  prcomp(MASS::Boston, tol = 0.36, scale. = TRUE)

  # Sparse PCA with different cardinalities per component. The number of components
  # is derived from the length of vector k.
  nsprcomp(MASS::Boston, k = c(13, 7, 5, 5, 5), scale. = TRUE)

  # Non-negative sparse PCA with four components. Note that the principal axes
  # naturally have a high degree of orthogonality, because each component
  # maximizes the additional variance not already explained.
  set.seed(1)
  nsprcomp(MASS::Boston, k = c(7, 5, 2, 2), nneg = TRUE, scale. = TRUE)

  # The optimization can get stuck in local optima. Increase the number of
  # random restarts or the number of power iterations to likely obtain decreasing
  # standard deviations.
  set.seed(1)
  (nspc &lt;- nsprcomp(MASS::Boston, k = c(7, 5, 2, 2), nneg = TRUE, scale. = TRUE,
                    nrestart = 10, em_tol = 1e-4, verbosity = 1))

  # continue the computation of components from a partial model
  nsprcomp(MASS::Boston, k = 3, ncomp = 5, nneg = TRUE, scale. = TRUE, partial_model = nspc)

  # The reconstruction error for each sample can be influenced using the
  # weighting vector omega. To reconstruct the data, the generalized
  # inverse of the pseudo-rotation matrix has to be used, because the constrained
  # principal axes are in general not pairwise orthogonal.
  set.seed(1)
  X &lt;- matrix(runif(5*10), 5)
  nspc &lt;- nsprcomp(X, omega = c(5, 1, 1, 1, 5), ncomp = 2, nneg = TRUE)
  X_hat &lt;- predict(nspc)%*%MASS::ginv(nspc$rotation) + matrix(1,5,1)%*%nspc$center
  rowSums((X - X_hat)^2)
})
</code></pre>

<hr>
<h2 id='peav'>Percentage Explained Additional Variance</h2><span id='topic+peav'></span>

<h3>Description</h3>

<p><code>peav</code> computes the percentage of the explained _additional_ variance of each
principal component, taking into account the possible non-orthogonality of
the pseudo-rotation matrix <code class="reqn">\mathbf{W}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>peav(x, w, center = TRUE, scale. = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="peav_+3A_x">x</code></td>
<td>
<p>a numeric data matrix with the observations as rows</p>
</td></tr>
<tr><td><code id="peav_+3A_w">w</code></td>
<td>
<p>a numeric data matrix with the principal axes as columns</p>
</td></tr>
<tr><td><code id="peav_+3A_center">center</code></td>
<td>
<p>a logical value indicating whether the empirical mean of 
<code>x</code> should be subtracted. Alternatively, a vector of length equal to 
the number of columns of <code>x</code> can be supplied. The value is passed to 
<code><a href="base.html#topic+scale">scale</a></code>.</p>
</td></tr>
<tr><td><code id="peav_+3A_scale.">scale.</code></td>
<td>
<p>a logical value indicating whether the columns of <code>x</code> 
should be scaled to have unit variance before the analysis takes place. The
default is <code>FALSE</code> for consistency with <code>prcomp</code>. Alternatively, 
a vector of length equal to the number of columns of <code>x</code> can be 
supplied.  The value is passed to <code><a href="base.html#topic+scale">scale</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The explained additional variance is computed using <code><a href="#topic+asdev">asdev</a></code> and
divided by the total variance of the data to obtain percentages.
<code>sum(peav(x, w))</code> is equal to one if <code class="reqn">\mathbf{W}</code> is an orthonormal
basis, e.g. the rotation matrix of a standard PCA.
</p>
<p><code>peav</code> is useful to compare the solutions of various constrained PCA 
methods w.r.t. standard PCA.
</p>


<h3>Note</h3>

<p>The method produces different results than the &quot;percentage explained 
variance&quot; (<code>pev</code>) computed by the <code>spca</code> function from the
<code>elasticnet</code> package.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+asdev">asdev</a></code>,  <code><a href="base.html#topic+scale">scale</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
