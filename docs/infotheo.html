<!DOCTYPE html><html><head><title>Help for package infotheo</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {infotheo}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#condentropy'><p>conditional entropy computation</p></a></li>
<li><a href='#condinformation'><p>conditional mutual information computation</p></a></li>
<li><a href='#discretize'><p>Unsupervized Data Discretization</p></a></li>
<li><a href='#entropy'><p>entropy computation</p></a></li>
<li><a href='#infotheo'><p>Information Theory package</p></a></li>
<li><a href='#interinformation'><p>interaction information computation</p></a></li>
<li><a href='#multiinformation'><p>multiinformation computation</p></a></li>
<li><a href='#mutinformation'><p>mutual information computation</p></a></li>
<li><a href='#natstobits'><p>convert nats into bits</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Information-Theoretic Measures</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2014-07-16</td>
</tr>
<tr>
<td>Author:</td>
<td>Patrick E. Meyer</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements various measures of information theory based on several entropy estimators.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Patrick E. Meyer &lt;software@meyerp.com&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://homepage.meyerp.com/software">http://homepage.meyerp.com/software</a></td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-04-08 10:26:51 UTC; hornik</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-04-08 11:00:24 UTC</td>
</tr>
</table>
<hr>
<h2 id='condentropy'>conditional entropy computation</h2><span id='topic+condentropy'></span>

<h3>Description</h3>

<p><code>condentropy</code> takes two random vectors, X and Y, as input and returns the 
conditional entropy, H(X|Y), in nats (base e), according to the entropy estimator <code>method</code>. 
If Y is not supplied the function returns the entropy of X - see <code><a href="#topic+entropy">entropy</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>condentropy(X, Y=NULL, method="emp")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="condentropy_+3A_x">X</code></td>
<td>
<p>data.frame denoting a random variable or random vector where columns contain variables/features and rows contain outcomes/samples.</p>
</td></tr>
<tr><td><code id="condentropy_+3A_y">Y</code></td>
<td>
<p>data.frame denoting a conditioning random variable or random vector where columns contain variables/features and rows contain outcomes/samples.</p>
</td></tr>
<tr><td><code id="condentropy_+3A_method">method</code></td>
<td>
<p>The name of the entropy estimator. The package implements four estimators : 
&quot;emp&quot;, &quot;mm&quot;, &quot;shrink&quot;, &quot;sg&quot; (default:&quot;emp&quot;) - see details. 
These estimators require discrete data values - see <code><a href="#topic+discretize">discretize</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> &quot;emp&quot; : This estimator computes the entropy of the empirical probability distribution.
</p>
</li>
<li><p> &quot;mm&quot; : This is the Miller-Madow asymptotic bias corrected empirical estimator.
</p>
</li>
<li><p> &quot;shrink&quot; : This is a shrinkage estimate of the entropy of a Dirichlet probability distribution.
</p>
</li>
<li><p> &quot;sg&quot; : This is the Schurmann-Grassberger estimate of the entropy of a Dirichlet probability distribution.
</p>
</li></ul>



<h3>Value</h3>

 <p><code>condentropy</code> returns the conditional entropy, H(X|Y), of X given Y in nats.</p>


<h3>Author(s)</h3>

<p>Patrick E. Meyer
</p>


<h3>References</h3>

<p>Meyer,  P. E.  (2008). Information-Theoretic Variable Selection and Network Inference from Microarray Data. PhD thesis of the Universite Libre de Bruxelles.
</p>
<p>Cover, T. M. and Thomas, J. A. (1990). Elements of Information Theory. John Wiley,
New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy">entropy</a></code>, <code><a href="#topic+mutinformation">mutinformation</a></code>, <code><a href="#topic+natstobits">natstobits</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(USArrests)
  dat&lt;-discretize(USArrests)
  H &lt;- condentropy(dat[,1], dat[,2], method = "mm")
</code></pre>

<hr>
<h2 id='condinformation'>conditional mutual information computation</h2><span id='topic+condinformation'></span>

<h3>Description</h3>

<p><code>condinformation</code> takes three random variables as input and computes the 
conditional mutual information in nats according to the entropy estimator <code>method</code>.
If S is not supplied the function returns the mutual information between X and Y - see <code><a href="#topic+mutinformation">mutinformation</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>condinformation(X, Y, S=NULL, method="emp")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="condinformation_+3A_x">X</code></td>
<td>
<p>vector/factor denoting a random variable or a data.frame denoting a random vector where columns contain variables/features and rows contain outcomes/samples.</p>
</td></tr>
<tr><td><code id="condinformation_+3A_y">Y</code></td>
<td>
<p>another random variable or random vector (vector/factor or data.frame).</p>
</td></tr>
<tr><td><code id="condinformation_+3A_s">S</code></td>
<td>
<p>the conditioning random variable or random vector (vector/factor or data.frame).</p>
</td></tr>
<tr><td><code id="condinformation_+3A_method">method</code></td>
<td>
<p>The name of the entropy estimator. The package implements four estimators : 
&quot;emp&quot;, &quot;mm&quot;, &quot;shrink&quot;, &quot;sg&quot; (default:&quot;emp&quot;) - see details. 
These estimators require discrete data values - see <code><a href="#topic+discretize">discretize</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> &quot;emp&quot; : This estimator computes the entropy of the empirical probability distribution.
</p>
</li>
<li><p> &quot;mm&quot; : This is the Miller-Madow asymptotic bias corrected empirical estimator.
</p>
</li>
<li><p> &quot;shrink&quot; : This is a shrinkage estimate of the entropy of a Dirichlet probability distribution.
</p>
</li>
<li><p> &quot;sg&quot; : This is the Schurmann-Grassberger estimate of the entropy of a Dirichlet probability distribution.
</p>
</li></ul>



<h3>Value</h3>

 <p><code>condinformation</code> returns the conditional mutual information, I(X;Y|S), in nats.</p>


<h3>Author(s)</h3>

<p>Patrick E. Meyer
</p>


<h3>References</h3>

<p>Meyer,  P. E.  (2008). Information-Theoretic Variable Selection and Network Inference from Microarray Data. PhD thesis of the Universite Libre de Bruxelles.
</p>
<p>Cover, T. M. and Thomas, J. A. (1990). Elements of Information Theory. John Wiley,
New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mutinformation">mutinformation</a></code>, <code><a href="#topic+multiinformation">multiinformation</a></code>, <code><a href="#topic+interinformation">interinformation</a></code>, <code><a href="#topic+natstobits">natstobits</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(USArrests)
  dat&lt;-discretize(USArrests)
  I &lt;- condinformation(dat[,1],dat[,2],dat[,3],method="emp")
</code></pre>

<hr>
<h2 id='discretize'>Unsupervized Data Discretization</h2><span id='topic+discretize'></span>

<h3>Description</h3>

<p><code>discretize</code> discretizes <code>data</code> using the equal frequencies or equal width binning algorithm. 
&quot;equalwidth&quot; and &quot;equalfreq&quot; discretizes each random variable (each column) of the data into <code>nbins</code>. 
&quot;globalequalwidth&quot; discretizes the range of the random vector <code>data</code> into <code>nbins</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discretize( X, disc="equalfreq", nbins=NROW(X)^(1/3) )</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discretize_+3A_x">X</code></td>
<td>
<p> A data.frame containing data to be discretized. The columns contains variables and the rows samples.</p>
</td></tr>
<tr><td><code id="discretize_+3A_disc">disc</code></td>
<td>
<p> The name of the discretization method to be used :&quot;equalfreq&quot;, &quot;equalwidth&quot; or &quot;globalequalwidth&quot; (default : &quot;equalfreq&quot;) - see references.</p>
</td></tr>
<tr><td><code id="discretize_+3A_nbins">nbins</code></td>
<td>
<p> Integer specifying the number of bins to be used for the discretization. By default the number of bins is set to 
<code class="reqn">{N}^(1/3)</code> where N is the number of samples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>discretize</code> returns the discretized dataset.</p>


<h3>Author(s)</h3>

<p>Patrick E. Meyer, Frederic Lafitte, Gianluca Bontempi, Korbinian Strimmer
</p>


<h3>References</h3>

<p>Meyer,  P. E.  (2008). Information-Theoretic Variable Selection and Network Inference from Microarray Data. PhD thesis of the Universite Libre de Bruxelles.
</p>
<p>Dougherty, J., Kohavi, R., and Sahami, M. (1995). Supervised and unsupervised
discretization of continuous features. In International Conference on Machine Learning.
</p>
<p>Yang, Y. and Webb, G. I. (2003). Discretization for naive-bayes learning: managing
discretization bias and variance. Technical Report 2003/131 School of Computer Science
and Software Engineering, Monash University.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(USArrests)
nbins&lt;- sqrt(NROW(USArrests))
ew.data &lt;- discretize(USArrests,"equalwidth", nbins)
ef.data &lt;- discretize(USArrests,"equalfreq", nbins)
gew.data &lt;- discretize(USArrests,"globalequalwidth", nbins)
</code></pre>

<hr>
<h2 id='entropy'>entropy computation</h2><span id='topic+entropy'></span>

<h3>Description</h3>

<p><code>entropy</code> takes the dataset as input and computes the 
entropy according 
to the entropy estimator <code>method</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy(X, method="emp")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="entropy_+3A_x">X</code></td>
<td>
<p>data.frame denoting a random vector  where columns contain variables/features and rows contain outcomes/samples.</p>
</td></tr>
<tr><td><code id="entropy_+3A_method">method</code></td>
<td>
<p>The name of the entropy estimator. The package implements four estimators : 
&quot;emp&quot;, &quot;mm&quot;, &quot;shrink&quot;, &quot;sg&quot; (default:&quot;emp&quot;) - see details. 
These estimators require discrete data values - see <code><a href="#topic+discretize">discretize</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> &quot;emp&quot; : This estimator computes the entropy of the empirical probability distribution.
</p>
</li>
<li><p> &quot;mm&quot; : This is the Miller-Madow asymptotic bias corrected empirical estimator.
</p>
</li>
<li><p> &quot;shrink&quot; : This is a shrinkage estimate of the entropy of a Dirichlet probability distribution.
</p>
</li>
<li><p> &quot;sg&quot; : This is the Schurmann-Grassberger estimate of the entropy of a Dirichlet probability distribution.
</p>
</li></ul>



<h3>Value</h3>

 <p><code>entropy</code> returns the entropy of the data in nats.</p>


<h3>Author(s)</h3>

<p>Patrick E. Meyer
</p>


<h3>References</h3>

<p>Meyer,  P. E.  (2008). Information-Theoretic Variable Selection and Network Inference from Microarray Data. PhD thesis of the Universite Libre de Bruxelles.
</p>
<p>J. Beirlant, E. J. Dudewica, L. Gyofi, and E. van der Meulen (1997). Nonparametric 
entropy estimation : An overview. Journal of Statistics.
</p>
<p>Hausser J. (2006). Improving entropy estimation and the inference of genetic regulatory networks.
Master thesis of the National Institute of Applied Sciences of Lyon.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+condentropy">condentropy</a></code>, <code><a href="#topic+mutinformation">mutinformation</a></code>, <code><a href="#topic+natstobits">natstobits</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(USArrests)
  H &lt;- entropy(discretize(USArrests),method="shrink")
</code></pre>

<hr>
<h2 id='infotheo'>Information Theory package</h2><span id='topic+infotheo'></span>

<h3>Description</h3>

<p>The package infotheo provide various estimators for computing information-theoretic measures from data
</p>


<h3>Author(s)</h3>

<p>Patrick E. Meyer
</p>


<h3>References</h3>

<p>Meyer,  P. E.  (2008). Information-Theoretic Variable Selection and Network Inference from Microarray Data. PhD thesis of the Universite Libre de Bruxelles.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy">entropy</a></code>, <code><a href="#topic+condentropy">condentropy</a></code>, <code><a href="#topic+mutinformation">mutinformation</a></code>, <code><a href="#topic+condinformation">condinformation</a></code>, <code><a href="#topic+multiinformation">multiinformation</a></code>, <code><a href="#topic+interinformation">interinformation</a></code>, <code><a href="#topic+natstobits">natstobits</a></code></p>

<hr>
<h2 id='interinformation'>interaction information computation</h2><span id='topic+interinformation'></span>

<h3>Description</h3>

<p><code>interinformation</code> takes a dataset as input and computes the 
the interaction information among the random variables in the dataset using   
the entropy estimator <code>method</code>. This measure is also called synergy or complementarity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interinformation(X, method="emp")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interinformation_+3A_x">X</code></td>
<td>
<p>data.frame denoting a random vector where columns contain variables/features and rows contain outcomes/samples.</p>
</td></tr>
<tr><td><code id="interinformation_+3A_method">method</code></td>
<td>
<p>The name of the entropy estimator. The package implements four estimators : 
&quot;emp&quot;, &quot;mm&quot;, &quot;shrink&quot;, &quot;sg&quot; (default:&quot;emp&quot;) - see details. 
These estimators require discrete data values - see <code><a href="#topic+discretize">discretize</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> &quot;emp&quot; : This estimator computes the entropy of the empirical probability distribution.
</p>
</li>
<li><p> &quot;mm&quot; : This is the Miller-Madow asymptotic bias corrected empirical estimator.
</p>
</li>
<li><p> &quot;shrink&quot; : This is a shrinkage estimate of the entropy of a Dirichlet probability distribution.
</p>
</li>
<li><p> &quot;sg&quot; : This is the Schurmann-Grassberger estimate of the entropy of a Dirichlet probability distribution.
</p>
</li></ul>



<h3>Value</h3>

 <p><code>interinformation</code> returns the interaction information (also called synergy or complementarity), in nats, among the random variables (columns of the data.frame).</p>


<h3>Author(s)</h3>

<p>Patrick E. Meyer
</p>


<h3>References</h3>

<p>Meyer,  P. E.  (2008). Information-Theoretic Variable Selection and Network Inference from Microarray Data. PhD thesis of the Universite Libre de Bruxelles.
</p>
<p>Jakulin, A. and Bratko, I. (2004). Testing the significance of attribute interactions.
In Proc. of 21st International Conference on Machine Learning (ICML).
</p>
<p>McGill, W. J. (1954). Multivariate information transmission. Psychometrika, 19.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+condinformation">condinformation</a></code>, <code><a href="#topic+multiinformation">multiinformation</a></code>, <code><a href="#topic+mutinformation">mutinformation</a></code>, <code><a href="#topic+natstobits">natstobits</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(USArrests)
  dat&lt;-discretize(USArrests)
  ii &lt;- interinformation(dat, method = "sg")
</code></pre>

<hr>
<h2 id='multiinformation'>multiinformation computation</h2><span id='topic+multiinformation'></span>

<h3>Description</h3>

<p><code>multiinformation</code> takes a dataset as input and computes the 
multiinformation (also called total correlation) among the random variables in the dataset.
The value is returned in nats using the entropy estimator <code>estimator</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiinformation(X, method ="emp")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiinformation_+3A_x">X</code></td>
<td>
<p>data.frame containing a set of random variables where columns contain variables/features and rows contain outcomes/samples.</p>
</td></tr>
<tr><td><code id="multiinformation_+3A_method">method</code></td>
<td>
<p>The name of the entropy estimator. The package implements four estimators : 
&quot;emp&quot;, &quot;mm&quot;, &quot;shrink&quot;, &quot;sg&quot; (default:&quot;emp&quot;) - see details. 
These estimators require discrete data values - see <code><a href="#topic+discretize">discretize</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> &quot;emp&quot; : This estimator computes the entropy of the empirical probability distribution.
</p>
</li>
<li><p> &quot;mm&quot; : This is the Miller-Madow asymptotic bias corrected empirical estimator.
</p>
</li>
<li><p> &quot;shrink&quot; : This is a shrinkage estimate of the entropy of a Dirichlet probability distribution.
</p>
</li>
<li><p> &quot;sg&quot; : This is the Schurmann-Grassberger estimate of the entropy of a Dirichlet probability distribution.
</p>
</li></ul>



<h3>Value</h3>

 <p><code>multiinformation</code> returns the multiinformation (also called total correlation) among the variables in the dataset (in nats).</p>


<h3>Author(s)</h3>

<p>Patrick E. Meyer
</p>


<h3>References</h3>

<p>Meyer,  P. E.  (2008). Information-Theoretic Variable Selection and Network Inference from Microarray Data. PhD thesis of the Universite Libre de Bruxelles.
</p>
<p>Studeny, M. and Vejnarova, J. (1998). The multiinformation function as a tool for
measuring stochastic dependence. In Proceedings of the NATO Advanced Study Institute
on Learning in graphical models,
</p>


<h3>See Also</h3>

<p><code><a href="#topic+condinformation">condinformation</a></code>, <code><a href="#topic+mutinformation">mutinformation</a></code>, <code><a href="#topic+interinformation">interinformation</a></code>, <code><a href="#topic+natstobits">natstobits</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(USArrests)
  dat&lt;-discretize(USArrests)
  M &lt;- multiinformation(dat)
</code></pre>

<hr>
<h2 id='mutinformation'>mutual information computation</h2><span id='topic+mutinformation'></span>

<h3>Description</h3>

<p><code>mutinformation</code> takes two random variables as input and computes the 
mutual information in nats according to the entropy estimator <code>method</code>.
If Y is not supplied and X is a matrix-like argument, the function returns a matrix of mutual 
information between all pairs of variables in the dataset X.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mutinformation(X, Y, method="emp")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mutinformation_+3A_x">X</code></td>
<td>
<p>vector/factor denoting a random variable or a data.frame denoting a random vector where columns contain variables/features and rows contain outcomes/samples.</p>
</td></tr>
<tr><td><code id="mutinformation_+3A_y">Y</code></td>
<td>
<p>another random variable or random vector (vector/factor or data.frame).</p>
</td></tr>
<tr><td><code id="mutinformation_+3A_method">method</code></td>
<td>
<p>The name of the entropy estimator. The package implements four estimators : 
&quot;emp&quot;, &quot;mm&quot;, &quot;shrink&quot;, &quot;sg&quot; (default:&quot;emp&quot;) - see details. 
These estimators require discrete data values - see <code><a href="#topic+discretize">discretize</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> &quot;emp&quot; : This estimator computes the entropy of the empirical probability distribution.
</p>
</li>
<li><p> &quot;mm&quot; : This is the Miller-Madow asymptotic bias corrected empirical estimator.
</p>
</li>
<li><p> &quot;shrink&quot; : This is a shrinkage estimate of the entropy of a Dirichlet probability distribution.
</p>
</li>
<li><p> &quot;sg&quot; : This is the Schurmann-Grassberger estimate of the entropy of a Dirichlet probability distribution.
</p>
</li></ul>



<h3>Value</h3>

 <p><code>mutinformation</code> returns the mutual information I(X;Y) in nats.</p>


<h3>Author(s)</h3>

<p>Patrick E. Meyer
</p>


<h3>References</h3>

<p>Meyer,  P. E.  (2008). Information-Theoretic Variable Selection and Network Inference from Microarray Data. PhD thesis of the Universite Libre de Bruxelles.
</p>
<p>Cover, T. M. and Thomas, J. A. (1990). Elements of Information Theory. John Wiley,
New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+condinformation">condinformation</a></code>, <code><a href="#topic+multiinformation">multiinformation</a></code>, <code><a href="#topic+interinformation">interinformation</a></code>, <code><a href="#topic+natstobits">natstobits</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(USArrests)
  dat&lt;-discretize(USArrests)
  #computes the MIM (mutual information matrix)
  I &lt;- mutinformation(dat,method= "emp")
  I2&lt;- mutinformation(dat[,1],dat[,2])
</code></pre>

<hr>
<h2 id='natstobits'>convert nats into bits</h2><span id='topic+natstobits'></span>

<h3>Description</h3>

<p><code>natstobits</code> takes a value in nats (a double) as input and returns the value in bits (a double). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>natstobits(H)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="natstobits_+3A_h">H</code></td>
<td>
<p>double denoting a value (in nats), as returned by one of the function of the infotheo package</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Information-theoretic quantities can have different units depending on the base of the logarithm used in their computation.
All the function of tha package use a base e, hence the unit is the nat. The value in bit is given by using the base 2, 
hence the conversion is done by multiplying by log2(e) = 1.442695.
</p>


<h3>Value</h3>

 <p><code>natstobits</code> returns a double that is the conversion of the nats value into bits.</p>


<h3>Author(s)</h3>

<p>Patrick E. Meyer
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(USArrests)
  H &lt;- entropy(discretize(USArrests))
  natstobits(H)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
