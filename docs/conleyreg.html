<!DOCTYPE html><html><head><title>Help for package conleyreg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {conleyreg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#conleyreg'><p>Conley standard error estimations</p></a></li>
<li><a href='#dist_mat'><p>Distance matrix estimation</p></a></li>
<li><a href='#rnd_locations'><p>Random location drawing</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Estimations using Conley Standard Errors</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.7</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions calculating Conley (1999) &lt;<a href="https://doi.org/10.1016%2FS0304-4076%2898%2900084-0">doi:10.1016/S0304-4076(98)00084-0</a>&gt; standard errors. The package started by merging and extending multiple packages and 
  other published scripts on this econometric technique. It strongly emphasizes computational optimization. Details are available in the function documentation and in 
  the vignette.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>base, stats, sf, Rcpp, data.table, lmtest, foreach, parallel,
doParallel, Rdpack, fixest, Matrix, lwgeom, s2, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-03-03 23:23:05 UTC; BAU2796</td>
</tr>
<tr>
<td>Author:</td>
<td>Christian Düben [aut, cre],
  Richard Bluhm [cph],
  Luis Calderon [cph],
  Darin Christensen [cph],
  Timothy Conley [cph],
  Thiemo Fetzer [cph],
  Leander Heldring [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Christian Düben &lt;christian.dueben@uni-hamburg.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-03-04 00:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='conleyreg'>Conley standard error estimations</h2><span id='topic+conleyreg'></span>

<h3>Description</h3>

<p>This function estimates ols, logit, probit, and poisson models with Conley standard errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conleyreg(
  formula,
  data,
  dist_cutoff,
  model = c("ols", "logit", "probit", "poisson"),
  unit = NULL,
  time = NULL,
  lat = NULL,
  lon = NULL,
  kernel = c("bartlett", "uniform"),
  lag_cutoff = 0,
  intercept = TRUE,
  verbose = TRUE,
  ncores = NULL,
  par_dim = c("cross-section", "time", "r", "cpp"),
  dist_comp = NULL,
  crs = NULL,
  st_distance = FALSE,
  dist_which = NULL,
  sparse = FALSE,
  batch = TRUE,
  batch_ram_opt = NULL,
  float = FALSE,
  rowwise = FALSE,
  reg_ram_opt = FALSE,
  dist_mat = NULL,
  dist_mat_conv = TRUE,
  vcov = FALSE,
  gof = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conleyreg_+3A_formula">formula</code></td>
<td>
<p>regression equation as formula or character string. Avoid interactions and transformations inside the equation. I.e. use
<code>y ~ x1 + x1_2, data = dplyr::mutate(data, x1_2 = x1^2)</code> instead of <code>y ~ x1 + x1^2, data = data)</code>.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_data">data</code></td>
<td>
<p>input data. Either (i) in non-spatial data frame format (includes tibbles and data tables) with columns denoting coordinates or (ii) in sf format. In
case of an sf object, all non-point geometry types are converted to spatial points, based on the feature's centroid. When using a non-spatial data frame format
with projected, i.e. non-longlat, coordinates, <code>crs</code> must be specified. Note that the projection can influence the computed distances, which is a general
phenomenon in GIS software and not specific to <code>conleyreg</code>. The computationally fastest option is to use a data table with coordinates in the crs in which
the distances are to be derived (longlat for spherical and projected for planar), and with time and unit set as keys in the panel case. An sf object as input is
the slowest option.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_dist_cutoff">dist_cutoff</code></td>
<td>
<p>the distance cutoff in km</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_model">model</code></td>
<td>
<p>the applied model. Either <code>"ols"</code> (default), <code>"logit"</code>, <code>"probit"</code> or <code>"poisson"</code>. <code>"logit"</code>, <code>"probit"</code>, and
<code>"poisson"</code> are currently restricted to cross-sectional applications.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_unit">unit</code></td>
<td>
<p>the variable identifying the cross-sectional dimension. Only needs to be specified, if data is not cross-sectional. Assumes that units do not change
their location over time.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_time">time</code></td>
<td>
<p>the variable identifying the time dimension. Only needs to be specified, if data is not cross-sectional.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_lat">lat</code></td>
<td>
<p>the variable specifying the latitude</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_lon">lon</code></td>
<td>
<p>the variable specifying the longitude</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_kernel">kernel</code></td>
<td>
<p>the kernel applied within the radius. Either <code>"bartlett"</code> (default) or <code>"uniform"</code>.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_lag_cutoff">lag_cutoff</code></td>
<td>
<p>the cutoff along the time dimension. Defaults to 0, meaning that standard errors are only adjusted cross-sectionally.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_intercept">intercept</code></td>
<td>
<p>logical specifying whether to include an intercept. Defaults to <code>TRUE</code>. Fixed effects models omit the intercept automatically.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_verbose">verbose</code></td>
<td>
<p>logical specifying whether to print messages on intermediate estimation steps. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_ncores">ncores</code></td>
<td>
<p>the number of CPU cores to use in the estimations. Defaults to the machine's number of CPUs.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_par_dim">par_dim</code></td>
<td>
<p>the dimension along which the function parallelizes in panel applications. Can be set to <code>"cross-section"</code> (default) or <code>"time"</code>. With
the former option, the function parallelizes the spatial correlation code in C++ using OpenMP and the serial correlation part in R using the parallel package.
With the latter option, it is the other way around. Use <code>"r"</code> and <code>"cpp"</code> to define parallelization based on the language rather than the dimension.
Some MAC users do not have access to OpenMP by default. <code>par_dim</code> is then always set to <code>"r"</code>. Thus, depending on the application, the function can be
notably faster on Windows and Linux than on MACs.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_dist_comp">dist_comp</code></td>
<td>
<p>choice between <code>"spherical"</code> and <code>"planar"</code> distance computations. When unspecified, the input data determines the method: longlat uses
spherical (Haversine) distances, alternatives (projected data) use planar (Euclidean) distances. When inserting projected data but specifying
<code>dist_comp = "spherical"</code>, the data is transformed to longlat. Combining unprojected data with <code>dist_comp = "planar"</code> transforms the data to an
azimuthal equidistant format centered at the data's centroid.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_crs">crs</code></td>
<td>
<p>the coordinate reference system, if the data is projected. Object of class crs or input string to <code>sf::st_crs</code>. This parameter can be omitted, if
the data is in longlat format (EPSG: 4326), i.e. not projected. If the projection does not use meters as units, this function converts to units to meters.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_st_distance">st_distance</code></td>
<td>
<p>logical specifying whether distances should be computed via <code>sf::st_distance</code> (<code>TRUE</code>) or via conleyreg's internal, computationally
optimized distance functions (<code>FALSE</code>). The default (<code>FALSE</code>) produces the same distances as <code>sf::st_distance</code> does with S2 enabled. I.e. it uses
Haversine (great circle) distances for longlat data and Euclidean distances otherwise. Cases in which you might want to set this argument to <code>TRUE</code> are e.g.
when you want enforce the GEOS approach to computing distances or when you are using an peculiar projection, for which the sf package might include further
procedures. Cross-sectional parallelization is not available when <code>st_distance = TRUE</code> and the function automatically switches to parallelization along the
time dimension, if the data is a panel and <code>ncores != 1</code>. Third and fourth dimensions, termed Z and M in sf, are not accounted for in any case. Note that
<code>sf::st_distance</code> is considerably slower than conleyreg's internal distance functions.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_dist_which">dist_which</code></td>
<td>
<p>the type of distance to use when <code>st_distance = TRUE</code>. If unspecified, the function defaults to great circle distances for longlat data and
to Euclidean distances otherwise. See <code>sf::st_distance</code> for options.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_sparse">sparse</code></td>
<td>
<p>logical specifying whether to use sparse rather than dense (regular) matrices in distance computations. Defaults to <code>FALSE</code>. Only has an effect
when <code>st_distance = FALSE</code>. Sparse matrices are more efficient than dense matrices, when the distance matrix has a lot of zeros arising from points located
outside the respective <code>dist_cutoff</code>. It is recommended to keep the default unless the machine is unable to allocate enough memory.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_batch">batch</code></td>
<td>
<p>logical specifying whether distances are inserted into a sparse matrix element by element (<code>FALSE</code>) or all at once as a batch (<code>TRUE</code>).
Defaults to <code>TRUE</code>. This argument only has an effect when <code>st_distance = FALSE</code> and <code>sparse = TRUE</code>. Batch insertion is faster than element-wise
insertion, but requires more memory.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_batch_ram_opt">batch_ram_opt</code></td>
<td>
<p>the degree to which batch insertion should be optimized for RAM usage. Can be set to one out of the three levels: <code>"none"</code>,
<code>"moderate"</code> (default), and <code>"heavy"</code>. Higher levels imply lower RAM usage, but also lower speeds.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_float">float</code></td>
<td>
<p>logical specifying whether distance matrices should use the float (<code>TRUE</code>) rather than the double (<code>FALSE</code>) data type. Floats are less
precise and than doubles and thereby occupy less space than doubles do. They should only be used when the machine's RAM is insufficient for both the dense and
the sparse matrix cases, as they affect the precision of distance values. The <code>float</code> option only has an effect in Bartlett kernel cases because uniform
kernel applications store the data in a smaller integer data type.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_rowwise">rowwise</code></td>
<td>
<p>logical specifying whether to store individual rows of the distance matrix only, instead of the full matrix. If <code>TRUE</code>, the function uses these
rows directly in the standard error correction. This option's advantage is that it induces the function to store only N x <code>ncores</code> cells, instead of the full
N x N matrix, lowering RAM requirements. The disadvantage is that the function needs to compute twice as many distance values as in the default case (<code>FALSE</code>),
since the symmetry of the matrix is not utilized. It hence sacrifices speed for lower RAM utilization. This parameter only has an effect in cross-sectional and
unbalanced panel applications with <code>st_distance = FALSE</code> and <code>sparse = FALSE</code>.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_reg_ram_opt">reg_ram_opt</code></td>
<td>
<p>logical specifying whether the regression should be optimized for RAM usage. Defaults to <code>FALSE</code>. Changing it to <code>TRUE</code> slows down
the function. This argument only affects the baseline estimation, not the standard error correction.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_dist_mat">dist_mat</code></td>
<td>
<p>a distance matrix. Pre-computing a distance matrix and passing it to this argument is only more efficient than having <code>conleyreg</code> derive it,
if you execute <code>conleyreg</code> multiple times with the same input data. In that case, it is recommended to compute the distance matrix via
<code><a href="#topic+dist_mat">dist_mat</a></code>, which is optimized for this purpose and also evaluates various other steps that are identical across regressions on the same
input data. Generally, you must not modify the input data between deriving the distance matrix and running <code>conleyreg</code>. That includes dropping observations
or changing values of the unit, time, or coordinate variables. In cross-sectional settings, you must not re-order rows either. If you compute distances through a
function other than <code><a href="#topic+dist_mat">dist_mat</a></code>, there are a few additional issues to account for. (i) In the panel scenario, you must order observations by
time and unit in ascending order. I.e. cells [1, 2] and [2, 1] of the distance matrix must refer to the distance between unit 1 and unit 2, cells [2, 3] and [3, 2]
to the distance between unit 2 and unit 3 etc. The unit numbers in this example refer to their rank when they are sorted. (ii) <code>dist_cutoff</code> does not refer to
kilometers, but to the units of the matrix. (iii) While in a balanced panel you only enter one matrix that is applied to all periods, you supply distances as a
list of matrices in the unbalanced case. The matrices must be sorted, with the first list element containing the first period's distance matrix etc. (iv) Zeros in
sparse matrices are interpreted as values above the distance cutoff and NaN values are interpreted as zeros. (v) The matrices in the list must all be of the same
type - all dense or all sparse. (vi) Distance matrices must only contain non-missing, finite numbers (and NaN in the case of sparse matrices).</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_dist_mat_conv">dist_mat_conv</code></td>
<td>
<p>logical specifying whether to convert the distance matrix to a list, if the panel turns out to be unbalanced because of missing values. This
setting is only relevant, if you enter a balanced panel's distance matrix not derived via <code><a href="#topic+dist_mat">dist_mat</a></code>. If <code>TRUE</code> (the default), the
function only drops rows with missing values. If <code>FALSE</code>, the function maintains the panel's balanced character by dropping units with missing values in at
least one period from the entire data set.</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_vcov">vcov</code></td>
<td>
<p>logical specifying whether to return variance-covariance matrix (<code>TRUE</code>) rather than the default <code>lmtest::coeftest</code> matrix of coefficient
estimates and standard errors (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="conleyreg_+3A_gof">gof</code></td>
<td>
<p>logical specifying whether to return goodness of fit measures. Defaults to <code>FALSE</code>. If <code>TRUE</code>, the function produces a list.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This code is an extension and modification of earlier Conley standard error implementations by (i) Richard Bluhm, (ii) Luis Calderon and Leander Heldring,
(iii) Darin Christensen and Thiemo Fetzer, and (iv) Timothy Conley. Results vary across implementations because of different distance functions and buffer shapes.
</p>
<p>This function has reasonable defaults. If your machine has insufficent RAM to allocate the default dense matrices, try sparse matrices. If the RAM error persists,
try setting a lower <code>dist_cutoff</code>, use floats, select a uniform kernel, experiment with <code>batch_ram_opt</code>, <code>reg_ram_opt</code>, or <code>batch</code>.
</p>
<p>Consult the vignette, <code>vignette("conleyreg_introduction", "conleyreg")</code>, for a more extensive discussion.
</p>


<h3>Value</h3>

<p>Returns a <code>lmtest::coeftest</code> matrix of coefficient estimates and standard errors by default. Can be changed to the variance-covariance matrix by
specifying <code>vcov = TRUE</code>. <code style="white-space: pre;">&#8288;

&#8288;</code>
</p>


<h3>References</h3>

<p>Calderon L, Heldring L (2020).
&ldquo;Spatial standard errors for several commonly used M-estimators.&rdquo;
Mimeo.<br /><br /> Conley TG (1999).
&ldquo;GMM estimation with cross sectional dependence.&rdquo;
<em>Journal of Econometrics</em>, <b>92</b>(1), 1-45.<br /><br /> Conley TG (2008).
&ldquo;Spatial Econometrics.&rdquo;
In Durlauf SN, Blume LE (eds.), <em>Microeconometrics</em>, 303-313.
London: Palgrave Macmillan.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Generate cross-sectional example data
data &lt;- rnd_locations(100, output_type = "data.frame")
data$y &lt;- sample(c(0, 1), 100, replace = TRUE)
data$x1 &lt;- stats::runif(100, -50, 50)

# Estimate ols model with Conley standard errors using a 1000 km radius
conleyreg(y ~ x1, data, 1000, lat = "lat", lon = "lon")

# Estimate logit model
conleyreg(y ~ x1, data, 1000, "logit", lat = "lat", lon = "lon")

# Estimate ols model with fixed effects
data$x2 &lt;- sample(1:5, 100, replace = TRUE)
conleyreg(y ~ x1 | x2, data, 1000, lat = "lat", lon = "lon")

# Estimate ols model using panel data
data$time &lt;- rep(1:10, each = 10)
data$unit &lt;- rep(1:10, times = 10)
conleyreg(y ~ x1, data, 1000, unit = "unit", time = "time", lat = "lat", lon = "lon")

# Estimate same model with an sf object of another projection as input
data &lt;- sf::st_as_sf(data, coords = c("lon", "lat"), crs = 4326) |&gt;
  sf::st_transform(crs = "+proj=aeqd")
conleyreg(y ~ x1, data, 1000)

## End(Not run)

</code></pre>

<hr>
<h2 id='dist_mat'>Distance matrix estimation</h2><span id='topic+dist_mat'></span>

<h3>Description</h3>

<p>This function estimates the distance matrix separately from Conley standard errors. Such step can be helpful when running multiple Conley standard error estimations
based on the same distance matrix. A pre-requisite of using this function is that the data must not be modified between applying this function and inserting the
results into <code>conleyreg</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist_mat(
  data,
  unit = NULL,
  time = NULL,
  lat = NULL,
  lon = NULL,
  dist_comp = NULL,
  dist_cutoff = NULL,
  crs = NULL,
  verbose = TRUE,
  ncores = NULL,
  par_dim = c("cross-section", "time", "r", "cpp"),
  sparse = FALSE,
  batch = TRUE,
  batch_ram_opt = NULL,
  dist_round = FALSE,
  st_distance = FALSE,
  dist_which = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist_mat_+3A_data">data</code></td>
<td>
<p>input data. Either (i) in non-spatial data frame format (includes tibbles and data tables) with columns denoting coordinates or (ii) in sf format. In
case of an sf object, all non-point geometry types are converted to spatial points, based on the feature's centroid. When using a non-spatial data frame format
the with projected, i.e. non-longlat, coordinates, <code>crs</code> must be specified. Note that the projection can influence the computed distances, which is a general
phenomenon in GIS software and not specific to conleyreg. The computationally fastest option is to use a data table with coordinates in the crs in which
the distances are to be derived (longlat for spherical and projected for planar), and with time and unit set as keys in the panel case. An sf object as input is
the slowest option.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_unit">unit</code></td>
<td>
<p>the variable identifying the cross-sectional dimension. Only needs to be specified, if data is not cross-sectional. Assumes that units do not change
their location over time.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_time">time</code></td>
<td>
<p>the variable identifying the time dimension. Only needs to be specified, if data is not cross-sectional.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_lat">lat</code></td>
<td>
<p>the variable specifying the latitude</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_lon">lon</code></td>
<td>
<p>the variable specifying the longitude</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_dist_comp">dist_comp</code></td>
<td>
<p>choice between <code>spherical</code> and <code>planar</code> distance computations. When unspecified, the input data determines the method: longlat uses
spherical (Haversine) distances, alternatives (projected data) use planar (Euclidean) distances. When inserting projected data but specifying
<code>dist_comp = "spherical"</code>, the data is transformed to longlat. Combining unprojected data with <code>dist_comp = "planar"</code> transforms the data to an
azimuthal equidistant format centered at the data's centroid.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_dist_cutoff">dist_cutoff</code></td>
<td>
<p>the distance cutoff in km. If not specified, the distances matrices contain all bilateral distances. If specified, the cutoff most be as least
as large as the largest distance cutoff in the Conley standard error corrections in which you use the resulting matrix. If you e.g. specify distance cutoffs of
100, 200, and 500 km in the subsequent <code>conleyreg</code> calls, <code>dist_cutoff</code> in this function must be set to at least 500. <code>dist_cutoff</code>
allows to pre-compute distance matrices also in applications where a full distance matrix would not fit into the computer's memory - conditional on that
<code>sparse = TRUE</code>.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_crs">crs</code></td>
<td>
<p>the coordinate reference system, if the data is projected. Object of class crs or input string to <code>sf::st_crs</code>. This parameter can be omitted, if
the data is in longlat format (EPSG: 4326), i.e. not projected. If the projection does not use meters as units, this function converts to units to meters.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_verbose">verbose</code></td>
<td>
<p>logical specifying whether to print messages on intermediate estimation steps. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_ncores">ncores</code></td>
<td>
<p>the number of CPU cores to use in the estimations. Defaults to the machine's number of CPUs.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_par_dim">par_dim</code></td>
<td>
<p>the dimension along which the function parallelizes in unbalanced panel applications. Can be set to <code>"cross-section"</code> (default) or
<code>"time"</code>. Use <code>"r"</code> and <code>"cpp"</code> to define parallelization based on the language rather than the dimension. In this function, <code>"r"</code> is
equivalent to <code>"time"</code> and parallelizes along the time dimension using the parallel package. <code>"cross-section"</code> is equivalent to <code>"cpp"</code> and
parallelizes along the cross-sectional dimension using OpenMP in C++. Some MAC users do not have access to OpenMP by default. <code>par_dim</code> is then always set to
<code>"r"</code>. Thus, depending on the application, the function can be notably faster on Windows and Linux than on MACs. When <code>st_distance = TRUE</code>, <code>par_dim</code>
defaults to <code>"time"</code>.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_sparse">sparse</code></td>
<td>
<p>logical specifying whether to use sparse rather than dense (regular) matrices in distance computations. Defaults to <code>FALSE</code>. Only has an effect
when <code>st_distance = FALSE</code>. Sparse matrices are more efficient than dense matrices, when the distance matrix has a lot of zeros arising from points located
outside the respective <code>dist_cutoff</code>. It is recommended to keep the default unless the machine is unable to allocate enough memory. The function always uses
dense matrices when <code>dist_cutoff</code> is not specified.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_batch">batch</code></td>
<td>
<p>logical specifying whether distances are inserted into a sparse matrix element by element (<code>FALSE</code>) or all at once as a batch (<code>TRUE</code>).
Defaults to <code>FALSE</code>. This argument only has an effect when <code>st_distance = FALSE</code> and <code>sparse = TRUE</code>. Batch insertion is faster than element-wise
insertion, but requires more memory.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_batch_ram_opt">batch_ram_opt</code></td>
<td>
<p>the degree to which batch insertion should be optimized for RAM usage. Can be set to one out of the three levels: <code>"none"</code>,
<code>"moderate"</code> (default), and <code>"heavy"</code>. Higher levels imply lower RAM usage, but also lower speeds.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_dist_round">dist_round</code></td>
<td>
<p>logical specifying whether to round distances to full kilometers. This further reduces memory consumption and can be a solution when even sparse
matrices cannot accomodate the data. Note, though, that this rounding introduces a bias.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_st_distance">st_distance</code></td>
<td>
<p>logical specifying whether distances should be computed via <code>sf::st_distance</code> (<code>TRUE</code>) or via conleyreg's internal, computationally
optimized distance functions (<code>FALSE</code>). The default (<code>FALSE</code>) produces the same distances as <code>sf::st_distance</code> does with S2 enabled. I.e. it uses
Haversine (great circle) distances for longlat data and Euclidean distances otherwise. Cases in which you might want to set this argument to <code>TRUE</code> are e.g.
when you want enforce the GEOS approach to computing distances or when you are using a peculiar projection, for which the sf package might include further
procedures. Cross-sectional parallelization is not available when <code>st_distance = TRUE</code> and the function automatically switches to parallelization along the
time dimension, if the data is a panel and <code>ncores != 1</code>. Third and fourth dimensions, termed Z and M in sf, are not accounted for in any case. Note that
<code>sf::st_distance</code> is considerably slower than conleyreg's internal distance functions.</p>
</td></tr>
<tr><td><code id="dist_mat_+3A_dist_which">dist_which</code></td>
<td>
<p>the type of distance to use when <code>st_distance = TRUE</code>. If unspecified, the function defaults to great circle distances for longlat data and
to Euclidean distances otherwise. See <code>sf::st_distance</code> for options.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function runs the distance matrix estimations separately from the Conley standard error correction. You can pass the resulting object to the
<code>dist_mat</code> argument in <code>conleyreg</code>, skipping the distance matrix computations and various checks in that function. Pre-computing the distance matrix
is only more efficient than deriving it via <code>conleyreg</code> when estimating various models that use the same distance matrices. The input data must not be
modified between calling this function and inserting the results into <code>conleyreg</code>. Do not reorder the observations, add or delete variables, or undertake
any other operation on the data.
</p>


<h3>Value</h3>

<p>Returns an object of S3 class <code>conley_dist</code>. It contains modified distance matrices, the used <code>dist_cutoff</code>, a sparse matrix identifier, and
information on the potential panel structure. In the cross-sectional case and the balanced panel case, the distances are stored in one matrix, while in unbalanced
panel applications, distances come as a list of matrices. The function optimizes the distance matrices with respect to computational performance, setting
distances beyond <code>dist_cutoff</code> to zero and actual off-diagonal zeros to NaN. Hence, these objects are only to be used in <code>conleyreg</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Generate cross-sectional example data
data &lt;- rnd_locations(100, output_type = "data.frame")
data$y &lt;- sample(c(0, 1), 100, replace = TRUE)
data$x1 &lt;- stats::runif(100, -50, 50)

# Compute distance matrix in cross-sectional case
dm &lt;- dist_mat(data, lat = "lat", lon = "lon")

# Compute distance matrix in panel case
data$time &lt;- rep(1:10, each = 10)
data$unit &lt;- rep(1:10, times = 10)
dm &lt;- dist_mat(data, unit = "unit", time = "time", lat = "lat", lon = "lon")

# Use distance matrix in conleyreg function
conleyreg(y ~ x1, data, 1000, dist_mat = dm)

## End(Not run)

</code></pre>

<hr>
<h2 id='rnd_locations'>Random location drawing</h2><span id='topic+rnd_locations'></span>

<h3>Description</h3>

<p>This function draws random locations in longlat format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnd_locations(
  nobs,
  xmin = -180,
  xmax = 180,
  ymin = -90,
  ymax = 90,
  output_type = c("data.table", "data.frame", "sf")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rnd_locations_+3A_nobs">nobs</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code id="rnd_locations_+3A_xmin">xmin</code></td>
<td>
<p>minimum longitude</p>
</td></tr>
<tr><td><code id="rnd_locations_+3A_xmax">xmax</code></td>
<td>
<p>maximum longitude</p>
</td></tr>
<tr><td><code id="rnd_locations_+3A_ymin">ymin</code></td>
<td>
<p>minimum latitude</p>
</td></tr>
<tr><td><code id="rnd_locations_+3A_ymax">ymax</code></td>
<td>
<p>maximum latitude</p>
</td></tr>
<tr><td><code id="rnd_locations_+3A_output_type">output_type</code></td>
<td>
<p>type of output object. Either <code>"data.table"</code> (default), <code>"data.frame"</code>, or <code>"sf"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, this function draws a global sample of random locations. You can restrict it to a certain region via specifying <code>xmin</code>, <code>xmax</code>, <code>ymin</code>,
and <code>ymax</code>. The function draws from a uniform spatial distribution that assumes the planet to be a perfect sphere. The spherical assumption is common in GIS
functions, but deviates from Earth's exact shape.
</p>


<h3>Value</h3>

<p>Returns a data.table, data.frame, or sf object of unprojected (longlat) points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- rnd_locations(1000)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
