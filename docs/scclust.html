<!DOCTYPE html><html lang="en"><head><title>Help for package scclust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {scclust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#scclust-package'><p>scclust: Size-Constrained Clustering</p></a></li>
<li><a href='#check_clustering'><p>Check clustering constraints</p></a></li>
<li><a href='#cluster_count'><p>Count the number of clusters</p></a></li>
<li><a href='#get_clustering_stats'><p>Get clustering statistics</p></a></li>
<li><a href='#hierarchical_clustering'><p>Hierarchical size-constrained clustering</p></a></li>
<li><a href='#is.scclust'><p>Check scclust object</p></a></li>
<li><a href='#sc_clustering'><p>Size-constrained clustering</p></a></li>
<li><a href='#scclust'><p>Constructor for scclust objects</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Size-Constrained Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-07-31</td>
</tr>
<tr>
<td>Description:</td>
<td>
    Provides wrappers for 'scclust', a C library for computationally efficient
    size-constrained clustering with near-optimal performance.
    See <a href="https://github.com/fsavje/scclust">https://github.com/fsavje/scclust</a> for more information.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4.0), distances</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>LicenseNote:</td>
<td>The scclust package includes the scclust C library
(distributed under the LGPLv2.1 license).</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/fsavje/scclust-R">https://github.com/fsavje/scclust-R</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/fsavje/scclust-R/issues">https://github.com/fsavje/scclust-R/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-07-31 16:11:53 UTC; fredriksavje</td>
</tr>
<tr>
<td>Author:</td>
<td>Fredrik Savje [aut, cre],
  Michael Higgins [aut],
  Jasjeet Sekhon [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Fredrik Savje &lt;rpackages@fredriksavje.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-07-31 22:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='scclust-package'>scclust: Size-Constrained Clustering</h2><span id='topic+scclust-package'></span>

<h3>Description</h3>

<p>The <code>scclust</code> package is an R wrapper for the <code>scclust</code> library.
The package provides functions to construct near-optimal size-constrained
clusterings. Subject to user-specified constraints on the size and composition
of the clusters, <code>scclust</code> constructs a clustering so that within-cluster
pair-wise distances are minimized.
</p>


<h3>Details</h3>

<p>The main clustering function is <code><a href="#topic+sc_clustering">sc_clustering</a></code>. Statistics about
clusters can be derived with the <code><a href="#topic+get_clustering_stats">get_clustering_stats</a></code>
function. To check if a clustering satisfies some set of
constraints, use <code><a href="#topic+check_clustering">check_clustering</a></code>. Use <code><a href="#topic+scclust">scclust</a></code> to
construct a <code>scclust</code> object from an existing clustering.
</p>
<p>Clusters can also be constructed with <code><a href="#topic+hierarchical_clustering">hierarchical_clustering</a></code>.
However, this function does not support type constraints and does not provide
optimality guarantees. Its main use is to refine clusterings constructed with
the <code><a href="#topic+sc_clustering">sc_clustering</a></code> function.
</p>
<p><code>scclust</code> was made with large data sets in mind, and it can cluster tens
of millions of data points within minutes on an ordinary desktop computer.
</p>
<p>See the package's website for more information:
<a href="https://github.com/fsavje/scclust-R">https://github.com/fsavje/scclust-R</a>.
</p>
<p>More information about the <code>scclust</code> library is found here:
<a href="https://github.com/fsavje/scclust">https://github.com/fsavje/scclust</a>.
</p>
<p>Bug reports and suggestions are greatly appreciated. They are best reported
here: <a href="https://github.com/fsavje/scclust-R/issues">https://github.com/fsavje/scclust-R/issues</a>.
</p>


<h3>References</h3>

<p>Higgins, Michael J., Fredrik Sävje and Jasjeet S. Sekhon (2016),
&lsquo;Improving massive experiments with threshold blocking&rsquo;,
<em>Proceedings of the National Academy of Sciences</em>, <b>113:27</b>, 7369&ndash;7376.
</p>
<p>Sävje, Fredrik and Michael J. Higgins and Jasjeet S. Sekhon (2017),
&lsquo;Generalized Full Matching&rsquo;, arXiv 1703.03882.
<a href="https://arxiv.org/abs/1703.03882">https://arxiv.org/abs/1703.03882</a>
</p>

<hr>
<h2 id='check_clustering'>Check clustering constraints</h2><span id='topic+check_clustering'></span>

<h3>Description</h3>

<p><code>check_clustering</code> checks whether a clustering satisfies constraints on
the size and composition of the clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_clustering(
  clustering,
  size_constraint = NULL,
  type_labels = NULL,
  type_constraints = NULL,
  primary_data_points = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_clustering_+3A_clustering">clustering</code></td>
<td>
<p>a <code><a href="#topic+scclust">scclust</a></code> object containing a non-empty clustering.</p>
</td></tr>
<tr><td><code id="check_clustering_+3A_size_constraint">size_constraint</code></td>
<td>
<p>an integer with the required minimum cluster size. If <code>NULL</code>, only
the type constraints will be checked.</p>
</td></tr>
<tr><td><code id="check_clustering_+3A_type_labels">type_labels</code></td>
<td>
<p>a vector containing the type of each data point. May be <code>NULL</code> when
<code>type_constraints</code> is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="check_clustering_+3A_type_constraints">type_constraints</code></td>
<td>
<p>a named integer vector containing type-specific size constraints. If
<code>NULL</code>, only the overall constraint will be checked.</p>
</td></tr>
<tr><td><code id="check_clustering_+3A_primary_data_points">primary_data_points</code></td>
<td>
<p>a vector specifying primary data points, either by point indices or with
a logical vector of length equal to the number of points.
<code>check_clustering</code> checks so all primary data points are assigned
to a cluster. <code>NULL</code> indicates that no such check should be done.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns <code>TRUE</code> if <code>clustering</code> satisfies the constraints, and
<code>FALSE</code> if it does not. Throws an error if <code>clustering</code> is an
invalid instance of the <code><a href="#topic+scclust">scclust</a></code> class.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+sc_clustering">sc_clustering</a></code> for details on how to specify the
<code>type_labels</code> and <code>type_constraints</code> parameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example scclust clustering
my_scclust &lt;- scclust(c("A", "A", "B", "C", "B",
                        "C", "C", "A", "B", "B"))


# Check so each cluster contains at least two data points
check_clustering(my_scclust, 2)
# &gt; TRUE


# Check so each cluster contains at least four data points
check_clustering(my_scclust, 4)
# &gt; FALSE


# Data point types
my_types &lt;- factor(c("x", "y", "y", "z", "z",
                     "x", "y", "z", "x", "x"))


# Check so each cluster contains at least one point of each type
check_clustering(my_scclust,
                 NULL,
                 my_types,
                 c("x" = 1, "y" = 1, "z" = 1))
# &gt; TRUE


# Check so each cluster contains one data point of both "x" and "z"
# and at least three points in total
check_clustering(my_scclust,
                 3,
                 my_types,
                 c("x" = 1, "z" = 1))
# &gt; TRUE


# Check so each cluster contains five data points of type "y"
check_clustering(my_scclust,
                 NULL,
                 my_types,
                 c("y" = 5))
# &gt; FALSE

</code></pre>

<hr>
<h2 id='cluster_count'>Count the number of clusters</h2><span id='topic+cluster_count'></span>

<h3>Description</h3>

<p><code>cluster_count</code> returns the number of clusters in a clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_count(clustering)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cluster_count_+3A_clustering">clustering</code></td>
<td>
<p>a <code><a href="#topic+scclust">scclust</a></code> object containing a non-empty clustering.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an integer with the number of clusters in <code>clustering</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example scclust clustering
my_scclust &lt;- scclust(c("A", "A", "B", "C", "B",
                        "C", "C", "A", "B", "B"))

cluster_count(my_scclust)
# &gt; 3

</code></pre>

<hr>
<h2 id='get_clustering_stats'>Get clustering statistics</h2><span id='topic+get_clustering_stats'></span>

<h3>Description</h3>

<p><code>get_clustering_stats</code> calculates statistics of a clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_clustering_stats(distances, clustering)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_clustering_stats_+3A_distances">distances</code></td>
<td>
<p>a <code><a href="distances.html#topic+distances">distances</a></code> object describing the distances
between the data points in <code>clustering</code>.</p>
</td></tr>
<tr><td><code id="get_clustering_stats_+3A_clustering">clustering</code></td>
<td>
<p>a <code><a href="#topic+scclust">scclust</a></code> object containing a non-empty clustering.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function reports the following measures:
</p>

<table>
<tr>
 <td style="text-align: left;">
  <code>num_data_points</code> </td><td style="text-align: left;"> total number of data points </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>num_assigned</code> </td><td style="text-align: left;"> number of points assigned to a cluster </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>num_clusters</code> </td><td style="text-align: left;"> number of clusters </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>min_cluster_size</code> </td><td style="text-align: left;"> size of the smallest cluster </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>max_cluster_size</code> </td><td style="text-align: left;"> size of the largest cluster </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>avg_cluster_size</code> </td><td style="text-align: left;">  average cluster size </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>sum_dists</code> </td><td style="text-align: left;"> sum of all within-cluster distances </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>min_dist</code> </td><td style="text-align: left;"> smallest within-cluster distance </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>max_dist</code> </td><td style="text-align: left;"> largest within-cluster distance </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>avg_min_dist</code> </td><td style="text-align: left;"> average of the clusters' smallest distances </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>avg_max_dist</code> </td><td style="text-align: left;"> average of the clusters' largest distances </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>avg_dist_weighted</code> </td><td style="text-align: left;"> average of the clusters' average distances
     weighed by cluster size </td>
</tr>
<tr>
 <td style="text-align: left;">
  <code>avg_dist_unweighted</code> </td><td style="text-align: left;"> average of the clusters' average distances
     (unweighed)  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Let <code class="reqn">d(i,j)</code> denote the distance between data points <code class="reqn">i</code>
and <code class="reqn">j</code>. Let <code class="reqn">c</code> be a cluster containing the indices of points
assigned to the cluster. Let </p>
<p style="text-align: center;"><code class="reqn">D(c) = \{d(i,j): i,j \in c \wedge i&gt;j\}</code>
</p>

<p>be a function returning all within-cluster distances in <code class="reqn">c</code>. Let
<code class="reqn">C</code> be a set containing all clusters.
</p>
<p><code>sum_dists</code> is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\sum_{c\in C} sum(D(c))</code>
</p>

<p><code>min_dist</code> is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\min_{c\in C} \min(D(c))</code>
</p>

<p><code>max_dist</code> is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\max_{c\in C} \max(D(c))</code>
</p>

<p><code>avg_min_dist</code> is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\sum_{c\in C} \frac{\min(D(c))}{|C|}</code>
</p>

<p><code>avg_max_dist</code> is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\sum_{c\in C} \frac{\max(D(c))}{|C|}</code>
</p>

<p>Let:
</p>
<p style="text-align: center;"><code class="reqn">AD(c) = \frac{sum(D(c))}{|D(c)|}</code>
</p>

<p>be the average within-cluster distance in cluster <code class="reqn">c</code>.
</p>
<p><code>avg_dist_weighted</code> is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\sum_{c\in C} \frac{|c| AD(c)}{num_assigned}</code>
</p>

<p>where <code class="reqn">num_assigned</code> is the number of assigned data
points (see above).
</p>
<p><code>avg_dist_unweighted</code> is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\sum_{c\in C} \frac{AD(c)}{|C|}</code>
</p>



<h3>Value</h3>

<p>Returns a list of class <code>clustering_stats</code> containing the statistics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>my_data_points &lt;- data.frame(x = c(0.1, 0.2, 0.3, 0.4, 0.5,
                                   0.6, 0.7, 0.8, 0.9, 1.0),
                             y = c(10, 9, 8, 7, 6,
                                   10, 9, 8, 7, 6))

my_distances &lt;- distances(my_data_points)

my_scclust &lt;- scclust(c("A", "A", "B", "C", "B",
                        "C", "C", "A", "B", "B"))

get_clustering_stats(my_distances, my_scclust)

# &gt;                     Value
# &gt; num_data_points     10.0000000
# &gt; num_assigned        10.0000000
# &gt; num_clusters         3.0000000
# &gt; min_cluster_size     3.0000000
# &gt; max_cluster_size     4.0000000
# &gt; avg_cluster_size     3.3333333
# &gt; sum_dists           18.2013097
# &gt; min_dist             0.5000000
# &gt; max_dist             3.0066593
# &gt; avg_min_dist         0.8366584
# &gt; avg_max_dist         2.4148611
# &gt; avg_dist_weighted    1.5575594
# &gt; avg_dist_unweighted  1.5847484

</code></pre>

<hr>
<h2 id='hierarchical_clustering'>Hierarchical size-constrained clustering</h2><span id='topic+hierarchical_clustering'></span>

<h3>Description</h3>

<p><code>hierarchical_clustering</code> serves two purposes. Its primary use is to
refine existing clusters subject to clustering constraints. That is, given
a (non-optimal) clustering satisfying some constraints, the function splits
clusters so to decrease within-cluster distances without violating the
constraints. The function can also be used to derive size-constrained
clusterings from scratch. In both cases, it uses a hierarchical clustering
algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hierarchical_clustering(
  distances,
  size_constraint,
  batch_assign = TRUE,
  existing_clustering = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hierarchical_clustering_+3A_distances">distances</code></td>
<td>
<p>a <code><a href="distances.html#topic+distances">distances</a></code> object with distances between the
data points.</p>
</td></tr>
<tr><td><code id="hierarchical_clustering_+3A_size_constraint">size_constraint</code></td>
<td>
<p>an integer with the required minimum cluster size.</p>
</td></tr>
<tr><td><code id="hierarchical_clustering_+3A_batch_assign">batch_assign</code></td>
<td>
<p>a logical indicating whether data points should be assigned in batches
when spliting clusters (see below for details).</p>
</td></tr>
<tr><td><code id="hierarchical_clustering_+3A_existing_clustering">existing_clustering</code></td>
<td>
<p>a <code><a href="#topic+scclust">scclust</a></code> object containing a non-empty clustering to
refine. If <code>NULL</code>, the function derives a clustering from scratch.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>While <code>hierarchical_clustering</code> can be used to derive size-constrained
clusters from scratch, its main purpose is to be used together with
<code><a href="#topic+sc_clustering">sc_clustering</a></code>. The clusters produced by the main clustering
function are guaranteed to be close to optimal (in particular, within a
constant factor of the optimal solution). However, it is occasionally
possible to refine the clustering. In particular,
<code><a href="#topic+sc_clustering">sc_clustering</a></code> tends produce large clusters in regions of the
metric space with many data points. In some cases, it is beneficial to divide
these clusters into smaller groups. <code>hierarchical_clustering</code> splits
these larger clusters so that all within-cluster distances weakly decrease
while respecting the overall the size constraint.
</p>
<p><code>hierarchical_clustering</code> implements a divisive hierarchical clustering
algorithm that respect size constraints. Starting from any clustering
satisfying the size constraints (which may be a single cluster containing
all data points), the function searches for clusters that can be broken into
two or more new clusters without violating the constraints. When such a
cluster is found, it breaks the cluster into two new clusters. It continues
in this fashion until all remaining clusters are unbreakable.
</p>
<p>Breakable clusters are broken in three stages. First, it tries to find two
data points as far as possible from each other. The two points are called
<em>centers</em>, and they are the starting points for the new clusters. The
remaining data points in the old cluster will be assigned to one of the
centers. In the second stage, each center picks the closest data points so
that the new two clusters exactly satisfy the size constraint. In the last
stage, data points that still are in the old cluster are assigned to the
cluster containing the closest center. The final stage is done either for
each point one-by-one or in batches (see below). When all data points are
assigned to a cluster, the old cluster is removed and the two new are added
to the clustering. If one or both of the new clusters are breakable, they
will go through the same procedure again.
</p>
<p>In some applications, it is desirable to avoid clusters that contain a number
of data points that are not multiples of <code>size_constraint</code>. After the
second stage described in the previous paragraph, both partial clusters are
exact multiples of the size constraint. By assigning remaining data points
in the third stage in batches of <code>size_constraint</code>, the function ensures,
to the greatest extent possible, that the size of the final clusters are
multiples of the size constraint.
</p>


<h3>Value</h3>

<p>Returns a <code><a href="#topic+scclust">scclust</a></code> object with the derived clustering.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sc_clustering">sc_clustering</a></code> is the main clustering function in the
package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Make example data
my_data &lt;- data.frame(x1 = rnorm(10000),
                      x2 = rnorm(10000),
                      x3 = rnorm(10000))

# Construct distance metric
my_dist &lt;- distances(my_data)

# Make clustering with `sc_clustering`
my_clustering &lt;- sc_clustering(my_dist, 3)

# Refine clustering with `hierarchical_clustering`
my_refined_clustering &lt;- hierarchical_clustering(my_dist,
                                                 size_constraint = 3,
                                                 existing_clustering = my_clustering)

# Make clustering from scratch with `hierarchical_clustering`
my_other_clustering &lt;- hierarchical_clustering(my_dist, 3)

</code></pre>

<hr>
<h2 id='is.scclust'>Check scclust object</h2><span id='topic+is.scclust'></span>

<h3>Description</h3>

<p><code>is.scclust</code> checks whether the provided object is a valid instance of
the <code><a href="#topic+scclust">scclust</a></code> class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.scclust(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.scclust_+3A_x">x</code></td>
<td>
<p>object to check.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>is.scclust</code> does not check whether the clustering itself is sensible
or whether the clustering satisfies some set of constraints. See
<code><a href="#topic+check_clustering">check_clustering</a></code> for that functionality.
</p>


<h3>Value</h3>

<p>Returns <code>TRUE</code> if <code>x</code> is a valid <code><a href="#topic+scclust">scclust</a></code> object,
otherwise <code>FALSE</code>.
</p>

<hr>
<h2 id='sc_clustering'>Size-constrained clustering</h2><span id='topic+sc_clustering'></span>

<h3>Description</h3>

<p><code>sc_clustering</code> constructs near-optimal size-constrained clusterings.
Subject to user-specified constraints on the size and composition of the
clusters, a clustering is constructed so that within-cluster pair-wise
distances are minimized. The function does not restrict the number of
clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sc_clustering(
  distances,
  size_constraint = NULL,
  type_labels = NULL,
  type_constraints = NULL,
  seed_method = "inwards_updating",
  primary_data_points = NULL,
  primary_unassigned_method = "closest_seed",
  secondary_unassigned_method = "ignore",
  seed_radius = NULL,
  primary_radius = "seed_radius",
  secondary_radius = "estimated_radius",
  batch_size = 100L
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sc_clustering_+3A_distances">distances</code></td>
<td>
<p>a <code><a href="distances.html#topic+distances">distances</a></code> object with distances between the
data points.</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_size_constraint">size_constraint</code></td>
<td>
<p>an integer with the required minimum cluster size.</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_type_labels">type_labels</code></td>
<td>
<p>a vector containing the type of each data point. May be <code>NULL</code> when
<code>type_constraints</code> is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_type_constraints">type_constraints</code></td>
<td>
<p>a named integer vector containing type-specific size constraints. If
<code>NULL</code>, only the overall constraint given by <code>size_constraint</code>
will be imposed on the clustering.</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_seed_method">seed_method</code></td>
<td>
<p>a character scalar indicating how seeds should be selected.</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_primary_data_points">primary_data_points</code></td>
<td>
<p>a vector specifying primary data points, either with point indices or with
a logical vector of length equal to the number of points. <code>NULL</code>
indicates that all data points are &quot;primary&quot;.</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_primary_unassigned_method">primary_unassigned_method</code></td>
<td>
<p>a character scalar indicating how unassigned (primary) points should be
assigned to clusters.</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_secondary_unassigned_method">secondary_unassigned_method</code></td>
<td>
<p>a character scalar indicating how unassigned secondary points should be
assigned to clusters.</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_seed_radius">seed_radius</code></td>
<td>
<p>a positive numeric scalar restricting the maximum length of an edge in the
graph used to construct the clustering. <code>NULL</code> indicates no restriction.
This parameter (together with <code>primary_radius</code> and <code>secondary_radius</code>)
can be used to control the maximum distance between points assigned to the
same cluster (see below for details).</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_primary_radius">primary_radius</code></td>
<td>
<p>a positive numeric scalar, a character scalar or NULL restricting the match
distance for unassigned primary points. If numeric, the value is used to
restrict the distances. If character, it must be one of &quot;no_radius&quot;,
&quot;seed_radius&quot; or &quot;estimated_radius&quot; (see below for details). <code>NULL</code>
indicates no radius restriction.</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_secondary_radius">secondary_radius</code></td>
<td>
<p>a positive numeric scalar, a character scalar or NULL restricting the match
distance for unassigned secondary points. If numeric, the value is used to
restrict the distances. If character, it must be one of &quot;no_radius&quot;,
&quot;seed_radius&quot; or &quot;estimated_radius&quot; (see below for details). <code>NULL</code>
indicates no radius restriction.</p>
</td></tr>
<tr><td><code id="sc_clustering_+3A_batch_size">batch_size</code></td>
<td>
<p>an integer scalar specifying batch size when <code>seed_method</code> is set to
&quot;batches&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sc_clustering</code> constructs a clustering so to minimize within-cluster
dissimilarities while ensuring that constraints on the size and composition
of the clusters are satisfied. It is possible to impose an overall size
constraint so that each cluster must contain at least a certain number of
points in total. It is also possible to impose constraints on the composition
of the clusters so that each cluster must contain a certain number of points
of different types. For example, in a sample with &quot;red&quot; and &quot;blue&quot; data
points, one can constrain the clustering so that each cluster must contain
at least 10 points in total of which at least 3 must be &quot;red&quot; and at least
2 must be &quot;blue&quot;.
</p>
<p>The function implements an algorithm that first summaries the distances
between data points in a sparse graph and then constructs the clustering
based on the graph. This admits fast execution while ensuring near-optimal
performance. In particular, the maximum within-cluster distance is garantueed
to be at most four times the maximum distance in the optimal clustering.
The average performance is much closer to optimal than the worst case bound.
</p>
<p>In more detail, the clustering algorithm has four steps:
</p>

<ol>
<li><p> Construct sparse graph encoding clustering constraints.
</p>
</li>
<li><p> Select a set of vertices in the graph to act as &quot;seeds&quot;.
</p>
</li>
<li><p> Construct initial clusters from the seeds' neighborhoods in the graph.
</p>
</li>
<li><p> Assign remaining data points to clusters.
</p>
</li></ol>

<p>Each data point is represented by a vertex in the sparse graph, and arcs are
weighted by the distance between the data points they connect. The graph is
constructed so that each vertex's neighborhood satisfies the clustering
constraints supplied by the user. For example, if the clusters must contain
at least five points, each closed neighborhood in the graph will contain five
vertices. <code>sc_clustering</code> constructs the graph that minimize the arc
weights subject to the neighborhood constraints; this ensures near-optimal
performance. The function selects &quot;seeds&quot; that have non-overlapping
neighborhood in the graph. By constructing clusters as supersets of the
neighborhoods, it ensures that the clustering will satisfy the constraints
imposed by the user.
</p>
<p>The <code>seed_method</code> option governs how the seeds are chosen. Any set of
seeds yields a near-optimal clustering, but, heuristically, performance can
be improved by picking the seeds more carefully. In most cases, smaller
clusters are desirable since they tend to minimize within-cluster distances.
As the number of data points is fixed, we minimize the cluster size by
maximizing the number of clusters (and, thus, the number of seeds). When
<code>seed_method</code> is set to &quot;lexical&quot;, seeds are chosen in lexical order.
This admits a fast solution, but the function might pick points that are
central in the graph. Central points tend to exclude many other points from
being seeds and, thus, lead to larger clusters.
</p>
<p>The &quot;exclusion_order&quot; and &quot;exclusion_updating&quot; options calculate for each
vertex how many other vertices are excluded if the vertex is picked as a seed.
By picking seeds that exclude few other vertices, the function avoids
central points and increases the number of seeds. &quot;exclusion_updating&quot;
updates the count after each picked seed so that already excluded vertices
are not counted twice; &quot;exclusion_order&quot; derives the count once. The former
option is, thus, better-performing but slower.
</p>
<p>Deriving the exclusion count when using the &quot;exclusion_order&quot; and &quot;exclusion_updating&quot;
options is an expensive operation, and it might not be feasible to do so in
large samples. This is particularly problematic when the data points have
equidistant nearest neighbors, which tends to happen when the dataset contains
only discrete variables. It also happens when the dataset contains many
identical data points. The exclusion count operation may in these cases take
several orders of magnitude longer to run than the rest of the operations
combined. To ensure sane behavior for the default options, the exclusion
count is not used by default. If the dataset contains at least one continuous
variable, it is generally safe to call <code>sc_clustering</code> with either
&quot;exclusion_order&quot; or &quot;exclusion_updating&quot;, and this will often improve performance
over the default.
</p>
<p>The &quot;inwards_order&quot; and &quot;inwards_updating&quot; options count the number of
inwards-pointing arcs in the graph, which approximates the exclusion count.
&quot;inwards_updating&quot; updates the count after each picked seed, while
&quot;inwards_order&quot; derives the count once. The inwards counting options work
well with nearly all types of data, and &quot;inwards_updating&quot; is the default.
</p>
<p>The &quot;batches&quot; option is identical to &quot;lexical&quot; but it derives the graph in
batches. This limits the use of memory to a value proportional to
<code>batch_size</code> irrespectively of the size of the dataset. This can be
useful when imposing large size constraints, which consume a lot of memory
in large datasets. The &quot;batches&quot; option is still experimental and can currently
only be used when one does not impose type constraints.
</p>
<p>Once the function has selected seeds so that no additional points can be
selected without creating overlap in the graph, it constructs the initial
clusters. A unique cluster label is assigned to each seed, and all points
in the seed's neighborhood is assigned the same label. Some
data points might not be in a neighborhood of a seed and will, therefore,
not be assigned to a cluster after the initial clusters have been formed.
<code>primary_unassigned_method</code> specifies how the unassigned points are
assigned. With the &quot;ignore&quot; option, the points are left unassigned. With the
&quot;any_neighbor&quot;, the function re-uses the sparse graph and assigns the
unassigned points to any adjacent cluster in the graph. The &quot;closest_assigned&quot;
option assigns the points to the clusters that contain their closest assigned
vertex, and the &quot;closest_seed&quot; option assigns to the cluster that contain
the closest seed. All these options ensure that the clustering is near-optimal.
</p>
<p>Occasionally, some data points are allowed to be
left unassigned.  Consider the following prediction problem as an example. We
have a set of data points with a known outcome value (&quot;training points&quot;) and
another set of points for which the outcome is unknown (&quot;prediction points&quot;).
We want to use the training points to predict the outcome for the prediction
points. By clustering the data, we can use the cluster mean of the training
points to predict the values of the prediction points in the same cluster. To
make this viable, we need to ensure that each cluster contains at least one
training point (the cluster mean would otherwise be undefined). We can impose
this constraint using the <code>type_labels</code> and <code>type_constraints</code>
options. We also need to make sure that each prediction point is assigned to
a cluster. We do, however, not need that all training points are assigned a
cluster; some training points might not provide useful information (e.g.,
if they are very dissimilar to all prediction points). In this case, by
specifying only the prediction points in <code>primary_data_points</code>, we
ensure that all those points are assigned to clusters. The points not
specified in <code>primary_data_points</code> (i.e., the training points) will
be assigned to clustering only insofar that it is needed to satisfy the
clustering constraints. This can lead to large improvements in the clustering
if the types of points are unevenly distributed in the metric space. Points
not specified in <code>primary_data_points</code> are called &quot;secondary&quot;.
</p>
<p>Generally, one does not want to discard all unassigned secondary points &ndash;
some of them will, occasionally, be close to a cluster and contain useful
information. Similar to <code>primary_unassigned_method</code>, we can use the
<code>secondary_unassigned_method</code> to specify how the leftover secondary
points should be assigned. The three possible options are &quot;ignore&quot;,
&quot;closest_assigned&quot; and &quot;closest_seed&quot;. In nearly all cases, it is beneficial
to impose a radius constraint when assigning secondary points (see below for
details).
</p>
<p><code>sc_clustering</code> tries to minimize within-cluster distances subject to
that all (primary) data points are assigned to clusters. In some cases, it
might be beneficial to leave some points unassigned to avoid clusters with
very dissimilar points. The function has options that can be used to
indirectly constrain the maximum within-cluster distance. Specifically, by
restricting the maximum distance in the four steps of the function, we can
bound the maximum within-cluster distance. The bound is, however, a blunt
tool. A too small bound might lead to that only a few data points are
assigned to clusters. If a bound is needed, it is recommended to use a very
liberal bound. This will avoid the very dissimilar clusters, but let the
function optimize the remaining cluster assignments.
</p>
<p>The <code>seed_radius</code> option limits the maximum distance between adjacent
vertices in the sparse graph. If the distance to a neighbor in a point's
neighborhood is greater than <code>seed_radius</code>, the neighborhood will be
deleted and the point is not allowed be a seed (it can, however, still be
in other points' neighborhoods). The <code>primary_radius</code> option limits
the maximum distance when a primary point is assigned to in the fourth step.
When <code>primary_radius</code> is set to a positive numeric value, this will be
used to as the radius restriction. &quot;no_radius&quot; and <code>NULL</code> indicate no
restriction. &quot;seed_radius&quot; indicates that the same restriction as
<code>seed_radius</code> should be used, and &quot;estimated_radius&quot; sets the
restriction to the estimated average distance between the seeds and their
neighbors. When <code>primary_unassigned_method</code> is set to &quot;any_neighbor&quot;,
<code>primary_radius</code> must be set to &quot;seed_radius&quot;.
</p>
<p>The way the radius constraints restrict the maximum within-cluster distance
depends on the <code>primary_unassigned_method</code> option. When
<code>primary_unassigned_method</code> is &quot;ignore&quot;, the maximum distance is bounded
by 2 * <code>seed_radius</code>. When <code>primary_unassigned_method</code> is
&quot;any_neighbor&quot;, it is bounded by 4 * <code>seed_radius</code>. When
<code>primary_unassigned_method</code> is &quot;closest_assigned&quot;, it is bounded by
2 * <code>seed_radius</code> + 2 * <code>primary_radius</code>. When
<code>primary_unassigned_method</code> is &quot;closest_seed&quot;, it is bounded by the
maximum of 2 * <code>seed_radius</code> and 2 * <code>primary_radius</code>.
</p>
<p>The <code>secondary_radius</code> option restricts how secondary points are
assigned, but is otherwise identical to the <code>primary_radius</code> option.
</p>


<h3>Value</h3>

<p>Returns a <code><a href="#topic+scclust">scclust</a></code> object with the derived clustering.
</p>


<h3>References</h3>

<p>Higgins, Michael J., Fredrik Sävje and Jasjeet S. Sekhon (2016),
&lsquo;Improving massive experiments with threshold blocking&rsquo;,
<em>Proceedings of the National Academy of Sciences</em>, <b>113:27</b>, 7369&ndash;7376.
</p>
<p>Sävje, Fredrik and Michael J. Higgins and Jasjeet S. Sekhon (2017),
&lsquo;Generalized Full Matching&rsquo;, arXiv 1703.03882.
<a href="https://arxiv.org/abs/1703.03882">https://arxiv.org/abs/1703.03882</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hierarchical_clustering">hierarchical_clustering</a></code> can be used to refine the clustering
constructed by <code>sc_clustering</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Make example data
my_data &lt;- data.frame(id = 1:50000,
                      type = factor(rbinom(50000, 3, 0.3),
                                    labels = c("A", "B", "C", "D")),
                      x1 = rnorm(50000),
                      x2 = rnorm(50000),
                      x3 = rnorm(50000))

# Construct distance metric
my_dist &lt;- distances(my_data,
                     id_variable = "id",
                     dist_variables = c("x1", "x2", "x3"))

# Make clustering with at least 3 data points in each cluster
my_clustering &lt;- sc_clustering(my_dist, 3)

# Check so clustering satisfies constraints
check_clustering(my_clustering, 3)
# &gt; TRUE

# Get statistics about the clustering
get_clustering_stats(my_dist, my_clustering)
# &gt; num_data_points        5.000000e+04
# &gt; ...

# Make clustering with at least one point of each type in each cluster
my_clustering &lt;- sc_clustering(my_dist,
                               type_labels = my_data$type,
                               type_constraints = c("A" = 1, "B" = 1,
                                                    "C" = 1, "D" = 1))

# Check so clustering satisfies constraints
check_clustering(my_clustering,
                 type_labels = my_data$type,
                 type_constraints = c("A" = 1, "B" = 1,
                                      "C" = 1, "D" = 1))
# &gt; TRUE

# Make clustering with at least 8 points in total of which at least
# one must be "A", two must be "B" and five can be any type
my_clustering &lt;- sc_clustering(my_dist,
                               size_constraint = 8,
                               type_labels = my_data$type,
                               type_constraints = c("A" = 1, "B" = 2))

</code></pre>

<hr>
<h2 id='scclust'>Constructor for scclust objects</h2><span id='topic+scclust'></span>

<h3>Description</h3>

<p>The <code>scclust</code> function constructs a <code>scclust</code> object from existing
cluster labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scclust(cluster_labels, unassigned_labels = NULL, ids = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="scclust_+3A_cluster_labels">cluster_labels</code></td>
<td>
<p>a vector containing each data point's cluster label.</p>
</td></tr>
<tr><td><code id="scclust_+3A_unassigned_labels">unassigned_labels</code></td>
<td>
<p>labels that denote unassigned data points. If <code>NULL</code>, <code>NA</code>
values in <code>cluster_labels</code> are used to denote unassigned points.</p>
</td></tr>
<tr><td><code id="scclust_+3A_ids">ids</code></td>
<td>
<p>IDs of the data points. Should be a vector of the same length as
<code>cluster_labels</code> or <code>NULL</code>. If <code>NULL</code>, the IDs are set
to <code>1:length(cluster_labels)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>scclust</code> does not derive clusters from sets of data points; see
<code><a href="#topic+sc_clustering">sc_clustering</a></code> and <code><a href="#topic+hierarchical_clustering">hierarchical_clustering</a></code> for
that functionality.
</p>


<h3>Value</h3>

<p>Returns a <code>scclust</code> object with the clustering described by the
provided labels.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 10 data points in 3 clusters
my_scclust1 &lt;- scclust(c("A", "A", "B", "C", "B",
                         "C", "C", "A", "B", "B"))

# 8 data points in 3 clusters, 2 points unassigned
my_scclust2 &lt;- scclust(c(1, 1, 2, 3, 2,
                         NA, 3, 1, NA, 2))

# Custom labels indicating unassiged points
my_scclust3 &lt;- scclust(c("A", "A", "B", "C", "NONE",
                         "C", "C", "NONE", "B", "B"),
                       unassigned_labels = "NONE")

# Two different labels indicating unassiged points
my_scclust4 &lt;- scclust(c("A", "A", "B", "C", "NONE",
                         "C", "C", "0", "B", "B"),
                       unassigned_labels = c("NONE", "0"))

# Custom data point IDs
my_labels5 &lt;- scclust(c("A", "A", "B", "C", "B",
                        "C", "C", "A", "B", "B"),
                      ids = letters[1:10])

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
