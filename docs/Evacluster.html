<!DOCTYPE html><html lang="en"><head><title>Help for package Evacluster</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Evacluster}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Evacluster-package'><p>Evaluation Clustering Methods for Disease Subtypes Diagnosis (Evacluster)</p></a></li>
<li><a href='#clusterStability'><p>clustering stability function</p></a></li>
<li><a href='#EMCluster'><p>Expectation Maximization Clustering</p></a></li>
<li><a href='#FuzzyCluster'><p>Fuzzy C-means Clustering Algorithm</p></a></li>
<li><a href='#getConsensusCluster'><p>Consensus Clustering Results</p></a></li>
<li><a href='#hierarchicalCluster'><p>hierarchical clustering</p></a></li>
<li><a href='#kmeansCluster'><p>K-means Clustering</p></a></li>
<li><a href='#nmfCluster'><p>Non-negative matrix factorization (NMF)</p></a></li>
<li><a href='#pamCluster'><p>Partitioning Around Medoids (PAM) Clustering</p></a></li>
<li><a href='#predict.EMCluster'><p>EMCluster prediction function</p></a></li>
<li><a href='#predict.FuzzyCluster'><p>FuzzyCluster prediction function</p></a></li>
<li><a href='#predict.hierarchicalCluster'><p>hierarchicalCluster prediction function</p></a></li>
<li><a href='#predict.kmeansCluster'><p>kmeansCluster prediction function</p></a></li>
<li><a href='#predict.nmfCluster'><p>nmfCluster prediction function</p></a></li>
<li><a href='#predict.pamCluster'><p>pamCluster prediction function</p></a></li>
<li><a href='#predict.tsneReductor'><p>tsneReductor prediction function</p></a></li>
<li><a href='#tsneReductor'><p>t-Distributed Stochastic Neighbor Embedding (t-SNE)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Evaluation Clustering Methods for Disease Subtypes Diagnosis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-03-25</td>
</tr>
<tr>
<td>Author:</td>
<td>Fahimeh Nezhadmoghadam,Jose Gerardo Tamez-Pena</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Fahimeh Nezhadmoghadam &lt;f.nejad.moghadam@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains a set of clustering methods and evaluation metrics to select the best number of the clusters based on clustering stability. Two references describe the methodology: Fahimeh Nezhadmoghadam, and Jose Tamez-Pena (2021)&lt;<a href="https://doi.org/10.1016%2Fj.compbiomed.2021.104753">doi:10.1016/j.compbiomed.2021.104753</a>&gt;, and Fahimeh Nezhadmoghadam, et al.(2021)&lt;<a href="https://doi.org/10.2174%2F1567205018666210831145825">doi:10.2174/1567205018666210831145825</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mlbench,EMCluster,inaparc,ppclust,FRESA.CAD,MASS,stats,class,NMF,cluster,Rtsne,proxy,uwot,mclust,graphics</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-03-31 18:27:26 UTC; fneja</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-04-01 07:50:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='Evacluster-package'>Evaluation Clustering Methods for Disease Subtypes Diagnosis (Evacluster)</h2><span id='topic+Evacluster-package'></span><span id='topic+Evacluster'></span>

<h3>Description</h3>

<p>Contains a set of clustering methods and evaluation metrics to select the best number of the clusters based on clustering stability.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
        Package: </td><td style="text-align: left;"> Evacluster</td>
</tr>
<tr>
 <td style="text-align: left;">
        Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
        Version: </td><td style="text-align: left;"> 0.1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
        Date: </td><td style="text-align: left;"> 2022-03-25</td>
</tr>
<tr>
 <td style="text-align: left;">
        License: </td><td style="text-align: left;"> LGPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
    </td>
</tr>

</table>

<p>Purpose: The design of clustering models and evaluation metrics for finding the cluster's number via computing clustering stability.
The best number of clusters is selected via consensus clustering and clustering stability. 
</p>


<h3>Author(s)</h3>

<p>Fahimeh Nezhadmoghadam, Jose Gerardo Tamez-Pena,
Maintainer: &lt;f.nejad.moghadam@gmail.com&gt;
</p>


<h3>References</h3>

<p>Nezhadmoghadam, Fahimeh, and Jose Tamez-Pena. &quot;Risk profiles for negative and positive COVID-19 hospitalized patients.(2021) <em>Computers in biology and medicine</em> 136 : 104753.<br />
Fahimeh Nezhadmoghadam, et al., Robust Discovery of Mild Cognitive impairment subtypes and their Risk of Alzheimer's Disease conversion using unsupervised machine learning and Gaussian Mixture Modeling (2021), <em>Current Alzheimer Research</em>, 18 (7), 595-606.</p>


<h3>Examples</h3>

<pre><code class='language-R'>    ## Not run: 
    ### Evacluster Package Examples ####
    library(datasets)
    data(iris)

   # Split data to training set and testing set
   rndSamples &lt;- sample(nrow(iris),100)
   trainData &lt;- iris[rndSamples,]
   testData &lt;- iris[-rndSamples,]

  
   ## Expectation Maximization Clustering
   # Perform Expectation Maximization Clustering on training set with 3 clusters 
   clsut &lt;- EMCluster(trainData[,1:4],3)
   
   # Predict the labels of the cluster for new data based on cluster labels of the training set
   pre &lt;- predict(clsut,testData[,1:4])
   
   
   ## Fuzzy C-means Clustering
   # Perform Fuzzy C-means Clustering on training set with 3 clusters 
   clsut &lt;- FuzzyCluster(trainData[,1:4],3)
   
   # Predict the labels of the new data 
   pre &lt;- predict(clsut,testData[,1:4])
   
   
   ## hierarchical clustering
   # Perform hierarchical clustering on training set with 3 clusters 
   clsut &lt;- hierarchicalCluster(trainData[,1:4],distmethod="euclidean",clusters=3)
   
   # Predict the labels of the new data 
   pre &lt;- predict(clsut,testData[,1:4])
   
   
   ## K-means Clustering
   # Perform K-means Clustering on training set with 3 clusters 
   clsut &lt;- kmeansCluster(trainData[,1:4],3)
   
   # Predict the labels of the new data 
   pre &lt;- predict(clsut,testData[,1:4])
   
   
   ## Partitioning Around Medoids (PAM) Clustering
   # Perform pam Clustering on training set with 3 clusters 
   clsut &lt;- pamCluster(trainData[,1:4],3)
   
   # Predict the labels of the new data 
   pre &lt;- predict(clsut,testData[,1:4])
   
   
   ## Non-negative matrix factorization (NMF)
   # Perform nmf Clustering on training set with 3 clusters 
   clsut &lt;- nmfCluster(trainData[,1:4],rank=3)
   
   # Predict the labels of the new data 
   pre &lt;- predict(clsut,testData[,1:4])
   
   
   ## t-Distributed Stochastic Neighbor Embedding (t-SNE)
   
   library(mlbench)
   data(Sonar)
 
   rndSamples &lt;- sample(nrow(Sonar),150)
   trainData &lt;- Sonar[rndSamples,]
   testData &lt;- Sonar[-rndSamples,]
 
   # Perform tSNE dimensionality reduction method on training data 
   tsne_trainData &lt;- tsneReductor(trainData[,1:60],dim = 3,perplexity = 10,max_iter = 1000)
   
   # performs an embedding of new data using an existing embedding
   tsne_testData &lt;- predict(tsne_trainData,k=3,testData[,1:60])
   
   
   ## clustering stability function
   # Compute the stability of clustering to select the best number of clusters.
   library(mlbench)
   data(Sonar)
 
   Sonar$Class &lt;- as.numeric(Sonar$Class)
   Sonar$Class[Sonar$Class == 1] &lt;- 0
   Sonar$Class[Sonar$Class == 2] &lt;- 1
   
   # Compute the stability of clustering using kmeans clustering, UMAP as 
   dimensionality reduction method, and feature selection technique
   
  ClustStab &lt;- clusterStability(data=Sonar, clustermethod=kmeansCluster, dimenreducmethod="UMAP",
                              n_components = 3,featureselection="yes", outcome="Class",
                              fs.pvalue = 0.05,randomTests = 100,trainFraction = 0.7,center=3)
   
   
   # Get the labels of the subjects that share the same connectivity
   clusterLabels &lt;- getConsensusCluster(ClustStab,who="training",thr=seq(0.80,0.30,-0.1))


     # Compute the stability of clustering using PAM clustering, tSNE as
     dimensionality reduction method, and feature selection technique
     
   ClustStab &lt;- clusterStability(data=Sonar, clustermethod=pamCluster, dimenreducmethod="tSNE",
                              n_components = 3, perplexity=10,max_iter=100,k_neighbor=2,
                             featureselection="yes", outcome="Class",fs.pvalue = 0.05,
                               randomTests = 100,trainFraction = 0.7,k=3)
          
    # Get the labels of the subjects that share the same connectivity
   clusterLabels &lt;- getConsensusCluster(ClustStab,who="training",thr=seq(0.80,0.30,-0.1))
                     
                     
    # Compute the stability of clustering using hierarchical clustering,
    PCA as dimensionality reduction method, and without applying feature selection
                                 
   ClustStab &lt;- clusterStability(data=Sonar, clustermethod=hierarchicalCluster, 
                               dimenreducmethod="PCA", n_components = 3,featureselection="no",
                               randomTests = 100,trainFraction = 0.7,distmethod="euclidean", 
                               clusters=3)
                               
 # Get the labels of the subjects that share the same connectivity
   clusterLabels &lt;- getConsensusCluster(ClustStab,who="training",thr=seq(0.80,0.30,-0.1))
   
   
   # Show the clustering stability resuldts
   mycolors &lt;- c("red","green","blue","yellow")
 
   ordermatrix &lt;- ClustStab$dataConcensus
 
   heatmapsubsample &lt;- sample(nrow(ordermatrix),70)
 
   orderindex &lt;- 10*clusterLabels + ClustStab$trainJaccardpoint
 
   orderindex &lt;- orderindex[heatmapsubsample]
   orderindex &lt;- order(orderindex)
   ordermatrix &lt;- ordermatrix[heatmapsubsample,heatmapsubsample]
   ordermatrix &lt;- ordermatrix[orderindex,orderindex]
   rowcolors &lt;- mycolors[1+clusterLabels[heatmapsubsample]]
   rowcolors &lt;- rowcolors[orderindex]
 
 
   hplot &lt;- gplots::heatmap.2(as.matrix(ordermatrix),Rowv=FALSE,Colv=FALSE,
                            RowSideColors = rowcolors,ColSideColors = rowcolors,dendrogram = "none",
                            trace="none",main="Cluster Co-Association \n (k=3)")
                            
   
   # Compare the PAC values of clustering stability with different numbers of clusters 
   
   ClustStab2 &lt;- clusterStability(data=Sonar, clustermethod=kmeansCluster, dimenreducmethod="UMAP",
                              n_components = 3,featureselection="yes", outcome="Class",
                              fs.pvalue = 0.05,randomTests = 100,trainFraction = 0.7,center=2)
 
   ClustStab3 &lt;- clusterStability(data=Sonar, clustermethod=kmeansCluster, dimenreducmethod="UMAP",
                                n_components = 3,featureselection="yes", outcome="Class",
                                fs.pvalue = 0.05,randomTests = 100,trainFraction = 0.7,center=3)
 
   ClustStab4 &lt;- clusterStability(data=Sonar, clustermethod=kmeansCluster, dimenreducmethod="UMAP",
                                n_components = 3,featureselection="yes", outcome="Class",
                                fs.pvalue = 0.05,randomTests = 100,trainFraction = 0.7,center=4)
                                
                                
   color_range&lt;- c(black="#FDFC74", orange="#76FF7A", skyblue="#B2EC5D")
 
 
   max.temp &lt;- c(ClustStab2$PAC,ClustStab3$PAC,ClustStab4$PAC) 
 
   barplot(max.temp,xlab = "Number of clusters",ylab = "PAC", names.arg = c( "2","3","4"), 
          ylim=c(0,0.3),col= color_range[1:length(c(1,6,2,6,1))])
                            
   
## End(Not run)
</code></pre>

<hr>
<h2 id='clusterStability'>clustering stability function</h2><span id='topic+clusterStability'></span>

<h3>Description</h3>

<p>This function computes the stability of clustering that helps to select the best number of clusters.
Feature selection and dimensionality reduction methods can be used before clustering 
the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusterStability(
  data = NULL,
  clustermethod = NULL,
  dimenreducmethod = NULL,
  n_components = 3,
  perplexity = 25,
  max_iter = 1000,
  k_neighbor = 3,
  featureselection = NULL,
  outcome = NULL,
  fs.pvalue = 0.05,
  randomTests = 20,
  trainFraction = 0.5,
  pac.thr = 0.1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="clusterStability_+3A_data">data</code></td>
<td>
<p>A Data set</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_clustermethod">clustermethod</code></td>
<td>
<p>The clustering method. This can be one of &quot;Mclust&quot;,&quot;pamCluster&quot;,&quot;kmeansCluster&quot;, &quot;hierarchicalCluster&quot;,and &quot;FuzzyCluster&quot;.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_dimenreducmethod">dimenreducmethod</code></td>
<td>
<p>The dimensionality reduction method. This must be one of &quot;UMAP&quot;,&quot;tSNE&quot;, and &quot;PCA&quot;.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_n_components">n_components</code></td>
<td>
<p>The dimension of the space that data embed into. It can be set to any integer value in the range of 2 to 100.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_perplexity">perplexity</code></td>
<td>
<p>The Perplexity parameter that determines the optimal number of neighbors in tSNE method.(it is only used in the tSNE reduction method)</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations for performing tSNE reduction method.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_k_neighbor">k_neighbor</code></td>
<td>
<p>The k_neighbor is used for computing the means of #neighbors with min distance (#Neighbor=sqrt(#Samples/k) for performing an embedding of new data using an existing embedding in the tSNE method.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_featureselection">featureselection</code></td>
<td>
<p>This parameter determines whether feature selection is applied before clustering data or not. if used, it should be &quot;yes&quot;, otherwisw &quot;no&quot;.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_outcome">outcome</code></td>
<td>
<p>The outcome feature is used for feature selection.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_fs.pvalue">fs.pvalue</code></td>
<td>
<p>The threshold pvalue used for feature selection process. The default value is 0.05.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_randomtests">randomTests</code></td>
<td>
<p>The number of iterations of the clustering process for computing the cluster stability.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_trainfraction">trainFraction</code></td>
<td>
<p>This parameter determines the ratio of training data. The default value is 0.5.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_pac.thr">pac.thr</code></td>
<td>
<p>The pac.thr is the thresold to use for computing the proportion of ambiguous clustering (PAC) score. It is as the fraction of sample pairs with consensus indices falling in the interval.The default value is 0.1.</p>
</td></tr>
<tr><td><code id="clusterStability_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to clusterStability().</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<ul>
<li><p> randIndex - A vector of the Rand Index that computes a similarity measure between two clusterings. 
</p>
</li>
<li><p> jaccIndex - A vector of jaccard Index that measures how frequently pairs of items are joined together in two clustering data sets.
</p>
</li>
<li><p> randomSamples - A vector with indexes of selected samples for training in each iteration.
</p>
</li>
<li><p> clusterLabels - A vector with clusters' labels in all iterations. jaccardpoint
</p>
</li>
<li><p> jaccardpoint - The corresponding Jaccard index for each data point of testing set
</p>
</li>
<li><p> averageNumberofClusters - The mean Number of Clusters.
</p>
</li>
<li><p> testConsesus - A vector of consensus clustering results of testing set.
</p>
</li>
<li><p> trainRandIndex - A vector of the Rand Index for training set.
</p>
</li>
<li><p> trainJaccIndex - A vector of the jaccard Index for training set.
</p>
</li>
<li><p> trainJaccardpoint - The corresponding Jaccard index for each data point of training set.
</p>
</li>
<li><p> PAC - The proportion of ambiguous clustering (PAC) score.
</p>
</li>
<li><p> dataConcensus - A vector of consensus clustering results of training set.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
library("mlbench")
data(Sonar)

Sonar$Class &lt;- as.numeric(Sonar$Class)
Sonar$Class[Sonar$Class == 1] &lt;- 0 
Sonar$Class[Sonar$Class == 2] &lt;- 1

ClustStab &lt;- clusterStability(data=Sonar, clustermethod=kmeansCluster, dimenreducmethod="UMAP",
                              n_components = 3,featureselection="yes", outcome="Class",
                              fs.pvalue = 0.05,randomTests = 100,trainFraction = 0.7,center=3)


ClustStab &lt;- clusterStability(data=Sonar, clustermethod=pamCluster, dimenreducmethod="tSNE",
                              n_components = 3, perplexity=10,max_iter=100,k_neighbor=2,
                              featureselection="yes", outcome="Class",fs.pvalue = 0.05,
                              randomTests = 100,trainFraction = 0.7,k=3)


ClustStab &lt;- clusterStability(data=Sonar, clustermethod=hierarchicalCluster, 
                              dimenreducmethod="PCA", n_components = 3,featureselection="no",
                              randomTests = 100,trainFraction = 0.7,distmethod="euclidean",
                              clusters=3)


</code></pre>

<hr>
<h2 id='EMCluster'>Expectation Maximization Clustering</h2><span id='topic+EMCluster'></span>

<h3>Description</h3>

<p>This function perform EM algorithm for model-based clustering of finite
mixture multivariate Gaussian distribution.The general purpose of clustering
is to detect clusters of data and to assign the data to the clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EMCluster(data = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EMCluster_+3A_data">data</code></td>
<td>
<p>A Data set</p>
</td></tr>
<tr><td><code id="EMCluster_+3A_...">...</code></td>
<td>
<p>k: The number of Clusters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels and a returned object from init.EM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(datasets)
data(iris)

rndSamples &lt;- sample(nrow(iris),100)
trainData &lt;- iris[rndSamples,]
testData &lt;- iris[-rndSamples,]

clsut &lt;- EMCluster(trainData[,1:4],3)
</code></pre>

<hr>
<h2 id='FuzzyCluster'>Fuzzy C-means Clustering Algorithm</h2><span id='topic+FuzzyCluster'></span>

<h3>Description</h3>

<p>This function works by assigning membership to each data point corresponding
to each cluster center based on the distance between the cluster center and
the data point. A data object is the member of all clusters with varying
degrees of fuzzy membership between 0 and 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FuzzyCluster(data = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="FuzzyCluster_+3A_data">data</code></td>
<td>
<p>A Data set</p>
</td></tr>
<tr><td><code id="FuzzyCluster_+3A_...">...</code></td>
<td>
<p>k: The number of Clusters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels and a R object of class &quot;fcm ppclust&quot;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(datasets)
data(iris)

rndSamples &lt;- sample(nrow(iris),100)
trainData &lt;- iris[rndSamples,]
testData &lt;- iris[-rndSamples,]

cls &lt;- FuzzyCluster(trainData[,1:4],3)
</code></pre>

<hr>
<h2 id='getConsensusCluster'>Consensus Clustering Results</h2><span id='topic+getConsensusCluster'></span>

<h3>Description</h3>

<p>This function gets the labels of the subjects that share the same connectivity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getConsensusCluster(object, who = "training", thr = seq(0.8, 0.3, -0.1))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getConsensusCluster_+3A_object">object</code></td>
<td>
<p>A object of &quot;clusterStability&quot; function result</p>
</td></tr>
<tr><td><code id="getConsensusCluster_+3A_who">who</code></td>
<td>
<p>This value shows the consensus clustering result of training and testing sets. If who=&quot;training&quot; for training set, otherwise other sets.</p>
</td></tr>
<tr><td><code id="getConsensusCluster_+3A_thr">thr</code></td>
<td>
<p>This is the seq function with three arguments that are: initial value, final value, and increment (or decrement for a declining sequence). This produces ascending or descending sequences.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of samples' labels with same connectivity.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("mlbench")
data(Sonar)

Sonar$Class &lt;- as.numeric(Sonar$Class)
Sonar$Class[Sonar$Class == 1] &lt;- 0 
Sonar$Class[Sonar$Class == 2] &lt;- 1

ClustStab &lt;- clusterStability(data=Sonar, clustermethod=kmeansCluster, dimenreducmethod="UMAP",
                              n_components = 3,featureselection="yes", outcome="Class",
                              fs.pvalue = 0.05,randomTests = 100,trainFraction = 0.7,center=3)

clusterLabels &lt;- getConsensusCluster(ClustStab,who="training",thr=seq(0.80,0.30,-0.1))

</code></pre>

<hr>
<h2 id='hierarchicalCluster'>hierarchical clustering</h2><span id='topic+hierarchicalCluster'></span>

<h3>Description</h3>

<p>This function seeks to build a hierarchy of clusters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hierarchicalCluster(data = NULL, distmethod = NULL, clusters = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hierarchicalCluster_+3A_data">data</code></td>
<td>
<p>A Data set</p>
</td></tr>
<tr><td><code id="hierarchicalCluster_+3A_distmethod">distmethod</code></td>
<td>
<p>The distance measure to be used. This must be one of &quot;euclidean&quot;, &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot; or &quot;minkowski&quot;.</p>
</td></tr>
<tr><td><code id="hierarchicalCluster_+3A_clusters">clusters</code></td>
<td>
<p>The number of Clusters</p>
</td></tr>
<tr><td><code id="hierarchicalCluster_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to  hclust function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(datasets)
data(iris)

rndSamples &lt;- sample(nrow(iris),100)
trainData &lt;- iris[rndSamples,]
testData &lt;- iris[-rndSamples,]

cls &lt;- hierarchicalCluster(trainData[,1:4],distmethod="euclidean",clusters=3)
</code></pre>

<hr>
<h2 id='kmeansCluster'>K-means Clustering</h2><span id='topic+kmeansCluster'></span>

<h3>Description</h3>

<p>This function classifies unlabeled data by grouping them by features,
rather than pre-defined categories. It splits the data into K different
clusters and describes the location of the center of each cluster. Then,
a new data point can be assigned a cluster (class) based on the closed
center of mass.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeansCluster(data = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kmeansCluster_+3A_data">data</code></td>
<td>
<p>A Data set</p>
</td></tr>
<tr><td><code id="kmeansCluster_+3A_...">...</code></td>
<td>
<p>center: The number of centers</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels and a R object of class &quot;kmeans&quot;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(datasets)
data(iris)

rndSamples &lt;- sample(nrow(iris),100)
trainData &lt;- iris[rndSamples,]
testData &lt;- iris[-rndSamples,]

cls &lt;- kmeansCluster(trainData[,1:4],3)
</code></pre>

<hr>
<h2 id='nmfCluster'>Non-negative matrix factorization (NMF)</h2><span id='topic+nmfCluster'></span>

<h3>Description</h3>

<p>This function factorizes samples matrix into (usually) two matrices W the cluster centroids
and H the cluster membership,
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nmfCluster(data = NULL, rank = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nmfCluster_+3A_data">data</code></td>
<td>
<p>A Data set</p>
</td></tr>
<tr><td><code id="nmfCluster_+3A_rank">rank</code></td>
<td>
<p>Specification of the factorization rank</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels, a R object of class &quot;nmf&quot; and the centers of the clusters
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(datasets)
data(iris)

rndSamples &lt;- sample(nrow(iris),100)
trainData &lt;- iris[rndSamples,]
testData &lt;- iris[-rndSamples,]

cls &lt;- nmfCluster(trainData[,1:4],rank=3)
</code></pre>

<hr>
<h2 id='pamCluster'>Partitioning Around Medoids (PAM) Clustering</h2><span id='topic+pamCluster'></span>

<h3>Description</h3>

<p>This function partitions (clustering) of the data into k clusters &quot;around medoids&quot;.
In contrast to the k-means algorithm, this clustering methods chooses actual data
points as centers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pamCluster(data = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pamCluster_+3A_data">data</code></td>
<td>
<p>A Data set</p>
</td></tr>
<tr><td><code id="pamCluster_+3A_...">...</code></td>
<td>
<p>k: The number of clusters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels and a R object of class &quot;pam cluster&quot;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(datasets)
data(iris)

rndSamples &lt;- sample(nrow(iris),100)
trainData &lt;- iris[rndSamples,]
testData &lt;- iris[-rndSamples,]

cls &lt;- pamCluster(trainData[,1:4],3)
</code></pre>

<hr>
<h2 id='predict.EMCluster'>EMCluster prediction function</h2><span id='topic+predict.EMCluster'></span>

<h3>Description</h3>

<p>This function predicts the labels of the cluster for new data based on
cluster labels of the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EMCluster'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.EMCluster_+3A_object">object</code></td>
<td>
<p>A returned object of EMCluster</p>
</td></tr>
<tr><td><code id="predict.EMCluster_+3A_...">...</code></td>
<td>
<p>New sample set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels
</p>

<hr>
<h2 id='predict.FuzzyCluster'>FuzzyCluster prediction function</h2><span id='topic+predict.FuzzyCluster'></span>

<h3>Description</h3>

<p>This function predicts the labels of the cluster for new data based on
cluster labels of the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FuzzyCluster'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.FuzzyCluster_+3A_object">object</code></td>
<td>
<p>A returned object of FuzzyCluster function</p>
</td></tr>
<tr><td><code id="predict.FuzzyCluster_+3A_...">...</code></td>
<td>
<p>New samples set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels
</p>

<hr>
<h2 id='predict.hierarchicalCluster'>hierarchicalCluster prediction function</h2><span id='topic+predict.hierarchicalCluster'></span>

<h3>Description</h3>

<p>This function predicts the labels of the cluster for new data based on
cluster labels of the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hierarchicalCluster'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.hierarchicalCluster_+3A_object">object</code></td>
<td>
<p>A returned object of hierarchicalCluster function</p>
</td></tr>
<tr><td><code id="predict.hierarchicalCluster_+3A_...">...</code></td>
<td>
<p>New samples set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels
</p>

<hr>
<h2 id='predict.kmeansCluster'>kmeansCluster prediction function</h2><span id='topic+predict.kmeansCluster'></span>

<h3>Description</h3>

<p>This function predicts the labels of the cluster for new data based on
cluster labels of the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'kmeansCluster'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.kmeansCluster_+3A_object">object</code></td>
<td>
<p>A returned object of kmeansCluster function</p>
</td></tr>
<tr><td><code id="predict.kmeansCluster_+3A_...">...</code></td>
<td>
<p>New samples set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels
</p>

<hr>
<h2 id='predict.nmfCluster'>nmfCluster prediction function</h2><span id='topic+predict.nmfCluster'></span>

<h3>Description</h3>

<p>This function predicts the labels of the cluster for new data based on
cluster labels of the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nmfCluster'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.nmfCluster_+3A_object">object</code></td>
<td>
<p>A returned object of nmfCluster</p>
</td></tr>
<tr><td><code id="predict.nmfCluster_+3A_...">...</code></td>
<td>
<p>New samples set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels
</p>

<hr>
<h2 id='predict.pamCluster'>pamCluster prediction function</h2><span id='topic+predict.pamCluster'></span>

<h3>Description</h3>

<p>This function predicts the labels of the cluster for new data based on
cluster labels of the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pamCluster'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.pamCluster_+3A_object">object</code></td>
<td>
<p>A returned object of pamCluster function</p>
</td></tr>
<tr><td><code id="predict.pamCluster_+3A_...">...</code></td>
<td>
<p>New samples set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of cluster labels
</p>

<hr>
<h2 id='predict.tsneReductor'>tsneReductor prediction function</h2><span id='topic+predict.tsneReductor'></span>

<h3>Description</h3>

<p>This function performs an embedding of new data using an existing embedding.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tsneReductor'
predict(object, k = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.tsneReductor_+3A_object">object</code></td>
<td>
<p>A returned object of tsneReductor function</p>
</td></tr>
<tr><td><code id="predict.tsneReductor_+3A_k">k</code></td>
<td>
<p>The number is used for computing the means of #neighbors with min distance
(#Neighbor=sqrt(#Samples/k).</p>
</td></tr>
<tr><td><code id="predict.tsneReductor_+3A_...">...</code></td>
<td>
<p>New samples set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>tsneY:An embedding of new data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("mlbench")
data(Sonar)

rndSamples &lt;- sample(nrow(Sonar),150)
trainData &lt;- Sonar[rndSamples,]
testData &lt;- Sonar[-rndSamples,]

tsne_trainData &lt;- tsneReductor(trainData[,1:60],dim = 3,perplexity = 10,max_iter = 1000)

tsne_testData &lt;- predict(tsne_trainData,k=3,testData[,1:60])
</code></pre>

<hr>
<h2 id='tsneReductor'>t-Distributed Stochastic Neighbor Embedding (t-SNE)</h2><span id='topic+tsneReductor'></span>

<h3>Description</h3>

<p>This method is an unsupervised, non-linear technique used for
data exploration and visualizing high-dimensional data.This function
constructs a low-dimensional embedding of high-dimensional
data, distances, or similarities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tsneReductor(data = NULL, dim = 2, perplexity = 30, max_iter = 500)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tsneReductor_+3A_data">data</code></td>
<td>
<p>Data matrix (each row is an observation, each column is a variable)</p>
</td></tr>
<tr><td><code id="tsneReductor_+3A_dim">dim</code></td>
<td>
<p>Integer number; Output dimensional (default=2)</p>
</td></tr>
<tr><td><code id="tsneReductor_+3A_perplexity">perplexity</code></td>
<td>
<p>numeric; Perplexity parameter (should not be bigger than
3 * perplexity &lt; nrow(X) - 1, default=30)</p>
</td></tr>
<tr><td><code id="tsneReductor_+3A_max_iter">max_iter</code></td>
<td>
<p>Integer; Number of iterations (default: 500)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>tsneY: A Matrix containing the new representations for the observation
with selected dimensions by user
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("mlbench")
data(Sonar)

rndSamples &lt;- sample(nrow(Sonar),150)
trainData &lt;- Sonar[rndSamples,]
testData &lt;- Sonar[-rndSamples,]

tsne_trainData &lt;- tsneReductor(trainData[,1:60],dim = 3,perplexity = 10,max_iter = 1000)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
