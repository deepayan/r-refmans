<!DOCTYPE html><html><head><title>Help for package clv</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {clv}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cls.attrib'><p>Mean, cluster size and center - cluster utilities</p></a></li>
<li><a href='#cls.scatt.data'><p>Intercluster distances and intracluster diameters - Internal Measures</p></a></li>
<li><a href='#cls.set.section'><p>Section of two subsets - External Measure utilities</p></a></li>
<li><a href='#cls.stab.sim.ind'><p>Cluster Stability - Similarity Index and Pattern-wise Stability Approaches</p></a></li>
<li><a href='#cls.stab.sim.ind.usr'><p>Cluster Stability - Similarity Index and Pattern-wise Stability Approaches with User Defined Cluster Algorithms</p></a></li>
<li><a href='#clv.Davies.Bouldin'><p>Davies-Bouldin Index -  Internal Measure</p></a></li>
<li><a href='#clv.Dens_bw'><p>Inter-cluster density - Internal Measure</p></a></li>
<li><a href='#clv.Dis'><p>Total separation between clusters - Internal Measure</p></a></li>
<li><a href='#clv.Dunn'><p>Dunn Index - Internal Measure</p></a></li>
<li><a href='#clv.Scatt'><p>Average scattering for clusters - Internal Measure</p></a></li>
<li><a href='#clv.SD+2C+20clv.SDbw'><p>SD, SDbw - Internal Measures</p></a></li>
<li><a href='#confusion.matrix'><p>Confusion Matrix - External Measures, Cluster Stability</p></a></li>
<li><a href='#connectivity'><p>Connectivity Index - Internal Measure</p></a></li>
<li><a href='#dot.product'><p>Cosine similarity measure - External Measure, Cluster Stability</p></a></li>
<li><a href='#similarity.index'><p>Similarity index based on confusion matrix - External Measure, Cluster Stability</p></a></li>
<li><a href='#std.ext'><p>Standard External Measures: Rand index, Jaccard coefficient etc.</p></a></li>
<li><a href='#wcls+2Fbcls.matrix'><p>Matrix Cluster Scatter Measures</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Cluster Validation Techniques</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3-2.4</td>
</tr>
<tr>
<td>Author:</td>
<td>Lukasz Nieweglowski &lt;wookashn@gmail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lukasz Nieweglowski &lt;wookashn@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Package contains most of the popular internal and external
        cluster validation methods ready to use for the most of the
        outputs produced by functions coming from package "cluster".
        Package contains also functions and examples of usage for
        cluster stability approach that might be applied to algorithms
        implemented in "cluster" package as well as user defined
        clustering algorithms.</td>
</tr>
<tr>
<td>Depends:</td>
<td>cluster, class</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-28 08:18:48 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-28 08:33:07 UTC</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
</table>
<hr>
<h2 id='cls.attrib'>Mean, cluster size and center - cluster utilities</h2><span id='topic+cls.attrib'></span>

<h3>Description</h3>

<p>Mean, center of each cluster, number of objects in each cluster - informations retrieved from partitioned data
using <code>cls.attrib</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cls.attrib(data, clust)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cls.attrib_+3A_data">data</code></td>
<td>

<p><code>numeric matrix</code> or <code>data.frame</code> where columns correspond to variables and rows to 
observations.
</p>
</td></tr>
<tr><td><code id="cls.attrib_+3A_clust">clust</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>As a result function returns object of <code>list</code> type which contains three objects with information about:<br />
<code>mean</code> - <code>numeric vector</code> which represents mean of given <code>data</code>, <br />
<code>cluster.center</code> - <code>numeric matrix</code> where columns correspond to variables and rows to observations,<br />
<code>cluster.size</code> - <code>integer vector</code> with information about size of each cluster.
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>See Also</h3>

<p>Result of function is mostly used to compute following indicies:
<code><a href="#topic+clv.Dis">clv.Dis</a></code>, <code><a href="#topic+wcls.matrix">wcls.matrix</a></code>, <code><a href="#topic+bcls.matrix">bcls.matrix</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create "data" matrix
mx &lt;- matrix(0,4,2)
mx[2,1] = mx[3,2] = mx[4,1] = mx[4,2] = 1
# give information about cluster assignment
clust = as.integer(c(1,1,2,2))
cls.attrib(mx,clust)
</code></pre>

<hr>
<h2 id='cls.scatt.data'>Intercluster distances and intracluster diameters - Internal Measures</h2><span id='topic+cls.scatt.data'></span><span id='topic+cls.scatt.diss.mx'></span>

<h3>Description</h3>

<p>Two functions which find most popular <em>intercluster distances</em> and <em>intracluster diameters</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cls.scatt.data(data, clust, dist="euclidean")
cls.scatt.diss.mx(diss.mx, clust)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cls.scatt.data_+3A_data">data</code></td>
<td>

<p><code>numeric matrix</code> or <code>data.frame</code> where columns correspond to variables and rows to 
observations
</p>
</td></tr>
<tr><td><code id="cls.scatt.data_+3A_diss.mx">diss.mx</code></td>
<td>

<p>square, symmetric <code>numeric matrix</code> or <code>data.frame</code>, representation of 
dissimilarity matrix where infomartion about distances between objects is stored.
</p>
</td></tr>
<tr><td><code id="cls.scatt.data_+3A_clust">clust</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning.
</p>
</td></tr>
<tr><td><code id="cls.scatt.data_+3A_dist">dist</code></td>
<td>

<p>chosen metric: &quot;euclidean&quot; (default value), &quot;manhattan&quot;, &quot;correlation&quot;
(variable enable only in <code>cls.scatt.data</code> function). 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Six <em>intercluster distances</em> and three <em>intracluster diameters</em> can be used to 
calculate such validity indices as <em>Dunn</em> and <em>Davies-Bouldin</em> like.
Let <code>d(x,y)</code> be a distance function between two objects comming from our data set.
</p>
<p><em>Intracluster diameters</em>
</p>
<p>The <em>complete diameter</em> represents the distance between two the most remote objects belonging 
to the same cluster. 
</p>
<p>diam1(C) = max{ d(x,y): x,y belongs to cluster C }
</p>
<p>The <em>average diameter</em> distance defines the average distance between all of the 
samples belonging to the same cluster. 
</p>
<p>diam2(C) = 1/|C|(|C|-1) * sum{ forall x,y belongs to cluster C and x != y } d(x,y)
</p>
<p>The <em>centroid diameter</em> distance reflects the double average distance between all of the 
samples and the cluster's center (v(C) - cluster center).
</p>
<p>diam3(C) = 1/|C| * sum{ forall x belonging to cluster C} d(x,v(C))
</p>
<p><em>Intercluster distances</em>
</p>
<p>The <em>single linkage</em> distance defines the closest distance between two samples 
belonging to two different clusters. 
</p>
<p>dist1(Ci,Cj) = min{ d(x,y): x belongs to Ci and y to Cj cluster }
</p>
<p>The <em>complete linkage</em> distance represents the distance between the most remote samples
belonging to two different clusters. 
</p>
<p>dist2(Ci,Cj) = max{ d(x,y): x belongs to Ci and y to Cj cluster }
</p>
<p>The <em>average linkage</em> distance defines the average distance between all of the samples 
belonging to two different clusters. 
</p>
<p>dist3(Ci,Cj) = 1/(|Ci|*|Cj|) * sum{ forall x belongs Ci and y to Cj } d(x,y)
</p>
<p>The <em>centroid linkage</em> distance reflects the distance between the centres of two clusters
(v(i), v(j) - clusters' centers).
</p>
<p>dist4(Ci,Cj) = d(v(i), V(j))
</p>
<p>The <em>average of centroids linkage</em> represents the distance between the centre of a cluster 
and all of samples belonging to a different cluster. 
</p>
<p>dist5(Ci,Cj) = 1/(|Ci|+|Cj|) * 
( sum{ forall x belongs Ci } d(x,v(j)) + sum{ forall y belongs Cj } d(y,v(i)) )
</p>
<p><em>Hausdorff metrics</em> are based on the discovery of a maximal distance from samples of one 
cluster to the nearest sample of another cluster. 
</p>
<p>dist6(Ci,Cj) = max{ distH(Ci,Cj), distH(Cj,Ci) }
</p>
<p>where: distH(A,B) = max{ min{ d(x,y): y belongs to B}: x belongs to A }
</p>


<h3>Value</h3>

<p><code>cls.scatt.data</code> returns an object of class <code>"list"</code>.
Intracluster diameters: 
<code>intracls.complete</code>,
<code>intracls.average</code>,
<code>intracls.centroid</code>,
are stored in vectors and intercluster distances:
<code>intercls.single</code>, 
<code>intercls.complete</code>,
<code>intercls.average</code>,
<code>intercls.centroid</code>,
<code>intercls.ave_to_cent</code>,
<code>intercls.hausdorff</code>
in symmetric matrices.
Vectors' lengths and both dimensions of each matrix are equal to number of clusters.
Additionally in result list <code>cluster.center</code> matrix (rows correspond to clusters centers) 
and <code>cluster.size</code> vector is given (information about size of each cluster).
<br /><br />
<code>cls.scatt.diss.mx</code> returns an object of class <code>"list"</code>.
Intracluster diameters: 
<code>intracls.complete</code>,
<code>intracls.average</code>,
are stored in vectors and intercluster distances:
<code>intercls.single</code>, 
<code>intercls.complete</code>,
<code>intercls.average</code>,
<code>intercls.hausdorff</code>
in symmetric matrices.
Vectors' lengths and both dimensions of each matrix are equal to number of clusters.
Additionally in result list <code>cluster.size</code> vector is given (information about size of each cluster).
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>J. Handl, J. Knowles and D. B. Kell <em>Computational cluster validation in post-genomic data analysis</em>,
<a href="http://bioinformatics.oxfordjournals.org/cgi/reprint/21/15/3201?ijkey=VbTHU29vqzwkGs2&amp;keytype=ref">http://bioinformatics.oxfordjournals.org/cgi/reprint/21/15/3201?ijkey=VbTHU29vqzwkGs2&amp;keytype=ref</a>
</p>
<p>N. Bolshakova, F. Azuajeb <em>Cluster validation techniques for genome expression data</em>,
<a href="http://citeseer.ist.psu.edu/552250.html">http://citeseer.ist.psu.edu/552250.html</a>
</p>


<h3>See Also</h3>

<p>Result used in: <code><a href="#topic+clv.Dunn">clv.Dunn</a></code>, <code><a href="#topic+clv.Davies.Bouldin">clv.Davies.Bouldin</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# cluster data
pam.mod &lt;- pam(iris.data,5) # create five clusters
v.pred &lt;- as.integer(pam.mod$clustering) # get cluster ids associated to given data objects

# compute intercluster distances and intracluster diameters
cls.scatt1 &lt;- cls.scatt.data(iris.data, v.pred)
cls.scatt2 &lt;- cls.scatt.data(iris.data, v.pred, dist="manhattan")
cls.scatt3 &lt;- cls.scatt.data(iris.data, v.pred, dist="correlation")

# the same using dissimilarity matrix
iris.diss.mx &lt;- as.matrix(daisy(iris.data))
cls.scatt4 &lt;- cls.scatt.diss.mx(iris.diss.mx, v.pred)
</code></pre>

<hr>
<h2 id='cls.set.section'>Section of two subsets - External Measure utilities</h2><span id='topic+cls.set.section'></span>

<h3>Description</h3>

<p>Function finds section of two different subsets comming from the same data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cls.set.section(clust1, clust2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cls.set.section_+3A_clust1">clust1</code></td>
<td>

<p>n x 2 integer <code>matrix</code> or <code>data.frame</code>. 
First column gives information about object number in data set in increasing order.
Second column store information about cluster id the object is assigned to.
If matrix is not an integer type, it will be coerced with warning.
</p>
</td></tr>
<tr><td><code id="cls.set.section_+3A_clust2">clust2</code></td>
<td>

<p>n x 2 integer <code>matrix</code> or <code>data.frame</code>.
First column gives information about object number in data set in increasing order.
Second column store information about cluster id the object is assigned to.
If matrix is not an integer type, it will be coerced with warning.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let A and B be two different subsamples of the same data set. 
Each subset is partitioned into P(A) and P(B) cluster sets. Information about object and cluster id's 
for pairs (A,P(A)) and (B,P(B)) are stored in <code>matrices clust1</code> and <code>clust2</code>. 
Function creates matrix which represents section of A and B. 
</p>


<h3>Value</h3>

<p><code>cls.set.section</code> returns a n x 3 integer <code>matrix</code>.
First column gives information about object number in dataset in increasing order.
Second column store information about cluster id the object is assigned to. 
Information is taken from <code>clust1 vector</code>
The same is for the third column but cluster id is taken from <code>vector clust2</code>.
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>See Also</h3>

<p>Function preapres data for further computation. Result mostly is used in: 
<code><a href="#topic+std.ext">std.ext</a></code>, <code><a href="#topic+dot.product">dot.product</a></code>, <code><a href="#topic+confusion.matrix">confusion.matrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create two different subsamples 
mx1 &lt;- matrix(as.integer( c(1,2,3,4,5,6,1,1,2,2,3,3) ), 6, 2 )
mx2 &lt;- matrix(as.integer( c(1,2,4,5,6,7,1,1,2,2,3,3) ), 6, 2 )
# find section
m = cls.set.section(mx1,mx2)
</code></pre>

<hr>
<h2 id='cls.stab.sim.ind'>Cluster Stability - Similarity Index and Pattern-wise Stability Approaches</h2><span id='topic+cls.stab.sim.ind'></span><span id='topic+cls.stab.opt.assign'></span>

<h3>Description</h3>

<p><code>cls.stab.sim.ind</code> and <code>cls.stab.opt.assign</code> reports validation measures for clustering results. Both functions return lists of  
cluster stability results computed according to similarity index and pattern-wise stability approaches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cls.stab.sim.ind( data, cl.num, rep.num, subset.ratio, clust.method,
                   method.type, sim.ind.type, fast, ... )
cls.stab.opt.assign( data, cl.num, rep.num, subset.ratio, clust.method,
                      method.type, fast, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cls.stab.sim.ind_+3A_data">data</code></td>
<td>

<p><code>numeric matrix</code> or <code>data.frame</code> where columns correspond to variables and rows to 
observations.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind_+3A_cl.num">cl.num</code></td>
<td>

<p>integer <code>vector</code> with information about numbers of cluster to which <code>data</code> will be partitioned.
If vector is not an integer type, it will be coerced with warning.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind_+3A_rep.num">rep.num</code></td>
<td>

<p>integer number which tells how many pairs of data subsets will be partitioned for particular number of clusters.
The results of partitioning for given pair of subsets is used to compute similarity indices (in case of <code>cls.stab.sim.ind</code>) 
or <em>pattern-wise stability</em> (in case of <code>cls.stab.opt.assign</code>, for more details see references). 
By default <code>rep.num</code> value is 10. If wrong argument is applied it will be repaced with default value.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind_+3A_subset.ratio">subset.ratio</code></td>
<td>

<p>a number comming from (0,1) section which tells how big data subsets should be. 0 means empty subset, 1 means all data. 
By default <code>subset.ratio</code> is set to 0.75. If wrong argument is applied it will be repaced with default value.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind_+3A_clust.method">clust.method</code></td>
<td>

<p>string vector with names of cluster algorithms to be used. Available are:
&quot;agnes&quot;, &quot;diana&quot;, &quot;hclust&quot;, &quot;kmeans&quot;, &quot;pam&quot;, &quot;clara&quot;. Combinations are also possible.
By default <code>c("agnes","pam")</code> vector is applied.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind_+3A_method.type">method.type</code></td>
<td>

<p>string vector with information useful only in context of &quot;agnes&quot; and &quot;hclust&quot; algorithms . Available are:
&quot;single&quot;, &quot;average&quot;, &quot;complete&quot;, &quot;ward&quot; and &quot;weighted&quot; (for more details see <code><a href="cluster.html#topic+agnes">agnes</a></code>,
<code><a href="stats.html#topic+hclust">hclust</a></code> ). The last type is applicable only for &quot;agnes&quot;. Combinations are also possible.
By default <code>c("single","average")</code> vector is applied.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind_+3A_sim.ind.type">sim.ind.type</code></td>
<td>

<p>string vector with information useful only for <code>cls.stab.sim.ind</code> function. User is able to choose which 
similarity indices (external measures) to use to compare two partitionings. Available are:
&quot;dot.pr&quot;, &quot;sim.ind&quot;, &quot;rand&quot;, &quot;jaccard&quot; (for more details see <code><a href="#topic+similarity.index">similarity.index</a></code>,
<code><a href="#topic+dot.product">dot.product</a></code>, <code><a href="#topic+std.ext">std.ext</a></code>). Combinations are also possible.
By default <code>c("dot.pr","sim.ind")</code> vector is applied.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind_+3A_fast">fast</code></td>
<td>

<p>logical argument which sets the way of computing cluster stability for hierarchical algorithms. By default it is set to
TRUE, which means that each result produced by hierarchical algorithm is partitioned for the number of clusters chosen in 
<code>cl.num</code> argument and given clustering results are put for further computation. In this way computation of cluster 
stability is faster. If wrong argument is applied it will be repaced with default value.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind_+3A_...">...</code></td>
<td>

<p>additional parameters for clustering algorithms. Note: use with caution! Different clustering methods chosen in <code>clust.method</code> have 
different set of parameter names - mixing them often disallow any cluster algorithm to run.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Both functions realize cluster stability approaches described in <em>Detecting stable clusters using principal component analysis</em> (see references).
</p>
<p>The <code>cls.stab.sim.ind</code> function realizes algorithm given in <em>chapter 3.1</em> where only cosine similarity index (see <code><a href="#topic+dot.product">dot.product</a></code>) 
is introduced as a similarity index between two different partitionings. This function realize this cluster stability approach also for other 
similarity indices such us <code><a href="#topic+similarity.index">similarity.index</a></code>, <code><a href="#topic+clv.Rand">clv.Rand</a></code> and <code><a href="#topic+clv.Jaccard">clv.Jaccard</a></code>.
The important thing is that <code>similarity index</code> (if chosen) produced by this function is not exactly the same as index produced by
<code><a href="#topic+similarity.index">similarity.index</a></code> function. The value of the <code><a href="#topic+similarity.index">similarity.index</a></code> is a number which depends on number of clusters.
Eg. if two &quot;n-clusters&quot; partitionings are compared the value always will be a number which belong to the <code>[1/n, 1]</code> section. That means the 
results produced by this similarity index are not comparable for different number of clusters. That's why each result is scaled thanks to 
the linear function <code>f:[1/n, 1] -&gt; [0, 1]</code> where &quot;n&quot; is a number of clusters.
The results' layout is described in <em>Value</em> section.
</p>
<p>The <code>cls.stab.opt.assign</code> function realizes algorithm given in <em>chapter 3.2</em> where <em>pattern-wise agreement</em> and 
<em>pattern-wise stability</em> was introduced. Function returns the lowest <em>pattern-wise stability</em> value for given number of
clusters. The results' layout is described in <em>Value</em> section.
</p>
<p>It often happens that clustering algorithms can't produce amount of clusters that user wants. In this situation only the warning is 
produced and cluster stability is computed for partitionings with unequal number of clusters.
</p>
<p>The cluster stability will not be calculated for all cluster numbers that are bigger than the subset size.
For example if <code>data</code> contains about 20 objects and the <code>subset.ratio</code> equals 0.5 then the highest cluster number to 
calculate is 10. In that case all elements above 10 will be removed from <code>cl.num</code> vector.
</p>


<h3>Value</h3>

<p><code>cls.stab.sim.ind</code> returns a list of lists of matrices. Each matrix consists of the set of external similarity indices (which one similarity 
index see below) where number of columns is equal to <code>cl.num</code> vector length and row number is equal to <code>rep.num</code> value what means
that each column contain a set of similarity indices computed for fixed number of clusters.
The order of the matricides depends on three input arguments: <code>clust.method</code>, <code>method.type</code>, and <code>sim.ind.type</code>.
Combination of <code>clust.method</code> and <code>method.type</code> give a names for elements listed in the first list. Each element of this list is also a 
list type where each element name correspond to one of similarity index type chosen thanks to <code>sim.ind.type</code> argument. 
The order of the names exactly match to the order given in those arguments description. It is easy to understand after considering the 
following example. <br />
Let say we are running <code>cls.stab.sim.ind</code> with default arguments then the results will be given in the following order:
<code>$agnes.single$dot.pr</code>, <code>$agnes.single$sim.ind</code>, <code>$agnes.average$dot.pr</code>, <code>$agnes.average$sim.ind</code>, <code>$pam$dot.pr</code>, 
<code>$pam$sim.ind</code>. <br /><br />
</p>
<p><code>cls.stab.opt.assign</code> returns a list of vectors. Each vector consists of the set of cluster stability indices described in 
<em>Detecting stable clusters using principal component analysis</em> (see references). Vector length is equal to <code>cl.num</code> vector length what 
means that each position in vector is assigned to proper clusters' number given in <code>cl.num</code> argument.
The order of the vectors depends on two input arguments: <code>clust.method</code>, <code>method.type</code>. The order of the names exactly match to the order 
given in arguments description. It is easy to understand after considering the following example. <br />
Let say we are running <code>cls.stab.opt.assign</code> with <code>c("pam", "kmeans", "hclust", "agnes")</code> as <code>clust.method</code> and <code>c("ward","average")</code> 
as <code>method.type</code> then the results will be given in the following order:
<code>$hclust.average</code>, <code>$hclust.ward</code>, <code>$agnes.average</code>, <code>$agnes.ward</code>, <code>$kmeans</code>, <code>$pam</code>.<br />
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>A. Ben-Hur and I. Guyon <em>Detecting stable clusters using principal component analysis</em>,
<a href="http://citeseerx.ist.psu.edu/">http://citeseerx.ist.psu.edu/</a>
</p>
<p>C. D. Giurcaneanu, I. Tabus, I. Shmulevich, W. Zhang <em>Stability-Based Cluster Analysis Applied To Microarray Data</em>, 
<a href="http://citeseerx.ist.psu.edu/">http://citeseerx.ist.psu.edu/</a>.
</p>
<p>T. Lange, V. Roth, M. L. Braun and J. M. Buhmann <em>Stability-Based Validation of Clustering Solutions</em>,
<a href="ml-pub.inf.ethz.ch/publications/papers/2004/lange.neco_stab.03.pdf">ml-pub.inf.ethz.ch/publications/papers/2004/lange.neco_stab.03.pdf</a>
</p>


<h3>See Also</h3>

<p>Advanced cluster stability functions:
<code><a href="#topic+cls.stab.sim.ind.usr">cls.stab.sim.ind.usr</a></code>, <code><a href="#topic+cls.stab.opt.assign.usr">cls.stab.opt.assign.usr</a></code>. 
</p>
<p>Functions that compare two different partitionings:
<code><a href="#topic+clv.Rand">clv.Rand</a></code>, <code><a href="#topic+dot.product">dot.product</a></code>, <code><a href="#topic+similarity.index">similarity.index</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# fix arguments for cls.stab.* function
iter = c(2,3,4,5,6,7,9,12,15)
smp.num = 5
ratio = 0.8

res1 = cls.stab.sim.ind( iris.data, iter, rep.num=smp.num, subset.ratio=0.7,
                         sim.ind.type=c("rand","dot.pr","sim.ind") )
res2 = cls.stab.opt.assign( iris.data, iter, clust.method=c("hclust","kmeans"),
                             method.type=c("single","average") )

print(res1)
boxplot(res1$agnes.average$sim.ind)
plot(res2$hclust.single)

</code></pre>

<hr>
<h2 id='cls.stab.sim.ind.usr'>Cluster Stability - Similarity Index and Pattern-wise Stability Approaches with User Defined Cluster Algorithms</h2><span id='topic+cls.stab.sim.ind.usr'></span><span id='topic+cls.stab.opt.assign.usr'></span><span id='topic+cls.alg'></span>

<h3>Description</h3>

<p><code>cls.stab.sim.ind.usr</code> and <code>cls.stab.opt.assign.usr</code> reports validation measures for clustering results. Both functions return lists of  
cluster stability results computed for user defined cluster algorithms according to similarity index and pattern-wise stability approaches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cls.stab.sim.ind.usr( data, cl.num, clust.alg, sim.ind.type, rep.num, subset.ratio )
cls.stab.opt.assign.usr( data, cl.num, clust.alg, rep.num, subset.ratio )
cls.alg( clust.method, clust.wrap, fast )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cls.stab.sim.ind.usr_+3A_data">data</code></td>
<td>

<p><code>numeric matrix</code> or <code>data.frame</code> where columns correspond to variables and rows to 
observations.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind.usr_+3A_cl.num">cl.num</code></td>
<td>

<p>integer <code>vector</code> with information about numbers of cluster to which <code>data</code> will be partitioned.
If vector is not an integer type, it will be coerced with warning.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind.usr_+3A_clust.alg">clust.alg</code></td>
<td>

<p>there are two possible types of input:
</p>
<p>1. clustering function that takes two arguments: &quot;data&quot; to be partitioned described in <code>data</code> section 
and &quot;clust.num&quot; that represents number of cluster to which data will be partitioned. Function represents partitioning algorithm.
</p>
<p>2. an object of type &quot;cls.alg&quot; returned by <code>cls.alg</code> function (see &quot;Details&quot; for explanation). Object represents 
hierarchical algorithm.
</p>
</td></tr> 
<tr><td><code id="cls.stab.sim.ind.usr_+3A_clust.method">clust.method</code></td>
<td>

<p>hierarchical clustering function that takes only one argument named &quot;data&quot; described in <code>data</code> section.
Function should return hierarchical structure that might be applied as parameter to <code>clust.wrap</code> function. 
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind.usr_+3A_clust.wrap">clust.wrap</code></td>
<td>

<p>cluster function that takes exactly two arguments: &quot;clust.res&quot; that represents the result of <code>clust.method</code> function and
&quot;clust.num&quot; which is the number of clusters to which &quot;clust.res&quot; is going to be cut. Function should return integer vector that 
represents object id (comming from <code>data</code> set) to cluster id (integer between 1 and <code>clust.num</code>) association.
</p>
</td></tr> 
<tr><td><code id="cls.stab.sim.ind.usr_+3A_sim.ind.type">sim.ind.type</code></td>
<td>

<p>string vector with information useful only for <code>cls.stab.sim.ind.usr</code> function. User is able to choose which 
similarity indicies (external measures) to use to compare two partitionings. Available are:
&quot;dot.pr&quot;, &quot;sim.ind&quot;, &quot;rand&quot;, &quot;jaccard&quot; (for more details see <code><a href="#topic+similarity.index">similarity.index</a></code>,
<code><a href="#topic+dot.product">dot.product</a></code>, <code><a href="#topic+std.ext">std.ext</a></code>). Combinations are also possible.
By default <code>c("dot.pr","sim.ind")</code> vector is applied.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind.usr_+3A_rep.num">rep.num</code></td>
<td>

<p>integer number which tells how many pairs of data subsets will be partitioned for particular number of clusters.
The results of partitioning for given pair of subsets is used to compute similarity indices (in case of <code>cls.stab.sim.ind.usr</code>) 
or <em>pattern-wise stability</em> (in case of <code>cls.stab.opt.assign.usr</code>, for more details see references). 
By default <code>rep.num</code> value is 10. If wrong argument is applied it will be repaced with default value.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind.usr_+3A_subset.ratio">subset.ratio</code></td>
<td>

<p>a number comming from (0,1) section which tells how big data subsets should be. 0 means empty subset, 1 means all data. 
By default <code>subset.ratio</code> is set to 0.75. If wrong argument is applied it will be repaced with default value.
</p>
</td></tr>
<tr><td><code id="cls.stab.sim.ind.usr_+3A_fast">fast</code></td>
<td>

<p>logical argument which sets the way of computing cluster stability for hierarchical algorithms. By default it is set to
TRUE, which means that each result produced by hierarchical algorithm is partitioned for the number of clusters chosen in 
<code>cl.num</code> argument and given clustering results are put for further computation. In this way computation of cluster 
stability is faster. If wrong argument is applied it will be repaced with default value.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Both functions realize cluster stability approaches described in <em>Detecting stable clusters using principal component analysis</em> 
chapters <em>3.1</em> and <em>3.2</em> (see references).
</p>
<p>The <code>cls.stab.sim.ind.usr</code> as well as <code>cls.stab.opt.assign.usr</code> do the same thing as <code><a href="#topic+cls.stab.sim.ind">cls.stab.sim.ind</a></code> and 
<code><a href="#topic+cls.stab.opt.assign">cls.stab.opt.assign</a></code> functions. Main difference is that using this functions user is able to define and apply its own cluster 
algorithm to measure its cluster stability. For that reason <code>clust.alg</code> argument is introduced. This argument may represent partitioning 
algorithm (by passing it directly as a function) or hierarchical algorithm (by passing an object of &quot;cls.alg&quot; type produced by <code>cls.alg</code> 
function).
</p>
<p>If a partitioning algorithm is going to be used the decalration of this function that represents this algorithm should always look 
like this: <code> function(data, clust.num) { ... return(integer.vector)} </code>.
As an output function should always return integer vector that represents single clustering result on <code>data</code>. 
</p>
<p>If a hierarchical algorithm is going to be used user has to use helper <code>cls.alg</code> function that produces an object of &quot;cls.alg&quot; type.
This object encapsulates a pair of methods that are used in hierarchical version (which is faster if the <code>fast</code> argument is not FALSE) 
of cluster stability approach. These methods are:<br />
1. <em>clust.method</em> - which builds hierarchical structure that might be cut. The declaration of this function should always look like 
this one: <kbd> function(data) { ... return(hierarchical.struct) } </kbd>,<br />
2. <em>clust.wrap</em> - which cuts this hierarchical structure to <code>clust.num</code> clusters. This function definition should always look 
like this one: <kbd> function(clust.res, clust.num) { ... return(integer.vector)} </kbd>. As an output function should 
always return integer vector that represents single clustering result on <code>clust.res</code>.
</p>
<p><code>cls.alg</code> function has also third argument that indicates if fast computation should be taken (when <code>TRUE</code>) or if these two 
methods should be converted to one partitioning algorithm and to be run as a normal partitioning algorithm.
</p>
<p>Well defined cluster functions &quot;f&quot; should always follow this rules (size(data) means number of object to be partitioned, 
res - integer vector with cluster ids):<br />
1. when <code>data</code> is empty or <code>cl.num</code> is less than 2 or more than <code>size(data)</code> then <code>f(data, cl.num)</code> returns error.
2. if <code>f(data, cl.num) -&gt; res</code> then length(res) == size(data),<br />
3. if <code>f(data, cl.num) -&gt; res</code> then for all &quot;elem&quot; in &quot;res&quot; the folowing condition is true: <code>0 &lt; elem &lt;= cl.num</code>.
</p>
<p>It often happens that clustering algorithms can't produce amount of clusters that user wants. In this situation only the warning is 
produced and cluster stability is computed for partitionings with unequal number of clusters.
</p>
<p>The cluster stability will not be calculated for all cluster numbers that are bigger than the subset size.
For example if <code>data</code> contains about 20 objects and the <code>subset.ratio</code> equals 0.5 then the highest cluster number to 
calculate is 10. In that case all elements above 10 will be removed from <code>cl.num</code> vector.
</p>


<h3>Value</h3>

<p><code>cls.stab.sim.ind.usr</code> returns a lists of matrices. Each matrix consists of the set of external similarity indices (which one similarity 
index see below) where number of columns is equal to <code>cl.num</code> vector length and row number is equal to <code>rep.num</code> value what means
that each column contain a set of similarity indices computed for fixed number of clusters. The order of the matrices depends on 
<code>sim.ind.type</code> argument. Each element of this list correspond to one of similarity index type chosen thanks to <code>sim.ind.type</code> argument. 
The order of the names exactly match to the order given in those arguments description.<br />
</p>
<p><code>cls.stab.opt.assign.usr</code> returns a vector. The vector consists of the set of cluster stability indices described in 
<em>Detecting stable clusters using principal component analysis</em> chapter <em>3.2</em> (see references). Vector length is equal to <code>cl.num</code> vector length what 
means that each position in vector is assigned to proper clusters' number given in <code>cl.num</code> argument.
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>A. Ben-Hur and I. Guyon <em>Detecting stable clusters using principal component analysis</em>,
<a href="http://citeseerx.ist.psu.edu/">http://citeseerx.ist.psu.edu/</a>
</p>
<p>C. D. Giurcaneanu, I. Tabus, I. Shmulevich, W. Zhang <em>Stability-Based Cluster Analysis Applied To Microarray Data</em>, 
<a href="http://citeseerx.ist.psu.edu/">http://citeseerx.ist.psu.edu/</a>.
</p>
<p>T. Lange, V. Roth, M. L. Braun and J. M. Buhmann <em>Stability-Based Validation of Clustering Solutions</em>,
<a href="ml-pub.inf.ethz.ch/publications/papers/2004/lange.neco_stab.03.pdf">ml-pub.inf.ethz.ch/publications/papers/2004/lange.neco_stab.03.pdf</a>
</p>


<h3>See Also</h3>

<p>Other cluster stability methods:
<code><a href="#topic+cls.stab.sim.ind">cls.stab.sim.ind</a></code>, <code><a href="#topic+cls.stab.opt.assign">cls.stab.opt.assign</a></code>.
</p>
<p>Functions that compare two different partitionings:
<code><a href="#topic+clv.Rand">clv.Rand</a></code>, <code><a href="#topic+dot.product">dot.product</a></code>,<code><a href="#topic+similarity.index">similarity.index</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# example of wrapper for partitioning algorithm 
pam.clust &lt;- function(data, clust.num) pam(data, clust.num, cluster.only=TRUE)

# example of wrapper for hierarchical algorithm
cutree.wrap &lt;- function(clust.res, clust.num)  cutree(clust.res, clust.num)
agnes.single &lt;- function(data) agnes(data, method="single") 

# converting hierarchical algorithm to partitioning one
agnes.part1 &lt;- function(data, clust.num) cutree.wrap( agnes.single(data), clust.num )
# the same using "cls.alg"
agnes.part2 &lt;- cls.alg(agnes.single, cutree.wrap, fast=FALSE)

# fix arguments for cls.stab.* function
iter = c(2,4,5,7,9,12,15)

res1 = cls.stab.sim.ind.usr( iris.data, iter, pam.clust, 
    sim.ind.type=c("rand","dot.pr","sim.ind"), rep.num=5, subset.ratio=0.7 )
res2 = cls.stab.opt.assign.usr( iris.data, iter, clust.alg=cls.alg(agnes.single, cutree.wrap) )

res3 = cls.stab.sim.ind.usr( iris.data, iter, agnes.part1,
     sim.ind.type=c("rand","dot.pr","sim.ind"), rep.num=5, subset.ratio=0.7 )
res4 = cls.stab.opt.assign.usr( iris.data, iter, clust.alg=agnes.part2 )

print(res1)
boxplot(res1$sim.ind)
plot(res2)


</code></pre>

<hr>
<h2 id='clv.Davies.Bouldin'>Davies-Bouldin Index -  Internal Measure</h2><span id='topic+clv.Davies.Bouldin'></span>

<h3>Description</h3>

<p>Function computes <em>Dunn index</em> - internal measure for given data and its partitioning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clv.Davies.Bouldin( index.list, intracls, intercls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clv.Davies.Bouldin_+3A_index.list">index.list</code></td>
<td>
 
<p>object returned by function <code><a href="#topic+cls.scatt.data">cls.scatt.data</a></code> or <code><a href="#topic+cls.scatt.diss.mx">cls.scatt.diss.mx</a></code>.
</p>
</td></tr>
<tr><td><code id="clv.Davies.Bouldin_+3A_intracls">intracls</code></td>
<td>

<p>string <code>vector</code> containing one or more names of intra cluster distances. 
Available are:
</p>

<table>
<tr>
 <td style="text-align: left;">
		1. if <code>index.list</code> is produced by <code><a href="#topic+cls.scatt.data">cls.scatt.data</a></code>:
			complete | average | centroid, </td>
</tr>
<tr>
 <td style="text-align: left;">
		2. if <code>index.list</code> is produced by <code><a href="#topic+cls.scatt.diss.mx">cls.scatt.diss.mx</a></code>: 
			complete | average.
		</td>
</tr>

</table>

</td></tr>
<tr><td><code id="clv.Davies.Bouldin_+3A_intercls">intercls</code></td>
<td>

<p>string <code>vector</code> containing one or more names of inter cluster diameters. 
Available are:
</p>

<table>
<tr>
 <td style="text-align: left;">
		1. if <code>index.list</code> is produced by <code><a href="#topic+cls.scatt.data">cls.scatt.data</a></code>: 
			single | complete | average | centroid | aveToCent | hausdorff. </td>
</tr>
<tr>
 <td style="text-align: left;">
		2. if <code>index.list</code> is produced by <code><a href="#topic+cls.scatt.diss.mx">cls.scatt.diss.mx</a></code>:
			single | complete | average | hausdorff.
		</td>
</tr>

</table>

</td></tr>
</table>


<h3>Details</h3>

<p><em>Davies-Bouldin</em> index is given by equation:
</p>
<p>DB = (1/|C|) sum{forall i in 1:|C|} max[ i != j ] { (diam(Ci) + diam(Cj))/dist(Ci,Cj) }
</p>

<table>
<tr>
 <td style="text-align: left;">
	i,j  </td><td style="text-align: left;"> - numbers of clusters which come from the same partitioning, </td>
</tr>
<tr>
 <td style="text-align: left;">
	<em>dist(Ck,Cl)</em> </td><td style="text-align: left;"> - inter cluster distance between clusters Ck and Cl, </td>
</tr>
<tr>
 <td style="text-align: left;">
	<em>diam(Cm)</em> </td><td style="text-align: left;"> - intra cluster diameter computed for cluster Cm, </td>
</tr>
<tr>
 <td style="text-align: left;"> 
	|C| </td><td style="text-align: left;"> - number of clusters.
	</td>
</tr>

</table>



<h3>Value</h3>

<p>As output user gets the matrix of <em>Davies-Bouldin</em> indices.
Matrix dimension depends on how many <em>diam</em> and <em>dist</em> measures are chosen by the user, 
normally <code>dim(D)=c(length(intercls),length(intracls))</code>.
Each pair: (inter-cluster dist, intra-cluster diam) have its own position in result matrix. 
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>M. Halkidi, Y. Batistakis, M. Vazirgiannis <em>Clustering Validity Checking Methods : Part II</em>,
<a href="http://citeseer.ist.psu.edu/537304.html">http://citeseer.ist.psu.edu/537304.html</a>
</p>


<h3>See Also</h3>

<p>Functions which produce <em>index.list</em> input argument: <code><a href="#topic+cls.scatt.data">cls.scatt.data</a></code>, <code><a href="#topic+cls.scatt.diss.mx">cls.scatt.diss.mx</a></code>.
Related functions: <a href="#topic+clv.Dunn">clv.Dunn</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# cluster data
agnes.mod &lt;- agnes(iris.data) # create cluster tree 
v.pred &lt;- as.integer(cutree(agnes.mod,5)) # "cut" the tree 

intraclust = c("complete","average","centroid")
interclust = c("single", "complete", "average","centroid", "aveToCent", "hausdorff")

# compute Davies-Bouldin indicies (also Dunn indicies)
# 1. optimal solution:

# compute intercluster distances and intracluster diameters
cls.scatt &lt;- cls.scatt.data(iris.data, v.pred, dist="manhattan")

# once computed valuse use in both functions
dunn1 &lt;- clv.Dunn(cls.scatt, intraclust, interclust)
davies1 &lt;- clv.Davies.Bouldin(cls.scatt, intraclust, interclust)

# 2. functional solution:

# define new Dunn and Davies.Bouldin functions
Dunn &lt;- function(data,clust) 
  clv.Dunn( cls.scatt.data(data,clust),
     intracls = c("complete","average","centroid"), 
     intercls = c("single", "complete", "average","centroid", "aveToCent", "hausdorff")
  )
Davies.Bouldin &lt;- function(data,clust) 
  clv.Davies.Bouldin( cls.scatt.data(data,clust),
    intracls = c("complete","average","centroid"),
    intercls = c("single", "complete", "average","centroid", "aveToCent", "hausdorff")
  )

# compute indicies
dunn2 &lt;- Dunn(iris.data, v.pred)
davies2 &lt;- Davies.Bouldin(iris.data, v.pred)
</code></pre>

<hr>
<h2 id='clv.Dens_bw'>Inter-cluster density - Internal Measure</h2><span id='topic+clv.DensBw'></span>

<h3>Description</h3>

<p>Function computes <em>inter-cluster density</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clv.DensBw(data, clust, scatt.obj, dist="euclidean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clv.Dens_bw_+3A_data">data</code></td>
<td>
<p><code>matrix</code> or <code>data.frame</code> where columns correspond to variables and rows to observations</p>
</td></tr>
<tr><td><code id="clv.Dens_bw_+3A_clust">clust</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning.</p>
</td></tr>
<tr><td><code id="clv.Dens_bw_+3A_scatt.obj">scatt.obj</code></td>
<td>
<p> object returned by <code><a href="#topic+clv.Scatt">clv.Scatt</a></code> function. </p>
</td></tr>
<tr><td><code id="clv.Dens_bw_+3A_dist">dist</code></td>
<td>
<p>chosen metric: &quot;euclidean&quot; (default value), &quot;manhattan&quot;, &quot;correlation&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The definition of <em>inter-cluster density</em> is given by equation: 
</p>
<p><code>Dens_bw</code> = 1/(|C|*(|C|-1)) * sum{forall i in 1:|C|} sum{forall j in 1:|C| and j != i}
<em>density</em>(u(i,j))/max{<em>density</em>(v(i)), <em>density</em>(v(j))}
</p>
<p>where:
</p>

<table>
<tr>
 <td style="text-align: left;">
		|C| </td><td style="text-align: left;"> - number of clusters, </td>
</tr>
<tr>
 <td style="text-align: left;">
		v(i), v(j) </td><td style="text-align: left;"> - centers of clusters i and j, </td>
</tr>
<tr>
 <td style="text-align: left;">
		u(i,j) </td><td style="text-align: left;"> - middle point of the line segment defined by the clusters' centers v(i), v(j), </td>
</tr>
<tr>
 <td style="text-align: left;">
		density(x) </td><td style="text-align: left;"> - see below.
	</td>
</tr>

</table>

<p>Let define function <em>f</em>(x,u):
</p>

<table>
<tr>
 <td style="text-align: left;">
		<em>f</em>(x,u) = 0 </td><td style="text-align: left;"> if dist(x,u) &gt; <em>stdev</em> (<em>stdev</em> is defined in 
						<code><a href="#topic+clv.Scatt">clv.Scatt</a></code>) </td>
</tr>
<tr>
 <td style="text-align: left;">
		<em>f</em>(x,u) = 1 </td><td style="text-align: left;"> otherwise 
	</td>
</tr>

</table>

<p>Function <em>f</em> is used in definition of <em>density</em>(u):
</p>
<p><em>density</em>(u) = sum{forall i in 1:n(i,j)} <em>f</em>(xi,u)
</p>
<p>where n(i,j) is the number of objects which belongs to clusters i and j and xi is such object.
</p>
<p>This value is used by <code><a href="#topic+clv.SDbw">clv.SDbw</a></code>.
</p>


<h3>Value</h3>

<p>As result <code>Dens_bw</code> value is returned.
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>See Also</h3>

 <p><code><a href="#topic+clv.SD">clv.SD</a></code> and <code><a href="#topic+clv.SDbw">clv.SDbw</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# cluster data
agnes.mod &lt;- agnes(iris.data) # create cluster tree 
v.pred &lt;- as.integer(cutree(agnes.mod,5)) # "cut" the tree 

# compute Dens_bw index
scatt &lt;- clv.Scatt(iris.data, v.pred)
dens.bw &lt;- clv.DensBw(iris.data, v.pred, scatt)
</code></pre>

<hr>
<h2 id='clv.Dis'>Total separation between clusters - Internal Measure</h2><span id='topic+clv.Dis'></span>

<h3>Description</h3>

<p>Function computes <em>total separation between clusters</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clv.Dis(cluster.center)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clv.Dis_+3A_cluster.center">cluster.center</code></td>
<td>

<p><code>numeric matrix</code> or <code>data.frame</code> where columns correspond to variables and rows 
cluster centers.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The definition of total separation between clusters is given by equation: 
</p>
<p><code>Dis</code> = (Dmax/Dmin) * sum{forall i in 1:|C|} 1 /( sum{forall j in 1:|C|} ||vi - vj|| )
</p>
<p>where:
</p>

<table>
<tr>
 <td style="text-align: left;">
	|C| </td><td style="text-align: left;"> - number of clusters, </td>
</tr>
<tr>
 <td style="text-align: left;">
	vi, vj </td><td style="text-align: left;"> - centers of clusters i and j, </td>
</tr>
<tr>
 <td style="text-align: left;">
	Dmax </td><td style="text-align: left;"> - defined as: max{||vi - vj||: vi,vj - centers of clusters }, </td>
</tr>
<tr>
 <td style="text-align: left;">
	Dmin </td><td style="text-align: left;"> - defined as: min{||vi - vj||: vi,vj - centers of clusters }, </td>
</tr>
<tr>
 <td style="text-align: left;">
	||x|| </td><td style="text-align: left;"> - means: sqrt(x*x').
	</td>
</tr>

</table>

<p>This value is a part of <code><a href="#topic+clv.SD">clv.SD</a></code> and <code><a href="#topic+clv.SDbw">clv.SDbw</a></code>.
</p>


<h3>Value</h3>

<p>As result <code>Dis</code> value is returned.
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>M. Haldiki, Y. Batistakis, M. Vazirgiannis <em>On Clustering Validation Techniques</em>,
<a href="http://citeseer.ist.psu.edu/513619.html">http://citeseer.ist.psu.edu/513619.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clv.SD">clv.SD</a></code> and <code><a href="#topic+clv.SDbw">clv.SDbw</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# cluster data
agnes.mod &lt;- agnes(iris.data) # create cluster tree 
v.pred &lt;- as.integer(cutree(agnes.mod,5)) # "cut" the tree 

# compute Dis index
scatt &lt;- clv.Scatt(iris.data, v.pred)
dis &lt;- clv.Dis(scatt$cluster.center)
</code></pre>

<hr>
<h2 id='clv.Dunn'>Dunn Index - Internal Measure</h2><span id='topic+clv.Dunn'></span>

<h3>Description</h3>

<p>Function computes <em>Dunn index</em> - internal measure for given data and its partitioning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clv.Dunn( index.list, intracls, intercls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clv.Dunn_+3A_index.list">index.list</code></td>
<td>
 
<p>object returned by function <code><a href="#topic+cls.scatt.data">cls.scatt.data</a></code> or <code><a href="#topic+cls.scatt.diss.mx">cls.scatt.diss.mx</a></code>.
</p>
</td></tr>
<tr><td><code id="clv.Dunn_+3A_intracls">intracls</code></td>
<td>

<p>string <code>vector</code> containing one or more names of intra cluster distances. 
Available are:
</p>

<table>
<tr>
 <td style="text-align: left;">
		1. if <code>index.list</code> is produced by <code><a href="#topic+cls.scatt.data">cls.scatt.data</a></code>:
			complete | average | centroid, </td>
</tr>
<tr>
 <td style="text-align: left;">
		2. if <code>index.list</code> is produced by <code><a href="#topic+cls.scatt.diss.mx">cls.scatt.diss.mx</a></code>: 
			complete | average.
		</td>
</tr>

</table>

</td></tr>
<tr><td><code id="clv.Dunn_+3A_intercls">intercls</code></td>
<td>

<p>string <code>vector</code> containing one or more names of inter cluster diameters. 
Available are:
</p>

<table>
<tr>
 <td style="text-align: left;">
		1. if <code>index.list</code> is produced by <code><a href="#topic+cls.scatt.data">cls.scatt.data</a></code>: 
			single | complete | average | centroid | aveToCent | hausdorff. </td>
</tr>
<tr>
 <td style="text-align: left;">
		2. if <code>index.list</code> is produced by <code><a href="#topic+cls.scatt.diss.mx">cls.scatt.diss.mx</a></code>:
			single | complete | average | hausdorff.
		</td>
</tr>

</table>

</td></tr>
</table>


<h3>Details</h3>

<p>Dunn index:
</p>
<p>D = [ min{ k,l - numbers of clusters } <em>dist</em>(Ck, Cl) ]/[  max{ m - cluster number } <em>diam</em>(Cm) ]
</p>

<table>
<tr>
 <td style="text-align: left;">
	k,l,m  </td><td style="text-align: left;"> - numbers of clusters which come from the same partitioning, </td>
</tr>
<tr>
 <td style="text-align: left;">
	<em>dist(Ck,Cl)</em> </td><td style="text-align: left;"> - inter cluster distance between clusters Ck and Cl, </td>
</tr>
<tr>
 <td style="text-align: left;">
	<em>diam(Cm)</em> </td><td style="text-align: left;"> - intra cluster diameter computed for cluster Cm. 
	</td>
</tr>

</table>



<h3>Value</h3>

<p>As output user gets matrix of <em>Dunn</em> indices.
Matrix dimension depends on how many <em>diam</em> and <em>dist</em> measures are chosen by the user, 
normally <code>dim(D)=c(length(intercls),length(intracls))</code>.
Each pair: (inter-cluster dist, intra-cluster diam) have its own position in result matrix. 
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>M. Halkidi, Y. Batistakis, M. Vazirgiannis <em>Clustering Validity Checking Methods : Part II</em>,
<a href="http://citeseer.ist.psu.edu/537304.html">http://citeseer.ist.psu.edu/537304.html</a>
</p>


<h3>See Also</h3>

<p>Functions which produce <em>index.list</em> input argument: <code><a href="#topic+cls.scatt.data">cls.scatt.data</a></code>, <code><a href="#topic+cls.scatt.diss.mx">cls.scatt.diss.mx</a></code>.
Related functions: <a href="#topic+clv.Davies.Bouldin">clv.Davies.Bouldin</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# cluster data
agnes.mod &lt;- agnes(iris.data) # create cluster tree 
v.pred &lt;- as.integer(cutree(agnes.mod,5)) # "cut" the tree 

intraclust = c("complete","average","centroid")
interclust = c("single", "complete", "average","centroid", "aveToCent", "hausdorff")

# compute Dunn indicies (also Davies-Bouldin indicies)
# 1. optimal solution:

# compute intercluster distances and intracluster diameters
cls.scatt &lt;- cls.scatt.data(iris.data, v.pred, dist="manhattan")

# once computed valuse use in both functions
dunn1 &lt;- clv.Dunn(cls.scatt, intraclust, interclust)
davies1 &lt;- clv.Davies.Bouldin(cls.scatt, intraclust, interclust)

# 2. functional solution:

# define new Dunn and Davies.Bouldin functions
Dunn &lt;- function(data,clust) 
  clv.Dunn( cls.scatt.data(data,clust),
     intracls = c("complete","average","centroid"), 
     intercls = c("single", "complete", "average","centroid", "aveToCent", "hausdorff")
  )
Davies.Bouldin &lt;- function(data,clust) 
  clv.Davies.Bouldin( cls.scatt.data(data,clust),
    intracls = c("complete","average","centroid"),
    intercls = c("single", "complete", "average","centroid", "aveToCent", "hausdorff")
  )

# compute indicies
dunn2 &lt;- Dunn(iris.data, v.pred)
davies2 &lt;- Davies.Bouldin(iris.data, v.pred)
</code></pre>

<hr>
<h2 id='clv.Scatt'>Average scattering for clusters - Internal Measure</h2><span id='topic+clv.Scatt'></span>

<h3>Description</h3>

<p>Function computes <em>average scattering for clusters</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clv.Scatt(data, clust, dist="euclidean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clv.Scatt_+3A_data">data</code></td>
<td>

<p><code>numeric matrix</code> or <code>data.frame</code> where columns correspond to variables and 
rows to observations
</p>
</td></tr>
<tr><td><code id="clv.Scatt_+3A_clust">clust</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning.
</p>
</td></tr>
<tr><td><code id="clv.Scatt_+3A_dist">dist</code></td>
<td>
<p>choosen metric: &quot;euclidean&quot; (default value),  &quot;manhattan&quot;, &quot;correlation&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <em>scatter for set</em> X assigned as <em>sigma</em>(X) be defined as
vector of variances computed for particular dimensions. 
<em>Average scattering for clusters</em> is defined as:
</p>
<p><code>Scatt</code> = (1/|C|) * sum{forall i in 1:|C|} ||<em>sigma</em>(Ci)||/||<em>sigma</em>(X)||
</p>
<p>where:
</p>

<table>
<tr>
 <td style="text-align: left;">
	|C| </td><td style="text-align: left;"> - number of clusters, </td>
</tr>
<tr>
 <td style="text-align: left;">
	i </td><td style="text-align: left;"> - cluster id, </td>
</tr>
<tr>
 <td style="text-align: left;">
	Ci </td><td style="text-align: left;"> - cluster with id 'i', </td>
</tr>
<tr>
 <td style="text-align: left;">
	X </td><td style="text-align: left;"> - set with all objects, </td>
</tr>
<tr>
 <td style="text-align: left;">
	||x|| </td><td style="text-align: left;"> - sqrt(x*x').
	</td>
</tr>

</table>

<p><em>Standard deviation</em> is defined as:
</p>
<p><code>stdev</code> = (1/|C|) * sqrt( sum{forall i in 1:|C|} ||<em>sigma</em>(Ci)||  )
</p>


<h3>Value</h3>

<p>As result <code>list</code> with three values is returned.
</p>

<table>
<tr>
 <td style="text-align: left;">
		<code>Scatt</code> </td><td style="text-align: left;"> - <em>average scattering for clusters</em> value, </td>
</tr>
<tr>
 <td style="text-align: left;">
		<code>stdev</code> </td><td style="text-align: left;"> - <em>standard deviation</em> value, </td>
</tr>
<tr>
 <td style="text-align: left;">
		<code>cluster.center</code> </td><td style="text-align: left;"> - numeric <code>matrix</code> where columns 
		correspond to variables and rows to cluster centers.
	</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>M. Haldiki, Y. Batistakis, M. Vazirgiannis <em>On Clustering Validation Techniques</em>,
<a href="http://citeseer.ist.psu.edu/513619.html">http://citeseer.ist.psu.edu/513619.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clv.SD">clv.SD</a></code> and <code><a href="#topic+clv.SDbw">clv.SDbw</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# cluster data
agnes.mod &lt;- agnes(iris.data) # create cluster tree 
v.pred &lt;- as.integer(cutree(agnes.mod,5)) # "cut" the tree 

# compute Scatt index
scatt &lt;- clv.Scatt(iris.data, v.pred)
</code></pre>

<hr>
<h2 id='clv.SD+2C+20clv.SDbw'>SD, SDbw - Internal Measures</h2><span id='topic+clv.SD'></span><span id='topic+clv.SDbw'></span>

<h3>Description</h3>

<p>Function computes <em>SD</em> and <em><code class="reqn">\textrm{S\_Dbw}</code> validity indices</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clv.SD(scatt, dis, alfa)
clv.SDbw(scatt, dens)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clv.SD+2B2C+2B20clv.SDbw_+3A_scatt">scatt</code></td>
<td>
 
<p><em>average scattering for cluster</em> value computed using <code><a href="#topic+clv.Scatt">clv.Scatt</a></code>
function.
</p>
</td></tr>
<tr><td><code id="clv.SD+2B2C+2B20clv.SDbw_+3A_dis">dis</code></td>
<td>
 
<p><em>total separation between clusters</em> value computed using <code><a href="#topic+clv.Dis">clv.Dis</a></code>
function.
</p>
</td></tr>
<tr><td><code id="clv.SD+2B2C+2B20clv.SDbw_+3A_dens">dens</code></td>
<td>
 
<p><em>inter-cluster density</em> value computed using <code><a href="#topic+clv.DensBw">clv.DensBw</a></code>
function.
</p>
</td></tr> 
<tr><td><code id="clv.SD+2B2C+2B20clv.SDbw_+3A_alfa">alfa</code></td>
<td>
 
<p>weighting factor (normally equal to Dis(cmax) where cmax is the maximum number 
of input clusters).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>SD validity index</em> is defined by equation:	
</p>
<p>SD = <em>scatt</em>*alfa + <em>dis</em>
</p>
<p>where <em>scatt</em> means <em>average scattering for clusters</em> defined in <code><a href="#topic+clv.Scatt">clv.Scatt</a></code>.
<em><code class="reqn">\textrm{S\_Dbw}</code> validity index</em> is defined by equation:
</p>
<p><code class="reqn">\textrm{S\_Dbw}</code> = <em>scatt</em> + <em>dens</em>
</p>
<p>where <em>dens</em> is defined in <code><a href="#topic+clv.DensBw">clv.DensBw</a></code>.
</p>


<h3>Value</h3>

<p>As result of <code>clv.SD</code> function <em>SD validity index</em> is returned.
As result of <code>clv.SDbw</code> function <em><code class="reqn">\textrm{S\_Dbw}</code> validity index</em> is returned.
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>M. Haldiki, Y. Batistakis, M. Vazirgiannis <em>On Clustering Validation Techniques</em>,
<a href="http://citeseer.ist.psu.edu/513619.html">http://citeseer.ist.psu.edu/513619.html</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+clv.Scatt">clv.Scatt</a></code>, <code><a href="#topic+clv.Dis">clv.Dis</a></code> and <code><a href="#topic+clv.DensBw">clv.DensBw</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># load and prepare
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# cluster data
agnes.mod &lt;- agnes(iris.data) # create cluster tree 
v.pred &lt;- as.integer(cutree(agnes.mod,5)) # "cut" the tree 

# prepare proper input data for SD and S_Dbw indicies
scatt &lt;- clv.Scatt(iris.data, v.pred)
dis &lt;- clv.Dis(scatt$cluster.center)
dens.bw &lt;- clv.DensBw(iris.data, v.pred, scatt)

# compute  SD and S_Dbw indicies
SD &lt;- clv.SD(scatt$Scatt, dis, alfa=5) # alfa is equal to number of clusters 
SDbw &lt;- clv.SDbw(scatt$Scatt, dens.bw)
</code></pre>

<hr>
<h2 id='confusion.matrix'>Confusion Matrix - External Measures, Cluster Stability</h2><span id='topic+confusion.matrix'></span>

<h3>Description</h3>

<p>For two different partitioning function computes <em>confusion matrix</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confusion.matrix(clust1, clust2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confusion.matrix_+3A_clust1">clust1</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning. 
</p>
</td></tr>
<tr><td><code id="confusion.matrix_+3A_clust2">clust2</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let P and P' be two different partitioning of the same data. Partitionings are represent as two  
vectors <code>clust1, clust2</code>. Both vectors should have the same length. 
Confusion matrix measures the size of intersection between clusters comming from P and P'
according to equation:
</p>
<p>M[i,j] = | intersection of P(i) and P'(j) |
</p>
<p>where:
</p>

<table>
<tr>
 <td style="text-align: left;">
		P(i)  </td><td style="text-align: left;"> - cluster which belongs to partitioning P, </td>
</tr>
<tr>
 <td style="text-align: left;"> 
		P'(j) </td><td style="text-align: left;"> - cluster which belongs to partitioning P', </td>
</tr>
<tr>
 <td style="text-align: left;">
		|A|   </td><td style="text-align: left;"> - cardinality of set A.	
	</td>
</tr>

</table>



<h3>Value</h3>

<p><code>cls.set.section</code> returns a n x m integer <code>matrix</code>
where n = |P| and m = |P'| defined above.
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>See Also</h3>

<p>Result used in <code><a href="#topic+similarity.index">similarity.index</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create two different subsamples 
mx1 &lt;- matrix(as.integer( c(1,2,3,4,5,6,1,1,2,2,3,3) ), 6, 2 )
mx2 &lt;- matrix(as.integer( c(1,2,4,5,6,7,1,1,2,2,3,3) ), 6, 2 )
# find section
m = cls.set.section(mx1,mx2)
confusion.matrix(as.integer(m[,2]),as.integer(m[,3]))
</code></pre>

<hr>
<h2 id='connectivity'>Connectivity Index - Internal Measure</h2><span id='topic+connectivity'></span><span id='topic+connectivity.diss.mx'></span>

<h3>Description</h3>

<p>Function evaluates <em>connectivity</em> index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>connectivity(data,clust,neighbour.num, dist="euclidean")
connectivity.diss.mx(diss.mx,clust,neighbour.num)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="connectivity_+3A_data">data</code></td>
<td>

<p><code>numeric matrix</code> or <code>data.frame</code> where columns correspond to variables and rows to 
observations
</p>
</td></tr>
<tr><td><code id="connectivity_+3A_diss.mx">diss.mx</code></td>
<td>

<p>square, symetric <code>numeric matrix</code> or <code>data.frame</code>, representation of 
dissimilarity matrix where infomartion about distances between objects is stored.
</p>
</td></tr>
<tr><td><code id="connectivity_+3A_clust">clust</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning. 
</p>
</td></tr>
<tr><td><code id="connectivity_+3A_neighbour.num">neighbour.num</code></td>
<td>
<p>value which tells how many nearest neighbors for every object should be checked.</p>
</td></tr>
<tr><td><code id="connectivity_+3A_dist">dist</code></td>
<td>
<p>chosen metric: &quot;euclidean&quot; (default value), &quot;manhattan&quot;, &quot;correlation&quot; 
(variable enable only in <code>connectivity</code> function).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For given data and its partitioning <em>connectivity</em> index is computed.
For choosen pattern <code>neighbour.num</code> nearest neighbours are found and sorted from closest 
to most further. Alghorithm checks if those neighbours are 
assigned to the same cluster. At the beggining <em>connectivity</em> value is equal 0 and increase 
with value:
</p>

<table>
<tr>
 <td style="text-align: left;">
	1/i </td><td style="text-align: left;"> when i-th nearest neighbour is not assigned to the same cluster, </td>
</tr>
<tr>
 <td style="text-align: left;">
	0   </td><td style="text-align: left;"> otherwise.
	</td>
</tr>

</table>

<p>Procedure is repeated for all patterns which comming from our data set. All values received 
for particular pattern are added and creates main <em>connectivity</em> index.
</p>


<h3>Value</h3>

<p><code>connectivity</code> returns a <em>connectivity</em> value.
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>J. Handl, J. Knowles and D. B. Kell <em>Sumplementary material to computational cluster validation in post-genomic data analysis</em>, 
<a href="http://dbkgroup.org/handl/clustervalidation/supplementary.pdf">http://dbkgroup.org/handl/clustervalidation/supplementary.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# cluster data
pam.mod &lt;- pam(iris.data,5) # create five clusters
v.pred &lt;- as.integer(pam.mod$clustering) # get cluster ids associated to gived data objects

# compute connectivity index using data and its clusterization
conn1 &lt;- connectivity(iris.data, v.pred, 10)
conn2 &lt;- connectivity(iris.data, v.pred, 10, dist="manhattan")
conn3 &lt;- connectivity(iris.data, v.pred, 10, dist="correlation")

# the same using dissimilarity matrix
iris.diss.mx &lt;- as.matrix(daisy(iris.data))
conn4 &lt;- connectivity.diss.mx(iris.diss.mx, v.pred, 10)
</code></pre>

<hr>
<h2 id='dot.product'>Cosine similarity measure - External Measure, Cluster Stability</h2><span id='topic+dot.product'></span>

<h3>Description</h3>

<p>Similarity index based on dot product is the measure which estimates how those two different partitionings, that comming from one 
dataset, are different from each other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dot.product(clust1, clust2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dot.product_+3A_clust1">clust1</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning. 
</p>
</td></tr>
<tr><td><code id="dot.product_+3A_clust2">clust2</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two input <code>vectors</code> keep information about two different partitionings of the same 
subset comming from one data set. For each partitioning (let say P and P') its matrix 
representation is created. Let P[i,j] and P'[i,j] each defines as:
</p>
<p>P[i,j] = 1 when object i and j belongs to the same cluster and i != j <br />
P[i,j] = 0 in other case
</p>
<p>Two matrices are needed to compute <em>dot product</em> using formula: 
</p>
<p>&lt;P,P'&gt; = sum(forall i and j) P[i,j]*P'[i,j]
</p>
<p>This <em>dot product</em> satisfy Cauchy-Schwartz inequality &lt;P,P'&gt; &lt;= &lt;P,P&gt;*&lt;P',P'&gt;.
As result we get <em>cosine similarity measure</em>: &lt;P,P'&gt;/sqrt(&lt;P,P&gt;*&lt;P',P'&gt;) 
</p>


<h3>Value</h3>

<p><code>dot.product</code> returns a <em>cosine similarity measure</em> of two partitionings. 
<code>NaN</code> is returned when in any partitioning each cluster contains only one object.
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>A. Ben-Hur and I. Guyon <em>Detecting stable clusters using principal component analysis</em>,
<a href="http://citeseer.ist.psu.edu/528061.html">http://citeseer.ist.psu.edu/528061.html</a>
</p>
<p>T. Lange, V. Roth, M. L. Braun and J. M. Buhmann <em>Stability-Based Validation of Clustering Solutions</em>,
<a href="ml-pub.inf.ethz.ch/publications/papers/2004/lange.neco_stab.03.pdf">ml-pub.inf.ethz.ch/publications/papers/2004/lange.neco_stab.03.pdf</a>
</p>


<h3>See Also</h3>

 
<p>Other external measures:
<code><a href="#topic+std.ext">std.ext</a></code>, <code><a href="#topic+similarity.index">similarity.index</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># dot.product function(and also similarity.index) is used to compute 
# cluster stability, additional stability functions will be 
# defined - as its arguments some additional functions (wrappers) 
# will be needed

# define wrappers
pam.wrapp &lt;-function(data)
{
	return( as.integer(data$clustering) )
}

identity &lt;- function(data) { return( as.integer(data) ) }

agnes.average &lt;- function(data, clust.num)
{
	return( cutree( agnes(data,method="average"), clust.num ) )
}

# define cluster stability function - cls.stabb

# cls.stabb arguments description:
# data - data to be clustered
# clust.num - number of clusters to which data will be clustered
# sample.num - number of pairs of data subsets to be clustered,
#              each clustered pair will be given as argument for 
#              dot.product and similarity.index functions 
# ratio - value comming from (0,1) section: 
#		  0 - means sample emtpy subset,
#		  1 - means chose all "data" objects
# method - cluster method (see wrapper functions)
# wrapp - function which extract information about cluster id assigned 
#         to each clustered object 

# as a result mean of dot.product (and similarity.index) results,
# computed for subsampled pairs of subsets is given
cls.stabb &lt;- function( data, clust.num, sample.num , ratio, method, wrapp  )
{
	dot.pr  = 0
	sim.ind = 0
	obj.num = dim(data)[1]

	for( j in 1:sample.num )
	{
		smp1 = sort( sample( 1:obj.num, ratio*obj.num ) )
		smp2 = sort( sample( 1:obj.num, ratio*obj.num ) )

		d1 = data[smp1,]
		cls1 = wrapp( method(d1,clust.num) )

		d2 = data[smp2,]
		cls2 = wrapp( method(d2,clust.num) )

		clsm1 = t(rbind(smp1,cls1))
		clsm2 = t(rbind(smp2,cls2))

		m = cls.set.section(clsm1, clsm2)
		cls1 = as.integer(m[,2])
		cls2 = as.integer(m[,3])
		cnf.mx = confusion.matrix(cls1,cls2)
		std.ms = std.ext(cls1,cls2)
		
		# external measures - compare partitioning
		dt = dot.product(cls1,cls2)
		si = similarity.index(cnf.mx)

		if( !is.nan(dt) ) dot.pr = dot.pr + dt/sample.num 
		sim.ind = sim.ind + si/sample.num 
	}
	return( c(dot.pr, sim.ind) )
}

# load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# fix arguments for cls.stabb function
iter = c(2,3,4,5,6,7,9,12,15)
smp.num = 5
sub.smp.ratio = 0.8

# cluster stability for PAM
print("PAM method:")
for( i in iter )
{
	result = cls.stabb(iris.data, clust.num=i, sample.num=smp.num,
           ratio=sub.smp.ratio, method=pam, wrapp=pam.wrapp)
	print(result)
}

# cluster stability for Agnes (average-link)
print("Agnes (single) method:")
for( i in iter )
{
	result = cls.stabb(iris.data, clust.num=i, sample.num=smp.num,
            ratio=sub.smp.ratio, method=agnes.average, wrapp=identity)
	print(result)
}
</code></pre>

<hr>
<h2 id='similarity.index'>Similarity index based on confusion matrix - External Measure, Cluster Stability</h2><span id='topic+similarity.index'></span>

<h3>Description</h3>

<p>Similarity index based on confusion matrix is the measure which estimates how those two different partitionings, that comming from one 
dataset, are different from each other.
For given <code>matrix</code> returned by <code><a href="#topic+confusion.matrix">confusion.matrix</a></code> function 
<em>similarity index</em> is found.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>similarity.index(cnf.mx)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="similarity.index_+3A_cnf.mx">cnf.mx</code></td>
<td>

<p>not negative, integer <code>matrix</code> or <code>data.frame</code> which represents 
object returned by <code><a href="#topic+confusion.matrix">confusion.matrix</a></code> function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let M is n x m (n &lt;= m) <em>confusion matrix</em> for partitionings P and P'.
Any one to one function <em>sigma</em>: {1,2,...,n} -&gt; {1,2,... ,m}.
is called <em>assignment</em> (or also <em>association</em>).
Using set of <em>assignment</em> functions, A(P,P') index defined as:
</p>
<p>A(P,P') = max{ sum( forall i in 1:length(<em>sigma</em>) )
M[i,<em>sigma(i)</em>]: <em>sigma</em> is an <em>assignment</em> }
</p>
<p>is found. (<em>Assignment</em> which satisfy above equation is called <em>optimal assignment</em>).
Using this value we can compute <em>similarity index</em> S(P.P') = (A(P,P') - 1)/(N - 1) where 
N is quantity of partitioned objects (here is equal to <code>sum(M)</code>).
</p>


<h3>Value</h3>

<p><code>similarity.index</code> returns value from section [0,1] which is a measure of similarity 
between two different partitionings. Value 1 means that we have two the same partitionings.
</p>


<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>C. D. Giurcaneanu, I. Tabus, I. Shmulevich, W. Zhang <em>Stability-Based Cluster Analysis Applied To Microarray Data</em>, 
<a href="http://citeseer.ist.psu.edu/577114.html">http://citeseer.ist.psu.edu/577114.html</a>.
</p>
<p>T. Lange, V. Roth, M. L. Braun and J. M. Buhmann <em>Stability-Based Validation of Clustering Solutions</em>,
<a href="ml-pub.inf.ethz.ch/publications/papers/2004/lange.neco_stab.03.pdf">ml-pub.inf.ethz.ch/publications/papers/2004/lange.neco_stab.03.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+confusion.matrix">confusion.matrix</a></code> as matrix representation of two partitionings.
Other functions created to compare two different partitionings:
<code><a href="#topic+std.ext">std.ext</a></code>, <code><a href="#topic+dot.product">dot.product</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># similarity.index function(and also dot.product) is used to compute 
# cluster stability, additional stability functions will be 
# defined - as its arguments some additional functions (wrappers) 
# will be needed

# define wrappers
pam.wrapp &lt;-function(data)
{
	return( as.integer(data$clustering) )
}

identity &lt;- function(data) { return( as.integer(data) ) }

agnes.average &lt;- function(data, clust.num)
{
	return( cutree( agnes(data,method="average"), clust.num ) )
}

# define cluster stability function - cls.stabb

# cls.stabb arguments description:
# data - data to be clustered
# clust.num - number of clusters to which data will be clustered
# sample.num - number of pairs of data subsets to be clustered,
#              each clustered pair will be given as argument for 
#              dot.product and similarity.index functions 
# ratio - value comming from (0,1) section: 
#		  0 - means sample emtpy subset,
#		  1 - means chose all "data" objects
# method - cluster method (see wrapper functions)
# wrapp - function which extract information about cluster id assigned 
#         to each clustered object 

# as a result mean of similarity.index (and dot.product) results,
# computed for subsampled pairs of subsets is given
cls.stabb &lt;- function( data, clust.num, sample.num , ratio, method, wrapp  )
{
	dot.pr  = 0
	sim.ind = 0
	obj.num = dim(data)[1]

	for( j in 1:sample.num )
	{
		smp1 = sort( sample( 1:obj.num, ratio*obj.num ) )
		smp2 = sort( sample( 1:obj.num, ratio*obj.num ) )

		d1 = data[smp1,]
		cls1 = wrapp( method(d1,clust.num) )

		d2 = data[smp2,]
		cls2 = wrapp( method(d2,clust.num) )

		clsm1 = t(rbind(smp1,cls1))
		clsm2 = t(rbind(smp2,cls2))

		m = cls.set.section(clsm1, clsm2)
		cls1 = as.integer(m[,2])
		cls2 = as.integer(m[,3])
		cnf.mx = confusion.matrix(cls1,cls2)
		std.ms = std.ext(cls1,cls2)
		
		# external measures - compare partitioning
		dt = dot.product(cls1,cls2)
		si = similarity.index(cnf.mx)

		if( !is.nan(dt) ) dot.pr = dot.pr + dt/sample.num 
		sim.ind = sim.ind + si/sample.num 
	}
	return( c(dot.pr, sim.ind) )
}

# load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# fix arguments for cls.stabb function
iter = c(2,3,4,5,6,7,9,12,15)
smp.num = 5
sub.smp.ratio = 0.8

# cluster stability for PAM
print("PAM method:")
for( i in iter )
{
	result = cls.stabb(iris.data, clust.num=i, sample.num=smp.num,
            ratio=sub.smp.ratio, method=pam, wrapp=pam.wrapp)
	print(result)
}

# cluster stability for Agnes (average-link)
print("Agnes (single) method:")
for( i in iter )
{
	result = cls.stabb(iris.data, clust.num=i, sample.num=smp.num,
            ratio=sub.smp.ratio, method=agnes.average, wrapp=identity)
	print(result)
}
</code></pre>

<hr>
<h2 id='std.ext'>Standard External Measures: Rand index, Jaccard coefficient etc. </h2><span id='topic+std.ext'></span><span id='topic+clv.Rand'></span><span id='topic+clv.Jaccard'></span><span id='topic+clv.Folkes.Mallows'></span><span id='topic+clv.Phi'></span><span id='topic+clv.Russel.Rao'></span>

<h3>Description</h3>

<p>Group of functions which compute standard external measures such as: 
Rand statistic and Folkes and Mallows index, Jaccard coefficient etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>std.ext(clust1, clust2)
clv.Rand(external.ind)
clv.Jaccard(external.ind)
clv.Folkes.Mallows(external.ind)
clv.Phi(external.ind)
clv.Russel.Rao(external.ind)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="std.ext_+3A_clust1">clust1</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning. 
</p>
</td></tr>
<tr><td><code id="std.ext_+3A_clust2">clust2</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning.
</p>
</td></tr>
<tr><td><code id="std.ext_+3A_external.ind">external.ind</code></td>
<td>
<p><code>vector</code> or <code>list</code> with four values SS,SD,DS,DD which are result of
function <code>std.ext</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two input <code>vectors</code> keep information about two different partitionings (let say P and P')
of the same data set X. We refer to a pair of points (xi, xj) (we assume that i != j) from the 
data set using the following terms:
</p>

<table>
<tr>
 <td style="text-align: left;">
		<code>SS</code> </td><td style="text-align: left;"> - number of pairs where both points belongs to the same cluster in 
		both partitionings, </td>
</tr>
<tr>
 <td style="text-align: left;">
		<code>SD</code> </td><td style="text-align: left;"> - number of pairs where both points belongs to the same cluster in partitioning P but 
		in P' do not, </td>
</tr>
<tr>
 <td style="text-align: left;">
		<code>DS</code> </td><td style="text-align: left;"> - number of pairs where in partitioning P both point belongs to different clusters 
		but in P' do not, </td>
</tr>
<tr>
 <td style="text-align: left;">
		<code>DD</code> </td><td style="text-align: left;"> - number of pairs where both objects belongs to different clusters in 
		both partitionings.
	</td>
</tr>

</table>

<p>Those values are used to compute (M = SS + SD + DS +DD):
</p>

<table>
<tr>
 <td style="text-align: left;">
	<em>Rand statistic</em> </td><td style="text-align: left;"> R = (SS + DD)/M </td>
</tr>
<tr>
 <td style="text-align: left;">
	<em>Jaccard coefficient</em> </td><td style="text-align: left;"> J = SS/(SS + SD + DS) </td>
</tr>
<tr>
 <td style="text-align: left;">
	<em>Folkes and Mallows index</em> </td><td style="text-align: left;"> FM = sqrt(SS/(SS + SD))*sqrt(SS/(SS + DS)) </td>
</tr>
<tr>
 <td style="text-align: left;">
	<em>Russel and Rao index</em> </td><td style="text-align: left;"> RR = SS/M </td>
</tr>
<tr>
 <td style="text-align: left;">
	<em>Phi index</em> </td><td style="text-align: left;"> Ph = (SS*DD - SD*DS)/((SS+SD)(SS+DS)(SD+DD)(DS+DD)).
	</td>
</tr>

</table>



<h3>Value</h3>


<table>
<tr>
 <td style="text-align: left;">
	<code>std.ext</code> returns a <code>list</code> containing four values: SS, SD, DS, DD. </td>
</tr>
<tr>
 <td style="text-align: left;">
	<code>clv.Rand</code> returns R value. </td>
</tr>
<tr>
 <td style="text-align: left;">
	<code>clv.Jaccard</code> returns J value. </td>
</tr>
<tr>
 <td style="text-align: left;">
	<code>clv.Folkes.Mallows</code> returns FM value. </td>
</tr>
<tr>
 <td style="text-align: left;">
	<code>clv.Phi</code> returns Ph value. </td>
</tr>
<tr>
 <td style="text-align: left;">
	<code>clv.Russel.Rao</code> returns RR value. 
	</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>G. Saporta and G. Youness <em>Comparing two partitions: Some Proposals and Experiments</em>. 
<a href="http://cedric.cnam.fr/PUBLIS/RC405.pdf">http://cedric.cnam.fr/PUBLIS/RC405.pdf</a>
</p>


<h3>See Also</h3>

<p>Other measures created to compare two partitionings: 
<code><a href="#topic+dot.product">dot.product</a></code>, <code><a href="#topic+similarity.index">similarity.index</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# cluster data
pam.mod &lt;- pam(iris.data,3) # create three clusters
v.pred &lt;- as.integer(pam.mod$clustering) # get cluster ids associated to given data objects
v.real &lt;- as.integer(iris$Species) # get also real cluster ids

# compare true clustering with those given by the algorithm
# 1. optimal solution:

# use only once std.ext function
std &lt;- std.ext(v.pred, v.real)
# to compute three indicies based on std.ext result
rand1 &lt;- clv.Rand(std)
jaccard1 &lt;- clv.Jaccard(std)
folk.mal1 &lt;- clv.Folkes.Mallows(std)

# 2. functional solution:

# prepare set of functions which compare two clusterizations
Rand &lt;- function(clust1,clust2) clv.Rand(std.ext(clust1,clust2))
Jaccard &lt;- function(clust1,clust2) clv.Jaccard(std.ext(clust1,clust2))
Folkes.Mallows &lt;- function(clust1,clust2) clv.Folkes.Mallows(std.ext(clust1,clust2))

# compute indicies
rand2 &lt;- Rand(v.pred,v.real)
jaccard2 &lt;- Jaccard(v.pred,v.real)
folk.mal2 &lt;- Folkes.Mallows(v.pred,v.real)
</code></pre>

<hr>
<h2 id='wcls+2Fbcls.matrix'>Matrix Cluster Scatter Measures</h2><span id='topic+wcls.matrix'></span><span id='topic+bcls.matrix'></span>

<h3>Description</h3>

<p>Functions compute two base matrix cluster scatter measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wcls.matrix(data,clust,cluster.center)
bcls.matrix(cluster.center,cluster.size,mean)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wcls+2B2Fbcls.matrix_+3A_data">data</code></td>
<td>

<p><code>numeric matrix</code> or <code>data.frame</code> where columns correspond to variables and 
rows to observations
</p>
</td></tr>
<tr><td><code id="wcls+2B2Fbcls.matrix_+3A_clust">clust</code></td>
<td>

<p>integer <code>vector</code> with information about cluster id the object is assigned to.
If vector is not integer type, it will be coerced with warning. 
</p>
</td></tr>
<tr><td><code id="wcls+2B2Fbcls.matrix_+3A_cluster.center">cluster.center</code></td>
<td>

<p><code>matrix</code> or <code>data.frame</code> where columns correspond to variables and rows to cluster centers 
defined by <code>data</code> and <code>clust</code> parameters. 
</p>
</td></tr>
<tr><td><code id="wcls+2B2Fbcls.matrix_+3A_cluster.size">cluster.size</code></td>
<td>

<p>integer <code>vector</code> with information about size of each cluster computed using <code>clust</code> 
vector.  
</p>
</td></tr>
<tr><td><code id="wcls+2B2Fbcls.matrix_+3A_mean">mean</code></td>
<td>
 <p><em>mean</em> of all data objects. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two base matrix scatter measures.
</p>
<p>1. <em>within-cluster scatter measure</em> defined as:
</p>
<p>W = sum(forall k in 1:cluster.num) W(k) 
</p>
<p>where W(k) = sum(forall x) (x - m(k))*(x - m(k))' 
</p>

<table>
<tr>
 <td style="text-align: left;">
	x </td><td style="text-align: left;"> - object belongs to cluster k, </td>
</tr>
<tr>
 <td style="text-align: left;">
	m(k) </td><td style="text-align: left;"> - center of cluster k.
	</td>
</tr>

</table>

<p>2. <em>between-cluster scatter measure</em> defined as:
</p>
<p>B = sum(forall k in 1:cluster.num) |C(k)|*( m(k) - m )*( m(k) - m )'
</p>

<table>
<tr>
 <td style="text-align: left;">
	|C(k)| </td><td style="text-align: left;"> - size of cluster k, </td>
</tr>
<tr>
 <td style="text-align: left;">
	m(k) </td><td style="text-align: left;"> - center of cluster k, </td>
</tr>
<tr>
 <td style="text-align: left;">
	m </td><td style="text-align: left;"> - center of all data objects.
	</td>
</tr>

</table>



<h3>Value</h3>


<table>
<tr>
 <td style="text-align: left;">
	<code>wcls.matrix</code> </td><td style="text-align: left;"> returns W matrix (<em>within-cluster scatter measure</em>),</td>
</tr>
<tr>
 <td style="text-align: left;">
	<code>bcls.matrix</code> </td><td style="text-align: left;"> returns B matrix (<em>between-cluster scatter measure</em>).
	</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Lukasz Nieweglowski</p>


<h3>References</h3>

<p>T. Hastie, R. Tibshirani, G. Walther <em>Estimating the number of data clusters via the Gap statistic</em>,
<a href="http://citeseer.ist.psu.edu/tibshirani00estimating.html">http://citeseer.ist.psu.edu/tibshirani00estimating.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load and prepare data
library(clv)
data(iris)
iris.data &lt;- iris[,1:4]

# cluster data
pam.mod &lt;- pam(iris.data,5) # create five clusters
v.pred &lt;- as.integer(pam.mod$clustering) # get cluster ids associated to given data objects

# compute cluster sizes, center of each cluster 
# and mean from data objects
cls.attr &lt;- cls.attrib(iris.data, v.pred)
center &lt;- cls.attr$cluster.center
size &lt;- cls.attr$cluster.size
iris.mean &lt;- cls.attr$mean

# compute matrix scatter measures
W.matrix &lt;- wcls.matrix(iris.data, v.pred, center)
B.matrix &lt;- bcls.matrix(center, size, iris.mean)
T.matrix &lt;- W.matrix + B.matrix

# example of indices based on W, B i T matrices
mx.scatt.crit1 = sum(diag(W.matrix))
mx.scatt.crit2 = sum(diag(B.matrix))/sum(diag(W.matrix))
mx.scatt.crit3 = det(W.matrix)/det(T.matrix)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
