<!DOCTYPE html><html><head><title>Help for package torchopt</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {torchopt}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#torchopt-package'><p>torchopt: Advanced Optimizers for Torch</p></a></li>
<li><a href='#optim_adabelief'><p>Adabelief optimizer</p></a></li>
<li><a href='#optim_adabound'><p>Adabound optimizer</p></a></li>
<li><a href='#optim_adahessian'><p>Adahessian optimizer</p></a></li>
<li><a href='#optim_adamw'><p>AdamW optimizer</p></a></li>
<li><a href='#optim_madgrad'><p>MADGRAD optimizer</p></a></li>
<li><a href='#optim_nadam'><p>Nadam optimizer</p></a></li>
<li><a href='#optim_qhadam'><p>QHAdam optimization algorithm</p></a></li>
<li><a href='#optim_radam'><p>AdamW optimizer</p></a></li>
<li><a href='#optim_swats'><p>SWATS optimizer</p></a></li>
<li><a href='#optim_yogi'><p>Yogi optimizer</p></a></li>
<li><a href='#state'><p>Imported function</p></a></li>
<li><a href='#state+26lt+3B-'><p>Imported function</p></a></li>
<li><a href='#test_optim'><p>Test optimization function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Advanced Optimizers for Torch</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.4</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Gilberto Camara &lt;gilberto.camara.inpe@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Optimizers for 'torch' deep learning library. These
    functions include recent results published in the literature and are
    not part of the optimizers offered in 'torch'. Prospective users
    should test these optimizers with their data, since performance
    depends on the specific problem being solved.  The packages includes
    the following optimizers: (a) 'adabelief' by Zhuang et al (2020),
    &lt;<a href="https://doi.org/10.48550/arXiv.2010.07468">doi:10.48550/arXiv.2010.07468</a>&gt;; (b) 'adabound' by Luo et al.(2019),
    &lt;<a href="https://doi.org/10.48550/arXiv.1902.09843">doi:10.48550/arXiv.1902.09843</a>&gt;; (c) 'adahessian' by Yao et al.(2021)
    &lt;<a href="https://doi.org/10.48550/arXiv.2006.00719">doi:10.48550/arXiv.2006.00719</a>&gt;; (d) 'adamw' by Loshchilov &amp; Hutter (2019),
    &lt;<a href="https://doi.org/10.48550/arXiv.1711.05101">doi:10.48550/arXiv.1711.05101</a>&gt;; (e) 'madgrad' by Defazio and Jelassi (2021),
    &lt;<a href="https://doi.org/10.48550/arXiv.2101.11075">doi:10.48550/arXiv.2101.11075</a>&gt;; (f) 'nadam' by Dozat (2019),
    <a href="https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf">https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf</a>; (g) 'qhadam' by
    Ma and Yarats(2019), &lt;<a href="https://doi.org/10.48550/arXiv.1810.06801">doi:10.48550/arXiv.1810.06801</a>&gt;; (h) 'radam' by Liu et al.
    (2019), &lt;<a href="https://doi.org/10.48550/arXiv.1908.03265">doi:10.48550/arXiv.1908.03265</a>&gt;; (i) 'swats' by Shekar and Sochee (2018),
    &lt;<a href="https://doi.org/10.48550/arXiv.1712.07628">doi:10.48550/arXiv.1712.07628</a>&gt;; (j) 'yogi' by Zaheer et al.(2019),
    <a href="https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization">https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization</a>. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/e-sensing/torchopt/">https://github.com/e-sensing/torchopt/</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, grDevices, stats, torch</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-06 12:45:26 UTC; gilbertocamara</td>
</tr>
<tr>
<td>Author:</td>
<td>Gilberto Camara [aut, cre],
  Rolf Simoes [aut],
  Daniel Falbel [aut],
  Felipe Souza [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-06 13:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='torchopt-package'>torchopt: Advanced Optimizers for Torch</h2><span id='topic+torchopt'></span><span id='topic+torchopt-package'></span>

<h3>Description</h3>

<p>Optimizers for 'torch' deep learning library. These functions include recent results published in the literature and are not part of the optimizers offered in 'torch'. Prospective users should test these optimizers with their data, since performance depends on the specific problem being solved. The packages includes the following optimizers: (a) 'adabelief' by Zhuang et al (2020), <a href="https://arxiv.org/abs/2010.07468">arXiv:2010.07468</a>; (b) 'adabound' by Luo et al.(2019), <a href="https://arxiv.org/abs/1902.09843">arXiv:1902.09843</a>; (c) 'adahessian' by Yao et al.(2021) <a href="https://arxiv.org/abs/2006.00719">arXiv:2006.00719</a>; (d) 'adamw' by Loshchilov &amp; Hutter (2019), <a href="https://arxiv.org/abs/1711.05101">arXiv:1711.05101</a>; (e) 'madgrad' by Defazio and Jelassi (2021), <a href="https://arxiv.org/abs/2101.11075">arXiv:2101.11075</a>; (f) 'nadam' by Dozat (2019), <a href="https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf">https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf</a>; (g) 'qhadam' by Ma and Yarats(2019), <a href="https://arxiv.org/abs/1810.06801">arXiv:1810.06801</a>; (h) 'radam' by Liu et al. (2019), <a href="https://arxiv.org/abs/1908.03265">arXiv:1908.03265</a>; (i) 'swats' by Shekar and Sochee (2018), <a href="https://arxiv.org/abs/1712.07628">arXiv:1712.07628</a>; (j) 'yogi' by Zaheer et al.(2019), &lt;https:://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization&gt;.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Gilberto Camara <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Rolf Simoes <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
</li>
<li><p> Daniel Falbel <a href="mailto:daniel.falbel@gmail.com">daniel.falbel@gmail.com</a>
</p>
</li>
<li><p> Felipe Souza <a href="mailto:felipe.carvalho@inpe.br">felipe.carvalho@inpe.br</a>
</p>
</li>
<li><p> Alber Sanchez <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/e-sensing/torchopt/">https://github.com/e-sensing/torchopt/</a>
</p>
</li></ul>


<hr>
<h2 id='optim_adabelief'>Adabelief optimizer</h2><span id='topic+optim_adabelief'></span>

<h3>Description</h3>

<p>R implementation of the adabelief optimizer proposed
by Zhuang et al (2020). We used the pytorch implementation
developed by the authors which is available at
https://github.com/jettify/pytorch-optimizer.
Thanks to Nikolay Novik of his work on python optimizers.
</p>
<p>The original implementation is licensed using the Apache-2.0 software license.
This implementation is also licensed using Apache-2.0 license.
</p>
<p>From the abstract by the paper by Zhuang et al (2021):
We propose Adabelief to simultaneously achieve three goals:
fast convergence as in adaptive methods, good generalization as in SGD,
and training stability. The intuition for AdaBelief is to adapt
the stepsize according to the &quot;belief&quot; in the current gradient direction.
Viewing the exponential moving average of the noisy gradient
as the prediction of the gradient at the next time step,
if the observed gradient greatly deviates from the prediction,
we distrust the current observation and take a small step;
if the observed gradient is close to the prediction,
we trust it and take a large step.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_adabelief(
  params,
  lr = 0.001,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 1e-06,
  weight_decouple = TRUE,
  fixed_decay = FALSE,
  rectify = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_adabelief_+3A_params">params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_adabelief_+3A_lr">lr</code></td>
<td>
<p>Learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_adabelief_+3A_betas">betas</code></td>
<td>
<p>Coefficients for computing running averages
of gradient and its square (default: (0.9, 0.999))</p>
</td></tr>
<tr><td><code id="optim_adabelief_+3A_eps">eps</code></td>
<td>
<p>Term added to the denominator to improve numerical
stability (default: 1e-16)</p>
</td></tr>
<tr><td><code id="optim_adabelief_+3A_weight_decay">weight_decay</code></td>
<td>
<p>Weight decay (L2 penalty) (default: 0)</p>
</td></tr>
<tr><td><code id="optim_adabelief_+3A_weight_decouple">weight_decouple</code></td>
<td>
<p>Use decoupled weight decay as is done in AdamW?</p>
</td></tr>
<tr><td><code id="optim_adabelief_+3A_fixed_decay">fixed_decay</code></td>
<td>
<p>This is used when weight_decouple is set as True.
When fixed_decay == True, weight decay is
W_new = W_old - W_old * decay.
When fixed_decay == False, the weight decay is
W_new = W_old - W_old * decay * learning_rate.
In this case, weight decay decreases with learning rate.</p>
</td></tr>
<tr><td><code id="optim_adabelief_+3A_rectify">rectify</code></td>
<td>
<p>Perform the rectified update similar to RAdam?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>


<h3>References</h3>

<p>Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda,
Nicha Dvornek, Xenophon Papademetris, James S. Duncan.
&quot;Adabelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients&quot;,
34th Conference on Neural Information Processing Systems (NeurIPS 2020),
Vancouver, Canada.
https://arxiv.org/abs/2010.07468
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_adabelief
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
</code></pre>

<hr>
<h2 id='optim_adabound'>Adabound optimizer</h2><span id='topic+optim_adabound'></span>

<h3>Description</h3>

<p>R implementation of the AdaBound optimizer proposed
by Luo et al.(2019). We used the implementation available at
https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/yogi.py.
Thanks to Nikolay Novik for providing the pytorch code.
</p>
<p>The original implementation is licensed using the Apache-2.0 software license.
This implementation is also licensed using Apache-2.0 license.
</p>
<p>AdaBound is a variant of the Adam stochastic optimizer which is
designed to be more robust to extreme learning rates.
Dynamic bounds are employed on learning rates,
where the lower and upper bound are initialized as zero and
infinity respectively, and they both smoothly converge to a
constant final step size. AdaBound can be regarded as an adaptive
method at the beginning of training, and thereafter it gradually and
smoothly transforms to SGD (or with momentum) as the time step increases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_adabound(
  params,
  lr = 0.001,
  betas = c(0.9, 0.999),
  final_lr = 0.1,
  gamma = 0.001,
  eps = 1e-08,
  weight_decay = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_adabound_+3A_params">params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_adabound_+3A_lr">lr</code></td>
<td>
<p>Learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_adabound_+3A_betas">betas</code></td>
<td>
<p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999))</p>
</td></tr>
<tr><td><code id="optim_adabound_+3A_final_lr">final_lr</code></td>
<td>
<p>Final (SGD) learning rate (default: 0.1)</p>
</td></tr>
<tr><td><code id="optim_adabound_+3A_gamma">gamma</code></td>
<td>
<p>Convergence speed of the bound functions
(default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_adabound_+3A_eps">eps</code></td>
<td>
<p>Term added to the denominator to improve numerical
stability (default: 1e-8)</p>
</td></tr>
<tr><td><code id="optim_adabound_+3A_weight_decay">weight_decay</code></td>
<td>
<p>Weight decay (L2 penalty) (default: 0)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>
<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>


<h3>References</h3>

<p>Liangchen Luo, Yuanhao Xiong, Yan Liu, Xu Sun,
&quot;Adaptive Gradient Methods with Dynamic Bound of Learning Rate&quot;,
International Conference on Learning Representations (ICLR), 2019.
https://arxiv.org/abs/1902.09843
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_adabound
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
</code></pre>

<hr>
<h2 id='optim_adahessian'>Adahessian optimizer</h2><span id='topic+optim_adahessian'></span>

<h3>Description</h3>

<p>R implementation of the Adahessian optimizer proposed
by Yao et al.(2020). The original implementation is available at
https://github.com/amirgholami/adahessian.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_adahessian(
  params,
  lr = 0.15,
  betas = c(0.9, 0.999),
  eps = 1e-04,
  weight_decay = 0,
  hessian_power = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_adahessian_+3A_params">params</code></td>
<td>
<p>Iterable of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_adahessian_+3A_lr">lr</code></td>
<td>
<p>Learning rate (default: 0.15).</p>
</td></tr>
<tr><td><code id="optim_adahessian_+3A_betas">betas</code></td>
<td>
<p>Coefficients for computing
running averages of gradient
and is square(default: (0.9, 0.999)).</p>
</td></tr>
<tr><td><code id="optim_adahessian_+3A_eps">eps</code></td>
<td>
<p>Term added to the denominator to improve
numerical stability (default: 1e-4).</p>
</td></tr>
<tr><td><code id="optim_adahessian_+3A_weight_decay">weight_decay</code></td>
<td>
<p>L2 penalty (default: 0).</p>
</td></tr>
<tr><td><code id="optim_adahessian_+3A_hessian_power">hessian_power</code></td>
<td>
<p>Hessian power (default: 1.0).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An optimizer object implementing the <code>step</code> and <code>zero_grad</code> methods.
</p>


<h3>Author(s)</h3>

<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>
<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>


<h3>References</h3>

<p>Yao, Z., Gholami, A., Shen, S., Mustafa, M., Keutzer, K.,
&amp; Mahoney, M. (2021).
ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning.
Proceedings of the AAAI Conference on Artificial Intelligence, 35(12),
10665-10673.
https://arxiv.org/abs/2006.00719
</p>

<hr>
<h2 id='optim_adamw'>AdamW optimizer</h2><span id='topic+optim_adamw'></span>

<h3>Description</h3>

<p>R implementation of the AdamW optimizer proposed
by Loshchilov &amp; Hutter (2019). We used the pytorch implementation
developed by Collin Donahue-Oponski available at:
https://gist.github.com/colllin/0b146b154c4351f9a40f741a28bff1e3
</p>
<p>From the abstract by the paper by Loshchilov &amp; Hutter (2019):
L2 regularization and weight decay regularization are equivalent for standard
stochastic gradient descent (when rescaled by the learning rate),
but as we demonstrate this is not the case for adaptive gradient algorithms,
such as Adam. While common implementations of these algorithms
employ L2 regularization (often calling it “weight decay”
in what may be misleading due to the inequivalence we expose),
we propose a simple modification to recover the original formulation of
weight decay regularization by decoupling the weight decay from the optimization
steps taken w.r.t. the loss function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_adamw(
  params,
  lr = 0.01,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 1e-06
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_adamw_+3A_params">params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_adamw_+3A_lr">lr</code></td>
<td>
<p>Learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_adamw_+3A_betas">betas</code></td>
<td>
<p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999))</p>
</td></tr>
<tr><td><code id="optim_adamw_+3A_eps">eps</code></td>
<td>
<p>Term added to the denominator to improve numerical
stability (default: 1e-8)</p>
</td></tr>
<tr><td><code id="optim_adamw_+3A_weight_decay">weight_decay</code></td>
<td>
<p>Weight decay (L2 penalty) (default: 1e-6)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>


<h3>References</h3>

<p>Ilya Loshchilov, Frank Hutter,
&quot;Decoupled Weight Decay Regularization&quot;,
International Conference on Learning Representations (ICLR) 2019.
https://arxiv.org/abs/1711.05101
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_adamw
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
</code></pre>

<hr>
<h2 id='optim_madgrad'>MADGRAD optimizer</h2><span id='topic+optim_madgrad'></span>

<h3>Description</h3>

<p>A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic
Optimization (MADGRAD) is a general purpose optimizer that
can be used in place of SGD or Adam may converge faster and generalize
better. Currently GPU-only. Typically, the same learning rate schedule
that is used for SGD or Adam may be used. The overall learning rate is
not comparable to either method and should be determined by a
hyper-parameter sweep.
</p>
<p>MADGRAD requires less weight decay than other methods, often as little as
zero. Momentum values used for SGD or Adam's beta1 should work here also.
</p>
<p>On sparse problems both weight_decay and momentum should be set to 0.
(not yet supported in the R implementation).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_madgrad(params, lr = 0.01, momentum = 0.9, weight_decay = 0, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_madgrad_+3A_params">params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_madgrad_+3A_lr">lr</code></td>
<td>
<p>Learning rate (default: 1e-2).</p>
</td></tr>
<tr><td><code id="optim_madgrad_+3A_momentum">momentum</code></td>
<td>
<p>Momentum value in  the range [0,1) (default: 0.9).</p>
</td></tr>
<tr><td><code id="optim_madgrad_+3A_weight_decay">weight_decay</code></td>
<td>
<p>Weight decay, i.e. a L2 penalty (default: 0).</p>
</td></tr>
<tr><td><code id="optim_madgrad_+3A_eps">eps</code></td>
<td>
<p>Term added to the denominator outside of
the root operation to improve numerical stability
(default: 1e-6).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Daniel Falbel, <a href="mailto:dfalbel@gmail.com">dfalbel@gmail.com</a>
</p>


<h3>References</h3>

<p>Aaron Defazio, Samy Jelassi,
&quot;Adaptivity without Compromise: A Momentumized, Adaptive, Dual
Averaged Gradient Method for Stochastic Optimization&quot;.
https://arxiv.org/abs/2101.11075
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_madgrad
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
</code></pre>

<hr>
<h2 id='optim_nadam'>Nadam optimizer</h2><span id='topic+optim_nadam'></span>

<h3>Description</h3>

<p>R implementation of the Nadam optimizer proposed
by Dazat (2016).
</p>
<p>From the abstract by the paper by Dozat (2016):
This work aims to improve upon the recently proposed and
rapidly popularized optimization algorithm Adam (Kingma &amp; Ba, 2014).
Adam has two main components—a momentum component and an adaptive
learning rate component. However, regular momentum can be shown conceptually
and empirically to be inferior to a similar algorithm known as
Nesterov’s accelerated gradient (NAG).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_nadam(
  params,
  lr = 0.002,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 0,
  momentum_decay = 0.004
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_nadam_+3A_params">params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_nadam_+3A_lr">lr</code></td>
<td>
<p>Learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_nadam_+3A_betas">betas</code></td>
<td>
<p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999)).</p>
</td></tr>
<tr><td><code id="optim_nadam_+3A_eps">eps</code></td>
<td>
<p>Term added to the denominator to improve numerical
stability (default: 1e-8).</p>
</td></tr>
<tr><td><code id="optim_nadam_+3A_weight_decay">weight_decay</code></td>
<td>
<p>Weight decay (L2 penalty) (default: 0).</p>
</td></tr>
<tr><td><code id="optim_nadam_+3A_momentum_decay">momentum_decay</code></td>
<td>
<p>Momentum_decay (default: 4e-3).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>


<h3>References</h3>

<p>Timothy Dozat,
&quot;Incorporating Nesterov Momentum into Adam&quot;,
International Conference on Learning Representations (ICLR) 2016.
https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_nadam
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
</code></pre>

<hr>
<h2 id='optim_qhadam'>QHAdam optimization algorithm</h2><span id='topic+optim_qhadam'></span>

<h3>Description</h3>

<p>R implementation of the QHAdam optimizer proposed
by Ma and Yarats(2019). We used the implementation available at
https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/qhadam.py.
Thanks to Nikolay Novik for providing the pytorch code.
</p>
<p>The original implementation has been developed by Facebook AI
and is licensed using the MIT license.
</p>
<p>From the the paper by Ma and Yarats(2019):
QHAdam is a QH augmented version of Adam, where we
replace both of Adam's moment estimators with quasi-hyperbolic terms.
QHAdam decouples the momentum term from the current gradient when
updating the weights, and decouples the mean squared gradients
term from the current squared gradient when updating the weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_qhadam(
  params,
  lr = 0.01,
  betas = c(0.9, 0.999),
  eps = 0.001,
  nus = c(1, 1),
  weight_decay = 0,
  decouple_weight_decay = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_qhadam_+3A_params">params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_qhadam_+3A_lr">lr</code></td>
<td>
<p>Learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_qhadam_+3A_betas">betas</code></td>
<td>
<p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999))</p>
</td></tr>
<tr><td><code id="optim_qhadam_+3A_eps">eps</code></td>
<td>
<p>Term added to the denominator to improve numerical
stability (default: 1e-8)</p>
</td></tr>
<tr><td><code id="optim_qhadam_+3A_nus">nus</code></td>
<td>
<p>Immediate discount factors used to
estimate the gradient and its square
(default: (1.0, 1.0))</p>
</td></tr>
<tr><td><code id="optim_qhadam_+3A_weight_decay">weight_decay</code></td>
<td>
<p>Weight decay (L2 penalty) (default: 0)</p>
</td></tr>
<tr><td><code id="optim_qhadam_+3A_decouple_weight_decay">decouple_weight_decay</code></td>
<td>
<p>Whether to decouple the weight
decay from the gradient-based optimization step.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Daniel Falbel, <a href="mailto:daniel.falble@gmail.com">daniel.falble@gmail.com</a>
</p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>


<h3>References</h3>

<p>Jerry Ma, Denis Yarats,
&quot;Quasi-hyperbolic momentum and Adam for deep learning&quot;.
https://arxiv.org/abs/1810.06801
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_qhadam
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}

</code></pre>

<hr>
<h2 id='optim_radam'>AdamW optimizer</h2><span id='topic+optim_radam'></span>

<h3>Description</h3>

<p>R implementation of the RAdam optimizer proposed
by Liu et al. (2019).
We used the implementation in PyTorch as a basis for our
implementation.
</p>
<p>From the abstract by the paper by Liu et al. (2019):
The learning rate warmup heuristic achieves remarkable success
in stabilizing training, accelerating convergence and improving
generalization for adaptive stochastic optimization algorithms
like RMSprop and Adam. Here, we study its mechanism in details.
Pursuing the theory behind warmup, we identify a problem of the
adaptive learning rate (i.e., it has problematically large variance
in the early stage), suggest warmup works as a variance reduction
technique, and provide both empirical and theoretical evidence to verify
our hypothesis. We further propose RAdam, a new variant of Adam,
by introducing a term to rectify the variance of the adaptive learning rate.
Extensive experimental results on image classification, language modeling,
and neural machine translation verify our intuition and demonstrate
the effectiveness and robustness of our proposed method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_radam(
  params,
  lr = 0.01,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_radam_+3A_params">params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_radam_+3A_lr">lr</code></td>
<td>
<p>Learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_radam_+3A_betas">betas</code></td>
<td>
<p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999))</p>
</td></tr>
<tr><td><code id="optim_radam_+3A_eps">eps</code></td>
<td>
<p>Term added to the denominator to improve numerical
stability (default: 1e-8)</p>
</td></tr>
<tr><td><code id="optim_radam_+3A_weight_decay">weight_decay</code></td>
<td>
<p>Weight decay (L2 penalty) (default: 0)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Daniel Falbel, <a href="mailto:daniel.falble@gmail.com">daniel.falble@gmail.com</a>
</p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>


<h3>References</h3>

<p>Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen,
Xiaodong Liu, Jianfeng Gao, Jiawei Han,
&quot;On the Variance of the Adaptive Learning Rate and Beyond&quot;,
International Conference on Learning Representations (ICLR) 2020.
https://arxiv.org/abs/1908.03265
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_radam
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
</code></pre>

<hr>
<h2 id='optim_swats'>SWATS optimizer</h2><span id='topic+optim_swats'></span>

<h3>Description</h3>

<p>R implementation of the SWATS optimizer proposed
by Shekar and Sochee (2018).
We used the implementation available at
https://github.com/jettify/pytorch-optimizer/
Thanks to Nikolay Novik for providing the pytorch code.
</p>
<p>From the abstract by the paper by Shekar and Sochee (2018):
Adaptive optimization methods such as Adam, Adagrad or RMSprop
have been found to generalize poorly compared to
Stochastic gradient descent (SGD). These methods tend to perform well i
in the initial portion of training but are outperformed by SGD at
later stages of training. We investigate a hybrid strategy that begins
training with an adaptive method and switches to SGD
when a triggering condition is satisfied.
The condition we propose relates to the projection of Adam
steps on the gradient subspace. By design, the monitoring process
for this condition adds very little overhead and does not increase
the number of hyperparameters in the optimizer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_swats(
  params,
  lr = 0.01,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 0,
  nesterov = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_swats_+3A_params">params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_swats_+3A_lr">lr</code></td>
<td>
<p>Learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_swats_+3A_betas">betas</code></td>
<td>
<p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999)).</p>
</td></tr>
<tr><td><code id="optim_swats_+3A_eps">eps</code></td>
<td>
<p>Term added to the denominator to improve numerical
stability (default: 1e-8).</p>
</td></tr>
<tr><td><code id="optim_swats_+3A_weight_decay">weight_decay</code></td>
<td>
<p>Weight decay (L2 penalty) (default: 0).</p>
</td></tr>
<tr><td><code id="optim_swats_+3A_nesterov">nesterov</code></td>
<td>
<p>Enables Nesterov momentum (default: False).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Daniel Falbel, <a href="mailto:daniel.falble@gmail.com">daniel.falble@gmail.com</a>
</p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>


<h3>References</h3>

<p>Nitish Shirish Keskar, Richard Socher
&quot;Improving Generalization Performance by Switching from Adam to SGD&quot;.
International Conference on Learning Representations (ICLR) 2018.
https://arxiv.org/abs/1712.07628
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_swats
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
</code></pre>

<hr>
<h2 id='optim_yogi'>Yogi optimizer</h2><span id='topic+optim_yogi'></span>

<h3>Description</h3>

<p>R implementation of the Yogi optimizer proposed
by Zaheer et al.(2019). We used the implementation available at
https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/yogi.py.
Thanks to Nikolay Novik for providing the pytorch code.
</p>
<p>The original implementation is licensed using the Apache-2.0 software license.
This implementation is also licensed using Apache-2.0 license.
</p>
<p>From the abstract by the paper by Zaheer et al.(2019):
Adaptive gradient methods that rely on scaling gradients
down by the square root of exponential moving averages
of past squared gradients, such RMSProp, Adam, Adadelta have
found wide application in optimizing the nonconvex problems
that arise in deep learning. However, it has been recently
demonstrated that such methods can fail to converge even
in simple convex optimization settings.
Yogi is a new adaptive optimization algorithm,
which controls the increase in effective learning rate,
leading to even better performance with similar theoretical
guarantees on convergence. Extensive experiments show that
Yogi with very little hyperparameter tuning outperforms
methods such as Adam in several challenging machine learning tasks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_yogi(
  params,
  lr = 0.01,
  betas = c(0.9, 0.999),
  eps = 0.001,
  initial_accumulator = 1e-06,
  weight_decay = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_yogi_+3A_params">params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td></tr>
<tr><td><code id="optim_yogi_+3A_lr">lr</code></td>
<td>
<p>Learning rate (default: 1e-3)</p>
</td></tr>
<tr><td><code id="optim_yogi_+3A_betas">betas</code></td>
<td>
<p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999))</p>
</td></tr>
<tr><td><code id="optim_yogi_+3A_eps">eps</code></td>
<td>
<p>Term added to the denominator to improve numerical
stability (default: 1e-8)</p>
</td></tr>
<tr><td><code id="optim_yogi_+3A_initial_accumulator">initial_accumulator</code></td>
<td>
<p>Initial values for first and
second moments.</p>
</td></tr>
<tr><td><code id="optim_yogi_+3A_weight_decay">weight_decay</code></td>
<td>
<p>Weight decay (L2 penalty) (default: 0)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>


<h3>References</h3>

<p>Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, Sanjiv Kumar,
&quot;Adaptive Methods for Nonconvex Optimization&quot;,
Advances in Neural Information Processing Systems 31 (NeurIPS 2018).
https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_yogi
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
</code></pre>

<hr>
<h2 id='state'>Imported function</h2><span id='topic+state'></span>

<h3>Description</h3>

<p>Code lifted from a internal function of madgrad package.
Get 'state' attribute of an object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>state(self)
</code></pre>


<h3>Author(s)</h3>

<p>Daniel Falbel, <a href="mailto:dfalbel@gmail.com">dfalbel@gmail.com</a>
</p>

<hr>
<h2 id='state+26lt+3B-'>Imported function</h2><span id='topic+state+3C-'></span>

<h3>Description</h3>

<p>Code lifted from a internal function of madgrad package.
Set 'state' attribute of an object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>state(self) &lt;- value
</code></pre>


<h3>Author(s)</h3>

<p>Daniel Falbel, <a href="mailto:dfalbel@gmail.com">dfalbel@gmail.com</a>
</p>

<hr>
<h2 id='test_optim'>Test optimization function</h2><span id='topic+test_optim'></span>

<h3>Description</h3>

<p><code>test_optim()</code> function is useful to visualize how optimizers solve the
minimization problem by showing the convergence path using a test function.
User can choose any test optimization
<a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization">functions</a>
provided by <code>torchopt</code>:
</p>
<p><code>"beale"</code>, <code>"booth"</code>, <code>"bukin_n6"</code>, <code>"easom"</code>, <code>"goldstein_price"</code>,
<code>"himmelblau"</code>, <code>"levi_n13"</code>, <code>"matyas"</code>, <code>"rastrigin"</code>,
<code>"rosenbrock"</code>, and <code>"sphere"</code>.
</p>
<p>Besides these functions, users can pass any function that receives two
numerical values and returns a scalar.
</p>
<p>Optimization functions are useful to evaluate characteristics of optimization
algorithms, such as convergence rate, precision, robustness, and performance.
These functions give an idea about the different situations that optimization
algorithms can face.
</p>
<p>Function <code>test_function()</code> plot the 2D-space of a test optimization function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_optim(
  optim,
  ...,
  opt_hparams = list(),
  test_fn = "beale",
  steps = 200,
  pt_start_color = "#5050FF7F",
  pt_end_color = "#FF5050FF",
  ln_color = "#FF0000FF",
  ln_weight = 2,
  bg_xy_breaks = 100,
  bg_z_breaks = 32,
  bg_palette = "viridis",
  ct_levels = 10,
  ct_labels = FALSE,
  ct_color = "#FFFFFF7F",
  plot_each_step = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_optim_+3A_optim">optim</code></td>
<td>
<p>Torch optimizer function.</p>
</td></tr>
<tr><td><code id="test_optim_+3A_...">...</code></td>
<td>
<p>Additional parameters (passed to <code>image</code> function).</p>
</td></tr>
<tr><td><code id="test_optim_+3A_opt_hparams">opt_hparams</code></td>
<td>
<p>A list with optimizer initialization parameters (default: <code>list()</code>).
If missing, for each optimizer its individual defaults will be used.</p>
</td></tr>
<tr><td><code id="test_optim_+3A_test_fn">test_fn</code></td>
<td>
<p>A test function (default <code>"beale"</code>). You can also pass
a list with 2 elements. The first should be a function that will be optimized
and the second is a function that returns a named vector with <code>x0</code>, <code>y0</code>
(the starting points) and <code>xmax</code>, <code>xmin</code>, <code>ymax</code> and <code>ymin</code> (the domain).
An example: <code>c(x0 = x0, y0 = y0, xmax = 5, xmin = -5, ymax = 5, ymin = -5)</code></p>
</td></tr>
<tr><td><code id="test_optim_+3A_steps">steps</code></td>
<td>
<p>Number of steps to run (default <code>200</code>).</p>
</td></tr>
<tr><td><code id="test_optim_+3A_pt_start_color">pt_start_color</code></td>
<td>
<p>Starting point color (default <code>"#5050FF7F"</code>)</p>
</td></tr>
<tr><td><code id="test_optim_+3A_pt_end_color">pt_end_color</code></td>
<td>
<p>Ending point color (default <code>"#FF5050FF"</code>)</p>
</td></tr>
<tr><td><code id="test_optim_+3A_ln_color">ln_color</code></td>
<td>
<p>Line path color (default <code>"#FF0000FF"</code>)</p>
</td></tr>
<tr><td><code id="test_optim_+3A_ln_weight">ln_weight</code></td>
<td>
<p>Line path weight (default <code>2</code>)</p>
</td></tr>
<tr><td><code id="test_optim_+3A_bg_xy_breaks">bg_xy_breaks</code></td>
<td>
<p>Background X and Y resolution (default <code>100</code>)</p>
</td></tr>
<tr><td><code id="test_optim_+3A_bg_z_breaks">bg_z_breaks</code></td>
<td>
<p>Background Z resolution (default <code>32</code>)</p>
</td></tr>
<tr><td><code id="test_optim_+3A_bg_palette">bg_palette</code></td>
<td>
<p>Background palette (default <code>"viridis"</code>)</p>
</td></tr>
<tr><td><code id="test_optim_+3A_ct_levels">ct_levels</code></td>
<td>
<p>Contour levels (default <code>10</code>)</p>
</td></tr>
<tr><td><code id="test_optim_+3A_ct_labels">ct_labels</code></td>
<td>
<p>Should show contour labels? (default <code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="test_optim_+3A_ct_color">ct_color</code></td>
<td>
<p>Contour color (default <code>"#FFFFFF7F"</code>)</p>
</td></tr>
<tr><td><code id="test_optim_+3A_plot_each_step">plot_each_step</code></td>
<td>
<p>Should output each step? (default <code>FALSE</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for producing animated gifs
</p>


<h3>Author(s)</h3>

<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
