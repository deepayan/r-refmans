<!DOCTYPE html><html lang="en"><head><title>Help for package hellmer</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {hellmer}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#hellmer-package'><p>hellmer: Batch Processing for Chat Models</p></a></li>
<li><a href='#batch'><p>Batch class for managing chat processing</p></a></li>
<li><a href='#capture'><p>Capture chat model response with proper handling</p></a></li>
<li><a href='#capture_with_retry'><p>Capture chat model response with proper handling and retries</p></a></li>
<li><a href='#chat_future'><p>Process a batch of prompts in parallel</p></a></li>
<li><a href='#chat_sequential'><p>Process a batch of prompts in sequence</p></a></li>
<li><a href='#chats'><p>Extract chat objects from a batch result</p></a></li>
<li><a href='#create_auth_error'><p>Create a standardized authentication error</p></a></li>
<li><a href='#create_results'><p>Create results object from batch</p></a></li>
<li><a href='#finish_successful_batch'><p>Finish successful batch processing</p></a></li>
<li><a href='#handle_batch_interrupt'><p>Handle batch interruption</p></a></li>
<li><a href='#is_auth_error'><p>Check if an error is an authentication error</p></a></li>
<li><a href='#process'><p>Process batch of prompts with progress tracking and retries</p></a></li>
<li><a href='#process_chunks'><p>Process chunks of prompts in parallel</p></a></li>
<li><a href='#process_future'><p>Process prompts in parallel chunks with error handling and state management</p></a></li>
<li><a href='#process_judgements'><p>Process structured data extraction with judgement</p></a></li>
<li><a href='#progress'><p>Get progress information from a batch result</p></a></li>
<li><a href='#texts'><p>Extract texts or structured data from a batch result</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Batch Processing for Chat Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Batch processing framework for 'ellmer' chat model interactions.
    Enables sequential and parallel processing of chat completions.
    Core capabilities include error handling with backoff, state persistence,
    progress tracking, and retry management.
    Parallel processing is implemented via the 'future' framework.
    Additional features include structured data extraction, tool integration,
    timeout handling, verbosity control, and sound notifications.
    Includes methods for returning chat texts, chat objects, progress status, 
    and structured data.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0), knitr, rmarkdown</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>beepr, cli, future, furrr, jsonlite, parallel, purrr, R.utils,
S7, utils</td>
</tr>
<tr>
<td>Depends:</td>
<td>ellmer</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://dylanpieper.github.io/hellmer/">https://dylanpieper.github.io/hellmer/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-14 16:17:31 UTC; dylanpieper</td>
</tr>
<tr>
<td>Author:</td>
<td>Dylan Pieper [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dylan Pieper &lt;dylanpieper@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-14 18:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='hellmer-package'>hellmer: Batch Processing for Chat Models</h2><span id='topic+hellmer'></span><span id='topic+hellmer-package'></span>

<h3>Description</h3>

<p>Batch processing framework for 'ellmer' chat model interactions. Enables sequential and parallel processing of chat completions. Core capabilities include error handling with backoff, state persistence, progress tracking, and retry management. Parallel processing is implemented via the 'future' framework. Additional features include structured data extraction, tool integration, timeout handling, verbosity control, and sound notifications. Includes methods for returning chat texts, chat objects, progress status, and structured data.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Dylan Pieper <a href="mailto:dylanpieper@gmail.com">dylanpieper@gmail.com</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://dylanpieper.github.io/hellmer/">https://dylanpieper.github.io/hellmer/</a>
</p>
</li></ul>


<hr>
<h2 id='batch'>Batch class for managing chat processing</h2><span id='topic+batch'></span>

<h3>Description</h3>

<p>Batch class for managing chat processing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>batch(
  prompts = list(),
  responses = list(),
  completed = integer(0),
  state_path = character(0),
  type_spec = NULL,
  judgements = integer(0),
  echo = character(0),
  input_type = character(0),
  max_retries = integer(0),
  initial_delay = integer(0),
  max_delay = integer(0),
  backoff_factor = integer(0),
  chunk_size = integer(0),
  workers = integer(0),
  plan = character(0),
  state = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="batch_+3A_prompts">prompts</code></td>
<td>
<p>List of prompts to process</p>
</td></tr>
<tr><td><code id="batch_+3A_responses">responses</code></td>
<td>
<p>List to store responses</p>
</td></tr>
<tr><td><code id="batch_+3A_completed">completed</code></td>
<td>
<p>Integer indicating number of completed prompts</p>
</td></tr>
<tr><td><code id="batch_+3A_state_path">state_path</code></td>
<td>
<p>Path to save state file</p>
</td></tr>
<tr><td><code id="batch_+3A_type_spec">type_spec</code></td>
<td>
<p>Type specification for structured data extraction</p>
</td></tr>
<tr><td><code id="batch_+3A_judgements">judgements</code></td>
<td>
<p>Number of judgements in a <code>batch_judge()</code> workflow (1 = initial extract + 1 judgement, 2 = initial extract + 2 judgements, etc.)</p>
</td></tr>
<tr><td><code id="batch_+3A_echo">echo</code></td>
<td>
<p>Level of output to display (&quot;none&quot;, &quot;text&quot;, &quot;all&quot;)</p>
</td></tr>
<tr><td><code id="batch_+3A_input_type">input_type</code></td>
<td>
<p>Type of input (&quot;vector&quot; or &quot;list&quot;)</p>
</td></tr>
<tr><td><code id="batch_+3A_max_retries">max_retries</code></td>
<td>
<p>Maximum number of retry attempts</p>
</td></tr>
<tr><td><code id="batch_+3A_initial_delay">initial_delay</code></td>
<td>
<p>Initial delay before first retry</p>
</td></tr>
<tr><td><code id="batch_+3A_max_delay">max_delay</code></td>
<td>
<p>Maximum delay between retries</p>
</td></tr>
<tr><td><code id="batch_+3A_backoff_factor">backoff_factor</code></td>
<td>
<p>Factor to multiply delay by after each retry</p>
</td></tr>
<tr><td><code id="batch_+3A_chunk_size">chunk_size</code></td>
<td>
<p>Size of chunks for parallel processing</p>
</td></tr>
<tr><td><code id="batch_+3A_workers">workers</code></td>
<td>
<p>Number of parallel workers</p>
</td></tr>
<tr><td><code id="batch_+3A_plan">plan</code></td>
<td>
<p>Parallel backend plan</p>
</td></tr>
<tr><td><code id="batch_+3A_state">state</code></td>
<td>
<p>Internal state tracking</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an S7 class object of class &quot;batch&quot; that represents a collection of prompts and their responses from chat models. The object contains all input parameters as properties and provides methods for:
</p>

<ul>
<li><p> Extracting text responses via <code>texts()</code> (includes structured data when a type specification is provided)
</p>
</li>
<li><p> Accessing full chat objects via <code>chats()</code>
</p>
</li>
<li><p> Tracking processing progress via <code>progress()</code>
</p>
</li></ul>

<p>The batch object manages prompt processing, tracks completion status, and handles retries for failed requests.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create a chat processor
chat &lt;- chat_sequential(chat_openai())

# Process a batch of prompts
batch &lt;- chat$batch(list(
  "What is R?",
  "Explain base R versus tidyverse",
  "Explain vectors, lists, and data frames"
))

# Check the progress if interrupted
batch$progress()

# Return the responses as a vector or list
batch$texts()

# Return the chat objects
batch$chats()

</code></pre>

<hr>
<h2 id='capture'>Capture chat model response with proper handling</h2><span id='topic+capture'></span>

<h3>Description</h3>

<p>Capture chat model response with proper handling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>capture(original_chat, prompt, type_spec = NULL, judgements = 0, echo = "text")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="capture_+3A_original_chat">original_chat</code></td>
<td>
<p>Original chat model object</p>
</td></tr>
<tr><td><code id="capture_+3A_prompt">prompt</code></td>
<td>
<p>Prompt text</p>
</td></tr>
<tr><td><code id="capture_+3A_type_spec">type_spec</code></td>
<td>
<p>Type specification for structured data</p>
</td></tr>
<tr><td><code id="capture_+3A_judgements">judgements</code></td>
<td>
<p>Number of judgements (1 = initial extract + 1 judgement, 2 = initial extract + 2 judgements, etc.)</p>
</td></tr>
<tr><td><code id="capture_+3A_echo">echo</code></td>
<td>
<p>Echo level (&quot;none&quot;, &quot;text&quot;, &quot;all&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing response information
</p>

<hr>
<h2 id='capture_with_retry'>Capture chat model response with proper handling and retries</h2><span id='topic+capture_with_retry'></span>

<h3>Description</h3>

<p>Capture chat model response with proper handling and retries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>capture_with_retry(
  original_chat,
  prompt,
  type_spec = NULL,
  echo = "text",
  judgements = 0,
  max_retries = 3L,
  initial_delay = 1,
  max_delay = 32,
  backoff_factor = 2,
  timeout = 60
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="capture_with_retry_+3A_original_chat">original_chat</code></td>
<td>
<p>Original chat model object</p>
</td></tr>
<tr><td><code id="capture_with_retry_+3A_prompt">prompt</code></td>
<td>
<p>Prompt text</p>
</td></tr>
<tr><td><code id="capture_with_retry_+3A_type_spec">type_spec</code></td>
<td>
<p>Type specification for structured data</p>
</td></tr>
<tr><td><code id="capture_with_retry_+3A_echo">echo</code></td>
<td>
<p>Echo level (&quot;none&quot;, &quot;text&quot;, &quot;all&quot;)</p>
</td></tr>
<tr><td><code id="capture_with_retry_+3A_judgements">judgements</code></td>
<td>
<p>Number of judgements (1 = initial extract + 1 judgement, 2 = initial extract + 2 judgements, etc.)</p>
</td></tr>
<tr><td><code id="capture_with_retry_+3A_max_retries">max_retries</code></td>
<td>
<p>Maximum number of retry attempts</p>
</td></tr>
<tr><td><code id="capture_with_retry_+3A_initial_delay">initial_delay</code></td>
<td>
<p>Initial delay in seconds before first retry</p>
</td></tr>
<tr><td><code id="capture_with_retry_+3A_max_delay">max_delay</code></td>
<td>
<p>Maximum delay in seconds between retries</p>
</td></tr>
<tr><td><code id="capture_with_retry_+3A_backoff_factor">backoff_factor</code></td>
<td>
<p>Factor to multiply delay by after each retry</p>
</td></tr>
<tr><td><code id="capture_with_retry_+3A_timeout">timeout</code></td>
<td>
<p>Timeout in seconds</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing response information
</p>

<hr>
<h2 id='chat_future'>Process a batch of prompts in parallel</h2><span id='topic+chat_future'></span>

<h3>Description</h3>

<p>Processes a batch of chat prompts using parallel workers.
Splits prompts into chunks for processing while maintaining state.
For sequential processing, use <code>chat_sequential()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_future(
  chat_model = NULL,
  workers = parallel::detectCores(),
  plan = "multisession",
  chunk_size = NULL,
  max_chunk_attempts = 3L,
  max_retries = 3L,
  initial_delay = 20,
  max_delay = 80,
  backoff_factor = 2,
  timeout = 60,
  beep = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_future_+3A_chat_model">chat_model</code></td>
<td>
<p>ellmer chat model function or object (e.g., <code>ellmer::chat_claude</code>)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_workers">workers</code></td>
<td>
<p>Number of parallel workers to use (default: number of CPU cores)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_plan">plan</code></td>
<td>
<p>Processing strategy to use: &quot;multisession&quot; for separate R sessions
or &quot;multicore&quot; for forked processes (default: &quot;multisession&quot;)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_chunk_size">chunk_size</code></td>
<td>
<p>Number of prompts to process in parallel at a time (default: number of prompts / 10)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_max_chunk_attempts">max_chunk_attempts</code></td>
<td>
<p>Maximum number of retry attempts for failed chunks (default: 3L)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_max_retries">max_retries</code></td>
<td>
<p>Maximum number of retry attempts per prompt (default: 3L)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_initial_delay">initial_delay</code></td>
<td>
<p>Initial delay in seconds before first retry (default: 20)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_max_delay">max_delay</code></td>
<td>
<p>Maximum delay in seconds between retries (default: 80)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_backoff_factor">backoff_factor</code></td>
<td>
<p>Factor to multiply delay by after each retry (default: 2)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_timeout">timeout</code></td>
<td>
<p>Maximum time in seconds to wait for each prompt response (default: 2)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_beep">beep</code></td>
<td>
<p>Logical to play a sound on batch completion, interruption, and error (default: TRUE)</p>
</td></tr>
<tr><td><code id="chat_future_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the underlying chat model (e.g., <code>system_prompt</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A batch object (S7 class) containing:
</p>

<ul>
<li><p> prompts: Original input prompts
</p>
</li>
<li><p> responses: Raw response data for completed prompts
</p>
</li>
<li><p> completed: Number of successfully processed prompts
</p>
</li>
<li><p> state_path: Path where batch state is saved
</p>
</li>
<li><p> type_spec: Type specification used for structured data
</p>
</li>
<li><p> texts: Function to extract text responses (includes structured data when a type specification is provided)
</p>
</li>
<li><p> chats: Function to extract chat objects
</p>
</li>
<li><p> progress: Function to get processing status
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Create a parallel chat processor
chat &lt;- chat_future(chat_openai, system_prompt = "Reply concisely, one sentence")

# Process a batch of prompts in parallel
batch &lt;- chat$batch(list(
  "What is R?",
  "Explain base R versus tidyverse",
  "Explain vectors, lists, and data frames"
))

# Check the progress if interrupted
batch$progress()

# Return the responses
batch$texts()

# Return the chat objects
batch$chats()

</code></pre>

<hr>
<h2 id='chat_sequential'>Process a batch of prompts in sequence</h2><span id='topic+chat_sequential'></span>

<h3>Description</h3>

<p>Processes a batch of chat prompts one at a time in sequential order.
Maintains state between runs and can resume interrupted processing.
For parallel processing, use <code>chat_future()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_sequential(
  chat_model = NULL,
  echo = "none",
  max_retries = 3L,
  initial_delay = 20,
  max_delay = 80,
  backoff_factor = 2,
  timeout = 60,
  beep = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_sequential_+3A_chat_model">chat_model</code></td>
<td>
<p>ellmer chat model function or object (e.g., <code>ellmer::chat_claude</code>)</p>
</td></tr>
<tr><td><code id="chat_sequential_+3A_echo">echo</code></td>
<td>
<p>Level of output to display: &quot;none&quot; for silent operation,
&quot;text&quot; for response text only, or &quot;all&quot; for full interaction (default: &quot;none&quot;)</p>
</td></tr>
<tr><td><code id="chat_sequential_+3A_max_retries">max_retries</code></td>
<td>
<p>Maximum number of retry attempts per prompt (default: 3L)</p>
</td></tr>
<tr><td><code id="chat_sequential_+3A_initial_delay">initial_delay</code></td>
<td>
<p>Initial delay in seconds before first retry (default: 20)</p>
</td></tr>
<tr><td><code id="chat_sequential_+3A_max_delay">max_delay</code></td>
<td>
<p>Maximum delay in seconds between retries (default: 80)</p>
</td></tr>
<tr><td><code id="chat_sequential_+3A_backoff_factor">backoff_factor</code></td>
<td>
<p>Factor to multiply delay by after each retry (default: 2)</p>
</td></tr>
<tr><td><code id="chat_sequential_+3A_timeout">timeout</code></td>
<td>
<p>Maximum time in seconds to wait for each prompt response (default: 60)</p>
</td></tr>
<tr><td><code id="chat_sequential_+3A_beep">beep</code></td>
<td>
<p>Logical to play a sound on batch completion, interruption, and error (default: TRUE)</p>
</td></tr>
<tr><td><code id="chat_sequential_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the underlying chat model (e.g., <code>system_prompt</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A batch object (S7 class) containing
</p>

<ul>
<li><p> prompts: Original input prompts
</p>
</li>
<li><p> responses: Raw response data for completed prompts
</p>
</li>
<li><p> completed: Number of successfully processed prompts
</p>
</li>
<li><p> state_path: Path where batch state is saved
</p>
</li>
<li><p> type_spec: Type specification used for structured data
</p>
</li>
<li><p> texts: Function to extract text responses (includes structured data when a type specification is provided)
</p>
</li>
<li><p> chats: Function to extract chat objects
</p>
</li>
<li><p> progress: Function to get processing status
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Create a sequential chat processor
chat &lt;- chat_sequential(chat_openai, system_prompt = "Reply concisely, one sentence")

# Process a batch of prompts in sequence
batch &lt;- chat$batch(list(
  "What is R?",
  "Explain base R versus tidyverse",
  "Explain vectors, lists, and data frames"
))

# Check the progress if interrupted
batch$progress()

# Return the responses
batch$texts()

# Return the chat objects
batch$chats()

</code></pre>

<hr>
<h2 id='chats'>Extract chat objects from a batch result</h2><span id='topic+chats'></span>

<h3>Description</h3>

<p>Extract chat objects from a batch result
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chats(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chats_+3A_x">x</code></td>
<td>
<p>A batch object</p>
</td></tr>
<tr><td><code id="chats_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of chat objects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create a chat processor
chat &lt;- chat_sequential(chat_openai())

# Process a batch of prompts
batch &lt;- chat$batch(list(
  "What is R?",
  "Explain base R versus tidyverse",
  "Explain vectors, lists, and data frames"
))

# Return the chat objects
batch$chats()

</code></pre>

<hr>
<h2 id='create_auth_error'>Create a standardized authentication error</h2><span id='topic+create_auth_error'></span>

<h3>Description</h3>

<p>Create a standardized authentication error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_auth_error(original_error)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_auth_error_+3A_original_error">original_error</code></td>
<td>
<p>Original error message or condition</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Structured error information
</p>

<hr>
<h2 id='create_results'>Create results object from batch</h2><span id='topic+create_results'></span>

<h3>Description</h3>

<p>Create results object from batch
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_results(result)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_results_+3A_result">result</code></td>
<td>
<p>Batch object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Results object with class &quot;batch&quot;
</p>

<hr>
<h2 id='finish_successful_batch'>Finish successful batch processing</h2><span id='topic+finish_successful_batch'></span>

<h3>Description</h3>

<p>Called after successful completion of batch processing to update progress
indicators and provide feedback
</p>


<h3>Usage</h3>

<pre><code class='language-R'>finish_successful_batch(pb, beep)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="finish_successful_batch_+3A_pb">pb</code></td>
<td>
<p>Progress bar object</p>
</td></tr>
<tr><td><code id="finish_successful_batch_+3A_beep">beep</code></td>
<td>
<p>Logical; whether to play success sound</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL (invisibly)
</p>

<hr>
<h2 id='handle_batch_interrupt'>Handle batch interruption</h2><span id='topic+handle_batch_interrupt'></span>

<h3>Description</h3>

<p>Handle batch interruption
</p>


<h3>Usage</h3>

<pre><code class='language-R'>handle_batch_interrupt(result, beep)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="handle_batch_interrupt_+3A_result">result</code></td>
<td>
<p>A batch object containing processing state</p>
</td></tr>
<tr><td><code id="handle_batch_interrupt_+3A_beep">beep</code></td>
<td>
<p>Logical indicating whether to play a sound</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL (called for side effects)
</p>

<hr>
<h2 id='is_auth_error'>Check if an error is an authentication error</h2><span id='topic+is_auth_error'></span>

<h3>Description</h3>

<p>Check if an error is an authentication error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_auth_error(error)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is_auth_error_+3A_error">error</code></td>
<td>
<p>Error message or condition</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if authentication error, FALSE otherwise
</p>

<hr>
<h2 id='process'>Process batch of prompts with progress tracking and retries</h2><span id='topic+process'></span>

<h3>Description</h3>

<p>Process batch of prompts with progress tracking and retries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process(
  chat_obj,
  prompts,
  type_spec = NULL,
  judgements = 0,
  state_path = tempfile("chat_", fileext = ".rds"),
  echo = "none",
  max_retries = 3L,
  initial_delay = 1,
  max_delay = 60,
  backoff_factor = 2,
  timeout = 60,
  beep = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="process_+3A_chat_obj">chat_obj</code></td>
<td>
<p>Chat model object</p>
</td></tr>
<tr><td><code id="process_+3A_prompts">prompts</code></td>
<td>
<p>List of prompts</p>
</td></tr>
<tr><td><code id="process_+3A_type_spec">type_spec</code></td>
<td>
<p>Type specification for structured data</p>
</td></tr>
<tr><td><code id="process_+3A_judgements">judgements</code></td>
<td>
<p>Number of judgements (1 = initial extract + 1 judgement, 2 = initial extract + 2 judgements, etc.)</p>
</td></tr>
<tr><td><code id="process_+3A_state_path">state_path</code></td>
<td>
<p>Path for saving state</p>
</td></tr>
<tr><td><code id="process_+3A_echo">echo</code></td>
<td>
<p>Echo level</p>
</td></tr>
<tr><td><code id="process_+3A_max_retries">max_retries</code></td>
<td>
<p>Maximum retry attempts</p>
</td></tr>
<tr><td><code id="process_+3A_initial_delay">initial_delay</code></td>
<td>
<p>Initial delay before retry</p>
</td></tr>
<tr><td><code id="process_+3A_max_delay">max_delay</code></td>
<td>
<p>Maximum delay between retries</p>
</td></tr>
<tr><td><code id="process_+3A_backoff_factor">backoff_factor</code></td>
<td>
<p>Factor to multiply delay</p>
</td></tr>
<tr><td><code id="process_+3A_timeout">timeout</code></td>
<td>
<p>Maximum time to wait</p>
</td></tr>
<tr><td><code id="process_+3A_beep">beep</code></td>
<td>
<p>Play sound on completion</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Batch results object
</p>

<hr>
<h2 id='process_chunks'>Process chunks of prompts in parallel</h2><span id='topic+process_chunks'></span>

<h3>Description</h3>

<p>Process chunks of prompts in parallel
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_chunks(
  chunks,
  result,
  chat_obj,
  type_spec,
  judgements,
  pb,
  state_path,
  echo,
  beep,
  timeout = 60,
  max_retries = 3L,
  initial_delay = 1,
  max_delay = 60,
  backoff_factor = 2
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="process_chunks_+3A_chunks">chunks</code></td>
<td>
<p>List of prompt chunks to process</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_result">result</code></td>
<td>
<p>A batch object to store results</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_chat_obj">chat_obj</code></td>
<td>
<p>Chat model object for making API calls</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_type_spec">type_spec</code></td>
<td>
<p>Type specification for structured data extraction</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_judgements">judgements</code></td>
<td>
<p>Number of judgements (1 = initial extract + 1 judgement, 2 = initial extract + 2 judgements, etc.)</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_pb">pb</code></td>
<td>
<p>Progress bar object</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_state_path">state_path</code></td>
<td>
<p>Path to save intermediate state</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_echo">echo</code></td>
<td>
<p>Level of output to display (&quot;none&quot;, &quot;text&quot;, &quot;all&quot;)</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_beep">beep</code></td>
<td>
<p>Logical indicating whether to play sounds</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_timeout">timeout</code></td>
<td>
<p>Maximum time in seconds to wait per prompt</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_max_retries">max_retries</code></td>
<td>
<p>Maximum number of retry attempts</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_initial_delay">initial_delay</code></td>
<td>
<p>Initial delay in seconds before first retry</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_max_delay">max_delay</code></td>
<td>
<p>Maximum delay in seconds between retries</p>
</td></tr>
<tr><td><code id="process_chunks_+3A_backoff_factor">backoff_factor</code></td>
<td>
<p>Factor to multiply delay by after each retry</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Updated batch object with processed results
</p>

<hr>
<h2 id='process_future'>Process prompts in parallel chunks with error handling and state management</h2><span id='topic+process_future'></span>

<h3>Description</h3>

<p>Process prompts in parallel chunks with error handling and state management
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_future(
  chat_obj,
  prompts,
  type_spec = NULL,
  judgements = 0,
  state_path = tempfile("chat_", fileext = ".rds"),
  workers = parallel::detectCores(),
  chunk_size = NULL,
  plan = "multisession",
  max_chunk_attempts = 3L,
  max_retries = 3L,
  initial_delay = 1,
  max_delay = 60,
  backoff_factor = 2,
  timeout = 60,
  beep = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="process_future_+3A_chat_obj">chat_obj</code></td>
<td>
<p>Chat model object for API calls</p>
</td></tr>
<tr><td><code id="process_future_+3A_prompts">prompts</code></td>
<td>
<p>Vector or list of prompts to process</p>
</td></tr>
<tr><td><code id="process_future_+3A_type_spec">type_spec</code></td>
<td>
<p>Optional type specification for structured data extraction</p>
</td></tr>
<tr><td><code id="process_future_+3A_judgements">judgements</code></td>
<td>
<p>Number of judgements (1 = initial extract + 1 judgement, 2 = initial extract + 2 judgements, etc.)</p>
</td></tr>
<tr><td><code id="process_future_+3A_state_path">state_path</code></td>
<td>
<p>Path to save intermediate state</p>
</td></tr>
<tr><td><code id="process_future_+3A_workers">workers</code></td>
<td>
<p>Number of parallel workers</p>
</td></tr>
<tr><td><code id="process_future_+3A_chunk_size">chunk_size</code></td>
<td>
<p>Number of prompts to process in parallel at a time</p>
</td></tr>
<tr><td><code id="process_future_+3A_plan">plan</code></td>
<td>
<p>Parallel backend</p>
</td></tr>
<tr><td><code id="process_future_+3A_max_chunk_attempts">max_chunk_attempts</code></td>
<td>
<p>Maximum retries per failed chunk</p>
</td></tr>
<tr><td><code id="process_future_+3A_max_retries">max_retries</code></td>
<td>
<p>Maximum retries per prompt</p>
</td></tr>
<tr><td><code id="process_future_+3A_initial_delay">initial_delay</code></td>
<td>
<p>Initial delay before first retry</p>
</td></tr>
<tr><td><code id="process_future_+3A_max_delay">max_delay</code></td>
<td>
<p>Maximum delay between retries</p>
</td></tr>
<tr><td><code id="process_future_+3A_backoff_factor">backoff_factor</code></td>
<td>
<p>Delay multiplier after each retry</p>
</td></tr>
<tr><td><code id="process_future_+3A_timeout">timeout</code></td>
<td>
<p>Maximum seconds per prompt</p>
</td></tr>
<tr><td><code id="process_future_+3A_beep">beep</code></td>
<td>
<p>Play sound on completion/error</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Batch results object
</p>

<hr>
<h2 id='process_judgements'>Process structured data extraction with judgement</h2><span id='topic+process_judgements'></span>

<h3>Description</h3>

<p>Process structured data extraction with judgement
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_judgements(chat_obj, prompt, type_spec, judgements = 0, echo = "none")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="process_judgements_+3A_chat_obj">chat_obj</code></td>
<td>
<p>Chat model object</p>
</td></tr>
<tr><td><code id="process_judgements_+3A_prompt">prompt</code></td>
<td>
<p>The prompt or text to analyze</p>
</td></tr>
<tr><td><code id="process_judgements_+3A_type_spec">type_spec</code></td>
<td>
<p>Type specification for structured data</p>
</td></tr>
<tr><td><code id="process_judgements_+3A_judgements">judgements</code></td>
<td>
<p>Number of judgements (1 = initial extract + 1 judgement, 2 = initial extract + 2 judgements, etc.)</p>
</td></tr>
<tr><td><code id="process_judgements_+3A_echo">echo</code></td>
<td>
<p>Level of output</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing extraction process
</p>

<hr>
<h2 id='progress'>Get progress information from a batch result</h2><span id='topic+progress'></span>

<h3>Description</h3>

<p>Get progress information from a batch result
</p>


<h3>Usage</h3>

<pre><code class='language-R'>progress(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="progress_+3A_x">x</code></td>
<td>
<p>A batch object</p>
</td></tr>
<tr><td><code id="progress_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing progress details
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create a chat processor
chat &lt;- chat_sequential(chat_openai())

# Process a batch of prompts
batch &lt;- chat$batch(list(
  "What is R?",
  "Explain base R versus tidyverse",
  "Explain vectors, lists, and data frames"
))

# Check the progress
batch$progress()

</code></pre>

<hr>
<h2 id='texts'>Extract texts or structured data from a batch result</h2><span id='topic+texts'></span>

<h3>Description</h3>

<p>Extract texts or structured data from a batch result
</p>


<h3>Usage</h3>

<pre><code class='language-R'>texts(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="texts_+3A_x">x</code></td>
<td>
<p>A batch object</p>
</td></tr>
<tr><td><code id="texts_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector or list of text responses. If a type specification was provided to the batch, structured data objects will be returned instead.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create a chat processor
chat &lt;- chat_sequential(chat_openai())

# Process a batch of prompts
batch &lt;- chat$batch(list(
  "What is R?",
  "Explain base R versus tidyverse",
  "Explain vectors, lists, and data frames"
))

# Extract text responses
batch$texts()

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
