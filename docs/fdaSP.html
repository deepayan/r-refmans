<!DOCTYPE html><html><head><title>Help for package fdaSP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fdaSP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#confband'><p>Function to plot the confidence bands</p></a></li>
<li><a href='#f2fSP'><p>Overlap Group Least Absolute Shrinkage and Selection Operator for function-on-function regression model</p></a></li>
<li><a href='#f2fSP_cv'><p>Cross-validation for Overlap Group Least Absolute Shrinkage and Selection Operator for function-on-function regression model</p></a></li>
<li><a href='#f2sSP'><p>Overlap Group Least Absolute Shrinkage and Selection Operator for scalar-on-function regression model</p></a></li>
<li><a href='#f2sSP_cv'><p>Cross-validation for Overlap Group Least Absolute Shrinkage and Selection Operator on scalar-on-function regression model</p></a></li>
<li><a href='#fdaSP-package'><p>Sparse Functional Data Analysis Methods</p></a></li>
<li><a href='#lmSP'><p>Sparse Adaptive Overlap Group Least Absolute Shrinkage and Selection Operator</p></a></li>
<li><a href='#lmSP_cv'><p>Cross-validation for Sparse Adaptive Overlap Group Least Absolute Shrinkage and Selection Operator</p></a></li>
<li><a href='#softhresh'><p>Function to solve the soft thresholding problem</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Sparse Functional Data Analysis Methods</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-09-12</td>
</tr>
<tr>
<td>Author:</td>
<td>Mauro Bernardi [aut, cre],
  Marco Stefanucci [aut],
  Antonio Canale [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mauro Bernardi &lt;mauro.bernardi@unipd.it&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides algorithms to fit linear regression models under several popular penalization techniques and functional linear regression models based on Majorizing-Minimizing (MM) and Alternating Direction Method of Multipliers (ADMM) techniques.
    See Boyd et al (2010) &lt;<a href="https://doi.org/10.1561%2F2200000016">doi:10.1561/2200000016</a>&gt; for complete introduction to the method.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>RColorBrewer, gglasso, glmnet, latex2exp, utils</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, Rdpack, grDevices, graphics, stats, parallel,
doParallel, foreach, splines, ks, pracma</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3.9000</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-05 13:43:15 UTC; maurobernardi</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-05 18:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='confband'>Function to plot the confidence bands</h2><span id='topic+confband'></span>

<h3>Description</h3>

<p>Function to plot the confidence bands
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confband(xV, yVmin, yVmax)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confband_+3A_xv">xV</code></td>
<td>
<p>the values for the x-axis.</p>
</td></tr>
<tr><td><code id="confband_+3A_yvmin">yVmin</code></td>
<td>
<p>the minimum values for the y-axis.</p>
</td></tr>
<tr><td><code id="confband_+3A_yvmax">yVmax</code></td>
<td>
<p>the maximum values for the y-axis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a polygon.
</p>

<hr>
<h2 id='f2fSP'>Overlap Group Least Absolute Shrinkage and Selection Operator for function-on-function regression model</h2><span id='topic+f2fSP'></span>

<h3>Description</h3>

<p>Overlap Group-LASSO for function-on-function regression model solves the following optimization problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_{\psi} ~ \frac{1}{2} \sum_{i=1}^n \int \left( y_i(s) - \int x_i(t) \psi(t,s) dt \right)^2 ds + \lambda \sum_{g=1}^{G} \Vert S_{g}T\psi \Vert_2</code>
</p>

<p>to obtain a sparse coefficient vector <code class="reqn">\psi=\mathsf{vec}(\Psi)\in\mathbb{R}^{ML}</code> for the functional penalized predictor <code class="reqn">x(t)</code>, where the coefficient matrix <code class="reqn">\Psi\in\mathbb{R}^{M\times L}</code>, 
the regression function <code class="reqn">\psi(t,s)=\varphi(t)^\intercal\Psi\theta(s)</code>, 
<code class="reqn">\varphi(t)</code> and <code class="reqn">\theta(s)</code> are two B-splines bases of order <code class="reqn">d</code> and dimension <code class="reqn">M</code> and <code class="reqn">L</code>, respectively. For each group <code class="reqn">g</code>, each row of 
the matrix <code class="reqn">S_g\in\mathbb{R}^{d\times ML}</code> has non-zero entries only for those bases belonging 
to that group. These values are provided by the arguments <code>groups</code> and <code>group_weights</code> (see below). 
Each basis function belongs to more than one group. The diagonal matrix <code class="reqn">T\in\mathbb{R}^{ML\times ML}</code> contains 
the basis-specific weights. These values are provided by the argument <code>var_weights</code> (see below).
The regularization path is computed for the overlap group-LASSO penalty at a grid of values for the regularization 
parameter <code class="reqn">\lambda</code> using the alternating direction method of multipliers (ADMM). See Boyd et al. (2011) and Lin et al. (2022) 
for details on the ADMM method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f2fSP(
  mY,
  mX,
  L,
  M,
  group_weights = NULL,
  var_weights = NULL,
  standardize.data = TRUE,
  splOrd = 4,
  lambda = NULL,
  lambda.min.ratio = NULL,
  nlambda = 30,
  overall.group = FALSE,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f2fSP_+3A_my">mY</code></td>
<td>
<p>an <code class="reqn">(n\times r_y)</code> matrix of observations of the functional response variable.</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_mx">mX</code></td>
<td>
<p>an <code class="reqn">(n\times r_x)</code> matrix of observations of the functional covariate.</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_l">L</code></td>
<td>
<p>number of elements of the B-spline basis vector <code class="reqn">\theta(s)</code>.</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_m">M</code></td>
<td>
<p>number of elements of the B-spline basis vector <code class="reqn">\varphi(t)</code>.</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_group_weights">group_weights</code></td>
<td>
<p>a vector of length <code class="reqn">G</code> containing group-specific weights. The default is square root of the group cardinality, see Bernardi et al. (2022).</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_var_weights">var_weights</code></td>
<td>
<p>a vector of length <code class="reqn">ML</code> containing basis-specific weights. The default is a vector where 
each entry is the reciprocal of the number of groups including that basis. See Bernardi et al. (2022) for details.</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_standardize.data">standardize.data</code></td>
<td>
<p>logical. Should data be standardized?</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_splord">splOrd</code></td>
<td>
<p>the order <code class="reqn">d</code> of the spline basis.</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_lambda">lambda</code></td>
<td>
<p>either a regularization parameter or a vector of regularization parameters. 
In this latter case the routine computes the whole path. If it is NULL values for lambda are provided by the routine.</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_lambda.min.ratio">lambda.min.ratio</code></td>
<td>
<p>smallest value for lambda, as a fraction of the maximum lambda value. If <code class="reqn">nr_y&gt;LM</code>, the default is 0.0001, and if <code class="reqn">nr_y&lt;LM</code>, the default is 0.01.</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_nlambda">nlambda</code></td>
<td>
<p>the number of lambda values - default is 30.</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_overall.group">overall.group</code></td>
<td>
<p>logical. If it is TRUE, an overall group including all penalized covariates is added.</p>
</td></tr>
<tr><td><code id="f2fSP_+3A_control">control</code></td>
<td>
<p>a list of control parameters for the ADMM algorithm. See ‘Details’.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing </p>

<dl>
<dt>sp.coefficients</dt><dd><p>an <code class="reqn">(M\times L)</code> solution matrix for the parameters <code class="reqn">\Psi</code>, which corresponds to the minimum in-sample MSE.</p>
</dd>
<dt>sp.coef.path</dt><dd><p>an <code class="reqn">(n_\lambda\times M \times L)</code> array of estimated <code class="reqn">\Psi</code> coefficients for each lambda.</p>
</dd>
<dt>sp.fun</dt><dd><p>an <code class="reqn">(r_x\times r_y)</code> matrix providing the estimated functional coefficient for <code class="reqn">\psi(t,s)</code>.</p>
</dd>
<dt>sp.fun.path</dt><dd><p>an <code class="reqn">(n_\lambda\times r_x\times r_y)</code> array providing the estimated functional coefficients for <code class="reqn">\psi(t,s)</code> for each lambda.</p>
</dd>
<dt>lambda</dt><dd><p>sequence of lambda.</p>
</dd>
<dt>lambda.min</dt><dd><p>value of lambda that attains the minimum in-sample MSE.</p>
</dd>
<dt>mse</dt><dd><p>in-sample mean squared error.</p>
</dd>
<dt>min.mse</dt><dd><p>minimum value of the in-sample MSE for the sequence of lambda.</p>
</dd>
<dt>convergence</dt><dd><p>logical. 1 denotes achieved convergence.</p>
</dd>
<dt>elapsedTime</dt><dd><p>elapsed time in seconds.</p>
</dd>
<dt>iternum</dt><dd><p>number of iterations.</p>
</dd>
</dl>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates,
</p>

<dl>
<dt>objval</dt><dd><p>objective function value.</p>
</dd>
<dt>r_norm</dt><dd><p>norm of primal residual.</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual.</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition.</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition.</p>
</dd>
</dl>

<p>Iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Details</h3>

<p>The control argument is a list that can supply any of the following components:</p>

<dl>
<dt>adaptation</dt><dd><p>logical. If it is TRUE, ADMM with adaptation is performed. The default value is TRUE. See Boyd et al. (2011) for details.</p>
</dd>
<dt>rho</dt><dd><p>an augmented Lagrangian parameter. The default value is 1.</p>
</dd>
<dt>tau.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 2. See Boyd et al. (2011) and Lin et al. (2022) for details.</p>
</dd>
<dt>mu.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 10. See Boyd et al. (2011) and Lin et al. (2022) for details.</p>
</dd>
<dt>abstol</dt><dd><p>absolute tolerance stopping criterion. The default value is sqrt(sqrt(.Machine$double.eps)).</p>
</dd>
<dt>reltol</dt><dd><p>relative tolerance stopping criterion. The default value is sqrt(.Machine$double.eps).</p>
</dd>
<dt>maxit</dt><dd><p>maximum number of iterations. The default value is 100.</p>
</dd>
<dt>print.out</dt><dd><p>logical. If it is TRUE, a message about the procedure is printed. The default value is TRUE.</p>
</dd>
</dl>



<h3>References</h3>

<p>Bernardi M, Canale A, Stefanucci M (2022).
&ldquo;Locally Sparse Function-on-Function Regression.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>0</b>(0), 1-15.
<a href="https://doi.org/10.1080/10618600.2022.2130926">doi:10.1080/10618600.2022.2130926</a>, https://doi.org/10.1080/10618600.2022.2130926.
</p>
<p>Boyd S, Parikh N, Chu E, Peleato B, Eckstein J (2011).
&ldquo;Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.&rdquo;
<em>Foundations and Trends® in Machine Learning</em>, <b>3</b>(1), 1-122.
ISSN 1935-8237, <a href="https://doi.org/10.1561/2200000016">doi:10.1561/2200000016</a>, <a href="http://dx.doi.org/10.1561/2200000016">http://dx.doi.org/10.1561/2200000016</a>.
</p>
<p>Jenatton R, Audibert J, Bach F (2011).
&ldquo;Structured variable selection with sparsity-inducing norms.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>12</b>, 2777&ndash;2824.
ISSN 1532-4435.
</p>
<p>Lin Z, Li H, Fang C (2022).
<em>Alternating direction method of multipliers for machine learning</em>.
Springer, Singapore.
ISBN 978-981-16-9839-2; 978-981-16-9840-8, <a href="https://doi.org/10.1007/978-981-16-9840-8">doi:10.1007/978-981-16-9840-8</a>, With forewords by Zongben Xu and Zhi-Quan Luo.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate sample data
set.seed(4321)
s  &lt;- seq(0, 1, length.out = 100)
t  &lt;- seq(0, 1, length.out = 100)
p1 &lt;- 5
p2 &lt;- 6
r  &lt;- 10
n  &lt;- 50

beta_basis1 &lt;- splines::bs(s, df = p1, intercept = TRUE)    # first basis for beta
beta_basis2 &lt;- splines::bs(s, df = p2, intercept = TRUE)    # second basis for beta

data_basis &lt;- splines::bs(s, df = r, intercept = TRUE)    # basis for X

x_0   &lt;- apply(matrix(rnorm(p1 * p2, sd = 1), p1, p2), 1, 
               fdaSP::softhresh, 1.5)  # regression coefficients 
x_fun &lt;- beta_basis2 %*% x_0 %*%  t(beta_basis1)  

fun_data &lt;- matrix(rnorm(n*r), n, r) %*% t(data_basis)
b        &lt;- fun_data %*% x_fun + rnorm(n * 100, sd = sd(fun_data %*% x_fun )/3)

## set the hyper-parameters
maxit          &lt;- 1000
rho_adaptation &lt;- FALSE
rho            &lt;- 1
reltol         &lt;- 1e-5
abstol         &lt;- 1e-5

## fit functional regression model
mod &lt;- f2fSP(mY = b, mX = fun_data, L = p1, M = p2,
             group_weights = NULL, var_weights = NULL, standardize.data = FALSE, splOrd = 4,
             lambda = NULL, nlambda = 30, lambda.min.ratio = NULL, 
             control = list("abstol" = abstol, 
                            "reltol" = reltol, 
                            "maxit" = maxit, 
                            "adaptation" = rho_adaptation, 
                            rho = rho, 
             "print.out" = FALSE))
 
mycol &lt;- function (n) {
palette &lt;- colorRampPalette(RColorBrewer::brewer.pal(11, "Spectral"))
palette(n)
}
cols &lt;- mycol(1000)

oldpar &lt;- par(mfrow = c(1, 2))
image(x_0, col = cols)
image(mod$sp.coefficients, col = cols)
par(oldpar)

oldpar &lt;- par(mfrow = c(1, 2))
image(x_fun, col = cols)
contour(x_fun, add = TRUE)
image(beta_basis2 %*% mod$sp.coefficients %*% t(beta_basis1), col = cols)
contour(beta_basis2 %*% mod$sp.coefficients %*% t(beta_basis1), add = TRUE)
par(oldpar)

</code></pre>

<hr>
<h2 id='f2fSP_cv'>Cross-validation for Overlap Group Least Absolute Shrinkage and Selection Operator for function-on-function regression model</h2><span id='topic+f2fSP_cv'></span>

<h3>Description</h3>

<p>Overlap Group-LASSO for function-on-function regression model solves the following optimization problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_{\psi} ~ \frac{1}{2} \sum_{i=1}^n \int \left( y_i(s) - \int x_i(t) \psi(t,s) dt \right)^2 ds + \lambda \sum_{g=1}^{G} \Vert S_{g}T\psi \Vert_2</code>
</p>

<p>to obtain a sparse coefficient vector <code class="reqn">\psi=\mathsf{vec}(\Psi)\in\mathbb{R}^{ML}</code> for the functional penalized predictor <code class="reqn">x(t)</code>, where the coefficient matrix <code class="reqn">\Psi\in\mathbb{R}^{M\times L}</code>, 
the regression function <code class="reqn">\psi(t,s)=\varphi(t)^\intercal\Psi\theta(s)</code>, 
<code class="reqn">\varphi(t)</code> and <code class="reqn">\theta(s)</code> are two B-splines bases of order <code class="reqn">d</code> and dimension <code class="reqn">M</code> and <code class="reqn">L</code>, respectively. For each group <code class="reqn">g</code>, each row of 
the matrix <code class="reqn">S_g\in\mathbb{R}^{d\times ML}</code> has non-zero entries only for those bases belonging 
to that group. These values are provided by the arguments <code>groups</code> and <code>group_weights</code> (see below). 
Each basis function belongs to more than one group. The diagonal matrix <code class="reqn">T\in\mathbb{R}^{ML\times ML}</code> contains 
the basis-specific weights. These values are provided by the argument <code>var_weights</code> (see below).
The regularization path is computed for the overlap group-LASSO penalty at a grid of values for the regularization 
parameter <code class="reqn">\lambda</code> using the alternating direction method of multipliers (ADMM). See Boyd et al. (2011) and Lin et al. (2022) 
for details on the ADMM method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f2fSP_cv(
  mY,
  mX,
  L,
  M,
  group_weights = NULL,
  var_weights = NULL,
  standardize.data = FALSE,
  splOrd = 4,
  lambda = NULL,
  lambda.min.ratio = NULL,
  nlambda = NULL,
  cv.fold = 5,
  overall.group = FALSE,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f2fSP_cv_+3A_my">mY</code></td>
<td>
<p>an <code class="reqn">(n\times r_y)</code> matrix of observations of the functional response variable.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_mx">mX</code></td>
<td>
<p>an <code class="reqn">(n\times r_x)</code> matrix of observations of the functional covariate.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_l">L</code></td>
<td>
<p>number of elements of the B-spline basis vector <code class="reqn">\theta(s)</code>.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_m">M</code></td>
<td>
<p>number of elements of the B-spline basis vector <code class="reqn">\varphi(t)</code>.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_group_weights">group_weights</code></td>
<td>
<p>a vector of length <code class="reqn">G</code> containing group-specific weights. The default is square root of the group cardinality, see Bernardi et al. (2022).</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_var_weights">var_weights</code></td>
<td>
<p>a vector of length <code class="reqn">ML</code> containing basis-specific weights. The default is a vector where 
each entry is the reciprocal of the number of groups including that basis. See Bernardi et al. (2022) for details.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_standardize.data">standardize.data</code></td>
<td>
<p>logical. Should data be standardized?</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_splord">splOrd</code></td>
<td>
<p>the order <code class="reqn">d</code> of the spline basis.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_lambda">lambda</code></td>
<td>
<p>either a regularization parameter or a vector of regularization parameters. 
In this latter case the routine computes the whole path. If it is NULL values for lambda are provided by the routine.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_lambda.min.ratio">lambda.min.ratio</code></td>
<td>
<p>smallest value for lambda, as a fraction of the maximum lambda value. If <code class="reqn">nr_y&gt;LM</code>, the default is 0.0001, and if <code class="reqn">nr_y&lt;LM</code>, the default is 0.01.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_nlambda">nlambda</code></td>
<td>
<p>the number of lambda values - default is 30.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_cv.fold">cv.fold</code></td>
<td>
<p>the number of folds - default is 5.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_overall.group">overall.group</code></td>
<td>
<p>logical. If it is TRUE, an overall group including all penalized covariates is added.</p>
</td></tr>
<tr><td><code id="f2fSP_cv_+3A_control">control</code></td>
<td>
<p>a list of control parameters for the ADMM algorithm. See ‘Details’.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing </p>

<dl>
<dt>sp.coefficients</dt><dd><p>an <code class="reqn">(M\times L)</code> solution matrix for the parameters <code class="reqn">\Psi</code>, which corresponds to the minimum cross-validated MSE.</p>
</dd>
<dt>sp.fun</dt><dd><p>an <code class="reqn">(r_x\times r_y)</code> matrix providing the estimated functional coefficient for <code class="reqn">\psi(t,s)</code> corresponding to the minimum cross-validated MSE.</p>
</dd> 
<dt>lambda</dt><dd><p>sequence of lambda.</p>
</dd>
<dt>lambda.min</dt><dd><p>value of lambda that attains the cross-validated minimum mean squared error.</p>
</dd>
<dt>indi.min.mse</dt><dd><p>index of the lambda sequence corresponding to lambda.min.</p>
</dd>
<dt>mse</dt><dd><p>cross-validated mean squared error.</p>
</dd>
<dt>min.mse</dt><dd><p>minimum value of the cross-validated MSE for the sequence of lambda.</p>
</dd>
<dt>mse.sd</dt><dd><p>standard deviation of the cross-validated mean squared error.</p>
</dd>
<dt>convergence</dt><dd><p>logical. 1 denotes achieved convergence.</p>
</dd>
<dt>elapsedTime</dt><dd><p>elapsed time in seconds.</p>
</dd>
<dt>iternum</dt><dd><p>number of iterations.</p>
</dd>
</dl>

<p>Iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Details</h3>

<p>The control argument is a list that can supply any of the following components:</p>

<dl>
<dt>adaptation</dt><dd><p>logical. If it is TRUE, ADMM with adaptation is performed. The default value is TRUE. See Boyd et al. (2011) for details.</p>
</dd>
<dt>rho</dt><dd><p>an augmented Lagrangian parameter. The default value is 1.</p>
</dd>
<dt>tau.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 2. See Boyd et al. (2011) and Lin et al. (2022) for details.</p>
</dd>
<dt>mu.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 10. See Boyd et al. (2011) and Lin et al. (2022) for details.</p>
</dd>
<dt>abstol</dt><dd><p>absolute tolerance stopping criterion. The default value is sqrt(sqrt(.Machine$double.eps)).</p>
</dd>
<dt>reltol</dt><dd><p>relative tolerance stopping criterion. The default value is sqrt(.Machine$double.eps).</p>
</dd>
<dt>maxit</dt><dd><p>maximum number of iterations. The default value is 100.</p>
</dd>
<dt>print.out</dt><dd><p>logical. If it is TRUE, a message about the procedure is printed. The default value is TRUE.</p>
</dd>
</dl>



<h3>References</h3>

<p>Bernardi M, Canale A, Stefanucci M (2022).
&ldquo;Locally Sparse Function-on-Function Regression.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>0</b>(0), 1-15.
<a href="https://doi.org/10.1080/10618600.2022.2130926">doi:10.1080/10618600.2022.2130926</a>, https://doi.org/10.1080/10618600.2022.2130926.
</p>
<p>Boyd S, Parikh N, Chu E, Peleato B, Eckstein J (2011).
&ldquo;Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.&rdquo;
<em>Foundations and Trends® in Machine Learning</em>, <b>3</b>(1), 1-122.
ISSN 1935-8237, <a href="https://doi.org/10.1561/2200000016">doi:10.1561/2200000016</a>, <a href="http://dx.doi.org/10.1561/2200000016">http://dx.doi.org/10.1561/2200000016</a>.
</p>
<p>Jenatton R, Audibert J, Bach F (2011).
&ldquo;Structured variable selection with sparsity-inducing norms.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>12</b>, 2777&ndash;2824.
ISSN 1532-4435.
</p>
<p>Lin Z, Li H, Fang C (2022).
<em>Alternating direction method of multipliers for machine learning</em>.
Springer, Singapore.
ISBN 978-981-16-9839-2; 978-981-16-9840-8, <a href="https://doi.org/10.1007/978-981-16-9840-8">doi:10.1007/978-981-16-9840-8</a>, With forewords by Zongben Xu and Zhi-Quan Luo.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate sample data
set.seed(4321)
s  &lt;- seq(0, 1, length.out = 100)
t  &lt;- seq(0, 1, length.out = 100)
p1 &lt;- 5
p2 &lt;- 6
r  &lt;- 10
n  &lt;- 50

beta_basis1 &lt;- splines::bs(s, df = p1, intercept = TRUE)    # first basis for beta
beta_basis2 &lt;- splines::bs(s, df = p2, intercept = TRUE)    # second basis for beta

data_basis &lt;- splines::bs(s, df = r, intercept = TRUE)    # basis for X

x_0   &lt;- apply(matrix(rnorm(p1 * p2, sd = 1), p1, p2), 1, 
               fdaSP::softhresh, 1.5)  # regression coefficients 
x_fun &lt;- beta_basis2 %*% x_0 %*%  t(beta_basis1)  

fun_data &lt;- matrix(rnorm(n*r), n, r) %*% t(data_basis)
b        &lt;- fun_data %*% x_fun + rnorm(n * 100, sd = sd(fun_data %*% x_fun )/3)

## set the hyper-parameters
maxit          &lt;- 1000
rho_adaptation &lt;- FALSE
rho            &lt;- 0.01
reltol         &lt;- 1e-5
abstol         &lt;- 1e-5

## fit functional regression model
mod_cv &lt;- f2fSP_cv(mY = b, mX = fun_data, L = p1, M = p2,
                   group_weights = NULL, var_weights = NULL, 
                   standardize.data = FALSE, splOrd = 4,
                   lambda = NULL, nlambda = 30, cv.fold = 5, 
                   lambda.min.ratio = NULL,
                   control = list("abstol" = abstol, 
                                  "reltol" = reltol, 
                                  "maxit" = maxit, 
                                  "adaptation" = rho_adaptation, 
                                  "rho" = rho, 
                                  "print.out" = FALSE))

### graphical presentation
plot(log(mod_cv$lambda), mod_cv$mse, type = "l", col = "blue", lwd = 2, bty = "n", 
     xlab = latex2exp::TeX("$\\log(\\lambda)$"), ylab = "Prediction Error", 
     ylim = range(mod_cv$mse - mod_cv$mse.sd, mod_cv$mse + mod_cv$mse.sd),
     main = "Cross-validated Prediction Error")
fdaSP::confband(xV = log(mod_cv$lambda), yVmin = mod_cv$mse - mod_cv$mse.sd, 
                yVmax = mod_cv$mse + mod_cv$mse.sd)       
abline(v = log(mod_cv$lambda[which(mod_cv$lambda == mod_cv$lambda.min)]), col = "red", lwd = 1.0)

### comparison with oracle error
mod &lt;- f2fSP(mY = b, mX = fun_data, L = p1, M = p2,
             group_weights = NULL, var_weights = NULL, 
             standardize.data = FALSE, splOrd = 4,
             lambda = NULL, nlambda = 30, lambda.min.ratio = NULL, 
             control = list("abstol" = abstol, 
                            "reltol" = reltol, 
                            "maxit" = maxit,
                            "adaptation" = rho_adaptation, 
                            "rho" = rho, 
                            "print.out" = FALSE))
err_mod &lt;- apply(mod$sp.coef.path, 1, function(x) sum((x - x_0)^2))
plot(log(mod$lambda), err_mod, type = "l", col = "blue", lwd = 2, 
     xlab = latex2exp::TeX("$\\log(\\lambda)$"), 
     ylab = "Estimation Error", main = "True Estimation Error", bty = "n")
abline(v = log(mod$lambda[which(err_mod == min(err_mod))]), col = "red", lwd = 1.0)
abline(v = log(mod_cv$lambda[which(mod_cv$lambda == mod_cv$lambda.min)]), 
       col = "red", lwd = 1.0, lty = 2)

</code></pre>

<hr>
<h2 id='f2sSP'>Overlap Group Least Absolute Shrinkage and Selection Operator for scalar-on-function regression model</h2><span id='topic+f2sSP'></span>

<h3>Description</h3>

<p>Overlap Group-LASSO for scalar-on-function regression model solves the following optimization problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_{\psi,\gamma} ~ \frac{1}{2} \sum_{i=1}^n \left( y_i - \int x_i(t) \psi(t) dt-z_i^\intercal\gamma \right)^2 + \lambda \sum_{g=1}^{G} \Vert S_{g}T\psi \Vert_2</code>
</p>

<p>to obtain a sparse coefficient vector <code class="reqn">\psi\in\mathbb{R}^{M}</code> for the functional penalized predictor <code class="reqn">x(t)</code> and a coefficient vector <code class="reqn">\gamma\in\mathbb{R}^q</code> for the unpenalized scalar predictors <code class="reqn">z_1,\dots,z_q</code>. The regression function is <code class="reqn">\psi(t)=\varphi(t)^\intercal\psi</code>
where <code class="reqn">\varphi(t)</code> is a B-spline basis of order <code class="reqn">d</code> and dimension <code class="reqn">M</code>. For each group <code class="reqn">g</code>, each row of 
the matrix <code class="reqn">S_g\in\mathbb{R}^{d\times M}</code> has non-zero entries only for those bases belonging 
to that group. These values are provided by the arguments <code>groups</code> and <code>group_weights</code> (see below). 
Each basis function belongs to more than one group. The diagonal matrix <code class="reqn">T\in\mathbb{R}^{M\times M}</code> contains 
the basis-specific weights. These values are provided by the argument <code>var_weights</code> (see below).
The regularization path is computed for the overlap group-LASSO penalty at a grid of values for the regularization 
parameter <code class="reqn">\lambda</code> using the alternating direction method of multipliers (ADMM). See Boyd et al. (2011) and Lin et al. (2022) 
for details on the ADMM method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f2sSP(
  vY,
  mX,
  mZ = NULL,
  M,
  group_weights = NULL,
  var_weights = NULL,
  standardize.data = TRUE,
  splOrd = 4,
  lambda = NULL,
  nlambda = 30,
  lambda.min.ratio = NULL,
  intercept = FALSE,
  overall.group = FALSE,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f2sSP_+3A_vy">vY</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of observations of the scalar response variable.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_mx">mX</code></td>
<td>
<p>a <code class="reqn">(n\times r)</code> matrix of observations of the functional covariate.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_mz">mZ</code></td>
<td>
<p>an <code class="reqn">(n\times q)</code> full column rank matrix of scalar predictors that are not penalized.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_m">M</code></td>
<td>
<p>number of elements of the B-spline basis vector <code class="reqn">\varphi(t)</code>.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_group_weights">group_weights</code></td>
<td>
<p>a vector of length <code class="reqn">G</code> containing group-specific weights. The default is square root of the group cardinality, see Bernardi et al. (2022).</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_var_weights">var_weights</code></td>
<td>
<p>a vector of length <code class="reqn">M</code> containing basis-specific weights. The default is a vector where 
each entry is the reciprocal of the number of groups including that basis. See Bernardi et al. (2022) for details.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_standardize.data">standardize.data</code></td>
<td>
<p>logical. Should data be standardized?</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_splord">splOrd</code></td>
<td>
<p>the order <code class="reqn">d</code> of the spline basis.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_lambda">lambda</code></td>
<td>
<p>either a regularization parameter or a vector of regularization parameters. 
In this latter case the routine computes the whole path. If it is NULL values for lambda are provided by the routine.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_nlambda">nlambda</code></td>
<td>
<p>the number of lambda values - default is 30.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_lambda.min.ratio">lambda.min.ratio</code></td>
<td>
<p>smallest value for lambda, as a fraction of the maximum lambda value. If <code class="reqn">n&gt;M</code>, the default is 0.0001, and if <code class="reqn">n&lt;M</code>, the default is 0.01.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_intercept">intercept</code></td>
<td>
<p>logical. If it is TRUE, a column of ones is added to the design matrix.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_overall.group">overall.group</code></td>
<td>
<p>logical. If it is TRUE, an overall group including all penalized covariates is added.</p>
</td></tr>
<tr><td><code id="f2sSP_+3A_control">control</code></td>
<td>
<p>a list of control parameters for the ADMM algorithm. See ‘Details’.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing </p>

<dl>
<dt>sp.coefficients</dt><dd><p>a length-<code class="reqn">M</code> solution vector for the parameters <code class="reqn">\psi</code>, which corresponds to the minimum in-sample MSE.</p>
</dd>
<dt>sp.coef.path</dt><dd><p>an <code class="reqn">(n_\lambda\times M)</code> matrix of estimated <code class="reqn">\psi</code> coefficients for each lambda.</p>
</dd>
<dt>sp.fun</dt><dd><p>a length-<code class="reqn">r</code> vector providing the estimated functional coefficient for <code class="reqn">\psi(t)</code>.</p>
</dd>
<dt>sp.fun.path</dt><dd><p>an <code class="reqn">(n_\lambda\times r)</code> matrix providing the estimated functional coefficients for <code class="reqn">\psi(t)</code> for each lambda.</p>
</dd>
<dt>coefficients</dt><dd><p>a length-<code class="reqn">q</code> solution vector for the parameters <code class="reqn">\gamma</code>, which corresponds to the minimum in-sample MSE. 
It is provided only when either the matrix <code class="reqn">Z</code> in input is not NULL or the intercept is set to TRUE.</p>
</dd>
<dt>coef.path</dt><dd><p>an <code class="reqn">(n_\lambda\times q)</code> matrix of estimated <code class="reqn">\gamma</code> coefficients for each lambda. 
It is provided only when either the matrix <code class="reqn">Z</code> in input is not NULL or the intercept is set to TRUE.</p>
</dd>
<dt>lambda</dt><dd><p>sequence of lambda.</p>
</dd>
<dt>lambda.min</dt><dd><p>value of lambda that attains the minimum in-sample MSE.</p>
</dd>
<dt>mse</dt><dd><p>in-sample mean squared error.</p>
</dd>
<dt>min.mse</dt><dd><p>minimum value of the in-sample MSE for the sequence of lambda.</p>
</dd>
<dt>convergence</dt><dd><p>logical. 1 denotes achieved convergence.</p>
</dd>
<dt>elapsedTime</dt><dd><p>elapsed time in seconds.</p>
</dd>
<dt>iternum</dt><dd><p>number of iterations.</p>
</dd>
</dl>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates,
</p>

<dl>
<dt>objval</dt><dd><p>objective function value.</p>
</dd>
<dt>r_norm</dt><dd><p>norm of primal residual.</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual.</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition.</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition.</p>
</dd>
</dl>

<p>Iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Details</h3>

<p>The control argument is a list that can supply any of the following components:</p>

<dl>
<dt>adaptation</dt><dd><p>logical. If it is TRUE, ADMM with adaptation is performed. The default value is TRUE. See Boyd et al. (2011) for details.</p>
</dd>
<dt>rho</dt><dd><p>an augmented Lagrangian parameter. The default value is 1.</p>
</dd>
<dt>tau.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 2. See Boyd et al. (2011) and Lin et al. (2022) for details.</p>
</dd>
<dt>mu.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 10. See Boyd et al. (2011) and Lin et al. (2022) for details.</p>
</dd>
<dt>abstol</dt><dd><p>absolute tolerance stopping criterion. The default value is sqrt(sqrt(.Machine$double.eps)).</p>
</dd>
<dt>reltol</dt><dd><p>relative tolerance stopping criterion. The default value is sqrt(.Machine$double.eps).</p>
</dd>
<dt>maxit</dt><dd><p>maximum number of iterations. The default value is 100.</p>
</dd>
<dt>print.out</dt><dd><p>logical. If it is TRUE, a message about the procedure is printed. The default value is TRUE.</p>
</dd>
</dl>



<h3>References</h3>

<p>Bernardi M, Canale A, Stefanucci M (2022).
&ldquo;Locally Sparse Function-on-Function Regression.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>0</b>(0), 1-15.
<a href="https://doi.org/10.1080/10618600.2022.2130926">doi:10.1080/10618600.2022.2130926</a>, https://doi.org/10.1080/10618600.2022.2130926.
</p>
<p>Boyd S, Parikh N, Chu E, Peleato B, Eckstein J (2011).
&ldquo;Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.&rdquo;
<em>Foundations and Trends® in Machine Learning</em>, <b>3</b>(1), 1-122.
ISSN 1935-8237, <a href="https://doi.org/10.1561/2200000016">doi:10.1561/2200000016</a>, <a href="http://dx.doi.org/10.1561/2200000016">http://dx.doi.org/10.1561/2200000016</a>.
</p>
<p>Jenatton R, Audibert J, Bach F (2011).
&ldquo;Structured variable selection with sparsity-inducing norms.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>12</b>, 2777&ndash;2824.
ISSN 1532-4435.
</p>
<p>Lin Z, Li H, Fang C (2022).
<em>Alternating direction method of multipliers for machine learning</em>.
Springer, Singapore.
ISBN 978-981-16-9839-2; 978-981-16-9840-8, <a href="https://doi.org/10.1007/978-981-16-9840-8">doi:10.1007/978-981-16-9840-8</a>, With forewords by Zongben Xu and Zhi-Quan Luo.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate sample data
set.seed(1)
n     &lt;- 40
p     &lt;- 18                                  # number of basis to GENERATE beta
r     &lt;- 100
s     &lt;- seq(0, 1, length.out = r)

beta_basis &lt;- splines::bs(s, df = p, intercept = TRUE)    # basis
coef_data  &lt;- matrix(rnorm(n*floor(p/2)), n, floor(p/2))        
fun_data   &lt;- coef_data %*% t(splines::bs(s, df = floor(p/2), intercept = TRUE))     

x_0   &lt;- apply(matrix(rnorm(p, sd=1),p,1), 1, fdaSP::softhresh, 1)  # regression coefficients 
x_fun &lt;- beta_basis %*% x_0                

b     &lt;- fun_data %*% x_fun + rnorm(n, sd = sqrt(crossprod(fun_data %*% x_fun ))/10)
l     &lt;- 10^seq(2, -4, length.out = 30)
maxit &lt;- 1000


## set the hyper-parameters
maxit          &lt;- 1000
rho_adaptation &lt;- TRUE
rho            &lt;- 1
reltol         &lt;- 1e-5
abstol         &lt;- 1e-5

mod &lt;- f2sSP(vY = b, mX = fun_data, M = p,
             group_weights = NULL, var_weights = NULL, standardize.data = FALSE, splOrd = 4,
             lambda = NULL, nlambda = 30, lambda.min = NULL, overall.group = FALSE, 
             control = list("abstol" = abstol, 
                            "reltol" = reltol, 
                            "adaptation" = rho_adaptation, 
                            "rho" = rho, 
                            "print.out" = FALSE)) 

# plot coefficiente path
matplot(log(mod$lambda), mod$sp.coef.path, type = "l", 
        xlab = latex2exp::TeX("$\\log(\\lambda)$"), ylab = "", bty = "n", lwd = 1.2)

</code></pre>

<hr>
<h2 id='f2sSP_cv'>Cross-validation for Overlap Group Least Absolute Shrinkage and Selection Operator on scalar-on-function regression model</h2><span id='topic+f2sSP_cv'></span>

<h3>Description</h3>

<p>Overlap Group-LASSO for scalar-on-function regression model solves the following optimization problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_{\psi,\gamma} ~ \frac{1}{2} \sum_{i=1}^n \left( y_i - \int x_i(t) \psi(t) dt-z_i^\intercal\gamma \right)^2 + \lambda \sum_{g=1}^{G} \Vert S_{g}T\psi \Vert_2</code>
</p>

<p>to obtain a sparse coefficient vector <code class="reqn">\psi\in\mathbb{R}^{M}</code> for the functional penalized predictor <code class="reqn">x(t)</code> and a coefficient vector <code class="reqn">\gamma\in\mathbb{R}^q</code> for the unpenalized scalar predictors <code class="reqn">z_1,\dots,z_q</code>. The regression function is <code class="reqn">\psi(t)=\varphi(t)^\intercal\psi</code>
where <code class="reqn">\varphi(t)</code> is a B-spline basis of order <code class="reqn">d</code> and dimension <code class="reqn">M</code>. 
For each group <code class="reqn">g</code>, each row of the matrix <code class="reqn">S_g\in\mathbb{R}^{d\times M}</code> has non-zero entries only for those bases belonging 
to that group. These values are provided by the arguments <code>groups</code> and <code>group_weights</code> (see below). 
Each basis function belongs to more than one group. The diagonal matrix <code class="reqn">T\in\mathbb{R}^{M\times M}</code> contains 
the basis specific weights. These values are provided by the argument <code>var_weights</code> (see below).
The regularization path is computed for the overlap group-LASSO penalty at a grid of values for the regularization 
parameter <code class="reqn">\lambda</code> using the alternating direction method of multipliers (ADMM). See Boyd et al. (2011) and Lin et al. (2022) 
for details on the ADMM method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f2sSP_cv(
  vY,
  mX,
  mZ = NULL,
  M,
  group_weights = NULL,
  var_weights = NULL,
  standardize.data = FALSE,
  splOrd = 4,
  lambda = NULL,
  lambda.min.ratio = NULL,
  nlambda = NULL,
  cv.fold = 5,
  intercept = FALSE,
  overall.group = FALSE,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f2sSP_cv_+3A_vy">vY</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of observations of the scalar response variable.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_mx">mX</code></td>
<td>
<p>a <code class="reqn">(n\times r)</code> matrix of observations of the functional covariate.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_mz">mZ</code></td>
<td>
<p>an <code class="reqn">(n\times q)</code> full column rank matrix of scalar predictors that are not penalized.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_m">M</code></td>
<td>
<p>number of elements of the B-spline basis vector <code class="reqn">\varphi(t)</code>.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_group_weights">group_weights</code></td>
<td>
<p>a vector of length <code class="reqn">G</code> containing group-specific weights. The default is square root of the group cardinality, see Bernardi et al. (2022).</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_var_weights">var_weights</code></td>
<td>
<p>a vector of length <code class="reqn">M</code> containing basis-specific weights. The default is a vector where 
each entry is the reciprocal of the number of groups including that basis. See Bernardi et al. (2022) for details.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_standardize.data">standardize.data</code></td>
<td>
<p>logical. Should data be standardized?</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_splord">splOrd</code></td>
<td>
<p>the order <code class="reqn">d</code> of the spline basis.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_lambda">lambda</code></td>
<td>
<p>either a regularization parameter or a vector of regularization parameters. 
In this latter case the routine computes the whole path. If it is NULL values for lambda are provided by the routine.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_lambda.min.ratio">lambda.min.ratio</code></td>
<td>
<p>smallest value for lambda, as a fraction of the maximum lambda value. If <code class="reqn">n&gt;M</code>, the default is 0.0001, and if <code class="reqn">n&lt;M</code>, the default is 0.01.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_nlambda">nlambda</code></td>
<td>
<p>the number of lambda values - default is 30.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_cv.fold">cv.fold</code></td>
<td>
<p>the number of folds - default is 5.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_intercept">intercept</code></td>
<td>
<p>logical. If it is TRUE, a column of ones is added to the design matrix.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_overall.group">overall.group</code></td>
<td>
<p>logical. If it is TRUE, an overall group including all penalized covariates is added.</p>
</td></tr>
<tr><td><code id="f2sSP_cv_+3A_control">control</code></td>
<td>
<p>a list of control parameters for the ADMM algorithm. See ‘Details’.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing </p>

<dl>
<dt>sp.coefficients</dt><dd><p>a length-<code class="reqn">M</code> solution vector solution vector for the parameters <code class="reqn">\psi</code>, which corresponds to the minimum cross-validated MSE.</p>
</dd>
<dt>sp.fun</dt><dd><p>a length-<code class="reqn">r</code> vector providing the estimated functional coefficient for <code class="reqn">\psi(t)</code> corresponding to the minimum cross-validated MSE.</p>
</dd>
<dt>coefficients</dt><dd><p>a length-<code class="reqn">q</code> solution vector for the parameters <code class="reqn">\gamma</code>, which corresponds to the minimum cross-validated MSE.
It is provided only when either the matrix <code class="reqn">Z</code> in input is not NULL or the intercept is set to TRUE.</p>
</dd> 
<dt>lambda</dt><dd><p>sequence of lambda.</p>
</dd>
<dt>lambda.min</dt><dd><p>value of lambda that attains the minimum cross-validated MSE.</p>
</dd>
<dt>mse</dt><dd><p>cross-validated mean squared error.</p>
</dd>
<dt>min.mse</dt><dd><p>minimum value of the cross-validated MSE for the sequence of lambda.</p>
</dd>
<dt>convergence</dt><dd><p>logical. 1 denotes achieved convergence.</p>
</dd>
<dt>elapsedTime</dt><dd><p>elapsed time in seconds.</p>
</dd>
<dt>iternum</dt><dd><p>number of iterations.</p>
</dd>
</dl>

<p>Iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Details</h3>

<p>The control argument is a list that can supply any of the following components:</p>

<dl>
<dt>adaptation</dt><dd><p>logical. If it is TRUE, ADMM with adaptation is performed. The default value is TRUE. See Boyd et al. (2011) for details.</p>
</dd>
<dt>rho</dt><dd><p>an augmented Lagrangian parameter. The default value is 1.</p>
</dd>
<dt>tau.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 2. See Boyd et al. (2011) and Lin et al. (2022) for details.</p>
</dd>
<dt>mu.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 10. See Boyd et al. (2011) and Lin et al. (2022) for details.</p>
</dd>
<dt>abstol</dt><dd><p>absolute tolerance stopping criterion. The default value is sqrt(sqrt(.Machine$double.eps)).</p>
</dd>
<dt>reltol</dt><dd><p>relative tolerance stopping criterion. The default value is sqrt(.Machine$double.eps).</p>
</dd>
<dt>maxit</dt><dd><p>maximum number of iterations. The default value is 100.</p>
</dd>
<dt>print.out</dt><dd><p>logical. If it is TRUE, a message about the procedure is printed. The default value is TRUE.</p>
</dd>
</dl>



<h3>References</h3>

<p>Bernardi M, Canale A, Stefanucci M (2022).
&ldquo;Locally Sparse Function-on-Function Regression.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>0</b>(0), 1-15.
<a href="https://doi.org/10.1080/10618600.2022.2130926">doi:10.1080/10618600.2022.2130926</a>, https://doi.org/10.1080/10618600.2022.2130926.
</p>
<p>Boyd S, Parikh N, Chu E, Peleato B, Eckstein J (2011).
&ldquo;Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.&rdquo;
<em>Foundations and Trends® in Machine Learning</em>, <b>3</b>(1), 1-122.
ISSN 1935-8237, <a href="https://doi.org/10.1561/2200000016">doi:10.1561/2200000016</a>, <a href="http://dx.doi.org/10.1561/2200000016">http://dx.doi.org/10.1561/2200000016</a>.
</p>
<p>Jenatton R, Audibert J, Bach F (2011).
&ldquo;Structured variable selection with sparsity-inducing norms.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>12</b>, 2777&ndash;2824.
ISSN 1532-4435.
</p>
<p>Lin Z, Li H, Fang C (2022).
<em>Alternating direction method of multipliers for machine learning</em>.
Springer, Singapore.
ISBN 978-981-16-9839-2; 978-981-16-9840-8, <a href="https://doi.org/10.1007/978-981-16-9840-8">doi:10.1007/978-981-16-9840-8</a>, With forewords by Zongben Xu and Zhi-Quan Luo.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate sample data and functional coefficients
set.seed(1)
n     &lt;- 40
p     &lt;- 18                                 
r     &lt;- 100
s     &lt;- seq(0, 1, length.out = r)

beta_basis &lt;- splines::bs(s, df = p, intercept = TRUE)    # basis
coef_data  &lt;- matrix(rnorm(n*floor(p/2)), n, floor(p/2))        
fun_data   &lt;- coef_data %*% t(splines::bs(s, df = floor(p/2), intercept = TRUE))     

x_0   &lt;- apply(matrix(rnorm(p, sd=1),p,1), 1, fdaSP::softhresh, 1)  
x_fun &lt;- beta_basis %*% x_0                

b     &lt;- fun_data %*% x_fun + rnorm(n, sd = sqrt(crossprod(fun_data %*% x_fun ))/10)
l     &lt;- 10^seq(2, -4, length.out = 30)
maxit &lt;- 1000


## set the hyper-parameters
maxit          &lt;- 1000
rho_adaptation &lt;- TRUE
rho            &lt;- 1
reltol         &lt;- 1e-5
abstol         &lt;- 1e-5

## run cross-validation
mod_cv &lt;- f2sSP_cv(vY = b, mX = fun_data, M = p,
                   group_weights = NULL, var_weights = NULL, standardize.data = FALSE, splOrd = 4,
                   lambda = NULL, lambda.min = 1e-5, nlambda = 30, cv.fold = 5, intercept = FALSE, 
                   control = list("abstol" = abstol, 
                                  "reltol" = reltol, 
                                  "adaptation" = rho_adaptation,
                                  "rho" = rho, 
                                  "print.out" = FALSE))
                                          
### graphical presentation
plot(log(mod_cv$lambda), mod_cv$mse, type = "l", col = "blue", lwd = 2, bty = "n", 
     xlab = latex2exp::TeX("$\\log(\\lambda)$"), ylab = "Prediction Error", 
     ylim = range(mod_cv$mse - mod_cv$mse.sd, mod_cv$mse + mod_cv$mse.sd),
     main = "Cross-validated Prediction Error")
fdaSP::confband(xV = log(mod_cv$lambda), yVmin = mod_cv$mse - mod_cv$mse.sd, 
                yVmax = mod_cv$mse + mod_cv$mse.sd)       
abline(v = log(mod_cv$lambda[which(mod_cv$lambda == mod_cv$lambda.min)]), 
       col = "red", lwd = 1.0)

### comparison with oracle error
mod &lt;- f2sSP(vY = b, mX = fun_data, M = p, 
             group_weights = NULL, var_weights = NULL, 
             standardize.data = FALSE, splOrd = 4,
             lambda = NULL, nlambda = 30, 
             lambda.min = 1e-5, intercept = FALSE,
             control = list("abstol" = abstol, 
                            "reltol" = reltol, 
                            "adaptation" = rho_adaptation, 
                            "rho" = rho, 
                            "print.out" = FALSE))
                                    
err_mod &lt;- apply(mod$sp.coef.path, 1, function(x) sum((x - x_0)^2))
plot(log(mod$lambda), err_mod, type = "l", col = "blue", 
     lwd = 2, xlab = latex2exp::TeX("$\\log(\\lambda)$"), 
     ylab = "Estimation Error", main = "True Estimation Error", bty = "n")
abline(v = log(mod$lambda[which(err_mod == min(err_mod))]), col = "red", lwd = 1.0)
abline(v = log(mod_cv$lambda[which(mod_cv$lambda == mod_cv$lambda.min)]), 
       col = "red", lwd = 1.0, lty = 2)                                      

</code></pre>

<hr>
<h2 id='fdaSP-package'>Sparse Functional Data Analysis Methods</h2><span id='topic+fdaSP-package'></span><span id='topic+fdaSP'></span>

<h3>Description</h3>

<p>Provides algorithms to fit linear regression models under several popular penalization techniques and functional linear regression models based on Majorizing-Minimizing (MM) and Alternating Direction Method of Multipliers (ADMM) techniques.
    See Boyd et al (2010) &lt;doi:10.1561/2200000016&gt; for complete introduction to the method.</p>


<h3>Package Content</h3>


<p>Index of help topics:
</p>
<pre>
confband                Function to plot the confidence bands
f2fSP                   Overlap Group Least Absolute Shrinkage and
                        Selection Operator for function-on-function
                        regression model
f2fSP_cv                Cross-validation for Overlap Group Least
                        Absolute Shrinkage and Selection Operator for
                        function-on-function regression model
f2sSP                   Overlap Group Least Absolute Shrinkage and
                        Selection Operator for scalar-on-function
                        regression model
f2sSP_cv                Cross-validation for Overlap Group Least
                        Absolute Shrinkage and Selection Operator on
                        scalar-on-function regression model
fdaSP-package           Sparse Functional Data Analysis Methods
lmSP                    Sparse Adaptive Overlap Group Least Absolute
                        Shrinkage and Selection Operator
lmSP_cv                 Cross-validation for Sparse Adaptive Overlap
                        Group Least Absolute Shrinkage and Selection
                        Operator
softhresh               Function to solve the soft thresholding problem
</pre>

<h3>Maintainer</h3>

<p>Mauro Bernardi &lt;mauro.bernardi@unipd.it&gt;</p>


<h3>Author(s)</h3>

<p>Mauro Bernardi [aut, cre], Marco Stefanucci [aut], Antonio Canale [ctb]</p>

<hr>
<h2 id='lmSP'>Sparse Adaptive Overlap Group Least Absolute Shrinkage and Selection Operator</h2><span id='topic+lmSP'></span>

<h3>Description</h3>

<p>Sparse Adaptive overlap group-LASSO, or sparse adaptive group <code class="reqn">L_2</code>-regularized regression, solves the following optimization problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_{\beta,\gamma} ~ \frac{1}{2}\|y-X\beta-Z\gamma\|_2^2 + \lambda\Big[(1-\alpha) \sum_{g=1}^G \|S_g T\beta\|_2+\alpha\Vert T_1\beta\Vert_1\Big]</code>
</p>

<p>to obtain a sparse coefficient vector <code class="reqn">\beta\in\mathbb{R}^p</code> for the matrix of penalized predictors <code class="reqn">X</code> and a coefficient vector <code class="reqn">\gamma\in\mathbb{R}^q</code> 
for the matrix of unpenalized predictors <code class="reqn">Z</code>. For each group <code class="reqn">g</code>, each row of 
the matrix <code class="reqn">S_g\in\mathbb{R}^{n_g\times p}</code> has non-zero entries only for those variables belonging 
to that group. These values are provided by the arguments <code>groups</code> and <code>group_weights</code> (see below). 
Each variable can belong to more than one group. The diagonal matrix <code class="reqn">T\in\mathbb{R}^{p\times p}</code> contains the variable-specific weights. These values are
provided by the argument <code>var_weights</code> (see below). The diagonal matrix <code class="reqn">T_1\in\mathbb{R}^{p\times p}</code> contains 
the variable-specific <code class="reqn">L_1</code> weights. These values are provided by the argument <code>var_weights_L1</code> (see below).
The regularization path is computed for the sparse adaptive overlap group-LASSO penalty at a grid of values for the regularization 
parameter <code class="reqn">\lambda</code> using the alternating direction method of multipliers (ADMM). See Boyd et al. (2011) and Lin et al. (2022) 
for details on the ADMM method. The regularization is a combination of <code class="reqn">L_2</code> and
<code class="reqn">L_1</code> simultaneous constraints. Different specifications of the <code>penalty</code> argument lead to different models choice:
</p>

<dl>
<dt>LASSO</dt><dd><p>The classical Lasso regularization (Tibshirani, 1996) can be obtained by specifying <code class="reqn">\alpha = 1</code> 
and the matrix <code class="reqn">T_1</code> as the <code class="reqn">p \times p</code> identity matrix. An adaptive version of this model (Zou, 2006) can be obtained if <code class="reqn">T_1</code> is 
a <code class="reqn">p \times p</code>  diagonal matrix of adaptive weights. See also Hastie et al. (2015) for further details.</p>
</dd>
<dt>GLASSO</dt><dd><p>The group-Lasso regularization (Yuan and Lin, 2006) can be obtained by specifying <code class="reqn">\alpha = 0</code>, 
non-overlapping groups in <code class="reqn">S_g</code> and by setting the matrix <code class="reqn">T</code> equal to the <code class="reqn">p \times p</code> identity matrix. 
An adaptive version of this model can be obtained if the matrix <code class="reqn">T</code> is a <code class="reqn">p \times p</code> diagonal matrix of adaptive weights. See also Hastie et al. (2015) for further details.</p>
</dd>
<dt>spGLASSO</dt><dd><p>The sparse group-Lasso regularization (Simon et al., 2011) can be obtained by specifying <code class="reqn">\alpha\in(0,1)</code>,  
non-overlapping groups in <code class="reqn">S_g</code> and by setting the matrices <code class="reqn">T</code> and <code class="reqn">T_1</code> equal to the <code class="reqn">p \times p</code> identity matrix. 
An adaptive version of this model can be obtained if the matrices <code class="reqn">T</code> and <code class="reqn">T_1</code> are <code class="reqn">p \times p</code>  
diagonal matrices of adaptive weights.</p>
</dd>
<dt>OVGLASSO</dt><dd><p>The overlap group-Lasso regularization (Jenatton et al., 2011) can be obtained by specifying 
<code class="reqn">\alpha = 0</code>, overlapping groups in <code class="reqn">S_g</code> and by setting the matrix <code class="reqn">T</code> equal to the <code class="reqn">p \times p</code> identity matrix. An adaptive version of this model can be obtained if the matrix <code class="reqn">T</code> is a <code class="reqn">p \times p</code>  
diagonal matrix of adaptive weights.</p>
</dd>
<dt>spOVGLASSO</dt><dd><p>The sparse overlap group-Lasso regularization (Jenatton et al., 2011) can be obtained by specifying 
<code class="reqn">\alpha\in(0,1)</code>, overlapping groups in <code class="reqn">S_g</code> and by setting the matrices <code class="reqn">T</code> and <code class="reqn">T_1</code> equal to the <code class="reqn">p \times p</code> identity matrix. 
An adaptive version of this model can be obtained if the matrices <code class="reqn">T</code> and <code class="reqn">T_1</code> are <code class="reqn">p \times p</code>  diagonal matrices of adaptive weights.</p>
</dd>
</dl>



<h3>Usage</h3>

<pre><code class='language-R'>lmSP(
  X,
  Z = NULL,
  y,
  penalty = c("LASSO", "GLASSO", "spGLASSO", "OVGLASSO", "spOVGLASSO"),
  groups,
  group_weights = NULL,
  var_weights = NULL,
  var_weights_L1 = NULL,
  standardize.data = TRUE,
  intercept = FALSE,
  overall.group = FALSE,
  lambda = NULL,
  alpha = NULL,
  lambda.min.ratio = NULL,
  nlambda = 30,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmSP_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of penalized predictors.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_z">Z</code></td>
<td>
<p>an <code class="reqn">(n\times q)</code> full column rank matrix of predictors that are not penalized.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_y">y</code></td>
<td>
<p>a length-<code class="reqn">n</code> response vector.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_penalty">penalty</code></td>
<td>
<p>choose one from the following options: 'LASSO', for the or adaptive-Lasso penalties, 'GLASSO', 
for the group-Lasso penalty, 'spGLASSO', for the sparse group-Lasso penalty, 'OVGLASSO', 
for the overlap group-Lasso penalty and 'spOVGLASSO', for the sparse overlap group-Lasso penalty.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_groups">groups</code></td>
<td>
<p>either a vector of length <code class="reqn">p</code> of consecutive integers describing the grouping of the coefficients, 
or a list with two elements: the first element is a vector of length <code class="reqn">\sum_{g=1}^G n_g</code> containing the variables belonging to each group, where <code class="reqn">n_g</code> is the cardinality of the <code class="reqn">g</code>-th group, 
while the second element is a vector of length <code class="reqn">G</code> containing the group lengths (see example below).</p>
</td></tr>
<tr><td><code id="lmSP_+3A_group_weights">group_weights</code></td>
<td>
<p>a vector of length <code class="reqn">G</code> containing group-specific weights. The default is square root of the group cardinality, see Yuan and Lin (2006).</p>
</td></tr>
<tr><td><code id="lmSP_+3A_var_weights">var_weights</code></td>
<td>
<p>a vector of length <code class="reqn">p</code> containing variable-specific weights. The default is a vector of ones.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_var_weights_l1">var_weights_L1</code></td>
<td>
<p>a vector of length <code class="reqn">p</code> containing variable-specific weights for the <code class="reqn">L_1</code> penalty. The default is a vector of ones.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_standardize.data">standardize.data</code></td>
<td>
<p>logical. Should data be standardized?</p>
</td></tr>
<tr><td><code id="lmSP_+3A_intercept">intercept</code></td>
<td>
<p>logical. If it is TRUE, a column of ones is added to the design matrix.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_overall.group">overall.group</code></td>
<td>
<p>logical. This setting is only available for the overlap group-LASSO and the sparse overlap group-LASSO penalties, otherwise it is set to NULL. If it is TRUE, an overall group including all penalized covariates is added.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_lambda">lambda</code></td>
<td>
<p>either a regularization parameter or a vector of regularization parameters. In this latter case the routine computes the whole path. If it is NULL values for lambda are provided by the routine.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_alpha">alpha</code></td>
<td>
<p>the sparse overlap group-LASSO mixing parameter, with <code class="reqn">0\leq\alpha\leq1</code>. This setting is only available for the sparse group-LASSO and the sparse overlap group-LASSO penalties, otherwise it is set to NULL. The LASSO and group-LASSO penalties are obtained
by specifying <code class="reqn">\alpha = 1</code> and <code class="reqn">\alpha = 0</code>, respectively.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_lambda.min.ratio">lambda.min.ratio</code></td>
<td>
<p>smallest value for lambda, as a fraction of the maximum lambda value. If <code class="reqn">n&gt;p</code>, the default is 0.0001, and if <code class="reqn">n&lt;p</code>, the default is 0.01.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_nlambda">nlambda</code></td>
<td>
<p>the number of lambda values - default is 30.</p>
</td></tr>
<tr><td><code id="lmSP_+3A_control">control</code></td>
<td>
<p>a list of control parameters for the ADMM algorithm. See ‘Details’.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing </p>

<dl>
<dt>sp.coefficients</dt><dd><p>a length-<code class="reqn">p</code> solution vector for the parameters <code class="reqn">\beta</code>. If <code class="reqn">n_\lambda&gt;1</code> then  the provided vector corresponds to the minimum in-sample MSE.</p>
</dd>
<dt>coefficients</dt><dd><p>a length-<code class="reqn">q</code> solution vector for the parameters <code class="reqn">\gamma</code>. If <code class="reqn">n_\lambda&gt;1</code> then the provided vector corresponds to the minimum in-sample MSE.
It is provided only when either the matrix <code class="reqn">Z</code> in input is not NULL or the intercept is set to TRUE.</p>
</dd>
<dt>sp.coef.path</dt><dd><p>an <code class="reqn">(n_\lambda\times p)</code> matrix of estimated <code class="reqn">\beta</code> coefficients for each lambda of the provided sequence.</p>
</dd>
<dt>coef.path</dt><dd><p>an <code class="reqn">(n_\lambda\times q)</code> matrix of estimated <code class="reqn">\gamma</code> coefficients for each lambda of the provided sequence.
It is provided only when either the matrix <code class="reqn">Z</code> in input is not NULL or the intercept is set to TRUE.</p>
</dd>
<dt>lambda</dt><dd><p>sequence of lambda.</p>
</dd>
<dt>lambda.min</dt><dd><p>value of lambda that attains the minimum in sample MSE.</p>
</dd>
<dt>mse</dt><dd><p>in-sample mean squared error.</p>
</dd>
<dt>min.mse</dt><dd><p>minimum value of the in-sample MSE for the sequence of lambda.</p>
</dd>
<dt>convergence</dt><dd><p>logical. 1 denotes achieved convergence.</p>
</dd>
<dt>elapsedTime</dt><dd><p>elapsed time in seconds.</p>
</dd>
<dt>iternum</dt><dd><p>number of iterations.</p>
</dd>
</dl>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates:
</p>

<dl>
<dt>objval</dt><dd><p>objective function value</p>
</dd>
<dt>r_norm</dt><dd><p>norm of primal residual</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition.</p>
</dd>
</dl>

<p>Iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Details</h3>

<p>The control argument is a list that can supply any of the following components:</p>

<dl>
<dt>adaptation</dt><dd><p>logical. If it is TRUE, ADMM with adaptation is performed. The default value is TRUE. See Boyd et al. (2011) for details.</p>
</dd>
<dt>rho</dt><dd><p>an augmented Lagrangian parameter. The default value is 1.</p>
</dd>
<dt>tau.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 2. See Boyd et al. (2011) for details.</p>
</dd>
<dt>mu.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 10. See Boyd et al. (2011) for details.</p>
</dd>
<dt>abstol</dt><dd><p>absolute tolerance stopping criterion. The default value is sqrt(sqrt(.Machine$double.eps)).</p>
</dd>
<dt>reltol</dt><dd><p>relative tolerance stopping criterion. The default value is sqrt(.Machine$double.eps).</p>
</dd>
<dt>maxit</dt><dd><p>maximum number of iterations. The default value is 100.</p>
</dd>
<dt>print.out</dt><dd><p>logical. If it is TRUE, a message about the procedure is printed. The default value is TRUE.</p>
</dd>
</dl>



<h3>References</h3>

<p>Bernardi M, Canale A, Stefanucci M (2022).
&ldquo;Locally Sparse Function-on-Function Regression.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>0</b>(0), 1-15.
<a href="https://doi.org/10.1080/10618600.2022.2130926">doi:10.1080/10618600.2022.2130926</a>, https://doi.org/10.1080/10618600.2022.2130926.
</p>
<p>Boyd S, Parikh N, Chu E, Peleato B, Eckstein J (2011).
&ldquo;Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.&rdquo;
<em>Foundations and Trends® in Machine Learning</em>, <b>3</b>(1), 1-122.
ISSN 1935-8237, <a href="https://doi.org/10.1561/2200000016">doi:10.1561/2200000016</a>, <a href="http://dx.doi.org/10.1561/2200000016">http://dx.doi.org/10.1561/2200000016</a>.
</p>
<p>Hastie T, Tibshirani R, Wainwright M (2015).
<em>Statistical learning with sparsity: the lasso and generalizations</em>,  number 143 in Monographs on statistics and applied probability.
CRC Press, Taylor &amp; Francis Group, Boca Raton.
ISBN 978-1-4987-1216-3.
</p>
<p>Jenatton R, Audibert J, Bach F (2011).
&ldquo;Structured variable selection with sparsity-inducing norms.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>12</b>, 2777&ndash;2824.
ISSN 1532-4435.
</p>
<p>Lin Z, Li H, Fang C (2022).
<em>Alternating direction method of multipliers for machine learning</em>.
Springer, Singapore.
ISBN 978-981-16-9839-2; 978-981-16-9840-8, <a href="https://doi.org/10.1007/978-981-16-9840-8">doi:10.1007/978-981-16-9840-8</a>, With forewords by Zongben Xu and Zhi-Quan Luo. 
</p>
<p>Simon N, Friedman J, Hastie T, Tibshirani R (2013).
&ldquo;A sparse-group lasso.&rdquo;
<em>J. Comput. Graph. Statist.</em>, <b>22</b>(2), 231&ndash;245.
ISSN 1061-8600, <a href="https://doi.org/10.1080/10618600.2012.681250">doi:10.1080/10618600.2012.681250</a>.
</p>
<p>Yuan M, Lin Y (2006).
&ldquo;Model selection and estimation in regression with grouped variables.&rdquo;
<em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <b>68</b>(1), 49&ndash;67.
</p>
<p>Zou H (2006).
&ldquo;The adaptive lasso and its oracle properties.&rdquo;
<em>J. Amer. Statist. Assoc.</em>, <b>101</b>(476), 1418&ndash;1429.
ISSN 0162-1459, <a href="https://doi.org/10.1198/016214506000000735">doi:10.1198/016214506000000735</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### generate sample data
set.seed(2023)
n    &lt;- 50
p    &lt;- 30 
X    &lt;- matrix(rnorm(n*p), n, p)

### Example 1, LASSO penalty

beta &lt;- apply(matrix(rnorm(p, sd = 1), p, 1), 1, fdaSP::softhresh, 1.5)
y    &lt;- X %*% beta + rnorm(n, sd = sqrt(crossprod(X %*% beta)) / 20)

### set regularization parameter grid
lam   &lt;- 10^seq(0, -2, length.out = 30)

### set the hyper-parameters of the ADMM algorithm
maxit      &lt;- 1000
adaptation &lt;- TRUE
rho        &lt;- 1
reltol     &lt;- 1e-5
abstol     &lt;- 1e-5

### run example
mod &lt;- lmSP(X = X, y = y, penalty = "LASSO", standardize.data = FALSE, intercept = FALSE, 
            lambda = lam, control = list("adaptation" = adaptation, "rho" = rho, 
                                         "maxit" = maxit, "reltol" = reltol, 
                                         "abstol" = abstol, "print.out" = FALSE)) 

### graphical presentation
matplot(log(lam), mod$sp.coef.path, type = "l", main = "Lasso solution path",
        bty = "n", xlab = latex2exp::TeX("$\\log(\\lambda)$"), ylab = "")

### Example 2, sparse group-LASSO penalty

beta &lt;- c(rep(4, 12), rep(0, p - 13), -2)
y    &lt;- X %*% beta + rnorm(n, sd = sqrt(crossprod(X %*% beta)) / 20)

### define groups of dimension 3 each
group1 &lt;- rep(1:10, each = 3)

### set regularization parameter grid
lam   &lt;- 10^seq(1, -2, length.out = 30)

### set the alpha parameter 
alpha &lt;- 0.5

### set the hyper-parameters of the ADMM algorithm
maxit         &lt;- 1000
adaptation    &lt;- TRUE
rho           &lt;- 1
reltol        &lt;- 1e-5
abstol        &lt;- 1e-5

### run example
mod &lt;- lmSP(X = X, y = y, penalty = "spGLASSO", groups = group1, standardize.data = FALSE,  
            intercept = FALSE, lambda = lam, alpha = 0.5, 
            control = list("adaptation" = adaptation, "rho" = rho, 
                           "maxit" = maxit, "reltol" = reltol, "abstol" = abstol, 
                           "print.out" = FALSE)) 

### graphical presentation
matplot(log(lam), mod$sp.coef.path, type = "l", main = "Sparse Group Lasso solution path",
        bty = "n", xlab = latex2exp::TeX("$\\log(\\lambda)$"), ylab = "")

</code></pre>

<hr>
<h2 id='lmSP_cv'>Cross-validation for Sparse Adaptive Overlap Group Least Absolute Shrinkage and Selection Operator</h2><span id='topic+lmSP_cv'></span>

<h3>Description</h3>

<p>Sparse Adaptive overlap group-LASSO, or sparse adaptive group <code class="reqn">L_2</code>-regularized regression, solves the following optimization problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_{\beta,\gamma} ~ \frac{1}{2}\|y-X\beta-Z\gamma\|_2^2 + \lambda\Big[(1-\alpha) \sum_{g=1}^G \|S_g T\beta\|_2+\alpha\Vert T_1\beta\Vert_1\Big]</code>
</p>

<p>to obtain a sparse coefficient vector <code class="reqn">\beta\in\mathbb{R}^p</code> for the matrix of penalized predictors <code class="reqn">X</code> and a coefficient vector <code class="reqn">\gamma\in\mathbb{R}^q</code> 
for the matrix of unpenalized predictors <code class="reqn">Z</code>. For each group <code class="reqn">g</code>, each row of 
the matrix <code class="reqn">S_g\in\mathbb{R}^{n_g\times p}</code> has non-zero entries only for those variables belonging 
to that group. These values are provided by the arguments <code>groups</code> and <code>group_weights</code> (see below). 
Each variable can belong to more than one group. The diagonal matrix <code class="reqn">T\in\mathbb{R}^{p\times p}</code> contains the variable-specific weights. These values are
provided by the argument <code>var_weights</code> (see below). The diagonal matrix <code class="reqn">T_1\in\mathbb{R}^{p\times p}</code> contains 
the variable-specific <code class="reqn">L_1</code> weights. These values are provided by the argument <code>var_weights_L1</code> (see below).
The regularization path is computed for the sparse adaptive overlap group-LASSO penalty at a grid of values for the regularization 
parameter <code class="reqn">\lambda</code> using the alternating direction method of multipliers (ADMM). See Boyd et al. (2011) and Lin et al. (2022) 
for details on the ADMM method. The regularization is a combination of <code class="reqn">L_2</code> and
<code class="reqn">L_1</code> simultaneous constraints. Different specifications of the <code>penalty</code> argument lead to different models choice:
</p>

<dl>
<dt>LASSO</dt><dd><p>The classical Lasso regularization (Tibshirani, 1996) can be obtained by specifying <code class="reqn">\alpha = 1</code> 
and the matrix <code class="reqn">T_1</code> as the <code class="reqn">p \times p</code> identity matrix. An adaptive version of this model (Zou, 2006) can be obtained if <code class="reqn">T_1</code> is 
a <code class="reqn">p \times p</code>  diagonal matrix of adaptive weights. See also Hastie et al. (2015) for further details.</p>
</dd>
<dt>GLASSO</dt><dd><p>The group-Lasso regularization (Yuan and Lin, 2006) can be obtained by specifying <code class="reqn">\alpha = 0</code>, 
non-overlapping groups in <code class="reqn">S_g</code> and by setting the matrix <code class="reqn">T</code> equal to the <code class="reqn">p \times p</code> identity matrix. 
An adaptive version of this model can be obtained if the matrix <code class="reqn">T</code> is a <code class="reqn">p \times p</code> diagonal matrix of adaptive weights. See also Hastie et al. (2015) for further details.</p>
</dd>
<dt>spGLASSO</dt><dd><p>The sparse group-Lasso regularization (Simon et al., 2011) can be obtained by specifying <code class="reqn">\alpha\in(0,1)</code>,  
non-overlapping groups in <code class="reqn">S_g</code> and by setting the matrices <code class="reqn">T</code> and <code class="reqn">T_1</code> equal to the <code class="reqn">p \times p</code> identity matrix. 
An adaptive version of this model can be obtained if the matrices <code class="reqn">T</code> and <code class="reqn">T_1</code> are <code class="reqn">p \times p</code>  
diagonal matrices of adaptive weights.</p>
</dd>
<dt>OVGLASSO</dt><dd><p>The overlap group-Lasso regularization (Jenatton et al., 2011) can be obtained by specifying 
<code class="reqn">\alpha = 0</code>, overlapping groups in <code class="reqn">S_g</code> and by setting the matrix <code class="reqn">T</code> equal to the <code class="reqn">p \times p</code> identity matrix. An adaptive version of this model can be obtained if the matrix <code class="reqn">T</code> is a <code class="reqn">p \times p</code>  
diagonal matrix of adaptive weights.</p>
</dd>
<dt>spOVGLASSO</dt><dd><p>The sparse overlap group-Lasso regularization (Jenatton et al., 2011) can be obtained by specifying 
<code class="reqn">\alpha\in(0,1)</code>, overlapping groups in <code class="reqn">S_g</code> and by setting the matrices <code class="reqn">T</code> and <code class="reqn">T_1</code> equal to the <code class="reqn">p \times p</code> identity matrix. 
An adaptive version of this model can be obtained if the matrices <code class="reqn">T</code> and <code class="reqn">T_1</code> are <code class="reqn">p \times p</code>  diagonal matrices of adaptive weights.</p>
</dd>
</dl>



<h3>Usage</h3>

<pre><code class='language-R'>lmSP_cv(
  X,
  Z = NULL,
  y,
  penalty = c("LASSO", "GLASSO", "spGLASSO", "OVGLASSO", "spOVGLASSO"),
  groups,
  group_weights = NULL,
  var_weights = NULL,
  var_weights_L1 = NULL,
  cv.fold = 5,
  standardize.data = TRUE,
  intercept = FALSE,
  overall.group = FALSE,
  lambda = NULL,
  alpha = NULL,
  lambda.min.ratio = NULL,
  nlambda = 30,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmSP_cv_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of penalized predictors.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_z">Z</code></td>
<td>
<p>an <code class="reqn">(n\times q)</code> full column rank matrix of predictors that are not penalized.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_y">y</code></td>
<td>
<p>a length-<code class="reqn">n</code> response vector.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_penalty">penalty</code></td>
<td>
<p>choose one from the following options: 'LASSO', for the or adaptive-Lasso penalties, 'GLASSO', 
for the group-Lasso penalty, 'spGLASSO', for the sparse group-Lasso penalty, 'OVGLASSO', 
for the overlap group-Lasso penalty and 'spOVGLASSO', for the sparse overlap group-Lasso penalty.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_groups">groups</code></td>
<td>
<p>either a vector of length <code class="reqn">p</code> of consecutive integers describing the grouping of the coefficients, 
or a list with two elements: the first element is a vector of length <code class="reqn">\sum_{g=1}^G n_g</code> containing the variables belonging to each group, where <code class="reqn">n_g</code> is the cardinality of the <code class="reqn">g</code>-th group, 
while the second element is a vector of length <code class="reqn">G</code> containing the group lengths (see example below).</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_group_weights">group_weights</code></td>
<td>
<p>a vector of length <code class="reqn">G</code> containing group-specific weights. The default is square root of the group cardinality, see Yuan and Lin (2006).</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_var_weights">var_weights</code></td>
<td>
<p>a vector of length <code class="reqn">p</code> containing variable-specific weights. The default is a vector of ones.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_var_weights_l1">var_weights_L1</code></td>
<td>
<p>a vector of length <code class="reqn">p</code> containing variable-specific weights for the <code class="reqn">L_1</code> penalty. The default is a vector of ones.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_cv.fold">cv.fold</code></td>
<td>
<p>the number of folds - default is 5.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_standardize.data">standardize.data</code></td>
<td>
<p>logical. Should data be standardized?</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_intercept">intercept</code></td>
<td>
<p>logical. If it is TRUE, a column of ones is added to the design matrix.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_overall.group">overall.group</code></td>
<td>
<p>logical. This setting is only available for the overlap group-LASSO and the sparse overlap group-LASSO penalties, otherwise it is set to NULL. If it is TRUE, an overall group including all penalized covariates is added.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_lambda">lambda</code></td>
<td>
<p>either a regularization parameter or a vector of regularization parameters. In this latter case the routine computes the whole path. If it is NULL values for lambda are provided by the routine.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_alpha">alpha</code></td>
<td>
<p>the sparse overlap group-LASSO mixing parameter, with <code class="reqn">0\leq\alpha\leq1</code>. This setting is only available for the sparse group-LASSO and the sparse overlap group-LASSO penalties, otherwise it is set to NULL. The LASSO and group-LASSO penalties are obtained
by specifying <code class="reqn">\alpha = 1</code> and <code class="reqn">\alpha = 0</code>, respectively.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_lambda.min.ratio">lambda.min.ratio</code></td>
<td>
<p>smallest value for lambda, as a fraction of the maximum lambda value. If <code class="reqn">n&gt;p</code>, the default is 0.0001, and if <code class="reqn">n&lt;p</code>, the default is 0.01.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_nlambda">nlambda</code></td>
<td>
<p>the number of lambda values - default is 30.</p>
</td></tr>
<tr><td><code id="lmSP_cv_+3A_control">control</code></td>
<td>
<p>a list of control parameters for the ADMM algorithm. See ‘Details’.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing </p>

<dl>
<dt>sp.coefficients</dt><dd><p>a length-<code class="reqn">p</code> solution vector for the parameters <code class="reqn">\beta</code>. If <code class="reqn">n_\lambda&gt;1</code> then  the provided vector corresponds to the minimum cross-validated MSE.</p>
</dd>
<dt>coefficients</dt><dd><p>a length-<code class="reqn">q</code> solution vector for the parameters <code class="reqn">\gamma</code>. If <code class="reqn">n_\lambda&gt;1</code> then  the provided vector corresponds to the minimum cross-validated MSE.
It is provided only when either the matrix <code class="reqn">Z</code> in input is not NULL or the intercept is set to TRUE.</p>
</dd>
<dt>sp.coef.path</dt><dd><p>an <code class="reqn">(n_\lambda\times p)</code> matrix of estimated <code class="reqn">\beta</code> coefficients for each lambda of the provided sequence.</p>
</dd>
<dt>coef.path</dt><dd><p>an <code class="reqn">(n_\lambda\times q)</code> matrix of estimated <code class="reqn">\gamma</code> coefficients for each lambda of the provided sequence. 
It is provided only when either the matrix <code class="reqn">Z</code> in input is not NULL or the intercept is set to TRUE.</p>
</dd>
<dt>lambda</dt><dd><p>sequence of lambda.</p>
</dd>
<dt>lambda.min</dt><dd><p>value of lambda that attains the minimum cross-validated MSE.</p>
</dd>
<dt>mse</dt><dd><p>cross-validated mean squared error.</p>
</dd>
<dt>min.mse</dt><dd><p>minimum value of the cross-validated MSE for the sequence of lambda.</p>
</dd>
<dt>convergence</dt><dd><p>logical. 1 denotes achieved convergence.</p>
</dd>
<dt>elapsedTime</dt><dd><p>elapsed time in seconds.</p>
</dd>
<dt>iternum</dt><dd><p>number of iterations.</p>
</dd>
</dl>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates:
</p>

<dl>
<dt>objval</dt><dd><p>objective function value</p>
</dd>
<dt>r_norm</dt><dd><p>norm of primal residual</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition.</p>
</dd>
</dl>

<p>Iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Details</h3>

<p>The control argument is a list that can supply any of the following components:</p>

<dl>
<dt>adaptation</dt><dd><p>logical. If it is TRUE, ADMM with adaptation is performed. The default value is TRUE. See Boyd et al. (2011) for details.</p>
</dd>
<dt>rho</dt><dd><p>an augmented Lagrangian parameter. The default value is 1.</p>
</dd>
<dt>tau.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 2. See Boyd et al. (2011) for details.</p>
</dd>
<dt>mu.ada</dt><dd><p>an adaptation parameter greater than one. Only needed if adaptation = TRUE. The default value is 10. See Boyd et al. (2011) for details.</p>
</dd>
<dt>abstol</dt><dd><p>absolute tolerance stopping criterion. The default value is sqrt(sqrt(.Machine$double.eps)).</p>
</dd>
<dt>reltol</dt><dd><p>relative tolerance stopping criterion. The default value is sqrt(.Machine$double.eps).</p>
</dd>
<dt>maxit</dt><dd><p>maximum number of iterations. The default value is 100.</p>
</dd>
<dt>print.out</dt><dd><p>logical. If it is TRUE, a message about the procedure is printed. The default value is TRUE.</p>
</dd>
</dl>



<h3>References</h3>

<p>Bernardi M, Canale A, Stefanucci M (2022).
&ldquo;Locally Sparse Function-on-Function Regression.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>0</b>(0), 1-15.
<a href="https://doi.org/10.1080/10618600.2022.2130926">doi:10.1080/10618600.2022.2130926</a>, https://doi.org/10.1080/10618600.2022.2130926.
</p>
<p>Boyd S, Parikh N, Chu E, Peleato B, Eckstein J (2011).
&ldquo;Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.&rdquo;
<em>Foundations and Trends® in Machine Learning</em>, <b>3</b>(1), 1-122.
ISSN 1935-8237, <a href="https://doi.org/10.1561/2200000016">doi:10.1561/2200000016</a>, <a href="http://dx.doi.org/10.1561/2200000016">http://dx.doi.org/10.1561/2200000016</a>.
</p>
<p>Hastie T, Tibshirani R, Wainwright M (2015).
<em>Statistical learning with sparsity: the lasso and generalizations</em>,  number 143 in Monographs on statistics and applied probability.
CRC Press, Taylor &amp; Francis Group, Boca Raton.
ISBN 978-1-4987-1216-3.
</p>
<p>Jenatton R, Audibert J, Bach F (2011).
&ldquo;Structured variable selection with sparsity-inducing norms.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>12</b>, 2777&ndash;2824.
ISSN 1532-4435.
</p>
<p>Lin Z, Li H, Fang C (2022).
<em>Alternating direction method of multipliers for machine learning</em>.
Springer, Singapore.
ISBN 978-981-16-9839-2; 978-981-16-9840-8, <a href="https://doi.org/10.1007/978-981-16-9840-8">doi:10.1007/978-981-16-9840-8</a>, With forewords by Zongben Xu and Zhi-Quan Luo. 
</p>
<p>Simon N, Friedman J, Hastie T, Tibshirani R (2013).
&ldquo;A sparse-group lasso.&rdquo;
<em>J. Comput. Graph. Statist.</em>, <b>22</b>(2), 231&ndash;245.
ISSN 1061-8600, <a href="https://doi.org/10.1080/10618600.2012.681250">doi:10.1080/10618600.2012.681250</a>.
</p>
<p>Yuan M, Lin Y (2006).
&ldquo;Model selection and estimation in regression with grouped variables.&rdquo;
<em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <b>68</b>(1), 49&ndash;67.
</p>
<p>Zou H (2006).
&ldquo;The adaptive lasso and its oracle properties.&rdquo;
<em>J. Amer. Statist. Assoc.</em>, <b>101</b>(476), 1418&ndash;1429.
ISSN 0162-1459, <a href="https://doi.org/10.1198/016214506000000735">doi:10.1198/016214506000000735</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### generate sample data
set.seed(2023)
n    &lt;- 50
p    &lt;- 30 
X    &lt;- matrix(rnorm(n * p), n, p)

### Example 1, LASSO penalty

beta &lt;- apply(matrix(rnorm(p, sd = 1), p, 1), 1, fdaSP::softhresh, 1.5)
y    &lt;- X %*% beta + rnorm(n, sd = sqrt(crossprod(X %*% beta)) / 20)

### set the hyper-parameters of the ADMM algorithm
maxit      &lt;- 1000
adaptation &lt;- TRUE
rho        &lt;- 1
reltol     &lt;- 1e-5
abstol     &lt;- 1e-5

### run cross-validation
mod_cv &lt;- lmSP_cv(X = X, y = y, penalty = "LASSO", 
                  standardize.data = FALSE, intercept = FALSE,
                  cv.fold = 5, nlambda = 30, 
                  control = list("adaptation" = adaptation, 
                                 "rho" = rho, 
                                 "maxit" = maxit, "reltol" = reltol, 
                                 "abstol" = abstol, 
                                 "print.out" = FALSE)) 

### graphical presentation
plot(log(mod_cv$lambda), mod_cv$mse, type = "l", col = "blue", lwd = 2, bty = "n", 
     xlab = latex2exp::TeX("$\\log(\\lambda)$"), ylab = "Prediction Error", 
     ylim = range(mod_cv$mse - mod_cv$mse.sd, mod_cv$mse + mod_cv$mse.sd),
     main = "Cross-validated Prediction Error")
fdaSP::confband(xV = log(mod_cv$lambda), yVmin = mod_cv$mse - mod_cv$mse.sd, 
                yVmax = mod_cv$mse + mod_cv$mse.sd)       
abline(v = log(mod_cv$lambda[which(mod_cv$lambda == mod_cv$lambda.min)]), 
       col = "red", lwd = 1.0)

### comparison with oracle error
mod &lt;- lmSP(X = X, y = y, penalty = "LASSO", 
            standardize.data = FALSE, 
            intercept = FALSE,
            nlambda = 30, 
            control = list("adaptation" = adaptation, 
                           "rho" = rho, 
                           "maxit" = maxit, "reltol" = reltol, 
                           "abstol" = abstol, 
                           "print.out" = FALSE)) 
                                         
err_mod &lt;- apply(mod$sp.coef.path, 1, function(x) sum((x - beta)^2))
plot(log(mod$lambda), err_mod, type = "l", col = "blue", lwd = 2, 
     xlab = latex2exp::TeX("$\\log(\\lambda)$"), 
     ylab = "Estimation Error", main = "True Estimation Error", bty = "n")
abline(v = log(mod$lambda[which(err_mod == min(err_mod))]), col = "red", lwd = 1.0)
abline(v = log(mod_cv$lambda[which(mod_cv$lambda == mod_cv$lambda.min)]), 
       col = "red", lwd = 1.0, lty = 2)

### Example 2, sparse group-LASSO penalty

beta &lt;- c(rep(4, 12), rep(0, p - 13), -2)
y    &lt;- X %*% beta + rnorm(n, sd = sqrt(crossprod(X %*% beta)) / 20)

### define groups of dimension 3 each
group1 &lt;- rep(1:10, each = 3)

### set regularization parameter grid
lam   &lt;- 10^seq(1, -2, length.out = 30)

### set the alpha parameter 
alpha &lt;- 0.5

### set the hyper-parameters of the ADMM algorithm
maxit         &lt;- 1000
adaptation    &lt;- TRUE
rho           &lt;- 1
reltol        &lt;- 1e-5
abstol        &lt;- 1e-5

### run cross-validation
mod_cv &lt;- lmSP_cv(X = X, y = y, penalty = "spGLASSO", 
                  groups = group1, cv.fold = 5, 
                  standardize.data = FALSE,  intercept = FALSE, 
                  lambda = lam, alpha = 0.5, 
                  control = list("adaptation" = adaptation, 
                                 "rho" = rho,
                                 "maxit" = maxit, "reltol" = reltol, 
                                 "abstol" = abstol, 
                                 "print.out" = FALSE)) 

### graphical presentation
plot(log(mod_cv$lambda), mod_cv$mse, type = "l", col = "blue", lwd = 2, bty = "n", 
     xlab = latex2exp::TeX("$\\log(\\lambda)$"), ylab = "Prediction Error", 
     ylim = range(mod_cv$mse - mod_cv$mse.sd, mod_cv$mse + mod_cv$mse.sd),
     main = "Cross-validated Prediction Error")
fdaSP::confband(xV = log(mod_cv$lambda), yVmin = mod_cv$mse - mod_cv$mse.sd, 
                yVmax = mod_cv$mse + mod_cv$mse.sd)       
abline(v = log(mod_cv$lambda[which(mod_cv$lambda == mod_cv$lambda.min)]), 
       col = "red", lwd = 1.0)

### comparison with oracle error
mod &lt;- lmSP(X = X, y = y, 
            penalty = "spGLASSO", 
            groups = group1, 
            standardize.data = FALSE, 
            intercept = FALSE,
            lambda = lam, 
            alpha = 0.5, 
            control = list("adaptation" = adaptation, "rho" = rho, 
                           "maxit" = maxit, "reltol" = reltol, "abstol" = abstol, 
                           "print.out" = FALSE)) 
                                         
err_mod &lt;- apply(mod$sp.coef.path, 1, function(x) sum((x - beta)^2))
plot(log(mod$lambda), err_mod, type = "l", col = "blue", lwd = 2, 
     xlab = latex2exp::TeX("$\\log(\\lambda)$"), 
     ylab = "Estimation Error", main = "True Estimation Error", bty = "n")
abline(v = log(mod$lambda[which(err_mod == min(err_mod))]), col = "red", lwd = 1.0)
abline(v = log(mod_cv$lambda[which(mod_cv$lambda == mod_cv$lambda.min)]), 
       col = "red", lwd = 1.0, lty = 2)

</code></pre>

<hr>
<h2 id='softhresh'>Function to solve the soft thresholding problem</h2><span id='topic+softhresh'></span>

<h3>Description</h3>

<p>Function to solve the soft thresholding problem
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softhresh(x, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="softhresh_+3A_x">x</code></td>
<td>
<p>the data value.</p>
</td></tr>
<tr><td><code id="softhresh_+3A_lambda">lambda</code></td>
<td>
<p>the lambda value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the solution to the soft thresholding operator.
</p>


<h3>References</h3>

<p>Hastie T, Tibshirani R, Wainwright M (2015).
<em>Statistical learning with sparsity: the lasso and generalizations</em>,  number 143 in Monographs on statistics and applied probability.
CRC Press, Taylor &amp; Francis Group, Boca Raton.
ISBN 978-1-4987-1216-3.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
