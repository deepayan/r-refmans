<!DOCTYPE html><html><head><title>Help for package tidyposterior</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tidyposterior}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#autoplot.posterior'><p>Visualize the Posterior Distributions of Model Statistics</p></a></li>
<li><a href='#autoplot.posterior_diff'><p>Visualize the Posterior Distributions of Model Differences</p></a></li>
<li><a href='#contrast_models'><p>Estimate the Difference Between Models</p></a></li>
<li><a href='#no_trans'><p>Simple Transformation Functions</p></a></li>
<li><a href='#perf_mod'><p>Bayesian Analysis of Resampling Statistics</p></a></li>
<li><a href='#precise_example'><p>Example Data Sets</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#summary.posterior'><p>Summarize the Posterior Distributions of Model Statistics</p></a></li>
<li><a href='#summary.posterior_diff'><p>Summarize Posterior Distributions of Model Differences</p></a></li>
<li><a href='#tidy.perf_mod'><p>Extract Posterior Distributions for Models</p></a></li>
<li><a href='#tidyposterior-package'><p>tidyposterior: Bayesian Analysis to Compare Models using Resampling Statistics</p></a></li>
<li><a href='#vec_restore.posterior'><p>Extra methods for the posterior class to work with dplyr verbs</p></a></li>
<li><a href='#vec_restore.posterior_diff'><p>Extra methods for the <code>posterior_diff</code> class to work with dplyr verbs</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Bayesian Analysis to Compare Models using Resampling Statistics</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian analysis used here to answer the question: "when
    looking at resampling results, are the differences between models
    'real'?" To answer this, a model can be created were the performance
    statistic is the resampling statistics (e.g. accuracy or RMSE). These
    values are explained by the model types. In doing this, we can get
    parameter estimates for each model's affect on performance and make
    statistical (and practical) comparisons between models. The methods
    included here are similar to Benavoli et al (2017)
    <a href="https://jmlr.org/papers/v18/16-305.html">https://jmlr.org/papers/v18/16-305.html</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://tidyposterior.tidymodels.org">https://tidyposterior.tidymodels.org</a>,
<a href="https://github.com/tidymodels/tidyposterior">https://github.com/tidymodels/tidyposterior</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tidymodels/tidyposterior/issues">https://github.com/tidymodels/tidyposterior/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr (&gt; 1.0.0), generics, ggplot2, purrr, rlang, rsample (&ge;
0.0.2), rstanarm (&ge; 2.21.1), stats, tibble, tidyr (&ge; 0.7.1),
tune (&ge; 0.2.0), utils, vctrs (&ge; 0.3.0), workflowsets</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, knitr, parsnip, rmarkdown, testthat (&ge; 3.0.0),
yardstick</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>tidymodels, tidyverse/tidytemplate</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-11 17:19:04 UTC; max</td>
</tr>
<tr>
<td>Author:</td>
<td>Max Kuhn <a href="https://orcid.org/0000-0003-2402-136X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Max Kuhn &lt;max@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-11 18:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='autoplot.posterior'>Visualize the Posterior Distributions of Model Statistics</h2><span id='topic+autoplot.posterior'></span><span id='topic+autoplot.perf_mod'></span><span id='topic+autoplot.perf_mod_workflow_set'></span>

<h3>Description</h3>

<p>For objects of classes <code>posterior</code> and <code>perf_mod</code>, <code>autoplot()</code> produces a
simple plot of posterior distributions. For workflow set objects, there are
several types of plots that can be produced.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'posterior'
autoplot(object, ...)

## S3 method for class 'perf_mod'
autoplot(object, ...)

## S3 method for class 'perf_mod_workflow_set'
autoplot(object, type = "intervals", prob = 0.9, size = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoplot.posterior_+3A_object">object</code></td>
<td>
<p>An object produced by <code><a href="#topic+perf_mod">perf_mod()</a></code>, <code><a href="#topic+tidy.perf_mod">tidy.perf_mod()</a></code>, or a
workflow set with computed results.</p>
</td></tr>
<tr><td><code id="autoplot.posterior_+3A_...">...</code></td>
<td>
<p>Options passed to <code>geom_line(stat = "density", ...)</code>.</p>
</td></tr>
<tr><td><code id="autoplot.posterior_+3A_type">type</code></td>
<td>
<p>A value of one of: <code>"intervals"</code> (for model rank versus posterior
probability using interval estimation), <code>"posteriors"</code> (density plots for
each model), or <code>"ROPE"</code> (for practical equivalence probabilities versus
workflow rank).</p>
</td></tr>
<tr><td><code id="autoplot.posterior_+3A_prob">prob</code></td>
<td>
<p>A number p (0 &lt; p &lt; 1) indicating the desired
probability mass to include in the intervals.</p>
</td></tr>
<tr><td><code id="autoplot.posterior_+3A_size">size</code></td>
<td>
<p>The size of an effective difference in the units of the chosen
metric. For example, a 5 percent increase in accuracy (<code>size = 0.05</code>)
between two models might be considered a &quot;real&quot; difference.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot()</a></code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ex_objects)
autoplot(posterior_samples)
</code></pre>

<hr>
<h2 id='autoplot.posterior_diff'>Visualize the Posterior Distributions of Model Differences</h2><span id='topic+autoplot.posterior_diff'></span>

<h3>Description</h3>

<p>A density is created for each contrast in a faceted grid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'posterior_diff'
autoplot(object, size = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoplot.posterior_diff_+3A_object">object</code></td>
<td>
<p>An object produced by <code><a href="#topic+contrast_models">contrast_models()</a></code>.</p>
</td></tr>
<tr><td><code id="autoplot.posterior_diff_+3A_size">size</code></td>
<td>
<p>The size of an effective difference. For example, a
5\
&quot;real&quot; difference.</p>
</td></tr>
<tr><td><code id="autoplot.posterior_diff_+3A_...">...</code></td>
<td>
<p>Options passed to <code>geom_line(stat = "density", ...)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot()</a></code> object using <code>geom_density</code>
faceted by the models being contrasted (when there are 2 or
more contrasts).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ex_objects)
library(ggplot2)
autoplot(contrast_samples)
</code></pre>

<hr>
<h2 id='contrast_models'>Estimate the Difference Between Models</h2><span id='topic+contrast_models'></span>

<h3>Description</h3>

<p>The posterior distributions created by <code><a href="#topic+perf_mod">perf_mod()</a></code> can be used to obtain
the posterior distribution of the difference(s) between models. One or more
comparisons can be computed at the same time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contrast_models(x, list_1 = NULL, list_2 = NULL, seed = sample.int(10000, 1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="contrast_models_+3A_x">x</code></td>
<td>
<p>An object produced by <code><a href="#topic+perf_mod">perf_mod()</a></code>.</p>
</td></tr>
<tr><td><code id="contrast_models_+3A_list_1">list_1</code>, <code id="contrast_models_+3A_list_2">list_2</code></td>
<td>
<p>Character vectors of equal length that specify the
specific pairwise contrasts. The contrast is parameterized as
<code>list_1[i] - list_2[i]</code>. If the defaults are left to <code>NULL</code>, all
combinations are evaluated.</p>
</td></tr>
<tr><td><code id="contrast_models_+3A_seed">seed</code></td>
<td>
<p>A single integer for sampling from the posterior.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a transformation was used when <code>x</code> was created, the inverse is
applied <em>before</em> the difference is computed.
</p>


<h3>Value</h3>

<p>A data frame of the posterior distribution(s) of the difference(s).
The object has an extra class of <code>"posterior_diff"</code>.
</p>

<hr>
<h2 id='no_trans'>Simple Transformation Functions</h2><span id='topic+no_trans'></span><span id='topic+logit_trans'></span><span id='topic+Fisher_trans'></span><span id='topic+ln_trans'></span><span id='topic+inv_trans'></span>

<h3>Description</h3>

<p>A set of objects are contained here to easily facilitate the
use of outcome transformations for modeling. For example, if
there is a large amount of variability in the resampling results
for the Kappa statistics, which lies between -1 and 1, assuming
normality may produce posterior estimates outside of the natural
bound. One way to solve this is to use a link function or assume
a prior that is appropriately bounded. Another approach is to
transform the outcome values prior to modeling using a Gaussian
prior and reverse-transforming the posterior estimates prior to
visualization and summarization. These object can help
facilitate this last approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>no_trans

logit_trans

Fisher_trans

ln_trans

inv_trans
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 2.
</p>
<p>An object of class <code>list</code> of length 2.
</p>
<p>An object of class <code>list</code> of length 2.
</p>
<p>An object of class <code>list</code> of length 2.
</p>
<p>An object of class <code>list</code> of length 2.
</p>


<h3>Details</h3>

<p>The <code>logit_trans</code> object is useful for model
performance statistics bounds in zero and one, such as accuracy
or the area under the ROC curve.
</p>
<p><code>ln_trans</code> and <code>inv_trans</code> can be useful when the statistics
are right-skewed and strictly positive.
</p>
<p><code>Fisher_trans</code> was originally used for correlation statistics
but can be used here for an metrics falling between -1 and 1,
such as Kappa.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>logit_trans$func(.5)
logit_trans$inv(0)
</code></pre>

<hr>
<h2 id='perf_mod'>Bayesian Analysis of Resampling Statistics</h2><span id='topic+perf_mod'></span><span id='topic+perf_mod.rset'></span><span id='topic+perf_mod.resamples'></span><span id='topic+perf_mod.data.frame'></span><span id='topic+perf_mod.tune_results'></span><span id='topic+perf_mod.workflow_set'></span>

<h3>Description</h3>

<p>Bayesian analysis used here to answer the question: &quot;when looking at
resampling results, are the differences between models 'real?'&quot; To answer
this, a model can be created were the <em>outcome</em> is the resampling statistics
(e.g. accuracy or RMSE). These values are explained by the model types. In
doing this, we can get parameter estimates for each model's affect on
performance and make statistical (and practical) comparisons between models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perf_mod(object, ...)

## S3 method for class 'rset'
perf_mod(object, transform = no_trans, hetero_var = FALSE, formula = NULL, ...)

## S3 method for class 'resamples'
perf_mod(
  object,
  transform = no_trans,
  hetero_var = FALSE,
  metric = object$metrics[1],
  ...
)

## S3 method for class 'data.frame'
perf_mod(object, transform = no_trans, hetero_var = FALSE, formula = NULL, ...)

## S3 method for class 'tune_results'
perf_mod(
  object,
  metric = NULL,
  transform = no_trans,
  hetero_var = FALSE,
  formula = NULL,
  filter = NULL,
  ...
)

## S3 method for class 'workflow_set'
perf_mod(
  object,
  metric = NULL,
  transform = no_trans,
  hetero_var = FALSE,
  formula = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perf_mod_+3A_object">object</code></td>
<td>
<p>Depending on the context (see Details below):
</p>

<ul>
<li><p> A data frame with <code>id</code> columns for the resampling groupds and metric
results in all of the other columns..
</p>
</li>
<li><p> An <code>rset</code> object (such as <code><a href="rsample.html#topic+vfold_cv">rsample::vfold_cv()</a></code>) containing the <code>id</code>
column(s) and at least two numeric columns of model performance
statistics (e.g. accuracy).
</p>
</li>
<li><p> An object from <code>caret::resamples</code>.
</p>
</li>
<li><p> An object with class <code>tune_results</code>, which could be produced by
<code>tune::tune_grid()</code>, <code>tune::tune_bayes()</code> or similar.
</p>
</li>
<li><p> A workflow set where all results contain the metric value given in the
<code>metric</code> argument value.
</p>
</li></ul>
</td></tr>
<tr><td><code id="perf_mod_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code><a href="rstanarm.html#topic+stan_glmer">rstanarm::stan_glmer()</a></code> such as
<code>verbose</code>, <code>prior</code>, <code>seed</code>, <code>refresh</code>, <code>family</code>, etc.</p>
</td></tr>
<tr><td><code id="perf_mod_+3A_transform">transform</code></td>
<td>
<p>An named list of transformation and inverse
transformation functions. See <code><a href="#topic+logit_trans">logit_trans()</a></code> as an example.</p>
</td></tr>
<tr><td><code id="perf_mod_+3A_hetero_var">hetero_var</code></td>
<td>
<p>A logical; if <code>TRUE</code>, then different
variances are estimated for each model group. Otherwise, the
same variance is used for each group. Estimating heterogeneous
variances may slow or prevent convergence.</p>
</td></tr>
<tr><td><code id="perf_mod_+3A_formula">formula</code></td>
<td>
<p>An optional model formula to use for the Bayesian hierarchical model
(see Details below).</p>
</td></tr>
<tr><td><code id="perf_mod_+3A_metric">metric</code></td>
<td>
<p>A single character value for the statistic from
the <code>resamples</code> object that should be analyzed.</p>
</td></tr>
<tr><td><code id="perf_mod_+3A_filter">filter</code></td>
<td>
<p>A conditional logic statement that can be used to filter the
statistics generated by <code>tune_results</code> using the tuning parameter values or
the <code>.config</code> column.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions can be used to process and analyze matched
resampling statistics from different models using a Bayesian generalized
linear model with effects for the model and the resamples.
</p>


<h4>Bayesian Model formula</h4>

<p>By default, a generalized linear model with Gaussian error and an identity
link is fit to the data and has terms for the predictive model grouping
variable. In this way, the performance metrics can be compared between
models.
</p>
<p>Additionally, random effect terms are also used. For most resampling
methods (except repeated <em>V</em>-fold cross-validation), a simple random
intercept model its used with an exchangeable (i.e. compound-symmetric)
variance structure. In the case of repeated cross-validation, two random
intercept terms are used; one for the repeat and another for the fold within
repeat. These also have exchangeable correlation structures.
</p>
<p>The above model specification assumes that the variance in the performance
metrics is the same across models. However, this is unlikely to be true in
some cases. For example, for simple binomial accuracy, it well know that the
variance is highest when the accuracy is near 50 percent. When the argument
<code>hetero_var = TRUE</code>, the variance structure uses random intercepts for each
model term. This may produce more realistic posterior distributions but may
take more time to converge.
</p>
<p>Examples of the default formulas are:
</p>
<pre>
   # One ID field and common variance:
     statistic ~ model + (model | id)

   # One ID field and heterogeneous variance:
     statistic ~ model + (model + 0 | id)

   # Repeated CV (id = repeat, id2 = fold within repeat)
   # with a common variance:
     statistic ~ model + (model | id2/id)

   # Repeated CV (id = repeat, id2 = fold within repeat)
   # with a heterogeneous variance:
     statistic ~ model + (model + 0| id2/id)

   # Default for unknown resampling method and
   # multiple ID fields:
     statistic ~ model + (model | idN/../id)
 </pre>
<p>Custom formulas should use <code>statistic</code> as the outcome variable and <code>model</code>
as the factor variable with the model names.
</p>
<p>Also, as shown in the package vignettes, the Gaussian assumption make be
unrealistic. In this case, there are at least two approaches that can be
used. First, the outcome statistics can be transformed prior to fitting the
model. For example, for accuracy, the logit transformation can be used to
convert the outcome values to be on the real line and a model is fit to
these data. Once the posterior distributions are computed, the inverse
transformation can be used to put them back into the original units. The
<code>transform</code> argument can be used to do this.
</p>
<p>The second approach would be to use a different error distribution from the
exponential family. For RMSE values, the Gamma distribution may produce
better results at the expense of model computational complexity. This can be
achieved by passing the <code>family</code> argument to <code>perf_mod</code> as one might with
the <code>glm</code> function.
</p>



<h4>Input formats</h4>

<p>There are several ways to give resampling results to the <code>perf_mod()</code> function. To
illustrate, here are some example objects using 10-fold cross-validation for a
simple two-class problem:
</p>
<div class="sourceCode r"><pre>   library(tidymodels)
   library(tidyposterior)
   library(workflowsets)

   data(two_class_dat, package = "modeldata")

   set.seed(100)
   folds &lt;- vfold_cv(two_class_dat)
</pre></div>
<p>We can define two different models (for simplicity, with no tuning parameters).
</p>
<div class="sourceCode r"><pre>   logistic_reg_glm_spec &lt;-
     logistic_reg() %&gt;%
     set_engine('glm')

   mars_earth_spec &lt;-
     mars(prod_degree = 1) %&gt;%
     set_engine('earth') %&gt;%
     set_mode('classification')
</pre></div>
<p>For tidymodels, the <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code> function can be used to estimate
performance for each model/resample:
</p>
<div class="sourceCode r"><pre>   rs_ctrl &lt;- control_resamples(save_workflow = TRUE)

   logistic_reg_glm_res &lt;-
     logistic_reg_glm_spec %&gt;%
     fit_resamples(Class ~ ., resamples = folds, control = rs_ctrl)

   mars_earth_res &lt;-
     mars_earth_spec %&gt;%
     fit_resamples(Class ~ ., resamples = folds, control = rs_ctrl)
</pre></div>
<p>From these, there are several ways to pass the results to <code>perf_mod()</code>.
</p>


<h5>Data Frame as Input</h5>

<p>The most general approach is to have a data frame with the resampling labels (i.e.,
one or more id columns) as well as columns for each model that you would like to
compare.
</p>
<p>For the model results above, <code><a href="tune.html#topic+collect_predictions">tune::collect_metrics()</a></code> can be used along with some
basic data manipulation steps:
</p>
<div class="sourceCode r"><pre>   logistic_roc &lt;-
     collect_metrics(logistic_reg_glm_res, summarize = FALSE) %&gt;%
     dplyr::filter(.metric == "roc_auc") %&gt;%
     dplyr::select(id, logistic = .estimate)

   mars_roc &lt;-
     collect_metrics(mars_earth_res, summarize = FALSE) %&gt;%
     dplyr::filter(.metric == "roc_auc") %&gt;%
     dplyr::select(id, mars = .estimate)

   resamples_df &lt;- full_join(logistic_roc, mars_roc, by = "id")
   resamples_df
</pre></div>
<div class="sourceCode"><pre>   ## # A tibble: 10 x 3
   ##   id     logistic  mars
   ##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;
   ## 1 Fold01    0.908 0.875
   ## 2 Fold02    0.904 0.917
   ## 3 Fold03    0.924 0.938
   ## 4 Fold04    0.881 0.881
   ## 5 Fold05    0.863 0.864
   ## 6 Fold06    0.893 0.889
   ## # â€¦ with 4 more rows
</pre></div>
<p>We can then give this directly to <code>perf_mod()</code>:
</p>
<div class="sourceCode r"><pre>   set.seed(101)
   roc_model_via_df &lt;- perf_mod(resamples_df, refresh = 0)
   tidy(roc_model_via_df) %&gt;% summary()
</pre></div>
<div class="sourceCode"><pre>   ## # A tibble: 2 x 4
   ##   model     mean lower upper
   ##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
   ## 1 logistic 0.892 0.879 0.906
   ## 2 mars     0.888 0.875 0.902
</pre></div>



<h5>rsample Object as Input</h5>

<p>Alternatively, the result columns can be merged back into the original <code>rsample</code>
object. The up-side to using this method is that <code>perf_mod()</code> will know exactly
which model formula to use for the Bayesian model:
</p>
<div class="sourceCode r"><pre>   resamples_rset &lt;-
     full_join(folds, logistic_roc, by = "id") %&gt;%
     full_join(mars_roc, by = "id")

   set.seed(101)
   roc_model_via_rset &lt;- perf_mod(resamples_rset, refresh = 0)
   tidy(roc_model_via_rset) %&gt;% summary()
</pre></div>
<div class="sourceCode"><pre>   ## # A tibble: 2 x 4
   ##   model     mean lower upper
   ##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
   ## 1 logistic 0.892 0.879 0.906
   ## 2 mars     0.888 0.875 0.902
</pre></div>



<h5>Workflow Set Object as Input</h5>

<p>Finally, for tidymodels, a workflow set object can be used. This is a collection of
models/preprocessing combinations in one object. We can emulate a workflow set using
the existing example results then pass that to <code>perf_mod()</code>:
</p>
<div class="sourceCode r"><pre>   example_wset &lt;-
     as_workflow_set(logistic = logistic_reg_glm_res, mars = mars_earth_res)

   set.seed(101)
   roc_model_via_wflowset &lt;- perf_mod(example_wset, refresh = 0)
   tidy(roc_model_via_rset) %&gt;% summary()
</pre></div>
<div class="sourceCode"><pre>   ## # A tibble: 2 x 4
   ##   model     mean lower upper
   ##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
   ## 1 logistic 0.892 0.879 0.906
   ## 2 mars     0.888 0.875 0.902
</pre></div>



<h5>caret resamples object</h5>

<p>The <code>caret</code> package can also be used. An equivalent set of models are created:
</p>
<div class="sourceCode r"><pre>   library(caret)

   set.seed(102)
   logistic_caret &lt;- train(Class ~ ., data = two_class_dat, method = "glm",
                           trControl = trainControl(method = "cv"))

   set.seed(102)
   mars_caret &lt;- train(Class ~ ., data = two_class_dat, method = "gcvEarth",
                       tuneGrid = data.frame(degree = 1),
                       trControl = trainControl(method = "cv"))
</pre></div>
<p>Note that these two models use the same resamples as one another due to setting the
seed prior to calling <code>train()</code>. However, these are different from the tidymodels
results used above (so the final results will be different).
</p>
<p><code>caret</code> has a <code>resamples()</code> function that can collect and collate the resamples.
This can also be given to <code>perf_mod()</code>:
</p>
<div class="sourceCode r"><pre>   caret_resamples &lt;- resamples(list(logistic = logistic_caret, mars = mars_caret))

   set.seed(101)
   roc_model_via_caret &lt;- perf_mod(caret_resamples, refresh = 0)
   tidy(roc_model_via_caret) %&gt;% summary()
</pre></div>
<div class="sourceCode"><pre>   ## # A tibble: 2 x 4
   ##   model     mean lower upper
   ##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
   ## 1 logistic 0.821 0.801 0.842
   ## 2 mars     0.822 0.802 0.842
</pre></div>




<h3>Value</h3>

<p>An object of class <code>perf_mod</code>. If a workfkow set is given in
<code>object</code>, there is an extra class of <code>"perf_mod_workflow_set"</code>.
</p>


<h3>References</h3>

<p>Kuhn and Silge (2021) <em>Tidy Models with R</em>, Chapter 11,
<a href="https://www.tmwr.org/compare.html">https://www.tmwr.org/compare.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tidy.perf_mod">tidy.perf_mod()</a></code>, <code><a href="#topic+contrast_models">contrast_models()</a></code>
</p>

<hr>
<h2 id='precise_example'>Example Data Sets</h2><span id='topic+precise_example'></span><span id='topic+noisy_example'></span><span id='topic+concrete_example'></span><span id='topic+ts_example'></span><span id='topic+ex_object'></span><span id='topic+posterior_samples'></span><span id='topic+contrast_samples'></span>

<h3>Description</h3>

<p>Example Data Sets
</p>


<h3>Details</h3>

<p>Several data sets are contained in the package
as examples. Each <em>simulates</em> an <code>rset</code> object but the <code>splits</code>
columns are not included to save space.
</p>

<ul>
<li><p><code>precise_example</code> contains the results of the classification
analysis of a real data set using 10-fold CV. The holdout data
sets contained thousands of examples and have precise
performance estimates. Three models were fit to the original
data and several performance metrics are included.
</p>
</li>
<li><p><code>noisy_example</code> was also generated from a regression data
simulation. The original data set was small (50 samples) and
10-repeated of 10-fold CV were used with four models. There is
an excessive of variability in the results (probably more than
the resample-to-resample variability). The RMSE distributions
show fairly right-skewed distributions.
</p>
</li>
<li><p><code>concrete_example</code> contains the results of the regression case
study from the book <em>Applied Predictive Modeling</em>. The original
data set contained 745 samples in the training set. 10-repeats
of 10-fold CV was also used and 13 models were fit to the data.
</p>
</li>
<li><p><code>ts_example</code> is from a data set where rolling-origin forecast
resampling was used. Each assessment set is the summary of 14
observations (i.e. 2 weeks). The analysis set consisted of a
base of about 5,500 samples plus the previous assessment sets.
Four regression models were applied to these data.
</p>
</li>
<li><p><code>ex_object</code> objects were generated from the <code>two_class_dat</code> data in
the <code>modeldata</code> package. Basic 10-fold cross validation was used to evaluate
the models. The <code>posterior_samples</code> object is samples of the posterior
distribution of the model ROC values while <code>contrast_samples</code> are posterior
probabilities form the differences in ROC values.
</p>
</li></ul>



<h3>Value</h3>

<p>Tibbles with the additional class <code>rset</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(precise_example)
precise_example
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+tidy'></span><span id='topic+autoplot'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+tidy">tidy</a></code></p>
</dd>
<dt>ggplot2</dt><dd><p><code><a href="ggplot2.html#topic+autoplot">autoplot</a></code></p>
</dd>
</dl>

<hr>
<h2 id='summary.posterior'>Summarize the Posterior Distributions of Model Statistics</h2><span id='topic+summary.posterior'></span>

<h3>Description</h3>

<p>Numerical summaries are created for each model including the
posterior mean and upper and lower credible intervals (aka
uncertainty intervals).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'posterior'
summary(object, prob = 0.9, seed = sample.int(10000, 1), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.posterior_+3A_object">object</code></td>
<td>
<p>An object produced by <code><a href="#topic+tidy.perf_mod">tidy.perf_mod()</a></code>.</p>
</td></tr>
<tr><td><code id="summary.posterior_+3A_prob">prob</code></td>
<td>
<p>A number p (0 &lt; p &lt; 1) indicating the desired
probability mass to include in the intervals.</p>
</td></tr>
<tr><td><code id="summary.posterior_+3A_seed">seed</code></td>
<td>
<p>A single integer for sampling from the posterior.</p>
</td></tr>
<tr><td><code id="summary.posterior_+3A_...">...</code></td>
<td>
<p>Not currently used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with summary statistics and a row for
each model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("ex_objects")

summary(posterior_samples)
</code></pre>

<hr>
<h2 id='summary.posterior_diff'>Summarize Posterior Distributions of Model Differences</h2><span id='topic+summary.posterior_diff'></span>

<h3>Description</h3>

<p>Credible intervals are created for the differences. Also,
region of practical equivalence (ROPE) statistics are computed
when the effective size of a difference is given.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'posterior_diff'
summary(object, prob = 0.9, size = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.posterior_diff_+3A_object">object</code></td>
<td>
<p>An object produced by <code><a href="#topic+contrast_models">contrast_models()</a></code>.</p>
</td></tr>
<tr><td><code id="summary.posterior_diff_+3A_prob">prob</code></td>
<td>
<p>A number p (0 &lt; p &lt; 1) indicating the desired
probability mass to include in the intervals.</p>
</td></tr>
<tr><td><code id="summary.posterior_diff_+3A_size">size</code></td>
<td>
<p>The size of an effective difference in the units of the chosen
metric. For example, a 5 percent increase in accuracy (<code>size = 0.05</code>)
between two models might be considered a &quot;real&quot; difference.</p>
</td></tr>
<tr><td><code id="summary.posterior_diff_+3A_...">...</code></td>
<td>
<p>Not currently used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ROPE estimates included in the results are the
columns <code>pract_neg</code>, <code>pract_equiv</code>, and <code>pract_pos</code>. <code>pract_neg</code>
integrates the portion of the posterior below <code>-size</code> (and
<code>pract_pos</code> is the upper integral starting at <code>size</code>). The
interpretation depends on whether the metric being analyzed is
better when larger or smaller. <code>pract_equiv</code> integrates between
<code style="white-space: pre;">&#8288;[-size, size]&#8288;</code>. If this is close to one, the two models are
unlikely to be practically different relative to <code>size</code>.
</p>


<h3>Value</h3>

<p>A data frame with interval and ROPE statistics for each
comparison.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("ex_objects")

summary(contrast_samples)
summary(contrast_samples, size = 0.025)
</code></pre>

<hr>
<h2 id='tidy.perf_mod'>Extract Posterior Distributions for Models</h2><span id='topic+tidy.perf_mod'></span>

<h3>Description</h3>

<p><code>tidy</code> can be used on an object produced by <code><a href="#topic+perf_mod">perf_mod()</a></code>
to create a data frame with a column for the model name and
the posterior predictive distribution values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'perf_mod'
tidy(x, seed = sample.int(10000, 1), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy.perf_mod_+3A_x">x</code></td>
<td>
<p>An object from <code><a href="#topic+perf_mod">perf_mod()</a></code></p>
</td></tr>
<tr><td><code id="tidy.perf_mod_+3A_seed">seed</code></td>
<td>
<p>A single integer for sampling from the posterior.</p>
</td></tr>
<tr><td><code id="tidy.perf_mod_+3A_...">...</code></td>
<td>
<p>Not currently used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that this posterior only reflects the variability
of the groups (i.e. the fixed effects). This helps answer the
question of which model is best <em>for this data set</em>. If does not
answer the question of which model would be best on a new
resample of the data (which would have greater variability).
</p>


<h3>Value</h3>

<p>A data frame with the additional class <code>"posterior"</code>
</p>

<hr>
<h2 id='tidyposterior-package'>tidyposterior: Bayesian Analysis to Compare Models using Resampling Statistics</h2><span id='topic+tidyposterior'></span><span id='topic+tidyposterior-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Bayesian analysis used here to answer the question: &quot;when looking at resampling results, are the differences between models 'real'?&quot; To answer this, a model can be created were the performance statistic is the resampling statistics (e.g. accuracy or RMSE). These values are explained by the model types. In doing this, we can get parameter estimates for each model's affect on performance and make statistical (and practical) comparisons between models. The methods included here are similar to Benavoli et al (2017) <a href="https://jmlr.org/papers/v18/16-305.html">https://jmlr.org/papers/v18/16-305.html</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Max Kuhn <a href="mailto:max@posit.co">max@posit.co</a> (<a href="https://orcid.org/0000-0003-2402-136X">ORCID</a>)
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Posit Software, PBC [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://tidyposterior.tidymodels.org">https://tidyposterior.tidymodels.org</a>
</p>
</li>
<li> <p><a href="https://github.com/tidymodels/tidyposterior">https://github.com/tidymodels/tidyposterior</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tidymodels/tidyposterior/issues">https://github.com/tidymodels/tidyposterior/issues</a>
</p>
</li></ul>


<hr>
<h2 id='vec_restore.posterior'>Extra methods for the posterior class to work with dplyr verbs</h2><span id='topic+vec_restore.posterior'></span><span id='topic+vec_proxy.posterior'></span><span id='topic+vec_ptype2.posterior.posterior'></span><span id='topic+vec_ptype2.posterior.tbl_df'></span><span id='topic+vec_ptype2.tbl_df.posterior'></span><span id='topic+vec_ptype2.posterior.data.frame'></span><span id='topic+vec_ptype2.data.frame.posterior'></span><span id='topic+vec_cast.posterior.posterior'></span><span id='topic+vec_cast.posterior.tbl_df'></span><span id='topic+vec_cast.tbl_df.posterior'></span><span id='topic+vec_cast.posterior.data.frame'></span><span id='topic+vec_cast.data.frame.posterior'></span>

<h3>Description</h3>

<p>Objects with class <code>posterior</code> are defined to be tibbles with required
columns <code>model</code> (character) and <code>posterior</code> (numeric). If operations on these
objects break those rules, they are down-cast to basic tibbles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vec_restore.posterior(x, to, ...)

vec_proxy.posterior(x, ...)

vec_ptype2.posterior.posterior(x, y, ..., x_arg = "", y_arg = "")

vec_ptype2.posterior.tbl_df(x, y, ..., x_arg = "", y_arg = "")

vec_ptype2.tbl_df.posterior(x, y, ..., x_arg = "", y_arg = "")

vec_ptype2.posterior.data.frame(x, y, ..., x_arg = "", y_arg = "")

vec_ptype2.data.frame.posterior(x, y, ..., x_arg = "", y_arg = "")

vec_cast.posterior.posterior(x, to, ..., x_arg = "", to_arg = "")

vec_cast.posterior.tbl_df(x, to, ..., x_arg = "", to_arg = "")

vec_cast.tbl_df.posterior(x, to, ..., x_arg = "", to_arg = "")

vec_cast.posterior.data.frame(x, to, ..., x_arg = "", to_arg = "")

vec_cast.data.frame.posterior(x, to, ..., x_arg = "", to_arg = "")
</code></pre>

<hr>
<h2 id='vec_restore.posterior_diff'>Extra methods for the <code>posterior_diff</code> class to work with dplyr verbs</h2><span id='topic+vec_restore.posterior_diff'></span><span id='topic+vec_proxy.posterior_diff'></span><span id='topic+vec_ptype2.posterior_diff.posterior_diff'></span><span id='topic+vec_ptype2.posterior_diff.tbl_df'></span><span id='topic+vec_ptype2.tbl_df.posterior_diff'></span><span id='topic+vec_ptype2.posterior_diff.data.frame'></span><span id='topic+vec_ptype2.data.frame.posterior_diff'></span><span id='topic+vec_cast.posterior_diff.posterior_diff'></span><span id='topic+vec_cast.posterior_diff.tbl_df'></span><span id='topic+vec_cast.tbl_df.posterior_diff'></span><span id='topic+vec_cast.posterior_diff.data.frame'></span><span id='topic+vec_cast.data.frame.posterior_diff'></span>

<h3>Description</h3>

<p>Objects with class <code>posterior_diff</code> are defined to be tibbles with required
columns <code>difference</code> (numeric) and character columns <code>model_1</code>, <code>model_2</code>,
and <code>contrast</code>. If operations on these objects break those rules, they are
down-cast to basic tibbles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vec_restore.posterior_diff(x, to, ...)

vec_proxy.posterior_diff(x, ...)

vec_ptype2.posterior_diff.posterior_diff(x, y, ..., x_arg = "", y_arg = "")

vec_ptype2.posterior_diff.tbl_df(x, y, ..., x_arg = "", y_arg = "")

vec_ptype2.tbl_df.posterior_diff(x, y, ..., x_arg = "", y_arg = "")

vec_ptype2.posterior_diff.data.frame(x, y, ..., x_arg = "", y_arg = "")

vec_ptype2.data.frame.posterior_diff(x, y, ..., x_arg = "", y_arg = "")

vec_cast.posterior_diff.posterior_diff(x, to, ..., x_arg = "", to_arg = "")

vec_cast.posterior_diff.tbl_df(x, to, ..., x_arg = "", to_arg = "")

vec_cast.tbl_df.posterior_diff(x, to, ..., x_arg = "", to_arg = "")

vec_cast.posterior_diff.data.frame(x, to, ..., x_arg = "", to_arg = "")

vec_cast.data.frame.posterior_diff(x, to, ..., x_arg = "", to_arg = "")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
