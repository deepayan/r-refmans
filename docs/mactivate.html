<!DOCTYPE html><html lang="en"><head><title>Help for package mactivate</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mactivate}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mactivate-package'>
<p>m-activation</p></a></li>
<li><a href='#df_hospitals_ortho'>
<p>Orthopedic Device Sales</p></a></li>
<li><a href='#f_control_mactivate'>
<p>Set Fitting Hyperparameters</p></a></li>
<li><a href='#f_dmss_dW'>
<p>Calculate Derivative of Cost Function wrt W</p></a></li>
<li><a href='#f_fit_gradient_01'>
<p>Fit Multivariate Regression Model with mactivate Using Gradient Descent</p></a></li>
<li><a href='#f_fit_gradient_logistic_01'>
<p>Fit Logistic Multivariate Regression Model with mactivate Using Gradient Descent</p></a></li>
<li><a href='#f_fit_hybrid_01'>
<p>Fit Multivariate Regression Model with mactivate Using Hybrid Method</p></a></li>
<li><a href='#f_logit_cost'>
<p>Logistic Cost</p></a></li>
<li><a href='#f_mactivate'>
<p>Map Activation Layer and Inputs to Polynomial Model Inputs</p></a></li>
<li><a href='#predict.mactivate_fit_gradient_01'>
<p>Predict from Fitted Gradient Model</p></a></li>
<li><a href='#predict.mactivate_fit_gradient_logistic_01'>
<p>Predict from Fitted Gradient Logistic Model</p></a></li>
<li><a href='#predict.mactivate_fit_hybrid_01'>
<p>Predict from Fitted Hybrid Model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Multiplicative Activation</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-08-02</td>
</tr>
<tr>
<td>Author:</td>
<td>Dave Zes</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dave Zes &lt;zesdave@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides methods and classes for adding m-activation ("multiplicative activation") layers to MLR or multivariate logistic regression models.  M-activation layers created in this library detect and add input interaction (polynomial) effects into a predictive model.  M-activation can detect high-order interactions &ndash; a traditionally non-trivial challenge.  Details concerning application, methodology, and relevant survey literature can be found in this library's vignette, "About."</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-08-02 16:01:07 UTC; davezes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-08-02 16:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='mactivate-package'>
m-activation
</h2><span id='topic+mactivate-package'></span><span id='topic+mactivate'></span>

<h3>Description</h3>

<p>Provides methods and classes for adding m-activation (&quot;multiplicative activation&quot;) layers to MLR or multivariate logistic regression models.  M-activation layers created in this library detect and add input interaction (polynomial) effects into a predictive model.  M-activation can detect high-order interactions &ndash; a traditionally non-trivial challenge.  Details concerning application, methodology, and relevant survey literature can be found in this library's vignette, &quot;About.&quot;
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> mactivate</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Multiplicative Activation</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.6.6</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2021-08-02</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Dave Zes</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Dave Zes &lt;zesdave@gmail.com&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Provides methods and classes for adding m-activation ("multiplicative activation") layers to MLR or multivariate logistic regression models.  M-activation layers created in this library detect and add input interaction (polynomial) effects into a predictive model.  M-activation can detect high-order interactions -- a traditionally non-trivial challenge.  Details concerning application, methodology, and relevant survey literature can be found in this library's vignette, "About."</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;=3)</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 3.5.0)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
df_hospitals_ortho      Orthopedic Device Sales
f_control_mactivate     Set Fitting Hyperparameters
f_dmss_dW               Calculate Derivative of Cost Function wrt W
f_fit_gradient_01       Fit Multivariate Regression Model with
                        mactivate Using Gradient Descent
f_fit_gradient_logistic_01
                        Fit Logistic Multivariate Regression Model with
                        mactivate Using Gradient Descent
f_fit_hybrid_01         Fit Multivariate Regression Model with
                        mactivate Using Hybrid Method
f_logit_cost            Logistic Cost
f_mactivate             Map Activation Layer and Inputs to Polynomial
                        Model Inputs
mactivate-package       m-activation
predict.mactivate_fit_gradient_01
                        Predict from Fitted Gradient Model
predict.mactivate_fit_gradient_logistic_01
                        Predict from Fitted Gradient Logistic Model
predict.mactivate_fit_hybrid_01
                        Predict from Fitted Hybrid Model
</pre>
<p>Please make sure to read Details in <code><a href="#topic+f_dmss_dW">f_dmss_dW</a></code> help page before using this library.
This package allows the user to extend the usual multivariate regression solution by adding (parallel) multiplicative &ldquo;activation layers.&rdquo;  These activation layers can be very useful for identifying input interactions, and, if the user wishes, transparently test the appropriateness of input transformations.  Three functions are provided for fitting data, <code><a href="#topic+f_fit_hybrid_01">f_fit_hybrid_01</a></code> and <code><a href="#topic+f_fit_gradient_01">f_fit_gradient_01</a></code> for a numeric response (usual MLR), and <code><a href="#topic+f_fit_gradient_logistic_01">f_fit_gradient_logistic_01</a></code> for a binary response (multivariate logistic regresssion).
The user is encouraged to consult the &ldquo;About&rdquo; vignette as well as the examples available in the respective functions' documentation for details about m-activation and practical examples of implementation.
</p>


<h3>Author(s)</h3>

<p>Dave Zes
</p>
<p>Maintainer: Dave Zes &lt;zesdave@gmail.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## please see docs for individual functions.
</code></pre>

<hr>
<h2 id='df_hospitals_ortho'>
Orthopedic Device Sales
</h2><span id='topic+df_hospitals_ortho'></span>

<h3>Description</h3>

<p>Sales data of orthopedic device company to client hospitals over almost 2 years. 15 variables, 4703 hospitals.  Unit of observation is a unique hospital.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(df_hospitals_ortho)</code></pre>


<h3>Format</h3>

<p>Variables are:
</p>
<p><code>zip</code>:               'character': Postal code.
</p>
<p><code>hid</code>:               'character': Hospital ID.
</p>
<p><code>city</code>:              'character': Hospital city.
</p>
<p><code>state</code>:             'character': Hospital state.
</p>
<p><code>tot_sales</code>:         'numeric': Total sales to hospital.
</p>
<p><code>tot_knee</code>:          'numeric': Number of knee operations.
</p>
<p><code>tot_hip</code>:           'numeric': Number of hip operations.
</p>
<p><code>beds</code>:              'numeric': Number of beds.
</p>
<p><code>rehab_beds</code>:        'numeric': Number of beds dedicated for rehabilitation.
</p>
<p><code>outpatient_visits</code>: 'numeric': Number of outpatient visits.
</p>
<p><code>adm_costs</code>:         'numeric': Administrative costs ($1000's / yr).
</p>
<p><code>revenue_inpatient</code>: 'numeric': Inpatient revenue.
</p>
<p><code>is_teaching</code>:       'numeric': Is teaching hospital?
</p>
<p><code>has_trauma</code>:        'numeric': Has trauma center?
</p>
<p><code>has_rehab</code>:         'numeric': Offers rehabilitation?
</p>


<h3>Details</h3>

<p>This data frame has attribute &lsquo;modelvars&rsquo; which gives names of numeric model variables.
</p>


<h3>Source</h3>

<p>Data adapted from &lsquo;c84.dat&rsquo; from Statistical Consulting, Javier Cabrera and Andrew McDougall.
</p>


<h3>References</h3>

<p>Statistical Consulting, Javier Cabrera and Andrew McDougall.  Springer, Piscataway, NJ, 2002.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(df_hospitals_ortho)

tail(df_hospitals_ortho)

dim(df_hospitals_ortho)

attr(df_hospitals_ortho, "modelvars")
</code></pre>

<hr>
<h2 id='f_control_mactivate'>
Set Fitting Hyperparameters
</h2><span id='topic+f_control_mactivate'></span>

<h3>Description</h3>

<p>Allows user a single function to tune the mactivate fitting algorithms, <code><a href="#topic+f_fit_gradient_01">f_fit_gradient_01</a></code>, <code><a href="#topic+f_fit_hybrid_01">f_fit_hybrid_01</a></code>, <code><a href="#topic+f_fit_gradient_logistic_01">f_fit_gradient_logistic_01</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_control_mactivate(
param_sensitivity = 10^9, 
bool_free_w = FALSE, 
w0_seed = 0.1, 
max_internal_iter = 500, 
w_col_search = "one", 
bool_headStart = FALSE, 
antifreeze = FALSE, 
ss_stop = 10^(-8), 
escape_rate = 1.004, 
step_size = 1/100, 
Wadj = 1/1, 
force_tries = 0, 
lambda = 0, 
tol = 10^(-8))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_control_mactivate_+3A_param_sensitivity">param_sensitivity</code></td>
<td>

<p>Large positive scalar numeric.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_bool_free_w">bool_free_w</code></td>
<td>

<p>Scalar logical.  Allow values of <code>W</code> to wander outside [0,1]?
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_w0_seed">w0_seed</code></td>
<td>

<p>Scalar numeric.  Usually in [0,1].  Initial value(s) for multiplicative activation layer, <code>W</code>.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_max_internal_iter">max_internal_iter</code></td>
<td>

<p>Scalar non-negative integer. <b>Hybrid only</b>.  How many activation descent passes to make before refitting primary effects.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_w_col_search">w_col_search</code></td>
<td>

<p>Scalar character.  When <code>one</code>, locating <code>W</code> and corresponding coefficients is done (progressively) one column at a time; when <code>all</code>, locating <code>W</code> and corresponding coefficients is done for current column and all previous columns; When <code>alternate</code>, locating <code>W</code> and corresponding coefficients is done (progressively) one column at a time, however, after each column is fitted, an additonal pass is made fitting current column and all previous columns.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_bool_headstart">bool_headStart</code></td>
<td>

<p>Scalar logical. <b>Gradient only</b>. When <code>TRUE</code>, fitting first locates initial primary effects as a &ldquo;head start&rdquo; to the subsequent gradient fitting.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_antifreeze">antifreeze</code></td>
<td>

<p>Scalar logical. <b>Hybrid only</b>. New w/v0.6.5.  When <code>FALSE</code>, backwards compatible.  When <code>TRUE</code>, prevents hanging (non-convergence) that may rarely occur when input space is highly correlated.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_ss_stop">ss_stop</code></td>
<td>

<p>Small positive scalar numeric.  Convergence tolerance.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_escape_rate">escape_rate</code></td>
<td>

<p>Scalar numeric no less than one and likely no greater than, say, 1.01.  Affinity for exiting a column search over <code>W</code>.  E.g., if 1, fitting may take a long time.  If 1.01, search for each column <code>W</code> will terminate relatively quickly.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_step_size">step_size</code></td>
<td>

<p>Positive scalar numeric.  Initial gradient step size (in both gradient and hybrid fitting algorithms) for all parameters.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_wadj">Wadj</code></td>
<td>

<p>Positive scalar numeric.  Control gradient step size (in both gradient and hybrid fitting algorithms) of <code>W</code>.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_force_tries">force_tries</code></td>
<td>

<p>Scalar non-negative integer.  Force a minimum number of fitting recursions.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_lambda">lambda</code></td>
<td>

<p>Scalar numeric.  Ridge regularizer.  The actual diagonal loading imposed upon the precision matrix is equal to <code>lambda</code> times its original diagonal.  A value of <code>0</code> applies no loading; a value of <code>1</code> doubles the diagonal values of the precision matrix.  This is applied to primary effects only.  With gradient MLR fitting, i.e., <code><a href="#topic+f_fit_gradient_01">f_fit_gradient_01</a></code>, this only applies when arg <code>bool_headStart</code> is set to <code>TRUE</code> (otherwise there'd be nothing to regularize).  With hybrid MLR fitting, i.e., <code><a href="#topic+f_fit_hybrid_01">f_fit_hybrid_01</a></code>, this regularization is applied at each LS step (see About vignette).  With logistic fitting, this arg <b>does nothing</b>.  Note that with logistic fitting, we can always add a small amount of white noise to <code>X</code>.
</p>
</td></tr>
<tr><td><code id="f_control_mactivate_+3A_tol">tol</code></td>
<td>

<p>Small positive scalar numeric. <b>Hybrid only</b>. Similar to arg <code>ss_stop</code> above, but controls convergence tolerance after both recursions in hybrid fitting have completed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fitting a mactivate model to data can/will be dramatically affected by these tuning hyperparameters.  
On one extreme, one set of hyperparameters may result in the fitting algorithm fruitlessly exiting almost immediately.  Another set of hyperparameters may send the fitting algorithm to run and run for hours.
While an ideal hyperparameterization will expeditiously fit the data.
</p>


<h3>Value</h3>

<p>Named list to be passed to <code>mact_control</code> arg in fitting functions.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+f_fit_gradient_01">f_fit_gradient_01</a></code>, <code><a href="#topic+f_fit_hybrid_01">f_fit_hybrid_01</a></code>, <code><a href="#topic+f_fit_gradient_logistic_01">f_fit_gradient_logistic_01</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


library(mactivate)

set.seed(777)

d &lt;- 20
N &lt;- 50000

X &lt;- matrix(rnorm(N*d, 0, 1), N, d)

colnames(X) &lt;- paste0("x", I(1:d))

############# primary effect slopes
b &lt;- rep_len( c(-1, 1), d )


ystar &lt;-
X %*% b +
1 * (X[ , 1]) * (X[ , 2]) * (X[ , 3]) -
1 * (X[ , 2]) * (X[ , 3]) * (X[ , 4]) * (X[ , 5])

Xall &lt;- X

errs &lt;- rnorm(N, 0, 1)
errs &lt;- 3 * (errs - mean(errs)) / sd(errs)

sd(errs)

y &lt;- ystar + errs ### response

yall &lt;- y
Nall &lt;- N



############# hybrid example


### this control setting will exit too quickly
### compare this with example below

xcmact &lt;-
f_control_mactivate(
param_sensitivity = 10^5,
w0_seed           = 0.1,
max_internal_iter = 500,
w_col_search      = "one",
ss_stop           = 10^(-5),
escape_rate       = 1.01,
Wadj              = 1/1,
lambda            = 1/1000,
tol               = 10^(-5)
)


m_tot &lt;- 4

Uall &lt;- Xall

xxnow &lt;- Sys.time()

xxls_out &lt;-
f_fit_hybrid_01(
X = Xall,
y = yall,
m_tot = m_tot,
U = Uall,
m_start = 1,
mact_control = xcmact,
verbosity = 1
)

cat( difftime(Sys.time(), xxnow, units="mins"), "\n" )

yhatG &lt;- predict(object=xxls_out, X0=Xall, U0=Uall, mcols=m_tot )

sqrt( mean( (yall  -  yhatG)^2 ) )





####################### this control setting should fit
####################### (will take a few minutes)

xcmact &lt;-
f_control_mactivate(
param_sensitivity = 10^10, ### make more sensitive
w0_seed           = 0.1,
max_internal_iter = 500,
w_col_search      = "one",
ss_stop           = 10^(-14), ### make stopping insensitive
escape_rate       = 1.001, #### discourage quitting descent
Wadj              = 1/1,
lambda            = 1/10000,
tol               = 10^(-14) ### make tolerance very small
)


m_tot &lt;- 4

Uall &lt;- Xall

xxnow &lt;- Sys.time()

xxls_out &lt;-
f_fit_hybrid_01(
X = Xall,
y = yall,
m_tot = m_tot,
U = Uall,
m_start = 1,
mact_control = xcmact,
verbosity = 1
)

cat( difftime(Sys.time(), xxnow, units="mins"), "\n" )

yhatG &lt;- predict(object=xxls_out, X0=Xall, U0=Uall, mcols=m_tot )

sqrt( mean( (yall  -  yhatG)^2 ) )


xxls_out

Xstar &lt;- f_mactivate(U=Uall, W=xxls_out[[ m_tot+1 ]][[ "What" ]])
colnames(Xstar) &lt;- paste0("xstar_", seq(1, m_tot))
Xall &lt;- cbind(Xall, Xstar)

xlm &lt;- lm(yall~Xall)
summary(xlm)



</code></pre>

<hr>
<h2 id='f_dmss_dW'>
Calculate Derivative of Cost Function wrt W
</h2><span id='topic+f_dmss_dW'></span>

<h3>Description</h3>

<p>Calculate the first derivative of objective function with respect to W, given data and requisite model parameter values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_dmss_dW(U, Xstar, W, yerrs, cc)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_dmss_dW_+3A_u">U</code></td>
<td>

<p>Numeric matrix, <code>N</code> x <code>d_u</code> of activation inputs.
</p>
</td></tr>
<tr><td><code id="f_dmss_dW_+3A_xstar">Xstar</code></td>
<td>

<p>Numeric matrix, <code>N</code> x <code>m</code>.  The &ldquo;new&rdquo; polynomial inputs created by applying the activation layer, <code>W</code>, to <code>U</code>.  Accomplished computationally with fun <code><a href="#topic+f_mactivate">f_mactivate</a></code>.
</p>
</td></tr>
<tr><td><code id="f_dmss_dW_+3A_w">W</code></td>
<td>

<p>Numeric matrix, <code>d_u</code> x <code>m</code>, the multiplicative activation layer.
</p>
</td></tr>
<tr><td><code id="f_dmss_dW_+3A_yerrs">yerrs</code></td>
<td>

<p>Numeric vector of length <code>N</code>.  <code>y</code> minus <code>yhat</code>.
</p>
</td></tr>
<tr><td><code id="f_dmss_dW_+3A_cc">cc</code></td>
<td>

<p>Numeric vector of length <code>m</code>.  Coefficients for <code>Xstar</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is really no need for user to call this function directly; this function is called by the fitting functions in this library.
<b>Important.</b> Computationally there are (at least) two ways to solve this derivative, one is O(Nd), the other is O(Nd^2) (d is the number of columns in <code>U</code>).  This function uses the first, computationally less expensive method.  It is not an approximation; the simplification occurs simply by dividing out the appropriate partial term rather than taking the full product of terms across <code>U</code>.  This has a very important implication of which we must be aware: <b>zeros in <code>U</code> may result in division by zero!</b>  This function will handle the errors, but the ultimate consequence of zeros in <code>U</code> is that the derivative returned by this function may not be accurate.  We should eliminate zeros in <code>U</code>.  Standardizing <code>U</code> is one good solution.  If zeros are only present because of &ldquo;one-hot&rdquo; indicators (dummies), another possible solution is to substitute -1 for 0 (actually not a bad practice anyway).
</p>


<h3>Value</h3>

<p>Numeric matrix, <code>d_u</code> x <code>m</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+f_mactivate">f_mactivate</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#######
</code></pre>

<hr>
<h2 id='f_fit_gradient_01'>
Fit Multivariate Regression Model with mactivate Using Gradient Descent
</h2><span id='topic+f_fit_gradient_01'></span>

<h3>Description</h3>

<p>Use simple gradient descent to locate model parameters, i.e., primary effects, multiplicative effects, and activation parameters, <code>W</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_fit_gradient_01(
X, 
y, 
m_tot, 
U = NULL, 
m_start = 1, 
mact_control = f_control_mactivate(), 
verbosity = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_fit_gradient_01_+3A_x">X</code></td>
<td>

<p>Numerical matrix, <code>N</code> x <code>d</code> of model inputs.  Do not include intercept term.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_01_+3A_y">y</code></td>
<td>

<p>Numerical vector of length <code>N</code>.  Model response, or output.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_01_+3A_m_tot">m_tot</code></td>
<td>

<p>Scalar non-negative integer.  Total number of columns of activation layer, <code>W</code>, over which to fit.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_01_+3A_u">U</code></td>
<td>

<p>Numerical matrix, <code>N</code> x <code>d_u</code> of model inputs to send to the activation layer, <code>W</code>.  The default, <code>NULL</code>, instructs this function to simply use arg <code>X</code>.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_01_+3A_m_start">m_start</code></td>
<td>

<p>Currently not used.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_01_+3A_mact_control">mact_control</code></td>
<td>

<p>Named list of class <code>control_mactivate_obj</code> as created by fun <code><a href="#topic+f_control_mactivate">f_control_mactivate</a></code> &mdash; fitting hyperparameters.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_01_+3A_verbosity">verbosity</code></td>
<td>

<p>Scalar integer.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Please make sure to read Details in <code><a href="#topic+f_dmss_dW">f_dmss_dW</a></code> help page before using this function.
</p>


<h3>Value</h3>

<p>An unnamed list of class <code>mactivate_fit_gradient_01</code> of length <code>m_tot + 1</code>.  Each node is a named list containing fitted parameter estimates.  The first top-level node of this object contains parameter estimates when fitting &lsquo;primary effects&rsquo; only (<code>W</code> has no columns), the second, parameter estimates for fitting with 1 column of <code>W</code>, and so on.
</p>


<h3>See Also</h3>

<p>Essentially equivalent to, but likely slower than: <code><a href="#topic+f_fit_hybrid_01">f_fit_hybrid_01</a></code>.  See <code><a href="#topic+f_fit_gradient_logistic_01">f_fit_gradient_logistic_01</a></code> for logistic data (binomial response).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xxnow &lt;- Sys.time()

library(mactivate)

set.seed(777)


d &lt;- 4
N &lt;- 2000

X &lt;- matrix(rnorm(N*d, 0, 1), N, d) ####

colnames(X) &lt;- paste0("x", I(1:d))

############# primary effects
b &lt;- rep_len( c(-1/2, 1/2), d )



###########

xxA &lt;- (X[ , 1]+1/3) * (X[ , 2]-1/3)
#xxA &lt;- (X[ , 1]+0/3) * (X[ , 2]-0/3)


ystar &lt;-
X %*% b +
2 * xxA


m_tot &lt;- 4
#############





xs2 &lt;- "y ~ . "


xtrue_formula &lt;- eval(parse(text=xs2))

xnoint_formula &lt;- eval(parse(text="y ~ . - xxA"))



yerrs &lt;- rnorm(N, 0, 3)

y &lt;- ystar + yerrs

## y &lt;- (y - mean(y)) / sd(y)


########## standardize X
Xall &lt;- t( ( t(X) - apply(X, 2, mean) ) / apply(X, 2, sd) )
yall &lt;- y
Nall &lt;- N


####### fold index
xxfoldNumber &lt;- rep_len(1:2, N)

ufolds &lt;- sort(unique(xxfoldNumber)) ; ufolds


############### predict
############### predict


dfx &lt;- data.frame("y"=yall, Xall, xxA)

tail(dfx)



################### incorrectly fit LM: no interactions

xlm &lt;- lm(xnoint_formula , data=dfx)
summary(xlm)
yhat &lt;- predict(xlm, newdata=dfx)
sqrt( mean( (yall - yhat)^2 ) )



################### correctly fit LM
xlm &lt;- lm(xtrue_formula, data=dfx)
summary(xlm)
yhat &lt;- predict(xlm, newdata=dfx)
sqrt( mean( (yall - yhat)^2 ) )





################ fit using gradient m-activation
######

m_tot &lt;- 4


xcmact_gradient &lt;-
f_control_mactivate(
param_sensitivity = 10^11,
bool_free_w       = TRUE,
w0_seed           = 0.05,
w_col_search      = "alternate",
bool_headStart    = TRUE,
ss_stop           = 10^(-12), ###
escape_rate       = 1.02,  #### 1.0002,
Wadj              = 1/1,
force_tries       = 0,
lambda            = 0
)

#### Fit

Uall &lt;- Xall

head(Uall)

xthis_fold &lt;- ufolds[ 1 ]

xndx_test &lt;- which( xxfoldNumber %in% xthis_fold )
xndx_train &lt;- setdiff( 1:Nall, xndx_test )

X_train &lt;- Xall[ xndx_train, , drop=FALSE ]
y_train &lt;- yall[ xndx_train ]
U_train &lt;- Uall[ xndx_train, , drop=FALSE ]


xxls_out &lt;-
f_fit_gradient_01(
X = X_train,
y = y_train,
m_tot = m_tot,
U = U_train,
m_start = 1,
mact_control = xcmact_gradient,
verbosity = 0
)



######### check test error

U_test &lt;- Uall[ xndx_test, , drop=FALSE ]
X_test &lt;- Xall[ xndx_test, , drop=FALSE ]
y_test &lt;- yall[ xndx_test ]


yhatTT &lt;- matrix(NA, length(xndx_test), m_tot+1)

for(iimm in 0:m_tot) {
    yhat_fold &lt;- predict(object=xxls_out, X0=X_test, U0=U_test, mcols=iimm )
    yhatTT[ , iimm + 1 ] &lt;- yhat_fold
}

errs_by_m &lt;- NULL
for(iimm in 1:ncol(yhatTT)) {
    yhatX &lt;- yhatTT[ , iimm]
    errs_by_m[ iimm ] &lt;- sqrt(mean( (y_test - yhatX)^2 ))
    cat(iimm, "::", errs_by_m[ iimm ])
}


##### plot test RMSE vs m

plot(0:(length(errs_by_m)-1), errs_by_m, type="l", xlab="m", ylab="RMSE Cost")





##################
xtrue_formula_use &lt;- xtrue_formula


xlm &lt;- lm(xnoint_formula , data=dfx[ xndx_train, ])
yhat &lt;- predict(xlm, newdata=dfx[ xndx_test, ])
cat("\n\n", "No interaction model RMSE:", sqrt( mean( (y_test - yhat)^2 ) ), "\n")


xlm &lt;- lm(xtrue_formula_use , data=dfx[ xndx_train, ])
yhat &lt;- predict(xlm, newdata=dfx[ xndx_test, ])
cat("\n\n", "'true' model RMSE:", sqrt( mean( (y_test - yhat)^2 ) ), "\n")


cat( "Runtime:", difftime(Sys.time(), xxnow, units="secs"), "\n" )

</code></pre>

<hr>
<h2 id='f_fit_gradient_logistic_01'>
Fit Logistic Multivariate Regression Model with mactivate Using Gradient Descent
</h2><span id='topic+f_fit_gradient_logistic_01'></span>

<h3>Description</h3>

<p>Use simple gradient descent to locate logistic model parameters, i.e., primary effects, multiplicative effects, and activation parameters, <code>W</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_fit_gradient_logistic_01(
X, 
y, 
m_tot, 
U = NULL, 
m_start = 1, 
mact_control = f_control_mactivate(), 
verbosity = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_fit_gradient_logistic_01_+3A_x">X</code></td>
<td>

<p>Numerical matrix, <code>N</code> x <code>d</code> of model inputs.  Do not include intercept term.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_logistic_01_+3A_y">y</code></td>
<td>

<p>Integer vector of length <code>N</code>, elements in <code>{0, 1}</code>.  Binomial model response, or output.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_logistic_01_+3A_m_tot">m_tot</code></td>
<td>

<p>Scalar non-negative integer.  Total number of columns of activation layer, <code>W</code>, over which to fit.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_logistic_01_+3A_u">U</code></td>
<td>

<p>Numerical matrix, <code>N</code> x <code>d_u</code> of model inputs to send to the activation layer, <code>W</code>.  The default, <code>NULL</code>, instructs this function to simply use arg <code>X</code>.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_logistic_01_+3A_m_start">m_start</code></td>
<td>

<p>Currently not used.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_logistic_01_+3A_mact_control">mact_control</code></td>
<td>

<p>Named list of class <code>control_mactivate_obj</code> as created by fun <code><a href="#topic+f_control_mactivate">f_control_mactivate</a></code> &mdash; fitting hyperparameters.
</p>
</td></tr>
<tr><td><code id="f_fit_gradient_logistic_01_+3A_verbosity">verbosity</code></td>
<td>

<p>Scalar integer.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Please make sure to read Details in <code><a href="#topic+f_dmss_dW">f_dmss_dW</a></code> help page before using this function.
</p>


<h3>Value</h3>

<p>An unnamed list of class <code>mactivate_fit_gradient_logistic_01</code> of length <code>m_tot + 1</code>.  Each node is a named list containing fitted parameter estimates.  The first top-level node of this object contains parameter estimates when fitting &lsquo;primary effects&rsquo; only (<code>W</code> has no columns), the second, parameter estimates for fitting with 1 column of <code>W</code>, and so on.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+f_fit_gradient_01">f_fit_gradient_01</a></code> or <code><a href="#topic+f_fit_gradient_logistic_01">f_fit_gradient_logistic_01</a></code> for MLR data (numerical response).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xxnow &lt;- Sys.time()

library(mactivate)

set.seed(777)


d &lt;- 4
N &lt;- 2400

X &lt;- matrix(rnorm(N*d, 0, 1), N, d) ####

colnames(X) &lt;- paste0("x", I(1:d))

############# primary effects
b &lt;- rep_len( c(-1/2, 1/2), d )


xxA &lt;- (X[ , 1]+1/3) * (X[ , 2]-1/3)
xxB &lt;- (X[ , 1]+0/3) * (X[ , 2]-0/3) * (X[ , 4]-1/3)


ystar &lt;-
X %*% b +
2 * xxA -
1 * xxB

xs2 &lt;- "y ~ . "

xtrue_formula &lt;- eval(parse(text=xs2))

xnoint_formula &lt;- eval(parse(text="y ~ . - xxA - xxB"))


ysigmoid &lt;- 1 / (1 + exp(-ystar))

range(ysigmoid)

y &lt;- rbinom(size=1, n=N ,prob=ysigmoid)


Nall &lt;- N

cov(X)

yall &lt;- y

Xall &lt;- X

### Xall &lt;- X + rnorm(prod(dim(X)), 0, 1/10000) ### add a little noise -- optional

sd(y)


dfx &lt;- data.frame("y"=yall, Xall, xxA, xxB)

tail(dfx)



################### incorrectly fit LM: no interactions
xglm &lt;- glm(xnoint_formula , data=dfx, family=binomial(link="logit"))
summary(xglm)
yhat &lt;- predict(xglm, newdata=dfx, type="response")
mean( f_logit_cost(y=yall, yhat=yhat) )


####### known true
xglm &lt;- glm(xtrue_formula , data=dfx, family=binomial(link="logit"))
summary(xglm)
yhat &lt;- predict(xglm, newdata=dfx, type="response")
mean( f_logit_cost(y=yall, yhat=yhat) )



xxfoldNumber &lt;- rep_len( 1:4, Nall )

ufolds &lt;- sort(unique(xxfoldNumber))

######################

xthis_fold &lt;- ufolds[ 1 ]

xndx_test &lt;- which( xxfoldNumber %in% xthis_fold )

xndx_train &lt;- setdiff( 1:Nall, xndx_test )

##################

X_train &lt;- Xall[ xndx_train, , drop=FALSE ]

X_test &lt;- Xall[ xndx_test, , drop=FALSE ]

y_train &lt;- yall[ xndx_train ]

y_test &lt;- yall[ xndx_test ]



###################

m_tot &lt;- 4

xcmact_gradient &lt;-
f_control_mactivate(
param_sensitivity = 10^11,
bool_free_w       = FALSE,
w0_seed           = 0.05,
#w_col_search      = "alternate",
w_col_search      = "one",
bool_headStart    = TRUE,
ss_stop           = 10^(-12), ### very small
escape_rate       = 1.02,
step_size         = 1,
Wadj              = 1/1,
force_tries       = 0,
lambda            = 1/1 #### does nothing here
)


Uall &lt;- Xall


X_train &lt;- Xall[ xndx_train, , drop=FALSE ]
y_train &lt;- yall[ xndx_train ]


xxls_out &lt;-
f_fit_gradient_logistic_01(
X = X_train,
y = y_train,
m_tot = m_tot,
U = X_train,
m_start = 1,
mact_control = xcmact_gradient,
verbosity = 0
)


######### check test error

U_test &lt;- Xall[ xndx_test, , drop=FALSE ]
X_test &lt;- Xall[ xndx_test, , drop=FALSE ]
y_test &lt;- yall[ xndx_test ]


yhatTT &lt;- matrix(NA, length(xndx_test), m_tot+1)

for(iimm in 0:m_tot) {
    yhat_fold &lt;- predict(object=xxls_out, X0=X_test, U0=U_test, mcols=iimm )
    yhatTT[ , iimm + 1 ] &lt;- yhat_fold[[ "p0hat" ]]
}

errs_by_m &lt;- NULL
for(iimm in 1:ncol(yhatTT)) {
    yhatX &lt;- yhatTT[ , iimm]
    errs_by_m[ iimm ] &lt;- mean( f_logit_cost(y=y_test, yhat=yhatX) )
    cat(iimm, "::", errs_by_m[ iimm ])
}

##### plot test Logit vs m

plot(0:(length(errs_by_m)-1), errs_by_m, type="l", xlab="m", ylab="Logit Cost")





################## test off 'correct' model
xtrue_formula_use &lt;- xtrue_formula



xglm &lt;- glm(xnoint_formula , data=dfx[ xndx_train, ], family=binomial(link="logit"))
yhat &lt;- predict(xglm, newdata=dfx[ xndx_test, ], type="response")
cat("\n\n", "No interaction model logit:", mean( f_logit_cost(y=y_test, yhat=yhat) ), "\n")


xglm &lt;- glm(xtrue_formula_use , data=dfx[ xndx_train, ], family=binomial(link="logit"))
yhat &lt;- predict(xglm, newdata=dfx[ xndx_test, ], type="response")
cat("\n\n", "'true' model logit:", mean( f_logit_cost(y=y_test, yhat=yhat) ) , "\n")


cat( "Runtime:", difftime(Sys.time(), xxnow, units="secs"), "\n" )

</code></pre>

<hr>
<h2 id='f_fit_hybrid_01'>
Fit Multivariate Regression Model with mactivate Using Hybrid Method
</h2><span id='topic+f_fit_hybrid_01'></span>

<h3>Description</h3>

<p>Use hybrid algorithm (essentially a flavor of EM) to locate model parameters, i.e., primary effects, multiplicative effects, and activation parameters, <code>W</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_fit_hybrid_01(
X, 
y, 
m_tot, 
U = NULL, 
m_start = 1, 
mact_control = f_control_mactivate(), 
verbosity = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_fit_hybrid_01_+3A_x">X</code></td>
<td>

<p>Numerical matrix, <code>N</code> x <code>d</code> of model inputs.  Do not include intercept term.
</p>
</td></tr>
<tr><td><code id="f_fit_hybrid_01_+3A_y">y</code></td>
<td>

<p>Numerical vector of length <code>N</code>.  Model response, or output.
</p>
</td></tr>
<tr><td><code id="f_fit_hybrid_01_+3A_m_tot">m_tot</code></td>
<td>

<p>Scalar non-negative integer.  Total number of columns of activation layer, <code>W</code>, over which to fit.
</p>
</td></tr>
<tr><td><code id="f_fit_hybrid_01_+3A_u">U</code></td>
<td>

<p>Numerical matrix, <code>N</code> x <code>d_u</code> of model inputs to send to the activation layer, <code>W</code>.  The default, <code>NULL</code>, instructs this function to simply use arg <code>X</code>.
</p>
</td></tr>
<tr><td><code id="f_fit_hybrid_01_+3A_m_start">m_start</code></td>
<td>

<p>Currently not used.
</p>
</td></tr>
<tr><td><code id="f_fit_hybrid_01_+3A_mact_control">mact_control</code></td>
<td>

<p>Named list of class <code>control_mactivate_obj</code> as created by fun <code><a href="#topic+f_control_mactivate">f_control_mactivate</a></code> &mdash; fitting hyperparameters.
</p>
</td></tr>
<tr><td><code id="f_fit_hybrid_01_+3A_verbosity">verbosity</code></td>
<td>

<p>Scalar integer.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Please make sure to read Details in <code><a href="#topic+f_dmss_dW">f_dmss_dW</a></code> help page before using this function.
</p>


<h3>Value</h3>

<p>An unnamed list of class <code>mactivate_fit_hybrid_01</code> of length <code>m_tot + 1</code>.  Each node is a named list containing fitted parameter estimates.  The first top-level node of this object contains parameter estimates when fitting &lsquo;primary effects&rsquo; only (<code>W</code> has no columns), the second, parameter estimates for fitting with 1 column of <code>W</code>, and so on.
</p>


<h3>See Also</h3>

<p>Essentially equivalent to, but likely faster than: <code><a href="#topic+f_fit_gradient_01">f_fit_gradient_01</a></code>.  See <code><a href="#topic+f_fit_gradient_logistic_01">f_fit_gradient_logistic_01</a></code> for logistic data (binomial response).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xxnow &lt;- Sys.time()

library(mactivate)

set.seed(777)


d &lt;- 4
N &lt;- 2000

X &lt;- matrix(rnorm(N*d, 0, 1), N, d) ####

colnames(X) &lt;- paste0("x", I(1:d))

############# primary effects
b &lt;- rep_len( c(-1/2, 1/2), d )



###########

xxA &lt;- (X[ , 1]+1/3) * (X[ , 2]-1/3)
#xxA &lt;- (X[ , 1]+0/3) * (X[ , 2]-0/3)


ystar &lt;-
X %*% b +
2 * xxA



#############





xs2 &lt;- "y ~ . "


xtrue_formula &lt;- eval(parse(text=xs2))

xnoint_formula &lt;- eval(parse(text="y ~ . - xxA"))



yerrs &lt;- rnorm(N, 0, 3)

y &lt;- ystar + yerrs

## y &lt;- (y - mean(y)) / sd(y)


########## standardize X
Xall &lt;- t( ( t(X) - apply(X, 2, mean) ) / apply(X, 2, sd) )
yall &lt;- y
Nall &lt;- N


####### fold index
xxfoldNumber &lt;- rep_len(1:2, N)

ufolds &lt;- sort(unique(xxfoldNumber)) ; ufolds


############### predict
############### predict


dfx &lt;- data.frame("y"=yall, Xall, xxA)

tail(dfx)



################### incorrectly fit LM: no interactions
xlm &lt;- lm(xnoint_formula , data=dfx)
summary(xlm)
yhat &lt;- predict(xlm, newdata=dfx)
sqrt( mean( (yall - yhat)^2 ) )



################### correctly fit LM
xlm &lt;- lm(xtrue_formula, data=dfx)
summary(xlm)
yhat &lt;- predict(xlm, newdata=dfx)
sqrt( mean( (yall - yhat)^2 ) )





################ fit using hybrid m-activation
######

m_tot &lt;- 4

xcmact_hybrid &lt;-
f_control_mactivate(
param_sensitivity = 10^12,
bool_free_w       = TRUE,
w0_seed           = 0.1, ### 0.01
w_col_search      = "alternate",
max_internal_iter = 500, #####
ss_stop           = 10^(-11), ###
escape_rate       = 1.02, ### 1.05
Wadj              = 1/1,
force_tries       = 0,
lambda            = 0/10000, ### hybrid only
tol               = 10^(-11) ### hybrid only
)

#### Fit

Uall &lt;- Xall

head(Uall)

xthis_fold &lt;- ufolds[ 1 ]

xndx_test &lt;- which( xxfoldNumber %in% xthis_fold )
xndx_train &lt;- setdiff( 1:Nall, xndx_test )

X_train &lt;- Xall[ xndx_train, , drop=FALSE ]
y_train &lt;- yall[ xndx_train ]
U_train &lt;- Uall[ xndx_train, , drop=FALSE ]


xxls_out &lt;-
f_fit_hybrid_01(
X = X_train,
y = y_train,
m_tot = m_tot,
U = U_train,
m_start = 1,
mact_control = xcmact_hybrid,
verbosity = 1
)



######### check test error

U_test &lt;- Uall[ xndx_test, , drop=FALSE ]
X_test &lt;- Xall[ xndx_test, , drop=FALSE ]
y_test &lt;- yall[ xndx_test ]


yhatTT &lt;- matrix(NA, length(xndx_test), m_tot+1)

for(iimm in 0:m_tot) {
    yhat_fold &lt;- predict(object=xxls_out, X0=X_test, U0=U_test, mcols=iimm )
    yhatTT[ , iimm + 1 ] &lt;- yhat_fold
}

errs_by_m &lt;- NULL
for(iimm in 1:ncol(yhatTT)) {
    yhatX &lt;- yhatTT[ , iimm]
    errs_by_m[ iimm ] &lt;- sqrt(mean( (y_test - yhatX)^2 ))
    cat(iimm, "::", errs_by_m[ iimm ])
}



plot(0:(length(errs_by_m)-1), errs_by_m, type="l", xlab="m", ylab="RMSE Cost")






##################
xtrue_formula_use &lt;- xtrue_formula


xlm &lt;- lm(xnoint_formula , data=dfx[ xndx_train, ])
yhat &lt;- predict(xlm, newdata=dfx[ xndx_test, ])
cat("\n\n", "No interaction model RMSE:", sqrt( mean( (y_test - yhat)^2 ) ), "\n")


xlm &lt;- lm(xtrue_formula_use , data=dfx[ xndx_train, ])
yhat &lt;- predict(xlm, newdata=dfx[ xndx_test, ])
cat("\n\n", "'true' model RMSE:", sqrt( mean( (y_test - yhat)^2 ) ), "\n")


cat( "Runtime:", difftime(Sys.time(), xxnow, units="secs"), "\n" )

</code></pre>

<hr>
<h2 id='f_logit_cost'>
Logistic Cost
</h2><span id='topic+f_logit_cost'></span>

<h3>Description</h3>

<p>Calculate the logistic cost of probability predictions of a dichotomous outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_logit_cost(y, yhat)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_logit_cost_+3A_y">y</code></td>
<td>

<p>Numeric vector.  The outcome vector.  Must be in {0, 1}.
</p>
</td></tr>
<tr><td><code id="f_logit_cost_+3A_yhat">yhat</code></td>
<td>

<p>Numeric vector.  Prediction vector.  Should be in (0, 1) &ndash; the open unit interval.  In an inferential setting, one should probably never make a prediction of zero or one; however, values of zero or one are allowed, provided they are &ldquo;correct&rdquo;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is included in this library as a convenience.  
</p>


<h3>Value</h3>

<p>A numeric vector of length equal to <code>y</code> and <code>yhat</code>.  The logistic cost associated with each corresponding prediction.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+f_fit_gradient_logistic_01">f_fit_gradient_logistic_01</a></code>, <code><a href="#topic+predict.mactivate_fit_gradient_logistic_01">predict.mactivate_fit_gradient_logistic_01</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(0, 0, 1, 1)
yhat &lt;- rep(1/2, length(y))

mean( f_logit_cost(y=y, yhat=yhat) )
</code></pre>

<hr>
<h2 id='f_mactivate'>
Map Activation Layer and Inputs to Polynomial Model Inputs
</h2><span id='topic+f_mactivate'></span>

<h3>Description</h3>

<p>Passes activation inputs, <code>U</code> into activation layer, <code>W</code>, to obtain new polynomial model inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_mactivate(U, W)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_mactivate_+3A_u">U</code></td>
<td>

<p>Numeric matrix, <code>N</code> x <code>d_u</code> of activation inputs.
</p>
</td></tr>
<tr><td><code id="f_mactivate_+3A_w">W</code></td>
<td>

<p>Numeric matrix, <code>d_u</code> x <code>m</code>, the multiplicative activation layer.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the multiplicative activations; it maps selected inputs, <code>U</code>, back into the input space using the m-activation layer(s).  In practice, the arg <code>W</code>, will be a fitted value, as created by the fitting functions.
</p>


<h3>Value</h3>

<p>Numeric matrix, <code>N</code> x <code>m</code>.  Referred to as <code>Xstar</code> elsewhere in this documentation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


library(mactivate)

set.seed(777)


d &lt;- 7
N &lt;- 15000

X &lt;- matrix(rnorm(N*d, 0, 1), N, d) ####

colnames(X) &lt;- paste0("x", I(1:d))

############# primary effects
b &lt;- rep_len( c(-1/4, 1/4), d )



###########

xxA &lt;- (X[ , 1]+1/3) * (X[ , 1]-1/3) * (X[ , 3]+1/3)
xxB &lt;- (X[ , 2]+0) * (X[ , 2]+1/3) * (X[ , 3]-0) * (X[ , 3]-1/3)
xxC &lt;- (X[ , 3]+1/3) * (X[ , 3]-1/3)

ystar &lt;-
X %*% b +
1/3 * xxA -
1/2 * xxB +
1/3 * xxC


#############

xs2 &lt;- "y ~ . "

xtrue_formula &lt;- eval(parse(text=xs2))

xnoint_formula &lt;- eval(parse(text="y ~ . - xxA - xxB - xxC"))



yerrs &lt;- rnorm(N, 0, 3)

y &lt;- ystar + yerrs

########## standardize X
Xall &lt;- t( ( t(X) - apply(X, 2, mean) ) / apply(X, 2, sd) )
yall &lt;- y
Nall &lt;- N


####### fold index
xxfoldNumber &lt;- rep_len(1:2, N)

ufolds &lt;- sort(unique(xxfoldNumber)) ; ufolds


############### predict
############### predict


dfx &lt;- data.frame("y"=yall, Xall, xxA, xxB, xxC)

tail(dfx)



################### incorrectly fit LM: no interactions

xlm &lt;- lm(xnoint_formula , data=dfx)
summary(xlm)
yhat &lt;- predict(xlm, newdata=dfx)
sqrt( mean( (yall - yhat)^2 ) )



################### correctly fit LM
xlm &lt;- lm(xtrue_formula, data=dfx)
summary(xlm)
yhat &lt;- predict(xlm, newdata=dfx)
sqrt( mean( (yall - yhat)^2 ) )





################ fit using hybrid m-activation
###### takes about 2 minutes

xcmact_hybrid &lt;-
f_control_mactivate(
param_sensitivity = 10^12,
bool_free_w       = TRUE,
w0_seed           = 0.1,
w_col_search      = "alternate",
max_internal_iter = 500, #####
ss_stop           = 10^(-14), ###
escape_rate       = 1.005,
Wadj              = 1/1,
force_tries       = 0,
lambda            = 0/10000, ###
tol               = 10^(-14) ###
)




#### Fit

m_tot &lt;- 7

Uall &lt;- cbind(Xall, Xall)
colnames(Uall) &lt;- paste0(rep(c("a_", "b_"), each=d), colnames(Uall))

head(Uall)

xthis_fold &lt;- ufolds[ 1 ]


xndx_test &lt;- which( xxfoldNumber %in% xthis_fold )
xndx_train &lt;- setdiff( 1:Nall, xndx_test )

X_train &lt;- Xall[ xndx_train, , drop=FALSE ]
y_train &lt;- yall[ xndx_train ]
U_train &lt;- Uall[ xndx_train, , drop=FALSE ]

xxnow &lt;- Sys.time()
xxls_out &lt;-
f_fit_hybrid_01(
X = X_train,
y = y_train,
m_tot = m_tot,
U = U_train,
m_start = 1,
mact_control = xcmact_hybrid,
verbosity = 1
)
cat( difftime(Sys.time(), xxnow, units="mins"), "\n" )



######### check test error

U_test &lt;- Uall[ xndx_test, , drop=FALSE ]
X_test &lt;- Xall[ xndx_test, , drop=FALSE ]
y_test &lt;- yall[ xndx_test ]


yhatTT &lt;- matrix(NA, length(xndx_test), m_tot+1)

for(iimm in 0:m_tot) {
    yhat_fold &lt;- predict(object=xxls_out, X0=X_test, U0=U_test, mcols=iimm )
    yhatTT[ , iimm + 1 ] &lt;- yhat_fold
}

errs_by_m &lt;- NULL
for(iimm in 1:ncol(yhatTT)) {
    yhatX &lt;- yhatTT[ , iimm]
    errs_by_m[ iimm ] &lt;- sqrt(mean( (y_test - yhatX)^2 ))
    cat(iimm, "::", errs_by_m[ iimm ])
}

plot(0:(length(errs_by_m)-1), errs_by_m, type="l", xlab="m", ylab="RMSE Cost")




##################

xthis_fold &lt;- ufolds[ 1 ]

xndx_test &lt;- which( xxfoldNumber %in% xthis_fold )
xndx_train &lt;- setdiff( 1:Nall, xndx_test )

xlm &lt;- lm(xtrue_formula , data=dfx[ xndx_train, ])
yhat &lt;- predict(xlm, newdata=dfx[ xndx_test, ])

sqrt( mean( (y_test - yhat)^2 ) )


################ hatXstar

X_test &lt;- Xall[ xndx_test, ]
y_test &lt;- yall[ xndx_test ]

Xstar_test &lt;- f_mactivate(U=U_test, W=xxls_out[[ length(xxls_out) ]][[ "What" ]])
Xi &lt;- cbind(X_test, Xstar_test)
xlm &lt;- lm(y_test ~ Xi)

sumxlm &lt;- summary(xlm)
print(sumxlm)

xcoefs &lt;- sumxlm$coefficients
xcoefs &lt;- xcoefs[ (2+d):nrow(xcoefs), ] ; xcoefs

xndox_cu &lt;- which( abs(xcoefs[ , "t value"]) &gt; 3 ) ; xndox_cu


bWhat &lt;- xxls_out[[ length(xxls_out) ]][[ "What" ]][ ,  xndox_cu ]
bWhat

wwmag &lt;- apply(bWhat, 1, function(x) { return(sum(abs(x)))} ) ; wwmag

plot(wwmag, type="h", lwd=4,
ylim=c(0, max(wwmag)),
main="W Coefficient Total Magnitute vs Input Term",
xlab="Column of U",
ylab="Sum of magnitudes in fitted W",
cex.lab=1.3
)





</code></pre>

<hr>
<h2 id='predict.mactivate_fit_gradient_01'>
Predict from Fitted Gradient Model
</h2><span id='topic+predict.mactivate_fit_gradient_01'></span>

<h3>Description</h3>

<p>Predict using fitted model returned by <code><a href="#topic+f_fit_gradient_01">f_fit_gradient_01</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mactivate_fit_gradient_01'
predict(object, X0, U0=NULL, mcols, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.mactivate_fit_gradient_01_+3A_object">object</code></td>
<td>

<p>A list of class 'mactivate_fit_gradient_01' as returned by f_fit_gradient_01().
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_gradient_01_+3A_x0">X0</code></td>
<td>

<p>Numeric matrix, <code>N</code> x <code>d</code>.  Model &lsquo;primary effect&rsquo; inputs.
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_gradient_01_+3A_u0">U0</code></td>
<td>

<p>Numeric matrix with <code>N</code> rows.  Inputs to pass to activation layer.
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_gradient_01_+3A_mcols">mcols</code></td>
<td>

<p>Scalar non-negative integer specifying which first columns of <code>W</code> to use.
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_gradient_01_+3A_...">...</code></td>
<td>

<p>Nothing else is required for this extension of the predict() function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>U0</code> is not provided, <code>X0</code> will be passed to activation layer.
</p>


<h3>Value</h3>

<p><code>yhat</code>.  Numeric vector of length <code>N</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+f_fit_gradient_01">f_fit_gradient_01</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>####### Please see examples in the fitting functions
</code></pre>

<hr>
<h2 id='predict.mactivate_fit_gradient_logistic_01'>
Predict from Fitted Gradient Logistic Model
</h2><span id='topic+predict.mactivate_fit_gradient_logistic_01'></span>

<h3>Description</h3>

<p>Predict using fitted model returned by <code><a href="#topic+f_fit_gradient_logistic_01">f_fit_gradient_logistic_01</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mactivate_fit_gradient_logistic_01'
predict(object, X0, U0=NULL, mcols, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.mactivate_fit_gradient_logistic_01_+3A_object">object</code></td>
<td>

<p>A list of class 'mactivate_fit_gradient_logistic_01' as returned by f_fit_gradient_logistic_01().
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_gradient_logistic_01_+3A_x0">X0</code></td>
<td>

<p>Numeric matrix, <code>N</code> x <code>d</code>.  Model &lsquo;primary effect&rsquo; inputs.
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_gradient_logistic_01_+3A_u0">U0</code></td>
<td>

<p>Numeric matrix with <code>N</code> rows.  Inputs to pass to activation layer.
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_gradient_logistic_01_+3A_mcols">mcols</code></td>
<td>

<p>Scalar non-negative integer specifying which first columns of <code>W</code> to use.
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_gradient_logistic_01_+3A_...">...</code></td>
<td>

<p>Nothing else is required for this extension of the predict() function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>U0</code> is not provided, <code>X0</code> will be passed to activation layer.
</p>


<h3>Value</h3>

<p>A named list with 2 elements:
</p>
<table role = "presentation">
<tr><td><code>y0hat</code></td>
<td>
<p>Vector of length <code>N</code>.  Linear predictions</p>
</td></tr>
<tr><td><code>p0hat</code></td>
<td>
<p>Vector of length <code>N</code>.  Probability predictions.  Similar to setting type='response' when predicting from <code>glm</code> logistic fitted model</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+f_fit_gradient_logistic_01">f_fit_gradient_logistic_01</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>####### Please see examples in the fitting functions
</code></pre>

<hr>
<h2 id='predict.mactivate_fit_hybrid_01'>
Predict from Fitted Hybrid Model
</h2><span id='topic+predict.mactivate_fit_hybrid_01'></span>

<h3>Description</h3>

<p>Predict using fitted model returned by <code><a href="#topic+f_fit_hybrid_01">f_fit_hybrid_01</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mactivate_fit_hybrid_01'
predict(object, X0, U0=NULL, mcols, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.mactivate_fit_hybrid_01_+3A_object">object</code></td>
<td>

<p>A list of class 'mactivate_fit_hybrid_01' as returned by <code><a href="#topic+f_fit_hybrid_01">f_fit_hybrid_01</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_hybrid_01_+3A_x0">X0</code></td>
<td>

<p>Numeric matrix, <code>N</code> x <code>d</code>.  Model &lsquo;primary effect&rsquo; inputs.
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_hybrid_01_+3A_u0">U0</code></td>
<td>

<p>Numeric matrix with <code>N</code> rows.  Inputs to pass to activation layer.
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_hybrid_01_+3A_mcols">mcols</code></td>
<td>

<p>Scalar non-negative integer specifying which first columns of <code>W</code> to use.
</p>
</td></tr>
<tr><td><code id="predict.mactivate_fit_hybrid_01_+3A_...">...</code></td>
<td>

<p>Nothing else is required for this extension of the predict() function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>U0</code> is not provided, <code>X0</code> will be passed to activation layer.
</p>


<h3>Value</h3>

<p><code>yhat</code>.  Numeric vector of length <code>N</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+f_fit_hybrid_01">f_fit_hybrid_01</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>####### Please see examples in the fitting functions
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
