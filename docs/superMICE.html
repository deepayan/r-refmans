<!DOCTYPE html><html><head><title>Help for package superMICE</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {superMICE}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#binarySuperLearner'><p>Function to generate imputations using SuperLearner for data with a binary outcome.</p></a></li>
<li><a href='#continuousSuperLearner'><p>Function to generate imputations using SuperLearner for data with a continuous outcome</p></a></li>
<li><a href='#gaussianKernel'><p>Kernel functions used for local imputation</p></a></li>
<li><a href='#jackknifeBandwidthSelection'><p>Jackknife method for selection bandwidth</p></a></li>
<li><a href='#jackknifeVariance'><p>Computes jackknife variance</p></a></li>
<li><a href='#localImputation'><p>Function to generate imputations using non-parametric and semi-parametric local imputation methods.</p></a></li>
<li><a href='#mice.impute.SuperLearner'><p>SuperLearner method for <code>mice</code> package.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>SuperLearner Method for MICE</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Aaron B. Shev</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Aaron B. Shev &lt;abshev@ucdavis.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Adds a Super Learner ensemble model method (using the 'SuperLearner' package)  
    to the 'mice' package. Laqueur, H. S., Shev, A. B., Kagawa, R. M. C. (2021) &lt;<a href="https://doi.org/10.1093%2Faje%2Fkwab271">doi:10.1093/aje/kwab271</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, mice, SuperLearner</td>
</tr>
<tr>
<td>Suggests:</td>
<td>arm, bartMachine, class, e1071, earth, extraTrees, gbm,
glmnet, ipred, KernelKnn, kernlab, LogicReg, MASS, nnet, party,
polspline, randomForest, ranger, rpart, speedglm, spls, xgboost</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-04 18:56:07 UTC; ashev</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-04 20:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='binarySuperLearner'>Function to generate imputations using SuperLearner for data with a binary outcome.</h2><span id='topic+binarySuperLearner'></span>

<h3>Description</h3>

<p>Function to generate imputations using SuperLearner for data with a binary outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binarySuperLearner(y, x, wy, SL.library, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binarySuperLearner_+3A_y">y</code></td>
<td>
<p>Vector of observed values of the variable to be imputed.</p>
</td></tr>
<tr><td><code id="binarySuperLearner_+3A_x">x</code></td>
<td>
<p>Numeric matrix of variables to be used as predictors in SuperLearner methods with rows corresponding to values in Y.</p>
</td></tr>
<tr><td><code id="binarySuperLearner_+3A_wy">wy</code></td>
<td>
<p>Logical vector of length <code>length(y)</code>. A <code>TRUE</code> value indicates
locations in <code>y</code> for which imputations are created.</p>
</td></tr>
<tr><td><code id="binarySuperLearner_+3A_sl.library">SL.library</code></td>
<td>
<p>Either a character vector of prediction algorithms or a list containing character vectors. A list of functions included in the SuperLearner package can be found with SuperLearner::listWrappers().</p>
</td></tr>
<tr><td><code id="binarySuperLearner_+3A_...">...</code></td>
<td>
<p>Further arguments passed to SuperLearner.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Binary Vector of randomly drawn imputed values.
</p>

<hr>
<h2 id='continuousSuperLearner'>Function to generate imputations using SuperLearner for data with a continuous outcome</h2><span id='topic+continuousSuperLearner'></span>

<h3>Description</h3>

<p>Function to generate imputations using SuperLearner for data with a continuous outcome
</p>


<h3>Usage</h3>

<pre><code class='language-R'>continuousSuperLearner(y, x, wy, SL.library, kernel, bw, bw.update, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="continuousSuperLearner_+3A_y">y</code></td>
<td>
<p>Vector of observed and missing/imputed values of the variable to be imputed.</p>
</td></tr>
<tr><td><code id="continuousSuperLearner_+3A_x">x</code></td>
<td>
<p>Numeric matrix of variables to be used as predictors in SuperLearner models
with rows corresponding to observed values of the variable to be imputed and
columns corresponding to individual predictor variables.</p>
</td></tr>
<tr><td><code id="continuousSuperLearner_+3A_wy">wy</code></td>
<td>
<p>Logical vector. A TRUE value indicates locations in <code>y</code> that are
missing or imputed.</p>
</td></tr>
<tr><td><code id="continuousSuperLearner_+3A_sl.library">SL.library</code></td>
<td>
<p>Either a character vector of prediction algorithms or a
list containing character vectors. A list of functions included in the
SuperLearner package can be found with <code>SuperLearner::listWrappers()</code>.</p>
</td></tr>
<tr><td><code id="continuousSuperLearner_+3A_kernel">kernel</code></td>
<td>
<p>one of <code>gaussian</code>, <code>uniform</code>, or <code>triangular</code>.
Specifies the kernel to be used in estimating the distribution around a missing value.</p>
</td></tr>
<tr><td><code id="continuousSuperLearner_+3A_bw">bw</code></td>
<td>
<p><code>NULL</code> or numeric value for bandwidth of kernel function (as standard deviations of the kernel).</p>
</td></tr>
<tr><td><code id="continuousSuperLearner_+3A_bw.update">bw.update</code></td>
<td>
<p>logical indicating whether bandwidths should be computed
every iteration or only on the first iteration.  Default is <code>TRUE</code>,
but <code>FALSE</code> may speed up the run time at the cost of accuracy.</p>
</td></tr>
<tr><td><code id="continuousSuperLearner_+3A_...">...</code></td>
<td>
<p>further arguments passed to <code>SuperLearner()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of randomly drawn imputed values.
</p>

<hr>
<h2 id='gaussianKernel'>Kernel functions used for local imputation</h2><span id='topic+gaussianKernel'></span>

<h3>Description</h3>

<p>Kernel functions used for local imputation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussianKernel(x, xcenter, bw = 1, lambda = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gaussianKernel_+3A_x">x</code></td>
<td>
<p>numeric vector of values to weight.</p>
</td></tr>
<tr><td><code id="gaussianKernel_+3A_xcenter">xcenter</code></td>
<td>
<p>numeric value to center the kernel.</p>
</td></tr>
<tr><td><code id="gaussianKernel_+3A_bw">bw</code></td>
<td>
<p>bandwidth of the kernel.</p>
</td></tr>
<tr><td><code id="gaussianKernel_+3A_lambda">lambda</code></td>
<td>
<p>kernel radius, function of <code>bw</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>kernel values for <code>x</code> centered at <code>xcenter</code>.
</p>

<hr>
<h2 id='jackknifeBandwidthSelection'>Jackknife method for selection bandwidth</h2><span id='topic+jackknifeBandwidthSelection'></span>

<h3>Description</h3>

<p>Jackknife method for selection bandwidth
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jackknifeBandwidthSelection(i, bwGrid, preds, y, delta, kernel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jackknifeBandwidthSelection_+3A_i">i</code></td>
<td>
<p>integer referring to the index of the missing value to be imputed.</p>
</td></tr>
<tr><td><code id="jackknifeBandwidthSelection_+3A_bwgrid">bwGrid</code></td>
<td>
<p>numeric vector of candidate bandwidth values</p>
</td></tr>
<tr><td><code id="jackknifeBandwidthSelection_+3A_preds">preds</code></td>
<td>
<p>numeric vector of predicted values for missing observations</p>
</td></tr>
<tr><td><code id="jackknifeBandwidthSelection_+3A_y">y</code></td>
<td>
<p>numeric vector of length <code>n</code> of observed and imputed values.</p>
</td></tr>
<tr><td><code id="jackknifeBandwidthSelection_+3A_delta">delta</code></td>
<td>
<p>Binary vector of length <code>length(y)</code> indicating missingness.
<code>1</code> where <code>y</code> is observed and <code>0</code> where <code>y</code> is missing.</p>
</td></tr>
<tr><td><code id="jackknifeBandwidthSelection_+3A_kernel">kernel</code></td>
<td>
<p>one of <code>gaussian</code>, <code>uniform</code>, or <code>triangular</code>.
Specifies the kernel to be used in estimating the distribution around a missing value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>bandwidth
</p>

<hr>
<h2 id='jackknifeVariance'>Computes jackknife variance</h2><span id='topic+jackknifeVariance'></span>

<h3>Description</h3>

<p>Computes jackknife variance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jackknifeVariance(j, kernMatrix, delta, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jackknifeVariance_+3A_j">j</code></td>
<td>
<p>integer index for deleted observation in the jackknife procedure.</p>
</td></tr>
<tr><td><code id="jackknifeVariance_+3A_kernmatrix">kernMatrix</code></td>
<td>
<p><code>(n-1)</code> by <code>m</code> matrix of kernel values centered at missing
observation <code>j</code> where <code>n</code> is the total number of observations and <code>m</code> is the
number of candidate bandwidths.</p>
</td></tr>
<tr><td><code id="jackknifeVariance_+3A_delta">delta</code></td>
<td>
<p>Binary vector of length <code>n</code> indicating missingness.
<code>1</code> where <code>y</code> is observed and <code>0</code> where <code>y</code> is missing.</p>
</td></tr>
<tr><td><code id="jackknifeVariance_+3A_y">y</code></td>
<td>
<p>numeric vector of length <code>n</code> of observed values and imputed values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a single numeric value for the estimate of the jackknife variance.
</p>

<hr>
<h2 id='localImputation'>Function to generate imputations using non-parametric and semi-parametric local imputation methods.</h2><span id='topic+localImputation'></span>

<h3>Description</h3>

<p>Function to generate imputations using non-parametric and semi-parametric local imputation methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>localImputation(
  i,
  preds,
  y,
  delta,
  bw = NULL,
  kernel = c("gaussian", "uniform", "triangular")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="localImputation_+3A_i">i</code></td>
<td>
<p>integer referring to the index of the missing value to be imputed.</p>
</td></tr>
<tr><td><code id="localImputation_+3A_preds">preds</code></td>
<td>
<p>numeric vector of predictions of missing values from SuperLearner.</p>
</td></tr>
<tr><td><code id="localImputation_+3A_y">y</code></td>
<td>
<p>numeric vector for variable to be imputed.</p>
</td></tr>
<tr><td><code id="localImputation_+3A_delta">delta</code></td>
<td>
<p>binary vector of length <code>length(y)</code> indicating missingness.
<code>1</code> where <code>y</code> is observed and <code>0</code> where <code>y</code> is missing.</p>
</td></tr>
<tr><td><code id="localImputation_+3A_bw">bw</code></td>
<td>
<p><code>NULL</code> or numeric value for bandwidth of kernel function (as standard deviations of the kernel).</p>
</td></tr>
<tr><td><code id="localImputation_+3A_kernel">kernel</code></td>
<td>
<p>one of <code>gaussian</code>, <code>uniform</code>, or <code>triangular</code>.
Specifies the kernel to be used in estimating the distribution around a missing value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of randomly drawn imputed values.
</p>

<hr>
<h2 id='mice.impute.SuperLearner'>SuperLearner method for <code>mice</code> package.</h2><span id='topic+mice.impute.SuperLearner'></span>

<h3>Description</h3>

<p>Method for the <code>mice</code> package that uses SuperLearner as the predctive
algorithm.  Model fitting is done using the <code>SuperLearner</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mice.impute.SuperLearner(
  y,
  ry,
  x,
  wy = NULL,
  SL.library,
  kernel = c("gaussian", "uniform", "triangular"),
  bw = c(0.1, 0.2, 0.25, 0.3, 0.5, 1, 2.5, 5, 10, 20),
  bw.update = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mice.impute.SuperLearner_+3A_y">y</code></td>
<td>
<p>Vector to be imputed</p>
</td></tr>
<tr><td><code id="mice.impute.SuperLearner_+3A_ry">ry</code></td>
<td>
<p>Logical vector of length <code>length(y)</code> indicating the the subset
<code>y[ry]</code> of elements in y to which the imputation model is fitted.
The <code>ry</code> generally distinguishes the observed (<code>TRUE</code>) and missing
values (<code>FALSE</code>) in <code>y</code>.</p>
</td></tr>
<tr><td><code id="mice.impute.SuperLearner_+3A_x">x</code></td>
<td>
<p>Numeric design matrix with <code>length(y)</code> rows with predictors
for <code>y</code>.
Matrix <code>x</code> may have no missing values.</p>
</td></tr>
<tr><td><code id="mice.impute.SuperLearner_+3A_wy">wy</code></td>
<td>
<p>Logical vector of length <code>length(y)</code>. A <code>TRUE</code> value indicates
locations in <code>y</code> for which imputations are created.</p>
</td></tr>
<tr><td><code id="mice.impute.SuperLearner_+3A_sl.library">SL.library</code></td>
<td>
<p>For SuperLearner: Either a character vector of prediction
algorithms or list containing character vectors as specified by the
SuperLearner package.  See details below.</p>
</td></tr>
<tr><td><code id="mice.impute.SuperLearner_+3A_kernel">kernel</code></td>
<td>
<p>One of &quot;gaussian&quot;, &quot;uniform&quot;, &quot;triangular&quot;.  Kernel function
used to compute weights.</p>
</td></tr>
<tr><td><code id="mice.impute.SuperLearner_+3A_bw">bw</code></td>
<td>
<p>NULL or numeric value for bandwidth of kernel function (as standard deviations of the kernel).</p>
</td></tr>
<tr><td><code id="mice.impute.SuperLearner_+3A_bw.update">bw.update</code></td>
<td>
<p>logical indicating whether bandwidths should be computed
every iteration or only on the first iteration.  Default is <code>TRUE</code>,
but <code>FALSE</code> may speed up the run time at the cost of accuracy.</p>
</td></tr>
<tr><td><code id="mice.impute.SuperLearner_+3A_...">...</code></td>
<td>
<p>Further arguments passed to <code>SuperLearner</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mice.impute.SuperLearner()</code> is a method for use with the <a href="mice.html#topic+mice">mice()</a> function that
implements the ensemble predictive model, SuperLearner (van der Laan, 2011),
into the mice (van Buuren, 2011) multiple imputation procedure. This function
is never called directly, instead a user that wishes to use SuperLearner
in MICE simply needs to set the argument <code>method = "SuperLearner"</code> in the
call to <code><a href="mice.html#topic+mice">mice()</a></code>. Arguments for the <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner()</a></code>
function are passed from mice as extra arguments in the <code><a href="mice.html#topic+mice">mice()</a></code> call.
</p>
<p>All MICE methods randomly generate imputed values for a number of data sets.
The approach of SuperMICE is to estimate parameters for a normal distribution
centered at the point estimate for an imputed value predicted by a SuperLearner model.
The point estimates are obtained by fitting a selection of different
predictive models on complete cases and determining an optimal weighted average
of candidate models to predict the missing cases. SuperMICE uses the implementation
of SuperLearner found in the <a href="SuperLearner.html#topic+SuperLearner">SuperLearner</a> package.
The models to be used with <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner()</a></code> are supplied by the user as a
character vector. For a full list of available methods see
<code><a href="SuperLearner.html#topic+listWrappers">listWrappers()</a></code>.
</p>
<p>SuperLearner models do not produce standard errors for estimates, so instead
we use a kernel based estimate of local variance around each point estimate
as the variance parameter in the normal distribution used to randomly sample values.
The kernel can be set by the user with the <code>kernel</code> argument as either
a gaussian kernel, uniform kernel, or triangular kernel. The user must also
supply a list of candidate bandwidths in the <code>bw</code> argument as a numeric
vector.  For more information on the variance and bandwidth selection
see Laqueur, et. al (2021). In every iteration the mice procedure, the optimal
bandwidth is reselected. This may be changed to select the bandwidth only
on the first iteration to speed up the total run time of the imputation by
changing <code>bw.update</code> to <code>FALSE</code>; however this may bias your results.
Note that this only applies to continuous response variables.  In the binary
case the variance is a function of the SuperLearner estimate.
</p>


<h3>Value</h3>

<p>Vector with imputed data, same type as <code>y</code>, and of length <code>sum(wy)</code>
</p>


<h3>References</h3>

<p>Laqueur, H. S., Shev, A. B., Kagawa, R. M. C. (2021). SuperMICE: An Ensemble
Machine Learning Approach to Multiple Imputation by Chained Equations.
American Journal of Epidemiology, kwab271,
<a href="https://doi.org/10.1093/aje/kwab271">doi:10.1093/aje/kwab271</a>.
</p>
<p>Van Buuren, S., Groothuis-Oudshoorn, K. (2011). <code>mice</code>: Multivariate
Imputation by Chained Equations in <code>R</code>. Journal of Statistical Software,
<strong>45</strong>(3), 1-67. <a href="https://doi.org/10.18637/jss.v045.i03">doi:10.18637/jss.v045.i03</a>.
</p>
<p>van der Laan, M. J., Polley, E. C. and Hubbard, A. E. (2008) Super Learner,
Statistical Applications of Genetics and Molecular Biology, 6, article 25.
</p>


<h3>See Also</h3>

<p><code><a href="mice.html#topic+mice">mice()</a></code>, <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  #Multiple imputation with missingness on a continuous variable.

  #Randomly generated data with missingness in x2. The probability of x2
  #  being missing increases with with value of x1.
  n &lt;- 20
  pmissing &lt;- 0.10
  x1 &lt;- runif(n, min = -3, max = 3)
  x2 &lt;- x1^2 + rnorm(n, mean = 0, sd = 1)
  error &lt;- rnorm(n, mean = 0, sd = 1)
  y &lt;- x1 + x2 + error
  f &lt;- ecdf(x1)
  x2 &lt;- ifelse(runif(x2) &lt; (f(x1) * 2 * pmissing), NA, x2)
  dat &lt;- data.frame(y, x1, x2)

  #Create vector of SuperLearner method names
  #  Note: see SuperLearner::listWrappers() for a full list of methods
  #    available.
  SL.lib &lt;- c("SL.mean", "SL.glm")

  #Run mice().
  #  Note 1: m &gt;= 30 and maxit &gt;= 10 are recommended outside of this
  #    toy example
  #  Note 2: a denser bandwidth grid is recommended, see default for bw
  #    argument for example.
  imp.SL &lt;- mice::mice(dat, m = 2, maxit = 2,
                       method = "SuperLearner",
                       print = TRUE, SL.library = SL.lib,
                       kernel = "gaussian",
                       bw = c(0.25, 1, 5))


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
