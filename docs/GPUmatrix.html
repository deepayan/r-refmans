<!DOCTYPE html><html lang="en"><head><title>Help for package GPUmatrix</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {GPUmatrix}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aperm'><p>Array Transposition</p></a></li>
<li><a href='#apply'><p>Apply Functions over 'gpu.matrix-class' margins</p></a></li>
<li><a href='#as_methods'><p>as_methods</p></a></li>
<li><a href='#cbind_rbind_methods'><p>cbind_rbind_methods</p></a></li>
<li><a href='#concatenate_gpu.matrix'><p>concatenate_gpu.matrix</p></a></li>
<li><a href='#cor_cov'><p>Correlation, Variance and Covariance for 'GPUmatrix' objects</p></a></li>
<li><a href='#density'><p>Kernel Density Estimation and Histograms</p></a></li>
<li><a href='#det'><p>Calculate the Determinant of a 'GPUMatrix'</p></a></li>
<li><a href='#diag'><p>diag</p></a></li>
<li><a href='#dim_and_names'><p>Number of rows and columns and its corresponding names</p></a></li>
<li><a href='#dist'><p>Distance Matrix Computation with GPU</p></a></li>
<li><a href='#expmGPU'><p>'GPUmatrix' Exponential</p></a></li>
<li><a href='#extract_gpu.matrix'><p>extract_gpu.matrix</p></a></li>
<li><a href='#fft'><p>Fast Discrete Fourier Transform (FFT)</p></a></li>
<li><a href='#gpu.matrix'><p>create and store a matrix in the GPU</p></a></li>
<li><a href='#gpu.matrix-class'><p>Class 'gpu.matrix' for matrix stored in GPU</p></a></li>
<li><a href='#GPUglm'><p>Fitting Generalized Linear Models using GPUmatrix objects</p></a></li>
<li><a href='#installTorch'>
<p>installTorch</p></a></li>
<li><a href='#kroneker'><p>kroneker Products</p></a></li>
<li><a href='#LR_GradientConjugate_gpumatrix'><p>Logistic Regression with Conjugate Gradient method</p></a></li>
<li><a href='#matrix_decomposition'><p>Decomposition of a matrix with GPU</p></a></li>
<li><a href='#matrix_general_operators_methods'><p>Return the first or last part of a GPUmatrix object</p></a></li>
<li><a href='#matrix_ranges'><p>Get different statistics for a gpu.matrix-class.</p></a></li>
<li><a href='#matrix-product'><p>Matrix Products</p></a></li>
<li><a href='#NMFgpumatrix'><p>Non negative factorization of a matrix</p></a></li>
<li><a href='#power_of_a_matrix'><p>Compute the kth power of a matrix.</p></a></li>
<li><a href='#qr_decomposition'><p>The QR Decomposition of a GPUmatrix object</p></a></li>
<li><a href='#round'><p>rounding of numers</p></a></li>
<li><a href='#solve_gpu.matrix'><p>Solve a System of Equations</p></a></li>
<li><a href='#sort'><p>sort</p></a></li>
<li><a href='#type+20of+20gpu.matrix'><p>Spicify type of 'GPUmatrix'</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Basic Linear Algebra with GPU</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Description:</td>
<td>GPUs are great resources for data analysis, especially in statistics and linear algebra. Unfortunately, very few packages connect R to the GPU, and none of them are transparent enough to run the computations on the GPU without substantial changes to the code. The maintenance of these packages is cumbersome: several of the earlier attempts have been removed from their respective repositories. It would be desirable to have a properly maintained R package that takes advantage of the GPU with minimal changes to the existing code. We have developed the GPUmatrix package (available on CRAN). GPUmatrix mimics the behavior of the Matrix package and extends R to use the GPU for computations. It includes single(FP32) and double(FP64) precision data types, and provides support for sparse matrices. It is easy to learn, and requires very few code changes to perform the operations on the GPU. GPUmatrix relies on either the Torch or Tensorflow R packages to perform the GPU operations. We have demonstrated its usefulness for several statistical applications and machine learning applications: non-negative matrix factorization, logistic regression and general linear models. We have also included a comparison of GPU and CPU performance on different matrix operations.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>torch, tensorflow, Matrix, matrixStats, float, MASS, knitr,
rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/Artistic-2.0">Artistic-2.0</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-01 08:48:04 UTC; clobatofern</td>
</tr>
<tr>
<td>Author:</td>
<td>Cesar Lobato-Fernandez [aut, cre],
  Juan A.Ferrer-Bonsoms [aut],
  Angel Rubio [aut, ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Cesar Lobato-Fernandez &lt;clobatofern@unav.es&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-01 09:02:36 UTC</td>
</tr>
</table>
<hr>
<h2 id='aperm'>Array Transposition</h2><span id='topic+t'></span><span id='topic+t-methods'></span><span id='topic+t+2Cgpu.matrix.tensorflow-method'></span><span id='topic+t+2Cgpu.matrix.torch-method'></span>

<h3>Description</h3>

<p><code>t</code> returns the transpose of a gpu.matrix-class object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'gpu.matrix.tensorflow'
t(x)
## S4 method for signature 'gpu.matrix.torch'
t(x)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aperm_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> to be transposed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>It returns a transposed version of <code>a</code>. The output is also a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> class object.</p>


<h3>See Also</h3>

<p>For more information: <code><a href="base.html#topic+t">t</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Not run: 

  a &lt;- gpu.matrix(1:9,nrow=3,ncol=3)
  t(a) #transpose of a.


## End(Not run)


</code></pre>

<hr>
<h2 id='apply'>Apply Functions over 'gpu.matrix-class' margins</h2><span id='topic+apply'></span><span id='topic+apply-methods'></span><span id='topic+apply+2Cgpu.matrix.tensorflow-method'></span><span id='topic+apply+2Cgpu.matrix.torch-method'></span>

<h3>Description</h3>

<p>This function mimics the 'base' function <code>'apply'</code> to operate on gpu.matrix-class objects: It returns a vector or a list of values obtained by applying a function to margins of a GPUmatrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow'
apply(X, MARGIN, FUN, ..., simplify)
## S4 method for signature 'gpu.matrix.torch'
apply(X, MARGIN, FUN, ..., simplify)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="apply_+3A_x">X</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> object.</p>
</td></tr>
<tr><td><code id="apply_+3A_margin">MARGIN</code></td>
<td>
<p>1 for rows and 2 for columns.</p>
</td></tr>
<tr><td><code id="apply_+3A_fun">FUN</code></td>
<td>
<p>function to be applied in the operation.</p>
</td></tr>
<tr><td><code id="apply_+3A_...">...</code></td>
<td>
<p>general additional parameters. Optional arguments to FUN.</p>
</td></tr>
<tr><td><code id="apply_+3A_simplify">simplify</code></td>
<td>
<p>a logical indicating whether results should be simplified if possible. Note that some methods that can be simplified when working with 'matrix' objects may not always be simplified for gpu.matrix objects. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>FUN</code> is found by a call to <code><a href="base.html#topic+match.fun">match.fun</a></code> as done in the base function <code><a href="base.html#topic+apply">apply</a></code>. Internally, apply will use the functions implemented to work with objects from the GPUmatrix library. If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>
<p>As in <code><a href="base.html#topic+apply">apply</a></code>, the arguments in <code>...</code> cannot have the same name as any of the other arguments to ensure possible errors.
</p>
<p>The parameter <code>simplify</code> indicates wheter the result should be simplified if possible. If the called <code>FUN</code> returns a gpu.matrix-class object, the result cannot be simplified. In these cases, the parameter <code>simplify</code> will work as if it was set to <code>FALSE</code> and the following warning message will be returned: &quot;If the function applied to the GPU matrix returns a tensor or another GPU matrix, then the 'simplify' argument will always be FALSE.&quot;</p>


<h3>Value</h3>

<p>The results of mimics the base function <code><a href="base.html#topic+apply">apply</a></code>.
</p>
<p>Each call to <code>FUN</code> will return a vector of length n. If <code>simplify</code> is TRUE and the result can be simplified, then apply will return a numeric vector of dimension <code>c(n,dim(x)[MARGIN])</code> if <code>n &gt; 1</code>. If <code>n = 1</code>, <code>apply</code> will return a numeric vector of length <code>dim(x)[MARGIN]</code>.
</p>
<p>If simplify is FALSE, apply will return a list of length <code>dim(x)[MARGIN]</code>.
</p>
<p>Note that if <code>simplify</code> is TRUE and the result of <code>FUN</code> is an object of class gpu.matrix, then the result cannot be simplified, so it will return a list of length <code>dim(x)[MARGIN]</code> and each element of this list will be of class gpu.matrix.
</p>
<p>For more details see <code><a href="base.html#topic+apply">apply</a></code>
</p>


<h3>See Also</h3>

<p>For more information see:
<code><a href="base.html#topic+apply">apply</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(installTorch()){

  a &lt;- gpu.matrix(rnorm(9),3,3)

  apply(a, 1, mean) #computes the mean of each row
  apply(a, 2, mean) #computes the mean of each column

}




</code></pre>

<hr>
<h2 id='as_methods'>as_methods</h2><span id='topic+as.array'></span><span id='topic+as.list'></span><span id='topic+as.matrix'></span><span id='topic+as.numeric'></span><span id='topic+as.vector'></span><span id='topic+is.numeric'></span><span id='topic+as.array-methods'></span><span id='topic+as.list-methods'></span><span id='topic+as.matrix-methods'></span><span id='topic+as.numeric-methods'></span><span id='topic+as.vector-methods'></span><span id='topic+is.numeric-methods'></span><span id='topic+as.array+2Cgpu.matrix.tensorflow-method'></span><span id='topic+as.array+2Cgpu.matrix.torch-method'></span><span id='topic+as.list+2Cgpu.matrix.tensorflow-method'></span><span id='topic+as.list+2Cgpu.matrix.torch-method'></span><span id='topic+as.matrix+2Cgpu.matrix.tensorflow-method'></span><span id='topic+as.matrix+2Cgpu.matrix.torch-method'></span><span id='topic+as.numeric+2Cgpu.matrix.tensorflow-method'></span><span id='topic+as.numeric+2Cgpu.matrix.torch-method'></span><span id='topic+as.vector+2Cgpu.matrix.tensorflow-method'></span><span id='topic+as.vector+2Cgpu.matrix.torch-method'></span><span id='topic+is.numeric+2Cgpu.matrix.torch-method'></span><span id='topic+is.numeric+2Cgpu.matrix.tensorflow-method'></span>

<h3>Description</h3>

<p>These functions mimic the 'base' functions of R that have the same name to operate on gpu.matrix-class objects:
</p>
<p>Function <code>as.matrix</code> attempts to turn its argument into a matrix.
Function <code>as.list</code> attempts to turn its argument into a list.
Function <code>as.numeric</code> attempts to turn its argument into a numeric.
Function <code>as.array</code> attempts to turn its argument into an array.
Function <code>as.vector</code> attempts to turn its argument into a vector.
Function <code>is.numeric</code> is a general test of an object being interpretable as numbers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'gpu.matrix.tensorflow'
as.array(x,...)
## S4 method for signature 'gpu.matrix.torch'
as.array(x,...)
## S4 method for signature 'gpu.matrix.tensorflow'
as.list(x,...)
## S4 method for signature 'gpu.matrix.torch'
as.list(x,...)
## S4 method for signature 'gpu.matrix.tensorflow'
as.matrix(x,...)
## S4 method for signature 'gpu.matrix.torch'
as.matrix(x,...)
## S4 method for signature 'gpu.matrix.tensorflow'
as.numeric(x,...)
## S4 method for signature 'gpu.matrix.torch'
as.numeric(x,...)
## S4 method for signature 'gpu.matrix.tensorflow'
as.vector(x,mode)
## S4 method for signature 'gpu.matrix.torch'
as.vector(x,mode)
## S4 method for signature 'gpu.matrix.torch'
is.numeric(x)
## S4 method for signature 'gpu.matrix.tensorflow'
is.numeric(x)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_methods_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> object.</p>
</td></tr>
<tr><td><code id="as_methods_+3A_...">...</code></td>
<td>
<p>(generalized) vectors or matrices. These can be given as named arguments.</p>
</td></tr>
<tr><td><code id="as_methods_+3A_mode">mode</code></td>
<td>
<p>Argument for <code>as.vector</code>. It mimics the argument of the same name of the base function <code>as.vector</code>: character string naming an atomic mode or &quot;list&quot; or &quot;expression&quot; or (except for vector) &quot;any&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that, if the input is a gpu.matrix with complex numbers: the function <code>is.numeric</code> will return FALSE, and the function <code>as.numeric</code> will only returns the real part and the following warning message: &quot;In asMethod(object) : imaginary parts discarded in coercion&quot;.
</p>
<p>The parameter <code>mode</code> of the function <code>as.vector</code> determines the storage mode of the result. For more details see <code><a href="base.html#topic+typeof">typeof</a></code>.
</p>


<h3>Value</h3>

<p>Given a gpu.matrix-class object:
</p>
<p>Function <code>as.matrix</code> turns the input gpu.matrix to a 'matrix' object.
</p>
<p>Function <code>as.list</code> turns the input gpu.matrix into a list.
</p>
<p>Function <code>as.numeric</code> turns the input gpu.matrix into a numeric vector.
</p>
<p>Function <code>as.array</code> turns the input gpu.matrix into an array (Since the gpu.matrix objects are always two-dimensional, this function is equivalent to <code>as.matrix</code>.).
</p>
<p>Function <code>as.vector</code> turns the input gpu.matrix into a vector.
</p>
<p>Function <code>is.numeric</code> returns TRUE or FAALSE if input can be interpretable as numbers.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+numeric">numeric</a></code>,
<code><a href="base.html#topic+array">array</a></code>,
<code><a href="base.html#topic+list">list</a></code>,
<code><a href="base.html#topic+matrix">matrix</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
a &lt;- gpu.matrix(c(rnorm(8),2+1i),nrow=3,ncol=3)
as.array(a)
as.list(a)
as.matrix(a)
as.numeric(a)
is.numeric(a)
as.character(a)
as.vector(a,mode = "list")
as.vector(a,mode = "character")
as.vector(a,mode = "logical")
as.vector(a,mode = "integer")
as.vector(a,mode = "double")
as.vector(a,mode = "complex")
as.vector(a,mode = "raw")



## End(Not run)

</code></pre>

<hr>
<h2 id='cbind_rbind_methods'>cbind_rbind_methods</h2><span id='topic+cbind2'></span><span id='topic+rbind2'></span><span id='topic+cbind2-methods'></span><span id='topic+rbind2-methods'></span><span id='topic+cbind2+2CANY+2Cgpu.matrix.tensorflow-method'></span><span id='topic+cbind2+2CANY+2Cgpu.matrix.torch-method'></span><span id='topic+cbind2+2Cgpu.matrix.tensorflow+2CANY-method'></span><span id='topic+cbind2+2Cgpu.matrix.torch+2CANY-method'></span><span id='topic+rbind2+2CANY+2Cgpu.matrix.tensorflow-method'></span><span id='topic+rbind2+2CANY+2Cgpu.matrix.torch-method'></span><span id='topic+rbind2+2Cgpu.matrix.tensorflow+2CANY-method'></span><span id='topic+rbind2+2Cgpu.matrix.torch+2CANY-method'></span>

<h3>Description</h3>

<p>Mimics the 'base' functions <code>'cbind'</code> and <code>'rbind'</code> to operate on <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> objects. The 'base' functions <code>'cbind'</code> and <code>'rbind'</code> internally call the methods <code>cbind2</code> and <code>rbind2</code>.
</p>
<p>Therefore, ss done in <code><a href="Matrix.html#topic+cbind2">cbind2</a></code> of the package 'Matrix', we have defined in 'GPUmatrix' the methods <code>cbind2</code> and <code>rbind2</code> to operate on <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> objects too.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'ANY,gpu.matrix.tensorflow'
cbind2(x,y)
## S4 method for signature 'ANY,gpu.matrix.torch'
rbind2(x,y)
## S4 method for signature 'gpu.matrix.tensorflow,ANY'
cbind2(x,y,...)
## S4 method for signature 'gpu.matrix.torch,ANY'
rbind2(x,y)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cbind_rbind_methods_+3A_x">x</code>, <code id="cbind_rbind_methods_+3A_y">y</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> object or any other matrix class.</p>
</td></tr>
<tr><td><code id="cbind_rbind_methods_+3A_...">...</code></td>
<td>
<p>(generalized) vectors or matrices. These can be given as named arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of using these functions is equivalent to using the basic <code>cbind</code> and <code>rbind</code> functions. For more details see <code><a href="base.html#topic+cbind">cbind</a></code>.
</p>
<p>Note that if one of the input values is a gpu.matrix-class object, then the output will also be a gpu.matrix-class object.
</p>
<p>The data type of the values of the resulting gpu.matrix-class object (corresponding to the <code>dtype</code> parameter of the gpu.matrix function) is the one that allows the integration of all input values. That is, if you call <code>cbind(a,b)</code> where <code>a</code> is a gpu.matrix-class object with values of &quot;int32&quot; and <code>b</code> is a gpu.matrix-class with values of &quot;float64&quot;, the result will be a gpu.matrix-class with values of &quot;float64&quot;.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+cbind">cbind</a></code>, <code><a href="base.html#topic+rbind">rbind</a></code>, <code><a href="Matrix.html#topic+cbind2">cbind2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

a &lt;- gpu.matrix(1:9,nrow=3,ncol=3)

#add new row
newrow &lt;- c(1,2,3)
a &lt;- rbind2(a,newrow)

#add new column
newcolumn &lt;- c(1,2,3,4)
a &lt;- cbind(a,newcolumn)

#add new rows from other gpu.marix
b &lt;- gpu.matrix(1:16,nrow=4,ncol=4)
d &lt;- rbind(a,b)

#add new columns from other gpu.marix
b &lt;- gpu.matrix(1:16,nrow=4,ncol=4)
d &lt;- cbind(a,b)



## End(Not run)

</code></pre>

<hr>
<h2 id='concatenate_gpu.matrix'>concatenate_gpu.matrix</h2><span id='topic+c-methods'></span><span id='topic+c+2Cgpu.matrix.tensorflow-method'></span><span id='topic+c+2Cgpu.matrix.torch-method'></span><span id='topic+c+2CnumMatrixLike-method'></span>

<h3>Description</h3>

<p>Mimics the 'base' function <code>'c'</code> to operate on <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> objects: function which &quot;combines its arguments to form a vector. All arguments are coerced to a common type which is the type of the returned value.&quot;
In most of the cases, the returned object is of type 'numeric'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow'
c(x,...,recursive)
## S4 method for signature 'gpu.matrix.torch'
c(x,...,recursive)
## S4 method for signature 'numMatrixLike'
c(x,...,recursive)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="concatenate_gpu.matrix_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> object</p>
</td></tr>
<tr><td><code id="concatenate_gpu.matrix_+3A_...">...</code></td>
<td>
<p>objects to be concatenated.</p>
</td></tr>
<tr><td><code id="concatenate_gpu.matrix_+3A_recursive">recursive</code></td>
<td>
<p>The same as <code>c</code>: Logical. If recursive = TRUE, the function recursively descends through lists (and pairlists) combining all their elements into a vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>It will return a vector of type 'numeric' with the combined values.
</p>


<h3>See Also</h3>

<p>See also: <code><a href="base.html#topic+c">c</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Not run: 

#add new value
a &lt;- gpu.matrix(1:5,nrow=1,ncol=5)
c(a,3)

#add other vector
c(a,a)

#add value to a gpu.matrix
a &lt;- gpu.matrix(1:9,nrow=3,ncol=3)
c(a,a)
#it will return a vector as in original c function.



## End(Not run)

</code></pre>

<hr>
<h2 id='cor_cov'>Correlation, Variance and Covariance for 'GPUmatrix' objects</h2><span id='topic+cor'></span><span id='topic+cor-methods'></span><span id='topic+cor+2Cgpu.matrix.tensorflow+2CANY+2CANY+2CANY-method'></span><span id='topic+cor+2Cgpu.matrix.tensorflow+2CANY+2Cmissing+2Ccharacter-method'></span><span id='topic+cor+2Cgpu.matrix.tensorflow+2Cmissing+2Cmissing+2Ccharacter-method'></span><span id='topic+cor+2CANY+2Cgpu.matrix.tensorflow+2CANY+2CANY-method'></span><span id='topic+cor+2Cgpu.matrix.tensorflow+2Cmissing+2CANY+2CANY-method'></span><span id='topic+cor+2CANY+2Cgpu.matrix.torch+2CANY+2CANY-method'></span><span id='topic+cor+2Cgpu.matrix.torch+2CANY+2CANY+2CANY-method'></span><span id='topic+cor+2Cgpu.matrix.torch+2CANY+2Cmissing+2Ccharacter-method'></span><span id='topic+cor+2Cgpu.matrix.torch+2Cmissing+2Cmissing+2Ccharacter-method'></span><span id='topic+cor+2Cgpu.matrix.torch+2Cmissing+2Cmissing+2Cmissing-method'></span><span id='topic+cor+2Cgpu.matrix.torch+2Cmissing+2CANY+2CANY-method'></span><span id='topic+cov2cor'></span><span id='topic+cov'></span><span id='topic+cov2cor-methods'></span><span id='topic+cov-methods'></span><span id='topic+cov2cor+2Cgpu.matrix.tensorflow-method'></span><span id='topic+cov2cor+2Cgpu.matrix.torch-method'></span><span id='topic+cov+2Cgpu.matrix.tensorflow-method'></span><span id='topic+cov+2CANY+2Cgpu.matrix.tensorflow-method'></span><span id='topic+cov+2Cgpu.matrix.tensorflow+2CANY-method'></span><span id='topic+cov+2Cgpu.matrix.tensorflow+2Cmissing-method'></span><span id='topic+cov+2Cgpu.matrix.torch-method'></span><span id='topic+cov+2CANY+2Cgpu.matrix.torch-method'></span><span id='topic+cov+2Cgpu.matrix.torch+2CANY-method'></span><span id='topic+cov+2Cgpu.matrix.torch+2Cmissing-method'></span>

<h3>Description</h3>

<p>These functions mimic the  <code>stats</code> functions <code>cov</code> and <code>cor</code> to compute on <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> objects: &quot;<code>cov</code> and <code>cor</code> compute the covariance and correlation of <code>x</code> and <code>y</code> if these are vectors. If <code>x</code> and <code>y</code> are matrices then the covariances (or correlations) between the columns of <code>x</code> and the columns of <code>y</code> are computed.&quot;
</p>
<p><code>cov2cor</code> scales a covariance matrix into the corresponding correlation matrix efficiently.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>

## S4 method for signature 'gpu.matrix.tensorflow,ANY,ANY,ANY'
cor(x,y)
## S4 method for signature 'gpu.matrix.tensorflow,ANY,missing,character'
cor(x,y,method)
## S4 method for signature 'gpu.matrix.tensorflow,missing,missing,character'
cor(x,y,method)
## S4 method for signature 'ANY,gpu.matrix.tensorflow,ANY,ANY'
cor(x,y)
## S4 method for signature 'gpu.matrix.tensorflow,missing,ANY,ANY'
cor(x,y)

## S4 method for signature 'ANY,gpu.matrix.torch,ANY,ANY'
cor(x,y)
## S4 method for signature 'gpu.matrix.torch,ANY,ANY,ANY'
cor(x,y)
## S4 method for signature 'gpu.matrix.torch,ANY,missing,character'
cor(x,y,method)
## S4 method for signature 'gpu.matrix.torch,missing,missing,character'
cor(x,y,method)
## S4 method for signature 'gpu.matrix.torch,missing,missing,missing'
cor(x,y)
## S4 method for signature 'gpu.matrix.torch,missing,ANY,ANY'
cor(x,y)



## S4 method for signature 'gpu.matrix.tensorflow'
cov(x,y)
## S4 method for signature 'ANY,gpu.matrix.tensorflow'
cov(x,y)
## S4 method for signature 'gpu.matrix.tensorflow,ANY'
cov(x,y)
## S4 method for signature 'gpu.matrix.tensorflow,missing'
cov(x,y)


## S4 method for signature 'gpu.matrix.torch'
cov(x,y)
## S4 method for signature 'ANY,gpu.matrix.torch'
cov(x,y)
## S4 method for signature 'gpu.matrix.torch,ANY'
cov(x,y)
## S4 method for signature 'gpu.matrix.torch,missing'
cov(x,y)

## S4 method for signature 'gpu.matrix.tensorflow'
cov2cor(V)
## S4 method for signature 'gpu.matrix.torch'
cov2cor(V)



</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cor_cov_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="cor_cov_+3A_y">y</code></td>
<td>
<p><code>NULL</code> (default) or a vector, matrix, data frame or <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> with compatible dimensions to x.</p>
</td></tr>
<tr><td><code id="cor_cov_+3A_method">method</code></td>
<td>
<p>a character string indicating which correlation coefficient (or covariance) is to be computed. One of <code>"pearson"</code> (default) or <code>"spearman"</code>.</p>
</td></tr>
<tr><td><code id="cor_cov_+3A_v">V</code></td>
<td>
<p>symmetric numeric gpu.matrix, usually positive definite such as a covariance matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions work in the same way as their counterparts in the 'stats' library. Note that the 'Kendal' method (implemented in the 'stats' library) is not available for working with gpu.matrix-class objects.
</p>
<p>Notice that the inputs can be either an object of class 'matrix', 'Matrix' or 'gpu.matrix'. User must be sure that the input values must be numeric.
</p>
<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>. The result will be a gpu.matrix object.
</p>
<p>For more details see <code><a href="stats.html#topic+cor">cor</a></code> and <code><a href="stats.html#topic+cov2cor">cov2cor</a></code>.
</p>


<h3>Value</h3>

<p>The result obtained by applying these functions will be a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> object. For each function the result will be:
</p>
<p>- <code>cor</code> correlation between <code>x</code> and <code>y</code> (when two vectors are the input) or the correlation between the columns of <code>x</code> and <code>y</code> if <code>x</code> and <code>y</code> are a gpu.matrix class object. If <code>y</code> is empty, is equivalent to <code>y=x</code>.
</p>
<p>- <code>cov</code> the same as <code>cor</code> but compute the covariance.
</p>
<p>- <code>cov2cor</code> scales a covariance matrix into the corresponding correlation matrix efficiently.
</p>


<h3>See Also</h3>

<p>For more information:
<code><a href="stats.html#topic+cor">cor</a></code>,
<code><a href="stats.html#topic+cov">cov</a></code>,
<code><a href="stats.html#topic+cov2cor">cov2cor</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
a &lt;- gpu.matrix(rnorm(10))
b &lt;- gpu.matrix(rnorm(10))
cor(a,b)

#example taken from stats corresponding help page:
longley_matrix &lt;- as.matrix(longley)
longley_gpu &lt;- as.gpu.matrix(longley_matrix)
C1 &lt;- cor(longley_gpu)
cov(longley_gpu)
cov2cor(cov(longley_gpu))



## End(Not run)

</code></pre>

<hr>
<h2 id='density'>Kernel Density Estimation and Histograms</h2><span id='topic+density'></span><span id='topic+density-methods'></span><span id='topic+density+2Cgpu.matrix.tensorflow-method'></span><span id='topic+density+2Cgpu.matrix.torch-method'></span><span id='topic+hist'></span><span id='topic+hist-methods'></span><span id='topic+hist+2Cgpu.matrix.tensorflow-method'></span><span id='topic+hist+2Cgpu.matrix.torch-method'></span>

<h3>Description</h3>

<p>The function <code>density</code> mimics the function <code>density</code> of the library <code>stats</code> to operate on gpu.matrix-class objects: &quot;It computes kernel density estimates. Its default method does so with the given kernel and bandwidth for univariate observations.&quot;
</p>
<p>The function <code>'hist'</code> mimics the function <code>'hist'</code> of the library <code>'graphics'</code> to operate on gpu.matrix-class objects: &quot;It computes a histogram of the given data values.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow'
density(x)
## S4 method for signature 'gpu.matrix.torch'
density(x)
## S4 method for signature 'gpu.matrix.tensorflow'
hist(x,...)
## S4 method for signature 'gpu.matrix.torch'
hist(x,...)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="density_+3A_x">x</code></td>
<td>
<p>the <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> object from which the estimate density is to be computed or the histogram is desired.</p>
</td></tr>
<tr><td><code id="density_+3A_...">...</code></td>
<td>
<p>further arguments and graphical parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The two functions (<code>density</code> and <code>hist</code>) have been programmed to call their corresponding counterpart functions with their default parameters. Therefore, the internal operations to obtain each graph are computed by the CPU, regardless of whether the input value is stored in the GPU.
</p>
<p>For more information on these functions see <code><a href="stats.html#topic+density">density</a></code>, and <code><a href="graphics.html#topic+hist">hist</a></code>.
</p>


<h3>Value</h3>

<p>The function <code>density</code> returns the same output as its counterpart function <code>density</code> from the library <code>stats</code>: It returns &quot;an object with class 'density' whose underlying structure is a list containing the following components.
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>the n coordinates of the points where the density is estimated.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the estimated density values. These will be non-negative, but can be zero.</p>
</td></tr>
<tr><td><code>bw</code></td>
<td>
<p>the bandwidth used.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the sample size after elimination of missing values.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call which produced the result.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>	the deparsed name of the x argument.</p>
</td></tr>
<tr><td><code>has.na</code></td>
<td>
<p>logical, for compatibility (always FALSE).</p>
</td></tr>
</table>
<p>The print method reports summary values on the x and y components.&quot; (taken from <code><a href="stats.html#topic+density">density</a></code>).
</p>
<p>On the other hand, the function <code>hist</code> returns the same output as its counterpart function <code>hist</code> from the library <code>graphics</code>: It returns &quot;an object of class 'histogram' which is a list with components:
</p>
<table role = "presentation">
<tr><td><code>breaks</code></td>
<td>
<p>the n+1n+1 cell boundaries (= breaks if that was a vector). These are the nominal breaks, not with the boundary fuzz.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>n integers; for each cell, the number of x[] inside.</p>
</td></tr>
<tr><td><code>density</code></td>
<td>
<p>values <code class="reqn">\hat{f}(x_i)</code>, as estimated density values. If <code>all(diff(breaks) == 1)</code>, the are the relative frequencies <code>counts/n</code> and in general satisfy <code class="reqn">\sum_i{\hat{f}(x_i)(b_{i+1}-b_i)=1}</code>, where <code class="reqn">b_i</code> = <code>breaks[i]</code></p>
</td></tr></table>
<p>.
</p>
<table role = "presentation">
<tr><td><code>mids</code></td>
<td>
<p>the n cell midpoints.</p>
</td></tr>
<tr><td><code>xname</code></td>
<td>
<p>a character string with the actual x argument name.</p>
</td></tr>
<tr><td><code>equidist</code></td>
<td>
<p>logical, indicating if the distances between breaks are all the same.&quot;</p>
</td></tr>
</table>
<p>(Taken from <code><a href="graphics.html#topic+hist">hist</a></code>)
</p>


<h3>See Also</h3>

<p>For more information see:
<code><a href="stats.html#topic+density">density</a></code>, and
<code><a href="graphics.html#topic+hist">hist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(installTorch()){

  a &lt;- gpu.matrix(rnorm(20*100),20,100)

  density(a[1,]) #density information
  plot(density(a[1,])) #plot the estimated density function

  hist(a[1,]) #plot the histogram

}


</code></pre>

<hr>
<h2 id='det'>Calculate the Determinant of a 'GPUMatrix'</h2><span id='topic+determinant'></span><span id='topic+determinant-methods'></span><span id='topic+determinant+2Cgpu.matrix.tensorflow+2Clogical-method'></span><span id='topic+determinant+2Cgpu.matrix.tensorflow+2Cmissing-method'></span><span id='topic+determinant+2Cgpu.matrix.torch+2Clogical-method'></span><span id='topic+determinant+2Cgpu.matrix.torch+2Cmissing-method'></span><span id='topic+det'></span><span id='topic+det-methods'></span><span id='topic+det+2Cgpu.matrix.tensorflow-method'></span><span id='topic+det+2Cgpu.matrix.torch-method'></span>

<h3>Description</h3>

<p>These functions mimic the 'base' functions <code>det</code> and <code>determinant</code> to operate on gpu.matrix-class objects: &quot;<code>det</code> calculates the determinant of a matrix. <code>determinant</code> is a generic function that returns separately the modulus of the determinant, optionally on the logarithm scale, and the sign of the determinant.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'gpu.matrix.tensorflow,logical'
determinant(x,logarithm,...)
## S4 method for signature 'gpu.matrix.tensorflow,missing'
determinant(x,logarithm,...)
## S4 method for signature 'gpu.matrix.torch,logical'
determinant(x,logarithm,...)
## S4 method for signature 'gpu.matrix.torch,missing'
determinant(x,logarithm,...)

## S4 method for signature 'gpu.matrix.tensorflow'
det(x,...)
## S4 method for signature 'gpu.matrix.torch'
det(x,...)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="det_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> object.</p>
</td></tr>
<tr><td><code id="det_+3A_...">...</code></td>
<td>
<p>Optional parameters. For more details seee <code><a href="base.html#topic+det">det</a></code></p>
</td></tr>
<tr><td><code id="det_+3A_logarithm">logarithm</code></td>
<td>
<p>logical; if TRUE (default) return the logarithm of the modulus of the determinant.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>det</code> and <code>determinant</code> internally call the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
</p>
<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p><code>det</code> returns the same output corresponding to the base function <code>det</code>, which is the determinant of <code>x</code>. The returned value is a object of class numeric sotred in the cpu.
</p>
<p><code>determinant</code> returns the corresponding output of the base function <code>determinant</code>, which is an object of class <code>det</code>, that contains the following components:
</p>
<table role = "presentation">
<tr><td><code>modulus</code></td>
<td>
<p>a numeric value. The modulus (absolute value) of the determinant if logarithm is FALSE; otherwise the logarithm of the modulus.</p>
</td></tr>
<tr><td><code>sign</code></td>
<td>
<p>integer; either +1 or -1 according to whether the determinant is positive or negative.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>For more information see:
<code><a href="base.html#topic+det">det</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

x &lt;- gpu.matrix(1:4,nrow=2, ncol = 2)
determinant(x) #modulus of the determinant.
det(x)#the determinant.


## End(Not run)

</code></pre>

<hr>
<h2 id='diag'>diag</h2><span id='topic+diag'></span><span id='topic+diag-methods'></span><span id='topic+diag+2Cgpu.matrix.tensorflow-method'></span><span id='topic+diag+2Cgpu.matrix.torch-method'></span><span id='topic+diag+3C-'></span><span id='topic+diag+3C--methods'></span><span id='topic+diag+3C-+2Cgpu.matrix.tensorflow+2Cnumeric-method'></span><span id='topic+diag+3C-+2Cgpu.matrix.torch+2Cnumeric-method'></span>

<h3>Description</h3>

<p>This function mimics the base function <code>'diag'</code> to operate on gpu.matrix-class objects: &quot;extract or replace the diagonal of a matrix, or constructs a diagonal matrix.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'gpu.matrix.tensorflow'
diag(x)
## S4 method for signature 'gpu.matrix.torch'
diag(x)
## S4 replacement method for signature 'gpu.matrix.tensorflow,numeric'
diag(x) &lt;- value
## S4 replacement method for signature 'gpu.matrix.torch,numeric'
diag(x) &lt;- value

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diag_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="diag_+3A_value">value</code></td>
<td>
<p>either a single value or a vector of length equal to that of the current diagonal.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Output corresponding to the base function <code>diag</code>:
If input <code>x</code> is a gpu.matrix-class object then <code>diag{x}</code> returns a numeric object with the diagonal of the matrix <code>x</code> (this output is not a gpu.matrix-class object).
</p>
<p>The replacement form <code>diag(x) &lt;- value</code> sets the diagonal of the matrix <code>x</code> to the given value(s).
</p>


<h3>See Also</h3>

<p>For more information see:
<code><a href="base.html#topic+diag">diag</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if(installTorch()){

  a &lt;- gpu.matrix(rnorm(9),nrow=3,ncol=3)

  diag(a) #shows the diagonal of matrix a

  diag(a) &lt;- c(10,0,100) #set the diagonal of matrix a
  a

}





</code></pre>

<hr>
<h2 id='dim_and_names'>Number of rows and columns and its corresponding names</h2><span id='topic+rownames'></span><span id='topic+colnames'></span><span id='topic+dimnames'></span><span id='topic+dim'></span><span id='topic+length'></span><span id='topic+ncol'></span><span id='topic+nrow'></span><span id='topic+rownames-methods'></span><span id='topic+colnames-methods'></span><span id='topic+dimnames-methods'></span><span id='topic+dim-methods'></span><span id='topic+length-methods'></span><span id='topic+ncol-methods'></span><span id='topic+nrow-methods'></span><span id='topic+rownames+2Cgpu.matrix.tensorflow-method'></span><span id='topic+rownames+2Cgpu.matrix.torch-method'></span><span id='topic+colnames+2Cgpu.matrix.tensorflow-method'></span><span id='topic+colnames+2Cgpu.matrix.torch-method'></span><span id='topic+dim+2Cgpu.matrix.tensorflow-method'></span><span id='topic+dim+2Cgpu.matrix.torch-method'></span><span id='topic+dimnames+2Cgpu.matrix.tensorflow-method'></span><span id='topic+dimnames+2Cgpu.matrix.torch-method'></span><span id='topic+length+2Cgpu.matrix.tensorflow-method'></span><span id='topic+length+2Cgpu.matrix.torch-method'></span><span id='topic+ncol+2Cgpu.matrix.tensorflow-method'></span><span id='topic+ncol+2Cgpu.matrix.torch-method'></span><span id='topic+nrow+2Cgpu.matrix.tensorflow-method'></span><span id='topic+nrow+2Cgpu.matrix.torch-method'></span><span id='topic+dim+3C-'></span><span id='topic+dimnames+3C-'></span><span id='topic+dim+3C--methods'></span><span id='topic+dimnames+3C--methods'></span><span id='topic+dim+3C-+2Cgpu.matrix.tensorflow+2Cvector-method'></span><span id='topic+dim+3C-+2Cgpu.matrix.torch+2Cvector-method'></span><span id='topic+dimnames+3C-+2Cgpu.matrix.tensorflow+2Cvector-method'></span><span id='topic+dimnames+3C-+2Cgpu.matrix.torch+2Cvector-method'></span>

<h3>Description</h3>

<p>These functions mimic the 'base' functions <code>rownames</code>, <code>colnames</code>, <code>dimnames</code>, <code>dim</code>, <code>length</code>, <code>ncol</code>, <code>nrow</code> to operate on gpu.matrix-class objects.
</p>
<p>The &quot;<code>dim</code> family functions&quot; set or get the dimension of a gpu.matrix-class object.
</p>
<p>The &quot;<code>rownames</code> and <code>colnames</code> family functions&quot; set or get the corresponding names of rows and columns of a gpu.matrix-class object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow'
rownames(x)
## S4 method for signature 'gpu.matrix.torch'
rownames(x)
## S4 method for signature 'gpu.matrix.tensorflow'
colnames(x)
## S4 method for signature 'gpu.matrix.torch'
colnames(x)
## S4 method for signature 'gpu.matrix.tensorflow'
dim(x)
## S4 method for signature 'gpu.matrix.torch'
dim(x)
## S4 method for signature 'gpu.matrix.tensorflow'
dimnames(x)
## S4 method for signature 'gpu.matrix.torch'
dimnames(x)
## S4 method for signature 'gpu.matrix.tensorflow'
length(x)
## S4 method for signature 'gpu.matrix.torch'
length(x)
## S4 method for signature 'gpu.matrix.tensorflow'
ncol(x)
## S4 method for signature 'gpu.matrix.torch'
ncol(x)
## S4 method for signature 'gpu.matrix.tensorflow'
nrow(x)
## S4 method for signature 'gpu.matrix.torch'
nrow(x)

## S4 replacement method for signature 'gpu.matrix.tensorflow,vector'
dim(x) &lt;- value
## S4 replacement method for signature 'gpu.matrix.torch,vector'
dim(x) &lt;- value
## S4 replacement method for signature 'gpu.matrix.tensorflow,vector'
dimnames(x) &lt;- value
## S4 replacement method for signature 'gpu.matrix.torch,vector'
dimnames(x) &lt;- value

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dim_and_names_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="dim_and_names_+3A_value">value</code></td>
<td>
<p>For <code>dim</code> a numeric vector of length 2 with the number of rows and number of columns. For <code>dimnames</code> a character or numeric vector of length 2 with the names of the rows and names of the columns. </p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>rownames</code> returns the names of the rows of a gpu.matrix-class object.
<code>colnames</code> returns the names of the columns of a gpu.matrix-class object.
</p>
<p><code>dim</code> returns the number of rows and columns of a gpu.matrix-class object and
<code>dim &lt;- </code> sets the number of rows and columns of a gpu.matrix-class object.
</p>
<p><code>dimnames</code> returns the names of the rows and columns of a gpu.matrix-class object and
<code>dimnames &lt;- </code> sets the names of the rows and columns of a gpu.matrix-class object.
</p>
<p><code>length</code> returns the length (ncol*nrow) of a gpu.matrix-class object.
</p>
<p><code>ncol</code> returns the number of columns of a gpu.matrix-class object.
</p>
<p><code>nrow</code> returns the number of rows of a gpu.matrix-class object.
</p>


<h3>See Also</h3>

<p>For more information:
<code><a href="base.html#topic+rownames">rownames</a></code>,
<code><a href="base.html#topic+colnames">colnames</a></code>,
<code><a href="base.html#topic+dim">dim</a></code>,
<code><a href="base.html#topic+dim+3C-">dim&lt;-</a></code>,
<code><a href="base.html#topic+dimnames">dimnames</a></code>,
<code><a href="base.html#topic+dimnames+3C-">dimnames&lt;-</a></code>,
<code><a href="base.html#topic+length">length</a></code>,
<code><a href="base.html#topic+ncol">ncol</a></code>,
<code><a href="base.html#topic+nrow">nrow</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

a &lt;- gpu.matrix(rnorm(9))

dim(a) &lt;- c(3,3) #sets the number of rows and columns.
dim(a) #shows the number of rows and the number of columns
ncol(a) #shows the number of columns
nrow(a) #shows the number of rows
length(a) #shows the lenght of the matrix (nrow*ncol)


dimnames(a) &lt;- list(c("r1","r2","r3"),c("c1","c2","c3")) #sets rows and column names
dimnames(a) #shows both the row and the col names

#these functions are equivalent to the following:
rownames(a) &lt;- c("r1","r2","r3") #adds rownames to a.
colnames(a) &lt;- c("c1","c2","c3") #adds colnames to a.
rownames(a) #shows rownames.
colnames(a) #shows colnames.


## End(Not run)

</code></pre>

<hr>
<h2 id='dist'>Distance Matrix Computation with GPU</h2><span id='topic+dist'></span><span id='topic+dist-methods'></span><span id='topic+dist+2Cgpu.matrix.torch-method'></span>

<h3>Description</h3>

<p>This function mimics the 'stats' function <code>dist</code>: 'computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix.'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist(x, method = "euclidean", diag = FALSE,
    upper = FALSE, p = 2)

## S4 method for signature 'gpu.matrix.torch'
dist(x,method,diag,upper,p)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dist_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="dist_+3A_method">method</code></td>
<td>
<p>the same as the 'stats' function <code>dist</code>: the distance measure to be used. Could be &quot;euclidean&quot;, &quot;maximum&quot;, &quot;manhattan&quot; or &quot;minkowski&quot;. Note that the &quot;canberra&quot;, &quot;binary&quot; methods are not included.</p>
</td></tr>
<tr><td><code id="dist_+3A_diag">diag</code></td>
<td>
<p>the same as the 'stats' function <code>dist</code>: logical value indicating if the diagonal of the distances should be printed. It is set to TRUE and cannot be changed.
</p>
</td></tr>
<tr><td><code id="dist_+3A_upper">upper</code></td>
<td>
<p>the same as the 'stats' function <code>dist</code>: logical value indicating whether the upper triangle of the distance matrix should be printed. It is set to TRUE and cannot be changed.</p>
</td></tr>
<tr><td><code id="dist_+3A_p">p</code></td>
<td>
<p>the same as the 'stats' function <code>dist</code>: The power of the Minkowski distance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function mimics the 'stat' function <code>dist</code>. The distance measures used are (taken from <code><a href="stats.html#topic+dist">dist</a></code>):
</p>
<p>euclidean: <code class="reqn">\sqrt{\sum_i(x_i-y_i)^2}</code>
</p>
<p>maximum: Maximum distance between two components of <code>x</code> and <code>y</code>.
</p>
<p>manhattan: Absolute distance between the tow vectors.
</p>
<p>minkowski: the <code>p</code> norm:  the ppth root of the sum of the ppth powers of the differences of the components.
</p>
<p>For more details see <code><a href="stats.html#topic+dist">dist</a></code>.
</p>
<p>The function <code>dist</code>  internally calls the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
</p>
<p>If the input gpu.matrix-class object is stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>The function returns a gpu.matrix-class object with the corresponding distances between the rows of the input gpu.matrix object.
</p>


<h3>See Also</h3>

<p>For more information see:
<code><a href="stats.html#topic+dist">dist</a></code>, and <code><a href="torch.html#topic+torch_cdist">torch_cdist</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

#the example compare the results with the
#'stats' function 'dist':

x &lt;- matrix(rnorm(100), nrow = 5)

dist(x,diag = TRUE,upper = TRUE,method = "euclidean")
dist(x = as.gpu.matrix(x),method = "euclidean")

dist(x,diag = TRUE,upper = TRUE,method = "maximum")
dist(x = as.gpu.matrix(x),method = "maximum")

dist(x,diag = TRUE,upper = TRUE,method = "manhattan")
dist(x = as.gpu.matrix(x),method = "manhattan")

dist(x,diag = TRUE,upper = TRUE,method = "minkowski")
dist(x = as.gpu.matrix(x),method = "minkowski")

dist(x,diag = TRUE,upper = TRUE,method = "minkowski",p = 23)
dist(x = as.gpu.matrix(x),method = "minkowski",p = 23)


## End(Not run)

</code></pre>

<hr>
<h2 id='expmGPU'>'GPUmatrix' Exponential</h2><span id='topic+expmGPU'></span><span id='topic+expmGPU-methods'></span><span id='topic+expmGPU+2Cgpu.matrix.tensorflow-method'></span><span id='topic+expmGPU+2Cgpu.matrix.torch-method'></span>

<h3>Description</h3>

<p>This function mimics the function <code>expm</code> of the library <code>Matrix</code> to operate on gpu.matrix-class objects: It &quot;computes the exponential of a matrix.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expmGPU(x)
## S4 method for signature 'gpu.matrix.tensorflow'
expmGPU(x)
## S4 method for signature 'gpu.matrix.torch'
expmGPU(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="expmGPU_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The exponential of a matrix is computed as:
<code class="reqn">\sum_{k=0}^\infty 1/k!X^k</code>.
</p>
<p>The function <code>expmGPU</code> internally calls the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
</p>
<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>
<p>Please note that this function works with float numbers (either float32 or float64). If the data type of <code>x</code> is integer, this function will not work. An example is shown below.
</p>


<h3>Value</h3>

<p>The matrix exponential of <code>x</code> as <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> class.
</p>


<h3>See Also</h3>

<p>For more information see  <code><a href="Matrix.html#topic+expm">expm</a></code>, and <code><a href="torch.html#topic+torch_matrix_exp">torch_matrix_exp</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  ## Not run: 
#build with a matrix that contains int number. It will  not work.
x &lt;- gpu.matrix(1:9,nrow=3,ncol = 3,dtype = "int")
x
try(expmGPU(x))

#need to be float and not int
x &lt;- gpu.matrix(1:9,nrow=3,ncol = 3,dtype = "float64")
expmGPU(x)


## End(Not run)


</code></pre>

<hr>
<h2 id='extract_gpu.matrix'>extract_gpu.matrix</h2><span id='topic++5B-methods'></span><span id='topic++5B+2Cgpu.matrix.tensorflow+2Cindex+2Cindex-method'></span><span id='topic++5B+2Cgpu.matrix.tensorflow+2Cindex+2Cmissing-method'></span><span id='topic++5B+2Cgpu.matrix.tensorflow+2Cmatrix+2Cmissing-method'></span><span id='topic++5B+2Cgpu.matrix.tensorflow+2Cmissing+2Cindex-method'></span><span id='topic++5B+2Cgpu.matrix.torch+2Cindex+2Cindex-method'></span><span id='topic++5B+2Cgpu.matrix.torch+2Cindex+2Cmissing-method'></span><span id='topic++5B+2Cgpu.matrix.torch+2Cmatrix+2Cmissing-method'></span><span id='topic++5B+2Cgpu.matrix.torch+2Cmissing+2Cindex-method'></span><span id='topic++5B+3C-+2Cgpu.matrix.tensorflow+2Cindex+2Cindex-method'></span><span id='topic++5B+3C-+2Cgpu.matrix.tensorflow+2Cindex+2Cmissing-method'></span><span id='topic++5B+3C-+2Cgpu.matrix.tensorflow+2Cmatrix+2Cmissing-method'></span><span id='topic++5B+3C-+2Cgpu.matrix.tensorflow+2Cmissing+2Cindex-method'></span><span id='topic++5B+3C-+2Cgpu.matrix.torch+2Cindex+2Cindex-method'></span><span id='topic++5B+3C-+2Cgpu.matrix.torch+2Cindex+2Cmissing-method'></span><span id='topic++5B+3C-+2Cgpu.matrix.torch+2Cmatrix+2Cmissing-method'></span><span id='topic++5B+3C-+2Cgpu.matrix.torch+2Cmissing+2Cindex-method'></span><span id='topic++5B+5B+2Cgpu.matrix.tensorflow+2Cindex-method'></span><span id='topic++5B+5B+2Cgpu.matrix.torch+2Cindex-method'></span><span id='topic++5B+5B+3C-+2Cgpu.matrix.tensorflow+2Cindex-method'></span><span id='topic++5B+5B+3C-+2Cgpu.matrix.torch+2Cindex-method'></span><span id='topic++5B'></span><span id='topic++5B+3C-'></span><span id='topic++5B+5B'></span><span id='topic++5B+5B+3C-'></span><span id='topic+--methods'></span><span id='topic+-'></span><span id='topic+-+2Cgpu.matrix.tensorflow+2Cmissing-method'></span><span id='topic+-+2Cgpu.matrix.torch+2Cmissing-method'></span><span id='topic++5B+3C--methods'></span><span id='topic++5B+5B-methods'></span><span id='topic++5B+5B+3C--methods'></span>

<h3>Description</h3>

<p>These operators mimic the base operators <code>[,[&lt;-, [[, and [[&lt;-</code> to compute on gpu.matrix-class objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow,missing'
e1 - e2
## S4 method for signature 'gpu.matrix.torch,missing'
e1 - e2
## S4 method for signature 'gpu.matrix.tensorflow,index,index'
x[i,j]
## S4 method for signature 'gpu.matrix.tensorflow,index,missing'
x[i,j,...,drop = TRUE]
## S4 method for signature 'gpu.matrix.tensorflow,matrix,missing'
x[i,j,...,drop = TRUE]
## S4 method for signature 'gpu.matrix.tensorflow,missing,index'
x[i,j]
## S4 method for signature 'gpu.matrix.torch,index,index'
x[i,j]
## S4 method for signature 'gpu.matrix.torch,index,missing'
x[i,j,...,drop = TRUE]
## S4 method for signature 'gpu.matrix.torch,matrix,missing'
x[i,j,...,drop = TRUE]
## S4 method for signature 'gpu.matrix.torch,missing,index'
x[i,j]
## S4 replacement method for signature 'gpu.matrix.tensorflow,index,index'
x[i,j] &lt;- value
## S4 replacement method for signature 'gpu.matrix.tensorflow,index,missing'
x[i,j] &lt;- value
## S4 replacement method for signature 'gpu.matrix.tensorflow,matrix,missing'
x[i,j] &lt;- value
## S4 replacement method for signature 'gpu.matrix.tensorflow,missing,index'
x[i,j] &lt;- value
## S4 replacement method for signature 'gpu.matrix.torch,index,index'
x[i,j] &lt;- value
## S4 replacement method for signature 'gpu.matrix.torch,index,missing'
x[i,j] &lt;- value
## S4 replacement method for signature 'gpu.matrix.torch,matrix,missing'
x[i,j] &lt;- value
## S4 replacement method for signature 'gpu.matrix.torch,missing,index'
x[i,j] &lt;- value
## S4 method for signature 'gpu.matrix.tensorflow,index'
x[[i,j,...]]
## S4 method for signature 'gpu.matrix.torch,index'
x[[i,j,...]]
## S4 replacement method for signature 'gpu.matrix.tensorflow,index'
x[[i]] &lt;- value
## S4 replacement method for signature 'gpu.matrix.torch,index'
x[[i]] &lt;- value

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_gpu.matrix_+3A_e1">e1</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="extract_gpu.matrix_+3A_e2">e2</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="extract_gpu.matrix_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> object from which extract element(s) or in which to replace element(s)</p>
</td></tr>
<tr><td><code id="extract_gpu.matrix_+3A_i">i</code>, <code id="extract_gpu.matrix_+3A_j">j</code>, <code id="extract_gpu.matrix_+3A_...">...</code></td>
<td>
<p>indices specifying elements to extract or replace.</p>
</td></tr>
<tr><td><code id="extract_gpu.matrix_+3A_value">value</code></td>
<td>
<p>typically an array-like R object of a similar class as <code>x</code>.</p>
</td></tr>
<tr><td><code id="extract_gpu.matrix_+3A_drop">drop</code></td>
<td>
<p>For matrices and arrays. If TRUE the result is coerced to the lowest possible dimension</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When replacing a value or values in a gpu.matrix, the gpu.matrix will not change its datatype (corresponding to the parameter <code>dtype</code> of the function <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>) based on the datatype in value. For example, the code <code>x[1,1] &lt;- value</code> where the array <code>x</code> is a gpu.matrix with integer values and <code>value</code> has 'double' values, only the integer part of <code>value</code> will be stored in <code>x[1,1]</code>.
</p>


<h3>See Also</h3>

<p>See Also <code><a href="base.html#topic+Extract">Extract</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Not run: 

a &lt;- gpu.matrix(1:9,nrow=3,ncol=3)
rownames(a) &lt;- c("R1","R2","R3")
colnames(a) &lt;- c("C1","C2","C3")

#return
a[3,3] # the element row 3 and column 3
a[6] # the 6th element
a[1,] # the first row
a[c(1,2),] # the first and second row
a[c(1,1),] # the first row twice
a[,1] # the first column
a[,c(1,2)] # the first and second column
a[,c(1,1)] # the first column twice

#replace
a[3,3] &lt;- 100 # replace the element 3,3
a[1,] &lt;- c(1,2,1) # replace the first row
a[,2] &lt;- c(0,0,0) # replace the second column
a[c(1,2),] &lt;- matrix(1:6,nrow = 2) # replace the first and second row




## End(Not run)

</code></pre>

<hr>
<h2 id='fft'>Fast Discrete Fourier Transform (FFT)</h2><span id='topic+fft'></span><span id='topic+fft-methods'></span><span id='topic+fft+2Cgpu.matrix.tensorflow-method'></span><span id='topic+fft+2Cgpu.matrix.tensorflow+2Cmissing-method'></span><span id='topic+fft+2Cgpu.matrix.torch-method'></span><span id='topic+fft+2Cgpu.matrix.torch+2Clogical-method'></span><span id='topic+fft+2Cgpu.matrix.torch+2Cmissing-method'></span><span id='topic+mvfft'></span><span id='topic+mvfft-methods'></span><span id='topic+mvfft+2Cgpu.matrix.torch-method'></span><span id='topic+mvfft+2Cgpu.matrix.torch+2Clogical-method'></span><span id='topic+mvfft+2Cgpu.matrix.torch+2Cmissing-method'></span><span id='topic+mvfft+2Cgpu.matrix.tensorflow-method'></span><span id='topic+mvfft+2Cgpu.matrix.tensorflow+2Cmissing-method'></span>

<h3>Description</h3>

<p>The function <code>fft</code>  mimics the function <code>fft</code> of the library 'stats' to compute on gpu.matrix-class objects: it &quot;Computes the Discrete Fourier Transform (DFT) of an array with a fast algorithm, the 'Fast Fourier Transform' (FFT).&quot;
</p>
<p>The function <code>mvfft</code> mimics the function <code>mvfft</code> of the library 'stats' which: &quot;takes a real or complex matrix as argument, and returns a similar shaped matrix, but with each column replaced by its discrete Fourier transform&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow'
fft(z)
## S4 method for signature 'gpu.matrix.torch'
fft(z)
## S4 method for signature 'gpu.matrix.torch,logical'
fft(z,inverse)

## S4 method for signature 'gpu.matrix.torch'
mvfft(z)
## S4 method for signature 'gpu.matrix.tensorflow'
mvfft(z)
## S4 method for signature 'gpu.matrix.torch,logical'
mvfft(z,inverse)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fft_+3A_z">z</code></td>
<td>
<p>a  <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> object containing the values to be transformed.</p>
</td></tr>
<tr><td><code id="fft_+3A_inverse">inverse</code></td>
<td>
<p>the same as in the library 'stats': &quot;if TRUE, the unnormalized inverse transform is computed (the inverse has a +in the exponent of <code class="reqn">e</code>, but here, we do not divide by <code>1/length(x)</code>)&quot;. By default is FALSE. Plea Note that this parameter only work for torch.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>fft</code> mimics the function <code><a href="stats.html#topic+fft">fft</a></code> to operate on gpu.matrix-class objects of one dimension. If the input gpu.matrix <code>z</code> has tow dimensions the function will not work, as the method for two dimensions is not implemented yet for gpu.matrix-class objects. In this case the function will display the following error message: &quot;FFT in gpu.matrix with 2 dimensions is not allowed yet&quot;.
</p>
<p>The function <code>mvfft</code> mimics the function <code><a href="stats.html#topic+mvfft">mvfft</a></code> to operate on gpu.matrix-class objects. This function will apply the discrete Fourier transform to each column of the input <code>z</code> matrix.
</p>
<p>Note that the <code>inverse</code> parameter only works for 'torch' for both <code>fft</code> and <code>mvfft</code> functions.
</p>
<p>The functions <code>fft</code> and <code>mvfft</code> internally call the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
</p>
<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>It returns a gpu.matrix-class object with the transformed values. To access the real and imaginary information use the function <code>Re()</code> for teh rea part and <code>Im()</code> for the imaginary part. Furthermore, the following code can be used:
<code>output@gm$real</code> for the real part and <code>output@gm$imag</code> for the imaginary part.
</p>


<h3>See Also</h3>

<p>For more information see:
<code><a href="stats.html#topic+fft">fft</a></code>, <code><a href="torch.html#topic+torch_fft_ifft">torch_fft_ifft</a></code>, and <code><a href="torch.html#topic+torch_fft_fft">torch_fft_fft</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(installTorch()){

x &lt;- gpu.matrix(1:4,ncol = 1)
output_gpu &lt;- fft(x)
output_matrix &lt;- fft(z = as.matrix(x))

#check results:
Re(output_gpu)
Re(output_matrix)
Im(output_gpu)
Im(output_matrix)


x &lt;- gpu.matrix(1:12,ncol = 3)
output_gpu &lt;- mvfft(x)
output_matrix &lt;- mvfft(as.matrix(x))

#check results:
Re(output_gpu)
Re(output_matrix)
Im(output_gpu)
Im(output_matrix)
}


</code></pre>

<hr>
<h2 id='gpu.matrix'>create and store a matrix in the GPU</h2><span id='topic+gpu.matrix'></span><span id='topic+as.gpu.matrix'></span><span id='topic+as.gpu.matrix-methods'></span><span id='topic+as.gpu.matrix+2CANY-method'></span>

<h3>Description</h3>

<p>Mimic the base <code>'matrix'</code> function to create a gpu.matrix-class object, that could be of class <code>gpu.matrix.torch</code> or <code>gpu.matrix.tensorflow</code> depending on the system installed in the computer.
</p>
<p>The matrix created will be stored in the GPU (by default) or in the CPU. The example section explains how to be sure where the matrix is stored.
</p>
<p>This function also mimics the function <code>Matrix</code> of the library 'Matrix'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpu.matrix(data = NULL, nrow = NULL, ncol = NULL,
           byrow = FALSE,dimnames = NULL,
           dtype=NULL, sparse=NULL,colnames=c(),
           rownames=c(),device=NULL,type=NULL)

as.gpu.matrix(x,...)
## S4 method for signature 'ANY'
as.gpu.matrix(x,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gpu.matrix_+3A_data">data</code>, <code id="gpu.matrix_+3A_x">x</code></td>
<td>

<p>a scalar, vector or matrix (both matrix or Matrix class).
</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_nrow">nrow</code></td>
<td>

<p>Number of rows of the matrix. By default the number of rows of data if data is an object of class matrix or Matrix.
</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_ncol">ncol</code></td>
<td>

<p>Number of columns of the matrix. By default the number of columns of data if data is an object of class matrix or Matrix.
</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_byrow">byrow</code></td>
<td>

<p>The same as  function <code>matrix</code>: &quot;logical. If FALSE (the default) the matrix is filled by columns, otherwise the matrix is filled by rows.&quot;
</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_dimnames">dimnames</code></td>
<td>

<p>The same as in  function <code>matrix</code>: &quot;A dimnames attribute for the matrix: NULL or a list of length 2 giving the row and column names respectively. An empty list is treated as NULL, and a list of length one as row names. The list can be named, and the list names will be used as names for the dimensions.&quot;
</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_dtype">dtype</code></td>
<td>

<p>data type. User can indicate &quot;float64&quot;, &quot;float32&quot; or &quot;int&quot; for &quot;int64&quot;. if not specified, dtype will correspond to the input data type.
</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_sparse">sparse</code></td>
<td>

<p>The same as in function <code>Matrix</code> of the library 'Matrix': &quot;logical or NULL, specifying if the result should be sparse or not. By default, it is made sparse when more than half of the entries are 0.&quot;
</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_colnames">colnames</code></td>
<td>

<p>A vector with the column names.
</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_rownames">rownames</code></td>
<td>

<p>A vector with the row names.
</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_type">type</code></td>
<td>
<p>If the gpu.matrix is 'torch' or &quot;tensorflow&quot;. If it is NULL, <code>gpu.matrix</code> will try to create a gpu.matrix.torch object.</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_device">device</code></td>
<td>
<p>It indicates the device to load cuda. If not indicated, 'device' will be set to 'cuda' if it is available.</p>
</td></tr>
<tr><td><code id="gpu.matrix_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>gpu.matrix</code> function mimics the <code>Matrix</code> function of the 'Matrix' library and the basic <code>matrix</code> function. If tensorflow and/or torch are properly installed and the <code>device</code> parameter is set to &quot;cuda&quot; (by default), then the created gpu.matrix object will be stored on the GPU. The example shows how to check this.
</p>
<p>The user can apply to the created gpu.matrix-class object -using the same operators- the basic functions that can be applied to a object of class 'matrix' and/or class 'Matrix'.
</p>
<p>It can also work with sparse matrices as the 'Matrix' library.
</p>


<h3>Value</h3>

<p>Returns a GPUmatrix object that can be either &quot;gpu.matrix.tensorflow&quot; or &quot;gpu.matrix.torch&quot;. For both torch and tensorflow the functions to be applied to a matrix are the same.
</p>
<p>If the gpu.matrix-class object is not sparse it will show on the console the matrix as it is. If the gpu.matrix is sparse, it will return to the console the position where there are number different from zero. The internal values of the matrix can be seen using the operator &quot;@&quot;.
</p>
<p>If the gpu.matrix-class object contains complex numbers, to access the real and imaginary information use the function <code>Re()</code> for teh rea part and <code>Im()</code> for the imaginary part. Furthermore, the following code can be used:
<code>output@gm$real</code> for the real part and <code>output@gm$imag</code> for the imaginary part.
</p>
<p>Even if the gpu.matrix-class object is sparse or not, both kind of matrices works equally with all functions.
</p>


<h3>Author(s)</h3>

<p>Cesar Lobato and Angel Rubio.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>, <code><a href="Matrix.html#topic+Matrix-class">Matrix</a></code>, and <code><a href="methods.html#topic+matrix-class">matrix</a></code>.
</p>
<p>For more details about the parameter <code>dtype</code> visit <code><a href="#topic+dtype">dtype</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Not run: 
## create a gpu.matrix.torch and check it is stored in the GPU.
a &lt;- gpu.matrix(1:9,nrow=3,ncol=3)
class(a)
a@gm$is_cuda

# the output of class(a) should be:
#[1] "gpu.matrix.torch"
#attr(,"package")
#[1] "GPUmatrix"

#the output of a@gm@device should have a similar shape:
#[1] TRUE

## create a gpu.matrix.torch and check it is stored in the CPU.
a &lt;- gpu.matrix(1:9,nrow=3,ncol=3, device="cpu")
class(a)
a@gm$is_cuda

# the output of class(a) should be:
#[1] "gpu.matrix.torch"
#attr(,"package")
#[1] "GPUmatrix"

#the output of a@gm@device should have a similar shape:
#[1] FALSE

## create a gpu.matrix.tensorflow and check it is stored in the GPU.
a &lt;- gpu.matrix(1:9,nrow=3,ncol=3,type="tensorflow")
class(a)
a@gm$device

# the output of class(a) should be:
#[1] "gpu.matrix.tensorflow"
#attr(,"package")
#[1] "GPUmatrix"

#the output of a@gm@device should have a similar shape:
#[1] "/job:localhost/replica:0/task:0/device:GPU:0"

#create a sparse
a &lt;- gpu.matrix(data=c(0,1,1,0,1,0),nrow = 3,ncol = 2,sparse = T)
a

#create a complex gpu.matrix
a &lt;- gpu.matrix(data=c(0+1i,1i,1,0,1,0),nrow = 3,ncol = 2)
a





## End(Not run)

</code></pre>

<hr>
<h2 id='gpu.matrix-class'>Class 'gpu.matrix' for matrix stored in GPU</h2><span id='topic+gpu.matrix-class'></span>

<h3>Description</h3>

<p>GPU computational power is a great resource for computational biology specifically in statistics and linear algebra. the gpu.matrix-class is a class of the GPUmatrix package, that store a matrix in the GPU.
</p>
<p>The GPUmatrix package is based on S4 objects in R and we have created a constructor function that acts similarly to the default <code>matrix</code> constructor in R for CPU matrices. The constructor function is <code><a href="#topic+gpu.matrix">gpu.matrix</a></code> and accepts the same parameters as <code>matrix</code>.
</p>


<h3>Slots</h3>

<p> Use the <code>@</code> operator to access the different slots:
</p>

<dl>
<dt>rownames</dt><dd><p>the row names of the gpu.matrix</p>
</dd>
<dt>colnames</dt><dd><p>the colunm names of the gpu.matrix</p>
</dd>
<dt>gm</dt><dd><p>the corresponding tensor</p>
</dd>
<dt>sparse</dt><dd><p>Logical: indicates if the gpu.matrix is sparse or not</p>
</dd>
<dt>type</dt><dd><p>If it is tensorflow or torch</p>
</dd>
</dl>



<h3>See Also</h3>

<p>See Also <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>, <code><a href="Matrix.html#topic+Matrix-class">Matrix</a></code>, and <code><a href="methods.html#topic+matrix-class">matrix</a></code>..
</p>

<hr>
<h2 id='GPUglm'>Fitting Generalized Linear Models using GPUmatrix objects</h2><span id='topic+GPUglm'></span><span id='topic+glm.fit.GPU'></span>

<h3>Description</h3>

<p>These functions mimic the functions <code>speedglm</code> and <code>speedglm.wfit</code> of the library 'speedglm' to compute on gpu.matrix-class objects. At the same time, these functions mimic the functions <code>glm</code>, and <code>glm.fit</code> from the library 'stats' to compute on large data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm.fit.GPU(x, y, intercept = TRUE, weights = NULL, family =
                   gaussian(), start = NULL, etastart = NULL, mustart =
                   NULL, offset = NULL, acc = 1e-08, maxit = 25, k = 2,
                   sparse = NULL, trace = FALSE, dtype = "float64", device =
                   NULL, type = NULL, ...)

GPUglm(...)
</code></pre>


<h3>Arguments</h3>

<p>As mentioned in the description, these functions mimic <code><a href="speedglm.html#topic+speedglm">speedglm</a></code>, so almost every parameter does too. There is only three new parameters explained below.
</p>
<p>The common parameters with <code>speedglm</code>:
</p>
<table role = "presentation">
<tr><td><code id="GPUglm_+3A_x">x</code></td>
<td>
<p>the same as <code>speedglm</code>: the design matrix of dimension <code>n*p</code> where <code>n</code> is the number of observations and <code>p</code> is the number of features. <code>x</code> can be either a 'matrix', 'Matrix' or 'gpu.matrix-class' object.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_y">y</code></td>
<td>
<p>the same as <code>speedglm</code>: a vector of <code>n</code> observations. <code>y</code> can be either a 'matrix', 'Matrix' or 'gpu.matrix-class' object.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_intercept">intercept</code></td>
<td>
<p>the same as <code>speedglm</code>: Logical. If first column of <code>x</code> should be consider as 'intercept' (default) or not. Notice that seting this parameter TRUE or FALSE will not change the design matrix used to fit the model.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_weights">weights</code></td>
<td>
<p>the same as <code>speedglm</code>: an optional vector of prior weights to be used in the fitting process. Should be NULL (default) or a numeric vector.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_family">family</code></td>
<td>
<p>the same as <code>speedglm</code>: a description of the error distribution and link function to be used in the model. For <code>glm.fit.GPU</code> this can be a character string naming a family function, a family function or the result of a call to a family function. (See <code><a href="stats.html#topic+family">family</a></code> for details of family functions.)</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_start">start</code></td>
<td>
<p>the same as <code>speedglm</code>: starting values for the parameters in the linear prediction.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_etastart">etastart</code></td>
<td>
<p>the same as <code>speedglm</code>: starting values for the linear predictor.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_mustart">mustart</code></td>
<td>
<p>the same as <code>speedglm</code>: starting values for the vector of means.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_offset">offset</code></td>
<td>
<p>the same as <code>speedglm</code>: this can be used to specify an a priori known component to be included in the linear predictor during fitting. This should be NULL or a numeric vector of length equal to the number of cases. One or more <code><a href="stats.html#topic+offset">offset</a></code> terms can be included in the formula instead or as well, and if more than one is specified their sum is used. See <code><a href="stats.html#topic+model.offset">model.offset</a></code>.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_acc">acc</code></td>
<td>
<p>the same as <code>speedglm</code>: tolerance to be used for the estimation (by default equal to: 1e-08).</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_maxit">maxit</code></td>
<td>
<p>the same as <code>speedglm</code>: maximum number of iterations.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_k">k</code></td>
<td>
<p>the same as <code>speedglm</code>: numeric, the penalty per parameter to be used; the default k = 2 is the classical AIC.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_sparse">sparse</code></td>
<td>
<p>if matrix <code>x</code> is desired to be treated as sparse. Not yet implemented.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_trace">trace</code></td>
<td>
<p>If the user wants to see the development of the iterations. By default FALSE</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_...">...</code></td>
<td>
<p>For <code>GPUglm</code>: arguments to be used to form the default control argument if it is not supplied directly.</p>
</td></tr>
</table>
<p>The <code>glm.fit.GPU</code> function internally initialises matrices of the 'GPUmatrix' class by calling the <code><a href="#topic+gpu.matrix">gpu.matrix</a></code> function. The following parameters correspond to this function:
</p>
<table role = "presentation">
<tr><td><code id="GPUglm_+3A_dtype">dtype</code></td>
<td>
<p>parameter of the function <code>gpu.matrix</code>: &quot;data type. User can indicate &quot;float64&quot;, &quot;float32&quot; or &quot;int&quot; for &quot;int64&quot;.&quot; By default it is set to 'float64'.</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_device">device</code></td>
<td>
<p>parameter of the function <code>gpu.matrix</code>:&quot;It indicates the device to load cuda. If not indicated, 'device' will be set to 'cuda' if it is available.&quot;</p>
</td></tr>
<tr><td><code id="GPUglm_+3A_type">type</code></td>
<td>
<p>parameter of the function <code>gpu.matrix</code>: &quot;If gpu.matrix is 'torch' (by default if type is NULL) or &quot;tensorflow&quot;.&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>GPUglm</code> function internally calls the <code>glm</code> function by selecting <code>glm.fit.GPU</code> as the method. The input parameters of the <code>GPUglm</code> function are equivalent to those of the <code>glm</code> function.
</p>
<p>If the gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>Both <code>glmGPU</code>, and <code>glm.fit.GPU</code> returns an object of class &quot;GPUglm&quot;. This object can be treated as a list. This object mimics the output of the function <code><a href="speedglm.html#topic+speedglm">speedglm</a></code>:
</p>
<table role = "presentation">
<tr><td><code>coefficients</code></td>
<td>
<p>the estimated coefficients.</p>
</td></tr>
<tr><td><code>logLik</code></td>
<td>
<p>the log likelihood of the fitted model.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>the number of iterations of IWLS used.</p>
</td></tr>
<tr><td><code>tol</code></td>
<td>
<p>the maximal value of tolerance reached.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>the maximal value of tolerance reached.</p>
</td></tr>
<tr><td><code>link</code></td>
<td>
<p>the link function used.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>the degrees of freedom of the model.</p>
</td></tr>
<tr><td><code>XTX</code></td>
<td>
<p>the product X'X (weighted, if the case).</p>
</td></tr>
<tr><td><code>dispersion</code></td>
<td>
<p>the estimated dispersion parameter of the model.</p>
</td></tr>
<tr><td><code>ok</code></td>
<td>
<p>the set of column indeces of the model matrix where the model has been fitted.</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>the rank of the model matrix.</p>
</td></tr>
<tr><td><code>RSS</code></td>
<td>
<p>the estimated residual sum of squares of the fitted model.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>TODO</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>the estimated Akaike Information Criterion.</p>
</td></tr>
<tr><td><code>offset</code></td>
<td>
<p>he model offset.</p>
</td></tr>
<tr><td><code>sparse</code></td>
<td>
<p>a logical value which indicates if the model matrix is sparse.</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>the estimated deviance of the fitted model.</p>
</td></tr>
<tr><td><code>nulldf</code></td>
<td>
<p>the degrees of freedom of the null model.</p>
</td></tr>
<tr><td><code>nulldev</code></td>
<td>
<p>the estimated deviance of the null model.</p>
</td></tr>
<tr><td><code>ngoodobs</code></td>
<td>
<p>the number of non-zero weighted observations.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the number of observations.</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>a logical value which indicates if an intercept has been used.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>a logical value which indicates if convergence was reached.</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p>the terms object used.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>xlevels</code></td>
<td>
<p>(where relevant) a record of the levels of the factors used in fitting.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>See also: <code><a href="speedglm.html#topic+speedglm">speedglm</a></code> and <code><a href="stats.html#topic+glm">glm</a></code>.
</p>
<p>Also of interest may be the function <code><a href="#topic+LR_GradientConjugate_gpumatrix">LR_GradientConjugate_gpumatrix</a></code> for logistic regression.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Not run: 
require(MASS,quietly = TRUE)
require(stats,quietly = TRUE)

# linear model (example taken from 'glm'):

utils::data(anorexia, package = "MASS")
anorex_glm &lt;- glm(Postwt ~ Prewt + Treat + offset(Prewt),
                  family = gaussian(), data = anorexia)
summary(anorex_glm)

#Using GPUglm:
anorex_GPUglm &lt;- GPUglm(Postwt ~ Prewt + Treat + offset(Prewt),
                        family = gaussian, data = anorexia)
summary(anorex_GPUglm)

#linear model using glm.fit.gpu
x &lt;- model.matrix(~Treat+Prewt,data=anorexia)
y &lt;- as.matrix(anorexia$Postwt)
s1_glm &lt;- glm.fit(x=x,y=y)
s1_gpu &lt;- glm.fit.GPU(x=x,y=y)

s1_glm$coefficients
s1_gpu$coefficients


# poisson (example taken from 'glm'):
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson())
summary(glm.D93)

gpu.glm.D93 &lt;- GPUglm(counts ~ outcome + treatment, family = poisson())
summary(gpu.glm.D93)

#logistic:
data(menarche)
glm.out &lt;- glm(cbind(Menarche, Total-Menarche) ~ Age, family=binomial(), data=menarche)
summary(glm.out)

glm.out_gpu &lt;- GPUglm(cbind(Menarche, Total-Menarche) ~ Age, family=binomial(), data=menarche)
summary(glm.out_gpu)

#can be also called using glm.fit.gpu:
new_menarche &lt;- data.frame(Age=rep(menarche$Age,menarche$Total))
observations &lt;- c()
for(i in 1:nrow(menarche)){
  observations &lt;- c(observations,rep(c(0,1),c(menarche$Total[i]-menarche$Menarche[i],
                                              menarche$Menarche[i])))
}
new_menarche$observations &lt;- observations
x &lt;- model.matrix(~Age,data=new_menarche)
head(new_menarche)
glm.fit_gpu &lt;- glm.fit.GPU(x=x,y=new_menarche$observations, family=binomial())
summary(glm.fit_gpu)

#GPUmatrix package also include the function 'LR_GradientConjugate_gpumatrix'
lr_gran_sol &lt;- LR_GradientConjugate_gpumatrix(X = x,y = observations)

#check results
glm.out$coefficients
glm.out_gpu$coefficients
glm.fit_gpu$coefficients
lr_gran_sol

## End(Not run)


</code></pre>

<hr>
<h2 id='installTorch'>
installTorch
</h2><span id='topic+installTorch'></span>

<h3>Description</h3>

<p>This function checks that the torch package is installed correctly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>installTorch</code></pre>

<hr>
<h2 id='kroneker'>kroneker Products</h2><span id='topic++25x+25'></span><span id='topic++25x+25-methods'></span><span id='topic++25x+25+2CANY+2Cgpu.matrix.tensorflow-method'></span><span id='topic++25x+25+2CANY+2Cgpu.matrix.torch-method'></span><span id='topic++25x+25+2Cgpu.matrix.tensorflow+2CANY-method'></span><span id='topic++25x+25+2Cgpu.matrix.torch+2CANY-method'></span>

<h3>Description</h3>

<p>Kroneker product of two gpu.matrix-class objects. This function mimics the 'base' function <code>'kronecker'</code> to operate on gpu.matrix-class objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'ANY,gpu.matrix.tensorflow'
X %x% Y
## S4 method for signature 'ANY,gpu.matrix.torch'
X %x% Y
## S4 method for signature 'gpu.matrix.tensorflow,ANY'
X %x% Y
## S4 method for signature 'gpu.matrix.torch,ANY'
X %x% Y
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kroneker_+3A_x">X</code></td>
<td>
<p>A <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="kroneker_+3A_y">Y</code></td>
<td>
<p>A <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> or a matrix or a numeric variable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>%x%</code> internally calls the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
</p>
<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>See Also</h3>

<p>See Also <code><a href="base.html#topic+kronecker">kronecker</a></code> and <code><a href="torch.html#topic+torch_kron">torch_kron</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Not run: 

a &lt;- gpu.matrix(1:9,nrow=3,ncol=3)
a %x% diag(1,3)



## End(Not run)


</code></pre>

<hr>
<h2 id='LR_GradientConjugate_gpumatrix'>Logistic Regression with Conjugate Gradient method</h2><span id='topic+LR_GradientConjugate_gpumatrix'></span>

<h3>Description</h3>

<p>The developed function performs the logistic regression using the Conjugate Gradient method. This method has shown to be very effective for logistic regression of big models  [1]. The code is general enough to accommodate standard R matrices, sparse matrices from the 'Matrix' package and, more interestingly, gpu.matrix-class objects from the GPUmatrix package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LR_GradientConjugate_gpumatrix(X, y, beta = NULL,
                               lambda = 0, iterations = 100,
                               tol = 1e-08)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="LR_GradientConjugate_gpumatrix_+3A_x">X</code></td>
<td>
<p>the design matrix. Could be either a object of class <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>, <code><a href="methods.html#topic+matrix-class">matrix</a></code>, or  <code><a href="Matrix.html#topic+Matrix-class">Matrix</a></code>.</p>
</td></tr>
<tr><td><code id="LR_GradientConjugate_gpumatrix_+3A_y">y</code></td>
<td>
<p>vector of observations.</p>
</td></tr>
<tr><td><code id="LR_GradientConjugate_gpumatrix_+3A_beta">beta</code></td>
<td>
<p>initial solution.</p>
</td></tr>
<tr><td><code id="LR_GradientConjugate_gpumatrix_+3A_lambda">lambda</code></td>
<td>
<p>numeric. Penalty factor also known as the L2 norm or L2 penalty, which is computed as the sum of the squared coefficients: <code class="reqn">\lambda||\beta_i||_2^2</code></p>
</td></tr>
<tr><td><code id="LR_GradientConjugate_gpumatrix_+3A_iterations">iterations</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="LR_GradientConjugate_gpumatrix_+3A_tol">tol</code></td>
<td>
<p>tolerance to be used for the estimation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>The function returns a vector containing the values of the coefficients. This returned vector will be a 'matrix', 'Matrix' or 'gpu.matrix-class' object depending on the class of the object <code>X</code>.
</p>


<h3>Author(s)</h3>

<p>Angel Rubio and Cesar Lobato.
</p>


<h3>References</h3>

<p>[1] Minka TP (2003). A comparison of numerical optimizers for logistic regression. URL: https://tminka.github.io/papers/logreg/minka-logreg.pdf.
</p>


<h3>See Also</h3>

<p>See also:  <code><a href="#topic+GPUglm">GPUglm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

#toy example:
set.seed(123)
m &lt;- 1000
n &lt;- 100
x &lt;- matrix(runif(m*n),m,n)
sol &lt;- rnorm(n)
y &lt;- rbinom(m, 1, prob = plogis(x%*%sol))
s2_granConj &lt;- LR_GradientConjugate_gpumatrix(X = x,y = y)

#the following compares LR_GradientConjugate_gpumatrix
# with glm and GPUglm:

require(MASS)
require(stats,quietly = TRUE)
#logistic:
data(menarche)
glm.out &lt;- glm(cbind(Menarche, Total-Menarche) ~ Age, family=binomial(), data=menarche)
summary(glm.out)

glm.out_gpu &lt;- GPUglm(cbind(Menarche, Total-Menarche) ~ Age, family=binomial(), data=menarche)
summary(glm.out_gpu)

#can be also called using glm.fit.gpu:
new_menarche &lt;- data.frame(Age=rep(menarche$Age,menarche$Total))
observations &lt;- c()
for(i in 1:nrow(menarche)){
  observations &lt;- c(observations,rep(c(0,1),c(menarche$Total[i]-menarche$Menarche[i],
                                              menarche$Menarche[i])))
}
new_menarche$observations &lt;- observations
x &lt;- model.matrix(~Age,data=new_menarche)
head(new_menarche)
glm.fit_gpu &lt;- glm.fit.GPU(x=x,y=new_menarche$observations, family=binomial())
summary(glm.fit_gpu)

#GPUmatrix package also include the function 'LR_GradientConjugate_gpumatrix'
lr_gran_sol &lt;- LR_GradientConjugate_gpumatrix(X = x,y = observations)

#check results
glm.out$coefficients
glm.out_gpu$coefficients
glm.fit_gpu$coefficients
lr_gran_sol


## End(Not run)

</code></pre>

<hr>
<h2 id='matrix_decomposition'>Decomposition of a matrix with GPU</h2><span id='topic+eigen'></span><span id='topic+eigen-methods'></span><span id='topic+eigen+2Cgpu.matrix.tensorflow-method'></span><span id='topic+eigen+2Cgpu.matrix.torch-method'></span><span id='topic+svd'></span><span id='topic+svd-methods'></span><span id='topic+svd+2Cgpu.matrix.tensorflow-method'></span><span id='topic+svd+2Cgpu.matrix.torch-method'></span><span id='topic+chol'></span><span id='topic+chol-methods'></span><span id='topic+chol+2Cgpu.matrix.tensorflow-method'></span><span id='topic+chol+2Cgpu.matrix.torch-method'></span>

<h3>Description</h3>

<p>These functions mimic the functions <code>eigen</code> ,<code>svd</code>,<code>chol</code> to operate on gpu.matrix-class objects:
</p>
<p><code>'eigen'</code> mimics the base function <code>'eigen'</code> that &quot;computes the eigenvalues and eigenvectors of a numeric (double, integer, logical) or complex matrix.&quot;
</p>
<p><code>'svd'</code> mimics the base function <code>'svd'</code> that &quot;computes the singular-value decomposition of a rectangular matrix.&quot;
</p>
<p><code>'chol'</code> mimics the base function <code>'chol'</code> that &quot;computes Compute the Cholesky factorization of a real symmetric positive-definite square matrix.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'gpu.matrix.tensorflow'
eigen(x)
## S4 method for signature 'gpu.matrix.torch'
eigen(x)

## S4 method for signature 'gpu.matrix.tensorflow'
svd(x)
## S4 method for signature 'gpu.matrix.torch'
svd(x)

## S4 method for signature 'gpu.matrix.tensorflow'
chol(x)
## S4 method for signature 'gpu.matrix.torch'
chol(x)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matrix_decomposition_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>. <code>X</code> must fulfil certain characteristics depending on the function to be called (see details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions mimic the behaviour of their respective 'base' functions.
</p>
<p>In the case of the <code>eigen</code> function, the input value can be a numeric or complex gpu.matrix class.
</p>
<p>For <code>svd</code> function, the input value could be a numeric or complex gpu.matrix-class object.
</p>
<p>For <code>chol</code> function, the input must be a positive-definite squere matrix.
</p>
<p>Internally, these functions call its corresponding function of the tensorflow or torch library depending on the type of input gpu.matrix-class.
</p>
<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>The output of these functions correspond to their equivalent base functions:
</p>
<p><code>eigen</code> mimics the base function <code>eigen</code> that computes the eigenvalues and eigenvectors of a numeric (double, integer, logical) or complex matrix. It returns a list with the following items:
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>a vector with the <code>P</code> eigenvalues of <code>x</code></p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>
<p>the eigenvectors of <code>x</code></p>
</td></tr>
</table>
<p><code>svd</code> mimics the base function <code>svd</code> that computes the singular-value decomposition of a rectangular matrix. It returns a list with the following items:
</p>
<table role = "presentation">
<tr><td><code>d</code></td>
<td>
<p>a vector containing the singular values of <code>x</code></p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a matrix whose columns contain the left singular vectors of <code>x</code></p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a matrix whose columns contain the right singular vectors of <code>x</code></p>
</td></tr>
</table>
<p><code>chol</code> mimics the base function <code>chol</code> that computes Compute the Cholesky factorization of a real symmetric positive-definite square matrix. It returns a gpu.matrix-class object with The upper triangular factor of the Cholesky decomposition, i.e., the matrix <code class="reqn">R</code> such that <code class="reqn">R'R = X</code>.
</p>


<h3>See Also</h3>

<p>For more information see:
<code><a href="base.html#topic+eigen">eigen</a></code>, <code><a href="base.html#topic+svd">svd</a></code>, <code><a href="base.html#topic+chol">chol</a></code>, <code><a href="torch.html#topic+linalg_eig">linalg_eig</a></code>, <code><a href="torch.html#topic+torch_svd">torch_svd</a></code>, and <code><a href="torch.html#topic+torch_cholesky">torch_cholesky</a></code>.
</p>
<p><code>chol</code> function is called by the function <code><a href="#topic+chol_solve">chol_solve</a></code>.
</p>
<p>For the <code>qr</code> decomposition see <code><a href="#topic+qr">qr</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
a &lt;- gpu.matrix(rnorm(9),3,3)
ein &lt;- eigen(a) #eigenvalues and eigenvectors
svd_return &lt;- svd(a) #svd of gpu.matrix a

ata &lt;- tcrossprod(a)
#ata is a real symmetric positive-definite square matrix.
chol(ata) #cholesky decomposition.



## End(Not run)

</code></pre>

<hr>
<h2 id='matrix_general_operators_methods'>Return the first or last part of a GPUmatrix object</h2><span id='topic+tail'></span><span id='topic+show'></span><span id='topic+head'></span><span id='topic+tail-methods'></span><span id='topic+show-methods'></span><span id='topic+head-methods'></span><span id='topic+tail+2Cgpu.matrix.tensorflow-method'></span><span id='topic+tail+2Cgpu.matrix.torch-method'></span><span id='topic+show+2Cgpu.matrix.tensorflow-method'></span><span id='topic+show+2Cgpu.matrix.torch-method'></span><span id='topic+head+2Cgpu.matrix.tensorflow-method'></span><span id='topic+head+2Cgpu.matrix.torch-method'></span><span id='topic+print'></span><span id='topic+print-methods'></span><span id='topic+print+2Cgpu.matrix.torch-method'></span>

<h3>Description</h3>

<p><code>head</code> and <code>tail</code> mimic the functions <code>head</code> and <code>tail</code> from <code>utils</code> to operate on gpu.matrix-class objects. By default <code>head</code> shows the first 6 rows of a matrix or first 6 elements of a vector or list. <code>tail</code> shows the last 6 rows of a matrix or last 6 elements of a vector or list.
</p>
<p>The function <code>show</code> mimics the function <code>show</code> of <code>methods</code> to compute on gpu.matrix-class objects: &quot;It display the object, by printing, plotting or whatever suits its class.&quot;
</p>
<p>The function <code>print</code> mimics the base function <code>print</code> to operate on gpu.matrix-class objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'gpu.matrix.tensorflow'
tail(x,...)
## S4 method for signature 'gpu.matrix.torch'
tail(x,...)
## S4 method for signature 'gpu.matrix.tensorflow'
show(object)
## S4 method for signature 'gpu.matrix.torch'
show(object)
## S4 method for signature 'gpu.matrix.tensorflow'
head(x,...)
## S4 method for signature 'gpu.matrix.torch'
head(x,...)

## S4 method for signature 'gpu.matrix.torch'
print(x)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matrix_general_operators_methods_+3A_x">x</code>, <code id="matrix_general_operators_methods_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="matrix_general_operators_methods_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from other methods.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>For more information see:
<code><a href="utils.html#topic+head">head</a></code>, <code><a href="utils.html#topic+tail">tail</a></code>, and <code><a href="methods.html#topic+show">show</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
a &lt;- gpu.matrix(rnorm(20*5),20,5)
head(a) #shows the first six row of every column
tail(a) #shows the las six row of every column

show(a) #show all the object
a #equivalente to run the function show.

## End(Not run)

</code></pre>

<hr>
<h2 id='matrix_ranges'>Get different statistics for a gpu.matrix-class.</h2><span id='topic+rowMaxs'></span><span id='topic+rowMaxs-methods'></span><span id='topic+rowMaxs+2Cgpu.matrix.tensorflow-method'></span><span id='topic+rowMaxs+2Cgpu.matrix.torch-method'></span><span id='topic+colMaxs'></span><span id='topic+colMaxs-methods'></span><span id='topic+colMaxs+2Cgpu.matrix.tensorflow-method'></span><span id='topic+colMaxs+2Cgpu.matrix.torch-method'></span><span id='topic+max-methods'></span><span id='topic+which.max-methods'></span><span id='topic+max+2Cgpu.matrix.tensorflow-method'></span><span id='topic+max+2Cgpu.matrix.torch-method'></span><span id='topic+which.max+2Cgpu.matrix.tensorflow-method'></span><span id='topic+which.max+2Cgpu.matrix.torch-method'></span><span id='topic+rowMins'></span><span id='topic+colMins'></span><span id='topic+min'></span><span id='topic+which.min'></span><span id='topic+rowMins-methods'></span><span id='topic+colMins-methods'></span><span id='topic+min-methods'></span><span id='topic+which.min-methods'></span><span id='topic+rowMins+2Cgpu.matrix.tensorflow-method'></span><span id='topic+rowMins+2Cgpu.matrix.torch-method'></span><span id='topic+colMins+2Cgpu.matrix.tensorflow-method'></span><span id='topic+colMins+2Cgpu.matrix.torch-method'></span><span id='topic+min+2Cgpu.matrix.tensorflow-method'></span><span id='topic+min+2Cgpu.matrix.torch-method'></span><span id='topic+which.min+2Cgpu.matrix.tensorflow-method'></span><span id='topic+which.min+2Cgpu.matrix.torch-method'></span><span id='topic+rowMeans'></span><span id='topic+colMeans'></span><span id='topic+mean'></span><span id='topic+rowMeans-methods'></span><span id='topic+colMeans-methods'></span><span id='topic+mean-methods'></span><span id='topic+rowMeans+2Cgpu.matrix.tensorflow-method'></span><span id='topic+rowMeans+2Cgpu.matrix.torch-method'></span><span id='topic+colMeans+2Cgpu.matrix.tensorflow-method'></span><span id='topic+colMeans+2Cgpu.matrix.torch-method'></span><span id='topic+mean+2Cgpu.matrix.tensorflow-method'></span><span id='topic+mean+2Cgpu.matrix.torch-method'></span><span id='topic+rowVars'></span><span id='topic+colVars'></span><span id='topic+rowVars-methods'></span><span id='topic+colVars-methods'></span><span id='topic+rowVars+2Cgpu.matrix.tensorflow-method'></span><span id='topic+rowVars+2Cgpu.matrix.torch-method'></span><span id='topic+colVars+2Cgpu.matrix.tensorflow-method'></span><span id='topic+colVars+2Cgpu.matrix.torch-method'></span><span id='topic+rowRanks'></span><span id='topic+colRanks'></span><span id='topic+rowRanks-methods'></span><span id='topic+colRanks-methods'></span><span id='topic+rowRanks+2Cgpu.matrix.tensorflow-method'></span><span id='topic+rowRanks+2Cgpu.matrix.torch-method'></span><span id='topic+colRanks+2Cgpu.matrix.tensorflow-method'></span><span id='topic+colRanks+2Cgpu.matrix.torch-method'></span><span id='topic+rowSums'></span><span id='topic+colSums'></span><span id='topic+sum'></span><span id='topic+rowSums-methods'></span><span id='topic+colSums-methods'></span><span id='topic+sum-methods'></span><span id='topic+rowSums+2Cgpu.matrix.tensorflow-method'></span><span id='topic+rowSums+2Cgpu.matrix.torch-method'></span><span id='topic+colSums+2Cgpu.matrix.tensorflow-method'></span><span id='topic+colSums+2Cgpu.matrix.torch-method'></span><span id='topic+sum+2Cgpu.matrix.tensorflow-method'></span><span id='topic+sum+2Cgpu.matrix.torch-method'></span>

<h3>Description</h3>

<p>Functions to summarise different values of a gpu.matrix-class object by rows or columns. Specifically: the maximum value, the index of the maximum value, the minimum value, the index of the minimum value, the mean, the variance, the sum of the values and the rank of the values.
</p>
<p>These functions mimic the corresponding function of <code>'base'</code>, <code>'matrixStats'</code> and <code>'Matrix'</code> libraries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow'
rowMaxs(x)
## S4 method for signature 'gpu.matrix.torch'
rowMaxs(x)
## S4 method for signature 'gpu.matrix.tensorflow'
colMaxs(x)
## S4 method for signature 'gpu.matrix.torch'
colMaxs(x)
## S4 method for signature 'gpu.matrix.tensorflow'
max(x)
## S4 method for signature 'gpu.matrix.torch'
max(x)

## S4 method for signature 'gpu.matrix.tensorflow'
rowMins(x)
## S4 method for signature 'gpu.matrix.torch'
rowMins(x)
## S4 method for signature 'gpu.matrix.tensorflow'
colMins(x)
## S4 method for signature 'gpu.matrix.torch'
colMins(x)
## S4 method for signature 'gpu.matrix.tensorflow'
min(x)
## S4 method for signature 'gpu.matrix.torch'
min(x)

## S4 method for signature 'gpu.matrix.tensorflow'
rowMeans(x)
## S4 method for signature 'gpu.matrix.torch'
rowMeans(x)
## S4 method for signature 'gpu.matrix.tensorflow'
colMeans(x)
## S4 method for signature 'gpu.matrix.torch'
colMeans(x)
## S4 method for signature 'gpu.matrix.tensorflow'
mean(x)
## S4 method for signature 'gpu.matrix.torch'
mean(x)

## S4 method for signature 'gpu.matrix.tensorflow'
rowVars(x)
## S4 method for signature 'gpu.matrix.torch'
rowVars(x)
## S4 method for signature 'gpu.matrix.tensorflow'
colVars(x)
## S4 method for signature 'gpu.matrix.torch'
colVars(x)

## S4 method for signature 'gpu.matrix.tensorflow'
rowRanks(x)
## S4 method for signature 'gpu.matrix.torch'
rowRanks(x)
## S4 method for signature 'gpu.matrix.tensorflow'
colRanks(x)
## S4 method for signature 'gpu.matrix.torch'
colRanks(x)


## S4 method for signature 'gpu.matrix.tensorflow'
rowSums(x)
## S4 method for signature 'gpu.matrix.torch'
rowSums(x)
## S4 method for signature 'gpu.matrix.tensorflow'
colSums(x)
## S4 method for signature 'gpu.matrix.torch'
colSums(x)
## S4 method for signature 'gpu.matrix.tensorflow'
sum(x)
## S4 method for signature 'gpu.matrix.torch'
sum(x)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matrix_ranges_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The value returned by almost each function is a numeric vector stored in the CPU. Only the function <code>rowRanks</code>, <code>colRanks</code>, and <code>sum</code> return a gpu.matrix-class object.
</p>
<p>These functions internally calls the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
If the input gpu.matrix-class object is stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p><code>max</code>, <code>rowMaxs</code>, <code>colMaxs</code> calculate the maximum value of a gpu.matrix-class object, of each row and of each column respectively. <code>which.max</code> determines the location of the maximum value.
</p>
<p><code>min</code>, <code>rowMins</code>, <code>colMins</code> calculate the minimum value of a gpu.matrix-class object, of each row and of each column respectively. <code>which.min</code> determines the location of the minimum value.
</p>
<p><code>mean</code>, <code>rowMeans</code>, <code>colMeans</code> calculate the mean (average) value of a gpu.matrix-class object, of each row and of each column respectively.
</p>
<p><code>rowVars</code>, <code>colVars</code> calculate the variance of each row and of each column of a gpu.matrix-class object respectively.
</p>
<p><code>rowRanks</code>, <code>colRanks</code>: given a gpu.matrix-class object, these functions return a gpu.matrix which rearranges each row and each column into ascending respectively.
</p>
<p><code>rowSums</code>, <code>colSums</code>, <code>sum</code> sum the value of a a gpu.matrix-class object, of each row and of each column respectively.
</p>


<h3>See Also</h3>

<p>For more information:
</p>
<p><code><a href="matrixStats.html#topic+rowMaxs">rowMaxs</a></code>,
<code><a href="matrixStats.html#topic+colMaxs">colMaxs</a></code>,
<code><a href="base.html#topic+max">max</a></code>,
<code><a href="base.html#topic+which.max">which.max</a></code>, and <code><a href="torch.html#topic+torch_max">torch_max</a></code>.
</p>
<p><code><a href="matrixStats.html#topic+rowMins">rowMins</a></code>,
<code><a href="matrixStats.html#topic+colMins">colMins</a></code>,
<code><a href="base.html#topic+min">min</a></code>,
<code><a href="base.html#topic+which.min">which.min</a></code>, and <code><a href="torch.html#topic+torch_min">torch_min</a></code>.
</p>
<p><code><a href="base.html#topic+rowMeans">rowMeans</a></code>,
<code><a href="base.html#topic+colMeans">colMeans</a></code>,
<code><a href="base.html#topic+mean">mean</a></code>, and <code><a href="torch.html#topic+torch_mean">torch_mean</a></code>.
</p>
<p><code><a href="matrixStats.html#topic+rowVars">rowVars</a></code>,
<code><a href="matrixStats.html#topic+colVars">colVars</a></code>, and <code><a href="torch.html#topic+torch_var">torch_var</a></code>.
</p>
<p><code><a href="matrixStats.html#topic+rowRanks">rowRanks</a></code>,
<code><a href="matrixStats.html#topic+colRanks">colRanks</a></code>, and <code><a href="torch.html#topic+torch_argsort">torch_argsort</a></code>.
</p>
<p><code><a href="base.html#topic+rowSums">rowSums</a></code>,
<code><a href="base.html#topic+colSums">colSums</a></code>,
<code><a href="base.html#topic+sum">sum</a></code>, and <code><a href="torch.html#topic+torch_sum">torch_sum</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
a &lt;- gpu.matrix(rnorm(9),3,3)

#the maximum value of a:
max(a)

#maximum of value in each row of a:
rowMaxs(a)

#maximum value in each column of a:
colMaxs(a)

#index of the maximum value of a:
which.max(a)

#minimum value of a:
min(a)

#minimum value in each row of a:
rowMins(a)

#minimum value in each column of a:
colMins(a)

#index of the minimum value in a:
which.min(a)

#mean of a:
mean(a)

#mean of each row of a:
rowMeans(a)

#mean of each column of a:
colMeans(a)

#variance of each row of a:
rowVars(a)

#variance of each column of a:
colVars(a)

#sum of all values of a:
sum(a)

#sum of each fow of a:
rowSums(a)

#sum of each column of a:
colSums(a)

#ranking of each row of a:
rowRanks(a)

#ranking of each columna of a:
colRanks(a)

## End(Not run)

</code></pre>

<hr>
<h2 id='matrix-product'>Matrix Products</h2><span id='topic++25+2A+25'></span><span id='topic++25+2A+25-methods'></span><span id='topic++25+2A+25+2CANY+2Cgpu.matrix.tensorflow-method'></span><span id='topic++25+2A+25+2CANY+2Cgpu.matrix.torch-method'></span><span id='topic++25+2A+25+2Cgpu.matrix.tensorflow+2CANY-method'></span><span id='topic++25+2A+25+2Cgpu.matrix.torch+2CANY-method'></span><span id='topic+crossprod'></span><span id='topic+crossprod-methods'></span><span id='topic+crossprod+2CANY+2Cgpu.matrix.tensorflow-method'></span><span id='topic+crossprod+2Cgpu.matrix.tensorflow+2CANY-method'></span><span id='topic+crossprod+2Cgpu.matrix.tensorflow+2Cmissing-method'></span><span id='topic+crossprod+2CANY+2Cgpu.matrix.torch-method'></span><span id='topic+crossprod+2Cgpu.matrix.torch+2CANY-method'></span><span id='topic+crossprod+2Cgpu.matrix.torch+2Cmissing-method'></span><span id='topic+tcrossprod'></span><span id='topic+tcrossprod-methods'></span><span id='topic+tcrossprod+2CANY+2Cgpu.matrix.tensorflow-method'></span><span id='topic+tcrossprod+2Cgpu.matrix.tensorflow+2CANY-method'></span><span id='topic+tcrossprod+2Cgpu.matrix.tensorflow+2Cmissing-method'></span><span id='topic+tcrossprod+2CANY+2Cgpu.matrix.torch-method'></span><span id='topic+tcrossprod+2Cgpu.matrix.torch+2CANY-method'></span><span id='topic+tcrossprod+2Cgpu.matrix.torch+2Cmissing-method'></span>

<h3>Description</h3>

<p>Mimic of the 'base' functions <code>%*%</code>, <code>crossprod</code>, <code>tcrossprod</code> to operate on gpu.matrix-class objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow,ANY'
x %*% y
## S4 method for signature 'gpu.matrix.torch,ANY'
x %*% y

## S4 method for signature 'gpu.matrix.tensorflow,ANY'
crossprod(x, y,...)
## S4 method for signature 'gpu.matrix.tensorflow,missing'
crossprod(x, y = NULL,...)

## S4 method for signature 'gpu.matrix.tensorflow,ANY'
tcrossprod(x, y,...)
## S4 method for signature 'gpu.matrix.tensorflow,missing'
tcrossprod(x, y = NULL,...)


## S4 method for signature 'gpu.matrix.torch,ANY'
crossprod(x, y,...)
## S4 method for signature 'gpu.matrix.torch,missing'
crossprod(x, y = NULL,...)

## S4 method for signature 'gpu.matrix.torch,ANY'
tcrossprod(x, y,...)
## S4 method for signature 'gpu.matrix.torch,missing'
tcrossprod(x, y = NULL,...)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matrix-product_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="matrix-product_+3A_y">y</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>, 'matrix' or 'Matrix' object. For the functions <code>tcrossprod</code> and <code>crossprod</code> is NULL (by default), that is equivalent to <code>x=y</code>.</p>
</td></tr>
<tr><td><code id="matrix-product_+3A_...">...</code></td>
<td>
<p>potentially more arguments passed to and from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internally, these functions call the appropriate tensorflow or torch function to perform the matrix product (depending on the type of input gpu.matrix-class).
</p>
<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>A gpu.matrix-class object with the result of the matrix product.
</p>


<h3>Methods</h3>


<dl>
<dt>%*%</dt><dd><p><code>signature(x = "gpu.matrix.tensorflow", y = "ANY")</code>:
Matrix multiplication</p>
</dd>
<dt>crossprod</dt><dd><p><code>signature(x = "gpu.matrix.tensorflow", y = "ANY")</code>:
Matrix multiplication</p>
</dd>
<dt>crossprod</dt><dd><p><code>signature(x = "gpu.matrix.tensorflow", y = "missing")</code>:
Matrix multiplication</p>
</dd>
<dt>tcrossprod</dt><dd><p><code>signature(x = "gpu.matrix.tensorflow", y = "ANY")</code>:
Matrix multiplication</p>
</dd>
<dt>tcrossprod</dt><dd><p><code>signature(x = "gpu.matrix.tensorflow", y = "missing")</code>:
Matrix multiplication</p>
</dd>
<dt>%*%</dt><dd><p><code>signature(x = "gpu.matrix.torch", y = "ANY")</code>:
Matrix multiplication</p>
</dd>
<dt>crossprod</dt><dd><p><code>signature(x = "gpu.matrix.torch", y = "ANY")</code>:
Matrix multiplication</p>
</dd>
<dt>crossprod</dt><dd><p><code>signature(x = "gpu.matrix.torch", y = "missing")</code>:
Matrix multiplication</p>
</dd>
<dt>tcrossprod</dt><dd><p><code>signature(x = "gpu.matrix.torch", y = "ANY")</code>:
Matrix multiplication</p>
</dd>
<dt>tcrossprod</dt><dd><p><code>signature(x = "gpu.matrix.torch", y = "missing")</code>:
Matrix multiplication</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="base.html#topic+tcrossprod">tcrossprod</a></code> in <span class="rlang"><b>R</b></span>'s base, and
<code><a href="#topic+crossprod">crossprod</a></code> and <code><a href="#topic++25+2A+25">%*%</a></code>.
<span class="pkg">Matrix</span> package <code><a href="Matrix.html#topic++25+26+25">%&amp;%</a></code> for boolean matrix product
methods.
Also see <code><a href="torch.html#topic+torch_matmul">torch_matmul</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
a &lt;- gpu.matrix(rnorm(12),nrow=4,ncol=3)
b &lt;- t(a)
b
crossprod(a,a)

b &lt;- a
b
tcrossprod(a)



## End(Not run)

</code></pre>

<hr>
<h2 id='NMFgpumatrix'>Non negative factorization of a matrix</h2><span id='topic+NMFgpumatrix'></span>

<h3>Description</h3>

<p>The non-negative factorization (NMF) of a matrix is an approximate factorization were an initial matrix <code>V</code> is approximated by the product of two matrix <code>W</code> and <code>H</code> so that,
</p>
<p><code class="reqn">V \approx WH</code>
</p>
<p>This function operates in the same way with the 'base' <code>matrix</code> objects as with gpu.matrix-class objects, and it does not require any additional changes beyond initializing the input matrix as a gpu.matrix-class object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NMFgpumatrix(V, k = 10, Winit = NULL,
             Hinit = NULL, tol = 1e-06,
             niter = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NMFgpumatrix_+3A_v">V</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>. Values in <code>V</code> must be <code class="reqn">\geq 0</code>.</p>
</td></tr>
<tr><td><code id="NMFgpumatrix_+3A_k">k</code></td>
<td>
<p>The inner dimension of the product of the matrices W and H. That is, it corresponds to the number of columns in W and the number of rows in H.</p>
</td></tr>
<tr><td><code id="NMFgpumatrix_+3A_winit">Winit</code></td>
<td>
<p>Initial value for matrix W. Initial values for <code>W</code> must be <code class="reqn">\geq 0</code>.</p>
</td></tr>
<tr><td><code id="NMFgpumatrix_+3A_hinit">Hinit</code></td>
<td>
<p>Initial value for matrix H. Initial values for <code>H</code> must be <code class="reqn">\geq 0</code>.</p>
</td></tr>
<tr><td><code id="NMFgpumatrix_+3A_tol">tol</code></td>
<td>
<p>tolerance to be used for the estimation.</p>
</td></tr>
<tr><td><code id="NMFgpumatrix_+3A_niter">niter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We have implemented our own non-negative matrix factorization (NMF) function using Lee
and Seung[1] multiplicative update rule:
</p>
<p><code class="reqn">W_{[i,j]}^{n+1} \leftarrow  W_{[i,j]}^{n} \frac{(V(H^{n+1})^T)_{[i,j]}}{(W^nH^{n+1}(H^{n+1})^T)_{[i,j]}}</code>
</p>
<p>and
</p>
<p><code class="reqn">H_{[i,j]}^{n+1} \leftarrow  H_{[i,j]}^{n} \frac{((W^{n})^TV)_{[i,j]}}{((W^n)^TW^{n}H^{n})_{[i,j]}}</code>
</p>
<p>to update the <code class="reqn">W</code> and <code class="reqn">H</code> respectively.
</p>
<p>Note that the values of V must be positive. If any value of V is negative, it will be set to 0. If this happens, the following warning message will be displayed: &quot;The values of V must be positive. Negative values in V are set to 0.
</p>
<p>If the user decides to initialise the values of W and H, they must also be positive. If there are negative values, they will be set to 0. The following warning message will be displayed: &quot;The Winit values must be positive. Negative values in Winit are set to 0&quot; if Winit has negative values or &quot;The values of Hinit must be positive. Negative values in Hinit are set to 0&quot; if Winit has negative values.
</p>
<p>In addition, Winit and Hinit must also have the correct dimensions. Winit must fulfil two conditions: <code>nrow(Winit) == nrow(V)</code> and  <code>ncol(Winit) == k</code>. If not, the function will stop with the following error message: &quot;The dimensions of the Winit matrix are incorrect. Please check that <code>nrow(Winit) == nrow(V)</code> and that <code>ncol(Winit) == k</code>&quot;. On the other hand, Hinit must fulfil two conditions: <code>nrow(Winit) == nrow(V)</code> and that <code>ncol(Winit) == k</code>. If not, the function will stop with the following error message: &quot;The dimensions of the Winit matrix are incorrect. Please check that <code>nrow(Winit) == nrow(V)</code> and that <code>ncol(Winit) == k</code>&quot;.
</p>
<p>If the input gpu.matrix-class object is stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>The function returns a list that contains the corresponding matrix <code class="reqn">W</code> and <code class="reqn">H</code>. If the input <code>V</code> matrix is a gpu.matrix-class object, then both <code class="reqn">W</code> and <code class="reqn">H</code> are also gpu.matrix-class objects.
</p>


<h3>Author(s)</h3>

<p>Angel Rubio and Cesar Lobato.
</p>


<h3>References</h3>

<p>[1] Lee, D., Seung, H. Learning the parts of objects by non-negative matrix factorization. Nature 401, 788791 (1999). https://doi.org/10.1038/44565
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(Matrix)
set.seed(1)
a1 &lt;- gpu.matrix(runif(90),nrow=30,ncol=3)
a2 &lt;- gpu.matrix(runif(30),nrow=3,ncol=10)
V &lt;- a1 %*% a2
b &lt;- NMFgpumatrix(V = V, k=3, tol = 1e-6)

#check result:
image(Matrix(as.matrix(V)))
image(Matrix(as.matrix(b$W %*% b$H)))


## End(Not run)

</code></pre>

<hr>
<h2 id='power_of_a_matrix'>Compute the kth power of a matrix.</h2><span id='topic++25+5E+25'></span><span id='topic++25+5E+25+2Cgpu.matrix.tensorflow+2Cnumeric-method'></span><span id='topic++25+5E+25+2Cgpu.matrix.torch+2Cnumeric-method'></span>

<h3>Description</h3>

<p>Comput the kth power of a squere matrix, i.e., multiply the gpu.matrix-class object by itself as many times as user indicates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow,numeric'
x %^% k
## S4 method for signature 'gpu.matrix.torch,numeric'
x %^% k
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="power_of_a_matrix_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="power_of_a_matrix_+3A_k">k</code></td>
<td>
<p>the power of the matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The input <code>x</code> gpu.matrix-class needs to be squere. This function internally call the method <code>%*%</code> as many times as required. If the input gpu.matrix-class object is stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>the nth power of the input gpu.matrix-class object. The returned matrix is also a gpu.matrix-class object.
</p>


<h3>See Also</h3>

<p>See also: <code><a href="#topic++25+2A+25">%*%</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

a &lt;- gpu.matrix(rnorm(9),nrow=3,ncol=3)
a %^% 5



## End(Not run)

</code></pre>

<hr>
<h2 id='qr_decomposition'>The QR Decomposition of a GPUmatrix object</h2><span id='topic+qr'></span><span id='topic+qr-methods'></span><span id='topic+qr+2Cgpu.matrix.tensorflow-method'></span><span id='topic+qr+2Cgpu.matrix.torch-method'></span><span id='topic+qr.Q'></span><span id='topic+qr.Q-methods'></span><span id='topic+qr.Q+2Clist-method'></span><span id='topic+qr.R'></span><span id='topic+qr.R-methods'></span><span id='topic+qr.R+2Clist-method'></span><span id='topic+qr.X'></span><span id='topic+qr.X-methods'></span><span id='topic+qr.X+2Clist-method'></span><span id='topic+qr.coef'></span><span id='topic+qr.coef-methods'></span><span id='topic+qr.coef+2Clist-method'></span><span id='topic+qr.qty'></span><span id='topic+qr.qty-methods'></span><span id='topic+qr.qty+2Clist-method'></span><span id='topic+qr.qy'></span><span id='topic+qr.qy-methods'></span><span id='topic+qr.qy+2Clist-method'></span><span id='topic+qr.resid'></span><span id='topic+qr.resid-methods'></span><span id='topic+qr.resid+2Clist-method'></span><span id='topic+qr.solve'></span><span id='topic+qr.solve-methods'></span><span id='topic+qr.solve+2CANY+2Cgpu.matrix.tensorflow-method'></span><span id='topic+qr.solve+2CANY+2Cgpu.matrix.torch-method'></span><span id='topic+qr.solve+2Cgpu.matrix.tensorflow+2CANY-method'></span><span id='topic+qr.solve+2Cgpu.matrix.tensorflow+2Cgpu.matrix.tensorflow-method'></span><span id='topic+qr.solve+2Cgpu.matrix.torch+2CANY-method'></span><span id='topic+qr.solve+2Cgpu.matrix.torch+2Cgpu.matrix.torch-method'></span><span id='topic+qr.solve+2Clist+2CANY-method'></span>

<h3>Description</h3>

<p>These functions mimic the base <code>qr</code> family functions to operate on gpu.matrix-class objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow'
qr(x,...)
## S4 method for signature 'gpu.matrix.torch'
qr(x,...)

## S4 method for signature 'list'
qr.Q(qr,complete,Dvec)
## S4 method for signature 'list'
qr.R(qr,complete)
## S4 method for signature 'list'
qr.X(qr,complete)

## S4 method for signature 'list'
qr.coef(qr,y)
## S4 method for signature 'list'
qr.qy(qr,y)
## S4 method for signature 'list'
qr.qty(qr,y)
## S4 method for signature 'list'
qr.resid(qr,y)
## S4 method for signature 'ANY,gpu.matrix.tensorflow'
qr.solve(a,b)
## S4 method for signature 'ANY,gpu.matrix.torch'
qr.solve(a,b)
## S4 method for signature 'gpu.matrix.tensorflow,ANY'
qr.solve(a,b)
## S4 method for signature 'gpu.matrix.tensorflow,gpu.matrix.tensorflow'
qr.solve(a,b)
## S4 method for signature 'gpu.matrix.torch,ANY'
qr.solve(a,b)
## S4 method for signature 'gpu.matrix.torch,gpu.matrix.torch'
qr.solve(a,b)
## S4 method for signature 'list,ANY'
qr.solve(a,b)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qr_decomposition_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="qr_decomposition_+3A_y">y</code>, <code id="qr_decomposition_+3A_b">b</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> corresponding to the right-hand side of equations <code>ax=b</code> or <code>ax=y</code>.</p>
</td></tr>
<tr><td><code id="qr_decomposition_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="qr_decomposition_+3A_qr">qr</code></td>
<td>
<p>a list resulting from the application of the function <code>qr</code>.</p>
</td></tr>
<tr><td><code id="qr_decomposition_+3A_complete">complete</code></td>
<td>
<p>The same as in 'base' function <code>qr.Q</code>, and <code>qr.X</code></p>
</td></tr>
<tr><td><code id="qr_decomposition_+3A_dvec">Dvec</code></td>
<td>
<p>The same as in 'base' function <code>qr.Q</code></p>
</td></tr>
<tr><td><code id="qr_decomposition_+3A_a">a</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> corresponding to the left-hand side of equations <code>ax=b</code> or <code>ax=y</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>qr</code> internally calls the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
</p>
<p>The QR decomposition can be used to solve the equation <code>Ax=b</code> for a given matrix A, and a vector of observations b. In this context, the functions <code>qr.coef</code>, and <code>qr.resid</code> return the coefficients, and residuals values. Moreover, the functions <code>qr.qy</code>, and <code>qr.qty</code> returns <code>Q %*% y</code> and <code>Q %*% t(y)</code>.
Note that if parameter <code>complete</code> is TRUE then an arbitrary orthogonal completion of the <b>X</b> and <b>Q</b> matrix or wheter the <b>R</b> matrix is to be completed by binding zero-value rows beneath the square upper triangle.
</p>
<p>The function <code>solve.qr</code> solves the system of equations <code>Ax=b</code> via the QR decomposition. This function internally calls the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
</p>
<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>The function <code>qr</code> returns a list with the following items:
</p>
<table role = "presentation">
<tr><td><code>q</code></td>
<td>
<p>The corresponding complete matrix <code>Q</code> resulting from the application of the QR decomposition to <code>a</code>. It is a gpu.matrix-class object.</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>The corresponding complete matrix <code>R</code> resulting from the application of the QR decomposition to <code>a</code>. It is a gpu.matrix-class object.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The matrix <code>a</code>. It is a gpu.matrix-class object.</p>
</td></tr>
</table>
<p>Please note that the output returned by this function is different from the 'base' function <code>qr</code>, which returns an object of the 'qr' class.
</p>
<p>After performing a QR decomposition on a matrix A, given the resulting object, the functions <code>qr.X</code>, <code>qr.Q</code>, and <code>qr.R</code> return the original matrix A, the matrix Q, and the matrix R respectively. The returned matrices are gpu.matrix-class objects.
</p>
<p>The functions <code>qr.coef</code> and <code>qr.resid</code> return the coefficients and residuals when fitting the equation <code>Ax=b</code>. In this context, the  functions <code>qr.qy</code>, and <code>qr.qty</code> returns <code>Q %*% y</code> and <code>Q %*% t(y)</code>. The resulting vectors are objects of the class gpu.matrix.
</p>
<p>The function <code>qr.solve</code> returns a gpu.matrix-class object containing the coefficients of the solution of the system of equations <code>Ax=b</code> by QR decomposition.
</p>


<h3>See Also</h3>

<p>See <code><a href="base.html#topic+qr">qr</a></code>, <code><a href="torch.html#topic+linalg_qr">linalg_qr</a></code>, <code><a href="torch.html#topic+torch_triangular_solve">torch_triangular_solve</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
## overdetermined system
A &lt;- gpu.matrix(runif(12),nrow =  4)
b &lt;- gpu.matrix(rnorm(4),ncol=1)
qr.solve(a = A, b)
qr_gpu &lt;- qr(A)
qr.solve(a=qr_gpu,b)
qr.coef(qr = qr_gpu,b)
qr.resid(qr = qr_gpu,b)
qr.qty(qr = qr_gpu,b)
qr.qy(qr = qr_gpu,b)
qr.X(qr = qr_gpu,complete = T)
qr.Q(qr = qr_gpu,complete = T)
qr.R(qr = qr_gpu,complete = T)


## underdetermined system
A &lt;- gpu.matrix(runif(12),nrow =  3)
b &lt;- gpu.matrix(rnorm(3),ncol=1)
qr.solve(a = A, b)
qr_gpu &lt;- qr(A)
qr.solve(a=qr_gpu,b)
qr.coef(qr = qr_gpu,b)
qr.resid(qr = qr_gpu,b)
qr.qty(qr = qr_gpu,b)
qr.qy(qr = qr_gpu,b)
qr.X(qr = qr_gpu,complete = T)
qr.Q(qr = qr_gpu,complete = T)
qr.R(qr = qr_gpu,complete = T)

## End(Not run)

</code></pre>

<hr>
<h2 id='round'>rounding of numers</h2><span id='topic+round'></span><span id='topic+round-methods'></span><span id='topic+round+2Cgpu.matrix.tensorflow+2CANY-method'></span><span id='topic+round+2Cgpu.matrix.torch+2Cmissing-method'></span><span id='topic+round+2Cgpu.matrix.torch+2Cnumeric-method'></span><span id='topic+round+2Cgpu.matrix.tensorflow+2Cmissing-method'></span><span id='topic+round+2Cgpu.matrix.tensorflow+2Cnumeric-method'></span>

<h3>Description</h3>

<p>It mimics the base function <code>'round'</code> to operate on gpu.matrix-class objects. This function rounds the values in its first argument to the specified number of decimal places (default 0).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpu.matrix.tensorflow,ANY'
round(x)
## S4 method for signature 'gpu.matrix.torch,missing'
round(x,digits)
## S4 method for signature 'gpu.matrix.torch,numeric'
round(x,digits)
## S4 method for signature 'gpu.matrix.tensorflow,missing'
round(x,digits)
## S4 method for signature 'gpu.matrix.tensorflow,numeric'
round(x,digits)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="round_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="round_+3A_digits">digits</code></td>
<td>
<p>integer indicating the number of decimal places (round) or significant digits (signif) to be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>round</code> internally calls the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
</p>
<p>The behaveour of the function mimics the 'base' function <code>round</code>. Note that for rounding off a 5, the function will consider &quot;go to the even digit&quot;. Therefore, <code>round(2.5) = 2</code> and <code>round(3.5) = 4</code>. For more details see <code><a href="base.html#topic+round">round</a></code>, and <code><a href="torch.html#topic+torch_round">torch_round</a></code>.
</p>
<p>If the input gpu.matrix-class object is stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>The function will return a gpu.matrix-class object with the rounded values.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+round">round</a></code>, and <code><a href="torch.html#topic+torch_round">torch_round</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

a &lt;- gpu.matrix(rnorm(9),3,3)
round(a,digits = 3) #round to the third digit



## End(Not run)

</code></pre>

<hr>
<h2 id='solve_gpu.matrix'>Solve a System of Equations</h2><span id='topic+solve'></span><span id='topic+solve-methods'></span><span id='topic+solve+2CANY+2Cgpu.matrix.tensorflow-method'></span><span id='topic+solve+2CANY+2Cgpu.matrix.torch-method'></span><span id='topic+solve+2Cgpu.matrix.tensorflow+2CANY-method'></span><span id='topic+solve+2Cgpu.matrix.tensorflow+2Cmissing-method'></span><span id='topic+solve+2Cgpu.matrix.torch+2CANY-method'></span><span id='topic+solve+2Cgpu.matrix.torch+2Cmissing-method'></span><span id='topic+ginv'></span><span id='topic+ginv-methods'></span><span id='topic+ginv+2Cgpu.matrix.torch-method'></span><span id='topic+ginv+2Cgpu.matrix.tensorflow-method'></span><span id='topic+chol_solve'></span><span id='topic+chol_solve-methods'></span><span id='topic+chol_solve+2CANY+2Cgpu.matrix.torch-method'></span><span id='topic+chol_solve+2CANY+2Cgpu.matrix.tensorflow-method'></span><span id='topic+chol_solve+2Cgpu.matrix.torch+2CANY-method'></span><span id='topic+chol_solve+2Cgpu.matrix.tensorflow+2CANY-method'></span>

<h3>Description</h3>

<p>The function <code>solve</code> mimics of the 'base' function <code>solve</code> to operate on gpu.matrix-class objects: it &quot;solves the equation <code>a %*% x = b</code>.&quot;
</p>
<p>The function <code>ginv</code> mimics the function <code>ginv</code> of package 'MASS' to operate on gpu.matrix-class objects: it &quot;Calculates the Moore-Penrose generalized inverse of a matrix X.&quot;
</p>
<p>The function <code>chol_solve</code> is a GPUmatrix own function. This function uses the Cholesky decomposition to solve a system of equations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'ANY,gpu.matrix.tensorflow'
solve(a,b)
## S4 method for signature 'ANY,gpu.matrix.torch'
solve(a,b)
## S4 method for signature 'gpu.matrix.tensorflow,ANY'
solve(a,b)
## S4 method for signature 'gpu.matrix.tensorflow,missing'
solve(a)
## S4 method for signature 'gpu.matrix.torch,ANY'
solve(a,b)
## S4 method for signature 'gpu.matrix.torch,missing'
solve(a)

## S4 method for signature 'gpu.matrix.torch'
ginv(X,tol)
## S4 method for signature 'gpu.matrix.tensorflow'
ginv(X,tol)

## S4 method for signature 'ANY,gpu.matrix.torch'
chol_solve(x,y)
## S4 method for signature 'ANY,gpu.matrix.tensorflow'
chol_solve(x,y)
## S4 method for signature 'gpu.matrix.torch,ANY'
chol_solve(x,y)
## S4 method for signature 'gpu.matrix.tensorflow,ANY'
chol_solve(x,y)


</code></pre>


<h3>Arguments</h3>

<p>These inputs correspond to the <code>solve</code> function:
</p>
<table role = "presentation">
<tr><td><code id="solve_gpu.matrix_+3A_a">a</code></td>
<td>
<p>a square numeric or complex <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code> containing the coefficients of the linear system.</p>
</td></tr>
<tr><td><code id="solve_gpu.matrix_+3A_b">b</code></td>
<td>
<p>a numeric or complex vector or matrix giving the right-hand side(s) of the linear system. If <code>b</code> missing, <code>solve</code> will return the inverse of <code>a</code>.</p>
</td></tr>
</table>
<p>These inputs correspond to the <code>chol_solve</code> function:
</p>
<table role = "presentation">
<tr><td><code id="solve_gpu.matrix_+3A_x">x</code></td>
<td>
<p>Given the equation <code>Ax=b</code>, <code>x</code> must be the transponsed of the cholesky decomposition of matrix <code>A</code> if <code>A</code> is a real symmetric positive-definite square matrix.</p>
</td></tr>
<tr><td><code id="solve_gpu.matrix_+3A_y">y</code></td>
<td>
<p>a numeric or complex vector or matrix giving the right-hand side(s) of the linear system.</p>
</td></tr>
</table>
<p>These inputs correspond to the <code>ginv</code> function:
</p>
<table role = "presentation">
<tr><td><code id="solve_gpu.matrix_+3A_x">X</code></td>
<td>
<p>Matrix for which the Moore-Penrose inverse is required.</p>
</td></tr>
<tr><td><code id="solve_gpu.matrix_+3A_tol">tol</code></td>
<td>
<p>A relative tolerance to detect zero singular values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions <code>solve</code>, and <code>ginv</code> internally calls the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
</p>
<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>The result of these functions is  an object of the class gpu.matrix.
</p>


<h3>See Also</h3>

<p>See also <code><a href="base.html#topic+solve">solve</a></code>, <code><a href="MASS.html#topic+ginv">ginv</a></code>, <code><a href="torch.html#topic+torch_inverse">torch_inverse</a></code>, and <code><a href="torch.html#topic+torch_pinverse">torch_pinverse</a></code>.
</p>
<p>For cholesky decomposition see <code><a href="base.html#topic+chol">chol</a></code> from base or
<code><a href="#topic+matrix_decomposition">matrix_decomposition</a></code> from GPUmatrix.
</p>
<p>Also see <code><a href="#topic+qr.solve">qr.solve</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

#solve a system of equations:
a &lt;- gpu.matrix(rnorm(9),nrow=3,ncol=3)
b &lt;- c(1,1,1)
betas &lt;- solve(a,b)
a %*% betas

#the inverse matrix
inv &lt;- solve(a)
a %*% inv

#inverse using ginv
inv_2 &lt;- ginv(a)
a %*% inv_2


#chol_solve: it can be applies only if
# in the equation Ax=b A is real symmetric positive-definite square matrix.
a &lt;- gpu.matrix(rnorm(9),3,3)
A &lt;- tcrossprod(a) #A is symmetrix positive-definite
b &lt;- gpu.matrix(rnorm(3))

x_solve &lt;- solve(A,b) #using solve to compare results
x_chol_solve &lt;- chol_solve(t(chol(A)),b) #using chol_solve
#NOTE: notice that the input for chol_solve is the Cholesky decomposition
# of matrix A.


## End(Not run)

</code></pre>

<hr>
<h2 id='sort'>sort</h2><span id='topic+sort'></span><span id='topic+sort-methods'></span><span id='topic+sort+2Cgpu.matrix.tensorflow-method'></span><span id='topic+sort+2Cgpu.matrix.torch-method'></span><span id='topic+sort+2Cgpu.matrix.tensorflow+2Clogical-method'></span><span id='topic+sort+2Cgpu.matrix.torch+2Clogical-method'></span>

<h3>Description</h3>

<p>This function mimics the 'base' function <code>sort</code> to operate on gpu.matrix-class objects.
This function sort the input matrix into ascending or descending order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'gpu.matrix.tensorflow,logical'
sort(x,decreasing)
## S4 method for signature 'gpu.matrix.torch,logical'
sort(x,decreasing)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sort_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="sort_+3A_decreasing">decreasing</code></td>
<td>
<p>Logical. Should the sort be increasing or decreasing? </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function internally calls the corresponding function of the library torch or tensorflow (depending on the type of input gpu.matrix-class).
</p>
<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Value</h3>

<p>Returns a gpu.matrix-class object that is a vector (or a matrix with one column) with the values sorted.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+sort">sort</a></code>, <code><a href="torch.html#topic+torch_sort">torch_sort</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
a &lt;- gpu.matrix(rnorm(9),nrow=3,ncol=3)
sort(a) #returns a vector with the data sorted.

## End(Not run)

</code></pre>

<hr>
<h2 id='type+20of+20gpu.matrix'>Spicify type of 'GPUmatrix'</h2><span id='topic+dtype'></span><span id='topic+dtype+3C-'></span><span id='topic+dtype-methods'></span><span id='topic+dtype+3C--methods'></span><span id='topic+dtype+2Cgpu.matrix.torch-method'></span><span id='topic+dtype+2Cgpu.matrix.tensorflow-method'></span><span id='topic+dtype+3C-+2Cgpu.matrix.torch-method'></span><span id='topic+dtype+3C-+2Cgpu.matrix.tensorflow-method'></span><span id='topic+to_dense'></span><span id='topic+to_sparse'></span><span id='topic+to_dense-methods'></span><span id='topic+to_sparse-methods'></span><span id='topic+to_dense+2Cgpu.matrix.torch-method'></span><span id='topic+to_dense+2Cgpu.matrix.tensorflow-method'></span><span id='topic+to_sparse+2Cgpu.matrix.torch-method'></span><span id='topic+to_sparse+2Cgpu.matrix.tensorflow-method'></span>

<h3>Description</h3>

<p><code>dtype</code> and <code>dtype&lt;-</code> are functions that show or set the number of bits to use to store the number. The possible options are &quot;float64&quot; for float64 (default), &quot;float32&quot; for float32 and &quot;int&quot; for int64.
float64 uses 64 bits, that means that float64's take up twice as much memory thatn float32, thus doing operations on them may be slower in some machine architectures. However, float64's can represent numbers much more accurately than 32 bit floats. They also allow much larger numbers to be stored.
</p>
<p><code>to_dense</code> is a function that transforms a sparse matrix to a dense matrix. On the other hand, <code>to_sparse</code> transforms a dense matrix to a sparse matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'gpu.matrix.torch'
to_dense(x)
## S4 method for signature 'gpu.matrix.tensorflow'
to_dense(x)
## S4 method for signature 'gpu.matrix.torch'
to_sparse(x)
## S4 method for signature 'gpu.matrix.tensorflow'
to_sparse(x)


## S4 method for signature 'gpu.matrix.torch'
dtype(x)
## S4 method for signature 'gpu.matrix.tensorflow'
dtype(x)
## S4 replacement method for signature 'gpu.matrix.torch'
dtype(x) &lt;- value
## S4 replacement method for signature 'gpu.matrix.tensorflow'
dtype(x) &lt;- value

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="type+2B20of+2B20gpu.matrix_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gpu.matrix-class">gpu.matrix</a></code>.</p>
</td></tr>
<tr><td><code id="type+2B20of+2B20gpu.matrix_+3A_value">value</code></td>
<td>
<p>type of gpu.matrix object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>dtype</code> and <code>dtype &lt;- </code> show or set the number of bits to use to store the number.
</p>
<p><code>to_dense</code> returns a dense gpu.matrix-class object while the function <code>to_sparse</code> returns a sparse gpu.matrix-class object.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+gpu.matrix">gpu.matrix</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

a &lt;- gpu.matrix(rnorm(9),3,3)

dtype(a) #bits used to store the numbers: it is float64 by default.

b &lt;- a
dtype(b) &lt;- "float32" #change to float32
b

b &lt;- a
dtype(b) &lt;- "int" #change to integer64 (int64)
b

#sparse or dense matrices
A &lt;- gpu.matrix(data=c(1,1,1,0,0,1,0,1,0),3,3)
A #A is a dense gpu.matrix

A_sparse &lt;- to_sparse(A) #transform A to a sparse matrix.
A_sparse #this matrix stores the where number different to 0 were placed.

to_dense(A_sparse) #transform A_sparse to a dense matrix and we obtain the orginal matrix A:
A


## End(Not run)



</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
