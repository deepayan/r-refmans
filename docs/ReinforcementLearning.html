<!DOCTYPE html><html lang="en"><head><title>Help for package ReinforcementLearning</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ReinforcementLearning}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#computePolicy'><p>Computes the reinforcement learning policy</p></a></li>
<li><a href='#epsilonGreedyActionSelection'><p>Performs <code class="reqn">\varepsilon</code>-greedy action selection</p></a></li>
<li><a href='#experienceReplay'><p>Performs experience replay</p></a></li>
<li><a href='#gridworldEnvironment'><p>Defines an environment for a gridworld example</p></a></li>
<li><a href='#lookupActionSelection'><p>Converts a name into an action selection function</p></a></li>
<li><a href='#lookupLearningRule'><p>Loads reinforcement learning algorithm</p></a></li>
<li><a href='#policy'><p>Computes the reinforcement learning policy</p></a></li>
<li><a href='#randomActionSelection'><p>Performs random action selection</p></a></li>
<li><a href='#ReinforcementLearning'><p>Performs reinforcement learning</p></a></li>
<li><a href='#replayExperience'><p>Performs experience replay</p></a></li>
<li><a href='#sampleExperience'><p>Sample state transitions from an environment function</p></a></li>
<li><a href='#sampleGridSequence'><p>Sample grid sequence</p></a></li>
<li><a href='#selectEpsilonGreedyAction'><p>Performs <code class="reqn">\varepsilon</code>-greedy action selection</p></a></li>
<li><a href='#selectRandomAction'><p>Performs random action selection</p></a></li>
<li><a href='#state'><p>Creates a state representation for arbitrary objects</p></a></li>
<li><a href='#tictactoe'><p>Game states of 100,000 randomly sampled Tic-Tac-Toe games.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Model-Free Reinforcement Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-03-02</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nicolas Proellochs &lt;nicolas.proellochs@wi.jlug.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs model-free reinforcement learning in R. This implementation enables the learning
    of an optimal policy based on sample sequences consisting of states, actions and rewards. In 
    addition, it supplies multiple predefined reinforcement learning algorithms, such as experience 
    replay. Methodological details can be found in Sutton and Barto (1998) &lt;ISBN:0262039249&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2, hash (&ge; 2.0), data.table</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-03-02 01:20:42 UTC; nproe</td>
</tr>
<tr>
<td>Author:</td>
<td>Nicolas Proellochs [aut, cre],
  Stefan Feuerriegel [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-03-02 06:00:15 UTC</td>
</tr>
</table>
<hr>
<h2 id='computePolicy'>Computes the reinforcement learning policy</h2><span id='topic+computePolicy'></span>

<h3>Description</h3>

<p>Computes reinforcement learning policy from a given state-action table Q.
The policy is the decision-making function of the agent and defines the learning
agent's behavior at a given time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>computePolicy(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="computePolicy_+3A_x">x</code></td>
<td>
<p>Variable which encodes the behavior of the agent. This can be
either a <code>matrix</code>, <code>data.frame</code> or an <code><a href="#topic+rl">rl</a></code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the learned policy.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ReinforcementLearning">ReinforcementLearning</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create exemplary state-action table (Q) with 2 actions and 3 states
Q &lt;- data.frame("up" = c(-1, 0, 1), "down" = c(-1, 1, 0))

# Show best possible action in each state
computePolicy(Q)

</code></pre>

<hr>
<h2 id='epsilonGreedyActionSelection'>Performs <code class="reqn">\varepsilon</code>-greedy action selection</h2><span id='topic+epsilonGreedyActionSelection'></span>

<h3>Description</h3>

<p>Deprecated. Please use [ReinforcementLearning::selectEpsilonGreedyAction()] instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>epsilonGreedyActionSelection(Q, state, epsilon)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="epsilonGreedyActionSelection_+3A_q">Q</code></td>
<td>
<p>State-action table of type <code>hash</code>.</p>
</td></tr>
<tr><td><code id="epsilonGreedyActionSelection_+3A_state">state</code></td>
<td>
<p>The current state.</p>
</td></tr>
<tr><td><code id="epsilonGreedyActionSelection_+3A_epsilon">epsilon</code></td>
<td>
<p>Exploration rate between 0 and 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character value defining the next action.
</p>


<h3>References</h3>

<p>Sutton and Barto (1998). &quot;Reinforcement Learning: An Introduction&quot;, MIT Press, Cambridge, MA.
</p>

<hr>
<h2 id='experienceReplay'>Performs experience replay</h2><span id='topic+experienceReplay'></span>

<h3>Description</h3>

<p>Deprecated. Please use [ReinforcementLearning::replayExperience()] instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>experienceReplay(D, Q, control, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="experienceReplay_+3A_d">D</code></td>
<td>
<p>A <code>dataframe</code> containing the input data for reinforcement learning.
Each row represents a state transition tuple <code>(s,a,r,s_new)</code>.</p>
</td></tr>
<tr><td><code id="experienceReplay_+3A_q">Q</code></td>
<td>
<p>Existing state-action table of type <code>hash</code>.</p>
</td></tr>
<tr><td><code id="experienceReplay_+3A_control">control</code></td>
<td>
<p>Control parameters defining the behavior of the agent.</p>
</td></tr>
<tr><td><code id="experienceReplay_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>hash</code> that contains the learned Q-table.
</p>


<h3>References</h3>

<p>Lin (1992). &quot;Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching&quot;, Machine Learning (8:3), pp. 293&ndash;321.
</p>
<p>Watkins (1992). &quot;Q-learning&quot;. Machine Learning (8:3), pp. 279&ndash;292.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ReinforcementLearning">ReinforcementLearning</a></code>
</p>

<hr>
<h2 id='gridworldEnvironment'>Defines an environment for a gridworld example</h2><span id='topic+gridworldEnvironment'></span>

<h3>Description</h3>

<p>Function defines an environment for a 2x2 gridworld example. Here an agent is intended to
navigate from an arbitrary starting position to a goal position. The grid is surrounded by a wall,
which makes it impossible for the agent to move off the grid. In addition, the agent faces a wall between s1 and s4.
If the agent reaches the goal position, it earns a reward of 10. Crossing each square of the grid results in
a negative reward of -1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gridworldEnvironment(state, action)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gridworldEnvironment_+3A_state">state</code></td>
<td>
<p>The current state.</p>
</td></tr>
<tr><td><code id="gridworldEnvironment_+3A_action">action</code></td>
<td>
<p>Action to be executed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the next state and the reward.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load gridworld environment
gridworld &lt;- gridworldEnvironment

# Define state and action
state &lt;- "s1"
action &lt;- "down"

# Observe next state and reward
gridworld(state, action)

</code></pre>

<hr>
<h2 id='lookupActionSelection'>Converts a name into an action selection function</h2><span id='topic+lookupActionSelection'></span>

<h3>Description</h3>

<p>Input is a name for the action selection, output is the corresponding function object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lookupActionSelection(type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lookupActionSelection_+3A_type">type</code></td>
<td>
<p>A string denoting the type of action selection. Allowed values are <code>epsilon-greedy</code> or <code>random</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Function that implements the specific learning rule.
</p>

<hr>
<h2 id='lookupLearningRule'>Loads reinforcement learning algorithm</h2><span id='topic+lookupLearningRule'></span>

<h3>Description</h3>

<p>Decides upon a learning rule for reinforcement learning.
Input is a name for the learning rule, while output is the corresponding function object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lookupLearningRule(type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lookupLearningRule_+3A_type">type</code></td>
<td>
<p>A string denoting the learning rule. Allowed values are <code>experienceReplay</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Function that implements the specific learning rule.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ReinforcementLearning">ReinforcementLearning</a></code>
</p>

<hr>
<h2 id='policy'>Computes the reinforcement learning policy</h2><span id='topic+policy'></span>

<h3>Description</h3>

<p>Deprecated. Please use [ReinforcementLearning::computePolicy()] instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>policy(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="policy_+3A_x">x</code></td>
<td>
<p>Variable which encodes the behavior of the agent. This can be
either a <code>matrix</code>, <code>data.frame</code> or an <code><a href="#topic+rl">rl</a></code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the learned policy.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ReinforcementLearning">ReinforcementLearning</a></code>
</p>

<hr>
<h2 id='randomActionSelection'>Performs random action selection</h2><span id='topic+randomActionSelection'></span>

<h3>Description</h3>

<p>Deprecated. Please use [ReinforcementLearning::selectRandomAction()] instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randomActionSelection(Q, state, epsilon)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="randomActionSelection_+3A_q">Q</code></td>
<td>
<p>State-action table of type <code>hash</code>.</p>
</td></tr>
<tr><td><code id="randomActionSelection_+3A_state">state</code></td>
<td>
<p>The current state.</p>
</td></tr>
<tr><td><code id="randomActionSelection_+3A_epsilon">epsilon</code></td>
<td>
<p>Exploration rate between 0 and 1 (not used).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character value defining the next action.
</p>

<hr>
<h2 id='ReinforcementLearning'>Performs reinforcement learning</h2><span id='topic+ReinforcementLearning'></span><span id='topic+rl'></span>

<h3>Description</h3>

<p>Performs model-free reinforcement learning. Requires input data in the form of sample sequences
consisting of states, actions and rewards. The result of the learning process is a state-action table and an
optimal policy that defines the best possible action in each state.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReinforcementLearning(data, s = "s", a = "a", r = "r",
  s_new = "s_new", learningRule = "experienceReplay", iter = 1,
  control = list(alpha = 0.1, gamma = 0.1, epsilon = 0.1), verbose = F,
  model = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ReinforcementLearning_+3A_data">data</code></td>
<td>
<p>A dataframe containing the input sequences for reinforcement learning.
Each row represents a state transition tuple <code>(s,a,r,s_new)</code>.</p>
</td></tr>
<tr><td><code id="ReinforcementLearning_+3A_s">s</code></td>
<td>
<p>A string defining the column name of the current state in <code>data</code>.</p>
</td></tr>
<tr><td><code id="ReinforcementLearning_+3A_a">a</code></td>
<td>
<p>A string defining the column name of the selected action for the current state in <code>data</code>.</p>
</td></tr>
<tr><td><code id="ReinforcementLearning_+3A_r">r</code></td>
<td>
<p>A string defining the column name of the reward in the current state in <code>data</code>.</p>
</td></tr>
<tr><td><code id="ReinforcementLearning_+3A_s_new">s_new</code></td>
<td>
<p>A string defining the column name of the next state in <code>data</code>.</p>
</td></tr>
<tr><td><code id="ReinforcementLearning_+3A_learningrule">learningRule</code></td>
<td>
<p>A string defining the selected reinforcement learning agent. The default value and
only option in the current package version is <code>experienceReplay</code>.</p>
</td></tr>
<tr><td><code id="ReinforcementLearning_+3A_iter">iter</code></td>
<td>
<p>(optional) Iterations to be done. iter is an integer greater than 0. By default, <code>iter</code> is set to 1.</p>
</td></tr>
<tr><td><code id="ReinforcementLearning_+3A_control">control</code></td>
<td>
<p>(optional) Control parameters defining the behavior of the agent.
Default: <code>alpha = 0.1</code>; <code>gamma = 0.1</code>; <code>epsilon = 0.1</code>.</p>
</td></tr>
<tr><td><code id="ReinforcementLearning_+3A_verbose">verbose</code></td>
<td>
<p>If true, progress report is shown. Default: <code>false</code>.</p>
</td></tr>
<tr><td><code id="ReinforcementLearning_+3A_model">model</code></td>
<td>
<p>(optional) Existing model of class <code>rl</code>. Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="ReinforcementLearning_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>rl</code> with the following components:
</p>

<dl>
<dt><code>Q</code></dt><dd><p>Resulting state-action table.</p>
</dd>
<dt><code>Q_hash</code></dt><dd><p>Resulting state-action table in <code>hash</code> format.</p>
</dd>
<dt><code>Actions</code></dt><dd><p>Set of actions.</p>
</dd>
<dt><code>States</code></dt><dd><p>Set of states.</p>
</dd>
<dt><code>Policy</code></dt><dd><p>Resulting policy defining the best possible action in each state.</p>
</dd>
<dt><code>RewardSequence</code></dt><dd><p>Rewards collected during each learning episode in <code>iter</code>.</p>
</dd>
<dt><code>Reward</code></dt><dd><p>Total reward collected during the last learning iteration in <code>iter</code>.</p>
</dd>
</dl>



<h3>References</h3>

<p>Sutton and Barto (1998). Reinforcement Learning: An Introduction, Adaptive
Computation and Machine Learning, MIT Press, Cambridge, MA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Sampling data (1000 grid sequences)
data &lt;- sampleGridSequence(1000)

# Setting reinforcement learning parameters
control &lt;- list(alpha = 0.1, gamma = 0.1, epsilon = 0.1)

# Performing reinforcement learning
model &lt;- ReinforcementLearning(data, s = "State", a = "Action", r = "Reward",
s_new = "NextState", control = control)

# Printing model
print(model)

# Plotting learning curve
plot(model)
</code></pre>

<hr>
<h2 id='replayExperience'>Performs experience replay</h2><span id='topic+replayExperience'></span>

<h3>Description</h3>

<p>Performs experience replay. Experience replay allows reinforcement learning agents to remember and reuse experiences from the past.
The algorithm requires input data in the form of sample sequences consisting of states, actions and rewards.
The result of the learning process is a state-action table Q that allows one to infer the best possible action in each state.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>replayExperience(D, Q, control, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="replayExperience_+3A_d">D</code></td>
<td>
<p>A <code>dataframe</code> containing the input data for reinforcement learning.
Each row represents a state transition tuple <code>(s,a,r,s_new)</code>.</p>
</td></tr>
<tr><td><code id="replayExperience_+3A_q">Q</code></td>
<td>
<p>Existing state-action table of type <code>hash</code>.</p>
</td></tr>
<tr><td><code id="replayExperience_+3A_control">control</code></td>
<td>
<p>Control parameters defining the behavior of the agent.</p>
</td></tr>
<tr><td><code id="replayExperience_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>hash</code> that contains the learned Q-table.
</p>


<h3>References</h3>

<p>Lin (1992). &quot;Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching&quot;, Machine Learning (8:3), pp. 293&ndash;321.
</p>
<p>Watkins (1992). &quot;Q-learning&quot;. Machine Learning (8:3), pp. 279&ndash;292.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ReinforcementLearning">ReinforcementLearning</a></code>
</p>

<hr>
<h2 id='sampleExperience'>Sample state transitions from an environment function</h2><span id='topic+sampleExperience'></span>

<h3>Description</h3>

<p>Function generates sample experience in the form of state transition tuples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleExperience(N, env, states, actions, actionSelection = "random",
  control = list(alpha = 0.1, gamma = 0.1, epsilon = 0.1),
  model = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sampleExperience_+3A_n">N</code></td>
<td>
<p>Number of samples.</p>
</td></tr>
<tr><td><code id="sampleExperience_+3A_env">env</code></td>
<td>
<p>An environment function.</p>
</td></tr>
<tr><td><code id="sampleExperience_+3A_states">states</code></td>
<td>
<p>A character vector defining the enviroment states.</p>
</td></tr>
<tr><td><code id="sampleExperience_+3A_actions">actions</code></td>
<td>
<p>A character vector defining the available actions.</p>
</td></tr>
<tr><td><code id="sampleExperience_+3A_actionselection">actionSelection</code></td>
<td>
<p>(optional) Defines the action selection mode of the reinforcement learning agent. Default: <code>random</code>.</p>
</td></tr>
<tr><td><code id="sampleExperience_+3A_control">control</code></td>
<td>
<p>(optional) Control parameters defining the behavior of the agent.
Default: <code>alpha = 0.1</code>; <code>gamma = 0.1</code>; <code>epsilon = 0.1</code>.</p>
</td></tr>
<tr><td><code id="sampleExperience_+3A_model">model</code></td>
<td>
<p>(optional) Existing model of class <code>rl</code>. Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sampleExperience_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>dataframe</code> containing the experienced state transition tuples <code>s,a,r,s_new</code>.
The individual columns are as follows:
</p>

<dl>
<dt><code>State</code></dt><dd><p>The current state.</p>
</dd>
<dt><code>Action</code></dt><dd><p>The selected action for the current state.</p>
</dd>
<dt><code>Reward</code></dt><dd><p>The reward in the current state.</p>
</dd>
<dt><code>NextState</code></dt><dd><p>The next state.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+ReinforcementLearning">ReinforcementLearning</a></code>
</p>
<p><code><a href="#topic+gridworldEnvironment">gridworldEnvironment</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Define environment
env &lt;- gridworldEnvironment

# Define states and actions
states &lt;- c("s1", "s2", "s3", "s4")
actions &lt;- c("up", "down", "left", "right")

# Sample 1000 training examples
data &lt;- sampleExperience(N = 1000, env = env, states = states, actions = actions)
</code></pre>

<hr>
<h2 id='sampleGridSequence'>Sample grid sequence</h2><span id='topic+sampleGridSequence'></span>

<h3>Description</h3>

<p>Function uses an environment function to generate sample experience in the form of state transition tuples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleGridSequence(N, actionSelection = "random", control = list(alpha
  = 0.1, gamma = 0.1, epsilon = 0.1), model = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sampleGridSequence_+3A_n">N</code></td>
<td>
<p>Number of samples.</p>
</td></tr>
<tr><td><code id="sampleGridSequence_+3A_actionselection">actionSelection</code></td>
<td>
<p>(optional) Defines the action selection mode of the reinforcement learning agent. Default: <code>random</code>.</p>
</td></tr>
<tr><td><code id="sampleGridSequence_+3A_control">control</code></td>
<td>
<p>(optional) Control parameters defining the behavior of the agent.
Default: <code>alpha = 0.1</code>; <code>gamma = 0.1</code>; <code>epsilon = 0.1</code>.</p>
</td></tr>
<tr><td><code id="sampleGridSequence_+3A_model">model</code></td>
<td>
<p>(optional) Existing model of class <code>rl</code>. Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sampleGridSequence_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>dataframe</code> containing the experienced state transition tuples <code>s,a,r,s_new</code>.
The individual columns are as follows:
</p>

<dl>
<dt><code>State</code></dt><dd><p>The current state.</p>
</dd>
<dt><code>Action</code></dt><dd><p>The selected action for the current state.</p>
</dd>
<dt><code>Reward</code></dt><dd><p>The reward in the current state.</p>
</dd>
<dt><code>NextState</code></dt><dd><p>The next state.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+gridworldEnvironment">gridworldEnvironment</a></code>
</p>
<p><code><a href="#topic+ReinforcementLearning">ReinforcementLearning</a></code>
</p>

<hr>
<h2 id='selectEpsilonGreedyAction'>Performs <code class="reqn">\varepsilon</code>-greedy action selection</h2><span id='topic+selectEpsilonGreedyAction'></span>

<h3>Description</h3>

<p>Implements <code class="reqn">\varepsilon</code>-greedy action selection. In this strategy, the agent explores the environment
by selecting an action at random with probability <code class="reqn">\varepsilon</code>. Alternatively, the agent exploits its
current knowledge by choosing the optimal action with probability <code class="reqn">1-\varepsilon</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectEpsilonGreedyAction(Q, state, epsilon)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="selectEpsilonGreedyAction_+3A_q">Q</code></td>
<td>
<p>State-action table of type <code>hash</code>.</p>
</td></tr>
<tr><td><code id="selectEpsilonGreedyAction_+3A_state">state</code></td>
<td>
<p>The current state.</p>
</td></tr>
<tr><td><code id="selectEpsilonGreedyAction_+3A_epsilon">epsilon</code></td>
<td>
<p>Exploration rate between 0 and 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character value defining the next action.
</p>


<h3>References</h3>

<p>Sutton and Barto (1998). &quot;Reinforcement Learning: An Introduction&quot;, MIT Press, Cambridge, MA.
</p>

<hr>
<h2 id='selectRandomAction'>Performs random action selection</h2><span id='topic+selectRandomAction'></span>

<h3>Description</h3>

<p>Performs random action selection. In this strategy, the agent always selects an action at random.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectRandomAction(Q, state, epsilon)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="selectRandomAction_+3A_q">Q</code></td>
<td>
<p>State-action table of type <code>hash</code>.</p>
</td></tr>
<tr><td><code id="selectRandomAction_+3A_state">state</code></td>
<td>
<p>The current state.</p>
</td></tr>
<tr><td><code id="selectRandomAction_+3A_epsilon">epsilon</code></td>
<td>
<p>Exploration rate between 0 and 1 (not used).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character value defining the next action.
</p>

<hr>
<h2 id='state'>Creates a state representation for arbitrary objects</h2><span id='topic+state'></span>

<h3>Description</h3>

<p>Converts object of any class to a reinforcement learning state of type character.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>state(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="state_+3A_x">x</code></td>
<td>
<p>An object of any class.</p>
</td></tr>
<tr><td><code id="state_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character value defining the state representation of the given object.
</p>

<hr>
<h2 id='tictactoe'>Game states of 100,000 randomly sampled Tic-Tac-Toe games.</h2><span id='topic+tictactoe'></span>

<h3>Description</h3>

<p>A dataset containing 406,541 game states of Tic-Tac-Toe.
The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game.
All states are observed from the perspective of player X, who is also assumed to have played first.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tictactoe
</code></pre>


<h3>Format</h3>

<p>A data frame with 406,541 rows and 4 variables:
</p>

<dl>
<dt>State</dt><dd><p>The current game state, i.e. the state of the 3x3 grid.</p>
</dd>
<dt>Action</dt><dd><p>The move of player X in the current game state.</p>
</dd>
<dt>NextState</dt><dd><p>The next observed state after action selection of players X and B.</p>
</dd>
<dt>Reward</dt><dd><p>Indicates terminal and non-terminal game states. Reward is +1 for 'win', 0 for 'draw', and -1 for 'loss'.</p>
</dd>
</dl>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
