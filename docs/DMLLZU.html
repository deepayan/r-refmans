<!DOCTYPE html><html><head><title>Help for package DMLLZU</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {DMLLZU}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dml_bagging'>
<p>Double Machine Learning based on bagging</p></a></li>
<li><a href='#dml_boosting'>
<p>Double Machine Learning based on boosting</p></a></li>
<li><a href='#dml_ensemble_lm'>
<p>dml_ensemble_lm</p></a></li>
<li><a href='#dml_ensemble_rf'>
<p>dml_ensemble_rf</p></a></li>
<li><a href='#dml_neural_network'>
<p>Double Machine Learning based on neural network</p></a></li>
<li><a href='#dml_random_forest'>
<p>Double Machine Learning based on random forest</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Double Machine Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Yang(2020,&lt;<a href="https://doi.org/10.1016%2Fj.jeconom.2020.01.018">doi:10.1016/j.jeconom.2020.01.018</a>&gt;) come up with Double Machine Learning model ,it is based on this model, using four machine learning methods&ndash; bagging, Boosting, random forest and neural network, and then  based on the four models   for two different combinations of the integrated model &ndash; linear model combination and random forest .  </td>
</tr>
<tr>
<td>Author:</td>
<td>Lixiong Yang [aut],
  Junchang Zhao [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Junchang Zhao &lt;zhaojch19@lzu.edu.cn&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>rlang, gbm ,lmtest ,nnet , sandwich,randomForest,caret,ISLR</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-05-21 10:26:42 UTC; zhao</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-05-21 12:12:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='dml_bagging'>
Double Machine Learning based on bagging
</h2><span id='topic+dml_bagging'></span>

<h3>Description</h3>

<p>The most famous representative of parallel ensemble learning. This method uses the self-help method to repeatedly sample from a single training set and generate several different self-help sampling training sets. Then, the self-help sampling training sets are used to fit the model and then the predicted values are obtained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dml_bagging(y,x,d,data,sed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dml_bagging_+3A_y">y</code>, <code id="dml_bagging_+3A_x">x</code>, <code id="dml_bagging_+3A_d">d</code>, <code id="dml_bagging_+3A_data">data</code>, <code id="dml_bagging_+3A_sed">sed</code></td>
<td>

</td></tr>
</table>


<h3>Value</h3>

<p>y   Dependent variable;
</p>
<p>d  Independent variable;
</p>
<p>x  Control variables;
</p>
<p>sed    A random seed;
</p>
<p>data    Data
</p>


<h3>Author(s)</h3>

<p>Lixiong Yang&lt;ylx@lzu.edu.cn&gt;
</p>


<h3>References</h3>

<p>Leo Breiman. (1996). Bagging Predictors. Machine Learning, 24(2), pp. 123-140. doi: 10.1023/A:1018054314350
</p>
<p>Jui-Chung Yang,,Hui-Ching Chuang &amp; Chung-Ming Kuan.(2020).Double machine learning with gradient boosting and its application to the Big N audit quality effect. Journal of Econometrics(1),.doi:10.1016/j.jeconom.2020.01.018
</p>
<p>Victor Chernozhukov,,Denis Chetverikov,,Mert Demirer,... &amp; James Robins.(2018).Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal(1),. doi:10.3386/w23564.
</p>


<h3>See Also</h3>

<p>https://github.com/lixiongyang
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ISLR)
attach(Auto)
data&lt;- Auto
y &lt;- data$mpg    #Dependent variable
d &lt;- data$origin   #Independent variable
x="weight+year +horsepower"      #Control variables;

dml_bagging(y,x,d,data,sed=123)

</code></pre>

<hr>
<h2 id='dml_boosting'>
Double Machine Learning based on boosting
</h2><span id='topic+dml_boosting'></span>

<h3>Description</h3>

<p>The biggest difference with other method, the trees of this  method are generated sequentially. Each tree is constructed using the information of the previous generated trees. Each tree is generated according to a modified version of the original data set, and finally these trees are combined to establish a prediction model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dml_boosting(y,x,d,data,sed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dml_boosting_+3A_y">y</code>, <code id="dml_boosting_+3A_x">x</code>, <code id="dml_boosting_+3A_d">d</code>, <code id="dml_boosting_+3A_data">data</code>, <code id="dml_boosting_+3A_sed">sed</code></td>
<td>

</td></tr>
</table>


<h3>Value</h3>

<p>y   Dependent variable;
</p>
<p>d   Independent variable;
</p>
<p>x   Control variable;
</p>
<p>sed    A random seed;
</p>
<p>data   Data
</p>


<h3>Author(s)</h3>

<p>Lixiong Yang&lt;ylx@lzu.edu.cn&gt;
</p>


<h3>References</h3>

<p>Jui-Chung Yang,,Hui-Ching Chuang &amp; Chung-Ming Kuan.(2020).Double machine learning with gradient boosting and its application to the Big N audit quality effect. Journal of Econometrics(1),. doi:10.1016/j.jeconom.2020.01.018
Victor Chernozhukov,,Denis Chetverikov,,Mert Demirer,... &amp; James Robins.(2018).Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal(1),. doi:10.3386/w23564.
</p>


<h3>See Also</h3>

<p>https://github.com/lixiongyang
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ISLR)
attach(Auto)
data&lt;- Auto
y &lt;- data$mpg    #Dependent variable
d &lt;- data$origin   #Independent variable
x="weight+year +horsepower"      #Control variables;

dml_boosting(y,x,d,data,sed=123)
</code></pre>

<hr>
<h2 id='dml_ensemble_lm'>
dml_ensemble_lm
</h2><span id='topic+dml_ensemble_lm'></span>

<h3>Description</h3>

<p>As an important integrated learning method, stacking  consists of at least two layers of structure, including a primary learner and a secondary learner or a meta-learner used to combine the learner.Stacking first trained the primary learner from the initial data set, and then generated a new data set used to train the secondary learner, in this data set, the output of the primary learner is taken as the sample input characteristics, and the initial sample mark is still taken as the sample mark. Integrate  the four  basic model through linear model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dml_ensemble_lm(y,x,d,data,sed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dml_ensemble_lm_+3A_y">y</code>, <code id="dml_ensemble_lm_+3A_x">x</code>, <code id="dml_ensemble_lm_+3A_d">d</code>, <code id="dml_ensemble_lm_+3A_data">data</code>, <code id="dml_ensemble_lm_+3A_sed">sed</code></td>
<td>

</td></tr>
</table>


<h3>Value</h3>

<p>y   Dependent variable;
</p>
<p>d   Independent variable;
</p>
<p>x   Control variable;
</p>
<p>sed    A random seed;
</p>
<p>data   Data
</p>


<h3>Author(s)</h3>

<p>Lixiong Yang&lt;ylx@lzu.edu.cn&gt;;
Junchang Zhao &lt;zhaojch19@lzu.edu.cn&gt;
</p>


<h3>References</h3>

<p>Wolpert David H.. (1992). Stacked generalization. 5(2), pp. 241-259. doi: 10.1016/S0893-6080(05)80023-1
</p>
<p>Jui-Chung Yang,,Hui-Ching Chuang &amp; Chung-Ming Kuan.(2020).Double machine learning with gradient boosting and its application to the Big N audit quality effect. Journal of Econometrics(1),.doi:10.1016/j.jeconom.2020.01.018
</p>
<p>Victor Chernozhukov,,Denis Chetverikov,,Mert Demirer,... &amp; James Robins.(2018).Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal(1),. doi:10.3386/w23564.
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+help">help</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ISLR)
attach(Auto)
data&lt;- Auto
y &lt;- data$mpg    #Dependent variable
d &lt;- data$origin   #Independent variable
x="weight+year +horsepower"      #Control variables;

dml_ensemble_lm(y,x,d,data,sed=123)
</code></pre>

<hr>
<h2 id='dml_ensemble_rf'>
dml_ensemble_rf
</h2><span id='topic+dml_ensemble_rf'></span>

<h3>Description</h3>

<p>As an important integrated learning method, stacking  consists of at least two layers of structure, including a primary learner and a secondary learner or a meta-learner used to combine the learner. Stacking first trained the primary learner from the initial data set, and then generated a new data set used to train the secondary learner, in this data set, the output of the primary learner is taken as the sample input characteristics, and the initial sample mark is still taken as the sample mark.Integrate  the four  basic model through random forest
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dml_ensemble_rf(y,x,d,data,sed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dml_ensemble_rf_+3A_y">y</code>, <code id="dml_ensemble_rf_+3A_x">x</code>, <code id="dml_ensemble_rf_+3A_d">d</code>, <code id="dml_ensemble_rf_+3A_data">data</code>, <code id="dml_ensemble_rf_+3A_sed">sed</code></td>
<td>

</td></tr>
</table>


<h3>Value</h3>

<p>y   Dependent variable;
</p>
<p>d   Independent variable;
</p>
<p>x  Control variable;
</p>
<p>sed    A random seed;
</p>
<p>data   Data
</p>


<h3>Author(s)</h3>

<p>Lixiong Yang&lt;ylx@lzu.edu.cn&gt;;
Junchang Zhao &lt;zhaojch19@lzu.edu.cn&gt;
</p>


<h3>References</h3>

<p>Jui-Chung Yang,,Hui-Ching Chuang &amp; Chung-Ming Kuan.(2020).Double machine learning with gradient boosting and its application to the Big N audit quality effect. Journal of Econometrics(1),.doi:10.1016/j.jeconom.2020.01.018
Victor Chernozhukov,,Denis Chetverikov,,Mert Demirer,... &amp; James Robins.(2018).Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal(1),. doi:10.3386/w23564.
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+help">help</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ISLR)
attach(Auto)
data&lt;- Auto
y &lt;- data$mpg    #Dependent variable
d &lt;- data$origin   #Independent variable
x="weight+year +horsepower"      #Control variables;

dml_ensemble_rf(y,x,d,data,sed=123)
</code></pre>

<hr>
<h2 id='dml_neural_network'>
Double Machine Learning based on neural network
</h2><span id='topic+dml_neural_network'></span>

<h3>Description</h3>

<p>Each node represents a specific output function, known as the excitation function.It is a mathematical model or a computational model that imitates the structure and function of biological  net. It is calculated by the connection of a large number of artificial neurons, mainly composed of nodes and the mutual connections between nodes.
Each connection between two nodes represents a weighted value for the signal passing through the connection, known as the weight.
The output of the network is different according to the connection mode of the network, the weight value and the excitation function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dml_neural_network(y,x,d,data,sed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dml_neural_network_+3A_y">y</code>, <code id="dml_neural_network_+3A_x">x</code>, <code id="dml_neural_network_+3A_d">d</code>, <code id="dml_neural_network_+3A_data">data</code>, <code id="dml_neural_network_+3A_sed">sed</code></td>
<td>

</td></tr>
</table>


<h3>Value</h3>

<p>y    Dependent variable;
</p>
<p>d    Independent variable;
</p>
<p>x    Control variable;
</p>
<p>sed   A random seed;
</p>
<p>data    Data
</p>


<h3>Author(s)</h3>

<p>Yang Lixiong
</p>


<h3>References</h3>

<p>Jui-Chung Yang,,Hui-Ching Chuang &amp; Chung-Ming Kuan.(2020).Double machine learning with gradient boosting and its application to the Big N audit quality effect. Journal of Econometrics(1),. doi:10.1016/j.jeconom.2020.01.018
</p>
<p>Victor Chernozhukov,,Denis Chetverikov,,Mert Demirer,... &amp; James Robins.(2018).Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal(1),. doi:10.3386/w23564.
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+help">help</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ISLR)
attach(Auto)
data&lt;- Auto
y &lt;- data$mpg    #Dependent variable
d &lt;- data$origin   #Independent variable
x="weight+year +horsepower"      #Control variables;

dml_neural_network(y,x,d,data,sed=123)
</code></pre>

<hr>
<h2 id='dml_random_forest'>
Double Machine Learning based on random forest
</h2><span id='topic+dml_random_forest'></span>

<h3>Description</h3>

<p>To establish a series of decision tree, the difference is the this method for each division point considering the decision tree, should be chosen from among all variables contain random sample with some of the variables as candidate variables, the explanatory variables can only be used in the split point from the selected part of the explanation variable selection, and then to make predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dml_random_forest(y,x,d,data,sed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dml_random_forest_+3A_y">y</code>, <code id="dml_random_forest_+3A_x">x</code>, <code id="dml_random_forest_+3A_d">d</code>, <code id="dml_random_forest_+3A_data">data</code>, <code id="dml_random_forest_+3A_sed">sed</code></td>
<td>

</td></tr>
</table>


<h3>Value</h3>

<p>y   Dependent variable;
</p>
<p>d   Independent variable;
</p>
<p>x   Control variable;
</p>
<p>sed   A random seed;
</p>
<p>data  Data
</p>


<h3>Author(s)</h3>

<p>Lixiong Yang&lt;ylx@lzu.edu.cn&gt;
</p>


<h3>References</h3>

<p>Leo Breiman. (2001). Random Forests. Machine Learning, 45(1), pp. 5-32. doi:10.1023/A:1010933404324
</p>
<p>Jui-Chung Yang,,Hui-Ching Chuang &amp; Chung-Ming Kuan.(2020).Double machine learning with gradient boosting and its application to the Big N audit quality effect. Journal of Econometrics(1).doi:10.1016/j.jeconom.2020.01.018
Victor Chernozhukov,,Denis Chetverikov,,Mert Demirer,... &amp; James Robins.(2018).Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal(1). doi:10.3386/w23564.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ISLR)
attach(Auto)
data&lt;- Auto
y &lt;- data$mpg    #Dependent variable
d &lt;- data$origin   #Independent variable
x="weight+year +horsepower"      #Control variables;

dml_random_forest(y,x,d,data,sed=123)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
