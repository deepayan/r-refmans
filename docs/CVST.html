<!DOCTYPE html><html lang="en"><head><title>Help for package CVST</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {CVST}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CVST-package'>
<p>Fast Cross-Validation via Sequential Testing</p></a></li>
<li><a href='#cochranq.test'>
<p>Cochran's Q Test with Permutation</p></a></li>
<li><a href='#constructCVSTModel'>
<p>Setup for a CVST Run.</p></a></li>
<li><a href='#constructData'>
<p>Construction and Handling of <code>CVST.data</code> Objects</p></a></li>
<li><a href='#constructLearner'>
<p>Construction of Specific Learners for CVST</p></a></li>
<li><a href='#constructParams'>
<p>Construct a Grid of Parameters</p></a></li>
<li><a href='#constructSequentialTest'>
<p>Construct and Handle Sequential Tests.</p></a></li>
<li><a href='#CV'>
<p>Perform a k-fold Cross-validation</p></a></li>
<li><a href='#fastCV'>
<p>The Fast Cross-Validation via Sequential Testing (CVST) Procedure</p></a></li>
<li><a href='#noisyDonoho'>
<p>Generate Donoho's Toy Data Sets</p></a></li>
<li><a href='#noisySine'>
<p>Regression and Classification Toy Data Set</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast Cross-Validation via Sequential Testing</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2-3</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-02-19</td>
</tr>
<tr>
<td>Depends:</td>
<td>kernlab,Matrix</td>
</tr>
<tr>
<td>Author:</td>
<td>Tammo Krueger, Mikio Braun</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The fast cross-validation via sequential testing (CVST) procedure is an improved cross-validation procedure which uses non-parametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating under-performing candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the capability of a full cross-validation. Additionally to the CVST the package contains an implementation of the ordinary k-fold cross-validation with a flexible and powerful set of helper objects and methods to handle the overall model selection process. The implementations of the Cochran's Q test with permutations and the sequential testing framework of Wald are generic and can therefore also be used in other contexts.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2.0)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-02-21 18:10:19 UTC; tammok</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-02-21 18:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='CVST-package'>
Fast Cross-Validation via Sequential Testing
</h2><span id='topic+CVST-package'></span><span id='topic+CVST'></span>

<h3>Description</h3>

<p>The fast cross-validation via sequential testing (CVST) procedure is an improved cross-validation procedure which uses non-parametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating under-performing candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the capability of a full cross-validation. Additionally to the CVST the package contains an implementation of the ordinary k-fold cross-validation with a flexible and powerful set of helper objects and methods to handle the overall model selection process. The implementations of the Cochran's Q test with permutations and the sequential testing framework of Wald are generic and can therefore also be used in other contexts.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> CVST</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Fast Cross-Validation via Sequential Testing</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.2-3</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2022-02-19</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> kernlab,Matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Tammo Krueger, Mikio Braun</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Tammo Krueger &lt;tammokrueger@googlemail.com&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> The fast cross-validation via sequential testing (CVST) procedure is an improved cross-validation procedure which uses non-parametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating under-performing candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the capability of a full cross-validation. Additionally to the CVST the package contains an implementation of the ordinary k-fold cross-validation with a flexible and powerful set of helper objects and methods to handle the overall model selection process. The implementations of the Cochran's Q test with permutations and the sequential testing framework of Wald are generic and can therefore also be used in other contexts.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;=2.0)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
CV                      Perform a k-fold Cross-validation
CVST-package            Fast Cross-Validation via Sequential Testing
cochranq.test           Cochran's Q Test with Permutation
constructCVSTModel      Setup for a CVST Run.
constructData           Construction and Handling of 'CVST.data'
                        Objects
constructLearner        Construction of Specific Learners for CVST
constructParams         Construct a Grid of Parameters
constructSequentialTest
                        Construct and Handle Sequential Tests.
fastCV                  The Fast Cross-Validation via Sequential
                        Testing (CVST) Procedure
noisyDonoho             Generate Donoho's Toy Data Sets
noisySine               Regression and Classification Toy Data Set
</pre>


<h3>Author(s)</h3>

<p>Tammo Krueger, Mikio Braun
</p>
<p>Maintainer: Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Tammo Krueger, Danny Panknin, and Mikio Braun.
Fast cross-validation via sequential testing.
Journal of Machine Learning Research 16 (2015) 1103-1155.
URL <a href="https://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf">https://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf</a>.
</p>
<p>Abraham Wald.
<em>Sequential Analysis</em>.
Wiley, 1947.
</p>
<p>W. G. Cochran.
The comparison of percentages in matched samples.
<em>Biometrika</em>, 37 (3-4):256&ndash;266, 1950.
</p>
<p>M. Friedman.
The use of ranks to avoid the assumption of normality implicit in the analysis of variance.
<em>Journal of the American Statistical Association</em>, 32 (200):675&ndash;701, 1937.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ns = noisySine(100)
svm = constructSVMLearner()
params = constructParams(kernel="rbfdot", sigma=10^(-3:3), nu=c(0.05, 0.1, 0.2, 0.3))
opt = fastCV(ns, svm, params, constructCVSTModel())
</code></pre>

<hr>
<h2 id='cochranq.test'>
Cochran's Q Test with Permutation
</h2><span id='topic+cochranq.test'></span>

<h3>Description</h3>

<p>Performs the Cochran's Q test on the data. If the data matrix contains
too few elements, the chisquare distribution of the test statistic is
replaced by a permutation variant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cochranq.test(mat)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cochranq.test_+3A_mat">mat</code></td>
<td>

<p>The data matrix with the individuals in the rows and treatments in the columns.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>htest</code> object with the usual entries.
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>References</h3>

<p>W. G. Cochran.
The comparison of percentages in matched samples.
<em>Biometrika</em>, 37 (3-4):256&ndash;266, 1950.
</p>
<p>Kashinath D. Patil.
Cochran's Q test: Exact distribution.
<em>Journal of the American Statistical Association</em>, 70 (349):186&ndash;189, 1975.
</p>
<p>Merle W. Tate and Sara M. Brown.
Note on the Cochran Q test.
<em>Journal of the American Statistical Association</em>, 65 (329):155&ndash;160, 1970.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mat = matrix(c(rep(0, 10), 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1), ncol=4)
cochranq.test(mat)
mat = matrix(c(rep(0, 7), 1, rep(0, 12), 1, 1, 0, 1,
rep(0, 5), 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1), nrow=8)
cochranq.test(mat)
</code></pre>

<hr>
<h2 id='constructCVSTModel'>
Setup for a CVST Run.
</h2><span id='topic+constructCVSTModel'></span>

<h3>Description</h3>

<p>This is an helper object of type <code>CVST.setup</code> conatining all
necessary parameters for a CVST run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>constructCVSTModel(steps = 10, beta = 0.1, alpha = 0.01,
similaritySignificance = 0.05, earlyStoppingSignificance = 0.05,
earlyStoppingWindow = 3, regressionSimilarityViaOutliers = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="constructCVSTModel_+3A_steps">steps</code></td>
<td>

<p>Number of steps CVST should run
</p>
</td></tr>
<tr><td><code id="constructCVSTModel_+3A_beta">beta</code></td>
<td>

<p>Significance level for H0.
</p>
</td></tr>
<tr><td><code id="constructCVSTModel_+3A_alpha">alpha</code></td>
<td>

<p>Significance level for H1.
</p>
</td></tr>
<tr><td><code id="constructCVSTModel_+3A_similaritysignificance">similaritySignificance</code></td>
<td>

<p>Significance level of the similarity test.
</p>
</td></tr>
<tr><td><code id="constructCVSTModel_+3A_earlystoppingsignificance">earlyStoppingSignificance</code></td>
<td>

<p>Significance level of the early stopping test.
</p>
</td></tr>
<tr><td><code id="constructCVSTModel_+3A_earlystoppingwindow">earlyStoppingWindow</code></td>
<td>

<p>Size of the early stopping window.
</p>
</td></tr>
<tr><td><code id="constructCVSTModel_+3A_regressionsimilarityviaoutliers">regressionSimilarityViaOutliers</code></td>
<td>

<p>Should the less strict outlier-based similarity measure for
regression tasks be used.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>CVST.setup</code> object suitable for <code><a href="#topic+fastCV">fastCV</a></code>.
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Tammo Krueger, Danny Panknin, and Mikio Braun.
Fast cross-validation via sequential testing.
Journal of Machine Learning Research 16 (2015) 1103-1155.
URL <a href="https://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf">https://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fastCV">fastCV</a></code>
</p>

<hr>
<h2 id='constructData'>
Construction and Handling of <code>CVST.data</code> Objects
</h2><span id='topic+constructData'></span><span id='topic+getN'></span><span id='topic+getSubset'></span><span id='topic+getX'></span><span id='topic+shuffleData'></span><span id='topic+isClassification'></span><span id='topic+isRegression'></span>

<h3>Description</h3>

<p>The CVST methods needs a structured interface to both regression and
classification data sets. These helper methods allow the construction
and consistence handling of these types of data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>constructData(x, y)
getN(data)
getSubset(data, subset)
getX(data, subset = NULL)
shuffleData(data)
isClassification(data)
isRegression(data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="constructData_+3A_x">x</code></td>
<td>

<p>The feature data as vector or matrix.
</p>
</td></tr>
<tr><td><code id="constructData_+3A_y">y</code></td>
<td>

<p>The observed values (regressands/labels) as list, vector or factor.
</p>
</td></tr>
<tr><td><code id="constructData_+3A_data">data</code></td>
<td>

<p>A <code>CVST.data</code> object generated via <code>constructData</code>.
</p>
</td></tr>
<tr><td><code id="constructData_+3A_subset">subset</code></td>
<td>

<p>A index set.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>constructData</code> returns a <code>CVST.data</code> object. <code>getN</code>
returns the number of data points in the data set. <code>getSubset</code>
returns a subset of the data as a <code>CVST.data</code> object, while
<code>getX</code> just return the feature data. <code>shuffleData</code> returns a
randomly shuffled instance of the data.
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nsine = noisySine(10)
isClassification(nsine)
isRegression(nsine)
getN(nsine)
getX(nsine)
nsineShuffeled = shuffleData(nsine)
getX(nsineShuffeled)
getSubset(nsineShuffeled, 1:3)
</code></pre>

<hr>
<h2 id='constructLearner'>
Construction of Specific Learners for CVST
</h2><span id='topic+constructLearner'></span><span id='topic+constructKlogRegLearner'></span><span id='topic+constructKRRLearner'></span><span id='topic+constructSVMLearner'></span><span id='topic+constructSVRLearner'></span>

<h3>Description</h3>

<p>These methods construct a <code>CVST.learner</code> object suitable for the
CVST method. These objects provide the common interface needed for the
<code><a href="#topic+CV">CV</a></code> and <code><a href="#topic+fastCV">fastCV</a></code> methods. We provide kernel
logistic regression, kernel ridge regression, support vector machines
and support vector regression as fully functional implementation templates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>constructLearner(learn, predict)
constructKlogRegLearner()
constructKRRLearner()
constructSVMLearner()
constructSVRLearner()
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="constructLearner_+3A_learn">learn</code></td>
<td>

<p>The learning methods which takes a <code>CVST.data</code> and list of
parameters and return a model.
</p>
</td></tr>
<tr><td><code id="constructLearner_+3A_predict">predict</code></td>
<td>

<p>The prediction method which takes a model and <code>CVST.data</code> and
returns the corresponding predictions.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The nu-SVM and nu-SVR are build on top the corresponding implementations of
the <code>kernlab</code> package (see reference). In the list of parameters these
implementations expect an entry named <code>kernel</code>, which gives the
name of the kernel that should be used, an entry named <code>nu</code>
specifying the nu parameter, and an entry named <code>C</code> giving the C
parameter for the nu-SVR.
</p>
<p>The KRR and KLR also expect <code>kernel</code> and necessary other
parameters to construct the kernel. Both methods expect a lambda
parameter and KLR additonally a tol and maxiter parameter in the
parameter list.
</p>
<p>Note that the lambda of KRR/KLR and the C parameter of SVR are scaled
by the data set size to allow for comparable results in the fast CV loop.
</p>


<h3>Value</h3>

<p>Returns a learner of type <code>CVST.learner</code> suitable for <code><a href="#topic+CV">CV</a></code> and <code><a href="#topic+fastCV">fastCV</a></code>.
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Alexandros Karatzoglou, Alexandros Smola, Kurt Hornik, Achim Zeileis.
kernlab - An S4 Package for Kernel Methods in R
<em>Journal of Statistical Software</em> Vol. 11, Issue 9, Nov 2004.
DOI: doi: <a href="https://doi.org/10.18637/jss.v011.i09">10.18637/jss.v011.i09</a>.
</p>
<p>Volker Roth.
Probabilistic discriminative kernel classifiers for multi-class problems.
In <em>Proceedings of the 23rd DAGM-Symposium on Pattern Recognition</em>, pages 246&ndash;253, 2001.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CV">CV</a></code>
<code><a href="#topic+fastCV">fastCV</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># SVM
ns = noisySine(100)
svm = constructSVMLearner()
p = list(kernel="rbfdot", sigma=100, nu=.1)
m = svm$learn(ns, p)
nsTest = noisySine(1000)
pred = svm$predict(m, nsTest)
sum(pred != nsTest$y) / getN(nsTest)
# Kernel logistic regression
klr = constructKlogRegLearner()
p = list(kernel="rbfdot", sigma=100, lambda=.1/getN(ns), tol=10e-6, maxiter=100)
m = klr$learn(ns, p)
pred = klr$predict(m, nsTest)
sum(pred != nsTest$y) / getN(nsTest)
# SVR
ns = noisySinc(100)
svr = constructSVRLearner()
p = list(kernel="rbfdot", sigma=100, nu=.1, C=1*getN(ns))
m = svr$learn(ns, p)
nsTest = noisySinc(1000)
pred = svr$predict(m, nsTest)
sum((pred - nsTest$y)^2) / getN(nsTest)
# Kernel ridge regression
krr = constructKRRLearner()
p = list(kernel="rbfdot", sigma=100, lambda=.1/getN(ns))
m = krr$learn(ns, p)
pred = krr$predict(m, nsTest)
sum((pred - nsTest$y)^2) / getN(nsTest)
</code></pre>

<hr>
<h2 id='constructParams'>
Construct a Grid of Parameters
</h2><span id='topic+constructParams'></span>

<h3>Description</h3>

<p>This is a helper function which, geiven a named list of parameter
choices, expand the complete grid and returns a <code>CVST.params</code>
object suitable for <code><a href="#topic+CV">CV</a></code> and <code><a href="#topic+fastCV">fastCV</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>constructParams(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="constructParams_+3A_...">...</code></td>
<td>

<p>The parameters that should be expanded.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>CVST.params</code> wich is basically a named list of
possible parameter vallues.
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fastCV">fastCV</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>params = constructParams(kernel="rbfdot", sigma=10^(-1:5), nu=c(0.1, 0.2))
# the expanded grid contains 14 parameter lists:
length(params)
</code></pre>

<hr>
<h2 id='constructSequentialTest'>
Construct and Handle Sequential Tests.
</h2><span id='topic+constructSequentialTest'></span><span id='topic+getCVSTTest'></span><span id='topic+testSequence'></span><span id='topic+plotSequence'></span>

<h3>Description</h3>

<p>These functions handle the construction and calculation with
sequential tests as introduced by Wald (1947). <code>getCVSTTest</code>
constructs a special sequential test as introduced in Krueger
(2011). <code>testSequence</code> test a sequence of 0/1 whether it is
distributed according to H0 or H1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>constructSequentialTest(piH0 = 0.5, piH1 = 0.9, beta, alpha)
getCVSTTest(steps, beta = 0.1, alpha = 0.01)
testSequence(st, s)
plotSequence(st, s)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="constructSequentialTest_+3A_pih0">piH0</code></td>
<td>

<p>Probability of the binomial distribution for H0.
</p>
</td></tr>
<tr><td><code id="constructSequentialTest_+3A_pih1">piH1</code></td>
<td>

<p>Probability of the binomial distribution for H1.
</p>
</td></tr>
<tr><td><code id="constructSequentialTest_+3A_beta">beta</code></td>
<td>

<p>Significance level for H0.
</p>
</td></tr>
<tr><td><code id="constructSequentialTest_+3A_alpha">alpha</code></td>
<td>

<p>Significance level for H1.
</p>
</td></tr>
<tr><td><code id="constructSequentialTest_+3A_steps">steps</code></td>
<td>

<p>Number of steps the CVST procedure should be executed.
</p>
</td></tr>
<tr><td><code id="constructSequentialTest_+3A_st">st</code></td>
<td>

<p>A sequential test of type <code>CVST.sequentialTest</code>.
</p>
</td></tr>
<tr><td><code id="constructSequentialTest_+3A_s">s</code></td>
<td>

<p>A sequence of 0/1 values.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>constructSequentialTest</code> and <code>getCVSTTest</code> return a
<code>CVST.sequentialTest</code> with the specified
properties. <code>testSequence</code> returns 1, if H1 can be expected, -1
if H0 can be accepted, and 0 if the test needs more data for a
decission. <code>plotSequence</code> gives a graphical impression of the
this testing procedure.
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Abraham Wald.
<em>Sequential Analysis</em>.
Wiley, 1947.
</p>
<p>Tammo Krueger, Danny Panknin, and Mikio Braun.
Fast cross-validation via sequential testing.
Journal of Machine Learning Research 16 (2015) 1103-1155.
URL <a href="https://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf">https://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fastCV">fastCV</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>st = getCVSTTest(10)
s = rbinom(10,1, .5)
plotSequence(st, s)
testSequence(st, s)
</code></pre>

<hr>
<h2 id='CV'>
Perform a k-fold Cross-validation
</h2><span id='topic+CV'></span>

<h3>Description</h3>

<p>Performs the usual k-fold cross-validation procedure on a given data
set, parameter grid and learner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CV(data, learner, params, fold = 5, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CV_+3A_data">data</code></td>
<td>

<p>The data set as <code>CVST.data</code> object.
</p>
</td></tr>
<tr><td><code id="CV_+3A_learner">learner</code></td>
<td>

<p>The learner as <code>CVST.learner</code> object.
</p>
</td></tr>
<tr><td><code id="CV_+3A_params">params</code></td>
<td>

<p>the parameter grid as <code>CVST.params</code> object.
</p>
</td></tr>
<tr><td><code id="CV_+3A_fold">fold</code></td>
<td>

<p>The number of folds that should be generated for each set of parameters.
</p>
</td></tr>
<tr><td><code id="CV_+3A_verbose">verbose</code></td>
<td>

<p>Should the procedure report the performance for each model?
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the optimal parameter settings as determined by k-fold cross-validation.
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>References</h3>

<p>M. Stone.
Cross-validatory choice and assessment of statistical predictions.
<em>Journal of the Royal Statistical Society. Series B</em>, 36(2):111&ndash;147, 1974.
</p>
<p>Sylvain Arlot, Alain Celisse, and Paul Painleve.
A survey of cross-validation procedures for model selection.
<em>Statistics Surveys</em>, 4:40&ndash;79, 2010.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fastCV">fastCV</a></code>
<code><a href="#topic+constructData">constructData</a></code>
<code><a href="#topic+constructLearner">constructLearner</a></code>
<code><a href="#topic+constructParams">constructParams</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ns = noisySine(100)
svm = constructSVMLearner()
params = constructParams(kernel="rbfdot", sigma=10^(-3:3), nu=c(0.05, 0.1, 0.2, 0.3))
opt = CV(ns, svm, params)
</code></pre>

<hr>
<h2 id='fastCV'>
The Fast Cross-Validation via Sequential Testing (CVST) Procedure
</h2><span id='topic+fastCV'></span>

<h3>Description</h3>

<p>CVST is an improved cross-validation procedure which uses non-parametric
testing coupled with sequential analysis to determine the best
parameter set on linearly increasing subsets of the data. By
eliminating underperforming candidates quickly and keeping promising
candidates as long as possible, the method speeds up the computation
while preserving the capability of a full cross-validation.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fastCV(train, learner, params, setup, test = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fastCV_+3A_train">train</code></td>
<td>

<p>The data set as <code>CVST.data</code> object.
</p>
</td></tr>
<tr><td><code id="fastCV_+3A_learner">learner</code></td>
<td>

<p>The learner as <code>CVST.learner</code> object.
</p>
</td></tr>
<tr><td><code id="fastCV_+3A_params">params</code></td>
<td>

<p>the parameter grid as <code>CVST.params</code> object.
</p>
</td></tr>
<tr><td><code id="fastCV_+3A_setup">setup</code></td>
<td>

<p>A <code>CVST.setup</code> object containing the necessary parameter for
the CVST procedure.
</p>
</td></tr>
<tr><td><code id="fastCV_+3A_test">test</code></td>
<td>

<p>An independent test set that should be used at each step. If
<code>NULL</code> then the remaining data after learning a model
at each step is used instead.
</p>
</td></tr>
<tr><td><code id="fastCV_+3A_verbose">verbose</code></td>
<td>

<p>Should the procedure report the performance after each step?
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the optimal parameter settings as determined by fast
cross-validation via sequential testing.
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Tammo Krueger, Danny Panknin, and Mikio Braun.
Fast cross-validation via sequential testing.
Journal of Machine Learning Research 16 (2015) 1103-1155.
URL <a href="https://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf">https://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CV">CV</a></code>
<code><a href="#topic+constructCVSTModel">constructCVSTModel</a></code>
<code><a href="#topic+constructData">constructData</a></code>
<code><a href="#topic+constructLearner">constructLearner</a></code>
<code><a href="#topic+constructParams">constructParams</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ns = noisySine(100)
svm = constructSVMLearner()
params = constructParams(kernel="rbfdot", sigma=10^(-3:3), nu=c(0.05, 0.1, 0.2, 0.3))
opt = fastCV(ns, svm, params, constructCVSTModel())
</code></pre>

<hr>
<h2 id='noisyDonoho'>
Generate Donoho's Toy Data Sets 
</h2><span id='topic+noisyDonoho'></span><span id='topic+heavisine'></span><span id='topic+doppler'></span><span id='topic+bumps'></span><span id='topic+blocks'></span>

<h3>Description</h3>

<p>This function allows to generate noisy variants of the toy signals
introduced by Donoho (see reference section). The scaling is chosen to
reflect the setting as discussed in the original paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>noisyDonoho(n, fun = doppler, sigma = 1)
blocks(x, scale = 3.656993)
bumps(x, scale = 10.52884)
doppler(x, scale = 24.22172)
heavisine(x, scale = 2.356934)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="noisyDonoho_+3A_n">n</code></td>
<td>

<p>Number of data points that should be generated.
</p>
</td></tr>
<tr><td><code id="noisyDonoho_+3A_fun">fun</code></td>
<td>

<p>Function to use to generate the data.
</p>
</td></tr>
<tr><td><code id="noisyDonoho_+3A_sigma">sigma</code></td>
<td>

<p>Standard deviation of the noise component.
</p>
</td></tr>
<tr><td><code id="noisyDonoho_+3A_x">x</code></td>
<td>

<p>Number of data points that should be generated.  
</p>
</td></tr>
<tr><td><code id="noisyDonoho_+3A_scale">scale</code></td>
<td>

<p>Scaling parameter.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a data set of type CVST.data
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>References</h3>

<p>David L. Donoho and Jain M. Johnstone.
Ideal spatial adaptation by wavelet shrinkage.
<em>Biometrika</em>, 81 (3) 425&ndash;455, 1994.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+constructData">constructData</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>bumpsSet = noisyDonoho(1000, fun=bumps)
plot(bumpsSet)
dopplerSet = noisyDonoho(1000, fun=doppler)
plot(dopplerSet)
</code></pre>

<hr>
<h2 id='noisySine'>
Regression and Classification Toy Data Set
</h2><span id='topic+noisySine'></span><span id='topic+noisySinc'></span>

<h3>Description</h3>

<p>Regression and Classification Toy Data Set based on the sine and sinc function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>noisySine(n, dim = 5, sigma = 0.25)
noisySinc(n, dim = 2, sigma = 0.1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="noisySine_+3A_n">n</code></td>
<td>

<p>Number of data points that should be generated.
</p>
</td></tr>
<tr><td><code id="noisySine_+3A_dim">dim</code></td>
<td>

<p>Intrinsic dimensionality of the data set (see references for details).
</p>
</td></tr>
<tr><td><code id="noisySine_+3A_sigma">sigma</code></td>
<td>

<p>Standard deviation of the noise component.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a data set of type CVST.data
</p>


<h3>Author(s)</h3>

<p>Tammo Krueger &lt;tammokrueger@googlemail.com&gt;
</p>


<h3>References</h3>

<p>Tammo Krueger, Danny Panknin, and Mikio Braun.
Fast cross-validation via sequential testing.
Journal of Machine Learning Research 16 (2015) 1103-1155.
URL <a href="https://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf">https://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+constructData">constructData</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nsine = noisySine(1000)
plot(nsine, col=nsine$y)
nsinc = noisySinc(1000)
plot(nsinc)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
