<!DOCTYPE html><html><head><title>Help for package SSLR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SSLR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#abalone'><p>Abalone</p></a></li>
<li><a href='#best_split'><p>An S4 method to best split</p></a></li>
<li><a href='#best_split,DecisionTreeClassifier-method'><p>Best Split function</p></a></li>
<li><a href='#breast'><p>Breast</p></a></li>
<li><a href='#calculate_gini'><p>Function calculate gini</p></a></li>
<li><a href='#cclsSSLR'><p>General Interface Pairwise Constrained Clustering By Local Search</p></a></li>
<li><a href='#check_value'><p>Check value in leaf</p></a></li>
<li><a href='#check_xy_interface'><p>Ceck interface x y</p></a></li>
<li><a href='#ckmeansSSLR'><p>General Interface COP K-Means Algorithm</p></a></li>
<li><a href='#cluster_labels'><p>Get labels of clusters</p></a></li>
<li><a href='#cluster_labels.model_sslr_fitted'><p>Cluster labels</p></a></li>
<li><a href='#coBC'><p>General Interface for CoBC model</p></a></li>
<li><a href='#coBCCombine'><p>Combining the hypothesis</p></a></li>
<li><a href='#coBCG'><p>CoBC generic method</p></a></li>
<li><a href='#coBCReg'><p>General Interface coBCReg model</p></a></li>
<li><a href='#coBCRegG'><p>Generic Interface coBCReg model</p></a></li>
<li><a href='#coffee'><p>Time series data set</p></a></li>
<li><a href='#constrained_kmeans'><p>General Interface Constrained KMeans</p></a></li>
<li><a href='#COREG'><p>General Interface for COREG model</p></a></li>
<li><a href='#DecisionTreeClassifier-class'><p>Class DecisionTreeClassifier</p></a></li>
<li><a href='#democratic'><p>General Interface for Democratic model</p></a></li>
<li><a href='#democraticCombine'><p>Combining the hypothesis of the classifiers</p></a></li>
<li><a href='#democraticG'><p>Democratic generic method</p></a></li>
<li><a href='#EMLeastSquaresClassifierSSLR'><p>General Interface for EMLeastSquaresClassifier model</p></a></li>
<li><a href='#EMNearestMeanClassifierSSLR'><p>General Interface for EMNearestMeanClassifier model</p></a></li>
<li><a href='#EntropyRegularizedLogisticRegressionSSLR'><p>General Interface for EntropyRegularizedLogisticRegression model</p></a></li>
<li><a href='#fit_decision_tree'><p>An S4 method to fit decision tree.</p></a></li>
<li><a href='#fit_decision_tree,DecisionTreeClassifier-method'><p>Fit decision tree</p></a></li>
<li><a href='#fit_random_forest,RandomForestSemisupervised-method'><p>Fit Random Forest</p></a></li>
<li><a href='#fit_x_u'><p>fit_x_u object</p></a></li>
<li><a href='#fit_x_u.model_sslr'><p>Fit with x , y (labeled data) and unlabeled data (x_U)</p></a></li>
<li><a href='#fit_xy.model_sslr'><p>Fit with x and y</p></a></li>
<li><a href='#fit.model_sslr'><p>Fit with formula and data</p></a></li>
<li><a href='#get_centers'><p>Get centers model of clustering</p></a></li>
<li><a href='#get_centers.model_sslr_fitted'><p>Cluster labels</p></a></li>
<li><a href='#get_class_max_prob'><p>Get most frequented</p></a></li>
<li><a href='#get_class_mean_prob'><p>Get mean probability over all trees as prob vector</p></a></li>
<li><a href='#get_function'><p>FUNCTION TO GET FUNCTION METHOD</p></a></li>
<li><a href='#get_function_generic'><p>FUNCTION TO GET FUNCTION METHOD</p></a></li>
<li><a href='#get_levels_categoric'><p>Function to get gtoup from gini index</p></a></li>
<li><a href='#get_most_frequented'><p>Get most frequented</p></a></li>
<li><a href='#get_value_mean'><p>Get value mean</p></a></li>
<li><a href='#get_x_y'><p>FUNCTION TO GET REAL X AND Y WITH FORMULA AND DATA</p></a></li>
<li><a href='#gini_or_variance'><p>Gini or Variance by column</p></a></li>
<li><a href='#gini_prob'><p>Function to compute Gini index</p></a></li>
<li><a href='#GRFClassifierSSLR'><p>General Interface for GRFClassifier (Label propagation using Gaussian Random Fields and Harmonic) model</p></a></li>
<li><a href='#grow_tree'><p>An S4 method to grow tree.</p></a></li>
<li><a href='#grow_tree,DecisionTreeClassifier-method'><p>Function grow tree</p></a></li>
<li><a href='#knn_regression'><p>knn_regression</p></a></li>
<li><a href='#LaplacianSVMSSLR'><p>General Interface for LaplacianSVM model</p></a></li>
<li><a href='#lcvqeSSLR'><p>General LCVQE Algorithm</p></a></li>
<li><a href='#LinearTSVMSSLR'><p>General Interface for LinearTSVM model</p></a></li>
<li><a href='#load_conclust'><p>Load conclust</p></a></li>
<li><a href='#load_parsnip'><p>Load parsnip</p></a></li>
<li><a href='#load_RANN'><p>Load parsnip</p></a></li>
<li><a href='#load_RSSL'><p>Load RSSL</p></a></li>
<li><a href='#MCNearestMeanClassifierSSLR'><p>General Interface for MCNearestMeanClassifier (Moment Constrained Semi-supervised Nearest Mean Classifier) model</p></a></li>
<li><a href='#mpckmSSLR'><p>General Interface MPC K-Means Algorithm</p></a></li>
<li><a href='#newDecisionTree'><p>Function to create DecisionTree</p></a></li>
<li><a href='#Node-class'><p>Class Node for Decision Tree</p></a></li>
<li><a href='#nullOrNumericOrCharacter-class'><p>An S4 class to represent a class with more types values: null, numeric or character</p></a></li>
<li><a href='#oneNN'><p>1-NN supervised classifier builder</p></a></li>
<li><a href='#predict_inputs'><p>An S4 method to predict inputs.</p></a></li>
<li><a href='#predict_inputs,DecisionTreeClassifier-method'><p>Predict inputs Decision Tree</p></a></li>
<li><a href='#predict,DecisionTreeClassifier-method'><p>Function to predict inputs in Decision Tree</p></a></li>
<li><a href='#predict,RandomForestSemisupervised-method'><p>Function to predict inputs in Decision Tree</p></a></li>
<li><a href='#predict.coBC'><p>Predictions of the coBC method</p></a></li>
<li><a href='#predict.COREG'><p>Predictions of the COREG method</p></a></li>
<li><a href='#predict.democratic'><p>Predictions of the Democratic method</p></a></li>
<li><a href='#predict.EMLeastSquaresClassifierSSLR'><p>Predict EMLeastSquaresClassifierSSLR</p></a></li>
<li><a href='#predict.EMNearestMeanClassifierSSLR'><p>Predict EMNearestMeanClassifierSSLR</p></a></li>
<li><a href='#predict.EntropyRegularizedLogisticRegressionSSLR'><p>Predict EntropyRegularizedLogisticRegressionSSLR</p></a></li>
<li><a href='#predict.LaplacianSVMSSLR'><p>Predict LaplacianSVMSSLR</p></a></li>
<li><a href='#predict.LinearTSVMSSLR'><p>Predict LinearTSVMSSLR</p></a></li>
<li><a href='#predict.MCNearestMeanClassifierSSLR'><p>Predict MCNearestMeanClassifierSSLR</p></a></li>
<li><a href='#predict.model_sslr_fitted'><p>Predictions of model_sslr_fitted class</p></a></li>
<li><a href='#predict.OneNN'><p>Model Predictions</p></a></li>
<li><a href='#predict.RandomForestSemisupervised_fitted'><p>Predictions of the SSLRDecisionTree_fitted method</p></a></li>
<li><a href='#predict.selfTraining'><p>Predictions of the Self-training method</p></a></li>
<li><a href='#predict.setred'><p>Predictions of the SETRED method</p></a></li>
<li><a href='#predict.snnrce'><p>Predictions of the SNNRCE method</p></a></li>
<li><a href='#predict.snnrceG'><p>Predictions of the SNNRCE method</p></a></li>
<li><a href='#predict.SSLRDecisionTree_fitted'><p>Predictions of the SSLRDecisionTree_fitted method</p></a></li>
<li><a href='#predict.triTraining'><p>Predictions of the Tri-training method</p></a></li>
<li><a href='#predict.TSVMSSLR'><p>Predict TSVMSSLR</p></a></li>
<li><a href='#predict.USMLeastSquaresClassifierSSLR'><p>Predict USMLeastSquaresClassifierSSLR</p></a></li>
<li><a href='#predict.WellSVMSSLR'><p>Predict WellSVMSSLR</p></a></li>
<li><a href='#predictions'><p>predictions unlabeled data</p></a></li>
<li><a href='#predictions.GRFClassifierSSLR'><p>predictions unlabeled data</p></a></li>
<li><a href='#predictions.model_sslr_fitted'><p>Predictions of unlabeled data</p></a></li>
<li><a href='#print.model_sslr'><p>Print model SSLR</p></a></li>
<li><a href='#RandomForestSemisupervised-class'><p>Class Random Forest</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#seeded_kmeans'><p>General Interface Seeded KMeans</p></a></li>
<li><a href='#selfTraining'><p>General Interface for Self-training model</p></a></li>
<li><a href='#selfTrainingG'><p>Self-training generic method</p></a></li>
<li><a href='#setred'><p>General Interface for SETRED model</p></a></li>
<li><a href='#setredG'><p>SETRED generic method</p></a></li>
<li><a href='#snnrce'><p>General Interface for SNNRCE model</p></a></li>
<li><a href='#SSLRDecisionTree'><p>General Interface Decision Tree model</p></a></li>
<li><a href='#SSLRRandomForest'><p>General Interface Random Forest model</p></a></li>
<li><a href='#train_generic'><p>FUNCTION TO TRAIN GENERIC MODEL</p></a></li>
<li><a href='#triTraining'><p>General Interface for Tri-training model</p></a></li>
<li><a href='#triTrainingCombine'><p>Combining the hypothesis</p></a></li>
<li><a href='#triTrainingG'><p>Tri-training generic method</p></a></li>
<li><a href='#TSVMSSLR'><p>General Interface for TSVM (Transductive SVM classifier using the convex concave procedure) model</p></a></li>
<li><a href='#USMLeastSquaresClassifierSSLR'><p>General Interface for USMLeastSquaresClassifier (Updated Second Moment Least Squares Classifier) model</p></a></li>
<li><a href='#WellSVMSSLR'><p>General Interface for WellSVM model</p></a></li>
<li><a href='#wine'><p>Wine recognition data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Semi-Supervised Classification, Regression and Clustering
Methods</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.3.3</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Francisco Jesús Palomares Alabarce &lt;fpalomares@correo.ugr.es&gt;</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://dicits.ugr.es/software/SSLR/">https://dicits.ugr.es/software/SSLR/</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Providing a collection of techniques for semi-supervised 
    classification, regression and clustering. In semi-supervised problem, both labeled and unlabeled
    data are used to train a classifier. The package includes a collection of 
    semi-supervised learning techniques: self-training, co-training, democratic, 
    decision tree, random forest, 'S3VM' ... etc, with a fairly intuitive interface 
    that is easy to use.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, parsnip, plyr, dplyr (&ge; 0.8.0.1), magrittr, purrr,
rlang (&ge; 0.3.1), proxy, methods, generics, utils, RANN,
foreach, RSSL, conclust</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>caret, tidymodels, e1071, C50, kernlab, testthat, doParallel,
tidyverse, factoextra, survival, covr, kknn, randomForest,
ranger, MASS, nlme, knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-07-21 14:27:30 UTC; FRAJE</td>
</tr>
<tr>
<td>Author:</td>
<td>Francisco Jesús Palomares Alabarce
    <a href="https://orcid.org/0000-0002-0499-7034"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  José Manuel Benítez
    <a href="https://orcid.org/0000-0002-2346-0793"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Isaac Triguero <a href="https://orcid.org/0000-0002-0150-0651"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Christoph Bergmeir
    <a href="https://orcid.org/0000-0002-3665-9021"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Mabel González <a href="https://orcid.org/0000-0003-0152-444X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-07-22 08:10:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='abalone'>Abalone</h2><span id='topic+abalone'></span>

<h3>Description</h3>

<p>Abalone
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(abalone)
</code></pre>


<h3>Format</h3>

<p>Predict the age of abalone from physical measurements
</p>


<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/Abalone">https://archive.ics.uci.edu/ml/datasets/Abalone</a>
</p>

<hr>
<h2 id='best_split'>An S4 method to best split</h2><span id='topic+best_split'></span>

<h3>Description</h3>

<p>An S4 method to best split
</p>


<h3>Usage</h3>

<pre><code class='language-R'>best_split(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="best_split_+3A_object">object</code></td>
<td>
<p>DecisionTree object</p>
</td></tr>
<tr><td><code id="best_split_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='best_split+2CDecisionTreeClassifier-method'>Best Split function</h2><span id='topic+best_split+2CDecisionTreeClassifier-method'></span>

<h3>Description</h3>

<p>Function to get best split in Decision Tree.
Find the best split for node. &quot;Beast&quot; means that the mean
of impurity is the least possible.
To find the best division. Let's iterate through all the features.
All threshold / feature pairs will be computed in the numerical
features. In the features that are not numerical,
We get the best group of possible values
will be obtained based on an algorithm with the function
get_levels_categoric
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'DecisionTreeClassifier'
best_split(object, X, y, parms)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="best_split+2B2CDecisionTreeClassifier-method_+3A_object">object</code></td>
<td>
<p>DecisionTree object</p>
</td></tr>
<tr><td><code id="best_split+2B2CDecisionTreeClassifier-method_+3A_x">X</code></td>
<td>
<p>is data</p>
</td></tr>
<tr><td><code id="best_split+2B2CDecisionTreeClassifier-method_+3A_y">y</code></td>
<td>
<p>is class values</p>
</td></tr>
<tr><td><code id="best_split+2B2CDecisionTreeClassifier-method_+3A_parms">parms</code></td>
<td>
<p>parms in function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with: best_idx name of the feature with the best split or Null if it not be found
best_thr: threshold found in the best split, or Null if it not be found
</p>

<hr>
<h2 id='breast'>Breast</h2><span id='topic+breast'></span>

<h3>Description</h3>

<p>Breast
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(breast)
</code></pre>


<h3>Format</h3>

<p>: Diagnostic Wisconsin Breast Cancer Database
</p>


<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)</a>
</p>

<hr>
<h2 id='calculate_gini'>Function calculate gini</h2><span id='topic+calculate_gini'></span>

<h3>Description</h3>

<p>Function to calculate gini index.
Formula is: 1 - n:num_classes sum probabilitie_class ^ 2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_gini(column_factor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calculate_gini_+3A_column_factor">column_factor</code></td>
<td>
<p>class values</p>
</td></tr>
</table>

<hr>
<h2 id='cclsSSLR'>General Interface Pairwise Constrained Clustering By Local Search</h2><span id='topic+cclsSSLR'></span>

<h3>Description</h3>

<p>Model from conclust <br />
This function takes an unlabeled dataset and two lists of must-link and cannot-link constraints
as input and produce a clustering as output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cclsSSLR(
  n_clusters = NULL,
  mustLink = NULL,
  cantLink = NULL,
  max_iter = 1,
  tabuIter = 100,
  tabuLength = 20
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cclsSSLR_+3A_n_clusters">n_clusters</code></td>
<td>
<p>A number of clusters to be considered. Default is NULL (num classes)</p>
</td></tr>
<tr><td><code id="cclsSSLR_+3A_mustlink">mustLink</code></td>
<td>
<p>A list of must-link constraints. NULL Default, constrints same label</p>
</td></tr>
<tr><td><code id="cclsSSLR_+3A_cantlink">cantLink</code></td>
<td>
<p>A list of cannot-link constraints. NULL Default, constrints with different label</p>
</td></tr>
<tr><td><code id="cclsSSLR_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum iterations in KMeans. Default is 1</p>
</td></tr>
<tr><td><code id="cclsSSLR_+3A_tabuiter">tabuIter</code></td>
<td>
<p>Number of iteration in Tabu search</p>
</td></tr>
<tr><td><code id="cclsSSLR_+3A_tabulength">tabuLength</code></td>
<td>
<p>The number of elements in the Tabu list</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This models only returns labels, not centers
</p>


<h3>References</h3>

<p>Tran Khanh Hiep, Nguyen Minh Duc, Bui Quoc Trung<br />
<em>Pairwise Constrained Clustering by Local Search</em><br />
2016
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(SSLR)
library(tidymodels)

data &lt;- iris

set.seed(1)
#% LABELED
cls &lt;- which(colnames(iris) == "Species")

labeled.index &lt;- createDataPartition(data$Species, p = .2, list = FALSE)
data[-labeled.index,cls] &lt;- NA


m &lt;- cclsSSLR(max_iter = 1) %&gt;% fit(Species ~ ., data)

#Get labels (assing clusters), type = "raw" return factor
labels &lt;- m %&gt;% cluster_labels()

print(labels)


</code></pre>

<hr>
<h2 id='check_value'>Check value in leaf</h2><span id='topic+check_value'></span>

<h3>Description</h3>

<p>Function to check value in leaf from numeric until character
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_value(value, threshold)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_value_+3A_value">value</code></td>
<td>
<p>is the value in leaf node</p>
</td></tr>
<tr><td><code id="check_value_+3A_threshold">threshold</code></td>
<td>
<p>in leaf node</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if &lt;= in numeric or %in% in factor
</p>

<hr>
<h2 id='check_xy_interface'>Ceck interface x y</h2><span id='topic+check_xy_interface'></span>

<h3>Description</h3>

<p>Check interface
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_xy_interface(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_xy_interface_+3A_x">x</code></td>
<td>
<p>data without class labels</p>
</td></tr>
<tr><td><code id="check_xy_interface_+3A_y">y</code></td>
<td>
<p>values class</p>
</td></tr>
</table>

<hr>
<h2 id='ckmeansSSLR'>General Interface COP K-Means Algorithm</h2><span id='topic+ckmeansSSLR'></span>

<h3>Description</h3>

<p>Model from conclust <br />
This function takes an unlabeled dataset and two lists of must-link and cannot-link constraints
as input and produce a clustering as output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ckmeansSSLR(n_clusters = NULL, mustLink = NULL, cantLink = NULL, max_iter = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ckmeansSSLR_+3A_n_clusters">n_clusters</code></td>
<td>
<p>A number of clusters to be considered. Default is NULL (num classes)</p>
</td></tr>
<tr><td><code id="ckmeansSSLR_+3A_mustlink">mustLink</code></td>
<td>
<p>A list of must-link constraints. NULL Default, constrints same label</p>
</td></tr>
<tr><td><code id="ckmeansSSLR_+3A_cantlink">cantLink</code></td>
<td>
<p>A list of cannot-link constraints. NULL Default, constrints with different label</p>
</td></tr>
<tr><td><code id="ckmeansSSLR_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum iterations in KMeans. Default is 10</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This models only returns labels, not centers
</p>


<h3>References</h3>

<p>Wagstaff, Cardie, Rogers, Schrodl<br />
<em>Constrained K-means Clustering with Background Knowledge</em><br />
2001
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(SSLR)
library(tidymodels)

data &lt;- iris

set.seed(1)
#% LABELED
cls &lt;- which(colnames(iris) == "Species")

labeled.index &lt;- createDataPartition(data$Species, p = .2, list = FALSE)
data[-labeled.index,cls] &lt;- NA


m &lt;- ckmeansSSLR() %&gt;% fit(Species ~ ., data)

#Get labels (assing clusters), type = "raw" return factor
labels &lt;- m %&gt;% cluster_labels()

print(labels)


</code></pre>

<hr>
<h2 id='cluster_labels'>Get labels of clusters</h2><span id='topic+cluster_labels'></span>

<h3>Description</h3>

<p>Cluster labels
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_labels(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_labels_+3A_object">object</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="cluster_labels_+3A_...">...</code></td>
<td>
<p>other parameters to be passed</p>
</td></tr>
</table>

<hr>
<h2 id='cluster_labels.model_sslr_fitted'>Cluster labels</h2><span id='topic+cluster_labels.model_sslr_fitted'></span>

<h3>Description</h3>

<p>Get labels of clusters
raw returns factor or numeric values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_sslr_fitted'
cluster_labels(object, type = "class", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_labels.model_sslr_fitted_+3A_object">object</code></td>
<td>
<p>model_sslr_fitted model built</p>
</td></tr>
<tr><td><code id="cluster_labels.model_sslr_fitted_+3A_type">type</code></td>
<td>
<p>of predict in principal model: class, raw</p>
</td></tr>
<tr><td><code id="cluster_labels.model_sslr_fitted_+3A_...">...</code></td>
<td>
<p>other parameters to be passed</p>
</td></tr>
</table>

<hr>
<h2 id='coBC'>General Interface for CoBC model</h2><span id='topic+coBC'></span>

<h3>Description</h3>

<p>Co-Training by Committee (CoBC) is a semi-supervised learning algorithm
with a co-training style. This algorithm trains <code>N</code> classifiers with the learning
scheme defined in the <code>learner</code> argument using a reduced set of labeled examples. For
each iteration, an unlabeled
example is labeled for a classifier if the most confident classifications assigned by the
other <code>N-1</code> classifiers agree on the labeling proposed. The unlabeled examples
candidates are selected randomly from a pool of size <code>u</code>.
The final prediction is the average of the estimates of the N regressors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coBC(learner, N = 3, perc.full = 0.7, u = 100, max.iter = 50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coBC_+3A_learner">learner</code></td>
<td>
<p>model from parsnip package for training a supervised base classifier
using a set of instances. This model need to have probability predictions in classification mode</p>
</td></tr>
<tr><td><code id="coBC_+3A_n">N</code></td>
<td>
<p>The number of classifiers used as committee members. All these classifiers
are trained using the <code>gen.learner</code> function. Default is 3.</p>
</td></tr>
<tr><td><code id="coBC_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage
of new labeled examples reaches this value the self-labeling process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="coBC_+3A_u">u</code></td>
<td>
<p>Number of unlabeled instances in the pool. Default is 100.</p>
</td></tr>
<tr><td><code id="coBC_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to execute in the self-labeling process.
Default is 50.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For regression tasks, labeling data is very expensive computationally. Its so slow.
This method trains an ensemble of diverse classifiers. To promote the initial diversity
the classifiers are trained from the reduced set of labeled examples by Bagging.
The stopping criterion is defined through the fulfillment of one of the following
criteria: the algorithm reaches the number of iterations defined in the <code>max.iter</code>
parameter or the portion of unlabeled set, defined in the <code>perc.full</code> parameter,
is moved to the enlarged labeled set of the classifiers.
</p>


<h3>Value</h3>

<p>(When model fit) A list object of class &quot;coBC&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final <code>N</code> base classifiers trained using the enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of <code>N</code> vectors of indexes related to the training instances
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the <code>N</code> models. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor in classification.</p>
</dd>
<dt>pred</dt><dd><p>The function provided in the <code>pred</code> argument.</p>
</dd>
<dt>pred.pars</dt><dd><p>The list provided in the <code>pred.pars</code> argument.</p>
</dd>
</dl>



<h3>References</h3>

<p>Avrim Blum and Tom Mitchell.<br />
<em>Combining labeled and unlabeled data with co-training.</em><br />
In Eleventh Annual Conference on Computational Learning Theory, COLT’ 98, pages 92-100, New York, NY, USA, 1998. ACM.
ISBN 1-58113-057-0. doi: 10.1145/279943.279962.<br /><br />
Mohamed Farouk Abdel-Hady, Mohamed Farouk Abdel-Hady and Günther Palm.<br />
<em>Semi-supervised Learning for Regression with Cotraining by Committee</em><br />
Institute of Neural Information Processing
University of Ulm
D-89069 Ulm, Germany
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(tidymodels)
library(caret)
library(SSLR)

data(wine)

set.seed(1)
train.index &lt;- createDataPartition(wine$Wine, p = .7, list = FALSE)
train &lt;- wine[ train.index,]
test  &lt;- wine[-train.index,]

cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(wine$Wine, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA

#We need a model with probability predictions from parsnip
#https://tidymodels.github.io/parsnip/articles/articles/Models.html
#It should be with mode = classification

#For example, with Random Forest
rf &lt;-  rand_forest(trees = 100, mode = "classification") %&gt;%
  set_engine("randomForest")


m &lt;- coBC(learner = rf,N = 3,
          perc.full = 0.7,
          u = 100,
          max.iter = 3) %&gt;% fit(Wine ~ ., data = train)

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)

</code></pre>

<hr>
<h2 id='coBCCombine'>Combining the hypothesis</h2><span id='topic+coBCCombine'></span>

<h3>Description</h3>

<p>This function combines the probabilities predicted by the committee of
classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coBCCombine(h.prob, classes)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coBCCombine_+3A_h.prob">h.prob</code></td>
<td>
<p>A list of probability matrices.</p>
</td></tr>
<tr><td><code id="coBCCombine_+3A_classes">classes</code></td>
<td>
<p>The classes in the same order that appear
in the columns of each matrix in <code>h.prob</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A probability matrix
</p>

<hr>
<h2 id='coBCG'>CoBC generic method</h2><span id='topic+coBCG'></span>

<h3>Description</h3>

<p>CoBC is a semi-supervised learning algorithm with a co-training
style. This algorithm trains <code>N</code> classifiers with the learning scheme defined in
<code>gen.learner</code> using a reduced set of labeled examples. For each iteration, an unlabeled
example is labeled for a classifier if the most confident classifications assigned by the
other <code>N-1</code> classifiers agree on the labeling proposed. The unlabeled examples
candidates are selected randomly from a pool of size <code>u</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coBCG(y, gen.learner, gen.pred, N = 3, perc.full = 0.7, u = 100, max.iter = 50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coBCG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_gen.learner">gen.learner</code></td>
<td>
<p>A function for training <code>N</code> supervised base classifiers.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_gen.pred">gen.pred</code></td>
<td>
<p>A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_n">N</code></td>
<td>
<p>The number of classifiers used as committee members. All these classifiers
are trained using the <code>gen.learner</code> function. Default is 3.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage
of new labeled examples reaches this value the self-labeling process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_u">u</code></td>
<td>
<p>Number of unlabeled instances in the pool. Default is 100.</p>
</td></tr>
<tr><td><code id="coBCG_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to execute in the self-labeling process.
Default is 50.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>coBCG can be helpful in those cases where the method selected as
base classifier needs a <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general coBC method,
please see <code><a href="#topic+coBC">coBC</a></code> function. Essentially, <code>coBC</code>
function is a wrapper of <code>coBCG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class &quot;coBCG&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final <code>N</code> base classifiers trained using the enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of <code>N</code> vectors of indexes related to the training instances
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the <code>N</code> models. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(SSLR)
library(caret)
## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, - cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx] # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
gen.learner1 &lt;- function(indexes, cls)
  caret::knn3(x = xtrain[indexes,], y = cls, k = 1)
gen.pred1 &lt;- function(model, indexes)
  predict(model, xtrain[indexes,])

set.seed(1)

trControl_coBCG &lt;- list(gen.learner = gen.learner1, gen.pred = gen.pred1)
md1 &lt;- train_generic(ytrain, method = "coBCG", trControl = trControl_coBCG)


# Predict probabilities per instances using each model
h.prob &lt;- lapply(
  X = md1$model,
  FUN = function(m) predict(m, xitest)
)
# Combine the predictions
cls1 &lt;- coBCCombine(h.prob, md1$classes)
table(cls1, yitest)

confusionMatrix(cls1, yitest)$overall[1]


## Example: Training from a distance matrix with 1-NN (oneNN) as base classifier.
dtrain &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
gen.learner2 &lt;- function(indexes, cls) {
  m &lt;- SSLR::oneNN(y = cls)
  attr(m, "tra.idxs") &lt;- indexes
  m
}

gen.pred2 &lt;- function(model, indexes) {
  tra.idxs &lt;- attr(model, "tra.idxs")
  d &lt;- dtrain[indexes, tra.idxs]
  prob &lt;- predict(model, d, distance.weighting = "none")
  prob
}

set.seed(1)

trControl_coBCG2 &lt;- list(gen.learner = gen.learner2, gen.pred = gen.pred2)
md2 &lt;- train_generic(ytrain, method = "coBCG", trControl = trControl_coBCG2)



# Predict probabilities per instances using each model
ditest &lt;- proxy::dist(x = xitest, y = xtrain[md2$instances.index,],
                      method = "euclidean", by_rows = TRUE)

h.prob &lt;- list()
ninstances &lt;- nrow(dtrain)
for (i in 1:length(md2$model)) {
  m &lt;- md2$model[[i]]
  D &lt;- ditest[, md2$model.index.map[[i]]]
  h.prob[[i]] &lt;- predict(m, D)
}
# Combine the predictions
cls2 &lt;- coBCCombine(h.prob, md2$classes)
table(cls2, yitest)

confusionMatrix(cls2, yitest)$overall[1]
</code></pre>

<hr>
<h2 id='coBCReg'>General Interface coBCReg model</h2><span id='topic+coBCReg'></span>

<h3>Description</h3>

<p>coBCReg is based on an
ensemble of N diverse regressors. At each iteration and for each regressor, the
companion committee labels the unlabeled examples then the regressor select
the most informative newly-labeled examples for itself, where the selection confidence
is based on estimating the validation error. The final prediction is the
average of the estimates of the N regressors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coBCReg(learner, N = 3, perc.full = 0.7, u = 100, max.iter = 50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coBCReg_+3A_learner">learner</code></td>
<td>
<p>model from parsnip package for training a supervised base classifier
using a set of instances. This model need to have probability predictions</p>
</td></tr>
<tr><td><code id="coBCReg_+3A_n">N</code></td>
<td>
<p>The number of classifiers used as committee members. All these classifiers
are trained using the <code>gen.learner</code> function. Default is 3.</p>
</td></tr>
<tr><td><code id="coBCReg_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage
of new labeled examples reaches this value the self-labeling process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="coBCReg_+3A_u">u</code></td>
<td>
<p>Number of unlabeled instances in the pool. Default is 100.</p>
</td></tr>
<tr><td><code id="coBCReg_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to execute in the self-labeling process.
Default is 50.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For regression tasks, labeling data is very expensive computationally. Its so slow.
</p>


<h3>References</h3>

<p>Mohamed Farouk Abdel-Hady, Mohamed Farouk Abdel-Hady and Günther Palm.<br />
<em>Semi-supervised Learning for Regression with Cotraining by Committee</em><br />
Institute of Neural Information Processing
University of Ulm
D-89069 Ulm, Germany
</p>

<hr>
<h2 id='coBCRegG'>Generic Interface coBCReg model</h2><span id='topic+coBCRegG'></span>

<h3>Description</h3>

<p>coBCReg is based on an
ensemble of N diverse regressors. At each iteration and for each regressor, the
companion committee labels the unlabeled examples then the regressor select
the most informative newly-labeled examples for itself, where the selection confidence
is based on estimating the validation error. The final prediction is the
average of the estimates of the N regressors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coBCRegG(
  y,
  gen.learner,
  gen.pred,
  N = 3,
  perc.full = 0.7,
  u = 100,
  max.iter = 50,
  gr = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coBCRegG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="coBCRegG_+3A_gen.learner">gen.learner</code></td>
<td>
<p>A function for training <code>N</code> supervised base classifiers.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="coBCRegG_+3A_gen.pred">gen.pred</code></td>
<td>
<p>A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
<tr><td><code id="coBCRegG_+3A_n">N</code></td>
<td>
<p>The number of classifiers used as committee members. All these classifiers
are trained using the <code>gen.learner</code> function. Default is 3.</p>
</td></tr>
<tr><td><code id="coBCRegG_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage
of new labeled examples reaches this value the self-labeling process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="coBCRegG_+3A_u">u</code></td>
<td>
<p>Number of unlabeled instances in the pool. Default is 100.</p>
</td></tr>
<tr><td><code id="coBCRegG_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to execute in the self-labeling process.
Default is 50.</p>
</td></tr>
<tr><td><code id="coBCRegG_+3A_gr">gr</code></td>
<td>
<p>growing rate</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For regression tasks, labeling data is very expensive computationally. Its so slow.
</p>


<h3>References</h3>

<p>Mohamed Farouk Abdel-Hady, Mohamed Farouk Abdel-Hady and Günther Palm.<br />
<em>Semi-supervised Learning for Regression with Cotraining by Committee</em><br />
Institute of Neural Information Processing
University of Ulm
D-89069 Ulm, Germany
</p>

<hr>
<h2 id='coffee'>Time series data set</h2><span id='topic+coffee'></span>

<h3>Description</h3>

<p>A dataset containing 56 times series z-normalized. Time series length is 286.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(coffee)
</code></pre>


<h3>Format</h3>

<p>A data frame with 56 rows and 287 variables including the class.
</p>


<h3>Source</h3>

<p><a href="https://www.cs.ucr.edu/~eamonn/time_series_data_2018/">https://www.cs.ucr.edu/~eamonn/time_series_data_2018/</a>
</p>

<hr>
<h2 id='constrained_kmeans'>General Interface Constrained KMeans</h2><span id='topic+constrained_kmeans'></span>

<h3>Description</h3>

<p>The initialization is the same as seeded kmeans,
the difference is that in the following steps the allocation of the clusters in
the labelled data does not change
</p>


<h3>Usage</h3>

<pre><code class='language-R'>constrained_kmeans(max_iter = 10, method = "euclidean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="constrained_kmeans_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum iterations in KMeans. Default is 10</p>
</td></tr>
<tr><td><code id="constrained_kmeans_+3A_method">method</code></td>
<td>
<p>distance method in KMeans: &quot;euclidean&quot;, &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot; or &quot;minkowski&quot;</p>
</td></tr>
</table>


<h3>References</h3>

<p>Sugato Basu, Arindam Banerjee, Raymond Mooney<br />
<em>Semi-supervised clustering by seeding</em><br />
July 2002
In Proceedings of 19th International Conference on Machine Learning
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(SSLR)
library(tidymodels)

data &lt;- iris

set.seed(1)
#% LABELED
cls &lt;- which(colnames(iris) == "Species")

labeled.index &lt;- createDataPartition(data$Species, p = .2, list = FALSE)
data[-labeled.index,cls] &lt;- NA


m &lt;- constrained_kmeans() %&gt;% fit(Species ~ ., data)

#Get labels (assing clusters), type = "raw" return factor
labels &lt;- m %&gt;% cluster_labels()

print(labels)


#Get centers
centers &lt;- m %&gt;% get_centers()

print(centers)

</code></pre>

<hr>
<h2 id='COREG'>General Interface for COREG model</h2><span id='topic+COREG'></span>

<h3>Description</h3>

<p>COREG is a semi-supervised learning for regression with a co-training style.
This technique uses two kNN regressors with different distance metrics.
For each iteration, each regressor labels the unlabeled
example which can be most confidently labeled for the other
learner, where the labeling confidence is estimated through
considering the consistency of the regressor with the labeled
example set. The final prediction is made by averaging the
predictions of both the refined kNN regressors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>COREG(max.iter = 50, k1 = 3, k2 = 5, p1 = 3, p2 = 5, u = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="COREG_+3A_max.iter">max.iter</code></td>
<td>
<p>maximum number of iterations to execute the self-labeling process.
Default is 50.</p>
</td></tr>
<tr><td><code id="COREG_+3A_k1">k1</code></td>
<td>
<p>parameter in first KNN</p>
</td></tr>
<tr><td><code id="COREG_+3A_k2">k2</code></td>
<td>
<p>parameter in second KNN</p>
</td></tr>
<tr><td><code id="COREG_+3A_p1">p1</code></td>
<td>
<p>distance order 1. Default is 3</p>
</td></tr>
<tr><td><code id="COREG_+3A_p2">p2</code></td>
<td>
<p>distance order 1. Default is 5</p>
</td></tr>
<tr><td><code id="COREG_+3A_u">u</code></td>
<td>
<p>Number of unlabeled instances in the pool. Default is 100.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>labeling data is very expensive computationally. Its so slow. For executing this model, we need RANN installed.
</p>


<h3>References</h3>

<p>Zhi-Hua Zhou and Ming Li.<br />
<em>Semi-Supervised Regression with Co-Training.</em><br />
National Laboratory for Novel Software Technology
Nanjing University, Nanjing 210093, China
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(SSLR)

m &lt;- COREG(max.iter = 1)

</code></pre>

<hr>
<h2 id='DecisionTreeClassifier-class'>Class DecisionTreeClassifier</h2><span id='topic+DecisionTreeClassifier-class'></span>

<h3>Description</h3>

<p>Class DecisionTreeClassifier
Slots: max_depth, n_classes_, n_features_,  tree_, classes,  min_samples_split,
min_samples_leaf
</p>

<hr>
<h2 id='democratic'>General Interface for Democratic model</h2><span id='topic+democratic'></span>

<h3>Description</h3>

<p>Democratic Co-Learning is a semi-supervised learning algorithm with a
co-training style. This algorithm trains N classifiers with different learning schemes
defined in list <code>gen.learners</code>. During the iterative process, the multiple classifiers
with different inductive biases label data for each other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>democratic(learners, schemes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="democratic_+3A_learners">learners</code></td>
<td>
<p>List of models from parsnip package for training a supervised base classifier
using a set of instances. This model need to have probability predictions</p>
</td></tr>
<tr><td><code id="democratic_+3A_schemes">schemes</code></td>
<td>
<p>List of schemes (col x names in each learner).
Default is null, it means that learner uses all x columns</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method trains an ensemble of diverse classifiers. To promote the initial diversity
the classifiers must represent different learning schemes.
When x.inst is <code>FALSE</code> all <code>learners</code> defined must be able to learn a classifier
from the precomputed matrix in <code>x</code>.
The iteration process of the algorithm ends when no changes occurs in
any model during a complete iteration.
The generation of the final hypothesis is
produced via a weigthed majority voting.
</p>


<h3>Value</h3>

<p>(When model fit) A list object of class &quot;democratic&quot; containing:
</p>

<dl>
<dt>W</dt><dd><p>A vector with the confidence-weighted vote assigned to each classifier.</p>
</dd>
<dt>model</dt><dd><p>A list with the final N base classifiers trained using the
enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of N vectors of indexes related to the training instances
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the N <code>models</code>. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>preds</dt><dd><p>The functions provided in the <code>preds</code> argument.</p>
</dd>
<dt>preds.pars</dt><dd><p>The set of lists provided in the <code>preds.pars</code> argument.</p>
</dd>
<dt>x.inst</dt><dd><p>The value provided in the <code>x.inst</code> argument.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(tidymodels)
library(caret)
library(SSLR)

data(wine)

set.seed(1)
train.index &lt;- createDataPartition(wine$Wine, p = .7, list = FALSE)
train &lt;- wine[ train.index,]
test  &lt;- wine[-train.index,]

cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(wine$Wine, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA

#We need a model with probability predictions from parsnip
#https://tidymodels.github.io/parsnip/articles/articles/Models.html
#It should be with mode = classification


rf &lt;-  rand_forest(trees = 100, mode = "classification") %&gt;%
  set_engine("randomForest")


bt &lt;-  boost_tree(trees = 100, mode = "classification") %&gt;%
  set_engine("C5.0")


m &lt;- democratic(learners = list(rf,bt)) %&gt;% fit(Wine ~ ., data = train)

#' \donttest{
#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)


#With schemes
set.seed(1)
m &lt;- democratic(learners = list(rf,bt),
                schemes = list(c("Malic.Acid","Ash"), c("Magnesium","Proline")) ) %&gt;%
  fit(Wine ~ ., data = train)


#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)

#'}
</code></pre>

<hr>
<h2 id='democraticCombine'>Combining the hypothesis of the classifiers</h2><span id='topic+democraticCombine'></span>

<h3>Description</h3>

<p>This function combines the probabilities predicted by the set of
classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>democraticCombine(pred, W, classes)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="democraticCombine_+3A_pred">pred</code></td>
<td>
<p>A list with the prediction for each classifier.</p>
</td></tr>
<tr><td><code id="democraticCombine_+3A_w">W</code></td>
<td>
<p>A vector with the confidence-weighted vote assigned to each classifier
during the training process.</p>
</td></tr>
<tr><td><code id="democraticCombine_+3A_classes">classes</code></td>
<td>
<p>the classes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The classification proposed.
</p>

<hr>
<h2 id='democraticG'>Democratic generic method</h2><span id='topic+democraticG'></span>

<h3>Description</h3>

<p>Democratic is a semi-supervised learning algorithm with a co-training
style. This algorithm trains N classifiers with different learning schemes defined in
list <code>gen.learners</code>. During the iterative process, the multiple classifiers with
different inductive biases label data for each other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>democraticG(y, gen.learners, gen.preds)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="democraticG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="democraticG_+3A_gen.learners">gen.learners</code></td>
<td>
<p>A list of functions for training N different supervised base classifiers.
Each function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="democraticG_+3A_gen.preds">gen.preds</code></td>
<td>
<p>A list of functions for predicting the probabilities per classes.
Each function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>democraticG can be helpful in those cases where the method selected as
base classifier needs a <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general democratic method,
please see <code><a href="#topic+democratic">democratic</a></code> function. Essentially, <code>democratic</code>
function is a wrapper of <code>democraticG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class &quot;democraticG&quot; containing:
</p>

<dl>
<dt>W</dt><dd><p>A vector with the confidence-weighted vote assigned to each classifier.</p>
</dd>
<dt>model</dt><dd><p>A list with the final N base classifiers trained using the
enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of N vectors of indexes related to the training instances
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the N <code>models</code>. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
</dl>



<h3>References</h3>

<p>Yan Zhou and Sally Goldman.<br />
<em>Democratic co-learning.</em><br />
In IEEE 16th International Conference on Tools with Artificial Intelligence (ICTAI),
pages 594-602. IEEE, Nov 2004. doi: 10.1109/ICTAI.2004.48.
</p>

<hr>
<h2 id='EMLeastSquaresClassifierSSLR'>General Interface for EMLeastSquaresClassifier model</h2><span id='topic+EMLeastSquaresClassifierSSLR'></span>

<h3>Description</h3>

<p>model from RSSL package
</p>
<p>An Expectation Maximization like approach to Semi-Supervised Least Squares Classification
</p>
<p>As studied in Krijthe &amp; Loog (2016), minimizes the total loss of the labeled and unlabeled objects by finding the weight vector and labels that minimize the total loss. The algorithm proceeds similar to EM, by subsequently applying a weight update and a soft labeling of the unlabeled objects. This is repeated until convergence.
</p>
<p>By default (method=&quot;block&quot;) the weights of the classifier are updated, after which the unknown labels are updated. method=&quot;simple&quot; uses LBFGS to do this update simultaneously. Objective=&quot;responsibility&quot; corresponds to the responsibility based, instead of the label based, objective function in Krijthe &amp; Loog (2016), which is equivalent to hard-label self-learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EMLeastSquaresClassifierSSLR(
  x_center = FALSE,
  scale = FALSE,
  verbose = FALSE,
  intercept = TRUE,
  lambda = 0,
  eps = 1e-09,
  y_scale = FALSE,
  alpha = 1,
  beta = 1,
  init = "supervised",
  method = "block",
  objective = "label",
  save_all = FALSE,
  max_iter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_scale">scale</code></td>
<td>
<p>Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_verbose">verbose</code></td>
<td>
<p>logical; Controls the verbosity of the output</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_lambda">lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_eps">eps</code></td>
<td>
<p>Stopping criterion for the minimization</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_y_scale">y_scale</code></td>
<td>
<p>logical; whether the target vector should be centered</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_alpha">alpha</code></td>
<td>
<p>numeric; the mixture of the new responsibilities and the old in each iteration of the algorithm (default: 1)</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_beta">beta</code></td>
<td>
<p>numeric; value between 0 and 1 that determines how much to move to the new solution from the old solution at each step of the block gradient descent</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_init">init</code></td>
<td>
<p>objective character; &quot;random&quot; for random initialization of labels, &quot;supervised&quot; to use supervised solution as initialization or a numeric vector with a coefficient vector to use to calculate the initialization</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_method">method</code></td>
<td>
<p>character; one of &quot;block&quot;, for block gradient descent or &quot;simple&quot; for LBFGS optimization (default=&quot;block&quot;)</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_objective">objective</code></td>
<td>
<p>character; &quot;responsibility&quot; for hard label self-learning or &quot;label&quot; for soft-label self-learning</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_save_all">save_all</code></td>
<td>
<p>logical; saves all classifiers trained during block gradient descent</p>
</td></tr>
<tr><td><code id="EMLeastSquaresClassifierSSLR_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; maximum number of iterations</p>
</td></tr>
</table>


<h3>References</h3>

<p>Krijthe, J.H. &amp; Loog, M., 2016. Optimistic Semi-supervised Least Squares Classification. In International Conference on Pattern Recognition (To Appear).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
#' \donttest{
library(tidymodels)
library(caret)
library(SSLR)

data(breast)

set.seed(1)
train.index &lt;- createDataPartition(breast$Class, p = .7, list = FALSE)
train &lt;- breast[ train.index,]
test  &lt;- breast[-train.index,]

cls &lt;- which(colnames(breast) == "Class")

#% LABELED
labeled.index &lt;- createDataPartition(breast$Class, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- EMLeastSquaresClassifierSSLR() %&gt;% fit(Class ~ ., data = train)

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Class", estimate = .pred_class)

#Accesing model from RSSL
model &lt;- m$model
#' }
</code></pre>

<hr>
<h2 id='EMNearestMeanClassifierSSLR'>General Interface for EMNearestMeanClassifier model</h2><span id='topic+EMNearestMeanClassifierSSLR'></span>

<h3>Description</h3>

<p>model from RSSL package
Semi-Supervised Nearest Mean Classifier using Expectation Maximization
</p>
<p>Expectation Maximization applied to the nearest mean classifier assuming Gaussian classes with a spherical covariance matrix.
</p>
<p>Starting from the supervised solution, uses the Expectation Maximization algorithm (see Dempster et al. (1977)) to iteratively update the means and shared covariance of the classes (Maximization step) and updates the responsibilities for the unlabeled objects (Expectation step).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EMNearestMeanClassifierSSLR(method = "EM", scale = FALSE, eps = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EMNearestMeanClassifierSSLR_+3A_method">method</code></td>
<td>
<p>character; Currently only &quot;EM&quot;</p>
</td></tr>
<tr><td><code id="EMNearestMeanClassifierSSLR_+3A_scale">scale</code></td>
<td>
<p>Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="EMNearestMeanClassifierSSLR_+3A_eps">eps</code></td>
<td>
<p>Stopping criterion for the maximinimization</p>
</td></tr>
</table>


<h3>References</h3>

<p>Dempster, A., Laird, N. &amp; Rubin, D., 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B, 39(1), pp.1-38.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(tidymodels)
library(caret)
library(SSLR)

data(breast)

set.seed(1)
train.index &lt;- createDataPartition(breast$Class, p = .7, list = FALSE)
train &lt;- breast[ train.index,]
test  &lt;- breast[-train.index,]

cls &lt;- which(colnames(breast) == "Class")

#% LABELED
labeled.index &lt;- createDataPartition(breast$Class, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- EMNearestMeanClassifierSSLR() %&gt;% fit(Class ~ ., data = train)

#Accesing model from RSSL
model &lt;- m$model

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Class", estimate = .pred_class)
</code></pre>

<hr>
<h2 id='EntropyRegularizedLogisticRegressionSSLR'>General Interface for EntropyRegularizedLogisticRegression model</h2><span id='topic+EntropyRegularizedLogisticRegressionSSLR'></span>

<h3>Description</h3>

<p>model from RSSL package
R Implementation of entropy regularized logistic regression implementation
as proposed by Grandvalet &amp; Bengio (2005). An extra term is added to the objective
function of logistic regression that penalizes the entropy of the posterior measured
on the unlabeled examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EntropyRegularizedLogisticRegressionSSLR(
  lambda = 0,
  lambda_entropy = 1,
  intercept = TRUE,
  init = NA,
  scale = FALSE,
  x_center = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EntropyRegularizedLogisticRegressionSSLR_+3A_lambda">lambda</code></td>
<td>
<p>l2 Regularization</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegressionSSLR_+3A_lambda_entropy">lambda_entropy</code></td>
<td>
<p>Weight of the labeled observations compared to the unlabeled observations</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegressionSSLR_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegressionSSLR_+3A_init">init</code></td>
<td>
<p>Initial parameters for the gradient descent</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegressionSSLR_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="EntropyRegularizedLogisticRegressionSSLR_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
</table>


<h3>References</h3>

<p>Grandvalet, Y. &amp; Bengio, Y., 2005. Semi-supervised learning by entropy
minimization. In L. K. Saul, Y. Weiss, &amp; L. Bottou, eds. Advances in Neural Information
Processing Systems 17. Cambridge, MA: MIT Press, pp. 529-536.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(tidymodels)
library(SSLR)

data(breast)

set.seed(1)
train.index &lt;- createDataPartition(breast$Class, p = .7, list = FALSE)
train &lt;- breast[ train.index,]
test  &lt;- breast[-train.index,]

cls &lt;- which(colnames(breast) == "Class")

#% LABELED
labeled.index &lt;- createDataPartition(breast$Class, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- EntropyRegularizedLogisticRegressionSSLR() %&gt;% fit(Class ~ ., data = train)


#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Class", estimate = .pred_class)
</code></pre>

<hr>
<h2 id='fit_decision_tree'>An S4 method to fit decision tree.</h2><span id='topic+fit_decision_tree'></span>

<h3>Description</h3>

<p>An S4 method to fit decision tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_decision_tree(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_decision_tree_+3A_object">object</code></td>
<td>
<p>DecisionTree object</p>
</td></tr>
<tr><td><code id="fit_decision_tree_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='fit_decision_tree+2CDecisionTreeClassifier-method'>Fit decision tree</h2><span id='topic+fit_decision_tree+2CDecisionTreeClassifier-method'></span>

<h3>Description</h3>

<p>method in class DecisionTreeClassifier used to build a Decision Tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'DecisionTreeClassifier'
fit_decision_tree(
  object,
  X,
  y,
  min_samples_split = 20,
  min_samples_leaf = ceiling(min_samples_split/3),
  w = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_decision_tree+2B2CDecisionTreeClassifier-method_+3A_object">object</code></td>
<td>
<p>A DecisionTreeClassifier object</p>
</td></tr>
<tr><td><code id="fit_decision_tree+2B2CDecisionTreeClassifier-method_+3A_x">X</code></td>
<td>
<p>A object that can be coerced as data.frame. Training instances</p>
</td></tr>
<tr><td><code id="fit_decision_tree+2B2CDecisionTreeClassifier-method_+3A_y">y</code></td>
<td>
<p>A vector with the labels of the training instances. In this vector
the unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="fit_decision_tree+2B2CDecisionTreeClassifier-method_+3A_min_samples_split">min_samples_split</code></td>
<td>
<p>the minimum number of observations to do split</p>
</td></tr>
<tr><td><code id="fit_decision_tree+2B2CDecisionTreeClassifier-method_+3A_min_samples_leaf">min_samples_leaf</code></td>
<td>
<p>the minimum number of any terminal leaf node</p>
</td></tr>
<tr><td><code id="fit_decision_tree+2B2CDecisionTreeClassifier-method_+3A_w">w</code></td>
<td>
<p>weight parameter ranging from 0 to 1</p>
</td></tr>
</table>

<hr>
<h2 id='fit_random_forest+2CRandomForestSemisupervised-method'>Fit Random Forest</h2><span id='topic+fit_random_forest+2CRandomForestSemisupervised-method'></span>

<h3>Description</h3>

<p>method in classRandomForestSemisupervised used to build a Decision Tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'RandomForestSemisupervised'
fit_random_forest(
  object,
  X,
  y,
  mtry = 2,
  trees = 500,
  min_n = 2,
  w = 0.5,
  replace = TRUE,
  tree_max_depth = Inf,
  sampsize = if (replace) nrow(X) else ceiling(0.632 * nrow(X)),
  min_samples_leaf = if (!is.null(y) &amp;&amp; !is.factor(y)) 5 else 1,
  allowParallel = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_object">object</code></td>
<td>
<p>A RandomForestSemisupervised object</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_x">X</code></td>
<td>
<p>A object that can be coerced as data.frame. Training instances</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_y">y</code></td>
<td>
<p>A vector with the labels of the training instances. In this vector
the unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_mtry">mtry</code></td>
<td>
<p>number of features in each decision tree</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_trees">trees</code></td>
<td>
<p>number of trees. Default is 5</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_min_n">min_n</code></td>
<td>
<p>number of minimum samples in each tree</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_w">w</code></td>
<td>
<p>weight parameter ranging from 0 to 1</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_replace">replace</code></td>
<td>
<p>replacing type in sampling</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_tree_max_depth">tree_max_depth</code></td>
<td>
<p>maximum tree depth. Default is Inf</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_sampsize">sampsize</code></td>
<td>
<p>Size of sample. Default if (replace) nrow(x) else ceiling(.632*nrow(x))</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_min_samples_leaf">min_samples_leaf</code></td>
<td>
<p>the minimum number of any terminal leaf node</p>
</td></tr>
<tr><td><code id="fit_random_forest+2B2CRandomForestSemisupervised-method_+3A_allowparallel">allowParallel</code></td>
<td>
<p>Execute Random Forest in parallel if doParallel is loaded.
Default is TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of decision trees
</p>

<hr>
<h2 id='fit_x_u'>fit_x_u object</h2><span id='topic+fit_x_u'></span>

<h3>Description</h3>

<p>fit_x_u
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_x_u(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_x_u_+3A_object">object</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="fit_x_u_+3A_...">...</code></td>
<td>
<p>other parameters to be passed</p>
</td></tr>
</table>

<hr>
<h2 id='fit_x_u.model_sslr'>Fit with x , y (labeled data) and unlabeled data (x_U)</h2><span id='topic+fit_x_u.model_sslr'></span>

<h3>Description</h3>

<p>Funtion to fit with x and y and x_U.
Function calcule y with NA values and append in y param
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_sslr'
fit_x_u(object, x = NULL, y = NULL, x_U = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_x_u.model_sslr_+3A_object">object</code></td>
<td>
<p>is the model</p>
</td></tr>
<tr><td><code id="fit_x_u.model_sslr_+3A_x">x</code></td>
<td>
<p>is a data frame or matrix with train dataset without objective feature.
X only have labeled data</p>
</td></tr>
<tr><td><code id="fit_x_u.model_sslr_+3A_y">y</code></td>
<td>
<p>is objective feature with labeled values</p>
</td></tr>
<tr><td><code id="fit_x_u.model_sslr_+3A_x_u">x_U</code></td>
<td>
<p>train unlabeled data without objective feature</p>
</td></tr>
<tr><td><code id="fit_x_u.model_sslr_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='fit_xy.model_sslr'>Fit with x and y</h2><span id='topic+fit_xy.model_sslr'></span>

<h3>Description</h3>

<p>Funtion to fit with x and y
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_sslr'
fit_xy(object, x = NULL, y = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_xy.model_sslr_+3A_object">object</code></td>
<td>
<p>is the model</p>
</td></tr>
<tr><td><code id="fit_xy.model_sslr_+3A_x">x</code></td>
<td>
<p>is a data frame or matrix with train dataset without objective feature.
X have labeled and unlabeled data</p>
</td></tr>
<tr><td><code id="fit_xy.model_sslr_+3A_y">y</code></td>
<td>
<p>is objective feature with labeled values and NA values in unlabeled data</p>
</td></tr>
<tr><td><code id="fit_xy.model_sslr_+3A_...">...</code></td>
<td>
<p>unused in this case</p>
</td></tr>
</table>

<hr>
<h2 id='fit.model_sslr'>Fit with formula and data</h2><span id='topic+fit.model_sslr'></span>

<h3>Description</h3>

<p>Funtion to fit through the formula
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_sslr'
fit(object, formula = NULL, data = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit.model_sslr_+3A_object">object</code></td>
<td>
<p>is the model</p>
</td></tr>
<tr><td><code id="fit.model_sslr_+3A_formula">formula</code></td>
<td>
<p>is the formula</p>
</td></tr>
<tr><td><code id="fit.model_sslr_+3A_data">data</code></td>
<td>
<p>is the total data train</p>
</td></tr>
<tr><td><code id="fit.model_sslr_+3A_...">...</code></td>
<td>
<p>unused in this case</p>
</td></tr>
</table>

<hr>
<h2 id='get_centers'>Get centers model of clustering</h2><span id='topic+get_centers'></span>

<h3>Description</h3>

<p>Centers clustering
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_centers(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_centers_+3A_object">object</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="get_centers_+3A_...">...</code></td>
<td>
<p>other parameters to be passed</p>
</td></tr>
</table>

<hr>
<h2 id='get_centers.model_sslr_fitted'>Cluster labels</h2><span id='topic+get_centers.model_sslr_fitted'></span>

<h3>Description</h3>

<p>Get labels of clusters
raw returns factor or numeric values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_sslr_fitted'
get_centers(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_centers.model_sslr_fitted_+3A_object">object</code></td>
<td>
<p>model_sslr_fitted model built</p>
</td></tr>
<tr><td><code id="get_centers.model_sslr_fitted_+3A_...">...</code></td>
<td>
<p>other parameters to be passed</p>
</td></tr>
</table>

<hr>
<h2 id='get_class_max_prob'>Get most frequented</h2><span id='topic+get_class_max_prob'></span>

<h3>Description</h3>

<p>Get value most frequented in vector
Used in predictions. It calls a predict with type = &quot;prob&quot;
in Decision Tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_class_max_prob(trees, input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_class_max_prob_+3A_trees">trees</code></td>
<td>
<p>trees list</p>
</td></tr>
<tr><td><code id="get_class_max_prob_+3A_input">input</code></td>
<td>
<p>is input to be predicted</p>
</td></tr>
</table>

<hr>
<h2 id='get_class_mean_prob'>Get mean probability over all trees as prob vector</h2><span id='topic+get_class_mean_prob'></span>

<h3>Description</h3>

<p>Get mean probability over all trees as prob vector.
It calls a predict with type = &quot;prob&quot;
in Decision Tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_class_mean_prob(trees, input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_class_mean_prob_+3A_trees">trees</code></td>
<td>
<p>trees list</p>
</td></tr>
<tr><td><code id="get_class_mean_prob_+3A_input">input</code></td>
<td>
<p>is input to be predicted</p>
</td></tr>
</table>

<hr>
<h2 id='get_function'>FUNCTION TO GET FUNCTION METHOD</h2><span id='topic+get_function'></span>

<h3>Description</h3>

<p>FUNCTION TO GET FUNCTION METHOD SPECIFIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_function(met)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_function_+3A_met">met</code></td>
<td>
<p>character</p>
</td></tr>
</table>


<h3>Value</h3>

<p>method_train (function)
</p>

<hr>
<h2 id='get_function_generic'>FUNCTION TO GET FUNCTION METHOD</h2><span id='topic+get_function_generic'></span>

<h3>Description</h3>

<p>FUNCTION TO GET FUNCTION METHOD GENERIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_function_generic(met)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_function_generic_+3A_met">met</code></td>
<td>
<p>character</p>
</td></tr>
</table>


<h3>Value</h3>

<p>method_train (function)
</p>

<hr>
<h2 id='get_levels_categoric'>Function to get gtoup from gini index</h2><span id='topic+get_levels_categoric'></span>

<h3>Description</h3>

<p>Function to get group from gini index.
Used in categorical variable
From: https://freakonometrics.hypotheses.org/20736
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_levels_categoric(column, Y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_levels_categoric_+3A_column">column</code></td>
<td>
<p>is the column</p>
</td></tr>
<tr><td><code id="get_levels_categoric_+3A_y">Y</code></td>
<td>
<p>values</p>
</td></tr>
</table>

<hr>
<h2 id='get_most_frequented'>Get most frequented</h2><span id='topic+get_most_frequented'></span>

<h3>Description</h3>

<p>Get value most frequented in vector
Used in predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_most_frequented(elements)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_most_frequented_+3A_elements">elements</code></td>
<td>
<p>vector with values</p>
</td></tr>
</table>

<hr>
<h2 id='get_value_mean'>Get value mean</h2><span id='topic+get_value_mean'></span>

<h3>Description</h3>

<p>Get value most frequented in vector
Used in predictions. It calls a predict with type = &quot;numeric&quot;
in Decision Tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_value_mean(trees, input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_value_mean_+3A_trees">trees</code></td>
<td>
<p>trees list</p>
</td></tr>
<tr><td><code id="get_value_mean_+3A_input">input</code></td>
<td>
<p>is input to be predicted</p>
</td></tr>
</table>

<hr>
<h2 id='get_x_y'>FUNCTION TO GET REAL X AND Y WITH FORMULA AND DATA</h2><span id='topic+get_x_y'></span>

<h3>Description</h3>

<p>FUNCTION TO GET REAL X AND Y WITH FORMULA AND DATA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_x_y(form, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_x_y_+3A_form">form</code></td>
<td>
<p>formula</p>
</td></tr>
<tr><td><code id="get_x_y_+3A_data">data</code></td>
<td>
<p>data values, matrix, dataframe..</p>
</td></tr>
</table>


<h3>Value</h3>

<p>x (matrix,dataframe...) and y(factor)
</p>

<hr>
<h2 id='gini_or_variance'>Gini or Variance by column</h2><span id='topic+gini_or_variance'></span>

<h3>Description</h3>

<p>function used to calculate the gini coefficient or
variance according to the type of the column. This function is called
for the creation of the decision tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gini_or_variance(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gini_or_variance_+3A_x">X</code></td>
<td>
<p>column to calculate variance or gini</p>
</td></tr>
</table>

<hr>
<h2 id='gini_prob'>Function to compute Gini index</h2><span id='topic+gini_prob'></span>

<h3>Description</h3>

<p>Function to compute Gini index
From: https://freakonometrics.hypotheses.org/20736
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gini_prob(y, classe)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gini_prob_+3A_y">y</code></td>
<td>
<p>values</p>
</td></tr>
<tr><td><code id="gini_prob_+3A_classe">classe</code></td>
<td>
<p>classes</p>
</td></tr>
</table>

<hr>
<h2 id='GRFClassifierSSLR'>General Interface for GRFClassifier (Label propagation using Gaussian Random Fields and Harmonic) model</h2><span id='topic+GRFClassifierSSLR'></span>

<h3>Description</h3>

<p>model from RSSL package
Implements the approach proposed in Zhu et al. (2003) to label propagation over
an affinity graph. Note, as in the original paper, we consider the transductive
scenario, so the implementation does not generalize to out of sample predictions.
The approach minimizes the squared difference in labels assigned to different objects,
where the contribution of each difference to the loss is weighted by the affinity between
the objects. The default in this implementation is to use a knn adjacency matrix based on euclidean
distance to determine this weight. Setting adjacency=&quot;heat&quot; will use an RBF kernel over
euclidean distances between objects to determine the weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GRFClassifierSSLR(
  adjacency = "nn",
  adjacency_distance = "euclidean",
  adjacency_k = 6,
  adjacency_sigma = 0.1,
  class_mass_normalization = TRUE,
  scale = FALSE,
  x_center = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GRFClassifierSSLR_+3A_adjacency">adjacency</code></td>
<td>
<p>character; &quot;nn&quot; for nearest neighbour graph or &quot;heat&quot; for radial basis adjacency matrix</p>
</td></tr>
<tr><td><code id="GRFClassifierSSLR_+3A_adjacency_distance">adjacency_distance</code></td>
<td>
<p>character; distance metric for nearest neighbour adjacency matrix</p>
</td></tr>
<tr><td><code id="GRFClassifierSSLR_+3A_adjacency_k">adjacency_k</code></td>
<td>
<p>integer; number of neighbours for the nearest neighbour adjacency matrix</p>
</td></tr>
<tr><td><code id="GRFClassifierSSLR_+3A_adjacency_sigma">adjacency_sigma</code></td>
<td>
<p>double; width of the rbf adjacency matrix</p>
</td></tr>
<tr><td><code id="GRFClassifierSSLR_+3A_class_mass_normalization">class_mass_normalization</code></td>
<td>
<p>logical; Should the Class Mass Normalization heuristic be applied? (default: TRUE)</p>
</td></tr>
<tr><td><code id="GRFClassifierSSLR_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="GRFClassifierSSLR_+3A_x_center">x_center</code></td>
<td>
<p>logical; Should the features be centered?</p>
</td></tr>
</table>


<h3>References</h3>

<p>Zhu, X., Ghahramani, Z. &amp; Lafferty, J., 2003
Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International Conference on Machine Learning. pp. 912-919.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(SSLR)
library(tidymodels)

data(wine)


cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(wine$Wine, p = .2, list = FALSE)
wine[-labeled.index,cls] &lt;- NA


m &lt;- GRFClassifierSSLR() %&gt;% fit(Wine ~ ., data = wine)

#Accesing model from RSSL
model &lt;- m$model

#Predictions of unlabeled
preds_unlabeled &lt;- m %&gt;% predictions()
print(preds_unlabeled)

preds_unlabeled &lt;- m %&gt;% predictions(type = "raw")
print(preds_unlabeled)

#Total
y_total &lt;- wine[,cls]
y_total[-labeled.index] &lt;- preds_unlabeled
</code></pre>

<hr>
<h2 id='grow_tree'>An S4 method to grow tree.</h2><span id='topic+grow_tree'></span>

<h3>Description</h3>

<p>An S4 method to grow tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grow_tree(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grow_tree_+3A_object">object</code></td>
<td>
<p>DecisionTree object</p>
</td></tr>
<tr><td><code id="grow_tree_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='grow_tree+2CDecisionTreeClassifier-method'>Function grow tree</h2><span id='topic+grow_tree+2CDecisionTreeClassifier-method'></span>

<h3>Description</h3>

<p>Function to grow tree in Decision Tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'DecisionTreeClassifier'
grow_tree(object, X, y, parms, depth = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grow_tree+2B2CDecisionTreeClassifier-method_+3A_object">object</code></td>
<td>
<p>DecisionTree instance</p>
</td></tr>
<tr><td><code id="grow_tree+2B2CDecisionTreeClassifier-method_+3A_x">X</code></td>
<td>
<p>data values</p>
</td></tr>
<tr><td><code id="grow_tree+2B2CDecisionTreeClassifier-method_+3A_y">y</code></td>
<td>
<p>classes</p>
</td></tr>
<tr><td><code id="grow_tree+2B2CDecisionTreeClassifier-method_+3A_parms">parms</code></td>
<td>
<p>parameters for grow tree</p>
</td></tr>
<tr><td><code id="grow_tree+2B2CDecisionTreeClassifier-method_+3A_depth">depth</code></td>
<td>
<p>depth in tree</p>
</td></tr>
</table>

<hr>
<h2 id='knn_regression'>knn_regression</h2><span id='topic+knn_regression'></span>

<h3>Description</h3>

<p>create model knn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knn_regression(k, x, y, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knn_regression_+3A_k">k</code></td>
<td>
<p>parameter in KNN model</p>
</td></tr>
<tr><td><code id="knn_regression_+3A_x">x</code></td>
<td>
<p>data</p>
</td></tr>
<tr><td><code id="knn_regression_+3A_y">y</code></td>
<td>
<p>vector labeled data</p>
</td></tr>
<tr><td><code id="knn_regression_+3A_p">p</code></td>
<td>
<p>distance order</p>
</td></tr>
</table>

<hr>
<h2 id='LaplacianSVMSSLR'>General Interface for LaplacianSVM model</h2><span id='topic+LaplacianSVMSSLR'></span>

<h3>Description</h3>

<p>model from RSSL package
Manifold regularization applied to the support vector machine as proposed in Belkin et al. (2006). As an adjacency matrix, we use the k nearest neighbour graph based on a chosen distance (default: euclidean).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LaplacianSVMSSLR(
  lambda = 1,
  gamma = 1,
  scale = TRUE,
  kernel = kernlab::vanilladot(),
  adjacency_distance = "euclidean",
  adjacency_k = 6,
  normalized_laplacian = FALSE,
  eps = 1e-09
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LaplacianSVMSSLR_+3A_lambda">lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td></tr>
<tr><td><code id="LaplacianSVMSSLR_+3A_gamma">gamma</code></td>
<td>
<p>numeric; Weight of the unlabeled data</p>
</td></tr>
<tr><td><code id="LaplacianSVMSSLR_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="LaplacianSVMSSLR_+3A_kernel">kernel</code></td>
<td>
<p>kernlab::kernel to use</p>
</td></tr>
<tr><td><code id="LaplacianSVMSSLR_+3A_adjacency_distance">adjacency_distance</code></td>
<td>
<p>character; distance metric used to construct adjacency graph from the dist function. Default: &quot;euclidean&quot;</p>
</td></tr>
<tr><td><code id="LaplacianSVMSSLR_+3A_adjacency_k">adjacency_k</code></td>
<td>
<p>integer; Number of of neighbours used to construct adjacency graph.</p>
</td></tr>
<tr><td><code id="LaplacianSVMSSLR_+3A_normalized_laplacian">normalized_laplacian</code></td>
<td>
<p>logical; If TRUE use the normalized Laplacian, otherwise, the Laplacian is used</p>
</td></tr>
<tr><td><code id="LaplacianSVMSSLR_+3A_eps">eps</code></td>
<td>
<p>numeric; Small value to ensure positive definiteness of the matrix in the QP formulation</p>
</td></tr>
</table>


<h3>References</h3>

<p>Belkin, M., Niyogi, P. &amp; Sindhwani, V., 2006. Manifold regularization:
A geometric framework for learning from labeled and unlabeled examples. Journal of
Machine Learning Research, 7, pp.2399-2434.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(tidymodels)
library(SSLR)

data(breast)

set.seed(1)
train.index &lt;- createDataPartition(breast$Class, p = .7, list = FALSE)
train &lt;- breast[ train.index,]
test  &lt;- breast[-train.index,]

cls &lt;- which(colnames(breast) == "Class")

#% LABELED
labeled.index &lt;- createDataPartition(breast$Class, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA

library(kernlab)
m &lt;- LaplacianSVMSSLR(kernel=kernlab::vanilladot()) %&gt;%
  fit(Class ~ ., data = train)


#Accesing model from RSSL
model &lt;- m$model

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Class", estimate = .pred_class)
</code></pre>

<hr>
<h2 id='lcvqeSSLR'>General LCVQE Algorithm</h2><span id='topic+lcvqeSSLR'></span>

<h3>Description</h3>

<p>Model from conclust <br />
This function takes an unlabeled dataset and two lists of must-link and cannot-link constraints
as input and produce a clustering as output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lcvqeSSLR(n_clusters = NULL, mustLink = NULL, cantLink = NULL, max_iter = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lcvqeSSLR_+3A_n_clusters">n_clusters</code></td>
<td>
<p>A number of clusters to be considered. Default is NULL (num classes)</p>
</td></tr>
<tr><td><code id="lcvqeSSLR_+3A_mustlink">mustLink</code></td>
<td>
<p>A list of must-link constraints. NULL Default, constrints same label</p>
</td></tr>
<tr><td><code id="lcvqeSSLR_+3A_cantlink">cantLink</code></td>
<td>
<p>A list of cannot-link constraints. NULL Default, constrints with different label</p>
</td></tr>
<tr><td><code id="lcvqeSSLR_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum iterations in KMeans. Default is 2</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This models only returns labels, not centers
</p>


<h3>References</h3>

<p>Dan Pelleg, Dorit Baras<br />
<em>K-means with large and noisy constraint sets</em><br />
2007
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(SSLR)
library(tidymodels)

data &lt;- iris

set.seed(1)
#% LABELED
cls &lt;- which(colnames(iris) == "Species")

labeled.index &lt;- createDataPartition(data$Species, p = .2, list = FALSE)
data[-labeled.index,cls] &lt;- NA


m &lt;- lcvqeSSLR(max_iter = 1) %&gt;% fit(Species ~ ., data)

#Get labels (assing clusters), type = "raw" return factor
labels &lt;- m %&gt;% cluster_labels()

print(labels)


</code></pre>

<hr>
<h2 id='LinearTSVMSSLR'>General Interface for LinearTSVM model</h2><span id='topic+LinearTSVMSSLR'></span>

<h3>Description</h3>

<p>model from RSSL package
Implementation of the Linear Support Vector Classifier. Can be solved in the Dual formulation, which is equivalent to <code><a href="RSSL.html#topic+SVM">SVM</a></code> or the Primal formulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LinearTSVMSSLR(
  C = 1,
  Cstar = 0.1,
  s = 0,
  x_center = FALSE,
  scale = FALSE,
  eps = 1e-06,
  verbose = FALSE,
  init = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LinearTSVMSSLR_+3A_c">C</code></td>
<td>
<p>Cost variable</p>
</td></tr>
<tr><td><code id="LinearTSVMSSLR_+3A_cstar">Cstar</code></td>
<td>
<p>numeric; Cost parameter of the unlabeled objects</p>
</td></tr>
<tr><td><code id="LinearTSVMSSLR_+3A_s">s</code></td>
<td>
<p>numeric; parameter controlling the loss function of the unlabeled objects</p>
</td></tr>
<tr><td><code id="LinearTSVMSSLR_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="LinearTSVMSSLR_+3A_scale">scale</code></td>
<td>
<p>Whether a z-transform should be applied (default: TRUE)</p>
</td></tr>
<tr><td><code id="LinearTSVMSSLR_+3A_eps">eps</code></td>
<td>
<p>Small value to ensure positive definiteness of the matrix in QP formulation</p>
</td></tr>
<tr><td><code id="LinearTSVMSSLR_+3A_verbose">verbose</code></td>
<td>
<p>logical; Controls the verbosity of the output</p>
</td></tr>
<tr><td><code id="LinearTSVMSSLR_+3A_init">init</code></td>
<td>
<p>numeric; Initial classifier parameters to start the convex concave procedure</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(tidymodels)
library(SSLR)

data(breast)

set.seed(1)
train.index &lt;- createDataPartition(breast$Class, p = .7, list = FALSE)
train &lt;- breast[ train.index,]
test  &lt;- breast[-train.index,]

cls &lt;- which(colnames(breast) == "Class")

#% LABELED
labeled.index &lt;- createDataPartition(breast$Class, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- LinearTSVMSSLR() %&gt;% fit(Class ~ ., data = train)


#Accesing model from RSSL
model &lt;- m$model
</code></pre>

<hr>
<h2 id='load_conclust'>Load conclust</h2><span id='topic+load_conclust'></span>

<h3>Description</h3>

<p>function to load conclust package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_conclust()
</code></pre>

<hr>
<h2 id='load_parsnip'>Load parsnip</h2><span id='topic+load_parsnip'></span>

<h3>Description</h3>

<p>function to load parsnip package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_parsnip()
</code></pre>

<hr>
<h2 id='load_RANN'>Load parsnip</h2><span id='topic+load_RANN'></span>

<h3>Description</h3>

<p>function to load parsnip package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_RANN()
</code></pre>

<hr>
<h2 id='load_RSSL'>Load RSSL</h2><span id='topic+load_RSSL'></span>

<h3>Description</h3>

<p>function to load RSSL package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_RSSL()
</code></pre>

<hr>
<h2 id='MCNearestMeanClassifierSSLR'>General Interface for MCNearestMeanClassifier (Moment Constrained Semi-supervised Nearest Mean Classifier) model</h2><span id='topic+MCNearestMeanClassifierSSLR'></span>

<h3>Description</h3>

<p>model from RSSL package
Update the means based on the moment constraints as defined in Loog (2010).
The means estimated using the labeled data are updated by making sure their
weighted mean corresponds to the overall mean on all (labeled and unlabeled) data.
Optionally, the estimated variance of the classes can be re-estimated after this
update is applied by setting update_sigma to <code>TRUE</code>. To get the true nearest mean
classifier, rather than estimate the class priors, set them to equal priors using, for
instance <code>prior=matrix(0.5,2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MCNearestMeanClassifierSSLR(
  update_sigma = FALSE,
  prior = NULL,
  x_center = FALSE,
  scale = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MCNearestMeanClassifierSSLR_+3A_update_sigma">update_sigma</code></td>
<td>
<p>logical; Whether the estimate of the variance should be updated
after the means have been updated using the unlabeled data</p>
</td></tr>
<tr><td><code id="MCNearestMeanClassifierSSLR_+3A_prior">prior</code></td>
<td>
<p>matrix; Class priors for the classes</p>
</td></tr>
<tr><td><code id="MCNearestMeanClassifierSSLR_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="MCNearestMeanClassifierSSLR_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Loog, M., 2010. Constrained Parameter Estimation for
Semi-Supervised Learning: The Case of the Nearest Mean Classifier.
In Proceedings of the 2010 European Conference on Machine
learning and Knowledge Discovery in Databases. pp. 291-304.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(tidymodels)
library(SSLR)

data(breast)

set.seed(1)
train.index &lt;- createDataPartition(breast$Class, p = .7, list = FALSE)
train &lt;- breast[ train.index,]
test  &lt;- breast[-train.index,]

cls &lt;- which(colnames(breast) == "Class")

#% LABELED
labeled.index &lt;- createDataPartition(breast$Class, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- MCNearestMeanClassifierSSLR() %&gt;% fit(Class ~ ., data = train)

#Accesing model from RSSL
model &lt;- m$model

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Class", estimate = .pred_class)
</code></pre>

<hr>
<h2 id='mpckmSSLR'>General Interface MPC K-Means Algorithm</h2><span id='topic+mpckmSSLR'></span>

<h3>Description</h3>

<p>Model from conclust <br />
This function takes an unlabeled dataset and two lists of must-link and cannot-link constraints
as input and produce a clustering as output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpckmSSLR(n_clusters = NULL, mustLink = NULL, cantLink = NULL, max_iter = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mpckmSSLR_+3A_n_clusters">n_clusters</code></td>
<td>
<p>A number of clusters to be considered. Default is NULL (num classes)</p>
</td></tr>
<tr><td><code id="mpckmSSLR_+3A_mustlink">mustLink</code></td>
<td>
<p>A list of must-link constraints. NULL Default, constrints same label</p>
</td></tr>
<tr><td><code id="mpckmSSLR_+3A_cantlink">cantLink</code></td>
<td>
<p>A list of cannot-link constraints. NULL Default, constrints with different label</p>
</td></tr>
<tr><td><code id="mpckmSSLR_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum iterations in KMeans. Default is 10</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This models only returns labels, not centers
</p>


<h3>References</h3>

<p>Bilenko, Basu, Mooney <br />
<em>Integrating Constraints and Metric Learning in Semi-Supervised Clustering</em><br />
2004
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(SSLR)
library(tidymodels)

data &lt;- iris

set.seed(1)
#% LABELED
cls &lt;- which(colnames(iris) == "Species")

labeled.index &lt;- createDataPartition(data$Species, p = .2, list = FALSE)
data[-labeled.index,cls] &lt;- NA



m &lt;- mpckmSSLR() %&gt;% fit(Species ~ ., data)

#Get labels (assing clusters), type = "raw" return factor
labels &lt;- m %&gt;% cluster_labels()

print(labels)


</code></pre>

<hr>
<h2 id='newDecisionTree'>Function to create DecisionTree</h2><span id='topic+newDecisionTree'></span>

<h3>Description</h3>

<p>Function to create DecisionTree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>newDecisionTree(max_depth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="newDecisionTree_+3A_max_depth">max_depth</code></td>
<td>
<p>max depth in tree</p>
</td></tr>
</table>

<hr>
<h2 id='Node-class'>Class Node for Decision Tree</h2><span id='topic+Node-class'></span>

<h3>Description</h3>

<p>Class Node for Decision Tree
Slots: gini, num_samples, num_samples_per_class, predicted_class_value, feature_index
threshold, left, right, probabilities
</p>

<hr>
<h2 id='nullOrNumericOrCharacter-class'>An S4 class to represent a class with more types values: null, numeric or character</h2><span id='topic+nullOrNumericOrCharacter-class'></span>

<h3>Description</h3>

<p>An S4 class to represent a class with more types values: null, numeric or character
</p>

<hr>
<h2 id='oneNN'>1-NN supervised classifier builder</h2><span id='topic+oneNN'></span>

<h3>Description</h3>

<p>Build a model using the given data to be able
to predict the label or the probabilities of other instances,
according to 1-NN algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oneNN(x = NULL, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oneNN_+3A_x">x</code></td>
<td>
<p>This argument is not used, the reason why he gets is to fulfill an agreement</p>
</td></tr>
<tr><td><code id="oneNN_+3A_y">y</code></td>
<td>
<p>a vector with the labels of training instances</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A model with the data needed to use 1-NN
</p>

<hr>
<h2 id='predict_inputs'>An S4 method to predict inputs.</h2><span id='topic+predict_inputs'></span>

<h3>Description</h3>

<p>An S4 method to predict inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_inputs(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_inputs_+3A_object">object</code></td>
<td>
<p>DecisionTree object</p>
</td></tr>
<tr><td><code id="predict_inputs_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predict_inputs+2CDecisionTreeClassifier-method'>Predict inputs Decision Tree</h2><span id='topic+predict_inputs+2CDecisionTreeClassifier-method'></span>

<h3>Description</h3>

<p>Function to predict one input in Decision Tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'DecisionTreeClassifier'
predict_inputs(object, inputs, type = "class")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_inputs+2B2CDecisionTreeClassifier-method_+3A_object">object</code></td>
<td>
<p>DecisionTree object</p>
</td></tr>
<tr><td><code id="predict_inputs+2B2CDecisionTreeClassifier-method_+3A_inputs">inputs</code></td>
<td>
<p>inputs to be predicted</p>
</td></tr>
<tr><td><code id="predict_inputs+2B2CDecisionTreeClassifier-method_+3A_type">type</code></td>
<td>
<p>type prediction, class or prob</p>
</td></tr>
</table>

<hr>
<h2 id='predict+2CDecisionTreeClassifier-method'>Function to predict inputs in Decision Tree</h2><span id='topic+predict+2CDecisionTreeClassifier-method'></span>

<h3>Description</h3>

<p>Function to predict inputs in Decision Tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'DecisionTreeClassifier'
predict(object, inputs, type = "class")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict+2B2CDecisionTreeClassifier-method_+3A_object">object</code></td>
<td>
<p>The Decision Tree object</p>
</td></tr>
<tr><td><code id="predict+2B2CDecisionTreeClassifier-method_+3A_inputs">inputs</code></td>
<td>
<p>data to be predicted</p>
</td></tr>
<tr><td><code id="predict+2B2CDecisionTreeClassifier-method_+3A_type">type</code></td>
<td>
<p>Is param to define the type of predict.
It can be &quot;class&quot;, to get class labels
Or &quot;prob&quot; to get probabilites for class in each input.
Default is &quot;class&quot;</p>
</td></tr>
</table>

<hr>
<h2 id='predict+2CRandomForestSemisupervised-method'>Function to predict inputs in Decision Tree</h2><span id='topic+predict+2CRandomForestSemisupervised-method'></span>

<h3>Description</h3>

<p>Function to predict inputs in Decision Tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'RandomForestSemisupervised'
predict(
  object,
  inputs,
  type = "class",
  confident = "max_prob",
  allowParallel = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict+2B2CRandomForestSemisupervised-method_+3A_object">object</code></td>
<td>
<p>The Decision Tree object</p>
</td></tr>
<tr><td><code id="predict+2B2CRandomForestSemisupervised-method_+3A_inputs">inputs</code></td>
<td>
<p>data to be predicted</p>
</td></tr>
<tr><td><code id="predict+2B2CRandomForestSemisupervised-method_+3A_type">type</code></td>
<td>
<p>class raw</p>
</td></tr>
<tr><td><code id="predict+2B2CRandomForestSemisupervised-method_+3A_confident">confident</code></td>
<td>
<p>Is param to define the type of predict.
It can be &quot;max_prob&quot;, to get class with sum of probability is the maximum
Or &quot;vote&quot; to get the most frequented class in all trees.
Default is &quot;max_prob&quot;</p>
</td></tr>
<tr><td><code id="predict+2B2CRandomForestSemisupervised-method_+3A_allowparallel">allowParallel</code></td>
<td>
<p>Execute Random Forest in parallel if doParallel is loaded.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.coBC'>Predictions of the coBC method</h2><span id='topic+predict.coBC'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>coBC</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coBC'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.coBC_+3A_object">object</code></td>
<td>
<p>coBC model built with the <code><a href="#topic+coBC">coBC</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.coBC_+3A_x">x</code></td>
<td>
<p>An object that can be coerced to a matrix.
Depending on how the model was built, <code>x</code> is interpreted as a matrix
with the distances between the unseen instances and the selected training instances,
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.coBC_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+coBC">coBC</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.COREG'>Predictions of the COREG method</h2><span id='topic+predict.COREG'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>COREG</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'COREG'
predict(object, x, type = "numeric", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.COREG_+3A_object">object</code></td>
<td>
<p>Self-training model built with the <code><a href="#topic+COREG">COREG</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.COREG_+3A_x">x</code></td>
<td>
<p>A object that is data</p>
</td></tr>
<tr><td><code id="predict.COREG_+3A_type">type</code></td>
<td>
<p>of predict in principal model (numeric)</p>
</td></tr>
<tr><td><code id="predict.COREG_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+COREG">COREG</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned (numeric).
</p>

<hr>
<h2 id='predict.democratic'>Predictions of the Democratic method</h2><span id='topic+predict.democratic'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>democratic</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'democratic'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.democratic_+3A_object">object</code></td>
<td>
<p>Democratic model built with the <code><a href="#topic+democratic">democratic</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.democratic_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix
with the distances between the unseen instances and the selected training instances,
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.democratic_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+democratic">democratic</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.EMLeastSquaresClassifierSSLR'>Predict EMLeastSquaresClassifierSSLR</h2><span id='topic+predict.EMLeastSquaresClassifierSSLR'></span>

<h3>Description</h3>

<p>Predict EMLeastSquaresClassifierSSLR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EMLeastSquaresClassifierSSLR'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.EMLeastSquaresClassifierSSLR_+3A_object">object</code></td>
<td>
<p>is the object</p>
</td></tr>
<tr><td><code id="predict.EMLeastSquaresClassifierSSLR_+3A_x">x</code></td>
<td>
<p>is the dataset</p>
</td></tr>
<tr><td><code id="predict.EMLeastSquaresClassifierSSLR_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.EMNearestMeanClassifierSSLR'>Predict EMNearestMeanClassifierSSLR</h2><span id='topic+predict.EMNearestMeanClassifierSSLR'></span>

<h3>Description</h3>

<p>Predict EMNearestMeanClassifierSSLR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EMNearestMeanClassifierSSLR'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.EMNearestMeanClassifierSSLR_+3A_object">object</code></td>
<td>
<p>is the object</p>
</td></tr>
<tr><td><code id="predict.EMNearestMeanClassifierSSLR_+3A_x">x</code></td>
<td>
<p>is the dataset</p>
</td></tr>
<tr><td><code id="predict.EMNearestMeanClassifierSSLR_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.EntropyRegularizedLogisticRegressionSSLR'>Predict EntropyRegularizedLogisticRegressionSSLR</h2><span id='topic+predict.EntropyRegularizedLogisticRegressionSSLR'></span>

<h3>Description</h3>

<p>Predict EntropyRegularizedLogisticRegressionSSLR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'EntropyRegularizedLogisticRegressionSSLR'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.EntropyRegularizedLogisticRegressionSSLR_+3A_object">object</code></td>
<td>
<p>is the object</p>
</td></tr>
<tr><td><code id="predict.EntropyRegularizedLogisticRegressionSSLR_+3A_x">x</code></td>
<td>
<p>is the dataset</p>
</td></tr>
<tr><td><code id="predict.EntropyRegularizedLogisticRegressionSSLR_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.LaplacianSVMSSLR'>Predict LaplacianSVMSSLR</h2><span id='topic+predict.LaplacianSVMSSLR'></span>

<h3>Description</h3>

<p>Predict LaplacianSVMSSLR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'LaplacianSVMSSLR'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.LaplacianSVMSSLR_+3A_object">object</code></td>
<td>
<p>is the object</p>
</td></tr>
<tr><td><code id="predict.LaplacianSVMSSLR_+3A_x">x</code></td>
<td>
<p>is the dataset</p>
</td></tr>
<tr><td><code id="predict.LaplacianSVMSSLR_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.LinearTSVMSSLR'>Predict LinearTSVMSSLR</h2><span id='topic+predict.LinearTSVMSSLR'></span>

<h3>Description</h3>

<p>Predict LinearTSVMSSLR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'LinearTSVMSSLR'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.LinearTSVMSSLR_+3A_object">object</code></td>
<td>
<p>is the object</p>
</td></tr>
<tr><td><code id="predict.LinearTSVMSSLR_+3A_x">x</code></td>
<td>
<p>is the dataset</p>
</td></tr>
<tr><td><code id="predict.LinearTSVMSSLR_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.MCNearestMeanClassifierSSLR'>Predict MCNearestMeanClassifierSSLR</h2><span id='topic+predict.MCNearestMeanClassifierSSLR'></span>

<h3>Description</h3>

<p>Predict MCNearestMeanClassifierSSLR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'MCNearestMeanClassifierSSLR'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.MCNearestMeanClassifierSSLR_+3A_object">object</code></td>
<td>
<p>is the object</p>
</td></tr>
<tr><td><code id="predict.MCNearestMeanClassifierSSLR_+3A_x">x</code></td>
<td>
<p>is the dataset</p>
</td></tr>
<tr><td><code id="predict.MCNearestMeanClassifierSSLR_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.model_sslr_fitted'>Predictions of model_sslr_fitted class</h2><span id='topic+predict.model_sslr_fitted'></span>

<h3>Description</h3>

<p>Predicts from model. There are different types: class, prob, raw
class returns tibble with one column
prob returns tibble with probabilities class columns
raw returns factor or numeric values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_sslr_fitted'
predict(object, x, type = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.model_sslr_fitted_+3A_object">object</code></td>
<td>
<p>model_sslr_fitted model built.</p>
</td></tr>
<tr><td><code id="predict.model_sslr_fitted_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix
with the distances between the unseen instances and the selected training instances,
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.model_sslr_fitted_+3A_type">type</code></td>
<td>
<p>of predict in principal model: class, raw, prob, vote, max_prob, numeric</p>
</td></tr>
<tr><td><code id="predict.model_sslr_fitted_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>tibble or vector.
</p>

<hr>
<h2 id='predict.OneNN'>Model Predictions</h2><span id='topic+predict.OneNN'></span>

<h3>Description</h3>

<p>This function predicts the class label of instances or its probability of
pertaining to each class based on the distance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'OneNN'
predict(object, dists, type = "prob", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.OneNN_+3A_object">object</code></td>
<td>
<p>A model of class OneNN built with <code><a href="#topic+oneNN">oneNN</a></code></p>
</td></tr>
<tr><td><code id="predict.OneNN_+3A_dists">dists</code></td>
<td>
<p>A matrix of distances between the instances to classify (by rows) and
the instances used to train the model (by column)</p>
</td></tr>
<tr><td><code id="predict.OneNN_+3A_type">type</code></td>
<td>
<p>A string that can take two values: <code>"class"</code> for computing the class of
the instances or <code>"prob"</code> for computing the probabilities of belonging to each class.</p>
</td></tr>
<tr><td><code id="predict.OneNN_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>type</code> is equal to <code>"class"</code> a vector of length equal to the rows number
of matrix <code>dists</code>, containing the predicted labels. If <code>type</code> is equal
to <code>"prob"</code> it returns a matrix which has <code>nrow(dists)</code> rows and a column for every
class, where each cell represents the probability that the instance belongs to the class,
according to 1NN.
</p>

<hr>
<h2 id='predict.RandomForestSemisupervised_fitted'>Predictions of the SSLRDecisionTree_fitted method</h2><span id='topic+predict.RandomForestSemisupervised_fitted'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the RandomForestSemisupervised_fitted model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'RandomForestSemisupervised_fitted'
predict(object, x, type = "class", confident = "max_prob", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.RandomForestSemisupervised_fitted_+3A_object">object</code></td>
<td>
<p>RandomForestSemisupervised_fitted.</p>
</td></tr>
<tr><td><code id="predict.RandomForestSemisupervised_fitted_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix
with the distances between the unseen instances and the selected training instances,
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.RandomForestSemisupervised_fitted_+3A_type">type</code></td>
<td>
<p>of predict in principal model</p>
</td></tr>
<tr><td><code id="predict.RandomForestSemisupervised_fitted_+3A_confident">confident</code></td>
<td>
<p>Is param to define the type of predict.
It can be &quot;max_prob&quot;, to get class with sum of probability is the maximum
Or &quot;vote&quot; to get the most frequented class in all trees.
Default is &quot;max_prob&quot;</p>
</td></tr>
<tr><td><code id="predict.RandomForestSemisupervised_fitted_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.selfTraining'>Predictions of the Self-training method</h2><span id='topic+predict.selfTraining'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>selfTraining</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'selfTraining'
predict(object, x, type = "class", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.selfTraining_+3A_object">object</code></td>
<td>
<p>Self-training model built with the <code><a href="#topic+selfTraining">selfTraining</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.selfTraining_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix
with the distances between the unseen instances and the selected training instances,
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.selfTraining_+3A_type">type</code></td>
<td>
<p>of predict in principal model</p>
</td></tr>
<tr><td><code id="predict.selfTraining_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+selfTraining">selfTraining</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.setred'>Predictions of the SETRED method</h2><span id='topic+predict.setred'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>setred</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'setred'
predict(object, x, col_name = ".pred_class", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.setred_+3A_object">object</code></td>
<td>
<p>SETRED model built with the <code><a href="#topic+setred">setred</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.setred_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix
with the distances between the unseen instances and the selected training instances,
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.setred_+3A_col_name">col_name</code></td>
<td>
<p>is the colname from returned tibble in class type.
The same from parsnip and tidymodels
Default is .pred_clas</p>
</td></tr>
<tr><td><code id="predict.setred_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+setred">setred</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.snnrce'>Predictions of the SNNRCE method</h2><span id='topic+predict.snnrce'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>snnrce</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'snnrce'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.snnrce_+3A_object">object</code></td>
<td>
<p>SNNRCE model built with the <code><a href="#topic+snnrce">snnrce</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.snnrce_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix
with the distances between the unseen instances and the selected training instances,
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.snnrce_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+snnrce">snnrce</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.snnrceG'>Predictions of the SNNRCE method</h2><span id='topic+predict.snnrceG'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>snnrceG</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'snnrceG'
predict(object, D, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.snnrceG_+3A_object">object</code></td>
<td>
<p>model instance</p>
</td></tr>
<tr><td><code id="predict.snnrceG_+3A_d">D</code></td>
<td>
<p>distance matrix</p>
</td></tr>
<tr><td><code id="predict.snnrceG_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.SSLRDecisionTree_fitted'>Predictions of the SSLRDecisionTree_fitted method</h2><span id='topic+predict.SSLRDecisionTree_fitted'></span>

<h3>Description</h3>

<p>Predicts the label of instances SSLRDecisionTree_fitted model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SSLRDecisionTree_fitted'
predict(object, x, type = "class", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SSLRDecisionTree_fitted_+3A_object">object</code></td>
<td>
<p>model SSLRDecisionTree_fitted.</p>
</td></tr>
<tr><td><code id="predict.SSLRDecisionTree_fitted_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix
with the distances between the unseen instances and the selected training instances,
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.SSLRDecisionTree_fitted_+3A_type">type</code></td>
<td>
<p>of predict in principal model</p>
</td></tr>
<tr><td><code id="predict.SSLRDecisionTree_fitted_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.triTraining'>Predictions of the Tri-training method</h2><span id='topic+predict.triTraining'></span>

<h3>Description</h3>

<p>Predicts the label of instances according to the <code>triTraining</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'triTraining'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.triTraining_+3A_object">object</code></td>
<td>
<p>Tri-training model built with the <code><a href="#topic+triTraining">triTraining</a></code> function.</p>
</td></tr>
<tr><td><code id="predict.triTraining_+3A_x">x</code></td>
<td>
<p>A object that can be coerced as matrix.
Depending on how was the model built, <code>x</code> is interpreted as a matrix
with the distances between the unseen instances and the selected training instances,
or a matrix of instances.</p>
</td></tr>
<tr><td><code id="predict.triTraining_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For additional help see <code><a href="#topic+triTraining">triTraining</a></code> examples.
</p>


<h3>Value</h3>

<p>Vector with the labels assigned.
</p>

<hr>
<h2 id='predict.TSVMSSLR'>Predict TSVMSSLR</h2><span id='topic+predict.TSVMSSLR'></span>

<h3>Description</h3>

<p>Predict TSVMSSLR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'TSVMSSLR'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.TSVMSSLR_+3A_object">object</code></td>
<td>
<p>is the object</p>
</td></tr>
<tr><td><code id="predict.TSVMSSLR_+3A_x">x</code></td>
<td>
<p>is the dataset</p>
</td></tr>
<tr><td><code id="predict.TSVMSSLR_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.USMLeastSquaresClassifierSSLR'>Predict USMLeastSquaresClassifierSSLR</h2><span id='topic+predict.USMLeastSquaresClassifierSSLR'></span>

<h3>Description</h3>

<p>Predict USMLeastSquaresClassifierSSLR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'USMLeastSquaresClassifierSSLR'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.USMLeastSquaresClassifierSSLR_+3A_object">object</code></td>
<td>
<p>is the object</p>
</td></tr>
<tr><td><code id="predict.USMLeastSquaresClassifierSSLR_+3A_x">x</code></td>
<td>
<p>is the dataset</p>
</td></tr>
<tr><td><code id="predict.USMLeastSquaresClassifierSSLR_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.WellSVMSSLR'>Predict WellSVMSSLR</h2><span id='topic+predict.WellSVMSSLR'></span>

<h3>Description</h3>

<p>Predict WellSVMSSLR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'WellSVMSSLR'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.WellSVMSSLR_+3A_object">object</code></td>
<td>
<p>is the object</p>
</td></tr>
<tr><td><code id="predict.WellSVMSSLR_+3A_x">x</code></td>
<td>
<p>is the dataset</p>
</td></tr>
<tr><td><code id="predict.WellSVMSSLR_+3A_...">...</code></td>
<td>
<p>This parameter is included for compatibility reasons.</p>
</td></tr>
</table>

<hr>
<h2 id='predictions'>predictions unlabeled data</h2><span id='topic+predictions'></span>

<h3>Description</h3>

<p>Predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictions(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predictions_+3A_object">object</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="predictions_+3A_...">...</code></td>
<td>
<p>other parameters to be passed</p>
</td></tr>
</table>

<hr>
<h2 id='predictions.GRFClassifierSSLR'>predictions unlabeled data</h2><span id='topic+predictions.GRFClassifierSSLR'></span>

<h3>Description</h3>

<p>Predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'GRFClassifierSSLR'
predictions(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predictions.GRFClassifierSSLR_+3A_object">object</code></td>
<td>
<p>object</p>
</td></tr>
<tr><td><code id="predictions.GRFClassifierSSLR_+3A_...">...</code></td>
<td>
<p>other parameters to be passed</p>
</td></tr>
</table>

<hr>
<h2 id='predictions.model_sslr_fitted'>Predictions of unlabeled data</h2><span id='topic+predictions.model_sslr_fitted'></span>

<h3>Description</h3>

<p>Predictions of unlabeled data (transductive)
raw returns factor or numeric values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_sslr_fitted'
predictions(object, type = "class", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predictions.model_sslr_fitted_+3A_object">object</code></td>
<td>
<p>model_sslr_fitted model built</p>
</td></tr>
<tr><td><code id="predictions.model_sslr_fitted_+3A_type">type</code></td>
<td>
<p>of predict in principal model: class, raw</p>
</td></tr>
<tr><td><code id="predictions.model_sslr_fitted_+3A_...">...</code></td>
<td>
<p>other parameters to be passed</p>
</td></tr>
</table>

<hr>
<h2 id='print.model_sslr'>Print model SSLR</h2><span id='topic+print.model_sslr'></span>

<h3>Description</h3>

<p>Print model SSLR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_sslr'
print(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.model_sslr_+3A_object">object</code></td>
<td>
<p>model_sslr object to print</p>
</td></tr>
</table>

<hr>
<h2 id='RandomForestSemisupervised-class'>Class Random Forest</h2><span id='topic+RandomForestSemisupervised-class'></span>

<h3>Description</h3>

<p>Class Random Forest
Slots: mtry, trees, min_n, w, classes, mode
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+fit'></span><span id='topic+fit_xy'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+fit">fit</a></code>, <code><a href="generics.html#topic+fit_xy">fit_xy</a></code></p>
</dd>
</dl>

<hr>
<h2 id='seeded_kmeans'>General Interface Seeded KMeans</h2><span id='topic+seeded_kmeans'></span>

<h3>Description</h3>

<p>The difference with traditional Kmeans is that in this method implemented,
at initialization, there are as many clusters as the number of classes that exist of the labelled data,
the average of the labelled data of a given class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>seeded_kmeans(max_iter = 10, method = "euclidean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="seeded_kmeans_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum iterations in KMeans. Default is 10</p>
</td></tr>
<tr><td><code id="seeded_kmeans_+3A_method">method</code></td>
<td>
<p>distance method in KMeans: &quot;euclidean&quot;, &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot; or &quot;minkowski&quot;</p>
</td></tr>
</table>


<h3>References</h3>

<p>Sugato Basu, Arindam Banerjee, Raymond Mooney<br />
<em>Semi-supervised clustering by seeding</em><br />
July 2002
In Proceedings of 19th International Conference on Machine Learning
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(SSLR)
library(tidymodels)

data &lt;- iris

set.seed(1)
#% LABELED
cls &lt;- which(colnames(iris) == "Species")

labeled.index &lt;- createDataPartition(data$Species, p = .2, list = FALSE)
data[-labeled.index,cls] &lt;- NA



m &lt;- seeded_kmeans() %&gt;% fit(Species ~ ., data)

#Get labels (assing clusters), type = "raw" return factor
labels &lt;- m %&gt;% cluster_labels()

print(labels)


#Get centers
centers &lt;- m %&gt;% get_centers()

print(centers)
</code></pre>

<hr>
<h2 id='selfTraining'>General Interface for Self-training model</h2><span id='topic+selfTraining'></span>

<h3>Description</h3>

<p>Self-training is a simple and effective semi-supervised
learning classification method. The self-training classifier is initially
trained with a reduced set of labeled examples. Then it is iteratively retrained
with its own most confident predictions over the unlabeled examples.
Self-training follows a wrapper methodology using a base supervised
classifier to establish the possible class of unlabeled instances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selfTraining(learner, max.iter = 50, perc.full = 0.7, thr.conf = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selfTraining_+3A_learner">learner</code></td>
<td>
<p>model from parsnip package for training a supervised base classifier
using a set of instances. This model need to have probability predictions
(or optionally a distance matrix) and it's corresponding classes.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_max.iter">max.iter</code></td>
<td>
<p>maximum number of iterations to execute the self-labeling process.
Default is 50.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage
of new labeled examples reaches this value the self-training process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="selfTraining_+3A_thr.conf">thr.conf</code></td>
<td>
<p>A number between 0 and 1 that indicates the confidence threshold.
At each iteration, only the newly labelled examples with a confidence greater than
this value (<code>thr.conf</code>) are added to the training set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For predicting the most accurate instances per iteration, <code>selfTraining</code>
uses the predictions obtained with the learner specified. To train a model
using the <code>learner</code> function, it is required a set of instances
(or a precomputed matrix between the instances if <code>x.inst</code> parameter is <code>FALSE</code>)
in conjunction with the corresponding classes.
Additionals parameters are provided to the <code>learner</code> function via the
<code>learner.pars</code> argument. The model obtained is a supervised classifier
ready to predict new instances through the <code>pred</code> function.
Using a similar idea, the additional parameters to the <code>pred</code> function
are provided using the <code>pred.pars</code> argument. The <code>pred</code> function returns
the probabilities per class for each new instance. The value of the
<code>thr.conf</code> argument controls the confidence of instances selected
to enlarge the labeled set for the next iteration.
</p>
<p>The stopping criterion is defined through the fulfillment of one of the following
criteria: the algorithm reaches the number of iterations defined in the <code>max.iter</code>
parameter or the portion of the unlabeled set, defined in the <code>perc.full</code> parameter,
is moved to the labeled set. In some cases, the process stops and no instances
are added to the original labeled set. In this case, the user must assign a more
flexible value to the <code>thr.conf</code> parameter.
</p>


<h3>Value</h3>

<p>(When model fit) A list object of class &quot;selfTraining&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of the training instances used to
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to <code>x</code> argument.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>pred</dt><dd><p>The function provided in the <code>pred</code> argument.</p>
</dd>
<dt>pred.pars</dt><dd><p>The list provided in the <code>pred.pars</code> argument.</p>
</dd>
</dl>



<h3>References</h3>

<p>David Yarowsky.<br />
<em>Unsupervised word sense disambiguation rivaling supervised methods.</em><br />
In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,
pages 189-196. Association for Computational Linguistics, 1995.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(tidymodels)
library(caret)
library(SSLR)

data(wine)

set.seed(1)
train.index &lt;- createDataPartition(wine$Wine, p = .7, list = FALSE)
train &lt;- wine[ train.index,]
test  &lt;- wine[-train.index,]

cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(train$Wine, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA

#We need a model with probability predictions from parsnip
#https://tidymodels.github.io/parsnip/articles/articles/Models.html
#It should be with mode = classification

#For example, with Random Forest
rf &lt;-  rand_forest(trees = 100, mode = "classification") %&gt;%
  set_engine("randomForest")


m &lt;- selfTraining(learner = rf,
                  perc.full = 0.7,
                  thr.conf = 0.5, max.iter = 10) %&gt;% fit(Wine ~ ., data = train)

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)

</code></pre>

<hr>
<h2 id='selfTrainingG'>Self-training generic method</h2><span id='topic+selfTrainingG'></span>

<h3>Description</h3>

<p>Self-training is a simple and effective semi-supervised
learning classification method. The self-training classifier is initially
trained with a reduced set of labeled examples. Then it is iteratively retrained
with its own most confident predictions over the unlabeled examples.
Self-training follows a wrapper methodology using one base supervised
classifier to establish the possible class of unlabeled instances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selfTrainingG(
  y,
  gen.learner,
  gen.pred,
  max.iter = 50,
  perc.full = 0.7,
  thr.conf = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selfTrainingG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="selfTrainingG_+3A_gen.learner">gen.learner</code></td>
<td>
<p>A function for training a supervised base classifier.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="selfTrainingG_+3A_gen.pred">gen.pred</code></td>
<td>
<p>A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
<tr><td><code id="selfTrainingG_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to execute the self-labeling process.
Default is 50.</p>
</td></tr>
<tr><td><code id="selfTrainingG_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage
of new labeled examples reaches this value the self-training process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="selfTrainingG_+3A_thr.conf">thr.conf</code></td>
<td>
<p>A number between 0 and 1 that indicates the confidence theshold.
At each iteration, only the newly labelled examples with a confidence greater than
this value (<code>thr.conf</code>) are added to the training set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SelfTrainingG can be helpful in those cases where the method selected as
base classifier needs <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general self-training method,
please see the <code><a href="#topic+selfTraining">selfTraining</a></code> function. Essentially, the <code>selfTraining</code>
function is a wrapper of the <code>selfTrainingG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class &quot;selfTrainingG&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of the training instances used to
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to the <code>y</code> argument.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(SSLR)

## Load Wine data set
data(wine)
cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, - cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x)


set.seed(20)

# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,]
ytrain &lt;- y[tra.idx]

# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA


# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of instances in xitest
# Use the unlabeled examples for transductive testing
xttest &lt;- x[tra.idx[tra.na.idx],] # transductive testing instances
yttest &lt;- y[tra.idx[tra.na.idx]] # classes of instances in xttest

library(caret)

#PREPARE DATA
data &lt;- cbind(xtrain, Class = ytrain)


dtrain &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
ditest &lt;- as.matrix(proxy::dist(x = xitest, y = xtrain, method = "euclidean", by_rows = TRUE))

ddata &lt;- cbind(dtrain, Class = ytrain)
ddata &lt;- as.data.frame(ddata)

ktrain &lt;- as.matrix(exp(-0.048 * dtrain ^ 2))
kdata &lt;- cbind(ktrain, Class = ytrain)
kdata &lt;- as.data.frame(kdata)

ktrain &lt;- as.matrix(exp(-0.048 * dtrain ^ 2))
kitest &lt;- as.matrix(exp(-0.048 * ditest ^ 2))



## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
gen.learner &lt;- function(indexes, cls)
  caret::knn3(x = xtrain[indexes,], y = cls, k = 1)
gen.pred &lt;- function(model, indexes)
  predict(model, xtrain[indexes,])


trControl_selfTrainingG1 &lt;- list(gen.learner = gen.learner, gen.pred = gen.pred)
md1 &lt;- train_generic(ytrain, method = "selfTrainingG", trControl = trControl_selfTrainingG1)

p1 &lt;- predict(md1$model, xitest, type = "class")
table(p1, yitest)

confusionMatrix(p1, yitest)$overall[1]


## Example: Training from a distance matrix with 1-NN (oneNN) as base classifier.
dtrain &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
gen.learner &lt;- function(indexes, cls) {
  m &lt;- SSLR::oneNN(y = cls)
  attr(m, "tra.idxs") &lt;- indexes
  m
}

gen.pred &lt;- function(model, indexes) {
  tra.idxs &lt;- attr(model, "tra.idxs")
  d &lt;- dtrain[indexes, tra.idxs]
  prob &lt;- predict(model, d, distance.weighting = "none")
  prob
}


trControl_selfTrainingG2 &lt;- list(gen.learner = gen.learner, gen.pred = gen.pred)
md2 &lt;- train_generic(ytrain, method = "selfTrainingG", trControl = trControl_selfTrainingG2)

ditest &lt;- proxy::dist(x = xitest, y = xtrain[md2$instances.index,],
                      method = "euclidean", by_rows = TRUE)
p2 &lt;- predict(md2$model, ditest, type = "class")
table(p2, yitest)

confusionMatrix(p2, yitest)$overall[1]
</code></pre>

<hr>
<h2 id='setred'>General Interface for SETRED model</h2><span id='topic+setred'></span>

<h3>Description</h3>

<p>SETRED (SElf-TRaining with EDiting) is a variant of the self-training
classification method (as implemented in the function <code><a href="#topic+selfTraining">selfTraining</a></code>) with a different addition mechanism.
The SETRED classifier is initially trained with a
reduced set of labeled examples. Then, it is iteratively retrained with its own most
confident predictions over the unlabeled examples. SETRED uses an amending scheme
to avoid the introduction of noisy examples into the enlarged labeled set. For each
iteration, the mislabeled examples are identified using the local information provided
by the neighborhood graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setred(
  dist = "Euclidean",
  learner,
  theta = 0.1,
  max.iter = 50,
  perc.full = 0.7,
  D = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setred_+3A_dist">dist</code></td>
<td>
<p>A distance function or the name of a distance available
in the <code>proxy</code> package to compute. Default is &quot;Euclidean&quot;
the distance matrix in the case that <code>D</code> is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="setred_+3A_learner">learner</code></td>
<td>
<p>model from parsnip package for training a supervised base classifier
using a set of instances. This model need to have probability predictions
(or optionally a distance matrix) and it's corresponding classes.</p>
</td></tr>
<tr><td><code id="setred_+3A_theta">theta</code></td>
<td>
<p>Rejection threshold to test the critical region. Default is 0.1.</p>
</td></tr>
<tr><td><code id="setred_+3A_max.iter">max.iter</code></td>
<td>
<p>maximum number of iterations to execute the self-labeling process.
Default is 50.</p>
</td></tr>
<tr><td><code id="setred_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage
of new labeled examples reaches this value the self-training process is stopped.
Default is 0.7.</p>
</td></tr>
<tr><td><code id="setred_+3A_d">D</code></td>
<td>
<p>A distance matrix between all the training instances. This matrix is used to
construct the neighborhood graph. Default is NULL, this means the
method create a matrix with dist param</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SETRED initiates the self-labeling process by training a model from the original
labeled set. In each iteration, the <code>learner</code> function detects unlabeled
examples for which it makes the most confident prediction and labels those examples
according to the <code>pred</code> function. The identification of mislabeled examples is
performed using a neighborhood graph created from the distance matrix.
Most examples possess the same label in a neighborhood. So if an example locates
in a neighborhood with too many neighbors from different classes, this example should
be considered problematic. The value of the <code>theta</code> argument controls the confidence
of the candidates selected to enlarge the labeled set. The lower this value is, the more
restrictive is the selection of the examples that are considered good.
For more information about the self-labeled process and the rest of the parameters, please
see <code><a href="#topic+selfTraining">selfTraining</a></code>.
</p>


<h3>Value</h3>

<p>(When model fit) A list object of class &quot;setred&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of the training instances used to
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to <code>x</code> argument.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>pred</dt><dd><p>The function provided in the <code>pred</code> argument.</p>
</dd>
<dt>pred.pars</dt><dd><p>The list provided in the <code>pred.pars</code> argument.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ming Li and ZhiHua Zhou.<br />
<em>Setred: Self-training with editing.</em><br />
In Advances in Knowledge Discovery and Data Mining, volume 3518 of Lecture Notes in
Computer Science, pages 611-621. Springer Berlin Heidelberg, 2005.
ISBN 978-3-540-26076-9. doi: 10.1007/11430919 71.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(tidymodels)
library(caret)
library(SSLR)

data(wine)

set.seed(1)
train.index &lt;- createDataPartition(wine$Wine, p = .7, list = FALSE)
train &lt;- wine[ train.index,]
test  &lt;- wine[-train.index,]

cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(wine$Wine, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA

#We need a model with probability predictions from parsnip
#https://tidymodels.github.io/parsnip/articles/articles/Models.html
#It should be with mode = classification

#For example, with Random Forest
rf &lt;-  rand_forest(trees = 100, mode = "classification") %&gt;%
  set_engine("randomForest")


m &lt;- setred(learner = rf,
            theta = 0.1,
            max.iter = 2,
            perc.full = 0.7) %&gt;% fit(Wine ~ ., data = train)


#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)



#Another example, with dist matrix

distance &lt;- as.matrix(proxy::dist(train[,-cls], method ="Euclidean",
                                  by_rows = TRUE, diag = TRUE, upper = TRUE))

m &lt;- setred(learner = rf,
            theta = 0.1,
            max.iter = 2,
            perc.full = 0.7,
            D = distance) %&gt;% fit(Wine ~ ., data = train)

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)
</code></pre>

<hr>
<h2 id='setredG'>SETRED generic method</h2><span id='topic+setredG'></span>

<h3>Description</h3>

<p>SETRED is a variant of the self-training classification method
(<code><a href="#topic+selfTraining">selfTraining</a></code>) with a different addition mechanism.
The SETRED classifier is initially trained with a
reduced set of labeled examples. Then it is iteratively retrained with its own most
confident predictions over the unlabeled examples. SETRED uses an amending scheme
to avoid the introduction of noisy examples into the enlarged labeled set. For each
iteration, the mislabeled examples are identified using the local information provided
by the neighborhood graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setredG(
  y,
  D,
  gen.learner,
  gen.pred,
  theta = 0.1,
  max.iter = 50,
  perc.full = 0.7
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setredG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="setredG_+3A_d">D</code></td>
<td>
<p>A distance matrix between all the training instances. This matrix is used to
construct the neighborhood graph.</p>
</td></tr>
<tr><td><code id="setredG_+3A_gen.learner">gen.learner</code></td>
<td>
<p>A function for training a supervised base classifier.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="setredG_+3A_gen.pred">gen.pred</code></td>
<td>
<p>A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
<tr><td><code id="setredG_+3A_theta">theta</code></td>
<td>
<p>Rejection threshold to test the critical region. Default is 0.1.</p>
</td></tr>
<tr><td><code id="setredG_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to execute the self-labeling process.
Default is 50.</p>
</td></tr>
<tr><td><code id="setredG_+3A_perc.full">perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage
of new labeled examples reaches this value the self-training process is stopped.
Default is 0.7.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SetredG can be helpful in those cases where the method selected as
base classifier needs a <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general setred method,
please see <code><a href="#topic+setred">setred</a></code> function. Essentially, <code>setred</code>
function is a wrapper of <code>setredG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class &quot;setredG&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of the training instances used to
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to the <code>y</code> argument.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(SSLR)
library(caret)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, - cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx] # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

# Compute distances between training instances
D &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))

## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
# Compute distances between training instances
D &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))

## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
gen.learner &lt;- function(indexes, cls)
  caret::knn3(x = xtrain[indexes,], y = cls, k = 1)
gen.pred &lt;- function(model, indexes)
  predict(model, xtrain[indexes,])

trControl_SETRED1 &lt;- list(D = D, gen.learner = gen.learner,
                             gen.pred = gen.pred)
md1 &lt;- train_generic(ytrain, method = "setredG", trControl = trControl_SETRED1)

'md1 &lt;- setredG(y = ytrain, D, gen.learner, gen.pred)'

cls1 &lt;- predict(md1$model, xitest, type = "class")
table(cls1, yitest)

confusionMatrix(cls1, yitest)$overall[1]


## Example: Training from a distance matrix with 1-NN (oneNN) as base classifier
gen.learner &lt;- function(indexes, cls) {
  m &lt;- SSLR::oneNN(y = cls)
  attr(m, "tra.idxs") &lt;- indexes
  m
}

gen.pred &lt;- function(model, indexes) {
  tra.idxs &lt;- attr(model, "tra.idxs")
  d &lt;- D[indexes, tra.idxs]
  prob &lt;- predict(model, d, distance.weighting = "none")
  prob
}

trControl_SETRED2 &lt;- list(D = D, gen.learner = gen.learner,
                          gen.pred = gen.pred)
md2 &lt;- train_generic(ytrain, method = "setredG", trControl = trControl_SETRED2)


ditest &lt;- proxy::dist(x = xitest, y = xtrain[md2$instances.index,],
                      method = "euclidean", by_rows = TRUE)

cls2 &lt;- predict(md2$model, ditest, type = "class")
table(cls2, yitest)

confusionMatrix(cls2, yitest)$overall[1]





</code></pre>

<hr>
<h2 id='snnrce'>General Interface for SNNRCE model</h2><span id='topic+snnrce'></span>

<h3>Description</h3>

<p>SNNRCE (Self-training Nearest Neighbor Rule using Cut Edges) is a variant
of the self-training classification method (<code><a href="#topic+selfTraining">selfTraining</a></code>) with a different
addition mechanism and a fixed learning scheme (1-NN). SNNRCE uses an amending scheme
to avoid the introduction of noisy examples into the enlarged labeled set.
The mislabeled examples are identified using the local information provided
by the neighborhood graph. A statistical test using cut edge weight is used to modify
the labels of the missclassified examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>snnrce(x.inst = TRUE, dist = "Euclidean", alpha = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="snnrce_+3A_x.inst">x.inst</code></td>
<td>
<p>A boolean value that indicates if <code>x</code> is or not an instance matrix.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="snnrce_+3A_dist">dist</code></td>
<td>
<p>A distance function available in the <code>proxy</code> package to compute
the distance matrix in the case that <code>x.inst</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="snnrce_+3A_alpha">alpha</code></td>
<td>
<p>Rejection threshold to test the critical region. Default is 0.1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SNNRCE initiates the self-labeling process by training a 1-NN from the original
labeled set. This method attempts to reduce the noise in examples by labeling those instances
with no cut edges in the initial stages of self-labeling learning.
These highly confident examples are added into the training set.
The remaining examples follow the standard self-training process until a minimum number
of examples will be labeled for each class. A statistical test using cut edge weight is used
to modify the labels of the missclassified examples The value of the <code>alpha</code> argument
defines the critical region where the candidates examples are tested. The higher this value
is, the more relaxed it is the selection of the examples that are considered mislabeled.
</p>


<h3>Value</h3>

<p>(When model fit) A list object of class &quot;snnrce&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of the training instances used to
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to <code>x</code> argument.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>x.inst</dt><dd><p>The value provided in the <code>x.inst</code> argument.</p>
</dd>
<dt>dist</dt><dd><p>The value provided in the <code>dist</code> argument when x.inst is <code>TRUE</code>.</p>
</dd>
<dt>xtrain</dt><dd><p>A matrix with the subset of training instances referenced by the indexes
<code>instances.index</code> when x.inst is <code>TRUE</code>.</p>
</dd>
</dl>



<h3>References</h3>

<p>Yu Wang, Xiaoyan Xu, Haifeng Zhao, and Zhongsheng Hua.<br />
<em>Semisupervised learning based on nearest neighbor rule and cut edges.</em><br />
Knowledge-Based Systems, 23(6):547-554, 2010. ISSN 0950-7051. doi: http://dx.doi.org/10.1016/j.knosys.2010.03.012.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(tidymodels)
library(caret)
library(SSLR)

data(wine)
set.seed(1)
train.index &lt;- createDataPartition(wine$Wine, p = .7, list = FALSE)
train &lt;- wine[ train.index,]
test  &lt;- wine[-train.index,]

cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(wine$Wine, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- snnrce(x.inst = TRUE,
            dist = "Euclidean",
            alpha = 0.1) %&gt;% fit(Wine ~ ., data = train)



predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)
</code></pre>

<hr>
<h2 id='SSLRDecisionTree'>General Interface Decision Tree model</h2><span id='topic+SSLRDecisionTree'></span>

<h3>Description</h3>

<p>Decision Tree is a simple and effective semi-supervised
learning method.
Based on the article &quot;Semi-supervised classification trees&quot;.
It also offers many parameters to modify the behavior of this method.
It is the same as the traditional Decision Tree
algorithm, but the difference is how the gini coefficient is calculated (classification).
In regression we use SSE metric (different from the original investigation)
It can be used in classification or regression. If Y is numeric is for regression, classification in another case
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSLRDecisionTree(
  max_depth = 30,
  w = 0.5,
  min_samples_split = 20,
  min_samples_leaf = ceiling(min_samples_split/3)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSLRDecisionTree_+3A_max_depth">max_depth</code></td>
<td>
<p>A number from 1 to Inf.
Is the maximum number of depth in Decision Tree
Default is 30</p>
</td></tr>
<tr><td><code id="SSLRDecisionTree_+3A_w">w</code></td>
<td>
<p>weight parameter ranging from 0 to 1. Default is 0.5</p>
</td></tr>
<tr><td><code id="SSLRDecisionTree_+3A_min_samples_split">min_samples_split</code></td>
<td>
<p>the minimum number of observations to do split. Default is 20</p>
</td></tr>
<tr><td><code id="SSLRDecisionTree_+3A_min_samples_leaf">min_samples_leaf</code></td>
<td>
<p>the minimum number of any terminal leaf node. Default is ceiling(min_samples_split/3)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In this model we can make predictions with prob type
</p>


<h3>References</h3>

<p>Jurica Levati, Michelangelo Ceci, Dragi Kocev, Saso Dzeroski.<br />
<em>Semi-supervised classification trees.</em><br />
Published online: 25 March 2017
© Springer Science Business Media New York 2017
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(SSLR)
library(tidymodels)

data(wine)

set.seed(1)
train.index &lt;- createDataPartition(wine$Wine, p = .7, list = FALSE)
train &lt;- wine[ train.index,]
test  &lt;- wine[-train.index,]

cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(wine$Wine, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- SSLRDecisionTree(min_samples_split = round(length(labeled.index) * 0.25),
                      w = 0.3,
                      ) %&gt;% fit(Wine ~ ., data = train)


#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)


#For probabilities
predict(m,test, type = "prob")

</code></pre>

<hr>
<h2 id='SSLRRandomForest'>General Interface Random Forest model</h2><span id='topic+SSLRRandomForest'></span>

<h3>Description</h3>

<p>Random Forest is a simple and effective semi-supervised
learning method. It is the same as the traditional Random Forest
algorithm, but the difference is that it use Semi supervised Decision Trees
It can be used in classification or regression. If Y is numeric is for regression, classification in another case
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSLRRandomForest(
  mtry = NULL,
  trees = 500,
  min_n = NULL,
  w = 0.5,
  replace = TRUE,
  tree_max_depth = Inf,
  sampsize = NULL,
  min_samples_leaf = NULL,
  allowParallel = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSLRRandomForest_+3A_mtry">mtry</code></td>
<td>
<p>number of features in each decision tree.
Default is null. This means that mtry = log(n_features) + 1</p>
</td></tr>
<tr><td><code id="SSLRRandomForest_+3A_trees">trees</code></td>
<td>
<p>number of trees. Default is 500</p>
</td></tr>
<tr><td><code id="SSLRRandomForest_+3A_min_n">min_n</code></td>
<td>
<p>number of minimum samples in each tree
Default is null. This means that uses all training data</p>
</td></tr>
<tr><td><code id="SSLRRandomForest_+3A_w">w</code></td>
<td>
<p>weight parameter ranging from 0 to 1. Default is 0.5</p>
</td></tr>
<tr><td><code id="SSLRRandomForest_+3A_replace">replace</code></td>
<td>
<p>replacing type in sampling. Default is true</p>
</td></tr>
<tr><td><code id="SSLRRandomForest_+3A_tree_max_depth">tree_max_depth</code></td>
<td>
<p>maximum tree depth. Default is Inf</p>
</td></tr>
<tr><td><code id="SSLRRandomForest_+3A_sampsize">sampsize</code></td>
<td>
<p>Size of sample. Default if (replace) nrow(x) else ceiling(.632*nrow(x))</p>
</td></tr>
<tr><td><code id="SSLRRandomForest_+3A_min_samples_leaf">min_samples_leaf</code></td>
<td>
<p>the minimum number of any terminal leaf node. Default is 1</p>
</td></tr>
<tr><td><code id="SSLRRandomForest_+3A_allowparallel">allowParallel</code></td>
<td>
<p>Execute Random Forest in parallel if doParallel is loaded.
Default is TRUE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We can use paralleling processing with doParallel package and allowParallel = TRUE.
</p>


<h3>References</h3>

<p>Jurica Levati, Michelangelo Ceci, Dragi Kocev, Saso Dzeroski.<br />
<em>Semi-supervised classification trees.</em><br />
Published online: 25 March 2017
© Springer Science Business Media New York 2017
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(SSLR)
library(tidymodels)

data(wine)

set.seed(1)
train.index &lt;- createDataPartition(wine$Wine, p = .7, list = FALSE)
train &lt;- wine[ train.index,]
test  &lt;- wine[-train.index,]

cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(train$Wine, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- SSLRRandomForest(trees = 5,  w = 0.3) %&gt;% fit(Wine ~ ., data = train)

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)


#For probabilities
predict(m,test, type = "prob")

</code></pre>

<hr>
<h2 id='train_generic'>FUNCTION TO TRAIN GENERIC MODEL</h2><span id='topic+train_generic'></span>

<h3>Description</h3>

<p>FUNCTION TO TRAIN GENERIC MODEL
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_generic(y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_generic_+3A_y">y</code></td>
<td>
<p>(optional) factor (classes)</p>
</td></tr>
<tr><td><code id="train_generic_+3A_...">...</code></td>
<td>
<p>list parms trControl (method...)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>model trained
</p>

<hr>
<h2 id='triTraining'>General Interface for Tri-training model</h2><span id='topic+triTraining'></span>

<h3>Description</h3>

<p>Tri-training is a semi-supervised learning algorithm with a co-training
style. This algorithm trains three classifiers with the same learning scheme from a
reduced set of labeled examples. For each iteration, an unlabeled example is labeled
for a classifier if the other two classifiers agree on the labeling proposed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>triTraining(learner)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="triTraining_+3A_learner">learner</code></td>
<td>
<p>model from parsnip package for training a supervised base classifier
using a set of instances. This model need to have probability predictions
(or optionally a distance matrix) and it's corresponding classes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tri-training initiates the self-labeling process by training three models from the
original labeled set, using the <code>learner</code> function specified.
In each iteration, the algorithm detects unlabeled examples on which two classifiers
agree with the classification and includes these instances in the enlarged set of the
third classifier under certain conditions. The generation of the final hypothesis is
produced via the majority voting. The iteration process ends when no changes occur in
any model during a complete iteration.
</p>


<h3>Value</h3>

<p>A list object of class &quot;triTraining&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final three base classifiers trained using the enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of three vectors of indexes related to the training instances
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the three models. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt><dd><p>The levels of <code>y</code> factor.</p>
</dd>
<dt>pred</dt><dd><p>The function provided in the <code>pred</code> argument.</p>
</dd>
<dt>pred.pars</dt><dd><p>The list provided in the <code>pred.pars</code> argument.</p>
</dd>
<dt>x.inst</dt><dd><p>The value provided in the <code>x.inst</code> argument.</p>
</dd>
</dl>



<h3>References</h3>

<p>ZhiHua Zhou and Ming Li.<br />
<em>Tri-training: exploiting unlabeled data using three classifiers.</em><br />
IEEE Transactions on Knowledge and Data Engineering, 17(11):1529-1541, Nov 2005. ISSN 1041-4347. doi: 10.1109/TKDE.2005. 186.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(tidymodels)
library(caret)
library(SSLR)

data(wine)

set.seed(1)
train.index &lt;- createDataPartition(wine$Wine, p = .7, list = FALSE)
train &lt;- wine[ train.index,]
test  &lt;- wine[-train.index,]

cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(wine$Wine, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA

#We need a model with probability predictions from parsnip
#https://tidymodels.github.io/parsnip/articles/articles/Models.html
#It should be with mode = classification

#For example, with Random Forest
rf &lt;-  rand_forest(trees = 100, mode = "classification") %&gt;%
  set_engine("randomForest")


m &lt;- triTraining(learner = rf) %&gt;% fit(Wine ~ ., data = train)


#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)
</code></pre>

<hr>
<h2 id='triTrainingCombine'>Combining the hypothesis</h2><span id='topic+triTrainingCombine'></span>

<h3>Description</h3>

<p>This function combines the predictions obtained
by the set of classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>triTrainingCombine(pred)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="triTrainingCombine_+3A_pred">pred</code></td>
<td>
<p>A list with the predictions of each classifiers</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of classes
</p>

<hr>
<h2 id='triTrainingG'>Tri-training generic method</h2><span id='topic+triTrainingG'></span>

<h3>Description</h3>

<p>Tri-training is a semi-supervised learning algorithm with a co-training
style. This algorithm trains three classifiers with the same learning scheme from a
reduced set of labeled examples. For each iteration, an unlabeled example is labeled
for a classifier if the other two classifiers agree on the labeling proposed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>triTrainingG(y, gen.learner, gen.pred)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="triTrainingG_+3A_y">y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the
unlabeled instances are specified with the value <code>NA</code>.</p>
</td></tr>
<tr><td><code id="triTrainingG_+3A_gen.learner">gen.learner</code></td>
<td>
<p>A function for training three supervised base classifiers.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td></tr>
<tr><td><code id="triTrainingG_+3A_gen.pred">gen.pred</code></td>
<td>
<p>A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>TriTrainingG can be helpful in those cases where the method selected as
base classifier needs a <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general triTraining method,
please see the <code><a href="#topic+triTraining">triTraining</a></code> function. Essentially, the <code>triTraining</code>
function is a wrapper of the <code>triTrainingG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class &quot;triTrainingG&quot; containing:
</p>

<dl>
<dt>model</dt><dd><p>The final three base classifiers trained using the enlarged labeled set.</p>
</dd>
<dt>model.index</dt><dd><p>List of three vectors of indexes related to the training instances
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt><dd><p>The indexes of all training instances used to
train the three models. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt><dd><p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(SSLR)
library(caret)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, - cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx] # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
gen.learner &lt;- function(indexes, cls)
  caret::knn3(x = xtrain[indexes,], y = cls, k = 1)
gen.pred &lt;- function(model, indexes)
  predict(model, xtrain[indexes,])

# Train
set.seed(1)

trControl_triTraining1 &lt;- list(gen.learner = gen.learner,
                                  gen.pred = gen.pred)
md1 &lt;- train_generic(ytrain, method = "triTrainingG", trControl = trControl_triTraining1)



# Predict testing instances using the three classifiers
pred &lt;- lapply(
  X = md1$model,
  FUN = function(m) predict(m, xitest, type = "class")
)
# Combine the predictions
cls1 &lt;- triTrainingCombine(pred)
table(cls1, yitest)

confusionMatrix(cls1, yitest)$overall[1]


## Example: Training from a distance matrix with 1-NN (oneNN) as base classifier.
dtrain &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
gen.learner &lt;- function(indexes, cls) {
  m &lt;- SSLR::oneNN(y = cls)
  attr(m, "tra.idxs") &lt;- indexes
  m
}

gen.pred &lt;- function(model, indexes) {
  tra.idxs &lt;- attr(model, "tra.idxs")
  d &lt;- dtrain[indexes, tra.idxs]
  prob &lt;- predict(model, d, distance.weighting = "none")
  prob
}

# Train
set.seed(1)

trControl_triTraining2 &lt;- list(gen.learner = gen.learner,
                               gen.pred = gen.pred)
md2 &lt;- train_generic(ytrain, method = "triTrainingG", trControl = trControl_triTraining2)

# Predict
ditest &lt;- proxy::dist(x = xitest, y = xtrain[md2$instances.index,],
                      method = "euclidean", by_rows = TRUE)

# Predict testing instances using the three classifiers
pred &lt;- mapply(
  FUN = function(m, indexes) {
    D &lt;- ditest[, indexes]
    predict(m, D, type = "class")
  },
  m = md2$model,
  indexes = md2$model.index.map,
  SIMPLIFY = FALSE
)
# Combine the predictions
cls2 &lt;- triTrainingCombine(pred)
table(cls2, yitest)

confusionMatrix(cls2, yitest)$overall[1]
</code></pre>

<hr>
<h2 id='TSVMSSLR'>General Interface for TSVM (Transductive SVM classifier using the convex concave procedure) model</h2><span id='topic+TSVMSSLR'></span>

<h3>Description</h3>

<p>model from RSSL package
Transductive SVM using the CCCP algorithm as proposed by Collobert et al. (2006)
implemented in R using the quadprog package. The implementation does not handle large
datasets very well, but can be useful for smaller datasets and visualization purposes.
C is the cost associated with labeled objects, while Cstar is the cost for the
unlabeled objects. s control the loss function used for the unlabeled objects: it
controls the size of the plateau for the symmetric ramp loss function. The balancing
constraint makes sure the label assignments of the unlabeled objects are similar to the
prior on the classes that was observed on the labeled data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TSVMSSLR(
  C = 1,
  Cstar = 0.1,
  kernel = kernlab::vanilladot(),
  balancing_constraint = TRUE,
  s = 0,
  x_center = TRUE,
  scale = FALSE,
  eps = 1e-09,
  max_iter = 20,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TSVMSSLR_+3A_c">C</code></td>
<td>
<p>numeric; Cost parameter of the SVM</p>
</td></tr>
<tr><td><code id="TSVMSSLR_+3A_cstar">Cstar</code></td>
<td>
<p>numeric; Cost parameter of the unlabeled objects</p>
</td></tr>
<tr><td><code id="TSVMSSLR_+3A_kernel">kernel</code></td>
<td>
<p>kernlab::kernel to use</p>
</td></tr>
<tr><td><code id="TSVMSSLR_+3A_balancing_constraint">balancing_constraint</code></td>
<td>
<p>logical; Whether a balancing constraint should be enfored that causes the fraction of objects assigned to each label in the unlabeled data to be similar to the label fraction in the labeled data.</p>
</td></tr>
<tr><td><code id="TSVMSSLR_+3A_s">s</code></td>
<td>
<p>numeric; parameter controlling the loss function of the unlabeled objects (generally values between -1 and 0)</p>
</td></tr>
<tr><td><code id="TSVMSSLR_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="TSVMSSLR_+3A_scale">scale</code></td>
<td>
<p>If TRUE, apply a z-transform to all observations in X and X_u before running the regression</p>
</td></tr>
<tr><td><code id="TSVMSSLR_+3A_eps">eps</code></td>
<td>
<p>numeric; Stopping criterion for the maximinimization</p>
</td></tr>
<tr><td><code id="TSVMSSLR_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; Maximum number of iterations</p>
</td></tr>
<tr><td><code id="TSVMSSLR_+3A_verbose">verbose</code></td>
<td>
<p>logical; print debugging messages, only works for vanilladot() kernel (default: FALSE)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Collobert, R. et al., 2006. Large scale transductive SVMs.
Journal of Machine Learning Research, 7, pp.1687-1712.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(caret)
library(tidymodels)
library(SSLR)

data(breast)

set.seed(1)
train.index &lt;- createDataPartition(breast$Class, p = .7, list = FALSE)
train &lt;- breast[ train.index,]
test  &lt;- breast[-train.index,]

cls &lt;- which(colnames(breast) == "Class")

#% LABELED
labeled.index &lt;- createDataPartition(breast$Class, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA

library(kernlab)
m &lt;- TSVMSSLR(kernel = kernlab::vanilladot()) %&gt;% fit(Class ~ ., data = train)


#Accesing model from RSSL
model &lt;- m$model

</code></pre>

<hr>
<h2 id='USMLeastSquaresClassifierSSLR'>General Interface for USMLeastSquaresClassifier (Updated Second Moment Least Squares Classifier) model</h2><span id='topic+USMLeastSquaresClassifierSSLR'></span>

<h3>Description</h3>

<p>model from RSSL package
This methods uses the closed form solution of the supervised least squares problem,
except that the second moment matrix (X'X) is exchanged with a second moment matrix that
is estimated based on all data. See for instance <cite>Shaffer1991</cite>, where in this
implementation we use all data to estimate E(X'X), instead of just the labeled data.
This method seems to work best when the data is first centered <code>x_center=TRUE</code>
and the outputs are scaled using <code>y_scale=TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>USMLeastSquaresClassifierSSLR(
  lambda = 0,
  intercept = TRUE,
  x_center = FALSE,
  scale = FALSE,
  y_scale = FALSE,
  ...,
  use_Xu_for_scaling = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="USMLeastSquaresClassifierSSLR_+3A_lambda">lambda</code></td>
<td>
<p>numeric; L2 regularization parameter</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifierSSLR_+3A_intercept">intercept</code></td>
<td>
<p>logical; Whether an intercept should be included</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifierSSLR_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifierSSLR_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifierSSLR_+3A_y_scale">y_scale</code></td>
<td>
<p>logical; whether the target vector should be centered</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifierSSLR_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
<tr><td><code id="USMLeastSquaresClassifierSSLR_+3A_use_xu_for_scaling">use_Xu_for_scaling</code></td>
<td>
<p>logical; whether the unlabeled objects should be used to determine the mean and scaling for the normalization</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shaffer, J.P., 1991. The Gauss-Markov Theorem and Random Regressors. The American Statistician, 45(4), pp.269-273.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(tidymodels)
library(caret)
library(SSLR)

data(breast)

set.seed(1)
train.index &lt;- createDataPartition(breast$Class, p = .7, list = FALSE)
train &lt;- breast[ train.index,]
test  &lt;- breast[-train.index,]

cls &lt;- which(colnames(breast) == "Class")

#% LABELED
labeled.index &lt;- createDataPartition(breast$Class, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- USMLeastSquaresClassifierSSLR() %&gt;% fit(Class ~ ., data = train)

#Accesing model from RSSL
model &lt;- m$model

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Class", estimate = .pred_class)

</code></pre>

<hr>
<h2 id='WellSVMSSLR'>General Interface for WellSVM model</h2><span id='topic+WellSVMSSLR'></span>

<h3>Description</h3>

<p>model from RSSL package
WellSVM is a minimax relaxation of the mixed integer programming problem of finding the
optimal labels for the unlabeled data in the SVM objective function.
This implementation is a translation of the Matlab implementation of Li (2013) into R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WellSVMSSLR(
  C1 = 1,
  C2 = 0.1,
  gamma = 1,
  x_center = TRUE,
  scale = FALSE,
  use_Xu_for_scaling = FALSE,
  max_iter = 20
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WellSVMSSLR_+3A_c1">C1</code></td>
<td>
<p>double; A regularization parameter for labeled data, default 1;</p>
</td></tr>
<tr><td><code id="WellSVMSSLR_+3A_c2">C2</code></td>
<td>
<p>double; A regularization parameter for unlabeled data, default 0.1;</p>
</td></tr>
<tr><td><code id="WellSVMSSLR_+3A_gamma">gamma</code></td>
<td>
<p>double; Gaussian kernel parameter, i.e., k(x,y) = exp(-gamma^2||x-y||^2/avg) where avg is the average distance among instances; when gamma = 0, linear kernel is used. default gamma = 1;</p>
</td></tr>
<tr><td><code id="WellSVMSSLR_+3A_x_center">x_center</code></td>
<td>
<p>logical;  Should the features be centered?</p>
</td></tr>
<tr><td><code id="WellSVMSSLR_+3A_scale">scale</code></td>
<td>
<p>logical; Should the features be normalized? (default: FALSE)</p>
</td></tr>
<tr><td><code id="WellSVMSSLR_+3A_use_xu_for_scaling">use_Xu_for_scaling</code></td>
<td>
<p>logical; whether the unlabeled objects should be used to determine the mean and scaling for the normalization</p>
</td></tr>
<tr><td><code id="WellSVMSSLR_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; Maximum number of iterations</p>
</td></tr>
</table>


<h3>References</h3>

<p>Y.-F. Li, I. W. Tsang, J. T. Kwok, and Z.-H. Zhou. Scalable and Convex Weakly Labeled SVMs. Journal of Machine Learning Research, 2013.
</p>
<p>R.-E. Fan, P.-H. Chen, and C.-J. Lin. Working set selection using second order information for training SVM. Journal of Machine Learning Research 6, 1889-1918, 2005.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(tidyverse)
library(tidymodels)
library(caret)
library(SSLR)

data(breast)

set.seed(1)
train.index &lt;- createDataPartition(breast$Class, p = .7, list = FALSE)
train &lt;- breast[ train.index,]
test  &lt;- breast[-train.index,]

cls &lt;- which(colnames(breast) == "Class")

#% LABELED
labeled.index &lt;- createDataPartition(breast$Class, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- WellSVMSSLR() %&gt;% fit(Class ~ ., data = train)

#Accesing model from RSSL
model &lt;- m$model

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Class", estimate = .pred_class)

</code></pre>

<hr>
<h2 id='wine'>Wine recognition data</h2><span id='topic+wine'></span>

<h3>Description</h3>

<p>This dataset is the result of a chemical analysis of wine grown in the same
region in Italy but derived from three different cultivars. The analysis determined
the quantities of 13 constituents found in each of the three types of wines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(wine)
</code></pre>


<h3>Format</h3>

<p>A data frame with 178 rows and 14 variables including the class.
</p>


<h3>Details</h3>

<p>The dataset is taken from the UCI data repository, to which it was donated
by Riccardo Leardi, University of Genova. The attributes are as follows:
</p>

<ul>
<li><p> Alcohol
</p>
</li>
<li><p> Malic acid
</p>
</li>
<li><p> Ash
</p>
</li>
<li><p> Alcalinity of ash
</p>
</li>
<li><p> Magnesium
</p>
</li>
<li><p> Total phenols
</p>
</li>
<li><p> Flavanoids
</p>
</li>
<li><p> Nonflavanoid phenols
</p>
</li>
<li><p> Proanthocyanins
</p>
</li>
<li><p> Color intensity
</p>
</li>
<li><p> Hue
</p>
</li>
<li><p> OD280/OD315 of diluted wines
</p>
</li>
<li><p> Proline
</p>
</li>
<li><p> Wine (class)
</p>
</li></ul>



<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/Wine">https://archive.ics.uci.edu/ml/datasets/Wine</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
