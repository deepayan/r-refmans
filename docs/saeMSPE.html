<!DOCTYPE html><html><head><title>Help for package saeMSPE</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {saeMSPE}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mspeFHdb'>
<p>Compute MSPE through double bootstrap method for Fay Herriot model</p></a></li>
<li><a href='#mspeFHjack'>
<p>Compute MSPE through Jackknife-based MSPE estimation method for Fay Herriot model</p></a></li>
<li><a href='#mspeFHlin'>
<p>Compute MSPE through linearization method for Fay Herriot model</p></a></li>
<li><a href='#mspeFHpb'>
<p>Compute MSPE through parameter bootstrap method for Fay Herriot model</p></a></li>
<li><a href='#mspeFHsumca'>
<p>Compute MSPE through Sumca method for Fay Herriot model</p></a></li>
<li><a href='#mspeNERdb'>
<p>Compute MSPE through double bootstrap(DB) method for Nested error regression model</p></a></li>
<li><a href='#mspeNERjack'>
<p>Compute MSPE through Jackknife-based MSPE estimation method for Nested error regression model</p></a></li>
<li><a href='#mspeNERlin'>
<p>Compute MSPE through linearization method for Nested error regression model</p></a></li>
<li><a href='#mspeNERpb'>
<p>Compute MSPE through parameter bootstrap method for Nested error regression model</p></a></li>
<li><a href='#mspeNERsumca'>
<p>Compute MSPE through Sumca method for Nested error regression model</p></a></li>
<li><a href='#saeMSPE-package'>
<p>Compute MSPE Estimates for the Fay Herriot Model and Nested Error Regression Model</p></a></li>
<li><a href='#varfh'>
<p>Estimates of the variance component using several methods for Fay Herriot model.</p></a></li>
<li><a href='#varner'>
<p>Estimates of the variance component using several methods for Nested error regression model.</p></a></li>
<li><a href='#wheatarea'><p> Wheat area measurement and satellite data.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Compute MSPE Estimates for the Fay Herriot Model and Nested
Error Regression Model</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-10-19</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Peiwen Xiao &lt;2569613200@qq.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>We describe a new R package entitled 'saeMSPE' for the well-known Fay Herriot model and nested error regression model in small area estimation. Based on this package, it is possible to easily compute various common mean squared predictive error (MSPE) estimators, as well as several existing variance component predictors as a byproduct, for these two models.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), Matrix, smallarea</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.7)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-10-21 11:53:38 UTC; 25696</td>
</tr>
<tr>
<td>Author:</td>
<td>Peiwen Xiao [aut, cre],
  Xiaohui Liu [aut],
  Yuzi Liu [aut],
  Shaochu Liu [aut],
  Jiming Jiang [ths]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-10-21 12:12:37 UTC</td>
</tr>
</table>
<hr>
<h2 id='mspeFHdb'>
Compute MSPE through double bootstrap method for Fay Herriot model
</h2><span id='topic+mspeFHdb'></span>

<h3>Description</h3>

<p>This function returns MSPE estimate with double bootstrap appoximation method for Fay Herriot model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspeFHdb(Y, X, D, K = 50, C = 50, method = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspeFHdb_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Fay Herriot model.
</p>
</td></tr>
<tr><td><code id="mspeFHdb_+3A_x">X</code></td>
<td>

<p>(matrix). Stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeFHdb_+3A_d">D</code></td>
<td>

<p>(vector). It represents the knowing sampling variance for Fay Herriot model.
</p>
</td></tr>
<tr><td><code id="mspeFHdb_+3A_k">K</code></td>
<td>

<p>(integer). It represents the first bootstrap sample number. Default value is 50.
</p>
</td></tr>
<tr><td><code id="mspeFHdb_+3A_c">C</code></td>
<td>

<p>(integer). It represents the second bootstrap sample number. Default value is 50.
</p>
</td></tr>
<tr><td><code id="mspeFHdb_+3A_method">method</code></td>
<td>

<p>It represents the variance component estimation method. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method was proposed by P. Hall and T. Maiti. Double bootstrap method uses boostrap tool twice for Fay Herriot model to avoid the unattractivitive bias correction: one is to estimate the estimator bias, the other is to correct for bias.
</p>
<p>Default value for <code>method</code> is 1, <code>method = 1</code> represents the MOM method , <code>method = 2</code> and <code>method = 3</code> represents ML and REML method, respectively.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>MSPE</code></td>
<td>
<p>(vector) MSPE estimate based on double bootstrap method.</p>
</td></tr>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) estimate of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>Ahat</code></td>
<td>
<p>(numeric) estimate of the variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>P. Hall and T. Maiti. On parametric bootstrap methods for small area prediction. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 2006.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(runif(10 * 3), 10, 3)
X[,1] = rep(1, 10) 
D = (1:10) / 10 + 0.5
Y = X %*% c(0.5,1,1.5) + rnorm(10, 0, sqrt(2)) + rnorm(10, 0, sqrt(D))
mspeFHdb(Y, X, D, K = 10, C = 10, 1)
</code></pre>

<hr>
<h2 id='mspeFHjack'>
Compute MSPE through Jackknife-based MSPE estimation method for Fay Herriot model
</h2><span id='topic+mspeFHjack'></span>

<h3>Description</h3>

<p>This function returns MSPE estimator with jackknife method for Fay Herriot model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspeFHjack(Y, X, D, method = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspeFHjack_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Fay Herriot model.
</p>
</td></tr>
<tr><td><code id="mspeFHjack_+3A_x">X</code></td>
<td>

<p>(matrix). It stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeFHjack_+3A_d">D</code></td>
<td>

<p>(vector). Stands for the known sampling variances of each small area levels.
</p>
</td></tr>
<tr><td><code id="mspeFHjack_+3A_method">method</code></td>
<td>

<p>The variance component estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This bias-corrected jackknife MSPE estimator was proposed by J. Jiang and L. S. M. Wan, it covers a fairly general class of mixed models which includes gLMM, mixed logistic model and some of the widely used mixed linear models as special cases.
</p>
<p>Default value for <code>method</code> is 1, <code>method = 1</code> represents the MOM method , <code>method = 2</code> and <code>method = 3</code> represents ML and REML method, respectively.
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>MSPE</code></td>
<td>
<p>(vector) MSPE estimates for Fay Herriot model.</p>
</td></tr>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>Ahat</code></td>
<td>
<p>(numeric) Estimates of the variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>M. H. Quenouille. Approximate tests of correlation in time series. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, 11(1):68-84, 1949.
</p>
<p>J. W. Tukey. Bias and confidence in not quite large samples. <em>Annals of Mathematical Statistics</em>, 29(2):614, 1958.
</p>
<p>J. Jiang and L. S. M. Wan. A unified jackknife theory for empirical best prediction with m estimation. <em>Annals of Statistics</em>, 30(6):1782-1810, 2002.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(runif(10 * 3), 10, 3)
X[,1] = rep(1, 10) 
D = (1:10) / 10 + 0.5
Y = X %*% c(0.5,1,1.5) + rnorm(10, 0, sqrt(2)) + rnorm(10, 0, sqrt(D))
mspeFHjack(Y, X, D, method = 1)
</code></pre>

<hr>
<h2 id='mspeFHlin'>
Compute MSPE through linearization method for Fay Herriot model
</h2><span id='topic+mspeFHlin'></span><span id='topic+mspeFHPR'></span><span id='topic+mspeFHDL'></span><span id='topic+mspeFHDRS'></span><span id='topic+mspeFHMPR'></span>

<h3>Description</h3>

<p>This function returns MSPE estimator with linearization method for Fay Herriot model. These include the seminal Prasad-Rao method and its generalizations by Datta-Lahiri, Datta-Rao-Smith and Liu et.al. All these methods are developed for general linear mixed effects models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspeFHlin(Y, X, D, method = "PR", var.method = "default")

mspeFHPR(Y, X, D, var.method = "default")

mspeFHDL(Y, X, D, var.method = "default")

mspeFHDRS(Y, X, D, var.method = "default")

mspeFHMPR(Y, X, D, var.method = "default")

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspeFHlin_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Fay Herriot model.
</p>
</td></tr>
<tr><td><code id="mspeFHlin_+3A_x">X</code></td>
<td>

<p>(matrix). It stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeFHlin_+3A_d">D</code></td>
<td>

<p>(vector). Stands for the known sampling variances of each small area levels.
</p>
</td></tr>
<tr><td><code id="mspeFHlin_+3A_method">method</code></td>
<td>

<p>The MSPE estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
<tr><td><code id="mspeFHlin_+3A_var.method">var.method</code></td>
<td>

<p>The variance component estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default <code>method</code> for <code>mspeFHlin</code> is &quot;PR&quot; ,proposed by N. G. N. Prasad and J. N. K. Rao, Prasad-Rao (PR) method uses Taylor series expansion to obtain a second-order approximation to the MSPE. Function <code>mspeFHlin</code> also provide the following methods:
</p>
<p>Method &quot;DL&quot; proposed by Datta and Lahiri , It advanced PR method to cover the cases when the variance components are estimated by ML and REML estimator. Set <code>method = "DL"</code>.
</p>
<p>Method &quot;DRS&quot; proposed by Datta and Smith, It focus on the second order unbiasedness appoximation when the variance component is replaced by Empirical Bayes estimator. Set <code>method = "DRS"</code>.
</p>
<p>Method &quot;MPR&quot; is a modified version of &quot;PR&quot;, It was proposed by Liu et al. It is a robust method that broaden the mean function from the linear form. Set <code>method = "MPR"</code>.
</p>
<p>Default <code>var.method</code> and available variance component estimation method for each method is list as follows:
</p>
<p>For <code>method = "PR"</code>, <code>var.method = "MOM"</code> is the only available variance component estimation method,
</p>
<p>For <code>method = "DL"</code>, <code>var.method = "ML"</code> or <code>var.method = "REML"</code> is available,
</p>
<p>For <code>method = "DRS"</code>, <code>var.method = "EB"</code> is the only available variance component estimation method,
</p>
<p>For <code>method = "MPR"</code>, <code>var.method = "OBP"</code> is the only available variance component estimation method.
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>MSPE</code></td>
<td>
<p>(vector) MSPE estimates for Fay Herriot model.</p>
</td></tr>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>Ahat</code></td>
<td>
<p>(numeric) Estimates of the variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>N. G. N. Prasad and J. N. K. Rao. The estimation of the mean squared error of small-area estimators. <em>Journal of the American Statistical Association</em>, 85(409):163-171, 1990.
</p>
<p>G. S. Datta and P. Lahiri. A unified measure of uncertainty of estimated best linear unbiased predictors in small area estimation problems. <em>Statistica Sinica</em>, 10(2):613-627, 2000.
</p>
<p>G. S. Datta and R. D. D. Smith. On measuring the variability of small area estimators under a basic area level model. <em>Biometrika</em>, 92(1):183-196, 2005.
</p>
<p>X. Liu, H. Ma, and J. Jiang. That prasad-rao is robust: Estimation of mean squared prediction error of observed best predictor under potential model misspecification. <em>Statistica Sinica</em>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(runif(10 * 3), 10, 3)
X[,1] = rep(1, 10) 
D = (1:10) / 10 + 0.5
Y = X %*% c(0.5,1,1.5) + rnorm(10, 0, sqrt(2)) + rnorm(10, 0, sqrt(D))
mspeFHlin(Y,X,D,method = "PR", var.method = "default")
</code></pre>

<hr>
<h2 id='mspeFHpb'>
Compute MSPE through parameter bootstrap method for Fay Herriot model
</h2><span id='topic+mspeFHpb'></span>

<h3>Description</h3>

<p>This function returns MSPE estimator with parameter bootstrap method for Fay Herriot model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspeFHpb(Y, X, D, K = 50, method = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspeFHpb_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Fay Herriot model.
</p>
</td></tr>
<tr><td><code id="mspeFHpb_+3A_x">X</code></td>
<td>

<p>(matrix). Stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeFHpb_+3A_d">D</code></td>
<td>

<p>(vector). It represents the knowing sampling variance for Fay Herriot model.
</p>
</td></tr>
<tr><td><code id="mspeFHpb_+3A_k">K</code></td>
<td>

<p>(integer). It represents the bootstrap sample number. Default value is 50.
</p>
</td></tr>
<tr><td><code id="mspeFHpb_+3A_method">method</code></td>
<td>

<p>The variance component estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method was proposed by Peter Hall and T. Maiti. Parametric bootstrap (pb) method uses bootstrap-based method to measure the accuracy of the EB estimator. In this case, only EB estimator is available (<code>method = 4</code>).
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>MSPE</code></td>
<td>
<p>(vector) MSPE estimates for Fay Herriot model.</p>
</td></tr>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>Ahat</code></td>
<td>
<p>(numeric) Estimates of the variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>F. B. Butar and P. Lahiri. On measures of uncertainty of empirical bayes small area estimators. <em>Journal of Statistical Planning and Inference</em>, 112(1-2):63-76, 2003.
</p>
<p>N. G. N. Prasad and J. N. K. Rao. The estimation of the mean squared error of small-area estimators. <em>Journal of the American Statistical Association</em>, 85(409):163-171, 1990.
</p>
<p>Peter Hall and T. Maiti. On parametric bootstrap methods for small area prediction. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 2006a.
</p>
<p>H. T. Maiti and T. Maiti. Nonparametric estimation of mean squared prediction error in nested error regression models. <em>Annals of Statistics</em>, 34(4):1733-1750, 2006b.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(runif(10 * 3), 10, 3)
X[,1] = rep(1, 10) 
D = (1:10) / 10 + 0.5
Y = X %*% c(0.5,1,1.5) + rnorm(10, 0, sqrt(2)) + rnorm(10, 0, sqrt(D))
mspeFHpb(Y, X, D, K = 50, method = 4)
</code></pre>

<hr>
<h2 id='mspeFHsumca'>
Compute MSPE through Sumca method for Fay Herriot model
</h2><span id='topic+mspeFHsumca'></span>

<h3>Description</h3>

<p>This function returns MSPE estimator with the combination of linearization and resampling appoximation method called &quot;Sumca&quot;, for Fay Herriot model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspeFHsumca(Y, X, D, K = 50, method = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspeFHsumca_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Fay Herriot model.
</p>
</td></tr>
<tr><td><code id="mspeFHsumca_+3A_x">X</code></td>
<td>

<p>(matrix). Stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeFHsumca_+3A_d">D</code></td>
<td>

<p>(vector). It represents the knowing sampling variance for Fay Herriot model.
</p>
</td></tr>
<tr><td><code id="mspeFHsumca_+3A_k">K</code></td>
<td>

<p>(integer). It represents the Monte-Carlo sample size for &quot;Sumca&quot;. Default value is 50.
</p>
</td></tr>
<tr><td><code id="mspeFHsumca_+3A_method">method</code></td>
<td>

<p>It represents the variance component estimation method. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method was proposed by J. Jiang, P. Lahiri, and T. Nguyen, sumca method combines the advantages of linearization and resampling methods and obtains unified, positive, low-computation burden and second-order unbiased MSPE estimators.
</p>
<p>Default value for <code>method</code> is 1, <code>method = 1</code> represents the MOM method , <code>method = 2</code> and <code>method = 3</code> represents ML and REML method, respectively.
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>MSPE</code></td>
<td>
<p>(vector) MSPE estimates for Fay Herriot model.</p>
</td></tr>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>Ahat</code></td>
<td>
<p>(numeric) Estimates of the variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>J. Jiang and M. Torabi. Sumca: simple; unified; monte carlo assisted approach to second order unbiased mean squared prediction error estimation. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 82(2):467-485, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(runif(10 * 3), 10, 3)
X[,1] = rep(1, 10) 
D = (1:10) / 10 + 0.5
Y = X %*% c(0.5,1,1.5) + rnorm(10, 0, sqrt(2)) + rnorm(10, 0, sqrt(D))
mspeFHsumca(Y, X, D, K = 50, method = 1)
</code></pre>

<hr>
<h2 id='mspeNERdb'>
Compute MSPE through double bootstrap(DB) method for Nested error regression model
</h2><span id='topic+mspeNERdb'></span>

<h3>Description</h3>

<p>This function returns MSPE estimator with double bootstrap method for Nested error regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspeNERdb(ni, X, Y, Xmean, K = 50, C = 50, method = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspeNERdb_+3A_ni">ni</code></td>
<td>

<p>(vector). It represents the sample number for every small area.
</p>
</td></tr>
<tr><td><code id="mspeNERdb_+3A_x">X</code></td>
<td>

<p>(matrix).  It represents the small area response.
</p>
</td></tr>
<tr><td><code id="mspeNERdb_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the design matrix.
</p>
</td></tr>
<tr><td><code id="mspeNERdb_+3A_xmean">Xmean</code></td>
<td>

<p>(matrix). Stands for the population mean of auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeNERdb_+3A_k">K</code></td>
<td>

<p>(integer). It represents the first bootstrap sample number. Default value is 50.
</p>
</td></tr>
<tr><td><code id="mspeNERdb_+3A_c">C</code></td>
<td>

<p>(integer). It represents the second bootstrap sample number. Default value is 50.
</p>
</td></tr>
<tr><td><code id="mspeNERdb_+3A_method">method</code></td>
<td>

<p>The variance component estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method was proposed by P. Hall and T. Maiti. Double bootstrap method uses boostrap tool twice for NER model to avoid the unattractivitive bias correction: one is to estimate the estimator bias, the other is to correct for bias.
</p>
<p>Default value for <code>method</code> is 1, <code>method = 1</code> represents the MOM method , <code>method = 2</code> and <code>method = 3</code> represents ML and REML method, respectively.
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>MSPE</code></td>
<td>
<p>(vector) MSPE estimates for NER model.</p>
</td></tr>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>sigvhat2</code></td>
<td>
<p>(numeric) Estimates of the area-specific variance component.</p>
</td></tr>
<tr><td><code>sigehat2</code></td>
<td>
<p>(numeric) Estimates of the random error variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>F. B. Butar and P. Lahiri. On measures of uncertainty of empirical bayes small area estimators. <em>Journal of Statistical Planning and Inference</em>, 112(1-2):63-76, 2003.
</p>
<p>N. G. N. Prasad and J. N. K. Rao. The estimation of the mean squared error of small-area estimators. <em>Journal of the American Statistical Association</em>, 85(409):163-171, 1990.
</p>
<p>Peter Hall and T. Maiti. On parametric bootstrap methods for small area prediction. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 2006a.
</p>
<p>H. T. Maiti and T. Maiti. Nonparametric estimation of mean squared prediction error in nested error regression models. <em>Annals of Statistics</em>, 34(4):1733-1750, 2006b.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### parameter setting 
Ni = 1000; sigmaX = 1.5; m = 10
beta = c(0.5, 1)
sigma_v2 = 0.8; sigma_e2 = 1
ni = sample(seq(1,10), m,replace = TRUE); n = sum(ni)
p = length(beta)
### population function
pop.model = function(Ni, sigmaX, beta, sigma_v2, sigma_e2, m){
  x = rnorm(m * Ni, 1, sqrt(sigmaX)); v = rnorm(m, 0, sqrt(sigma_v2)); y = numeric(m * Ni)
  theta = numeric(m); kk = 1
  for(i in 1 : m){
    sumx = 0
    for(j in 1:Ni){
      sumx = sumx + x[kk]
      y[kk] = beta[1] + beta[2] * x[kk] + v[i] + rnorm(1, 0, sqrt(sigma_e2))
      kk = kk + 1
    }
    meanx = sumx/Ni
    theta[i] = beta[1] + beta[2] * meanx + v[i]
  }
  group = rep(seq(m), each = Ni)
  x = cbind(rep(1, m*Ni), x)
  data = cbind(x, y, group)
  return(list(data = data, theta = theta))
} 
### sample function
sampleXY = function(Ni, ni, m, Population){
  Indx = c()
  for(i in 1:m){
    Indx = c(Indx, sample(c(((i - 1) * Ni + 1) : (i * Ni)), ni[i]))
  }
  Sample = Population[Indx, ]; Nonsample = Population[-Indx, ]
  return(list(Sample, Nonsample))
} 
### data generation process
Population = pop.model(Ni, sigmaX, beta, sigma_v2, sigma_e2, m)$data
XY = sampleXY(Ni, ni, m, Population)[[1]]
X = XY[, 1:p]
Y = XY[, p+1]
Xmean = matrix(NA, m, p)
for(tt in 1: m){
  Xmean[tt, ] = colMeans(Population[which(Population[,p+2] == tt), 1:p])
}
### mspe result
mspeNERdb(ni, X, Y, Xmean, 10, 10, method = 1)
</code></pre>

<hr>
<h2 id='mspeNERjack'>
Compute MSPE through Jackknife-based MSPE estimation method for Nested error regression model
</h2><span id='topic+mspeNERjack'></span>

<h3>Description</h3>

<p>This function returns MSPE estimator with Jackknife-based MSPE estimation method for Nested error regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspeNERjack(ni, X, Y, Xmean, method = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspeNERjack_+3A_ni">ni</code></td>
<td>

<p>(vector). It represents the sample number for every small area.
</p>
</td></tr>
<tr><td><code id="mspeNERjack_+3A_x">X</code></td>
<td>

<p>(matrix). Stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeNERjack_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Nested error regression model.
</p>
</td></tr>
<tr><td><code id="mspeNERjack_+3A_xmean">Xmean</code></td>
<td>

<p>(matrix). Stands for the population mean of auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeNERjack_+3A_method">method</code></td>
<td>

<p>The MSPE estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This bias-corrected jackknife MSPE estimator was proposed by J. Jiang and L. S. M. Wan, it covers a fairly general class of mixed models which includes gLMM, mixed logistic model and some of the widely used mixed linear models as special cases.
</p>
<p>Default value for <code>method</code> is 1, <code>method = 1</code> represents the MOM method , <code>method = 2</code> and <code>method = 3</code> represents ML and REML method, respectively.
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>MSPE</code></td>
<td>
<p>(vector) MSPE estimates for NER model.</p>
</td></tr>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>sigvhat2</code></td>
<td>
<p>(numeric) Estimates of the area-specific variance component.</p>
</td></tr>
<tr><td><code>sigehat2</code></td>
<td>
<p>(numeric) Estimates of the random error variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>M. H. Quenouille. Approximate tests of correlation in time series. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, 11(1):68-84, 1949.
</p>
<p>J. W. Tukey. Bias and confidence in not quite large samples. <em>Annals of Mathematical Statistics</em>, 29(2):614, 1958.
</p>
<p>J. Jiang and L. S. M. Wan. A unified jackknife theory for empirical best prediction with m estimation. <em>Annals of Statistics</em>, 30(6):1782-1810, 2002.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### parameter setting 
Ni = 1000; sigmaX = 1.5; m = 5
beta = c(0.5, 1)
sigma_v2 = 0.8; sigma_e2 = 1
ni = sample(seq(1,10), m,replace = TRUE); n = sum(ni)
p = length(beta)
### population function
pop.model = function(Ni, sigmaX, beta, sigma_v2, sigma_e2, m){
  x = rnorm(m * Ni, 1, sqrt(sigmaX)); v = rnorm(m, 0, sqrt(sigma_v2)); y = numeric(m * Ni)
  theta = numeric(m); kk = 1
  for(i in 1 : m){
    sumx = 0
    for(j in 1:Ni){
      sumx = sumx + x[kk]
      y[kk] = beta[1] + beta[2] * x[kk] + v[i] + rnorm(1, 0, sqrt(sigma_e2))
      kk = kk + 1
    }
    meanx = sumx/Ni
    theta[i] = beta[1] + beta[2] * meanx + v[i]
  }
  group = rep(seq(m), each = Ni)
  x = cbind(rep(1, m*Ni), x)
  data = cbind(x, y, group)
  return(list(data = data, theta = theta))
} 
### sample function
sampleXY = function(Ni, ni, m, Population){
  Indx = c()
  for(i in 1:m){
    Indx = c(Indx, sample(c(((i - 1) * Ni + 1) : (i * Ni)), ni[i]))
  }
  Sample = Population[Indx, ]; Nonsample = Population[-Indx, ]
  return(list(Sample, Nonsample))
} 
### data generation process
Population = pop.model(Ni, sigmaX, beta, sigma_v2, sigma_e2, m)$data
XY = sampleXY(Ni, ni, m, Population)[[1]]
X = XY[, 1:p]
Y = XY[, p+1]
Xmean = matrix(NA, m, p)
for(tt in 1: m){
  Xmean[tt, ] = colMeans(Population[which(Population[,p+2] == tt), 1:p])
}
### mspe result
mspeNERjack(ni, X, Y, Xmean, method = 1)
</code></pre>

<hr>
<h2 id='mspeNERlin'>
Compute MSPE through linearization method for Nested error regression model
</h2><span id='topic+mspeNERlin'></span><span id='topic+mspeNERPR'></span><span id='topic+mspeNERDL'></span>

<h3>Description</h3>

<p>This function returns MSPE estimator with linearization method for Nested error regression model. These include the seminal Prasad-Rao method and its generalizations by Datta-Lahiri. All these methods are developed for general linear mixed effects models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspeNERlin(ni, X, Y, X.mean, method = "PR", var.method = "default")

mspeNERPR(ni, X, Y, X.mean, var.method = "default")

mspeNERDL(ni, X, Y, X.mean, var.method = "default")

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspeNERlin_+3A_ni">ni</code></td>
<td>

<p>(vector). It represents the sample number for every small area.
</p>
</td></tr>
<tr><td><code id="mspeNERlin_+3A_x">X</code></td>
<td>

<p>(matrix). Stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeNERlin_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Nested error regression model.
</p>
</td></tr>
<tr><td><code id="mspeNERlin_+3A_x.mean">X.mean</code></td>
<td>

<p>(matrix). Stands for the population mean of auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeNERlin_+3A_method">method</code></td>
<td>

<p>The MSPE estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
<tr><td><code id="mspeNERlin_+3A_var.method">var.method</code></td>
<td>

<p>The variance component estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default <code>method</code> for <code>mspeNERlin</code> is &quot;PR&quot; ,proposed by N. G. N. Prasad and J. N. K. Rao, Prasad-Rao (PR) method uses Taylor series expansion to obtain a second-order approximation to the MSPE. Function <code>mspeNERlin</code> also provide the following method:
</p>
<p>Method &quot;DL&quot; advanced PR method to cover the cases when the variance components are estimated by ML and REML estimator. Set <code>method = "DL"</code>.
</p>
<p>For <code>method = "PR"</code>, <code>var.method = "MOM"</code> is the only available variance component estimation method,
</p>
<p>For <code>method = "DL"</code>, <code>var.method = "ML"</code> or <code>var.method = "REML"</code> are available.
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>MSPE</code></td>
<td>
<p>(vector) MSPE estimates for NER model.</p>
</td></tr>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>sigvhat2</code></td>
<td>
<p>(numeric) Estimates of the area-specific variance component.</p>
</td></tr>
<tr><td><code>sigehat2</code></td>
<td>
<p>(numeric) Estimates of the random error variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>N. G. N. Prasad and J. N. K. Rao. The estimation of the mean squared error of small-area estimators. <em>Journal of the American Statistical Association</em>, 85(409):163-171, 1990.
</p>
<p>G. S. Datta and P. Lahiri. A unified measure of uncertainty of estimated best linear unbiased predictors in small area estimation problems. <em>Statistica Sinica</em>, 10(2):613-627, 2000.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### parameter setting 
Ni = 1000; sigmaX = 1.5; K = 100; C = 50; m = 10
beta = c(0.5, 1)
sigma_v2 = 0.8; sigma_e2 = 1
ni = sample(seq(1,10), m,replace = TRUE); n = sum(ni)
p = length(beta)
### population function
pop.model = function(Ni, sigmaX, beta, sigma_v2, sigma_e2, m){
  x = rnorm(m * Ni, 1, sqrt(sigmaX)); v = rnorm(m, 0, sqrt(sigma_v2)); y = numeric(m * Ni)
  theta = numeric(m); kk = 1
  for(i in 1 : m){
    sumx = 0
    for(j in 1:Ni){
      sumx = sumx + x[kk]
      y[kk] = beta[1] + beta[2] * x[kk] + v[i] + rnorm(1, 0, sqrt(sigma_e2))
      kk = kk + 1
    }
    meanx = sumx/Ni
    theta[i] = beta[1] + beta[2] * meanx + v[i]
  }
  group = rep(seq(m), each = Ni)
  x = cbind(rep(1, m*Ni), x)
  data = cbind(x, y, group)
  return(list(data = data, theta = theta))
} 
### sample function
sampleXY = function(Ni, ni, m, Population){
  Indx = c()
  for(i in 1:m){
    Indx = c(Indx, sample(c(((i - 1) * Ni + 1) : (i * Ni)), ni[i]))
  }
  Sample = Population[Indx, ]; Nonsample = Population[-Indx, ]
  return(list(Sample, Nonsample))
} 
### data generation process
Population = pop.model(Ni, sigmaX, beta, sigma_v2, sigma_e2, m)$data
XY = sampleXY(Ni, ni, m, Population)[[1]]
X = XY[, 1:p]
Y = XY[, p+1]
Xmean = matrix(NA, m, p)
for(tt in 1: m){
  Xmean[tt, ] = colMeans(Population[which(Population[,p+2] == tt), 1:p])
}
### mspe result
mspeNERlin(ni, X, Y, Xmean, method = "PR", var.method = "default")
</code></pre>

<hr>
<h2 id='mspeNERpb'>
Compute MSPE through parameter bootstrap method for Nested error regression model
</h2><span id='topic+mspeNERpb'></span>

<h3>Description</h3>

<p>This function returns MSPE estimator with parameter bootstrap appoximation method for Nested error regression model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspeNERpb(ni, X, Y, Xmean, K = 50, method = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspeNERpb_+3A_ni">ni</code></td>
<td>

<p>(vector). It represents the sample number for every small area.
</p>
</td></tr>
<tr><td><code id="mspeNERpb_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Nested error regression model.
</p>
</td></tr>
<tr><td><code id="mspeNERpb_+3A_x">X</code></td>
<td>

<p>(matrix). Stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeNERpb_+3A_xmean">Xmean</code></td>
<td>

<p>(matrix). Stands for the population mean of auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeNERpb_+3A_k">K</code></td>
<td>

<p>(integer). It represents the bootstrap sample number. Default value is 50.
</p>
</td></tr>
<tr><td><code id="mspeNERpb_+3A_method">method</code></td>
<td>

<p>The variance component estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method was proposed by Peter Hall and T. Maiti. Parametric bootstrap (pb) method uses bootstrap-based method to measure the accuracy of EB estimator. In this case, only EB estimator is available (<code>method = 4</code>).
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>MSPE</code></td>
<td>
<p>(vector) MSPE estimates for NER model.</p>
</td></tr>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>sigvhat2</code></td>
<td>
<p>(numeric) Estimates of the area-specific variance component.</p>
</td></tr>
<tr><td><code>sigehat2</code></td>
<td>
<p>(numeric) Estimates of the random error variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>F. B. Butar and P. Lahiri. On measures of uncertainty of empirical bayes small area estimators. <em>Journal of Statistical Planning and Inference</em>, 112(1-2):63-76, 2003.
</p>
<p>N. G. N. Prasad and J. N. K. Rao. The estimation of the mean squared error of small-area estimators. <em>Journal of the American Statistical Association</em>, 85(409):163-171, 1990.
</p>
<p>Peter Hall and T. Maiti. On parametric bootstrap methods for small area prediction. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 2006a.
</p>
<p>H. T. Maiti and T. Maiti. Nonparametric estimation of mean squared prediction error in nested error regression models. <em>Annals of Statistics]</em>, 34(4):1733-1750, 2006b.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### parameter setting 
Ni = 1000; sigmaX = 1.5; K = 50; C = 50; m = 10
beta = c(0.5, 1)
sigma_v2 = 0.8; sigma_e2 = 1
ni = sample(seq(1,10), m,replace = TRUE); n = sum(ni)
p = length(beta)
### population function
pop.model = function(Ni, sigmaX, beta, sigma_v2, sigma_e2, m){
  x = rnorm(m * Ni, 1, sqrt(sigmaX)); v = rnorm(m, 0, sqrt(sigma_v2)); y = numeric(m * Ni)
  theta = numeric(m); kk = 1
  for(i in 1 : m){
    sumx = 0
    for(j in 1:Ni){
      sumx = sumx + x[kk]
      y[kk] = beta[1] + beta[2] * x[kk] + v[i] + rnorm(1, 0, sqrt(sigma_e2))
      kk = kk + 1
    }
    meanx = sumx/Ni
    theta[i] = beta[1] + beta[2] * meanx + v[i]
  }
  group = rep(seq(m), each = Ni)
  x = cbind(rep(1, m*Ni), x)
  data = cbind(x, y, group)
  return(list(data = data, theta = theta))
} 
### sample function
sampleXY = function(Ni, ni, m, Population){
  Indx = c()
  for(i in 1:m){
    Indx = c(Indx, sample(c(((i - 1) * Ni + 1) : (i * Ni)), ni[i]))
  }
  Sample = Population[Indx, ]; Nonsample = Population[-Indx, ]
  return(list(Sample, Nonsample))
} 
### data generation process
Population = pop.model(Ni, sigmaX, beta, sigma_v2, sigma_e2, m)$data
XY = sampleXY(Ni, ni, m, Population)[[1]]
X = XY[, 1:p]
Y = XY[, p+1]
Xmean = matrix(NA, m, p)
for(tt in 1: m){
  Xmean[tt, ] = colMeans(Population[which(Population[,p+2] == tt), 1:p])
}
### mspe result
mspeNERpb(ni, X, Y, Xmean, 50)
</code></pre>

<hr>
<h2 id='mspeNERsumca'>
Compute MSPE through Sumca method for Nested error regression model
</h2><span id='topic+mspeNERsumca'></span>

<h3>Description</h3>

<p>This function returns MSPE estimator with the combination of linearization and resampling appoximation method for Nested error regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspeNERsumca(ni, X, Y, Xmean, K = 50, method = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspeNERsumca_+3A_ni">ni</code></td>
<td>

<p>(vector). It represents the sample number for every small area.
</p>
</td></tr>
<tr><td><code id="mspeNERsumca_+3A_x">X</code></td>
<td>

<p>(matrix). Stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeNERsumca_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Nested error regression model.
</p>
</td></tr>
<tr><td><code id="mspeNERsumca_+3A_xmean">Xmean</code></td>
<td>

<p>(matrix). Stands for the population mean of auxiliary values.
</p>
</td></tr>
<tr><td><code id="mspeNERsumca_+3A_k">K</code></td>
<td>

<p>(integer). It represents the Monte-Carlo sample size for &quot;Sumca&quot;. Default value is 50.
</p>
</td></tr>
<tr><td><code id="mspeNERsumca_+3A_method">method</code></td>
<td>

<p>The MSPE estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method was proposed by J. Jiang, P. Lahiri, and T. Nguyen, sumca method combines the advantages of linearization and resampling methods and obtains unified, positive, low-computation burden and second-order unbiased MSPE estimators.
</p>
<p>Default value for <code>method</code> is 1, <code>method = 1</code> represents the MOM method , <code>method = 2</code> and <code>method = 3</code> represents ML and REML method, respectively.
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>MSPE</code></td>
<td>
<p>(vector) MSPE estimates for NER model.</p>
</td></tr>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>sigvhat2</code></td>
<td>
<p>(numeric) Estimates of the area-specific variance component.</p>
</td></tr>
<tr><td><code>sigehat2</code></td>
<td>
<p>(numeric) Estimates of the random error variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>J. Jiang and M. Torabi. Sumca: simple; unified; monte carlo assisted approach to second order unbiased mean squared prediction error estimation. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 82(2):467-485, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### parameter setting 
Ni = 1000; sigmaX = 1.5; m = 10
beta = c(0.5, 1)
sigma_v2 = 0.8; sigma_e2 = 1
ni = sample(seq(1,10), m,replace = TRUE); n = sum(ni)
p = length(beta)
### population function
pop.model = function(Ni, sigmaX, beta, sigma_v2, sigma_e2, m){
  x = rnorm(m * Ni, 1, sqrt(sigmaX)); v = rnorm(m, 0, sqrt(sigma_v2)); y = numeric(m * Ni)
  theta = numeric(m); kk = 1
  for(i in 1 : m){
    sumx = 0
    for(j in 1:Ni){
      sumx = sumx + x[kk]
      y[kk] = beta[1] + beta[2] * x[kk] + v[i] + rnorm(1, 0, sqrt(sigma_e2))
      kk = kk + 1
    }
    meanx = sumx/Ni
    theta[i] = beta[1] + beta[2] * meanx + v[i]
  }
  group = rep(seq(m), each = Ni)
  x = cbind(rep(1, m*Ni), x)
  data = cbind(x, y, group)
  return(list(data = data, theta = theta))
} 
### sample function
sampleXY = function(Ni, ni, m, Population){
  Indx = c()
  for(i in 1:m){
    Indx = c(Indx, sample(c(((i - 1) * Ni + 1) : (i * Ni)), ni[i]))
  }
  Sample = Population[Indx, ]; Nonsample = Population[-Indx, ]
  return(list(Sample, Nonsample))
} 
### data generation process
Population = pop.model(Ni, sigmaX, beta, sigma_v2, sigma_e2, m)$data
XY = sampleXY(Ni, ni, m, Population)[[1]]
X = XY[, 1:p]
Y = XY[, p+1]
Xmean = matrix(NA, m, p)
for(tt in 1: m){
  Xmean[tt, ] = colMeans(Population[which(Population[,p+2] == tt), 1:p])
}
### mspe result
mspeNERsumca(ni, X, Y, Xmean, 50, method = 1)
</code></pre>

<hr>
<h2 id='saeMSPE-package'>
Compute MSPE Estimates for the Fay Herriot Model and Nested Error Regression Model
</h2><span id='topic+saeMSPE-package'></span><span id='topic+saeMSPE'></span>

<h3>Description</h3>

<p>We describe a new R package entitled 'saeMSPE' for the well-known Fay Herriot model and nested error regression model in small area estimation. Based on this package, it is possible to easily compute various common mean squared predictive error (MSPE) estimators, as well as several existing variance component predictors as a byproduct, for these two models.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> saeMSPE</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.2</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2022-10-19</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;=2)</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> Matrix, smallarea</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, Shaochu Liu
</p>
<p>Maintainer: Peiwen Xiao &lt;2569613200@qq.com&gt;
</p>


<h3>References</h3>

<p>- V. Arora and P. Lahiri. On the superiority of the bayesian method over the blup in small area estimation problems. Statistica Sinica, 7(4):1053-1063, 1997.
</p>
<p>- G. E. Battese, R. M. Harter, andW. A. Fuller. An error components model for prediction of county crop areas using survey and satellite data. Journal of the American Statistical Association, 83(401):28-36,1988.
</p>
<p>- F. B. Butar and P. Lahiri. On measures of uncertainty of empirical bayes small area estimators. Journal of Statistical Planning and Inference, 112(1-2):63-76, 2003.
</p>
<p>- G. S. Datta and P. Lahiri. A unified measure of uncertainty of estimated best linear unbiased predictors in small area estimation problems. Statistica Sinica, 10(2):613-627, 2000.
</p>
<p>- G. S. Datta, J. N. K. Rao, and D. D. Smith. On measuring the variability of small area estimators under a basic area level model. Biometrika, 92(1):183-196, 2005.
</p>
<p>- D. Eddelbuettel and C. Sanderson. Rcpparmadillo: Accelerating r with high-performance c++ linear algebra. Computational Statistics and Data Analysis, 71(March):1054-1063, 2014.
</p>
<p>- D. Eddelbuettel, R. Francois, J. J. Allaire, K. Ushey, and Q. Kou. Rcpp: Seamless r and c++ integration. Journal of Statistical Software, 40(i08), 2011.
</p>
<p>- R. E. Fay and R. A. Herriot. Estimates of income for small places: An application of james-stein procedures to census data. Journal of the American Statistical Association, 74(366a):269-277, 1979.
</p>
<p>- W. González, M. J. Lombardía, I. Molina, D. Morales, and L. Santamaría. Bootstrap mean squared error of a small-area eblup. Journal of Statistical Computation and Simulation, 78(5):443-462, 2008.
</p>
<p>- P. Hall and T. Maiti. On parametric bootstrap methods for small area prediction. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2006a.
</p>
<p>- P. Hall and T. Maiti. Nonparametric estimation of mean squared prediction error in nested error regression models. Annals of Statistics, 34(4):1733-1750, 2006b.
</p>
<p>- C. R. Henderson. Best linear unbiased estimation and prediction under a selection model. Biometrics, 31(2):423-447, 1975.
</p>
<p>- J. Jiang. Asymptotic Analysis of Mixed Effects Models: Theory, Applications, and Open Problems. 09 2017. ISBN 978-1-3151-1928-1. doi: 10.1201/9781315119281.
</p>
<p>- J. Jiang and T. Nguyen. Linear and Generalized Linear Mixed Models and Their Applications. 01 2021. ISBN 978-1-0716-1281-1. doi: 10.1007/978-1-0716-1282-8.
</p>
<p>- J. Jiang and M. Torabi. Sumca: simple; unified; monte carlo assisted approach to second order unbiased mean squared prediction error estimation. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(2):467-485, 2020
</p>
<p>- J. Jiang and L. S. M.Wan. A unified jackknife theory for empirical best prediction with m estimation.
Annals of Statistics, 30(6):1782-1810, 2002.
</p>
<p>- J. Jiang, T. Nguyen, and J. S. Rao. Best predictive small area estimation. Journal of the American Statistical Association, 106(494):732-745, 2011.
</p>
<p>- J. Jiang, P. Lahiri, and T. Nguyen. A unified monte carlo jackknife for small area estimation after model selection. Annals of Mathematiacal Sciences and Applications, 3(2):405-438, 2018.
</p>
<p>- R. N. Kackar and D. A. Harville. Approximations for standard errors of estimators of fixed and random effects in mixed linear models. Journal of the American Statistical Association, 1984.
</p>
<p>- X. Liu, H. Ma, and J. Jiang. That prasad-rao is robust: Estimation of mean squared prediction error of observed best predictor under potential model misspecification. Statistica Sinica, 2020.
</p>
<p>- N. G. N. Prasad and J. N. K. Rao. The estimation of the mean squared error of small area estimators. Journal of the American Statistical Association, 85(409):163-171, 1990.
</p>
<p>- M. H. Quenouille. Approximate tests of correlation in time series. Journal of the Royal Statistical Society. Series B (Methodological), 11(1):68-84, 1949.
</p>
<p>- J. N. K. Rao and I. Molina. Small area estimation. Wiley, 2015.
</p>
<p>- J. W. Tukey. Bias and confidence in not quite large samples. Annals of Mathematical Statistics, 29(2):614, 1958.
</p>
<p>- Y. You and B. Chapman. Small area estimation using area level models and estimated sampling variances. Survey Methodology, 32(1):97-103, 2006.
</p>

<hr>
<h2 id='varfh'>
Estimates of the variance component using several methods for Fay Herriot model.
</h2><span id='topic+varfh'></span><span id='topic+varOBP'></span>

<h3>Description</h3>

<p>This function returns the estimate of variance component with several existing method for Fay Herriot model. This function does not accept missing values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varfh(Y, X, D, method)

varOBP(Y, X, D)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varfh_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Fay Herriot model.
</p>
</td></tr>
<tr><td><code id="varfh_+3A_x">X</code></td>
<td>

<p>(matrix). It stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="varfh_+3A_d">D</code></td>
<td>

<p>(vector). It represents the knowing sampling variance for Fay Herriot model.
</p>
</td></tr>
<tr><td><code id="varfh_+3A_method">method</code></td>
<td>

<p>Variance component estimation method. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default value for <code>method</code> is 1, It represents the moment estimator, Also called ANOVA estimator, The available variance component estimation method are list as follows:
</p>
<p><code>method = 1</code> represents the moment (MOM) estimator, ; 
</p>
<p><code>method = 2</code> represents the restricted maximum likelihood (REML) estimator; 
</p>
<p><code>method = 3</code> represents the maximum likelihood (ML) estimator;  
</p>
<p><code>method = 4</code> represents the empirical bayesian (EB) estimator;  
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>Ahat</code></td>
<td>
<p>(numeric) Estimates of the variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>J. Jiang. Linear and Generalized Linear Mixed Models and Their Applications. 2007.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X = matrix(runif(10 * 3), 10, 3)
X[,1] = rep(1, 10) 
D = (1:10) / 10 + 0.5
Y = X %*% c(0.5,1,1.5) + rnorm(10, 0, sqrt(2)) + rnorm(10, 0, sqrt(D))
varOBP(Y, X, D)
varfh(Y, X, D, 1)
</code></pre>

<hr>
<h2 id='varner'>
Estimates of the variance component using several methods for Nested error regression model.
</h2><span id='topic+varner'></span>

<h3>Description</h3>

<p>This function returns the estimate of variance component with several existing method for Nested error regression model. This function does not accept missing values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varner(ni, X, Y, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varner_+3A_ni">ni</code></td>
<td>

<p>(vector). It represents the sample number for every small area.
</p>
</td></tr>
<tr><td><code id="varner_+3A_x">X</code></td>
<td>

<p>(matrix). Stands for the available auxiliary values.
</p>
</td></tr>
<tr><td><code id="varner_+3A_y">Y</code></td>
<td>

<p>(vector). It represents the response value for Nested error regression model.
</p>
</td></tr>
<tr><td><code id="varner_+3A_method">method</code></td>
<td>

<p>The variance component estimation method to be used. See &quot;Details&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default value for <code>method</code> is 1, It represents the moment estimator, Also called ANOVA estimator, The available variance component estimation method are list as follows:
</p>
<p><code>method = 1</code> represents the MOM estimator; 
</p>
<p><code>method = 2</code> represents the restricted maximum likelihood (REML) estimator; 
</p>
<p><code>method = 3</code> represents the maximum likelihood (ML) estimator;  
</p>
<p><code>method = 4</code> represents the empirical bayesian (EB) estimator;  
</p>


<h3>Value</h3>

<p>This function returns a list with components:
</p>
<table>
<tr><td><code>bhat</code></td>
<td>
<p>(vector) Estimates of the unknown regression coefficients.</p>
</td></tr>
<tr><td><code>sigvhat2</code></td>
<td>
<p>(numeric) Estimates of the area-specific variance component.</p>
</td></tr>
<tr><td><code>sigehat2</code></td>
<td>
<p>(numeric) Estimates of the random error variance component.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peiwen Xiao, Xiaohui Liu, Yuzi Liu, Jiming Jiang, and Shaochu Liu
</p>


<h3>References</h3>

<p>J. Jiang. Linear and Generalized Linear Mixed Models and Their Applications. 2007.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### parameter setting 
Ni = 1000; sigmaX = 1.5; m = 10
beta = c(0.5, 1)
sigma_v2 = 0.8; sigma_e2 = 1
ni = sample(seq(1,10), m,replace = TRUE); n = sum(ni)
p = length(beta)
### population function
pop.model = function(Ni, sigmaX, beta, sigma_v2, sigma_e2, m){
  x = rnorm(m * Ni, 1, sqrt(sigmaX)); v = rnorm(m, 0, sqrt(sigma_v2)); y = numeric(m * Ni)
  theta = numeric(m); kk = 1
  for(i in 1 : m){
    sumx = 0
    for(j in 1:Ni){
      sumx = sumx + x[kk]
      y[kk] = beta[1] + beta[2] * x[kk] + v[i] + rnorm(1, 0, sqrt(sigma_e2))
      kk = kk + 1
    }
    meanx = sumx/Ni
    theta[i] = beta[1] + beta[2] * meanx + v[i]
  }
  group = rep(seq(m), each = Ni)
  x = cbind(rep(1, m*Ni), x)
  data = cbind(x, y, group)
  return(list(data = data, theta = theta))
} 
### sample function
sampleXY = function(Ni, ni, m, Population){
  Indx = c()
  for(i in 1:m){
    Indx = c(Indx, sample(c(((i - 1) * Ni + 1) : (i * Ni)), ni[i]))
  }
  Sample = Population[Indx, ]; Nonsample = Population[-Indx, ]
  return(list(Sample, Nonsample))
} 
### data generation process
Population = pop.model(Ni, sigmaX, beta, sigma_v2, sigma_e2, m)$data
XY = sampleXY(Ni, ni, m, Population)[[1]]
X = XY[, 1:p]
Y = XY[, p+1]
### variance component estimate
varner(ni,X,Y,1)
</code></pre>

<hr>
<h2 id='wheatarea'> Wheat area measurement and satellite data.</h2><span id='topic+wheatarea'></span>

<h3>Description</h3>

 
<p>Wheat area data measured at the scene in the block of Yanzhou District, Jining City, Shandong Province. The data corresponding to each block comes from the ArcGIS platform. The whole dataset consists of a total number of 458 villages and 14750 wheat blocks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(wheatarea)</code></pre>


<h3>Format</h3>

<p>A data frame with 14708 observations on the following 3 variables.
</p>

<dl>
<dt><code>pixel</code>:</dt><dd><p> Pixel sizes of each wheat blocks.</p>
</dd>
<dt><code>F_AREA</code>:</dt><dd><p> Field inspection area of each wheat blocks.</p>
</dd>
<dt><code>code</code>:</dt><dd><p> Street code.</p>
</dd>
</dl>



<h3>Source</h3>

 
<p>- Liu Y, Qu W, Cui Z, Liu X, Xu W, Jiang j,. (2021). Estimation of Wheat Growing Area via Mixed Model Prediction Using Satellite Data. Journal of Applied Statistics and Management. 
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
