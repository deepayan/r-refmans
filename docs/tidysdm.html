<!DOCTYPE html><html><head><title>Help for package tidysdm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tidysdm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#%&gt;%'><p>Pipe operator</p></a></li>
<li><a href='#add_member'><p>Add best member of workflow to a simple ensemble</p></a></li>
<li><a href='#add_repeat'><p>Add repeat(s) to a repeated ensemble</p></a></li>
<li><a href='#autoplot.simple_ensemble'><p>Plot the results of a simple ensemble</p></a></li>
<li><a href='#autoplot.spatial_initial_split'><p>Create a ggplot for a spatial initial rsplit.</p></a></li>
<li><a href='#blockcv2rsample'><p>Convert an object created with <code>blockCV</code> to an <code>rsample</code> object</p></a></li>
<li><a href='#boyce_cont'><p>Boyce continuous index (BCI)</p></a></li>
<li><a href='#calib_class_thresh'><p>Calibrate class thresholds</p></a></li>
<li><a href='#check_coords_names'><p>Check that we have a valid pair of coordinate names</p></a></li>
<li><a href='#check_sdm_presence'><p>Check that the column with presences is correctly formatted</p></a></li>
<li><a href='#check_splits_balance'><p>Check the balance of presences vs pseudoabsences among splits</p></a></li>
<li><a href='#collect_metrics.simple_ensemble'><p>Obtain and format results produced by tuning functions for ensemble objects</p></a></li>
<li><a href='#conf_matrix_df'><p>Make a confusion matrix dataframe for multiple thresholds</p></a></li>
<li><a href='#control_ensemble_grid'><p>Control wrappers</p></a></li>
<li><a href='#dist_pres_vs_bg'><p>Distance between the distribution of climate values for presences vs background</p></a></li>
<li><a href='#explain_tidysdm'><p>Create explainer from your tidysdm ensembles.</p></a></li>
<li><a href='#filter_high_cor'><p>Filter to retain only variables below a given correlation threshold</p></a></li>
<li><a href='#form_resp'><p>Get the response variable from a formula</p></a></li>
<li><a href='#gam_formula'><p>Create a formula for gam</p></a></li>
<li><a href='#geom_split_violin'><p>Split violin geometry for ggplots</p></a></li>
<li><a href='#grid_cellsize'><p>Get default grid cellsize for a given dataset</p></a></li>
<li><a href='#grid_offset'><p>Get default grid cellsize for a given dataset</p></a></li>
<li><a href='#horses'><p>Coordinates of radiocarbon dates for horses</p></a></li>
<li><a href='#kap_max'><p>Maximum Cohen's Kappa</p></a></li>
<li><a href='#km2m'><p>Convert a geographic distance from km to m</p></a></li>
<li><a href='#lacerta'><p>Coordinates of presences for Iberian emerald lizard</p></a></li>
<li><a href='#lacerta_ensemble'><p>A simple ensemble for the lacerta data</p></a></li>
<li><a href='#lacerta_rep_ens'><p>A repeat ensemble for the lacerta data</p></a></li>
<li><a href='#maxent'><p>Maxent model</p></a></li>
<li><a href='#maxent_params'><p>Parameters for maxent models</p></a></li>
<li><a href='#maxnet_fit'><p>Wrapper to fit maxnet models with formulae</p></a></li>
<li><a href='#maxnet_predict'><p>Wrapper to predict maxnet models</p></a></li>
<li><a href='#optim_thresh'><p>Find threshold that optimises a given metric</p></a></li>
<li><a href='#optim_thresh_kap_max'><p>Find threshold that maximises Kappa</p></a></li>
<li><a href='#optim_thresh_sens'><p>Find threshold that gives a target sensitivity</p></a></li>
<li><a href='#optim_thresh_tss_max'><p>Find threshold that maximises TSS</p></a></li>
<li><a href='#out_of_range_warning'><p>Warn if some times are outside the range of time steps from a raster</p></a></li>
<li><a href='#plot_pres_vs_bg'><p>Plot presences vs background</p></a></li>
<li><a href='#predict_raster'><p>Make predictions for a whole raster</p></a></li>
<li><a href='#predict.repeat_ensemble'><p>Predict for a repeat ensemble set</p></a></li>
<li><a href='#predict.simple_ensemble'><p>Predict for a simple ensemble set</p></a></li>
<li><a href='#prob_metrics_sf'><p>Probability metrics for <code>sf</code> objects</p></a></li>
<li><a href='#prob_to_binary'><p>simple function to convert probability to binary classes</p></a></li>
<li><a href='#recipe.sf'><p>Recipe for <code>sf</code> objects</p></a></li>
<li><a href='#repeat_ensemble'><p>Repeat ensemble</p></a></li>
<li><a href='#sample_pseudoabs'><p>Sample pseudo-absence (or background) points for SDM analysis</p></a></li>
<li><a href='#sample_pseudoabs_time'><p>Sample pseudo-absence (or background) points for SDM analysis for points with a time point.</p></a></li>
<li><a href='#sdm_metric_set'><p>Metric set for SDM</p></a></li>
<li><a href='#sdm_spec_boost_tree'><p>Model specification for a Boosted Trees model for SDM</p></a></li>
<li><a href='#sdm_spec_gam'><p>Model specification for a GAM for SDM</p></a></li>
<li><a href='#sdm_spec_glm'><p>Model specification for a GLM for SDM</p></a></li>
<li><a href='#sdm_spec_maxent'><p>Model specification for a MaxEnt for SDM</p></a></li>
<li><a href='#sdm_spec_rand_forest'><p>Model specification for a Random Forest for SDM</p></a></li>
<li><a href='#simple_ensemble'><p>Simple ensemble</p></a></li>
<li><a href='#spatial_initial_split'><p>Simple Training/Test Set Splitting for spatial data</p></a></li>
<li><a href='#thin_by_cell'><p>Thin point dataset to have 1 observation per raster cell</p></a></li>
<li><a href='#thin_by_cell_time'><p>Thin point dataset to have 1 observation per raster cell per time slice</p></a></li>
<li><a href='#thin_by_dist'><p>Thin points dataset based on geographic distance</p></a></li>
<li><a href='#thin_by_dist_time'><p>Thin points dataset based on geographic and temporal distance</p></a></li>
<li><a href='#tidysdm'><p>tidysdm</p></a></li>
<li><a href='#tss'><p>TSS - True Skill Statistics</p></a></li>
<li><a href='#tss_max'><p>Maximum TSS - True Skill Statistics</p></a></li>
<li><a href='#y2d'><p>Convert a time interval from years to days</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Species Distribution Models with Tidymodels</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.4</td>
</tr>
<tr>
<td>Description:</td>
<td>Fit species distribution models (SDMs) using the 'tidymodels' framework, 
  which provides a standardised interface to define models and process their 
  outputs. 'tidysdm' expands 'tidymodels' by providing methods for spatial objects,
  as well as a number of specialised functions to process presences and pseudoabsences
  for contemporary and palaeo datasets. The full 
  functionalities of the package are described
  in Leonardi et al. (2023) &lt;<a href="https://doi.org/10.1101%2F2023.07.24.550358">doi:10.1101/2023.07.24.550358</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/AGPL-3">AGPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-GB</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/EvolEcolGroup/tidysdm">https://github.com/EvolEcolGroup/tidysdm</a>,
<a href="https://evolecolgroup.github.io/tidysdm/">https://evolecolgroup.github.io/tidysdm/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/EvolEcolGroup/tidysdm/issues">https://github.com/EvolEcolGroup/tidysdm/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>tidymodels, spatialsample, R (&ge; 3.50)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dials, DALEX, DALEXtra, dplyr, ggplot2, lubridate, magrittr,
maxnet, parsnip, patchwork, recipes, rsample, rlang (&ge; 1.0.0),
stats, sf, terra, tibble, tune, workflows, workflowsets,
yardstick</td>
</tr>
<tr>
<td>Suggests:</td>
<td>blockCV, data.table, doParallel, earth, kernlab, knitr,
overlapping, pastclim (&ge; 2.0.0), ranger, rmarkdown, spelling,
stacks, testthat (&ge; 3.0.0), tidyterra, xgboost, V8</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-05 20:08:47 UTC; andrea</td>
</tr>
<tr>
<td>Author:</td>
<td>Michela Leonardi [aut],
  Margherita Colucci [aut],
  Andrea Manica [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andrea Manica &lt;am315@cam.ac.uk&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-05 20:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of calling <code>rhs(lhs)</code>.
</p>

<hr>
<h2 id='add_member'>Add best member of workflow to a simple ensemble</h2><span id='topic+add_member'></span><span id='topic+add_member.default'></span><span id='topic+add_member.tune_results'></span><span id='topic+add_member.workflow_set'></span>

<h3>Description</h3>

<p>This function adds member(s) to a <code><a href="#topic+simple_ensemble">simple_ensemble()</a></code> object, taking the
best member from each workflow provided. It is possible to pass individual
<code>tune_results</code> objects from a tuned <code>workflow</code>, or a
<code><a href="workflowsets.html#topic+workflow_set">workflowsets::workflow_set()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_member(x, member, ...)

## Default S3 method:
add_member(x, member, ...)

## S3 method for class 'tune_results'
add_member(x, member, metric = NULL, id = NULL, ...)

## S3 method for class 'workflow_set'
add_member(x, member, metric = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_member_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+simple_ensemble">simple_ensemble</a> to which member(s) will be added</p>
</td></tr>
<tr><td><code id="add_member_+3A_member">member</code></td>
<td>
<p>a  <code>tune_results</code>, or a <code><a href="workflowsets.html#topic+workflow_set">workflowsets::workflow_set</a></code></p>
</td></tr>
<tr><td><code id="add_member_+3A_...">...</code></td>
<td>
<p>not used at the moment.</p>
</td></tr>
<tr><td><code id="add_member_+3A_metric">metric</code></td>
<td>
<p>A character string (or NULL) for which metric to optimize.
If NULL, the first metric is used.</p>
</td></tr>
<tr><td><code id="add_member_+3A_id">id</code></td>
<td>
<p>the name to be given to this workflow in the <code>wflow_id</code> column.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+simple_ensemble">simple_ensemble</a> with additional member(s)
</p>

<hr>
<h2 id='add_repeat'>Add repeat(s) to a repeated ensemble</h2><span id='topic+add_repeat'></span><span id='topic+add_repeat.default'></span><span id='topic+add_repeat.simple_ensemble'></span><span id='topic+add_repeat.list'></span>

<h3>Description</h3>

<p>This function adds repeat(s) to a <code><a href="#topic+repeat_ensemble">repeat_ensemble</a></code> object, where each
repeat is a <code><a href="#topic+simple_ensemble">simple_ensemble</a></code>. All repeats must contain the same members,
selected using the same metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_repeat(x, rep, ...)

## Default S3 method:
add_repeat(x, rep, ...)

## S3 method for class 'simple_ensemble'
add_repeat(x, rep, ...)

## S3 method for class 'list'
add_repeat(x, rep, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_repeat_+3A_x">x</code></td>
<td>
<p>a <a href="#topic+repeat_ensemble">repeat_ensemble</a> to which repeat(s) will be added</p>
</td></tr>
<tr><td><code id="add_repeat_+3A_rep">rep</code></td>
<td>
<p>a repeat, as a single <code><a href="#topic+simple_ensemble">simple_ensemble</a></code>, or a list of
<code><a href="#topic+simple_ensemble">simple_ensemble</a></code> objects</p>
</td></tr>
<tr><td><code id="add_repeat_+3A_...">...</code></td>
<td>
<p>not used at the moment.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+repeat_ensemble">repeat_ensemble</a> with additional repeat(s)
</p>

<hr>
<h2 id='autoplot.simple_ensemble'>Plot the results of a simple ensemble</h2><span id='topic+autoplot.simple_ensemble'></span>

<h3>Description</h3>

<p>This <code>autoplot()</code> method plots performance metrics that have been
ranked using a metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'simple_ensemble'
autoplot(
  object,
  rank_metric = NULL,
  metric = NULL,
  std_errs = stats::qnorm(0.95),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoplot.simple_ensemble_+3A_object">object</code></td>
<td>
<p>A <code><a href="#topic+simple_ensemble">simple_ensemble</a></code> whose elements have results.</p>
</td></tr>
<tr><td><code id="autoplot.simple_ensemble_+3A_rank_metric">rank_metric</code></td>
<td>
<p>A character string for which metric should be used to rank
the results. If none is given, the first metric in the metric set is used
(after filtering by the <code>metric</code> option).</p>
</td></tr>
<tr><td><code id="autoplot.simple_ensemble_+3A_metric">metric</code></td>
<td>
<p>A character vector for which metrics (apart from <code>rank_metric</code>)
to be included in the visualization. If NULL (the default), all available
metrics will be plotted</p>
</td></tr>
<tr><td><code id="autoplot.simple_ensemble_+3A_std_errs">std_errs</code></td>
<td>
<p>The number of standard errors to plot (if the standard error
exists).</p>
</td></tr>
<tr><td><code id="autoplot.simple_ensemble_+3A_...">...</code></td>
<td>
<p>Other options to pass to <code>autoplot()</code>. Currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is intended to produce a default plot to visualize helpful
information across all possible applications of a <code><a href="#topic+simple_ensemble">simple_ensemble</a></code>. More
sophisticated plots can be produced using standard <code>ggplot2</code> code for
plotting.
</p>
<p>The x-axis is the workflow rank in the set (a value of one being the best)
versus the performance metric(s) on the y-axis. With multiple metrics, there
will be facets for each metric, with the <code>rank_metric</code> first (if any was
provided; otherwise the metric used to create the <code><a href="#topic+simple_ensemble">simple_ensemble</a></code> will
be used).
</p>
<p>If multiple resamples are used, confidence bounds are shown for each result
(95% confidence, by default).
</p>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># we use the two_class_example from `workflowsets`
two_class_ens &lt;- simple_ensemble() %&gt;%
  add_member(two_class_res, metric = "roc_auc")
autoplot(two_class_ens)

</code></pre>

<hr>
<h2 id='autoplot.spatial_initial_split'>Create a ggplot for a spatial initial rsplit.</h2><span id='topic+autoplot.spatial_initial_split'></span>

<h3>Description</h3>

<p>This method provides a good visualization method for a spatial
initial rsplit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'spatial_initial_split'
autoplot(object, ..., alpha = 0.6)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoplot.spatial_initial_split_+3A_object">object</code></td>
<td>
<p>A <code>spatial_initial_rsplit</code> object.
Note that only resamples made from
<code>sf</code> objects  create <code>spatial_initial_rsplit</code> objects;
this function will not work for
resamples made with non-spatial tibbles or data.frames.</p>
</td></tr>
<tr><td><code id="autoplot.spatial_initial_split_+3A_...">...</code></td>
<td>
<p>Options passed to <code><a href="ggplot2.html#topic+ggsf">ggplot2::geom_sf()</a></code>.</p>
</td></tr>
<tr><td><code id="autoplot.spatial_initial_split_+3A_alpha">alpha</code></td>
<td>
<p>Opacity, passed to <code><a href="ggplot2.html#topic+ggsf">ggplot2::geom_sf()</a></code>.
Values of alpha range from 0 to 1, with lower values corresponding to more
transparent colors.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This plot method is a wrapper around the standard <code>spatial_rsplit</code> method,
but it re-labels the folds as <em>Testing</em> and <em>Training</em> following the
convention for a standard <code>initial_split</code> object
</p>


<h3>Value</h3>

<p>A ggplot object with each fold assigned a color, made using
<code><a href="ggplot2.html#topic+ggsf">ggplot2::geom_sf()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(123)
block_initial &lt;- spatial_initial_split(boston_canopy,
  prop = 1 / 5, spatial_block_cv
)
autoplot(block_initial)
</code></pre>

<hr>
<h2 id='blockcv2rsample'>Convert an object created with <code>blockCV</code> to an <code>rsample</code> object</h2><span id='topic+blockcv2rsample'></span>

<h3>Description</h3>

<p>This function creates objects created with <code>blockCV</code> to <code>rsample</code> objects
that can be used by <code>tidysdm</code>. BlockCV provides more sophisticated sampling
options than the <code>spatialsample</code> library. For example, it is possible to
stratify the sampling to ensure that presences and absences are evenly
distributed among the folds (see the example below).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blockcv2rsample(x, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blockcv2rsample_+3A_x">x</code></td>
<td>
<p>a object created with a <code>blockCV</code> function</p>
</td></tr>
<tr><td><code id="blockcv2rsample_+3A_data">data</code></td>
<td>
<p>the <code>sf</code> object used to create <code>x</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that currently only objects of type <code>cv_spatial</code> and <code>cv_cluster</code> are
supported.
</p>


<h3>Value</h3>

<p>an <code>rsample</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(blockCV)
points &lt;- read.csv(system.file("extdata/", "species.csv", package = "blockCV"))
pa_data &lt;- sf::st_as_sf(points, coords = c("x", "y"), crs = 7845)
sb1 &lt;- cv_spatial(
  x = pa_data,
  column = "occ", # the response column to balance the folds
  k = 5, # number of folds
  size = 350000, # size of the blocks in metres
  selection = "random", # random blocks-to-fold
  iteration = 10
) # find evenly dispersed folds
sb1_rsample &lt;- blockcv2rsample(sb1, pa_data)
class(sb1_rsample)
autoplot(sb1_rsample)

</code></pre>

<hr>
<h2 id='boyce_cont'>Boyce continuous index (BCI)</h2><span id='topic+boyce_cont'></span><span id='topic+boyce_cont.data.frame'></span><span id='topic+boyce_cont.sf'></span><span id='topic+boyce_cont_vec'></span>

<h3>Description</h3>

<p>This function the Boyce Continuous Index, a measure of model accuracy appropriate
for Species Distribution Models with presence only data (i.e. using pseudoabsences
or background). The algorithm used here comes from the package <code>enmSdm</code>, and uses multiple
overlapping windows.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boyce_cont(data, ...)

## S3 method for class 'data.frame'
boyce_cont(
  data,
  truth,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = "first",
  case_weights = NULL
)

## S3 method for class 'sf'
boyce_cont(data, ...)

boyce_cont_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  event_level = "first",
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boyce_cont_+3A_data">data</code></td>
<td>
<p>Either a data.frame containing the columns specified by the truth
and estimate arguments, or a table/matrix where the true class
results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="boyce_cont_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more dplyr selector functions to choose which variables contain the class probabilities. If truth is binary, only 1 column should be selected, and it should correspond to the value of event_level. Otherwise, there should be as many columns as factor levels of truth and the ordering of the columns should be the same as the factor levels of truth.</p>
</td></tr>
<tr><td><code id="boyce_cont_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results (that is a factor). This should be an unquoted column name although this argument is passed by expression and supports quasiquotation (you can unquote column names). For _vec() functions, a factor vector.</p>
</td></tr>
<tr><td><code id="boyce_cont_+3A_estimator">estimator</code></td>
<td>
<p>One of &quot;binary&quot;, &quot;hand_till&quot;, &quot;macro&quot;, or &quot;macro_weighted&quot; to specify the type of averaging to be done. &quot;binary&quot; is only relevant for the two class case. The others are general methods for calculating multiclass metrics. The default will automatically choose &quot;binary&quot; if truth is binary, &quot;hand_till&quot; if truth has &gt;2 levels and case_weights isn't specified, or &quot;macro&quot; if truth has &gt;2 levels and case_weights is specified (in which case &quot;hand_till&quot; isn't well-defined).</p>
</td></tr>
<tr><td><code id="boyce_cont_+3A_na_rm">na_rm</code></td>
<td>
<p>A logical value indicating whether NA values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="boyce_cont_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either &quot;first&quot; or &quot;second&quot; to specify which level of truth to consider as the &quot;event&quot;. This argument is only applicable when estimator = &quot;binary&quot;. The default uses an internal helper that generally defaults to &quot;first&quot;</p>
</td></tr>
<tr><td><code id="boyce_cont_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This should be an unquoted column name that evaluates to a numeric column in data. For _vec() functions, a numeric vector.</p>
</td></tr>
<tr><td><code id="boyce_cont_+3A_estimate">estimate</code></td>
<td>
<p>If truth is binary, a numeric vector of class probabilities corresponding to the &quot;relevant&quot; class. Otherwise, a matrix with as many columns as factor levels of truth. It is assumed that these are in the same order as the levels of truth.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is no multiclass version of this function, it only operates on binary
predictions (e.g. presences and absences in SDMs).
</p>


<h3>Value</h3>

<p>A tibble with columns .metric, .estimator, and .estimate and 1 row of values.
For grouped data frames, the number of rows returned will be the same as the
number of groups.
</p>


<h3>References</h3>

<p>Boyce, M.S., P.R. Vernier, S.E. Nielsen and F.K.A. Schmiegelow. 2002.
Evaluating resource selection functions. Ecol. Model., 157, 281-300.
</p>
<p>Hirzel, A.H., G. Le Lay, V. Helfer, C. Randin and A. Guisan. 2006.
Evaluating the ability of habitat suitability models to predict
species presences. Ecol. Model., 199, 142-152.
</p>


<h3>See Also</h3>

<p>Other class probability metrics: 
<code><a href="#topic+kap_max">kap_max</a>()</code>,
<code><a href="#topic+tss_max">tss_max</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>boyce_cont(two_class_example, truth, Class1)

</code></pre>

<hr>
<h2 id='calib_class_thresh'>Calibrate class thresholds</h2><span id='topic+calib_class_thresh'></span>

<h3>Description</h3>

<p>Predict for a new dataset by using a simple ensemble. Predictions from individual
models are combined according to <code>fun</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calib_class_thresh(object, class_thresh, metric_thresh = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calib_class_thresh_+3A_object">object</code></td>
<td>
<p>an simple_ensemble object</p>
</td></tr>
<tr><td><code id="calib_class_thresh_+3A_class_thresh">class_thresh</code></td>
<td>
<p>probability threshold used to convert probabilities into
classes. It can be a number (between 0 and 1), or a character metric (currently
&quot;tss_max&quot;, &quot;kap_max&quot; or &quot;sensitivity&quot;). For sensitivity, an additional target value is passed
along as a second element of a vector, e.g. c(&quot;sensitivity&quot;,0.8).</p>
</td></tr>
<tr><td><code id="calib_class_thresh_+3A_metric_thresh">metric_thresh</code></td>
<td>
<p>a vector of length 2 giving a metric and its threshold,
which will be used to prune
which models in the ensemble will be used for the prediction. The 'metrics'
need to have been computed when the workflow was tuned. Examples are
c(&quot;accuracy&quot;,0.8) or c(&quot;boyce_cont&quot;,0.7)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+simple_ensemble">simple_ensemble</a> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test_ens &lt;- simple_ensemble() %&gt;%
  add_member(two_class_res[1:3, ], metric = "roc_auc")
test_ens &lt;- calib_class_thresh(test_ens, class_thresh = "tss_max")
test_ens &lt;- calib_class_thresh(test_ens, class_thresh = "kap_max")
test_ens &lt;- calib_class_thresh(test_ens, class_thresh = c("sens", 0.9))
</code></pre>

<hr>
<h2 id='check_coords_names'>Check that we have a valid pair of coordinate names</h2><span id='topic+check_coords_names'></span>

<h3>Description</h3>

<p>This internal function checks that coords (as passed to functions) is a valid
set of names, or, if NULL, that we have standard variable names in data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_coords_names(data, coords)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_coords_names_+3A_data">data</code></td>
<td>
<p>a data.frame containing the locations.</p>
</td></tr>
<tr><td><code id="check_coords_names_+3A_coords">coords</code></td>
<td>
<p>a vector of length two giving the names of the &quot;x&quot; and &quot;y&quot;
coordinates, of points is a data.frame and does not use standard names.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of length 2 with the valid names, in the correct order
</p>

<hr>
<h2 id='check_sdm_presence'>Check that the column with presences is correctly formatted</h2><span id='topic+check_sdm_presence'></span>

<h3>Description</h3>

<p>In <code>tidysdm</code>, the string defining presences should be the first level of
the response factor. This function checks that the column is correctly formatted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_sdm_presence(.data, .col, presence_level = "presence")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_sdm_presence_+3A_.data">.data</code></td>
<td>
<p>a <code>data.frame</code> or <code>tibble</code>, or a derived object such as an <code>sf</code> data.frame</p>
</td></tr>
<tr><td><code id="check_sdm_presence_+3A_.col">.col</code></td>
<td>
<p>the column containing the presences</p>
</td></tr>
<tr><td><code id="check_sdm_presence_+3A_presence_level">presence_level</code></td>
<td>
<p>the string used to define the presence level of <code>.col</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if correctly formatted
</p>

<hr>
<h2 id='check_splits_balance'>Check the balance of presences vs pseudoabsences among splits</h2><span id='topic+check_splits_balance'></span>

<h3>Description</h3>

<p>Check the balance of presences vs pseudoabsences among splits
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_splits_balance(splits, .col)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_splits_balance_+3A_splits">splits</code></td>
<td>
<p>the data splits (an <code>rset</code> or <code>split</code> object), generated by a function such as
<code><a href="spatialsample.html#topic+spatial_block_cv">spatialsample::spatial_block_cv()</a></code></p>
</td></tr>
<tr><td><code id="check_splits_balance_+3A_.col">.col</code></td>
<td>
<p>the column containing the presences</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble of the number of presences and pseudoabsences in the assessment
and analysis
set of each split (or training and testing in an initial split)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lacerta_thin &lt;- readRDS(system.file("extdata/lacerta_climate_sf.RDS",
  package = "tidysdm"
))
lacerta_cv &lt;- spatial_block_cv(lacerta_thin, v = 5)
check_splits_balance(lacerta_cv, class)

</code></pre>

<hr>
<h2 id='collect_metrics.simple_ensemble'>Obtain and format results produced by tuning functions for ensemble objects</h2><span id='topic+collect_metrics.simple_ensemble'></span><span id='topic+collect_metrics.repeat_ensemble'></span>

<h3>Description</h3>

<p>Return a tibble of performance metrics for all models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'simple_ensemble'
collect_metrics(x, ...)

## S3 method for class 'repeat_ensemble'
collect_metrics(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collect_metrics.simple_ensemble_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+simple_ensemble">simple_ensemble</a></code> or <code><a href="#topic+repeat_ensemble">repeat_ensemble</a></code> object</p>
</td></tr>
<tr><td><code id="collect_metrics.simple_ensemble_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When applied to a ensemble, the metrics that are returned
do not contain the actual tuning parameter columns and values (unlike when
these collect functions are run on other objects). The reason is that ensembles
contain different types of models or models with different tuning
parameters.
</p>


<h3>Value</h3>

<p>A tibble.
</p>


<h3>See Also</h3>

<p><code><a href="tune.html#topic+collect_predictions">tune::collect_metrics()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>collect_metrics(lacerta_ensemble)
collect_metrics(lacerta_rep_ens)
</code></pre>

<hr>
<h2 id='conf_matrix_df'>Make a confusion matrix dataframe for multiple thresholds</h2><span id='topic+conf_matrix_df'></span>

<h3>Description</h3>

<p>Create the confusion matrix for multiple thresholds, using to optimise tss
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conf_matrix_df(presences, absences)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conf_matrix_df_+3A_presences">presences</code></td>
<td>
<p>Probabilities for presences</p>
</td></tr>
<tr><td><code id="conf_matrix_df_+3A_absences">absences</code></td>
<td>
<p>probabilities for absences</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame of thresholds with columns <em>thres</em>, <em>tp</em>, <em>fp</em>, <em>fn</em>, and <em>tn</em>
</p>

<hr>
<h2 id='control_ensemble_grid'>Control wrappers</h2><span id='topic+control_ensemble_grid'></span><span id='topic+control_ensemble_resamples'></span><span id='topic+control_ensemble_bayes'></span>

<h3>Description</h3>

<p>Supply these light wrappers as the <code>control</code> argument in a
<code><a href="tune.html#topic+tune_grid">tune::tune_grid()</a></code>, <code><a href="tune.html#topic+tune_bayes">tune::tune_bayes()</a></code>, or <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code>
call to return the needed elements for use in an ensemble.
These functions will return the appropriate control grid to ensure that
assessment set predictions and information on model specifications and
preprocessors, is supplied in the resampling results object!
</p>
<p>To integrate ensemble settings with your existing control settings, note
that these functions just call the appropriate <code style="white-space: pre;">&#8288;tune::control_*&#8288;</code> function
with the arguments <code style="white-space: pre;">&#8288;save_pred = TRUE, save_workflow = TRUE&#8288;</code>.
</p>
<p>These wrappers are equivalent to the ones used in the <code>stacks</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_ensemble_grid()

control_ensemble_resamples()

control_ensemble_bayes()
</code></pre>


<h3>Value</h3>

<p>A <a href="tune.html#topic+control_grid">tune::control_grid</a>, <a href="tune.html#topic+control_bayes">tune::control_bayes</a>,
or <a href="tune.html#topic+control_grid">tune::control_resamples</a> object.
</p>


<h3>See Also</h3>

<p>See the vignettes for examples of these functions used in context.
</p>

<hr>
<h2 id='dist_pres_vs_bg'>Distance between the distribution of climate values for presences vs background</h2><span id='topic+dist_pres_vs_bg'></span>

<h3>Description</h3>

<p>For each environmental variable, this function computes the density functions
of presences and absences and returns (1-overlap), which is a measure of the
distance between the two distributions. Variables with a high distance are good
candidates for SDMs, as species occurrences are confined to a subset
of the available background.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist_pres_vs_bg(.data, .col)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist_pres_vs_bg_+3A_.data">.data</code></td>
<td>
<p>a <code>data.frame</code> (or derived object, such as <code>tibble</code>, or
<code>sf</code>) with values for the bioclimate variables for presences and background</p>
</td></tr>
<tr><td><code id="dist_pres_vs_bg_+3A_.col">.col</code></td>
<td>
<p>the column containing the presences; it assumes presences to be
the first level of this factor</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a name vector of distances
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This should be updated to use a dataset from tidysdm
data("bradypus", package = "maxnet")
bradypus_tb &lt;- tibble::as_tibble(bradypus) %&gt;%
  dplyr::mutate(presence = relevel(
    factor(
      dplyr::case_match(presence, 1 ~ "presence", 0 ~ "absence")
    ),
    ref = "presence"
  )) %&gt;%
  select(-ecoreg)

bradypus_tb %&gt;% dist_pres_vs_bg(presence)

</code></pre>

<hr>
<h2 id='explain_tidysdm'>Create explainer from your tidysdm ensembles.</h2><span id='topic+explain_tidysdm'></span><span id='topic+explain_tidysdm.default'></span><span id='topic+explain_tidysdm.simple_ensemble'></span><span id='topic+explain_tidysdm.repeat_ensemble'></span>

<h3>Description</h3>

<p>DALEX is designed to explore and explain the behaviour of Machine Learning
methods. This function creates a DALEX explainer (see <code><a href="DALEX.html#topic+explain">DALEX::explain()</a></code>), which can then be queried
by multiple function to create explanations of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>explain_tidysdm(
  model,
  data,
  y,
  predict_function,
  predict_function_target_column,
  residual_function,
  ...,
  label,
  verbose,
  precalculate,
  colorize,
  model_info,
  type,
  by_workflow
)

## Default S3 method:
explain_tidysdm(
  model,
  data = NULL,
  y = NULL,
  predict_function = NULL,
  predict_function_target_column = NULL,
  residual_function = NULL,
  ...,
  label = NULL,
  verbose = TRUE,
  precalculate = TRUE,
  colorize = !isTRUE(getOption("knitr.in.progress")),
  model_info = NULL,
  type = "classification",
  by_workflow = FALSE
)

## S3 method for class 'simple_ensemble'
explain_tidysdm(
  model,
  data = NULL,
  y = NULL,
  predict_function = NULL,
  predict_function_target_column = NULL,
  residual_function = NULL,
  ...,
  label = NULL,
  verbose = TRUE,
  precalculate = TRUE,
  colorize = !isTRUE(getOption("knitr.in.progress")),
  model_info = NULL,
  type = "classification",
  by_workflow = FALSE
)

## S3 method for class 'repeat_ensemble'
explain_tidysdm(
  model,
  data = NULL,
  y = NULL,
  predict_function = NULL,
  predict_function_target_column = NULL,
  residual_function = NULL,
  ...,
  label = NULL,
  verbose = TRUE,
  precalculate = TRUE,
  colorize = !isTRUE(getOption("knitr.in.progress")),
  model_info = NULL,
  type = "classification",
  by_workflow = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="explain_tidysdm_+3A_model">model</code></td>
<td>
<p>object - a model to be explained</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_data">data</code></td>
<td>
<p>data.frame or matrix - data which will be used to calculate the explanations. If not provided, then it will be extracted from the model. Data should be passed without a target column (this shall be provided as the <code>y</code> argument). NOTE: If the target variable is present in the <code>data</code>, some of the functionalities may not work properly.</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_y">y</code></td>
<td>
<p>numeric vector with outputs/scores. If provided, then it shall have the same size as <code>data</code></p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_predict_function">predict_function</code></td>
<td>
<p>function that takes two arguments: model and new data and returns a numeric vector with predictions.   By default it is <code>yhat</code>.</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_predict_function_target_column">predict_function_target_column</code></td>
<td>
<p>Character or numeric containing either column name or column number in the model prediction object of the class that should be considered as positive (i.e. the class that is associated with probability 1). If NULL, the second column of the output will be taken for binary classification. For a multiclass classification setting, that parameter cause switch to binary classification mode with one vs others probabilities.</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_residual_function">residual_function</code></td>
<td>
<p>function that takes four arguments: model, data, target vector y and predict function (optionally). It should return a numeric vector with model residuals for given data. If not provided, response residuals (<code class="reqn">y-\hat{y}</code>) are calculated. By default it is <code>residual_function_default</code>.</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_...">...</code></td>
<td>
<p>other parameters</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_label">label</code></td>
<td>
<p>character - the name of the model. By default it's extracted from the 'class' attribute of the model</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_verbose">verbose</code></td>
<td>
<p>logical. If TRUE (default) then diagnostic messages will be printed</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_precalculate">precalculate</code></td>
<td>
<p>logical. If TRUE (default) then <code>predicted_values</code> and <code>residual</code> are calculated when explainer is created.
This will happen also if <code>verbose</code> is TRUE. Set both <code>verbose</code> and <code>precalculate</code> to FALSE to omit calculations.</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_colorize">colorize</code></td>
<td>
<p>logical. If TRUE (default) then <code>WARNINGS</code>, <code>ERRORS</code> and <code>NOTES</code> are colorized. Will work only in the R console. Now by default it is <code>FALSE</code> while knitting and <code>TRUE</code> otherwise.</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_model_info">model_info</code></td>
<td>
<p>a named list (<code>package</code>, <code>version</code>, <code>type</code>) containing information about model. If <code>NULL</code>, <code>DALEX</code> will seek for information on it's own.</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_type">type</code></td>
<td>
<p>type of a model, either <code>classification</code> or <code>regression</code>. If not specified then <code>type</code> will be extracted from <code>model_info</code>.</p>
</td></tr>
<tr><td><code id="explain_tidysdm_+3A_by_workflow">by_workflow</code></td>
<td>
<p>boolean determining whether a list of explainer, one per model,
should be returned instead of a single explainer for the ensemble</p>
</td></tr>
</table>


<h3>Value</h3>

<p>explainer object <code><a href="DALEX.html#topic+explain">DALEX::explain</a></code> ready to work with DALEX
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# using the whole ensemble
lacerta_explainer &lt;- explain_tidysdm(tidysdm::lacerta_ensemble)
# by workflow
explainer_list &lt;- explain_tidysdm(tidysdm::lacerta_ensemble,
  by_workflow = TRUE
)

</code></pre>

<hr>
<h2 id='filter_high_cor'>Filter to retain only variables below a given correlation threshold</h2><span id='topic+filter_high_cor'></span><span id='topic+filter_high_cor.default'></span><span id='topic+filter_high_cor.SpatRaster'></span><span id='topic+filter_high_cor.data.frame'></span><span id='topic+filter_high_cor.matrix'></span><span id='topic+filter_high_cor_algorithm'></span>

<h3>Description</h3>

<p>This method finds a subset of variable such that all have a correlation
below a certain cutoff. There are methods for <code><a href="terra.html#topic+SpatRaster-class">terra::SpatRaster</a></code>,
<code><a href="base.html#topic+data.frame">data.frame</a></code>, and to work directly on a correlation matrix that was
previously estimated. For <code>data.frame</code>, only numeric variables will be
considered.
The algorithm is based on <code>caret::findCorrelation</code>, using the <code>exact</code> option.
The absolute values of pair-wise correlations are considered. If two
variables have a high correlation, the function looks at the mean absolute
correlation of each variable and removes the variable with the largest mean
absolute correlation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE, to_keep = NULL)

## Default S3 method:
filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE, to_keep = NULL)

## S3 method for class 'SpatRaster'
filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE, to_keep = NULL)

## S3 method for class 'data.frame'
filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE, to_keep = NULL)

## S3 method for class 'matrix'
filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE, to_keep = NULL)

filter_high_cor_algorithm(x, cutoff = 0.7, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_high_cor_+3A_x">x</code></td>
<td>
<p>A <code><a href="terra.html#topic+SpatRaster-class">terra::SpatRaster</a></code> object, a data.frame (with only numeric
variables), or a correlation matrix</p>
</td></tr>
<tr><td><code id="filter_high_cor_+3A_cutoff">cutoff</code></td>
<td>
<p>A numeric value for the pair-wise absolute correlation cutoff</p>
</td></tr>
<tr><td><code id="filter_high_cor_+3A_verbose">verbose</code></td>
<td>
<p>A boolean for printing the details</p>
</td></tr>
<tr><td><code id="filter_high_cor_+3A_names">names</code></td>
<td>
<p>a logical; should the column names be returned <code>TRUE</code> or
the column index <code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="filter_high_cor_+3A_to_keep">to_keep</code></td>
<td>
<p>A vector of variable names that we want to force in the set
(note that the function will return an error if the correlation among any of
those variables is higher than the cutoff).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are several function in the package <code>subselect</code>
that can also be used to accomplish
the same goal but tend to retain more predictors.
</p>


<h3>Value</h3>

<p>A vector of names of columns that are below the correlation threshold
(when <code>names = TRUE</code>), otherwise a vector of indices. Note that the indices
are only for numeric variables (i.e. if factors are present, the indices do
not take them into account).
</p>

<hr>
<h2 id='form_resp'>Get the response variable from a formula</h2><span id='topic+form_resp'></span>

<h3>Description</h3>

<p>This is the counterpart of <a href="rsample.html#topic+form_pred">rsample::form_pred</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_resp(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_resp_+3A_x">x</code></td>
<td>
<p>a formula</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: this might not behave well with functions such as log(y). But neither does form_pred
</p>
<p>modified from
https://stackoverflow.com/questions/13217322/how-to-reliably-get-dependent-variable-name-from-formula-object
</p>


<h3>Value</h3>

<p>character the name of the response
</p>

<hr>
<h2 id='gam_formula'>Create a formula for gam</h2><span id='topic+gam_formula'></span>

<h3>Description</h3>

<p>This function takes the formula from a recipe, and turns numeric predictors
into smooths with a given k. This formula can be passed to a workflow or
workflow set when fitting a gam.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam_formula(object, k = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gam_formula_+3A_object">object</code></td>
<td>
<p>a <a href="recipes.html#topic+recipe">recipes::recipe</a>, already trained</p>
</td></tr>
<tr><td><code id="gam_formula_+3A_k">k</code></td>
<td>
<p>the <em>k</em> value for the smooth</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a formula
</p>

<hr>
<h2 id='geom_split_violin'>Split violin geometry for ggplots</h2><span id='topic+geom_split_violin'></span>

<h3>Description</h3>

<p>This geometry displays the density distribution of two groups side by side,
as two halves of a violin. Note that an empty<code>x</code> aesthetic has to be provided even
if you want to plot a single variable (see example below).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>geom_split_violin(
  mapping = NULL,
  data = NULL,
  stat = "ydensity",
  position = "identity",
  nudge = 0,
  ...,
  draw_quantiles = NULL,
  trim = TRUE,
  scale = "area",
  na.rm = FALSE,
  show.legend = NA,
  inherit.aes = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="geom_split_violin_+3A_mapping">mapping</code></td>
<td>
<p>Set of aesthetic mappings created by <code><a href="ggplot2.html#topic+aes">aes()</a></code>. If specified and
<code>inherit.aes = TRUE</code> (the default), it is combined with the default mapping
at the top level of the plot. You must supply <code>mapping</code> if there is no plot
mapping.</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_data">data</code></td>
<td>
<p>The data to be displayed in this layer. There are three
options:
</p>
<p>If <code>NULL</code>, the default, the data is inherited from the plot
data as specified in the call to <code><a href="ggplot2.html#topic+ggplot">ggplot()</a></code>.
</p>
<p>A <code>data.frame</code>, or other object, will override the plot
data. All objects will be fortified to produce a data frame. See
<code><a href="ggplot2.html#topic+fortify">fortify()</a></code> for which variables will be created.
</p>
<p>A <code>function</code> will be called with a single argument,
the plot data. The return value must be a <code>data.frame</code>, and
will be used as the layer data. A <code>function</code> can be created
from a <code>formula</code> (e.g. <code>~ head(.x, 10)</code>).</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_stat">stat</code></td>
<td>
<p>Use to override the default connection between <code><a href="ggplot2.html#topic+geom_violin">geom_violin()</a></code>
and <code><a href="ggplot2.html#topic+stat_ydensity">stat_ydensity()</a></code>.</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_position">position</code></td>
<td>
<p>Position adjustment, either as a string naming the adjustment
(e.g. <code>"jitter"</code> to use <code>position_jitter</code>), or the result of a call to a
position adjustment function. Use the latter if you need to change the
settings of the adjustment.</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_nudge">nudge</code></td>
<td>
<p>Add space between the half-violin and the middle of the space
allotted to a given factor on the x-axis.</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_...">...</code></td>
<td>
<p>Other arguments passed on to <code><a href="ggplot2.html#topic+layer">layer()</a></code>. These are
often aesthetics, used to set an aesthetic to a fixed value, like
<code>colour = "red"</code> or <code>size = 3</code>. They may also be parameters
to the paired geom/stat.</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_draw_quantiles">draw_quantiles</code></td>
<td>
<p>If <code>not(NULL)</code> (default), draw horizontal lines
at the given quantiles of the density estimate.</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_trim">trim</code></td>
<td>
<p>If <code>TRUE</code> (default), trim the tails of the violins
to the range of the data. If <code>FALSE</code>, don't trim the tails.</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_scale">scale</code></td>
<td>
<p>if &quot;area&quot; (default), all violins have the same area (before trimming
the tails). If &quot;count&quot;, areas are scaled proportionally to the number of
observations. If &quot;width&quot;, all violins have the same maximum width.</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_na.rm">na.rm</code></td>
<td>
<p>If <code>FALSE</code>, the default, missing values are removed with
a warning. If <code>TRUE</code>, missing values are silently removed.</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_show.legend">show.legend</code></td>
<td>
<p>logical. Should this layer be included in the legends?
<code>NA</code>, the default, includes if any aesthetics are mapped.
<code>FALSE</code> never includes, and <code>TRUE</code> always includes.
It can also be a named logical vector to finely select the aesthetics to
display.</p>
</td></tr>
<tr><td><code id="geom_split_violin_+3A_inherit.aes">inherit.aes</code></td>
<td>
<p>If <code>FALSE</code>, overrides the default aesthetics,
rather than combining with them. This is most useful for helper functions
that define both data and aesthetics and shouldn't inherit behaviour from
the default plot specification, e.g. <code><a href="ggplot2.html#topic+borders">borders()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The implementation is based on https://stackoverflow.com/questions/35717353/split-violin-plot-with-ggplot2.
Credit goes to @jan-jlx for providing a complete implementation on StackOverflow, and to
Trang Q. Nguyen for adding the nudge parameter.
</p>


<h3>Value</h3>

<p>a <code><a href="ggplot2.html#topic+layer">ggplot2::layer</a></code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("bradypus", package = "maxnet")
bradypus_tb &lt;- tibble::as_tibble(bradypus) %&gt;% dplyr::mutate(presence = relevel(
  factor(
    dplyr::case_match(presence, 1 ~ "presence", 0 ~ "absence")
  ),
  ref = "presence"
))

ggplot(bradypus_tb, aes(
  x = "",
  y = cld6190_ann,
  fill = presence
)) +
  geom_split_violin(nudge = 0.01)

</code></pre>

<hr>
<h2 id='grid_cellsize'>Get default grid cellsize for a given dataset</h2><span id='topic+grid_cellsize'></span>

<h3>Description</h3>

<p>This function facilitates using <a href="spatialsample.html#topic+spatial_block_cv">spatialsample::spatial_block_cv</a> multiple
times in an analysis. <a href="spatialsample.html#topic+spatial_block_cv">spatialsample::spatial_block_cv</a> creates a grid
based on the object in <code>data</code>. However, if spatial blocks are generated
multiple times in an analysis (e.g. for a <code><a href="#topic+spatial_initial_split">spatial_initial_split()</a></code>, and then
subsequently for cross-validation on the training dataset), it might be desirable to keep the
same grid). By applying this function to the largest dataset, usually the
full dataset before <code><a href="#topic+spatial_initial_split">spatial_initial_split()</a></code>. The resulting cellsize can
be used as an option in <a href="spatialsample.html#topic+spatial_block_cv">spatialsample::spatial_block_cv</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grid_cellsize(data, n = c(10, 10))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grid_cellsize_+3A_data">data</code></td>
<td>
<p>a <a href="sf.html#topic+sf">sf::sf</a> dataset used to size the grid</p>
</td></tr>
<tr><td><code id="grid_cellsize_+3A_n">n</code></td>
<td>
<p>the number of cells in the grid, defaults to c(10,10), which is also
the default for <code><a href="sf.html#topic+st_make_grid">sf::st_make_grid()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the cell size
</p>

<hr>
<h2 id='grid_offset'>Get default grid cellsize for a given dataset</h2><span id='topic+grid_offset'></span>

<h3>Description</h3>

<p>This function facilitates using <a href="spatialsample.html#topic+spatial_block_cv">spatialsample::spatial_block_cv</a> multiple
times in an analysis. <a href="spatialsample.html#topic+spatial_block_cv">spatialsample::spatial_block_cv</a> creates a grid
based on the object in <code>data</code>. However, if spatial blocks are generated
multiple times in an analysis (e.g. for a <code><a href="#topic+spatial_initial_split">spatial_initial_split()</a></code>, and then
subsequently for cross-validation on the training dataset), it might be desirable to keep the
same grid). By applying this function to the largest dataset, usually the
full dataset before <code><a href="#topic+spatial_initial_split">spatial_initial_split()</a></code>. The resulting cellsize can
be used as an option in <a href="spatialsample.html#topic+spatial_block_cv">spatialsample::spatial_block_cv</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grid_offset(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grid_offset_+3A_data">data</code></td>
<td>
<p>a <a href="sf.html#topic+sf">sf::sf</a> dataset used to size the grid</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the grid offset
</p>

<hr>
<h2 id='horses'>Coordinates of radiocarbon dates for horses</h2><span id='topic+horses'></span>

<h3>Description</h3>

<p>Coordinates for presences of horses from 22k to 8k YBP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>horses
</code></pre>


<h3>Format</h3>

<p>An <code>tibble</code> with 1,297 rows and 3 variables:
</p>

<dl>
<dt>latitude</dt><dd><p>latitudes in degrees</p>
</dd>
<dt>longitude</dt><dd><p>longitudes in degrees</p>
</dd>
<dt>time_bp</dt><dd><p>time in years before present</p>
</dd>
</dl>


<hr>
<h2 id='kap_max'>Maximum Cohen's Kappa</h2><span id='topic+kap_max'></span><span id='topic+kap_max.data.frame'></span><span id='topic+kap_max.sf'></span><span id='topic+kap_max_vec'></span>

<h3>Description</h3>

<p>Cohen's Kappa (<code><a href="yardstick.html#topic+kap">yardstick::kap()</a></code>) is a measure similar to <code><a href="yardstick.html#topic+accuracy">yardstick::accuracy()</a></code>, but it normalises
the observed accuracy by the value that would be expected by chance (this
helps for unbalanced cases when one class is predominant).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kap_max(data, ...)

## S3 method for class 'data.frame'
kap_max(
  data,
  truth,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = "first",
  case_weights = NULL
)

## S3 method for class 'sf'
kap_max(data, ...)

kap_max_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  event_level = "first",
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kap_max_+3A_data">data</code></td>
<td>
<p>Either a data.frame containing the columns specified by the truth
and estimate arguments, or a table/matrix where the true class
results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="kap_max_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more dplyr selector functions to choose which variables contain the class probabilities. If truth is binary, only 1 column should be selected, and it should correspond to the value of event_level. Otherwise, there should be as many columns as factor levels of truth and the ordering of the columns should be the same as the factor levels of truth.</p>
</td></tr>
<tr><td><code id="kap_max_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results (that is a factor). This should be an unquoted column name although this argument is passed by expression and supports quasiquotation (you can unquote column names). For _vec() functions, a factor vector.</p>
</td></tr>
<tr><td><code id="kap_max_+3A_estimator">estimator</code></td>
<td>
<p>One of &quot;binary&quot;, &quot;hand_till&quot;, &quot;macro&quot;, or &quot;macro_weighted&quot; to specify the type of averaging to be done. &quot;binary&quot; is only relevant for the two class case. The others are general methods for calculating multiclass metrics. The default will automatically choose &quot;binary&quot; if truth is binary, &quot;hand_till&quot; if truth has &gt;2 levels and case_weights isn't specified, or &quot;macro&quot; if truth has &gt;2 levels and case_weights is specified (in which case &quot;hand_till&quot; isn't well-defined).</p>
</td></tr>
<tr><td><code id="kap_max_+3A_na_rm">na_rm</code></td>
<td>
<p>A logical value indicating whether NA values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="kap_max_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either &quot;first&quot; or &quot;second&quot; to specify which level of truth to consider as the &quot;event&quot;. This argument is only applicable when estimator = &quot;binary&quot;. The default uses an internal helper that generally defaults to &quot;first&quot;</p>
</td></tr>
<tr><td><code id="kap_max_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This should be an unquoted column name that evaluates to a numeric column in data. For _vec() functions, a numeric vector.</p>
</td></tr>
<tr><td><code id="kap_max_+3A_estimate">estimate</code></td>
<td>
<p>If truth is binary, a numeric vector of class probabilities corresponding to the &quot;relevant&quot; class. Otherwise, a matrix with as many columns as factor levels of truth. It is assumed that these are in the same order as the levels of truth.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calibrates the probability threshold to classify presences to maximises kappa.
</p>
<p>There is no multiclass version of this function, it only operates on binary
predictions (e.g. presences and absences in SDMs).
</p>


<h3>Value</h3>

<p>A tibble with columns .metric, .estimator, and .estimate and 1 row of values.
For grouped data frames, the number of rows returned will be the same as the
number of groups.
</p>


<h3>References</h3>

<p>Cohen, J. (1960). &quot;A coefficient of agreement for nominal
scales&quot;. <em>Educational and Psychological Measurement</em>. 20 (1): 37-46.
</p>
<p>Cohen, J. (1968). &quot;Weighted kappa: Nominal scale agreement provision for
scaled disagreement or partial credit&quot;. <em>Psychological
Bulletin</em>. 70 (4): 213-220.
</p>


<h3>See Also</h3>

<p>Other class probability metrics: 
<code><a href="#topic+boyce_cont">boyce_cont</a>()</code>,
<code><a href="#topic+tss_max">tss_max</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kap_max(two_class_example, truth, Class1)

</code></pre>

<hr>
<h2 id='km2m'>Convert a geographic distance from km to m</h2><span id='topic+km2m'></span>

<h3>Description</h3>

<p>This function takes distance in km and converts it into meters, the
units generally used by geographic operations in <code>R</code>. This is a trivial
conversion, but this functions ensures that no zeroes are lost along the way!
</p>


<h3>Usage</h3>

<pre><code class='language-R'>km2m(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="km2m_+3A_x">x</code></td>
<td>
<p>the number of km</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the number of meters
</p>


<h3>Examples</h3>

<pre><code class='language-R'>km2m(10000)
km2m(1)

</code></pre>

<hr>
<h2 id='lacerta'>Coordinates of presences for Iberian emerald lizard</h2><span id='topic+lacerta'></span>

<h3>Description</h3>

<p>Coordinates for presences of <em>Lacerta schreiberi</em>. The variables are as
follows:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lacerta
</code></pre>


<h3>Format</h3>

<p>An <code>tibble</code> with 1,297 rows and 3 variables:
</p>

<dl>
<dt>ID</dt><dd><p>ids from GBIF</p>
</dd>
<dt>latitude</dt><dd><p>latitudes in degrees</p>
</dd>
<dt>longitude</dt><dd><p>longitudes in degrees</p>
</dd>
</dl>


<hr>
<h2 id='lacerta_ensemble'>A simple ensemble for the lacerta data</h2><span id='topic+lacerta_ensemble'></span>

<h3>Description</h3>

<p>Ensemble SDM for <em>Lacerta schreiberi</em>, as generated in the vignette.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lacerta_ensemble
</code></pre>


<h3>Format</h3>

<p>A <code><a href="#topic+simple_ensemble">simple_ensemble</a></code> object
</p>

<hr>
<h2 id='lacerta_rep_ens'>A repeat ensemble for the lacerta data</h2><span id='topic+lacerta_rep_ens'></span>

<h3>Description</h3>

<p>Ensemble SDM for <em>Lacerta schreiberi</em>, as generated in the vignette.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lacerta_rep_ens
</code></pre>


<h3>Format</h3>

<p>A <code><a href="#topic+repeat_ensemble">repeat_ensemble</a></code> object
</p>

<hr>
<h2 id='maxent'>Maxent model</h2><span id='topic+maxent'></span>

<h3>Description</h3>

<p><a href="#topic+maxent">maxent</a> defines a MaxEnt model for binary outcomes as used in Species
Distribution Models.
A good guide to how options of a Maxent model work can be found in
https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2013.07872.x
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxent(
  mode = "classification",
  engine = "maxnet",
  feature_classes = NULL,
  regularization_multiplier = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxent_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model. The only
possible value for this model is &quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="maxent_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting. Currently only &quot;maxnet&quot; is available.</p>
</td></tr>
<tr><td><code id="maxent_+3A_feature_classes">feature_classes</code></td>
<td>
<p>character, continuous feature classes desired, either
&quot;default&quot; or any subset of &quot;lqpht&quot; (for example, &quot;lh&quot;)</p>
</td></tr>
<tr><td><code id="maxent_+3A_regularization_multiplier">regularization_multiplier</code></td>
<td>
<p>numeric, a constant to adjust regularization</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="parsnip.html#topic+model_spec">model_spec</a></code> for a <code>maxent</code> model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# format the data
data("bradypus", package = "maxnet")
bradypus_tb &lt;- tibble::as_tibble(bradypus) %&gt;%
  dplyr::mutate(presence = relevel(
    factor(
      dplyr::case_match(presence, 1 ~ "presence", 0 ~ "absence")
    ),
    ref = "presence"
  )) %&gt;%
  select(-ecoreg)

# fit the model, and make some predictions
maxent_spec &lt;- maxent(feature_classes = "lq")
maxent_fitted &lt;- maxent_spec %&gt;%
  fit(presence ~ ., data = bradypus_tb)
pred_prob &lt;- predict(maxent_fitted, new_data = bradypus[, -1], type = "prob")
pred_class &lt;- predict(maxent_fitted, new_data = bradypus[, -1], type = "class")

# Now with tuning
maxent_spec &lt;- maxent(
  regularization_multiplier = tune(),
  feature_classes = tune()
)
set.seed(452)
cv &lt;- vfold_cv(bradypus_tb, v = 2)
maxent_tune_res &lt;- maxent_spec %&gt;%
  tune_grid(presence ~ ., cv, grid = 3)
show_best(maxent_tune_res, metric = "roc_auc")

</code></pre>

<hr>
<h2 id='maxent_params'>Parameters for maxent models</h2><span id='topic+maxent_params'></span><span id='topic+regularization_multiplier'></span><span id='topic+feature_classes'></span>

<h3>Description</h3>

<p>These parameters are auxiliary to MaxEnt models using the &quot;maxnet&quot; engine.
These functions are used by the tuning functions, and the user will rarely
access them directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regularization_multiplier(range = c(0.5, 3), trans = NULL)

feature_classes(values = c("l", "lq", "lqp", "lqph", "lqpht"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxent_params_+3A_range">range</code></td>
<td>
<p>A two-element vector holding the defaults for the smallest and
largest possible values, respectively. If a transformation is specified,
these values should be in the transformed units.</p>
</td></tr>
<tr><td><code id="maxent_params_+3A_trans">trans</code></td>
<td>
<p>A trans object from the scales package, such as scales::log10_trans()
or scales::reciprocal_trans(). If not provided, the default is used which
matches the units used in range. If no transformation, NULL.</p>
</td></tr>
<tr><td><code id="maxent_params_+3A_values">values</code></td>
<td>
<p>For <code>feature_classes()</code>, a character string of
any subset of &quot;lqpht&quot; (for example, &quot;lh&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>param</code> object that can be used for tuning.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>regularization_multiplier()
feature_classes()
</code></pre>

<hr>
<h2 id='maxnet_fit'>Wrapper to fit maxnet models with formulae</h2><span id='topic+maxnet_fit'></span>

<h3>Description</h3>

<p>This function is a wrapper around <a href="maxnet.html#topic+maxnet">maxnet::maxnet</a>, which takes a formula with data
as well exposing parameters for normalisation in a manner compatible with
<code>parsnip</code>. Users are unlikely to use this function directly.  For the
<code>parsnip</code> model specification for MaxEnt, see <code><a href="#topic+maxent">maxent()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxnet_fit(
  formula,
  data,
  regmult = 1,
  classes = "default",
  regfun = maxnet::maxnet.default.regularization,
  addsamplestobackground = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxnet_fit_+3A_formula">formula</code></td>
<td>
<p>a formula defining the outcome and the predictors</p>
</td></tr>
<tr><td><code id="maxnet_fit_+3A_data">data</code></td>
<td>
<p>a data.frame with the outcomes and predictors</p>
</td></tr>
<tr><td><code id="maxnet_fit_+3A_regmult">regmult</code></td>
<td>
<p>numeric, a constant to adjust regularization</p>
</td></tr>
<tr><td><code id="maxnet_fit_+3A_classes">classes</code></td>
<td>
<p>character, continuous feature classes desired, either
&quot;default&quot; or any subset of &quot;lqpht&quot; (for example, &quot;lh&quot;)</p>
</td></tr>
<tr><td><code id="maxnet_fit_+3A_regfun">regfun</code></td>
<td>
<p>function, computes regularization constant for each feature</p>
</td></tr>
<tr><td><code id="maxnet_fit_+3A_addsamplestobackground">addsamplestobackground</code></td>
<td>
<p>logical, if TRUE then add to the background any
presence sample that is not already there</p>
</td></tr>
<tr><td><code id="maxnet_fit_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The response needs to be a factor with the class representing presences
as the reference level of the factor (as expected by other
classification models).
A good guide to how options of a Maxent model work can be found in
https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2013.07872.x
</p>


<h3>Value</h3>

<p>Maxnet returns an object of class <code>maxnet</code>, which is a list
consisting of a glmnet model with the following elements added:
</p>

<dl>
<dt>betas</dt><dd><p> nonzero coefficients of the fitted model </p>
</dd>
<dt>alpha</dt><dd><p> constant offset making the exponential model sum to one
over the background data </p>
</dd>
<dt>entropy</dt><dd><p> entropy of the exponential model </p>
</dd>
<dt>penalty.factor</dt><dd><p> the regularization constants used for each feature </p>
</dd>
<dt>featuremins</dt><dd><p> minimum of each feature, to be used for clamping </p>
</dd>
<dt>featuremaxs</dt><dd><p> maximum of each feature, to be used for clamping </p>
</dd>
<dt>varmin</dt><dd><p> minimum of each predictor, to be used for clamping </p>
</dd>
<dt>varmax</dt><dd><p> maximum of each predictor, to be used for clamping </p>
</dd>
<dt>samplemeans</dt><dd><p> mean of each predictor over samples (majority for factors) </p>
</dd>
<dt>levels</dt><dd><p> levels of each predictor that is a factor </p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
# we repeat the example in the `maxnet` package
data("bradypus", package = "maxnet")
bradypus_tb &lt;- tibble::as_tibble(bradypus) %&gt;%
  dplyr::mutate(presence = relevel(
    factor(
      dplyr::case_match(presence, 1 ~ "presence", 0 ~ "absence")
    ),
    ref = "presence"
  ))
mod &lt;- maxnet_fit(presence ~ ., data = bradypus_tb, classes = "lq")
plot(mod, "tmp6190_ann")

</code></pre>

<hr>
<h2 id='maxnet_predict'>Wrapper to predict maxnet models</h2><span id='topic+maxnet_predict'></span>

<h3>Description</h3>

<p>This function is a wrapper around the <code>predict</code> method for <a href="maxnet.html#topic+maxnet">maxnet::maxnet</a>,
making the function compatible with
<code>parsnip</code>. Users are unlikely to use this function directly.  For the
<code>parsnip</code> model specification for MaxEnt, see <code><a href="#topic+maxent">maxent()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxnet_predict(
  object,
  newdata,
  type = c("class", "prob"),
  maxnet_type = c("cloglog", "link", "exponential", "logistic"),
  clamp = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxnet_predict_+3A_object">object</code></td>
<td>
<p>the <a href="maxnet.html#topic+maxnet">maxnet::maxnet</a> object</p>
</td></tr>
<tr><td><code id="maxnet_predict_+3A_newdata">newdata</code></td>
<td>
<p>the dataframe of new data</p>
</td></tr>
<tr><td><code id="maxnet_predict_+3A_type">type</code></td>
<td>
<p>either &quot;prob&quot; or &quot;class&quot;</p>
</td></tr>
<tr><td><code id="maxnet_predict_+3A_maxnet_type">maxnet_type</code></td>
<td>
<p>the transformation used for the prediction</p>
</td></tr>
<tr><td><code id="maxnet_predict_+3A_clamp">clamp</code></td>
<td>
<p>logical, defining whether clamping to observed ranges should
be used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble of predictions
</p>

<hr>
<h2 id='optim_thresh'>Find threshold that optimises a given metric</h2><span id='topic+optim_thresh'></span>

<h3>Description</h3>

<p>This function returns the threshold to turn probabilities into binary classes
whilst optimising a given metric. Currently available for <code><a href="#topic+tss_max">tss_max</a></code>, <code><a href="#topic+kap_max">kap_max</a></code> and
<code>sensitivity</code> (for which a target sensitivity is required).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_thresh(truth, estimate, metric, event_level = "first")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_thresh_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results (that is a
factor). This should be an unquoted column name although this argument is
passed by expression and supports quasiquotation (you can unquote column
names). For _vec() functions, a factor vector.</p>
</td></tr>
<tr><td><code id="optim_thresh_+3A_estimate">estimate</code></td>
<td>
<p>the predicted probability for the event</p>
</td></tr>
<tr><td><code id="optim_thresh_+3A_metric">metric</code></td>
<td>
<p>character of metric to be optimised. Currently only &quot;tss_max&quot;,
&quot;kap_max&quot;, and &quot;sensitivity&quot; with a given target
(e.g. c(&quot;sensitivity&quot;,0.8))</p>
</td></tr>
<tr><td><code id="optim_thresh_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either &quot;first&quot; or &quot;second&quot; to specify
which level of truth to consider as the &quot;event&quot;. This argument is only
applicable when estimator = &quot;binary&quot;. The default uses an internal helper
that generally defaults to &quot;first&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the probability threshold for the event
</p>


<h3>Examples</h3>

<pre><code class='language-R'>optim_thresh(two_class_example$truth, two_class_example$Class1, metric = c("tss_max"))
optim_thresh(two_class_example$truth, two_class_example$Class1, metric = c("sens", 0.9))
</code></pre>

<hr>
<h2 id='optim_thresh_kap_max'>Find threshold that maximises Kappa</h2><span id='topic+optim_thresh_kap_max'></span>

<h3>Description</h3>

<p>This is an internal function returns the threshold to turn probabilities into binary classes
to maximise kappa
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_thresh_kap_max(presences, absences)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_thresh_kap_max_+3A_presences">presences</code></td>
<td>
<p>Probabilities for presences.</p>
</td></tr>
<tr><td><code id="optim_thresh_kap_max_+3A_absences">absences</code></td>
<td>
<p>Provabilities for absences</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the probability threshold for the event
</p>

<hr>
<h2 id='optim_thresh_sens'>Find threshold that gives a target sensitivity</h2><span id='topic+optim_thresh_sens'></span>

<h3>Description</h3>

<p>This is an internal function returns the threshold to turn probabilities into binary classes
for a given target sensitivity
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_thresh_sens(presences, absences, sens_target)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_thresh_sens_+3A_presences">presences</code></td>
<td>
<p>Probabilities for presences.</p>
</td></tr>
<tr><td><code id="optim_thresh_sens_+3A_absences">absences</code></td>
<td>
<p>Provabilities for absences</p>
</td></tr>
<tr><td><code id="optim_thresh_sens_+3A_sens_target">sens_target</code></td>
<td>
<p>the target sensitivity</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the probability threshold for the event
</p>

<hr>
<h2 id='optim_thresh_tss_max'>Find threshold that maximises TSS</h2><span id='topic+optim_thresh_tss_max'></span>

<h3>Description</h3>

<p>This is an internal function returns the threshold to turn probabilities into binary classes
to maximise TSS
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_thresh_tss_max(presences, absences)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_thresh_tss_max_+3A_presences">presences</code></td>
<td>
<p>Probabilities for presences.</p>
</td></tr>
<tr><td><code id="optim_thresh_tss_max_+3A_absences">absences</code></td>
<td>
<p>Provabilities for absences</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the probability threshold for the event
</p>

<hr>
<h2 id='out_of_range_warning'>Warn if some times are outside the range of time steps from a raster</h2><span id='topic+out_of_range_warning'></span>

<h3>Description</h3>

<p>This function helps making sure that, when we assign times to time_step
layers of a raster, we do not have values which are badly out of range
</p>


<h3>Usage</h3>

<pre><code class='language-R'>out_of_range_warning(times, time_steps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="out_of_range_warning_+3A_times">times</code></td>
<td>
<p>the times of the locations</p>
</td></tr>
<tr><td><code id="out_of_range_warning_+3A_time_steps">time_steps</code></td>
<td>
<p>the time steps from the raster</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL return
</p>

<hr>
<h2 id='plot_pres_vs_bg'>Plot presences vs background</h2><span id='topic+plot_pres_vs_bg'></span>

<h3>Description</h3>

<p>Create a composite plots contrasting the distribution of multiple variables
for presences vs the background.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_pres_vs_bg(.data, .col)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_pres_vs_bg_+3A_.data">.data</code></td>
<td>
<p>a <code><a href="base.html#topic+data.frame">data.frame</a></code> (or derived object, such as <code><a href="tibble.html#topic+tibble">tibble::tibble</a></code>, or
<code><a href="sf.html#topic+sf">sf::st_sf</a></code>) with values for the bioclimate variables for presences and background</p>
</td></tr>
<tr><td><code id="plot_pres_vs_bg_+3A_.col">.col</code></td>
<td>
<p>the column containing the presences; it assumes presences to be
the first level of this factor</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>patchwork</code> composite plot
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("bradypus", package = "maxnet")
bradypus_tb &lt;- tibble::as_tibble(bradypus) %&gt;%
  dplyr::mutate(presence = relevel(
    factor(
      dplyr::case_match(presence, 1 ~ "presence", 0 ~ "absence")
    ),
    ref = "presence"
  )) %&gt;%
  select(-ecoreg)

bradypus_tb %&gt;% plot_pres_vs_bg(presence)

</code></pre>

<hr>
<h2 id='predict_raster'>Make predictions for a whole raster</h2><span id='topic+predict_raster'></span><span id='topic+predict_raster.default'></span>

<h3>Description</h3>

<p>This function allows to use a raster as data to make predictions from a
variety of <a href="tidymodels.html#topic+tidymodels">tidymodels</a> objects, such as <code><a href="#topic+simple_ensemble">simple_ensemble</a></code> or <code>stacks::linear_stack</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_raster(object, raster, ...)

## Default S3 method:
predict_raster(object, raster, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_raster_+3A_object">object</code></td>
<td>
<p>the <code><a href="tidymodels.html#topic+tidymodels">tidymodels</a></code> object of interest</p>
</td></tr>
<tr><td><code id="predict_raster_+3A_raster">raster</code></td>
<td>
<p>the <code><a href="terra.html#topic+SpatRaster-class">terra::SpatRaster</a></code> with the input data. It has to include
levels with the same names as the variables used in <code>object</code></p>
</td></tr>
<tr><td><code id="predict_raster_+3A_...">...</code></td>
<td>
<p>parameters to be passed to the standard <code>predict()</code> function
for the appropriate object type.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="terra.html#topic+SpatRaster-class">terra::SpatRaster</a></code> with the predictions
</p>

<hr>
<h2 id='predict.repeat_ensemble'>Predict for a repeat ensemble set</h2><span id='topic+predict.repeat_ensemble'></span>

<h3>Description</h3>

<p>Predict for a new dataset by using a repeat ensemble. Predictions from individual
models are combined according to <code>fun</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'repeat_ensemble'
predict(
  object,
  new_data,
  type = "prob",
  fun = "mean",
  metric_thresh = NULL,
  class_thresh = NULL,
  members = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.repeat_ensemble_+3A_object">object</code></td>
<td>
<p>an repeat_ensemble object</p>
</td></tr>
<tr><td><code id="predict.repeat_ensemble_+3A_new_data">new_data</code></td>
<td>
<p>a data frame in which to look for variables with which to predict.</p>
</td></tr>
<tr><td><code id="predict.repeat_ensemble_+3A_type">type</code></td>
<td>
<p>the type of prediction, &quot;prob&quot; or &quot;class&quot;.</p>
</td></tr>
<tr><td><code id="predict.repeat_ensemble_+3A_fun">fun</code></td>
<td>
<p>string defining the aggregating function. It can take values
<code>mean</code>, <code>median</code>, <code>weighted_mean</code>, <code>weighted_median</code> and <code>none</code>. It is possible
to combine multiple functions, except for &quot;none&quot;. If it
is set to &quot;none&quot;, only the individual member predictions are returned (this
automatically sets <code>member</code> to TRUE)</p>
</td></tr>
<tr><td><code id="predict.repeat_ensemble_+3A_metric_thresh">metric_thresh</code></td>
<td>
<p>a vector of length 2 giving a metric and its threshold,
which will be used to prune
which models in the ensemble will be used for the prediction. The 'metrics'
need to have been computed when the workflow was tuned. Examples are
c(&quot;accuracy&quot;,0.8) or c(&quot;boyce_cont&quot;,0.7)</p>
</td></tr>
<tr><td><code id="predict.repeat_ensemble_+3A_class_thresh">class_thresh</code></td>
<td>
<p>probability threshold used to convert probabilities into
classes. It can be a number (between 0 and 1), or a character metric (currently
&quot;tss_max&quot; or &quot;sensitivity&quot;). For sensitivity, an additional target value is passed
along as a second element of a vector, e.g. c(&quot;sensitivity&quot;,0.8).</p>
</td></tr>
<tr><td><code id="predict.repeat_ensemble_+3A_members">members</code></td>
<td>
<p>boolean defining whether individual predictions for each member
should be added to the ensemble prediction. The columns for individual members
have the name of the workflow a a prefix, separated by &quot;.&quot; from the usual
column names of the predictions.</p>
</td></tr>
<tr><td><code id="predict.repeat_ensemble_+3A_...">...</code></td>
<td>
<p>not used in this method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble of predictions
</p>

<hr>
<h2 id='predict.simple_ensemble'>Predict for a simple ensemble set</h2><span id='topic+predict.simple_ensemble'></span>

<h3>Description</h3>

<p>Predict for a new dataset by using a simple ensemble. Predictions from individual
models are combined according to <code>fun</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'simple_ensemble'
predict(
  object,
  new_data,
  type = "prob",
  fun = "mean",
  metric_thresh = NULL,
  class_thresh = NULL,
  members = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.simple_ensemble_+3A_object">object</code></td>
<td>
<p>an simple_ensemble object</p>
</td></tr>
<tr><td><code id="predict.simple_ensemble_+3A_new_data">new_data</code></td>
<td>
<p>a data frame in which to look for variables with which to predict.</p>
</td></tr>
<tr><td><code id="predict.simple_ensemble_+3A_type">type</code></td>
<td>
<p>the type of prediction, &quot;prob&quot; or &quot;class&quot;.</p>
</td></tr>
<tr><td><code id="predict.simple_ensemble_+3A_fun">fun</code></td>
<td>
<p>string defining the aggregating function. It can take values
<code>mean</code>, <code>median</code>, <code>weighted_mean</code>, <code>weighted_median</code> and <code>none</code>. It is possible
to combine multiple functions, except for &quot;none&quot;. If it
is set to &quot;none&quot;, only the individual member predictions are returned (this
automatically sets <code>member</code> to TRUE)</p>
</td></tr>
<tr><td><code id="predict.simple_ensemble_+3A_metric_thresh">metric_thresh</code></td>
<td>
<p>a vector of length 2 giving a metric and its threshold,
which will be used to prune
which models in the ensemble will be used for the prediction. The 'metrics'
need to have been computed when the workflow was tuned. Examples are
c(&quot;accuracy&quot;,0.8) or c(&quot;boyce_cont&quot;,0.7)</p>
</td></tr>
<tr><td><code id="predict.simple_ensemble_+3A_class_thresh">class_thresh</code></td>
<td>
<p>probability threshold used to convert probabilities into
classes. It can be a number (between 0 and 1), or a character metric (currently
&quot;tss_max&quot; or &quot;sensitivity&quot;). For sensitivity, an additional target value is passed
along as a second element of a vector, e.g. c(&quot;sensitivity&quot;,0.8).</p>
</td></tr>
<tr><td><code id="predict.simple_ensemble_+3A_members">members</code></td>
<td>
<p>boolean defining whether individual predictions for each member
should be added to the ensemble prediction. The columns for individual members
have the name of the workflow a a prefix, separated by &quot;.&quot; from the usual
column names of the predictions.</p>
</td></tr>
<tr><td><code id="predict.simple_ensemble_+3A_...">...</code></td>
<td>
<p>not used in this method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble of predictions
</p>

<hr>
<h2 id='prob_metrics_sf'>Probability metrics for <code>sf</code> objects</h2><span id='topic+prob_metrics_sf'></span><span id='topic+average_precision.sf'></span><span id='topic+brier_class.sf'></span><span id='topic+classification_cost.sf'></span><span id='topic+gain_capture.sf'></span><span id='topic+mn_log_loss.sf'></span><span id='topic+pr_auc.sf'></span><span id='topic+roc_auc.sf'></span><span id='topic+roc_aunp.sf'></span><span id='topic+roc_aunu.sf'></span>

<h3>Description</h3>

<p><code>tidysdm</code> provides methods to handle <a href="sf.html#topic+sf">sf::sf</a> objects for the following
<a href="yardstick.html#topic+yardstick">yardstick</a> metrics:
</p>
<p><code><a href="yardstick.html#topic+average_precision">yardstick::average_precision()</a></code>
</p>
<p><code><a href="yardstick.html#topic+brier_class">yardstick::brier_class()</a></code>
</p>
<p><code><a href="yardstick.html#topic+classification_cost">yardstick::classification_cost()</a></code>
</p>
<p><code><a href="yardstick.html#topic+gain_capture">yardstick::gain_capture()</a></code>
</p>
<p><code><a href="yardstick.html#topic+mn_log_loss">yardstick::mn_log_loss()</a></code>
</p>
<p><code><a href="yardstick.html#topic+pr_auc">yardstick::pr_auc()</a></code>
</p>
<p><code><a href="yardstick.html#topic+roc_auc">yardstick::roc_auc()</a></code>
</p>
<p><code><a href="yardstick.html#topic+roc_aunp">yardstick::roc_aunp()</a></code>
</p>
<p><code><a href="yardstick.html#topic+roc_aunu">yardstick::roc_aunu()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sf'
average_precision(data, ...)

## S3 method for class 'sf'
brier_class(data, ...)

## S3 method for class 'sf'
classification_cost(data, ...)

## S3 method for class 'sf'
gain_capture(data, ...)

## S3 method for class 'sf'
mn_log_loss(data, ...)

## S3 method for class 'sf'
pr_auc(data, ...)

## S3 method for class 'sf'
roc_auc(data, ...)

## S3 method for class 'sf'
roc_aunp(data, ...)

## S3 method for class 'sf'
roc_aunu(data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prob_metrics_sf_+3A_data">data</code></td>
<td>
<p>an <a href="sf.html#topic+sf">sf::sf</a> object</p>
</td></tr>
<tr><td><code id="prob_metrics_sf_+3A_...">...</code></td>
<td>
<p>any other parameters to pass to the <code>data.frame</code> version of
the metric.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with columns <code>.metric</code>, <code>.estimator</code>, and <code>.estimate</code>
and 1 row of values.
</p>

<hr>
<h2 id='prob_to_binary'>simple function to convert probability to binary classes</h2><span id='topic+prob_to_binary'></span>

<h3>Description</h3>

<p>simple function to convert probability to binary classes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prob_to_binary(x, thresh, class_levels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prob_to_binary_+3A_x">x</code></td>
<td>
<p>a vector of probabilities</p>
</td></tr>
<tr><td><code id="prob_to_binary_+3A_thresh">thresh</code></td>
<td>
<p>the threshold to convert to binary</p>
</td></tr>
<tr><td><code id="prob_to_binary_+3A_class_levels">class_levels</code></td>
<td>
<p>the binary levels</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of binary values
</p>

<hr>
<h2 id='recipe.sf'>Recipe for <code>sf</code> objects</h2><span id='topic+recipe.sf'></span><span id='topic+spatial_recipe'></span>

<h3>Description</h3>

<p>This method for <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code> handles the
case when <code>x</code> is an <a href="sf.html#topic+sf">sf::sf</a> object, as commonly
used in Species Distribution Model, and generates a <code>spatial_recipe</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sf'
recipe(x, ...)

spatial_recipe(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recipe.sf_+3A_x">x</code></td>
<td>
<p>An <a href="sf.html#topic+sf">sf::sf</a> data frame.</p>
</td></tr>
<tr><td><code id="recipe.sf_+3A_...">...</code></td>
<td>
<p>parameters to be passed to <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><a href="recipes.html#topic+recipes">recipes</a> are not natively compatible with <a href="sf.html#topic+sf">sf::sf</a> objects. The problem is that
the <code>geometry</code> column of <a href="sf.html#topic+sf">sf::sf</a> objects is a list, which is incompatible with
the translation of formulae in <a href="recipes.html#topic+recipe">recipe</a>. This method strips the <code>geometry</code>
column from the <a href="base.html#topic+data.frame">data.frame</a> and replaces it with a simple <code>X</code> and <code>Y</code> columns
before any further operations, thus allowing
the usual processing by <code><a href="recipes.html#topic+recipe">recipe()</a></code> to succeed (<code>X</code> and <code>Y</code> are give the role
of coords in a spatial recipe). When prepping and baking a <code>spatial_recipe</code>,
if a data.frame or tibble without coordinates is used as <code>training</code> or
<code>new_data</code>, dummy <code>X</code> and <code>Y</code> columns
are generated and filled with NAs.
NOTE that order matters! You need to use the syntax
<code>recipe(x=sf_obj, formula=class~.)</code> for the method to successfully detect
the <a href="sf.html#topic+sf">sf::sf</a> object. Starting with <code>formula</code> will fail.
</p>


<h3>Value</h3>

<p>An object of class <code>spatial_recipe</code>, which is a derived version of
<a href="recipes.html#topic+recipe">recipes::recipe</a> , see
the manpage for <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code> for details.
</p>

<hr>
<h2 id='repeat_ensemble'>Repeat ensemble</h2><span id='topic+repeat_ensemble'></span>

<h3>Description</h3>

<p>An ensemble based multiple sets of pseudoabsences/background. This object
is a collection (list) of <code><a href="#topic+simple_ensemble">simple_ensemble</a></code> objects for which predictions will
be combined in a simple way (e.g. by taking either the mean or median). Each
<code><a href="#topic+simple_ensemble">simple_ensemble</a></code> contains the best version of a each given model type
following turning; all simple ensembles will need to have the same metric
estimated during the cv process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>repeat_ensemble(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="repeat_ensemble_+3A_...">...</code></td>
<td>
<p>not used, this function just creates an empty <code>repeat_ensemble</code>
object. Members are added with <code>add_best_candidates()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>an empty <code>repeat_ensemble</code>
</p>

<hr>
<h2 id='sample_pseudoabs'>Sample pseudo-absence (or background) points for SDM analysis</h2><span id='topic+sample_pseudoabs'></span>

<h3>Description</h3>

<p>This function samples pseudo-absence (or background, the naming is a matter
of semantics) points from a raster given a set of presences.
The locations returned as the center points of the sampled cells, which can
not overlap with the presences. The following methods are implemented:
</p>

<ul>
<li><p> 'random': pseudo-absences/background randomly sampled from the region covered by the
raster (i.e. not NAs).
</p>
</li>
<li><p> 'dist_min': pseudo-absences/background randomly sampled from the region excluding a buffer
of 'dist_min' from presences (distances in 'm' for lonlat rasters, and in map
units for projected rasters).
</p>
</li>
<li><p> 'dist_max': pseudo-absences/background randomly sampled from the unioned buffers
of 'dist_max' from presences (distances in 'm' for lonlat rasters, and in map
units for projected rasters). Using the union of buffers means that areas that
are in multiple buffers are not oversampled. This is also referred to as &quot;thickening&quot;.
</p>
</li>
<li><p> 'dist_disc': pseudo-absences/background randomly sampled from the unioned discs around presences
with the two values of 'dist_disc' defining the minimum and maximum distance from
presences.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>sample_pseudoabs(
  data,
  raster,
  n,
  coords = NULL,
  method = "random",
  class_label = "pseudoabs",
  return_pres = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_pseudoabs_+3A_data">data</code></td>
<td>
<p>An <code><a href="sf.html#topic+sf">sf::sf</a></code> data frame, or a data frame with coordinate variables.
These can be defined in <code>coords</code>, unless they have standard names
(see details below).</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_+3A_raster">raster</code></td>
<td>
<p>the <a href="terra.html#topic+SpatRaster-class">terra::SpatRaster</a> from which cells will be sampled</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_+3A_n">n</code></td>
<td>
<p>number of pseudoabsence/background points to sample</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_+3A_coords">coords</code></td>
<td>
<p>a vector of length two giving the names of the &quot;x&quot; and &quot;y&quot;
coordinates, as found in <code>data</code>. If left to NULL, the function will
try to guess the columns based on standard names <code>c("x", "y")</code>, <code>c("X","Y")</code>,
<code>c("longitude", "latitude")</code>, or <code>c("lon", "lat")</code></p>
</td></tr>
<tr><td><code id="sample_pseudoabs_+3A_method">method</code></td>
<td>
<p>sampling method. One of 'random', 'dist_min', 'dist_max', or
'dist_disc'. Threshold distances are set as additional elements of a vector,
e.g c('dist_min',70000) or c('dist_disc',50000,200000).</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_+3A_class_label">class_label</code></td>
<td>
<p>the label given to the sampled points. Defaults to <code>pseudoabs</code></p>
</td></tr>
<tr><td><code id="sample_pseudoabs_+3A_return_pres">return_pres</code></td>
<td>
<p>return presences together with pseudoabsences/background
in a single tibble</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <a href="tibble.html#topic+tibble">tibble::tibble</a>. If presences are returned, the
presence level is set as the reference (to match the expectations in the
<code>yardstick</code> package that considers the first level to be the event)
</p>

<hr>
<h2 id='sample_pseudoabs_time'>Sample pseudo-absence (or background) points for SDM analysis for points with a time point.</h2><span id='topic+sample_pseudoabs_time'></span>

<h3>Description</h3>

<p>This function samples pseudo-absence (or background, the naming is a matter
of semantics) points from a raster given a set of presences.
The locations returned as the center points of the sampled cells, which can
not overlap with the presences. The following methods are implemented:
</p>

<ul>
<li><p> 'random': pseudo-absences/background randomly sampled from the region covered by the
raster (i.e. not NAs).
</p>
</li>
<li><p> 'dist_min': pseudo-absences/background randomly sampled from the region excluding a buffer
of 'dist_min' from presences (distances in 'm' for lonlat rasters, and in map
units for projected rasters).
</p>
</li>
<li><p> 'dist_max': pseudo-absences/background randomly sampled from the unioned buffers
of 'dist_max' from presences (distances in 'm' for lonlat rasters, and in map
units for projected rasters). Using the union of buffers means that areas that
are in multiple buffers are not oversampled. This is also referred to as &quot;thickening&quot;.
</p>
</li>
<li><p> 'dist_disc': pseudo-absences/background randomly sampled from the unioned discs around presences
with the two values of 'dist_disc' defining the minimum and maximum distance from
presences.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>sample_pseudoabs_time(
  data,
  raster,
  n_per_presence,
  coords = NULL,
  time_col = "time",
  lubridate_fun = c,
  method = "random",
  class_label = "pseudoabs",
  return_pres = TRUE,
  time_buffer = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_pseudoabs_time_+3A_data">data</code></td>
<td>
<p>An <code><a href="sf.html#topic+sf">sf::sf</a></code> data frame, or a data frame with coordinate variables.
These can be defined in <code>coords</code>, unless they have standard names
(see details below).</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_time_+3A_raster">raster</code></td>
<td>
<p>the <a href="terra.html#topic+SpatRaster-class">terra::SpatRaster</a> or <a href="terra.html#topic+SpatRaster-class">terra::SpatRasterDataset</a> from which cells will be sampled.
If a <a href="terra.html#topic+SpatRaster-class">terra::SpatRasterDataset</a>, the first dataset will be used to define which cells are valid,
and which are NAs.</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_time_+3A_n_per_presence">n_per_presence</code></td>
<td>
<p>number of pseudoabsence/background points to sample for
each presence</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_time_+3A_coords">coords</code></td>
<td>
<p>a vector of length two giving the names of the &quot;x&quot; and &quot;y&quot;
coordinates, as found in <code>data</code>. If left to NULL, the function will
try to guess the columns based on standard names <code>c("x", "y")</code>, <code>c("X","Y")</code>,
<code>c("longitude", "latitude")</code>, or <code>c("lon", "lat")</code></p>
</td></tr>
<tr><td><code id="sample_pseudoabs_time_+3A_time_col">time_col</code></td>
<td>
<p>The name of the column with time; if time is not a lubridate object,
use <code>lubridate_fun</code> to provide a function that can be used to convert appropriately</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_time_+3A_lubridate_fun">lubridate_fun</code></td>
<td>
<p>function to convert the time column into a lubridate object</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_time_+3A_method">method</code></td>
<td>
<p>sampling method. One of 'random', 'dist_min', 'dist_max', or
'dist_disc'.</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_time_+3A_class_label">class_label</code></td>
<td>
<p>the label given to the sampled points. Defaults to <code>pseudoabs</code></p>
</td></tr>
<tr><td><code id="sample_pseudoabs_time_+3A_return_pres">return_pres</code></td>
<td>
<p>return presences together with pseudoabsences/background
in a single tibble</p>
</td></tr>
<tr><td><code id="sample_pseudoabs_time_+3A_time_buffer">time_buffer</code></td>
<td>
<p>the buffer on the time axis around presences that defines their effect when
sampling pseudoabsences. If set to zero, presences have an effect only on the time step to which
they are assigned in <code>raster</code>; if a positive value, it defines the number of days before
and after the date provided in the <code>time</code> column for which the presence should be considered
(e.g. 20 days means that a presence is considered in all time steps equivalent to plus and minus
twenty days from its date).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <a href="tibble.html#topic+tibble">tibble::tibble</a>. If presences are returned, the
presence level is set as the reference (to match the expectations in the
<code>yardstick</code> package that considers the first level to be the event)
</p>

<hr>
<h2 id='sdm_metric_set'>Metric set for SDM</h2><span id='topic+sdm_metric_set'></span>

<h3>Description</h3>

<p>This function returns a <a href="yardstick.html#topic+metric_set">yardstick::metric_set</a> that includes <code><a href="#topic+boyce_cont">boyce_cont()</a></code>,
<code><a href="yardstick.html#topic+roc_auc">yardstick::roc_auc()</a></code> and <code><a href="#topic+tss_max">tss_max()</a></code>, the most commonly used metrics for
SDM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdm_metric_set(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdm_metric_set_+3A_...">...</code></td>
<td>
<p>additional metrics to be added to the
<code><a href="yardstick.html#topic+metric_set">yardstick::metric_set</a></code>. See the help to <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> for
constraints on the type of metrics that can be mixed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set</a></code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sdm_metric_set()
sdm_metric_set(accuracy)
</code></pre>

<hr>
<h2 id='sdm_spec_boost_tree'>Model specification for a Boosted Trees model for SDM</h2><span id='topic+sdm_spec_boost_tree'></span>

<h3>Description</h3>

<p>This function returns a <a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> for a Boosted Trees model to
be used as a classifier of presences and absences in Species Distribution Model.
It uses the library <code>xgboost</code> to fit boosted trees; to use another library, simply build the
<a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdm_spec_boost_tree(..., tune = c("sdm", "all", "custom", "none"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdm_spec_boost_tree_+3A_...">...</code></td>
<td>
<p>parameters to be passed to <code><a href="parsnip.html#topic+boost_tree">parsnip::boost_tree()</a></code> to
customise the model. See the help of that function for details.</p>
</td></tr>
<tr><td><code id="sdm_spec_boost_tree_+3A_tune">tune</code></td>
<td>
<p>character defining the tuning strategy. Valid strategies are:
</p>

<ul>
<li><p> &quot;sdm&quot; chooses hyperparameters that are most important to tune for
an sdm (for <em>boost_tree</em>: 'mtry', 'trees', 'tree_depth', 'learn_rate',
'loss_reduction', and 'stop_iter')
</p>
</li>
<li><p> &quot;all&quot; tunes all hyperparameters (for <em>boost_tree</em>: 'mtry', 'trees',
'tree_depth', 'learn_rate',
'loss_reduction', 'stop_iter','min_n' and 'sample_size')
</p>
</li>
<li><p> &quot;custom&quot; passes the options from '...'
</p>
</li>
<li><p> &quot;none&quot; does not tune any hyperparameter
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> of the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>standard_bt_spec &lt;- sdm_spec_boost_tree()
full_bt_spec &lt;- sdm_spec_boost_tree(tune = "all")
custom_bt_spec &lt;- sdm_spec_boost_tree(tune = "custom", mtry = tune())
</code></pre>

<hr>
<h2 id='sdm_spec_gam'>Model specification for a GAM for SDM</h2><span id='topic+sdm_spec_gam'></span>

<h3>Description</h3>

<p>This function returns a <a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> for a General Additive Model to
be used as a classifier of presences and absences in Species Distribution Model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdm_spec_gam(..., tune = "none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdm_spec_gam_+3A_...">...</code></td>
<td>
<p>parameters to be passed to <code><a href="parsnip.html#topic+gen_additive_mod">parsnip::gen_additive_mod()</a></code> to
customise the model. See the help of that function for details.</p>
</td></tr>
<tr><td><code id="sdm_spec_gam_+3A_tune">tune</code></td>
<td>
<p>character defining the tuning strategy. As there are no hyperparameters
to tune in a <em>gam</em>, the only valid option is &quot;none&quot;. This parameter is present
for consistency with other <code style="white-space: pre;">&#8288;sdm_spec_*&#8288;</code> functions, but it does nothing in this
case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> of the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>my_gam_spec &lt;- sdm_spec_gam()
</code></pre>

<hr>
<h2 id='sdm_spec_glm'>Model specification for a GLM for SDM</h2><span id='topic+sdm_spec_glm'></span>

<h3>Description</h3>

<p>This function returns a <a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> for a Generalised Linear Model to
be used as a classifier of presences and absences in Species Distribution Model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdm_spec_glm(..., tune = "none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdm_spec_glm_+3A_...">...</code></td>
<td>
<p>parameters to be passed to <code><a href="parsnip.html#topic+logistic_reg">parsnip::logistic_reg()</a></code> to customise
the model. See the help of that function for details.</p>
</td></tr>
<tr><td><code id="sdm_spec_glm_+3A_tune">tune</code></td>
<td>
<p>character defining the tuning strategy. As there are no hyperparameters
to tune in a <em>glm</em>, the only valid option is &quot;none&quot;. This parameter is present
for consistency with other <code style="white-space: pre;">&#8288;sdm_spec_*&#8288;</code> functions, but it does nothing in this
case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> of the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>my_spec_glm &lt;- sdm_spec_glm()
</code></pre>

<hr>
<h2 id='sdm_spec_maxent'>Model specification for a MaxEnt for SDM</h2><span id='topic+sdm_spec_maxent'></span>

<h3>Description</h3>

<p>This function returns a <a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> for a MaxEnt model to
be used as a classifier of presences and absences in Species Distribution
Models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdm_spec_maxent(..., tune = c("sdm", "all", "custom", "none"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdm_spec_maxent_+3A_...">...</code></td>
<td>
<p>parameters to be passed to <code><a href="#topic+maxent">maxent()</a></code> to
customise the model. See the help of that function for details.</p>
</td></tr>
<tr><td><code id="sdm_spec_maxent_+3A_tune">tune</code></td>
<td>
<p>character defining the tuning strategy. Valid strategies are:
</p>

<ul>
<li><p> &quot;sdm&quot; chooses hyperparameters that are most important to tune for
an sdm (for <em>maxent</em>, 'mtry')
</p>
</li>
<li><p> &quot;all&quot; tunes all hyperparameters (for <em>maxent</em>, 'mtry', 'trees' and 'min')
</p>
</li>
<li><p> &quot;custom&quot; passes the options from '...'
</p>
</li>
<li><p> &quot;none&quot; does not tune any hyperparameter
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> of the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test_maxent_spec &lt;- sdm_spec_maxent(tune = "sdm")
test_maxent_spec
# setting specific values
sdm_spec_maxent(tune = "custom", feature_classes = "lq")
</code></pre>

<hr>
<h2 id='sdm_spec_rand_forest'>Model specification for a Random Forest for SDM</h2><span id='topic+sdm_spec_rand_forest'></span><span id='topic+sdm_spec_rf'></span>

<h3>Description</h3>

<p>This function returns a <a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> for a Random Forest to
be used as a classifier of presences and absences in Species Distribution
Models. It uses the library <code>ranger</code> to fit boosted trees; to use another
library, simply build the
<a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdm_spec_rand_forest(..., tune = c("sdm", "all", "custom", "none"))

sdm_spec_rf(..., tune = c("sdm", "all", "custom", "none"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdm_spec_rand_forest_+3A_...">...</code></td>
<td>
<p>parameters to be passed to <code><a href="parsnip.html#topic+rand_forest">parsnip::rand_forest()</a></code> to
customise the model. See the help of that function for details.</p>
</td></tr>
<tr><td><code id="sdm_spec_rand_forest_+3A_tune">tune</code></td>
<td>
<p>character defining the tuning strategy. Valid strategies are:
</p>

<ul>
<li><p> &quot;sdm&quot; chooses hyperparameters that are most important to tune for
an sdm (for <em>rf</em>, 'mtry')
</p>
</li>
<li><p> &quot;all&quot; tunes all hyperparameters (for <em>rf</em>, 'mtry', 'trees' and 'min')
</p>
</li>
<li><p> &quot;custom&quot; passes the options from '...'
</p>
</li>
<li><p> &quot;none&quot; does not tune any hyperparameter
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sdm_spec_rf()</code> is simply a short form for <code>sm_spec_rand_forest()</code>.
</p>


<h3>Value</h3>

<p>a <a href="parsnip.html#topic+model_spec">parsnip::model_spec</a> of the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test_rf_spec &lt;- sdm_spec_rf(tune = "sdm")
test_rf_spec
# combining tuning with specific values for other hyperparameters
sdm_spec_rf(tune = "sdm", trees = 100)
</code></pre>

<hr>
<h2 id='simple_ensemble'>Simple ensemble</h2><span id='topic+simple_ensemble'></span>

<h3>Description</h3>

<p>A simple ensemble is a collection of workflows for which predictions will
be combined in a simple way (e.g. by taking either the mean or median). Usually
these workflows will consists each of the best version of a given model type
following turning. The workflows are fitted to the full training dataset
before making predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simple_ensemble(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simple_ensemble_+3A_...">...</code></td>
<td>
<p>not used, this function just creates an empty <code>simple_ensemble</code>
object. Members are added with <code>add_best_candidates()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>an empty <code>simple_ensemble</code>. This is a tibble with columns:
</p>

<ul>
<li> <p><code>wflow_id</code>: the name of the workflows for which the best model was
chosen
</p>
</li>
<li> <p><code>workflow</code>: the trained workflow objects
</p>
</li>
<li> <p><code>metrics</code>: metrics based on the crossvalidation resampling used
to tune the models
</p>
</li></ul>


<hr>
<h2 id='spatial_initial_split'>Simple Training/Test Set Splitting for spatial data</h2><span id='topic+spatial_initial_split'></span>

<h3>Description</h3>

<p><code>spatial_initial_split</code> creates a single binary split of the data into a training
set and testing set. All strategies from the package <a href="spatialsample.html#topic+spatialsample">spatialsample</a> are available;
a random split from that strategy will be used to generate the initial split.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spatial_initial_split(data, prop, strategy, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spatial_initial_split_+3A_data">data</code></td>
<td>
<p>A dataset (data.frame or tibble)</p>
</td></tr>
<tr><td><code id="spatial_initial_split_+3A_prop">prop</code></td>
<td>
<p>The proportion of data to be retained for modelling/analysis.</p>
</td></tr>
<tr><td><code id="spatial_initial_split_+3A_strategy">strategy</code></td>
<td>
<p>A sampling strategy from <a href="spatialsample.html#topic+spatialsample">spatialsample</a></p>
</td></tr>
<tr><td><code id="spatial_initial_split_+3A_...">...</code></td>
<td>
<p>parameters to be passed to the <code>strategy</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>rsplit</code> object that can be used with the <a href="rsample.html#topic+initial_split">rsample::training</a> and <a href="rsample.html#topic+initial_split">rsample::testing</a>
functions to extract the data in each split.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
block_initial &lt;- spatial_initial_split(boston_canopy, prop = 1 / 5, spatial_block_cv)
testing(block_initial)
training(block_initial)
</code></pre>

<hr>
<h2 id='thin_by_cell'>Thin point dataset to have 1 observation per raster cell</h2><span id='topic+thin_by_cell'></span>

<h3>Description</h3>

<p>This function thins a dataset so that only one observation per cell
is retained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thin_by_cell(data, raster, coords = NULL, drop_na = TRUE, agg_fact = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="thin_by_cell_+3A_data">data</code></td>
<td>
<p>An <code><a href="sf.html#topic+sf">sf::sf</a></code> data frame, or a data frame with coordinate variables.
These can be defined in <code>coords</code>, unless they have standard names
(see details below).</p>
</td></tr>
<tr><td><code id="thin_by_cell_+3A_raster">raster</code></td>
<td>
<p>A <code><a href="terra.html#topic+SpatRaster-class">terra::SpatRaster</a></code> object that defined the grid</p>
</td></tr>
<tr><td><code id="thin_by_cell_+3A_coords">coords</code></td>
<td>
<p>a vector of length two giving the names of the &quot;x&quot; and &quot;y&quot;
coordinates, as found in <code>data</code>. If left to NULL, the function will
try to guess the columns based on standard names <code>c("x", "y")</code>, <code>c("X","Y")</code>,
<code>c("longitude", "latitude")</code>, or <code>c("lon", "lat")</code></p>
</td></tr>
<tr><td><code id="thin_by_cell_+3A_drop_na">drop_na</code></td>
<td>
<p>boolean on whether locations that are NA in the raster should be dropped.</p>
</td></tr>
<tr><td><code id="thin_by_cell_+3A_agg_fact">agg_fact</code></td>
<td>
<p>positive integer. Aggregation factor expressed as number of cells
in each direction (horizontally and vertically). Or two integers (horizontal
and vertical aggregation factor) or three integers (when also aggregating over layers).
Defaults to NULL, which implies no aggregation (i.e. thinning is done on the
grid of <code>raster</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Further thinning can be achieved by aggregating cells in the raster
before thinning, as achieved by setting <code>agg_fact</code> &gt; 1 (aggregation works in a
manner equivalent to <code><a href="terra.html#topic+aggregate">terra::aggregate()</a></code>).
</p>


<h3>Value</h3>

<p>An object of class <code><a href="sf.html#topic+sf">sf::sf</a></code> or <code><a href="base.html#topic+data.frame">data.frame</a></code>, the same as &quot;data&quot;.
</p>

<hr>
<h2 id='thin_by_cell_time'>Thin point dataset to have 1 observation per raster cell per time slice</h2><span id='topic+thin_by_cell_time'></span>

<h3>Description</h3>

<p>This function thins a dataset so that only one observation per cell per time
slice is retained. We use a raster with layers as time slices to define the
data cube on which thinning is enforced (see details below on how time should be
formatted).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thin_by_cell_time(
  data,
  raster,
  coords = NULL,
  time_col = "time",
  lubridate_fun = c,
  drop_na = TRUE,
  agg_fact = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="thin_by_cell_time_+3A_data">data</code></td>
<td>
<p>An <code><a href="sf.html#topic+sf">sf::sf</a></code> data frame, or a data frame with coordinate variables.
These can be defined in <code>coords</code>, unless they have standard names
(see details below).</p>
</td></tr>
<tr><td><code id="thin_by_cell_time_+3A_raster">raster</code></td>
<td>
<p>A <code><a href="terra.html#topic+SpatRaster-class">terra::SpatRaster</a></code> object that defined the grid with layers
corresponding to the time slices (times should be set as either POSIXlt or
&quot;years&quot;, see <code><a href="terra.html#topic+time">terra::time()</a></code> for details), or a <code><a href="terra.html#topic+SpatRaster-class">terra::SpatRasterDataset</a></code>
where the first dataset will be
used (again, times for that dataset should be set as either POSIXlt or
&quot;years&quot;)
<code>terra::time()</code></p>
</td></tr>
<tr><td><code id="thin_by_cell_time_+3A_coords">coords</code></td>
<td>
<p>a vector of length two giving the names of the &quot;x&quot; and &quot;y&quot;
coordinates, as found in <code>data</code>. If left to NULL, the function will
try to guess the columns based on standard names <code>c("x", "y")</code>, <code>c("X","Y")</code>,
<code>c("longitude", "latitude")</code>, or <code>c("lon", "lat")</code></p>
</td></tr>
<tr><td><code id="thin_by_cell_time_+3A_time_col">time_col</code></td>
<td>
<p>The name of the column with time; if time is not a lubridate object,
use <code>lubridate_fun</code> to provide a function that can be used to convert appropriately</p>
</td></tr>
<tr><td><code id="thin_by_cell_time_+3A_lubridate_fun">lubridate_fun</code></td>
<td>
<p>function to convert the time column into a lubridate object</p>
</td></tr>
<tr><td><code id="thin_by_cell_time_+3A_drop_na">drop_na</code></td>
<td>
<p>boolean on whether locations that are NA in the raster should be dropped.</p>
</td></tr>
<tr><td><code id="thin_by_cell_time_+3A_agg_fact">agg_fact</code></td>
<td>
<p>positive integer. Aggregation factor expressed as number of cells
in each direction (horizontally and vertically). Or two integers (horizontal
and vertical aggregation factor) or three integers (when also aggregating over layers).
Defaults to NULL, which implies no aggregation (i.e. thinning is done on the
grid of <code>raster</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Further spatial thinning can be achieved by aggregating cells in the raster
before thinning, as achieved by setting <code>agg_fact</code> &gt; 1 (aggregation works in a
manner equivalent to <code><a href="terra.html#topic+aggregate">terra::aggregate()</a></code>).
</p>


<h3>Value</h3>

<p>An object of class <code><a href="sf.html#topic+sf">sf::sf</a></code> or <code><a href="base.html#topic+data.frame">data.frame</a></code>, the same as &quot;data&quot;.
</p>

<hr>
<h2 id='thin_by_dist'>Thin points dataset based on geographic distance</h2><span id='topic+thin_by_dist'></span>

<h3>Description</h3>

<p>This function thins a dataset so that only observations that have a distance
from each other greater than &quot;dist_min&quot; are retained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thin_by_dist(data, dist_min, coords = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="thin_by_dist_+3A_data">data</code></td>
<td>
<p>An <code><a href="sf.html#topic+sf">sf::sf</a></code> data frame, or a data frame with coordinate variables.
These can be defined in <code>coords</code>, unless they have standard names
(see details below).</p>
</td></tr>
<tr><td><code id="thin_by_dist_+3A_dist_min">dist_min</code></td>
<td>
<p>Minimum distance between points (in units appropriate for
the projection, or meters for lonlat data).</p>
</td></tr>
<tr><td><code id="thin_by_dist_+3A_coords">coords</code></td>
<td>
<p>A vector of length two giving the names of the &quot;x&quot; and &quot;y&quot;
coordinates, as found in <code>data</code>. If left to NULL, the function will
try to guess the columns based on standard names <code>c("x", "y")</code>, <code>c("X","Y")</code>,
<code>c("longitude", "latitude")</code>, or <code>c("lon", "lat")</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Distances are measured in the appropriate units for the projection used. In case
of raw latitude and longitude (e.g. as provided in a data.frame), the crs is set
to WGS84, and units are set to meters.
</p>
<p>This function is a modified version of the algorithm in <code>spThin</code>, adapted
to work on <code>sf</code> objects.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="sf.html#topic+sf">sf::sf</a></code> or <code><a href="base.html#topic+data.frame">data.frame</a></code>, the same as &quot;data&quot;.
</p>

<hr>
<h2 id='thin_by_dist_time'>Thin points dataset based on geographic and temporal distance</h2><span id='topic+thin_by_dist_time'></span>

<h3>Description</h3>

<p>This function thins a dataset so that only observations that have a distance
from each other greater than &quot;dist_min&quot; in space and &quot;interval_min&quot; in time are retained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thin_by_dist_time(
  data,
  dist_min,
  interval_min,
  coords = NULL,
  time_col = "time",
  lubridate_fun = c
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="thin_by_dist_time_+3A_data">data</code></td>
<td>
<p>An <code><a href="sf.html#topic+sf">sf::sf</a></code> data frame, or a data frame with coordinate variables.
These can be defined in <code>coords</code>, unless they have standard names
(see details below).</p>
</td></tr>
<tr><td><code id="thin_by_dist_time_+3A_dist_min">dist_min</code></td>
<td>
<p>Minimum distance between points (in units appropriate for
the projection, or meters for lonlat data).</p>
</td></tr>
<tr><td><code id="thin_by_dist_time_+3A_interval_min">interval_min</code></td>
<td>
<p>Minimum time interval between points, in days.</p>
</td></tr>
<tr><td><code id="thin_by_dist_time_+3A_coords">coords</code></td>
<td>
<p>A vector of length two giving the names of the &quot;x&quot; and &quot;y&quot;
coordinates, as found in <code>data</code>. If left to NULL, the function will
try to guess the columns based on standard names <code>c("x", "y")</code>, <code>c("X","Y")</code>,
<code>c("longitude", "latitude")</code>, or <code>c("lon", "lat")</code></p>
</td></tr>
<tr><td><code id="thin_by_dist_time_+3A_time_col">time_col</code></td>
<td>
<p>The name of the column with time; if time is not a lubridate object,
use <code>lubridate_fun</code> to provide a function that can be used to convert appropriately</p>
</td></tr>
<tr><td><code id="thin_by_dist_time_+3A_lubridate_fun">lubridate_fun</code></td>
<td>
<p>function to convert the time column into a lubridate object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Geographic distances are measured in the appropriate units for the projection used. In case
of raw latitude and longitude (e.g. as provided in a data.frame), the crs is set
to WGS84, and units are set to meters. Time interval are estimated in days. Note that for
very long time period, the simple conversion x years = 365 * x days might lead
to slightly shorter intervals than expected, as it ignores leap years. The function
<code><a href="#topic+y2d">y2d()</a></code> provides a closer approximation.
</p>
<p>This function an algorithm analogous to <code>spThin</code>, with the exception that
neighbours are defined in terms of both space and time.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="sf.html#topic+sf">sf::sf</a></code> or <code><a href="base.html#topic+data.frame">data.frame</a></code>, the same as &quot;data&quot;.
</p>

<hr>
<h2 id='tidysdm'>tidysdm</h2><span id='topic+tidysdm'></span>

<h3>Description</h3>

<p>This <code>R</code> library facilitates the fitting of Species Distribution
Models with <code>tidymodels</code>.
</p>


<h3>Details</h3>

<p>The functionalities of <code>tidysdm</code> are described in
Leonardi et al. (2023) <a href="https://doi.org/10.1101/2023.07.24.550358">doi:10.1101/2023.07.24.550358</a>. Please cite it if you
use <code>tidysdm</code> in your research.
</p>
<p>On its dedicated <a href="https://evolecolgroup.github.io/tidysdm/">website</a>, you can find
Articles giving you a step-by-step <a href="https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html">overview of the package</a>,
how to use <a href="https://evolecolgroup.github.io/tidysdm/articles/a1_palaeodata_application.html"><code>tidysdm</code> on palaeodata</a>,
examples of <a href="https://evolecolgroup.github.io/tidysdm/articles/a2_tidymodels_additions.html">advanced modelling approaches using <code>tidymodels</code> features</a>,
and a <a href="https://evolecolgroup.github.io/tidysdm/articles/a3_troubleshooting.html">troubleshooting guide for when models fail</a>.
There is also a
<a href="https://evolecolgroup.github.io/tidysdm/dev/">development version</a> of the site
updated for the <code>dev</code> version (on the top left, the version number is in
red, and will be in the format x.x.x.9xxx, indicating it is a
development version).
</p>

<hr>
<h2 id='tss'>TSS - True Skill Statistics</h2><span id='topic+tss'></span><span id='topic+tss.data.frame'></span>

<h3>Description</h3>

<p>The True Skills Statistic, which is defined as
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tss(data, ...)

## S3 method for class 'data.frame'
tss(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = "first",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tss_+3A_data">data</code></td>
<td>
<p>Either a data.frame containing the columns specified by the truth
and estimate arguments, or a table/matrix where the true class
results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="tss_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="tss_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results (that is a factor). This should be an unquoted column name although this argument is passed by expression and supports quasiquotation (you can unquote column names). For _vec() functions, a factor vector.</p>
</td></tr>
<tr><td><code id="tss_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class results (that is also factor). As with truth this can be specified different ways but the primary method is to use an unquoted variable name. For _vec() functions, a factor vector.</p>
</td></tr>
<tr><td><code id="tss_+3A_estimator">estimator</code></td>
<td>
<p>One of: &quot;binary&quot;, &quot;macro&quot;, &quot;macro_weighted&quot;, or &quot;micro&quot; to specify the type of averaging to be done. &quot;binary&quot; is only relevant for the two class case. The other three are general methods for calculating multiclass metrics. The default will automatically choose &quot;binary&quot; or &quot;macro&quot; based on estimate.</p>
</td></tr>
<tr><td><code id="tss_+3A_na_rm">na_rm</code></td>
<td>
<p>A logical value indicating whether NA values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="tss_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This should be an unquoted column name that evaluates to a numeric column in data. For _vec() functions, a numeric vector.</p>
</td></tr>
<tr><td><code id="tss_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either &quot;first&quot; or &quot;second&quot; to specify which level of truth to consider as the &quot;event&quot;. This argument is only applicable when estimator = &quot;binary&quot;. The default is &quot;first&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>sensitivity</em>+<em>specificity</em> +1
</p>
<p>This function is a wrapper around <code><a href="yardstick.html#topic+j_index">yardstick::j_index()</a></code>, another name for the
same quantity. Note that this function takes the classes as predicted by the
model without any calibration (i.e. making a split at 0.5 probability). This
is usually not the metric used for Species Distribution Models, where the
threshold is recalibrated to maximise TSS; for that purpose, use <code><a href="#topic+tss_max">tss_max()</a></code>.
</p>


<h3>Value</h3>

<p>A tibble with columns .metric, .estimator, and .estimate and 1 row of values.
For grouped data frames, the number of rows returned will be the same as the
number of groups.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
tss(two_class_example, truth, predicted)
# Multiclass
library(dplyr)
data(hpc_cv)
# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  tss(obs, pred)
</code></pre>

<hr>
<h2 id='tss_max'>Maximum TSS - True Skill Statistics</h2><span id='topic+tss_max'></span><span id='topic+tss_max.data.frame'></span><span id='topic+tss_max.sf'></span><span id='topic+tss_max_vec'></span>

<h3>Description</h3>

<p>The True Skills Statistic, which is defined as
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tss_max(data, ...)

## S3 method for class 'data.frame'
tss_max(
  data,
  truth,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = "first",
  case_weights = NULL
)

## S3 method for class 'sf'
tss_max(data, ...)

tss_max_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  event_level = "first",
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tss_max_+3A_data">data</code></td>
<td>
<p>Either a data.frame containing the columns specified by the truth
and estimate arguments, or a table/matrix where the true class
results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="tss_max_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more dplyr selector functions to choose which variables contain the class probabilities. If truth is binary, only 1 column should be selected, and it should correspond to the value of event_level. Otherwise, there should be as many columns as factor levels of truth and the ordering of the columns should be the same as the factor levels of truth.</p>
</td></tr>
<tr><td><code id="tss_max_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results (that is a factor). This should be an unquoted column name although this argument is passed by expression and supports quasiquotation (you can unquote column names). For _vec() functions, a factor vector.</p>
</td></tr>
<tr><td><code id="tss_max_+3A_estimator">estimator</code></td>
<td>
<p>One of &quot;binary&quot;, &quot;hand_till&quot;, &quot;macro&quot;, or &quot;macro_weighted&quot; to specify the type of averaging to be done. &quot;binary&quot; is only relevant for the two class case. The others are general methods for calculating multiclass metrics. The default will automatically choose &quot;binary&quot; if truth is binary, &quot;hand_till&quot; if truth has &gt;2 levels and case_weights isn't specified, or &quot;macro&quot; if truth has &gt;2 levels and case_weights is specified (in which case &quot;hand_till&quot; isn't well-defined).</p>
</td></tr>
<tr><td><code id="tss_max_+3A_na_rm">na_rm</code></td>
<td>
<p>A logical value indicating whether NA values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="tss_max_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either &quot;first&quot; or &quot;second&quot; to specify which level of truth to consider as the &quot;event&quot;. This argument is only applicable when estimator = &quot;binary&quot;. The default uses an internal helper that generally defaults to &quot;first&quot;</p>
</td></tr>
<tr><td><code id="tss_max_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This should be an unquoted column name that evaluates to a numeric column in data. For _vec() functions, a numeric vector.</p>
</td></tr>
<tr><td><code id="tss_max_+3A_estimate">estimate</code></td>
<td>
<p>If truth is binary, a numeric vector of class probabilities corresponding to the &quot;relevant&quot; class. Otherwise, a matrix with as many columns as factor levels of truth. It is assumed that these are in the same order as the levels of truth.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>sensitivity</em>+<em>specificity</em> +1
</p>
<p>This function calibrates the probability threshold to classify presences to maximise the
TSS.
</p>
<p>There is no multiclass version of this function, it only operates on binary
predictions (e.g. presences and absences in SDMs).
</p>


<h3>Value</h3>

<p>A tibble with columns .metric, .estimator, and .estimate and 1 row of values.
For grouped data frames, the number of rows returned will be the same as the
number of groups.
</p>


<h3>See Also</h3>

<p>Other class probability metrics: 
<code><a href="#topic+boyce_cont">boyce_cont</a>()</code>,
<code><a href="#topic+kap_max">kap_max</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tss_max(two_class_example, truth, Class1)

</code></pre>

<hr>
<h2 id='y2d'>Convert a time interval from years to days</h2><span id='topic+y2d'></span>

<h3>Description</h3>

<p>This function takes takes a time interval in years and converts into days,
the unit commonly used in time operations in <code>R</code>. The simple conversion
x * 365 does not work for large number of years, due to the presence of
leap years.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>y2d(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="y2d_+3A_x">x</code></td>
<td>
<p>the number of years of the interval</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>difftime</code> object (in days)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y2d(1)
y2d(1000)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
