<!DOCTYPE html><html><head><title>Help for package NbClust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {NbClust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#NbClust'><p>NbClust Package for determining the best number of clusters</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Determining the Best Number of Clusters in a Data Set</td>
</tr>
<tr>
<td>Version:</td>
<td>3.0.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-04-13</td>
</tr>
<tr>
<td>Author:</td>
<td>Malika Charrad and Nadia Ghazzali and Veronique Boiteau and Azam Niknafs  </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Malika Charrad &lt;malika.charrad.1@ulaval.ca&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>It provides 30 indexes for determining the optimal number of clusters in a data set and offers the best clustering scheme from different results to the user. </td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://sites.google.com/site/malikacharrad/research/nbclust-package">https://sites.google.com/site/malikacharrad/research/nbclust-package</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-02 10:08:05 UTC; hornik</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-02 13:01:42 UTC</td>
</tr>
</table>
<hr>
<h2 id='NbClust'>NbClust Package for determining the best number of clusters</h2><span id='topic+NbClust'></span>

<h3>Description</h3>

<p><code>NbClust</code> package provides 30 indices for determining the number of clusters and proposes to user the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NbClust(data = NULL, diss = NULL, distance = "euclidean", min.nc = 2, max.nc = 15, 

method = NULL, index = "all", alphaBeale = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NbClust_+3A_data">data</code></td>
<td>
<p>matrix or dataset.</p>
</td></tr>
<tr><td><code id="NbClust_+3A_diss">diss</code></td>
<td>
<p>dissimilarity matrix to be used. By default, <code>diss=NULL</code>, but if it is replaced by a dissimilarity matrix, distance should be &quot;NULL&quot;.
</p>
</td></tr>
<tr><td><code id="NbClust_+3A_distance">distance</code></td>
<td>
<p>the distance measure to be used to compute the dissimilarity matrix. This must be one of: &quot;euclidean&quot;, &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot;, &quot;minkowski&quot; or &quot;NULL&quot;. By default, distance=&quot;euclidean&quot;. 
If the distance is &quot;NULL&quot;, the dissimilarity matrix (diss) should be given by the user. If distance is not &quot;NULL&quot;, the dissimilarity matrix should be &quot;NULL&quot;.</p>
</td></tr>
<tr><td><code id="NbClust_+3A_min.nc">min.nc</code></td>
<td>
<p>minimal number of clusters, between 1  and (number of objects - 1)</p>
</td></tr>
<tr><td><code id="NbClust_+3A_max.nc">max.nc</code></td>
<td>
<p>maximal number of clusters, between 2 and (number of objects - 1), greater or equal to min.nc. By default, max.nc=15.</p>
</td></tr>
<tr><td><code id="NbClust_+3A_method">method</code></td>
<td>
<p>the cluster analysis method to be used. This should be one of: &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot;, &quot;centroid&quot;, &quot;kmeans&quot;.</p>
</td></tr>
<tr><td><code id="NbClust_+3A_index">index</code></td>
<td>
<p>the index to be calculated. This should be one of : &quot;kl&quot;, &quot;ch&quot;, &quot;hartigan&quot;, &quot;ccc&quot;, &quot;scott&quot;, &quot;marriot&quot;, &quot;trcovw&quot;, &quot;tracew&quot;, &quot;friedman&quot;,  &quot;rubin&quot;, &quot;cindex&quot;,  &quot;db&quot;, &quot;silhouette&quot;, &quot;duda&quot;, &quot;pseudot2&quot;, &quot;beale&quot;,  &quot;ratkowsky&quot;, &quot;ball&quot;, &quot;ptbiserial&quot;, &quot;gap&quot;, &quot;frey&quot;, &quot;mcclain&quot;, &quot;gamma&quot;, &quot;gplus&quot;, &quot;tau&quot;, &quot;dunn&quot;, &quot;hubert&quot;, &quot;sdindex&quot;, &quot;dindex&quot;, &quot;sdbw&quot;, &quot;all&quot; (all indices except GAP, Gamma, Gplus and Tau), &quot;alllong&quot; (all indices with Gap, Gamma, Gplus and Tau included).</p>
</td></tr>
<tr><td><code id="NbClust_+3A_alphabeale">alphaBeale</code></td>
<td>
<p>significance value for Beale's index.</p>
</td></tr>
</table>


<h3>Details</h3>


<ol>
<li> <p><b>Notes on the &quot;Distance&quot; argument<br /></b>  
The following distance measures are written for two vectors <b>x</b> and <b>y</b>. They are used when the data is a <strong>d</strong>-dimensional vector arising from measuring <b>d</b> characteristics on each of <b>n</b> objects or individuals.
</p>

<ul>
<li> <p><strong>Euclidean distance</strong> : Usual square distance between the two vectors (2 norm).
</p>
<p style="text-align: center;"><code class="reqn">d(x,y)=\left(\sum_{j=1}^{d}\left(x_{j}-y_{j}\right)^{2}\right) ^{\frac{1}{2}}</code>
</p>

</li>
<li> <p><strong>Maximum distance</strong>: Maximum distance between two components of <b>x</b> and <b>y</b> (supremum norm).
</p>
<p style="text-align: center;"><code class="reqn">d(x,y)=\sup_{1\leq j\leq d}\left|x_{j}-y_{j}\right|</code>
</p>

</li>
<li> <p><strong>Manhattan distance</strong> : Absolute distance between the two vectors (1 norm).
</p>
<p style="text-align: center;"><code class="reqn">d(x,y)=\sum_{j=1}^{d}\left|x_{j}-y_{j}\right|</code>
</p>

</li>
<li> <p><strong>Canberra distance</strong> : Terms with zero numerator and denominator are omitted from the sum and treated as if the values were missing.
</p>
<p style="text-align: center;"><code class="reqn">d(x,y)=\sum_{j=1}^{d}\frac{\left|x_{j}-y_{j}\right|}{\left|x_{j}\right|+\left|y_{j}\right|}</code>
</p>

</li>
<li> <p><strong>Binary distance</strong> : The vectors are regarded as binary bits, so non-zero elements are &quot;on&quot; and zero elements are &quot;off&quot;. The distance is the proportion of bits in which only one is on amongst those in which at least one is on.
</p>
</li>
<li> <p><strong>Minkowski distance</strong> : The <b>p</b> norm, the <code class="reqn">p^{th}</code> root of the sum of the <code class="reqn">p^{th}</code> powers of the differences of the components.
</p>
<p style="text-align: center;"><code class="reqn">d(x,y)=\left(\sum_{j=1}^{d}\left|x_{j}-y_{j}\right|^{p}\right) ^{\frac{1}{p}}</code>
</p>

</li></ul>

</li>
<li> <p><b>Notes on the &quot;method&quot; argument<br /></b>
The following aggregation methods are available in this package. 
</p>

<ul>
<li> <p><b>Ward</b> : Ward method minimizes the total within-cluster variance. At each step the pair of clusters with minimum cluster distance are merged. To implement this method, at each step find the pair of clusters that leads to minimum increase in total within-cluster variance after merging.
Two different algorithms are found in the literature for Ward clustering. The one used by option &quot;ward.D&quot; (equivalent to the only Ward option &quot;ward&quot; in R versions &lt;= 3.0.3) does not implement Ward's (1963) clustering criterion, whereas option &quot;ward.D2&quot; implements that criterion (Murtagh and Legendre 2013). With the latter, the dissimilarities are squared before cluster updating.
</p>
</li>
<li> <p><b>Single</b> : The distance <code class="reqn">D_{ij}</code> between two clusters <code class="reqn">C_{i}</code> and <code class="reqn">C_{j}</code> is the minimum distance between two points <code class="reqn">x</code> and <code class="reqn">y</code>, with <code class="reqn">x \in C_{i}, y \in C_{j}</code>. 
</p>
<p style="text-align: center;"><code class="reqn">D_{ij}=min_{x\in C_{i}, y\in C_{j}}d(x,y)</code>
</p>

<p>A drawback of this method is the so-called chaining phenomenon: clusters may be forced together due to single elements being close to each other, even though many of the elements in each cluster may be very distant to each other.
</p>
</li>
<li> <p><b>Complete</b> : The distance <code class="reqn">D_{ij}</code> between two clusters <code class="reqn">C_{i}</code> and <code class="reqn">C_{j}</code> is the maximum distance between two points <code class="reqn">x</code> and <code class="reqn">y</code>, with <code class="reqn">x \in C_{i}, y \in C_{j}</code>.
</p>
<p style="text-align: center;"><code class="reqn">D_{ij}=max_{x\in C_{i}, y\in C_{j}}d(x,y)</code>
</p>

</li>
<li> <p><b>Average</b> : The distance <code class="reqn">D_{ij}</code> between two clusters <code class="reqn">C_{i}</code> and <code class="reqn">C_{j}</code> is the mean of the distances between the pair of points x and y, where <code class="reqn">x \in C_{i}, y \in C_{j}</code>.
</p>
<p style="text-align: center;"><code class="reqn">D_{ij}=sum_{x\in C_{i}, y\in C_{j}}\frac{d(x,y)}{n_{i}\times n_{j}}</code>
</p>
<p> where <code class="reqn">n_{i}</code> and <code class="reqn">n_{j}</code> are respectively the number of elements in clusters <code class="reqn">C_{i}</code> and <code class="reqn">C_{j}</code>. 
This method has the tendency to form clusters with the same variance and, in particular, small variance.  
</p>
</li>
<li> <p><b>McQuitty</b> : The distance between clusters <code class="reqn">C_{i}</code> and <code class="reqn">C_{j}</code> is the weighted mean of the between-cluster dissimilarities:
</p>
<p style="text-align: center;"><code class="reqn">D_{ij}=\left(D_{ik}+D_{il}\right)/2</code>
</p>

<p>where cluster <code class="reqn">C_{j}</code> is formed from the aggregation of clusters <code class="reqn">C_{k}</code> and <code class="reqn">C_{l}</code>.  
</p>
</li>
<li> <p><b>Median</b> : The distance <code class="reqn">D_{ij}</code> between two clusters <code class="reqn">C_{i}</code> and <code class="reqn">C_{j}</code> is given by the following formula:  
</p>
<p style="text-align: center;"><code class="reqn">D_{ij}=\frac{(D_{ik}+D_{il})}{2}-\frac{D_{kl}}{4}</code>
</p>
  
<p>where cluster <code class="reqn">C_{j}</code> is formed by the aggregation of clusters <code class="reqn">C_{k}</code> and <code class="reqn">C_{l}</code>.    
</p>
</li>
<li> <p><b>Centroid</b> : The distance <code class="reqn">D_{ij}</code> between two clusters <code class="reqn">C_{i}</code> and <code class="reqn">C_{j}</code> is the squared euclidean distance between the gravity centers of the two clusters, i.e. between the mean vectors of the two clusters, <code class="reqn">\bar{x_{i}}</code> and <code class="reqn">\bar{x_{j}}</code> respectively.
</p>
<p style="text-align: center;"><code class="reqn">D_{ij}=\left\Vert \bar{x_{i}}-\bar{x_{j}}\right\Vert ^{2}</code>
</p>

<p>This method is more robust than others in terms of isolated points.   
</p>
</li>
<li> <p><b>Kmeans</b> : This method is said to be a reallocation method. Here is the general principle:
</p>

<ol>
<li><p> Select as many points as the number of desired clusters to create initial centers.
</p>
</li>
<li><p> Each observation is then associated with the nearest center to create temporary clusters.
</p>
</li>
<li><p> The gravity centers of each temporary cluster is calculated and these become the new clusters centers.
</p>
</li>
<li><p> Each observation is reallocated to the cluster which has the closest center.
</p>
</li>
<li><p> This procedure is iterated until convergence.
</p>
</li></ol>

</li></ul>

</li>
<li> <p><b>Notes on the &quot;Index&quot; argument<br /><br /></b> 
</p>
<p>The table below summarizes indices implemented in NbClust and the criteria used to select the optimal number of clusters. <br /> 
</p>

<table>
<tr>
 <td style="text-align: left;">
      <b>Index in NbClust</b>         </td><td style="text-align: left;">      <b>Optimal number of clusters</b> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
      
1.  "kl" or "all" or "alllong"        </td><td style="text-align: left;"> Maximum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Krzanowski and Lai 1988)         </td>
</tr>
<tr>
 <td style="text-align: left;">
2.  "ch" or "all" or "alllong"        </td><td style="text-align: left;"> Maximum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Calinski and Harabasz 1974)      </td>
</tr>
<tr>
 <td style="text-align: left;">
3.  "hartigan" or "all" or "alllong"  </td><td style="text-align: left;"> Maximum difference between </td>
</tr>
<tr>
 <td style="text-align: left;">
     (Hartigan 1975)                  </td><td style="text-align: left;"> hierarchy levels of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
4.  "ccc" or "all" or "alllong"       </td><td style="text-align: left;"> Maximum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Sarle 1983)                                                      </td>
</tr>
<tr>
 <td style="text-align: left;"> 
5.  "scott" or "all" or "alllong"     </td><td style="text-align: left;"> Maximum difference between </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Scott and Symons 1971)           </td><td style="text-align: left;">   hierarchy levels of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
6.  "marriot" or "all" or "alllong"   </td><td style="text-align: left;"> Max. value of second differences</td>
</tr>
<tr>
 <td style="text-align: left;">
    (Marriot 1971)                    </td><td style="text-align: left;">  between levels of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
7.  "trcovw" or "all" or "alllong"    </td><td style="text-align: left;"> Maximum difference between </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Milligan and Cooper 1985)        </td><td style="text-align: left;"> hierarchy levels of the index </td>
</tr>
<tr>
 <td style="text-align: left;">                            
8.  "tracew" or "all" or "alllong"    </td><td style="text-align: left;"> Maximum value of absolute second </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Milligan and Cooper 1985)        </td><td style="text-align: left;"> differences between levels of the index</td>
</tr>
<tr>
 <td style="text-align: left;">
9.  "friedman" or "all" or "alllong"  </td><td style="text-align: left;"> Maximum difference between </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Friedman and Rubin 1967)         </td><td style="text-align: left;"> hierarchy levels of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
10.  "rubin" or "all" or "alllong"    </td><td style="text-align: left;"> Minimum value of second differences </td>
</tr>
<tr>
 <td style="text-align: left;">  
    (Friedman and Rubin 1967)         </td><td style="text-align: left;">        between levels of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
11. "cindex" or "all" or "alllong"    </td><td style="text-align: left;"> Minimum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Hubert and Levin 1976)                                           </td>
</tr>
<tr>
 <td style="text-align: left;">   
12.  "db" or "all" or "alllong"       </td><td style="text-align: left;"> Minimum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
     (Davies and Bouldin 1979)                                        </td>
</tr>
<tr>
 <td style="text-align: left;">    
13.   "silhouette" or "all" or "alllong" </td><td style="text-align: left;"> Maximum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Rousseeuw 1987)                                                    </td>
</tr>
<tr>
 <td style="text-align: left;">  
14. "duda" or "all" or "alllong"      </td><td style="text-align: left;"> Smallest <code class="reqn">n_{c}</code> such that index &gt; criticalValue </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Duda and Hart 1973)                                                 </td>
</tr>
<tr>
 <td style="text-align: left;">    
15. "pseudot2" or "all" or "alllong"  </td><td style="text-align: left;"> Smallest <code class="reqn">n_{c}</code> such that index &lt; criticalValue </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Duda and Hart 1973)                                                   </td>
</tr>
<tr>
 <td style="text-align: left;"> 
16. "beale" or "all" or "alllong"      </td><td style="text-align: left;"> <code class="reqn">n_{c}</code> such that critical value of the index &gt;= alpha </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Beale 1969)                                                           </td>
</tr>
<tr>
 <td style="text-align: left;">  
17. "ratkowsky" or "all" or "alllong" </td><td style="text-align: left;"> Maximum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Ratkowsky and Lance 1978)                                          </td>
</tr>
<tr>
 <td style="text-align: left;"> 
18. "ball" or "all" or "alllong"      </td><td style="text-align: left;"> Maximum difference between hierarchy </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Ball and Hall 1965)              </td><td style="text-align: left;">  levels of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
19. "ptbiserial" or "all" or "alllong" </td><td style="text-align: left;"> Maximum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Milligan 1980, 1981)                                              </td>
</tr>
<tr>
 <td style="text-align: left;"> 
20. "gap" or "alllong"                 </td><td style="text-align: left;"> Smallest <code class="reqn">n_{c}</code> such that criticalValue &gt;= 0 </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Tibshirani et al. 2001)                                                                  </td>
</tr>
<tr>
 <td style="text-align: left;">   
21.  "frey" or "all" or "alllong"   </td><td style="text-align: left;"> the cluster level before that index value &lt; 1.00 </td>
</tr>
<tr>
 <td style="text-align: left;">
     (Frey and Van Groenewoud 1972)                                                       </td>
</tr>
<tr>
 <td style="text-align: left;">
22. "mcclain" or "all" or "alllong"</td><td style="text-align: left;"> Minimum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (McClain and Rao 1975)  </td>
</tr>
<tr>
 <td style="text-align: left;">
23. "gamma" or "alllong" </td><td style="text-align: left;"> Maximum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Baker and Hubert 1975)     </td>
</tr>
<tr>
 <td style="text-align: left;">
24. "gplus" or "alllong"</td><td style="text-align: left;"> Minimum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Rohlf 1974) (Milligan 1981)  </td>
</tr>
<tr>
 <td style="text-align: left;">
25. "tau" or "alllong" </td><td style="text-align: left;"> Maximum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Rohlf 1974) (Milligan 1981)  </td>
</tr>
<tr>
 <td style="text-align: left;">
26. "dunn" or "all" or "alllong" </td><td style="text-align: left;"> Maximum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Dunn 1974)       </td>
</tr>
<tr>
 <td style="text-align: left;">
27. "hubert" or "all" or "alllong" </td><td style="text-align: left;"> Graphical method </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Hubert and Arabie 1985)  </td>
</tr>
<tr>
 <td style="text-align: left;">
28. "sdindex" or "all" or "alllong" </td><td style="text-align: left;"> Minimum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Halkidi et al. 2000)   </td>
</tr>
<tr>
 <td style="text-align: left;">
29. "dindex" or "all" or "alllong" </td><td style="text-align: left;"> Graphical method</td>
</tr>
<tr>
 <td style="text-align: left;">
    (Lebart et al. 2000)    </td>
</tr>
<tr>
 <td style="text-align: left;">
30. "sdbw" or "all" or "alllong" </td><td style="text-align: left;"> Minimum value of the index </td>
</tr>
<tr>
 <td style="text-align: left;">
    (Halkidi and Vazirgiannis 2001) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

</li></ol>



<h3>Value</h3>

<table>
<tr><td><code>All.index</code></td>
<td>
<p>Values of indices for each partition of the dataset obtained with a number of
clusters between min.nc and max.nc.</p>
</td></tr>
<tr><td><code>All.CriticalValues</code></td>
<td>
<p> Critical values of some indices for each partition obtained with a number of clusters between min.nc and max.nc. </p>
</td></tr>
<tr><td><code>Best.nc</code></td>
<td>
<p> Best number of clusters proposed by each index and the corresponding index value.</p>
</td></tr>
<tr><td><code>Best.partition</code></td>
<td>
<p>Partition that corresponds to the best number of clusters</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Malika Charrad, Nadia Ghazzali, Veronique Boiteau and Azam Niknafs  
</p>


<h3>References</h3>

<p>Charrad M., Ghazzali N., Boiteau V., Niknafs A. (2014). &quot;NbClust: An R Package for Determining the Relevant Number of Clusters in a Data Set.&quot;,
&quot;Journal of Statistical Software, 61(6), 1-36.&quot;, &quot;URL http://www.jstatsoft.org/v61/i06/&quot;.<br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## DATA MATRIX IS GIVEN

## A 2-dimensional example

set.seed(1)
x&lt;-rbind(matrix(rnorm(100,sd=0.1),ncol=2),
         matrix(rnorm(100,mean=1,sd=0.2),ncol=2),
         matrix(rnorm(100,mean=5,sd=0.1),ncol=2),
         matrix(rnorm(100,mean=7,sd=0.2),ncol=2))
         
res&lt;-NbClust(x, distance = "euclidean", min.nc=2, max.nc=8, 
            method = "complete", index = "ch")
            
res$All.index
res$Best.nc
res$Best.partition

## A 5-dimensional example

set.seed(1)
x&lt;-rbind(matrix(rnorm(150,sd=0.3),ncol=5),
          matrix(rnorm(150,mean=3,sd=0.2),ncol=5),
          matrix(rnorm(150,mean=1,sd=0.1),ncol=5),
          matrix(rnorm(150,mean=6,sd=0.3),ncol=5),
          matrix(rnorm(150,mean=9,sd=0.3),ncol=5))

res&lt;-NbClust(x, distance = "euclidean", min.nc=2, max.nc=10, 
            method = "ward.D", index = "all")

res$All.index
res$Best.nc
res$All.CriticalValues
res$Best.partition

## A real data example

data&lt;-iris[,-c(5)] 
res&lt;-NbClust(data, diss=NULL, distance = "euclidean", min.nc=2, max.nc=6, 
            method = "ward.D2", index = "kl") 
res$All.index
res$Best.nc
res$Best.partition
            
res&lt;-NbClust(data, diss=NULL, distance = "euclidean", min.nc=2, max.nc=6, 
            method = "kmeans", index = "hubert")
res$All.index


res&lt;-NbClust(data, diss=NULL, distance = "manhattan", min.nc=2, max.nc=6, 
            method = "complete", index = "all") 
res$All.index
res$Best.nc
res$All.CriticalValues
res$Best.partition

## Examples with a dissimilarity matrix

## Data matrix is given

set.seed(1)
x&lt;-rbind(matrix(rnorm(150,sd=0.3),ncol=3),
          matrix(rnorm(150,mean=3,sd=0.2),ncol=3),
          matrix(rnorm(150,mean=5,sd=0.3),ncol=3))
diss_matrix&lt;- dist(x, method = "euclidean", diag=FALSE)
res&lt;-NbClust(x, diss=diss_matrix, distance = NULL, min.nc=2, max.nc=6, 
            method = "ward.D", index = "ch")  
res$All.index
res$Best.nc
res$Best.partition

data&lt;-iris[,-c(5)]
diss_matrix&lt;- dist(data, method = "euclidean", diag=FALSE)
NbClust(data, diss=diss_matrix, distance = NULL, min.nc=2, max.nc=6, 
            method = "ward.D2", index = "all")   
res$All.index
res$Best.nc
res$All.CriticalValues
res$Best.partition      
    
set.seed(1)    
x&lt;-rbind(matrix(rnorm(20,sd=0.1),ncol=2),
         matrix(rnorm(20,mean=1,sd=0.2),ncol=2),
         matrix(rnorm(20,mean=5,sd=0.1),ncol=2),
         matrix(rnorm(20,mean=7,sd=0.2),ncol=2))
diss_matrix&lt;- dist(x, method = "euclidean", diag=FALSE)
res&lt;-NbClust(x, diss=diss_matrix, distance = NULL, min.nc=2, max.nc=6, 
            method = "ward.D2", index = "alllong")
res$All.index
res$Best.nc
res$All.CriticalValues
res$Best.partition

## Data matrix is not available. Only the dissimilarity matrix is given
## In this case, only these indices can be computed : frey, mcclain, cindex, silhouette and dunn

res&lt;-NbClust(diss=diss_matrix, distance = NULL, min.nc=2, max.nc=6, 
            method = "ward.D2", index = "silhouette")
res$All.index
res$Best.nc
res$All.CriticalValues
res$Best.partition

            
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
