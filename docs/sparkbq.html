<!DOCTYPE html><html lang="en"><head><title>Help for package sparkbq</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparkbq}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bigquery_defaults'><p>Google BigQuery Default Settings</p></a></li>
<li><a href='#default_bigquery_type'><p>Default BigQuery import/export type</p></a></li>
<li><a href='#default_billing_project_id'><p>Default Google BigQuery Billing Project ID</p></a></li>
<li><a href='#default_dataset_location'><p>Default Google BigQuery Dataset Location</p></a></li>
<li><a href='#default_gcs_bucket'><p>Default Google BigQuery GCS Bucket</p></a></li>
<li><a href='#default_service_account_key_file'><p>Default Google BigQuery Service Account Key File</p></a></li>
<li><a href='#spark_read_bigquery'><p>Reading data from Google BigQuery</p></a></li>
<li><a href='#spark_write_bigquery'><p>Writing data to Google BigQuery</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Google 'BigQuery' Support for 'sparklyr'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www.mirai-solutions.com">http://www.mirai-solutions.com</a>,
<a href="https://github.com/miraisolutions/sparkbq">https://github.com/miraisolutions/sparkbq</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/miraisolutions/sparkbq/issues">https://github.com/miraisolutions/sparkbq/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>A 'sparklyr' extension package providing an integration with Google 'BigQuery'.
  It supports direct import/export where records are directly streamed from/to 'BigQuery'.
  In addition, data may be imported/exported via intermediate data extracts on Google 'Cloud Storage'.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>sparklyr (&ge; 0.7.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>dplyr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> | file LICENSE</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Spark (&gt;= 2.2.x)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-12-18 17:03:34 UTC; simon</td>
</tr>
<tr>
<td>Author:</td>
<td>Mirai Solutions GmbH [aut],
  Martin Studer [cre],
  Nicola Lambiase [ctb],
  Omer Demirel [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Martin Studer &lt;martin.studer@mirai-solutions.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-12-18 18:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bigquery_defaults'>Google BigQuery Default Settings</h2><span id='topic+bigquery_defaults'></span>

<h3>Description</h3>

<p>Sets default values for several Google BigQuery related settings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigquery_defaults(billingProjectId, gcsBucket, datasetLocation = "US",
  serviceAccountKeyFile = NULL, type = "direct")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bigquery_defaults_+3A_billingprojectid">billingProjectId</code></td>
<td>
<p>Default Google Cloud Platform project ID for billing purposes.
This is the project on whose behalf to perform BigQuery operations.</p>
</td></tr>
<tr><td><code id="bigquery_defaults_+3A_gcsbucket">gcsBucket</code></td>
<td>
<p>Google Cloud Storage (GCS) bucket to use for storing temporary files.
Temporary files are used when importing through BigQuery load jobs and exporting through
BigQuery extraction jobs (i.e. when using data extracts such as Parquet, Avro, ORC, ...).
The service account specified in <code>serviceAccountKeyFile</code> needs to be given appropriate
rights. This should be the name of an existing storage bucket.</p>
</td></tr>
<tr><td><code id="bigquery_defaults_+3A_datasetlocation">datasetLocation</code></td>
<td>
<p>Geographic location where newly created datasets should reside.
&quot;EU&quot; or &quot;US&quot;. Defaults to &quot;US&quot;.</p>
</td></tr>
<tr><td><code id="bigquery_defaults_+3A_serviceaccountkeyfile">serviceAccountKeyFile</code></td>
<td>
<p>Google Cloud service account key file to use for authentication
with Google Cloud services. The use of service accounts is highly recommended. Specifically,
the service account will be used to interact with BigQuery and Google Cloud Storage (GCS).
If not specified, Google application default credentials (ADC) will be used, which is the default.</p>
</td></tr>
<tr><td><code id="bigquery_defaults_+3A_type">type</code></td>
<td>
<p>Default BigQuery import/export type to use. Options include &quot;direct&quot;,
&quot;parquet&quot;, &quot;avro&quot;, &quot;orc&quot;, &quot;json&quot; and &quot;csv&quot;. Defaults to &quot;direct&quot;.
Please note that only &quot;direct&quot; and &quot;avro&quot; are supported for both importing and exporting. <br />
&quot;csv&quot; and &quot;json&quot; are not recommended due to their lack of type safety.
</p>
<p>See the table below for supported type and import/export combinations.
</p>

<table>
<tr>
 <td style="text-align: left;">
                                         </td><td style="text-align: center;"> Direct </td><td style="text-align: center;"> Parquet </td><td style="text-align: center;"> Avro </td><td style="text-align: center;"> ORC </td><td style="text-align: center;"> JSON </td><td style="text-align: center;"> CSV  </td>
</tr>
<tr>
 <td style="text-align: left;">
  Import to Spark (export from BigQuery) </td><td style="text-align: center;"> X      </td><td style="text-align: center;">         </td><td style="text-align: center;"> X    </td><td style="text-align: center;">     </td><td style="text-align: center;"> X    </td><td style="text-align: center;"> X    </td>
</tr>
<tr>
 <td style="text-align: left;">
  Export from Spark (import to BigQuery) </td><td style="text-align: center;"> X      </td><td style="text-align: center;"> X       </td><td style="text-align: center;"> X    </td><td style="text-align: center;"> X   </td><td style="text-align: center;">      </td><td style="text-align: center;">      </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> of set options with previous values.
</p>


<h3>References</h3>

<p><a href="https://github.com/miraisolutions/spark-bigquery">https://github.com/miraisolutions/spark-bigquery</a>
<a href="https://cloud.google.com/bigquery/pricing">https://cloud.google.com/bigquery/pricing</a>
<a href="https://cloud.google.com/bigquery/docs/dataset-locations">https://cloud.google.com/bigquery/docs/dataset-locations</a>
<a href="https://cloud.google.com/bigquery/docs/authentication/service-account-file">https://cloud.google.com/bigquery/docs/authentication/service-account-file</a>
<a href="https://cloud.google.com/docs/authentication/">https://cloud.google.com/docs/authentication/</a>
<a href="https://cloud.google.com/bigquery/docs/authentication/">https://cloud.google.com/bigquery/docs/authentication/</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-orc">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-orc</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spark_read_bigquery">spark_read_bigquery</a></code>, <code><a href="#topic+spark_write_bigquery">spark_write_bigquery</a></code>,
<code><a href="#topic+default_billing_project_id">default_billing_project_id</a></code>, <code><a href="#topic+default_gcs_bucket">default_gcs_bucket</a></code>,
<code><a href="#topic+default_dataset_location">default_dataset_location</a></code>
</p>

<hr>
<h2 id='default_bigquery_type'>Default BigQuery import/export type</h2><span id='topic+default_bigquery_type'></span>

<h3>Description</h3>

<p>Returns the default BigQuery import/export type. It defaults to
&quot;direct&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_bigquery_type()
</code></pre>


<h3>See Also</h3>

<p><code><a href="#topic+bigquery_defaults">bigquery_defaults</a></code>
</p>

<hr>
<h2 id='default_billing_project_id'>Default Google BigQuery Billing Project ID</h2><span id='topic+default_billing_project_id'></span>

<h3>Description</h3>

<p>Returns the default Google BigQuery billing project ID.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_billing_project_id()
</code></pre>


<h3>See Also</h3>

<p><code><a href="#topic+bigquery_defaults">bigquery_defaults</a></code>
</p>

<hr>
<h2 id='default_dataset_location'>Default Google BigQuery Dataset Location</h2><span id='topic+default_dataset_location'></span>

<h3>Description</h3>

<p>Returns the default Google BigQuery dataset location. It defaults
to &quot;US&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_dataset_location()
</code></pre>


<h3>References</h3>

<p><a href="https://cloud.google.com/bigquery/docs/dataset-locations">https://cloud.google.com/bigquery/docs/dataset-locations</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bigquery_defaults">bigquery_defaults</a></code>
</p>

<hr>
<h2 id='default_gcs_bucket'>Default Google BigQuery GCS Bucket</h2><span id='topic+default_gcs_bucket'></span>

<h3>Description</h3>

<p>Returns the default Google BigQuery GCS bucket.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_gcs_bucket()
</code></pre>


<h3>See Also</h3>

<p><code><a href="#topic+bigquery_defaults">bigquery_defaults</a></code>
</p>

<hr>
<h2 id='default_service_account_key_file'>Default Google BigQuery Service Account Key File</h2><span id='topic+default_service_account_key_file'></span>

<h3>Description</h3>

<p>Returns the default service account key file to use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_service_account_key_file()
</code></pre>


<h3>References</h3>

<p><a href="https://cloud.google.com/bigquery/docs/authentication/service-account-file">https://cloud.google.com/bigquery/docs/authentication/service-account-file</a>
<a href="https://cloud.google.com/docs/authentication/">https://cloud.google.com/docs/authentication/</a>
<a href="https://cloud.google.com/bigquery/docs/authentication/">https://cloud.google.com/bigquery/docs/authentication/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bigquery_defaults">bigquery_defaults</a></code>
</p>

<hr>
<h2 id='spark_read_bigquery'>Reading data from Google BigQuery</h2><span id='topic+spark_read_bigquery'></span>

<h3>Description</h3>

<p>This function reads data stored in a Google BigQuery table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_read_bigquery(sc, name,
  billingProjectId = default_billing_project_id(),
  projectId = billingProjectId, datasetId = NULL, tableId = NULL,
  sqlQuery = NULL, type = default_bigquery_type(),
  gcsBucket = default_gcs_bucket(),
  serviceAccountKeyFile = default_service_account_key_file(),
  additionalParameters = NULL, memory = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_read_bigquery_+3A_sc">sc</code></td>
<td>
<p><code><a href="sparklyr.html#topic+spark_connection">spark_connection</a></code> provided by sparklyr.</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_name">name</code></td>
<td>
<p>The name to assign to the newly generated table (see also
<code><a href="sparklyr.html#topic+spark_read_source">spark_read_source</a></code>).</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_billingprojectid">billingProjectId</code></td>
<td>
<p>Google Cloud Platform project ID for billing purposes.
This is the project on whose behalf to perform BigQuery operations.
Defaults to <code>default_billing_project_id()</code>.</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_projectid">projectId</code></td>
<td>
<p>Google Cloud Platform project ID of BigQuery dataset.
Defaults to <code>billingProjectId</code>.</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_datasetid">datasetId</code></td>
<td>
<p>Google BigQuery dataset ID (may contain letters, numbers and underscores).
Either both of <code>datasetId</code> and <code>tableId</code> or <code>sqlQuery</code> must be specified.</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_tableid">tableId</code></td>
<td>
<p>Google BigQuery table ID (may contain letters, numbers and underscores).
Either both of <code>datasetId</code> and <code>tableId</code> or <code>sqlQuery</code> must be specified.</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_sqlquery">sqlQuery</code></td>
<td>
<p>Google BigQuery SQL query. Either both of <code>datasetId</code> and <code>tableId</code>
or <code>sqlQuery</code> must be specified. The query must be specified in standard SQL
(SQL-2011). Legacy SQL is not supported. Tables are specified as
'&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;table_id&gt;'.</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_type">type</code></td>
<td>
<p>BigQuery import type to use. Options include &quot;direct&quot;, &quot;avro&quot;,
&quot;json&quot; and &quot;csv&quot;. Defaults to <code>default_bigquery_type()</code>.
See <a href="#topic+bigquery_defaults">bigquery_defaults</a> for more details about the supported types.</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_gcsbucket">gcsBucket</code></td>
<td>
<p>Google Cloud Storage (GCS) bucket to use for storing temporary files.
Temporary files are used when importing through BigQuery load jobs and exporting through
BigQuery extraction jobs (i.e. when using data extracts such as Parquet, Avro, ORC, ...).
The service account specified in <code>serviceAccountKeyFile</code> needs to be given appropriate
rights. This should be the name of an existing storage bucket.</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_serviceaccountkeyfile">serviceAccountKeyFile</code></td>
<td>
<p>Google Cloud service account key file to use for authentication
with Google Cloud services. The use of service accounts is highly recommended. Specifically,
the service account will be used to interact with BigQuery and Google Cloud Storage (GCS).</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_additionalparameters">additionalParameters</code></td>
<td>
<p>Additional spark-bigquery options. See
<a href="https://github.com/miraisolutions/spark-bigquery">https://github.com/miraisolutions/spark-bigquery</a> for more information.</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_memory">memory</code></td>
<td>
<p><code>logical</code> specifying whether data should be loaded eagerly into
memory, i.e. whether the table should be cached. Note that eagerly caching prevents
predicate pushdown (e.g. in conjunction with <code><a href="dplyr.html#topic+filter">filter</a></code>) and therefore
the default is <code>FALSE</code>. See also <code><a href="sparklyr.html#topic+spark_read_source">spark_read_source</a></code>.</p>
</td></tr>
<tr><td><code id="spark_read_bigquery_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="sparklyr.html#topic+spark_read_source">spark_read_source</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tbl_spark</code> which provides a <code>dplyr</code>-compatible reference to a
Spark DataFrame.
</p>


<h3>References</h3>

<p><a href="https://github.com/miraisolutions/spark-bigquery">https://github.com/miraisolutions/spark-bigquery</a>
<a href="https://cloud.google.com/bigquery/docs/datasets">https://cloud.google.com/bigquery/docs/datasets</a>
<a href="https://cloud.google.com/bigquery/docs/tables">https://cloud.google.com/bigquery/docs/tables</a>
<a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/">https://cloud.google.com/bigquery/docs/reference/standard-sql/</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv</a>
<a href="https://cloud.google.com/bigquery/pricing">https://cloud.google.com/bigquery/pricing</a>
<a href="https://cloud.google.com/bigquery/docs/dataset-locations">https://cloud.google.com/bigquery/docs/dataset-locations</a>
<a href="https://cloud.google.com/docs/authentication/">https://cloud.google.com/docs/authentication/</a>
<a href="https://cloud.google.com/bigquery/docs/authentication/">https://cloud.google.com/bigquery/docs/authentication/</a>
</p>


<h3>See Also</h3>

<p><code><a href="sparklyr.html#topic+spark_read_source">spark_read_source</a></code>, <code><a href="#topic+spark_write_bigquery">spark_write_bigquery</a></code>,
<code><a href="#topic+bigquery_defaults">bigquery_defaults</a></code>
</p>
<p>Other Spark serialization routines: <code><a href="#topic+spark_write_bigquery">spark_write_bigquery</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
config &lt;- spark_config()

sc &lt;- spark_connect(master = "local", config = config)

bigquery_defaults(
  billingProjectId = "&lt;your_billing_project_id&gt;",
  gcsBucket = "&lt;your_gcs_bucket&gt;",
  datasetLocation = "US",
  serviceAccountKeyFile = "&lt;your_service_account_key_file&gt;",
  type = "direct")

# Reading the public shakespeare data table
# https://cloud.google.com/bigquery/public-data/
# https://cloud.google.com/bigquery/sample-tables
shakespeare &lt;-
  spark_read_bigquery(
    sc,
    name = "shakespeare",
    projectId = "bigquery-public-data",
    datasetId = "samples",
    tableId = "shakespeare")

## End(Not run)
</code></pre>

<hr>
<h2 id='spark_write_bigquery'>Writing data to Google BigQuery</h2><span id='topic+spark_write_bigquery'></span>

<h3>Description</h3>

<p>This function writes data to a Google BigQuery table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_write_bigquery(data,
  billingProjectId = default_billing_project_id(),
  projectId = billingProjectId, datasetId, tableId,
  type = default_bigquery_type(), gcsBucket = default_gcs_bucket(),
  datasetLocation = default_dataset_location(),
  serviceAccountKeyFile = default_service_account_key_file(),
  additionalParameters = NULL, mode = "error", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_write_bigquery_+3A_data">data</code></td>
<td>
<p>Spark DataFrame to write to Google BigQuery.</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_billingprojectid">billingProjectId</code></td>
<td>
<p>Google Cloud Platform project ID for billing purposes.
This is the project on whose behalf to perform BigQuery operations.
Defaults to <code>default_billing_project_id()</code>.</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_projectid">projectId</code></td>
<td>
<p>Google Cloud Platform project ID of BigQuery dataset.
Defaults to <code>billingProjectId</code>.</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_datasetid">datasetId</code></td>
<td>
<p>Google BigQuery dataset ID (may contain letters, numbers and underscores).</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_tableid">tableId</code></td>
<td>
<p>Google BigQuery table ID (may contain letters, numbers and underscores).</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_type">type</code></td>
<td>
<p>BigQuery export type to use. Options include &quot;direct&quot;, &quot;parquet&quot;,
&quot;avro&quot;, &quot;orc&quot;. Defaults to <code>default_bigquery_type()</code>.
See <a href="#topic+bigquery_defaults">bigquery_defaults</a> for more details about the supported types.</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_gcsbucket">gcsBucket</code></td>
<td>
<p>Google Cloud Storage (GCS) bucket to use for storing temporary files.
Temporary files are used when importing through BigQuery load jobs and exporting through
BigQuery extraction jobs (i.e. when using data extracts such as Parquet, Avro, ORC, ...).
The service account specified in <code>serviceAccountKeyFile</code> needs to be given appropriate
rights. This should be the name of an existing storage bucket.</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_datasetlocation">datasetLocation</code></td>
<td>
<p>Geographic location where newly created datasets should reside.
&quot;EU&quot; or &quot;US&quot;. Defaults to &quot;US&quot;. Only needs to be specified if the dataset does not yet exist.
It is ignored if it is specified and the dataset already exists.</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_serviceaccountkeyfile">serviceAccountKeyFile</code></td>
<td>
<p>Google Cloud service account key file to use for authentication
with Google Cloud services. The use of service accounts is highly recommended. Specifically,
the service account will be used to interact with BigQuery and Google Cloud Storage (GCS).</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_additionalparameters">additionalParameters</code></td>
<td>
<p>Additional spark-bigquery options. See
<a href="https://github.com/miraisolutions/spark-bigquery">https://github.com/miraisolutions/spark-bigquery</a> for more information.</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_mode">mode</code></td>
<td>
<p>Specifies the behavior when data or table already exist. One of &quot;overwrite&quot;,
&quot;append&quot;, &quot;ignore&quot; or &quot;error&quot; (default).</p>
</td></tr>
<tr><td><code id="spark_write_bigquery_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="sparklyr.html#topic+spark_write_source">spark_write_source</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>NULL</code>. This is a side-effecting function.
</p>


<h3>References</h3>

<p><a href="https://github.com/miraisolutions/spark-bigquery">https://github.com/miraisolutions/spark-bigquery</a>
<a href="https://cloud.google.com/bigquery/docs/datasets">https://cloud.google.com/bigquery/docs/datasets</a>
<a href="https://cloud.google.com/bigquery/docs/tables">https://cloud.google.com/bigquery/docs/tables</a>
<a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/">https://cloud.google.com/bigquery/docs/reference/standard-sql/</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-orc">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-orc</a>
<a href="https://cloud.google.com/bigquery/pricing">https://cloud.google.com/bigquery/pricing</a>
<a href="https://cloud.google.com/bigquery/docs/dataset-locations">https://cloud.google.com/bigquery/docs/dataset-locations</a>
<a href="https://cloud.google.com/docs/authentication/">https://cloud.google.com/docs/authentication/</a>
<a href="https://cloud.google.com/bigquery/docs/authentication/">https://cloud.google.com/bigquery/docs/authentication/</a>
</p>


<h3>See Also</h3>

<p><code><a href="sparklyr.html#topic+spark_write_source">spark_write_source</a></code>, <code><a href="#topic+spark_read_bigquery">spark_read_bigquery</a></code>,
<code><a href="#topic+bigquery_defaults">bigquery_defaults</a></code>
</p>
<p>Other Spark serialization routines: <code><a href="#topic+spark_read_bigquery">spark_read_bigquery</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
config &lt;- spark_config()

sc &lt;- spark_connect(master = "local", config = config)

bigquery_defaults(
  billingProjectId = "&lt;your_billing_project_id&gt;",
  gcsBucket = "&lt;your_gcs_bucket&gt;",
  datasetLocation = "US",
  serviceAccountKeyFile = "&lt;your_service_account_key_file&gt;",
  type = "direct")

# Copy mtcars to Spark
spark_mtcars &lt;- dplyr::copy_to(sc, mtcars, "spark_mtcars", overwrite = TRUE)

spark_write_bigquery(
  data = spark_mtcars,
  datasetId = "&lt;your_dataset_id&gt;",
  tableId = "mtcars",
  mode = "overwrite")

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
