<!DOCTYPE html><html lang="en"><head><title>Help for package LOMAR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {LOMAR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#apply_transformation'><p>apply_transformation</p></a></li>
<li><a href='#ary2ps'><p>ary2ps</p></a></li>
<li><a href='#binning'><p>binning</p></a></li>
<li><a href='#circle_hough_transform'><p>Circle Hough transform</p></a></li>
<li><a href='#coloc_index'><p>coloc_index</p></a></li>
<li><a href='#costWd'><p>costWd</p></a></li>
<li><a href='#cpd'><p>cpd</p></a></li>
<li><a href='#crop_point_set'><p>crop_point_set</p></a></li>
<li><a href='#denoise'><p>denoise</p></a></li>
<li><a href='#dist_to_boundary'><p>dist_to_boundary</p></a></li>
<li><a href='#dist_to_line'><p>dist_to_line</p></a></li>
<li><a href='#downsample'><p>downsample</p></a></li>
<li><a href='#find_elbow'><p>find_elbow</p></a></li>
<li><a href='#Gaussian_Wd'><p>Gaussian_Wd</p></a></li>
<li><a href='#get_kernel_matrix'><p>get_kernel_matrix</p></a></li>
<li><a href='#get_persistence_diagrams'><p>get_persistence_diagrams</p></a></li>
<li><a href='#get_shape'><p>get_shape</p></a></li>
<li><a href='#get_surface_area'><p>get_surface_area</p></a></li>
<li><a href='#GMM_Wd'><p>GMM_Wd</p></a></li>
<li><a href='#gradientWd'><p>gradientWd</p></a></li>
<li><a href='#group_events'><p>group_events</p></a></li>
<li><a href='#icp'><p>icp</p></a></li>
<li><a href='#idx2rowcol'><p>idx2rowcol</p></a></li>
<li><a href='#img2ps'><p>img2ps</p></a></li>
<li><a href='#jrmpc'><p>jrmpc</p></a></li>
<li><a href='#local_densities'><p>local_densities</p></a></li>
<li><a href='#locprec2cov'><p>locprec2cov</p></a></li>
<li><a href='#locs_from_csv'><p>locs_from_csv</p></a></li>
<li><a href='#locs2ps'><p>locs2ps</p></a></li>
<li><a href='#multiple_registration'><p>multiple_registration</p></a></li>
<li><a href='#point_sets_from_locs'><p>point_sets_from_locs</p></a></li>
<li><a href='#point_sets_from_tiffs'><p>point_sets_from_tiffs</p></a></li>
<li><a href='#points_from_roi'><p>points_from_roi</p></a></li>
<li><a href='#points2img'><p>points2img</p></a></li>
<li><a href='#ps2ary'><p>ps2ary</p></a></li>
<li><a href='#pssk'><p>pssk</p></a></li>
<li><a href='#q2dr'><p>Get derivative of 3D rotation matrix from quaternion</p></a></li>
<li><a href='#q2r'><p>Convert quaternion to rotation matrix</p>
http://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation</a></li>
<li><a href='#restore_coordinates'><p>restore_coordinates</p></a></li>
<li><a href='#rotx'><p>rotx</p></a></li>
<li><a href='#roty'><p>roty</p></a></li>
<li><a href='#rotz'><p>rotz</p></a></li>
<li><a href='#scale_alpha_shape'><p>scale_alpha_shape</p></a></li>
<li><a href='#shape_features_3d'><p>shape_features_3d</p></a></li>
<li><a href='#sliced_Wd'><p>sliced_Wd</p></a></li>
<li><a href='#standardize_coordinates'><p>standardize_coordinates</p></a></li>
<li><a href='#tr'><p>tr</p></a></li>
<li><a href='#wgmmreg'><p>wgmmreg</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Localization Microscopy Data Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jean-Karim Heriche &lt;heriche@embl.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Read, register and compare point sets from single molecule localization microscopy.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://git.embl.de/heriche/lomar">https://git.embl.de/heriche/lomar</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, FNN, stats, data.table, parallel, doParallel, foreach,
proxy, reshape2, pracma, transport, RANN, ff, dbscan, EBImage,
tools, rhdf5, mclust, methods, abind, alphashape3d</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>BH (&ge; 1.78.0-0), Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++, gmp, fftw3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-26 17:17:52 UTC; heriche</td>
</tr>
<tr>
<td>Author:</td>
<td>Jean-Karim Heriche
    <a href="https://orcid.org/0000-0001-6867-9425"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [cre, aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-26 18:20:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='apply_transformation'>apply_transformation</h2><span id='topic+apply_transformation'></span>

<h3>Description</h3>

<p>Apply rotation and translation to a point set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>apply_transformation(X, R, t, s)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="apply_transformation_+3A_x">X</code></td>
<td>
<p>a point set as an N x D matrix</p>
</td></tr>
<tr><td><code id="apply_transformation_+3A_r">R</code></td>
<td>
<p>D x D rotation matrix</p>
</td></tr>
<tr><td><code id="apply_transformation_+3A_t">t</code></td>
<td>
<p>1 x D translation vector</p>
</td></tr>
<tr><td><code id="apply_transformation_+3A_s">s</code></td>
<td>
<p>scaling factor</p>
</td></tr>
</table>


<h3>Value</h3>

<p>transformed point set as a N x D matrix
</p>

<hr>
<h2 id='ary2ps'>ary2ps</h2><span id='topic+ary2ps'></span>

<h3>Description</h3>

<p>Convert a 4d array to a list of 3d point sets. 
The points are formed by extracting the coordinates of array values strictly above the given cut-off (default 0).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ary2ps(ary, bkg = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ary2ps_+3A_ary">ary</code></td>
<td>
<p>a 4d array with last dimension indexing instances.</p>
</td></tr>
<tr><td><code id="ary2ps_+3A_bkg">bkg</code></td>
<td>
<p>Extract points for array values strictly above this (default = 0)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of point sets.
</p>

<hr>
<h2 id='binning'>binning</h2><span id='topic+binning'></span>

<h3>Description</h3>

<p>Binning in 1D, 2D or 3D.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binning(x, y, nbins, xrange = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="binning_+3A_x">x</code></td>
<td>
<p>design matrix, dimension n x d with d in 1:3.</p>
</td></tr>
<tr><td><code id="binning_+3A_y">y</code></td>
<td>
<p>either a response vector of length n or NULL.</p>
</td></tr>
<tr><td><code id="binning_+3A_nbins">nbins</code></td>
<td>
<p>vector of length d containing number of bins for each dimension, may be set to NULL.</p>
</td></tr>
<tr><td><code id="binning_+3A_xrange">xrange</code></td>
<td>
<p>range for endpoints of bins for each dimension, either matrix of dimension 2 x d or NULL. xrange is increased if the cube defined does not contain all design points.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Copied from package aws which is no longer in CRAN.
Original author:  Joerg Polzehl (polzehl@wias-berlin.de)
who adapted code of function binning in package sm.
</p>


<h3>Value</h3>

<p>a list with elements:
</p>

<ul>
<li><p> x matrix of coordinates of non-empty bin centers
</p>
</li>
<li><p> x.freq number of observations in nonempty bins
</p>
</li>
<li><p> midpoints.x1 bin centers in dimension 1
</p>
</li>
<li><p> midpoints.x2 bin centers in dimension 2
</p>
</li>
<li><p> midpoints.x3 bin centers in dimension 3
</p>
</li>
<li><p> breaks.x1 break points dimension 1
</p>
</li>
<li><p> breaks.x2 break points dimension 2
</p>
</li>
<li><p> breaks.x3 break points dimension 3
</p>
</li>
<li><p> table.freq number of observations per bin
</p>
</li>
<li><p> means means of y in non-empty bins (if y isn't NULL)
</p>
</li>
<li><p> devs standard deviations of y in non-empty bins (if y isn't NULL)
</p>
</li></ul>


<hr>
<h2 id='circle_hough_transform'>Circle Hough transform</h2><span id='topic+circle_hough_transform'></span>

<h3>Description</h3>

<p>Extract coordinates of the centres of circles from a 2D image using the Hough transform
</p>


<h3>Usage</h3>

<pre><code class='language-R'>circle_hough_transform(
  pixels,
  rmin,
  rmax,
  threshold,
  resolution = 360,
  min.separation = rmin/4,
  ncpu = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="circle_hough_transform_+3A_pixels">pixels</code></td>
<td>
<p>input data, either a matrix representing a 2D image or a data frame of signal coordinates with columns x, y.
For images, background is expected to be 0 and signal to have positive values.</p>
</td></tr>
<tr><td><code id="circle_hough_transform_+3A_rmin">rmin</code></td>
<td>
<p>minimum search radius.</p>
</td></tr>
<tr><td><code id="circle_hough_transform_+3A_rmax">rmax</code></td>
<td>
<p>maximum search radius.</p>
</td></tr>
<tr><td><code id="circle_hough_transform_+3A_threshold">threshold</code></td>
<td>
<p>score threshold between 0 and 1.</p>
</td></tr>
<tr><td><code id="circle_hough_transform_+3A_resolution">resolution</code></td>
<td>
<p>number of steps in the circle transform (default: 360). This represents the maximum number of votes a point can get.</p>
</td></tr>
<tr><td><code id="circle_hough_transform_+3A_min.separation">min.separation</code></td>
<td>
<p>distance between circle centres below which overlapping circles are considered the same and merged (default to 0.25*rmin)</p>
</td></tr>
<tr><td><code id="circle_hough_transform_+3A_ncpu">ncpu</code></td>
<td>
<p>number of threads to use to speed up computation (default: 1)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame with columns x, y, r and score
</p>


<h3>Examples</h3>

<pre><code class='language-R'>point.set &lt;- data.frame(x = c(-9.8,-5.2,12.5,2.5,4.5,1.3,-0.2,0.4,9.3,-1.4,0.5,-1.1,-7.7),
                        y = c(-4.2,1.5,-0.5,12,-3,-7.2,10.9,6.7,-1.3,10,6.7,-6.2,2.9))
circles &lt;- circle_hough_transform(pixels = point.set, rmin = 3, rmax = 6, resolution = 100,
                                  threshold = 0.1, ncpu = 1)
</code></pre>

<hr>
<h2 id='coloc_index'>coloc_index</h2><span id='topic+coloc_index'></span>

<h3>Description</h3>

<p>Compute a co-localization index between two sets of points. 
Adapted from:
Willems and MacGillavry, A coordinate-based co-localization index to quantify
and visualize spatial associations in single-molecule localization microscopy.
Sci Rep 12, 4676 (2022). https://doi.org/10.1038/s41598-022-08746-4
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coloc_index(
  P1,
  locprec1 = NULL,
  locprecz1 = NULL,
  P2,
  locprec2 = NULL,
  locprecz2 = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="coloc_index_+3A_p1">P1</code></td>
<td>
<p>a point set as matrix or data frame with columns x,y,z.</p>
</td></tr>
<tr><td><code id="coloc_index_+3A_locprec1">locprec1</code></td>
<td>
<p>(optional) localization precision in x,y for P1</p>
</td></tr>
<tr><td><code id="coloc_index_+3A_locprecz1">locprecz1</code></td>
<td>
<p>(optional) localization precision along z for P1</p>
</td></tr>
<tr><td><code id="coloc_index_+3A_p2">P2</code></td>
<td>
<p>a point set as matrix or data frame with columns x,y,z.</p>
</td></tr>
<tr><td><code id="coloc_index_+3A_locprec2">locprec2</code></td>
<td>
<p>(optional) localization precision in x,y for P2</p>
</td></tr>
<tr><td><code id="coloc_index_+3A_locprecz2">locprecz2</code></td>
<td>
<p>(optional) localization precision along z for P2</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This can be seen as measuring the similarity between two spatial distributions.
Co-clustering in dense structures can give values above 1.
</p>
<p>Localization precision is optional but if used then all locprec parameters
must be specified.
</p>


<h3>Value</h3>

<p>a list with two elements:
</p>

<ul>
<li><p> vector of co-localization indices for points in P1 relative to P2
</p>
</li>
<li><p> vector of co-localization indices for points in P2 relative to P1
</p>
</li></ul>


<hr>
<h2 id='costWd'>costWd</h2><span id='topic+costWd'></span>

<h3>Description</h3>

<p>Objective function to minimize when using GMMs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>costWd(Tr, X, Y, CX, CY, w1 = NULL, w2 = NULL, S = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="costWd_+3A_tr">Tr</code></td>
<td>
<p>Transformation vector as translation vector + rotation (angle in 2d, quaternion in 3d))</p>
</td></tr>
<tr><td><code id="costWd_+3A_x">X</code></td>
<td>
<p>matrix of means of first GMM (i.e. reference point set)</p>
</td></tr>
<tr><td><code id="costWd_+3A_y">Y</code></td>
<td>
<p>matrix of means of second GMM (i.e. moving point set)</p>
</td></tr>
<tr><td><code id="costWd_+3A_cx">CX</code></td>
<td>
<p>array of covariance matrices of first GMM such that X[i,] has covariance matrix CX[,,i]</p>
</td></tr>
<tr><td><code id="costWd_+3A_cy">CY</code></td>
<td>
<p>array of covariance matrices of second GMM such that Y[i,] has covariance matrix CY[,,i]</p>
</td></tr>
<tr><td><code id="costWd_+3A_w1">w1</code></td>
<td>
<p>(optional) vector of mixture weights of first GMM.</p>
</td></tr>
<tr><td><code id="costWd_+3A_w2">w2</code></td>
<td>
<p>(optional) vector of mixture weights of second GMM.</p>
</td></tr>
<tr><td><code id="costWd_+3A_s">S</code></td>
<td>
<p>(optional) array of pre-computed sqrtm(sqrtm(CX[,,i]) %*% CY[,,j] %*% sqrtm(CX[,,i]))</p>
</td></tr>
</table>


<h3>Value</h3>

<p>cost value
</p>

<hr>
<h2 id='cpd'>cpd</h2><span id='topic+cpd'></span>

<h3>Description</h3>

<p>Affine and rigid registration of two point sets using the coherent point drift algorithm. 
See: Myronenko A., Song X. (2010): &quot;Point-Set Registration: Coherent Point Drift&quot;,
IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 32, issue 12, pp. 2262-2275.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpd(
  X,
  Y,
  w = 0,
  weights = NULL,
  scale = FALSE,
  maxIter = 100,
  subsample = NULL,
  tol = 1e-04
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cpd_+3A_x">X</code></td>
<td>
<p>reference point set, a N x D matrix</p>
</td></tr>
<tr><td><code id="cpd_+3A_y">Y</code></td>
<td>
<p>point set to transform, a M x D matrix,</p>
</td></tr>
<tr><td><code id="cpd_+3A_w">w</code></td>
<td>
<p>noise weight in the range [0, 1)</p>
</td></tr>
<tr><td><code id="cpd_+3A_weights">weights</code></td>
<td>
<p>a M x N matrix of point correspondence weights</p>
</td></tr>
<tr><td><code id="cpd_+3A_scale">scale</code></td>
<td>
<p>logical (default: FALSE), whether to use scaling</p>
</td></tr>
<tr><td><code id="cpd_+3A_maxiter">maxIter</code></td>
<td>
<p>maximum number of iterations to perform (default: 100)</p>
</td></tr>
<tr><td><code id="cpd_+3A_subsample">subsample</code></td>
<td>
<p>if set, use this randomly selected fraction of the points</p>
</td></tr>
<tr><td><code id="cpd_+3A_tol">tol</code></td>
<td>
<p>tolerance for determining convergence</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of
</p>

<ul>
<li><p> Y: transformed point set, 
</p>
</li>
<li><p> R: rotation matrix, 
</p>
</li>
<li><p> t: translation vector, 
</p>
</li>
<li><p> s: scaling factor, 
</p>
</li>
<li><p> P: matrix of correspondence probabilities between the two point sets,
</p>
</li>
<li><p> sigma: final variance,
</p>
</li>
<li><p> iter: number of iterations performed,
</p>
</li>
<li><p> converged: boolean, whether the algorithm has converged.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data.file1 &lt;- system.file("test_data", "parasaurolophusA.txt", package = "LOMAR",
 mustWork = TRUE)
PS1 &lt;- read.csv(data.file1, sep = '\t', header = FALSE)
data.file2 &lt;- system.file("test_data", "parasaurolophusB.txt", package = "LOMAR",
 mustWork = TRUE)
PS2 &lt;- read.csv(data.file2, sep = '\t', header = FALSE)
transformation &lt;- cpd(PS1, PS2, maxIter = 10, tol = 1e-3)
## Not run: 
# Visualize registration outcome
library(rgl)
plot3d(PS1, col = "blue")
points3d(PS2, col = "green")
points3d(transformation[['Y']], col = "magenta")

## End(Not run)
</code></pre>

<hr>
<h2 id='crop_point_set'>crop_point_set</h2><span id='topic+crop_point_set'></span>

<h3>Description</h3>

<p>Retain points in the set that are within the given distance from the geometric median of the set.
Using the geometric median is more robust than using the centre of mass (i.e. mean).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crop_point_set(point.set, size, center = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="crop_point_set_+3A_point.set">point.set</code></td>
<td>
<p>a point set as a matrix with columns x,y,z.</p>
</td></tr>
<tr><td><code id="crop_point_set_+3A_size">size</code></td>
<td>
<p>vector of distances from the target region centre along each axis. 
Points are discarded if they are outside the ellipsoid defined by size and centred on 
the given position.</p>
</td></tr>
<tr><td><code id="crop_point_set_+3A_center">center</code></td>
<td>
<p>(optional) coordinates of the centre of the target region. If not given, 
default to the geometric median of the point set.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>point set as a matrix with columns x,y,z.
</p>

<hr>
<h2 id='denoise'>denoise</h2><span id='topic+denoise'></span>

<h3>Description</h3>

<p>Point density is estimated using a Gaussian mixture model and points in low
density regions are considered as noise and removed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>denoise(points, k = 16, prob = 0.3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="denoise_+3A_points">points</code></td>
<td>
<p>a data frame with columns x,y,z.</p>
</td></tr>
<tr><td><code id="denoise_+3A_k">k</code></td>
<td>
<p>integer, number of mixture components for the GMM</p>
</td></tr>
<tr><td><code id="denoise_+3A_prob">prob</code></td>
<td>
<p>probability level in the range [0,1] to identify high density regions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a point set
</p>

<hr>
<h2 id='dist_to_boundary'>dist_to_boundary</h2><span id='topic+dist_to_boundary'></span>

<h3>Description</h3>

<p>Given a point set and an alpha-shape, get the distance of each point
to the closest boundary point of the alpha-shape.
Points inside the shape get negative values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist_to_boundary(points, shape)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dist_to_boundary_+3A_points">points</code></td>
<td>
<p>a data frame with x,y,z columns</p>
</td></tr>
<tr><td><code id="dist_to_boundary_+3A_shape">shape</code></td>
<td>
<p>an object of class ashape3d with a single alpha value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of distances (negative values indicate points inside the shape)
</p>

<hr>
<h2 id='dist_to_line'>dist_to_line</h2><span id='topic+dist_to_line'></span>

<h3>Description</h3>

<p>Compute distance between a set of points and a line defined by two points
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist_to_line(pts, a = NULL, b = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dist_to_line_+3A_pts">pts</code></td>
<td>
<p>a data frame or matrix with 3 columns of coordinates</p>
</td></tr>
<tr><td><code id="dist_to_line_+3A_a">a</code></td>
<td>
<p>vector of coordinates of a point on the line</p>
</td></tr>
<tr><td><code id="dist_to_line_+3A_b">b</code></td>
<td>
<p>a second point on the line</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of distances
</p>

<hr>
<h2 id='downsample'>downsample</h2><span id='topic+downsample'></span>

<h3>Description</h3>

<p>Weighted downsampling of a point set.
If point weights are not provided, they are computed to be proportional to the local density
around each point.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>downsample(point.set, n = NULL, k = NULL, weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="downsample_+3A_point.set">point.set</code></td>
<td>
<p>a point set</p>
</td></tr>
<tr><td><code id="downsample_+3A_n">n</code></td>
<td>
<p>integer, sample size.</p>
</td></tr>
<tr><td><code id="downsample_+3A_k">k</code></td>
<td>
<p>integer, number of nearest neighbours to consider to estimate local density</p>
</td></tr>
<tr><td><code id="downsample_+3A_weights">weights</code></td>
<td>
<p>a vector of probability weights</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a point set
</p>

<hr>
<h2 id='find_elbow'>find_elbow</h2><span id='topic+find_elbow'></span>

<h3>Description</h3>

<p>Find elbow in a 2D curve represented by a list of ordered values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_elbow(values)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="find_elbow_+3A_values">values</code></td>
<td>
<p>vector of values in decreasing order</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function finds the point with maximum distance from the line between
the first and last points.
Adapted from StackOverflow:
http://stackoverflow.com/questions/2018178/finding-the-best-trade-off-point-on-a-curve
</p>


<h3>Value</h3>

<p>index and value of the selected point
</p>

<hr>
<h2 id='Gaussian_Wd'>Gaussian_Wd</h2><span id='topic+Gaussian_Wd'></span>

<h3>Description</h3>

<p>Compute 2-Wasserstein distance between two Gaussian distributions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Gaussian_Wd(m1, m2, S1, S2, S = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Gaussian_Wd_+3A_m1">m1</code></td>
<td>
<p>mean of first distribution</p>
</td></tr>
<tr><td><code id="Gaussian_Wd_+3A_m2">m2</code></td>
<td>
<p>mean of second distribution</p>
</td></tr>
<tr><td><code id="Gaussian_Wd_+3A_s1">S1</code></td>
<td>
<p>variance of first distribution</p>
</td></tr>
<tr><td><code id="Gaussian_Wd_+3A_s2">S2</code></td>
<td>
<p>variance of second distribution</p>
</td></tr>
<tr><td><code id="Gaussian_Wd_+3A_s">S</code></td>
<td>
<p>(optional) matrix of pre-computed sqrtm(sqrtm(S1) %*% S2 %*% sqrtm(S1))</p>
</td></tr>
</table>


<h3>Value</h3>

<p>distance value
</p>

<hr>
<h2 id='get_kernel_matrix'>get_kernel_matrix</h2><span id='topic+get_kernel_matrix'></span>

<h3>Description</h3>

<p>Compute kernel/distance matrix between persistence diagrams.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_kernel_matrix(
  Diag = NULL,
  method = c("sWd", "pssk"),
  dimensions = NULL,
  return.dist = FALSE,
  M = NULL,
  sigma = NULL,
  ncpu = 1,
  cluster.type = "PSOCK"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_kernel_matrix_+3A_diag">Diag</code></td>
<td>
<p>list of persistence diagrams as n x 3 matrices</p>
</td></tr>
<tr><td><code id="get_kernel_matrix_+3A_method">method</code></td>
<td>
<p>which kernel or distance to compute. One of sWd (for sliced Wasserstein kernel) or pssk (for the persistence scale-space kernel)</p>
</td></tr>
<tr><td><code id="get_kernel_matrix_+3A_dimensions">dimensions</code></td>
<td>
<p>vector of the dimensions of the topological features to consider, if NULL (default) use all available dimensions</p>
</td></tr>
<tr><td><code id="get_kernel_matrix_+3A_return.dist">return.dist</code></td>
<td>
<p>logical (default: FALSE) for method sWd, whether to return the sliced Wasserstein distance matrix instead of the kernel.</p>
</td></tr>
<tr><td><code id="get_kernel_matrix_+3A_m">M</code></td>
<td>
<p>number of slices for the sliced Wasserstein kernel</p>
</td></tr>
<tr><td><code id="get_kernel_matrix_+3A_sigma">sigma</code></td>
<td>
<p>kernel bandwidth</p>
</td></tr>
<tr><td><code id="get_kernel_matrix_+3A_ncpu">ncpu</code></td>
<td>
<p>number of parallel threads to use for computation</p>
</td></tr>
<tr><td><code id="get_kernel_matrix_+3A_cluster.type">cluster.type</code></td>
<td>
<p>type of multicore cluster to use, either PSOCK (default) or FORK</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>PS &lt;- list(data.frame(x = c(2.4,-6.9,4.6,-0.7,-3.3,-4.9,-3.5,-3.5,4.2,-7),
                      y = c(5.7,1.9,4.8,3.4,-3,-2.1,7.2,1.8,6.1,-1.6),
                      z = c(2.7,-0.1,-0.7,-0.6,0.4,-1.5,-0.6,-0.9,2.2,0.7)),
           data.frame(x = c(0,0,3.1,-5.6,-5,-7.4,-0.7,-7.7,-6.7,4,4.2,0.2,5.8,3.9,3.9),
                      y = c(6.3,-6.1,-3.5,4.6,-4.1,0.3,8.8,-2.3,2.9,3.7,-1.4,-3.9,5.5,-1.2,-6.7),
                      z = c(-1.5,1.7,-0.4,-1.4,1.8,1.7,-0.9,-1.8,-0.5,1.7,1.3,0.5,-1.4,1.6,-0.1)),
           data.frame(x = c(-9.8,-5.2,12.5,2.5,4.5,1.3,-0.2,0.4,9.3,-1.4,0.5,-1.1,-7.7),
                      y = c(-4.2,1.5,-0.5,12,-3,-7.2,10.9,6.7,-1.3,10,6.7,-6.2,2.9),
                      z = c(3.4,-3.8,-1.4,1.8,3.5,2.5,2.6,-4.8,-3.8,3.9,4.1,-3.6,-4)))
Dgs &lt;- get_persistence_diagrams(point.sets = PS, maxdimension = 1, maxscale = 5, ncpu = 1)
K &lt;- get_kernel_matrix(Diag = Dgs, method = 'sWd', dimensions = c(0,1), M = 10, sigma = 5)
</code></pre>

<hr>
<h2 id='get_persistence_diagrams'>get_persistence_diagrams</h2><span id='topic+get_persistence_diagrams'></span>

<h3>Description</h3>

<p>Compute persistence diagrams for a list of point sets.
By default, compute persistent homology from the Vietoris-Rips filtration.
If use.dtm is TRUE, compute instead the persistent homology of the sublevel
set of the distance to measure evaluated over a grid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_persistence_diagrams(
  point.sets = NULL,
  maxdimension = NULL,
  maxscale = NULL,
  use.dtm = FALSE,
  m0 = NULL,
  grid.by = NULL,
  ncpu = 1,
  cluster.type = "PSOCK"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_persistence_diagrams_+3A_point.sets">point.sets</code></td>
<td>
<p>list of point sets, each as a data frame with columns x,y,z</p>
</td></tr>
<tr><td><code id="get_persistence_diagrams_+3A_maxdimension">maxdimension</code></td>
<td>
<p>maximum dimension of the homological features to be computed</p>
</td></tr>
<tr><td><code id="get_persistence_diagrams_+3A_maxscale">maxscale</code></td>
<td>
<p>limit of the Vietoris-Rips filtration</p>
</td></tr>
<tr><td><code id="get_persistence_diagrams_+3A_use.dtm">use.dtm</code></td>
<td>
<p>logical (default: FALSE), whether to use the distance to measure function</p>
</td></tr>
<tr><td><code id="get_persistence_diagrams_+3A_m0">m0</code></td>
<td>
<p>parameter for the dtm function</p>
</td></tr>
<tr><td><code id="get_persistence_diagrams_+3A_grid.by">grid.by</code></td>
<td>
<p>vector of space between points of the grid for the dtm function along each dimension</p>
</td></tr>
<tr><td><code id="get_persistence_diagrams_+3A_ncpu">ncpu</code></td>
<td>
<p>number of parallel threads to use for computation</p>
</td></tr>
<tr><td><code id="get_persistence_diagrams_+3A_cluster.type">cluster.type</code></td>
<td>
<p>type of multicore cluster to use, either PSOCK (default) or FORK</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of persistence diagrams as n x 3 matrices. Each row is a topological feature
and the columns are dimension, birth and death of the feature.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>PS &lt;- list(data.frame(x = c(2.4,-6.9,4.6,-0.7,-3.3,-4.9,-3.5,-3.5,4.2,-7),
                      y = c(5.7,1.9,4.8,3.4,-3,-2.1,7.2,1.8,6.1,-1.6),
                      z = c(2.7,-0.1,-0.7,-0.6,0.4,-1.5,-0.6,-0.9,2.2,0.7)),
           data.frame(x = c(0,0,3.1,-5.6,-5,-7.4,-0.7,-7.7,-6.7,4,4.2,0.2,5.8,3.9,3.9),
                      y = c(6.3,-6.1,-3.5,4.6,-4.1,0.3,8.8,-2.3,2.9,3.7,-1.4,-3.9,5.5,-1.2,-6.7),
                      z = c(-1.5,1.7,-0.4,-1.4,1.8,1.7,-0.9,-1.8,-0.5,1.7,1.3,0.5,-1.4,1.6,-0.1)))
Diags &lt;- get_persistence_diagrams(point.sets = PS, maxdimension = 1, maxscale = 5, ncpu = 1)
</code></pre>

<hr>
<h2 id='get_shape'>get_shape</h2><span id='topic+get_shape'></span>

<h3>Description</h3>

<p>Get the the alpha-shape of a point set.
If not given, the function automatically determines alpha using a downsampled 
point set. 
As a consequence, alpha and therefore the computed shape can vary slightly
between runs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_shape(points, alpha = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_shape_+3A_points">points</code></td>
<td>
<p>a data frame with columns x, y, z.</p>
</td></tr>
<tr><td><code id="get_shape_+3A_alpha">alpha</code></td>
<td>
<p>(optional) positive number</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an alpha-shape object of class ashape3d
</p>

<hr>
<h2 id='get_surface_area'>get_surface_area</h2><span id='topic+get_surface_area'></span>

<h3>Description</h3>

<p>Compute the surface area of an alpha-shape by summing the surfaces of the
boundary triangles
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_surface_area(as)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_surface_area_+3A_as">as</code></td>
<td>
<p>an alpha-shape object of class ashape3d</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric value
</p>

<hr>
<h2 id='GMM_Wd'>GMM_Wd</h2><span id='topic+GMM_Wd'></span>

<h3>Description</h3>

<p>Compute 2-Wasserstein distance between two Gaussian mixture models
See: Delon J, Desolneux A. (2019) A Wasserstein-type distance in the space of Gaussian Mixture Models. hal-02178204v2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GMM_Wd(m1, m2, S1, S2, w1 = NULL, w2 = NULL, S = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GMM_Wd_+3A_m1">m1</code></td>
<td>
<p>matrix of means of first GMM</p>
</td></tr>
<tr><td><code id="GMM_Wd_+3A_m2">m2</code></td>
<td>
<p>matrix of means of second GMM</p>
</td></tr>
<tr><td><code id="GMM_Wd_+3A_s1">S1</code></td>
<td>
<p>array of covariance matrices of first GMM such that m1[i,] has covariance matrix S1[,,i]</p>
</td></tr>
<tr><td><code id="GMM_Wd_+3A_s2">S2</code></td>
<td>
<p>array of covariance matrices of second GMM such that m2[i,] has covariance matrix S2[,,i]</p>
</td></tr>
<tr><td><code id="GMM_Wd_+3A_w1">w1</code></td>
<td>
<p>(optional) vector of mixture weights of first GMM.</p>
</td></tr>
<tr><td><code id="GMM_Wd_+3A_w2">w2</code></td>
<td>
<p>(optional) vector of mixture weights of second GMM.</p>
</td></tr>
<tr><td><code id="GMM_Wd_+3A_s">S</code></td>
<td>
<p>(optional) array of pre-computed sqrtm(sqrtm(S1[,,i]) %*% S2[,,j] %*% sqrtm(S1[,,i]))</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of distance value d and optimal transport matrix ot
</p>

<hr>
<h2 id='gradientWd'>gradientWd</h2><span id='topic+gradientWd'></span>

<h3>Description</h3>

<p>Gradient of the objective function with respect to rotation and translation parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gradientWd(Tr, X, Y, CX, CY, w1 = NULL, w2 = NULL, S = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gradientWd_+3A_tr">Tr</code></td>
<td>
<p>Transformation vector as translation vector + rotation (angle in 2d, quaternion in 3d))</p>
</td></tr>
<tr><td><code id="gradientWd_+3A_x">X</code></td>
<td>
<p>matrix of means of first GMM (i.e. reference point set)</p>
</td></tr>
<tr><td><code id="gradientWd_+3A_y">Y</code></td>
<td>
<p>matrix of means of second GMM (i.e. moving point set)</p>
</td></tr>
<tr><td><code id="gradientWd_+3A_cx">CX</code></td>
<td>
<p>array of covariance matrices of first GMM such that X[i,] has covariance matrix C1[,,i]</p>
</td></tr>
<tr><td><code id="gradientWd_+3A_cy">CY</code></td>
<td>
<p>array of covariance matrices of second GMM such that Y[i,] has covariance matrix C2[,,i]</p>
</td></tr>
<tr><td><code id="gradientWd_+3A_w1">w1</code></td>
<td>
<p>(optional) vector of mixture weights of first GMM.</p>
</td></tr>
<tr><td><code id="gradientWd_+3A_w2">w2</code></td>
<td>
<p>(optional) vector of mixture weights of second GMM.</p>
</td></tr>
<tr><td><code id="gradientWd_+3A_s">S</code></td>
<td>
<p>(optional) array of pre-computed sqrtm(sqrtm(CX[,,i]) %*% CY[,,j] %*% sqrtm(CX[,,i]))</p>
</td></tr>
</table>


<h3>Value</h3>

<p>gradient vector
</p>

<hr>
<h2 id='group_events'>group_events</h2><span id='topic+group_events'></span>

<h3>Description</h3>

<p>Localisation events are grouped by recursively clustering mutual nearest neighbours.
Neighbours are determined using the Mahalanobis distance to account for anisotropy in 
the localisation precision. Since the Mahalanobis distance has approximately a 
chi-squared distribution, a distance threshold can be chosen from a chi-squared table 
where the number of degrees of freedom is the dimension and alpha can be seen
as the probability of missing a localization event generated from the same fluorophore
as the event under consideration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group_events(points, locprec = NULL, locprecz = NULL, p = 0.1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="group_events_+3A_points">points</code></td>
<td>
<p>a data frame with columns x,y,z.</p>
</td></tr>
<tr><td><code id="group_events_+3A_locprec">locprec</code></td>
<td>
<p>localization precision in x,y</p>
</td></tr>
<tr><td><code id="group_events_+3A_locprecz">locprecz</code></td>
<td>
<p>localization precision along z, defaults to locprec</p>
</td></tr>
<tr><td><code id="group_events_+3A_p">p</code></td>
<td>
<p>confidence level, see description. Defaults to 0.1</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with two elements:
</p>

<ul>
<li><p> points: a point set as data frame with columns x,y,z
</p>
</li>
<li><p> membership: a vector of integers indicating the cluster to which each input point is allocated.
</p>
</li></ul>


<hr>
<h2 id='icp'>icp</h2><span id='topic+icp'></span>

<h3>Description</h3>

<p>Rigid registration of two point sets using the iterative closest point algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>icp(
  X,
  Y,
  weights = NULL,
  iterations = 100,
  subsample = NULL,
  scale = FALSE,
  tol = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="icp_+3A_x">X</code></td>
<td>
<p>reference point set, a N x D matrix</p>
</td></tr>
<tr><td><code id="icp_+3A_y">Y</code></td>
<td>
<p>point set to transform, a M x D matrix,</p>
</td></tr>
<tr><td><code id="icp_+3A_weights">weights</code></td>
<td>
<p>vector of length nrow(Y) containing weights for each point in Y. Not implemented.</p>
</td></tr>
<tr><td><code id="icp_+3A_iterations">iterations</code></td>
<td>
<p>number of iterations to perform (default: 100)</p>
</td></tr>
<tr><td><code id="icp_+3A_subsample">subsample</code></td>
<td>
<p>if set, use this randomly selected fraction of the points</p>
</td></tr>
<tr><td><code id="icp_+3A_scale">scale</code></td>
<td>
<p>logical (default: FALSE), whether to use scaling.</p>
</td></tr>
<tr><td><code id="icp_+3A_tol">tol</code></td>
<td>
<p>tolerance for determining convergence</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of
</p>

<ul>
<li><p> Y: transformed point set, a M x D matrix,
</p>
</li>
<li><p> R: rotation matrix, 
</p>
</li>
<li><p> t: translation vector, 
</p>
</li>
<li><p> s: scaling factor, 
</p>
</li>
<li><p> iter: number of iterations performed,
</p>
</li>
<li><p> conv: boolean, whether the algorithm has converged.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data.file1 &lt;- system.file("test_data", "parasaurolophusA.txt", package = "LOMAR",
 mustWork = TRUE)
PS1 &lt;- read.csv(data.file1, sep = '\t', header = FALSE)
data.file2 &lt;- system.file("test_data", "parasaurolophusB.txt", package = "LOMAR",
 mustWork = TRUE)
PS2 &lt;- read.csv(data.file2, sep = '\t', header = FALSE)
transformation &lt;- icp(PS1, PS2, iterations = 10, tol = 1e-3)
## Not run: 
# Visualize registration outcome
library(rgl)
plot3d(PS1, col = "blue")
points3d(PS2, col = "green")
points3d(transformation[['Y']], col = "magenta")

## End(Not run)
</code></pre>

<hr>
<h2 id='idx2rowcol'>idx2rowcol</h2><span id='topic+idx2rowcol'></span>

<h3>Description</h3>

<p>Convert indices into a dist object to row, column coordinates of the corresponding distance matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>idx2rowcol(idx, n)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="idx2rowcol_+3A_idx">idx</code></td>
<td>
<p>vector of indices</p>
</td></tr>
<tr><td><code id="idx2rowcol_+3A_n">n</code></td>
<td>
<p>size of the n x n distance matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with two columns nr and nc
</p>

<hr>
<h2 id='img2ps'>img2ps</h2><span id='topic+img2ps'></span>

<h3>Description</h3>

<p>Read an image into a point set. 
The points are formed by extracting the coordinates of voxel values strictly above the given cut-off (default 0).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>img2ps(img = NULL, bkg = 0, crop.size = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="img2ps_+3A_img">img</code></td>
<td>
<p>either a 2d or 3d array or a path to a file containing a 2d or 3d image.</p>
</td></tr>
<tr><td><code id="img2ps_+3A_bkg">bkg</code></td>
<td>
<p>Extract points for values strictly above this (default = 0).</p>
</td></tr>
<tr><td><code id="img2ps_+3A_crop.size">crop.size</code></td>
<td>
<p>vector (of length 2 or 3) containing the desired reduced size of the images along each dimension, e.g. c(30,30,30).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a point set as matrix with columns x,y[,z]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>img.file &lt;- system.file("test_data/img", "alien1_3d.tif", package = "LOMAR",
 mustWork = TRUE) 
point_set &lt;- img2ps(img = img.file, bkg = 0)
</code></pre>

<hr>
<h2 id='jrmpc'>jrmpc</h2><span id='topic+jrmpc'></span>

<h3>Description</h3>

<p>Joint registration of multiple point sets
See: G.  D.  Evangelidis,  D.  Kounades-Bastian,  R.  Horaud,  andE. Z. Psarakis. 
A generative model for the joint registration of multiple point sets. 
In European Conference on Computer Vision, pages 109–122. Springer, 2014
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jrmpc(
  V,
  C = NULL,
  K = NULL,
  g = NULL,
  initialPriors = NULL,
  updatePriors = TRUE,
  maxIter = 100,
  fixedVarIter = 0,
  tol = 0.01,
  initializeBy = NULL,
  model.selection = FALSE,
  model.selection.threshold = NULL,
  rotation.only = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jrmpc_+3A_v">V</code></td>
<td>
<p>list of point sets as N x D matrices</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_c">C</code></td>
<td>
<p>(optional) list of arrays of covariance matrices with C[[j]][,,i] the covariance matrix associated with point i of set j.</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_k">K</code></td>
<td>
<p>(optional) number of components of the GMM, defaults to the average number of points in a set.</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_g">g</code></td>
<td>
<p>(optional) proportion of noisy points, defaults to 1/K. If set, priors will be initialized uniformly.</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_initialpriors">initialPriors</code></td>
<td>
<p>(optional) vector of length K of prior probabilities. Defaults to uniform distribution using g.
If set, will determine g so it is an error to specify g with initialPriors.</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_updatepriors">updatePriors</code></td>
<td>
<p>logical, whether to update priors at each iteration (default: TRUE).</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_maxiter">maxIter</code></td>
<td>
<p>maximum number of iterations to perform (default: 100).</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_fixedvariter">fixedVarIter</code></td>
<td>
<p>number of iterations before starting variance updates</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_tol">tol</code></td>
<td>
<p>tolerance for determining convergence (default: 1e-2).</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_initializeby">initializeBy</code></td>
<td>
<p>(optional) how to initialize the GMM means. Defaults to distributing the means on the surface of the sphere enclosing all (centred) sets.
Currently supported values are:
</p>

<ul>
<li><p> 'sampling': sample from the data,
</p>
</li>
<li><p> a K x D matrix of points
</p>
</li></ul>
</td></tr>
<tr><td><code id="jrmpc_+3A_model.selection">model.selection</code></td>
<td>
<p>whether to perform model selection (default: FALSE). 
If set to TRUE, GMM components with no support in the data are deleted.</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_model.selection.threshold">model.selection.threshold</code></td>
<td>
<p>value below which we consider a GMM component has no support, set to 1/K if not explicitly given</p>
</td></tr>
<tr><td><code id="jrmpc_+3A_rotation.only">rotation.only</code></td>
<td>
<p>if set to TRUE, no translation is performed (default: FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of 
</p>

<ul>
<li><p> Y: list of transformed point sets as N x d matrices,
</p>
</li>
<li><p> R: list of d x d rotation matrices, one for each point set in V,
</p>
</li>
<li><p> t: list of translation vectors, one for each point set in V,
</p>
</li>
<li><p> M: centres of the GMM,
</p>
</li>
<li><p> S: variances of the GMM.
</p>
</li>
<li><p> a: list of posterior probabilities as N x K matrices
</p>
</li>
<li><p> iter: number of iterations
</p>
</li>
<li><p> conv: error value used to evaluate convergence relative to tol
</p>
</li>
<li><p> z: support scores of the GMM components
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- read.csv(system.file("test_data", "parasaurolophusA.txt", package="LOMAR",
 mustWork = TRUE), sep = "\t")
Y &lt;- read.csv(system.file("test_data", "parasaurolophusB.txt", package="LOMAR",
mustWork = TRUE), sep = "\t")
Z &lt;- read.csv(system.file("test_data", "parasaurolophusC.txt", package="LOMAR",
mustWork = TRUE), sep = "\t")
PS &lt;- list(X, Y, Z)
C &lt;- list()
for(i in 1:3) {
 cv &lt;- diag(0.1, ncol(PS[[i]])) + jitter(0.01, amount = 0.01)
 cv &lt;- replicate(nrow(PS[[i]]), cv)
 C[[i]] &lt;- cv
}
transformation &lt;- jrmpc(PS, C = C, K = 100, maxIter = 20, tol = 0.01, 
model.selection = TRUE)
## Not run: 
# Visualize registration outcome
library(rgl)
colours &lt;- c("blue", "green", "magenta")
Yt &lt;- transformation[['Y']]
plot3d(Yt[[1]], col = colours[1])
for(i in 2:length(Yt)) {
 points3d(Yt[[i]], col = colours[i])
}
# Visualize GMM centres highlighting those with high variance
GMM &lt;- as.data.frame(cbind(transformation[['M']], transformation[['S']]))
colnames(GMM) &lt;- c("x", "y", "z", "S")
colours &lt;- rep("blue", nrow(GMM))
# Find high variance components
threshold &lt;- quantile(transformation[['S']], 0.75)
high.var.idx &lt;- which(transformation[['S']]&gt;threshold)
colours[high.var.idx] &lt;- "red"
plot3d(GMM[, c("x", "y", "z")], col = colours, type = 's', size = 2, box = FALSE, xlab = '', 
      ylab = '', zlab = '', xlim = c(-0.15,0.15), ylim = c(-0.15, 0.15), 
      zlim = c(-0.15, 0.15))

## End(Not run)
</code></pre>

<hr>
<h2 id='local_densities'>local_densities</h2><span id='topic+local_densities'></span>

<h3>Description</h3>

<p>Compute local point density at each point of a point set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local_densities(X, k = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="local_densities_+3A_x">X</code></td>
<td>
<p>point set, a N x D matrix</p>
</td></tr>
<tr><td><code id="local_densities_+3A_k">k</code></td>
<td>
<p>(optional) number of nearest neighbors used (defaults to all points).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Local density is computed as in Ning X, Li F, Tian G, Wang Y (2018) 
An efficient outlier removal method for scattered point cloud data. 
PLOS ONE 13(8):e0201280. https://doi.org/10.1371/journal.pone.0201280
</p>


<h3>Value</h3>

<p>vector of density value for each point
</p>

<hr>
<h2 id='locprec2cov'>locprec2cov</h2><span id='topic+locprec2cov'></span>

<h3>Description</h3>

<p>Converts localization precision columns to a list of arrays of covariance matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>locprec2cov(point.sets, scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="locprec2cov_+3A_point.sets">point.sets</code></td>
<td>
<p>a list of n point sets with locprec columns (locprecz column required for 3D data)</p>
</td></tr>
<tr><td><code id="locprec2cov_+3A_scale">scale</code></td>
<td>
<p>logical, whether to scale the localization precision by the variance of the coordinates</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of 2x2xn or 3x3xn arrays.
</p>

<hr>
<h2 id='locs_from_csv'>locs_from_csv</h2><span id='topic+locs_from_csv'></span>

<h3>Description</h3>

<p>Reads and filters single molecule localization events from a csv file as typically output by the SMAP software.
The main columns of interest are the coordinates (x, y, z), point set membership (site) and localization 
precision (locprec and locprecz).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>locs_from_csv(
  file = NULL,
  roi = NULL,
  channels = NULL,
  frame.filter = NULL,
  llrel.filter = NULL,
  locprec.filter = 0,
  locprecz.filter = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="locs_from_csv_+3A_file">file</code></td>
<td>
<p>a csv file with columns x[nm], y[nm], z[nm] and optionally site[numbers], channel, locprec[nm] and locprecz[nm], other columns are ignored.</p>
</td></tr>
<tr><td><code id="locs_from_csv_+3A_roi">roi</code></td>
<td>
<p>region of interest, keep points within the specified volume. Must be a data frame with columns x,y,z and rows min and max defining a bounding box.</p>
</td></tr>
<tr><td><code id="locs_from_csv_+3A_channels">channels</code></td>
<td>
<p>vector of integers indicating which channel(s) of a multicolour experiment to get data from.</p>
</td></tr>
<tr><td><code id="locs_from_csv_+3A_frame.filter">frame.filter</code></td>
<td>
<p>vector of min and max values, filter out points from frames outside the specified range.</p>
</td></tr>
<tr><td><code id="locs_from_csv_+3A_llrel.filter">llrel.filter</code></td>
<td>
<p>vector of min and max values, filter out points on log-likelihood (for fitted data).</p>
</td></tr>
<tr><td><code id="locs_from_csv_+3A_locprec.filter">locprec.filter</code></td>
<td>
<p>filter out points with locprec value greater than the specified number. Points with locprec == 0 are also removed.</p>
</td></tr>
<tr><td><code id="locs_from_csv_+3A_locprecz.filter">locprecz.filter</code></td>
<td>
<p>filter out points with locprecz value greater than the specified number. Points with locprecz == 0 are also removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame with columns x,y,z, optionally site, locprec and locprecz.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.file &lt;- system.file("test_data", "simulated_NUP107_data.csv", package = "LOMAR",
 mustWork = TRUE)
locs &lt;- locs_from_csv(file = data.file, locprec.filter = 20)
</code></pre>

<hr>
<h2 id='locs2ps'>locs2ps</h2><span id='topic+locs2ps'></span>

<h3>Description</h3>

<p>Cluster localizations into point sets using DBSCAN
</p>


<h3>Usage</h3>

<pre><code class='language-R'>locs2ps(
  points,
  eps,
  minPts,
  keep.locprec = TRUE,
  keep.channel = TRUE,
  cluster.2d = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="locs2ps_+3A_points">points</code></td>
<td>
<p>a point set as a data frame of coordinates with columns x,y,z.</p>
</td></tr>
<tr><td><code id="locs2ps_+3A_eps">eps</code></td>
<td>
<p>DBSCAN parameter, size of the epsilon neighbourhood</p>
</td></tr>
<tr><td><code id="locs2ps_+3A_minpts">minPts</code></td>
<td>
<p>DBSCAN parameter, number of minimum points in the eps region</p>
</td></tr>
<tr><td><code id="locs2ps_+3A_keep.locprec">keep.locprec</code></td>
<td>
<p>logical (default: TRUE), whether to preserve the localization precision columns</p>
</td></tr>
<tr><td><code id="locs2ps_+3A_keep.channel">keep.channel</code></td>
<td>
<p>logical (default: TRUE), whether to preserve channel information column</p>
</td></tr>
<tr><td><code id="locs2ps_+3A_cluster.2d">cluster.2d</code></td>
<td>
<p>logical (default: FALSE), whether to cluster only using x,y (and ignore z)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of matrices with columns x,y,z and eventually locprec[z] and names set to the cluster indices.
</p>

<hr>
<h2 id='multiple_registration'>multiple_registration</h2><span id='topic+multiple_registration'></span>

<h3>Description</h3>

<p>Registration of multiple point sets using tree-guided progressive registration 
followed by iterative refinement.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiple_registration(PS, registration, refine.iter = 20, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="multiple_registration_+3A_ps">PS</code></td>
<td>
<p>list of point sets</p>
</td></tr>
<tr><td><code id="multiple_registration_+3A_registration">registration</code></td>
<td>
<p>pairwise registration method to use</p>
</td></tr>
<tr><td><code id="multiple_registration_+3A_refine.iter">refine.iter</code></td>
<td>
<p>Maximum number of refinement iterations (default: 20)</p>
</td></tr>
<tr><td><code id="multiple_registration_+3A_...">...</code></td>
<td>
<p>additional arguments to the registration method</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of 
</p>

<ul>
<li><p> Y: list of transformed point sets as N x d matrices
</p>
</li></ul>


<hr>
<h2 id='point_sets_from_locs'>point_sets_from_locs</h2><span id='topic+point_sets_from_locs'></span>

<h3>Description</h3>

<p>Extracts list of point sets from a data frame of single molecule localization coordinates.
By default, uses point set membership indicated in the site column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>point_sets_from_locs(
  locs = NULL,
  channels = NULL,
  min.cardinality = NULL,
  max.cardinality = NULL,
  crop.size = NULL,
  keep.locprec = TRUE,
  sample.size = NULL,
  ignore.site = FALSE,
  cluster.points = FALSE,
  eps = NULL,
  minPts = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="point_sets_from_locs_+3A_locs">locs</code></td>
<td>
<p>a data frame with columns x[nm], y[nm], z[nm] and optionally site[numbers], locprec[nm] and locprecz[nm], other columns are ignored.</p>
</td></tr>
<tr><td><code id="point_sets_from_locs_+3A_channels">channels</code></td>
<td>
<p>vector of integers indicating which channel(s) of a multicolour experiment to extract point sets from.</p>
</td></tr>
<tr><td><code id="point_sets_from_locs_+3A_min.cardinality">min.cardinality</code></td>
<td>
<p>filter out point sets with less than the specified number of points.</p>
</td></tr>
<tr><td><code id="point_sets_from_locs_+3A_max.cardinality">max.cardinality</code></td>
<td>
<p>filter out point sets with more than the specified number of points.</p>
</td></tr>
<tr><td><code id="point_sets_from_locs_+3A_crop.size">crop.size</code></td>
<td>
<p>remove points from a set if they are further away than the specified distance from the center of the set.</p>
</td></tr>
<tr><td><code id="point_sets_from_locs_+3A_keep.locprec">keep.locprec</code></td>
<td>
<p>logical (default:TRUE). Whether to keep locprec information for each point.</p>
</td></tr>
<tr><td><code id="point_sets_from_locs_+3A_sample.size">sample.size</code></td>
<td>
<p>returns this number of randomly selected point sets. Selects the point sets after applying eventual filtering.</p>
</td></tr>
<tr><td><code id="point_sets_from_locs_+3A_ignore.site">ignore.site</code></td>
<td>
<p>logical (default: FALSE), set to TRUE if point set membership is not present or needed.</p>
</td></tr>
<tr><td><code id="point_sets_from_locs_+3A_cluster.points">cluster.points</code></td>
<td>
<p>logical (default: FALSE), whether to cluster the points using DBSCAN (only if ignore.site is also TRUE).</p>
</td></tr>
<tr><td><code id="point_sets_from_locs_+3A_eps">eps</code></td>
<td>
<p>DBSCAN parameter, size of the epsilon neighbourhood</p>
</td></tr>
<tr><td><code id="point_sets_from_locs_+3A_minpts">minPts</code></td>
<td>
<p>DBSCAN parameter, number of minimum points in the eps region</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of matrices with columns x,y,z, optionally locprec and name set to the value of the site column (if applicable).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.file &lt;- system.file("test_data", "simulated_NUP107_data.csv", package = "LOMAR",
 mustWork = TRUE)
locs &lt;- locs_from_csv(file = data.file, locprec.filter = 20)
point.sets &lt;- point_sets_from_locs(locs, keep.locprec = TRUE, min.cardinality = 15)
</code></pre>

<hr>
<h2 id='point_sets_from_tiffs'>point_sets_from_tiffs</h2><span id='topic+point_sets_from_tiffs'></span>

<h3>Description</h3>

<p>Read in single molecule localization events from a series of 3D images in TIFF files where each image file
represents a point set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>point_sets_from_tiffs(
  image_dir = NULL,
  pattern = NULL,
  image.size = NULL,
  sample.size = NULL,
  sample.first = FALSE,
  min.cardinality = NULL,
  max.cardinality = NULL,
  crop.size = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="point_sets_from_tiffs_+3A_image_dir">image_dir</code></td>
<td>
<p>path to a directory containing the TIFF files.</p>
</td></tr>
<tr><td><code id="point_sets_from_tiffs_+3A_pattern">pattern</code></td>
<td>
<p>regular expression, select images whose file path matches the given pattern.</p>
</td></tr>
<tr><td><code id="point_sets_from_tiffs_+3A_image.size">image.size</code></td>
<td>
<p>vector of length 3 containing the size of the images along each dimension, e.g. c(40,40,40).</p>
</td></tr>
<tr><td><code id="point_sets_from_tiffs_+3A_sample.size">sample.size</code></td>
<td>
<p>if set, selects this number of images at random. A sample size larger than the available number of samples produces a warning and is ignored.</p>
</td></tr>
<tr><td><code id="point_sets_from_tiffs_+3A_sample.first">sample.first</code></td>
<td>
<p>if TRUE, samples are selected before applying any eventual filtering. This is more efficient as it avoids reading all data files.</p>
</td></tr>
<tr><td><code id="point_sets_from_tiffs_+3A_min.cardinality">min.cardinality</code></td>
<td>
<p>if set, filter out all point sets with less than the specified number of points.</p>
</td></tr>
<tr><td><code id="point_sets_from_tiffs_+3A_max.cardinality">max.cardinality</code></td>
<td>
<p>if set, filter out all point sets with more than the specified number of points.</p>
</td></tr>
<tr><td><code id="point_sets_from_tiffs_+3A_crop.size">crop.size</code></td>
<td>
<p>vector of length 3 containing the desired reduced size of the images along each dimension, e.g. c(30,30,30).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with two elements:
</p>

<ul>
<li><p> point.sets: a list of point sets as matrices with columns x,y,z and 
</p>
</li>
<li><p> file.names: a vector of paths to the TIFF files from which the point sets were extracted.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data.dir &lt;- system.file("test_data/img", package = "LOMAR", mustWork = TRUE) 
point_sets &lt;- point_sets_from_tiffs(image_dir = data.dir, pattern = "\\.tiff?$",
 image.size = c(64, 64, 4), min.cardinality = 10)
</code></pre>

<hr>
<h2 id='points_from_roi'>points_from_roi</h2><span id='topic+points_from_roi'></span>

<h3>Description</h3>

<p>Extract points within given bounding box.
Points are translated so that (0,0,0) correspond to the bounding box corner defined by
roi['min',c('x','y','z')]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>points_from_roi(points, roi)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="points_from_roi_+3A_points">points</code></td>
<td>
<p>a point set as a data frame of coordinates with columns x,y,z.</p>
</td></tr>
<tr><td><code id="points_from_roi_+3A_roi">roi</code></td>
<td>
<p>a data frame with columns x,y,z and rows min and max defining a bounding box</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame with same columns as input
</p>

<hr>
<h2 id='points2img'>points2img</h2><span id='topic+points2img'></span>

<h3>Description</h3>

<p>Convert a data frame of point coordinates into an image.
Expected photon count at each voxel is computed as in:
F. Huang, S. L. Schwartz, J. M. Byars, and K. A. Lidke, “Simultaneous multiple-emitter fitting for single
molecule super-resolution imaging,” Biomed. Opt. Express 2(5), 1377–1393 (2011).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>points2img(points, voxel.size, method, channels = NULL, ncpu = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="points2img_+3A_points">points</code></td>
<td>
<p>a point set as a data frame of coordinates with columns x,y,z.</p>
</td></tr>
<tr><td><code id="points2img_+3A_voxel.size">voxel.size</code></td>
<td>
<p>a numeric vector of length 3 indicating the size of the voxel along x,y and z in the same unit as the coordinates (e.g. nm)</p>
</td></tr>
<tr><td><code id="points2img_+3A_method">method</code></td>
<td>
<p>how to calculate voxel values. Available methods are:
</p>

<ul>
<li><p> 'histogram': value is the number of points (i.e. emitters) in the voxel
</p>
</li>
<li><p> 'photon': value is the expected number of photons from the points in the voxel. Input data frame must have columns locprec, locprecz and phot[on].
</p>
</li></ul>
</td></tr>
<tr><td><code id="points2img_+3A_channels">channels</code></td>
<td>
<p>vector of channels to consider, must be values present in the input data frame channel column</p>
</td></tr>
<tr><td><code id="points2img_+3A_ncpu">ncpu</code></td>
<td>
<p>number of threads to use to speed up computation (default: 1)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an array of dimensions x,y,z and channels if applicable
</p>


<h3>Examples</h3>

<pre><code class='language-R'>point.set &lt;- data.frame(x = c(-9.8,-5.2,12.5,2.5,4.5,1.3,-0.2,0.4,9.3,-1.4,0.5,-1.1,-7.7),
                        y = c(-4.2,1.5,-0.5,12,-3,-7.2,10.9,6.7,-1.3,10,6.7,-6.2,2.9),
                        z = c(3.4,-3.8,-1.4,1.8,3.5,2.5,2.6,-4.8,-3.8,3.9,4.1,-3.6,-4))
img &lt;- points2img(point.set, voxel.size = c(2,2,2), method = 'histogram')
</code></pre>

<hr>
<h2 id='ps2ary'>ps2ary</h2><span id='topic+ps2ary'></span>

<h3>Description</h3>

<p>Convert a list of 3d point sets to a 4d array.
Also works for 2d point sets to 3d array conversion.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ps2ary(point.sets, dims)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ps2ary_+3A_point.sets">point.sets</code></td>
<td>
<p>a list of point sets.</p>
</td></tr>
<tr><td><code id="ps2ary_+3A_dims">dims</code></td>
<td>
<p>vector of dimensions of the axes (x,y in 2d, x,y,z in 3d).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 3d or 4d array.
</p>

<hr>
<h2 id='pssk'>pssk</h2><span id='topic+pssk'></span>

<h3>Description</h3>

<p>Compute the persistence scale-space kernel on persistence diagrams. 
Reference: Jan Reininghaus, Stefan Huber, Ulrich Bauer, and Roland Kwitt. A stable multi-scale kernel for topological machine learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 4741–4748, 2015.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pssk(Dg1 = NULL, Dg2 = NULL, sigma = NULL, dimensions = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pssk_+3A_dg1">Dg1</code></td>
<td>
<p>a persistence diagram as a n1 x 3 matrix where each row is a topological feature
and the columns are dimension, birth and death of the feature.</p>
</td></tr>
<tr><td><code id="pssk_+3A_dg2">Dg2</code></td>
<td>
<p>another persistence diagram as a n2 x 3 matrix</p>
</td></tr>
<tr><td><code id="pssk_+3A_sigma">sigma</code></td>
<td>
<p>kernel bandwidth</p>
</td></tr>
<tr><td><code id="pssk_+3A_dimensions">dimensions</code></td>
<td>
<p>vector of the dimensions of the topological features to consider, if NULL (default) use all available dimensions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>kernel value
</p>


<h3>Examples</h3>

<pre><code class='language-R'>D1 &lt;- matrix(c(0,0,0,1,1,0,0,0,1.5, 3.5,2,2.5,3, 4, 6), ncol = 3, byrow = FALSE)
D2 &lt;- matrix(c(0,0,1,1,0, 0, 1.2, 2, 1.4, 3.2,4.6,6.5), ncol = 3, byrow = FALSE)
K &lt;- pssk(Dg1 = D1, Dg2 = D2, sigma = 1)
</code></pre>

<hr>
<h2 id='q2dr'>Get derivative of 3D rotation matrix from quaternion</h2><span id='topic+q2dr'></span>

<h3>Description</h3>

<p>Get derivative of 3D rotation matrix from quaternion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>q2dr(q)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="q2dr_+3A_q">q</code></td>
<td>
<p>quaternion</p>
</td></tr>
</table>


<h3>Value</h3>

<p>derivative of rotation matrix
</p>

<hr>
<h2 id='q2r'>Convert quaternion to rotation matrix
http://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation</h2><span id='topic+q2r'></span>

<h3>Description</h3>

<p>Convert quaternion to rotation matrix
http://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>q2r(q)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="q2r_+3A_q">q</code></td>
<td>
<p>quaternion</p>
</td></tr>
</table>


<h3>Value</h3>

<p>rotation matrix
</p>

<hr>
<h2 id='restore_coordinates'>restore_coordinates</h2><span id='topic+restore_coordinates'></span>

<h3>Description</h3>

<p>Restore coordinates from mean 0 and standard deviation 1
to their original distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>restore_coordinates(X, mu, sigma)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="restore_coordinates_+3A_x">X</code></td>
<td>
<p>standardized point set as N x D matrix</p>
</td></tr>
<tr><td><code id="restore_coordinates_+3A_mu">mu</code></td>
<td>
<p>1 x D vector of means</p>
</td></tr>
<tr><td><code id="restore_coordinates_+3A_sigma">sigma</code></td>
<td>
<p>standard deviation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>N X D matrix of unstandardized coordinates
</p>

<hr>
<h2 id='rotx'>rotx</h2><span id='topic+rotx'></span>

<h3>Description</h3>

<p>Create a rotation matrix representing a rotation of theta radians about the x-axis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rotx(theta)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rotx_+3A_theta">theta</code></td>
<td>
<p>angle in radians</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 3x3 rotation matrix
</p>

<hr>
<h2 id='roty'>roty</h2><span id='topic+roty'></span>

<h3>Description</h3>

<p>Create a rotation matrix representing a rotation of theta radians about the y-axis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roty(theta)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="roty_+3A_theta">theta</code></td>
<td>
<p>angle in radians</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 3x3 rotation matrix
</p>

<hr>
<h2 id='rotz'>rotz</h2><span id='topic+rotz'></span>

<h3>Description</h3>

<p>Create a rotation matrix representing a rotation of theta radians about the z-axis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rotz(theta)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rotz_+3A_theta">theta</code></td>
<td>
<p>angle in radians</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 3x3 rotation matrix
</p>

<hr>
<h2 id='scale_alpha_shape'>scale_alpha_shape</h2><span id='topic+scale_alpha_shape'></span>

<h3>Description</h3>

<p>Uniformly scale an alpha-shape.
Note that this computes the alpha-shape of the scaled point set 
associated with the input alpha-shape.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scale_alpha_shape(as, s)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="scale_alpha_shape_+3A_as">as</code></td>
<td>
<p>an alpha-shape object of class ashape3d</p>
</td></tr>
<tr><td><code id="scale_alpha_shape_+3A_s">s</code></td>
<td>
<p>scaling factor</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class ashape3d
</p>

<hr>
<h2 id='shape_features_3d'>shape_features_3d</h2><span id='topic+shape_features_3d'></span>

<h3>Description</h3>

<p>Compute shape features of a 3D alpha-shape object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shape_features_3d(as)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shape_features_3d_+3A_as">as</code></td>
<td>
<p>an alpha-shape object of class ashape3d</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Features are:
- major.axis, minor.axis and least.axis: Lengths of the axes of the fitted ellipsoid
- elongation: from 0 (line) to 1 (globular)
- flatness: from 0 (flat) to 1 (spherical)
- max.feret.diameter: Maximum Feret diameter
- max.inscribed.radius: Radius of the largest inscribed sphere
- sphericity: from 0 (not spherical) to 1 (perfect sphere)
- concavity: fraction of the convex hull volume not in the object
- volume
- area: area of the surface of the alpha-shape
</p>


<h3>Value</h3>

<p>a named vector of numeric values or NULL if no non-singular vertices
</p>

<hr>
<h2 id='sliced_Wd'>sliced_Wd</h2><span id='topic+sliced_Wd'></span>

<h3>Description</h3>

<p>Compute sliced Wasserstein distance or kernel. 
Reference: Mathieu Carriere, Marco Cuturi, and Steve Oudot. Sliced Wasserstein kernel for persistence diagrams. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 664–673, 2017.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sliced_Wd(Dg1, Dg2, M = 10, sigma = 1, dimensions = NULL, return.dist = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sliced_Wd_+3A_dg1">Dg1</code></td>
<td>
<p>a persistence diagram as a n1 x 3 matrix where each row is a topological feature
and the columns are dimension, birth and death of the feature.</p>
</td></tr>
<tr><td><code id="sliced_Wd_+3A_dg2">Dg2</code></td>
<td>
<p>another persistence diagram as a n2 x 3 matrix</p>
</td></tr>
<tr><td><code id="sliced_Wd_+3A_m">M</code></td>
<td>
<p>number of slices (default: 10)</p>
</td></tr>
<tr><td><code id="sliced_Wd_+3A_sigma">sigma</code></td>
<td>
<p>kernel bandwidth (default: 1)</p>
</td></tr>
<tr><td><code id="sliced_Wd_+3A_dimensions">dimensions</code></td>
<td>
<p>vector of the dimensions of the topological features to consider, if NULL (default) use all available dimensions</p>
</td></tr>
<tr><td><code id="sliced_Wd_+3A_return.dist">return.dist</code></td>
<td>
<p>logical (default: FALSE). Whether to return the kernel or distance value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>kernel or distance value
</p>


<h3>Examples</h3>

<pre><code class='language-R'>D1 &lt;- matrix(c(0,0,0,1,1,0,0,0,1.5, 3.5,2,2.5,3, 4, 6), ncol = 3, byrow = FALSE)
D2 &lt;- matrix(c(0,0,1,1,0, 0, 1.2, 2, 1.4, 3.2,4.6,6.5), ncol = 3, byrow = FALSE)
K &lt;- sliced_Wd(Dg1 = D1, Dg2 = D2, M = 10, sigma = 1, return.dist = TRUE)
</code></pre>

<hr>
<h2 id='standardize_coordinates'>standardize_coordinates</h2><span id='topic+standardize_coordinates'></span>

<h3>Description</h3>

<p>Transform coordinates to have mean 0 and standard deviation 1
</p>


<h3>Usage</h3>

<pre><code class='language-R'>standardize_coordinates(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="standardize_coordinates_+3A_x">X</code></td>
<td>
<p>point set as N x D matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of 
X: standardized matrix, 
mu: vector of means, 
sigma: standard deviation
</p>

<hr>
<h2 id='tr'>tr</h2><span id='topic+tr'></span>

<h3>Description</h3>

<p>Compute the trace of a matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tr(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tr_+3A_x">x</code></td>
<td>
<p>matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>trace of the matrix
</p>

<hr>
<h2 id='wgmmreg'>wgmmreg</h2><span id='topic+wgmmreg'></span>

<h3>Description</h3>

<p>Rigid registration of two point sets by minimizing the Wasserstein distance between GMMs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wgmmreg(
  X,
  Y,
  CX,
  CY,
  wx = NULL,
  wy = NULL,
  maxIter = 200,
  subsample = NULL,
  tol = 1e-08
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wgmmreg_+3A_x">X</code></td>
<td>
<p>reference point set, a N x D matrix</p>
</td></tr>
<tr><td><code id="wgmmreg_+3A_y">Y</code></td>
<td>
<p>point set to transform, a M x D matrix,</p>
</td></tr>
<tr><td><code id="wgmmreg_+3A_cx">CX</code></td>
<td>
<p>array of covariance matrices for each point in X</p>
</td></tr>
<tr><td><code id="wgmmreg_+3A_cy">CY</code></td>
<td>
<p>array of covariance matrices for each point in Y</p>
</td></tr>
<tr><td><code id="wgmmreg_+3A_wx">wx</code></td>
<td>
<p>(optional) vector of mixture weights for X.</p>
</td></tr>
<tr><td><code id="wgmmreg_+3A_wy">wy</code></td>
<td>
<p>(optional) vector of mixture weights for Y.</p>
</td></tr>
<tr><td><code id="wgmmreg_+3A_maxiter">maxIter</code></td>
<td>
<p>maximum number of iterations to perform (default: 200)</p>
</td></tr>
<tr><td><code id="wgmmreg_+3A_subsample">subsample</code></td>
<td>
<p>if set, use this randomly selected fraction of the points</p>
</td></tr>
<tr><td><code id="wgmmreg_+3A_tol">tol</code></td>
<td>
<p>tolerance for determining convergence (default: 1e-8)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of 
</p>

<ul>
<li><p> Y: transformed point set, 
</p>
</li>
<li><p> R: rotation matrix, 
</p>
</li>
<li><p> t: translation vector,
</p>
</li>
<li><p> c: final value of the cost function,
</p>
</li>
<li><p> converged: logical, whether the algorithm converged.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data.file1 &lt;- system.file("test_data", "parasaurolophusA.txt", package = "LOMAR",
 mustWork = TRUE)
PS1 &lt;- read.csv(data.file1, sep = '\t', header = FALSE)
data.file2 &lt;- system.file("test_data", "parasaurolophusB.txt", package = "LOMAR",
 mustWork = TRUE)
C1 &lt;- diag(0.1, ncol(PS1)) + jitter(0.01, amount = 0.01)
C1 &lt;- replicate(nrow(PS1),C1)
PS2 &lt;- read.csv(data.file2, sep = '\t', header = FALSE)
C2 &lt;- diag(0.1, ncol(PS2)) + jitter(0.01, amount = 0.01)
C2 &lt;- replicate(nrow(PS2),C2)
transformation &lt;- wgmmreg(PS1, PS2, C1, C2, subsample = 0.1, maxIter = 30, tol = 1e-4)
## Not run: 
# Visualize registration outcome
library(rgl)
plot3d(PS1, col = "blue")
points3d(PS2, col = "green")
points3d(transformation[['Y']], col = "magenta")

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
