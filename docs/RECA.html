<!DOCTYPE html><html lang="en"><head><title>Help for package RECA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RECA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#RECA-package'><p>RECA: Relevant Component Analysis for Supervised Distance Metric Learning</p></a></li>
<li><a href='#rca'><p>Relevant Component Analysis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Relevant Component Analysis for Supervised Distance Metric
Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.7</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nan Xiao &lt;me@nanx.me&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Relevant Component Analysis (RCA) tries to find a linear
    transformation of the feature space such that the effect of irrelevant
    variability is reduced in the transformed space.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> | file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://nanx.me/RECA/">https://nanx.me/RECA/</a>, <a href="https://github.com/nanxstats/RECA">https://github.com/nanxstats/RECA</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/nanxstats/RECA/issues">https://github.com/nanxstats/RECA/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-05-17 20:30:54 UTC; nanx</td>
</tr>
<tr>
<td>Author:</td>
<td>Nan Xiao <a href="https://orcid.org/0000-0002-0250-5673"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-05-17 20:40:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='RECA-package'>RECA: Relevant Component Analysis for Supervised Distance Metric Learning</h2><span id='topic+RECA'></span><span id='topic+RECA-package'></span>

<h3>Description</h3>

<p>Relevant Component Analysis (RCA) tries to find a linear
transformation of the feature space such that the effect of irrelevant
variability is reduced in the transformed space.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Nan Xiao <a href="mailto:me@nanx.me">me@nanx.me</a> (0000-0002-0250-5673)
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://nanx.me/RECA/">https://nanx.me/RECA/</a>
</p>
</li>
<li> <p><a href="https://github.com/nanxstats/RECA">https://github.com/nanxstats/RECA</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/nanxstats/RECA/issues">https://github.com/nanxstats/RECA/issues</a>
</p>
</li></ul>


<hr>
<h2 id='rca'>Relevant Component Analysis</h2><span id='topic+rca'></span>

<h3>Description</h3>

<p><code>rca</code> performs relevant component analysis (RCA) for the given data.
It takes a data set and a set of positive constraints as arguments
and returns a linear transformation of the data space into better
representation, alternatively, a Mahalanobis metric over the data space.
</p>
<p>The new representation is known to be optimal in an information
theoretic sense under a constraint of keeping equivalent data
points close to each other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rca(x, chunks, useD = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rca_+3A_x">x</code></td>
<td>
<p><code>n * d</code> matrix or data frame of original data.</p>
</td></tr>
<tr><td><code id="rca_+3A_chunks">chunks</code></td>
<td>
<p>a vector of size <code>N</code> describing the chunklets:
<code>-1</code> in the <code>i</code>-th place says that point <code>i</code> does not
belong to any chunklet; integer <code>j</code> in place <code>i</code> says
that point <code>i</code> belongs to chunklet <code>j</code>;
The chunklets indexes should be <code>1:number-of-chunklets</code>.</p>
</td></tr>
<tr><td><code id="rca_+3A_used">useD</code></td>
<td>
<p>optional. When not given, RCA is done in the
original dimension and <code>B</code> is full rank. When <code>useD</code> is given,
RCA is preceded by constraints based LDA which reduces
the dimension to <code>useD</code>. <code>B</code> in this case is of rank <code>useD</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The three returned objects are just different forms of the same output.
If one is interested in a Mahalanobis metric over the original data space,
the first argument is all she/he needs. If a transformation into another
space (where one can use the Euclidean metric) is preferred, the second
returned argument is sufficient. Using <code>A</code> and <code>B</code> are equivalent
in the following sense:
</p>
<p>if <code>y1 = A * x1</code>, <code>y2 = A * y2</code>  then
</p>
<p><code>(x2 - x1)^T * B * (x2 - x1) = (y2 - y1)^T * (y2 - y1)</code>
</p>


<h3>Value</h3>

<p>A list of the RCA results:
</p>

<ul>
<li> <p><code>B</code>: The RCA suggested Mahalanobis matrix.
Distances between data points <code>x1</code>, <code>x2</code> should be
computed by <code>(x2 - x1)^T * B * (x2 - x1)</code>
</p>
</li>
<li> <p><code>RCA</code>: The RCA suggested transformation of the data.
The data should be transformed by <code>RCA * data</code>
</p>
</li>
<li> <p><code>newX</code>: The data after the RCA transformation.
<code>newX = data * RCA</code>
</p>
</li></ul>



<h3>Note</h3>

<p>Note that any different sets of instances (chunklets),
e.g. <code>{1, 3, 7}</code> and <code>{4, 6}</code>, might belong to the
same class and might belong to different classes.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>References</h3>

<p>Aharon Bar-Hillel, Tomer Hertz, Noam Shental, and Daphna Weinshall (2003).
Learning Distance Functions using Equivalence Relations.
<em>Proceedings of 20th International Conference on
Machine Learning (ICML2003)</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("MASS") # generate synthetic multivariate normal data
set.seed(42)
k &lt;- 100L # sample size of each class
n &lt;- 3L # specify how many classes
N &lt;- k * n # total sample size
x1 &lt;- mvrnorm(k, mu = c(-16, 8), matrix(c(15, 1, 2, 10), ncol = 2))
x2 &lt;- mvrnorm(k, mu = c(0, 0), matrix(c(15, 1, 2, 10), ncol = 2))
x3 &lt;- mvrnorm(k, mu = c(16, -8), matrix(c(15, 1, 2, 10), ncol = 2))
x &lt;- as.data.frame(rbind(x1, x2, x3)) # predictors
y &lt;- gl(n, k) # response

# fully labeled data set with 3 classes
# need to use a line in 2D to classify
plot(x[, 1L], x[, 2L],
  bg = c("#E41A1C", "#377EB8", "#4DAF4A")[y],
  pch = rep(c(22, 21, 25), each = k)
)
abline(a = -10, b = 1, lty = 2)
abline(a = 12, b = 1, lty = 2)

# generate synthetic chunklets
chunks &lt;- vector("list", 300)
for (i in 1:100) chunks[[i]] &lt;- sample(1L:100L, 10L)
for (i in 101:200) chunks[[i]] &lt;- sample(101L:200L, 10L)
for (i in 201:300) chunks[[i]] &lt;- sample(201L:300L, 10L)

chks &lt;- x[unlist(chunks), ]

# make "chunklet" vector to feed the chunks argument
chunksvec &lt;- rep(-1L, nrow(x))
for (i in 1L:length(chunks)) {
  for (j in 1L:length(chunks[[i]])) {
    chunksvec[chunks[[i]][j]] &lt;- i
  }
}

# relevant component analysis
rcs &lt;- rca(x, chunksvec)

# learned transformation of the data
rcs$RCA

# learned Mahalanobis distance metric
rcs$B

# whitening transformation applied to the chunklets
chkTransformed &lt;- as.matrix(chks) %*% rcs$RCA

# original data after applying RCA transformation
# easier to classify - using only horizontal lines
xnew &lt;- rcs$newX
plot(xnew[, 1L], xnew[, 2L],
  bg = c("#E41A1C", "#377EB8", "#4DAF4A")[gl(n, k)],
  pch = c(rep(22, k), rep(21, k), rep(25, k))
)
abline(a = -15, b = 0, lty = 2)
abline(a = 16, b = 0, lty = 2)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
