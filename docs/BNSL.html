<!DOCTYPE html><html><head><title>Help for package BNSL</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BNSL}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bnsl'><p>Bayesian Network Structure Learning</p></a></li>
<li><a href='#bnsl_p'><p>Bayesian Network Structure Learning</p></a></li>
<li><a href='#BNSL-package'>
<p>Bayesian Network Structure Learning</p></a></li>
<li><a href='#cmi'><p>Bayesian Estimation of Conditional Mutual Information</p></a></li>
<li><a href='#FFtable'><p>A faster version of fftable</p></a></li>
<li><a href='#kruskal'><p>Given a weight matrix, generate its maximum weight forest</p></a></li>
<li><a href='#mi'><p>Bayesian Estimation of Mutual Information</p></a></li>
<li><a href='#mi_matrix'><p>Generating its Mutual Information Estimations Matrix</p></a></li>
<li><a href='#parent.set'><p>Parent Set</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Network Structure Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-1-13</td>
</tr>
<tr>
<td>Author:</td>
<td>Joe Suzuki and Jun Kawahara</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Joe Suzuki &lt;j-suzuki@sigmath.es.osaka-u.ac.jp&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>bnlearn, igraph</td>
</tr>
<tr>
<td>Description:</td>
<td>From a given data frame, this package learns its Bayesian network structure based on a selected score.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-01-24 09:01:33 UTC; joe</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-01-24 10:00:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='bnsl'>Bayesian Network Structure Learning</h2><span id='topic+bnsl'></span>

<h3>Description</h3>

<p>The function outputs the Bayesian network structure given a dataset based on 
an assumed criterion.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'> bnsl(df, tw = 0, proc = 1, s=0, n=0, ss=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bnsl_+3A_df">df</code></td>
<td>
<p>a dataframe.</p>
</td></tr>
<tr><td><code id="bnsl_+3A_tw">tw</code></td>
<td>
<p>the upper limit of the parent set.</p>
</td></tr>
<tr><td><code id="bnsl_+3A_proc">proc</code></td>
<td>
<p>the criterion based on which the BNSL solution is sought.
proc=1,2, and 3 indicates that the structure learning is based on Jeffreys [1], MDL [2,3], and BDeu [3]</p>
</td></tr>
<tr><td><code id="bnsl_+3A_s">s</code></td>
<td>
<p>The value computed when obtaining the bound.</p>
</td></tr>
<tr><td><code id="bnsl_+3A_n">n</code></td>
<td>
<p>The number of samples.</p>
</td></tr>
<tr><td><code id="bnsl_+3A_ss">ss</code></td>
<td>
<p>The BDeu parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Bayesian network structure in the bn class of bnlearn.
</p>


<h3>Author(s)</h3>

<p>Joe Suzuki and Jun Kawahara
</p>


<h3>References</h3>

<p>[1] Suzuki, J. &ldquo;An Efficient Bayesian Network Structure Learning Strategy&quot;, New Generation Computing, December 2016.
[2] Suzuki, J. &ldquo;A construction of Bayesian networks from databases based on an MDL principle&quot;, 
Uncertainty in Artificial Intelligence, pages 266-273, Washington D.C. July, 1993.
[3] Suzuki, J. &ldquo;Learning Bayesian Belief Networks Based on the Minimum Description Length Principle: An Efficient Algorithm Using the B &amp; B Technique&quot;,
International Conference on Machine Learning, Bali, Italy, July 1996&quot;
[4] Suzuki, J. &ldquo;A Theoretical Analysis of the BDeu Scores in Bayesian Network Structure Learning&quot;, Behaviormetrika 1(1):1-20,
[5] Suzuki, J.  and Kawahara, J., &ldquo;Branch and Bound for Regular Bayesian Network Structure learning&quot;, Uncertainty in Artificial Intelligence, pages 212-221,  Sydney, Australia, August 2017.
[6] Suzuki, J. &ldquo;Forest Learning from Data and its Universal Coding&quot;, IEEE Transactions on Information Theory, Dec. 2018.
January 2017.
</p>


<h3>See Also</h3>

<p>parent</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(bnlearn)
bnsl(asia)
</code></pre>

<hr>
<h2 id='bnsl_p'>Bayesian Network Structure Learning</h2><span id='topic+bnsl_p'></span>

<h3>Description</h3>

<p>The function outputs the Bayesian network structure given a dataset based on 
an assumed criterion.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'> bnsl_p(df, psl, tw = 0, proc = 1, s=0, n=0, ss=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bnsl_p_+3A_df">df</code></td>
<td>
<p>a dataframe.</p>
</td></tr>
<tr><td><code id="bnsl_p_+3A_psl">psl</code></td>
<td>
<p>the list of parent sets.</p>
</td></tr>
<tr><td><code id="bnsl_p_+3A_tw">tw</code></td>
<td>
<p>the upper limit of the parent set.</p>
</td></tr>
<tr><td><code id="bnsl_p_+3A_proc">proc</code></td>
<td>
<p>the criterion based on which the BNSL solution is sought.
proc=1,2, and 3 indicates that the structure learning is based on Jeffreys [1], MDL [2,3], and BDeu [3]</p>
</td></tr>
<tr><td><code id="bnsl_p_+3A_s">s</code></td>
<td>
<p>The value computed when obtaining the bound.</p>
</td></tr>
<tr><td><code id="bnsl_p_+3A_n">n</code></td>
<td>
<p>The number of samples.</p>
</td></tr>
<tr><td><code id="bnsl_p_+3A_ss">ss</code></td>
<td>
<p>The BDeu parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Bayesian network structure in the bn class of bnlearn.
</p>


<h3>Author(s)</h3>

<p>Joe Suzuki and Jun Kawahara
</p>


<h3>References</h3>

<p>[1] Suzuki, J. &ldquo;An Efficient Bayesian Network Structure Learning Strategy&quot;, New Generation Computing, December 2016.
[2] Suzuki, J. &ldquo;A construction of Bayesian networks from databases based on an MDL principle&quot;, 
Uncertainty in Artificial Intelligence, pages 266-273, Washington D.C. July, 1993.
[3] Suzuki, J. &ldquo;Learning Bayesian Belief Networks Based on the Minimum Description Length Principle: An Efficient Algorithm Using the B &amp; B Technique&quot;,
International Conference on Machine Learning, Bali, Italy, July 1996&quot;
[4] Suzuki, J. &ldquo;A Theoretical Analysis of the BDeu Scores in Bayesian Network Structure Learning&quot;, Behaviormetrika 1(1):1-20,
January 2017.
</p>


<h3>See Also</h3>

<p>parent</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(bnlearn)
p0 &lt;- parent.set(lizards, 0)
p1 &lt;- parent.set(lizards, 1)
p2 &lt;- parent.set(lizards, 2)
bnsl_p(lizards, list(p0, p1, p2))
</code></pre>

<hr>
<h2 id='BNSL-package'>
Bayesian Network Structure Learning
</h2><span id='topic+BNSL-package'></span><span id='topic+BNSL'></span>

<h3>Description</h3>

<p>From a given dataframe,this package learn a Bayesian network structure based on a seletcted score.
</p>


<h3>Details</h3>

<p>Currently,this package estimates of mutual information and conditional mutual information,
and combines them to construct either a Bayesian network or a undirected forest, 
any undirected forest can be a Bayesian network by adding appropriate directions. 
</p>


<h3>Author(s)</h3>

<p>Joe Suzuki and Jun Kawahara
</p>
<p>Maintainer: Joe Suzuki &lt;j-suzuki@sigmath.es.osaka-u.ac.jp&gt;
</p>


<h3>References</h3>

<p>[1]   Suzuki, J., &ldquo;A theoretical analysis of the BDeu scores in Bayesian network structure learning&quot;, Behaviormetrika, 2017.
[2]   Suzuki, J., &ldquo;A novel Chow-Liu algorithm and its application to gene differential analysis&quot;, International Journal of Approximate Reasoning, 2017.
[3]   Suzuki, J., &ldquo;Efficient Bayesian network structure learning for maximizing the posterior probability&quot;, Next-Generation Computing, 2017.
[4]   Suzuki, J., &ldquo;An estimator of mutual information and its application to independence testing&quot;, Entropy, Vol.18, No.4, 2016. 
[5]   Suzuki, J., &ldquo;Consistency of learning Bayesian network structures with continuous variables: An information theoretic approach&quot;. Entropy, Vol.17, No.8, 5752-5770, 2015.
[6]   Suzuki. J.,  &ldquo;Learning Bayesian network structures when discrete and continuous variables are present. In Lecture Note on Artificial Intelligence, the sixth European workshop on Probabilistic Graphical Models, Vol. 8754, pp. 471-486,Utrecht, Netherlands, Sept. 2014. Springer-Verlag.
[7]   Suzuki. J.,  &ldquo;The Bayesian Chow-Liu algorithms&quot;, In the sixth European workshop on Probabilistic Graphical Models, pp. 315-322, Granada, Spain, Sept.2012.
[8] Suzuki, J.  and Kawahara, J., &ldquo;Branch and Bound for Regular Bayesian Network Structure learning&quot;, Uncertainty in Artificial Intelligence, pages 212-221,  Sydney, Australia, August 2017.
[9] Suzuki, J. &ldquo;Forest Learning from Data and its Universal Coding&quot;, IEEE Transactions on Information Theory, Dec. 2018.
January 2017.
</p>

<hr>
<h2 id='cmi'>Bayesian Estimation of Conditional Mutual Information</h2><span id='topic+cmi'></span>

<h3>Description</h3>

<p>A standard estimator of conditional mutual information calculates the maximal likelihood value. 
However, the estimator takes positive values even the pair follows a distribution of two independent variables. 
On the other hand, the estimator in this package detects conditional independence as well as consistently estimates 
the true conditional mutual information value as the length grows based on Jeffrey's prior, 
Bayesian Dirichlet equivalent uniform (BDeu [1]), 
and the MDL principle. It also estimates the conditional mutual information value 
even when one of the pair is continuous (see [2]).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> cmi(x, y, z, proc=0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cmi_+3A_x">x</code></td>
<td>
<p>a numeric vector.</p>
</td></tr>
<tr><td><code id="cmi_+3A_y">y</code></td>
<td>
<p>a numeric vector.</p>
</td></tr>
<tr><td><code id="cmi_+3A_z">z</code></td>
<td>
<p>a numeric vector. x, y and z should have an equal length.</p>
</td></tr>
<tr><td><code id="cmi_+3A_proc">proc</code></td>
<td>
<p>the estimation is based on Jeffrey's prior, the MDL principle, and BDeu for proc=0,1,2, respectively. If the argument proc is missing, proc=0 (Jeffreys') is assumed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the estimation of conditional mutual information between the two numeric vectors based on the selected criterion,
where the natural logarithm base is assumed.
</p>


<h3>Author(s)</h3>

<p>Joe Suzuki and Jun Kawahara
</p>


<h3>References</h3>

<p>[1]   Suzuki, J., &ldquo;A theoretical analysis of the BDeu scores in Bayesian network structure learning&quot;, Behaviormetrika, 2017.
[2]   Suzuki, J., &ldquo;An estimator of mutual information and its application to independence testing&quot;, Entropy, Vol.18, No.4, 2016. 
[3]   Suzuki. J.  &ldquo;The Bayesian Chow-Liu algorithms&quot;, In the sixth European workshop on Probabilistic Graphical Models, pp. 315-322, Granada, Spain, Sept.2012.
</p>


<h3>See Also</h3>

<p>cmi</p>


<h3>Examples</h3>

<pre><code class='language-R'>n=100

x=c(rbinom(n,1,0.2), rbinom(n,1,0.8))
y=c(rbinom(n,1,0.8), rbinom(n,1,0.2))
z=c(rep(1,n),rep(0,n))
cmi(x,y,z,proc=0); cmi(x,y,z,1); cmi(x,y,z,2) 

x=c(rbinom(n,1,0.2), rbinom(n,1,0.8))
u=rbinom(2*n,1,0.1)
y=(x+u)
z=c(rep(1,n),rep(0,n))
cmi(x,y,z); cmi(x,y,z,proc=1); cmi(x,y,z,2) 

</code></pre>

<hr>
<h2 id='FFtable'>A faster version of fftable</h2><span id='topic+FFtable'></span>

<h3>Description</h3>

<p>The same procedure as fftable prepared by the R language. The program is written using Rcpp.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> FFtable(df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FFtable_+3A_df">df</code></td>
<td>
<p>a dataframe.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a frequency table of the last column based on the states that are determined by the other columns.
</p>


<h3>Author(s)</h3>

<p>Joe Suzuki and Jun Kawahara
</p>


<h3>See Also</h3>

<p> fftable </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(bnlearn)
FFtable(asia)
</code></pre>

<hr>
<h2 id='kruskal'>Given a weight matrix, generate its maximum weight forest</h2><span id='topic+kruskal'></span>

<h3>Description</h3>

<p>The function lists the edges of an forest generated by Kruskal's algorithm given its weight matrix
in which each weight should be symmetric but may be negative.
The forest is a spanning tree if the elements of the matrix take positive values.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'> kruskal(W)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kruskal_+3A_w">W</code></td>
<td>
<p>a matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix object of size n x 2 for matrix size n x n in which each row expresses an edge
when the vertexes are expressed by 1 through n.
</p>


<h3>Author(s)</h3>

<p>Joe Suzuki and Jun Kawahara
</p>


<h3>References</h3>

<p>[1]  Suzuki. J.  &ldquo;The Bayesian Chow-Liu algorithms&quot;, In the sixth European workshop on Probabilistic Graphical Models, pp. 315-322, Granada, Spain, Sept.2012.</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(igraph)
library(bnlearn)
df=asia
mi.mat=mi_matrix(df)
edge.list=kruskal(mi.mat)
edge.list
g=graph_from_edgelist(edge.list, directed=FALSE)
V(g)$label=colnames(df)
plot(g)
</code></pre>

<hr>
<h2 id='mi'>Bayesian Estimation of Mutual Information</h2><span id='topic+mi'></span>

<h3>Description</h3>

<p>A standard estimator of mutual information calculates the maximal likelihood value. However, the estimator takes positive values
even the pair follows a distribution of two independent variables. On the other hand, the estimator in this package detects independence as well as consistently estimates 
the true mutual information value as the length grows based on Jeffrey's prior, Bayesian Dirichlet equivalent uniform (BDeu [1]), 
and the MDL principle. It also estimates the mutual information value even when one of the pair is continuous (see [2]).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> mi(x, y, proc=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mi_+3A_x">x</code></td>
<td>
<p>a numeric vector.</p>
</td></tr>
<tr><td><code id="mi_+3A_y">y</code></td>
<td>
<p>a numeric vector. x and y should have a equal length.</p>
</td></tr>
<tr><td><code id="mi_+3A_proc">proc</code></td>
<td>
<p>the estimation is based on Jeffrey's prior, the MDL principle, and BDeu for proc=0,1,2, respectively. 
If one of the two is continuous, proc=10 should be chosen.
If the argument proc is missing, proc=0 (Jeffreys') is assumed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the estimation of mutual information between the two numeric vectors based on the selected criterion,
where the natural logarithm base is assumed.
</p>


<h3>Author(s)</h3>

<p>Joe Suzuki and Jun Kawahara
</p>


<h3>References</h3>

<p>[1]   Suzuki, J., &ldquo;A theoretical analysis of the BDeu scores in Bayesian network structure learning&quot;, Behaviormetrika, 2017.
[2]   Suzuki, J., &ldquo;An estimator of mutual information and its application to independence testing&quot;, Entropy, Vol.18, No.4, 2016. 
[3]   Suzuki. J.  &ldquo;The Bayesian Chow-Liu algorithms&quot;, In the sixth European workshop on Probabilistic Graphical Models, pp. 315-322, Granada, Spain, Sept.2012.
</p>


<h3>See Also</h3>

<p>cmi</p>


<h3>Examples</h3>

<pre><code class='language-R'>n=100

x=rbinom(n,1,0.5); y=rbinom(n,1,0.5); mi(x,y)

z=rbinom(n,1,0.1); y=(x+z)

mi(x,y); mi(x,y,proc=1); mi(x,y,2) 

x=rnorm(n); y=rnorm(n); mi(x,y,proc=10)

x=rnorm(n); z=rnorm(n); y=0.9*x+sqrt(1-0.9^2)*z; mi(x,y,proc=10)

</code></pre>

<hr>
<h2 id='mi_matrix'>Generating its Mutual Information Estimations Matrix</h2><span id='topic+mi_matrix'></span>

<h3>Description</h3>

<p>The estimators in this package detect independence as well as consistently estimates 
the true conditional mutual information value as the length grows based on Jeffrey's prior,
Bayesian Dirichlet equivalent uniform (BDeu [1]), 
and the MDL principle. It also estimates the conditional mutual information value 
even when one of the pair is continuous (see [2]).
Given a data frame each column of which may be either discrete or continuous, this function generates its mutual information estimation matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mi_matrix(df, proc=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mi_matrix_+3A_df">df</code></td>
<td>
<p>a data frame.</p>
</td></tr>
<tr><td><code id="mi_matrix_+3A_proc">proc</code></td>
<td>
<p>given two discrete vectors of equal length, the function estimates the mutual information
based on Jeffrey's prior, the MDL principle, and BDeu for proc=0,1,2, respectively. If one of the columns is continuous, proc=10 should be chosen.
If the argument proc is missing, proc=0 (Jeffreys') is assumed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the estimation of mutual information between the two numeric vectors based on the selected criterion,
where the natural logarithm base is assumed.
</p>


<h3>Author(s)</h3>

<p>Joe Suzuki and Jun Kawahara
</p>


<h3>References</h3>

<p>[1]   Suzuki, J., &ldquo;A theoretical analysis of the BDeu scores in Bayesian network structure learning&quot;, Behaviormetrika, 2017.
[2]   Suzuki, J., &ldquo;An estimator of mutual information and its application to independence testing&quot;, Entropy, Vol.18, No.4, 2016. 
[3]   Suzuki. J.,  &ldquo;A novel Chow?Liu algorithm and its application to gene differential analysis&quot;, 
International Journal of Approximate Reasoning, Vol. 80, 2017.
</p>


<h3>See Also</h3>

<p>mi</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(bnlearn)
mi_matrix(asia)
mi_matrix(asia,proc=1)
mi_matrix(asia,proc=2)
mi_matrix(asia,proc=3)
</code></pre>

<hr>
<h2 id='parent.set'>Parent Set</h2><span id='topic+parent.set'></span>

<h3>Description</h3>

<p>This function estimates a parent set of h in each subset w as follows:
Suppose we are given a subset w of the p-1 variables excluding h, where p is the number of columns in df.
Then, a score is defined for each subset w, where
the score expresses how well the subset is likely to be the true parent set of h in w. 
Currently, a Bayesian score (Jeffreys' prior) is applied. 
This function computes the maximum score z and its subset y of w. 
This function computes y and z for all w, where w and y are exprssed by binary sequences of length p, respectively.
When the computation is heavy, it can be reduced by specifying the maximum size of w, 
If tw is zero (default), the tw value is set to p-1, Otherwise, the tw value expresses the maximum size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> parent.set(df, h, tw=0, proc=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parent.set_+3A_df">df</code></td>
<td>
<p>a data frame.</p>
</td></tr>
<tr><td><code id="parent.set_+3A_h">h</code></td>
<td>
<p>an integer from 0 to p-1, where p is the number of columns in df.</p>
</td></tr>
<tr><td><code id="parent.set_+3A_tw">tw</code></td>
<td>
<p>an integer from 0 to p-1, where p is the number of columns in df.</p>
</td></tr>
<tr><td><code id="parent.set_+3A_proc">proc</code></td>
<td>
<p>the parent sets are estimated based on Jeffreys' (proc=0,1) [1], MDL (proc=2) [2,3], and BDeu (proc=3) [4].</p>
</td></tr> 
</table>


<h3>Value</h3>

<p>the data frame in which each row consists of the triples (w,y,z):
w is a subset of the p-1 variables excluding h; y is the parent set for w; and
z is the score of the parent set.
</p>


<h3>Author(s)</h3>

<p>Joe Suzuki and Jun Kawahara
</p>


<h3>References</h3>

<p>[1]  Suzuki, J., &ldquo;An Efficient Bayesian Network Structure Learning Strategy&quot;, New Generation Computing, December 2016.
[2]  Suzuki, A., &ldquo;Construction of Bayesian Networks from Databases Based on an MDL Principle&quot;, 
Proceedings of the Ninth Annual Conference on Uncertainty in Artificial Intelligence, 
The Catholic University of America, Providence, Washington, DC, USA, July 9-11, 1993.
[3]  Suzuki, J., &ldquo;Learning Bayesian Belief Networks Based on the Minimum Description Length Principle: An Efficient Algorithm Using the B &amp; B Technique.&quot;,
Proceedings of the Thirteenth International Conference (ICML '96), Bari, Italy, July 3-6, 1996.
[4]  Suzuki, J., &ldquo;A theoretical analysis of the BDeu scores in Bayesian network structure learning&quot;, Behaviormetrika, 2017.
</p>


<h3>See Also</h3>

<p>cmi</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(bnlearn)
df=asia
parent.set(df,7)
parent.set(df,7,1)
parent.set(df,7,2)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
