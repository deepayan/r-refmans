<!DOCTYPE html><html><head><title>Help for package mixdir</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mixdir}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#find_defining_features'><p>Find the n defining features</p></a></li>
<li><a href='#find_predictive_features'><p>Find the top predictive features and values for each latent class</p></a></li>
<li><a href='#find_typical_features'><p>Find the most typical features and values for each latent class</p></a></li>
<li><a href='#mixdir'><p>Cluster high dimensional categorical datasets</p></a></li>
<li><a href='#mushroom'><p>Properties of 8124 mushrooms.</p></a></li>
<li><a href='#plot_features'><p>Plot cluster distribution for a subset of features features</p></a></li>
<li><a href='#predict.mixdir'><p>Predict the class of a new observation.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Cluster High Dimensional Categorical Datasets</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Scalable Bayesian clustering of categorical datasets. The package implements a hierarchical Dirichlet 
    (Process) mixture  of multinomial distributions. It is thus a probabilistic latent class model (LCM) and can be used
    to reduce the  dimensionality of hierarchical data and cluster individuals into latent classes. It can automatically
    infer an appropriate number of latent classes or find k classes, as defined by the user.  The model is based on a
    paper by Dunson and Xing (2009) &lt;<a href="https://doi.org/10.1198%2Fjasa.2009.tm08439">doi:10.1198/jasa.2009.tm08439</a>&gt;, but implements a scalable variational inference algorithm so that it is
    applicable to large datasets. It is described and tested in the accompanying paper by 
    Ahlmann-Eltze and Yau (2018) &lt;<a href="https://doi.org/10.1109%2FDSAA.2018.00068">doi:10.1109/DSAA.2018.00068</a>&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/const-ae/mixdir">https://github.com/const-ae/mixdir</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, tibble, purrr, dplyr, rmutil, pheatmap, mcclust,
ggplot2, tidyr, utils</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>extraDistr, Rcpp</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-09-20 14:51:59 UTC; ahlmanne</td>
</tr>
<tr>
<td>Author:</td>
<td>Constantin Ahlmann-Eltze
    <a href="https://orcid.org/0000-0002-3762-068X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Christopher Yau <a href="https://orcid.org/0000-0001-7615-8523"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ths]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Constantin Ahlmann-Eltze &lt;artjom31415@googlemail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-09-20 15:10:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='find_defining_features'>Find the n defining features</h2><span id='topic+find_defining_features'></span>

<h3>Description</h3>

<p>Reduce the dimensionality of a dataset by calculating how important each feature is
for inferring the clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_defining_features(mixdir_obj, X, n_features = Inf,
  measure = c("JS", "ARI"), subsample_size = Inf, step_size = Inf,
  exponential_decay = TRUE, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_defining_features_+3A_mixdir_obj">mixdir_obj</code></td>
<td>
<p>the result from a call to <code>mixdir()</code>. It needs to have the
fields category_prob. category_prob a list of a list of a named vector with probabilities
for each feature, latent class and possible category.</p>
</td></tr>
<tr><td><code id="find_defining_features_+3A_x">X</code></td>
<td>
<p>the original dataset that was used for clustering.</p>
</td></tr>
<tr><td><code id="find_defining_features_+3A_n_features">n_features</code></td>
<td>
<p>the number of dimensions that should be selected. If it is
<code>Inf</code> (the default) all features are returned ordered by importance
(most important first).</p>
</td></tr>
<tr><td><code id="find_defining_features_+3A_measure">measure</code></td>
<td>
<p>The measure used to assess the loss of clustering quality
if a variable is removed. Two measures are implemented: &quot;JS&quot; short for
Jensen-Shannon divergence comparing the original class probabilities
and the new predicted class probabilities (smaller is better),
&quot;ARI&quot; short for adjusted Rand index compares the overlap of the original
and the predicted classes (requires the <code>mcclust</code> package) (1 is perfect,
0 is as good as random).</p>
</td></tr>
<tr><td><code id="find_defining_features_+3A_subsample_size">subsample_size</code></td>
<td>
<p>Running this method on the full dataset can be slow,
but one can easily speed up the calculation by randomly selecting
a subset of rows from X without usually disproportionately hurting the
selection performance.</p>
</td></tr>
<tr><td><code id="find_defining_features_+3A_step_size">step_size</code></td>
<td>
<p>The method can either remove each feature individually
and return the n features that caused the greatest quality loss
(<code>step=Inf</code>) or iteratively remove the least important one until
the the size of the remaining features equal <code>n_features</code>
(<code>step=1</code>). Using a smaller step size increases the sensitivity
of the selection process, but takes longer to calculate.</p>
</td></tr>
<tr><td><code id="find_defining_features_+3A_exponential_decay">exponential_decay</code></td>
<td>
<p>Boolean or number. Alternative way of
calculating how many features to remove each step. The default is
to always remove the least important 50% of the features
(<code>exponential_decay=2</code>).</p>
</td></tr>
<tr><td><code id="find_defining_features_+3A_verbose">verbose</code></td>
<td>
<p>Boolean indicating if status messages should be printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Iteratively find the variable, whose removal least affects the
clustering compared with the original. If <code>n_features</code> is a finite number
the quality is a single number and reflects how good those n features maintain
the original clustering. If <code>n_features=Inf</code>, the method returns all features
ordered by decreasing importance. The accompanying quality vector contains the
&quot;cumulative&quot; loss if the corresponding variable would be removed.
Note that depending on the step size scheme the quality can differ. For example
if all variables are removed in one step (<code>step_size=Inf</code> and
<code>exponential_decay=FALSE</code>) the quality is not cumulative, but simply the
quality of the clustering excluding the corresponding feature. In that
sense the quality vector should not be used as a definitive answer, but
should only be used as a guidance to see where there are jumps in the quality.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_predictive_features">find_predictive_features</a></code> <code><a href="#topic+find_typical_features">find_typical_features</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  data("mushroom")
  res &lt;- mixdir(mushroom[1:100, ], n_latent=20)
  find_defining_features(res, mushroom[1:100, ], n_features=3)
  find_defining_features(res, mushroom[1:100, ], n_features=Inf)
  
</code></pre>

<hr>
<h2 id='find_predictive_features'>Find the top predictive features and values for each latent class</h2><span id='topic+find_predictive_features'></span>

<h3>Description</h3>

<p>Find the top predictive features and values for each latent class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_predictive_features(mixdir_obj, top_n = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_predictive_features_+3A_mixdir_obj">mixdir_obj</code></td>
<td>
<p>the result from a call to <code>mixdir()</code>. It needs to have the
fields lambda and category_prob. lambda a vector of probabilities for each category.
category_prob a list of a list of a named vector with probabilities
for each feature, latent class and possible category.</p>
</td></tr>
<tr><td><code id="find_predictive_features_+3A_top_n">top_n</code></td>
<td>
<p>the number of top answers per category that will be returned. Default: 10.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with four columns: column, answer, class and probability.
The probability column contains the chance that an observation belongs to
the latent class if all that is known about that observation that
<code>`column`=`category`</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_typical_features">find_typical_features</a></code> <code><a href="#topic+find_defining_features">find_defining_features</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data("mushroom")
  res &lt;- mixdir(mushroom[1:30, ], beta=1)
  find_predictive_features(res, top_n=3)

</code></pre>

<hr>
<h2 id='find_typical_features'>Find the most typical features and values for each latent class</h2><span id='topic+find_typical_features'></span>

<h3>Description</h3>

<p>Find the most typical features and values for each latent class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_typical_features(mixdir_obj, top_n = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_typical_features_+3A_mixdir_obj">mixdir_obj</code></td>
<td>
<p>the result from a call to <code>mixdir()</code>. It needs to have the
fields lambda and category_prob. lambda a vector of probabilities for each category.
category_prob a list of a list of a named vector with probabilities
for each feature, latent class and possible category.</p>
</td></tr>
<tr><td><code id="find_typical_features_+3A_top_n">top_n</code></td>
<td>
<p>the number of top answers per category that will be returned. Default: 10.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with four columns: column, answer, class and probability.
The probability column contains the chance to see the answer in that column.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_predictive_features">find_predictive_features</a></code> <code><a href="#topic+find_defining_features">find_defining_features</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data("mushroom")
  res &lt;- mixdir(mushroom[1:30, ], beta=1)
  find_typical_features(res, top_n=3)

</code></pre>

<hr>
<h2 id='mixdir'>Cluster high dimensional categorical datasets</h2><span id='topic+mixdir'></span>

<h3>Description</h3>

<p>Cluster high dimensional categorical datasets
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixdir(X, n_latent = 3, alpha = NULL, beta = NULL,
  select_latent = FALSE, max_iter = 100, epsilon = 0.001,
  na_handle = c("ignore", "category"), repetitions = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixdir_+3A_x">X</code></td>
<td>
<p>A matrix or data.frame of size (N_ind x N_quest) that contains the categorical responses.
The values can be characters, integers or factors. The most flexibility is provided if factors are used.</p>
</td></tr>
<tr><td><code id="mixdir_+3A_n_latent">n_latent</code></td>
<td>
<p>The number of latent factors that are used to approximate the model. Default: 3.</p>
</td></tr>
<tr><td><code id="mixdir_+3A_alpha">alpha</code></td>
<td>
<p>A single number or a vector of two numbers in case select_latent=TRUE. If it is NULL alpha
is initialized to 1. It serves as prior for the Dirichlet distributions over the latent groups. They
serve as pseudo counts of individuals per group.</p>
</td></tr>
<tr><td><code id="mixdir_+3A_beta">beta</code></td>
<td>
<p>A single number. If it is NULL beta is initialized to 0.1.
It serves as a prior for the Dirichlet distributions over the categorical responses. Large numbers
favor an equal distribution of responses for a question of the individuals in the same latent group,
small numbers indicate that individuals of the same latent group usually answer a question the same way.</p>
</td></tr>
<tr><td><code id="mixdir_+3A_select_latent">select_latent</code></td>
<td>
<p>A boolean that indicates if the exact number n_latent should be used or if a Dirichlet
Process prior is used that shrinks the number of used latent variables appropriately (can be controlled
with alpha=c(a1, a2) and beta). Default: FALSE.</p>
</td></tr>
<tr><td><code id="mixdir_+3A_max_iter">max_iter</code></td>
<td>
<p>The maximum number of iterations.</p>
</td></tr>
<tr><td><code id="mixdir_+3A_epsilon">epsilon</code></td>
<td>
<p>A number that indicates the numerical precision necessary to consider the algorithm converged.</p>
</td></tr>
<tr><td><code id="mixdir_+3A_na_handle">na_handle</code></td>
<td>
<p>Either &quot;ignore&quot; or &quot;category&quot;. If it is &quot;category&quot; all <code>NA</code>'s in the dataset are converted to
the string &quot;(Missing)&quot; and treated as their own category. If it is &quot;ignore&quot; the <code>NA</code>'s are treated as missing completely
at random and are ignored during the parameter updates.</p>
</td></tr>
<tr><td><code id="mixdir_+3A_repetitions">repetitions</code></td>
<td>
<p>A number specifying how often to repeat the calculation with different initializations. Automatically
selects the best run (i.e. max(ELBO)). Default: 1.</p>
</td></tr>
<tr><td><code id="mixdir_+3A_...">...</code></td>
<td>
<p>Additional parameters passed on to the underlying functions. The parameters are verbose, phi_init,
zeta_init and if select_latent=FALSE omega_init or if select_latent=TRUE kappa1_init and kappa2_init.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses a mixture of multinomials to fit the model.
The full model specification is
</p>
<p style="text-align: center;"><code class="reqn"> \lambda | \alpha \sim DirichletProcess(\alpha)</code>
</p>

<p style="text-align: center;"><code class="reqn"> z_i | \lambda \sim Multinomial(\lambda)</code>
</p>

<p style="text-align: center;"><code class="reqn"> U_{j,k} | \beta \sim Dirichlet(\beta)</code>
</p>

<p style="text-align: center;"><code class="reqn"> X_{i,j} | U_j, z_i=k \sim Multinomial(U_{j,k})</code>
</p>

<p>In case that <code>select_latent=FALSE</code> the first line is replaced with
</p>
<p style="text-align: center;"><code class="reqn"> \lambda | \alpha \sim Dirichlet(\alpha)</code>
</p>

<p>The initial inspiration came from  Dunson and Xing (2009) who proposed a Gibbs
sampling algorithm to solve this model. To speed up inference
a variational inference approach was derived and implemented in this package.
</p>


<h3>Value</h3>

<p>A list that is tagged with the class &quot;mixdir&quot; containing 8 elements:
</p>

<dl>
<dt>converged</dt><dd><p>a boolean indicator if the model has converged</p>
</dd>
<dt>convergence</dt><dd><p>a numerical vector with the ELBO of each iteration</p>
</dd>
<dt>ELBO</dt><dd><p>the final ELBO of the converged model</p>
</dd>
<dt>lambda</dt><dd><p>a numerical vector with the <code>n_latent</code> class probabilities</p>
</dd>
<dt>pred_class</dt><dd><p>an integer vector with the the most likely class assignment
for each individual.</p>
</dd>
<dt>class_prob</dt><dd><p>a matrix of size <code>n_ind x n_latent</code> which has for each
individual the probability to belong to class k.</p>
</dd>
<dt>category_prob</dt><dd><p>a list with one entry for each feature (i.e. column of X).
Each entry is again a list with one entry for each class, that contains the
probability of individuals of that class to answer with a specific response.</p>
</dd>
<dt>specific_params</dt><dd><p>A list whose content depends on the parameter <code>select_latent</code>.
If <code>select_latent=FALSE</code> it contains the two entries omega and phi which
are the Dirichlet hyperparameters that the model has fitted. If <code>select_latent=TRUE</code>
it contains kappa1, kappa2 and phi, which are the hyperparameters for the
Dirichlet Process and the Dirichlet of the answer.</p>
</dd>
<dt>na_handle</dt><dd><p>a string indicating the method used to handle missing values. This
is important for subsequent calls to <code>predict.mixdir</code>.</p>
</dd>
</dl>



<h3>References</h3>

<p>1. C. Ahlmann-Eltze and C. Yau, &quot;MixDir: Scalable Bayesian Clustering for High-Dimensional Categorical Data&quot;, 2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA), Turin, Italy, 2018, pp. 526-539.
</p>
<p>2. Dunson, D. B. and Xing, C. Nonparametric Bayes Modeling of Multivariate Categorical Data. J. Am. Stat. Assoc. 104, 1042–1051 (2009).
</p>
<p>3. Blei, D. M., Ng, A. Y. and Jordan, M. I. Latent Dirichlet Allocation. J. Macine Learn. Res. 3, 993–1022 (2003).
</p>
<p>4. Blei, D. M. and Jordan, M. I. Variational inference for Dirichlet process mixtures. Bayesian Anal. 1, 121–144 (2006).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data("mushroom")
  res &lt;- mixdir(mushroom[1:30, ])

</code></pre>

<hr>
<h2 id='mushroom'>Properties of 8124 mushrooms.</h2><span id='topic+mushroom'></span>

<h3>Description</h3>

<p>A dataset containing 23 categorical properties of 23 different species of gilled
mushrooms including a categorization if it is edible or not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mushroom
</code></pre>


<h3>Format</h3>

<p>A data frame with 8124 rows and 23 columns:
</p>

<dl>
<dt>bruises</dt><dd><p><code>bruises</code> <code>no</code></p>
</dd>
<dt>cap-color</dt><dd><p><code>brown</code> <code>yellow</code> <code>white</code> <code>gray</code> <code>red</code> <code>pink</code> <code>buff</code> <code>purple</code> <code>cinnamon</code> <code>green</code></p>
</dd>
<dt>cap-shape</dt><dd><p><code>convex</code> <code>bell</code> <code>sunken</code> <code>flat</code> <code>knobbed</code> <code>conical</code></p>
</dd>
<dt>cap-surface</dt><dd><p><code>smooth</code> <code>scaly</code> <code>fibrous</code> <code>grooves</code></p>
</dd>
<dt>edible</dt><dd><p><code>poisonous</code> <code>edible</code></p>
</dd>
<dt>gill-attachment</dt><dd><p><code>free</code> <code>attached</code></p>
</dd>
<dt>gill-color</dt><dd><p><code>black</code> <code>brown</code> <code>gray</code> <code>pink</code> <code>white</code> <code>chocolate</code> <code>purple</code> <code>red</code> <code>buff</code> <code>green</code> <code>yellow</code> <code>orange</code></p>
</dd>
<dt>gill-size</dt><dd><p><code>narrow</code> <code>broad</code></p>
</dd>
<dt>gill-spacing</dt><dd><p><code>close</code> <code>crowded</code></p>
</dd>
<dt>habitat</dt><dd><p><code>urban</code> <code>grasses</code> <code>meadows</code> <code>woods</code> <code>paths</code> <code>waste</code> <code>leaves</code></p>
</dd>
<dt>odor</dt><dd><p><code>pungent</code> <code>almond</code> <code>anise</code> <code>none</code> <code>foul</code> <code>creosote</code> <code>fishy</code> <code>spicy</code> <code>musty</code></p>
</dd>
<dt>population</dt><dd><p><code>scattered</code> <code>numerous</code> <code>abundant</code> <code>several</code> <code>solitary</code> <code>clustered</code></p>
</dd>
<dt>ring-number</dt><dd><p><code>one</code> <code>two</code> <code>none</code></p>
</dd>
<dt>ring-type</dt><dd><p><code>pendant</code> <code>evanescent</code> <code>large</code> <code>flaring</code> <code>none</code></p>
</dd>
<dt>spore-print-color</dt><dd><p><code>black</code> <code>brown</code> <code>purple</code> <code>chocolate</code> <code>white</code> <code>green</code> <code>orange</code> <code>yellow</code> <code>buff</code></p>
</dd>
<dt>stalk-color-above-ring</dt><dd><p><code>white</code> <code>gray</code> <code>pink</code> <code>brown</code> <code>buff</code> <code>red</code> <code>orange</code> <code>cinnamon</code> <code>yellow</code></p>
</dd>
<dt>stalk-color-below-ring</dt><dd><p><code>white</code> <code>pink</code> <code>gray</code> <code>buff</code> <code>brown</code> <code>red</code> <code>yellow</code> <code>orange</code> <code>cinnamon</code></p>
</dd>
<dt>stalk-root</dt><dd><p><code>equal</code> <code>club</code> <code>bulbous</code> <code>rooted</code> <code>NA</code></p>
</dd>
<dt>stalk-shape</dt><dd><p><code>enlarging</code> <code>tapering</code></p>
</dd>
<dt>stalk-surface-above-ring</dt><dd><p><code>smooth</code> <code>fibrous</code> <code>silky</code> <code>scaly</code></p>
</dd>
<dt>stalk-surface-below-ring</dt><dd><p><code>smooth</code> <code>fibrous</code> <code>scaly</code> <code>silky</code></p>
</dd>
<dt>veil-color</dt><dd><p><code>white</code> <code>brown</code> <code>orange</code> <code>yellow</code></p>
</dd>
<dt>veil-type</dt><dd><p><code>partial</code></p>
</dd>
</dl>


<h3>Details</h3>

<p>The records are drawn from
G. H. Lincoff (1981) (Pres.),
<em>The Audubon Society Field Guide to North American Mushrooms</em>.
New York: Alfred A. Knopf.
(See pages 500&ndash;525 for the Agaricus and Lepiota Family.)
</p>
<p>The Guide clearly states that there is no simple rule for determining
the edibility of a mushroom; no rule like &ldquo;leaflets three, let
it be&rdquo; for Poisonous Oak and Ivy.
</p>
<p>The actual dataset from the UCI repository has been cleaned up to properly
label the missing values and have the full category names instead of their
abbreviations.
</p>


<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/Mushroom">https://archive.ics.uci.edu/ml/datasets/Mushroom</a>
</p>


<h3>References</h3>

<p>Blake, C.L. &amp; Merz, C.J. (1998).
UCI Repository of Machine Learning Databases.
Irvine, CA: University of California, Department of Information and
Computer Science.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data("mushroom")
  summary(mushroom)

</code></pre>

<hr>
<h2 id='plot_features'>Plot cluster distribution for a subset of features features</h2><span id='topic+plot_features'></span>

<h3>Description</h3>

<p>Plot cluster distribution for a subset of features features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_features(features, category_prob,
  classes = seq_len(length(category_prob[[1]])))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_features_+3A_features">features</code></td>
<td>
<p>a character vector with feature names</p>
</td></tr>
<tr><td><code id="plot_features_+3A_category_prob">category_prob</code></td>
<td>
<p>a list over all features containing a
list of the probability of each answer for every class. It
is usually obtained from the result of a call to <code>mixdir()</code>.</p>
</td></tr>
<tr><td><code id="plot_features_+3A_classes">classes</code></td>
<td>
<p>numerical vector specifying which latent classes are plotted. By default all.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>  
    data("mushroom")
    res &lt;- mixdir(mushroom[1:100, ], n_latent=4)
    plot_features(c("bruises", "edible"), res$category_prob)

    res2 &lt;- mixdir(mushroom[1:100, ], n_latent=20)
    def_feats &lt;- find_defining_features(res2, mushroom[1:100, ], n_features=Inf)
    plot_features(def_feats$features[1:6], category_prob = res2$category_prob,
                  classes=which(res$lambda &gt; 0.01))
   
</code></pre>

<hr>
<h2 id='predict.mixdir'>Predict the class of a new observation.</h2><span id='topic+predict.mixdir'></span>

<h3>Description</h3>

<p>Predict the class of a new observation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mixdir'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.mixdir_+3A_object">object</code></td>
<td>
<p>the result from a call to <code>mixdir()</code>. It needs to have the
fields lambda, category_prob and na_handle. lambda is a vector of probabilities for each category.
category_prob a list of a list of a named vector with probabilities
for each feature, latent class and possible category. na_handle must either be
&quot;ignore&quot; or &quot;category&quot; depending how NA's should be handled.</p>
</td></tr>
<tr><td><code id="predict.mixdir_+3A_newdata">newdata</code></td>
<td>
<p>a named vector with a single new observation or a data.frame
with the same structure as the original data used for fitting the model.
Missing features or features not encountered during training are replaced by
NA.</p>
</td></tr>
<tr><td><code id="predict.mixdir_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of with the same number of rows as the input and one column for each latent class.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data("mushroom")
  X &lt;- as.matrix(mushroom)[1:30, ]

  res &lt;- mixdir(X)

  # Predict Class
  predict(res, mushroom[40:45, ])
  predict(res, c(`gill-color`="black"))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
