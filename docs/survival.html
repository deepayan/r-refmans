<!DOCTYPE html><html><head><title>Help for package survival</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {survival}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aareg'>
<p>Aalen's additive regression model for censored data</p></a></li>
<li><a href='#aeqSurv'><p>Adjudicate near ties in a Surv object</p></a></li>
<li><a href='#aggregate.survfit'><p>Average survival curves</p></a></li>
<li><a href='#agreg.fit'><p>Cox model fitting functions</p></a></li>
<li><a href='#aml'><p>Acute Myelogenous Leukemia survival data</p></a></li>
<li><a href='#anova.coxph'><p>Analysis of Deviance for a Cox model.</p></a></li>
<li><a href='#attrassign'><p>Create new-style &quot;assign&quot; attribute</p></a></li>
<li><a href='#basehaz'><p>Alias for the survfit function</p></a></li>
<li><a href='#bladder'><p>Bladder Cancer Recurrences</p></a></li>
<li><a href='#blogit'>
<p>Bounded link functions</p></a></li>
<li><a href='#brier'><p>Compute the Brier score for a Cox model</p></a></li>
<li><a href='#cch'><p>Fits proportional hazards regression model to case-cohort data</p></a></li>
<li><a href='#cgd'><p>Chronic Granulotamous Disease data</p></a></li>
<li><a href='#cgd0'><p>Chronic Granulotomous Disease data</p></a></li>
<li><a href='#cipoisson'><p>Confidence limits for the Poisson</p></a></li>
<li><a href='#clogit'><p>Conditional logistic regression</p></a></li>
<li><a href='#cluster'>
<p>Identify clusters.</p></a></li>
<li><a href='#colon'><p>Chemotherapy for Stage B/C colon cancer</p></a></li>
<li><a href='#concordance'><p>Compute the concordance statistic for data or a model</p></a></li>
<li><a href='#concordancefit'><p>Compute the concordance</p></a></li>
<li><a href='#cox.zph'>
<p>Test the Proportional Hazards Assumption of a Cox Regression</p></a></li>
<li><a href='#coxph'>
<p>Fit Proportional Hazards Regression Model</p></a></li>
<li><a href='#coxph.control'><p>Ancillary arguments for controlling coxph fits</p></a></li>
<li><a href='#coxph.detail'>
<p>Details of a Cox Model Fit</p></a></li>
<li><a href='#coxph.object'>
<p>Proportional Hazards Regression Object</p></a></li>
<li><a href='#coxph.wtest'><p>Compute a quadratic form</p></a></li>
<li><a href='#coxphms.object'>
<p>Multi-state Proportional Hazards Regression Object</p></a></li>
<li><a href='#coxsurv.fit'>
<p>A direct interface to the &lsquo;computational engine&rsquo; of survfit.coxph</p></a></li>
<li><a href='#diabetic'><p>Ddiabetic retinopathy</p></a></li>
<li><a href='#dsurvreg'>
<p>Distributions available in survreg.</p></a></li>
<li><a href='#finegray'><p>Create data for a Fine-Gray model</p></a></li>
<li><a href='#flchain'><p>Assay of serum free light chain for 7874 subjects.</p></a></li>
<li><a href='#frailty'>
<p>Random effects terms</p></a></li>
<li><a href='#gbsg'><p>Breast cancer data sets used in Royston and Altman (2013)</p></a></li>
<li><a href='#heart'><p>Stanford Heart Transplant data</p></a></li>
<li><a href='#hoel'><p>Mouse cancer data</p></a></li>
<li><a href='#is.ratetable'>
<p>Verify that an object is of class ratetable.</p></a></li>
<li><a href='#kidney'><p>Kidney catheter data</p></a></li>
<li><a href='#levels.Surv'><p>Return the states of a multi-state Surv object</p></a></li>
<li><a href='#lines.survfit'>
<p>Add Lines or Points to a Survival Plot</p></a></li>
<li><a href='#logan'><p>Data from the 1972-78 GSS data used by Logan</p></a></li>
<li><a href='#logLik.coxph'><p>logLik method for a Cox model</p></a></li>
<li><a href='#lung'><p>NCCTG Lung Cancer Data</p></a></li>
<li><a href='#mgus'><p>Monoclonal gammopathy data</p></a></li>
<li><a href='#mgus2'><p>Monoclonal gammopathy data</p></a></li>
<li><a href='#model.frame.coxph'><p>Model.frame method for coxph objects</p></a></li>
<li><a href='#model.matrix.coxph'>
<p>Model.matrix method for coxph models</p></a></li>
<li><a href='#myeloid'><p>Acute myeloid leukemia</p></a></li>
<li><a href='#myeloma'>
<p>Survival times of patients with multiple myeloma</p></a></li>
<li><a href='#nafld'><p>Non-alcoholic fatty liver disease</p></a></li>
<li><a href='#neardate'>
<p>Find the index of the closest value in data set 2, for each entry in</p>
data set one.</a></li>
<li><a href='#nsk'>
<p>Natural splines with knot heights as the basis.</p></a></li>
<li><a href='#nwtco'><p>Data from the National Wilm's Tumor Study</p></a></li>
<li><a href='#ovarian'><p>Ovarian Cancer Survival Data</p></a></li>
<li><a href='#pbc'><p>Mayo Clinic Primary Biliary Cholangitis Data</p></a></li>
<li><a href='#pbcseq'><p>Mayo Clinic Primary Biliary Cirrhosis, sequential data</p></a></li>
<li><a href='#plot.aareg'>
<p>Plot an aareg object.</p></a></li>
<li><a href='#plot.cox.zph'>
<p>Graphical Test of Proportional Hazards</p></a></li>
<li><a href='#plot.survfit'>
<p>Plot method for <code>survfit</code> objects</p></a></li>
<li><a href='#predict.coxph'>
<p>Predictions for a Cox model</p></a></li>
<li><a href='#predict.survreg'>
<p>Predicted Values for a &lsquo;survreg&rsquo; Object</p></a></li>
<li><a href='#print.aareg'>
<p>Print an aareg object</p></a></li>
<li><a href='#print.summary.coxph'>
<p>Print method for summary.coxph objects</p></a></li>
<li><a href='#print.summary.survexp'><p>Print Survexp Summary</p></a></li>
<li><a href='#print.summary.survfit'>
<p>Print Survfit Summary</p></a></li>
<li><a href='#print.survfit'>
<p>Print a Short Summary of a Survival Curve</p></a></li>
<li><a href='#pseudo'>
<p>Pseudo values for survival.</p></a></li>
<li><a href='#pspline'><p>Smoothing splines using a pspline basis</p></a></li>
<li><a href='#pyears'>
<p>Person Years</p></a></li>
<li><a href='#quantile.survfit'><p>Quantiles from a survfit object</p></a></li>
<li><a href='#ratetable'><p>Allow ratetable() terms in a model</p></a></li>
<li><a href='#ratetableDate'><p>Convert date objects to ratetable form</p></a></li>
<li><a href='#ratetables'>
<p>Census Data Sets for the Expected Survival and Person Years Functions</p></a></li>
<li><a href='#rats'><p>Rat treatment data from Mantel et al</p></a></li>
<li><a href='#rats2'><p>Rat data from Gail et al.</p></a></li>
<li><a href='#reliability'><p>Reliability data sets</p></a></li>
<li><a href='#residuals.coxph'><p>Calculate Residuals for a &lsquo;coxph&rsquo; Fit</p></a></li>
<li><a href='#residuals.survfit'><p>IJ residuals from a survfit object.</p></a></li>
<li><a href='#residuals.survreg'><p>Compute Residuals for &lsquo;survreg&rsquo; Objects</p></a></li>
<li><a href='#retinopathy'><p>Diabetic Retinopathy</p></a></li>
<li><a href='#rhDNase'><p>rhDNASE data set</p></a></li>
<li><a href='#ridge'><p> Ridge regression</p></a></li>
<li><a href='#rotterdam'><p>Breast cancer data set used in Royston and Altman (2013)</p></a></li>
<li><a href='#royston'><p>Compute Royston's D for a Cox model</p></a></li>
<li><a href='#rttright'><p>Compute redistribute-to-the-right weights</p></a></li>
<li><a href='#solder'><p>Data from a soldering experiment</p></a></li>
<li><a href='#stanford2'><p>More Stanford Heart Transplant data</p></a></li>
<li><a href='#statefig'><p>Draw a state space figure.</p></a></li>
<li><a href='#strata'>
<p>Identify Stratification Variables</p></a></li>
<li><a href='#summary.aareg'>
<p>Summarize an aareg fit</p></a></li>
<li><a href='#summary.coxph'>
<p>Summary method for Cox models</p></a></li>
<li><a href='#summary.pyears'><p>Summary function for pyears objecs</p></a></li>
<li><a href='#summary.survexp'><p>Summary function for a survexp object</p></a></li>
<li><a href='#summary.survfit'>
<p>Summary of a Survival Curve</p></a></li>
<li><a href='#Surv'>
<p>Create a Survival Object</p></a></li>
<li><a href='#Surv-methods'><p>Methods for Surv objects</p></a></li>
<li><a href='#Surv2'><p>Create a survival object</p></a></li>
<li><a href='#Surv2data'><p>Convert data from timecourse to (time1,time2) style</p></a></li>
<li><a href='#survcheck'><p>Checks of a survival data set</p></a></li>
<li><a href='#survcondense'><p>Shorten a (time1, time2) survival dataset</p></a></li>
<li><a href='#survdiff'>
<p>Test Survival Curve Differences</p></a></li>
<li><a href='#survexp'>
<p>Compute Expected Survival</p></a></li>
<li><a href='#survexp.fit'>
<p>Compute Expected Survival</p></a></li>
<li><a href='#survexp.object'>
<p>Expected Survival Curve Object</p></a></li>
<li><a href='#survfit'><p>Create survival curves</p></a></li>
<li><a href='#survfit.coxph'>
<p>Compute a Survival Curve from a Cox model</p></a></li>
<li><a href='#survfit.formula'>
<p>Compute a Survival Curve for Censored Data</p></a></li>
<li><a href='#survfit.matrix'><p>Create Aalen-Johansen estimates of multi-state survival from</p>
a matrix of hazards.</a></li>
<li><a href='#survfit.object'>
<p>Survival Curve Object</p></a></li>
<li><a href='#survfit0'>
<p>Convert the format of a survfit object.</p></a></li>
<li><a href='#survfitcoxph.fit'>
<p>A direct interface to the &lsquo;computational engine&rsquo; of survfit.coxph</p></a></li>
<li><a href='#survival-deprecated'><p>Deprecated functions in package <span class="pkg">survival</span></p></a></li>
<li><a href='#survival-internal'><p>Internal survival functions</p></a></li>
<li><a href='#survobrien'>
<p>O'Brien's Test for Association of a Single Variable with Survival</p></a></li>
<li><a href='#survreg'>
<p>Regression for a Parametric Survival Model</p></a></li>
<li><a href='#survreg.control'><p>Package options for survreg and coxph</p></a></li>
<li><a href='#survreg.distributions'><p>Parametric Survival Distributions</p></a></li>
<li><a href='#survreg.object'>
<p>Parametric Survival Model Object</p></a></li>
<li><a href='#survregDtest'><p>Verify a survreg distribution</p></a></li>
<li><a href='#survSplit'><p>Split a survival data set at specified times</p></a></li>
<li><a href='#tcut'><p>Factors for person-year calculations</p></a></li>
<li><a href='#tmerge'><p>Time based merge for survival data</p></a></li>
<li><a href='#tobin'><p>Tobin's Tobit data</p></a></li>
<li><a href='#transplant'><p>Liver transplant waiting list</p></a></li>
<li><a href='#udca'><p>Data from a trial of usrodeoxycholic acid</p></a></li>
<li><a href='#untangle.specials'>
<p>Help Process the &lsquo;specials&rsquo; Argument of the &lsquo;terms&rsquo; Function.</p></a></li>
<li><a href='#uspop2'><p>Projected US Population</p></a></li>
<li><a href='#vcov.coxph'><p>Variance-covariance matrix</p></a></li>
<li><a href='#veteran'><p>Veterans' Administration Lung Cancer study</p></a></li>
<li><a href='#xtfrm.Surv'><p>Sorting order for Surv objects</p></a></li>
<li><a href='#yates'><p>Population prediction</p></a></li>
<li><a href='#yates_setup'><p>Method for adding new models to the <code>yates</code> function.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Survival Analysis</td>
</tr>
<tr>
<td>Priority:</td>
<td>recommended</td>
</tr>
<tr>
<td>Version:</td>
<td>3.7-0</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-06-01</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, Matrix, methods, splines, stats, utils</td>
</tr>
<tr>
<td>LazyData:</td>
<td>Yes</td>
</tr>
<tr>
<td>LazyDataCompression:</td>
<td>xz</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>Yes</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains the core survival analysis routines, including
	     definition of Surv objects, 
	     Kaplan-Meier and Aalen-Johansen (multi-state) curves, Cox models,
	     and parametric accelerated failure time models.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/therneau/survival">https://github.com/therneau/survival</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-03 15:17:04 UTC; therneau</td>
</tr>
<tr>
<td>Author:</td>
<td>Terry M Therneau [aut, cre],
  Thomas Lumley [ctb, trl] (original S-&gt;R port and R maintainer until
    2009),
  Atkinson Elizabeth [ctb],
  Crowson Cynthia [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Terry M Therneau &lt;therneau.terry@mayo.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-05 16:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='aareg'>
Aalen's additive regression model for censored data
</h2><span id='topic+aareg'></span>

<h3>Description</h3>

<p>Returns an object of class <code>"aareg"</code> that
represents an Aalen model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aareg(formula, data, weights, subset, na.action,
   qrtol=1e-07, nmin, dfbeta=FALSE, taper=1,
   test = c('aalen', 'variance', 'nrisk'), cluster,
    model=FALSE, x=FALSE, y=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aareg_+3A_formula">formula</code></td>
<td>

<p>a formula object, with the response on the left of a &lsquo;~&rsquo; operator and 
the terms,
separated by <code>+</code> operators, on the right. 
The response must be a <code>Surv</code> object.
Due to a particular computational approach that is used, the model
MUST include an intercept term.  If &quot;-1&quot; is used in the model
formula the program will ignore it.
</p>
</td></tr>
<tr><td><code id="aareg_+3A_data">data</code></td>
<td>

<p>data frame in which to interpret the variables named in the
<code>formula</code>,
<code>subset</code>, and <code>weights</code> arguments.
This may also be a single number to handle
some speci al cases &ndash; see below for details. If
<code>data</code> is missing, the variables in the
model formula should be in the search path.
</p>
</td></tr>
<tr><td><code id="aareg_+3A_weights">weights</code></td>
<td>

<p>vector of observation weights. If supplied, the fitting algorithm
minimizes the sum of the weights multiplied by the squared residuals
(see below for additional technical details). The length of
<code>weights</code> must be the same as the number of
observations. The weights must be nonnegative and it i s recommended
that they be strictly positive, since zero weights are ambiguous.  To
exclude particular observations from the model, use the
<code>subset</code> argument instead of zero weights.
</p>
</td></tr>
<tr><td><code id="aareg_+3A_subset">subset</code></td>
<td>

<p>expression specifying which subset of observations should be used in
the fit. Th is can be a logical vector (which is replicated to have
length equal to the numb er of observations), a numeric vector
indicating the observation numbers to be included, or a character
vector of the observation names that should be included.  All
observations are included by default.
</p>
</td></tr>
<tr><td><code id="aareg_+3A_na.action">na.action</code></td>
<td>

<p>a function to filter missing data. This is applied to the
<code>model.fr ame</code> after any
<code>subset</code> argument has be en applied. The
default is <code>na.fail</code>, which returns a n
error if any missing values are found. An alternative is
<code>na.excl ude</code>, which deletes observations
that contain one or more missing values.
</p>
</td></tr>
<tr><td><code id="aareg_+3A_qrtol">qrtol</code></td>
<td>

<p>tolerance for detection of singularity in the QR decomposition
</p>
</td></tr>
<tr><td><code id="aareg_+3A_nmin">nmin</code></td>
<td>

<p>minimum number of observations for an estimate; defaults to 3 times the
number of covariates.
This essentially truncates the computations near the tail of the data
set, when n is small and the calculations can become numerically
unstable.
</p>
</td></tr>
<tr><td><code id="aareg_+3A_dfbeta">dfbeta</code></td>
<td>

<p>should the array of dfbeta residuals be computed.  This implies computation
of the sandwich variance estimate.
The residuals will always be computed if there is a
<code>cluster</code> term in the model formula.
</p>
</td></tr>
<tr><td><code id="aareg_+3A_taper">taper</code></td>
<td>

<p>allows for a smoothed variance estimate.
Var(x), where x is the set of covariates, is an important component of the
calculations for the Aalen regression model.  
At any given time point t, it is computed over all subjects who are still
at risk at time t.
The tape argument allows smoothing these estimates,
for example <code>taper=(1:4)/4</code> would cause
the variance estimate used at any event time to be a weighted average
of the estimated variance matrices at the last 4 death times,
with a weight of 1 for the current death time and decreasing to 1/4 for
prior event times.
The default value gives the standard Aalen model.
</p>
</td></tr>
<tr><td><code id="aareg_+3A_test">test</code></td>
<td>

<p>selects the weighting to be used, for computing an overall &ldquo;average&rdquo;
coefficient vector over time and the subsequent test for equality to zero.
</p>
</td></tr>
<tr><td><code id="aareg_+3A_cluster">cluster</code></td>
<td>
<p>the clustering group, optional.  The variable will be
searched for in the data argument.</p>
</td></tr>
<tr><td><code id="aareg_+3A_model">model</code>, <code id="aareg_+3A_x">x</code>, <code id="aareg_+3A_y">y</code></td>
<td>

<p>should copies of the model frame, the x matrix of predictors, or the
response vector y be included in the saved result.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Aalen model assumes that the cumulative hazard H(t) for a subject
can be expressed as a(t) + X B(t), where a(t) is a time-dependent intercept
term, X is the vector of covariates for the subject (possibly time-dependent),
and B(t) is a time-dependent matrix of coefficients.
The estimates are inherently non-parametric; a fit of the model will normally
be followed by one or more plots of the estimates.
</p>
<p>The estimates may become unstable near the tail of a data set, since the
increment to B at time t is based on the subjects still at risk at time
t.  The tolerance and/or nmin parameters may act to truncate the estimate
before the last death.
The <code>taper</code> argument can also be used to smooth
out the tail of the curve.
In practice, the addition of a taper such as 1:10 appears to have little
effect on death times when n is still reasonably large, but can considerably
dampen wild occilations in the tail of the plot.  
</p>


<h3>Value</h3>

<p>an object of class <code>"aareg"</code> 
representing the fit, with the following components:
</p>
<table>
<tr><td><code>n</code></td>
<td>
<p>vector containing the number of observations in the data set,
the number of event times, and the number of event times used in the
computation</p>
</td></tr>
<tr><td><code>times</code></td>
<td>
<p>vector of sorted event times, which may contain
duplicates</p>
</td></tr>
<tr><td><code>nrisk</code></td>
<td>
<p>vector containing the number of subjects at risk, of the
same length as <code>times</code></p>
</td></tr>
<tr><td><code>coefficient</code></td>
<td>
<p>matrix of coefficients, with one row per event and
one column per covariate</p>
</td></tr>
<tr><td><code>test.statistic</code></td>
<td>
<p>the value of the test statistic, a vector with
one element per covariate</p>
</td></tr>
<tr><td><code>test.var</code></td>
<td>
<p>variance-covariance matrix for the test</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>the type of test; a copy of the <code>test</code> argument
above</p>
</td></tr>
<tr><td><code>tweight</code></td>
<td>
<p>matrix of weights used in the computation, one row per
event</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a copy of the call that produced this result</p>
</td></tr>
</table>


<h3>References</h3>

<p>Aalen, O.O. (1989). A linear regression model for the analysis of life times.
Statistics in Medicine, 8:907-925.
</p>
<p>Aalen, O.O (1993). Further results on the non-parametric linear model in
survival analysis.  Statistics in Medicine. 12:1569-1588.
</p>


<h3>See Also</h3>

<p>print.aareg, summary.aareg, plot.aareg
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Fit a model to the lung cancer data set
lfit &lt;- aareg(Surv(time, status) ~ age + sex + ph.ecog, data=lung,
                     nmin=1)
## Not run: 
lfit
Call:
aareg(formula = Surv(time, status) ~ age + sex + ph.ecog, data = lung, nmin = 1
        )

  n=227 (1 observations deleted due to missing values)
    138 out of 138 unique event times used

              slope      coef se(coef)     z        p 
Intercept  5.26e-03  5.99e-03 4.74e-03  1.26 0.207000
      age  4.26e-05  7.02e-05 7.23e-05  0.97 0.332000
      sex -3.29e-03 -4.02e-03 1.22e-03 -3.30 0.000976
  ph.ecog  3.14e-03  3.80e-03 1.03e-03  3.70 0.000214

Chisq=26.73 on 3 df, p=6.7e-06; test weights=aalen

plot(lfit[4], ylim=c(-4,4))  # Draw a plot of the function for ph.ecog

## End(Not run)
lfit2 &lt;- aareg(Surv(time, status) ~ age + sex + ph.ecog, data=lung,
                  nmin=1, taper=1:10)
## Not run: lines(lfit2[4], col=2)  # Nearly the same, until the last point

# A fit to the mulitple-infection data set of children with
# Chronic Granuomatous Disease.  See section 8.5 of Therneau and Grambsch.
fita2 &lt;- aareg(Surv(tstart, tstop, status) ~ treat + age + inherit +
                         steroids + cluster(id), data=cgd)
## Not run: 
  n= 203 
    69 out of 70 unique event times used

                     slope      coef se(coef) robust se     z        p
Intercept         0.004670  0.017800 0.002780  0.003910  4.55 5.30e-06
treatrIFN-g      -0.002520 -0.010100 0.002290  0.003020 -3.36 7.87e-04
age              -0.000101 -0.000317 0.000115  0.000117 -2.70 6.84e-03
inheritautosomal  0.001330  0.003830 0.002800  0.002420  1.58 1.14e-01
steroids          0.004620  0.013200 0.010600  0.009700  1.36 1.73e-01

Chisq=16.74 on 4 df, p=0.0022; test weights=aalen

## End(Not run)
</code></pre>

<hr>
<h2 id='aeqSurv'>Adjudicate near ties in a Surv object</h2><span id='topic+aeqSurv'></span>

<h3>Description</h3>

<p>The check for tied survival times can fail due
to floating point imprecision, which can make actual ties appear to
be distinct values.
Routines that depend on correct identification of ties pairs will then
give incorrect results, e.g., a Cox model.
This function rectifies these.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aeqSurv(x, tolerance = sqrt(.Machine$double.eps))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aeqSurv_+3A_x">x</code></td>
<td>
<p>a Surv object</p>
</td></tr>
<tr><td><code id="aeqSurv_+3A_tolerance">tolerance</code></td>
<td>
<p>the tolerance used to detect values that will
be considered equal</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This routine is called by both <code>survfit</code> and <code>coxph</code> to
deal with the issue of ties that get incorrectly broken due to
floating point imprecision.  See the short vignette on tied times
for a simple example.  Use the <code>timefix</code> argument of
<code>survfit</code> or <code>coxph.control</code> to control the option
if desired.
</p>
<p>The rule for &lsquo;equality&rsquo; is identical to that used by the
<code>all.equal</code> routine.  Pairs of values that are within round off
error of each other are replaced by the smaller value.
An error message is generated if this process causes a 0 length
time interval to be created.  
</p>


<h3>Value</h3>

<p>a Surv object identical to the original, but with ties restored.</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit">survfit</a></code>, <code><a href="#topic+coxph.control">coxph.control</a></code></p>

<hr>
<h2 id='aggregate.survfit'>Average survival curves</h2><span id='topic+aggregate.survfit'></span>

<h3>Description</h3>

<p>For a survfit object containing multiple curves, create average curves
over a grouping.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survfit'
aggregate(x, by = NULL, FUN = mean, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregate.survfit_+3A_x">x</code></td>
<td>
<p>a <code>survfit</code> object which has a data dimension.</p>
</td></tr>
<tr><td><code id="aggregate.survfit_+3A_by">by</code></td>
<td>
<p>an optional list or vector of grouping elements, each as
long as <code>dim(x)['data']</code>. </p>
</td></tr>
<tr><td><code id="aggregate.survfit_+3A_fun">FUN</code></td>
<td>
<p>a function to compute the summary statistic of interest. </p>
</td></tr>
<tr><td><code id="aggregate.survfit_+3A_...">...</code></td>
<td>
<p>optional further arguments to FUN.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The primary use of this is to take an average over multiple survival
curves that were created from a modeling function.  That is, a
marginal estimate of the survival.
It is primarily used to average over multiple predicted curves from a
Cox model.
</p>


<h3>Value</h3>

<p>a <code>survfit</code> object of lower dimension.</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit">survfit</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>cfit &lt;- coxph(Surv(futime, death) ~ sex + age*hgb, data=mgus2)
# marginal effect of sex, after adjusting for the others
dummy &lt;- rbind(mgus2, mgus2)
dummy$sex &lt;- rep(c("F", "M"), each=nrow(mgus2)) # population data set
dummy &lt;- na.omit(dummy)   # don't count missing hgb in our "population
csurv &lt;- survfit(cfit, newdata=dummy)
dim(csurv)  # 2 * 1384 survival curves
csurv2 &lt;- aggregate(csurv, dummy$sex)
</code></pre>

<hr>
<h2 id='agreg.fit'>Cox model fitting functions</h2><span id='topic+agreg.fit'></span><span id='topic+coxph.fit'></span>

<h3>Description</h3>

<p>These are the the functions called by coxph that do the actual
computation.
In certain situations, e.g. a simulation, it may be advantageous to
call these directly rather than the usual <code>coxph</code> call using
a model formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>agreg.fit(x, y, strata, offset, init, control, weights, method,
rownames, resid=TRUE, nocenter=NULL)
coxph.fit(x, y, strata, offset, init, control, weights, method,
rownames, resid=TRUE, nocenter=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="agreg.fit_+3A_x">x</code></td>
<td>
<p>Matix of predictors.  This should <em>not</em> include an
intercept.</p>
</td></tr>
<tr><td><code id="agreg.fit_+3A_y">y</code></td>
<td>
<p>a <code>Surv</code> object containing either 2 columns (coxph.fit)
or 3 columns (agreg.fit).</p>
</td></tr>
<tr><td><code id="agreg.fit_+3A_strata">strata</code></td>
<td>
<p>a vector containing the stratification, or NULL</p>
</td></tr>
<tr><td><code id="agreg.fit_+3A_offset">offset</code></td>
<td>
<p>optional offset vector</p>
</td></tr>
<tr><td><code id="agreg.fit_+3A_init">init</code></td>
<td>
<p>initial values for the coefficients</p>
</td></tr>
<tr><td><code id="agreg.fit_+3A_control">control</code></td>
<td>
<p>the result of a call to <code>coxph.control</code></p>
</td></tr>
<tr><td><code id="agreg.fit_+3A_weights">weights</code></td>
<td>
<p>optional vector of weights</p>
</td></tr>
<tr><td><code id="agreg.fit_+3A_method">method</code></td>
<td>
<p>method for handling ties, one of &quot;breslow&quot; or &quot;efron&quot;</p>
</td></tr>
<tr><td><code id="agreg.fit_+3A_rownames">rownames</code></td>
<td>
<p>this is only needed for a NULL model, in which case it
contains the rownames (if any) of the original data.</p>
</td></tr>
<tr><td><code id="agreg.fit_+3A_resid">resid</code></td>
<td>
<p>compute and return residuals.</p>
</td></tr>
<tr><td><code id="agreg.fit_+3A_nocenter">nocenter</code></td>
<td>
<p>an optional list of values. Any column of the X matrix
whose values lie strictly within that set will not be recentered.
Note that the coxph function has (-1, 0, 1) as the default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This routine does no checking that arguments are the proper length or
type.
Only use it if you know what you are doing!
</p>
<p>The <code>resid</code> and <code>concordance</code> arguments will save some compute
time for calling routines that only need the likelihood,
the generation of a permutation distribution for instance.
</p>


<h3>Value</h3>

<p>a list containing results of the fit</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code></p>

<hr>
<h2 id='aml'>Acute Myelogenous Leukemia survival data</h2><span id='topic+aml'></span><span id='topic+leukemia'></span>

<h3>Description</h3>

<p>Survival in patients with Acute Myelogenous Leukemia.
The question at the time was whether the standard course of
chemotherapy should be extended ('maintainance') for additional
cycles.</p>


<h3>Usage</h3>

<pre><code class='language-R'>aml
leukemia
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    time:</td><td style="text-align: left;"> survival or censoring time</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> censoring status</td>
</tr>
<tr>
 <td style="text-align: left;">
    x: </td><td style="text-align: left;"> maintenance chemotherapy given? (factor)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Rupert G. Miller (1997),
<em>Survival Analysis</em>.
John Wiley &amp; Sons.
ISBN: 0-471-25218-2.
</p>

<hr>
<h2 id='anova.coxph'>Analysis of Deviance for a Cox model.</h2><span id='topic+anova.coxph'></span><span id='topic+anova.coxphlist'></span>

<h3>Description</h3>

<p>Compute an analysis of deviance table for one or more Cox model fits,
based on the log partial likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coxph'
anova(object, ...,  test = 'Chisq')
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova.coxph_+3A_object">object</code></td>
<td>
<p>An object of class <code>coxph</code></p>
</td></tr>
<tr><td><code id="anova.coxph_+3A_...">...</code></td>
<td>
<p>Further <code>coxph</code> objects</p>
</td></tr>
<tr><td><code id="anova.coxph_+3A_test">test</code></td>
<td>
<p>a character string. The appropriate test is a chisquare, all
other choices result in no test being done.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Specifying a single object gives a sequential analysis of deviance
table for that fit.  That is, the reductions in the model
Cox log-partial-likelihood
as each term of the formula is added in turn are given in as
the rows of a table, plus the log-likelihoods themselves.
A robust variance estimate is normally used in situations where the
model may be mis-specified, e.g., multiple events per subject.
In this case a comparison of likelihood values does not make
sense (differences no longer have a chi-square distribution),
and <code>anova</code> will refuse to print results.
</p>
<p>If more than one object is specified, the table has a row for the
degrees of freedom and loglikelihood for each model. For all
but the first model, the change in degrees of freedom and loglik
is also given. (This only make statistical sense if the models are
nested.)  It is conventional to list the models from smallest to
largest, but this is up to the user.
</p>
<p>The table will optionally contain test statistics (and P values)
comparing the reduction in loglik for each row.
</p>


<h3>Value</h3>

<p>An object of class <code>"anova"</code> inheriting from class <code>"data.frame"</code>.
</p>


<h3>Warning</h3>

<p>The comparison between two or more models by <code>anova</code>
will only be valid if they
are fitted to the same dataset. This may be a problem if there are
missing values.</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>, <code><a href="stats.html#topic+anova">anova</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- coxph(Surv(futime, fustat) ~ resid.ds *rx + ecog.ps, data = ovarian) 
anova(fit)
fit2 &lt;- coxph(Surv(futime, fustat) ~ resid.ds +rx + ecog.ps, data=ovarian)
anova(fit2,fit)
 </code></pre>

<hr>
<h2 id='attrassign'>Create new-style &quot;assign&quot; attribute</h2><span id='topic+attrassign.default'></span><span id='topic+attrassign'></span><span id='topic+attrassign.lm'></span>

<h3>Description</h3>

<p>The <code>"assign"</code> attribute on model matrices describes which columns
come from which terms in the model formula. It has two versions. R uses
the original version, but the alternate version found in
S-plus is sometimes useful.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>attrassign(object, ...)
## Default S3 method:
attrassign(object, tt,...)
## S3 method for class 'lm'
attrassign(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="attrassign_+3A_object">object</code></td>
<td>
<p>model matrix or linear model object</p>
</td></tr>
<tr><td><code id="attrassign_+3A_tt">tt</code></td>
<td>
<p>terms object</p>
</td></tr>
<tr><td><code id="attrassign_+3A_...">...</code></td>
<td>
<p>further arguments for other methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For instance consider the following
</p>
<pre>
    survreg(Surv(time, status) ~ age + sex + factor(ph.ecog), lung)
  </pre>
<p>R gives the compact for for assign, a vector (0, 1, 2, 3, 3, 3);
which can be
read as &ldquo;the first column of the X matrix (intercept) goes with none
of the terms, the second column of X goes with term 1 of the model
equation, the third column of X with term 2, and columns 4-6 with
term 3&rdquo;.
</p>
<p>The alternate (S-Plus default) form is a list
</p>
<pre>
       $(Intercept)     1
       $age             2
       $sex             3
       $factor(ph.ecog) 4 5 6
     </pre>


<h3>Value</h3>

<p>A list with names corresponding to the term names and elements
that are vectors indicating which columns come from which terms
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+terms">terms</a></code>,<code><a href="stats.html#topic+model.matrix">model.matrix</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>formula &lt;- Surv(time,status)~factor(ph.ecog)
tt &lt;- terms(formula)
mf &lt;- model.frame(tt,data=lung)
mm &lt;- model.matrix(tt,mf)
## a few rows of data
mm[1:3,]
## old-style assign attribute
attr(mm,"assign")
## alternate style assign attribute
attrassign(mm,tt)
</code></pre>

<hr>
<h2 id='basehaz'>Alias for the survfit function</h2><span id='topic+basehaz'></span>

<h3>Description</h3>

<p>Compute the predicted survival curve for a Cox model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>basehaz(fit, newdata, centered=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="basehaz_+3A_fit">fit</code></td>
<td>
<p>a coxph fit</p>
</td></tr>
<tr><td><code id="basehaz_+3A_newdata">newdata</code></td>
<td>
<p>a data frame containing one row for each predicted
survival curve, said row contains the covariate values for that curve</p>
</td></tr>
<tr><td><code id="basehaz_+3A_centered">centered</code></td>
<td>
<p>ignored if the <code>newdata</code> argument is present.
Otherwise, if TRUE return data from a predicted survival curve
for the covariate values <code>fit$mean</code>, if FALSE return a
prediction for all covariates equal to zero.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an alias for <code>survfit.coxph</code>, which does the
actual work and has a richer set of options.
Look at that help file for more discussion and explanation.
This alias exists primarily because some users look for predicted survival
estimates under this name. 
</p>
<p>The function returns a data frame containing the <code>time</code>,
<code>cumhaz</code> and optionally the strata (if the fitted Cox model used
a strata statement), which are copied from the <code>survfit</code> result.
</p>
<p>If H(t; z) is the predicted cumulative hazard for an observation with
covariate vector z, then H(t;x) = H(t;z) r(x,z)
where r(x,z)= exp(beta[1](x[1]- z[1]) + beta[2](x[2]-z[2]) + ...) =
<code>exp(sum(coef(fit) * (x-z)))</code> is the
Cox model's hazard ratio for covariate vector x vs covariate vector z.
That is, 
the cumulative hazard H for a single reference value z is sufficient to
provide the hazard for any covariate values.
The predicted survival curve is S(t; x)= exp(-H(t;x)).
There is not a simple transformation for the variance of H, however.
</p>
<p>Many textbooks refer to H(t; 0) as &quot;the&quot; baseline hazard
for a Cox model; this is returned by the <code>centered= FALSE</code>
option.
However, due to potential overflow or underflow in the exp() function
this can be a very bad idea in practice. The authors do not recommend
this option, but for users who insist: caveat emptor.
Offset terms can pose a particular challenge for the underlying code
and are always recentered; to override this use the newdata argument
and include the offset as one of the variables.
</p>


<h3>Value</h3>

<p>a data frame with variable names of <code>hazard</code>, <code>time</code> and
optionally <code>strata</code>.  The first is actually the cumulative hazard.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit.coxph">survfit.coxph</a></code></p>

<hr>
<h2 id='bladder'>Bladder Cancer Recurrences</h2><span id='topic+bladder'></span><span id='topic+bladder1'></span><span id='topic+bladder2'></span>

<h3>Description</h3>

<p>Data on recurrences of bladder cancer, used by many people
to demonstrate methodology for recurrent event modelling.
</p>
<p>Bladder1 is the full data set from the study. It contains all three treatment
arms and all recurrences for 118 subjects; the maximum observed number
of recurrences is 9.
</p>
<p>Bladder is the data set that appears most commonly in the literature. 
It uses only the 85 subjects with nonzero follow-up who were
assigned to either thiotepa or placebo, and only the first four recurrences
for any patient.  The status variable is 1 for
recurrence and 0 for everything else (including death for any reason).
The data set is laid out in the competing risks format of the paper by
Wei, Lin, and Weissfeld.
</p>
<p>Bladder2 uses the same subset of subjects as bladder, but formatted in the
(start, stop] or Anderson-Gill style.  
Note that in transforming from the WLW to the AG style data set there
is a quite common programming mistake that leads to extra follow-up time
for 12 subjects: all those with follow-up beyond their 4th recurrence.
This &quot;follow-up&quot; is a side effect of throwing away all events after the
fourth while retaining the last follow-up time variable from the
original data.  The bladder2 data set found here does not make this
mistake, but some analyses in the literature have done so; it results
in the addition of a small amount of immortal time bias and 
shrinks the fitted coefficients towards zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bladder1
bladder
bladder2
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>

<p>bladder1
</p>

<table>
<tr>
 <td style="text-align: left;">
    id:</td><td style="text-align: left;"> Patient id</td>
</tr>
<tr>
 <td style="text-align: left;">
    treatment:</td><td style="text-align: left;"> Placebo, pyridoxine (vitamin B6), or thiotepa</td>
</tr>
<tr>
 <td style="text-align: left;">
    number:</td><td style="text-align: left;"> Initial number of tumours (8=8 or more)</td>
</tr>
<tr>
 <td style="text-align: left;">
    size:</td><td style="text-align: left;"> Size (cm) of largest initial tumour</td>
</tr>
<tr>
 <td style="text-align: left;">
    recur:</td><td style="text-align: left;"> Number of recurrences </td>
</tr>
<tr>
 <td style="text-align: left;">
    start,stop:</td><td style="text-align: left;"> The start and end time of each time interval</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> End of interval code, 0=censored, 1=recurrence, </td>
</tr>
<tr>
 <td style="text-align: left;">
           </td><td style="text-align: left;"> 2=death from bladder disease, 3=death other/unknown cause</td>
</tr>
<tr>
 <td style="text-align: left;">
    rtumor:</td><td style="text-align: left;"> Number of tumors found at the time of a recurrence</td>
</tr>
<tr>
 <td style="text-align: left;">
    rsize:</td><td style="text-align: left;"> Size of largest tumor at a recurrence</td>
</tr>
<tr>
 <td style="text-align: left;">
    enum:</td><td style="text-align: left;"> Event number (observation number within patient)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>

<p>bladder
</p>

<table>
<tr>
 <td style="text-align: left;">
    id:</td><td style="text-align: left;"> Patient id</td>
</tr>
<tr>
 <td style="text-align: left;">
    rx:</td><td style="text-align: left;"> Treatment 1=placebo  2=thiotepa</td>
</tr>
<tr>
 <td style="text-align: left;">
    number:</td><td style="text-align: left;"> Initial number of tumours (8=8 or more)</td>
</tr>
<tr>
 <td style="text-align: left;">
    size:</td><td style="text-align: left;"> size (cm) of largest initial tumour</td>
</tr>
<tr>
 <td style="text-align: left;">
    stop:</td><td style="text-align: left;"> recurrence or censoring time</td>
</tr>
<tr>
 <td style="text-align: left;">
    enum:</td><td style="text-align: left;"> which recurrence (up to 4)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>

<p>bladder2 
</p>

<table>
<tr>
 <td style="text-align: left;">
    id:</td><td style="text-align: left;"> Patient id</td>
</tr>
<tr>
 <td style="text-align: left;">
    rx:</td><td style="text-align: left;"> Treatment 1=placebo  2=thiotepa</td>
</tr>
<tr>
 <td style="text-align: left;">
    number:</td><td style="text-align: left;"> Initial number of tumours (8=8 or more)</td>
</tr>
<tr>
 <td style="text-align: left;">
    size:</td><td style="text-align: left;"> size (cm) of largest initial tumour</td>
</tr>
<tr>
 <td style="text-align: left;">
    start:</td><td style="text-align: left;"> start of interval (0 or previous recurrence time)</td>
</tr>
<tr>
 <td style="text-align: left;">
    stop:</td><td style="text-align: left;"> recurrence or censoring time</td>
</tr>
<tr>
 <td style="text-align: left;">
    enum:</td><td style="text-align: left;"> which recurrence (up to 4)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Andrews DF, Hertzberg AM (1985), 
DATA: A Collection of Problems from Many Fields for the Student 
and Research Worker, New York: Springer-Verlag.
</p>
<p>LJ Wei, DY Lin, L Weissfeld (1989),
Regression analysis of multivariate incomplete failure time data by
modeling marginal distributions.
<em>Journal of the American Statistical Association</em>,
<b>84</b>.
</p>

<hr>
<h2 id='blogit'>
Bounded link functions
</h2><span id='topic+blogit'></span><span id='topic+bcloglog'></span><span id='topic+bprobit'></span><span id='topic+blog'></span>

<h3>Description</h3>

<p>Alternate link functions that impose bounds on the input of their link function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blogit(edge = 0.05)
bprobit(edge= 0.05)
bcloglog(edge=.05)
blog(edge=.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blogit_+3A_edge">edge</code></td>
<td>
<p>input values less than the cutpoint are replaces with the
cutpoint.  For all be <code>blog</code> input values greater than (1-edge)
are replaced with (1-edge)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When using survival psuedovalues for binomial regression, the raw data can be
outside the range (0,1), yet we want to restrict the predicted values
to lie within that range.  A natural way to deal with this is to use
<code>glm</code> with <code>family = gaussian(link= "logit")</code>.
But this will fail.
The reason is that the <code>family</code> object has a component
<code>linkfun</code> that does not accept values outside of (0,1).
</p>
<p>This function is only used to create initial values for the iteration
step, however. Mapping the offending input argument into the range
of (egde, 1-edge) before computing the link results in starting
estimates that are good enough.  The final result of the fit will be
no different than if explicit starting estimates were given using the
<code>etastart</code> or <code>mustart</code> arguments.
These functions create copies of the logit, probit, and complimentary
log-log families that differ from the standard ones only in this
use of a bounded input argument, and are called a &quot;bounded logit&quot; =
<code>blogit</code>, etc.
</p>
<p>The same argument hold when using RMST (area under the curve)
pseudovalues along with a log link to ensure positive predictions,
though in this case only the lower boundary needs to be mapped.  
</p>


<h3>Value</h3>

<p>a <code>family</code> object of the same form as <code>make.family</code>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+stats">stats</a>{make.family}</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>py &lt;- pseudo(survfit(Surv(time, status) ~1, lung), time=730) #2 year survival
range(py)
pfit &lt;- glm(py ~ ph.ecog, data=lung, family=gaussian(link=blogit()))
# For each +1 change in performance score, the odds of 2 year survival
#  are multiplied by 1/2  = exp of the coefficient.
</code></pre>

<hr>
<h2 id='brier'>Compute the Brier score for a Cox model</h2><span id='topic+brier'></span>

<h3>Description</h3>

<p>Compute the Brier score, for a coxph model</p>


<h3>Usage</h3>

<pre><code class='language-R'>brier(fit, times, newdata, ties = TRUE, detail = FALSE, timefix = TRUE, 
      efron = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brier_+3A_fit">fit</code></td>
<td>
<p>result of a <code>coxph</code> fit</p>
</td></tr>
<tr><td><code id="brier_+3A_times">times</code></td>
<td>
<p>time points at which to create the score</p>
</td></tr>
<tr><td><code id="brier_+3A_newdata">newdata</code></td>
<td>
<p>optional, used to validate a prior fit with new data</p>
</td></tr>
<tr><td><code id="brier_+3A_ties">ties</code></td>
<td>
<p>if TRUE, treate tied event/censoring times properly</p>
</td></tr>
<tr><td><code id="brier_+3A_detail">detail</code></td>
<td>
<p>if TRUE, the returned object has more detail.  This can
be useful for debugging or for instruction.</p>
</td></tr>
<tr><td><code id="brier_+3A_timefix">timefix</code></td>
<td>
<p>deal with near ties in the data.  See the tied times vignette.</p>
</td></tr>
<tr><td><code id="brier_+3A_efron">efron</code></td>
<td>
<p>use the same survival estimate for the NULL model as was
used in the coxph call</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Far more details are found in the vignette. At any time point tau, the
scaled Brier score is essentially the R-squared statistic where y =
the 0/1 variable &quot;event at or before tau&quot;, yhat is the probability of an
event by tau, as predicted by the model, and the ybar is the predicted
probablity without covariate, normally from a Kaplan-Meier.
If <code class="reqn">R^2= 1- \sum(y- \hat y)^2/\sum (y- \mu)^2</code>, the Brier score is formally only
the numerator of the second term.  The rescaled value is much more
useful, however.
</p>
<p>Many, perhaps even most algorithms do not properly deal with a tied
censoring time/event time pair.  The <code>tied</code> option is present
mostly verify that we get the same answer, when we make the same
mistake.  The numerical size of the inaccuracy is very small; just large
enough to generate concern that this function is incorrect.
</p>
<p>A sensible argument can be made that the NULL model should be a
<code>coxph</code> call with no covariates, rather than the Kaplan-Meier;
but it turns out that the effect is very slight.
This is allowed by the <code>efron</code> argument.
</p>


<h3>Value</h3>

<p> a list with components
</p>
<table>
<tr><td><code>rsquared</code></td>
<td>
<p>the <code class="reqn">R^2</code> value, a scaled Brier score.
This will be a vector with one entry for each time point.</p>
</td></tr>
<tr><td><code>brier</code></td>
<td>
<p>the brier score, a vector with one entry per time point</p>
</td></tr>
<tr><td><code>times</code></td>
<td>
<p>the time points at which the score was computed</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+rttright">rttright</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>cfit &lt;- coxph(Surv(rtime, recur) ~ age + meno + size + pmin(nodes,11), 
              data= rotterdam)
round(cfit$concordance["concordance"], 3)  # some predictive power
brier(cfit, times=c(4,6)*365.25)   # values at 4 and 6 years
</code></pre>

<hr>
<h2 id='cch'>Fits proportional hazards regression model to case-cohort data</h2><span id='topic+cch'></span>

<h3>Description</h3>

<p> Returns estimates and standard errors from relative risk
regression fit to data from case-cohort studies. A choice is available
among the Prentice, Self-Prentice and Lin-Ying methods for unstratified
data. For stratified data the choice is between Borgan I, a
generalization of the Self-Prentice estimator for unstratified
case-cohort data, and Borgan II, a generalization of the Lin-Ying
estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cch(formula, data, subcoh, id, stratum=NULL, cohort.size,
    method =c("Prentice","SelfPrentice","LinYing","I.Borgan","II.Borgan"),
    robust=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cch_+3A_formula">formula</code></td>
<td>

<p>A formula object that must have a <code><a href="#topic+Surv">Surv</a></code> object as the response. 
The Surv object must be of type <code>"right"</code>, or of type <code>"counting"</code>.
</p>
</td></tr>
<tr><td><code id="cch_+3A_subcoh">subcoh</code></td>
<td>

<p>Vector of indicators for subjects sampled as part of the
sub-cohort. Code <code>1</code> or <code>TRUE</code> for members of the
sub-cohort, <code>0</code> or <code>FALSE</code> for others. If <code>data</code> is a
data frame then <code>subcoh</code> may be a one-sided formula.
</p>
</td></tr>
<tr><td><code id="cch_+3A_id">id</code></td>
<td>

<p>Vector of unique identifiers, or formula specifying such a vector.
</p>
</td></tr>
<tr><td><code id="cch_+3A_stratum">stratum</code></td>
<td>
<p>A vector of stratum indicators or a formula specifying
such a vector</p>
</td></tr>
<tr><td><code id="cch_+3A_cohort.size">cohort.size</code></td>
<td>

<p>Vector with size of each stratum original cohort from which subcohort was sampled
</p>
</td></tr>
<tr><td><code id="cch_+3A_data">data</code></td>
<td>

<p>An optional data frame in which to interpret the variables 
occurring in the formula. 
</p>
</td></tr>
<tr><td><code id="cch_+3A_method">method</code></td>
<td>

<p>Three procedures are available. The default method is &quot;Prentice&quot;, with 
options for &quot;SelfPrentice&quot; or &quot;LinYing&quot;. </p>
</td></tr>
<tr><td><code id="cch_+3A_robust">robust</code></td>
<td>
<p>For <code>"LinYing"</code> only, if <code>robust=TRUE</code>, use design-based standard errors even for
phase I</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Implements methods for case-cohort data analysis described by Therneau and
Li (1999). The three methods differ in the choice of &quot;risk sets&quot; used to
compare the covariate values of the failure with those of others at risk at
the time of failure. &quot;Prentice&quot; uses the sub-cohort members &quot;at risk&quot; plus
the failure if that occurs outside the sub-cohort and is score unbiased.
&quot;SelfPren&quot; (Self-Prentice) uses just the sub-cohort members &quot;at risk&quot;. These
two have the same asymptotic variance-covariance matrix. &quot;LinYing&quot; (Lin-Ying)
uses the all members of the sub-cohort and all failures outside the sub-cohort
who are &quot;at risk&quot;. The methods also differ in the weights given to different
score contributions.
</p>
<p>The <code>data</code> argument must not have missing values for any variables
in the model.  There must not be any censored observations outside the subcohort.
</p>


<h3>Value</h3>

<p>An object of class &quot;cch&quot;  incorporating a list of estimated regression coefficients and two estimates of their 
asymptotic variance-covariance matrix.
</p>
<table>
<tr><td><code>coef</code></td>
<td>

<p>regression coefficients.
</p>
</td></tr>
<tr><td><code>naive.var</code></td>
<td>

<p>Self-Prentice model based variance-covariance matrix.
</p>
</td></tr>
<tr><td><code>var</code></td>
<td>

<p>Lin-Ying empirical variance-covariance matrix. 
</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Norman Breslow, modified by Thomas Lumley</p>


<h3>References</h3>

<p>Prentice, RL (1986). A case-cohort design for epidemiologic cohort studies and
disease prevention trials. Biometrika 73: 1&ndash;11.
</p>
<p>Self, S and Prentice, RL (1988). Asymptotic distribution theory and efficiency
results for case-cohort studies. Annals of Statistics 16: 64&ndash;81.
</p>
<p>Lin, DY and Ying, Z (1993). Cox regression with incomplete covariate measurements.
Journal of the American Statistical Association 88: 1341&ndash;1349.
</p>
<p>Barlow, WE (1994). Robust variance estimation for the case-cohort design. Biometrics
50: 1064&ndash;1072
</p>
<p>Therneau, TM and Li, H (1999). Computing the Cox model for case-cohort designs.
Lifetime Data Analysis 5: 99&ndash;112.
</p>
<p>Borgan, <code class="reqn">O</code>, Langholz, B, Samuelsen, SO, Goldstein, L and Pogoda, J (2000)
Exposure stratified case-cohort designs. Lifetime Data Analysis 6, 39-58.
</p>


<h3>See Also</h3>

<p><code>twophase</code> and <code>svycoxph</code> in the &quot;survey&quot; package for
more general two-phase designs. <a href="http://faculty.washington.edu/tlumley/survey/">http://faculty.washington.edu/tlumley/survey/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The complete Wilms Tumor Data 
## (Breslow and Chatterjee, Applied Statistics, 1999)
## subcohort selected by simple random sampling.
##

subcoh &lt;- nwtco$in.subcohort
selccoh &lt;- with(nwtco, rel==1|subcoh==1)
ccoh.data &lt;- nwtco[selccoh,]
ccoh.data$subcohort &lt;- subcoh[selccoh]
## central-lab histology 
ccoh.data$histol &lt;- factor(ccoh.data$histol,labels=c("FH","UH"))
## tumour stage
ccoh.data$stage &lt;- factor(ccoh.data$stage,labels=c("I","II","III","IV"))
ccoh.data$age &lt;- ccoh.data$age/12 # Age in years

##
## Standard case-cohort analysis: simple random subcohort 
##

fit.ccP &lt;- cch(Surv(edrel, rel) ~ stage + histol + age, data =ccoh.data,
   subcoh = ~subcohort, id=~seqno, cohort.size=4028)


fit.ccP

fit.ccSP &lt;- cch(Surv(edrel, rel) ~ stage + histol + age, data =ccoh.data,
   subcoh = ~subcohort, id=~seqno, cohort.size=4028, method="SelfPren")

summary(fit.ccSP)

##
## (post-)stratified on instit
##
stratsizes&lt;-table(nwtco$instit)
fit.BI&lt;- cch(Surv(edrel, rel) ~ stage + histol + age, data =ccoh.data,
   subcoh = ~subcohort, id=~seqno, stratum=~instit, cohort.size=stratsizes,
   method="I.Borgan")

summary(fit.BI)
</code></pre>

<hr>
<h2 id='cgd'>Chronic Granulotamous Disease data</h2><span id='topic+cgd'></span><span id='topic+cgd.raw'></span>

<h3>Description</h3>

<p>Data are from a placebo controlled trial of gamma
interferon in chronic granulotomous disease (CGD).
Contains the data on time to serious infections observed through
end of study for each patient. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cgd
data(cgd)
</code></pre>


<h3>Format</h3>


<dl>
<dt>id</dt><dd><p>subject identification number</p>
</dd>
<dt>center</dt><dd><p>enrolling center </p>
</dd>
<dt>random</dt><dd><p>date of randomization </p>
</dd>
<dt>treatment</dt><dd><p>placebo or gamma interferon </p>
</dd>
<dt>sex</dt><dd><p>sex</p>
</dd>
<dt>age</dt><dd><p>age in years, at study entry </p>
</dd>
<dt>height</dt><dd><p>height in cm at study entry</p>
</dd>
<dt>weight</dt><dd><p>weight in kg at study entry</p>
</dd>
<dt>inherit</dt><dd><p>pattern of inheritance </p>
</dd>
<dt>steroids</dt><dd><p>use of steroids at study entry,1=yes</p>
</dd>
<dt>propylac</dt><dd><p>use of prophylactic antibiotics at study entry</p>
</dd>
<dt>hos.cat</dt><dd><p>a categorization of the centers into 4 groups</p>
</dd>
<dt>tstart, tstop</dt><dd><p>start and end of each time interval </p>
</dd>
<dt>status</dt><dd><p>1=the interval ends with an infection </p>
</dd>
<dt>enum</dt><dd><p>observation number within subject</p>
</dd>
</dl>



<h3>Details</h3>

<p>The <code>cgd0</code> data set is in the form found in the references,
with one line per patient and no recoding of the variables.
The <code>cgd</code> data set (this one) has been cast into (start, stop]
format with one line per event, and covariates
such as center recoded as factors
to include meaningful labels. 
</p>


<h3>Source</h3>

<p>Fleming and Harrington, Counting Processes and Survival Analysis,
appendix D.2. 
</p>


<h3>See Also</h3>

<p><code>link{cgd0}</code></p>

<hr>
<h2 id='cgd0'>Chronic Granulotomous Disease data</h2><span id='topic+cgd0'></span>

<h3>Description</h3>

<p>Data are from a placebo controlled trial of gamma
interferon in chronic granulotomous disease (CGD).
Contains the data on time to serious infections observed through
end of study for each patient. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cgd0</code></pre>


<h3>Format</h3>


<dl>
<dt>id</dt><dd><p>subject identification number</p>
</dd>
<dt>center</dt><dd><p>enrolling center </p>
</dd>
<dt>random</dt><dd><p>date of randomization </p>
</dd>
<dt>treatment</dt><dd><p>placebo or gamma interferon </p>
</dd>
<dt>sex</dt><dd><p>sex</p>
</dd>
<dt>age</dt><dd><p>age in years, at study entry </p>
</dd>
<dt>height</dt><dd><p>height in cm at study entry</p>
</dd>
<dt>weight</dt><dd><p>weight in kg at study entry</p>
</dd>
<dt>inherit</dt><dd><p>pattern of inheritance </p>
</dd>
<dt>steroids</dt><dd><p>use of steroids at study entry,1=yes</p>
</dd>
<dt>propylac</dt><dd><p>use of prophylactic antibiotics at study entry</p>
</dd>
<dt>hos.cat</dt><dd><p>a categorization of the centers into 4 groups</p>
</dd>
<dt>futime</dt><dd><p>days to last follow-up</p>
</dd>
<dt>etime1-etime7</dt><dd><p>up to 7 infection times for the subject</p>
</dd>
</dl>



<h3>Details</h3>

<p>The <code>cgdraw</code> data set (this one) is in the form found in the references,
with one line per patient and no recoding of the variables.
</p>
<p>The <code>cgd</code> data set has been further processed so as to have one
line per event, with covariates such as center recoded as factors
to include meaningful labels. </p>


<h3>Source</h3>

<p>Fleming and Harrington, Counting Processes and Survival Analysis,
appendix D.2. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cgd">cgd</a></code></p>

<hr>
<h2 id='cipoisson'>Confidence limits for the Poisson</h2><span id='topic+cipoisson'></span>

<h3>Description</h3>

<p>Confidence interval calculation for Poisson rates.</p>


<h3>Usage</h3>

<pre><code class='language-R'>cipoisson(k, time = 1, p = 0.95, method = c("exact", "anscombe"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cipoisson_+3A_k">k</code></td>
<td>
<p>Number of successes</p>
</td></tr>
<tr><td><code id="cipoisson_+3A_time">time</code></td>
<td>
<p>Total time on trial</p>
</td></tr>
<tr><td><code id="cipoisson_+3A_p">p</code></td>
<td>
<p>Probability level for the (two-sided) interval</p>
</td></tr>
<tr><td><code id="cipoisson_+3A_method">method</code></td>
<td>
<p>The method for computing the interval.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The likelihood method is based on equation 10.10 of Feller, which relates
poisson probabilities to tail area of the gamma distribution.
The Anscombe approximation is based on the fact that sqrt(k + 3/8)
has a nearly constant variance of 1/4, along with a continuity
correction.
</p>
<p>There are many other proposed intervals: Patil and Kulkarni list and
evaluate 19 different suggestions from the literature!.  The exact
intervals can be overly broad for very small values of <code>k</code>, many of
the other approaches try to shrink the lengths, with varying success.
</p>


<h3>Value</h3>

<p>a vector, matrix, or array.
If both <code>k</code> and <code>time</code> are single values the result is a
vector of length 2 containing the lower an upper limits.
If either or both are vectors the result is a matrix with two columns.
If <code>k</code> is a matrix or array, the result will be an array with one
more dimension; in this case the dimensions and dimnames (if any) of
<code>k</code> are preserved.
</p>


<h3>References</h3>

<p>F.J. Anscombe (1949). Transformations of Poisson, binomial and
negative-binomial data. Biometrika, 35:246-254.
</p>
<p>W.F. Feller (1950). An Introduction to Probability Theory and its
Applications, Volume 1, Chapter 6, Wiley.
</p>
<p>V. V. Patil and H.F. Kulkarni (2012).  Comparison of confidence
intervals for the poisson mean: some new aspects.
Revstat 10:211-227.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+Poisson">ppois</a></code>, <code><a href="stats.html#topic+Poisson">qpois</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cipoisson(4) # 95\% confidence limit 
# lower    upper  
# 1.089865 10.24153 
ppois(4, 10.24153)     #chance of seeing 4 or fewer events with large rate  
# [1] 0.02500096 
1-ppois(3, 1.08986)    #chance of seeing 4 or more, with a small rate 
# [1] 0.02499961

</code></pre>

<hr>
<h2 id='clogit'>Conditional logistic regression </h2><span id='topic+clogit'></span>

<h3>Description</h3>

<p>Estimates a logistic regression model by maximising the conditional
likelihood. Uses a model formula of the form
<code>case.status~exposure+strata(matched.set)</code>.
The default is to use the exact conditional likelihood, a commonly
used approximate conditional likelihood is provided for compatibility
with older software.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clogit(formula, data, weights, subset, na.action,
 method=c("exact", "approximate", "efron", "breslow"),
 ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clogit_+3A_formula">formula</code></td>
<td>
<p>Model formula</p>
</td></tr>
<tr><td><code id="clogit_+3A_data">data</code></td>
<td>
<p>data frame </p>
</td></tr>
<tr><td><code id="clogit_+3A_weights">weights</code></td>
<td>
<p>optional, names the variable containing case weights</p>
</td></tr>
<tr><td><code id="clogit_+3A_subset">subset</code></td>
<td>
<p>optional, subset the data</p>
</td></tr>
<tr><td><code id="clogit_+3A_na.action">na.action</code></td>
<td>
<p>optional na.action argument.  By default the
global option <code>na.action</code> is used.</p>
</td></tr>
<tr><td><code id="clogit_+3A_method">method</code></td>
<td>
<p>use the correct (exact) calculation in the conditional
likelihood or one of the approximations</p>
</td></tr>
<tr><td><code id="clogit_+3A_...">...</code></td>
<td>
<p>optional arguments, which will be passed to
<code>coxph.control</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>It turns out that the loglikelihood for a conditional logistic
regression model = loglik from a Cox model with a particular data
structure.  Proving this is a nice homework exercise for a PhD
statistics class; not too hard, but the fact that it is true is
surprising.
</p>
<p>When a well tested Cox model routine is available many packages use
this &lsquo;trick&rsquo; rather than writing a new software routine from
scratch, and this is what the clogit routine does.
In detail,  a stratified Cox model with each case/control group
assigned to its own stratum, time set to a constant,
status of 1=case 0=control,
and using the exact partial likelihood has the same likelihood formula
as a conditional logistic regression.  The clogit routine creates
the necessary dummy variable of times (all 1) and the strata,
then calls coxph.
</p>
<p>The computation of the exact partial likelihood can be very slow,
however.  If a particular strata had say 10 events out of 20 subjects
we have to add up a denominator that involves all possible ways of
choosing 10 out of 20, which is 20!/(10! 10!) = 184756 terms. Gail et
al describe a fast recursion method which partly ameliorates
this; it was incorporated into version 2.36-11 of the survival
package.  The computation remains infeasible for very large groups of
ties, say 100 ties out of 500 subjects, and may even lead to integer
overflow for the subscripts &ndash; in this latter case the routine will
refuse to undertake the task.  The Efron approximation is normally a
sufficiently accurate substitute.
</p>
<p>Most of the time conditional logistic modeling 
is applied data with 1 case + k controls per set, in
which case all of the approximations for ties lead to exactly the
same result.  
The 'approximate' option maps to the
Breslow approximation for the Cox model, for historical reasons.
</p>
<p>Case weights are not allowed when the exact option is used, as the
likelihood is not defined for fractional weights.
Even with integer case weights it is not clear how they should be  
handled.  For instance if
there are two deaths in a strata, one with weight=1 and one with
weight=2, should the likelihood calculation consider all subsets of
size 2 or all subsets of size 3?
Consequently, case weights are ignored by the routine in this case.
</p>


<h3>Value</h3>

<p>An object of class <code>"clogit"</code>, which is a wrapper for a
<code>"coxph"</code> object.
</p>


<h3>References</h3>

<p>Michell H Gail, Jay H Lubin and Lawrence V Rubinstein.  Likelihood
calculations for matched case-control studies and survival studies with
tied death times.  Biometrika 68:703-707, 1980.
</p>
<p>John A. Logan. A multivariate model for mobility tables.
Am J Sociology 89:324-349, 1983.
</p>


<h3>Author(s)</h3>

<p>Thomas Lumley</p>


<h3>See Also</h3>

<p><code><a href="#topic+strata">strata</a></code>,<code><a href="#topic+coxph">coxph</a></code>,<code><a href="stats.html#topic+glm">glm</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: clogit(case ~ spontaneous + induced + strata(stratum), data=infert)

# A multinomial response recoded to use clogit
#  The revised data set has one copy per possible outcome level, with new
#  variable tocc = target occupation for this copy, and case = whether
#  that is the actual outcome for each subject.
# See the reference below for the data.
resp &lt;- levels(logan$occupation)
n &lt;- nrow(logan)
indx &lt;- rep(1:n, length(resp))
logan2 &lt;- data.frame(logan[indx,],
                     id = indx,
                     tocc = factor(rep(resp, each=n)))
logan2$case &lt;- (logan2$occupation == logan2$tocc)
clogit(case ~ tocc + tocc:education + strata(id), logan2)
</code></pre>

<hr>
<h2 id='cluster'>
Identify clusters. 
</h2><span id='topic+cluster'></span>

<h3>Description</h3>

<p>This is a special function used in the context of survival models.  It 
identifies correlated groups of observations, and is used on the right hand 
side of a formula.
This style is now discouraged, use the <code>cluster</code> option instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_+3A_x">x</code></td>
<td>

<p>A character, factor, or numeric variable. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function's only action is semantic, to mark a variable as the 
cluster indicator.
The resulting variance is what is known as the &ldquo;working independence&rdquo;
variance  in a GEE model.
Note that one cannot use both a frailty term and a cluster term in the
same model, the first is a mixed-effects approach to correlation and the
second a GEE approach, and these don't mix.
</p>


<h3>Value</h3>

<p><code>x</code> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>,  <code><a href="#topic+survreg">survreg</a></code>   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>marginal.model &lt;- coxph(Surv(time, status) ~ rx, data= rats, cluster=litter,
                         subset=(sex=='f'))
frailty.model  &lt;- coxph(Surv(time, status) ~ rx + frailty(litter), rats,
                         subset=(sex=='f'))
</code></pre>

<hr>
<h2 id='colon'>Chemotherapy for Stage B/C colon cancer</h2><span id='topic+colon'></span>

<h3>Description</h3>

<p>These are data from one of the first successful trials of
adjuvant chemotherapy for colon cancer. Levamisole is a low-toxicity
compound previously used to treat worm infestations in animals; 5-FU
is a moderately toxic (as these things go) chemotherapy agent. There
are two records per person, one for recurrence and one for death</p>


<h3>Usage</h3>

<pre><code class='language-R'>colon
       data(cancer, package="survival")
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
id:</td><td style="text-align: left;"> id</td>
</tr>
<tr>
 <td style="text-align: left;">
study:</td><td style="text-align: left;"> 1 for all patients</td>
</tr>
<tr>
 <td style="text-align: left;">
rx:</td><td style="text-align: left;"> Treatment - Obs(ervation), Lev(amisole), Lev(amisole)+5-FU</td>
</tr>
<tr>
 <td style="text-align: left;">
sex:</td><td style="text-align: left;">  1=male</td>
</tr>
<tr>
 <td style="text-align: left;">
age:</td><td style="text-align: left;"> in years</td>
</tr>
<tr>
 <td style="text-align: left;">
obstruct:</td><td style="text-align: left;"> obstruction of colon by tumour</td>
</tr>
<tr>
 <td style="text-align: left;">
perfor:</td><td style="text-align: left;"> perforation of colon</td>
</tr>
<tr>
 <td style="text-align: left;">
adhere:</td><td style="text-align: left;"> adherence to nearby organs</td>
</tr>
<tr>
 <td style="text-align: left;">
nodes:</td><td style="text-align: left;"> number of lymph nodes with detectable cancer</td>
</tr>
<tr>
 <td style="text-align: left;">
time:</td><td style="text-align: left;"> days until event or censoring</td>
</tr>
<tr>
 <td style="text-align: left;">
status:</td><td style="text-align: left;"> censoring status</td>
</tr>
<tr>
 <td style="text-align: left;">
differ:</td><td style="text-align: left;"> differentiation of tumour (1=well, 2=moderate, 3=poor)</td>
</tr>
<tr>
 <td style="text-align: left;">
extent:</td><td style="text-align: left;"> Extent of local spread (1=submucosa, 2=muscle, 3=serosa,
4=contiguous structures)</td>
</tr>
<tr>
 <td style="text-align: left;">
surg:</td><td style="text-align: left;"> time from surgery to registration (0=short, 1=long)</td>
</tr>
<tr>
 <td style="text-align: left;">
node4:</td><td style="text-align: left;"> more than 4 positive lymph nodes</td>
</tr>
<tr>
 <td style="text-align: left;">
etype:</td><td style="text-align: left;"> event type: 1=recurrence,2=death</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<h3>Note</h3>

<p>The study is originally described in Laurie (1989). 
The main report is found in Moertel (1990).  This data set is closest
to that of the final report in Moertel (1991).
A version of the data with less follow-up time was used in the
paper by Lin (1994).
</p>
<p>Peter Higgins has pointed out a data inconsistency, revealed by
<code>table(colon$nodes, colon$node4)</code>.  We don't know which of the
two variables is actually correct so have elected not to 'fix' it.
(Real data has warts, why not have some in the example data too?)
</p>


<h3>References</h3>

<p>JA Laurie, CG Moertel, TR Fleming, HS Wieand, JE Leigh, J Rubin,
GW McCormack, JB Gerstner, JE Krook and J Malliard.  Surgical
adjuvant therapy of large-bowel carcinoma: An evaluation of
levamisole and the combination of levamisole and fluorouracil:
The North Central Cancer Treatment Group and the Mayo Clinic.
J Clinical Oncology, 7:1447-1456, 1989.
</p>
<p>DY Lin.  Cox regression analysis of multivariate failure time data:
the marginal approach.  Statistics in Medicine, 13:2233-2247, 1994.
</p>
<p>CG Moertel, TR Fleming, JS MacDonald, 
DG Haller, JA Laurie, PJ Goodman, JS Ungerleider, 
WA Emerson, DC Tormey, JH Glick, MH Veeder and JA Maillard. 
Levamisole and fluorouracil for adjuvant therapy of 
resected colon carcinoma. New England J of Medicine, 332:352-358,
1990.
</p>
<p>CG Moertel,  TR Fleming,  JS MacDonald, DG Haller, JA Laurie, CM Tangen,
JS Ungerleider, WA Emerson, DC Tormey, JH Glick, MH Veeder and JA
Maillard, Fluorouracil plus Levamisole as an effective adjuvant
therapy after resection of stage II colon carcinoma: a final report.
Annals of Internal Med, 122:321-326, 1991.
</p>

<hr>
<h2 id='concordance'>Compute the concordance statistic for data or a model</h2><span id='topic+concordance'></span><span id='topic+concordance.coxph'></span><span id='topic+concordance.formula'></span><span id='topic+concordance.lm'></span><span id='topic+concordance.survreg'></span>

<h3>Description</h3>

<p>The concordance statistic compute the agreement between an observed
response and a predictor.  It is closely related to Kendall's tau-a and
tau-b, Goodman's gamma, and Somers' d, all of which can also be
calculated from the results of this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concordance(object, ...)
## S3 method for class 'formula'
concordance(object, data, weights, subset, na.action,
  cluster, ymin, ymax, timewt= c("n", "S", "S/G", "n/G2", "I"),
  influence=0, ranks = FALSE, reverse=FALSE, timefix=TRUE, keepstrata=10, ...)
## S3 method for class 'lm'
concordance(object, ..., newdata, cluster, ymin, ymax,
  influence=0, ranks=FALSE, timefix=TRUE, keepstrata=10)
## S3 method for class 'coxph'
concordance(object, ..., newdata, cluster, ymin, ymax,
  timewt= c("n", "S", "S/G", "n/G2", "I"), influence=0,
  ranks=FALSE, timefix=TRUE, keepstrata=10)
## S3 method for class 'survreg'
concordance(object, ..., newdata, cluster, ymin, ymax,
  timewt= c("n", "S", "S/G", "n/G2", "I"), influence=0,
  ranks=FALSE, timefix=TRUE, keepstrata=10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="concordance_+3A_object">object</code></td>
<td>
<p>a fitted model or a formula.  The formula should be of
the form <code>y ~x</code>  or <code>y ~ x + strata(z)</code> with a single
numeric or survival response and a single predictor.
Counts of concordant, discordant and tied pairs 
are computed separately per stratum, and then added.
</p>
</td></tr>
<tr><td><code id="concordance_+3A_data">data</code></td>
<td>

<p>a data.frame in which to interpret the variables named in 
the <code>formula</code>, or in the <code>subset</code> and the <code>weights</code>
argument. Only applicable if <code>object</code> is a formula.
</p>
</td></tr>
<tr><td><code id="concordance_+3A_weights">weights</code></td>
<td>

<p>optional vector of case weights.
Only applicable if <code>object</code> is a formula.
</p>
</td></tr>
<tr><td><code id="concordance_+3A_subset">subset</code></td>
<td>

<p>expression indicating which subset of the rows of data should be used in 
the fit.   Only applicable if <code>object</code> is a formula.
</p>
</td></tr>
<tr><td><code id="concordance_+3A_na.action">na.action</code></td>
<td>

<p>a missing-data filter function.  This is applied to the model.frame
after any subset argument has been used.  Default is
<code>options()\$na.action</code>. Only applicable if <code>object</code> is a formula.
</p>
</td></tr>
<tr><td><code id="concordance_+3A_...">...</code></td>
<td>
<p>multiple fitted models are allowed.  Only applicable if
<code>object</code> is a model object.</p>
</td></tr>
<tr><td><code id="concordance_+3A_newdata">newdata</code></td>
<td>
<p>optional, a new data frame in which to evaluate (but
not refit) the models</p>
</td></tr>
<tr><td><code id="concordance_+3A_cluster">cluster</code></td>
<td>
<p>optional grouping vector for calculating the robust
variance</p>
</td></tr>
<tr><td><code id="concordance_+3A_ymin">ymin</code>, <code id="concordance_+3A_ymax">ymax</code></td>
<td>
<p>compute the concordance over the restricted range
ymin &lt;= y &lt;= ymax.  (For survival data this is a time range.)
</p>
</td></tr>
<tr><td><code id="concordance_+3A_timewt">timewt</code></td>
<td>
<p>the weighting to be applied.  The overall statistic is a
weighted mean over event times.
</p>
</td></tr>
<tr><td><code id="concordance_+3A_influence">influence</code></td>
<td>
<p>1= return the dfbeta vector, 2= return the full
influence matrix, 3 = return both
</p>
</td></tr>
<tr><td><code id="concordance_+3A_ranks">ranks</code></td>
<td>
<p>if TRUE, return a data frame containing the
scaled ranks that make up the overall score.  
</p>
</td></tr>
<tr><td><code id="concordance_+3A_reverse">reverse</code></td>
<td>
<p>if TRUE then assume that larger <code>x</code> values predict
smaller response values <code>y</code>; a proportional hazards model is
the common example of this, larger hazard = shorter survival.</p>
</td></tr>
<tr><td><code id="concordance_+3A_timefix">timefix</code></td>
<td>
<p>correct for possible rounding error.  See the
vignette on tied times for more explanation. Essentially, exact ties
are an important part of the concordance computatation, but &quot;exact&quot;
can be a subtle issue with floating point numbers.
</p>
</td></tr>
<tr><td><code id="concordance_+3A_keepstrata">keepstrata</code></td>
<td>
<p>either TRUE, FALSE, or an integer value.
Computations are always done within stratum, then added. If the
total number of strata greater than <code>keepstrata</code>, or
<code>keepstrata=FALSE</code>, those subtotals are not kept in the output.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The concordance is an estimate of
<code class="reqn">Pr(x_i &lt; x_j | y_i &lt; y_j)</code>,
for a model fit replace <code class="reqn">x</code> with <code class="reqn">\hat y</code>, the
predicted response from the model.
For a survival outcome some pairs of values
are not comparable, e.g., censored at time 5 and a death at time 6,
as we do not know if the first observation will or will not outlive
the second.  In this case the total number of evaluable pairs is smaller.
</p>
<p>Relatations to other statistics:
For continuous x and y, 2C- 1 is equal to Somers' d.
If the response is binary, C is equal to the area under the receiver
operating curve or AUC.
For a survival response and binary predictor C is the numerator of
the Gehan-Wilcoxon test.
</p>
<p>A naive compuation requires adding up over all n(n-1)/2 comparisons,
which can be quite slow for large data sets.
This routine uses an O(n log(n)) algorithm.
At each uncensored event time y, compute the rank of x for the subject
who had the event as compared to the x values for all others with a longer
survival, where the rank has value between 0 and 1.
The concordance is a weighted mean of these ranks,
determined by the <code>timewt</code> option. The rank vector can be
efficiently updated as subjects are added to the risk set.
For further details see the vignette.
</p>
<p>The variance is based on an infinetesimal jackknife.  One advantage of
this approach is that it also gives a valid covariance for the
covariance based on multiple different predicted values, even if those
predictions come from quite different models.  See for instance the
example below which has a poisson and two non-nested Cox models.
This has been useful to compare a machine learning model to a Cox
model fit, say.
It is absolutely critical, however, that the predicted values line up
exactly, with the same observation in each row; otherwise the result
will be nonsense.  (Be alert to the impact of missing values.)
</p>
<p>The <code>timewt</code> option is only applicable to censored data.  In this
case the default corresponds to Harrell's C statistic, which is
closely related to the Gehan-Wilcoxon test;
<code>timewt="S"</code> corrsponds to the Peto-Wilcoxon,
<code>timewt="S/G"</code> is suggested by Schemper, and
<code>timewt="n/G2"</code> corresponds to Uno's C.
It turns out that the Schemper and Uno weights are computationally
identical, we have retained both option labels as a user convenience.
The <code>timewt= "I"</code> option is related to the log-rank
statistic.
</p>
<p>When the number of strata is very large, such as in a conditional
logistic regression for instance (<code>clogit</code> function), a much
faster computation is available when the individual strata results
are not retained; use <code>keepstrata=FALSE</code> or <code>keepstrata=0</code>
to do so. In the general case the <code>keepstrata = 10</code>
default simply keeps the printout managable: it retains and prints
per-strata counts if the number of strata is &lt;= 10.
</p>


<h3>Value</h3>

<p>An object of class <code>concordance</code> containing the following
components:
</p>
<table>
<tr><td><code>concordance</code></td>
<td>
<p>the estimated concordance value or values</p>
</td></tr>
<tr><td><code>count</code></td>
<td>
<p>a vector containing the number of concordant pairs,
discordant, tied on x but not y, tied on y but not x, and tied on
both x and y</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the number of observations</p>
</td></tr>
<tr><td><code>var</code></td>
<td>
<p>a vector containing the estimated variance of the
concordance based on the infinitesimal jackknife (IJ) method.
If there are multiple models it contains the estimtated
variance/covariance matrix.</p>
</td></tr>
<tr><td><code>cvar</code></td>
<td>
<p>a vector containing the estimated variance(s) of the
concordance values, based on the variance formula for the associated
score test from a proportional hazards model.  (This was the primary
variance used in the <code>survConcordance</code> function.)</p>
</td></tr>
<tr><td><code>dfbeta</code></td>
<td>
<p>optional, the vector of leverage estimates for the
concordance</p>
</td></tr>
<tr><td><code>influence</code></td>
<td>
<p>optional, the matrix of leverage values for each of
the counts, one row per observation</p>
</td></tr>
<tr><td><code>ranks</code></td>
<td>
<p>optional, a data frame containing the Somers' d rank
at each event time, along with the time weight, and the case weight of
the observation.  The time weighted sum of the ranks will equal
concordant pairs - discordant pairs.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>A coxph model that has a numeric failure may have undefined
predicted values, in which case the concordance will be NULL.
</p>
<p>Computation for an existing coxph model along with <code>newdata</code> has
some subtleties with respect to extra arguments in the original call.
These include
</p>

<ul>
<li><p> tt() terms in the model.  This is not supported with newdata.
</p>
</li>
<li><p> subset.  Any subset clause in the original call is ignored,
i.e., not applied to the new data.
</p>
</li>
<li><p> strata() terms in the model.  The new data is expected to
have the strata variable(s) found in the original data set,
with concordance computed within strata.
The levels of the strata variable need not be
the same as in the original data.
</p>
</li>
<li><p> id or cluster directives.  This has not yet been sorted out.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>References</h3>

<p>F Harrell, R Califf, D Pryor, K Lee and R Rosati, 
Evaluating the yield of medical tests, J Am Medical Assoc, 1982.
</p>
<p>R Peto and J Peto,
Asymptotically efficient rank invariant test procedures (with
discussion), J Royal Stat Soc A, 1972.
</p>
<p>M Schemper, Cox analysis of survival data with non-proportional
hazard functions, The Statistician, 1992.
</p>
<p>H Uno, T Cai, M Pencina, R D'Agnostino and Lj Wei,
On the C-statistics for evaluating overall adequacy of risk 
prediction procedures with censored survival data,
Statistics in Medicine, 2011.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>fit1 &lt;- coxph(Surv(ptime, pstat) ~ age + sex + mspike, mgus2)
concordance(fit1, timewt="n/G2")  # Uno's weighting

# logistic regression 
fit2 &lt;- glm(I(sex=='M') ~ age + log(creatinine), binomial, data= flchain)
concordance(fit2)  # equal to the AUC

# compare multiple models 
options(na.action = na.exclude)   # predict all 1384 obs, including missing
fit3 &lt;- glm(pstat ~ age + sex + mspike + offset(log(ptime)), 
            poisson, data= mgus2)
fit4 &lt;- coxph(Surv(ptime, pstat) ~ age + sex + mspike, mgus2)
fit5 &lt;- coxph(Surv(ptime, pstat) ~ age + sex + hgb + creat, mgus2)

tdata &lt;- mgus2; tdata$ptime &lt;- 60   # prediction at 60 months
p3 &lt;- -predict(fit3, newdata=tdata) 
p4 &lt;- -predict(fit4) # high risk scores predict shorter survival
p5 &lt;- -predict(fit5)
options(na.action = na.omit)      # return to the R default

cfit &lt;- concordance(Surv(ptime, pstat) ~p3 +  p4 + p5, mgus2)
cfit
round(coef(cfit), 3)
round(cov2cor(vcov(cfit)), 3)  # high correlation

test &lt;- c(1, -1, 0)  # contrast vector for model 1 - model 2 
round(c(difference = test %*% coef(cfit),
        sd= sqrt(test %*% vcov(cfit) %*% test)), 3)
</code></pre>

<hr>
<h2 id='concordancefit'>Compute the concordance</h2><span id='topic+concordancefit'></span>

<h3>Description</h3>

<p>This is the working routine behind the <code>concordance</code> function.  It
is not meant to be called by users, but is available for other packages
to use.  Input arguments, for instance, are assumed to all be the
correct length and type, and missing values are not allowed: the calling
routine is responsible for these things.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concordancefit(y, x, strata, weights, ymin = NULL, ymax = NULL,
 timewt = c("n", "S", "S/G", "n/G2", "I"), cluster, influence =0,
 ranks = FALSE, reverse = FALSE, timefix = TRUE, keepstrata=10, 
 std.err = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="concordancefit_+3A_y">y</code></td>
<td>
<p>the response.  It can be numeric, factor, or a Surv object</p>
</td></tr>
<tr><td><code id="concordancefit_+3A_x">x</code></td>
<td>
<p>the predictor, a numeric vector</p>
</td></tr>
<tr><td><code id="concordancefit_+3A_strata">strata</code></td>
<td>
<p>optional numeric vector that stratifies the data</p>
</td></tr>
<tr><td><code id="concordancefit_+3A_weights">weights</code></td>
<td>
<p>options vector of case weights</p>
</td></tr>
<tr><td><code id="concordancefit_+3A_ymin">ymin</code>, <code id="concordancefit_+3A_ymax">ymax</code></td>
<td>
<p>restrict the comparison to response values in this
range</p>
</td></tr>
<tr><td><code id="concordancefit_+3A_timewt">timewt</code></td>
<td>
<p>the time weighting to be used</p>
</td></tr>
<tr><td><code id="concordancefit_+3A_cluster">cluster</code>, <code id="concordancefit_+3A_influence">influence</code>, <code id="concordancefit_+3A_ranks">ranks</code>, <code id="concordancefit_+3A_reverse">reverse</code>, <code id="concordancefit_+3A_timefix">timefix</code></td>
<td>
<p>see the help for the
<code>concordance</code> function</p>
</td></tr>
<tr><td><code id="concordancefit_+3A_keepstrata">keepstrata</code></td>
<td>
<p>either TRUE, FALSE, or an integer value.
Computations are always done within stratum, then added. If the
total number of strata greater than <code>keepstrata</code>, or
<code>keepstrata=FALSE</code>, those subtotals are not kept in the output.
</p>
</td></tr>
<tr><td><code id="concordancefit_+3A_std.err">std.err</code></td>
<td>
<p>compute the standard error; not doing so saves some
compute time.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is provided for those who want a &ldquo;direct&rdquo; call to the
concordance calculations, without using the formula interface.  A
primary use has been other packages.   The routine does minimal
checking of its input arguments, under the assumption that this has
already been taken care of by the calling routine.
</p>


<h3>Value</h3>

<p>a list containing the results</p>


<h3>Author(s)</h3>

<p> Terry Therneau</p>


<h3>See Also</h3>

 <p><code><a href="#topic+concordance">concordance</a></code></p>

<hr>
<h2 id='cox.zph'>
Test the Proportional Hazards Assumption of a Cox Regression
</h2><span id='topic+cox.zph'></span><span id='topic++5B.cox.zph'></span><span id='topic+print.cox.zph'></span>

<h3>Description</h3>

<p>Test the proportional hazards assumption for a Cox regression model fit 
(<code>coxph</code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cox.zph(fit, transform="km", terms=TRUE, singledf=FALSE, global=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cox.zph_+3A_fit">fit</code></td>
<td>

<p>the result of fitting a Cox regression model, using the
<code>coxph</code> or <code>coxme</code> functions. 
</p>
</td></tr>
<tr><td><code id="cox.zph_+3A_transform">transform</code></td>
<td>

<p>a character string specifying how the survival times should be transformed 
before the test is performed. 
Possible values are <code>"km"</code>, <code>"rank"</code>, <code>"identity"</code> or a 
function of one argument. 
</p>
</td></tr>
<tr><td><code id="cox.zph_+3A_terms">terms</code></td>
<td>
<p>if TRUE, do a test for each term in the model rather than
for each separate covariate.  For a factor variable with k levels,
for instance, this would lead to a k-1 degree of freedom test.  The
plot for such variables will be a single curve evaluating the linear
predictor over time.</p>
</td></tr>
<tr><td><code id="cox.zph_+3A_singledf">singledf</code></td>
<td>
<p>use a single degree of freedom test for terms that
have multiple coefficients, i.e., the test that corresponds most
closely to the plot.  If <code>terms=FALSE</code> this argument has no
effect.</p>
</td></tr>
<tr><td><code id="cox.zph_+3A_global">global</code></td>
<td>

<p>should a global chi-square test be done, in addition to the 
per-variable or per-term tests tests. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The computations require the original <code>x</code> matrix of the Cox model fit. 
Thus it saves time if the <code>x=TRUE</code> option is used in <code>coxph</code>. 
This function would usually be followed by both a plot and a print of 
the result. 
The plot gives an estimate of the time-dependent coefficient
<code class="reqn">\beta(t)</code>. 
If the proportional hazards assumption holds then the true
<code class="reqn">\beta(t)</code> function would be a  horizontal line.
The <code>table</code> component provides the results of a formal score test
for slope=0, a linear fit to the plot would approximate the test.
</p>
<p>Random effects terms such a <code>frailty</code> or random effects in a
<code>coxme</code> model are not checked for proportional hazards, rather
they are treated as a fixed offset in model.
</p>
<p>If the model contains strata by covariate interactions, then the
<code>y</code> matrix may contain structural zeros, i.e., deaths (rows) that
had no role in estimation of a given coefficient (column).
These are marked as NA.
If an entire row is NA, for instance after subscripting a
<code>cox.zph</code> object, that row is removed.
</p>


<h3>Value</h3>

<p>an object of class <code>"cox.zph"</code>, with components: 
</p>
<table>
<tr><td><code>table</code></td>
<td>

<p>a matrix with one row for each variable, and optionally a last row for 
the global test. 
Columns of the matrix contain a score test of for addition of the
time-dependent term, the degrees of freedom,
and the two-sided p-value. 
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>

<p>the transformed time axis. 
</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>the untransformed time values; there is one entry for each
event time in the data</p>
</td></tr>
<tr><td><code>strata</code></td>
<td>
<p>for a stratified <code>coxph model</code>, the stratum of
each of the events</p>
</td></tr>
<tr><td><code>y</code></td>
<td>

<p>the matrix of scaled Schoenfeld residuals.  There will be one column per 
term or per variable (depending on the <code>terms</code> option above),
and one row per event.  The row labels are a rounded form of the
original times.
</p>
</td></tr>
<tr><td><code>var</code></td>
<td>
<p>a variance matrix for the covariates, used to create an
approximate standard error band for plots</p>
</td></tr>
<tr><td><code>transform</code></td>
<td>
<p>the transform of time that was used</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>the calling sequence for the routine. 
</p>
</td></tr></table>


<h3>Note</h3>

<p>In versions of the package before survival3.0 the function
computed a fast approximation to the score test.  Later versions
compute the actual score test.
</p>


<h3>References</h3>

<p>P. Grambsch and T. Therneau (1994), 
Proportional hazards tests and diagnostics based on weighted residuals. 
<em>Biometrika,</em>
<b>81</b>, 515-26. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>,  <code><a href="#topic+Surv">Surv</a></code>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- coxph(Surv(futime, fustat) ~ age + ecog.ps,  
             data=ovarian) 
temp &lt;- cox.zph(fit) 
print(temp)                  # display the results 
plot(temp)                   # plot curves 
</code></pre>

<hr>
<h2 id='coxph'>
Fit Proportional Hazards Regression Model 
</h2><span id='topic+coxph'></span><span id='topic+print.coxph.null'></span><span id='topic+print.coxph.penal'></span><span id='topic+coxph.penalty'></span><span id='topic+coxph.getdata'></span><span id='topic+summary.coxph.penal'></span>

<h3>Description</h3>

<p>Fits a Cox proportional hazards regression model. 
Time dependent variables, time dependent strata, multiple events per subject, 
and other extensions are incorporated using the counting process formulation 
of Andersen and Gill. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coxph(formula, data=, weights, subset, 
      na.action, init, control, 
      ties=c("efron","breslow","exact"), 
      singular.ok=TRUE, robust, 
      model=FALSE, x=FALSE, y=TRUE, tt, method=ties,
      id, cluster, istate, statedata, nocenter=c(-1, 0, 1), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coxph_+3A_formula">formula</code></td>
<td>

<p>a formula object, with the response on the left of a <code>~</code> operator, and 
the terms on the right.  The response must be a survival object as 
returned by the <code>Surv</code> function.  For a multi-state model the
formula may be a list of formulas.
</p>
</td></tr>
<tr><td><code id="coxph_+3A_data">data</code></td>
<td>

<p>a data.frame in which to interpret the variables named in 
the <code>formula</code>, or in the <code>subset</code> and the <code>weights</code>
argument. 
</p>
</td></tr>
<tr><td><code id="coxph_+3A_weights">weights</code></td>
<td>

<p>vector of case weights, see the note below.
For a thorough discussion of these see the
book by Therneau and Grambsch.
</p>
</td></tr>
<tr><td><code id="coxph_+3A_subset">subset</code></td>
<td>

<p>expression indicating which subset of the rows of data should be used in 
the fit.    All observations are included by default. 
</p>
</td></tr>
<tr><td><code id="coxph_+3A_na.action">na.action</code></td>
<td>

<p>a missing-data filter function.  This is applied to the model.frame
after any 
subset argument has been used.  Default is <code>options()\$na.action</code>. 
</p>
</td></tr>
<tr><td><code id="coxph_+3A_init">init</code></td>
<td>

<p>vector of initial values of the iteration.  Default initial 
value is zero for all variables. 
</p>
</td></tr>
<tr><td><code id="coxph_+3A_control">control</code></td>
<td>

<p>Object of class <code><a href="#topic+coxph.control">coxph.control</a></code> specifying iteration limit
and other control options. Default is <code>coxph.control(...)</code>.
</p>
</td></tr>
<tr><td><code id="coxph_+3A_ties">ties</code></td>
<td>

<p>a character string specifying the method for tie handling.  If there  
are no tied death times all the methods are equivalent.
The Efron approximation is used as the default, it is more 
accurate when dealing with tied death times, and is as efficient 
computationally. (But see below for multi-state models.)
The &ldquo;exact partial likelihood&rdquo; is 
equivalent to a conditional logistic model, and is appropriate when
the times are a small set of discrete values.
</p>
</td></tr>
<tr><td><code id="coxph_+3A_singular.ok">singular.ok</code></td>
<td>

<p>logical value indicating how to handle collinearity in the model matrix. 
If <code>TRUE</code>, the program will automatically skip over columns of the X 
matrix that are linear combinations of earlier columns.  In this case the 
coefficients for such columns will be NA, and the variance matrix will 
contain zeros. For ancillary calculations, such as the linear
predictor, 
the missing coefficients are treated as zeros. 
</p>
</td></tr>
<tr><td><code id="coxph_+3A_robust">robust</code></td>
<td>
<p>should a robust variance be computed.
The default is TRUE if: there is a <code>cluster</code> argument, there
are case weights that are not 0 or 1, or there are <code>id</code> values
with more than one event. 
</p>
</td></tr>
<tr><td><code id="coxph_+3A_id">id</code></td>
<td>
<p>optional variable name that identifies subjects.  Only
necessary when a subject can have multiple rows in the data, and
there is more than one event type.  This variable will normally be
found in <code>data</code>.</p>
</td></tr>
<tr><td><code id="coxph_+3A_cluster">cluster</code></td>
<td>
<p>optional variable which clusters the observations, for
the purposes of a robust variance.  If present, it implies
<code>robust</code>.
This variable will normally be found in <code>data</code>.</p>
</td></tr>
<tr><td><code id="coxph_+3A_istate">istate</code></td>
<td>
<p>optional variable giving the current state at the start
each interval. This variable will normally be found in <code>data</code>.</p>
</td></tr>
<tr><td><code id="coxph_+3A_statedata">statedata</code></td>
<td>
<p>optional data set used to describe multistate models.</p>
</td></tr>
<tr><td><code id="coxph_+3A_model">model</code></td>
<td>

<p>logical value: if <code>TRUE</code>, the model frame is returned in component
<code>model</code>. 
</p>
</td></tr>
<tr><td><code id="coxph_+3A_x">x</code></td>
<td>

<p>logical value: if <code>TRUE</code>, the x matrix is returned in
component <code>x</code>. 
</p>
</td></tr>
<tr><td><code id="coxph_+3A_y">y</code></td>
<td>

<p>logical value: if <code>TRUE</code>, the response vector is returned in
component <code>y</code>. 
</p>
</td></tr>
<tr><td><code id="coxph_+3A_tt">tt</code></td>
<td>
<p>optional list of time-transform functions.</p>
</td></tr>
<tr><td><code id="coxph_+3A_method">method</code></td>
<td>
<p>alternate name for the <code>ties</code> argument.</p>
</td></tr>
<tr><td><code id="coxph_+3A_nocenter">nocenter</code></td>
<td>
<p>columns of the X matrix whose values lie strictly
within this set are not recentered.  Remember that a factor
variable becomes a set of 0/1 columns.</p>
</td></tr>
<tr><td><code id="coxph_+3A_...">...</code></td>
<td>
<p>Other arguments will be passed to <code><a href="#topic+coxph.control">coxph.control</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The proportional hazards model is usually expressed in terms of a 
single survival time value for each person, with possible censoring. 
Andersen and Gill reformulated the same problem as a counting process; 
as time marches onward we observe the events for a subject, rather 
like watching a Geiger counter. 
The data for a subject is presented as multiple rows or &quot;observations&quot;, 
each 
of which applies to an interval of observation (start, stop].
</p>
<p>The routine internally scales and centers data to avoid overflow in
the argument to the exponential function.  These actions do not change
the result, but lead to more numerical stability.
Any column of the X matrix whose values lie within <code>nocenter</code> list
are not recentered.  The practical consequence of the default is to not
recenter dummy variables corresponding to factors.
However, arguments to offset are not scaled since there are situations
where a large offset value is a purposefully used.
In general, however, users should not avoid very large numeric values
for an offset due to possible loss of precision in the estimates.
</p>


<h3>Value</h3>

<p>an object of class <code>coxph</code> representing the fit. 
See <code>coxph.object</code> and <code>coxphms.object</code> for details.  
</p>


<h3>Side Effects</h3>

<p>Depending on the call, the <code>predict</code>, <code>residuals</code>,
and <code>survfit</code> routines may 
need to reconstruct the x matrix created by <code>coxph</code>.
It is possible for this to fail, as in the example below in
which the predict function is unable to find <code>tform</code>.
</p>
<pre>  tfun &lt;- function(tform) coxph(tform, data=lung)
  fit &lt;- tfun(Surv(time, status) ~ age)
  predict(fit)</pre>
<p>In such a case add the <code>model=TRUE</code> option to the
<code>coxph</code> call to obviate the
need for reconstruction, at the expense of a larger <code>fit</code>
object.  
</p>


<h3>Case weights</h3>

<p>Case weights are treated as replication weights, i.e., a case weight of
2 is equivalent to having 2 copies of that subject's observation.
When computers were much smaller grouping like subjects together was a
common trick to used to conserve memory.  Setting all weights to 2 for
instance will give the same coefficient estimate but halve the variance.
When the Efron approximation for ties (default) is employed replication
of the data will not give exactly the same coefficients as the weights option,
and in this case the weighted fit is arguably the correct one.
</p>
<p>When the model includes a <code>cluster</code> term or the <code>robust=TRUE</code>
option the computed variance treats any weights as sampling weights;
setting all weights to 2 will in this case give the same variance as weights of 1.
</p>


<h3>Special terms</h3>

<p>There are three special terms that may be used in the model equation. 
A <code>strata</code> term identifies a stratified Cox model; separate baseline 
hazard functions are fit for each strata. 
The <code>cluster</code> term is used to compute a robust variance for the model. 
The term <code>+ cluster(id)</code> where each value of <code>id</code> is unique is
equivalent to 
specifying the <code>robust=TRUE</code> argument.
If the <code>id</code> variable is not 
unique, it is assumed that it identifies clusters of correlated
observations.
The robust estimate arises from many different arguments and thus has
had many labels.  It is variously known as the
Huber sandwich estimator, White's estimate (linear models/econometrics),
the Horvitz-Thompson estimate (survey sampling), the working
independence variance (generalized estimating equations), the
infinitesimal jackknife, and the Wei, Lin, Weissfeld (WLW) estimate.
</p>
<p>A time-transform term allows variables to vary dynamically in time.  In
this case the <code>tt</code> argument will be a function or a list of
functions (if there are more than one tt() term in the model) giving the
appropriate transform.   See the examples below.
</p>
<p>One user mistake that has recently arisen is to slavishly follow the
advice of some coding guides and prepend <code>survival::</code> onto
everthing, including the special terms, e.g.,
<code>survival::coxph(survival:Surv(time, status) ~ age +
    survival::cluster(inst), data=lung)</code>
First, this is unnecessary: arguments within the <code>coxph</code> call will
be evaluated within the survival namespace, so another package's Surv or
cluster function would not be noticed.
(Full qualification of the coxph call itself may be protective, however.)
Second, and more importantly, the call just above will not give the
correct answer.  The specials are recognized by their name, and
<code>survival::cluster</code> is not the same as <code>cluster</code>; the above model would
treat <code>inst</code> as an ordinary variable.
A similar issue arises from using <code>stats::offset</code> as a term, in
either survival or glm models.
</p>


<h3>Convergence</h3>

<p>In certain data cases the actual MLE estimate of a 
coefficient is infinity, e.g., a dichotomous variable where one of the 
groups has no events.  When this happens the associated coefficient 
grows at a steady pace and a race condition will exist in the fitting 
routine: either the log likelihood converges, the information matrix 
becomes effectively singular, an argument to exp becomes too large for 
the computer hardware, or the maximum number of interactions is
exceeded.
(Most often number 1 is the first to occur.)
The routine attempts to detect when this has happened,
not always successfully.
The primary consequence for the user is that the Wald statistic =
coefficient/se(coefficient) is not valid in this case and should be
ignored; the likelihood ratio and score tests remain valid however.
</p>


<h3>Ties</h3>

<p>There are three possible choices for handling tied event times.
The Breslow approximation is the easiest to program and hence became the
first option coded for almost all computer routines. It then ended up
as the default option when other options were added in order to &quot;maintain
backwards compatability&quot;.  The Efron option is more accurate if there are
a large number of ties, and it is the default option here.
In practice the number of ties is usually small, in which case all the
methods are statistically indistinguishable.
</p>
<p>Using the &quot;exact partial likelihood&quot; approach the Cox partial likelihood
is equivalent to that for matched logistic regression.  (The
<code>clogit</code> function uses the <code>coxph</code> code to do the fit.)
It is technically appropriate when the time scale is discrete and has
only a few unique values, and some packages refer to this as the
&quot;discrete&quot; option.  There is also an &quot;exact marginal likelihood&quot; due to
Prentice which is not implemented here.
</p>
<p>The calculation of the exact partial likelihood is numerically intense.
Say for instance 180 subjects are at risk on day 7 of which 15 had an event;
then the code needs to compute sums over all 180-choose-15 &gt; 10^43 different
possible subsets of size 15.  There is an efficient recursive algorithm
for this task, but even with this the computation can be insufferably
long.  With (start, stop) data it is much worse since the recursion
needs to start anew for each unique start time.
</p>
<p>Multi state models are a more difficult case.
First of all, a proper extension of the Efron argument is much more
difficult to do, and this author is not yet fully convinced that the
resulting algorithm is defensible.  
Secondly, the current code for Efron case does not consistently
compute that extended logic (and extension would require major changes
in the code).  Due to this
complexity, the default is <code>ties='breslow'</code> for the multistate
case. If <code>ties='efron'</code> is selected the current code will, in
effect, only apply to to tied transitions of the same type.
</p>
<p>A separate issue is that of artificial ties due to floating-point
imprecision. See the vignette on this topic for a full explanation or
the <code>timefix</code> option in <code>coxph.control</code>.
Users may need to add <code>timefix=FALSE</code> for simulated data sets.
</p>


<h3>Penalized regression</h3>

<p><code>coxph</code> can maximise a penalised partial likelihood with
arbitrary user-defined penalty.  Supplied penalty functions include
ridge regression (<a href="#topic+ridge">ridge</a>), smoothing splines
(<a href="#topic+pspline">pspline</a>), and frailty models (<a href="#topic+frailty">frailty</a>).
</p>


<h3>References</h3>

<p>Andersen, P. and Gill, R. (1982). 
Cox's regression model for counting processes, a large sample study. 
<em>Annals of Statistics</em>
<b>10</b>, 1100-1120. 
</p>
<p>Therneau, T., Grambsch, P., Modeling Survival Data: Extending the Cox Model. 
Springer-Verlag, 2000.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph.object">coxph.object</a></code>, <code><a href="#topic+coxphms.object">coxphms.object</a></code>,
<code><a href="#topic+coxph.control">coxph.control</a></code>,
<code><a href="#topic+cluster">cluster</a></code>,  <code><a href="#topic+strata">strata</a></code>,  <code><a href="#topic+Surv">Surv</a></code>,
<code><a href="#topic+survfit">survfit</a></code>, <code><a href="#topic+pspline">pspline</a></code>. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create the simplest test data set 
test1 &lt;- list(time=c(4,3,1,1,2,2,3), 
              status=c(1,1,1,0,1,1,0), 
              x=c(0,2,1,1,1,0,0), 
              sex=c(0,0,0,0,1,1,1)) 
# Fit a stratified model 
coxph(Surv(time, status) ~ x + strata(sex), test1) 
# Create a simple data set for a time-dependent model 
test2 &lt;- list(start=c(1,2,5,2,1,7,3,4,8,8), 
              stop=c(2,3,6,7,8,9,9,9,14,17), 
              event=c(1,1,1,1,1,1,1,0,0,0), 
              x=c(1,0,0,1,0,1,1,1,0,0)) 
summary(coxph(Surv(start, stop, event) ~ x, test2)) 

#
# Create a simple data set for a time-dependent model
#
test2 &lt;- list(start=c(1, 2, 5, 2, 1, 7, 3, 4, 8, 8),
                stop =c(2, 3, 6, 7, 8, 9, 9, 9,14,17),
                event=c(1, 1, 1, 1, 1, 1, 1, 0, 0, 0),
                x    =c(1, 0, 0, 1, 0, 1, 1, 1, 0, 0) )


summary( coxph( Surv(start, stop, event) ~ x, test2))

# Fit a stratified model, clustered on patients 

bladder1 &lt;- bladder[bladder$enum &lt; 5, ] 
coxph(Surv(stop, event) ~ (rx + size + number) * strata(enum),
      cluster = id, bladder1)

# Fit a time transform model using current age
coxph(Surv(time, status) ~ ph.ecog + tt(age), data=lung,
     tt=function(x,t,...) pspline(x + t/365.25))
</code></pre>

<hr>
<h2 id='coxph.control'>Ancillary arguments for controlling coxph fits</h2><span id='topic+coxph.control'></span>

<h3>Description</h3>

<p>This is used to set various numeric parameters controlling a Cox model fit.
Typically it would only be used in a call to <code>coxph</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coxph.control(eps = 1e-09, toler.chol = .Machine$double.eps^0.75,
iter.max = 20, toler.inf = sqrt(eps), outer.max = 10, timefix=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coxph.control_+3A_eps">eps</code></td>
<td>
<p>Iteration continues until the relative change in the log partial
likelihood is less than eps, or the absolute change is less than
sqrt(eps).  Must be positive.</p>
</td></tr>
<tr><td><code id="coxph.control_+3A_toler.chol">toler.chol</code></td>
<td>
<p>Tolerance for detection of singularity during a Cholesky
decomposition of the variance matrix, i.e., for detecting a redundant predictor
variable.</p>
</td></tr>
<tr><td><code id="coxph.control_+3A_iter.max">iter.max</code></td>
<td>
<p>Maximum number of iterations to attempt for convergence.</p>
</td></tr>
<tr><td><code id="coxph.control_+3A_toler.inf">toler.inf</code></td>
<td>
<p>Tolerance criteria for the warning message about a possible
infinite coefficient value.</p>
</td></tr>
<tr><td><code id="coxph.control_+3A_outer.max">outer.max</code></td>
<td>
<p>For a penalized coxph model, e.g. with pspline terms, there
is an outer loop of iteration to determine the penalty parameters; maximum
number of iterations for this outer loop.</p>
</td></tr>
<tr><td><code id="coxph.control_+3A_timefix">timefix</code></td>
<td>
<p>Resolve any near ties in the time variables.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The convergence tolerances are a balance.  Users think they want THE maximum
point of the likelihood surface, and for well behaved data sets where
this is quadratic near the max a high accuracy is fairly inexpensive:
the number of correct digits approximately doubles with each iteration.
Conversely, a drop of .0001 from the maximum in any given direction
will be correspond to only about 1/20 of a standard error change in the
coefficient.  Statistically, more precision than this is straining at a
gnat.  Based on this the author originally had set the tolerance to
1e-5, but relented in the face of multiple
&quot;why is the answer different than package X&quot; queries.  
</p>
<p>Asking for results that are too close to machine precision
(double.eps) is a fool's errand; a reasonable critera is often the
square root of that precision.  The Cholesky decompostion needs to be
held to a higher standard than the overall convergence criterion, however.
The <code>tolerance.inf</code> value controls a warning message; if it is
too small incorrect warnings can appear, if too large some actual cases of
an infinite coefficient will not be detected.
</p>
<p>The most difficult cases are data sets where the MLE coefficient is
infinite; an example is a data set where at each death time,
it was the subject with the largest
covariate value who perished.  In that situation the coefficient
increases at each iteration while the log-likelihood asymptotes to a
maximum.  As iteration proceeds there is a race condition
condition for three endpoint: exp(coef) overflows,
the Hessian matrix become singular, or the change in loglik is small
enough to satisfy the convergence criterion.  The first two are
difficult to anticipate and lead to numeric diffculties, which is
another argument for moderation in the choice of <code>eps</code>.
</p>
<p>See the vignette &quot;Roundoff error and tied times&quot; for a more
detailed explanation of the <code>timefix</code> option.  In short, when
time intervals are created via subtraction then two time intervals that are
actually identical can appear to be different due to floating point
round off error, which in turn can make <code>coxph</code> and
<code>survfit</code> results dependent
on things such as the order in which operations were done or the
particular computer that they were run on.
Such cases are unfortunatedly not rare in practice.
The <code>timefix=TRUE</code> option adds
logic similar to <code>all.equal</code> to ensure reliable results.
In analysis of simulated data sets, however, where often by defintion there
can be no duplicates, the option will often need to be set to 
<code>FALSE</code> to avoid spurious merging of close numeric values.
</p>


<h3>Value</h3>

<p>a list containing the values of each of the above constants
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>
</p>

<hr>
<h2 id='coxph.detail'>
Details of a Cox Model Fit
</h2><span id='topic+coxph.detail'></span>

<h3>Description</h3>

<p>Returns the individual contributions to the first and second derivative
matrix, at each unique event time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coxph.detail(object, riskmat=FALSE, rorder=c("data", "time"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coxph.detail_+3A_object">object</code></td>
<td>

<p>a Cox model object, i.e., the result of <code>coxph</code>.
</p>
</td></tr>
<tr><td><code id="coxph.detail_+3A_riskmat">riskmat</code></td>
<td>

<p>include the at-risk indicator matrix in the output?
</p>
</td></tr>
<tr><td><code id="coxph.detail_+3A_rorder">rorder</code></td>
<td>
<p>should the rows of <code>x</code>, <code>y</code> and <code>riskmat</code>
be returned in the original data order, or sorted by time within strata.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function may be useful for those who wish to investigate new methods or
extensions to the Cox model.  The example below shows one way to calculate
the Schoenfeld residuals.
</p>


<h3>Value</h3>

<p>a list with components
</p>
<table>
<tr><td><code>time</code></td>
<td>

<p>the vector of unique event times
</p>
</td></tr>
<tr><td><code>nevent</code></td>
<td>

<p>the number of events at each of these time points.
</p>
</td></tr>
<tr><td><code>means</code></td>
<td>

<p>a matrix with one row for each event time and one column for each variable
in the Cox model, containing the weighted mean of the variable at that time,
over all subjects still at risk at that time.  The weights are the risk
weights <code>exp(x %*% fit$coef)</code>.
</p>
</td></tr>
<tr><td><code>nrisk</code></td>
<td>

<p>number of subjects at risk.
</p>
</td></tr>
<tr><td><code>score</code></td>
<td>

<p>the contribution to the score vector (first derivative of the log
partial likelihood) at each time point.
</p>
</td></tr>
<tr><td><code>imat</code></td>
<td>

<p>the contribution to the information matrix (second derivative of the
log partial likelihood) at each time point.
</p>
</td></tr>
<tr><td><code>hazard</code></td>
<td>

<p>the hazard increment.  Note that the hazard and variance of the
hazard are always for some particular future subject.  This routine
uses <code>object$means</code> as the future subject.
</p>
</td></tr>
<tr><td><code>varhaz</code></td>
<td>

<p>the variance of the hazard increment.
</p>
</td></tr>
<tr><td><code>x</code>, <code>y</code></td>
<td>

<p>copies of the input data.
</p>
</td></tr>
<tr><td><code>strata</code></td>
<td>

<p>only present for a stratified Cox model, this is
a table giving the number of time points of component <code>time</code> that
were contributed by each of the strata.
</p>
</td></tr>
<tr><td><code>wtrisk</code></td>
<td>
<p>the weighted number at risk</p>
</td></tr>
<tr><td><code>riskmat</code></td>
<td>

<p>a matrix with one row for each observation and one colum for each
unique event time,
containing a 0/1 value to indicate whether that observation was (1) or
was not (0) at risk at the given time point.  Rows are in the order
of the original data (after removal of any missings by
<code>coxph</code>), or in time order.
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>, <code><a href="#topic+residuals.coxph">residuals.coxph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit   &lt;- coxph(Surv(futime,fustat) ~ age + rx + ecog.ps, ovarian, x=TRUE)
fitd  &lt;- coxph.detail(fit)
#  There is one Schoenfeld residual for each unique death.  It is a
# vector (covariates for the subject who died) - (weighted mean covariate
# vector at that time).  The weighted mean is defined over the subjects
# still at risk, with exp(X beta) as the weight.

events &lt;- fit$y[,2]==1
etime  &lt;- fit$y[events,1]   #the event times --- may have duplicates
indx   &lt;- match(etime, fitd$time)
schoen &lt;- fit$x[events,] - fitd$means[indx,]
</code></pre>

<hr>
<h2 id='coxph.object'>
Proportional Hazards Regression Object 
</h2><span id='topic+coxph.object'></span><span id='topic+extractAIC.coxph.penal'></span><span id='topic+print.coxph'></span>

<h3>Description</h3>

<p>This class of objects is returned by the <code>coxph</code> class of functions 
to represent a fitted proportional hazards model. 
Objects of this class have methods for the functions <code>print</code>, 
<code>summary</code>, <code>residuals</code>, <code>predict</code> and <code>survfit</code>. 
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="coxph.object_+3A_coefficients">coefficients</code></td>
<td>

<p>the vector of coefficients.
If the model is over-determined there will be missing 
values in the vector corresponding to the redundant columns in the model 
matrix. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_var">var</code></td>
<td>

<p>the variance matrix of the coefficients.  Rows and columns corresponding to 
any missing coefficients are set to zero. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_naive.var">naive.var</code></td>
<td>

<p>this component will be present only if the <code>robust</code> option was true.  If so, 
the <code>var</code> component will contain the robust estimate of variance, and this 
component will contain the ordinary estimate. (A far better name would
be <code>asymp.var</code> since it contains the model-based asympotitic
variance estimate, which is not necessarily &quot;naive&quot;; but that ship has sailed.)
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_loglik">loglik</code></td>
<td>

<p>a vector of length 2 containing the log-likelihood with the initial values and 
with the final values of the coefficients. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_score">score</code></td>
<td>

<p>value of the efficient score test, at the initial value of the coefficients. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_rscore">rscore</code></td>
<td>

<p>the robust log-rank statistic, if a robust variance was requested. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_wald.test">wald.test</code></td>
<td>

<p>the Wald test of whether the final coefficients differ from the initial values. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_iter">iter</code></td>
<td>

<p>number of iterations used. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_linear.predictors">linear.predictors</code></td>
<td>

<p>the vector of linear predictors, one per subject.  Note that this
vector has been centered, see <code>predict.coxph</code> for more details.
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_residuals">residuals</code></td>
<td>

<p>the martingale residuals. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_means">means</code></td>
<td>

<p>vector of values used as the reference for each covariate.
For instance, a later call to <code>predict(fit, type='risk')</code> will
give the hazard ratio between an observation and this reference.
(For most covariates this will contain the mean.)
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_n">n</code></td>
<td>

<p>the number of observations used in the fit. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_nevent">nevent</code></td>
<td>

<p>the number of events (usually deaths) used in the fit. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_n.id">n.id</code></td>
<td>
<p>if the call had an <code>id</code> argument, the number of unique
id values</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_concordance">concordance</code></td>
<td>
<p>a vector of length 6, containing the number of pairs
that are concordant, discordant, tied on x, tied on y, and tied on both,
followed by the standard error of the concordance statistic.</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_first">first</code></td>
<td>
<p>the first derivative vector at the solution.</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_weights">weights</code></td>
<td>

<p>the vector of case weights, if one was used. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_method">method</code></td>
<td>

<p>the method used for handling tied survival times.
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_na.action">na.action</code></td>
<td>

<p>the na.action attribute, if any, that was returned by the <code>na.action</code> 
routine. 
</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_timefix">timefix</code></td>
<td>
<p>the value of the timefix option used in the fit</p>
</td></tr>
<tr><td><code id="coxph.object_+3A_...">...</code></td>
<td>

<p>The object will also contain the following, for documentation see the <code>lm</code> 
object: <code>terms</code>, <code>assign</code>, <code>formula</code>, <code>call</code>, and, optionally, <code>x</code>, <code>y</code>, 
and/or <code>frame</code>. 
</p>
</td></tr>
</table>


<h3>Components</h3>

<p>The following components must be included in a legitimate <code>coxph</code> 
object. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>,  <code><a href="#topic+coxph.detail">coxph.detail</a></code>,  <code><a href="#topic+cox.zph">cox.zph</a></code>,  <code><a href="#topic+residuals.coxph">residuals.coxph</a></code>,  <code><a href="#topic+survfit">survfit</a></code>,  <code><a href="#topic+survreg">survreg</a></code>.   
</p>

<hr>
<h2 id='coxph.wtest'>Compute a quadratic form</h2><span id='topic+coxph.wtest'></span>

<h3>Description</h3>

<p>This function is used internally by several survival routines.  It
computes a simple quadratic form, while properly dealing with missings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coxph.wtest(var, b, toler.chol = 1e-09)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coxph.wtest_+3A_var">var</code></td>
<td>
<p>variance matrix</p>
</td></tr>
<tr><td><code id="coxph.wtest_+3A_b">b</code></td>
<td>
<p>vector</p>
</td></tr>
<tr><td><code id="coxph.wtest_+3A_toler.chol">toler.chol</code></td>
<td>
<p>tolerance for the internal cholesky decomposition</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Compute b' V-inverse b.  Equivalent to sum(b * solve(V,b)), except for
the case of redundant covariates in the original model, which lead to
NA values in V and b.
</p>


<h3>Value</h3>

<p>a real number</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>

<hr>
<h2 id='coxphms.object'>
Multi-state Proportional Hazards Regression Object 
</h2><span id='topic+coxphms.object'></span>

<h3>Description</h3>

<p>This class of objects is returned by the <code>coxph</code> class of functions 
to represent a fitted hazards model, when the model has
multiple states.  The object inherits from the <code>coxph</code> class.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="coxphms.object_+3A_states">states</code></td>
<td>
<p>a character vector listing the states in the model</p>
</td></tr>
<tr><td><code id="coxphms.object_+3A_cmap">cmap</code></td>
<td>
<p>the coefficient map. A matrix containing
a column for each transition and a row for each coefficient, the value
maps that transition/coefficient pair to a position in the coefficient
vector.
If a particular covariate is not used by a transition the matrix
will contain a zero in that position, if two transitions share a
coefficient the matrix will contain repeats.</p>
</td></tr>
<tr><td><code id="coxphms.object_+3A_smap">smap</code></td>
<td>
<p>the stratum map.
The row labeled &lsquo;(Baseline)&rsquo; identifies transitions that do or do not
share a baseline hazard.  Further rows correspond to strata() terms
in the model, each of which may apply to some transitions and not others.
</p>
</td></tr>
<tr><td><code id="coxphms.object_+3A_rmap">rmap</code></td>
<td>
<p>mapping for the residuals and linear predictors.  A two
column matrix with one row for each element of the vectors
and two columns, the first contains the data row and the second the
transition.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In a multi-state model a set of intermediate observations is created
during the computation, with a separate set of data rows for each
transition.  An observation (id and time interval) that is at risk for
more than one transition will for instance have a linear predictor and
residual for each of the potential transitions.  As a result the vector
of linear predictors will be longer than the number of observations.
The <code>rmap</code> matrix shows the mapping.
</p>


<h3>Components</h3>

<p>The object has all the components of a <code>coxph</code> object, with the
following additions and variations.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>,  <code><a href="#topic+coxph.object">coxph.object</a></code>
</p>

<hr>
<h2 id='coxsurv.fit'>
A direct interface to the &lsquo;computational engine&rsquo; of survfit.coxph
</h2><span id='topic+coxsurv.fit'></span>

<h3>Description</h3>

<p>This program is mainly supplied to allow other packages to invoke the
survfit.coxph function at a &lsquo;data&rsquo; level rather than a &lsquo;user&rsquo; level.
It does no checks on the input data that is provided, which can lead
to unexpected errors if that data is wrong.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coxsurv.fit(ctype, stype, se.fit, varmat, cluster, 
            y, x, wt, risk, position, strata, oldid,
            y2, x2, risk2, strata2, id2, unlist=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coxsurv.fit_+3A_stype">stype</code></td>
<td>
<p>survival curve computation: 1=direct, 2=exp(-cumulative
hazard)</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_ctype">ctype</code></td>
<td>
<p>cumulative hazard computation: 1=Breslow, 2=Efron</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_se.fit">se.fit</code></td>
<td>
<p>if TRUE, compute standard errors</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_varmat">varmat</code></td>
<td>
<p>the variance matrix of the coefficients
</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_cluster">cluster</code></td>
<td>
<p>vector to control robust variance</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_y">y</code></td>
<td>
<p>the response variable used in the Cox model.  (Missing values
removed of course.)
</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_x">x</code></td>
<td>
<p>covariate matrix used in the Cox model
</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_wt">wt</code></td>
<td>
<p>weight vector for the Cox model. If the model was unweighted
use a vector of 1s.
</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_risk">risk</code></td>
<td>
<p>the risk score exp(X beta + offset) from the fitted Cox model.</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_position">position</code></td>
<td>
<p>optional argument controlling what is counted as
'censored'.  Due to time dependent covariates, for instance, a
subject might have start, stop times of (1,5)(5,30)(30,100).  Times
5 and 30 are not 'real' censorings.  Position is 1 for a real start,
2 for an actual end, 3 for both, 0 for neither.</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_strata">strata</code></td>
<td>
<p>strata variable used in the Cox model. This will be a
factor.</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_oldid">oldid</code></td>
<td>
<p>identifier for subjects with multiple rows in the
original data.</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_y2">y2</code>, <code id="coxsurv.fit_+3A_x2">x2</code>, <code id="coxsurv.fit_+3A_risk2">risk2</code>, <code id="coxsurv.fit_+3A_strata2">strata2</code></td>
<td>
<p>variables for the hypothetical subjects,
for which prediction is desired</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_id2">id2</code></td>
<td>
<p>optional; if present and not NULL this should be
a vector of identifiers of length <code>nrow(x2)</code>.
A non-null value signifies that <code>x2</code> contains time dependent
covariates, in which case this identifies which rows of <code>x2</code> go
with each subject.
</p>
</td></tr>
<tr><td><code id="coxsurv.fit_+3A_unlist">unlist</code></td>
<td>
<p>if <code>FALSE</code> the result will be a list with one
element for each strata.  Otherwise the strata are &ldquo;unpacked&rdquo; into
the form found in a <code>survfit</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing nearly all the components of a <code>survfit</code>
object.  All that is missing is to add the confidence intervals, the
type of the original model's response (as in a coxph object), and the
class.
</p>


<h3>Note</h3>

<p>The source code for for both this function and
<code>survfit.coxph</code> is written using noweb.  For complete
documentation see the <code>inst/sourcecode.pdf</code> file.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit.coxph">survfit.coxph</a></code>
</p>

<hr>
<h2 id='diabetic'>Ddiabetic retinopathy</h2><span id='topic+diabetic'></span>

<h3>Description</h3>

<p>Partial results from a trial of laser coagulation for the treatment
of diabetic retinopathy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diabetic
data(diabetic, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 394 observations on the following 8 variables.
</p>

<dl>
<dt><code>id</code></dt><dd><p>subject id</p>
</dd>
<dt><code>laser</code></dt><dd><p>laser type: <code>xenon</code> or <code>argon</code></p>
</dd>
<dt><code>age</code></dt><dd><p>age at diagnosis</p>
</dd>
<dt><code>eye</code></dt><dd><p>a factor with levels of <code>left</code> <code>right</code></p>
</dd>
<dt><code>trt</code></dt><dd><p>treatment: 0 = no treatment, 1= laser</p>
</dd>
<dt><code>risk</code></dt><dd><p>risk group of 6-12</p>
</dd>
<dt><code>time</code></dt><dd><p>time to event or last follow-up</p>
</dd>
<dt><code>status</code></dt><dd><p>status of 0= censored or 1 = visual loss</p>
</dd>
</dl>



<h3>Details</h3>

<p>The 197 patients in this dataset were a 50% random sample of the
patients with &quot;high-risk&quot; diabetic retinopathy as defined by the
Diabetic Retinopathy Study (DRS).  Each patient had one eye randomized
to laser treatment and the other eye received no treatment.  For each
eye, the event of interest was the time from initiation of treatment
to the time when visual acuity dropped below 5/200 two visits in a row.
Thus there is a built-in lag time of
approximately 6 months (visits were every 3 months).  Survival times
in this dataset are therefore the actual time to blindness in months,
minus the minimum possible time to event (6.5 months).  Censoring was
caused by death, dropout, or end of the study.
</p>


<h3>References</h3>

<p>Huster, Brookmeyer and Self, Biometrics, 1989.
</p>
<p>American Journal of Ophthalmology, 1976, 81:4, pp 383-396
</p>


<h3>Examples</h3>

<pre><code class='language-R'># juvenile diabetes is defined as and age less than 20
juvenile &lt;- 1*(diabetic$age &lt; 20)
coxph(Surv(time, status) ~ trt + juvenile, cluster= id,
            data= diabetic)
</code></pre>

<hr>
<h2 id='dsurvreg'>
Distributions available in survreg.
</h2><span id='topic+dsurvreg'></span><span id='topic+psurvreg'></span><span id='topic+qsurvreg'></span><span id='topic+rsurvreg'></span>

<h3>Description</h3>

<p>Density, cumulative distribution function, quantile function and random
generation for the set of distributions
supported by the <code>survreg</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dsurvreg(x, mean, scale=1, distribution='weibull', parms)
psurvreg(q, mean, scale=1, distribution='weibull', parms)
qsurvreg(p, mean, scale=1, distribution='weibull', parms)
rsurvreg(n, mean, scale=1, distribution='weibull', parms)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dsurvreg_+3A_x">x</code></td>
<td>

<p>vector of quantiles. 
Missing values (<code>NA</code>s) are allowed. 
</p>
</td></tr>
<tr><td><code id="dsurvreg_+3A_q">q</code></td>
<td>

<p>vector of quantiles. 
Missing values (<code>NA</code>s) are allowed. 
</p>
</td></tr>
<tr><td><code id="dsurvreg_+3A_p">p</code></td>
<td>

<p>vector of probabilities. 
Missing values (<code>NA</code>s) are allowed. 
</p>
</td></tr>
<tr><td><code id="dsurvreg_+3A_n">n</code></td>
<td>
<p>number of random deviates to produce</p>
</td></tr>
<tr><td><code id="dsurvreg_+3A_mean">mean</code></td>
<td>
<p>vector of location (linear predictor) parameters for the model.  
This is replicated to be the same length as <code>p</code>, <code>q</code>
or <code>n</code>.  
</p>
</td></tr>
<tr><td><code id="dsurvreg_+3A_scale">scale</code></td>
<td>

<p>vector of (positive) scale factors.
This is replicated to be the same length as <code>p</code>,
<code>q</code> or <code>n</code>.
</p>
</td></tr>
<tr><td><code id="dsurvreg_+3A_distribution">distribution</code></td>
<td>

<p>character string giving the name of the distribution.  This must be one
of the elements of <code>survreg.distributions</code>
</p>
</td></tr>
<tr><td><code id="dsurvreg_+3A_parms">parms</code></td>
<td>

<p>optional parameters, if any, of the distribution.  For the t-distribution
this is the degrees of freedom.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Elements of <code>q</code> or 
<code>p</code> that are missing will cause the corresponding 
elements of the result to be missing. 
</p>
<p>The <code>location</code> and <code>scale</code>
values are as they would be for <code>survreg</code>.
The label &quot;mean&quot; was an unfortunate choice (made in mimicry of qnorm);
a more correct label would be &quot;linear predictor&quot;.  
Since almost none of these distributions are symmetric the location
parameter is not actually a mean.
</p>
<p>The <code>survreg</code> routines use the parameterization found in chapter
2 of Kalbfleisch and Prentice. 
Translation to the usual parameterization found in a textbook is not
always obvious.
For example, the Weibull distribution has cumulative distribution
function
<code class="reqn">F(t) = 1 - e^{-(\lambda t)^p}</code>.
The actual fit uses the fact that <code class="reqn">\log(t)</code> has an extreme
value distribution, with location and scale of
<code class="reqn">\alpha, \sigma</code>, which are the location and scale parameters
reported by the <code>survreg</code> function.
The parameters are related by <code class="reqn">\sigma= 1/p</code> and
<code class="reqn">\alpha = -\log(\lambda</code>.
The <code>stats::dweibull</code> routine is parameterized in terms of
shape and scale parameters which correspond to <code class="reqn">p</code> and
<code class="reqn">1/\lambda</code> in the K and P notation.
Combining these we see that shape = <code class="reqn">1/\sigma</code> and
scale = <code class="reqn">\exp{alpha}</code>.
</p>


<h3>Value</h3>

<p>density (<code>dsurvreg</code>), 
probability (<code>psurvreg</code>), 
quantile (<code>qsurvreg</code>), or 
for the requested distribution with mean and scale
parameters <code>mean</code> and 
<code>sd</code>. 
</p>


<h3>References</h3>

<p>Kalbfleisch, J. D. and Prentice, R. L. (1970).
<em>The Statistical Analysis of Failure Time Data</em>
Wiley, New York. 
</p>


<h3>References</h3>

<p>Kalbfleisch, J. D. and Prentice, R. L., The statistical analysis of
failure time data, Wiley, 2002.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survreg">survreg</a></code>, 
<code><a href="stats.html#topic+Normal">Normal</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># List of distributions available
names(survreg.distributions)
## Not run: 
 [1] "extreme"     "logistic"    "gaussian"    "weibull"     "exponential"
 [6] "rayleigh"    "loggaussian" "lognormal"   "loglogistic" "t"          

## End(Not run)
# Compare results
all.equal(dsurvreg(1:10, 2, 5, dist='lognormal'), dlnorm(1:10, 2, 5))

# Hazard function for a Weibull distribution
x   &lt;- seq(.1, 3, length=30)
haz &lt;- dsurvreg(x, 2, 3)/ (1-psurvreg(x, 2, 3))
## Not run: 
plot(x, haz, log='xy', ylab="Hazard") #line with slope (1/scale -1)

## End(Not run)

# Estimated CDF of a simple Weibull
fit &lt;- survreg(Surv(time, status) ~ 1, data=lung)
pp &lt;- 1:99/100  
q1 &lt;- qsurvreg(pp, coef(fit), fit$scale)
q2 &lt;- qweibull(pp, shape= 1/fit$scale, scale= exp(coef(fit)))
all.equal(q1, q2)
## Not run: 
plot(q1, pp, type='l', xlab="Months", ylab="CDF")

## End(Not run)
# per the help page for dweibull, the mean is scale * gamma(1 + 1/shape)
c(mean = exp(coef(fit))* gamma(1 + fit$scale))

</code></pre>

<hr>
<h2 id='finegray'>Create data for a Fine-Gray model</h2><span id='topic+finegray'></span>

<h3>Description</h3>

<p>The Fine-Gray model can be fit by first creating a special data set,
and then fitting a weighted Cox model to the result.  This routine creates the
data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>finegray(formula, data, weights, subset, na.action= na.pass, etype,
    prefix="fg", count, id, timefix=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="finegray_+3A_formula">formula</code></td>
<td>
<p>a standard model formula, with survival on the left and
covariates on the right.  
</p>
</td></tr>
<tr><td><code id="finegray_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by as.data.frame to a data frame) containing the variables
in the model.
</p>
</td></tr>
<tr><td><code id="finegray_+3A_weights">weights</code></td>
<td>
<p>optional vector of observation weights</p>
</td></tr>
<tr><td><code id="finegray_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in
the fitting process.
</p>
</td></tr>
<tr><td><code id="finegray_+3A_na.action">na.action</code></td>
<td>

<p>a function which indicates what should happen when the data contain
NAs.  The default is set by the na.action setting of options.
</p>
</td></tr>
<tr><td><code id="finegray_+3A_etype">etype</code></td>
<td>

<p>the event type for which a data set will be generated.  The default is
to use whichever is listed first in the multi-state survival object.
</p>
</td></tr>
<tr><td><code id="finegray_+3A_prefix">prefix</code></td>
<td>
<p>the routine will add 4 variables to the data set: a start
and end time for each interval, status, and a weight for the
interval. The default names of these are &quot;fgstart&quot;, &quot;fgstop&quot;, &quot;fgstatus&quot;,
and &quot;fgwt&quot;; the <code>prefix</code> argument determines the initial portion of
the new names.
</p>
</td></tr>
<tr><td><code id="finegray_+3A_count">count</code></td>
<td>
<p>a variable name in the output data set for an optional
variable that will contain
the the replication count for each row of the input data.  If a row is
expanded into multiple lines it will contain 1, 2, etc.
</p>
</td></tr>
<tr><td><code id="finegray_+3A_id">id</code></td>
<td>
<p>optional, the variable name in the data set which identifies
subjects.</p>
</td></tr>
<tr><td><code id="finegray_+3A_timefix">timefix</code></td>
<td>
<p>process times through the <code>aeqSurv</code> function to
eliminate potential roundoff issues.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function expects a multi-state survival expression or variable as
the left hand side of the formula, e.g. <code>Surv(atime, astat)</code>
where <code>astat</code> is a factor whose first level represents censoring
and remaining levels are states.  The output data set will contain simple
survival data (status = 0 or 1) for a single endpoint of interest.
For exposition
call this endpoint A and lump all others as endpoint B.
In the output data set subjects who experience endpoint B become
censored observations 
whose times are artificially extended to the right, with a
decreasing case weight from interval to interval.  
The output data set will normally contain many more rows than the
input.
</p>
<p>The algorithm allows for delayed entry, and only a limited form of
time-dependent covariates.  That is, when subjects with endpoint B are
extended, those future covariate values stay constant; so there is an
implicit assumption that no more changes would have occurred if
the event had not intervened and follow-up had been longer. 
For predictable time-dependent covariates the final data set could be
further processed to fix this, but this is not included in the
function.  Geskus for example considers an example with different
calendar epochs, corresponding to a change in standard medical
practice for the disese, as a covariate.
dependent covariates.  
If there are time dependent covariates or delayed entry, e.g.., the input data
set had <code>Surv(entry, exit, stat)</code> as the left hand side, then
an <code>id</code> statement is required.  The program does data checks
in this case, and needs to know which rows belong to each subject.
</p>
<p>The output data set will often have gaps. Say that there were events
at time 50 and 100 (and none between) and censoring at 60, 70, and 80.
Formally, a non event subjects at risk from 50 to 100 will have
different weights in each of
the 3 intervals 50-60, 60-70, and 80-100, but because the middle
interval does not span any event times the subsequent Cox model will
never use that row.  The <code>finegray</code> output omits such rows.
</p>
<p>See the competing risks vignette for more details.
</p>


<h3>Value</h3>

<p>a data frame</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>References</h3>

<p>Fine JP and Gray RJ (1999) A proportional hazards model for the
subdistribution of a competing risk. JASA 94:496-509.
</p>
<p>Geskus RB (2011). Cause-Specific Cumulative Incidence Estimation and the
Fine and Gray Model Under Both Left Truncation and Right Censoring.
Biometrics 67, 39-49.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>, <code><a href="#topic+aeqSurv">aeqSurv</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Treat time to death and plasma cell malignancy as competing risks
etime &lt;- with(mgus2, ifelse(pstat==0, futime, ptime))
event &lt;- with(mgus2, ifelse(pstat==0, 2*death, 1))
event &lt;- factor(event, 0:2, labels=c("censor", "pcm", "death"))

# FG model for PCM
pdata &lt;- finegray(Surv(etime, event) ~ ., data=mgus2)
fgfit &lt;- coxph(Surv(fgstart, fgstop, fgstatus) ~ age + sex,
                     weight=fgwt, data=pdata)

# Compute the weights separately by sex
adata &lt;- finegray(Surv(etime, event) ~ . + strata(sex),
             data=mgus2, na.action=na.pass)
</code></pre>

<hr>
<h2 id='flchain'>Assay of serum free light chain for 7874 subjects.</h2><span id='topic+flchain'></span>

<h3>Description</h3>

<p>This is a stratified random sample containing 1/2 of the subjects from
a study of the relationship between serum free light chain (FLC)
and mortality.  The original sample contains samples on
approximately 2/3 of the residents of Olmsted County aged 50 or greater.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flchain
data(flchain, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 7874 persons containing the following variables.
</p>

<dl>
<dt><code>age </code></dt><dd><p>age in years</p>
</dd>
<dt><code>sex</code></dt><dd><p>F=female, M=male</p>
</dd>
<dt><code>sample.yr</code></dt><dd><p>the calendar year in which a blood sample
was obtained</p>
</dd>
<dt><code>kappa</code></dt><dd><p>serum free light chain, kappa portion</p>
</dd>
<dt><code>lambda</code></dt><dd><p>serum free light chain, lambda portion</p>
</dd>
<dt><code>flc.grp</code></dt><dd><p>the FLC group for the subject, as used in the
original analysis</p>
</dd>
<dt><code>creatinine</code></dt><dd><p>serum creatinine</p>
</dd>
<dt><code>mgus</code></dt><dd><p>1 if the subject had been diagnosed with
monoclonal gammapothy (MGUS)</p>
</dd>
<dt><code>futime</code></dt><dd><p>days from enrollment until death.  Note that
there are 3 subjects whose sample was obtained on their death date.</p>
</dd>
<dt><code>death</code></dt><dd><p>0=alive at last contact date, 1=dead</p>
</dd>
<dt><code>chapter</code></dt><dd><p>for those who died, a grouping of their
primary cause of death by chapter headings of the International
Code of Diseases ICD-9</p>
</dd>
</dl>



<h3>Details</h3>

<p>In 1995 Dr. Robert Kyle embarked on a study to determine the
prevalence of monoclonal gammopathy of undetermined significance
(MGUS) in Olmsted County, Minnesota, a condition which is
normally only found by chance from a test (serum electrophoresis)
which is ordered for other causes.  Later work suggested that one
component of immunoglobulin production, the serum free light chain,
might be a possible marker for immune disregulation.  In 2010
Dr. Angela Dispenzieri and colleagues assayed FLC levels on those
samples from the original study for which they had patient permission and from
which sufficient material remained for further testing.  They found
that elevated FLC levels were indeed associated with higher death
rates.
</p>
<p>Patients were recruited when they came to the clinic for other
appointments, with a final random sample of those who had not yet
had a visit since the study began.  An interesting side question is
whether there are differences between early, mid, and late recruits.
</p>
<p>This data set contains an age and sex stratified random sample that
includes 7874 of the original 15759 subjects.  The original subject
identifiers and dates have been removed to protect patient identity.
Subsampling was done to further protect this information.
</p>


<h3>Source</h3>

<p>The primary investigator (A Dispenzieri) and statistician (T
Therneau) for the study.</p>


<h3>References</h3>

<p>A Dispenzieri, J Katzmann, R Kyle, D Larson, T Therneau, C Colby,
R Clark, G Mead, S Kumar, 
LJ Melton III and  SV Rajkumar (2012).
Use of monclonal serum immunoglobulin free light chains to predict 
overall survival in the general population,
Mayo Clinic Proceedings 87:512-523.
</p>
<p>R Kyle, T Therneau, SV Rajkumar,
D Larson, M Plevak, J Offord,
A Dispenzieri, J Katzmann, and LJ Melton, III, 2006,
Prevalence of monoclonal gammopathy of undetermined significance,
New England J Medicine 354:1362-1369.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(flchain)
age.grp &lt;-  cut(flchain$age, c(49,54, 59,64, 69,74,79, 89, 110),
               labels= paste(c(50,55,60,65,70,75,80,90),
                             c(54,59,64,69,74,79,89,109), sep='-'))
table(flchain$sex, age.grp)
</code></pre>

<hr>
<h2 id='frailty'>
Random effects terms
</h2><span id='topic+frailty'></span><span id='topic+frailty.gamma'></span><span id='topic+frailty.gaussian'></span><span id='topic+frailty.t'></span>

<h3>Description</h3>

<p>The frailty function allows one to add a simple random effects term to
a Cox model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>frailty(x, distribution="gamma", ...)
frailty.gamma(x, sparse = (nclass &gt; 5), theta, df, eps = 1e-05,
         method = c("em","aic", "df", "fixed"), ...) 
frailty.gaussian(x, sparse = (nclass &gt; 5), theta, df,
         method =c("reml","aic", "df", "fixed"), ...)
frailty.t(x, sparse = (nclass &gt; 5), theta, df, eps = 1e-05, tdf = 5,
         method = c("aic", "df", "fixed"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="frailty_+3A_x">x</code></td>
<td>

<p>the variable to be entered as a random effect.  
It is always treated as a factor.
</p>
</td></tr>
<tr><td><code id="frailty_+3A_distribution">distribution</code></td>
<td>

<p>either the <code>gamma</code>, 
<code>gaussian</code> or <code>t</code>
distribution may be specified.
The routines <code>frailty.gamma</code>,
<code>frailty.gaussian</code> and 
<code>frailty.t</code> do the actual work.
</p>
</td></tr>
<tr><td><code id="frailty_+3A_...">...</code></td>
<td>
<p>Arguments for specific distribution, including (but not
limited to) </p>
</td></tr>
<tr><td><code id="frailty_+3A_sparse">sparse</code></td>
<td>

<p>cutoff for using a sparse coding of the data matrix.  
If the total number of levels of <code>x</code> is larger
than this value, then a sparse matrix approximation is used.
The correct cutoff is still a matter of exploration: if the number of
levels is very large (thousands) then the non-sparse calculation may not be
feasible in terms of both memory and compute time.
Likewise, the accuracy of the sparse approximation appears to be related to
the maximum proportion of subjects in any one class, being best when no
one class has a large membership.
</p>
</td></tr>
<tr><td><code id="frailty_+3A_theta">theta</code></td>
<td>

<p>if specified, this fixes the variance of the random effect.
If not, the variance is a parameter, and a best solution is sought.
Specifying this implies <code>method='fixed'</code>.
</p>
</td></tr>
<tr><td><code id="frailty_+3A_df">df</code></td>
<td>

<p>if specified, this fixes the degrees of freedom for the random effect.
Specifying this implies <code>method='df'</code>.
Only one of <code>theta</code> or 
<code>df</code> should be specified.
</p>
</td></tr>
<tr><td><code id="frailty_+3A_method">method</code></td>
<td>

<p>the method used to select a solution for theta, the variance of the
random effect.  
The <code>fixed</code> corresponds to a user-specified
value, and no iteration is done.
The <code>df</code> selects the variance such that the
degrees of freedom for the random effect matches a user specified value.
The <code>aic</code> method seeks to 
maximize Akaike's information criteria 
2*(partial likelihood - df).
The <code>em</code> and <code>reml</code>
methods are specific to Cox models with gamma and gaussian random effects,
respectively.
Please see further discussion below.
</p>
</td></tr>
<tr><td><code id="frailty_+3A_tdf">tdf</code></td>
<td>

<p>the degrees of freedom for the t-distribution.
</p>
</td></tr>
<tr><td><code id="frailty_+3A_eps">eps</code></td>
<td>

<p>convergence criteria for the iteration on theta.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>frailty</code> plugs into the general penalized
modeling framework provided by the <code>coxph</code> 
and <code>survreg</code> routines.  
This framework deals with likelihood, penalties, and degrees of freedom;
these aspects work well with either parent routine.
</p>
<p>Therneau, Grambsch, and Pankratz show how maximum likelihood estimation for
the Cox model with a gamma frailty can be accomplished using a general
penalized routine, and Ripatti and Palmgren work through a similar argument
for the Cox model with a gaussian frailty.  Both of these are specific to
the Cox model.  
Use of gamma/ml or gaussian/reml with 
<code>survreg</code> does not lead to valid results. 
</p>
<p>The extensible structure of the penalized methods is such that the penalty
function, such as <code>frailty</code> or
<code>pspine</code>, is completely separate from the modeling
routine.  The strength of this is that a user can plug in any penalization
routine they choose.  A weakness is that it is very difficult for the
modeling routine to know whether a sensible penalty routine has been
supplied.
</p>
<p>Note that use of a frailty term implies a mixed effects model and use of
a cluster term implies a GEE approach; these cannot be mixed.
</p>
<p>The <code>coxme</code> package has superseded
this method.  It is faster, more stable, and more flexible.
</p>


<h3>Value</h3>

<p>this function is used in the model statement of either
<code>coxph</code> or <code>survreg</code>.
It's results are used internally.
</p>


<h3>References</h3>

<p>S Ripatti and J Palmgren, Estimation of multivariate frailty 
models using penalized partial likelihood,
Biometrics, 56:1016-1022, 2000.
</p>
<p>T Therneau, P Grambsch and VS Pankratz,
Penalized survival models and frailty,
J Computational and Graphical Statistics, 12:156-175, 2003.
</p>


<h3>See Also</h3>

<p><a href="#topic+coxph">coxph</a>, <a href="#topic+survreg">survreg</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Random institutional effect
coxph(Surv(time, status) ~ age + frailty(inst, df=4), lung)

# Litter effects for the rats data
rfit2a &lt;- coxph(Surv(time, status) ~ rx +
                  frailty.gaussian(litter, df=13, sparse=FALSE), rats,
                  subset= (sex=='f'))
rfit2b &lt;- coxph(Surv(time, status) ~ rx +
                  frailty.gaussian(litter, df=13, sparse=TRUE), rats,
                  subset= (sex=='f'))
</code></pre>

<hr>
<h2 id='gbsg'>Breast cancer data sets used in Royston and Altman (2013)</h2><span id='topic+gbsg'></span>

<h3>Description</h3>

<p>The <code>gbsg</code> data set contains patient records from a 1984-1989 trial 
conducted by the German Breast Cancer Study Group (GBSG) of 720 patients
with node positive breast cancer; it retains the 686 patients with 
complete data for the prognostic variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbsg
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data set with 686 observations and 11 variables.
</p>

<dl>
<dt><code>pid</code></dt><dd><p>patient identifier</p>
</dd>
<dt><code>age</code></dt><dd><p>age, years</p>
</dd>
<dt><code>meno</code></dt><dd><p>menopausal status (0= premenopausal, 1= postmenopausal)</p>
</dd>
<dt><code>size</code></dt><dd><p>tumor size, mm</p>
</dd>
<dt><code>grade</code></dt><dd><p>tumor grade</p>
</dd>
<dt><code>nodes</code></dt><dd><p>number of positive lymph nodes</p>
</dd>
<dt><code>pgr</code></dt><dd><p>progesterone receptors (fmol/l)</p>
</dd>
<dt><code>er</code></dt><dd><p>estrogen receptors (fmol/l)</p>
</dd>
<dt><code>hormon</code></dt><dd><p>hormonal therapy, 0= no, 1= yes</p>
</dd>
<dt><code>rfstime</code></dt><dd><p>recurrence free survival time; days to first of reccurence, death or last follow-up</p>
</dd>
<dt><code>status</code></dt><dd><p>0= alive without recurrence, 1= recurrence or
death</p>
</dd>
</dl>


<h3>Details</h3>

<p>These data sets are used in the paper by Royston and Altman.
The Rotterdam data is used to create a fitted model, and the GBSG data for 
validation of the model.  The paper gives references for the data source.
</p>


<h3>References</h3>

<p>Patrick Royston and Douglas Altman, External validation of a Cox prognostic
model: principles and methods.  BMC Medical Research Methodology 2013, 13:33
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rotterdam">rotterdam</a></code>
</p>

<hr>
<h2 id='heart'>Stanford Heart Transplant data</h2><span id='topic+jasa1'></span><span id='topic+jasa'></span><span id='topic+heart'></span>

<h3>Description</h3>

<p>Survival of patients on the waiting list for the Stanford
heart transplant program.</p>


<h3>Usage</h3>

<pre><code class='language-R'>heart
data(heart, package="survival")</code></pre>


<h3>Format</h3>

<p>jasa: original data
</p>

<table>
<tr>
 <td style="text-align: left;">
   birth.dt:</td><td style="text-align: left;"> birth date </td>
</tr>
<tr>
 <td style="text-align: left;">
   accept.dt:</td><td style="text-align: left;"> acceptance into program </td>
</tr>
<tr>
 <td style="text-align: left;">
   tx.date:</td><td style="text-align: left;"> transplant date </td>
</tr>
<tr>
 <td style="text-align: left;">
   fu.date:</td><td style="text-align: left;"> end of followup </td>
</tr>
<tr>
 <td style="text-align: left;">
   fustat:</td><td style="text-align: left;"> dead or alive </td>
</tr>
<tr>
 <td style="text-align: left;">
   surgery:</td><td style="text-align: left;"> prior bypass surgery</td>
</tr>
<tr>
 <td style="text-align: left;">
   age: </td><td style="text-align: left;"> age (in years)</td>
</tr>
<tr>
 <td style="text-align: left;">
   futime:</td><td style="text-align: left;"> followup time</td>
</tr>
<tr>
 <td style="text-align: left;">
   wait.time:</td><td style="text-align: left;"> time before transplant</td>
</tr>
<tr>
 <td style="text-align: left;">
   transplant:</td><td style="text-align: left;"> transplant indicator</td>
</tr>
<tr>
 <td style="text-align: left;">
   mismatch:</td><td style="text-align: left;"> mismatch score</td>
</tr>
<tr>
 <td style="text-align: left;">
   hla.a2:</td><td style="text-align: left;"> particular type of mismatch</td>
</tr>
<tr>
 <td style="text-align: left;">
   mscore:</td><td style="text-align: left;"> another mismatch score</td>
</tr>
<tr>
 <td style="text-align: left;">
   reject:</td><td style="text-align: left;"> rejection occurred</td>
</tr>
<tr>
 <td style="text-align: left;">
 </td>
</tr>

</table>

<p>jasa1, heart: processed data
</p>

<table>
<tr>
 <td style="text-align: left;">
    start, stop, event: </td><td style="text-align: left;"> Entry and exit time and status for this interval of time</td>
</tr>
<tr>
 <td style="text-align: left;">
    age:</td><td style="text-align: left;"> age-48 years</td>
</tr>
<tr>
 <td style="text-align: left;">
    year:</td><td style="text-align: left;"> year of acceptance (in years after 1 Nov 1967)</td>
</tr>
<tr>
 <td style="text-align: left;">
    surgery:</td><td style="text-align: left;"> prior bypass surgery 1=yes</td>
</tr>
<tr>
 <td style="text-align: left;">
    transplant: </td><td style="text-align: left;"> received transplant 1=yes</td>
</tr>
<tr>
 <td style="text-align: left;">
    id:</td><td style="text-align: left;"> patient id</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>J Crowley and M Hu (1977),
Covariance analysis of heart transplant survival data.
<em>Journal of the American Statistical Association</em>,
<b>72</b>, 27&ndash;36.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stanford2">stanford2</a></code></p>

<hr>
<h2 id='hoel'>Mouse cancer data</h2><span id='topic+hoel'></span>

<h3>Description</h3>

<p>Days until occurence of cancer for male mice
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("cancer")</code></pre>


<h3>Format</h3>

<p>A data frame with 181 observations on the following 4 variables.
</p>

<dl>
<dt><code>trt</code></dt><dd><p>treatment assignment: <code>Control</code> or <code>Germ-free</code></p>
</dd>
<dt><code>days</code></dt><dd><p>days until death</p>
</dd>
<dt><code>outcome</code></dt><dd><p>outcome: <code>censor</code>, <code>thymic
	lymphoma</code>, <code>reticulum cell sarcoma</code> <code>other causes</code></p>
</dd>
<dt><code>id</code></dt><dd><p>mouse id</p>
</dd>
</dl>



<h3>Details</h3>

<p>Two groups of male mice were given 300 rads of radiation and followed
for cancer incidence.  One group was maintained in a germ free
environment.  The data set is used as an example of competing risks in
Kalbfleisch and Prentice.  The germ-free environment has little effect
on the rate of occurence of thymic lymphoma, but significantly delays
the other causes of death.
</p>


<h3>Note</h3>

<p>The Ontology Search website defines reticulm cell sarcoma as
&quot;An antiquated term that refers to a non-Hodgkin lymphoma composed of
diffuse infiltrates of large, often anaplastic lymphocytes&quot;.
</p>


<h3>Source</h3>

<p>The data can be found in appendix I of Kalbfleisch and Prentice.
</p>


<h3>References</h3>

<p>Hoel, D.G. (1972), A representation of mortality data by competing
risks.  Biometrics 33, 1-30.
Kalbfleisch, J.D. and Prentice, R.L. (1980). The statistical analysis of
failure time data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>hsurv &lt;- survfit(Surv(days, outcome) ~ trt, data = hoel, id= id)
plot(hsurv, lty=1:2, col=rep(1:3, each=2), lwd=2, xscale=30.5,
      xlab="Months", ylab= "Death")
legend("topleft", c("Lymphoma control", "Lymphoma germ free",
                    "Sarcoma control", "Sarcoma germ free",
                    "Other control", "Other germ free"),
       col=rep(1:3, each=2), lty=1:2, lwd=2, bty='n')
hfit &lt;- coxph(Surv(days, outcome) ~ trt, data= hoel, id = id)
</code></pre>

<hr>
<h2 id='is.ratetable'>
Verify that an object is of class ratetable. 
</h2><span id='topic+is.ratetable'></span><span id='topic+Math.ratetable'></span><span id='topic+Ops.ratetable'></span>

<h3>Description</h3>

<p>The function verifies not only the <code>class</code> attribute, but the 
structure of the object. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.ratetable(x, verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.ratetable_+3A_x">x</code></td>
<td>

<p>the object to be verified. 
</p>
</td></tr>
<tr><td><code id="is.ratetable_+3A_verbose">verbose</code></td>
<td>

<p>if <code>TRUE</code> and the object is not a ratetable, 
then return a character string describing the way(s) in which <code>x</code> 
fails to be a proper ratetable object. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Rate tables are used by the <code>pyears</code> and <code>survexp</code> functions, and normally 
contain death rates for some population, categorized by age, sex, or other 
variables.  They have a fairly rigid structure, and the <code>verbose</code> option 
can help in creating a new rate table. 
</p>


<h3>Value</h3>

<p>returns <code>TRUE</code> if <code>x</code> is a ratetable, and <code>FALSE</code> or a description if it is not. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pyears">pyears</a></code>,  <code><a href="#topic+survexp">survexp</a></code>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>is.ratetable(survexp.us)  # True
is.ratetable(lung)        # False
</code></pre>

<hr>
<h2 id='kidney'>Kidney catheter data</h2><span id='topic+kidney'></span>

<h3>Description</h3>

<p>Data on the recurrence times to infection, at the point of insertion of
the catheter, for kidney patients using portable dialysis equipment.
Catheters may be removed for reasons other than infection, in which case
the observation is censored.  Each patient has exactly 2 observations.
</p>
<p>This data has often been used to illustrate the use of random effects
(frailty) in a survival model.  However, one of the males (id 21) is a
large outlier, with much longer survival than his peers.  If this
observation is removed no evidence remains for a random subject effect.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kidney
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
patient:</td><td style="text-align: left;"> id</td>
</tr>
<tr>
 <td style="text-align: left;">
time:</td><td style="text-align: left;"> time</td>
</tr>
<tr>
 <td style="text-align: left;">
status:</td><td style="text-align: left;"> event status</td>
</tr>
<tr>
 <td style="text-align: left;">
age:</td><td style="text-align: left;"> in years</td>
</tr>
<tr>
 <td style="text-align: left;">
sex:</td><td style="text-align: left;"> 1=male, 2=female</td>
</tr>
<tr>
 <td style="text-align: left;">
disease:</td><td style="text-align: left;">  disease type (0=GN, 1=AN, 2=PKD, 3=Other)</td>
</tr>
<tr>
 <td style="text-align: left;">
frail:</td><td style="text-align: left;"> frailty estimate from original paper</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<h3>Note</h3>

<p>The original paper ignored the issue of tied times and so is not
exactly reproduced by the survival package.
</p>


<h3>Source</h3>

<p>CA McGilchrist, CW Aisbett (1991),
Regression with frailty in survival analysis. 
<em>Biometrics</em> <b>47</b>, 461&ndash;66.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kfit &lt;- coxph(Surv(time, status)~ age + sex + disease + frailty(id), kidney)
kfit0 &lt;- coxph(Surv(time, status)~ age + sex + disease, kidney)
kfitm1 &lt;- coxph(Surv(time,status) ~ age + sex + disease + 
		frailty(id, dist='gauss'), kidney)
</code></pre>

<hr>
<h2 id='levels.Surv'>Return the states of a multi-state Surv object
</h2><span id='topic+levels.Surv'></span>

<h3>Description</h3>

<p>For a multi-state <code>Surv</code> object, this will return the names
of the states.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Surv'
levels(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="levels.Surv_+3A_x">x</code></td>
<td>
<p>a <code>Surv</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>for a multi-state <code>Surv</code> object, the vector of state names
(excluding censoring); or NULL for an ordinary <code>Surv</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y1 &lt;- Surv(c(1,5, 9, 17,21, 30),
           factor(c(0, 1, 2,1,0,2), 0:2, c("censored", "progression", "death")))
levels(y1)

y2 &lt;- Surv(1:6, rep(0:1, 3))
y2
levels(y2)
</code></pre>

<hr>
<h2 id='lines.survfit'>
Add Lines or Points to a Survival Plot 
</h2><span id='topic+lines.survfit'></span><span id='topic+points.survfit'></span><span id='topic+lines.survexp'></span>

<h3>Description</h3>

<p>Often used to add the expected survival curve(s) to a Kaplan-Meier plot 
generated with <code>plot.survfit</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survfit'
lines(x, type="s", pch=3, col=1, lty=1,
        lwd=1, cex=1, mark.time=FALSE, xmax,
        fun, conf.int=FALSE,
        conf.times, conf.cap=.005, conf.offset=.012,
        conf.type = c("log", "log-log", "plain", "logit", "arcsin"),
        mark, noplot="(s0)", cumhaz= FALSE,  ...)
## S3 method for class 'survexp'
lines(x, type="l", ...)
## S3 method for class 'survfit'
points(x, fun, censor=FALSE, col=1, pch,
        noplot="(s0)", cumhaz=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lines.survfit_+3A_x">x</code></td>
<td>

<p>a survival object, generated from the <code>survfit</code> or <code>survexp</code> functions. 
</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_type">type</code></td>
<td>

<p>the line type, as described in <code>lines</code>.  The default is a step function 
for <code>survfit</code> objects, and a connected line for <code>survexp</code>
objects.
All other arguments for <code>lines.survexp</code> are identical to those
for <code>lines.survfit</code>.
</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_col">col</code>, <code id="lines.survfit_+3A_lty">lty</code>, <code id="lines.survfit_+3A_lwd">lwd</code>, <code id="lines.survfit_+3A_cex">cex</code></td>
<td>

<p>vectors giving the mark symbol, color, line type, line width and
character size for the added curves.  Of this set only color is
applicable to <code>points</code>.
</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_pch">pch</code></td>
<td>
<p>plotting characters for points,  in the style of
<code>matplot</code>, i.e., either a single string of characters of which
the first will be used for the first curve, etc; or a vector
of characters or integers, one element per curve.
</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_mark">mark</code></td>
<td>
<p>a historical alias for <code>pch</code></p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_censor">censor</code></td>
<td>
<p>should censoring times be displayed for the <code>points</code>
function?
</p>
</td></tr>  
<tr><td><code id="lines.survfit_+3A_mark.time">mark.time</code></td>
<td>

<p>controls the labeling of the curves.   
If <code>FALSE</code>, no labeling is done.   
If <code>TRUE</code>, then curves are marked at each censoring time.   
If <code>mark.time</code> is a numeric vector, then curves are marked at  
the specified time points. 
</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_xmax">xmax</code></td>
<td>
<p>optional cutoff for the right hand of the curves.</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_fun">fun</code></td>
<td>

<p>an arbitrary function defining a transformation of the survival curve. 
For example <code>fun=log</code> is an alternative way to draw a log-survival curve 
(but with the axis labeled with log(S) values). 
Four often used transformations can be specified with a character 
argument instead: &quot;log&quot; is the same as using the <code>log=T</code> option, 
&quot;event&quot; plots cumulative events (f(y) = 1-y), 
&quot;cumhaz&quot; plots the cumulative hazard function (f(y) = -log(y)) 
and &quot;cloglog&quot; creates a complimentary log-log survival plot  
(f(y) = log(-log(y))) along with log scale for the x-axis. 
</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_conf.int">conf.int</code></td>
<td>

<p>if <code>TRUE</code>, confidence bands for the curves are also plotted. 
If set to <code>"only"</code>, then only the CI bands are plotted, and the curve 
itself is left off.   
This can be useful for fine control over the colors or line types of a
plot.
A numeric value, e.g. <code>conf.int = .90</code>, can be used to 
</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_conf.times">conf.times</code></td>
<td>
<p>optional vector of times at which to place a
confidence bar on the curve(s).  If present, these will be used
instead of confidence bands.</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_conf.cap">conf.cap</code></td>
<td>
<p>width of the horizontal cap on top of the confidence
bars; only used if conf.times is used.  A value of 1 is the width of
the plot region.</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_conf.offset">conf.offset</code></td>
<td>
<p>the offset for confidence bars, when there are
multiple curves on the plot.  A value of 1 is the width of the plot
region. If this is a single number then each curve's bars are offset
by this amount from the prior curve's bars, if it is a vector the values are
used directly.</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_conf.type">conf.type</code></td>
<td>

<p>One of <code>"plain"</code>, <code>"log"</code> (the default),
<code>"log-log"</code>, <code>"logit"</code>, or <code>"none"</code>.  Only
enough of the string to uniquely identify it is necessary.
The first option causes confidence intervals not to be
generated.  The second causes the standard intervals
<code>curve +- k *se(curve)</code>, where k is determined from
<code>conf.int</code>.  The log option calculates intervals based on the
cumulative hazard or log(survival). The log-log option bases the
intervals on the log hazard or log(-log(survival)), and the
logit option on log(survival/(1-survival)). 
</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_noplot">noplot</code></td>
<td>
<p>for multi-state models, curves with this label will not
be plotted.  The default corresponds to an unspecified state.</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_cumhaz">cumhaz</code></td>
<td>
<p>plot the cumulative hazard, rather than the survival or
probability in state.</p>
</td></tr>
<tr><td><code id="lines.survfit_+3A_...">...</code></td>
<td>
<p>other graphical parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When the <code>survfit</code> function creates a multi-state survival curve
the resulting object has class &lsquo;survfitms&rsquo;.  The only difference in
the plots is that that it defaults to a curve that goes from lower
left to upper right (starting at 0), where survival curves default
to starting at 1 and going down.  All other options are identical.
</p>
<p>If the user set an explicit range in an earlier <code>plot.survfit</code>
call, e.g. via <code>xlim</code> or <code>xmax</code>, subsequent calls to
this function remember the right hand cutoff. This memory can be
erased by <code>options(plot.survfit) &lt;- NULL</code>. 
</p>


<h3>Value</h3>

<p>a list with components <code>x</code> and <code>y</code>, containing the coordinates of the 
last point on each of the curves (but not of the confidence limits). 
This may be useful for labeling. 
</p>


<h3>Side Effects</h3>

<p>one or more curves are added to the current plot. 
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+lines">lines</a></code>,  <code><a href="graphics.html#topic+par">par</a></code>,  <code><a href="#topic+plot.survfit">plot.survfit</a></code>,  <code><a href="#topic+survfit">survfit</a></code>,  <code><a href="#topic+survexp">survexp</a></code>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- survfit(Surv(time, status==2) ~ sex, pbc,subset=1:312)
plot(fit, mark.time=FALSE, xscale=365.25,
        xlab='Years', ylab='Survival')
lines(fit[1], lwd=2)    #darken the first curve and add marks


# Add expected survival curves for the two groups,
#   based on the US census data
# The data set does not have entry date, use the midpoint of the study
efit &lt;- survexp(~sex, data=pbc, times= (0:24)*182, ratetable=survexp.us, 
                 rmap=list(sex=sex, age=age*365.35, year=as.Date('1979/01/01')))
temp &lt;- lines(efit, lty=2, lwd=2:1)
text(temp, c("Male", "Female"), adj= -.1) #labels just past the ends
title(main="Primary Biliary Cirrhosis, Observed and Expected")

</code></pre>

<hr>
<h2 id='logan'>Data from the 1972-78 GSS data used by Logan</h2><span id='topic+logan'></span>

<h3>Description</h3>

<p>Intergenerational occupational mobility data with covariates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logan
data(logan, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 838 observations on the following 4 variables.
</p>

<dl>
<dt>occupation</dt><dd><p>subject's occupation, a factor with levels
<code>farm</code>, <code>operatives</code>, <code>craftsmen</code>, <code>sales</code>,
and <code>professional</code></p>
</dd>
<dt>focc</dt><dd><p>father's occupation</p>
</dd>
<dt>education</dt><dd><p>total years of schooling, 0 to 20</p>
</dd>
<dt>race</dt><dd><p>levels of <code>non-black</code> and <code>black</code></p>
</dd>
</dl>



<h3>Source</h3>

<p>General Social Survey data, see the web site for detailed information
on the variables.
<a href="https://gss.norc.org/">https://gss.norc.org/</a>.
</p>


<h3>References</h3>

<p>Logan, John A. (1983). A Multivariate Model for Mobility Tables.
<cite>American Journal of Sociology</cite> 89: 324-349.</p>

<hr>
<h2 id='logLik.coxph'>logLik method for a Cox model</h2><span id='topic+logLik.coxph'></span><span id='topic+logLik.survreg'></span>

<h3>Description</h3>

<p>The logLik function for survival models</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coxph'
logLik(object, ...)
## S3 method for class 'survreg'
logLik(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logLik.coxph_+3A_object">object</code></td>
<td>
<p>the result of a <code>coxph</code> or <code>survreg</code> fit</p>
</td></tr>
<tr><td><code id="logLik.coxph_+3A_...">...</code></td>
<td>
<p>optional arguments for other instances of the method</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The logLik function is used by summary functions in R such as
<code>AIC</code>.
For a Cox model, this method returns the partial likelihood.
The number of degrees of freedom (df) used by the fit and the effective
number of observations (nobs) are added as attributes.
Per Raftery and others, the effective number of observations is the
taken to be the number of events in the data set.
</p>
<p>For a <code>survreg</code> model the proper value for the effective number
of observations is still an open question (at least to this author).
For right censored data the approach of <code>logLik.coxph</code> is the
possible the most sensible, but for interval censored observations
the result is unclear.  The code currently does not add a <em>nobs</em>
attribute.
</p>


<h3>Value</h3>

<p>an object of class <code>logLik</code></p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>References</h3>

<p>Robert E. Kass and Adrian E. Raftery (1995). &quot;Bayes Factors&quot;. J.
American Statistical Assoc. 90 (430): 791.
</p>
<p>Raftery A.E. (1995), &quot;Bayesian Model Selection in Social Research&quot;,
Sociological methodology, 111-196.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+logLik">logLik</a></code></p>

<hr>
<h2 id='lung'>NCCTG Lung Cancer Data</h2><span id='topic+cancer'></span><span id='topic+lung'></span>

<h3>Description</h3>

<p>Survival in patients with advanced lung cancer from the North
Central Cancer Treatment Group.  Performance
scores rate how well the patient can perform usual daily activities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lung
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    inst:</td><td style="text-align: left;"> Institution code</td>
</tr>
<tr>
 <td style="text-align: left;">
    time:</td><td style="text-align: left;"> Survival time in days</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> censoring status 1=censored, 2=dead</td>
</tr>
<tr>
 <td style="text-align: left;">
    age:</td><td style="text-align: left;"> Age in years</td>
</tr>
<tr>
 <td style="text-align: left;">
    sex:</td><td style="text-align: left;">  Male=1 Female=2</td>
</tr>
<tr>
 <td style="text-align: left;">
    ph.ecog:</td><td style="text-align: left;"> ECOG performance score as rated by the physician.
    0=asymptomatic, 1= symptomatic but completely ambulatory, 2= in bed
    &lt;50% of the day, 3= in bed &gt; 50% of the day but not bedbound, 4 =
    bedbound</td>
</tr>
<tr>
 <td style="text-align: left;">
    ph.karno:</td><td style="text-align: left;"> Karnofsky performance score (bad=0-good=100) rated by physician</td>
</tr>
<tr>
 <td style="text-align: left;">
    pat.karno:</td><td style="text-align: left;"> Karnofsky performance score as rated by patient</td>
</tr>
<tr>
 <td style="text-align: left;">
    meal.cal:</td><td style="text-align: left;"> Calories consumed at meals</td>
</tr>
<tr>
 <td style="text-align: left;">
    wt.loss:</td><td style="text-align: left;"> Weight loss in last six months (pounds)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Note</h3>

<p>The use of 1/2 for alive/dead instead of the usual 0/1 is a historical
footnote.
For data contained on punch cards, IBM 360 Fortran treated blank as a zero,
which led to a policy within the section of Biostatistics to never
use &quot;0&quot; as a data value since one could not distinguish it from a
missing value.
The policy became a habit, as is often the case; and the 1/2 coding
endured long beyond the demise of punch cards and Fortran.
</p>


<h3>Source</h3>

<p>Terry Therneau</p>


<h3>References</h3>

<p>Loprinzi CL. Laurie JA. Wieand HS. Krook JE. Novotny PJ.
Kugler JW. Bartel J. Law M. Bateman M. Klatt NE. et al.
Prospective evaluation of prognostic variables from patient-completed
questionnaires. North Central Cancer Treatment Group.
Journal of Clinical Oncology. 12(3):601-7, 1994. </p>

<hr>
<h2 id='mgus'>Monoclonal gammopathy data</h2><span id='topic+mgus'></span><span id='topic+mgus1'></span>

<h3>Description</h3>

<p>Natural history of 241 subjects with monoclonal gammopathy of
undetermined significance (MGUS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mgus
mgus1
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>

<p>mgus: A data frame with 241 observations on the following 12 variables.
</p>

<table>
<tr>
 <td style="text-align: left;">
    id:</td><td style="text-align: left;"> subject id </td>
</tr>
<tr>
 <td style="text-align: left;">
    age:</td><td style="text-align: left;"> age in years at the detection of MGUS </td>
</tr>
<tr>
 <td style="text-align: left;">
    sex:</td><td style="text-align: left;"> <code>male</code> or <code>female</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
    dxyr:</td><td style="text-align: left;"> year of diagnosis </td>
</tr>
<tr>
 <td style="text-align: left;">
    pcdx:</td><td style="text-align: left;"> for subjects who progress to a plasma cell malignancy </td>
</tr>
<tr>
 <td style="text-align: left;">
       </td><td style="text-align: left;"> the subtype of malignancy: multiple myeloma (MM) is the </td>
</tr>
<tr>
 <td style="text-align: left;">
       </td><td style="text-align: left;"> most common, followed by amyloidosis (AM), macroglobulinemia (MA),</td>
</tr>
<tr>
 <td style="text-align: left;">
      </td><td style="text-align: left;"> and other lymphprolifative disorders (LP) </td>
</tr>
<tr>
 <td style="text-align: left;">
    pctime:</td><td style="text-align: left;"> days from MGUS until diagnosis of a plasma cell malignancy </td>
</tr>
<tr>
 <td style="text-align: left;">
    futime:</td><td style="text-align: left;"> days from diagnosis to last follow-up </td>
</tr>
<tr>
 <td style="text-align: left;">
    death:</td><td style="text-align: left;"> 1= follow-up is until death </td>
</tr>
<tr>
 <td style="text-align: left;">
    alb:</td><td style="text-align: left;"> albumin level at MGUS diagnosis </td>
</tr>
<tr>
 <td style="text-align: left;">
    creat:</td><td style="text-align: left;"> creatinine at MGUS diagnosis </td>
</tr>
<tr>
 <td style="text-align: left;">
    hgb:</td><td style="text-align: left;"> hemoglobin at MGUS diagnosis </td>
</tr>
<tr>
 <td style="text-align: left;">
    mspike:</td><td style="text-align: left;"> size of the monoclonal protein spike at diagnosis </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td>
</tr>

</table>

<p>mgus1: The same data set in start,stop format. Contains the id, age, sex, and
laboratory variable described above along with
</p>

<table>
<tr>
 <td style="text-align: left;"> 
    start, stop:</td><td style="text-align: left;"> sequential intervals of time for each
      subject </td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> =1 if the interval ends in an event </td>
</tr>
<tr>
 <td style="text-align: left;">
    event:</td><td style="text-align: left;"> a factor containing the event type: censor, death, or plasma cell malignancy </td>
</tr>
<tr>
 <td style="text-align: left;">
    enum: </td><td style="text-align: left;"> event number for each subject: 1 or 2
  </td>
</tr>

</table>



<h3>Details</h3>

<p>Plasma cells are responsible for manufacturing immunoglobulins, an
important part of the immune defense. At any given time there are
estimated to be about <code class="reqn">10^6</code> different immunoglobulins in the circulation
at any one time.  When a patient has a plasma cell malignancy the
distribution will become dominated by a single isotype, the product of
the malignant clone, visible as a spike on a serum protein
electrophoresis. Monoclonal gammopathy of undertermined significance
(MGUS) is the presence of such a spike, but in a patient with no
evidence of overt malignancy.  This data set of 241 sequential subjects
at Mayo Clinic was the groundbreaking study defining the natural history
of such subjects.
Due to the diligence of the principle investigator 0 subjects have
been lost to follow-up.
</p>
<p>Three subjects had MGUS detected on the day of death.  In data set
<code>mgus1</code> these subjects have the time to MGUS coded as .5 day before
the death in order to avoid tied times.
</p>
<p>These data sets were updated in Jan 2015 to correct some small errors.
</p>


<h3>Source</h3>

<p>Mayo Clinic data courtesy of Dr. Robert Kyle.
</p>


<h3>References</h3>

<p>R Kyle, Benign monoclonal gammopathy &ndash; after 20 to 35 years of
follow-up,
Mayo Clinic Proc 1993; 68:26-36. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create the competing risk curves for time to first of death or PCM
sfit &lt;- survfit(Surv(start, stop, event) ~ sex, mgus1, id=id,
                subset=(enum==1))
print(sfit)  # the order of printout is the order in which they plot

plot(sfit, xscale=365.25, lty=c(2,2,1,1), col=c(1,2,1,2),
     xlab="Years after MGUS detection", ylab="Proportion")
legend(0, .8, c("Death/male", "Death/female", "PCM/male", "PCM/female"),
       lty=c(1,1,2,2), col=c(2,1,2,1), bty='n')

title("Curves for the first of plasma cell malignancy or death")
# The plot shows that males have a higher death rate than females (no
# surprise) but their rates of conversion to PCM are essentially the same.
</code></pre>

<hr>
<h2 id='mgus2'>Monoclonal gammopathy data</h2><span id='topic+mgus2'></span>

<h3>Description</h3>

<p>Natural history of 1341 sequential patients with monoclonal
gammopathy of undetermined significance (MGUS).  This is a superset of
the <code>mgus</code> data, at a later point in the accrual process
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mgus2
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 1384 observations on the following 10 variables.
</p>

<dl>
<dt><code>id</code></dt><dd><p>subject identifier</p>
</dd>
<dt><code>age</code></dt><dd><p>age at diagnosis, in years</p>
</dd>
<dt><code>sex</code></dt><dd><p>a factor with levels <code>F</code> <code>M</code></p>
</dd>
<dt><code>dxyr</code></dt><dd><p>year of diagnosis</p>
</dd>
<dt><code>hgb</code></dt><dd><p>hemoglobin</p>
</dd>
<dt><code>creat</code></dt><dd><p>creatinine</p>
</dd>
<dt><code>mspike</code></dt><dd><p>size of the monoclonal serum splike</p>
</dd>
<dt><code>ptime</code></dt><dd><p>time until progression to a plasma cell
malignancy (PCM) or last contact, in months</p>
</dd>
<dt><code>pstat</code></dt><dd><p>occurrence of PCM: 0=no, 1=yes </p>
</dd>
<dt><code>futime</code></dt><dd><p>time until death or last contact, in months</p>
</dd>
<dt><code>death</code></dt><dd><p>occurrence of death: 0=no, 1=yes</p>
</dd>
</dl>



<h3>Details</h3>

<p>This is an extension of the study found in the <code>mgus</code> data set,
containing enrollment through 1994 and follow-up through 1999.
</p>


<h3>Source</h3>

<p>Mayo Clinic data courtesy of Dr. Robert Kyle.  All patient
identifiers have been removed, age rounded to the nearest year, and
follow-up times rounded to the nearest month.</p>


<h3>References</h3>

<p>R. Kyle, T. Therneau, V. Rajkumar, J. Offord, D. Larson, M. Plevak,
and L. J. Melton III, A long-terms study of prognosis in monoclonal
gammopathy of undertermined significance. New Engl J Med, 346:564-569 (2002).
</p>

<hr>
<h2 id='model.frame.coxph'>Model.frame method for coxph objects</h2><span id='topic+model.frame.coxph'></span>

<h3>Description</h3>

<p> Recreate the model frame of a coxph fit. </p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coxph'
model.frame(formula, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model.frame.coxph_+3A_formula">formula</code></td>
<td>
<p>the result of a <code>coxph</code> fit</p>
</td></tr>
<tr><td><code id="model.frame.coxph_+3A_...">...</code></td>
<td>
<p>other arguments to <code>model.frame</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details, see the manual page for the generic function.
This function would rarely be called by a user, it is mostly used
inside functions like <code>residual</code> that need to recreate the data
set from a model in order to do further calculations.
</p>


<h3>Value</h3>

<p>the model frame used in the original fit, or a parallel one for
new data.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+model.frame">model.frame</a></code></p>

<hr>
<h2 id='model.matrix.coxph'>
Model.matrix method for coxph models
</h2><span id='topic+model.matrix.coxph'></span>

<h3>Description</h3>

<p>Reconstruct the model matrix for a cox model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coxph'
model.matrix(object, data=NULL, contrast.arg =
 object$contrasts, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model.matrix.coxph_+3A_object">object</code></td>
<td>
<p>the result of a <code>coxph</code> model</p>
</td></tr>
<tr><td><code id="model.matrix.coxph_+3A_data">data</code></td>
<td>
<p>optional, a data frame from which to obtain the data</p>
</td></tr>
<tr><td><code id="model.matrix.coxph_+3A_contrast.arg">contrast.arg</code></td>
<td>
<p>optional, a contrasts object describing how
factors should be coded</p>
</td></tr>
<tr><td><code id="model.matrix.coxph_+3A_...">...</code></td>
<td>
<p>other possible argument to <code>model.frame</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>When there is a <code>data</code> argument this function differs from most
of the other <code>model.matrix</code> methods in that the response variable
for the original formula is <em>not</em> required to be in the data.
</p>
<p>If the data frame contains a <code>terms</code> attribute then it is
assumed to be the result of a call to <code>model.frame</code>, otherwise
a call to <code>model.frame</code> is applied with the data as an argument.
</p>


<h3>Value</h3>

<p>The model matrix for the fit
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+model.matrix">model.matrix</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>fit1 &lt;- coxph(Surv(time, status) ~ age + factor(ph.ecog), data=lung)
xfit &lt;- model.matrix(fit1)

fit2 &lt;- coxph(Surv(time, status) ~ age + factor(ph.ecog), data=lung,
                                 x=TRUE)
all.equal(model.matrix(fit1), fit2$x)
</code></pre>

<hr>
<h2 id='myeloid'>Acute myeloid leukemia</h2><span id='topic+myeloid'></span>

<h3>Description</h3>

<p>This simulated data set is based on a trial in acute myeloid
leukemia.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>myeloid
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 646 observations on the following 9 variables.
</p>

<dl>
<dt><code>id</code></dt><dd><p>subject identifier, 1-646</p>
</dd>
<dt><code>trt</code></dt><dd><p>treatment arm A or B</p>
</dd>
<dt><code>sex</code></dt><dd><p>f=female, m=male</p>
</dd>
<dt><code>flt3</code></dt><dd><p>mutations of the FLT3 gene, a factor with levels
of A, B, C</p>
</dd>
<dt><code>futime</code></dt><dd><p>time to death or last follow-up</p>
</dd>
<dt><code>death</code></dt><dd><p>1 if <code>futime</code> is a death, 0 for censoring</p>
</dd>
<dt><code>txtime</code></dt><dd><p>time to hematropetic stem cell transplant</p>
</dd>
<dt><code>crtime</code></dt><dd><p>time to complete response</p>
</dd>
<dt><code>rltime</code></dt><dd><p>time to relapse of disease</p>
</dd>
</dl>



<h3>Details</h3>

<p>This data set is used to illustrate multi-state survival curves.
It is based on the actual study in the reference below.
A subset of subjects was de-identifed, reordered, and then all of the
time values randomly perturbed.
</p>
<p>Mutations in the FLT3 domain occur in about 1/3 of AML patients,
the additional agent in treatment arm B was presumed to target this anomaly.
All subjects had a FLT mutation, either internal tandem duplications (ITD)
(divided into low vs high) +- mutations in the TKD domain, or TKD mutations
only.  This was a stratification factor for treatment assignment in
the study.   The levels of A, B, C correspond to increasing severity of the
mutation burden.
</p>


<h3>References</h3>

<p>Le-Rademacher JG, Peterson RA, Therneau TM, Sanford BL, Stone RM,
Mandrekar SJ.  Application of multi-state models in cancer clinical trials. 
Clin Trials. 2018 Oct; 15 (5):489-498
</p>


<h3>Examples</h3>

<pre><code class='language-R'>coxph(Surv(futime, death) ~ trt + flt3, data=myeloid)
# See the mstate vignette for a more complete analysis
</code></pre>

<hr>
<h2 id='myeloma'>
Survival times of patients with multiple myeloma
</h2><span id='topic+myeloma'></span>

<h3>Description</h3>

<p>Survival times of 3882 subjects with multiple myeloma, seen at Mayo
Clinic from 1947&ndash;1996.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>myeloma
data("cancer", package="survival")</code></pre>


<h3>Format</h3>

<p>A data frame with 3882 observations on the following 5 variables.
</p>

<dl>
<dt><code>id</code></dt><dd><p>subject identifier</p>
</dd>
<dt><code>year</code></dt><dd><p>year of entry into the study</p>
</dd>
<dt><code>entry</code></dt><dd><p>time from diagnosis of MM until entry (days)</p>
</dd>
<dt><code>futime</code></dt><dd><p>follow up time (days)</p>
</dd>
<dt><code>death</code></dt><dd><p>status at last follow-up: 0 = alive, 1 = death</p>
</dd>
</dl>



<h3>Details</h3>

<p>Subjects who were diagnosed at Mayo will have <code>entry</code> =0, those who
were diagnosed elsewhere and later referred will have positive values.
</p>


<h3>References</h3>

<p>R. Kyle, Long term survival in multiple myeloma.
New Eng J Medicine, 1997
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Incorrect survival curve, which ignores left truncation
fit1 &lt;- survfit(Surv(futime, death) ~ 1, myeloma)
# Correct curve
fit2 &lt;- survfit(Surv(entry, futime, death) ~1, myeloma)
</code></pre>

<hr>
<h2 id='nafld'>Non-alcoholic fatty liver disease</h2><span id='topic+nafld1'></span><span id='topic+nafld2'></span><span id='topic+nafld3'></span>

<h3>Description</h3>

<p>Data sets containing the data from a population study of non-alcoholic
fatty liver disease (NAFLD).  Subjects with the condition and a set of
matched control subjects were followed forward for metabolic
conditions, cardiac endpoints, and death.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nafld1
       nafld2
       nafld3
data(nafld, package="survival")
</code></pre>


<h3>Format</h3>

<p><code>nafld1</code> is a data frame with 17549 observations on the following 10 variables.
</p>

<dl>
<dt><code>id</code></dt><dd><p>subject identifier</p>
</dd>
<dt><code>age</code></dt><dd><p>age at entry to the study</p>
</dd>
<dt><code>male</code></dt><dd><p>0=female, 1=male</p>
</dd>
<dt><code>weight</code></dt><dd><p>weight in kg</p>
</dd>
<dt><code>height</code></dt><dd><p>height in cm</p>
</dd>
<dt><code>bmi</code></dt><dd><p>body mass index</p>
</dd>
<dt><code>case.id</code></dt><dd><p>the id of the NAFLD case to whom this subject
is matched</p>
</dd>
<dt><code>futime</code></dt><dd><p>time to death or last follow-up</p>
</dd>
<dt><code>status</code></dt><dd><p>0= alive at last follow-up, 1=dead</p>
</dd>
</dl>

<p><code>nafld2</code> is a data frame with 400123 observations and 4 variables
containing laboratory data
</p>

<dl>
<dt><code>id</code></dt><dd><p>subject identifier</p>
</dd>
<dt><code>days</code></dt><dd><p>days since index date</p>
</dd>
<dt><code>test</code></dt><dd><p>the type of value recorded</p>
</dd>
<dt><code>value</code></dt><dd><p>the numeric value</p>
</dd>
</dl>

<p><code>nafld3</code> is a data frame with 34340 observations and 3 variables
containing outcomes
</p>

<dl>
<dt><code>id</code></dt><dd><p>subject identifier</p>
</dd>
<dt><code>days</code></dt><dd><p>days since index date</p>
</dd>
<dt><code>event</code></dt><dd><p>the endpoint that occurred</p>
</dd>
</dl>



<h3>Details</h3>

<p>The primary reference for this study is Allen (2018).
Nonalcoholic fatty liver disease (NAFLD) was renamed metabolic
dysfunction-associated steatotic liver disease (MASLD) in June 2023.
The new name is intended to better reflect the disease's underlying causes, 
identify subgroups of patients, and avoid stigmatizing words.
</p>
<p>The incidence of MASLD has been rising
rapidly in the last decade and it is now one of the main drivers of
hepatology practice <cite>Tapper2018</cite>.
It is essentially the presence of excess fat in the
liver, and parallels the ongoing obesity epidemic.
Approximately 20-25% of MASLD patients will develop the inflammatory state
of metabolic dysfunction associated steatohepatitis (MASH), leading to fibrosis 
and eventual end-stage liver disease.
MASLD can be accurately diagnosed by MRI methods, 
but MASH diagnosis currently requires a biopsy.
</p>
<p>The current study constructed a population cohort of
all adult MASLD subjects from 1997 to 2014  along with 4 potential
controls for each case.
To protect patient confidentiality all time intervals are in days since
the index date; none of the dates from the original data were retained.
Subject age is their integer age at the index date, and the subject
identifier is an arbitrary integer. 
As a final protection, we include only a 90% random sample of the data.
As a consequence analyses results will not exactly match the
original paper.
</p>
<p>There are 3 data sets: <code>nafld1</code> contains baseline data and has
one observation per subject, <code>nafld2</code> has one observation for
each (time dependent) continuous measurement,
and <code>nafld3</code> has one observation for
each yes/no outcome that occured.
</p>


<h3>Source</h3>

<p>Data obtained from the author.</p>


<h3>References</h3>

<p>AM Allen, TM Therneau, JJ Larson, A Coward, VK Somers and PS Kamath,
Nonalcoholic Fatty Liver Disease Incidence and Impact on Metabolic
Burden and Death: A 20 Year Community Study,
Hepatology 67:1726-1736, 2018.
</p>

<hr>
<h2 id='neardate'>
Find the index of the closest value in data set 2, for each entry in
data set one.
</h2><span id='topic+neardate'></span>

<h3>Description</h3>

<p>A common task in medical work is to find the closest lab value to some
index date, for each subject.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neardate(id1, id2, y1, y2, best = c("after", "prior"),
nomatch = NA_integer_)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neardate_+3A_id1">id1</code></td>
<td>
<p>vector of subject identifiers for the index group</p>
</td></tr>
<tr><td><code id="neardate_+3A_id2">id2</code></td>
<td>
<p>vector of identifiers for the reference group</p>
</td></tr>
<tr><td><code id="neardate_+3A_y1">y1</code></td>
<td>
<p>normally a vector of dates for the index group,
but any orderable data type is allowed</p>
</td></tr>
<tr><td><code id="neardate_+3A_y2">y2</code></td>
<td>
<p>reference set of dates</p>
</td></tr>
<tr><td><code id="neardate_+3A_best">best</code></td>
<td>
<p>if <code>best='prior'</code> find the index of the first
y2 value less than or equal to
the target y1 value, for each subject.
If <code>best='after'</code> find the first y2 value which is greater than
or equal to the target y1 value, for each subject.</p>
</td></tr>
<tr><td><code id="neardate_+3A_nomatch">nomatch</code></td>
<td>
<p>the value to return for items without a match</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This routine is closely related to <code>match</code> and to
<code>findInterval</code>, the first of which finds exact matches and
the second closest matches.  This finds the closest matching date
within sets of exactly matching identifiers.  
Closest date matching is often needed in clinical studies.  For
example data set 1 might contain the subject identifier and the date
of some procedure and data set set 2 has the dates and values for
laboratory tests, and the query is to find the first
test value after the intervention but no closer than 7 days.
</p>
<p>The <code>id1</code> and <code>id2</code> arguments are similar to <code>match</code> in
that we are searching for instances of <code>id1</code> that will be found
in <code>id2</code>, and the result is the same length as <code>id1</code>.
However, instead of returning the first match with <code>id2</code> this
routine returns the one that best matches with respect to <code>y1</code>.
</p>
<p>The <code>y1</code> and <code>y2</code> arguments need not be dates, the function
works for any data type such that the expression
<code>c(y1, y2)</code> gives a sensible, sortable result.
Be careful about matching Date and DateTime values and the impact of
time zones, however, see <code><a href="base.html#topic+as.POSIXct">as.POSIXct</a></code>. 
If <code>y1</code> and <code>y2</code> are not of the same class the user is
on their own.
Since there exist pairs of unmatched data types where the result could
be sensible, the routine will in this case proceed under the assumption
that &quot;the user knows what they are doing&quot;.  Caveat emptor.
</p>


<h3>Value</h3>

<p>the index of the matching observations in the second data set, or
the <code>nomatch</code> value for no successful match</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+match">match</a></code>, <code><a href="base.html#topic+findInterval">findInterval</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data1 &lt;- data.frame(id = 1:10,
                    entry.dt = as.Date(paste("2011", 1:10, "5", sep='-')))
temp1 &lt;- c(1,4,5,1,3,6,9, 2,7,8,12,4,6,7,10,12,3)
data2 &lt;- data.frame(id = c(1,1,1,2,2,4,4,5,5,5,6,8,8,9,10,10,12),
                    lab.dt = as.Date(paste("2011", temp1, "1", sep='-')),
                    chol = round(runif(17, 130, 280)))

#first cholesterol on or after enrollment
indx1 &lt;- neardate(data1$id, data2$id, data1$entry.dt, data2$lab.dt)
data2[indx1, "chol"]

# Closest one, either before or after. 
# 
indx2 &lt;- neardate(data1$id, data2$id, data1$entry.dt, data2$lab.dt, 
                   best="prior")
ifelse(is.na(indx1), indx2, # none after, take before
       ifelse(is.na(indx2), indx1, #none before
       ifelse(abs(data2$lab.dt[indx2]- data1$entry.dt) &lt;
              abs(data2$lab.dt[indx1]- data1$entry.dt), indx2, indx1)))

# closest date before or after, but no more than 21 days prior to index
indx2 &lt;- ifelse((data1$entry.dt - data2$lab.dt[indx2]) &gt;21, NA, indx2)
ifelse(is.na(indx1), indx2, # none after, take before
       ifelse(is.na(indx2), indx1, #none before
       ifelse(abs(data2$lab.dt[indx2]- data1$entry.dt) &lt;
              abs(data2$lab.dt[indx1]- data1$entry.dt), indx2, indx1)))
</code></pre>

<hr>
<h2 id='nsk'>
Natural splines with knot heights as the basis.
</h2><span id='topic+nsk'></span>

<h3>Description</h3>

<p>Create the design matrix for a natural spline, such that the coefficient
of the resulting fit are the values of the function at the knots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nsk(x, df = NULL, knots = NULL, intercept = FALSE, b = 0.05, 
    Boundary.knots = quantile(x, c(b, 1 - b), na.rm = TRUE))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nsk_+3A_x">x</code></td>
<td>
<p>the predictor variable.  Missing values are allowed.
</p>
</td></tr>
<tr><td><code id="nsk_+3A_df">df</code></td>
<td>

<p>degrees of freedom. One can supply df rather than knots; ns() then
chooses df - 1 - intercept knots at suitably chosen quantiles of x
(which will ignore missing values).
The default, df = NULL, sets the number of inner knots as length(knots).
</p>
</td></tr>
<tr><td><code id="nsk_+3A_knots">knots</code></td>
<td>

<p>breakpoints that define the spline. The default is no knots;
together with the natural boundary conditions this results in a
basis for linear regression on x.
Typical values are the mean or median for one knot, quantiles for
more 
knots. See also Boundary.knots.
</p>
</td></tr>
<tr><td><code id="nsk_+3A_intercept">intercept</code></td>
<td>

<p>if TRUE, an intercept is included in the basis; default is FALSE
</p>
</td></tr>
<tr><td><code id="nsk_+3A_b">b</code></td>
<td>
<p>default placement of the boundary knots.  A value of
<code>bs=0</code> will replicate the default behavior of <code>ns</code>.
</p>
</td></tr>
<tr><td><code id="nsk_+3A_boundary.knots">Boundary.knots</code></td>
<td>

<p>boundary points at which to impose the natural boundary conditions and
anchor the B-spline basis.  Beyond these points the function is
assumed to be linear.
If both knots and Boundary.knots are supplied, the basis parameters do
not depend on x. Data can extend beyond Boundary.knots
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>nsk</code> function behaves identically to the <code>ns</code> function,
with two exceptions.  The primary one is that the returned basis is
such that coefficients correspond to the value of the fitted function
at the knot points.  If <code>intercept = FALSE</code>, there will be k-1
coefficients corresponding to the k knots, and they will be the difference
in predicted value between knots 2-k and knot 1.
The primary advantage to the basis is that the coefficients are
directly interpretable.  A second is that tests for the linear and
non-linear components are simple contrasts.
</p>
<p>The second differnce with <code>ns</code> is one of opinion with respect to
the default position for the boundary knots.  The default here is
closer to that found in the <code>rms::rcs</code> function.
</p>
<p>This function is a trial if a new idea, it's future inclusion in the
package is not yet guarranteed.
</p>


<h3>Value</h3>

<p>A matrix of dimension length(x) * df where either df was supplied or,
if knots were supplied, df = length(knots) + 1 + intercept.
Attributes are returned that correspond to the arguments to kns,
and explicitly give the knots, Boundary.knots etc for use by predict.kns().
</p>


<h3>Note</h3>

<p>A thin flexible metal or wooden strip is called a spline, and is the
traditional method for laying out a smooth curve, e.g., for a ship's
hull or an airplane wing.  Pins are put into a board and the strip is
passed through them, each pin is a 'knot'.
</p>
<p>A mathematical spline is a piecewise function between each knot.  A
linear spline will be a set of connected line segments, a quadratic
spline is a set of connected local quadratic functions, constrained to
have a continuous first derivative, a cubic spline is cubic between
each knot, constrained to have continuous first and second
derivatives, and etc.  Mathematical splines are not an exact
representation of natural splines: being a physical object the wood or
metal strip will have continuous derivatives of all orders.  Cubic
splines are commonly used because they are sufficiently smooth to
look natural to the human eye.
</p>
<p>If the mathematical spline is further constrained to be linear beyond
the end knots, this is often called a 'natural spline', due to the
fact that a wooden or metal spline will also be linear beyond the last
knots.  Another name for the same object is a 'restricted cubic
spline', since it is achieved in code by adding further constraints.
Given a vector of data points and a set of knots, it is possible to
create a basis matrix X with one column per knot, such that ordinary
regression of X on y will fit the cubic spline function, hence these
are also called 'regression splines'. (One of these three labels is no
better or worse than another, in our opinion).
</p>
<p>Given a basis matrix X with k columns, the
matrix Z= XT for any k by k nonsingular matrix T is is also a basis
matrix, and will result in identical predicted values, but a new set
of coefficients gamma = (T-inverse) beta in place of beta.  One can
choose the basis functions so that X is easy to construct, to
make the regression numerically stable, to make tests easier, or based
on other considerations.
It seems as though every spline library returns a different basis
set, which unfortunately makes fits difficult to compare between packages.
This is yet one more basis set, chosen to make the coefficients
more interpretable. 
</p>


<h3>See Also</h3>

<p><code><a href="splines.html#topic+ns">ns</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># make some dummy data
tdata &lt;- data.frame(x= lung$age, y = 10*log(lung$age-35) + rnorm(228, 0, 2))
fit1 &lt;- lm(y ~ -1 + nsk(x, df=4, intercept=TRUE) , data=tdata)
fit2 &lt;- lm(y ~ nsk(x, df=3), data=tdata)

# the knots (same for both fits)
knots &lt;- unlist(attributes(fit1$model[[2]])[c('Boundary.knots', 'knots')])
sort(unname(knots))
unname(coef(fit1))  # predictions at the knot points

unname(coef(fit1)[-1] - coef(fit1)[1])  # differences: yhat[2:4] - yhat[1]
unname(coef(fit2))[-1]                  # ditto

## Not run: 
plot(y ~ x, data=tdata)
points(sort(knots), coef(fit1), col=2, pch=19)
coef(fit)[1] + c(0, coef(fit)[-1])

## End(Not run)
</code></pre>

<hr>
<h2 id='nwtco'>Data from the National Wilm's Tumor Study</h2><span id='topic+nwtco'></span>

<h3>Description</h3>

<p>Measurement error example. Tumor histology predicts
survival, but prediction is stronger with central lab histology than
with the local institution determination.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nwtco
data(nwtco, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 4028 observations on the following 9 variables.
</p>

<dl>
<dt><code>seqno</code></dt><dd><p>id number</p>
</dd>
<dt><code>instit</code></dt><dd><p>Histology from local institution</p>
</dd>
<dt><code>histol</code></dt><dd><p>Histology from central lab</p>
</dd>
<dt><code>stage</code></dt><dd><p>Disease stage</p>
</dd>
<dt><code>study</code></dt><dd><p>study</p>
</dd>
<dt><code>rel</code></dt><dd><p>indicator for relapse</p>
</dd>
<dt><code>edrel</code></dt><dd><p>time to relapse</p>
</dd>
<dt><code>age</code></dt><dd><p>age in months</p>
</dd>
<dt><code>in.subcohort</code></dt><dd><p>Included in the subcohort for the example in the
paper</p>
</dd>
</dl>



<h3>References</h3>

<p>NE Breslow and N Chatterjee (1999),
Design and analysis of two-phase studies with binary outcome applied
to Wilms tumour prognosis.
<em>Applied Statistics</em> <b>48</b>, 457&ndash;68.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>with(nwtco, table(instit,histol))
anova(coxph(Surv(edrel,rel)~histol+instit,data=nwtco))
anova(coxph(Surv(edrel,rel)~instit+histol,data=nwtco))
</code></pre>

<hr>
<h2 id='ovarian'>Ovarian Cancer Survival Data</h2><span id='topic+ovarian'></span>

<h3>Description</h3>

<p>Survival in a randomised trial comparing two treatments for
ovarian cancer</p>


<h3>Usage</h3>

<pre><code class='language-R'>ovarian
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    futime:</td><td style="text-align: left;"> survival or censoring time</td>
</tr>
<tr>
 <td style="text-align: left;">
    fustat:</td><td style="text-align: left;"> censoring status</td>
</tr>
<tr>
 <td style="text-align: left;">
    age: </td><td style="text-align: left;"> in years</td>
</tr>
<tr>
 <td style="text-align: left;">
    resid.ds:</td><td style="text-align: left;"> residual disease present (1=no,2=yes)</td>
</tr>
<tr>
 <td style="text-align: left;">
    rx:</td><td style="text-align: left;"> treatment group</td>
</tr>
<tr>
 <td style="text-align: left;">
    ecog.ps:</td><td style="text-align: left;"> ECOG performance status (1 is better, see reference)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Terry Therneau</p>


<h3>References</h3>

<p>Edmunson, J.H., Fleming, T.R., Decker, D.G.,
Malkasian, G.D., Jefferies, J.A., Webb, M.J., and Kvols, L.K.,
Different Chemotherapeutic Sensitivities and Host Factors Affecting
Prognosis in Advanced Ovarian Carcinoma vs. Minimal Residual Disease.
Cancer Treatment Reports, 63:241-47, 1979.
</p>

<hr>
<h2 id='pbc'>Mayo Clinic Primary Biliary Cholangitis Data</h2><span id='topic+pbc'></span>

<h3>Description</h3>

<p>Primary biliary cholangitis
is an autoimmune disease leading to
destruction of the small bile ducts in the liver.  Progression is
slow but inexhortable, eventually leading to cirrhosis and liver
decompensation.
The condition has been recognised since at least 1851 and was named
&quot;primary biliary cirrhosis&quot; in 1949.
Because cirrhosis is a feature only of advanced disease, a change of
its name to &quot;primary biliary cholangitis&quot; was proposed by patient
advocacy groups in 2014.
</p>
<p>This data is from the Mayo Clinic trial in PBC conducted between 1974
and 1984.  
A total of 424 PBC patients, referred to Mayo Clinic during that ten-year
interval, met eligibility criteria for the randomized placebo controlled
trial of the drug D-penicillamine.  The first 312 cases in the data set
participated in the randomized trial and contain largely complete data.  The
additional 112 cases did not participate in the clinical trial, but consented
to have basic measurements recorded and to be followed for survival.  Six of
those cases were lost to follow-up shortly after diagnosis, so the data here
are on an additional 106 cases as well as the 312 randomized participants.
</p>
<p>A nearly identical data set found in appendix D of Fleming and Harrington;
this version has fewer missing values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pbc
data(pbc, package="survival")
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    age:</td><td style="text-align: left;"> in years</td>
</tr>
<tr>
 <td style="text-align: left;">
    albumin:</td><td style="text-align: left;"> serum albumin (g/dl)</td>
</tr>
<tr>
 <td style="text-align: left;">
    alk.phos:</td><td style="text-align: left;"> alkaline phosphotase (U/liter)</td>
</tr>
<tr>
 <td style="text-align: left;">
    ascites:</td><td style="text-align: left;"> presence of ascites </td>
</tr>
<tr>
 <td style="text-align: left;">
    ast:</td><td style="text-align: left;"> aspartate aminotransferase, once called SGOT (U/ml)</td>
</tr>
<tr>
 <td style="text-align: left;">
    bili:</td><td style="text-align: left;"> serum bilirunbin (mg/dl)</td>
</tr>
<tr>
 <td style="text-align: left;">
    chol:</td><td style="text-align: left;"> serum cholesterol (mg/dl)</td>
</tr>
<tr>
 <td style="text-align: left;">
    copper:</td><td style="text-align: left;"> urine copper (ug/day)</td>
</tr>
<tr>
 <td style="text-align: left;">
    edema:</td><td style="text-align: left;"> 0 no edema, 0.5 untreated or successfully treated</td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> 1 edema despite diuretic therapy</td>
</tr>
<tr>
 <td style="text-align: left;">
    hepato:</td><td style="text-align: left;"> presence of hepatomegaly or enlarged liver</td>
</tr>
<tr>
 <td style="text-align: left;">
    id:</td><td style="text-align: left;"> case number</td>
</tr>
<tr>
 <td style="text-align: left;">
    platelet:</td><td style="text-align: left;"> platelet count</td>
</tr>
<tr>
 <td style="text-align: left;">
    protime:</td><td style="text-align: left;"> standardised blood clotting time</td>
</tr>
<tr>
 <td style="text-align: left;">
    sex:</td><td style="text-align: left;"> m/f</td>
</tr>
<tr>
 <td style="text-align: left;">
    spiders:</td><td style="text-align: left;"> blood vessel malformations in the skin</td>
</tr>
<tr>
 <td style="text-align: left;">
    stage:</td><td style="text-align: left;"> histologic stage of disease (needs biopsy)</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> status at endpoint, 0/1/2 for censored, transplant, dead</td>
</tr>
<tr>
 <td style="text-align: left;">
    time: </td><td style="text-align: left;"> number of days between registration and the earlier of death,</td>
</tr>
<tr>
 <td style="text-align: left;">
       </td><td style="text-align: left;"> transplantion, or study analysis in July, 1986</td>
</tr>
<tr>
 <td style="text-align: left;">
    trt:</td><td style="text-align: left;"> 1/2/NA for D-penicillmain, placebo, not randomised</td>
</tr>
<tr>
 <td style="text-align: left;">
    trig:</td><td style="text-align: left;"> triglycerides (mg/dl)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>T Therneau and P Grambsch (2000),
<em>Modeling Survival Data: Extending the Cox Model</em>,
Springer-Verlag, New York.
ISBN: 0-387-98784-3.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pbcseq">pbcseq</a></code></p>

<hr>
<h2 id='pbcseq'>Mayo Clinic Primary Biliary Cirrhosis, sequential data</h2><span id='topic+pbcseq'></span>

<h3>Description</h3>

<p>This data is a continuation of the PBC data set,
and contains the follow-up laboratory data for each study patient.
An analysis based on the data can be found in Murtagh, et. al.
</p>
<p>The primary PBC data set contains only baseline measurements 
of the laboratory 
parameters.  This data set contains multiple laboratory results, but
only on the 312 randomized patients.  Some baseline data values in this file
differ from the original PBC file, for instance, the data errors in
prothrombin time and age which were discovered after the original analysis
(see Fleming and Harrington, figure 4.6.7). It also contains further follow-up.
</p>
<p>One feature of the data deserves special comment.  The last
observation before death or liver transplant often has many more
missing covariates than other data rows.  The original clinical
protocol for these patients specified visits at 6 months, 1 year, and
annually thereafter.  At these protocol visits lab values were
obtained for a pre-specified battery of tests.  &quot;Extra&quot; visits,
often undertaken because of worsening medical condition, did not
necessarily have all this lab work. 
The missing values are thus potentially informative.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pbcseq
data(pbc, package="survival")
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    id:</td><td style="text-align: left;"> case number</td>
</tr>
<tr>
 <td style="text-align: left;">
    age:</td><td style="text-align: left;"> in years</td>
</tr>
<tr>
 <td style="text-align: left;">
    sex:</td><td style="text-align: left;"> m/f</td>
</tr>
<tr>
 <td style="text-align: left;">
    trt:</td><td style="text-align: left;"> 1/2/NA for D-penicillmain, placebo, not randomised</td>
</tr>
<tr>
 <td style="text-align: left;">
    time:</td><td style="text-align: left;"> number of days between registration and the earlier of death,</td>
</tr>
<tr>
 <td style="text-align: left;">
       </td><td style="text-align: left;"> transplantion, or study analysis in July, 1986</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> status at endpoint, 0/1/2 for censored, transplant, dead</td>
</tr>
<tr>
 <td style="text-align: left;">
    day:</td><td style="text-align: left;"> number of days between enrollment and this visit date</td>
</tr>
<tr>
 <td style="text-align: left;">
       </td><td style="text-align: left;"> all measurements below refer to this date</td>
</tr>
<tr>
 <td style="text-align: left;">
    albumin:</td><td style="text-align: left;"> serum albumin (mg/dl)</td>
</tr>
<tr>
 <td style="text-align: left;">
    alk.phos:</td><td style="text-align: left;"> alkaline phosphotase (U/liter)</td>
</tr>
<tr>
 <td style="text-align: left;">
    ascites:</td><td style="text-align: left;"> presence of ascites </td>
</tr>
<tr>
 <td style="text-align: left;">
    ast:</td><td style="text-align: left;"> aspartate aminotransferase, once called SGOT (U/ml)</td>
</tr>
<tr>
 <td style="text-align: left;">
    bili:</td><td style="text-align: left;"> serum bilirunbin (mg/dl)</td>
</tr>
<tr>
 <td style="text-align: left;">
    chol:</td><td style="text-align: left;"> serum cholesterol (mg/dl)</td>
</tr>
<tr>
 <td style="text-align: left;">
    copper:</td><td style="text-align: left;"> urine copper (ug/day)</td>
</tr>
<tr>
 <td style="text-align: left;">
    edema:</td><td style="text-align: left;"> 0 no edema, 0.5 untreated or successfully treated</td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> 1 edema despite diuretic therapy</td>
</tr>
<tr>
 <td style="text-align: left;">
    hepato:</td><td style="text-align: left;"> presence of hepatomegaly or enlarged liver</td>
</tr>
<tr>
 <td style="text-align: left;">
    platelet:</td><td style="text-align: left;"> platelet count</td>
</tr>
<tr>
 <td style="text-align: left;">
    protime:</td><td style="text-align: left;"> standardised blood clotting time</td>
</tr>
<tr>
 <td style="text-align: left;">
    spiders:</td><td style="text-align: left;"> blood vessel malformations in the skin</td>
</tr>
<tr>
 <td style="text-align: left;">
    stage:</td><td style="text-align: left;"> histologic stage of disease (needs biopsy)</td>
</tr>
<tr>
 <td style="text-align: left;">
    trig:</td><td style="text-align: left;"> triglycerides (mg/dl)</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>T Therneau and P Grambsch,
&quot;Modeling Survival Data: Extending the Cox Model&quot;,
Springer-Verlag, New York, 2000.
ISBN: 0-387-98784-3.
</p>


<h3>References</h3>

<p>Murtaugh PA. Dickson ER. Van Dam GM. Malinchoc M. Grambsch PM.
Langworthy AL. Gips CH.  &quot;Primary biliary cirrhosis: prediction of short-term
survival based on repeated patient visits.&quot; Hepatology. 20(1.1):126-34, 1994.
</p>
<p>Fleming T and Harrington D., &quot;Counting Processes and Survival Analysis&quot;,
Wiley, New York, 1991.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pbc">pbc</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Create the start-stop-event triplet needed for coxph
first &lt;- with(pbcseq, c(TRUE, diff(id) !=0)) #first id for each subject
last  &lt;- c(first[-1], TRUE)  #last id

time1 &lt;- with(pbcseq, ifelse(first, 0, day))
time2 &lt;- with(pbcseq, ifelse(last,  futime, c(day[-1], 0)))
event &lt;- with(pbcseq, ifelse(last,  status, 0))

fit1 &lt;- coxph(Surv(time1, time2, event) ~ age + sex + log(bili), pbcseq)
</code></pre>

<hr>
<h2 id='plot.aareg'>
Plot an aareg object.
</h2><span id='topic+plot.aareg'></span>

<h3>Description</h3>

<p>Plot the estimated coefficient function(s) from a fit
of Aalen's additive regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'aareg'
plot(x, se=TRUE, maxtime, type='s', ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.aareg_+3A_x">x</code></td>
<td>

<p>the result of a call to the <code>aareg</code> function
</p>
</td></tr>
<tr><td><code id="plot.aareg_+3A_se">se</code></td>
<td>

<p>if TRUE, standard error bands are included on the plot
</p>
</td></tr>
<tr><td><code id="plot.aareg_+3A_maxtime">maxtime</code></td>
<td>

<p>upper limit for the x-axis.
</p>
</td></tr>
<tr><td><code id="plot.aareg_+3A_type">type</code></td>
<td>

<p>graphical parameter for the type of line, default is &quot;steps&quot;.
</p>
</td></tr>
<tr><td><code id="plot.aareg_+3A_...">...</code></td>
<td>

<p>other graphical parameters such as line type, color, or axis labels.
</p>
</td></tr>
</table>


<h3>Side Effects</h3>

<p>A plot is produced on the current graphical device.
</p>


<h3>References</h3>

<p>Aalen, O.O. (1989). A linear regression model for the analysis of life times.
Statistics in Medicine, 8:907-925.
</p>


<h3>See Also</h3>

<p>aareg
</p>

<hr>
<h2 id='plot.cox.zph'>
Graphical Test of Proportional Hazards 
</h2><span id='topic+plot.cox.zph'></span>

<h3>Description</h3>

<p>Displays a graph of the scaled Schoenfeld residuals, along with a smooth curve. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cox.zph'
plot(x, resid=TRUE, se=TRUE, df=4, nsmo=40, var,
        xlab="Time", ylab, lty=1:2, col=1, lwd=1, hr=FALSE, plot=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cox.zph_+3A_x">x</code></td>
<td>

<p>result of the <code>cox.zph</code> function. 
</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_resid">resid</code></td>
<td>

<p>a logical value, if <code>TRUE</code> the residuals are included on the plot, as well as the smooth fit. 
</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_se">se</code></td>
<td>

<p>a logical value, if <code>TRUE</code>, confidence bands at two standard errors 
will be added. 
</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_df">df</code></td>
<td>

<p>the degrees of freedom for the fitted natural spline, <code>df=2</code> leads 
to a linear fit. 
</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_nsmo">nsmo</code></td>
<td>
<p>number of points to use for the lines</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_var">var</code></td>
<td>

<p>the set of variables for which plots are desired.  By default, plots are 
produced in turn for each variable of a model.  Selection of a single variable 
allows other features to be added to the plot, e.g., a horizontal line at 
zero or a main title. 
</p>
<p>This has been superseded by a subscripting method; see the example below. 
</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_hr">hr</code></td>
<td>
<p>if TRUE, label the y-axis using the estimated hazard ratio
rather than the estimated coefficient.  (The plot does not change,
only the axis label.)</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_xlab">xlab</code></td>
<td>
<p>label for the x-axis of the plot</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_ylab">ylab</code></td>
<td>
<p>optional label for the y-axis of the plot.  If missing a
default label is provided.  This can be a vector of labels.</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_lty">lty</code>, <code id="plot.cox.zph_+3A_col">col</code>, <code id="plot.cox.zph_+3A_lwd">lwd</code></td>
<td>
<p>line type, color, and line width for the overlaid
curve.  Each of these can be vector of length 2, in which case the
second element is used for the confidence interval.</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_plot">plot</code></td>
<td>
<p>if FALSE, return a list containing the x and y values of
the curve, instead of drawing a plot</p>
</td></tr>
<tr><td><code id="plot.cox.zph_+3A_...">...</code></td>
<td>

<p>additional graphical arguments passed to the <code>plot</code> function. 
</p>
</td></tr>
</table>


<h3>Side Effects</h3>

<p>a plot is produced on the current graphics device. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>,  <code><a href="#topic+cox.zph">cox.zph</a></code>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vfit &lt;- coxph(Surv(time,status) ~ trt + factor(celltype) + 
              karno + age, data=veteran, x=TRUE) 
temp &lt;- cox.zph(vfit) 
plot(temp, var=3)      # Look at Karnofsy score, old way of doing plot 
plot(temp[3])     # New way with subscripting 
abline(0, 0, lty=3) 
# Add the linear fit as well  
abline(lm(temp$y[,3] ~ temp$x)$coefficients, lty=4, col=3)  
title(main="VA Lung Study") 
</code></pre>

<hr>
<h2 id='plot.survfit'>
Plot method for <code>survfit</code> objects 
</h2><span id='topic+plot.survfit'></span>

<h3>Description</h3>

<p>A plot of survival curves is produced, one curve for each strata. 
The <code>log=T</code> option does extra work to avoid log(0), and to try to create a 
pleasing result.  If there are zeros, they are plotted by default at 
0.8 times the smallest non-zero value on the curve(s).
</p>
<p>Curves are plotted in the same order as they are listed by <code>print</code>
(which gives a 1 line summary of each).
This will be the order in which <code>col</code>, <code>lty</code>, etc are used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survfit'
plot(x, conf.int=, mark.time=FALSE, 
 pch=3, col=1, lty=1, lwd=1, cex=1, log=FALSE, xscale=1, yscale=1,  
 xlim, ylim, xmax, fun, 
 xlab="", ylab="", xaxs="r",  conf.times, conf.cap=.005,
 conf.offset=.012,
 conf.type = c("log", "log-log", "plain", "logit", "arcsin"),
 mark, noplot="(s0)", cumhaz=FALSE,
 firstx, ymin, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.survfit_+3A_x">x</code></td>
<td>

<p>an object of class <code>survfit</code>, usually returned by the
<code>survfit</code> function. 
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_conf.int">conf.int</code></td>
<td>

<p>determines whether pointwise confidence intervals will be plotted.
The default is to 
do so if there is only 1 curve, i.e., no strata, using 95% confidence
intervals
Alternatively, this can be a numeric value giving the desired
confidence level.
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_mark.time">mark.time</code></td>
<td>

<p>controls the labeling of the curves.  If set to <code>FALSE</code>, no
labeling is done. 
If <code>TRUE</code>, then curves are marked at each censoring time.
If <code>mark</code> is a
numeric vector then curves are marked at the specified time points. 
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_pch">pch</code></td>
<td>

<p>vector of characters which will be used to label the curves. 
The <code>points</code> help file contains examples of the possible marks.
A single string such as &quot;abcd&quot; is treated as a vector
<code>c("a", "b", "c", "d")</code>.
The vector is reused cyclically if it is shorter than the number of
curves.  If it is present this implies <code>mark.time = TRUE</code>.
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_col">col</code></td>
<td>

<p>a vector of integers specifying colors for each curve. 
The default value is 1. 
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_lty">lty</code></td>
<td>

<p>a vector of integers specifying line types for each curve. 
The default value is 1. 
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_lwd">lwd</code></td>
<td>

<p>a vector of numeric values for line widths. The default value is 1. 
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_cex">cex</code></td>
<td>

<p>a numeric value specifying the size of the marks. 
This is not treated as a vector; all marks have the same size.
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_log">log</code></td>
<td>

<p>a logical value, if TRUE the y axis wll be on a log scale. 
Alternately, one of the standard character strings &quot;x&quot;, &quot;y&quot;, or &quot;xy&quot;
can be given to specific logarithmic horizontal and/or vertical axes.
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_xscale">xscale</code></td>
<td>

<p>a numeric value used like <code>yscale</code> for labels on the x axis.  
A value of 365.25 will give labels in years instead of the original days.  
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_yscale">yscale</code></td>
<td>

<p>a numeric value used to multiply the labels on the y axis. 
A value of 100, for instance, would be used to give a percent scale. 
Only the labels are 
changed, not the actual plot coordinates, so that adding a curve with 
&quot;<code>lines(surv.exp(...))</code>&quot;, say,  
will perform as it did without the <code>yscale</code> argument. 
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_xlim">xlim</code>, <code id="plot.survfit_+3A_ylim">ylim</code></td>
<td>
<p>optional limits for the plotting region.
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_xmax">xmax</code></td>
<td>

<p>the maximum horizontal plot coordinate.  This can be used to shrink
the range of a plot.  It shortens the curve before plotting it, so
that unlike using the <code>xlim</code> graphical parameter, warning
messages about out of bounds points are not generated.
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_fun">fun</code></td>
<td>

<p>an arbitrary function defining a transformation of the survival
(or probability in state, or cumulative hazard) curves. 
For example <code>fun=log</code> is an alternative way to draw a log-survival curve 
(but with the axis labeled with log(S) values), 
and <code>fun=sqrt</code> would generate a curve on square root scale. 
Four often used transformations can be specified with a character 
argument instead: <code>"S"</code> gives the usual survival curve,
<code>"log"</code> is the same as using the <code>log=T</code> option, 
<code>"event"</code> or <code>"F"</code> plots the empirical CDF <code class="reqn">F(t)= 1-S(t)</code>
(f(y) = 1-y), and
<code>"cloglog"</code> creates a complimentary log-log survival plot (f(y) = 
log(-log(y)) along with log scale for the x-axis).
The terms <code>"identity"</code> and <code>"surv"</code> are
allowed as synonyms for <code>type="S"</code>.
The argument <code>"cumhaz"</code> causes the cumulative hazard function
to be plotted.
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_xlab">xlab</code></td>
<td>

<p>label given to the x-axis. 
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_ylab">ylab</code></td>
<td>

<p>label given to the y-axis. 
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_xaxs">xaxs</code></td>
<td>

<p>either <code>"S"</code> for a survival curve or a standard x axis style as
listed in <code>par</code>; &quot;r&quot; (regular) is the R default.
Survival curves have historically been displayed with the curve
touching the y-axis,
but not touching the bounding box of the plot on the other 3 sides,
Type <code>"S"</code> accomplishes this by manipulating the plot range and
then using the <code>"i"</code> style internally.
The &quot;S&quot; style is becoming increasingly less common, however.
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_conf.times">conf.times</code></td>
<td>
<p>optional vector of times at which to place a
confidence bar on the curve(s).  If present, these will be used
instead of confidence bands.</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_conf.cap">conf.cap</code></td>
<td>
<p>width of the horizontal cap on top of the confidence
bars; only used if conf.times is used.  A value of 1 is the width of
the plot region.</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_conf.offset">conf.offset</code></td>
<td>
<p>the offset for confidence bars, when there are
multiple curves on the plot.  A value of 1 is the width of the plot
region. If this is a single number then each curve's bars are offset
by this amount from the prior curve's bars, if it is a vector the values are
used directly.</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_conf.type">conf.type</code></td>
<td>

<p>One of <code>"plain"</code>, <code>"log"</code> (the default),
<code>"log-log"</code> or <code>"logit"</code>.  Only
enough of the string to uniquely identify it is necessary.
The first option causes confidence intervals not to be
generated.  The second causes the standard intervals
<code>curve +- k *se(curve)</code>, where k is determined from
<code>conf.int</code>.  The log option calculates intervals based on the
cumulative hazard or log(survival). The log-log option bases the
intervals on the log hazard or log(-log(survival)), and the
logit option on log(survival/(1-survival)). 
</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_mark">mark</code></td>
<td>
<p>a historical alias for <code>pch</code></p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_noplot">noplot</code></td>
<td>
<p>for multi-state models, curves with this label will not
be plotted.  (Also see the <code>istate0</code> argument in
<code>survcheck</code>.)</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_cumhaz">cumhaz</code></td>
<td>
<p>plot the cumulative hazard rather than the probability
in state or survival.  Optionally, this can be a numeric vector
specifying which columns of the <code>cumhaz</code> component to plot.</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_ymin">ymin</code></td>
<td>
<p> this will normally be given as part of the <code>ylim</code>
argument</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_firstx">firstx</code></td>
<td>
<p>this will normally be given as part of the <code>xlim</code>
argument.</p>
</td></tr>
<tr><td><code id="plot.survfit_+3A_...">...</code></td>
<td>
<p>other arguments that will be passed forward to the
underlying plot method, such as xlab or ylab.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the object contains a cumulative hazard curve, then
<code>fun='cumhaz'</code> will plot that curve, otherwise it will plot
-log(S) as an approximation.  Theoretically, S =
<code class="reqn">\exp(-\Lambda)</code> where S is the survival and
<code class="reqn">\Lambda</code> is the cumulative hazard.  The same relationship
holds for estimates of S and <code class="reqn">\Lambda</code> only in special cases,
but the approximation is often close.
</p>
<p>When the <code>survfit</code> function creates a multi-state survival curve
the resulting object also has class &lsquo;survfitms&rsquo;.
Competing risk curves are a common case.
In this situation the <code>fun</code> argument is ignored.
</p>
<p>When the <code>conf.times</code> argument is used, the confidence bars are
offset by <code>conf.offset</code> units to avoid overlap.
The bar on each curve are the confidence interval for the time point
at which the bar is drawn, i.e., different time points for each curve.
If curves are steep at that point, the visual impact can sometimes
substantially differ for positive and negative values of
<code>conf.offset</code>.
</p>


<h3>Value</h3>

<p>a list with components <code>x</code> and <code>y</code>, containing the coordinates of the last point 
on each of the curves (but not the confidence limits).   
This may be useful for labeling. 
</p>


<h3>Note</h3>

<p>In prior versions the behavior of <code>xscale</code> and
<code>yscale</code> differed: the first changed the scale both for the plot
and for all subsequent actions such as adding a legend, whereas <code>yscale</code>
affected only the axis label.  This was normalized in version 2-36.4,
and both parameters now only affect the labeling.
</p>
<p>In versions prior to approximately 2.36 a <code>survfit</code> object did
not contain the cumulative hazard as a separate result, and the use of
fun=&quot;cumhaz&quot; would plot the approximation -log(surv) to the cumulative
hazard.  When cumulative hazards were added to the object, the
<code>cumhaz=TRUE</code> argument to the plotting function was added.
In version 2.3-8 the use of fun=&quot;cumhaz&quot; became a synonym for
<code>cumhaz=TRUE</code>.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+points.survfit">points.survfit</a></code>,
<code><a href="#topic+lines.survfit">lines.survfit</a></code>,   
<code><a href="graphics.html#topic+par">par</a></code>,  
<code><a href="#topic+survfit">survfit</a></code>  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>leukemia.surv &lt;- survfit(Surv(time, status) ~ x, data = aml) 
plot(leukemia.surv, lty = 2:3) 
legend(100, .9, c("Maintenance", "No Maintenance"), lty = 2:3) 
title("Kaplan-Meier Curves\nfor AML Maintenance Study") 
lsurv2 &lt;- survfit(Surv(time, status) ~ x, aml, type='fleming') 
plot(lsurv2, lty=2:3, fun="cumhaz", 
	xlab="Months", ylab="Cumulative Hazard") 
</code></pre>

<hr>
<h2 id='predict.coxph'>
Predictions for a Cox model 
</h2><span id='topic+predict.coxph'></span><span id='topic+predict.coxph.penal'></span>

<h3>Description</h3>

<p>Compute fitted values and regression terms for a model fitted by
<code><a href="#topic+coxph">coxph</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coxph'
predict(object, newdata,
type=c("lp", "risk", "expected", "terms", "survival"),
se.fit=FALSE, na.action=na.pass, terms=names(object$assign), collapse,
reference=c("strata", "sample", "zero"),  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.coxph_+3A_object">object</code></td>
<td>

<p>the results of a coxph fit. 
</p>
</td></tr>
<tr><td><code id="predict.coxph_+3A_newdata">newdata</code></td>
<td>

<p>Optional new data at which to do predictions.
If absent predictions are for the  data frame used in the original
fit. 
When coxph has been called with a formula argument created in another
context, i.e., coxph has been called within another function and the
formula was passed as an argument to that function, there can be
problems finding the data set.  See the note below.
</p>
</td></tr>
<tr><td><code id="predict.coxph_+3A_type">type</code></td>
<td>

<p>the type of predicted value. 
Choices are the linear predictor (<code>"lp"</code>), the risk score exp(lp)
(<code>"risk"</code>), 
the expected number of events given the covariates and follow-up time 
(<code>"expected"</code>), and the terms of the linear predictor
(<code>"terms"</code>).
The survival probability for a subject is equal to exp(-expected).
</p>
</td></tr>
<tr><td><code id="predict.coxph_+3A_se.fit">se.fit</code></td>
<td>

<p>if TRUE, pointwise standard errors are produced for the predictions. 
</p>
</td></tr>
<tr><td><code id="predict.coxph_+3A_na.action">na.action</code></td>
<td>

<p>applies only when the <code>newdata</code> argument is present, and defines
the missing value action for the new data.  The default is to include
all observations.
When there is no newdata, then the behavior of missing is dictated by
the na.action option of the original fit.</p>
</td></tr>
<tr><td><code id="predict.coxph_+3A_terms">terms</code></td>
<td>

<p>if type=&quot;terms&quot;, this argument can be used to specify which terms should be 
included; the default is all. 
</p>
</td></tr>
<tr><td><code id="predict.coxph_+3A_collapse">collapse</code></td>
<td>

<p>optional vector of subject identifiers.  
If specified, the output will contain one entry per subject rather than one 
entry per observation. 
</p>
</td></tr>
<tr><td><code id="predict.coxph_+3A_reference">reference</code></td>
<td>
<p>reference for centering predictions, see details below</p>
</td></tr>
<tr><td><code id="predict.coxph_+3A_...">...</code></td>
<td>
<p>For future methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Cox model is a <em>relative</em> risk model; predictions
of type &quot;linear predictor&quot;, &quot;risk&quot;, and &quot;terms&quot; are all
relative to the sample from which they came.  By default, the reference
value for each of these is the mean covariate within strata.
The underlying reason is both statistical and practial.
First, a Cox model only predicts relative risks
between pairs of subjects within the same strata, and hence the addition
of a constant to any covariate, either overall or only within a
particular stratum, has no effect on the fitted results.
Second, downstream calculations depend on the risk score exp(linear
predictor), which will fall prey to numeric overflow for a linear
predictor greater than <code>.Machine\$double.max.exp</code>.
The <code>coxph</code> routines try to approximately center the predictors out
of self protection.  
Using the <code>reference="strata"</code> option is the safest centering,
since strata occassionally have different means.
When the results of <code>predict</code> are used in further calculations it
may be desirable to use a single reference level for all observations.
Use of <code>reference="sample"</code> will use <code>object$means</code>, which agrees
with the <code>linear.predictors</code> component of the coxph object.
Predictions of <code>type="terms"</code> are almost invariably passed
forward to further calculation, so for these we default to using
the sample as the reference.
A reference of <code>"zero"</code> causes no centering to be done.
</p>
<p>Predictions of type &quot;expected&quot; or &quot;survival&quot; incorporate the baseline
hazard and are thus absolute instead of relative; the
<code>reference</code> option has no effect on these.
These values depend on the follow-up time for the future subjects as
well as covariates so the <code>newdata</code> argument needs to include both
the right and <em>left</em> hand side variables from the formula.
(The status variable will not be used, but is required since the
underlying code needs to reconstruct the entire formula.)
</p>
<p>Models that contain a <code>frailty</code> term are a special case: due
to the technical difficulty, when there is a <code>newdata</code> argument the
predictions will always be for a random effect of zero.
</p>
<p>For predictions of type expected a user will normally want to use
<code class="reqn">\Lambda(t_i)</code>, i.e., the cumulative hazard at the individual
follow-up time <code class="reqn">t_i</code>of each individual subject.
This is E in the martingale residual O-E and plays a natural role in
assessments of model validation (Crowson 2016).
For predictions of type survival, on the other hand, a user
will normally want S(tau), where tau is a single pre-specified time
point which is the same for all subjects, e.g., predicted 5 year
survival.  The <code>newdata</code> data set should contain actual survival
time(s) for each subject for the first case, as the survival time variable(s),
and the target time tau in the second case; (0, tau) for (time1, time2) data.
</p>


<h3>Value</h3>

<p>a vector or matrix of predictions, or a list containing the predictions 
(element &quot;fit&quot;) and their standard errors (element &quot;se.fit&quot;) if the se.fit 
option is TRUE. 
</p>


<h3>Note</h3>

<p>Some predictions can be obtained directly from the coxph object, and for
others it is necessary for the routine to have the entirety of the
original data set, e.g., for type = <code>terms</code> or if standard errors
are requested.
This extra information is saved in the coxph object if
<code>model=TRUE</code>, if not the original data is reconstructed.
If it is known that such residuals will be required overall execution will be
slightly faster if the model information is saved. 
</p>
<p>In some cases the reconstruction can fail.
The most common is when coxph has been called inside another function
and the formula was passed as one of the arguments to that enclosing
function.  Another is when the data set has changed between the original
call and the time of the prediction call.
In each of these the simple solution is to add <code>model=TRUE</code> to the
original coxph call.
</p>


<h3>References</h3>

<p>C Crowson, E Atkinson and T Therneau, 
Assessing calibration of prognostic risk scores,
Stat Methods Med Res, 2016.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+predict">predict</a></code>,<code><a href="#topic+coxph">coxph</a></code>,<code><a href="stats.html#topic+termplot">termplot</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>options(na.action=na.exclude) # retain NA in predictions
fit &lt;- coxph(Surv(time, status) ~ age + ph.ecog + strata(inst), lung)
#lung data set has status coded as 1/2
mresid &lt;- (lung$status-1) - predict(fit, type='expected') #Martingale resid 
predict(fit,type="lp")
predict(fit,type="expected")
predict(fit,type="risk",se.fit=TRUE)
predict(fit,type="terms",se.fit=TRUE)

# For someone who demands reference='zero'
pzero &lt;- function(fit)
  predict(fit, reference="sample") + sum(coef(fit) * fit$means, na.rm=TRUE)
</code></pre>

<hr>
<h2 id='predict.survreg'>
Predicted Values for a &lsquo;survreg&rsquo; Object
</h2><span id='topic+predict.survreg'></span><span id='topic+predict.survreg.penal'></span>

<h3>Description</h3>

<p>Predicted values for a <code>survreg</code> object 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survreg'
predict(object, newdata,  
type=c("response", "link", "lp", "linear", "terms", "quantile",  
	"uquantile"),
 se.fit=FALSE, terms=NULL, p=c(0.1, 0.9), na.action=na.pass, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.survreg_+3A_object">object</code></td>
<td>

<p>result of a model fit using the <code>survreg</code> function. 
</p>
</td></tr>
<tr><td><code id="predict.survreg_+3A_newdata">newdata</code></td>
<td>

<p>data for prediction.  If absent predictions are for the 
subjects used in the original fit. 
</p>
</td></tr>
<tr><td><code id="predict.survreg_+3A_type">type</code></td>
<td>

<p>the type of predicted value.  
This can be on the original scale of the data (response), 
the linear predictor (<code>"linear"</code>, with <code>"lp"</code> as an allowed abbreviation), 
a predicted quantile on the original scale of the data (<code>"quantile"</code>), 
a quantile on the linear predictor scale (<code>"uquantile"</code>),
or the matrix of terms for the linear predictor (<code>"terms"</code>).
At this time <code>"link"</code> and linear predictor (<code>"lp"</code>) are identical. 
</p>
</td></tr>
<tr><td><code id="predict.survreg_+3A_se.fit">se.fit</code></td>
<td>

<p>if <code>TRUE</code>, include the standard errors of the prediction in the result. 
</p>
</td></tr>
<tr><td><code id="predict.survreg_+3A_terms">terms</code></td>
<td>

<p>subset of terms.  The default for residual type <code>"terms"</code> is a matrix with 
one column for every term (excluding the intercept) in the model. 
</p>
</td></tr>
<tr><td><code id="predict.survreg_+3A_p">p</code></td>
<td>

<p>vector of percentiles.  This is used only for quantile predictions. 
</p>
</td></tr>
<tr><td><code id="predict.survreg_+3A_na.action">na.action</code></td>
<td>

<p>applies only when the <code>newdata</code> argument is present, and defines
the missing value action for the new data.  The default is to include
all observations.</p>
</td></tr>
<tr><td><code id="predict.survreg_+3A_...">...</code></td>
<td>
<p>for future methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector or matrix of predicted values. 
</p>


<h3>References</h3>

<p>Escobar and Meeker (1992). Assessing influence in regression analysis with 
censored data. <em>Biometrics,</em> 48, 507-528. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survreg">survreg</a></code>,  <code><a href="#topic+residuals.survreg">residuals.survreg</a></code>   
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Draw figure 1 from Escobar and Meeker, 1992.
fit &lt;- survreg(Surv(time,status) ~ age + I(age^2), data=stanford2, 
	dist='lognormal') 
with(stanford2, plot(age, time, xlab='Age', ylab='Days', 
	xlim=c(0,65), ylim=c(.1, 10^5), log='y', type='n'))
with(stanford2, points(age, time, pch=c(2,4)[status+1], cex=.7))
pred &lt;- predict(fit, newdata=list(age=1:65), type='quantile', 
	         p=c(.1, .5, .9)) 
matlines(1:65, pred, lty=c(2,1,2), col=1) 

# Predicted Weibull survival curve for a lung cancer subject with
#  ECOG score of 2
lfit &lt;- survreg(Surv(time, status) ~ ph.ecog, data=lung)
pct &lt;- 1:98/100   # The 100th percentile of predicted survival is at +infinity
ptime &lt;- predict(lfit, newdata=data.frame(ph.ecog=2), type='quantile',
                 p=pct, se=TRUE)
matplot(cbind(ptime$fit, ptime$fit + 2*ptime$se.fit,
                         ptime$fit - 2*ptime$se.fit)/30.5, 1-pct,
        xlab="Months", ylab="Survival", type='l', lty=c(1,2,2), col=1)
</code></pre>

<hr>
<h2 id='print.aareg'>
Print an aareg object
</h2><span id='topic+print.aareg'></span>

<h3>Description</h3>

<p>Print out a fit of Aalen's additive regression model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'aareg'
print(x, maxtime, test=c("aalen", "nrisk"),scale=1,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.aareg_+3A_x">x</code></td>
<td>

<p>the result of a call to the <code>aareg</code> function
</p>
</td></tr>
<tr><td><code id="print.aareg_+3A_maxtime">maxtime</code></td>
<td>

<p>the upper time point to be used in the test for non-zero slope
</p>
</td></tr>
<tr><td><code id="print.aareg_+3A_test">test</code></td>
<td>

<p>the weighting to be used in the test for non-zero slope.
The default weights are based on the variance of each coefficient, as
a function of time.  The alternative weight is proportional to the number
of subjects still at risk at each time point.
</p>
</td></tr>
<tr><td><code id="print.aareg_+3A_scale">scale</code></td>
<td>
<p>scales the coefficients.  
For some data sets, the coefficients of the Aalen model will be very
small (10-4); this simply multiplies the printed values by a constant,
say 1e6, to make the printout easier to read.</p>
</td></tr>
<tr><td><code id="print.aareg_+3A_...">...</code></td>
<td>
<p>for future methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimated increments in the coefficient estimates can become quite
unstable near the end of follow-up, due to the small number of observations
still at risk in a data set.
Thus, the test for slope will sometimes be more powerful if this last
&lsquo;tail&rsquo; is excluded.
</p>


<h3>Value</h3>

<p>the calling argument is returned.
</p>


<h3>Side Effects</h3>

<p>the results of the fit are displayed.
</p>


<h3>References</h3>

<p>Aalen, O.O. (1989). A linear regression model for the analysis of life times.
Statistics in Medicine, 8:907-925.
</p>


<h3>See Also</h3>

<p>aareg
</p>

<hr>
<h2 id='print.summary.coxph'>
Print method for summary.coxph objects
</h2><span id='topic+print.summary.coxph'></span>

<h3>Description</h3>

<p>Produces a printed summary of a fitted coxph model 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.coxph'
print(x, digits=max(getOption("digits") - 3, 3),
  signif.stars = getOption("show.signif.stars"), expand=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.coxph_+3A_x">x</code></td>
<td>

<p>the result of a call to <code>summary.coxph</code> 
</p>
</td></tr>
<tr><td><code id="print.summary.coxph_+3A_digits">digits</code></td>
<td>
<p>significant digits to print</p>
</td></tr>
<tr><td><code id="print.summary.coxph_+3A_signif.stars">signif.stars</code></td>
<td>

<p>Show stars to highlight small p-values
</p>
</td></tr>
<tr><td><code id="print.summary.coxph_+3A_expand">expand</code></td>
<td>
<p>if the summary is for a multi-state coxph fit, print the
results in an expanded format.</p>
</td></tr>
<tr><td><code id="print.summary.coxph_+3A_...">...</code></td>
<td>
<p>For future methods</p>
</td></tr>
</table>

<hr>
<h2 id='print.summary.survexp'>Print Survexp Summary</h2><span id='topic+print.summary.survexp'></span>

<h3>Description</h3>

<p>Prints the results of <code>summary.survexp</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.survexp'
print(x, digits = max(options()$digits - 4, 3), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.survexp_+3A_x">x</code></td>
<td>

<p>an object of class <code>summary.survexp</code>.
</p>
</td></tr>
<tr><td><code id="print.summary.survexp_+3A_digits">digits</code></td>
<td>

<p>the number of digits to use in printing the result.
</p>
</td></tr>
<tr><td><code id="print.summary.survexp_+3A_...">...</code></td>
<td>
<p>for future methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code>, with the invisible flag set to prevent further printing.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code>link{summary.survexp}</code>, <code><a href="#topic+survexp">survexp</a></code></p>

<hr>
<h2 id='print.summary.survfit'>
Print Survfit Summary 
</h2><span id='topic+print.summary.survfit'></span>

<h3>Description</h3>

<p>Prints the result of <code>summary.survfit</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.survfit'
print(x, digits = max(options() $digits-4, 3), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.survfit_+3A_x">x</code></td>
<td>

<p>an object of class <code>"summary.survfit"</code>, which is the result of the 
<code>summary.survfit</code> function. 
</p>
</td></tr>
<tr><td><code id="print.summary.survfit_+3A_digits">digits</code></td>
<td>

<p>the number of digits to use in printing the numbers. 
</p>
</td></tr>
<tr><td><code id="print.summary.survfit_+3A_...">...</code></td>
<td>
<p>for future methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code>, with the invisible flag set to prevent printing. 
</p>


<h3>Side Effects</h3>

<p>prints the summary created by <code>summary.survfit</code>. 
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+options">options</a></code>,  <code><a href="base.html#topic+print">print</a></code>,  <code><a href="#topic+summary.survfit">summary.survfit</a></code>.   
</p>

<hr>
<h2 id='print.survfit'>
Print a Short Summary of a Survival Curve 
</h2><span id='topic+print.survfit'></span>

<h3>Description</h3>

<p>Print number of observations, number of events, the restricted
mean survival and its  
standard error, and the median survival with confidence limits for the  
median. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survfit'
print(x, scale=1, digits = max(options()$digits - 4,3),
    print.rmean=getOption("survfit.print.rmean"),
    rmean = getOption('survfit.rmean'),...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.survfit_+3A_x">x</code></td>
<td>

<p>the result of a call to the <code>survfit</code> function. 
</p>
</td></tr>
<tr><td><code id="print.survfit_+3A_scale">scale</code></td>
<td>

<p>a numeric value to rescale the survival time, e.g., 
if the input data to survfit were in days, 
<code>scale=365</code> would scale the printout to years. 
</p>
</td></tr>
<tr><td><code id="print.survfit_+3A_digits">digits</code></td>
<td>
<p>Number of digits to print</p>
</td></tr>
<tr><td><code id="print.survfit_+3A_print.rmean">print.rmean</code>, <code id="print.survfit_+3A_rmean">rmean</code></td>
<td>
<p>Options for computation and display of the
restricted mean.</p>
</td></tr>
<tr><td><code id="print.survfit_+3A_...">...</code></td>
<td>
<p>for future results</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The mean and its variance are based on a truncated estimator.  That is, if the 
last observation(s) is not a death, then the survival curve estimate does not 
go to zero and the mean is undefined.
There are four possible approaches to resolve this, which are selected by the
<code>rmean</code> option.
The first is to set the upper limit to a constant, e.g.,<code>rmean=365</code>.
In this case the reported mean would be the expected number of days, out
of the first 365, that would be experienced by each group.  This is
useful if interest focuses on a fixed period.
Other options are <code>"none"</code> (no estimate), <code>"common"</code> and
<code>"individual"</code>.
The <code>"common"</code> option uses the maximum time for all curves in the
object as a common upper limit for the auc calculation.
For the <code>"individual"</code>options the mean is computed as the area
under each curve,
over the range from 0 to the maximum observed time for that curve.
Since the end point is random, values for different curves are not
comparable and the printed standard errors are an underestimate as
they do not take into account this random variation.  This option is
provided mainly for backwards compatability, as this estimate was the
default (only) one in earlier releases of the code.
Note that SAS (as of version 9.3) uses the integral up to the last
<em>event</em> time of each individual curve; we consider this the worst
of the choices and do not provide an option for that calculation.
</p>
<p>The median and its confidence interval are defined by drawing a horizontal 
line at 0.5 on the plot of the survival curve and its confidence bands.
If that line does not intersect the curve, then the median is undefined.
The intersection of the line with the lower CI band defines the lower limit 
for the median's interval, and similarly for the upper band.  If any of the 
intersections is not a point then we use the center of the intersection interval, 
e.g., if the survival curve were exactly equal to 0.5 over an interval.
When data is uncensored this agrees with the usual definition of a median.
</p>


<h3>Value</h3>

<p>x, with the invisible flag set to prevent printing.
(The default for all print functions in R is to return the object
passed to them; print.survfit complies with this pattern.  If you want to
capture these printed results for further processing, see the
<code>table</code> component of <code>summary.survfit</code>.)  
</p>


<h3>Side Effects</h3>

<p>The number of observations, the number of events,  the median survival
with its confidence interval, and optionally the
restricted mean survival (<code>rmean</code>) and its standard error, are printed.
If there are multiple curves, there is one line of output for each. 
</p>


<h3>References</h3>

<p>Miller, Rupert G., Jr. (1981).  
<em>Survival Analysis.</em>
New York:Wiley, p 71. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.survfit">summary.survfit</a></code>, <code><a href="#topic+quantile.survfit">quantile.survfit</a></code> 
</p>

<hr>
<h2 id='pseudo'>
Pseudo values for survival.
</h2><span id='topic+pseudo'></span>

<h3>Description</h3>

<p>Produce pseudo values from a survival curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pseudo(fit, times, type, collapse= TRUE, data.frame=FALSE, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pseudo_+3A_fit">fit</code></td>
<td>
<p>a <code>survfit</code> object, or one that inherits that class.   
</p>
</td></tr>
<tr><td><code id="pseudo_+3A_times">times</code></td>
<td>

<p>a vector of time points, at which to evaluate the pseudo values.
</p>
</td></tr>
<tr><td><code id="pseudo_+3A_type">type</code></td>
<td>

<p>the type of value, either the probabilty in state <code>pstate</code>,
the cumulative hazard <code>cumhaz</code> or the expected sojourn time in
the state <code>sojourn</code>.  
</p>
</td></tr>
<tr><td><code id="pseudo_+3A_collapse">collapse</code></td>
<td>

<p>if the original survfit call had an <code>id</code> variable, return one
residual per unique id
</p>
</td></tr>
<tr><td><code id="pseudo_+3A_data.frame">data.frame</code></td>
<td>
<p>if TRUE, return the data in &quot;long&quot; form as a
data.frame with id, state (or transition), curve, time, residual
and pseudo as variables.</p>
</td></tr>
<tr><td><code id="pseudo_+3A_...">...</code></td>
<td>

<p>other arguments to the <code>residuals.survfit</code> function, which does
the majority of the work, e.g.,  <code>weighted</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes pseudo values based on a first order Taylor
series, also known as the &quot;infinitesimal jackknife&quot; (IJ) or &quot;dfbeta&quot;
residuals.  To be completely correct the results of this function
could perhaps be
called &lsquo;IJ pseudo values&rsquo; or even pseudo psuedo-values.
For moderate to large data, however, the resulta will
be almost identical, numerically, to the ordinary jackknife.
</p>
<p>A primary advantage of this approach is computational speed.
Other features, neither good nor bad, are that they will agree with
robust standard errors of other survival package estimates,
which are based on the IJ, and that the mean of the estimates, over
subjects, is exactly the underlying survival estimate.
</p>
<p>For the <code>type</code> variable, <code>surv</code> is an acceptable synonym for
<code>pstate</code>, <code>chaz</code> for <code>cumhaz</code>, and
<code>rmst</code>,<code>rmts</code>
and <code>auc</code> are equivalent to <code>sojourn</code>.
All of these are case insensitive.
</p>
<p>If the orginal <code>survfit</code> call produced multiple curves, the internal
computations are done separately for each curve.
The result from this routine is simply n times the IJ value, where n is
the number of uniue subjects in the respective curve.
(If the the <code>survfit</code> call included and <code>id</code> option, n is
the number of unique id values, otherwise the number of rows in the data set.) 
IJ values are well defined for all variants of the Aalen-Johansen
estimate, as computed by the <code>survfit</code> function; indeed, they are
the basis for standard errors of the result.
</p>
<p>Understanding of the properties of the pseudo-values is still
evolving.  Validity has been verified for the probability in state and
sojourn times whenever all subjects start in the same state;
this includes for instance the usual Kaplan-Meier and competing risks cases.
On the other hand, one must be cautious when the data includes
left-truncation (Parner); and pseudo-values for the cumulative hazard
have not been widely explored.
When a given subject is spread across multiple (time1, time2) windows
with different weights for each of those portions, which can happen
with time-dependent inverse probability of censoring (IPW) weights for
instance, the current thought is to set both collapse and weight to
FALSE, with clustering and weighting as part of the subsequent GEE
model; but this is quite tentative.
As understanding evolves, treat this routine's results as a reseach
tool, not production, for these more complex models.
</p>


<h3>Value</h3>

<p>A vector, matrix, or array.  The first dimension is always the number of
observations in <code>fit</code> object, in the same order as the original
data set (less any missing values that were removed when creating the
survfit object);
the second, if applicable, corresponds to <code>fit$states</code>, e.g.,
multi-state
survival, and the last dimension to the selected time points.
(If there are multiple rows for a given id, there is only one
pseudovalue per unique id.)
</p>
<p>For the data.frame option, a data frame containing values for id,
time, and pseudo.  If the original <code>survfit</code> call contained an
<code>id</code> statement, then the values in the <code>id</code> column will be
taken from that variable.  If the <code>id</code> statement has a simple
form, e.g., <code>id = patno</code>, then the name of the id column will
be &lsquo;patno&rsquo;, otherwise it will be named &lsquo;(id)&rsquo;.
</p>


<h3>Note</h3>

<p>The code will be slightly faster if the <code>model=TRUE</code> option is
used in the <code>survfit</code> call.  It may be essential if the
survfit/pseudo pair is used inside another function.
</p>


<h3>References</h3>

<p>PK Andersen and M Pohar-Perme, Pseudo-observations in surivival
analysis, Stat Methods Medical Res, 2010; 19:71-99
</p>
<p>ET Parner, PK Andersen and M Overgaard, Regression models for censored
time-to-event data using infinitesimal jack-knife pseudo-observations, with
applications to left-truncation, Lifetime Data Analysis, 2023, 29:654-671
</p>


<h3>See Also</h3>

<p><code><a href="#topic+residuals.survfit">residuals.survfit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit1 &lt;- survfit(Surv(time, status) ~ 1, data=lung)
yhat &lt;- pseudo(fit1, times=c(365, 730))
dim(yhat)
lfit &lt;- lm(yhat[,1] ~ ph.ecog + age + sex, data=lung)

# Restricted Mean Time in State (RMST) 
rms &lt;- pseudo(fit1, times= 730, type='RMST') # 2 years
rfit &lt;- lm(rms ~ ph.ecog + sex, data=lung)
rhat &lt;- predict(rfit, newdata=expand.grid(ph.ecog=0:3, sex=1:2), se.fit=TRUE)
# print it out nicely
temp1 &lt;- cbind(matrix(rhat$fit, 4,2))
temp2 &lt;- cbind(matrix(rhat$se.fit, 4, 2))
temp3 &lt;- cbind(temp1[,1], temp2[,1], temp1[,2], temp2[,2])
dimnames(temp3) &lt;- list(paste("ph.ecog", 0:3), 
                        c("Male RMST", "(se)", "Female RMST", "(se)"))

round(temp3, 1)
# compare this to the fully non-parametric estimate
fit2 &lt;- survfit(Surv(time, status) ~ ph.ecog, data=lung)
print(fit2, rmean=730)
# the estimate for ph.ecog=3 is very unstable (n=1), pseudovalues smooth it.
#
# In all the above we should be using the robust variance, e.g., svyglm, but
#  a recommended package can't depend on external libraries.
# See the vignette for a more complete exposition.
</code></pre>

<hr>
<h2 id='pspline'>Smoothing splines using a pspline basis</h2><span id='topic+pspline'></span><span id='topic+psplineinverse'></span>

<h3>Description</h3>

<p>Specifies a penalised spline basis for the predictor.
This is done by fitting a comparatively small set of splines and
penalising the integrated second derivative.
Traditional smoothing splines use one basis per observation, but several
authors have pointed out that the final results of the fit are 
indistinguishable for any number of basis functions greater than about 
2-3 times the degrees of freedom. 
Eilers and Marx point out that if the basis functions are evenly spaced,
this leads to significant computational simplification, they refer
to the result as a p-spline.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pspline(x, df=4, theta, nterm=2.5 * df, degree=3, eps=0.1, method,
   Boundary.knots=range(x), intercept=FALSE, penalty=TRUE, combine, ...)

psplineinverse(x)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pspline_+3A_x">x</code></td>
<td>
<p>for psline: a covariate vector.
The function does not apply to factor variables.
For psplineinverse x will be the result of a pspline call.</p>
</td></tr>
<tr><td><code id="pspline_+3A_df">df</code></td>
<td>
<p>the desired degrees of freedom.
One of the arguments <code>df</code> or <code>theta</code>' must be given, but not both.
If <code>df=0</code>, then the AIC = (loglik -df) is used to choose an
&quot;optimal&quot; degrees of freedom.  If AIC is chosen, then an optional
argument &lsquo;caic=T&rsquo; can be used to specify the corrected AIC of
Hurvich et. al.
</p>
</td></tr>
<tr><td><code id="pspline_+3A_theta">theta</code></td>
<td>
<p>roughness penalty for the fit.
It is a monotone function of the degrees of freedom, with theta=1
corresponding to a linear fit and theta=0 to an unconstrained fit
of nterm degrees of freedom.
</p>
</td></tr>
<tr><td><code id="pspline_+3A_nterm">nterm</code></td>
<td>
<p> number of splines in the basis </p>
</td></tr>
<tr><td><code id="pspline_+3A_degree">degree</code></td>
<td>
<p> degree of splines </p>
</td></tr>
<tr><td><code id="pspline_+3A_eps">eps</code></td>
<td>
<p>accuracy for <code>df</code> </p>
</td></tr>
<tr><td><code id="pspline_+3A_method">method</code></td>
<td>
<p>the method for choosing the tuning parameter <code>theta</code>.
If theta is given, then 'fixed' is assumed.
If the degrees of freedom is given, then 'df' is assumed.
If method='aic' then the degrees of freedom is chosen automatically using
Akaike's information criterion.</p>
</td></tr>
<tr><td><code id="pspline_+3A_...">...</code></td>
<td>
<p>optional arguments to the control function</p>
</td></tr>
<tr><td><code id="pspline_+3A_boundary.knots">Boundary.knots</code></td>
<td>
<p>the spline is linear beyond the boundary knots.
These default to the range of the data.</p>
</td></tr>
<tr><td><code id="pspline_+3A_intercept">intercept</code></td>
<td>
<p>if TRUE, the basis functions include the intercept.</p>
</td></tr>
<tr><td><code id="pspline_+3A_penalty">penalty</code></td>
<td>
<p>if FALSE a large number of attributes having to do with
penalized fits are excluded.  This is useful to create a pspline
basis matrix for other uses.</p>
</td></tr>
<tr><td><code id="pspline_+3A_combine">combine</code></td>
<td>
<p>an optional vector of increasing integers.  If two
adjacent values of <code>combine</code> are equal, then the corresponding
coefficients of the fit are forced to be equal.  This is useful for
monotone fits, see the vignette for more details.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>pspline, coxph.penalty</code> containing the spline basis,
with the appropriate attributes to be
recognized as a penalized term by the coxph or survreg functions.
</p>
<p>For psplineinverse the original x vector is reconstructed.
</p>


<h3>References</h3>

<p>Eilers, Paul H. and Marx, Brian D. (1996).
Flexible smoothing with B-splines and penalties.
Statistical Science, 11, 89-121.
</p>
<p>Hurvich, C.M. and Simonoff, J.S. and Tsai, Chih-Ling (1998).
Smoothing parameter selection in nonparametric regression using
an improved Akaike information criterion,
JRSSB, volume 60, 271&ndash;293.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>,<code><a href="#topic+survreg">survreg</a></code>,<code><a href="#topic+ridge">ridge</a></code>,
<code><a href="#topic+frailty">frailty</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lfit6 &lt;- survreg(Surv(time, status)~pspline(age, df=2), lung)
plot(lung$age, predict(lfit6), xlab='Age', ylab="Spline prediction")
title("Cancer Data")
fit0 &lt;- coxph(Surv(time, status) ~ ph.ecog + age, lung)
fit1 &lt;- coxph(Surv(time, status) ~ ph.ecog + pspline(age,3), lung)
fit3 &lt;- coxph(Surv(time, status) ~ ph.ecog + pspline(age,8), lung)
fit0
fit1
fit3
</code></pre>

<hr>
<h2 id='pyears'>
Person Years 
</h2><span id='topic+pyears'></span>

<h3>Description</h3>

<p>This function computes the person-years of follow-up time contributed by a 
cohort of subjects, stratified into subgroups. 
It also computes the number of subjects who contribute to each cell of the 
output table, and optionally the number of events and/or expected number of 
events in each cell. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pyears(formula, data, weights, subset, na.action,  rmap,
       ratetable, scale=365.25, expect=c('event', 'pyears'),
       model=FALSE, x=FALSE, y=FALSE, data.frame=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pyears_+3A_formula">formula</code></td>
<td>

<p>a formula object. 
The response variable will be a vector of follow-up times 
for each subject, or a <code>Surv</code> object containing the survival
time and an event indicator. 
The predictors consist of optional grouping variables 
separated by + operators (exactly as in <code>survfit</code>),
time-dependent grouping 
variables such as age (specified with <code>tcut</code>), and optionally a 
<code>ratetable</code> term. 
This latter matches each subject to his/her expected cohort. 
</p>
</td></tr>
<tr><td><code id="pyears_+3A_data">data</code></td>
<td>

<p>a data frame in which to interpret the variables named in 
the <code>formula</code>, or in the <code>subset</code> and the <code>weights</code>
argument. 
</p>
</td></tr>
<tr><td><code id="pyears_+3A_weights">weights</code></td>
<td>

<p>case weights. 
</p>
</td></tr>
<tr><td><code id="pyears_+3A_subset">subset</code></td>
<td>

<p>expression saying that only a subset of the rows of the data 
should be used in the fit. 
</p>
</td></tr>
<tr><td><code id="pyears_+3A_na.action">na.action</code></td>
<td>

<p>a missing-data filter function, applied to the model.frame, after any 
<code>subset</code> argument has been used.
Default is <code>options()$na.action</code>. 
</p>
</td></tr>
<tr><td><code id="pyears_+3A_rmap">rmap</code></td>
<td>

<p>an optional list that maps data set names to the ratetable names.  See
the details section below.
</p>
</td></tr>
<tr><td><code id="pyears_+3A_ratetable">ratetable</code></td>
<td>

<p>a table of event rates, such as <code>survexp.uswhite</code>. 
</p>
</td></tr>
<tr><td><code id="pyears_+3A_scale">scale</code></td>
<td>

<p>a scaling for the results.  As most rate tables are in units/day, the 
default value of 365.25 causes the output to be reported in years. 
</p>
</td></tr>
<tr><td><code id="pyears_+3A_expect">expect</code></td>
<td>

<p>should the output table include the expected number of events, or the
expected number of person-years of observation.  This is only valid with
a rate table.
</p>
</td></tr>
<tr><td><code id="pyears_+3A_data.frame">data.frame</code></td>
<td>

<p>return a data frame rather than a set of arrays.</p>
</td></tr>
<tr><td><code id="pyears_+3A_model">model</code>, <code id="pyears_+3A_x">x</code>, <code id="pyears_+3A_y">y</code></td>
<td>

<p>If any of these is true, then the
model frame, the model matrix, and/or the vector of response times will be
returned as components of the final result.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Because <code>pyears</code> may have several time variables, it is necessary that all 
of them be in the same units.  For instance, in the call 
</p>
<pre>
  py &lt;- pyears(futime ~ rx, rmap=list(age=age, sex=sex, year=entry.dt),
                    ratetable=survexp.us) 
</pre>
<p>the natural unit of the ratetable is hazard per day, it is important that
<code>futime</code>, 
<code>age</code> and <code>entry.dt</code> all be in days. 
Given the wide range of possible inputs, 
it is difficult for the routine to do sanity checks of this aspect. 
</p>
<p>The ratetable being used may have different variable names than the user's
data set, this is dealt with by the <code>rmap</code> argument.  
The rate table for the above calculation was <code>survexp.us</code>, a call to
<code>summary{survexp.us}</code> reveals that it expects to have variables 
<code>age</code> = age in days, <code>sex</code>, and <code>year</code> = the date of study
entry, we create them in the <code>rmap</code> line.  The sex variable is not
mapped, therefore the code assumes that it exists in <code>mydata</code> in the
correct format.  (Note: for factors such as sex, the program will match on
any unique abbreviation, ignoring case.)
</p>
<p>A special function <code>tcut</code> is needed to specify time-dependent cutpoints. 
For instance, assume that age is in years, and that the desired final 
arrays have as one of their margins the age groups 0-2, 2-10, 10-25, and 25+. 
A subject who enters the study at age 4 and remains under observation for 
10 years will contribute follow-up time to both the 2-10 and 10-25 
subsets.  If <code>cut(age, c(0,2,10,25,100))</code> were used in the formula, the 
subject would be classified according to his starting age only. 
The <code>tcut</code> function has the same arguments as <code>cut</code>, 
but produces a different output object which allows the <code>pyears</code> function 
to correctly track the subject. 
</p>
<p>The results of <code>pyears</code> are normally used as input to further calculations. 
The <code>print</code> routine, therefore, is designed to give only a summary of the 
table. 
</p>


<h3>Value</h3>

<p>a list with components: 
</p>
<table>
<tr><td><code>pyears</code></td>
<td>

<p>an array containing the person-years of exposure. (Or other units, depending 
on the rate table and the scale).
The dimension and dimnames of the array correspond to the variables on
the right hand side of the model equation.
</p>
</td></tr>
<tr><td><code>n</code></td>
<td>

<p>an array containing the number of subjects who contribute time to each cell 
of the <code>pyears</code> array. 
</p>
</td></tr>
<tr><td><code>event</code></td>
<td>

<p>an array containing the observed number of events.  This will be present only 
if the response variable is a <code>Surv</code> object. 
</p>
</td></tr>
<tr><td><code>expected</code></td>
<td>

<p>an array containing the expected number of events (or person years if 
<code>expect ="pyears"</code>).  This will be present only if there was a 
<code>ratetable</code> term. 
</p>
</td></tr>
<tr><td><code>data</code></td>
<td>

<p>if the <code>data.frame</code> option was set, a data frame containing the
variables <code>n</code>, <code>event</code>, <code>pyears</code> and <code>event</code> that
supplants the four arrays listed above,
along with variables corresponding to each dimension.
There will be one row for each cell in the arrays.</p>
</td></tr>
<tr><td><code>offtable</code></td>
<td>

<p>the number of person-years of exposure in the cohort that was not part of 
any cell in the <code>pyears</code> array.  This is often useful as an error check;
if 
there is a mismatch of units between two variables, nearly all the person 
years may be off table. 
</p>
</td></tr>
<tr><td><code>tcut</code></td>
<td>
<p>whether the call included any time-dependent cutpoints.</p>
</td></tr>
<tr><td><code>summary</code></td>
<td>

<p>a summary of the rate-table matching. This is also useful as an error 
check. 
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>an image of the call to the function. 
</p>
</td></tr>
<tr><td><code>observations</code></td>
<td>
<p>the number of observations in the input data set,
after any missings were removed.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>

<p>the <code>na.action</code> attribute contributed by an <code>na.action</code>
routine, if any. 
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ratetable">ratetable</a></code>,  <code><a href="#topic+survexp">survexp</a></code>,  <code><a href="#topic+Surv">Surv</a></code>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Look at progression rates jointly by calendar date and age
# 
temp.yr  &lt;- tcut(mgus$dxyr, 55:92, labels=as.character(55:91)) 
temp.age &lt;- tcut(mgus$age, 34:101, labels=as.character(34:100))
ptime &lt;- ifelse(is.na(mgus$pctime), mgus$futime, mgus$pctime)
pstat &lt;- ifelse(is.na(mgus$pctime), 0, 1)
pfit &lt;- pyears(Surv(ptime/365.25, pstat) ~ temp.yr + temp.age + sex,  mgus,
     data.frame=TRUE) 
# Turn the factor back into numerics for regression
tdata &lt;- pfit$data
tdata$age &lt;- as.numeric(as.character(tdata$temp.age))
tdata$year&lt;- as.numeric(as.character(tdata$temp.yr))
fit1 &lt;- glm(event ~ year + age+ sex +offset(log(pyears)),
             data=tdata, family=poisson)
## Not run: 
# fit a gam model 
gfit.m &lt;- gam(y ~ s(age) + s(year) + offset(log(time)),  
                        family = poisson, data = tdata) 

## End(Not run)

# Example #2  Create the hearta data frame: 
hearta &lt;- by(heart, heart$id,  
             function(x)x[x$stop == max(x$stop),]) 
hearta &lt;- do.call("rbind", hearta) 
# Produce pyears table of death rates on the surgical arm
#  The first is by age at randomization, the second by current age
fit1 &lt;- pyears(Surv(stop/365.25, event) ~ cut(age + 48, c(0,50,60,70,100)) + 
       surgery, data = hearta, scale = 1)
fit2 &lt;- pyears(Surv(stop/365.25, event) ~ tcut(age + 48, c(0,50,60,70,100)) + 
       surgery, data = hearta, scale = 1)
fit1$event/fit1$pyears  #death rates on the surgery and non-surg arm

fit2$event/fit2$pyears  #death rates on the surgery and non-surg arm
</code></pre>

<hr>
<h2 id='quantile.survfit'>Quantiles from a survfit object</h2><span id='topic+quantile.survfit'></span><span id='topic+quantile.survfitms'></span><span id='topic+median.survfit'></span>

<h3>Description</h3>

<p>Retrieve quantiles and confidence intervals for them from
a survfit or Surv object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survfit'
quantile(x, probs = c(0.25, 0.5, 0.75), conf.int = TRUE,
  scale, tolerance= sqrt(.Machine$double.eps), ...)
## S3 method for class 'survfitms'
quantile(x, probs = c(0.25, 0.5, 0.75), conf.int = TRUE,
  scale, tolerance= sqrt(.Machine$double.eps), ...)
## S3 method for class 'survfit'
median(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quantile.survfit_+3A_x">x</code></td>
<td>
<p>a result of the survfit function</p>
</td></tr>
<tr><td><code id="quantile.survfit_+3A_probs">probs</code></td>
<td>
<p>numeric vector of probabilities with values in [0,1]</p>
</td></tr>
<tr><td><code id="quantile.survfit_+3A_conf.int">conf.int</code></td>
<td>
<p>should lower and upper confidence limits be returned?</p>
</td></tr>
<tr><td><code id="quantile.survfit_+3A_scale">scale</code></td>
<td>
<p>optional scale factor, e.g., <code>scale=365.25</code> would
return results in years if the fit object were in days.</p>
</td></tr>
<tr><td><code id="quantile.survfit_+3A_tolerance">tolerance</code></td>
<td>
<p>tolerance for checking that the survival curve exactly
equals one of the quantiles</p>
</td></tr>
<tr><td><code id="quantile.survfit_+3A_...">...</code></td>
<td>
<p>optional arguments for other methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kth quantile for a survival curve S(t) is the location at which
a horizontal line at height p= 1-k intersects the plot of S(t).
Since S(t) is a step function, it is possible for the curve to have a
horizontal segment at exactly 1-k, in which case the midpoint of the
horizontal segment is returned.  This mirrors the standard behavior of
the median when data is uncensored.  If the survival curve does not
fall to 1-k, then that quantile is undefined.
</p>
<p>In order to be consistent with other quantile functions, the argument
<code>prob</code> of this function applies to the cumulative distribution
function F(t) = 1-S(t).  
</p>
<p>Confidence limits for the values are based on the intersection of the
horizontal line at 1-k with the upper and lower limits for the
survival curve.  Hence confidence limits use the same
p-value as was in effect when the curve was created, and will differ
depending on the <code>conf.type</code> option of <code>survfit</code>.
If the survival curves have no confidence bands, confidence limits for
the quantiles are not available.
</p>
<p>When a horizontal segment of the survival curve exactly matches one of
the requested quantiles the returned value will be the midpoint of the
horizontal segment; this agrees with the usual definition of a median
for uncensored data.  Since the survival curve is computed as a series
of products, however, there may be round off error.
Assume for instance a sample of size 20 with no tied times and no
censoring.  The survival curve after the 10th death is
(19/20)(18/19)(17/18) ... (10/11) = 10/20, but the computed result will
not be exactly 0.5. Any horizontal segment whose absolute difference
with a requested percentile is less than <code>tolerance</code> is
considered to be an exact match.
</p>


<h3>Value</h3>

<p>The quantiles will be a vector if the <code>survfit</code> object contains
only a single curve, otherwise it will be a matrix or array.  In
this case the last dimension will index the quantiles.
</p>
<p>If confidence limits are requested, then result will be a list with
components
<code>quantile</code>, <code>lower</code>, and <code>upper</code>, otherwise it is the
vector or matrix of quantiles.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit">survfit</a></code>, <code><a href="#topic+print.survfit">print.survfit</a></code>,
<code><a href="#topic+qsurvreg">qsurvreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- survfit(Surv(time, status) ~ ph.ecog, data=lung)
quantile(fit)

cfit &lt;- coxph(Surv(time, status) ~ age + strata(ph.ecog), data=lung)
csurv&lt;- survfit(cfit, newdata=data.frame(age=c(40, 60, 80)),
                  conf.type ="none")
temp &lt;- quantile(csurv, 1:5/10)
temp[2,3,]  # quantiles for second level of ph.ecog, age=80
quantile(csurv[2,3], 1:5/10)  # quantiles of a single curve, same result
</code></pre>

<hr>
<h2 id='ratetable'>Allow ratetable() terms in a model</h2><span id='topic+ratetable'></span>

<h3>Description</h3>

<p>This function supports ratetable() terms in a model
statement, within survexp and pyears.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ratetable(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ratetable_+3A_...">...</code></td>
<td>
<p>the named dimensions of a rate table</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This way of mapping a rate table's variable names to a user data frame
has been superseded, instead use the <code>rmap</code> argument of the
survexp, pyears, or survdiff routines.  The function remains only to
allow older code to be run.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>

<hr>
<h2 id='ratetableDate'>Convert date objects to ratetable form</h2><span id='topic+ratetableDate'></span>

<h3>Description</h3>

<p>This method converts dates from various forms into
the internal form used in <code>ratetable</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ratetableDate(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ratetableDate_+3A_x">x</code></td>
<td>
<p>a date.  The function currently has methods for Date, date,
POSIXt, timeDate, and chron objects.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is useful for those who create new ratetables, but is
normally invisible to users.
It is used internally by the <code>survexp</code> and <code>pyears</code>
functions to map the various date formats; if a new method is added
then those routines will automatically be adapted to the new date type.
</p>


<h3>Value</h3>

<p>a numeric vector, the number of days since 1/1/1960.</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+pyears">pyears</a></code>, <code><a href="#topic+survexp">survexp</a></code></p>

<hr>
<h2 id='ratetables'>
Census Data Sets for the Expected Survival and Person Years Functions
</h2><span id='topic+survexp.us'></span><span id='topic+survexp.usr'></span><span id='topic+survexp.mn'></span>

<h3>Description</h3>

<p>Census data sets for the expected survival and person years functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(survexp, package="survival")
</code></pre>


<h3>Details</h3>


<dl>
<dt>survexp.us</dt><dd>
<p>total United States population, by age and sex, 1940 to 2012.
</p>
</dd>
<dt>survexp.usr</dt><dd>
<p>United States population, by age, sex and race, 1940 to 2014.
Race is white or black.  For 1960 and 1970 the black
population values were not reported separately, so the nonwhite
values were used.  (Over the years, the reported tables have
differed wrt reporting non-white and/or black.)
</p>
</dd>
<dt>survexp.mn</dt><dd>
<p>total Minnesota population, by age and sex, 1970 to 2013.
</p>
</dd>
</dl>

<p>Each of these tables contains the daily hazard rate for a matched
subject from the population, defined as <code class="reqn">-\log(1-q)/365.25</code> where
<code class="reqn">q</code> is the 1 year probability of death as reported in the original
tables from the US Census.  For age 25 in 1970, for instance,
<code class="reqn">p = 1-q</code> is is the
probability that a subject who becomes 25 years of age in 1970 will
achieve his/her 26th birthday.  The tables are recast in terms of
hazard per day for computational convenience.
</p>
<p>Each table is stored as an array, with additional attributes, and
can be subset and manipulated as standard R arrays.
See the help page for <code>ratetable</code> for details.
</p>
<p>All numeric dimensions of a rate table must be in the same units.
The <code>survexp.us</code> rate table contains daily hazard rates, the age
cutpoints are in days, and the calendar year cutpoints are a Date.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ratetable">ratetable</a></code>, <code><a href="#topic+survexp">survexp</a></code>,
<code><a href="#topic+pyears">pyears</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>survexp.uswhite &lt;- survexp.usr[,,"white",]
</code></pre>

<hr>
<h2 id='rats'>Rat treatment data from Mantel et al</h2><span id='topic+rats'></span>

<h3>Description</h3>

<p>Rat treatment data from Mantel et al.
Three rats were chosen from each of 100 litters, one of which was
treated with a drug, and then all followed for tumor incidence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rats
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    litter:</td><td style="text-align: left;"> litter number from 1 to 100</td>
</tr>
<tr>
 <td style="text-align: left;">
    rx:</td><td style="text-align: left;"> treatment,(1=drug, 0=control) </td>
</tr>
<tr>
 <td style="text-align: left;">
    time:</td><td style="text-align: left;"> time to tumor or last follow-up</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> event status, 1=tumor and 0=censored</td>
</tr>
<tr>
 <td style="text-align: left;">
    sex:</td><td style="text-align: left;"> male or female
  </td>
</tr>

</table>



<h3>Note</h3>

<p>Since only 2/150 of the male rats have a tumor, most analyses use
only females (odd numbered litters), e.g. Lee et al.</p>


<h3>Source</h3>

<p>N. Mantel, N. R. Bohidar and J. L. Ciminera.
Mantel-Haenszel analyses of litter-matched time to response data,
with modifications for recovery of interlitter information.
Cancer Research, 37:3863-3868, 1977.  
</p>


<h3>References</h3>

<p>E. W. Lee, L. J. Wei, and D. Amato,
Cox-type regression analysis for large number of small groups of
correlated failure time observations,
in &quot;Survival Analysis, State of the Art&quot;, Kluwer, 1992.
</p>

<hr>
<h2 id='rats2'>Rat data from Gail et al.</h2><span id='topic+rats2'></span>

<h3>Description</h3>

<p>48 rats were injected with a carcinogen, and then
randomized to either drug or placebo.  The number of tumors ranges
from 0 to 13; all rats were  censored at 6 months after randomization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rats2
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    rat:</td><td style="text-align: left;"> id</td>
</tr>
<tr>
 <td style="text-align: left;">
    trt:</td><td style="text-align: left;"> treatment,(1=drug, 0=control) </td>
</tr>
<tr>
 <td style="text-align: left;">
    observation:</td><td style="text-align: left;"> within rat</td>
</tr>
<tr>
 <td style="text-align: left;">
    start:</td><td style="text-align: left;"> entry time</td>
</tr>
<tr>
 <td style="text-align: left;">
    stop:</td><td style="text-align: left;"> exit time</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> event status, 1=tumor, 0=censored</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>MH Gail, TJ Santner, and CC Brown (1980),
An analysis of comparative carcinogenesis experiments based on
multiple times to tumor.
<em>Biometrics</em> <b>36</b>, 255&ndash;266.
</p>

<hr>
<h2 id='reliability'>Reliability data sets</h2><span id='topic+reliability'></span><span id='topic+capacitor'></span><span id='topic+cracks'></span><span id='topic+genfan'></span><span id='topic+ifluid'></span><span id='topic+imotor'></span><span id='topic+turbine'></span><span id='topic+valveSeat'></span>

<h3>Description</h3>

<p>A set of data for simple reliablility analyses, taken from the book by
Meeker and Escobar.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(reliability, package="survival")
</code></pre>


<h3>Details</h3>


<ul>
<li> <p><code>capacitor</code>:
Data from a factorial experiment on the life of glass capacitors as a
function of voltage and operating temperature.  There were 8 capacitors
at each combination of temperature and voltage.
Testing at each combination was terminated after the fourth failure.
</p>

<ul>
<li> <p><code>temperature</code>: temperature in degrees celcius
</p>
</li>
<li> <p><code>voltage</code>: applied voltage
</p>
</li>
<li> <p><code>time</code>: time to failure
</p>
</li>
<li> <p><code>status</code>: 1=failed, 0=censored
</p>
</li></ul>

</li>
<li> <p><code>cracks</code>: Data on the time until the development of cracks
in a set of 167 identical turbine parts.
The parts were inspected at 8 selected times.
</p>

<ul>
<li><p> day: time of inspection
</p>
</li>
<li><p> fail: number of fans found to have cracks, at this inspection
</p>
</li></ul>

</li>
<li><p> Data set <code>genfan</code>: Time to failure of 70 diesel engine fans.
</p>

<ul>
<li> <p><code>hours</code>: hours of service
</p>
</li>
<li> <p><code>status</code>: 1=failure, 0=censored
</p>
</li></ul>

<p>Data set <code>ifluid</code>:
A data frame with two variables describing the time to electrical
breakdown of an insulating fluid.
</p>

<ul>
<li> <p><code>time</code>: hours to breakdown
</p>
</li>
<li> <p><code>voltage</code>: test voltage in kV
</p>
</li></ul>

</li>
<li><p> Data set <code>imotor</code>: Breakdown of motor insulation as a function of
temperature.
</p>

<ul>
<li><p> temp: temperature of the test
</p>
</li>
<li><p> time: time to failure or censoring
</p>
</li>
<li><p> status: 0=censored, 1=failed
</p>
</li></ul>

</li>
<li><p> Data set <code>turbine</code>:
Each of 432 turbine wheels was inspected
once to determine whether a crack had developed in the wheel or not.
</p>

<ul>
<li><p> hours: time of inspection (100s of hours)
</p>
</li>
<li><p> inspected: number that were inspected
</p>
</li>
<li><p> failed: number that failed
</p>
</li></ul>

</li>
<li><p> Data set <code>valveSeat</code>:
Time to replacement of valve seats for 41 diesel engines.  More than
one seat may be replaced at a particular service, leading to duplicate
times in the data set.  The final inspection time for each engine will
have status=0.
</p>

<ul>
<li><p> id: engine identifier
</p>
</li>
<li><p> time: time of the inspection, in days
</p>
</li>
<li><p> status: 1=replacement occured, 0= not
</p>
</li></ul>

</li></ul>



<h3>References</h3>

<p>Meeker and Escobar, Statistical Methods for Reliability Data, 1998.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>survreg(Surv(time, status) ~ temperature + voltage, capacitor)

# Replacement of valve seats.  In this case the cumulative hazard is the 
#  natural target, an estimate of the number of replacements by a given time
#  (known as the cumulative mean function = CMF in relability).
# When two valve seats failed at the same inspection, we need to jitter one
#  of the times, to avoid a (time1, time2) interval of length 0
ties &lt;- which(with(valveSeat, diff(id)==0 &amp; diff(time)==0))  #first of a tie
temp &lt;- valveSeat$time
temp[ties] &lt;- temp[ties] - .1 # jittered time
vdata &lt;- valveSeat
vdata$time1 &lt;- ifelse(!duplicated(vdata$id), 0, c(0, temp[-length(temp)]))
vdata$time2 &lt;- temp
fit2 &lt;- survfit(Surv(time1, time2, status) ~1, vdata, id=id)
## Not run: 
plot(fit2, cumhaz= TRUE, xscale= 365.25, 
      xlab="Years in service", ylab = "Expected number of repairs")

## End(Not run)
</code></pre>

<hr>
<h2 id='residuals.coxph'>Calculate Residuals for a &lsquo;coxph&rsquo; Fit
</h2><span id='topic+residuals.coxph.penal'></span><span id='topic+residuals.coxph.null'></span><span id='topic+residuals.coxph'></span><span id='topic+residuals.coxphms'></span>

<h3>Description</h3>

<p>Calculates martingale, deviance, score or Schoenfeld residuals for a
Cox proportional hazards model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coxph'
residuals(object,
       type=c("martingale", "deviance", "score", "schoenfeld",
	      "dfbeta", "dfbetas", "scaledsch","partial"),
       collapse=FALSE, weighted= (type %in% c("dfbeta", "dfbetas")), ...)
## S3 method for class 'coxphms'
residuals(object,
       type=c("martingale", "score", "schoenfeld",
	      "dfbeta", "dfbetas", "scaledsch"),
       collapse=FALSE, weighted= FALSE, ...)
## S3 method for class 'coxph.null'
residuals(object,
       type=c("martingale", "deviance","score","schoenfeld"),
       collapse=FALSE, weighted= FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.coxph_+3A_object">object</code></td>
<td>

<p>an object inheriting from class <code>coxph</code>, representing a fitted Cox
regression model.
Typically this is the output from the <code>coxph</code> function.
</p>
</td></tr>
<tr><td><code id="residuals.coxph_+3A_type">type</code></td>
<td>

<p>character string indicating the type of residual desired.
Possible values are <code>"martingale"</code>, <code>"deviance"</code>, <code>"score"</code>, <code>"schoenfeld"</code>,
&quot;dfbeta&quot;', <code>"dfbetas"</code>, <code>"scaledsch"</code> and <code>"partial"</code>.
Only enough of the string to determine a unique match is required.
</p>
</td></tr>
<tr><td><code id="residuals.coxph_+3A_collapse">collapse</code></td>
<td>

<p>vector indicating which rows to collapse (sum) over.
In time-dependent models more than one row data can pertain
to a single individual.
If there were 4 individuals represented by 3, 1, 2 and 4 rows of data
respectively, then <code>collapse=c(1,1,1, 2, 3,3, 4,4,4,4)</code> could be used to
obtain per subject rather than per observation residuals.
</p>
</td></tr>
<tr><td><code id="residuals.coxph_+3A_weighted">weighted</code></td>
<td>

<p>if <code>TRUE</code> and the model was fit with case weights, then the weighted
residuals are returned.
</p>
</td></tr><tr><td><code id="residuals.coxph_+3A_...">...</code></td>
<td>
<p>other unused arguments</p>
</td></tr></table>


<h3>Value</h3>

<p>For martingale and deviance residuals, the returned object is a vector
with one element for each subject (without <code>collapse</code>).
For score residuals it is a matrix
with one row per subject and one column per variable.
The row order will match the input data for the original fit.
For Schoenfeld residuals, the returned object is a matrix with one row
for each event and one column per variable.  The rows are ordered by time
within strata, and an attribute <code>strata</code> is attached that contains the
number of observations in each strata.
The scaled Schoenfeld residuals are used in the <code>cox.zph</code> function.
</p>
<p>The score residuals are each individual's contribution to the score vector.
Two transformations of
this are often more useful: <code>dfbeta</code> is the approximate change in the
coefficient vector if that observation were dropped,
and <code>dfbetas</code> is the approximate change in the coefficients, scaled by
the standard error for the coefficients.
</p>


<h3>NOTE</h3>

<p>For deviance residuals, the status variable may need to be reconstructed.
For score and Schoenfeld residuals, the X matrix will need to be reconstructed.
</p>


<h3>References</h3>

<p>T. Therneau, P. Grambsch, and T. Fleming. &quot;Martingale based residuals
for survival models&quot;, <em>Biometrika</em>, March 1990.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
 fit &lt;- coxph(Surv(start, stop, event) ~ (age + surgery)* transplant,
               data=heart)
 mresid &lt;- resid(fit, collapse=heart$id)
</code></pre>

<hr>
<h2 id='residuals.survfit'>IJ residuals from a survfit object.</h2><span id='topic+residuals.survfit'></span>

<h3>Description</h3>

<p>Return infinitesimal jackknife residuals from a survfit object, for
the survival, cumulative hazard, or restricted mean time in state (RMTS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survfit'
residuals(object, times, 
    type="pstate", collapse=FALSE, weighted= collapse, data.frame=FALSE,
    extra = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.survfit_+3A_object">object</code></td>
<td>
<p>a <code>survfit</code> object</p>
</td></tr>
<tr><td><code id="residuals.survfit_+3A_times">times</code></td>
<td>
<p>a vector of times at which the residuals are desired</p>
</td></tr>
<tr><td><code id="residuals.survfit_+3A_type">type</code></td>
<td>
<p>the type of residual, see below</p>
</td></tr>
<tr><td><code id="residuals.survfit_+3A_collapse">collapse</code></td>
<td>
<p>add the residuals for all subjects in a cluster</p>
</td></tr>
<tr><td><code id="residuals.survfit_+3A_weighted">weighted</code></td>
<td>
<p>weight the residuals by each observation's weight</p>
</td></tr>
<tr><td><code id="residuals.survfit_+3A_data.frame">data.frame</code></td>
<td>
<p>if FALSE return a matrix or array</p>
</td></tr>
<tr><td><code id="residuals.survfit_+3A_extra">extra</code></td>
<td>
<p>return extra information when <code>data.frame=FALSE</code>.
(This is used internally by the psuedo function.)</p>
</td></tr>
<tr><td><code id="residuals.survfit_+3A_...">...</code></td>
<td>
<p>arguments for other methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is designed to efficiently compute the per-observation
residuals for a Kaplan-Meier or Aalen-Johansen curve, also known as
infinitesimal jackknife (IJ) values, at a small number of time points.
Common usages are the creation of psuedo-values (via the <code>pseudo</code> function)
and IJ estimates of variance.
The residuals matrix has a value for each observation and time point
pair.
For a multi-state model the state will be a third dimension.
</p>
<p>The residuals are the impact of each observation or cluster on the
resulting probability in state curves at the given time points,
the cumulative hazard curve at those time points,
or the expected sojourn time in each state up to the given time points.
For a simple Kaplan-Meier the <code>survfit</code> object contains only the
probability in the &quot;initial&quot; state, i.e., the survival fraction.
In this case the sojourn time, the expected amount of time spent in
the initial state, up to the specified endpoint, is commonly known as the
restricted mean survival time (RMST).
For a multistate model this same quantity is more often referred to as the
restricted mean time in state (RMTS).
It can be computed as the area under the respective probability in state curve.
</p>
<p>The program allows any of <code>pstate</code>, <code>surv</code>, <code>cumhaz</code>,
<code>chaz</code>, <code>sojourn</code>, <code>rmst</code>, <code>rmts</code> or <code>auc</code>
for the type argument, ignoring upper/lowercase, so
users can choose whichever abbreviation they like best.
</p>
<p>When <code>collapse=TRUE</code> the result has the cluster identifier (which
defaults to the <code>id</code> variable) as the dimname for the first
dimension. 
If the <code>fit</code> object contains more than one curve, and the same
identifier is reused in two different curves this approach does not work
and the routine will stop with an error.
In principle this is not necessary, e.g., the result could contain two rows
with the same label, showing the separate effect on each curve,
but this was deemed too confusing. 
</p>


<h3>Value</h3>

<p>A matrix or array with one row per observation or cluster, and one column
for each value in <code>times</code>.  For a multi-state model the three
dimensions are observation, state, and time.  For cumulative hazard,
the second dimension is the set of transitions.  (A competing risks
model for instance has 3 states and 2 transitions.)
</p>


<h3>Note</h3>

<p>The first column of the data frame identifies the origin of the
row.  If there was an <code>id</code> variable in the <code>survfit</code> call it
will contain the values of that variable and be labeled with the
variable name, or &quot;(id)&quot; if there was an expression rather than a
name. (For example, <code>survfit(....  id= abc$def[z])</code>).  If there
was no <code>id</code> variable the label will be &quot;(row)&quot;, and the column
will contain the row number of the survfit data.  For a matrix result
the first component of dimnames has similar structure.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit">survfit</a></code>, <code><a href="#topic+survfit.formula">survfit.formula</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- survfit(Surv(time, status) ~ x, aml)
resid(fit, times=c(24, 48), type="RMTS")
</code></pre>

<hr>
<h2 id='residuals.survreg'>Compute Residuals for &lsquo;survreg&rsquo; Objects</h2><span id='topic+residuals.survreg'></span><span id='topic+residuals.survreg.penal'></span>

<h3>Description</h3>

<p>This is a method for the function <code><a href="stats.html#topic+residuals">residuals</a></code> for objects
inheriting from class <code>survreg</code>.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survreg'
residuals(object, type=c("response", "deviance","dfbeta","dfbetas",
"working","ldcase","ldresp","ldshape", "matrix"), rsigma=TRUE,
collapse=FALSE, weighted=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.survreg_+3A_object">object</code></td>
<td>

<p>an object inheriting from class <code>survreg</code>.
</p>
</td></tr>
<tr><td><code id="residuals.survreg_+3A_type">type</code></td>
<td>

<p>type of residuals, with choices of <code>"response"</code>, <code>"deviance"</code>,
<code>"dfbeta"</code>, <code>"dfbetas"</code>, <code>"working"</code>, <code>"ldcase"</code>, <code>"lsresp"</code>,
<code>"ldshape"</code>, and <code>"matrix"</code>. 
</p>
</td></tr>
<tr><td><code id="residuals.survreg_+3A_rsigma">rsigma</code></td>
<td>

<p>include the scale parameters in the variance matrix, when doing computations.
(I can think of no good reason not to).
</p>
</td></tr>
<tr><td><code id="residuals.survreg_+3A_collapse">collapse</code></td>
<td>

<p>optional vector of subject groups.  If given, this must be of the same
length as the residuals, and causes the result to be per group residuals.
</p>
</td></tr>
<tr><td><code id="residuals.survreg_+3A_weighted">weighted</code></td>
<td>

<p>give weighted residuals?  Normally residuals are unweighted.
</p>
</td></tr><tr><td><code id="residuals.survreg_+3A_...">...</code></td>
<td>
<p>other unused arguments</p>
</td></tr></table>


<h3>Value</h3>

<p>A vector or matrix of residuals is returned.
Response residuals are on the scale of the original data,
working residuals are on the scale of the linear predictor,
and deviance residuals are on log-likelihood scale.
The dfbeta residuals are a matrix, where the ith row gives the
approximate change in the coefficients due to the addition of subject i.
The dfbetas matrix contains the dfbeta residuals, with each column
scaled by the standard deviation of that coefficient.
</p>
<p>The matrix type produces a matrix based on derivatives of the log-likelihood
function.  Let <code class="reqn">L</code> be the log-likelihood, <code class="reqn">p</code> be the linear predictor <code class="reqn">X\beta</code>,
and <code class="reqn">s</code> be <code class="reqn">\log(\sigma)</code>.  Then the 6 columns of the matrix are
<code class="reqn">L</code>, <code class="reqn">dL/dp</code>,<code class="reqn">\partial^2L/\partial p^2</code>,
<code class="reqn">dL/ds</code>, <code class="reqn">\partial^2L/\partial s^2</code>  and
<code class="reqn">\partial^2L/\partial p\partial s</code>.
Diagnostics based on these quantities
are discussed in the book and article by Escobar and Meeker.
The main ones are the likelihood displacement residuals for perturbation
of a case weight (<code>ldcase</code>), the response value (<code>ldresp</code>),
and the <code>shape</code>.
</p>
<p>For a transformed distribution such as the log-normal or Weibull,
matrix residuals are based on the log-likelihood of the transformed data
log(y). 
For a monotone function f the density of f(X) is the density of X
divided by the derivative of f (the Jacobian), so subtract log(derivative) from
each uncensored observation's loglik value in order to match the
<code>loglik</code> component of the result.  The other colums of the matrix
residual are unchanged by the transformation.
</p>


<h3>References</h3>

<p>Escobar, L. A. and Meeker, W. Q. (1992).
Assessing influence in regression analysis with censored data.
<em>Biometrics</em>
<b>48</b>, 507-528.
</p>
<p>Escobar, L. A. and Meeker, W. Q. (1998).
Statistical Methods for Reliablilty Data.  Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.survreg">predict.survreg</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- survreg(Surv(futime, death) ~ age + sex, mgus2)
summary(fit)   # age and sex are both important

rr  &lt;- residuals(fit, type='matrix')
sum(rr[,1]) - with(mgus2, sum(log(futime[death==1]))) # loglik

plot(mgus2$age, rr[,2], col= (1+mgus2$death)) # ldresp
</code></pre>

<hr>
<h2 id='retinopathy'>Diabetic Retinopathy</h2><span id='topic+retinopathy'></span>

<h3>Description</h3>

<p>A trial of laser coagulation as a treatment to delay
diabetic retinopathy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>retinopathy
data(retinopathy, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 394 observations on the following 9 variables.
</p>

<dl>
<dt><code>id</code></dt><dd><p>numeric subject id</p>
</dd>
<dt><code>laser</code></dt><dd><p>type of laser used: <code>xenon</code> <code>argon</code></p>
</dd>
<dt><code>eye</code></dt><dd><p>which eye was treated: <code>right</code> <code>left</code></p>
</dd>
<dt><code>age</code></dt><dd><p>age at diagnosis of diabetes</p>
</dd>
<dt><code>type</code></dt><dd><p>type of diabetes: <code>juvenile</code> <code>adult</code>,
(diagnosis before age 20)</p>
</dd>
<dt><code>trt</code></dt><dd><p>0 = control eye, 1 = treated eye</p>
</dd>
<dt><code>futime</code></dt><dd><p>time to loss of vision or last follow-up</p>
</dd>
<dt><code>status</code></dt><dd><p>0 = censored, 1 = loss of vision in this eye</p>
</dd>
<dt><code>risk</code></dt><dd><p>a risk score for the eye.  This high risk
subset is defined as a score of 6 or greater in at least one eye.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The 197 patients in this dataset were a 50% random sample of the
patients with &quot;high-risk&quot; diabetic retinopathy as defined by the
Diabetic Retinopathy Study (DRS).  Each patient had one eye randomized
to laser treatment and the other eye received no treatment,
and has two observations in the data set.
For each
eye, the event of interest was the time from initiation of treatment
to the time when visual acuity dropped below 5/200 two visits in a row.
Thus there is a built-in lag time of
approximately 6 months (visits were every 3 months).  Survival times
in this dataset are the actual time to vision loss in months,
minus the minimum possible time to event (6.5 months).  Censoring was
caused by death, dropout, or end of the study.
</p>


<h3>References</h3>

<p>W. J. Huster, R. Brookmeyer and S. G. Self (1989).
Modelling paired survival data with covariates,
Biometrics 45:145-156.
</p>
<p>A. L. Blair, D. R. Hadden, J. A. Weaver, D. B. Archer, P. B. Johnston
and C. J. Maguire (1976).  The 5-year prognosis for vision in diabetes,
American Journal of Ophthalmology, 81:383-396.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>coxph(Surv(futime, status) ~ type + trt, cluster= id, retinopathy)
</code></pre>

<hr>
<h2 id='rhDNase'>rhDNASE data set</h2><span id='topic+rhDNase'></span>

<h3>Description</h3>

<p>Results of a randomized trial of rhDNase for the treatment of cystic
fibrosis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rhDNase
data(rhDNase, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 767 observations on the following 8 variables.
</p>

<dl>
<dt><code>id</code></dt><dd><p>subject id</p>
</dd>
<dt><code>inst</code></dt><dd><p>enrolling institution</p>
</dd>
<dt><code>trt</code></dt><dd><p>treatment arm: 0=placebo, 1= rhDNase</p>
</dd>
<dt><code>entry.dt</code></dt><dd><p>date of entry into the study</p>
</dd>
<dt><code>end.dt</code></dt><dd><p>date of last follow-up</p>
</dd>
<dt><code>fev</code></dt><dd><p>forced expriatory volume at enrollment, a measure
of lung capacity</p>
</dd>
<dt><code>ivstart</code></dt><dd><p>days from enrollment to the start of IV antibiotics</p>
</dd>
<dt><code>ivstop</code></dt><dd><p>days from enrollment to the cessation of
IV antibiotics</p>
</dd>
</dl>



<h3>Details</h3>

<p>In patients with cystic fibrosis, extracellular DNA is released by
leukocytes that accumulate in the airways in response to chronic bacterial
infection.
This excess DNA thickens the mucus, which then cannot be cleared from the
lung by the cilia.  The accumulation leads to exacerbations of
respiratory symptoms and progressive deterioration of lung function.
At the time of this study 
more than 90% of cystic fibrosis patients eventually died of lung
disease.
</p>
<p>Deoxyribonuclease I (DNase I) is a
human enzyme normally present in the mucus of human lungs that digests
extracellular DNA.
Genentech, Inc. cloned a highly purified recombinant DNase I (rhDNase or
Pulmozyme) which when delivered to the lungs in an aerosolized form cuts
extracellular DNA, reducing the viscoelasticity of airway
secretions and improving clearance.
In 1992 the company
conducted a randomized double-blind trial comparing rhDNase to placebo.
Patients were then monitored for
pulmonary exacerbations, along with measures of lung volume and flow.
The primary endpoint was the time until
first pulmonary exacerbation; however, data on all exacerbations were
collected for 169 days.
</p>
<p>The definition of an exacerbation was an infection that required the use
of intravenous (IV) antibiotics.  Subjects had 0&ndash;5 such episodes during
the trial, those with more than one have multiple rows in the data
set, those with none have NA for the IV start and end times.
A few subjects were infected at the time of enrollment, subject 173 for
instance has a first infection interval of -21 to 7.  We do not count this
first infection as an &quot;event&quot;, and the subject first enters the risk set
at day 7.
Subjects who have an event are not considered to be at risk for another
event during the course of antibiotics, nor for an additional 6 days
after they end.  (If the symptoms reappear immediately after cessation
then from a medical standpoint this would not be a new infection.)
</p>
<p>This data set reproduces the data in Therneau and Grambsch, it does not
exactly reproduce those in Therneau and Hamilton due to data set updates.
</p>


<h3>References</h3>

<p>T. M. Therneau and P. M. Grambsch, Modeling Survival Data: Extending
the Cox Model, Springer, 2000.
</p>
<p>T. M. Therneau and S.A. Hamilton,
rhDNase as an example of recurrent event analysis, Statistics
in Medicine, 16:2029-2047, 1997.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build the start-stop data set for analysis, and
#  replicate line 2 of table 8.13 in the book
first &lt;- subset(rhDNase, !duplicated(id)) #first row for each subject
dnase &lt;- tmerge(first, first, id=id, tstop=as.numeric(end.dt -entry.dt))

# Subjects whose fu ended during the 6 day window are the reason for
#  this next line
temp.end &lt;- with(rhDNase, pmin(ivstop+6, end.dt-entry.dt))
dnase &lt;- tmerge(dnase, rhDNase, id=id,
                       infect=event(ivstart),
                       end=  event(temp.end))
# toss out the non-at-risk intervals, and extra variables
#  3 subjects had an event on their last day of fu, infect=1 and end=1
dnase &lt;- subset(dnase, (infect==1 | end==0), c(id:trt, fev:infect))
agfit &lt;- coxph(Surv(tstart, tstop, infect) ~ trt + fev, cluster=id,
                 data=dnase)
</code></pre>

<hr>
<h2 id='ridge'> Ridge regression</h2><span id='topic+ridge'></span>

<h3>Description</h3>

<p>When used in a <a href="#topic+coxph">coxph</a> or <a href="#topic+survreg">survreg</a> model formula,
specifies a ridge regression term.  The likelihood is penalised by
<code>theta</code>/2 time the sum of squared coefficients. If <code>scale=T</code>
the penalty is calculated for coefficients based on rescaling the
predictors to have unit variance. If <code>df</code> is specified then <code>theta</code> is chosen based on an approximate degrees of freedom.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridge(..., theta, df=nvar/2, eps=0.1, scale=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridge_+3A_...">...</code></td>
<td>
<p>predictors to be ridged </p>
</td></tr>
<tr><td><code id="ridge_+3A_theta">theta</code></td>
<td>
<p>penalty is <code>theta</code>/2 time sum of squared coefficients </p>
</td></tr>
<tr><td><code id="ridge_+3A_df">df</code></td>
<td>
<p>Approximate degrees of freedom </p>
</td></tr>
<tr><td><code id="ridge_+3A_eps">eps</code></td>
<td>
<p> Accuracy required for <code>df</code> </p>
</td></tr>
<tr><td><code id="ridge_+3A_scale">scale</code></td>
<td>
<p> Scale variables before applying penalty? </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>coxph.penalty</code> containing the data and
control functions.  </p>


<h3>Note</h3>

<p>If the expression <code>ridge(x1, x2, x3, ...)</code> is too many characters
long then the
internal terms() function will add newlines to the variable name and
then the coxph routine simply gets lost.  (Some labels will have the newline
and some won't.)
One solution is to bundle all of the variables into a single matrix and
use that matrix as the argument to <code>ridge</code> so as to shorten the call,
e.g. <code>mdata$many &lt;- as.matrix(mydata[,5:53])</code>.
</p>


<h3>References</h3>

<p>Gray (1992) &quot;Flexible methods of analysing survival data using splines, with applications to breast cancer prognosis&quot; JASA 87:942&ndash;951
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+coxph">coxph</a></code>,<code><a href="#topic+survreg">survreg</a></code>,<code><a href="#topic+pspline">pspline</a></code>,<code><a href="#topic+frailty">frailty</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
coxph(Surv(futime, fustat) ~ rx + ridge(age, ecog.ps, theta=1),
	      ovarian)

lfit0 &lt;- survreg(Surv(time, status) ~1, lung)
lfit1 &lt;- survreg(Surv(time, status) ~ age + ridge(ph.ecog, theta=5), lung)
lfit2 &lt;- survreg(Surv(time, status) ~ sex + ridge(age, ph.ecog, theta=1), lung)
lfit3 &lt;- survreg(Surv(time, status) ~ sex + age + ph.ecog, lung)

</code></pre>

<hr>
<h2 id='rotterdam'>Breast cancer data set used in Royston and Altman (2013)</h2><span id='topic+rotterdam'></span>

<h3>Description</h3>

<p>The <code>rotterdam</code> data set includes 2982 primary breast cancers patients
whose records were included in the Rotterdam tumor bank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rotterdam
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 2982 observations on the following 15 variables.
</p>

<dl>
<dt><code>pid</code></dt><dd><p>patient identifier</p>
</dd>
<dt><code>year</code></dt><dd><p>year of surgery</p>
</dd>
<dt><code>age</code></dt><dd><p>age at surgery</p>
</dd>
<dt><code>meno</code></dt><dd><p>menopausal status (0= premenopausal, 1= postmenopausal)</p>
</dd>
<dt><code>size</code></dt><dd><p>tumor size, a factor with levels <code>&lt;=20</code> <code>20-50</code> <code>&gt;50</code></p>
</dd>
<dt><code>grade</code></dt><dd><p>differentiation grade</p>
</dd>
<dt><code>nodes</code></dt><dd><p>number of positive lymph nodes</p>
</dd>
<dt><code>pgr</code></dt><dd><p>progesterone receptors (fmol/l)</p>
</dd>
<dt><code>er</code></dt><dd><p>estrogen receptors (fmol/l)</p>
</dd>
<dt><code>hormon</code></dt><dd><p>hormonal treatment (0=no, 1=yes)</p>
</dd>
<dt><code>chemo</code></dt><dd><p>chemotherapy</p>
</dd>
<dt><code>rtime</code></dt><dd><p>days to relapse or last follow-up</p>
</dd>
<dt><code>recur</code></dt><dd><p>0= no relapse, 1= relapse</p>
</dd>
<dt><code>dtime</code></dt><dd><p>days to death or last follow-up</p>
</dd>
<dt><code>death</code></dt><dd><p>0= alive, 1= dead</p>
</dd>
</dl>



<h3>Details</h3>

<p>These data sets are used in the paper by Royston and Altman that is
referenced below.
The Rotterdam data is used to create a fitted model, and the GBSG data for 
validation of the model.  The paper gives references for the data
source.
</p>
<p>There are 43 subjects who have died without recurrence, but whose death
time is greater than the censoring time for recurrence.
A common way that this happens is that a death date is updated in the
health record sometime after the research study ended, and said value
is then picked up when a study data set is created.
Vital status information can come from many sources: a patient visit for
another condition, correspondence, financial transactions, or social media.
But this raises serious questions about censoring.
For instance subject 40 is censored for recurrence at 4.2 years and died
at 6.6 years; when creating the endpoint of recurrence free survival
(earlier of recurrence or death), treating them as a death at 6.6 years
implicitly assumes that they were recurrence free just before death.
For this to be true we would have to assume that if they had progressed in
the 2.4 year interval before death (while off study),
that this information would also have been noted
in their general medical record, and would also be captured in
the study data set.
However, that may be unlikely.  Death information is often in a
centralized location in electronic health records, easily accessed by a
programmer and merged with the study data, while recurrence may
require manual review.  How best to address this is an open issue.
</p>


<h3>References</h3>

<p>Patrick Royston and Douglas Altman, External validation of a Cox prognostic
model: principles and methods.  BMC Medical Research Methodology 2013, 13:33
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbsg">gbsg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># liberal definition of rfs (count later deaths)
rfs  &lt;- pmax(rotterdam$recur, rotterdam$death)
rfstime &lt;- with(rotterdam, ifelse(recur==1, rtime, dtime))
fit1 &lt;- coxph(Surv(rfstime, rfs) ~ pspline(age) + meno + size + 
        pspline(nodes) + er,  data = rotterdam)

# conservative (no deaths after last fu for recurrence)
ignore &lt;- with(rotterdam, recur ==0 &amp; death==1 &amp; rtime &lt; dtime)
table(ignore)
rfs2 &lt;- with(rotterdam, ifelse(recur==1 | ignore, recur, death))
rfstime2 &lt;- with(rotterdam, ifelse(recur==1 | ignore, rtime, dtime))
fit2 &lt;- coxph(Surv(rfstime2, rfs2) ~ pspline(age) + meno + size + 
        pspline(nodes) + er,  data = rotterdam)

# Note: Both age and nodes show non-linear effects.
# Royston and Altman used fractional polynomials for the nonlinear terms
</code></pre>

<hr>
<h2 id='royston'>Compute Royston's D for a Cox model</h2><span id='topic+royston'></span>

<h3>Description</h3>

<p>Compute the D statistic proposed by Royston and Sauerbrei along with
several pseudo- R square values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>royston(fit, newdata, ties = TRUE, adjust = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="royston_+3A_fit">fit</code></td>
<td>
<p>a coxph fit</p>
</td></tr>
<tr><td><code id="royston_+3A_newdata">newdata</code></td>
<td>
<p>optional validation data set</p>
</td></tr>
<tr><td><code id="royston_+3A_ties">ties</code></td>
<td>
<p>make a correction for ties in the risk score</p>
</td></tr>
<tr><td><code id="royston_+3A_adjust">adjust</code></td>
<td>
<p>adjust for possible overfitting</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These values are called pseudo R-squared since they involve only
the linear predictor, and not the outcome.
<code>R.D</code> is the value that corresponsds the Royston and Sauerbrei
<code class="reqn">D</code> statistic.  <code>R.KO</code> is the value proposed by Kent and
O'Quigley, <code>R.N</code> is the value proposed by Nagelkerke, and
<code>C.GH</code> corresponds to Goen and Heller's concordance measure.
</p>
<p>An adjustment for D is based on the ratio
r= (number of events)/(number of coefficients). For models which have
sufficient sample size (r&gt;20) the adjustment will be small.
</p>
<p>The Nagelkerke value is the Cox-Snell R-squared divided by a scaling
constant. The two separate values are present in the result of
<code>summary.coxph</code> as a 2 element vector <code>rsq</code>, and were listed as
&quot;Rsquare&quot; and &quot;max possible&quot; in older versions of the print routine.
(Since superseded in the default printout by the concordance.)
The Nagelkerke estimate is not returned when <code>newdata</code> is present.
</p>


<h3>Value</h3>

<p>a vector containing the value of D, the estimated standard error
of D, and three or four pseudo R-squared values.
</p>


<h3>References</h3>

<p>M. Goen and G. Heller, Concordance probability and discriminatory power
in proportional hazards regression.  Biometrika 92:965-970, 2005.
</p>
<p>N. Nagelkerke, J. Oosting, J. and A. Hart, A simple test for goodness of fit
of Cox's proportional hazards model.  Biometrics 40:483-486, 1984.
</p>
<p>P. Royston and W. Sauerbrei, A new measure of prognostic separation in
survival data.  Statistics in Medicine 23:723-748, 2004.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An example used in Royston and Sauerbrei
pbc2 &lt;- na.omit(pbc)  # no missing values
cfit &lt;- coxph(Surv(time, status==2) ~ age + log(bili) + edema + albumin +
                   stage + copper, data=pbc2, ties="breslow")
royston(cfit)
</code></pre>

<hr>
<h2 id='rttright'>Compute redistribute-to-the-right weights</h2><span id='topic+rttright'></span>

<h3>Description</h3>

<p>For many survival estimands, one approach is to redistribute each
censored observation's weight to those other observations with a longer
survival time (think of distributing an estate to the heirs). Then
compute on the remaining, uncensored data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rttright(formula, data, weights, subset, na.action, times, id, timefix = TRUE,
         renorm= TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rttright_+3A_formula">formula</code></td>
<td>

<p>a formula object, which must have a 
<code>Surv</code> object as the  
response on the left of the <code>~</code> operator and, if desired, terms  
separated by + operators on the right. 
Each unique combination of predictors will define a separate strata.
</p>
</td></tr>
<tr><td><code id="rttright_+3A_data">data</code></td>
<td>

<p>a data frame in which to interpret the variables named in the formula, 
<code>subset</code> and <code>weights</code> arguments. 
</p>
</td></tr>
<tr><td><code id="rttright_+3A_weights">weights</code></td>
<td>

<p>The weights must be nonnegative and it is strongly recommended that  
they be strictly positive, since zero weights are ambiguous, compared 
to use of the <code>subset</code> argument.
</p>
</td></tr>
<tr><td><code id="rttright_+3A_subset">subset</code></td>
<td>

<p>expression saying that only a subset of the rows of the data 
should be used in the fit. 
</p>
</td></tr>
<tr><td><code id="rttright_+3A_na.action">na.action</code></td>
<td>

<p>a missing-data filter function, applied to the model frame, after any 
<code>subset</code> argument has been used. 
Default is <code>options()$na.action</code>. 
</p>
</td></tr>
<tr><td><code id="rttright_+3A_times">times</code></td>
<td>
<p>a vector of time points, for which to return updated
weights.  If missing, a time after the largest time in the data is
assumed.</p>
</td></tr>
<tr><td><code id="rttright_+3A_id">id</code></td>
<td>
<p>optional: if the data set has multiple rows per subject, a
a variable containing the subect identifier of each row.</p>
</td></tr>
<tr><td><code id="rttright_+3A_timefix">timefix</code></td>
<td>
<p>correct for possible round-off error</p>
</td></tr>
<tr><td><code id="rttright_+3A_renorm">renorm</code></td>
<td>
<p>the resulting weights sum to 1 within each group</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>formula</code> argument is treated exactly the same as in the
<code>survfit</code> function.
</p>
<p>Redistribution is recursive: redistribute the weight of the first
censored observation to all those with longer time, which may include
other censored observations.  Then redistribute the next smallest and
etc. up to the specified <code>time</code> value.
After re-distributing the weight for a censored observation to other
observations that are not censored, ordinary non-censored methods can
often be applied.  For example, redistribution of the weights,
followed by computation of the weighted cumulative distribution
function, reprises the Kaplan-Meier estimator.
</p>
<p>A primary use of this routine is illustration of methods or
exploration of new methods.  Methods that use RTTR directly, such as
the Brier score, will often do these compuations internally.
</p>
<p>A covariate on the right hand side of the formula causes
redistribution to occur within group; a censoring in group 1
redistributes weights to others in group 1, etc.  This is appropriate
when the censoring pattern depends upon group.
</p>


<h3>Value</h3>

<p>a vector or matrix of weights, with one column for each requested time</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit">survfit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>afit &lt;- survfit(Surv(time, status) ~1, data=aml)
rwt  &lt;- rttright(Surv(time, status) ~1, data=aml)

# Reproduce a Kaplan-Meier
index &lt;- order(aml$time)
cdf &lt;- cumsum(rwt[index])  # weighted CDF
cdf &lt;- cdf[!duplicated(aml$time[index], fromLast=TRUE)]  # remove duplicate times
cbind(time=afit$time, KM= afit$surv, RTTR= 1-cdf)

# Hormonal patients have a diffent censoring pattern
wt2 &lt;- rttright(Surv(dtime, death) ~ hormon, rotterdam, times= 365*c(3, 5))
dim(wt2)
</code></pre>

<hr>
<h2 id='solder'>Data from a soldering experiment</h2><span id='topic+solder'></span>

<h3>Description</h3>

<p>In 1988 an experiment was designed and implemented at one of AT&amp;T's
factories to investigate alternatives in the &quot;wave soldering&quot; procedure
for mounting electronic componentes to printed circuit boards.
The experiment varied a number of factors relevant to the process.
The response, measured by eye, is the number of visible solder skips.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solder
data(solder, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 900 observations on the following 6 variables.
</p>

<dl>
<dt><code>Opening</code></dt><dd><p>the amount of clearance around the mounting
pad (3 levels)</p>
</dd>
<dt><code>Solder</code></dt><dd><p>the amount of solder (Thick or Thin)</p>
</dd>
<dt><code>Mask</code></dt><dd><p>type and thickness of the material used for the
solder mask (A1.5, A3, A6, B3, B6)</p>
</dd>
<dt><code>PadType</code></dt><dd><p>the geometry and size of the mounting pad (10 levels)</p>
</dd>
<dt><code>Panel</code></dt><dd><p>each board was divided into 3 panels</p>
</dd>
<dt><code>skips</code></dt><dd><p>the number of skips</p>
</dd>
</dl>



<h3>Details</h3>

<p>This data set is used as a detailed example in chapter 1 of Chambers and
Hastie.
Observations 1-360 and 541-900 form a balanced design of 3*2*10*3= 180
observations for four of the pad types (A1.5, A3, B3, B6),
while rows 361-540 match 3 of the 6 Solder*Opening combinations with
pad type A6 and the other 3 with pad type A3.
</p>


<h3>References</h3>

<p>J Chambers and T Hastie, Statistical models in S.  Chapman and Hall, 1993.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The balanced subset used by Chambers and Hastie
#   contains the first 180 of each mask and deletes mask A6. 
index &lt;- 1 + (1:nrow(solder)) - match(solder$Mask, solder$Mask)
solder.balance &lt;- droplevels(subset(solder, Mask != "A6" &amp; index &lt;= 180))
</code></pre>

<hr>
<h2 id='stanford2'>More Stanford Heart Transplant data</h2><span id='topic+stanford2'></span>

<h3>Description</h3>

<p>This contains the Stanford Heart Transplant data in a different
format.  The main data set is in <code><a href="#topic+heart">heart</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stanford2</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    id:  </td><td style="text-align: left;"> ID number</td>
</tr>
<tr>
 <td style="text-align: left;">
    time:</td><td style="text-align: left;"> survival or censoring time</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> censoring status</td>
</tr>
<tr>
 <td style="text-align: left;">
    age: </td><td style="text-align: left;"> in years</td>
</tr>
<tr>
 <td style="text-align: left;">
    t5: </td><td style="text-align: left;"> T5 mismatch score</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>LA Escobar and WQ Meeker Jr (1992),
Assessing influence in regression analysis with censored data.
<em>Biometrics</em> <b>48</b>, 507&ndash;528.
Page 519.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.survreg">predict.survreg</a></code>,
<code><a href="#topic+heart">heart</a></code>
</p>

<hr>
<h2 id='statefig'>Draw a state space figure.</h2><span id='topic+statefig'></span>

<h3>Description</h3>

<p>For multi-state survival models it is useful to have a figure that
shows the states and the possible transitions between them.
This function creates a simple &quot;box and arrows&quot; figure.  It's goal
was simplicity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>statefig(layout, connect, margin = 0.03, box = TRUE, cex = 1, col = 1,
  lwd=1, lty=1, bcol=col, acol=col, alwd=lwd, alty=lty, offset=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="statefig_+3A_layout">layout</code></td>
<td>
<p>describes the layout of the boxes on the page.  See the
detailed description below.
</p>
</td></tr>
<tr><td><code id="statefig_+3A_connect">connect</code></td>
<td>
<p>a square matrix with one row for each state.
If <code>connect[i,j] !=0</code> then an arrow is drawn from state i to
state j.  The row names of the matrix are used as the labels for the
states.
</p>
</td></tr>
<tr><td><code id="statefig_+3A_margin">margin</code></td>
<td>
<p>the fraction of white space between the label and the
surrounding box, and between the box and the arrows, as a function
of the plot region size.
</p>
</td></tr>
<tr><td><code id="statefig_+3A_box">box</code></td>
<td>
<p>should boxes be drawn?  TRUE or FALSE.
</p>
</td></tr>
<tr><td><code id="statefig_+3A_cex">cex</code>, <code id="statefig_+3A_col">col</code>, <code id="statefig_+3A_lty">lty</code>, <code id="statefig_+3A_lwd">lwd</code></td>
<td>
<p>default graphical parameters used for the
text and boxes.  The last 3 can be a vector of values.
</p>
</td></tr>
<tr><td><code id="statefig_+3A_bcol">bcol</code></td>
<td>
<p>color for the box, if it differs from that used for the
text.</p>
</td></tr>
<tr><td><code id="statefig_+3A_acol">acol</code>, <code id="statefig_+3A_alwd">alwd</code>, <code id="statefig_+3A_alty">alty</code></td>
<td>
<p>color, line type and line width for the arrows.</p>
</td></tr>
<tr><td><code id="statefig_+3A_offset">offset</code></td>
<td>
<p>used to slight offset the arrows between two boxes x and y
if there is a transition in both directions.  The default of 0
leads to a double headed arrow in this case &ndash; to arrows are drawn but
they coincide.  A positive value causes each arrow to shift to the
left, from the view of someone standing at the foot of a arrow and
looking towards the arrowhead, a negative offset shifts to the right.
A value of 1 corresponds to the size of the plotting region.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The arguments for color, line type and line width can all be vectors,
in which case they are recycled as needed.  Boxes and text are drawn
in the order of the rownames of <code>connect</code>, and arrows are drawn
in the usual R matrix order. 
</p>
<p>The <code>layout</code> argument is normally a vector of integers, e.g., the
vector (1, 3, 2) describes a layout with 3 columns.  The first has a
single state, the second column has 3 states and the third has 2.
The coordinates of the plotting region are 0 to 1 for both x and y.
Within a column the centers of the boxes are evenly spaced, with 1/2 a
space between the boxes and the margin, e.g., 4 boxes would be at 1/8,
3/8, 5/8 and 7/8.  If <code>layout</code> were a 1 column matrix with values
of (1, 3, 2) then the layout will have three rows with 1, 3, and 2
boxes per row, respectively.  Alternatively, the user can supply a
2 column matrix that directly gives the centers.
</p>
<p>The values of the connect matrix should be 0 for pairs of states that
do not have a transition and values between 0 and 2 for those that do.
States are connected by an arc that passes through the centers of the
two boxes and a third point that is between them.  Specifically,
consider a line segment joining the two centers and erect a second
segment at right angles to the midpoint of length d times the distance
from center to midpoint.  The arc passes through this point.  A value
of d=0 gives a straight line, d=1 a right hand half circle centered
on the midpoint and d= -1 a left hand half circle.  
The <code>connect</code> matrix contains values of d+1 with -1 &lt; d &lt; 1.
</p>
<p>The connecting arrow are drawn from (center of box 1 + offset) to
(center of box 2 + offset), where the the amount of offset (white
space) is determined by the <code>box</code> and <code>margin</code> parameters.
If a pair of states are too close together this can result in an
arrow that points the wrong way.  
</p>


<h3>Value</h3>

<p>a matrix containing the centers of the boxes, with the invisible
attribute set.</p>


<h3>Note</h3>

<p>The goal of this function is to make &ldquo;good enough&rdquo; figures as simply
as possible,
and thereby to encourage users to draw them.
The <code>layout</code> argument was inspired by the <code>diagram</code> package,
which can draw more complex and well decorated figures, e.g., many
different shapes, shading, 
multiple types of connecting lines, etc., but at the
price of greater complexity.
</p>
<p>Because curved lines are drawn as a set of short line segments, line
types have almost no effect for that case.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>Examples</h3>

<pre><code class='language-R'># Draw a simple competing risks figure
states &lt;- c("Entry", "Complete response", "Relapse", "Death")
connect &lt;- matrix(0, 4, 4, dimnames=list(states, states))
connect[1, -1] &lt;- c(1.1, 1, 0.9)
statefig(c(1, 3), connect)
</code></pre>

<hr>
<h2 id='strata'>
Identify Stratification Variables 
</h2><span id='topic+strata'></span>

<h3>Description</h3>

<p>This is a special function used in the context of the Cox survival model. 
It identifies stratification variables when they appear on the right hand 
side of a formula. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strata(..., na.group=FALSE, shortlabel, sep=', ')
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strata_+3A_...">...</code></td>
<td>

<p>any number of variables.  All must be the same length. 
</p>
</td></tr>
<tr><td><code id="strata_+3A_na.group">na.group</code></td>
<td>

<p>a logical variable, if <code>TRUE</code>, then missing values are treated as a 
distinct level of each variable. 
</p>
</td></tr>
<tr><td><code id="strata_+3A_shortlabel">shortlabel</code></td>
<td>
<p>if <code>TRUE</code> omit variable names from resulting
factor labels.  The default action is to omit the names if all of the
arguments are factors, and none of them was named.</p>
</td></tr>
<tr><td><code id="strata_+3A_sep">sep</code></td>
<td>

<p>the character used to separate groups, in the created label
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When used outside of a <code>coxph</code> formula the result of the function
is essentially identical to the <code>interaction</code> function, 
though the labels from <code>strata</code> are often more verbose. 
</p>


<h3>Value</h3>

<p>a new factor, whose levels are all possible combinations of the factors 
supplied as arguments. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>, <code><a href="base.html#topic+interaction">interaction</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- factor(rep(1:3,4), labels=c("low", "medium", "high"))
b &lt;- factor(rep(1:4,3))
levels(strata(b))
levels(strata(a,b,shortlabel=TRUE))

coxph(Surv(futime, fustat) ~ age + strata(rx), data=ovarian) 
</code></pre>

<hr>
<h2 id='summary.aareg'>
Summarize an aareg fit
</h2><span id='topic+summary.aareg'></span>

<h3>Description</h3>

<p>Creates the overall test statistics for an Aalen additive regression model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'aareg'
summary(object, maxtime, test=c("aalen", "nrisk"), scale=1,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.aareg_+3A_object">object</code></td>
<td>

<p>the result of a call to the <code>aareg</code> function
</p>
</td></tr>
<tr><td><code id="summary.aareg_+3A_maxtime">maxtime</code></td>
<td>

<p>truncate the input to the model at time &quot;maxtime&quot;
</p>
</td></tr>
<tr><td><code id="summary.aareg_+3A_test">test</code></td>
<td>

<p>the relative time weights that will be used to compute the test
</p>
</td></tr>
<tr><td><code id="summary.aareg_+3A_scale">scale</code></td>
<td>

<p>scales the coefficients.  
For some data sets, the coefficients of the Aalen model will be very
small (10-4); this simply multiplies the printed values by a constant,
say 1e6, to make the printout easier to read.
</p>
</td></tr>
<tr><td><code id="summary.aareg_+3A_...">...</code></td>
<td>
<p>for future methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is not uncommon for the very right-hand tail of the plot to have
large outlying values, particularly for the standard error.  
The <code>maxtime</code> parameter can then be used to
truncate the range so as to avoid these.
This gives an updated value for the test statistics, without refitting the
model.
</p>
<p>The slope is based on a weighted linear regression to the cumulative
coefficient plot, and may be a useful measure of the overall size
of the effect.  For instance when two models include a common variable,
&quot;age&quot; for instance, this may help to assess how much the fit changed due
to the other variables, in leiu of overlaying the two plots.  (Of course
the plots are often highly non-linear, so it is only a rough substitute).
The slope is not directly related to the test statistic, as the latter
is invariant to any monotone transformation of time.
</p>


<h3>Value</h3>

<p>a list is returned with the following components
</p>
<table>
<tr><td><code>table</code></td>
<td>

<p>a matrix with rows for the intercept and each covariate, and columns
giving a slope estimate, the test statistic, it's standard error,
the z-score and a p-value
</p>
</td></tr>
<tr><td><code>test</code></td>
<td>

<p>the time weighting used for computing the test statistics
</p>
</td></tr>
<tr><td><code>test.statistic</code></td>
<td>

<p>the vector of test statistics
</p>
</td></tr>
<tr><td><code>test.var</code></td>
<td>

<p>the model based variance matrix for the test statistic
</p>
</td></tr>
<tr><td><code>test.var2</code></td>
<td>

<p>optionally, a robust variance matrix for the test statistic
</p>
</td></tr>
<tr><td><code>chisq</code></td>
<td>

<p>the overall test (ignoring the intercept term) for significance of
any variable
</p>
</td></tr>
<tr><td><code>n</code></td>
<td>

<p>a vector containing the number of observations, the number of unique death
times used in the computation, and the total number of unique death times
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>aareg, plot.aareg
</p>


<h3>Examples</h3>

<pre><code class='language-R'>afit &lt;- aareg(Surv(time, status) ~ age + sex + ph.ecog, data=lung,
     dfbeta=TRUE)
summary(afit)
## Not run: 
              slope   test se(test) robust se     z        p 
Intercept  5.05e-03    1.9     1.54      1.55  1.23 0.219000
      age  4.01e-05  108.0   109.00    106.00  1.02 0.307000
      sex -3.16e-03  -19.5     5.90      5.95 -3.28 0.001030
  ph.ecog  3.01e-03   33.2     9.18      9.17  3.62 0.000299

Chisq=22.84 on 3 df, p=4.4e-05; test weights=aalen

## End(Not run)

summary(afit, maxtime=600)
## Not run: 
              slope   test se(test) robust se      z        p 
Intercept  4.16e-03   2.13     1.48      1.47  1.450 0.146000
      age  2.82e-05  85.80   106.00    100.00  0.857 0.392000
      sex -2.54e-03 -20.60     5.61      5.63 -3.660 0.000256
  ph.ecog  2.47e-03  31.60     8.91      8.67  3.640 0.000271

Chisq=27.08 on 3 df, p=5.7e-06; test weights=aalen

## End(Not run)</code></pre>

<hr>
<h2 id='summary.coxph'>
Summary method for Cox models 
</h2><span id='topic+summary.coxph'></span>

<h3>Description</h3>

<p>Produces a summary of a fitted coxph model 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coxph'
summary(object, conf.int=0.95, scale=1,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.coxph_+3A_object">object</code></td>
<td>

<p>the result of a coxph fit 
</p>
</td></tr>
<tr><td><code id="summary.coxph_+3A_conf.int">conf.int</code></td>
<td>

<p>level for computation of the confidence intervals. 
If set to FALSE no confidence intervals are printed 
</p>
</td></tr>
<tr><td><code id="summary.coxph_+3A_scale">scale</code></td>
<td>

<p>vector of scale factors for the coefficients, defaults to 1. 
The printed coefficients, se, and confidence intervals will be
associated with one scale unit. 
</p>
</td></tr>
<tr><td><code id="summary.coxph_+3A_...">...</code></td>
<td>
<p>for future methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>summary.coxph</code>, with components:
</p>
<table>
<tr><td><code>n</code>, <code>nevent</code></td>
<td>
<p>number of observations and number of events,
respectively, in the fit</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>the log partial likelihood at the initial and final
values</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>a matrix with one row for each coefficient, and
columns containing the coefficient, the hazard ratio exp(coef),
standard error, Wald statistic, and P value.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a matrix with one row for each coefficient, containing
the confidence limits for exp(coef)</p>
</td></tr>
<tr><td><code>logtest</code>, <code>sctest</code>, <code>waldtest</code></td>
<td>
<p>the overall likelihood ratio, score,
and Wald test statistics for the model</p>
</td></tr>
<tr><td><code>concordance</code></td>
<td>
<p>the concordance statistic and its standard error</p>
</td></tr>
<tr><td><code>used.robust</code></td>
<td>
<p>whether an asymptotic or robust variance was used</p>
</td></tr>
<tr><td><code>rsq</code></td>
<td>
<p>an approximate R^2 based on Nagelkerke (Biometrika 1991).</p>
</td></tr>
<tr><td><code>fail</code></td>
<td>
<p>a message, if the underlying coxph call failed</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a copy of the call</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>information on missing values</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The pseudo r-squared of Nagelkerke is attractive because it is simple,
but further work has shown that it has poor properties and it is now
deprecated.  The value is no longer printed by default, and will
eventually be removed from the object.
The <code>royston</code> function now includes it along with several other
measures of association.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>, <code><a href="#topic+print.coxph">print.coxph</a></code>  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- coxph(Surv(time, status) ~ age + sex, lung) 
summary(fit)
</code></pre>

<hr>
<h2 id='summary.pyears'>Summary function for pyears objecs</h2><span id='topic+summary.pyears'></span>

<h3>Description</h3>

<p>Create a printable table of a person-years result.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pyears'
summary(object, header = TRUE, call = header, n = TRUE,
event = TRUE, pyears = TRUE, expected = TRUE, rate = FALSE, rr =expected,
ci.r = FALSE, ci.rr = FALSE, totals=FALSE, legend = TRUE, vline = FALSE,
vertical= TRUE, nastring=".", conf.level = 0.95,
scale = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.pyears_+3A_object">object</code></td>
<td>
<p>a pyears object</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_header">header</code></td>
<td>
<p>print out a header giving the total number of
observations, events, person-years, and total time (if any)
omitted from the table</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_call">call</code></td>
<td>
<p>print out a copy of the call</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_n">n</code>, <code id="summary.pyears_+3A_event">event</code>, <code id="summary.pyears_+3A_pyears">pyears</code>, <code id="summary.pyears_+3A_expected">expected</code></td>
<td>
<p>logical arguments: should these
elements be printed in the table?</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_rate">rate</code>, <code id="summary.pyears_+3A_ci.r">ci.r</code></td>
<td>
<p>logical arguments: should the incidence rate and/or
its confidence interval be given in the table?</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_rr">rr</code>, <code id="summary.pyears_+3A_ci.rr">ci.rr</code></td>
<td>
<p>logical arguments: should the hazard ratio and/or
its confidence interval be given in the table?</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_totals">totals</code></td>
<td>
<p>should row and column totals be added?</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_legend">legend</code></td>
<td>
<p>should a legend be included in the printout?</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_vline">vline</code></td>
<td>
<p>should vertical lines be included in the printed tables?</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_vertical">vertical</code></td>
<td>
<p>when there is only a single predictor, should the
table be printed with the predictor on the left (vertical=TRUE) or
across the top (vertical=FALSE)?</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_nastring">nastring</code></td>
<td>
<p>what to use for missing values in the table.  Some of
these are structural, e.g., risk ratios for a cell with no follow-up time.</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for any confidence intervals</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_scale">scale</code></td>
<td>
<p>a scaling factor for printed rates</p>
</td></tr>
<tr><td><code id="summary.pyears_+3A_...">...</code></td>
<td>
<p>optional arguments which will be passed to the
<code>format</code> function; common choices would be digits=2 or nsmall=1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>pyears</code> function is often used to create initial
descriptions of a survival or time-to-event variable; the type of
material that is often found in &ldquo;table 1&rdquo; of a paper.  The summary
routine prints this information out using one of pandoc table styles.
A primary reason for choosing this style is that Rstudio is then able
to automatically render the results in multiple formats: html, rtf,
latex, etc.
</p>
<p>If the <code>pyears</code> call has only a single covariate then the table
will have that covariate as one margin and the statistics of interest
as the other.
If the <code>pyears</code> call has two predictors then those two predictors
are used as margins of the table, while each cell of the table
contains the statistics of interest as multiple rows within the cell.
If there are more than two predictors then multiple tables are
produced, in the same order as the standard R printout for an array.
</p>
<p>The &quot;N&quot; entry of a pyears object is the number of observations which
contributed to a particular cell.  When the original call includes
<code>tcut</code> objects then a single observation may contribute to
multiple cells.
</p>


<h3>Value</h3>

<p>a copy of the object</p>


<h3>Notes</h3>

<p>The pandoc system has four table types: with or without vertical bars,
and with single or multiple rows of data in each cell.
This routine produces all 4 styles depending on options, but currently
not all of them are recognized by the Rstudio-pandoc pipeline.
(And we don't yet see why.)
</p>


<h3>Author(s)</h3>

<p>Terry Therneau and Elizabeth Atkinson</p>


<h3>See Also</h3>

<p><code><a href="#topic+cipoisson">cipoisson</a></code>, <code><a href="#topic+pyears">pyears</a></code>, <code><a href="base.html#topic+format">format</a></code></p>

<hr>
<h2 id='summary.survexp'>Summary function for a survexp object</h2><span id='topic+summary.survexp'></span>

<h3>Description</h3>

<p>Returns a list containing the values of the survival at
specified times.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survexp'
summary(object, times, scale = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.survexp_+3A_object">object</code></td>
<td>

<p>the result of a call to the <code>survexp</code> function
</p>
</td></tr>
<tr><td><code id="summary.survexp_+3A_times">times</code></td>
<td>

<p>vector of times; 
the returned matrix will contain 1 row for each time. 
Missing values are not allowed.  
</p>
</td></tr>
<tr><td><code id="summary.survexp_+3A_scale">scale</code></td>
<td>

<p>numeric value to rescale the survival time, e.g., if the input data to 
<code>survfit</code> were in 
days, <code>scale = 365.25</code> would scale the output to years. 
</p>
</td></tr>
<tr><td><code id="summary.survexp_+3A_...">...</code></td>
<td>
<p>For future methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A primary use of this function is to retrieve survival at fixed time
points, which will be properly interpolated by the function.
</p>


<h3>Value</h3>

<p>a list with the following components: 
</p>
<table>
<tr><td><code>surv</code></td>
<td>

<p>the estimate of survival at time t. 
</p>
</td></tr>
<tr><td><code>time</code></td>
<td>

<p>the timepoints on the curve. 
</p>
</td></tr>
<tr><td><code>n.risk</code></td>
<td>

<p>In expected survival each subject from the data set is matched to a
hypothetical person from the parent population, matched on the
characteristics of the parent population.
The number at risk is the number of those hypothetical
subject who are still part of the calculation.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+survexp">survexp</a></code>
</p>

<hr>
<h2 id='summary.survfit'>
Summary of a Survival Curve 
</h2><span id='topic+summary.survfit'></span><span id='topic+summary.survfitms'></span>

<h3>Description</h3>

<p>Returns a list containing the survival curve, confidence limits for the 
curve, and other information. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'survfit'
summary(object, times, censored=FALSE, scale=1,
  extend=FALSE, rmean=getOption('survfit.rmean'), data.frame=FALSE, dosum, ...)
  ## S3 method for class 'survfitms'
summary(object, times, censored=FALSE, scale=1,
  extend=FALSE, rmean=getOption('survfit.rmean'), data.frame=FALSE, ...)
  </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.survfit_+3A_object">object</code></td>
<td>

<p>the result of a call to the <code>survfit</code> function. 
</p>
</td></tr>
<tr><td><code id="summary.survfit_+3A_times">times</code></td>
<td>

<p>vector of times; 
the returned matrix will contain 1 row for each time. 
The vector will be sorted into increasing order;
missing values are not allowed. 
If <code>censored=T</code>, the default <code>times</code> vector contains all
the unique times in <code>fit</code>, otherwise
the default <code>times</code> vector uses only the event (death) times. 
</p>
</td></tr>
<tr><td><code id="summary.survfit_+3A_censored">censored</code></td>
<td>

<p>logical value:  should the censoring times be included in the output?
This is ignored if the <code>times</code> argument is present. 
</p>
</td></tr>
<tr><td><code id="summary.survfit_+3A_scale">scale</code></td>
<td>

<p>numeric value to rescale the survival time, e.g., if the input data to 
<code>survfit</code> were in 
days, <code>scale = 365.25</code> would scale the output to years. 
</p>
</td></tr>
<tr><td><code id="summary.survfit_+3A_extend">extend</code></td>
<td>

<p>logical value: if TRUE, prints information for all specified <code>times</code>, 
even if there are no subjects left at the end of the specified
<code>times</code>. 
This is only used if the <code>times</code> argument is present. 
</p>
</td></tr>
<tr><td><code id="summary.survfit_+3A_rmean">rmean</code></td>
<td>
<p>Show restricted mean: see
<code><a href="#topic+print.survfit">print.survfit</a></code> for details</p>
</td></tr>
<tr><td><code id="summary.survfit_+3A_data.frame">data.frame</code></td>
<td>
<p>if TRUE, return the results as a data frame, rather
than a summary.survfit object</p>
</td></tr>
<tr><td><code id="summary.survfit_+3A_dosum">dosum</code></td>
<td>
<p>only applicable if <code>times</code> is present, see details below</p>
</td></tr>
<tr><td><code id="summary.survfit_+3A_...">...</code></td>
<td>
<p>for future methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>if <code>data.frame = TRUE</code>, a data frame with columns of time,
n.risk, n.event, n.censor, surv, cumhaz, strata (if present) and
data (the row of newdata for survfit.coxph).  Also std.err, std.chaz,
upper and lower if the curve had se.fit=TRUE.
</p>
<p>if <code>data.frame = FALSE</code>, a list with the following components: 
</p>
<table>
<tr><td><code>surv</code></td>
<td>

<p>the estimate of survival at time t+0. 
</p>
</td></tr>
<tr><td><code>time</code></td>
<td>

<p>the timepoints on the curve. 
</p>
</td></tr>
<tr><td><code>n.risk</code></td>
<td>

<p>the number of subjects at risk at time t-0 
(but see the comments on weights in the <code>survfit</code> help file). 
</p>
</td></tr>
<tr><td><code>n.event</code></td>
<td>

<p>if the <code>times</code> argument is missing, then this column is the number of 
events that occurred at time t. 
Otherwise, it is the cumulative number of events that have occurred 
since the last time listed until time t+0. 
</p>
</td></tr>
<tr><td><code>n.entered</code></td>
<td>

<p>This is present only for counting process survival data.
If the <code>times</code> argument is 
missing, this column is the number of subjects that entered at time t. 
Otherwise, it is the cumulative number of subjects that have entered 
since the last time listed until time t.  
</p>
</td></tr>
<tr><td><code>n.exit.censored</code></td>
<td>

<p>if the <code>times</code> argument is 
missing, this column is the number of subjects that left without an 
event at time t. 
Otherwise, it is the cumulative number of subjects that have left 
without an event 
since the last time listed until time t+0.  
This is only present for counting process survival data.
</p>
</td></tr>
<tr><td><code>std.err</code></td>
<td>

<p>the standard error of the survival value. 
</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>

<p>level of confidence for the confidence intervals of survival. 
</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>

<p>lower confidence limits for the curve. 
</p>
</td></tr>
<tr><td><code>upper</code></td>
<td>

<p>upper confidence limits for the curve. 
</p>
</td></tr>
<tr><td><code>strata</code></td>
<td>

<p>indicates stratification of curve estimation.  
If <code>strata</code> is not <code>NULL</code>, 
there are multiple curves in the result and the <code>surv</code>, <code>time</code>, <code>n.risk</code>, etc.  
vectors will contain multiple curves, pasted end to end.  
The levels of <code>strata</code> (a factor) are the labels for the curves. 
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>the statement used to create the <code>fit</code> object. 
</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>

<p>same as for <code>fit</code>, if present. 
</p>
</td></tr>
<tr><td><code>table</code></td>
<td>

<p>table of information that is returned from <code>print.survfit</code> function. 
</p>
</td></tr>
<tr><td><code>type</code></td>
<td>

<p>type of data censoring.  Passed through from the fit object. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This routine has two uses: printing out a survival curve at specified
time points (often yearly), or extracting the values at specified time
points for further processing.
In the first case we normally want <code>extend=FALSE</code>, i.e., don't print out
data past the end of the curve.  If the <code>times</code> option only
contains values beyond the last point in the curve then there is nothing
to print and an error message will result.
For the second usage we often want <code>extend=TRUE</code>, so that the
results will have a predictable length. 
If <code>data.frame = TRUE</code> then either might be desired.
</p>
<p>For a printout at fixed times, for example yearly values for a curve,
the printed number of events will by default be the total number of
events that have occured since the prior line of printout, and likewise
for number of censored and number at entry, <code>dosum = TRUE</code>.
Alternately, the routine can return the number of events/censors/entry at
that time, <code>dosum=FALSE</code>.
The default for <code>dosum</code> is TRUE if the <code>times</code> vector is
strictly increasing and FALSE otherwise.
</p>
<p>For a survfitms object replace the <code>surv</code> component with
<code>pstate</code>.  Also, a data frame will not include the cumulative
hazard since it has a different multiplicity: one column per transition
rather than one per state.
</p>
<p>The <code>survfit</code> object itself will have a row of information at each
censoring or event time, the default is not save information on each unique
entry time.  For printout at two time points t1, t2, this function will
give the the number at risk at the smallest event times that are &gt;= t1
and &gt;= t2, respectively, the survival curve at the largest recorded times
&lt;= t1 and &lt;= t2, and the number of events and censorings in the interval
t1 &lt; t &lt;= t2.
</p>
<p>When the routine is called with counting process data many users are
confused by counts that are too large.
For example, <code>Surv(c(0,0, 5, 5), c(2, 3, 8, 10), c(1, 0, 1, 0))</code>
followed by a request for the values at time 4.
The <code>survfit</code> object has entries only at times 2, 3, 8, and 10;
there are 2 subjects at risk at time 8, so that is what will be printed.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit">survfit</a></code>,  <code><a href="#topic+print.summary.survfit">print.summary.survfit</a></code>   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary( survfit( Surv(futime, fustat)~1, data=ovarian))
summary( survfit( Surv(futime, fustat)~rx, data=ovarian))
</code></pre>

<hr>
<h2 id='Surv'>
Create a Survival Object 
</h2><span id='topic+Surv'></span><span id='topic+is.Surv'></span><span id='topic++5B.Surv'></span>

<h3>Description</h3>

<p>Create a survival object, usually used as a response variable in a model 
formula. Argument matching is special for this function, see Details below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Surv(time, time2, event,
    type=c('right', 'left', 'interval', 'counting', 'interval2', 'mstate'),
    origin=0)
is.Surv(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Surv_+3A_time">time</code></td>
<td>

<p>for right censored data, this is the follow up time.  For interval
data, the first argument is the starting time for the interval. 
</p>
</td></tr>
<tr><td><code id="Surv_+3A_event">event</code></td>
<td>

<p>The status indicator, normally 0=alive, 1=dead.  Other choices are
<code>TRUE</code>/<code>FALSE</code> (<code>TRUE</code> = death) or 1/2 (2=death). For
interval censored data, the status indicator is 0=right censored,
1=event at <code>time</code>, 2=left censored, 3=interval censored.
For multiple endpoint data the event variable will be a factor,
whose first level is treated as censoring.
Although unusual, the event indicator can be omitted, in which case
all subjects are assumed to have an event.
</p>
</td></tr>
<tr><td><code id="Surv_+3A_time2">time2</code></td>
<td>

<p>ending time of the interval for interval censored  or counting
process data only.  Intervals are assumed to be open on the left and
closed on the right, <code>(start, end]</code>.  For counting process
data, <code>event</code> indicates whether an event occurred at the end of
the interval.
</p>
</td></tr>
<tr><td><code id="Surv_+3A_type">type</code></td>
<td>

<p>character string specifying the type of censoring. Possible values
are <code>"right"</code>, <code>"left"</code>, <code>"counting"</code>,
<code>"interval"</code>, <code>"interval2"</code> or <code>"mstate"</code>.  
</p>
</td></tr>
<tr><td><code id="Surv_+3A_origin">origin</code></td>
<td>

<p>for counting process data, the hazard function origin.  This option
was intended to be used in conjunction with a model containing
time dependent
strata in order to align the subjects properly when they cross over
from one strata to another, but it has rarely proven useful.</p>
</td></tr>
<tr><td><code id="Surv_+3A_x">x</code></td>
<td>

<p>any R object.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When the <code>type</code> argument is missing the code assumes a type based
on the following rules:
</p>

<ul>
<li><p> If there are two unnamed arguments, they will match <code>time</code> and
<code>event</code> in that order.  If there are three unnamed arguments
they match <code>time</code>, <code>time2</code> and <code>event</code>.
</p>
</li>
<li><p> If the event variable is a factor then type <code>mstate</code> is
assumed.  Otherwise type <code>right</code> if there is no <code>time2</code>
argument, and type <code>counting</code> if there is.
</p>
</li></ul>

<p>As a consequence the <code>type</code> argument will normally be omitted.
</p>
<p>When the survival type is &quot;mstate&quot; then the status variable will be
treated as a factor.  The first level of the factor is taken to
represent censoring and remaining ones a transition to the given
state.  (If the status variable is a factor then <code>mstate</code> is assumed.)
</p>
<p>Interval censored data can be represented in two ways.  For the first
use <code>type = "interval"</code> and the codes shown above.  In that usage the
value of the <code>time2</code> argument is ignored unless event=3.
The second approach is to think of each observation as a time
interval with (-infinity, t2) for left censored, (t1, infinity) for
right censored, (t,t) for exact and (t1, t2) for an interval.
This is the approach used for type = interval2.  Infinite values can
be represented either by actual infinity (Inf) or NA.
The second form has proven to be the more useful one.
</p>
<p>Presently, the only methods allowing interval censored data are the 
parametric models computed by <code>survreg</code> and survival curves
computed by <code>survfit</code>; for both of these, 
the distinction between open and closed intervals
is unimportant.  
The distinction is important for counting process data and 
the Cox model. 
</p>
<p>The function tries to distinguish between the use of 0/1 and 1/2 coding for 
censored data via the condition 
<code>if (max(status)==2)</code>. 
If 1/2 coding is used and all the subjects are censored, it will 
guess wrong.
In any questionable case it is safer to use logical coding,
e.g., <code>Surv(time, status==3)</code> would indicate that '3' is
the code for an event.
For multi-state survival the status variable will be a factor, whose
first level is assumed to correspond to censoring.
</p>
<p>Surv objects can be subscripted either as a vector, e.g.
<code>x[1:3]</code> using a single subscript,
in which case the <code>drop</code> argument is ignored and the result will be
a survival object; 
or as a matrix by using two subscripts.
If the second subscript is missing and <code>drop=F</code>
(the default),
the result of the subscripting will be a Surv object, e.g., 
<code>x[1:3,,drop=F]</code>,
otherwise the result will be a matrix (or vector), in accordance with
the default behavior for subscripting matrices. 
</p>


<h3>Value</h3>

<p>An object of class <code>Surv</code>.  There are methods for <code>print</code>,
<code>is.na</code>, and subscripting survival objects.   <code>Surv</code> objects
are implemented as a matrix of 2 or 3 columns that has further
attributes. These include the type (left censored, right censored,
counting process, etc.) and labels for the states for multi-state
objects.  Any attributes of the input arguments are also preserved
in <code>inputAttributes</code>.  This may be useful for other packages that
have attached further information to data items such as labels; none
of the routines in the survival package make use of these
values, however.
</p>
<p>In the case of <code>is.Surv</code>, a logical value <code>TRUE</code> if <code>x</code>
inherits from class <code>"Surv"</code>, otherwise an <code>FALSE</code>.
</p>


<h3>Note</h3>

<p>The use of 1/2 coding for status is an interesting historical
artifact.
For data contained on punch cards, IBM 360 Fortran treated blank as a zero,
which led to a policy within the Mayo Clinic section of Biostatistics to never
use &quot;0&quot; as a data value since one could not distinguish it from a
missing value.
Policy became habit, as is often the case, and the use of 1/2 coding for
alive/dead endured long after the demise of the punch cards that had
sired the practice.
At the time <code>Surv</code> was written many Mayo data sets still used this
obsolete convention, e.g., the <code>lung</code> data set found in the package.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coxph">coxph</a></code>,  
<code><a href="#topic+survfit">survfit</a></code>,  
<code><a href="#topic+survreg">survreg</a></code>, <code><a href="#topic+lung">lung</a></code>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>with(aml, Surv(time, status))
survfit(Surv(time, status) ~ ph.ecog, data=lung)
Surv(heart$start, heart$stop, heart$event) 
</code></pre>

<hr>
<h2 id='Surv-methods'>Methods for Surv objects</h2><span id='topic+Math.Surv'></span><span id='topic+Ops.Surv'></span><span id='topic+Summary.Surv'></span><span id='topic+anyDuplicated.Surv'></span><span id='topic+as.character.Surv'></span><span id='topic+as.data.frame.Surv'></span><span id='topic+as.matrix.Surv'></span><span id='topic+c.Surv'></span><span id='topic+duplicated.Surv'></span><span id='topic+format.Surv'></span><span id='topic+head.Surv'></span><span id='topic+is.na.Surv'></span><span id='topic+length.Surv'></span><span id='topic+mean.Surv'></span><span id='topic+median.Surv'></span><span id='topic+names.Surv'></span><span id='topic+names+3C-.Surv'></span><span id='topic+quantile.Surv'></span><span id='topic+plot.Surv'></span><span id='topic+rep.Surv'></span><span id='topic+rep.int.Surv'></span><span id='topic+rep_len.Surv'></span><span id='topic+rev.Surv'></span><span id='topic+t.Surv'></span><span id='topic+tail.Surv'></span><span id='topic+unique.Surv'></span>

<h3>Description</h3>

<p>The list of methods that apply to <code>Surv</code> objects</p>


<h3>Usage</h3>

<pre><code class='language-R'>    ## S3 method for class 'Surv'
anyDuplicated(x, ...)
    ## S3 method for class 'Surv'
as.character(x, ...)
    ## S3 method for class 'Surv'
as.data.frame(x, ...)
    ## S3 method for class 'Surv'
as.matrix(x, ...)
    ## S3 method for class 'Surv'
c(...)
    ## S3 method for class 'Surv'
duplicated(x, ...)
    ## S3 method for class 'Surv'
format(x, ...)
    ## S3 method for class 'Surv'
head(x, ...)
    ## S3 method for class 'Surv'
is.na(x)
    ## S3 method for class 'Surv'
length(x)
    ## S3 method for class 'Surv'
mean(x, ...)
    ## S3 method for class 'Surv'
median(x, na.rm=FALSE, ...)
    ## S3 method for class 'Surv'
names(x)
    ## S3 replacement method for class 'Surv'
names(x) &lt;- value
    ## S3 method for class 'Surv'
quantile(x, probs, na.rm=FALSE, ...)
    ## S3 method for class 'Surv'
plot(x, ...)
    ## S3 method for class 'Surv'
rep(x, ...)
    ## S3 method for class 'Surv'
rep.int(x, ...)
    ## S3 method for class 'Surv'
rep_len(x, ...)
    ## S3 method for class 'Surv'
rev(x)
    ## S3 method for class 'Surv'
t(x)
    ## S3 method for class 'Surv'
tail(x, ...)
    ## S3 method for class 'Surv'
unique(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Surv-methods_+3A_x">x</code></td>
<td>
<p>a <code>Surv</code> object</p>
</td></tr>
<tr><td><code id="Surv-methods_+3A_probs">probs</code></td>
<td>
<p>a vector of probabilities</p>
</td></tr>
<tr><td><code id="Surv-methods_+3A_na.rm">na.rm</code></td>
<td>
<p>remove missing values from the calculation</p>
</td></tr>
<tr><td><code id="Surv-methods_+3A_value">value</code></td>
<td>
<p>a character vector of up to the same length as <code>x</code>, or
<code>NULL</code></p>
</td></tr>
<tr><td><code id="Surv-methods_+3A_...">...</code></td>
<td>
<p>other arguments to the method</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions extend the standard methods to <code>Surv</code> objects.
(There is no central index of R methods, so there may well be useful
candidates that the author has missed.)
The arguments and results from these are mostly as expected, with the
following further details:
</p>

<ul>
<li><p> The <code>as.character</code> function uses &quot;5+&quot; for right censored
at time 5, &quot;5-&quot; for left censored at time 5, &quot;[2,7]&quot; for an
observation that was interval censored between 2 and 7,
&quot;(1,6]&quot; for a counting process data denoting an observation which
was at risk from time 1 to 6, with an event at time 6, and
&quot;(1,6+]&quot; for an observation over the same interval but not ending
with and event.
For a multi-state survival object the type of event is appended to
the event time using &quot;:type&quot;.
</p>
</li>
<li><p> The <code>print</code> and <code>format</code> methods make use of
<code>as.character</code>.
</p>
</li>
<li><p> The <code>length</code> of a <code>Surv</code> object is the number of
survival times it contains, not the number of items required to
encode it, e.g., <code>x &lt;- Surv(1:4, 5:8, c(1,0,1,0)); length(x)</code>
has a value of 4.
Likewise <code>names(x)</code> will be NULL or a vector of length 4.
(For technical reasons, any names are actually stored in the
<code>rownames</code> attribute of the object.)
</p>
</li>
<li><p> For a multi-state survival object <code>levels</code> returns the
names of the endpoints, otherwise it is NULL.
</p>
</li>
<li><p> The <code>median</code>, <code>quantile</code> and <code>plot</code> methods
first construct a survival curve using <code>survfit</code>, then apply
the appropriate method to that curve.
</p>
</li>
<li><p> The <code>xtfrm</code> method, which underlies sort and order,
sorts by time, with censored after uncensored within a tied time.
For an interval censored observation the midpoint is used.
For (time1, time2) counting process data, sorting is by time2, censoring,
and then time1.
</p>
</li>
<li><p> The <code>unique</code> method treats censored and uncensored
observations at the same time as distinct, it returns a Surv object.
</p>
</li>
<li><p> The concatonation method <code>c()</code> is asymmetric, its first
argument determines the execution path.  For instance
<code>c(Surv(1:4), Surv(5:6))</code> will return a Surv object of length 6,
<code>c(Surv(1:4), 5:6)</code> will give an error, and
<code>c(5:6, Surv(1:4))</code> is equivalent to
<code>c(5:6, as.vector(Surv(1:4)))</code> which is a numeric of length 10. 
</p>
</li></ul>



<h3>See Also</h3>

 <p><code><a href="#topic+Surv">Surv</a></code></p>

<hr>
<h2 id='Surv2'>Create a survival object</h2><span id='topic+Surv2'></span>

<h3>Description</h3>

<p>Create a survival object from a timeline style data set. This will
almost always be the response variable in a formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Surv2(time, event, repeated=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Surv2_+3A_time">time</code></td>
<td>
<p>a timeline variable, such as age, time from enrollment,
date, etc.</p>
</td></tr>
<tr><td><code id="Surv2_+3A_event">event</code></td>
<td>
<p>the outcome at that time.  This can be a 0/1 variable,
TRUE/FALSE, or a factor.
If the latter, the first level of the factor corresponds to
&lsquo;no event was observed at this time&rsquo;.</p>
</td></tr>
<tr><td><code id="Surv2_+3A_repeated">repeated</code></td>
<td>
<p>if the same level of the outcome repeats, without an
intervening event of another type, should this be treated as a new event?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is still experimental. 
</p>
<p>When used in a <code>coxph</code> or <code>survfit</code> model,
Surv2 acts as a trigger to internally convert a timeline style data
set into counting process style data, which is then acted on by the
routine.
</p>
<p>The <code>repeated</code> argument controls how repeated instances of the same event
code are treated.  If TRUE, they are treated as new events, an example
where this might be desired is repeated infections in a subject.
If FALSE, then repeats are not a new
event.  An example would be a data set where we wanted to use
diabetes, say, as an endpoint, but this is repeated at each medical
visit.
</p>


<h3>Value</h3>

<p>An object of class <code>Surv2</code>.  There are methods for <code>print</code>,
<code>is.na</code> and subscripting.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Surv2data">Surv2data</a></code>, <code><a href="#topic+coxph">coxph</a></code>,  
<code><a href="#topic+survfit">survfit</a></code>  
</p>

<hr>
<h2 id='Surv2data'>Convert data from timecourse to (time1,time2) style
</h2><span id='topic+Surv2data'></span>

<h3>Description</h3>

<p>The multi-state survival functions <code>coxph</code> and <code>survfit</code>
allow for two forms of input data.  This routine converts between them.
The function is normally called behind the scenes when <code>Surv2</code> is
as the response. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Surv2data(formula, data, subset, id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Surv2data_+3A_formula">formula</code></td>
<td>
<p>a model formula</p>
</td></tr>
<tr><td><code id="Surv2data_+3A_data">data</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="Surv2data_+3A_subset">subset</code></td>
<td>
<p>optional, selects rows of the data to be retained</p>
</td></tr>
<tr><td><code id="Surv2data_+3A_id">id</code></td>
<td>
<p>a variable that identified multiple rows for the same
subject, normally found in the referenced data set</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For timeline style data, each row is uniquely identified by an
(identifier, time) pair.  The time could be a date, time from entry to a
study, age, etc, (there may often be more than one time variable).
The identifier and time cannot be missing.
The remaining covariates represent values that were observed at that
time point. Often, a given covariate is observed at only a subset of
times and is missing at others.  At the time of death, in particular,
often only the identifier, time, and status indicator are known.
</p>
<p>In the resulting data set missing covariates are replaced by their
last known value, and the response y will be a Surv(time1, time2,
endpoint) object.
</p>


<h3>Value</h3>

<p>a list with elements
</p>
<table>
<tr><td><code>mf</code></td>
<td>
<p>an updated model frame (fewer rows, unchanged columns)</p>
</td></tr>
<tr><td><code>S2.y</code></td>
<td>
<p>the constructed response variable</p>
</td></tr>
<tr><td><code>S2.state</code></td>
<td>
<p>the current state for each of the rows</p>
</td></tr>
</table>

<hr>
<h2 id='survcheck'>Checks of a survival data set</h2><span id='topic+survcheck'></span>

<h3>Description</h3>

<p>Perform a set of consistency checks on survival data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survcheck(formula, data, subset, na.action, id, istate, istate0="(s0)", 
timefix=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survcheck_+3A_formula">formula</code></td>
<td>
<p>a model formula with a <code>Surv</code> object as the
response</p>
</td></tr>
<tr><td><code id="survcheck_+3A_data">data</code></td>
<td>
<p>data frame in which to find the <code>id</code>,
<code>istate</code> and formula variables</p>
</td></tr>
<tr><td><code id="survcheck_+3A_subset">subset</code></td>
<td>
<p>expression indicating which subset of the rows of data
should be used in the fit.    All observations are included by default. 
</p>
</td></tr>
<tr><td><code id="survcheck_+3A_na.action">na.action</code></td>
<td>

<p>a missing-data filter function.  This is applied to the model.frame
after any 
subset argument has been used.  Default is <code>options()\$na.action</code>. 
</p>
</td></tr>
<tr><td><code id="survcheck_+3A_id">id</code></td>
<td>
<p>an identifier that labels unique subjects</p>
</td></tr>
<tr><td><code id="survcheck_+3A_istate">istate</code></td>
<td>
<p>an optional vector giving the current state at the start
of each interval</p>
</td></tr>
<tr><td><code id="survcheck_+3A_istate0">istate0</code></td>
<td>
<p>default label for the initial state of each subject (at
their first interval) when <code>istate</code> is missing</p>
</td></tr>
<tr><td><code id="survcheck_+3A_timefix">timefix</code></td>
<td>
<p>process times through the <code>aeqSurv</code> function to
eliminate potential roundoff issues.</p>
</td></tr>
<tr><td><code id="survcheck_+3A_...">...</code></td>
<td>
<p>other arguments, which are ignored (but won't give an
error if someone added <code>weights</code> for instance)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This routine will examine a multi-state data set for consistency of
the data.  The basic rules are that if a subject is at risk they have
to be somewhere, can not be two places at once, and should make
sensible transitions from state to state. It reports the number of
instances of the following conditions:
</p>

<dl>
<dt>overlap</dt><dd><p>two observations for the same subject that overlap in
time, e.g. intervals of (0, 100) and (90, 120).  
If <code>y</code> is simple (time, status) survival then 
intervals implicitly start at 0, so in that case any duplicate
identifiers will generate an overlap.</p>
</dd>
<dt>gap</dt><dd><p>one or more gaps in a subject's timeline; where they are
in the same state at their return as when they left.</p>
</dd>
<dt>jump</dt><dd><p>a hole in a subject's timeline, where they are in one
state at the end of the prior interval, but a new state in the
at the start subsequent interval.</p>
</dd>
<dt>teleport</dt><dd><p>two adjacent intervals for a subject, with the
first interval ending in one state and the subsequent interval
starting in another.  They have instantaneously changed states
in 0 units of time.</p>
</dd>
<dt>duplicate</dt><dd><p>not currently used</p>
</dd>
</dl>

<p>The total number of occurences of each is present in the <code>flags</code>
vector. Optional components give the location and identifiers of the
flagged observations.
The <code>Surv</code> function has already flagged any 0 length intervals as errors.
</p>
<p>One important caveat is that survcheck does not deal with reuse of an id
value. For instance, a multi-institutional data set where the same
subject identifier happens to have been used for two different
subjects in two different institutions.  The routine is likely
generate a &quot;false positive&quot; error in this case, but this is simply
unavoidable.  Since the routine is used internally by <code>survfit</code>,
<code>coxph</code>, etc. the same errors will appear in other routines in
the survival package.
</p>


<h3>Value</h3>

<p>a list with components
</p>
<table>
<tr><td><code>states</code></td>
<td>
<p>the vector of possible states, a union of what appears
in the Surv object and <code>istate</code>, with initial states first</p>
</td></tr>
<tr><td><code>transitions</code></td>
<td>
<p>a matrix giving the count of transitions from one
state to another</p>
</td></tr>
<tr><td><code>statecount</code></td>
<td>
<p>table of the number of visits per state, e.g., 18
subjects had 2 visits to the &quot;infection&quot; state</p>
</td></tr>
<tr><td><code>flags</code></td>
<td>
<p>a vector giving the counts of each check</p>
</td></tr>
<tr><td><code>istate</code></td>
<td>
<p>a constructed istate that best satisfies all the checks</p>
</td></tr>
<tr><td><code>overlap</code></td>
<td>
<p>a list with the row number and id of overlaps (not
present if there are no overlaps)</p>
</td></tr>
<tr><td><code>gaps</code></td>
<td>
<p>a list with the row number and id of gaps (not present if
there are no gaps)</p>
</td></tr>
<tr><td><code>teleport</code></td>
<td>
<p>a list with the row number and id of inconsistent
rows (not present if there are none)</p>
</td></tr>
<tr><td><code>jumps</code></td>
<td>
<p>a list with the row number and id of jumps (not present
if there are no jumps)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For data sets with time-dependent covariates, a given subject will often
have intermediate rows with a status of &lsquo;no event at this time&rsquo;, coded
as the first level of the factor variable in the Surv() call.
For instance a subject who started in state 'a' at time 0, transitioned to state
'b' at time 10, had a covariate <code>x</code> change from 135 to 156 at time
20, and a final transition to state 'c' at time 30.
The response would be <code>Surv(c(0, 10, a), c(10, 20, censor),
  c(20,0,c))</code> where the state variable is a factor with levels of censor,
a, b, c.
The state variable records <em>changes</em> in state, and there was no
change at time 20.
The <code>istate</code> variable would be (a, b, b); it contains the <em>current</em>
state, and the value is unchanged when status = censored.
(It behaves like a <code>tdc</code> variable from <code>tmerge</code>).
</p>
<p>The intermediate time above is not actually censoring, i.e., a point at
which follow-up for the observation ceases.
The 'censor' label is traditional, but 'none' may be a more accurate choice.
</p>
<p>When there are intermediate observations <code>istate</code> is not
simply a lagged version of the state, and may be more challenging to
create.
One approach is to let <code>survcheck</code> do the work: call it with
an <code>istate</code> argument that is correct for the first row of each
subject, or no <code>istate</code> argument at all, and then insert the
returned value into a data frame.
</p>

<hr>
<h2 id='survcondense'>Shorten a (time1, time2) survival dataset
</h2><span id='topic+survcondense'></span>

<h3>Description</h3>

<p>Counting process data sets can sometimes grow to be unweildy, this can
be used to compact one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survcondense(formula, data, subset, weights, na.action= na.pass, id, 
             start = "tstart", end = "tstop", event = "event")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survcondense_+3A_formula">formula</code></td>
<td>

<p>a formula object, with the response on the left of a <code>~</code> operator, and 
the terms on the right.  The response must be a survival object as 
returned by the <code>Surv</code> function. 
</p>
</td></tr>
<tr><td><code id="survcondense_+3A_data">data</code></td>
<td>

<p>a data.frame in which to interpret the variables named in 
the <code>formula</code> and the <code>id</code> argument
argument. 
</p>
</td></tr>
<tr><td><code id="survcondense_+3A_subset">subset</code></td>
<td>
<p>optional subset expression to apply to the data set</p>
</td></tr>
<tr><td><code id="survcondense_+3A_weights">weights</code></td>
<td>
<p>optional variable name for case weights</p>
</td></tr>
<tr><td><code id="survcondense_+3A_na.action">na.action</code></td>
<td>
<p>optional removal of missing values</p>
</td></tr>
<tr><td><code id="survcondense_+3A_id">id</code></td>
<td>
<p>variable name that identifies subjects</p>
</td></tr>
<tr><td><code id="survcondense_+3A_start">start</code></td>
<td>
<p>optional character string, giving the name of the start
time variable in the result</p>
</td></tr>
<tr><td><code id="survcondense_+3A_end">end</code></td>
<td>
<p>optional character string, giving the name of the stop
time variable in the result</p>
</td></tr>
<tr><td><code id="survcondense_+3A_event">event</code></td>
<td>
<p>optional character string, giving the name of the event
variable in the result</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Through the use of the <code>survSplit</code> and <code>tmerge</code> functions, a
counting process data set will gain more and more rows of data.
Occassionally it is useful to collapse this surplus back down, e.g.,
when interest is to be focused on only a few covariates, or for
debugging.  The right hand side of <code>formula</code> will often have only
a few variables, in this use.
</p>
<p>If a row of data is censored, and represents the same covariates and
identifier as the row below it, then the two rows can be merged
together using a single (time1, time2) interval.  The compression can
sometimes be large.
</p>
<p>The <code>start</code>, <code>stop</code> and <code>end</code> options are used when the
left hand side of the formula has expressions that are not a simple
name, e.g. <code>Surv(time1, time2, death | progression)</code> would be a
case where <code>event</code> is used to set the outcome variable's name.
</p>


<h3>Value</h3>

<p>a data frame</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+survSplit">survSplit</a></code>,<code><a href="#topic+tmerge">tmerge</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dim(aml)
test1 &lt;- survSplit(Surv(time, status) ~ ., data=aml, 
                   cut=c(10, 20, 30), id="newid")
dim(test1)

# remove the added rows
test2 &lt;- survcondense(Surv(tstart, time, status) ~ x, test1, id=newid)
dim(test2)
</code></pre>

<hr>
<h2 id='survdiff'>
Test Survival Curve Differences
</h2><span id='topic+survdiff'></span><span id='topic+print.survdiff'></span>

<h3>Description</h3>

<p>Tests if there is a difference between two or more survival curves using
the <code class="reqn">G^\rho</code> family of tests, or for a single curve against a known alternative.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survdiff(formula, data, subset, na.action, rho=0, timefix=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survdiff_+3A_formula">formula</code></td>
<td>

<p>a formula expression as for other survival models, of the form
<code>Surv(time, status) ~ predictors</code>.  For a one-sample test, the predictors
must consist of a single <code>offset(sp)</code> term, where <code>sp</code> is a vector giving the
survival probability of each subject.  For a k-sample test, each unique
combination of predictors defines a subgroup.
A <code>strata</code> term may be used to produce a stratified test.
To cause missing values in the predictors to be treated as a separate
group, rather than being omitted, use the <code>strata</code> function with its
<code>na.group=T</code> argument.
</p>
</td></tr>
<tr><td><code id="survdiff_+3A_data">data</code></td>
<td>

<p>an optional data frame in which to interpret the variables occurring in the
formula.
</p>
</td></tr>
<tr><td><code id="survdiff_+3A_subset">subset</code></td>
<td>

<p>expression indicating which subset of the rows of data should be used in
the fit.  This can be a logical vector (which is replicated to have
length equal to the number of observations), a numeric vector indicating
which observation numbers are to be included (or excluded if negative),
or a character vector of row names to be included.  All observations are
included by default.
</p>
</td></tr>
<tr><td><code id="survdiff_+3A_na.action">na.action</code></td>
<td>

<p>a missing-data filter function.  This is applied to the <code>model.frame</code> after any
subset argument has been used.  Default is <code>options()$na.action</code>.
</p>
</td></tr>
<tr><td><code id="survdiff_+3A_rho">rho</code></td>
<td>

<p>a scalar parameter that controls the type of test.
</p>
</td></tr>
<tr><td><code id="survdiff_+3A_timefix">timefix</code></td>
<td>
<p>process times through the <code>aeqSurv</code> function to
eliminate potential roundoff issues.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with components:
</p>
<table>
<tr><td><code>n</code></td>
<td>

<p>the number of subjects in each group.
</p>
</td></tr>
<tr><td><code>obs</code></td>
<td>

<p>the weighted observed number of events in each group.
If there are strata, this will be a matrix with one column per stratum.
</p>
</td></tr>
<tr><td><code>exp</code></td>
<td>

<p>the weighted expected number of events in each group.
If there are strata, this will be a matrix with one column per stratum.
</p>
</td></tr>
<tr><td><code>chisq</code></td>
<td>

<p>the chisquare statistic for a test of equality.
</p>
</td></tr>
<tr><td><code>var</code></td>
<td>

<p>the variance matrix of the test.
</p>
</td></tr>
<tr><td><code>strata</code></td>
<td>

<p>optionally, the number of subjects contained in each stratum.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>
<p>the p-value corresponding to the Chisquare statistic</p>
</td></tr>
</table>


<h3>Description</h3>

<p>This function implements the G-rho family of
Harrington and Fleming (1982), with weights on each death of <code class="reqn">S(t)^\rho</code>,
where <code class="reqn">S(t)</code> is the Kaplan-Meier estimate of survival.
With <code>rho = 0</code> this is the log-rank or Mantel-Haenszel test,
and with <code>rho = 1</code> it is equivalent to the Peto &amp; Peto modification
of the Gehan-Wilcoxon test.
</p>
<p>Peto and Peto show that the Gehan-Wilcoxon test can be badly biased if
the two groups have different censoring patterns, and proposed an
alternative.  Prentice and Marek later showed an actual example where this
issue occurs.  For most data sets the Gehan-Wilcoxon and
Peto-Peto-Prentice variant will hardly differ, however.
</p>
<p>If the right hand side of the formula consists only of an offset term,
then a one sample test is done.
To cause missing values in the predictors to be treated as a separate
group, rather than being omitted, use the <code>factor</code> function with its
<code>exclude</code> argument to recode the righ-hand-side covariate.
</p>


<h3>References</h3>

<p>Harrington, D. P. and Fleming, T. R. (1982).
A class of rank test procedures for censored survival data.
Biometrika, 553-566.
</p>
<p>Peto R. Peto and Peto, J. (1972) Asymptotically efficient rank invariant
test procedures (with discussion), JRSSA, 185-206.
</p>
<p>Prentice, R. and Marek, P. (1979)  A qualitative discrepancy between
censored data rank tests, Biometics, 861&ndash;867.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Two-sample test
survdiff(Surv(futime, fustat) ~ rx,data=ovarian)

## Stratified 7-sample test

survdiff(Surv(time, status) ~ pat.karno + strata(inst), data=lung)

## Expected survival for heart transplant patients based on
## US mortality tables
expect &lt;- survexp(futime ~ 1, data=jasa, cohort=FALSE,
                  rmap= list(age=(accept.dt - birth.dt), sex=1, year=accept.dt),
                  ratetable=survexp.us)
## actual survival is much worse (no surprise)
survdiff(Surv(jasa$futime, jasa$fustat) ~ offset(expect))

# The free light chain data set is close to the population.
e2 &lt;- survexp(futime ~ 1, data=flchain, cohort=FALSE,
              rmap= list(age= age*365.25, sex=sex, 
                         year=as.Date(paste0(sample.yr, "-07-01"))),
              ratetable= survexp.mn)
survdiff(Surv(futime, death) ~ offset(e2), flchain)
</code></pre>

<hr>
<h2 id='survexp'>
Compute Expected Survival 
</h2><span id='topic+survexp'></span><span id='topic+print.survexp'></span>

<h3>Description</h3>

<p>Returns either the expected survival of a cohort of subjects, or the 
individual expected survival for each subject. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survexp(formula, data, weights, subset, na.action, rmap, times,
        method=c("ederer", "hakulinen", "conditional", "individual.h",
                 "individual.s"),
        cohort=TRUE, conditional=FALSE,
        ratetable=survival::survexp.us, scale=1,
        se.fit, model=FALSE, x=FALSE, y=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survexp_+3A_formula">formula</code></td>
<td>

<p>formula object.  The response variable is a vector of follow-up times 
and is optional.  The predictors consist of optional grouping variables 
separated by the <code>+</code> operator (as in <code>survfit</code>), and is often
<code>~1</code>, i.e., expected survival for the entire group.
</p>
</td></tr>
<tr><td><code id="survexp_+3A_data">data</code></td>
<td>

<p>data frame in which to interpret the variables named in 
the <code>formula</code>, <code>subset</code> and <code>weights</code> arguments. 
</p>
</td></tr>
<tr><td><code id="survexp_+3A_weights">weights</code></td>
<td>

<p>case weights.  This is most useful when conditional survival for a known
population is desired, e.g., the data set would contain all unique
age/sex combinations and the weights would be the proportion of each.
</p>
</td></tr>
<tr><td><code id="survexp_+3A_subset">subset</code></td>
<td>

<p>expression indicating a subset of the rows of <code>data</code> to be used in the fit. 
</p>
</td></tr>
<tr><td><code id="survexp_+3A_na.action">na.action</code></td>
<td>

<p>function to filter missing data. This is applied to the model frame after  
<code>subset</code> has been applied.  Default is <code>options()$na.action</code>.
</p>
</td></tr>
<tr><td><code id="survexp_+3A_rmap">rmap</code></td>
<td>

<p>an optional list that maps data set names to the ratetable names.  See
the details section below.
</p>
</td></tr>
<tr><td><code id="survexp_+3A_times">times</code></td>
<td>

<p>vector of follow-up times at which the resulting survival curve is  
evaluated.  If absent, the result will be reported for each unique  
value of the vector of times supplied in the response value of
the <code>formula</code>. 
</p>
</td></tr>
<tr><td><code id="survexp_+3A_method">method</code></td>
<td>
<p>computational method for the creating the survival curves.
The <code>individual</code> option does not create a curve, rather it
retrieves the predicted survival <code>individual.s</code> or cumulative
hazard <code>individual.h</code> for each subject.
The default is to use <code>method='ederer'</code> if the formula has no
response, and  <code>method='hakulinen'</code> otherwise.</p>
</td></tr>
<tr><td><code id="survexp_+3A_cohort">cohort</code></td>
<td>
<p>logical value.  This argument has been superseded by the
<code>method</code> argument.  To maintain backwards compatability,
if is present and FALSE, it implies <code>method='individual.s'</code>.</p>
</td></tr>
<tr><td><code id="survexp_+3A_conditional">conditional</code></td>
<td>
<p>logical value.  This argument has been superseded by the
<code>method</code> argument.  To maintain backwards compatability,
if it is present and TRUE it implies <code>method='conditional'</code>.</p>
</td></tr>
<tr><td><code id="survexp_+3A_ratetable">ratetable</code></td>
<td>
<p> a table of event rates,
such as <code>survexp.mn</code>, or a fitted Cox model.
Note the <code>survival::</code> prefix in the default argument is present
to avoid the (rare) case of a user who expects the default table
but just happens to have an object named &quot;survexp.us&quot; in their own
directory.</p>
</td></tr> 
<tr><td><code id="survexp_+3A_scale">scale</code></td>
<td>

<p>numeric value to scale the results.  If <code>ratetable</code> is in units/day, 
<code>scale = 365.25</code> causes the output to be reported in years. 
</p>
</td></tr>
<tr><td><code id="survexp_+3A_se.fit">se.fit</code></td>
<td>

<p>compute the standard error of the predicted survival.  
This argument is currently ignored.  Standard errors are not a defined
concept for population rate tables (they are treated as coming from a
complete census), and for Cox models the calculation is hard.  Despite
good intentions standard errors for this latter case have not been
coded and validated.
</p>
</td></tr>
<tr><td><code id="survexp_+3A_model">model</code>, <code id="survexp_+3A_x">x</code>, <code id="survexp_+3A_y">y</code></td>
<td>

<p>flags to control what is returned.  If any of these is true, then the
model frame, the model matrix, and/or the vector of response times will be
returned as components of the final result, with the same names as the
flag arguments.
</p>
</td></tr></table>


<h3>Details</h3>

<p>Individual expected survival is usually used in models or testing, to 
&lsquo;correct&rsquo; for the age and sex composition of a group of subjects. 
For instance, assume that birth date, entry date into the study, 
sex and actual survival time are all known for a group of subjects. 
The <code>survexp.us</code> population tables contain expected death rates 
based on calendar year, sex and age. 
Then 
</p>
<pre>
  haz &lt;- survexp(fu.time ~ 1, data=mydata, 
                      rmap = list(year=entry.dt, age=(birth.dt-entry.dt)),
                      method='individual.h')) 
</pre>
<p>gives for each subject the total hazard experienced up to their observed 
death time or last follow-up time (variable fu.time) 
This probability can be used as a rescaled time value in models: 
</p>
<pre>
glm(status ~ 1 + offset(log(haz)), family=poisson) 
glm(status ~ x + offset(log(haz)), family=poisson) 
</pre>
<p>In the first model, a test for intercept=0 is the one sample log-rank 
test of whether the observed group of subjects has equivalent survival to 
the baseline population.  The second model tests for an effect of variable 
<code>x</code> after adjustment for age and sex. 
</p>
<p>The ratetable being used may have different variable names than the user's
data set, this is dealt with by the <code>rmap</code> argument.  
The rate table for the above calculation was <code>survexp.us</code>, a call to
<code>summary{survexp.us}</code> reveals that it expects to have variables 
<code>age</code> = age in days, <code>sex</code>, and <code>year</code> = the date of study
entry, we create them in the <code>rmap</code> line.  The sex variable was not
mapped, therefore the function assumes that it exists in <code>mydata</code> in the
correct format.  (Note: for factors such as sex, the program will match on
any unique abbreviation, ignoring case.)
</p>
<p>Cohort survival is used to produce an overall survival curve.  This is then 
added to the Kaplan-Meier plot of the study group for visual comparison 
between these subjects and the population at large.  There are three common 
methods of computing cohort survival. 
In the &quot;exact method&quot; of Ederer the cohort is not censored, for this case 
no response variable is required in the formula.
Hakulinen recommends censoring 
the cohort at the anticipated censoring time of each patient, and Verheul 
recommends censoring the cohort at the actual observation time of each 
patient. 
The last of these is the conditional method. 
These are obtained by using the respective time values as the 
follow-up time or response in the formula.
</p>


<h3>Value</h3>

<p>if <code>cohort=TRUE</code> an object of class <code>survexp</code>, 
otherwise a vector of per-subject expected survival values. 
The former contains the number of subjects at risk 
and the expected survival for the cohort at each requested time.
The cohort survival is the hypothetical survival for a cohort of
subjects enrolled from the population at large, but matching the data
set on the factors found in the rate table.
</p>


<h3>References</h3>

<p>Berry, G. (1983). The analysis of mortality by the subject-years method. 
<em>Biometrics</em>, 39:173-84.
</p>
<p>Ederer, F., Axtell, L. and Cutler, S. (1961). 
The relative survival rate: a statistical methodology. 
<em>Natl Cancer Inst Monogr</em>, 6:101-21.
</p>
<p>Hakulinen, T. (1982). 
Cancer survival corrected for heterogeneity in patient withdrawal. 
<em>Biometrics</em>, 38:933-942.
</p>
<p>Therneau, T. and Grambsch, P. (2000).
Modeling survival data: Extending the Cox model.
Springer.  Chapter 10.
</p>
<p>Verheul, H., Dekker, E., Bossuyt, P., Moulijn, A. and Dunning, A. (1993). 
Background mortality in clinical survival studies. 
<em>Lancet</em>, 341: 872-875.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit">survfit</a></code>, <code><a href="#topic+pyears">pyears</a></code>,  <code><a href="#topic+survexp.us">survexp.us</a></code>,
<code><a href="#topic+ratetable">ratetable</a></code>, <code><a href="#topic+survexp.fit">survexp.fit</a></code>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 
# Stanford heart transplant data
#  We don't have sex in the data set, but know it to be nearly all males.
# Estimate of conditional survival  
fit1 &lt;- survexp(futime ~ 1, rmap=list(sex="male", year=accept.dt,   
          age=(accept.dt-birth.dt)), method='conditional', data=jasa)
summary(fit1, times=1:10*182.5, scale=365) #expected survival by 1/2 years

# Estimate of expected  survival stratified by prior surgery 
survexp(~ surgery, rmap= list(sex="male", year=accept.dt,  
	age=(accept.dt-birth.dt)), method='ederer', data=jasa,
        times=1:10 * 182.5) 

## Compare the survival curves for the Mayo PBC data to Cox model fit
## 
pfit &lt;-coxph(Surv(time,status&gt;0) ~ trt + log(bili) + log(protime) + age +
                platelet, data=pbc)
plot(survfit(Surv(time, status&gt;0) ~ trt, data=pbc), mark.time=FALSE)
lines(survexp( ~ trt, ratetable=pfit, data=pbc), col='purple')
</code></pre>

<hr>
<h2 id='survexp.fit'>
Compute Expected Survival 
</h2><span id='topic+survexp.fit'></span>

<h3>Description</h3>

<p>Compute expected survival times. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survexp.fit(group, x, y, times, death, ratetable)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survexp.fit_+3A_group">group</code></td>
<td>
<p>if there are multiple survival curves this identifies the group,
otherwise it is a constant.  Must be an integer.</p>
</td></tr>
<tr><td><code id="survexp.fit_+3A_x">x</code></td>
<td>
<p>A matrix whose columns match the dimensions of the <code>ratetable</code>, 
in the correct order. 
</p>
</td></tr>
<tr><td><code id="survexp.fit_+3A_y">y</code></td>
<td>

<p>the follow up time for each subject. 
</p>
</td></tr>
<tr><td><code id="survexp.fit_+3A_times">times</code></td>
<td>

<p>the vector of times at which a result will be computed. 
</p>
</td></tr>
<tr><td><code id="survexp.fit_+3A_death">death</code></td>
<td>

<p>a logical value, if <code>TRUE</code> the conditional survival is computed, 
if <code>FALSE</code> the cohort survival is computed. See
<code><a href="#topic+survexp">survexp</a></code> for more details.
</p>
</td></tr>
<tr><td><code id="survexp.fit_+3A_ratetable">ratetable</code></td>
<td>

<p>a rate table, such as <code>survexp.uswhite</code>. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For conditional survival <code>y</code> must be the time of last follow-up or death for 
each subject. 
For cohort survival it must be the potential censoring time for 
each subject, ignoring death. 
</p>
<p>For an exact estimate <code>times</code> should be a superset of <code>y</code>, so that each 
subject at risk is at risk for the entire sub-interval of time. 
For a large data set, however, this can use an inordinate amount of 
storage and/or compute time.  If the <code>times</code> spacing is more coarse than 
this, an actuarial approximation is used which should, however, be extremely 
accurate as long as all of the returned values are &gt; .99. 
</p>
<p>For a subgroup of size 1 and <code>times</code> &gt; <code>y</code>, 
the conditional method reduces to exp(-h) where 
h is the expected cumulative hazard for the subject over his/her 
observation time.  This is used to compute individual expected survival. 
</p>


<h3>Value</h3>

<p>A list containing the number of subjects and the expected survival(s) 
at each time point. 
If there are multiple groups, these will be 
matrices with one column per group. 
</p>


<h3>Warning</h3>

<p>Most users will call the higher level routine <code>survexp</code>. 
Consequently, this function has very few error checks on its input arguments. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survexp">survexp</a></code>,  <code><a href="#topic+survexp.us">survexp.us</a></code>.   
</p>

<hr>
<h2 id='survexp.object'>
Expected Survival Curve Object 
</h2><span id='topic+survexp.object'></span>

<h3>Description</h3>

<p>This class of objects is returned by the <code>survexp</code> class of functions
to represent a fitted survival curve. 
</p>
<p>Objects of this class have methods for <code>summary</code>, and inherit
the <code>print</code>, <code>plot</code>, <code>points</code> and <code>lines</code>
methods from <code>survfit</code>.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="survexp.object_+3A_surv">surv</code></td>
<td>

<p>the estimate of survival at time t+0. 
This may be a vector or a matrix. 
</p>
</td></tr>
<tr><td><code id="survexp.object_+3A_n.risk">n.risk</code></td>
<td>

<p>the number of subjects who contribute at this time.
</p>
</td></tr>
<tr><td><code id="survexp.object_+3A_time">time</code></td>
<td>

<p>the time points at which the curve has a step. 
</p>
</td></tr>
<tr><td><code id="survexp.object_+3A_std.err">std.err</code></td>
<td>

<p>the standard error of the cumulative hazard or -log(survival). 
</p>
</td></tr>
<tr><td><code id="survexp.object_+3A_strata">strata</code></td>
<td>

<p>if there are multiple curves, this component gives the number of elements of 
the <code>time</code> etc. vectors corresponding to the first curve,
the second curve, 
and so on.  The names of the elements are labels for the curves. 
</p>
</td></tr>
<tr><td><code id="survexp.object_+3A_method">method</code></td>
<td>
<p>the estimation method used.  One of &quot;Ederer&quot;, &quot;Hakulinen&quot;,
or &quot;conditional&quot;.</p>
</td></tr>
<tr><td><code id="survexp.object_+3A_na.action">na.action</code></td>
<td>

<p>the returned value from the na.action function, if any.  It will be used 
in the printout of the curve, e.g., the number of observations deleted due 
to missing values. 
</p>
</td></tr>
<tr><td><code id="survexp.object_+3A_call">call</code></td>
<td>

<p>an image of the call that produced the object. 
</p>
</td></tr>
</table>


<h3>Structure</h3>

<p>The following components must be included in a legitimate 
<code>survfit</code> 
object. 
</p>


<h3>Subscripts</h3>

<p>Survexp objects that contain multiple survival curves can be subscripted. 
This is most often used to plot a subset of the curves. 
</p>


<h3>Details</h3>

<p>In expected survival each subject from the data set is matched to a
hypothetical person from the parent population, matched on the
characteristics of the parent population.
The number at risk printed here is the number of those hypothetical
subject who are still part of the calculation.
In particular, for the Ederer method all hypotheticals are retained for
all time, so <code>n.risk</code> will be a constant.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.survfit">plot.survfit</a></code>,  
<code><a href="#topic+summary.survexp">summary.survexp</a></code>,
<code><a href="#topic+print.survfit">print.survfit</a></code>,
<code><a href="#topic+survexp">survexp</a></code>.   
</p>

<hr>
<h2 id='survfit'>Create survival curves</h2><span id='topic+survfit'></span>

<h3>Description</h3>

<p>This function creates survival curves from either a formula (e.g. the 
Kaplan-Meier), a previously fitted Cox model, or a previously fitted
accelerated failure time model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survfit(formula, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survfit_+3A_formula">formula</code></td>
<td>
<p>either a formula or a previously fitted model</p>
</td></tr>
<tr><td><code id="survfit_+3A_...">...</code></td>
<td>
<p>other arguments to the specific method</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A survival curve is based on a tabulation of the number at risk and
number of events at each unique death time.  When time is a floating
point number the definition of &quot;unique&quot; is subject to interpretation.
The code uses factor() to define the set.
For further details see the documentation for the appropriate method, i.e.,
<code>?survfit.formula</code> or <code>?survfit.coxph</code>.  
</p>
<p>A survfit object may contain a single curve, a set of curves (vector), a
matrix of curves, or even a 3 way array: <code>dim(fit)</code> will reveal
the dimensions.
Predicted curves from a <code>coxph</code> model have one row for each
stratum in the Cox model fit and one column for each specified
covariate set.
Curves from a multi-state model have one row for each stratum and
a column for each state, the strata correspond to predictors on the
right hand side of the equation.  The default printing and plotting
order for curves is by column, as with other matrices.
</p>


<h3>Value</h3>

<p>An object of class <code>survfit</code> containing one or more survival curves.
</p>


<h3>Note</h3>

<p>Older releases of the code also allowed the specification for 
a single curve
to omit the right hand of the formula, i.e.,
<code>survfit(Surv(time, status))</code>, in which case the formula argument
is not actually a formula.
Handling this case required some non-standard and fairly fragile 
manipulations, and this case is no longer supported.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit.formula">survfit.formula</a></code>,
<code><a href="#topic+survfit.coxph">survfit.coxph</a></code>,
<code><a href="#topic+survfit.object">survfit.object</a></code>, <code><a href="#topic+print.survfit">print.survfit</a></code>,
<code><a href="#topic+plot.survfit">plot.survfit</a></code>, <code><a href="#topic+quantile.survfit">quantile.survfit</a></code>,
<code><a href="#topic+residuals.survfit">residuals.survfit</a></code>, <code><a href="#topic+summary.survfit">summary.survfit</a></code>
</p>

<hr>
<h2 id='survfit.coxph'>
Compute a Survival Curve from a Cox model
</h2><span id='topic+survfit.coxph'></span><span id='topic+survfit.coxphms'></span>

<h3>Description</h3>

<p>Computes the predicted survivor function for a Cox proportional 
hazards model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coxph'
survfit(formula, newdata, 
        se.fit=TRUE, conf.int=.95, individual=FALSE, stype=2, ctype,
        conf.type=c("log","log-log","plain","none", "logit", "arcsin"),
        censor=TRUE, start.time, id, influence=FALSE,
        na.action=na.pass, type, time0=FALSE, ...)
## S3 method for class 'coxphms'
survfit(formula, newdata, 
        se.fit=FALSE, conf.int=.95, individual=FALSE, stype=2, ctype,
        conf.type=c("log","log-log","plain","none", "logit", "arcsin"),
        censor=TRUE, start.time, id, influence=FALSE,
        na.action=na.pass, type, p0=NULL, time0= FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survfit.coxph_+3A_formula">formula</code></td>
<td>

<p>A <code>coxph</code> object. 
</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_newdata">newdata</code></td>
<td>

<p>a data frame with the same variable names as those that appear 
in the <code>coxph</code> formula. One curve is produced per row.
The curve(s) produced will be representative of a cohort whose 
covariates correspond to the values in <code>newdata</code>. 
</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_se.fit">se.fit</code></td>
<td>

<p>a logical value indicating whether standard errors should be 
computed.  Default is <code>TRUE</code> for standard models, <code>FALSE</code>
for multi-state (code not yet present for that case.)
</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_conf.int">conf.int</code></td>
<td>

<p>the level for a two-sided confidence interval on the survival curve(s). 
Default is 0.95. 
</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_individual">individual</code></td>
<td>
<p>deprecated argument, replaced by the general
<code>id</code></p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_stype">stype</code></td>
<td>
<p>computation of the survival curve, 1=direct, 2=
exponenial of the cumulative hazard.</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_ctype">ctype</code></td>
<td>
<p>whether the cumulative hazard computation should have a
correction for ties, 1=no, 2=yes.</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_conf.type">conf.type</code></td>
<td>

<p>One of <code>"none"</code>, <code>"plain"</code>, <code>"log"</code> (the default),
<code>"log-log"</code> or <code>"logit"</code>.  Only
enough of the string to uniquely identify it is necessary.
The first option causes confidence intervals not to be
generated.  The second causes the standard intervals
<code>curve +- k *se(curve)</code>, where k is determined from
<code>conf.int</code>.  The log option calculates intervals based on the
cumulative hazard or log(survival). The log-log option uses
the log hazard or log(-log(survival)), and the logit
log(survival/(1-survival)).
</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_censor">censor</code></td>
<td>
<p>if FALSE time points at which there are no events (only
censoring) are not included in the result.</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_id">id</code></td>
<td>
<p>optional variable name of subject identifiers.  If this is
present, it will be search for in the <code>newdata</code> data frame.
Each group of rows in <code>newdata</code> with the same subject id represents
the covariate path through time of a single subject, and the result
will contain one curve per subject.  If the <code>coxph</code> fit had
strata then that must also be specified in <code>newdata</code>.
If <code>newid</code> is not present, then each
individual row of <code>newdata</code> is presumed to represent a distinct
subject.</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_start.time">start.time</code></td>
<td>
<p>optional starting time, a single numeric value.
If present the returned curve contains survival after
<code>start.time</code> conditional on surviving to <code>start.time</code>.
</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_influence">influence</code></td>
<td>
<p>option to return the influence values</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_na.action">na.action</code></td>
<td>
<p>the na.action to be used on the newdata argument</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_type">type</code></td>
<td>
<p>older argument that encompassed <code>stype</code> and
<code>ctype</code>, now deprecated</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_p0">p0</code></td>
<td>
<p>optional, a vector of probabilities.  The returned curve
will be for a cohort with this mixture of starting states.  Most
often a single state is chosen</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_time0">time0</code></td>
<td>
<p>include the starting time for the curve in the output</p>
</td></tr>
<tr><td><code id="survfit.coxph_+3A_...">...</code></td>
<td>
<p>for future methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This routine produces Pr(state) curves based on a <code>coxph</code>
model fit. For single state models it produces the single curve for
S(t) = Pr(remain in initial state at time t), known as the survival
curve; for multi-state models a matrix giving probabilities for all states.
The <code>stype</code> argument states the type of estimate, and defaults
to the exponential of the cumulative hazard, better known as the Breslow
estimate.  For a multi-state Cox model this involves the exponential
of a matrix. 
The argument <code>stype=1</code> uses a non-exponential or &lsquo;direct&rsquo;
estimate.  For a single endpoint coxph model the code evaluates the
Kalbfleich-Prentice estimate, and for a multi-state model it uses an
analog of the Aalen-Johansen estimator.  The latter approach is the
default in the <code>mstate</code> package.
</p>
<p>The <code>ctype</code> option affects the estimated cumulative hazard, and
if <code>stype=2</code> the estimated P(state) curves as well.  If not
present it is chosen so as to be concordant with the 
<code>ties</code> option in the <code>coxph</code> call. (For multistate
<code>coxphms</code> objects, only <code>ctype=1</code> is currently implemented.)
Likewise
the choice between a model based and robust variance estimate for the
curve will mirror the choice made in the <code>coxph</code> call,
any clustering is also inherited from the parent model.
</p>
<p>If the <code>newdata</code> argument is missing, then a curve is produced
for a single &quot;pseudo&quot; subject with
covariate values equal to the <code>means</code> component of the fit.
The resulting curve(s) rarely make scientific sense, but 
the default remains due to an unwarranted belief by many that it
represents an &quot;average&quot; curve, and it's use as a default in other
packages. For coxph, the <code>means</code> component will contain the value
0 for any 0/1 or TRUE/FALSE variables, and the mean value in the data
for others.  Its primary reason for this default is to
increase numerical accuracy in internal computations of the routine
via recentering the X matrix;
there is no reason to assume this represents an &lsquo;interesting&rsquo;
hypothetical subject for prediction of their survival curve. 
Users are strongly advised to use the newdata argument;
predictions from a multistate coxph model require the newdata argument.
</p>
<p>If the <code>coxph</code> model contained an offset term, then the data set
in the <code>newdata</code> argument should also contain that variable.
</p>
<p>When the original model contains time-dependent covariates, then the
path of that covariate through time needs to be specified in order to
obtain a predicted curve. This requires <code>newdata</code> to contain
multiple lines for each hypothetical subject which gives the covariate
values, time interval, and strata for each line (a subject can change
strata), along with an <code>id</code> variable
which demarks which rows belong to each subject.
The time interval must have the same (start, stop, status)
variables as the original model: although the status variable is not
used and thus can be set to a dummy value of 0 or 1, it is necessary for
the response to be recognized as a <code>Surv</code> object.
Last, although predictions with a time-dependent covariate path can be
useful, it is very easy to create a prediction that is senseless.  Users
are encouraged to seek out a text that discusses the issue in detail.
</p>
<p>When a model contains strata but no time-dependent covariates the user
of this routine has a choice.
If newdata argument does not contain strata variables then the returned
object will be a matrix of survival curves with one row for each strata
in the model and one column for each row in newdata.
(This is the historical behavior of the routine.)
If newdata does contain strata variables, then the result will contain
one curve per row of newdata, based on the indicated stratum of the
original model.  In the rare case of a model with strata by covariate
interactions the strata variable must be included in newdata, the
routine does not allow it to be omitted (predictions become too confusing).
(Note that the model Surv(time, status) ~ age*strata(sex) expands internally to
strata(sex) + age:sex; the sex variable is needed for the second term
of the model.)
</p>
<p>See <code><a href="#topic+survfit">survfit</a></code> for more details about the counts (number of
events, number at risk, etc.)
</p>


<h3>Value</h3>

<p>an object of class <code>"survfit"</code>.  
See <code>survfit.object</code> for 
details. Methods defined for survfit objects are  
<code>print</code>, <code>plot</code>, 
<code>lines</code>, and <code>points</code>. 
</p>


<h3>Notes</h3>

<p>If the following pair of lines is used inside of another function then
the <code>model=TRUE</code> argument must be added to the coxph call:
<code>fit &lt;- coxph(...); survfit(fit)</code>.
This is a consequence of the non-standard evaluation process used by the
<code>model.frame</code> function when a formula is involved.
</p>
<p>Let <code class="reqn">\log[S(t; z)]</code> be the log of the survival curve
for a fixed covariate vector <code class="reqn">z</code>, then
<code class="reqn">\log[S(t; x)]= e^{(x-z)\beta}\log[S(t; z)]</code>
is the log of the curve for any new covariate vector <code class="reqn">x</code>.  
There is an unfortunate tendency to refer to the reference curve with
<code class="reqn">z=0</code> as &lsquo;THE&rsquo; baseline hazard.  However, any <code class="reqn">z</code> can be used as
the reference point, and more importantly, if <code class="reqn">x-z</code> is large the
compuation can suffer severe roundoff error.  It is always safest to
provide the desired <code class="reqn">x</code> values directly via <code>newdata</code>.
</p>


<h3>References</h3>

<p>Fleming, T. H. and Harrington, D. P. (1984).  Nonparametric estimation of the 
survival distribution in censored data.  <em>Comm. in Statistics</em>  
<b>13</b>, 2469-86. 
</p>
<p>Kalbfleisch, J. D. and Prentice, R. L. (1980).
<em>The Statistical Analysis of Failure Time Data.</em>
New York:Wiley. 
</p>
<p>Link, C. L. (1984). Confidence intervals for the survival 
function using Cox's proportional hazards model with  
covariates.  <em>Biometrics</em>  
<b>40</b>, 601-610.
</p>
<p>Therneau T and Grambsch P (2000), Modeling Survival Data: Extending the
Cox Model, Springer-Verlag.
</p>
<p>Tsiatis, A. (1981). A large sample study of the estimate 
for the integrated hazard function in Cox's regression 
model for survival data. <em>Annals of Statistics</em>  
<b>9</b>, 93-108. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+print.survfit">print.survfit</a></code>,  
<code><a href="#topic+plot.survfit">plot.survfit</a></code>,  
<code><a href="#topic+lines.survfit">lines.survfit</a></code>,   
<code><a href="#topic+coxph">coxph</a></code>,  
<code><a href="#topic+Surv">Surv</a></code>,  
<code><a href="#topic+strata">strata</a></code>.   
</p>

<hr>
<h2 id='survfit.formula'>
Compute a Survival Curve for Censored Data 
</h2><span id='topic+survfit.formula'></span><span id='topic++5B.survfit'></span>

<h3>Description</h3>

<p>Computes an estimate of a survival curve for censored data using
the Aalen-Johansen estimator.  For ordinary (single event) survival
this reduces to the Kaplan-Meier estimate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
survfit(formula, data, weights, subset, na.action,  
        stype=1, ctype=1, id, cluster, robust, istate, timefix=TRUE,
        etype, model=FALSE, error, entry=FALSE, time0=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survfit.formula_+3A_formula">formula</code></td>
<td>

<p>a formula object, which must have a 
<code>Surv</code> object as the  
response on the left of the <code>~</code> operator and, if desired, terms  
separated by + operators on the right. 
One of the terms may be a <code>strata</code> object.
For a single survival curve the right hand side should be <code>~ 1</code>.
</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_data">data</code></td>
<td>

<p>a data frame in which to interpret the variables named in the formula, 
<code>subset</code> and <code>weights</code> arguments. 
</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_weights">weights</code></td>
<td>

<p>The weights must be nonnegative and it is strongly recommended that  
they be strictly positive, since zero weights are ambiguous, compared 
to use of the <code>subset</code> argument.
</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_subset">subset</code></td>
<td>

<p>expression saying that only a subset of the rows of the data 
should be used in the fit. 
</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_na.action">na.action</code></td>
<td>

<p>a missing-data filter function, applied to the model frame, after any 
<code>subset</code> argument has been used. 
Default is <code>options()$na.action</code>. 
</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_stype">stype</code></td>
<td>
<p>the method to be used estimation of the survival curve:
1 = direct,  2 = exp(cumulative hazard). </p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_ctype">ctype</code></td>
<td>
<p>the method to be used for estimation of the cumulative
hazard: 1 = Nelson-Aalen formula, 2 = Fleming-Harrington correction
for tied events.</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_id">id</code></td>
<td>

<p>identifies individual subjects, when a given person can have multiple
lines of data.
</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_cluster">cluster</code></td>
<td>
<p>used to group observations for the infinitesimal
jackknife variance estimate, defaults to the value of id.</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_robust">robust</code></td>
<td>
<p>logical, should the function compute a robust variance.
For multi-state survival curves or interval censored data
this is true by default.
For single state data see details, below.</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_istate">istate</code></td>
<td>
<p>for multi-state models, identifies the initial state of
each subject or observation.  This also forces <code>time0 =TRUE</code>.</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_timefix">timefix</code></td>
<td>
<p>process times through the <code>aeqSurv</code> function to
eliminate potential roundoff issues.</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_etype">etype</code></td>
<td>

<p>a variable giving the type of event.  This has been superseded by
multi-state Surv objects and is deprecated; see example below.
</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_model">model</code></td>
<td>
<p>include a copy of the model frame in the output</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_error">error</code></td>
<td>
<p>this argument is no longer used</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_entry">entry</code></td>
<td>
<p>if TRUE, the output will contain <code>n.enter</code> which is
the number of observations entering the risk set at any time; extra
rows of output are created, if needed, for each unique entry time.
Only applicable if there is an <code>id</code> statement.
</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_time0">time0</code></td>
<td>
<p>if TRUE, the output will include estimates at the
starting point of the curve or &lsquo;time 0&rsquo;.  See discussion below.
</p>
</td></tr>
<tr><td><code id="survfit.formula_+3A_...">...</code></td>
<td>

<p>The following additional arguments are passed to internal functions
called by <code>survfit</code>.
</p>

<dl>
<dt>se.fit</dt><dd>
<p>a logical value indicating whether standard errors should be 
computed.  Default is <code>TRUE</code>. For a multistate model, where
the infinitesimal jackknife (robust) standar error is used, the
compute time for the standard error is O(ndp) where n = number of
observations, d = number of events and p = number of states,
while that for all other portions of the output
(<code>pstate</code>, <code>cumhaz</code> and counts) is O((n+d)p).
For a moderate to large data set the compute time difference
between nd and n+d can be huge; using <code>se.fit = FALSE</code> may
be a wise choice.
</p>
</dd>
<dt>conf.type</dt><dd>
<p>One of <code>"none"</code>, <code>"plain"</code>, <code>"log"</code> (the default),
<code>"log-log"</code>, <code>"logit"</code> or <code>"arcsin"</code>.  Only
enough of the string to uniquely identify it is necessary.
The first option causes confidence intervals not to be
generated.  The second causes the standard intervals
<code>curve +- k *se(curve)</code>, where k is determined from
<code>conf.int</code>.  The log option calculates intervals based on the
cumulative hazard or log(survival). The log-log option bases the
intervals on the log hazard or log(-log(survival)), the
logit option on log(survival/(1-survival))
and arcsin on arcsin(survival). 
</p>
</dd>
<dt>conf.lower</dt><dd>
<p>a character string to specify modified lower limits to the curve, the 
upper limit remains unchanged.  
Possible values are <code>"usual"</code> (unmodified), 
<code>"peto"</code>, 
and <code>"modified"</code>.  The modified lower limit 
is based on an &quot;effective n&quot; argument.  The confidence 
bands will agree with the usual calculation at each death time, but unlike 
the usual bands the confidence interval becomes wider at each censored 
observation.  The extra width is obtained by multiplying the usual 
variance by a factor m/n, where n is the number currently at risk and 
m is the number at risk at the last death time.  (The bands thus agree 
with the un-modified bands at each death time.) 
This is especially useful for survival curves with a long flat tail. 
</p>
<p>The Peto lower limit is based on the same &quot;effective n&quot; argument as the 
modified limit, but also replaces the usual Greenwood variance term with 
a simple approximation.  It is known to be conservative. 
</p>
</dd>
<dt>start.time</dt><dd>
<p>numeric value specifying a time to start calculating survival
information.
The resulting curve is the survival conditional on surviving to
<code>start.time</code>.
</p>
</dd>
<dt>conf.int</dt><dd>
<p>the level for a two-sided confidence interval on the survival curve(s). 
Default is 0.95. 
</p>
</dd>
<dt>influence</dt><dd><p>a logical value indicating whether to return the
infinitesimal jackknife (influence) values for each subject.
See details below.
</p>
</dd>
<dt>p0</dt><dd><p>this applies only to multi-state curves. 
An optional vector giving the initial probability across
the states. If this is missing, then p0 is estimated using the
frequency of the starting states of all observations at risk
at <code>start.time</code>, or if that is not specified, at
the time of the first event.</p>
</dd>
<dt>entry</dt><dd><p>by default, the survfit routines only return
information at the event/censoring times. If <code>entry=TRUE</code> then
also return a <code>n.enter</code> component containing the number
who joined the risk set at each time; if necessary add extra
rows to the output for each unique entry time. 
This is only applicable for (time1, time2) survival data, and
if there is an <code>id</code> statement.  If a single subject had
times of (0,10), (10, 20), (25,30) with an event at 30,
then time 10 is not an entry or censoring time,
but 20 counts as censored and 25 as an entry. 
</p>
</dd>
<dt>type</dt><dd><p>an older argument that combined <code>stype</code> and
<code>ctype</code>, now deprecated.  Legal values were &quot;kaplan-meier&quot;
which is equivalent to <code>stype=1, ctype=1</code>, &quot;fleming-harrington&quot;
which is equivalent to <code>stype=2, ctype=1</code>, and &quot;fh2&quot; which
is equivalent to <code>stype=2, ctype=2.</code>
</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

<p>If there is a <code>data</code> argument, then variables in the <code>formula</code>,
<code>weights</code>, <code>subset</code>, <code>id</code>, <code>cluster</code> and
<code>istate</code> arguments will be searched for in that data set.
</p>
<p>The routine returns both an estimated probability in state and an
estimated cumulative hazard estimate.
For simple survival the probability in state = probability alive, i.e,
the estimated survival. For multi-state it will be a matrix with one
row per time and a column per state, rows sum to 1.
The cumulative hazard estimate is the Nelson-Aalen (NA) estimate or the
Fleming-Harrington (FH) estimate, the latter includes a correction for
tied event times.  The estimated probability in state can estimated
either using the exponential of the cumulative hazard, or as a direct
estimate using the Aalen-Johansen approach.
For single state data the AJ estimate reduces to the Kaplan-Meier and
the probability in state to the survival curve; 
for competing risks data the AJ reduces to the cumulative incidence (CI)
estimator.
For backward compatability the <code>type</code> argument can be used instead.
</p>
<p>When the data set includes left censored or interval censored data (or both),
then the EM approach of Turnbull is used to compute the overall curve.
Currently this algorithm is very slow, only applies to simple survival
(not multi-state), and defaults to a robust variance.  Other R
packages are available which implement the iterative convex minorant
(ICM) algorithm for
interval censored data, which is much faster than Turnbull's method.
Based on Sun (2001) the robust variance may be preferred, as the naive estimate
ignores the estimation of the weights.  The standard estimate can be
obtained with <code>robust= FALSE</code>.
</p>
<p>Without interval or left censored data (the usual case) the
underlying algorithm for the routine is the Aalen-Johansen
estimate, of which the Kaplan-Meier (for single outcome data) and the
cumulative incidence (CI) estimate (for competing risks) are each a special
case. For multi-state, the estimate can be written as
<code class="reqn">p(t_0)H(t_1)H(t_2)\ldots</code> where <code class="reqn">p(t_0)</code>
is the prevalance vector across the states at starting
point <code class="reqn">t_0</code>, <code class="reqn">t_1, t_2, \ldots</code> are the
times at which events (transitions between states) occur, and H are
square transtion matrices with a row for each state.
</p>
<p>Starting point: When diffent subjects (<code>id</code>) start at different
time points, data using age as the time scale for instance,
deciding the default &quot;time 0&quot; can be complex.  This value is the
starting point for the restricted mean estimate (area under the
curve), the initial prevalence p0, and the first
row of output if <code>time0 = TRUE</code>.  The order of the decision is
</p>

<ol>
<li><p> For a 2 column response (simple survival or competing risks)
use the minimum of 0 and the smallest time value (times can be
negative).
</p>
</li>
<li><p> If all subjects start in the same state, start at the same time,
or if <code>p0</code> is specified, use the minimum observed starting
time.  If there is no <code>istate</code> argument all observations are
assumed to start in a state &quot;(s0)&quot;.
</p>
</li>
<li><p> Use the minimum observed event time, if the number at risk at
that time is &gt;0 for every curve that will be created.
</p>
</li>
<li><p> Use the minimum event time for each curve, separately.
</p>
</li></ol>

<p>The last two above are a failsafe to prevent the routine from basing
the initial prevalence of the states on none or only a handful of
observations.   That does not mean such curves
will be scientfically sensible: when using age scale the user may wish
to specify an explicit starting time.
If <code>time0 = TRUE</code> the first row of output for each curve will be
at the starting time, 
otherwise the first event time (for each curve separately).
</p>
<p>Robust variance:
If a <code>robust</code> is TRUE, or for multi-state
curves, then the standard
errors of the results will be based on an infinitesimal jackknife (IJ)
estimate, otherwise the standard model based estimate will be used.
For single state curves, the default for <code>robust</code> will be TRUE 
if one of: there is a <code>cluster</code> argument, there
are non-integer weights, or there is a <code>id</code> statement
and at least one of the id values has multiple events, and FALSE otherwise.
The default represents our best guess about when one would most
often desire a robust variance.
When there are non-integer case weights and (time1, time2) survival
data the routine is at an impasse: a robust variance likely is called
for, but requires either <code>id</code> or <code>cluster</code> information to be
done correctly; it will default to robust=FALSE if they are not present.
</p>
<p>With the IJ estimate, the leverage values themselves can be returned
as an array using the <code>influence</code> argument.
Be forwarned that this array can be huge. Post fit influence using the
<code>resid</code> method is more flexible and would normally be preferred,
in particular to get influence at only a select set of time points.
The <code>influence</code> option is currently used mostly in the package's
validity checks.
</p>
<p>Let <code class="reqn">U(t)</code> be the matrix of IJ values at time t, which has
one row per observation, one column per state. The robust variance
compuation uses the collapsed weighted matrix
<code>rowsum(wU, cluster)</code>,
where w is the vector of weights and cluster is the grouping (most often
the id). The result for each curve is an array with dimensions
(number of clusters, number of states, number of times), or a matrix
for single state data.  When there are multiple curves, the
influence is a list with one element per curve.
</p>


<h3>Value</h3>

<p>an object of class <code>"survfit"</code>.  
See <code>survfit.object</code> for 
details. Some of the methods defined for survfit objects are  
<code>print</code>, <code>plot</code>, 
<code>lines</code>, <code>points</code> and <code>residual</code>. 
</p>


<h3>References</h3>

<p>Dorey, F. J. and Korn, E. L. (1987).  Effective sample sizes for confidence 
intervals for survival probabilities.  <em>Statistics in Medicine</em> 
<b>6</b>, 679-87. 
</p>
<p>Fleming, T. H. and Harrington, D. P. (1984).  Nonparametric estimation of the 
survival distribution in censored data.  <em>Comm. in Statistics</em>  
<b>13</b>, 2469-86. 
</p>
<p>Kalbfleisch, J. D. and Prentice, R. L. (1980).
<em>The Statistical Analysis of Failure Time Data.</em>
New York:Wiley. 
</p>
<p>Kyle, R. A. (1997).
Moncolonal gammopathy of undetermined significance and solitary
plasmacytoma. Implications for progression to overt multiple myeloma},
<em>Hematology/Oncology Clinics N. Amer.</em>
<b>11</b>, 71-87.
</p>
<p>Link, C. L. (1984). Confidence intervals for the survival 
function using Cox's proportional hazards model with  
covariates.  <em>Biometrics</em>  
<b>40</b>, 601-610.
</p>
<p>Sun, J. (2001). Variance estimation of a survival function
for interval-censored data. <em>Stat Med</em> <b>20</b>, 1949-1957.
</p>
<p>Turnbull, B. W. (1974).  Nonparametric estimation of a survivorship
function with doubly censored data. <em>J Am Stat Assoc</em>,
<b>69</b>, 169-173. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit.coxph">survfit.coxph</a></code> for survival curves from Cox models,
<code><a href="#topic+survfit.object">survfit.object</a></code> for a description of the components of a
survfit object,
<code><a href="#topic+print.survfit">print.survfit</a></code>,  
<code><a href="#topic+plot.survfit">plot.survfit</a></code>,  
<code><a href="#topic+lines.survfit">lines.survfit</a></code>,
<code><a href="#topic+residuals.survfit">residuals.survfit</a></code>,
<code><a href="#topic+coxph">coxph</a></code>,  
<code><a href="#topic+Surv">Surv</a></code>.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#fit a Kaplan-Meier and plot it 
fit &lt;- survfit(Surv(time, status) ~ x, data = aml) 
plot(fit, lty = 2:3) 
legend(100, .8, c("Maintained", "Nonmaintained"), lty = 2:3) 

#fit a Cox proportional hazards model and plot the  
#predicted survival for a 60 year old 
fit &lt;- coxph(Surv(futime, fustat) ~ age, data = ovarian) 
plot(survfit(fit, newdata=data.frame(age=60)),
     xscale=365.25, xlab = "Years", ylab="Survival") 

# Here is the data set from Turnbull
#  There are no interval censored subjects, only left-censored (status=3),
#  right-censored (status 0) and observed events (status 1)
#
#                             Time
#                         1    2   3   4
# Type of observation
#           death        12    6   2   3
#          losses         3    2   0   3
#      late entry         2    4   2   5
#
tdata &lt;- data.frame(time  =c(1,1,1,2,2,2,3,3,3,4,4,4),
                    status=rep(c(1,0,2),4),
                    n     =c(12,3,2,6,2,4,2,0,2,3,3,5))
fit  &lt;- survfit(Surv(time, time, status, type='interval') ~1, 
              data=tdata, weight=n)

#
# Three curves for patients with monoclonal gammopathy.
#  1. KM of time to PCM, ignoring death (statistically incorrect)
#  2. Competing risk curves (also known as "cumulative incidence")
#  3. Multi-state, showing Pr(in each state, at time t)
#
fitKM &lt;- survfit(Surv(stop, event=='pcm') ~1, data=mgus1,
                    subset=(start==0))
fitCR &lt;- survfit(Surv(stop, event) ~1,
                    data=mgus1, subset=(start==0))
fitMS &lt;- survfit(Surv(start, stop, event) ~ 1, id=id, data=mgus1)
## Not run: 
# CR curves show the competing risks
plot(fitCR, xscale=365.25, xmax=7300, mark.time=FALSE,
            col=2:3, xlab="Years post diagnosis of MGUS",
            ylab="P(state)")
lines(fitKM, fun='event', xmax=7300, mark.time=FALSE,
            conf.int=FALSE)
text(3652, .4, "Competing risk: death", col=3)
text(5840, .15,"Competing risk: progression", col=2)
text(5480, .30,"KM:prog")

## End(Not run)
</code></pre>

<hr>
<h2 id='survfit.matrix'>Create Aalen-Johansen estimates of multi-state survival from
a matrix of hazards.</h2><span id='topic+survfit.matrix'></span>

<h3>Description</h3>

<p>This allows one to create the Aalen-Johansen estimate of P, a matrix
with one column per state and one row per time, starting with the
individual hazard estimates.  Each row of P will sum to 1.
Note that this routine has been superseded by the use of multi-state
Cox models, and will eventually be removed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'matrix'
survfit(formula, p0, method = c("discrete", "matexp"),
        start.time, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survfit.matrix_+3A_formula">formula</code></td>
<td>
<p>a matrix of lists, each element of which is either NULL or
a survival curve object. 
</p>
</td></tr>
<tr><td><code id="survfit.matrix_+3A_p0">p0</code></td>
<td>
<p>the initial state vector.  The names of this vector are used
as the names of the states in the output object.  If there are
multiple curves then <code>p0</code> can be a matrix with one row per curve.
</p>
</td></tr>
<tr><td><code id="survfit.matrix_+3A_method">method</code></td>
<td>

<p>use a product of discrete hazards, or a product of matrix exponentials.
See details below.
</p>
</td></tr>
<tr><td><code id="survfit.matrix_+3A_start.time">start.time</code></td>
<td>
<p>optional; start the calculations at a given starting
point</p>
</td></tr>
<tr><td><code id="survfit.matrix_+3A_...">...</code></td>
<td>
<p>further arguments used by other survfit methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>On input the matrix should contain a set of predicted curves for each
possible transition, and NULL in other positions.  Each of the
predictions will have been obtained from the relevant Cox model. 
This approach for multistate curves is easy to use but has some
caveats.  
First, the input curves must be consistent.
The routine checks as best it can, but can easy be fooled.
For instance, if one were to fit two Cox models, obtain
predictions for males and females from one, and for treatment A and B
from the other, this routine will create two curves but they are
not meaningful.
A second issue is that standard errors are not produced.
</p>
<p>The names of the resulting states are taken from the names of the
vector of initial state probabilities.  If they are missing, then the
dimnames of the input matrix are used, and lacking that the labels
'1', '2', etc. are used.
</p>
<p>For the usual Aalen-Johansen estimator the multiplier at each event
time is the matrix of hazards H (also written as I + dA).
When using predicted survival curves from a Cox model, however,
it is possible to get predicted hazards that are greater than 1, which
leads to probabilities less than 0.
If the <code>method</code> argument is not supplied and the input
curves are derived from a Cox model this routine instead uses the
approximation expm(H-I) as the multiplier, which always gives valid
probabilities.
(This is also the standard approach for ordinary
survival curves from a Cox model.) 
</p>


<h3>Value</h3>

<p>a survfitms object</p>


<h3>Note</h3>

<p>The R syntax for creating a matrix of lists is very fussy.</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit">survfit</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>etime &lt;- with(mgus2, ifelse(pstat==0, futime, ptime))
event &lt;- with(mgus2, ifelse(pstat==0, 2*death, 1))
event &lt;- factor(event, 0:2, labels=c("censor", "pcm", "death"))

cfit1 &lt;- coxph(Surv(etime, event=="pcm") ~ age + sex, mgus2)
cfit2 &lt;- coxph(Surv(etime, event=="death") ~ age + sex, mgus2)

# predicted competing risk curves for a 72 year old with mspike of 1.2
# (median values), male and female.
# The survfit call is a bit faster without standard errors.
newdata &lt;- expand.grid(sex=c("F", "M"), age=72, mspike=1.2)

AJmat &lt;- matrix(list(), 3,3)
AJmat[1,2] &lt;- list(survfit(cfit1, newdata, std.err=FALSE))
AJmat[1,3] &lt;- list(survfit(cfit2, newdata, std.err=FALSE))
csurv  &lt;- survfit(AJmat, p0 =c(entry=1, PCM=0, death=0))
</code></pre>

<hr>
<h2 id='survfit.object'>
Survival Curve Object 
</h2><span id='topic+survfit.object'></span><span id='topic+survfitms.object'></span>

<h3>Description</h3>

<p>This class of objects is returned by the <code>survfit</code> class of functions
to represent a fitted survival curve.
For a multi-state model the object has class <code>c('survfitms', 'survfit')</code>.
</p>
<p>Objects of this class have methods for the functions <code>print</code>,
<code>summary</code>, <code>plot</code>, <code>points</code> and <code>lines</code>. The
<code><a href="#topic+print.survfit">print.survfit</a></code> method does more computation than is typical
for a print method and is documented on a separate page.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="survfit.object_+3A_n">n</code></td>
<td>

<p>total number of observations in each curve.
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_time">time</code></td>
<td>

<p>the time points at which the curve has a step. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_n.risk">n.risk</code></td>
<td>

<p>the number of subjects at risk at t. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_n.event">n.event</code></td>
<td>

<p>the number of events that occur at time t. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_n.enter">n.enter</code></td>
<td>

<p>for counting process data only, and only if there was an <code>id</code>
argument, the number of subjects that enter the risk set during the
current interval.  If there are event/censoring times at 1, 3, 5 for
instance, someone who enters at time 1 is counted in the (1, 3]
interval, i.e., appears in the row for time 3. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_n.censor">n.censor</code></td>
<td>

<p>for counting process data only,
the number of subjects who exit the risk set,
without an event,  at time t. 
(For right censored data, this number can be computed from the successive
values of the number at risk).
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_surv">surv</code></td>
<td>

<p>the estimate of survival at time t+0. 
This may be a vector or a matrix. The latter occurs when a set of
survival curves is created from a single Cox model, in which case
there is one column for each covariate set. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_pstate">pstate</code></td>
<td>

<p>a multi-state survival will have the <code>pstate</code> component
instead of <code>surv</code>.
It will be a matrix containing the estimated probability
of each state at each time, one column per state.
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_std.err">std.err</code></td>
<td>

<p>for a survival curve this contains standard error of the cumulative
hazard or -log(survival), for a multi-state curve it contains the
standard error of prev.  This difference is a reflection of
the fact that each is the natural calculation for that case.
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_cumhaz">cumhaz</code></td>
<td>
<p>optional.  Contains the cumulative
hazard for each possible transition.
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_counts">counts</code></td>
<td>
<p>optional. If weights were used, the <code>n.risk</code> etc
elements contain weighted sums; the <code>counts</code> matrix will
contain unweighted values.  Weighted values are normally more useful
for further computation, unweighted may be preferred for labeling or
printout.
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_strata">strata</code></td>
<td>

<p>if there are multiple curves, this component gives the number of
elements of the <code>time</code>  vector corresponding to the first curve,
the second curve, and so on.
The names of the elements are labels for the curves. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_upper">upper</code></td>
<td>
<p>optional
upper confidence limit for the survival curve or pstate
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_lower">lower</code></td>
<td>
<p>options 
lower confidence limit for the survival curve or pstate
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_t0">t0</code></td>
<td>
<p>optional, the starting time for the curve</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_p0">p0</code>, <code id="survfit.object_+3A_sp0">sp0</code></td>
<td>
<p>for a multistate object, the distribution of starting
states.  If the curve has a strata dimension, this will be a matrix
one row per stratum.  The <code>sp0</code> element has the standard error
of p0, if p0 was estimated.
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_newdata">newdata</code></td>
<td>
<p>for survival curves from a fitted model, this contains
the covariate values for the curves
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_n.id">n.id</code></td>
<td>
<p>the total number of unique id values that contributed to
the curve.  This is only available if the original call used the id
option.
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_conf.type">conf.type</code></td>
<td>

<p>the approximation used to compute the confidence limits. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_conf.int">conf.int</code></td>
<td>

<p>the level of the confidence limits, e.g. 90 or 95%. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_transitions">transitions</code></td>
<td>
<p>for multi-state data, the total number
of transitions of each type.</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_na.action">na.action</code></td>
<td>

<p>the returned value from the na.action function, if any.  It will be used 
in the printout of the curve, e.g., the number of observations deleted due 
to missing values. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_call">call</code></td>
<td>

<p>an image of the call that produced the object. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_type">type</code></td>
<td>

<p>type of survival censoring. 
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_influence.p">influence.p</code>, <code id="survfit.object_+3A_influence.c">influence.c</code></td>
<td>
<p>optional influence
matrices for the <code>pstate</code> (or <code>surv</code>) and for the
<code>cumhaz</code> estimates.
A list with one element per stratum, each
element of the list is an array indexed by subject, time, state.
</p>
</td></tr>
<tr><td><code id="survfit.object_+3A_version">version</code></td>
<td>
<p>the version of the object.  Will be missing, 2, or 3</p>
</td></tr>
</table>


<h3>Structure</h3>

<p>The following components must be included in a legitimate 
<code>survfit</code> or <code>survfitms</code> object. 
</p>


<h3>Subscripts</h3>

<p>Survfit objects can be subscripted. 
This is often used to plot a subset of the curves, for instance.
From the user's point of view the <code>survfit</code> object appears to be
a vector, matrix, or array of curves.
The first dimension is always the underlying number of curves or
&ldquo;strata&rdquo;;
for multi-state models the state is always the last dimension.
Predicted curves from a Cox model can have a second dimension
which is the number of different covariate prediction vectors.
</p>


<h3>Details</h3>

<p>The <code>survfit</code> object has evolved over time: when first created
there was no thought of multi-state models for instance.  This evolution
has almost entirely been accomplished by the addition of new elements.
</p>
<p>For both plots of the curves and computation of the restricted mean
time in state (RMTS) we need the concept of a starting point t0 and
starting prevalence of the states p0 for each curve.
(Sojourn time, area under the curve and restricted mean survival time
are other labels for the RMTS).
Time 0 is not, by default, included as part of the standard
tableau of results, i.e., time, number at risk, number of events, etc.
For simple survival with a 0/1 status variable, the starting state p0 is
the obvious default of &quot;everyone alive&quot;, and t0 is formally not discernable
from the data and so was left out. 
(A design decision made in 1986, and now far too late to change.)
However, for plots t0 is assumed to be the minimum of 0 and all observed
times. Negative survival times are unusual but not invalid.
Multi-state survival curves include <code>t0</code> and <code>p0</code> as a part of
the returned object. The first is a single value for all curves, the
second is per curve.  
</p>
<p>The <code>survfit0</code> routine can be used to add these values to the main
curve data, this is done by the default print, plot, and summary methods
for survfit objects.  The methods vignette has discussion of the
rationale of how t0 and p0 are chosen in the multi-state case.
Notice that if there is an event at time t0, e.g., a death on day 0 for
competing risks, then p0 will contain the prevalence just before that
event occured.  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.survfit">plot.survfit</a></code>,  
<code><a href="#topic+summary.survfit">summary.survfit</a></code>,
<code><a href="#topic+print.survfit">print.survfit</a></code>,
<code><a href="#topic+survfit">survfit</a></code>,
<code><a href="#topic+survfit0">survfit0</a></code>
</p>

<hr>
<h2 id='survfit0'>
Convert the format of a survfit object.
</h2><span id='topic+survfit0'></span>

<h3>Description</h3>

<p>Add the point for a starting time (&quot;time 0&quot;) to a survfit object's
elements.  This is useful for plotting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survfit0(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survfit0_+3A_x">x</code></td>
<td>
<p>a survfit object</p>
</td></tr>
<tr><td><code id="survfit0_+3A_...">...</code></td>
<td>
<p>any other arguments are ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Survival curves are traditionally plotted forward from time 0, but
since the true starting time is not known as a part of the data,
the <code>survfit</code> routine does not include a time 0 value in
the resulting object.
Someone might look at cumulative mortgage defaults versus calendar
year, for instance, with the &lsquo;time&rsquo; value a Date object.
The plotted curve probably should not start at 0 = 1970-01-01.
Due to this uncertainty, it was decided not to include a &quot;time 0&quot; as
part of a survfit object. 
Whether that (1989) decision was wise or foolish,
it is now far too late to change it. (We tried it once as a
trial, resulting in over 20 errors in the survival test suite.  We
extrapolated that it might break 1/3 of the other CRAN packages
that depend on survival, if made a default.)
Many curves do include a value <code>t0</code> for &quot;time 0&quot;,
which is where the survfit
routine has surmised that the curve would start.
</p>
<p>One problem with this choice is that some functions must choose a
starting point, plots and computation of the restricted mean survival
time are two primary examples.
This utility function is used by <code>plot.survfit</code> and
<code>summary.survfit</code> to fill in that gap.
</p>
<p>The value used for this first time point is the first one below
</p>

<ol>
<li><p> a <code>t0</code> value found in the  in the object. 
</p>
</li>
<li><p> for single state survival </p>

<ul>
<li><p> min(0, time) for Surv(time, status) data
</p>
</li>
<li><p> min(time1) for Surv(time1, time2, status) data
</p>
</li></ul>

</li>
<li><p> for multi state survival </p>

<ul>
<li><p> min(0, time) for Surv(time, event) data, e.g., competing
risks
</p>
</li>
<li><p> min(time1) for Surv(time1, time2, event) data, if everyone
starts in the same state
</p>
</li></ul>

</li></ol>

<p>(Remember that negative times are allowed in Surv objects.)
</p>
<p>This function will add a new time point at the front of each curve,
but only if said time point is less than existing points in the
curve.  If there were a death on day 0, for instance, it will not add a
(time=0, survival=1) point.
(The question of whether the plotted curve in this case should or
should not start with a vertical segment can be debated ad nauseum.
It has no effect on the area under the curve (RMST), and the summary
for time 0 should report the smaller value.) 
</p>
<p>The resulting object is <em>not</em> currently
guarranteed to work with functions that further manipulate a
<code>survfit</code> object such as subscripting, aggregation, pseudovalues,
etc. (remember the 20 errors).  Rather it is intended as a penultimate
step, most often when creating a plot or summary of the curve(s).
</p>


<h3>Value</h3>

<p>a reformulated version of the object with an initial data point added.
The <code>time</code>, <code>surv</code>, <code>pstate</code>, <code>cumhaz</code>,
<code>std.err</code>, <code>std.cumhaz</code> and other components will all be aligned,
so as to make plots and summaries easier to produce.
</p>

<hr>
<h2 id='survfitcoxph.fit'>
A direct interface to the &lsquo;computational engine&rsquo; of survfit.coxph
</h2><span id='topic+survfitcoxph.fit'></span>

<h3>Description</h3>

<p>This program is mainly supplied to allow other packages to invoke the
survfit.coxph function at a &lsquo;data&rsquo; level rather than a &lsquo;user&rsquo; level.
It does no checks on the input data that is provided, which can lead
to unexpected errors if that data is wrong.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survfitcoxph.fit(y, x, wt, x2, risk, newrisk, strata, se.fit, survtype,
vartype, varmat, id, y2, strata2, unlist=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survfitcoxph.fit_+3A_y">y</code></td>
<td>
<p>the response variable used in the Cox model.  (Missing values
removed of course.)
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_x">x</code></td>
<td>
<p>covariate matrix used in the Cox model
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_wt">wt</code></td>
<td>
<p>weight vector for the Cox model. If the model was unweighted
use a vector of 1s.
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_x2">x2</code></td>
<td>
<p>matrix describing the hypothetical subjects for which a
curve is desired.  Must have the same number of columns as <code>x</code>.
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_risk">risk</code></td>
<td>
<p>the risk score exp(X beta) from the fitted Cox model.  If
the model had an offset, include it in the argument to exp.
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_newrisk">newrisk</code></td>
<td>
<p>risk scores for the hypothetical subjects
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_strata">strata</code></td>
<td>
<p>strata variable used in the Cox model. This will be a factor.
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_se.fit">se.fit</code></td>
<td>
<p>if <code>TRUE</code> the standard errors of the curve(s) are returned
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_survtype">survtype</code></td>
<td>
<p>1=Kalbfleisch-Prentice, 2=Nelson-Aalen, 3=Efron.  It is
usual to match this to the approximation for ties used in the
<code>coxph</code> model: KP for &lsquo;exact&rsquo;, N-A for &lsquo;breslow&rsquo; and Efron for &lsquo;efron&rsquo;.
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_vartype">vartype</code></td>
<td>
<p>1=Greenwood, 2=Aalen, 3=Efron
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_varmat">varmat</code></td>
<td>
<p>the variance matrix of the coefficients
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_id">id</code></td>
<td>
<p>optional; if present and not NULL this should be
a vector of identifiers of length <code>nrow(x2)</code>.
A mon-null value signifies that <code>x2</code> contains time dependent
covariates, in which case this identifies which rows of <code>x2</code> go
with each subject.
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_y2">y2</code></td>
<td>
<p>survival times, for time dependent prediction.  It gives
the time range (time1,time2] for each row of <code>x2</code>.  Note: this
must be a Surv object and thus contains a status indicator, which is
never used in the routine, however.  
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_strata2">strata2</code></td>
<td>
<p>vector of strata indicators for <code>x2</code>.  This must
be a factor.
</p>
</td></tr>
<tr><td><code id="survfitcoxph.fit_+3A_unlist">unlist</code></td>
<td>
<p>if <code>FALSE</code> the result will be a list with one
element for each strata.  Otherwise the strata are &ldquo;unpacked&rdquo; into
the form found in a <code>survfit</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing nearly all the components of a <code>survfit</code>
object.  All that is missing is to add the confidence intervals, the
type of the original model's response (as in a coxph object), and the
class.
</p>


<h3>Note</h3>

<p>The source code for for both this function and
<code>survfit.coxph</code> is written using noweb.  For complete
documentation see the <code>inst/sourcecode.pdf</code> file.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+survfit.coxph">survfit.coxph</a></code>
</p>

<hr>
<h2 id='survival-deprecated'>Deprecated functions in package <span class="pkg">survival</span></h2><span id='topic+survival-deprecated'></span><span id='topic+survConcordance'></span><span id='topic+survConcordance.fit'></span>

<h3>Description</h3>

<p>These functions are temporarily retained for compatability with older programs,
and may transition to defunct status.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survConcordance(formula, data, weights, subset, na.action) # use concordance
survConcordance.fit(y, x, strata, weight)    # use concordancefit
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survival-deprecated_+3A_formula">formula</code></td>
<td>

<p>a formula object, with the response on the left of a <code>~</code> operator, and 
the terms on the right.  The response must be a survival object as 
returned by the <code>Surv</code> function. 
</p>
</td></tr>
<tr><td><code id="survival-deprecated_+3A_data">data</code></td>
<td>
<p>a data frame
</p>
</td></tr>
<tr><td><code id="survival-deprecated_+3A_weights">weights</code>, <code id="survival-deprecated_+3A_subset">subset</code>, <code id="survival-deprecated_+3A_na.action">na.action</code></td>
<td>
<p>as for <code>coxph</code></p>
</td></tr>
<tr><td><code id="survival-deprecated_+3A_x">x</code>, <code id="survival-deprecated_+3A_y">y</code>, <code id="survival-deprecated_+3A_strata">strata</code>, <code id="survival-deprecated_+3A_weight">weight</code></td>
<td>
<p>predictor, response, strata, and weight
vectors for the direct call</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="base.html#topic+Deprecated">Deprecated</a></code>
</p>

<hr>
<h2 id='survival-internal'>Internal survival functions</h2><span id='topic+survival-internal'></span><span id='topic+agexact.fit'></span><span id='topic+as.matrix.ratetable'></span><span id='topic+coxpenal.df'></span><span id='topic+coxpenal.fit'></span><span id='topic+is.na.coxph.penalty'></span><span id='topic+match.ratetable'></span><span id='topic+survfitCI'></span><span id='topic+survfitKM'></span><span id='topic+survfitTurnbull'></span><span id='topic+survreg.fit'></span><span id='topic+survpenal.fit'></span><span id='topic+survdiff.fit'></span><span id='topic++5B.coxph.penalty'></span><span id='topic+coef.survfit'></span><span id='topic+vcov.survfit'></span><span id='topic+confint.survfit'></span>

<h3>Description</h3>

<p>Internal survival functions</p>


<h3>Usage</h3>

<pre><code class='language-R'>survreg.fit(x, y, weights, offset, init, controlvals, dist, scale = 0,
    nstrat = 1, strata, parms = NULL,assign)
survpenal.fit(x, y, weights, offset, init, controlvals, dist, scale = 0,
    nstrat = 1, strata, pcols, pattr, assign, parms = NULL)
survdiff.fit(y, x, strat, rho = 0)
match.ratetable(R, ratetable)
## S3 method for class 'ratetable'
as.matrix(x, ...)
## S3 method for class 'coxph.penalty'
is.na(x)
coxpenal.df(hmat, hinv, fdiag, assign.list, ptype, nvar, pen1,
    pen2, sparse)
coxpenal.fit(x, y, strata, offset, init, control, weights, method,
    rownames, pcols, pattr, assign, nocenter)
agexact.fit(x, y, strata, offset, init, control, weights, method,
    rownames, resid=TRUE, nocenter=NULL) 
survfitKM(x, y, weights=rep(1,length(x)),
                       stype=1, ctype=1, 
                       se.fit=TRUE,
                       conf.int= .95,
                       conf.type=c('log',  'log-log',  'plain', 'none',
                                   'logit', 'arcsin'),
                       conf.lower=c('usual', 'peto', 'modified'),
                       start.time, id, cluster, robust,
                       influence=FALSE, type, entry, time0) 
survfitTurnbull(x, y, weights,
                       type=c('kaplan-meier', 'fleming-harrington', 'fh2'),
                       error=c('greenwood', "tsiatis"), se.fit=TRUE,
                       conf.int= .95,
                       conf.type=c('log',  'log-log',  'plain', 'none',
                                   'logit', 'arcsin'),
                       conf.lower=c('usual', 'peto', 'modified'),
                       start.time, robust, cluster, time0)
</code></pre>


<h3>Details</h3>

<p>The arguments to these routines are not guaranteed to stay the
same from release to release &ndash; call them at your own risk!</p>

<hr>
<h2 id='survobrien'>
O'Brien's Test for Association of a Single Variable with Survival
</h2><span id='topic+survobrien'></span>

<h3>Description</h3>

<p>Peter O'Brien's test for association of a single variable with survival 
This test is proposed in Biometrics, June 1978. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survobrien(formula, data, subset, na.action, transform)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survobrien_+3A_formula">formula</code></td>
<td>

<p>a valid formula for a cox model.
</p>
</td></tr>
<tr><td><code id="survobrien_+3A_data">data</code></td>
<td>

<p>a data.frame in which to interpret the variables named in 
the <code>formula</code>, or in the <code>subset</code> and the <code>weights</code>
argument. 
</p>
</td></tr>
<tr><td><code id="survobrien_+3A_subset">subset</code></td>
<td>

<p>expression indicating which subset of the rows of data should be used in 
the fit.    All observations are included by default. 
</p>
</td></tr>
<tr><td><code id="survobrien_+3A_na.action">na.action</code></td>
<td>

<p>a missing-data filter function.  This is applied to the model.frame
after any 
subset argument has been used.  Default is <code>options()\$na.action</code>. 
</p>
</td></tr>
<tr><td><code id="survobrien_+3A_transform">transform</code></td>
<td>
<p>the transformation function to be applied at each
time point. The default is O'Brien's suggestion logit(tr) where
tr = (rank(x)- 1/2)/ length(x) is the rank shifted to the range
0-1 and logit(x) = log(x/(1-x)) is the logit transform.
</p>
</td></tr></table>


<h3>Value</h3>

<p>a new data frame.  The response variables will be column names
returned by the <code>Surv</code> function, i.e., &quot;time&quot; and &quot;status&quot; for
simple survival data, or &quot;start&quot;, &quot;stop&quot;, &quot;status&quot; for counting
process data.  Each individual event time is identified by the
value of the variable <code>.strata.</code>.  Other variables retain
their original names.  If a
predictor variable is a factor or is protected with <code>I()</code>, it is 
retained as is.  Other predictor variables have been replaced with 
time-dependent logit scores. 
</p>
<p>The new data frame will have many 
more rows that the original data, approximately the original number
of rows * number of deaths/2. 
</p>


<h3>Method</h3>

<p>A time-dependent cox model can now be fit to the new data. 
The univariate statistic, as originally proposed, is equivalent to 
single variable score tests from the time-dependent model. 
This equivalence is the rationale for using the time dependent model as a 
multivariate extension of the original paper. 
</p>
<p>In O'Brien's method, the x variables are re-ranked at each death time.  A 
simpler method, proposed by Prentice, ranks the data only once at the 
start. The results are usually similar. 
</p>


<h3>Note</h3>

<p>A prior version of the routine returned new time variables rather than
a strata.  Unfortunately, that strategy does not work if the original
formula has a strata statement.  This new data set will be the same
size, but the <code>coxph</code> routine will process it slightly faster.
</p>


<h3>References</h3>

<p>O'Brien, Peter, &quot;A Nonparametric Test for Association with Censored Data&quot;, 
<em>Biometrics</em> 34: 243-250, 1978. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survdiff">survdiff</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>xx &lt;- survobrien(Surv(futime, fustat) ~ age + factor(rx) + I(ecog.ps), 
			       data=ovarian) 
coxph(Surv(time, status) ~ age + strata(.strata.), data=xx) 
</code></pre>

<hr>
<h2 id='survreg'>
Regression for a Parametric Survival Model 
</h2><span id='topic+survreg'></span><span id='topic+model.frame.survreg'></span><span id='topic+labels.survreg'></span><span id='topic+print.survreg.penal'></span><span id='topic+print.summary.survreg'></span><span id='topic+survReg'></span><span id='topic+anova.survreg'></span><span id='topic+anova.survreglist'></span>

<h3>Description</h3>

<p>Fit a parametric survival regression model.
These are location-scale models for an arbitrary transform of the time
variable; the most common cases use a log transformation, leading to
accelerated failure time models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survreg(formula, data, weights, subset, 
        na.action, dist="weibull", init=NULL, scale=0, 
        control,parms=NULL,model=FALSE, x=FALSE,
        y=TRUE, robust=FALSE, cluster, score=FALSE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survreg_+3A_formula">formula</code></td>
<td>

<p>a formula expression as for other regression models. 
The response is usually a survival object as returned by the <code>Surv</code> function. 
See the documentation for <code>Surv</code>, <code>lm</code> and <code>formula</code> for details. 
</p>
</td></tr>
<tr><td><code id="survreg_+3A_data">data</code></td>
<td>

<p>a data frame in which to interpret the variables named in 
the <code>formula</code>, <code>weights</code> or the <code>subset</code> arguments. 
</p>
</td></tr>
<tr><td><code id="survreg_+3A_weights">weights</code></td>
<td>
<p>optional vector of case weights</p>
</td></tr>
<tr><td><code id="survreg_+3A_subset">subset</code></td>
<td>

<p>subset of the observations to be used in the fit
</p>
</td></tr>
<tr><td><code id="survreg_+3A_na.action">na.action</code></td>
<td>

<p>a missing-data filter function, applied to the model.frame, after any 
<code>subset</code> argument has been used.  Default is <code>options()\$na.action</code>. 
</p>
</td></tr>
<tr><td><code id="survreg_+3A_dist">dist</code></td>
<td>

<p>assumed distribution for y variable. 
If the argument is a character string, then it is assumed to name an
element from <code><a href="#topic+survreg.distributions">survreg.distributions</a></code>. These include
<code>"weibull"</code>, <code>"exponential"</code>, <code>"gaussian"</code>,
<code>"logistic"</code>,<code>"lognormal"</code> and <code>"loglogistic"</code>.
Otherwise, it is assumed to be a user defined list conforming to the
format described in <code><a href="#topic+survreg.distributions">survreg.distributions</a></code>.
</p>
</td></tr>
<tr><td><code id="survreg_+3A_parms">parms</code></td>
<td>

<p>a list of fixed parameters.  For the t-distribution for instance this is
the degrees of freedom; most of the distributions have no parameters.
</p>
</td></tr>
<tr><td><code id="survreg_+3A_init">init</code></td>
<td>

<p>optional vector of initial values for the parameters.
</p>
</td></tr>
<tr><td><code id="survreg_+3A_scale">scale</code></td>
<td>

<p>optional fixed value for the scale.  If set to &lt;=0 then the scale is
estimated.
</p>
</td></tr>
<tr><td><code id="survreg_+3A_control">control</code></td>
<td>

<p>a list of control values, in the format produced by
<code><a href="#topic+survreg.control">survreg.control</a></code>. The default value is <code>survreg.control()</code>
</p>
</td></tr>
<tr><td><code id="survreg_+3A_model">model</code>, <code id="survreg_+3A_x">x</code>, <code id="survreg_+3A_y">y</code></td>
<td>

<p>flags to control what is returned.  If any of these is true, then the
model frame, the model matrix, and/or the vector of response times will be
returned as components of the final result, with the same names as the
flag arguments.</p>
</td></tr>
<tr><td><code id="survreg_+3A_score">score</code></td>
<td>
<p>return the score vector. (This is expected to be zero upon
successful convergence.)
</p>
</td></tr>
<tr><td><code id="survreg_+3A_robust">robust</code></td>
<td>
<p>Use robust sandwich error instead of the asymptotic
formula.  Defaults to TRUE if there is a <code>cluster</code> argument.</p>
</td></tr>
<tr><td><code id="survreg_+3A_cluster">cluster</code></td>
<td>
<p>Optional variable that identifies groups of subjects,
used in computing the robust variance.  Like <code>model</code> variables,
this is searched for in the dataset pointed to by the <code>data</code>
argument.
</p>
</td></tr>  
<tr><td><code id="survreg_+3A_...">...</code></td>
<td>

<p>other arguments which will be passed to <code>survreg.control</code>.
</p>
</td></tr></table>


<h3>Details</h3>

<p>All the distributions are cast into a location-scale framework, based
on chapter 2.2 of Kalbfleisch and Prentice.  The resulting
parameterization of the distributions is sometimes (e.g. gaussian)
identical to the usual form found in statistics textbooks, but other
times (e.g. Weibull) it is not.  See the book for detailed formulas.
</p>
<p>When using weights be aware of the difference between replication
weights and sampling weights.  In the former, a weight of '2' means
that there are two identical observations, which have been combined
into a single row of data.  With sampling weights there is a single
observed value, with a weight used to achieve balance with respect to
some population.  To get proper variance with replication weights use
the default variance, for sampling weights use the robust variance.
Replication weights were once common (when computer memory was much
smaller) but are now rare.  
</p>


<h3>Value</h3>

<p>an object of class <code>survreg</code> is returned.
</p>


<h3>References</h3>

<p>Kalbfleisch, J. D. and Prentice, R. L., The statistical analysis of
failure time data, Wiley, 2002.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survreg.object">survreg.object</a></code>, <code><a href="#topic+survreg.distributions">survreg.distributions</a></code>,
<code><a href="#topic+pspline">pspline</a></code>, <code><a href="#topic+frailty">frailty</a></code>, <code><a href="#topic+ridge">ridge</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Fit an exponential model: the two fits are the same
survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian, dist='weibull',
                                    scale=1)
survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian,
        dist="exponential")

#
# A model with different baseline survival shapes for two groups, i.e.,
#   two different scale parameters
survreg(Surv(time, status) ~ ph.ecog + age + strata(sex), lung)

# There are multiple ways to parameterize a Weibull distribution. The survreg 
# function embeds it in a general location-scale family, which is a 
# different parameterization than the rweibull function, and often leads
# to confusion.
#   survreg's scale  =    1/(rweibull shape)
#   survreg's intercept = log(rweibull scale)
#   For the log-likelihood all parameterizations lead to the same value.
y &lt;- rweibull(1000, shape=2, scale=5)
survreg(Surv(y)~1, dist="weibull")

# Economists fit a model called `tobit regression', which is a standard
# linear regression with Gaussian errors, and left censored data.
tobinfit &lt;- survreg(Surv(durable, durable&gt;0, type='left') ~ age + quant,
	            data=tobin, dist='gaussian')
</code></pre>

<hr>
<h2 id='survreg.control'>Package options for survreg and coxph</h2><span id='topic+survreg.control'></span>

<h3>Description</h3>

<p>This functions checks and packages the fitting options for 
<code><a href="#topic+survreg">survreg</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survreg.control(maxiter=30, rel.tolerance=1e-09, 
toler.chol=1e-10, iter.max, debug=0, outer.max=10)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survreg.control_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations </p>
</td></tr>
<tr><td><code id="survreg.control_+3A_rel.tolerance">rel.tolerance</code></td>
<td>
<p>relative tolerance to declare convergence </p>
</td></tr>
<tr><td><code id="survreg.control_+3A_toler.chol">toler.chol</code></td>
<td>
<p>Tolerance to declare Cholesky decomposition singular</p>
</td></tr>
<tr><td><code id="survreg.control_+3A_iter.max">iter.max</code></td>
<td>
<p>same as <code>maxiter</code></p>
</td></tr>
<tr><td><code id="survreg.control_+3A_debug">debug</code></td>
<td>
<p>print debugging information</p>
</td></tr>
<tr><td><code id="survreg.control_+3A_outer.max">outer.max</code></td>
<td>
<p>maximum number of outer iterations for choosing
penalty parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the same elements as the input
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+survreg">survreg</a></code></p>

<hr>
<h2 id='survreg.distributions'>Parametric Survival Distributions</h2><span id='topic+survreg.distributions'></span>

<h3>Description</h3>

<p> List of distributions for accelerated failure models. These are
location-scale families for some transformation of time. The entry
describes  the cdf <code class="reqn">F</code> and density <code class="reqn">f</code> of a canonical member of
the family.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survreg.distributions
</code></pre>


<h3>Format</h3>

<p>There are two basic formats, the first defines a distribution de novo,
the second defines a new distribution in terms of an old one.
</p>

<table>
<tr>
 <td style="text-align: left;">
    name:</td><td style="text-align: left;"> name of distribution</td>
</tr>
<tr>
 <td style="text-align: left;">
    variance:</td><td style="text-align: left;"> function(parms) returning the variance (currently unused)</td>
</tr>
<tr>
 <td style="text-align: left;">
    init(x,weights,...):</td><td style="text-align: left;"> Function returning an initial</td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> estimate of the mean and variance </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> (used for initial values in the iteration)</td>
</tr>
<tr>
 <td style="text-align: left;">
    density(x,parms):</td><td style="text-align: left;"> Function returning a matrix with columns <code class="reqn">F</code>,
    <code class="reqn">1-F</code>, <code class="reqn">f</code>, <code class="reqn">f'/f</code>, and <code class="reqn">f''/f</code></td>
</tr>
<tr>
 <td style="text-align: left;">
    quantile(p,parms):</td><td style="text-align: left;"> Quantile function</td>
</tr>
<tr>
 <td style="text-align: left;">
    scale:</td><td style="text-align: left;"> Optional fixed value for the scale parameter</td>
</tr>
<tr>
 <td style="text-align: left;">
    parms:</td><td style="text-align: left;"> Vector of default values and names for any additional parameters</td>
</tr>
<tr>
 <td style="text-align: left;">
    deviance(y,scale,parms):</td><td style="text-align: left;"> Function returning the deviance for a</td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> saturated model; used only for deviance residuals.
  </td>
</tr>

</table>

<p>and to define one distribution in terms of another
</p>

<table>
<tr>
 <td style="text-align: left;">
    name:</td><td style="text-align: left;"> name of distribution</td>
</tr>
<tr>
 <td style="text-align: left;">
    dist:</td><td style="text-align: left;"> name of parent distribution</td>
</tr>
<tr>
 <td style="text-align: left;">
    trans:</td><td style="text-align: left;"> transformation (eg log)</td>
</tr>
<tr>
 <td style="text-align: left;">
    dtrans:</td><td style="text-align: left;"> derivative of transformation</td>
</tr>
<tr>
 <td style="text-align: left;">
    itrans:</td><td style="text-align: left;"> inverse of transformation</td>
</tr>
<tr>
 <td style="text-align: left;">
    scale:</td><td style="text-align: left;"> Optional fixed value for scale parameter</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Details</h3>

<p>There are four basic distributions:<code>extreme</code>, <code>gaussian</code>,
<code>logistic</code> and <code>t</code>. The last three
are parametrised in the same way as the distributions already present in
<span class="rlang"><b>R</b></span>. The extreme value cdf is
</p>
<p style="text-align: center;"><code class="reqn">F=1-e^{-e^t}.</code>
</p>

<p>When the logarithm of survival time has one of the first three distributions
we obtain respectively <code>weibull</code>, <code>lognormal</code>, and
<code>loglogistic</code>. The location-scale parameterization of a Weibull
distribution found in <code>survreg</code> is not the same as the parameterization
of <code><a href="stats.html#topic+rweibull">rweibull</a></code>.
</p>
<p>The other predefined distributions are defined in terms of these. The
<code>exponential</code> and <code>rayleigh</code> distributions are Weibull
distributions with fixed <code>scale</code> of 1 and 0.5 respectively, and
<code>loggaussian</code> is a synonym for <code>lognormal</code>.
</p>
<p>For speed parts of the three most commonly used distributions
are hardcoded in C; for this reason the elements of <code>survreg.distributions</code>
with names of &quot;Extreme value&quot;, &quot;Logistic&quot; and &quot;Gaussian&quot; should not be
modified.  (The order of these in the list is not important, recognition
is by name.)
As an alternative to modifying <code>survreg.distributions</code>
a new distribution can be specified as a separate list.
This is the preferred method of addition and is illustrated below.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survreg">survreg</a></code>, <code><a href="stats.html#topic+pweibull">pweibull</a></code>,
<code><a href="stats.html#topic+pnorm">pnorm</a></code>,<code><a href="stats.html#topic+plogis">plogis</a></code>, <code><a href="stats.html#topic+pt">pt</a></code>,
<code><a href="#topic+survregDtest">survregDtest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># time transformation
survreg(Surv(time, status) ~ ph.ecog + sex, dist='weibull', data=lung)
# change the transformation to work in years
# intercept changes by log(365), everything else stays the same
my.weibull &lt;- survreg.distributions$weibull
my.weibull$trans &lt;- function(y) log(y/365)
my.weibull$itrans &lt;- function(y) 365*exp(y)
survreg(Surv(time, status) ~ ph.ecog + sex, lung, dist=my.weibull)

# Weibull parametrisation
y&lt;-rweibull(1000, shape=2, scale=5)
survreg(Surv(y)~1, dist="weibull")
# survreg scale parameter maps to 1/shape, linear predictor to log(scale)

# Cauchy fit
mycauchy &lt;- list(name='Cauchy',
                 init= function(x, weights, ...) 
                      c(median(x), mad(x)),
                 density= function(x, parms) {
                      temp &lt;- 1/(1 + x^2)
                      cbind(.5 + atan(x)/pi, .5+ atan(-x)/pi,
                            temp/pi, -2 *x*temp, 2*temp*(4*x^2*temp -1))
                      },
                 quantile= function(p, parms) tan((p-.5)*pi),
                 deviance= function(...) stop('deviance residuals not defined')
                 )
survreg(Surv(log(time), status) ~ ph.ecog + sex, lung, dist=mycauchy)
</code></pre>

<hr>
<h2 id='survreg.object'>
Parametric Survival Model Object
</h2><span id='topic+survreg.object'></span><span id='topic+print.survreg'></span><span id='topic+summary.survreg'></span>

<h3>Description</h3>

<p>This class of objects is returned by the <code>survreg</code> function
to represent a fitted parametric survival model.
Objects of this class have methods for the functions <code>print</code>,
<code>summary</code>, <code>predict</code>, and <code>residuals</code>.
</p>


<h3>COMPONENTS</h3>

<p>The following components must be included in a legitimate <code>survreg</code> object.
</p>

<dl>
<dt>coefficients</dt><dd>
<p>the coefficients of the <code>linear.predictors</code>, which multiply  the
columns of the model
matrix.
It does not include the estimate of error (sigma).
The names of the coefficients are the names of the
single-degree-of-freedom effects (the columns of the
model matrix).
If the model is over-determined there will
be missing values in the coefficients corresponding to non-estimable
coefficients.
</p>
</dd>
<dt>icoef</dt><dd>
<p>coefficients of the baseline model, which will contain the intercept
and log(scale), or multiple scale factors for a stratified model.
</p>
</dd>
<dt>var</dt><dd>
<p>the variance-covariance matrix for the parameters, including the log(scale)
parameter(s).
</p>
</dd>
<dt>loglik</dt><dd>
<p>a vector of length 2, containing the log-likelihood for the baseline and
full models. 
</p>
</dd>
<dt>iter</dt><dd>
<p>the number of iterations required
</p>
</dd>
<dt>linear.predictors</dt><dd>
<p>the linear predictor for each subject.
</p>
</dd>
<dt>df</dt><dd>
<p>the degrees of freedom for the final model.  For a penalized model
this will be a vector with one element per term.
</p>
</dd>
<dt>scale</dt><dd>
<p>the scale factor(s), with length equal to the number of strata.
</p>
</dd>
<dt>idf</dt><dd>
<p>degrees of freedom for the initial model.
</p>
</dd>
<dt>means</dt><dd>
<p>a vector of the column means of the coefficient matrix.
</p>
</dd>
<dt>dist</dt><dd>
<p>the distribution used in the fit.</p>
</dd>
<dt>weights</dt><dd><p>included for a weighted fit.</p>
</dd>
</dl>

<p>The object will also have the following components found in 
other model results (some are optional):
<code>linear predictors</code>, <code>weights</code>, <code>x</code>, <code>y</code>, <code>model</code>, 
<code>call</code>, <code>terms</code> and <code>formula</code>.
See <code>lm</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+survreg">survreg</a></code>, <code><a href="stats.html#topic+lm">lm</a></code>
</p>

<hr>
<h2 id='survregDtest'>Verify a survreg distribution</h2><span id='topic+survregDtest'></span>

<h3>Description</h3>

<p>This routine is called by <code>survreg</code> to verify that a distribution
object is valid.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survregDtest(dlist, verbose = F)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survregDtest_+3A_dlist">dlist</code></td>
<td>
<p>the list describing a survival distribution</p>
</td></tr>
<tr><td><code id="survregDtest_+3A_verbose">verbose</code></td>
<td>
<p>return a simple TRUE/FALSE from the test for validity
(the default), or a verbose description of any flaws.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the <code>survreg</code> function rejects your user-supplied distribution
as invalid, this routine will tell you why it did so.
</p>


<h3>Value</h3>

<p>TRUE if the distribution object passes the tests, and either FALSE or a
vector of character strings if not.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+survreg.distributions">survreg.distributions</a></code>, <code><a href="#topic+survreg">survreg</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># An invalid distribution (it should have "init =" on line 2)
#  surveg would give an error message
mycauchy &lt;- list(name='Cauchy',
                 init&lt;- function(x, weights, ...) 
                      c(median(x), mad(x)),
                 density= function(x, parms) {
                      temp &lt;- 1/(1 + x^2)
                      cbind(.5 + atan(temp)/pi, .5+ atan(-temp)/pi,
                            temp/pi, -2 *x*temp, 2*temp^2*(4*x^2*temp -1))
                      },
                 quantile= function(p, parms) tan((p-.5)*pi),
                 deviance= function(...) stop('deviance residuals not defined')
                 )

survregDtest(mycauchy, TRUE)
</code></pre>

<hr>
<h2 id='survSplit'>Split a survival data set at specified times </h2><span id='topic+survSplit'></span>

<h3>Description</h3>

<p>Given a survival data set and a set of specified cut times, split
each record into multiple subrecords at each cut time.  The new data
set will be in &lsquo;counting process&rsquo; format, with a start time, stop
time, and event status for each record. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survSplit(formula, data, subset, na.action=na.pass,
            cut, start="tstart", id, zero=0, episode,
                              end="tstop", event="event")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survSplit_+3A_formula">formula</code></td>
<td>
<p>a model formula</p>
</td></tr>
<tr><td><code id="survSplit_+3A_data">data</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="survSplit_+3A_subset">subset</code>, <code id="survSplit_+3A_na.action">na.action</code></td>
<td>
<p>rows of the data to be retained</p>
</td></tr>
<tr><td><code id="survSplit_+3A_cut">cut</code></td>
<td>
<p>the vector of timepoints to cut at</p>
</td></tr>
<tr><td><code id="survSplit_+3A_start">start</code></td>
<td>
<p>character string with the name of a start time variable (will
be created if needed) </p>
</td></tr>
<tr><td><code id="survSplit_+3A_id">id</code></td>
<td>
<p>character string with the name of new id variable to
create (optional).  This can be useful if the data set does not
already contain an identifier.</p>
</td></tr>
<tr><td><code id="survSplit_+3A_zero">zero</code></td>
<td>
<p>If <code>start</code> doesn't already exist, this is the time
that the original records start.</p>
</td></tr>
<tr><td><code id="survSplit_+3A_episode">episode</code></td>
<td>
<p>character string with the name of new episode
variable (optional)</p>
</td></tr>
<tr><td><code id="survSplit_+3A_end">end</code></td>
<td>
<p>character string with the name of event time variable </p>
</td></tr>
<tr><td><code id="survSplit_+3A_event">event</code></td>
<td>
<p>character string with the name of censoring indicator </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each interval in the original data is cut at the given points; if
an original row were (15, 60] with a cut vector of (10,30, 40) the
resulting data set would have intervals of (15,30], (30,40] and
(40, 60].
</p>
<p>Each row in the final data set will lie completely within one of the
cut intervals. Which interval for each row of the output is shown by the
<code>episode</code> variable, where 1= less than the first cutpoint, 2=
between the first and the second, etc.
For the example above the values would be 2, 3, and 4.
</p>
<p>The routine is called with a formula as the first
argument. 
The right hand side of the formula can be used to delimit variables
that should be retained; normally one will use <code> ~ .</code> as a
shorthand to retain them all.  The routine
will try to retain variable names, e.g. <code>Surv(adam, joe, fred)~.</code>
will result in a data set with those same variable names for
<code>tstart</code>, <code>end</code>, and <code>event</code> options rather than
the defaults.  Any user specified values for these options will be
used if they are present, of course.
However, the routine is not sophisticated; it only does this
substitution for simple names.  A call of <code>Surv(time, stat==2)</code>
for instance will not retain &quot;stat&quot; as the name of the event variable.
</p>
<p>Rows of data with a missing time or status are copied across
unchanged, unless the na.action argument is changed from its default
value of <code>na.pass</code>.  But in the latter case any row
that is missing for any variable will be removed, which is rarely
what is desired.
</p>


<h3>Value</h3>

<p>New, longer, data frame.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Surv">Surv</a></code>, <code><a href="base.html#topic+cut">cut</a></code>, <code><a href="stats.html#topic+reshape">reshape</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>fit1 &lt;- coxph(Surv(time, status) ~ karno + age + trt, veteran)
plot(cox.zph(fit1)[1])
# a cox.zph plot of the data suggests that the effect of Karnofsky score
#  begins to diminish by 60 days and has faded away by 120 days.
# Fit a model with separate coefficients for the three intervals.
#
vet2 &lt;- survSplit(Surv(time, status) ~., veteran,
                   cut=c(60, 120), episode ="timegroup")
fit2 &lt;- coxph(Surv(tstart, time, status) ~ karno* strata(timegroup) +
                age + trt, data= vet2)
c(overall= coef(fit1)[1],
  t0_60  = coef(fit2)[1],
  t60_120= sum(coef(fit2)[c(1,4)]),
  t120   = sum(coef(fit2)[c(1,5)]))

# Sometimes we want to split on one scale and analyse on another
#  Add a "current age" variable to the mgus2 data set.
temp1 &lt;- mgus2
temp1$endage &lt;- mgus2$age + mgus2$futime/12    # futime is in months
temp1$startage &lt;- temp1$age
temp2 &lt;- survSplit(Surv(age, endage, death) ~ ., temp1, cut=25:100,
                   start= "age1", end= "age2")

# restore the time since enrollment scale
temp2$time1 &lt;- (temp2$age1 - temp2$startage)*12
temp2$time2 &lt;- (temp2$age2 - temp2$startage)*12

# In this data set, initial age and current age have similar utility
mfit1 &lt;- coxph(Surv(futime, death) ~ age + sex, data=mgus2)
mfit2 &lt;- coxph(Surv(time1, time2, death) ~ age1 + sex, data=temp2)
</code></pre>

<hr>
<h2 id='tcut'>Factors for person-year calculations</h2><span id='topic+tcut'></span><span id='topic++5B.tcut'></span><span id='topic+levels.tcut'></span>

<h3>Description</h3>

<p>Attaches categories for person-year calculations to a variable without
losing the underlying continuous representation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tcut(x, breaks, labels, scale=1)
## S3 method for class 'tcut'
levels(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tcut_+3A_x">x</code></td>
<td>
<p>numeric/date variable </p>
</td></tr>
<tr><td><code id="tcut_+3A_breaks">breaks</code></td>
<td>
<p>breaks between categories, which are right-continuous </p>
</td></tr>
<tr><td><code id="tcut_+3A_labels">labels</code></td>
<td>
<p>labels for categories </p>
</td></tr>
<tr><td><code id="tcut_+3A_scale">scale</code></td>
<td>
<p>Multiply <code>x</code> and <code>breaks</code> by this.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>tcut</code>
</p>


<h3>See Also</h3>

 <p><code><a href="base.html#topic+cut">cut</a></code>, <code><a href="#topic+pyears">pyears</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># For pyears, all time variable need to be on the same scale; but
# futime is in months and age is in years
test &lt;- mgus2
test$years &lt;- test$futime/30.5   # follow-up in years

# first grouping based on years from starting age (= current age)
# second based on years since enrollment (all start at 0)
test$agegrp &lt;- tcut(test$age, c(0,60, 70, 80, 100), 
                     c("&lt;=60", "60-70", "70-80", "&gt;80"))
test$fgrp  &lt;- tcut(rep(0, nrow(test)), c(0, 1, 5, 10, 100),
                   c("0-1yr", "1-5yr", "5-10yr", "&gt;10yr"))

# death rates per 1000, by age group
pfit1 &lt;- pyears(Surv(years, death) ~ agegrp, scale =1000, data=test)
round(pfit1$event/ pfit1$pyears) 

#death rates per 100, by follow-up year and age
# there are excess deaths in the first year, within each age stratum
pfit2 &lt;- pyears(Surv(years, death) ~ fgrp + agegrp, scale =1000, data=test)
round(pfit2$event/ pfit2$pyears)  
</code></pre>

<hr>
<h2 id='tmerge'>Time based merge for survival data</h2><span id='topic+tmerge'></span>

<h3>Description</h3>

<p>A common task in survival analysis is the creation of start,stop data
sets which have multiple intervals for each subject, along with the
covariate values that apply over that interval.  This function aids
in the creation of such data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tmerge(data1, data2,  id,..., tstart, tstop, options)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tmerge_+3A_data1">data1</code></td>
<td>
<p>the primary data set, to which new variables and/or
observation will be added</p>
</td></tr>
<tr><td><code id="tmerge_+3A_data2">data2</code></td>
<td>
<p>second data set in which all the other arguments
will be found</p>
</td></tr>
<tr><td><code id="tmerge_+3A_id">id</code></td>
<td>
<p>subject identifier</p>
</td></tr>
<tr><td><code id="tmerge_+3A_...">...</code></td>
<td>
<p>operations that add new variables or intervals, see
below</p>
</td></tr>
<tr><td><code id="tmerge_+3A_tstart">tstart</code></td>
<td>
<p>optional variable to define the valid time range for
each subject, only used on an initial call</p>
</td></tr>
<tr><td><code id="tmerge_+3A_tstop">tstop</code></td>
<td>
<p>optional variable to define the valid time range for
each subject, only used on an initial call</p>
</td></tr>
<tr><td><code id="tmerge_+3A_options">options</code></td>
<td>
<p>a list of options.  Valid ones are idname, tstartname,
tstopname, delay, na.rm, and tdcstart.  See the explanation below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The program is often run in multiple passes, the first of which
defines the basic structure, and subsequent ones that add new
variables to that structure.  For a more complete explanation of how this
routine works refer to the vignette on time-dependent variables.
</p>
<p>There are 4 types of operational arguments: a time dependent covariate
(tdc), cumulative count (cumtdc), event (event) or cumulative event
(cumevent).
Time dependent covariates change their values before an event,
events are outcomes.
</p>

<ul>
<li><p>newname = tdc(y, x, init): A new time dependent covariate
variable will created. 
The argument <code>y</code> is assumed to be on the
scale of the start and end time, and each instance describes the
occurrence of a &quot;condition&quot; at that time.
The second argument <code>x</code> is optional. In the  case where
<code>x</code> is missing the count variable starts at 0 for each subject
and becomes 1 at the time of the event.
If <code>x</code> is present the value of the time dependent covariate
is initialized to value of <code>init</code>, if present, or
the <code>tdcstart</code> option otherwise, and is updated to the
value of <code>x</code> at each observation.
If the option <code>na.rm=TRUE</code> missing values of <code>x</code> are
first removed, i.e., the update will not create missing values. 
</p>
</li>
<li><p>newname = cumtdc(y,x, init): Similar to tdc, except that the event
count is accumulated over time for each subject.  The variable
<code>x</code> must be numeric.

</p>
</li>
<li><p>newname = event(y,x): Mark an event at time y.
In the usual case that <code>x</code> is missing the new 0/1 variable
will be similar to the 0/1 status variable of a survival time.

</p>
</li>
<li><p>newname = cumevent(y,x): Cumulative events.

</p>
</li></ul>

<p>The function adds three new variables to the output data set: 
<code>tstart</code>, <code>tstop</code>, and <code>id</code>.  The <code>options</code> argument
can be used to change these names.
If, in the first call, the <code>id</code> argument is a simple name, that
variable name will be used as the default for the <code>idname</code> option.
If <code>data1</code> contains the <code>tstart</code> variable then that is used as
the starting point for the created time intervals, otherwise the initial
interval for each id will begin at 0 by default.
This will lead to an invalid interval and subsequent error if say a
death time were &lt;= 0.
</p>
<p>The <code>na.rm</code> option affects creation of time-dependent covariates.
Should a data row in <code>data2</code> that has a missing value for the
variable be ignored or should it generate an
observation with a value of NA?  The default of TRUE causes
the last non-missing value to be carried forward.
The <code>delay</code> option causes a time-dependent covariate's new
value to be delayed, see the vignette for an example.
</p>


<h3>Value</h3>

<p>a data frame with two extra attributes <code>tm.retain</code> and
<code>tcount</code>.
The first contains the names of the key variables, and which names
correspond to tdc or event variables.
The tcount variable contains counts of the match types.
New time values that occur before the first interval for a subject
are &quot;early&quot;, those after the last interval for a subject are &quot;late&quot;,
and those that fall into a gap are of type &quot;gap&quot;. 
All these are are considered to be outside the specified time frame for the
given subject.  An event of this type will be discarded.
An observation in <code>data2</code> whose identifier matches no rows in
<code>data1</code> is of type &quot;missid&quot; and is also discarded.
A time-dependent covariate value will be applied to later intervals but
will not generate a new time point in the output.
</p>
<p>The most common type will usually be &quot;within&quot;, corresponding to
those new times that
fall inside an existing interval and cause it to be split into two.
Observations that fall exactly on the edge of an interval but within the
(min, max] time for a subject are counted
as being on a &quot;leading&quot; edge, &quot;trailing&quot; edge or &quot;boundary&quot;.
The first corresponds for instance
to an occurrence at 17 for someone with an intervals of (0,15] and (17, 35].
A <code>tdc</code> at time 17  will affect this interval
but an <code>event</code> at 17 would be ignored.  An <code>event</code>
occurrence at 15 would count in the (0,15] interval.
The last case is where the main data set has touching
intervals for a subject, e.g. (17, 28] and (28,35] and a new occurrence
lands at the join.  Events will go to the earlier interval and counts
to the latter one.  A last column shows the number of additions
where the id and time point were identical.
When this occurs, the <code>tdc</code> and <code>event</code> operators will use
the final value in the data (last edit wins), but ignoring missing,
while <code>cumtdc</code> and <code>cumevent</code> operators add up the values.
</p>
<p>These extra attributes are ephemeral and will be discarded
if the dataframe is modified.  This is intentional, since they will
become invalid if for instance a subset were selected. 
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+neardate">neardate</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># The pbc data set contains baseline data and follow-up status
# for a set of subjects with primary biliary cirrhosis, while the
# pbcseq data set contains repeated laboratory values for those
# subjects.  
# The first data set contains data on 312 subjects in a clinical trial plus
# 106 that agreed to be followed off protocol, the second data set has data
# only on the trial subjects.
temp &lt;- subset(pbc, id &lt;= 312, select=c(id:sex, stage)) # baseline data
pbc2 &lt;- tmerge(temp, temp, id=id, endpt = event(time, status))
pbc2 &lt;- tmerge(pbc2, pbcseq, id=id, ascites = tdc(day, ascites),
               bili = tdc(day, bili), albumin = tdc(day, albumin),
               protime = tdc(day, protime), alk.phos = tdc(day, alk.phos))

fit &lt;- coxph(Surv(tstart, tstop, endpt==2) ~ protime + log(bili), data=pbc2)
</code></pre>

<hr>
<h2 id='tobin'>Tobin's Tobit data</h2><span id='topic+tobin'></span>

<h3>Description</h3>

<p>Economists fit a parametric censored data model called the
&lsquo;tobit&rsquo;.  These data are from Tobin's original paper. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tobin
data(tobin, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 20 observations on the following 3 variables.
</p>

<dl>
<dt>durable</dt><dd><p>Durable goods purchase</p>
</dd>
<dt>age</dt><dd><p>Age in years</p>
</dd>
<dt>quant</dt><dd><p>Liquidity ratio (x 1000)</p>
</dd>
</dl>



<h3>Source</h3>

<p>J Tobin (1958),
Estimation of relationships for limited dependent variables.
<em>Econometrica</em> <b>26</b>, 24&ndash;36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tfit &lt;- survreg(Surv(durable, durable&gt;0, type='left') ~age + quant,
                data=tobin, dist='gaussian')

predict(tfit,type="response")

</code></pre>

<hr>
<h2 id='transplant'>Liver transplant waiting list</h2><span id='topic+transplant'></span>

<h3>Description</h3>

<p>Subjects on a liver transplant waiting list from 1990-1999, and their
disposition: received a transplant, died while waiting, withdrew from
the list, or censored.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transplant
data(transplant, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 815 (transplant) observations
on the following 6 variables.
</p>

<dl>
<dt><code>age</code></dt><dd><p>age at addition to the waiting list</p>
</dd>
<dt><code>sex</code></dt><dd><p><code>m</code> or <code>f</code></p>
</dd>
<dt><code>abo</code></dt><dd><p>blood type: <code>A</code>, <code>B</code>, <code>AB</code>  or <code>O</code></p>
</dd>
<dt><code>year</code></dt><dd><p>year in which they entered the waiting list</p>
</dd>
<dt><code>futime</code></dt><dd><p>time from entry to final disposition</p>
</dd>
<dt><code>event</code></dt><dd><p>final disposition: <code>censored</code>,
<code>death</code>,
<code>ltx</code> or <code>withdraw</code></p>
</dd>
</dl>



<h3>Details</h3>

<p>This represents the transplant experience in a particular region,
over a time period in which liver transplant became much more widely
recognized as a viable treatment modality.
The number of liver transplants rises over the period, but the number of
subjects added to the liver transplant waiting list grew much faster.
Important questions addressed by the data are the change in waiting
time, who waits, and whether there was an consequent increase in deaths
while on the list.
</p>
<p>Blood type is an important consideration.  Donor livers from subjects
with blood type O can be used by patients with A, B, AB or 0 blood
types, whereas an AB liver can only be used by an AB recipient.
Thus type O subjects on the waiting list are at a disadvantage, since
the pool of competitors is larger for type O donor livers.
</p>
<p>This data is of historical interest and provides a useful example of
competing risks, but it has little relevance to current
practice.  Liver allocation policies have evolved and now depend
directly on each individual patient's risk and need, assessments of which are
regularly updated while a patient is on the waiting list.
The overall organ shortage remains acute, however.
</p>
<p>The <code>transplant</code> data set was a version used early in the analysis,
<code>transplant2</code> has several additions and corrections, and
was the final data set and matches the paper.
</p>


<h3>References</h3>

<p>Kim WR, Therneau TM, Benson JT, Kremers WK, Rosen CB, Gores GJ, Dickson
ER. 
Deaths on the liver transplant waiting list: An analysis of competing risks. 
Hepatology 2006 Feb; 43(2):345-51.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#since event is a factor, survfit creates competing risk curves
pfit &lt;- survfit(Surv(futime, event) ~ abo, transplant)
pfit[,2]  #time to liver transplant, by blood type
plot(pfit[,2], mark.time=FALSE, col=1:4, lwd=2, xmax=735,
       xscale=30.5, xlab="Months", ylab="Fraction transplanted",
       xaxt = 'n')
temp &lt;- c(0, 6, 12, 18, 24)
axis(1, temp*30.5, temp)
legend(450, .35, levels(transplant$abo), lty=1, col=1:4, lwd=2)

# competing risks for type O
plot(pfit[4,], xscale=30.5, xmax=735, col=1:3, lwd=2)
legend(450, .4, c("Death", "Transpant", "Withdrawal"), col=1:3, lwd=2)
</code></pre>

<hr>
<h2 id='udca'>Data from a trial of usrodeoxycholic acid
</h2><span id='topic+udca'></span><span id='topic+udca1'></span><span id='topic+udca2'></span>

<h3>Description</h3>

<p>Data from a trial of ursodeoxycholic acid (UDCA) in patients with primary
biliary cirrohosis (PBC).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udca
udca2
data(udca, package="survival")
</code></pre>


<h3>Format</h3>

<p>A data frame with 170 observations on the following 15 variables.
</p>

<dl>
<dt><code>id</code></dt><dd><p>subject identifier</p>
</dd>
<dt><code>trt</code></dt><dd><p>treatment of 0=placebo, 1=UDCA</p>
</dd>
<dt><code>entry.dt</code></dt><dd><p>date of entry into the study</p>
</dd>
<dt><code>last.dt</code></dt><dd><p>date of last on-study visit</p>
</dd>
<dt><code>stage</code></dt><dd><p>stage of disease</p>
</dd>
<dt><code>bili</code></dt><dd><p>bilirubin value at entry</p>
</dd>
<dt><code>riskscore</code></dt><dd><p>the Mayo PBC risk score at entry</p>
</dd>
<dt><code>death.dt</code></dt><dd><p>date of death</p>
</dd>
<dt><code>tx.dt</code></dt><dd><p>date of liver transplant</p>
</dd>
<dt><code>hprogress.dt</code></dt><dd><p>date of histologic progression</p>
</dd>
<dt><code>varices.dt</code></dt><dd><p>appearance of esphogeal varices</p>
</dd>
<dt><code>ascites.dt</code></dt><dd><p>appearance of ascites</p>
</dd>
<dt><code>enceph.dt</code></dt><dd><p>appearance of encephalopathy</p>
</dd>
<dt><code>double.dt</code></dt><dd><p>doubling of initial bilirubin</p>
</dd>
<dt><code>worsen.dt</code></dt><dd><p>worsening of symptoms by two stages</p>
</dd>
</dl>



<h3>Details</h3>

<p>This data set is used in the Therneau and Grambsh.  The <code>udca1</code>
data set contains the baseline variables along with the time until the
first endpoint (any of death, transplant, ..., worsening).
The <code>udca2</code> data set treats all of the endpoints as parallel
events and has a stratum for each.
</p>


<h3>References</h3>

<p>T. M. Therneau and P. M. Grambsch, Modeling survival data: extending the Cox
model.  Springer, 2000.
</p>
<p>K. D. Lindor, E. R. Dickson, W. P Baldus, R.A. Jorgensen, J. Ludwig,
P. A. Murtaugh, J. M. Harrison, R. H. Weisner, M. L. Anderson,
S. M. Lange, G. LeSage, S. S. Rossi and A. F. Hofman.
Ursodeoxycholic acid in the treatment of primary biliary cirrhosis.
Gastroenterology, 106:1284-1290, 1994.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># values found in table 8.3 of the book
fit1 &lt;- coxph(Surv(futime, status) ~ trt + log(bili) + stage,
          cluster =id , data=udca1)
fit2 &lt;- coxph(Surv(futime, status) ~ trt + log(bili) + stage +
          strata(endpoint), cluster=id,  data=udca2)

</code></pre>

<hr>
<h2 id='untangle.specials'>
Help Process the &lsquo;specials&rsquo; Argument of the &lsquo;terms&rsquo; Function.
</h2><span id='topic+untangle.specials'></span>

<h3>Description</h3>

<p>Given a <code>terms</code> structure and a desired special name, this returns an
index appropriate for subscripting the <code>terms</code> structure and another
appropriate for the data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>untangle.specials(tt, special, order=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="untangle.specials_+3A_tt">tt</code></td>
<td>

<p>a <code>terms</code> object.
</p>
</td></tr>
<tr><td><code id="untangle.specials_+3A_special">special</code></td>
<td>

<p>the name of a special function, presumably used in the terms object.
</p>
</td></tr>
<tr><td><code id="untangle.specials_+3A_order">order</code></td>
<td>

<p>the order of the desired terms.  If set to 2, interactions with the special
function will be included.
</p>
</td></tr></table>


<h3>Value</h3>

<p>a list with two components:
</p>
<table>
<tr><td><code>vars</code></td>
<td>

<p>a vector of variable names, as would be found in the data frame, of the
specials.
</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>

<p>a numeric vector, suitable for subscripting the terms structure, that indexes
the terms in the expanded model formula which involve the special.
</p>
</td></tr></table>


<h3>Examples</h3>

<pre><code class='language-R'>formula &lt;- Surv(tt,ss) ~ x + z*strata(id)
tms &lt;- terms(formula, specials="strata")
## the specials attribute
attr(tms, "specials")
## main effects 
untangle.specials(tms, "strata")
## and interactions
untangle.specials(tms, "strata", order=1:2)
</code></pre>

<hr>
<h2 id='uspop2'>Projected US Population</h2><span id='topic+uspop2'></span>

<h3>Description</h3>

<p>US population by age and sex, for 2000 through 2020</p>


<h3>Format</h3>

<p>The data is a matrix with dimensions age, sex, and calendar year.
Age goes from 0 through 100, where the value for age 100 is the total
for all ages of 100 or greater.
</p>


<h3>Details</h3>

<p> This data is often used as a &quot;standardized&quot; population for
epidemiology studies.</p>


<h3>Source</h3>

<p>NP2008_D1:  Projected Population by Single Year of Age, Sex, Race, and Hispanic 
Origin for the United States: July 1, 2000 to July 1, 2050,
www.census.gov/population/projections.
</p>


<h3>See Also</h3>

<p><code><a href="datasets.html#topic+uspop">uspop</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>us50 &lt;- uspop2[51:101,, "2000"]  #US 2000 population, 50 and over
age &lt;- as.integer(dimnames(us50)[[1]])
smat &lt;- model.matrix( ~ factor(floor(age/5)) -1)
ustot &lt;- t(smat) %*% us50  #totals by 5 year age groups
temp &lt;- c(50,55, 60, 65, 70, 75, 80, 85, 90, 95)
dimnames(ustot) &lt;- list(c(paste(temp, temp+4, sep="-"), "100+"),
                         c("male", "female"))
</code></pre>

<hr>
<h2 id='vcov.coxph'>Variance-covariance matrix</h2><span id='topic+vcov.coxph'></span><span id='topic+vcov.survreg'></span>

<h3>Description</h3>

<p>Extract and return the variance-covariance matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coxph'
vcov(object, complete=TRUE, ...)
## S3 method for class 'survreg'
vcov(object, complete=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcov.coxph_+3A_object">object</code></td>
<td>
<p>a fitted model object</p>
</td></tr>
<tr><td><code id="vcov.coxph_+3A_complete">complete</code></td>
<td>
<p>logical indicating if the full variance-covariance
matrix should be returned.  This has an effect only for an
over-determined fit where some of the coefficients are undefined,
and <code>coef(object)</code> contains corresponding NA values.
If <code>complete=TRUE</code> the returned matrix will have row/column for
each coefficient, if FALSE it will contain rows/columns
corresponding to the non-missing coefficients.
The coef() function has a simpilar <code>complete</code> argument.
</p>
</td></tr>
<tr><td><code id="vcov.coxph_+3A_...">...</code></td>
<td>
<p>additional arguments for method functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the <code>coxph</code> and <code>survreg</code> functions the returned matrix
is a particular generalized inverse:  the row and column corresponding
to any NA coefficients will be zero.  This is a side effect of the
generalized cholesky decomposion used in the unerlying compuatation.
</p>


<h3>Value</h3>

<p>a matrix</p>

<hr>
<h2 id='veteran'>Veterans' Administration Lung Cancer study</h2><span id='topic+veteran'></span>

<h3>Description</h3>

<p>Randomised trial of two treatment regimens for lung cancer.
This is a standard survival analysis data set.</p>


<h3>Usage</h3>

<pre><code class='language-R'>veteran
data(cancer, package="survival")
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    trt:</td><td style="text-align: left;"> 1=standard 2=test</td>
</tr>
<tr>
 <td style="text-align: left;">
    celltype:</td><td style="text-align: left;"> 1=squamous,  2=smallcell,  3=adeno,  4=large</td>
</tr>
<tr>
 <td style="text-align: left;">
    time:</td><td style="text-align: left;"> survival time</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> censoring status</td>
</tr>
<tr>
 <td style="text-align: left;">
    karno:</td><td style="text-align: left;"> Karnofsky performance score (100=good)</td>
</tr>
<tr>
 <td style="text-align: left;">
    diagtime:</td><td style="text-align: left;"> months from diagnosis to randomisation</td>
</tr>
<tr>
 <td style="text-align: left;">
    age:</td><td style="text-align: left;"> in years</td>
</tr>
<tr>
 <td style="text-align: left;">
    prior:</td><td style="text-align: left;"> prior therapy 0=no, 10=yes</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>D Kalbfleisch and RL Prentice (1980),
<em>The Statistical Analysis of Failure Time Data</em>.
Wiley, New York.
</p>

<hr>
<h2 id='xtfrm.Surv'>Sorting order for Surv objects</h2><span id='topic+xtfrm.Surv'></span><span id='topic+sort.Surv'></span><span id='topic+order.Surv'></span>

<h3>Description</h3>

<p>Sort survival objects into a partial order, which is the same one
used internally for many of the calculations.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Surv'
xtfrm(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xtfrm.Surv_+3A_x">x</code></td>
<td>
<p>a <code>Surv</code> object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This creates a partial ordering of survival objects.
The result is sorted in time order, for tied pairs of times right censored
events come after observed events (censor after death), and left
censored events are sorted before observed events.
For counting process data <code>(tstart, tstop, status)</code> the ordering
is by stop time, status, and start time, again with censoring last.
Interval censored data is sorted using the midpoint of each interval.
</p>
<p>The <code>xtfrm</code> routine is used internally by <code>order</code> and
<code>sort</code>, so these results carry over to those routines.
</p>


<h3>Value</h3>

<p>a vector of integers which will have the same sort order as
<code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+sort">sort</a></code>, <code><a href="base.html#topic+order">order</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>test &lt;- c(Surv(c(10, 9,9, 8,8,8,7,5,5,4), rep(1:0, 5)), Surv(6.2, NA))
test
sort(test)
</code></pre>

<hr>
<h2 id='yates'>Population prediction</h2><span id='topic+yates'></span>

<h3>Description</h3>

<p>Compute population marginal means (PMM) from a model fit, for
a chosen population and statistic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>yates(fit, term, population = c("data", "factorial", "sas"),
levels, test = c("global", "trend", "pairwise"), predict = "linear",
options, nsim = 200, method = c("direct", "sgtt"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="yates_+3A_fit">fit</code></td>
<td>
<p>a model fit.  Examples using lm, glm, and coxph objects
are given in the vignette.
</p>
</td></tr>
<tr><td><code id="yates_+3A_term">term</code></td>
<td>
<p>the term from the model whic is to be evaluated.
This can be written as a character string or as a formula.
</p>
</td></tr>
<tr><td><code id="yates_+3A_population">population</code></td>
<td>
<p>the population to be used for the adjusting
variables.  User can supply their own data frame or select one
of the built in choices.
The argument also allows &quot;empirical&quot; and &quot;yates&quot; as aliases for
data and factorial, respectively, and ignores case.
</p>
</td></tr>
<tr><td><code id="yates_+3A_levels">levels</code></td>
<td>
<p>optional, what values for <code>term</code> should be used.
</p>
</td></tr>
<tr><td><code id="yates_+3A_test">test</code></td>
<td>
<p>the test for comparing the population predictions.
</p>
</td></tr>
<tr><td><code id="yates_+3A_predict">predict</code></td>
<td>
<p>what to predict.  For a glm model this might be the
'link' or 'response'.  For a coxph model it can be linear, risk, or
survival.  User written functions are allowed.
</p>
</td></tr>
<tr><td><code id="yates_+3A_options">options</code></td>
<td>
<p>optional arguments for the prediction method.
</p>
</td></tr>
<tr><td><code id="yates_+3A_nsim">nsim</code></td>
<td>
<p>number of simulations used to compute a variance for the
predictions.  This is not needed for the linear predictor.
</p>
</td></tr>
<tr><td><code id="yates_+3A_method">method</code></td>
<td>
<p>the computational approach for testing equality of the
population predictions.  Either the direct approach or the algorithm
used by the SAS glim procedure for &quot;type 3&quot; tests.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The many options and details of this function are best described in a
vignette on population prediction.
</p>


<h3>Value</h3>

<p>an object of class <code>yates</code> with components of
</p>
<table>
<tr><td><code>estimate</code></td>
<td>
<p>a data frame with one row for each level of the term,
and columns containing the level, the mean population predicted
value (mppv) and its standard deviation.</p>
</td></tr>
<tr><td><code>tests</code></td>
<td>
<p>a matrix giving the test statistics</p>
</td></tr>
<tr><td><code>mvar</code></td>
<td>
<p>the full variance-covariance matrix of the mppv values</p>
</td></tr>
<tr><td><code>summary</code></td>
<td>
<p>optional: any further summary if the values provided by
the prediction method.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit1 &lt;- lm(skips ~ Solder*Opening + Mask, data = solder)
yates(fit1, ~Opening, population = "factorial")

fit2 &lt;- coxph(Surv(time, status) ~ factor(ph.ecog)*sex + age, lung)
yates(fit2, ~ ph.ecog, predict="risk")  # hazard ratio
</code></pre>

<hr>
<h2 id='yates_setup'>Method for adding new models to the <code>yates</code> function.
</h2><span id='topic+yates_setup'></span>

<h3>Description</h3>

<p>This is a method which is called by the <code>yates</code>
function, in order to setup the code to handle a particular
model type.  Methods for glm, coxph, and default are part of
the survival package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>yates_setup(fit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="yates_setup_+3A_fit">fit</code></td>
<td>
<p>a fitted model object</p>
</td></tr>
<tr><td><code id="yates_setup_+3A_...">...</code></td>
<td>
<p>optional arguments for some methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the predicted value should be the linear predictor, the function
should return NULL.  The <code>yates</code> routine has particularly efficient
code for this case.
Otherwise it should return a prediction function or a list of two
elements containing the prediction function and a summary function.
The prediction function will be passed the linear predictor as a single
argument and should return a vector of predicted values.
</p>


<h3>Note</h3>

<p>See the vignette on population prediction for more details.</p>


<h3>Author(s)</h3>

<p>Terry Therneau</p>


<h3>See Also</h3>

<p><code><a href="#topic+yates">yates</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
