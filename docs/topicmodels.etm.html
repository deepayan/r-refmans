<!DOCTYPE html><html><head><title>Help for package topicmodels.etm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {topicmodels.etm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as.matrix.ETM'><p>Get matrices out of an ETM object</p></a></li>
<li><a href='#ETM'><p>Topic Modelling in Semantic Embedding Spaces</p></a></li>
<li><a href='#ng20'><p>Bag of words sample of the 20 newsgroups dataset</p></a></li>
<li><a href='#plot.ETM'><p>Plot functionality for an ETM object</p></a></li>
<li><a href='#predict.ETM'><p>Predict functionality for an ETM object.</p></a></li>
<li><a href='#summary.ETM'><p>Project ETM embeddings using UMAP</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Topic Modelling in Embedding Spaces</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jan Wijffels &lt;jwijffels@bnosac.be&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Find topics in texts which are semantically embedded using techniques like word2vec or Glove. 
    This topic modelling technique models each word with a categorical distribution whose natural parameter is the inner product between a word embedding and an embedding of its assigned topic.
    The techniques are explained in detail in the paper 'Topic Modeling in Embedding Spaces' by Adji B. Dieng, Francisco J. R. Ruiz, David M. Blei (2019), available at &lt;<a href="https://doi.org/10.48550/arXiv.1907.04907">doi:10.48550/arXiv.1907.04907</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>LibTorch (https://pytorch.org/)</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, stats, Matrix, torch (&ge; 0.5.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>udpipe (&ge; 0.8.4), word2vec, uwot, tinytest, textplot (&ge;
0.2.0), ggrepel, ggalt</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-11-05 11:01:22 UTC; Jan</td>
</tr>
<tr>
<td>Author:</td>
<td>Jan Wijffels [aut, cre, cph] (R implementation),
  BNOSAC [cph] (R implementation),
  Adji B. Dieng [ctb, cph] (original Python implementation in inst/orig),
  Francisco J. R. Ruiz [ctb, cph] (original Python implementation in
    inst/orig),
  David M. Blei [ctb, cph] (original Python implementation in inst/orig)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-11-08 08:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as.matrix.ETM'>Get matrices out of an ETM object</h2><span id='topic+as.matrix.ETM'></span>

<h3>Description</h3>

<p>Convenience function to extract 
</p>

<ul>
<li><p>embeddings of the topic centers
</p>
</li>
<li><p>embeddings of the words used in the model
</p>
</li>
<li><p>words emmitted by each topic (beta), which is the softmax-transformed inner product of word embedding and topic embeddings
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ETM'
as.matrix(x, type = c("embedding", "beta"), which = c("topics", "words"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.matrix.ETM_+3A_x">x</code></td>
<td>
<p>an object of class <code>ETM</code></p>
</td></tr>
<tr><td><code id="as.matrix.ETM_+3A_type">type</code></td>
<td>
<p>character string with the type of information to extract: either 'beta' (words emttied by each topic) or 'embedding' (embeddings of words or topic centers). Defaults to 'embedding'.</p>
</td></tr>
<tr><td><code id="as.matrix.ETM_+3A_which">which</code></td>
<td>
<p>a character string with either 'words' or 'topics' to get either the embeddings of the words used in the model or the embedding of the topic centers. Defaults to 'topics'. Only used if type = 'embedding'.</p>
</td></tr>
<tr><td><code id="as.matrix.ETM_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric matrix containing, depending on the value supplied in <code>type</code> 
either the embeddings of the topic centers, the embeddings of the words or the words emitted by each topic
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ETM">ETM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(torch)
library(topicmodels.etm)
path  &lt;- system.file(package = "topicmodels.etm", "example", "example_etm.ckpt")
model &lt;- torch_load(path)

topic.centers     &lt;- as.matrix(model, type = "embedding", which = "topics")
word.embeddings   &lt;- as.matrix(model, type = "embedding", which = "words")
topic.terminology &lt;- as.matrix(model, type = "beta")

</code></pre>

<hr>
<h2 id='ETM'>Topic Modelling in Semantic Embedding Spaces</h2><span id='topic+ETM'></span>

<h3>Description</h3>

<p>ETM is a generative topic model combining traditional topic models (LDA) with word embeddings (word2vec). <br />
</p>

<ul>
<li><p>It models each word with a categorical distribution whose natural parameter is the inner product between
a word embedding and an embedding of its assigned topic.
</p>
</li>
<li><p>The model is fitted using an amortized variational inference algorithm on top of libtorch.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>ETM(
  k = 20,
  embeddings,
  dim = 800,
  activation = c("relu", "tanh", "softplus", "rrelu", "leakyrelu", "elu", "selu",
    "glu"),
  dropout = 0.5,
  vocab = rownames(embeddings)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ETM_+3A_k">k</code></td>
<td>
<p>the number of topics to extract</p>
</td></tr>
<tr><td><code id="ETM_+3A_embeddings">embeddings</code></td>
<td>
<p>either a matrix with pretrained word embeddings or an integer with the dimension of the word embeddings. Defaults to 50 if not provided.</p>
</td></tr>
<tr><td><code id="ETM_+3A_dim">dim</code></td>
<td>
<p>dimension of the variational inference hyperparameter theta (passed on to <code><a href="torch.html#topic+nn_linear">nn_linear</a></code>). Defaults to 800.</p>
</td></tr>
<tr><td><code id="ETM_+3A_activation">activation</code></td>
<td>
<p>character string with the activation function of theta. Either one of 'relu', 'tanh', 'softplus', 'rrelu', 'leakyrelu', 'elu', 'selu', 'glu'. Defaults to 'relu'.</p>
</td></tr>
<tr><td><code id="ETM_+3A_dropout">dropout</code></td>
<td>
<p>dropout percentage on the variational distribution for theta (passed on to <code><a href="torch.html#topic+nn_dropout">nn_dropout</a></code>). Defaults to 0.5.</p>
</td></tr>
<tr><td><code id="ETM_+3A_vocab">vocab</code></td>
<td>
<p>a character vector with the words from the vocabulary. Defaults to the rownames of the <code>embeddings</code> argument.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class ETM which is a torch <code>nn_module</code> containing o.a.
</p>

<ul>
<li><p>num_topics: the number of topics
</p>
</li>
<li><p>vocab: character vector with the terminology used in the model
</p>
</li>
<li><p>vocab_size: the number of words in <code>vocab</code>
</p>
</li>
<li><p>rho: The word embeddings
</p>
</li>
<li><p>alphas: The topic embeddings
</p>
</li></ul>



<h3>Methods</h3>


<dl>
<dt><code>fit(data, optimizer, epoch, batch_size, normalize = TRUE, clip = 0, lr_anneal_factor = 4, lr_anneal_nonmono = 10)</code></dt><dd><p>Fit the model on a document term matrix by splitting the data in 70/30 training/test set and updating the model weights.</p>
</dd>
</dl>



<h3>Arguments</h3>


<dl>
<dt>data</dt><dd><p>bag of words document term matrix in <code>dgCMatrix</code> format</p>
</dd>
<dt>optimizer</dt><dd><p>object of class <code>torch_Optimizer</code></p>
</dd>
<dt>epoch</dt><dd><p>integer with the number of iterations to train</p>
</dd>
<dt>batch_size</dt><dd><p>integer with the size of the batch</p>
</dd>
<dt>normalize</dt><dd><p>logical indicating to normalize the bag of words data</p>
</dd>
<dt>clip</dt><dd><p>number between 0 and 1 indicating to do gradient clipping - passed on to <code><a href="torch.html#topic+nn_utils_clip_grad_norm_">nn_utils_clip_grad_norm_</a></code></p>
</dd>
<dt>lr_anneal_factor</dt><dd><p>divide the learning rate by this factor when the loss on the test set is monotonic for at least <code>lr_anneal_nonmono</code> training iterations</p>
</dd>
<dt>lr_anneal_nonmono</dt><dd><p>number of iterations after which learning rate annealing is executed if the loss does not decreases</p>
</dd>
</dl>



<h3>References</h3>

<p><a href="https://arxiv.org/pdf/1907.04907.pdf">https://arxiv.org/pdf/1907.04907.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(torch)
library(topicmodels.etm)
library(word2vec)
library(udpipe)
data(brussels_reviews_anno, package = "udpipe")
##
## Toy example with pretrained embeddings
##

## a. build word2vec model
x          &lt;- subset(brussels_reviews_anno, language %in% "nl")
x          &lt;- paste.data.frame(x, term = "lemma", group = "doc_id") 
set.seed(4321)
w2v        &lt;- word2vec(x = x$lemma, dim = 15, iter = 20, type = "cbow", min_count = 5)
embeddings &lt;- as.matrix(w2v)

## b. build document term matrix on nouns + adjectives, align with the embedding terms
dtm &lt;- subset(brussels_reviews_anno, language %in% "nl" &amp; upos %in% c("NOUN", "ADJ"))
dtm &lt;- document_term_frequencies(dtm, document = "doc_id", term = "lemma")
dtm &lt;- document_term_matrix(dtm)
dtm &lt;- dtm_conform(dtm, columns = rownames(embeddings))
dtm &lt;- dtm[dtm_rowsums(dtm) &gt; 0, ]

## create and fit an embedding topic model - 8 topics, theta 100-dimensional
if (torch::torch_is_installed()) {

set.seed(4321)
torch_manual_seed(4321)
model       &lt;- ETM(k = 8, dim = 100, embeddings = embeddings, dropout = 0.5)
optimizer   &lt;- optim_adam(params = model$parameters, lr = 0.005, weight_decay = 0.0000012)
overview    &lt;- model$fit(data = dtm, optimizer = optimizer, epoch = 40, batch_size = 1000)
scores      &lt;- predict(model, dtm, type = "topics")

lastbatch   &lt;- subset(overview$loss, overview$loss$batch_is_last == TRUE)
plot(lastbatch$epoch, lastbatch$loss)
plot(overview$loss_test)

## show top words in each topic
terminology &lt;- predict(model, type = "terms", top_n = 7)
terminology

##
## Toy example without pretrained word embeddings
##
set.seed(4321)
torch_manual_seed(4321)
model       &lt;- ETM(k = 8, dim = 100, embeddings = 15, dropout = 0.5, vocab = colnames(dtm))
optimizer   &lt;- optim_adam(params = model$parameters, lr = 0.005, weight_decay = 0.0000012)
overview    &lt;- model$fit(data = dtm, optimizer = optimizer, epoch = 40, batch_size = 1000)
terminology &lt;- predict(model, type = "terms", top_n = 7)
terminology





}
</code></pre>

<hr>
<h2 id='ng20'>Bag of words sample of the 20 newsgroups dataset</h2><span id='topic+ng20'></span>

<h3>Description</h3>

<p>Data available at <a href="https://github.com/bnosac-dev/ETM/tree/master/data/20ng">https://github.com/bnosac-dev/ETM/tree/master/data/20ng</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ng20)
str(ng20$vocab)
str(ng20$bow_tr$tokens)
str(ng20$bow_tr$counts)
</code></pre>

<hr>
<h2 id='plot.ETM'>Plot functionality for an ETM object</h2><span id='topic+plot.ETM'></span>

<h3>Description</h3>

<p>Convenience function allowing to plot 
</p>

<ul>
<li><p>the evolution of the loss on the training / test set in order to inspect training convergence
</p>
</li>
<li><p>the <code>ETM</code> model in 2D dimensional space using a umap projection. 
This plot uses function <code><a href="textplot.html#topic+textplot_embedding_2d">textplot_embedding_2d</a></code> from the textplot R package and
plots the top_n most emitted words of each topic and the topic centers in 2 dimensions
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ETM'
plot(
  x,
  type = c("loss", "topics"),
  which,
  top_n = 4,
  title = "ETM topics",
  subtitle = "",
  encircle = FALSE,
  points = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ETM_+3A_x">x</code></td>
<td>
<p>an object of class <code>ETM</code></p>
</td></tr>
<tr><td><code id="plot.ETM_+3A_type">type</code></td>
<td>
<p>character string with the type of plot to generate: either 'loss' or 'topics'</p>
</td></tr>
<tr><td><code id="plot.ETM_+3A_which">which</code></td>
<td>
<p>an integer vector of topics to plot, used in case type = 'topics'. Defaults to all topics. See the example below.</p>
</td></tr>
<tr><td><code id="plot.ETM_+3A_top_n">top_n</code></td>
<td>
<p>passed on to <code>summary.ETM</code> in order to visualise the top_n most relevant words for each topic. Defaults to 4.</p>
</td></tr>
<tr><td><code id="plot.ETM_+3A_title">title</code></td>
<td>
<p>passed on to textplot_embedding_2d, used in case type = 'topics'</p>
</td></tr>
<tr><td><code id="plot.ETM_+3A_subtitle">subtitle</code></td>
<td>
<p>passed on to textplot_embedding_2d, used in case type = 'topics'</p>
</td></tr>
<tr><td><code id="plot.ETM_+3A_encircle">encircle</code></td>
<td>
<p>passed on to textplot_embedding_2d, used in case type = 'topics'</p>
</td></tr>
<tr><td><code id="plot.ETM_+3A_points">points</code></td>
<td>
<p>passed on to textplot_embedding_2d, used in case type = 'topics'</p>
</td></tr>
<tr><td><code id="plot.ETM_+3A_...">...</code></td>
<td>
<p>arguments passed on to <code><a href="#topic+summary.ETM">summary.ETM</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>In case <code>type</code> is set to 'topics', maps the topic centers and most emitted words for each topic
to 2D using <code><a href="#topic+summary.ETM">summary.ETM</a></code> and returns a ggplot object by calling <code><a href="textplot.html#topic+textplot_embedding_2d">textplot_embedding_2d</a></code>. <br />
For type 'loss', makes a base graphics plot and returns invisibly nothing.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ETM">ETM</a></code>, <code><a href="#topic+summary.ETM">summary.ETM</a></code>, <code><a href="textplot.html#topic+textplot_embedding_2d">textplot_embedding_2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(torch)
library(topicmodels.etm)
path  &lt;- system.file(package = "topicmodels.etm", "example", "example_etm.ckpt")
model &lt;- torch_load(path)
plot(model, type = "loss")



library(torch)
library(topicmodels.etm)
library(textplot)
library(uwot)
library(ggrepel)
library(ggalt)
path  &lt;- system.file(package = "topicmodels.etm", "example", "example_etm.ckpt")
model &lt;- torch_load(path)
plt   &lt;- plot(model, type = "topics", top_n = 7, which = c(1, 2, 14, 16, 18, 19),
              metric = "cosine", n_neighbors = 15, 
              fast_sgd = FALSE, n_threads = 2, verbose = TRUE,
              title = "ETM Topics example")
plt

</code></pre>

<hr>
<h2 id='predict.ETM'>Predict functionality for an ETM object.</h2><span id='topic+predict.ETM'></span>

<h3>Description</h3>

<p>Predict to which ETM topic a text belongs or extract which words are emitted for each topic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ETM'
predict(
  object,
  newdata,
  type = c("topics", "terms"),
  batch_size = nrow(newdata),
  normalize = TRUE,
  top_n = 10,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ETM_+3A_object">object</code></td>
<td>
<p>an object of class <code>ETM</code></p>
</td></tr>
<tr><td><code id="predict.ETM_+3A_newdata">newdata</code></td>
<td>
<p>bag of words document term matrix in <code>dgCMatrix</code> format. Only used in case type = 'topics'.</p>
</td></tr>
<tr><td><code id="predict.ETM_+3A_type">type</code></td>
<td>
<p>a character string with either 'topics' or 'terms' indicating to either predict to which
topic a document encoded as a set of bag of words belongs to or to extract the most emitted terms for each topic</p>
</td></tr>
<tr><td><code id="predict.ETM_+3A_batch_size">batch_size</code></td>
<td>
<p>integer with the size of the batch in order to do chunkwise predictions in chunks of <code>batch_size</code> rows. Defaults to the whole dataset provided in <code>newdata</code>.
Only used in case type = 'topics'.</p>
</td></tr>
<tr><td><code id="predict.ETM_+3A_normalize">normalize</code></td>
<td>
<p>logical indicating to normalize the bag of words data. Defaults to <code>TRUE</code> similar as the default when building the <code>ETM</code> model. 
Only used in case type = 'topics'.</p>
</td></tr>
<tr><td><code id="predict.ETM_+3A_top_n">top_n</code></td>
<td>
<p>integer with the number of most relevant words for each topic to extract. Only used in case type = 'terms'.</p>
</td></tr>
<tr><td><code id="predict.ETM_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns for
</p>

<ul>
<li><p>type 'topics': a matrix with topic probabilities of dimension nrow(newdata) x the number of topics
</p>
</li>
<li><p>type 'terms': a list of data.frame's where each data.frame has columns term, beta and rank indicating the
top_n most emitted terms for that topic. List element 1 corresponds to the top terms emitted by topic 1, element 2 to topic 2 ...
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+ETM">ETM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(torch)
library(topicmodels.etm)
path  &lt;- system.file(package = "topicmodels.etm", "example", "example_etm.ckpt")
model &lt;- torch_load(path)

# Get most emitted words for each topic
terminology  &lt;- predict(model, type = "terms", top_n = 5)
terminology

# Get topics probabilities for each document
path   &lt;- system.file(package = "topicmodels.etm", "example", "example_dtm.rds")
dtm    &lt;- readRDS(path)
dtm    &lt;- head(dtm, n = 5)
scores &lt;- predict(model, newdata = dtm, type = "topics")
scores

</code></pre>

<hr>
<h2 id='summary.ETM'>Project ETM embeddings using UMAP</h2><span id='topic+summary.ETM'></span>

<h3>Description</h3>

<p>Uses the uwot package to map the word embeddings and the center of the topic embeddings to a 2-dimensional space
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ETM'
summary(object, type = c("umap"), n_components = 2, top_n = 20, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.ETM_+3A_object">object</code></td>
<td>
<p>object of class <code>ETM</code></p>
</td></tr>
<tr><td><code id="summary.ETM_+3A_type">type</code></td>
<td>
<p>character string with the type of summary to extract. Defaults to 'umap', no other summary information currently implemented.</p>
</td></tr>
<tr><td><code id="summary.ETM_+3A_n_components">n_components</code></td>
<td>
<p>the dimension of the space to embed into. Passed on to <code><a href="uwot.html#topic+umap">umap</a></code>. Defaults to 2.</p>
</td></tr>
<tr><td><code id="summary.ETM_+3A_top_n">top_n</code></td>
<td>
<p>passed on to <code><a href="#topic+predict.ETM">predict.ETM</a></code> to get the <code>top_n</code> most relevant words for each topic in the 2-dimensional space</p>
</td></tr>
<tr><td><code id="summary.ETM_+3A_...">...</code></td>
<td>
<p>further arguments passed onto <code><a href="uwot.html#topic+umap">umap</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with elements
</p>

<ul>
<li><p>center: a matrix with the embeddings of the topic centers
</p>
</li>
<li><p>words: a matrix with the embeddings of the words
</p>
</li>
<li><p>embed_2d: a data.frame which contains a lower dimensional presentation in 2D of the topics and the top_n words associated with
the topic, containing columns type, term, cluster (the topic number), rank, beta, x, y, weight; where type is either 'words' or 'centers', x/y contain the lower dimensional 
positions in 2D of the word and weight is the emitted beta scaled to the highest beta within a topic where the topic center always gets weight 0.8
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="uwot.html#topic+umap">umap</a></code>, <code><a href="#topic+ETM">ETM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(torch)
library(topicmodels.etm)
library(uwot)
path     &lt;- system.file(package = "topicmodels.etm", "example", "example_etm.ckpt")
model    &lt;- torch_load(path)
overview &lt;- summary(model, 
                    metric = "cosine", n_neighbors = 15, 
                    fast_sgd = FALSE, n_threads = 1, verbose = TRUE) 
overview$center
overview$embed_2d

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
