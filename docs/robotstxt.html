<!DOCTYPE html><html><head><title>Help for package robotstxt</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {robotstxt}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#%&gt;%'><p>re-export magrittr pipe operator</p></a></li>
<li><a href='#as.list.robotstxt_text'><p>Method as.list() for class robotstxt_text</p></a></li>
<li><a href='#fix_url'><p>fix_url</p></a></li>
<li><a href='#get_robotstxt'><p>downloading robots.txt file</p></a></li>
<li><a href='#get_robotstxts'><p>function to get multiple robotstxt files</p></a></li>
<li><a href='#guess_domain'><p>function guessing domain from path</p></a></li>
<li><a href='#http_domain_changed'><p>http_domain_changed</p></a></li>
<li><a href='#http_subdomain_changed'><p>http_subdomain_changed</p></a></li>
<li><a href='#http_was_redirected'><p>http_was_redirected</p></a></li>
<li><a href='#is_suspect_robotstxt'><p>is_suspect_robotstxt</p></a></li>
<li><a href='#is_valid_robotstxt'><p>function that checks if file is valid / parsable robots.txt file</p></a></li>
<li><a href='#list_merge'><p>Merge a number of named lists in sequential order</p></a></li>
<li><a href='#named_list'><p>make automatically named list</p></a></li>
<li><a href='#null_to_defeault'><p>null_to_defeault</p></a></li>
<li><a href='#parse_robotstxt'><p>function parsing robots.txt</p></a></li>
<li><a href='#parse_url'><p>parse_url</p></a></li>
<li><a href='#paths_allowed'><p>check if a bot has permissions to access page(s)</p></a></li>
<li><a href='#paths_allowed_worker_spiderbar'><p>paths_allowed_worker spiderbar flavor</p></a></li>
<li><a href='#print.robotstxt'><p>printing robotstxt</p></a></li>
<li><a href='#print.robotstxt_text'><p>printing robotstxt_text</p></a></li>
<li><a href='#remove_domain'><p>function to remove domain from path</p></a></li>
<li><a href='#request_handler_handler'><p>request_handler_handler</p></a></li>
<li><a href='#robotstxt'><p>Generate a representations of a robots.txt file</p></a></li>
<li><a href='#rt_cache'><p>get_robotstxt() cache</p></a></li>
<li><a href='#rt_get_comments'><p>extracting comments from robots.txt</p></a></li>
<li><a href='#rt_get_fields'><p>extracting permissions from robots.txt</p></a></li>
<li><a href='#rt_get_fields_worker'><p>extracting robotstxt fields</p></a></li>
<li><a href='#rt_get_rtxt'><p>load robots.txt files saved along with the package</p></a></li>
<li><a href='#rt_get_useragent'><p>extracting HTTP useragents from robots.txt</p></a></li>
<li><a href='#rt_last_http'><p>storage for http request response objects</p></a></li>
<li><a href='#rt_list_rtxt'><p>list robots.txt files saved along with the package</p></a></li>
<li><a href='#rt_request_handler'><p>rt_request_handler</p></a></li>
<li><a href='#sanitize_path'><p>making paths uniform</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Date:</td>
<td>2020-09-03</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A 'robots.txt' Parser and 'Webbot'/'Spider'/'Crawler'
Permissions Checker</td>
</tr>
<tr>
<td>Version:</td>
<td>0.7.13</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions to download and parse 'robots.txt' files.
        Ultimately the package makes it easy to check if bots
        (spiders, crawler, scrapers, ...) are allowed to access specific
        resources on a domain.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ropensci/robotstxt/issues">https://github.com/ropensci/robotstxt/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://docs.ropensci.org/robotstxt/">https://docs.ropensci.org/robotstxt/</a>,
<a href="https://github.com/ropensci/robotstxt">https://github.com/ropensci/robotstxt</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>stringr (&ge; 1.0.0), httr (&ge; 1.0.0), spiderbar (&ge; 0.2.0),
future (&ge; 1.6.2), future.apply (&ge; 1.0.0), magrittr, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, dplyr, testthat, covr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-09-03 19:07:34 UTC; peter</td>
</tr>
<tr>
<td>Author:</td>
<td>Peter Meissner [aut, cre],
  Kun Ren [aut, cph] (Author and copyright holder of list_merge.R.),
  Oliver Keys [ctb] (original release code review),
  Rich Fitz John [ctb] (original release code review)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Peter Meissner &lt;retep.meissner@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-09-03 19:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>re-export magrittr pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>re-export magrittr pipe operator
</p>

<hr>
<h2 id='as.list.robotstxt_text'>Method as.list() for class robotstxt_text</h2><span id='topic+as.list.robotstxt_text'></span>

<h3>Description</h3>

<p>Method as.list() for class robotstxt_text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'robotstxt_text'
as.list(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.list.robotstxt_text_+3A_x">x</code></td>
<td>
<p>class robotstxt_text object to be transformed into list</p>
</td></tr>
<tr><td><code id="as.list.robotstxt_text_+3A_...">...</code></td>
<td>
<p>further arguments (inherited from <code>base::as.list()</code>)</p>
</td></tr>
</table>

<hr>
<h2 id='fix_url'>fix_url</h2><span id='topic+fix_url'></span>

<h3>Description</h3>

<p>fix_url
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fix_url(url)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fix_url_+3A_url">url</code></td>
<td>
<p>a character string containing a single URL</p>
</td></tr>
</table>

<hr>
<h2 id='get_robotstxt'>downloading robots.txt file</h2><span id='topic+get_robotstxt'></span>

<h3>Description</h3>

<p>downloading robots.txt file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_robotstxt(
  domain,
  warn = getOption("robotstxt_warn", TRUE),
  force = FALSE,
  user_agent = utils::sessionInfo()$R.version$version.string,
  ssl_verifypeer = c(1, 0),
  encoding = "UTF-8",
  verbose = FALSE,
  rt_request_handler = robotstxt::rt_request_handler,
  rt_robotstxt_http_getter = robotstxt::get_robotstxt_http_get,
  on_server_error = on_server_error_default,
  on_client_error = on_client_error_default,
  on_not_found = on_not_found_default,
  on_redirect = on_redirect_default,
  on_domain_change = on_domain_change_default,
  on_file_type_mismatch = on_file_type_mismatch_default,
  on_suspect_content = on_suspect_content_default
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_robotstxt_+3A_domain">domain</code></td>
<td>
<p>domain from which to download robots.txt file</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_warn">warn</code></td>
<td>
<p>warn about being unable to download domain/robots.txt because of</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_force">force</code></td>
<td>
<p>if TRUE instead of using possible cached results the function
will re-download the robotstxt file HTTP response status 404. If this
happens,</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_user_agent">user_agent</code></td>
<td>
<p>HTTP user-agent string to be used to retrieve robots.txt
file from domain</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_ssl_verifypeer">ssl_verifypeer</code></td>
<td>
<p>analog to CURL option
<a href="https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html">https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html</a> &ndash; and
might help with robots.txt file retrieval in some cases</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_encoding">encoding</code></td>
<td>
<p>Encoding of the robots.txt file.</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_verbose">verbose</code></td>
<td>
<p>make function print out more information</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_rt_request_handler">rt_request_handler</code></td>
<td>
<p>handler function that handles request according to
the event handlers specified</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_rt_robotstxt_http_getter">rt_robotstxt_http_getter</code></td>
<td>
<p>function that executes HTTP request</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_on_server_error">on_server_error</code></td>
<td>
<p>request state handler for any 5xx status</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_on_client_error">on_client_error</code></td>
<td>
<p>request state handler for any 4xx HTTP status that is
not 404</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_on_not_found">on_not_found</code></td>
<td>
<p>request state handler for HTTP status 404</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_on_redirect">on_redirect</code></td>
<td>
<p>request state handler for any 3xx HTTP status</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_on_domain_change">on_domain_change</code></td>
<td>
<p>request state handler for any 3xx HTTP status where
domain did change as well</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_on_file_type_mismatch">on_file_type_mismatch</code></td>
<td>
<p>request state handler for content type other
than 'text/plain'</p>
</td></tr>
<tr><td><code id="get_robotstxt_+3A_on_suspect_content">on_suspect_content</code></td>
<td>
<p>request state handler for content that seems to be
something else than a robots.txt file (usually a JSON, XML or HTML)</p>
</td></tr>
</table>

<hr>
<h2 id='get_robotstxts'>function to get multiple robotstxt files</h2><span id='topic+get_robotstxts'></span>

<h3>Description</h3>

<p>function to get multiple robotstxt files
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_robotstxts(
  domain,
  warn = TRUE,
  force = FALSE,
  user_agent = utils::sessionInfo()$R.version$version.string,
  ssl_verifypeer = c(1, 0),
  use_futures = FALSE,
  verbose = FALSE,
  rt_request_handler = robotstxt::rt_request_handler,
  rt_robotstxt_http_getter = robotstxt::get_robotstxt_http_get,
  on_server_error = on_server_error_default,
  on_client_error = on_client_error_default,
  on_not_found = on_not_found_default,
  on_redirect = on_redirect_default,
  on_domain_change = on_domain_change_default,
  on_file_type_mismatch = on_file_type_mismatch_default,
  on_suspect_content = on_suspect_content_default
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_robotstxts_+3A_domain">domain</code></td>
<td>
<p>domain from which to download robots.txt file</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_warn">warn</code></td>
<td>
<p>warn about being unable to download domain/robots.txt because of</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_force">force</code></td>
<td>
<p>if TRUE instead of using possible cached results the function
will re-download the robotstxt file HTTP response status 404. If this
happens,</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_user_agent">user_agent</code></td>
<td>
<p>HTTP user-agent string to be used to retrieve robots.txt
file from domain</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_ssl_verifypeer">ssl_verifypeer</code></td>
<td>
<p>analog to CURL option
<a href="https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html">https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html</a>
&ndash; and might help with robots.txt file retrieval in some cases</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_use_futures">use_futures</code></td>
<td>
<p>Should future::future_lapply be used for possible
parallel/async retrieval or not. Note: check out help
pages and vignettes of package future on how to set up
plans for future execution because the robotstxt package
does not do it on its own.</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_verbose">verbose</code></td>
<td>
<p>make function print out more information</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_rt_request_handler">rt_request_handler</code></td>
<td>
<p>handler function that handles request according to
the event handlers specified</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_rt_robotstxt_http_getter">rt_robotstxt_http_getter</code></td>
<td>
<p>function that executes HTTP request</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_on_server_error">on_server_error</code></td>
<td>
<p>request state handler for any 5xx status</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_on_client_error">on_client_error</code></td>
<td>
<p>request state handler for any 4xx HTTP status that is
not 404</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_on_not_found">on_not_found</code></td>
<td>
<p>request state handler for HTTP status 404</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_on_redirect">on_redirect</code></td>
<td>
<p>request state handler for any 3xx HTTP status</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_on_domain_change">on_domain_change</code></td>
<td>
<p>request state handler for any 3xx HTTP status where
domain did change as well</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_on_file_type_mismatch">on_file_type_mismatch</code></td>
<td>
<p>request state handler for content type other
than 'text/plain'</p>
</td></tr>
<tr><td><code id="get_robotstxts_+3A_on_suspect_content">on_suspect_content</code></td>
<td>
<p>request state handler for content that seems to be
something else than a robots.txt file (usually a JSON, XML or HTML)</p>
</td></tr>
</table>

<hr>
<h2 id='guess_domain'>function guessing domain from path</h2><span id='topic+guess_domain'></span>

<h3>Description</h3>

<p>function guessing domain from path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>guess_domain(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="guess_domain_+3A_x">x</code></td>
<td>
<p>path aka URL from which to infer domain</p>
</td></tr>
</table>

<hr>
<h2 id='http_domain_changed'>http_domain_changed</h2><span id='topic+http_domain_changed'></span>

<h3>Description</h3>

<p>http_domain_changed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>http_domain_changed(response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="http_domain_changed_+3A_response">response</code></td>
<td>
<p>an httr response object, e.g. from a call to httr::GET()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>logical of length 1 indicating whether or not any domain change
happened during the HTTP request
</p>

<hr>
<h2 id='http_subdomain_changed'>http_subdomain_changed</h2><span id='topic+http_subdomain_changed'></span>

<h3>Description</h3>

<p>http_subdomain_changed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>http_subdomain_changed(response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="http_subdomain_changed_+3A_response">response</code></td>
<td>
<p>an httr response object, e.g. from a call to httr::GET()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>logical of length 1 indicating whether or not any domain change
happened during the HTTP request
</p>

<hr>
<h2 id='http_was_redirected'>http_was_redirected</h2><span id='topic+http_was_redirected'></span>

<h3>Description</h3>

<p>http_was_redirected
</p>


<h3>Usage</h3>

<pre><code class='language-R'>http_was_redirected(response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="http_was_redirected_+3A_response">response</code></td>
<td>
<p>an httr response object, e.g. from a call to httr::GET()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>logical of length 1 indicating whether or not any redirect happened
during the HTTP request
</p>

<hr>
<h2 id='is_suspect_robotstxt'>is_suspect_robotstxt</h2><span id='topic+is_suspect_robotstxt'></span>

<h3>Description</h3>

<p>function that checks if file is valid / parsable robots.txt file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_suspect_robotstxt(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_suspect_robotstxt_+3A_text">text</code></td>
<td>
<p>content of a robots.txt file provides as character vector</p>
</td></tr>
</table>

<hr>
<h2 id='is_valid_robotstxt'>function that checks if file is valid / parsable robots.txt file</h2><span id='topic+is_valid_robotstxt'></span>

<h3>Description</h3>

<p>function that checks if file is valid / parsable robots.txt file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_valid_robotstxt(text, check_strickt_ascii = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_valid_robotstxt_+3A_text">text</code></td>
<td>
<p>content of a robots.txt file provides as character vector</p>
</td></tr>
<tr><td><code id="is_valid_robotstxt_+3A_check_strickt_ascii">check_strickt_ascii</code></td>
<td>
<p>whether or not to check if content does adhere to the specification of RFC to use plain text aka ASCII</p>
</td></tr>
</table>

<hr>
<h2 id='list_merge'>Merge a number of named lists in sequential order</h2><span id='topic+list_merge'></span>

<h3>Description</h3>

<p>Merge a number of named lists in sequential order
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_merge(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="list_merge_+3A_...">...</code></td>
<td>
<p>named lists</p>
</td></tr>
</table>


<h3>Details</h3>

<p>List merging is usually useful in the merging of program
settings or configuraion with multiple versions across time,
or multiple administrative levels. For example, a program
settings may have an initial version in which most keys are
defined and specified. In later versions, partial modifications
are recorded. In this case, list merging can be useful to merge
all versions of settings in release order of these versions. The
result is an fully updated settings with all later modifications
applied.
</p>


<h3>Author(s)</h3>

<p>Kun Ren &lt;mail@renkun.me&gt;
</p>
<p>The function merges a number of lists in sequential order
by <code>modifyList</code>, that is, the later list always
modifies the former list and form a merged list, and the
resulted list is again being merged with the next list.
The process is repeated until all lists in <code>...</code> or
<code>list</code> are exausted.
</p>

<hr>
<h2 id='named_list'>make automatically named list</h2><span id='topic+named_list'></span>

<h3>Description</h3>

<p>make automatically named list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>named_list(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="named_list_+3A_...">...</code></td>
<td>
<p>things to be put in list</p>
</td></tr>
</table>

<hr>
<h2 id='null_to_defeault'>null_to_defeault</h2><span id='topic+null_to_defeault'></span>

<h3>Description</h3>

<p>null_to_defeault
</p>


<h3>Usage</h3>

<pre><code class='language-R'>null_to_defeault(x, d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="null_to_defeault_+3A_x">x</code></td>
<td>
<p>value to check and return</p>
</td></tr>
<tr><td><code id="null_to_defeault_+3A_d">d</code></td>
<td>
<p>value to return in case x is NULL</p>
</td></tr>
</table>

<hr>
<h2 id='parse_robotstxt'>function parsing robots.txt</h2><span id='topic+parse_robotstxt'></span>

<h3>Description</h3>

<p>function parsing robots.txt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_robotstxt(txt)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parse_robotstxt_+3A_txt">txt</code></td>
<td>
<p>content of the robots.txt file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with useragents, comments, permissions, sitemap
</p>

<hr>
<h2 id='parse_url'>parse_url</h2><span id='topic+parse_url'></span>

<h3>Description</h3>

<p>parse_url
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_url(url)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parse_url_+3A_url">url</code></td>
<td>
<p>url to parse into its components</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with columns protocol, domain, path
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
url &lt;-
c(
  "google.com",
  "google.com/",
  "www.google.com",
  "http://google.com",
  "https://google.com",
  "sub.domain.whatever.de"
  "s-u-b.dom-ain.what-ever.de"
)

parse_url(url)

## End(Not run)

</code></pre>

<hr>
<h2 id='paths_allowed'>check if a bot has permissions to access page(s)</h2><span id='topic+paths_allowed'></span>

<h3>Description</h3>

<p>check if a bot has permissions to access page(s)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>paths_allowed(
  paths = "/",
  domain = "auto",
  bot = "*",
  user_agent = utils::sessionInfo()$R.version$version.string,
  check_method = c("spiderbar"),
  warn = getOption("robotstxt_warn", TRUE),
  force = FALSE,
  ssl_verifypeer = c(1, 0),
  use_futures = TRUE,
  robotstxt_list = NULL,
  verbose = FALSE,
  rt_request_handler = robotstxt::rt_request_handler,
  rt_robotstxt_http_getter = robotstxt::get_robotstxt_http_get,
  on_server_error = on_server_error_default,
  on_client_error = on_client_error_default,
  on_not_found = on_not_found_default,
  on_redirect = on_redirect_default,
  on_domain_change = on_domain_change_default,
  on_file_type_mismatch = on_file_type_mismatch_default,
  on_suspect_content = on_suspect_content_default
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="paths_allowed_+3A_paths">paths</code></td>
<td>
<p>paths for which to check bot's permission, defaults to &quot;/&quot;. Please, note that path to a folder should end with a trailing slash (&quot;/&quot;).</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_domain">domain</code></td>
<td>
<p>Domain for which paths should be checked. Defaults to &quot;auto&quot;.
If set to &quot;auto&quot; function will try to guess the domain by parsing the paths
argument. Note however, that these are educated guesses which might utterly
fail. To be on the safe side, provide appropriate domains manually.</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_bot">bot</code></td>
<td>
<p>name of the bot, defaults to &quot;*&quot;</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_user_agent">user_agent</code></td>
<td>
<p>HTTP user-agent string to be used to retrieve robots.txt
file from domain</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_check_method">check_method</code></td>
<td>
<p>at the moment only kept for backward compatibility reasons - do not use parameter anymore &ndash;&gt; will let the function simply use the default</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_warn">warn</code></td>
<td>
<p>suppress warnings</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_force">force</code></td>
<td>
<p>if TRUE instead of using possible cached results the function
will re-download the robotstxt file HTTP response status 404. If this
happens,</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_ssl_verifypeer">ssl_verifypeer</code></td>
<td>
<p>analog to CURL option
<a href="https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html">https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html</a> &ndash; and
might help with robots.txt file retrieval in some cases</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_use_futures">use_futures</code></td>
<td>
<p>Should future::future_lapply be used for possible
parallel/async retrieval or not. Note: check out help
pages and vignettes of package future on how to set up
plans for future execution because the robotstxt package
does not do it on its own.</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_robotstxt_list">robotstxt_list</code></td>
<td>
<p>either NULL &ndash; the default &ndash; or a list of character
vectors with one vector per path to check</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_verbose">verbose</code></td>
<td>
<p>make function print out more information</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_rt_request_handler">rt_request_handler</code></td>
<td>
<p>handler function that handles request according to
the event handlers specified</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_rt_robotstxt_http_getter">rt_robotstxt_http_getter</code></td>
<td>
<p>function that executes HTTP request</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_on_server_error">on_server_error</code></td>
<td>
<p>request state handler for any 5xx status</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_on_client_error">on_client_error</code></td>
<td>
<p>request state handler for any 4xx HTTP status that is
not 404</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_on_not_found">on_not_found</code></td>
<td>
<p>request state handler for HTTP status 404</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_on_redirect">on_redirect</code></td>
<td>
<p>request state handler for any 3xx HTTP status</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_on_domain_change">on_domain_change</code></td>
<td>
<p>request state handler for any 3xx HTTP status where
domain did change as well</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_on_file_type_mismatch">on_file_type_mismatch</code></td>
<td>
<p>request state handler for content type other
than 'text/plain'</p>
</td></tr>
<tr><td><code id="paths_allowed_+3A_on_suspect_content">on_suspect_content</code></td>
<td>
<p>request state handler for content that seems to be
something else than a robots.txt file (usually a JSON, XML or HTML)</p>
</td></tr>
</table>

<hr>
<h2 id='paths_allowed_worker_spiderbar'>paths_allowed_worker spiderbar flavor</h2><span id='topic+paths_allowed_worker_spiderbar'></span>

<h3>Description</h3>

<p>paths_allowed_worker spiderbar flavor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>paths_allowed_worker_spiderbar(domain, bot, paths, robotstxt_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="paths_allowed_worker_spiderbar_+3A_domain">domain</code></td>
<td>
<p>Domain for which paths should be checked. Defaults to &quot;auto&quot;.
If set to &quot;auto&quot; function will try to guess the domain by parsing the paths
argument. Note however, that these are educated guesses which might utterly
fail. To be on the safe side, provide appropriate domains manually.</p>
</td></tr>
<tr><td><code id="paths_allowed_worker_spiderbar_+3A_bot">bot</code></td>
<td>
<p>name of the bot, defaults to &quot;*&quot;</p>
</td></tr>
<tr><td><code id="paths_allowed_worker_spiderbar_+3A_paths">paths</code></td>
<td>
<p>paths for which to check bot's permission, defaults to &quot;/&quot;. Please, note that path to a folder should end with a trailing slash (&quot;/&quot;).</p>
</td></tr>
<tr><td><code id="paths_allowed_worker_spiderbar_+3A_robotstxt_list">robotstxt_list</code></td>
<td>
<p>either NULL &ndash; the default &ndash; or a list of character
vectors with one vector per path to check</p>
</td></tr>
</table>

<hr>
<h2 id='print.robotstxt'>printing robotstxt</h2><span id='topic+print.robotstxt'></span>

<h3>Description</h3>

<p>printing robotstxt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'robotstxt'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.robotstxt_+3A_x">x</code></td>
<td>
<p>robotstxt instance to be printed</p>
</td></tr>
<tr><td><code id="print.robotstxt_+3A_...">...</code></td>
<td>
<p>goes down the sink</p>
</td></tr>
</table>

<hr>
<h2 id='print.robotstxt_text'>printing robotstxt_text</h2><span id='topic+print.robotstxt_text'></span>

<h3>Description</h3>

<p>printing robotstxt_text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'robotstxt_text'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.robotstxt_text_+3A_x">x</code></td>
<td>
<p>character vector aka robotstxt$text to be printed</p>
</td></tr>
<tr><td><code id="print.robotstxt_text_+3A_...">...</code></td>
<td>
<p>goes down the sink</p>
</td></tr>
</table>

<hr>
<h2 id='remove_domain'>function to remove domain from path</h2><span id='topic+remove_domain'></span>

<h3>Description</h3>

<p>function to remove domain from path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_domain(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove_domain_+3A_x">x</code></td>
<td>
<p>path aka URL from which to first infer domain and then remove it</p>
</td></tr>
</table>

<hr>
<h2 id='request_handler_handler'>request_handler_handler</h2><span id='topic+request_handler_handler'></span>

<h3>Description</h3>

<p>Helper function to handle robotstxt handlers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>request_handler_handler(request, handler, res, info = TRUE, warn = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="request_handler_handler_+3A_request">request</code></td>
<td>
<p>the request object returned by call to httr::GET()</p>
</td></tr>
<tr><td><code id="request_handler_handler_+3A_handler">handler</code></td>
<td>
<p>the handler either a character string entailing various options or a function producing a specific list, see return.</p>
</td></tr>
<tr><td><code id="request_handler_handler_+3A_res">res</code></td>
<td>
<p>a list a list with elements '[handler names], ...', 'rtxt', and 'cache'</p>
</td></tr>
<tr><td><code id="request_handler_handler_+3A_info">info</code></td>
<td>
<p>info to add to problems list</p>
</td></tr>
<tr><td><code id="request_handler_handler_+3A_warn">warn</code></td>
<td>
<p>if FALSE warnings and messages are suppressed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with elements '[handler name]', 'rtxt', and 'cache'
</p>

<hr>
<h2 id='robotstxt'>Generate a representations of a robots.txt file</h2><span id='topic+robotstxt'></span>

<h3>Description</h3>

<p>The function generates a list that entails data resulting from parsing a robots.txt file
as well as a function called check that enables to ask the representation if bot (or
particular bots) are allowed to access a resource on the domain.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>robotstxt(
  domain = NULL,
  text = NULL,
  user_agent = NULL,
  warn = getOption("robotstxt_warn", TRUE),
  force = FALSE,
  ssl_verifypeer = c(1, 0),
  encoding = "UTF-8",
  verbose = FALSE,
  on_server_error = on_server_error_default,
  on_client_error = on_client_error_default,
  on_not_found = on_not_found_default,
  on_redirect = on_redirect_default,
  on_domain_change = on_domain_change_default,
  on_file_type_mismatch = on_file_type_mismatch_default,
  on_suspect_content = on_suspect_content_default
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="robotstxt_+3A_domain">domain</code></td>
<td>
<p>Domain for which to generate a representation. If text equals to NULL,
the function will download the file from server - the default.</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_text">text</code></td>
<td>
<p>If automatic download of the robots.txt is not preferred, the text can be
supplied directly.</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_user_agent">user_agent</code></td>
<td>
<p>HTTP user-agent string to be used to retrieve robots.txt
file from domain</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_warn">warn</code></td>
<td>
<p>warn about being unable to download domain/robots.txt because of</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_force">force</code></td>
<td>
<p>if TRUE instead of using possible cached results the function
will re-download the robotstxt file HTTP response status 404. If this
happens,</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_ssl_verifypeer">ssl_verifypeer</code></td>
<td>
<p>analog to CURL option
<a href="https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html">https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html</a> &ndash; and
might help with robots.txt file retrieval in some cases</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_encoding">encoding</code></td>
<td>
<p>Encoding of the robots.txt file.</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_verbose">verbose</code></td>
<td>
<p>make function print out more information</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_on_server_error">on_server_error</code></td>
<td>
<p>request state handler for any 5xx status</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_on_client_error">on_client_error</code></td>
<td>
<p>request state handler for any 4xx HTTP status that is
not 404</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_on_not_found">on_not_found</code></td>
<td>
<p>request state handler for HTTP status 404</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_on_redirect">on_redirect</code></td>
<td>
<p>request state handler for any 3xx HTTP status</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_on_domain_change">on_domain_change</code></td>
<td>
<p>request state handler for any 3xx HTTP status where
domain did change as well</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_on_file_type_mismatch">on_file_type_mismatch</code></td>
<td>
<p>request state handler for content type other
than 'text/plain'</p>
</td></tr>
<tr><td><code id="robotstxt_+3A_on_suspect_content">on_suspect_content</code></td>
<td>
<p>request state handler for content that seems to be
something else than a robots.txt file (usually a JSON, XML or HTML)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object (list) of class robotstxt with parsed data from a
robots.txt (domain, text, bots, permissions, host, sitemap, other) and one
function to (check()) to check resource permissions.
</p>


<h3>Fields</h3>


<dl>
<dt><code>domain</code></dt><dd><p>character vector holding domain name for which the robots.txt
file is valid; will be set to NA if not supplied on initialization</p>
</dd>
<dt><code>text</code></dt><dd><p>character vector of text of robots.txt file; either supplied on
initialization or automatically downloaded from domain supplied on
initialization</p>
</dd>
<dt><code>bots</code></dt><dd><p>character vector of bot names mentioned in robots.txt</p>
</dd>
<dt><code>permissions</code></dt><dd><p>data.frame of bot permissions found in robots.txt file</p>
</dd>
<dt><code>host</code></dt><dd><p>data.frame of host fields found in robots.txt file</p>
</dd>
<dt><code>sitemap</code></dt><dd><p>data.frame of sitemap fields found in robots.txt file</p>
</dd>
<dt><code>other</code></dt><dd><p>data.frame of other - none of the above - fields found in
robots.txt file</p>
</dd>
<dt><code>check()</code></dt><dd><p>Method to check for bot permissions. Defaults to the
domains root and no bot in particular. check() has two arguments:
paths and bot. The first is for supplying the paths for which to check
permissions and the latter to put in the name of the bot.
Please, note that path to a folder should end with a trailing slash (&quot;/&quot;).</p>
</dd>
</dl>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
rt &lt;- robotstxt(domain="google.com")
rt$bots
rt$permissions
rt$check( paths = c("/", "forbidden"), bot="*")

## End(Not run)

</code></pre>

<hr>
<h2 id='rt_cache'>get_robotstxt() cache</h2><span id='topic+rt_cache'></span>

<h3>Description</h3>

<p>get_robotstxt() cache
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rt_cache
</code></pre>


<h3>Format</h3>

<p>An object of class <code>environment</code> of length 0.
</p>

<hr>
<h2 id='rt_get_comments'>extracting comments from robots.txt</h2><span id='topic+rt_get_comments'></span>

<h3>Description</h3>

<p>extracting comments from robots.txt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rt_get_comments(txt)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rt_get_comments_+3A_txt">txt</code></td>
<td>
<p>content of the robots.txt file</p>
</td></tr>
</table>

<hr>
<h2 id='rt_get_fields'>extracting permissions from robots.txt</h2><span id='topic+rt_get_fields'></span>

<h3>Description</h3>

<p>extracting permissions from robots.txt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rt_get_fields(txt, regex = "", invert = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rt_get_fields_+3A_txt">txt</code></td>
<td>
<p>content of the robots.txt file</p>
</td></tr>
<tr><td><code id="rt_get_fields_+3A_regex">regex</code></td>
<td>
<p>regular expression specify field</p>
</td></tr>
<tr><td><code id="rt_get_fields_+3A_invert">invert</code></td>
<td>
<p>invert selection made via regex?</p>
</td></tr>
</table>

<hr>
<h2 id='rt_get_fields_worker'>extracting robotstxt fields</h2><span id='topic+rt_get_fields_worker'></span>

<h3>Description</h3>

<p>extracting robotstxt fields
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rt_get_fields_worker(txt, type = "all", regex = NULL, invert = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rt_get_fields_worker_+3A_txt">txt</code></td>
<td>
<p>content of the robots.txt file</p>
</td></tr>
<tr><td><code id="rt_get_fields_worker_+3A_type">type</code></td>
<td>
<p>name or names of the fields to be returned, defaults to all
fields</p>
</td></tr>
<tr><td><code id="rt_get_fields_worker_+3A_regex">regex</code></td>
<td>
<p>subsetting field names via regular expressions</p>
</td></tr>
<tr><td><code id="rt_get_fields_worker_+3A_invert">invert</code></td>
<td>
<p>field selection</p>
</td></tr>
</table>

<hr>
<h2 id='rt_get_rtxt'>load robots.txt files saved along with the package</h2><span id='topic+rt_get_rtxt'></span>

<h3>Description</h3>

<p>load robots.txt files saved along with the package:
these functions are very handy for testing (not used otherwise)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rt_get_rtxt(name = sample(rt_list_rtxt(), 1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rt_get_rtxt_+3A_name">name</code></td>
<td>
<p>name of the robots.txt files, defaults to a random drawn file ;-)</p>
</td></tr>
</table>

<hr>
<h2 id='rt_get_useragent'>extracting HTTP useragents from robots.txt</h2><span id='topic+rt_get_useragent'></span>

<h3>Description</h3>

<p>extracting HTTP useragents from robots.txt
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rt_get_useragent(txt)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rt_get_useragent_+3A_txt">txt</code></td>
<td>
<p>content of the robots.txt file</p>
</td></tr>
</table>

<hr>
<h2 id='rt_last_http'>storage for http request response objects</h2><span id='topic+rt_last_http'></span><span id='topic+get_robotstxt_http_get'></span>

<h3>Description</h3>

<p>storage for http request response objects
</p>
<p>get_robotstxt() worker function to execute HTTP request
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rt_last_http

get_robotstxt_http_get(
  domain,
  user_agent = utils::sessionInfo()$R.version$version.string,
  ssl_verifypeer = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rt_last_http_+3A_domain">domain</code></td>
<td>
<p>the domain to get tobots.txt. file for</p>
</td></tr>
<tr><td><code id="rt_last_http_+3A_user_agent">user_agent</code></td>
<td>
<p>the user agent to use for HTTP request header</p>
</td></tr>
<tr><td><code id="rt_last_http_+3A_ssl_verifypeer">ssl_verifypeer</code></td>
<td>
<p>analog to CURL option
<a href="https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html">https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html</a>
&ndash; and might help with robots.txt file retrieval in some cases</p>
</td></tr>
</table>


<h3>Format</h3>

<p>An object of class <code>environment</code> of length 1.
</p>

<hr>
<h2 id='rt_list_rtxt'>list robots.txt files saved along with the package</h2><span id='topic+rt_list_rtxt'></span>

<h3>Description</h3>

<p>list robots.txt files saved along with the package:
these functions ar very handy for testing (not used otherwise)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rt_list_rtxt()
</code></pre>

<hr>
<h2 id='rt_request_handler'>rt_request_handler</h2><span id='topic+rt_request_handler'></span><span id='topic+on_server_error_default'></span><span id='topic+on_client_error_default'></span><span id='topic+on_not_found_default'></span><span id='topic+on_redirect_default'></span><span id='topic+on_domain_change_default'></span><span id='topic+on_sub_domain_change_default'></span><span id='topic+on_file_type_mismatch_default'></span><span id='topic+on_suspect_content_default'></span>

<h3>Description</h3>

<p>A helper function for get_robotstxt() that will extract the robots.txt file
from the HTTP request result object. furthermore it will inform
get_robotstxt() if the request should be cached and which problems occured.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rt_request_handler(
  request,
  on_server_error = on_server_error_default,
  on_client_error = on_client_error_default,
  on_not_found = on_not_found_default,
  on_redirect = on_redirect_default,
  on_domain_change = on_domain_change_default,
  on_sub_domain_change = on_sub_domain_change_default,
  on_file_type_mismatch = on_file_type_mismatch_default,
  on_suspect_content = on_suspect_content_default,
  warn = TRUE,
  encoding = "UTF-8"
)

on_server_error_default

on_client_error_default

on_not_found_default

on_redirect_default

on_domain_change_default

on_sub_domain_change_default

on_file_type_mismatch_default

on_suspect_content_default
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rt_request_handler_+3A_request">request</code></td>
<td>
<p>result of an HTTP request (e.g. httr::GET())</p>
</td></tr>
<tr><td><code id="rt_request_handler_+3A_on_server_error">on_server_error</code></td>
<td>
<p>request state handler for any 5xx status</p>
</td></tr>
<tr><td><code id="rt_request_handler_+3A_on_client_error">on_client_error</code></td>
<td>
<p>request state handler for any 4xx HTTP status that is
not 404</p>
</td></tr>
<tr><td><code id="rt_request_handler_+3A_on_not_found">on_not_found</code></td>
<td>
<p>request state handler for HTTP status 404</p>
</td></tr>
<tr><td><code id="rt_request_handler_+3A_on_redirect">on_redirect</code></td>
<td>
<p>request state handler for any 3xx HTTP status</p>
</td></tr>
<tr><td><code id="rt_request_handler_+3A_on_domain_change">on_domain_change</code></td>
<td>
<p>request state handler for any 3xx HTTP status where
domain did change as well</p>
</td></tr>
<tr><td><code id="rt_request_handler_+3A_on_sub_domain_change">on_sub_domain_change</code></td>
<td>
<p>request state handler for any 3xx HTTP status where
domain did change but only to www-sub_domain</p>
</td></tr>
<tr><td><code id="rt_request_handler_+3A_on_file_type_mismatch">on_file_type_mismatch</code></td>
<td>
<p>request state handler for content type other
than 'text/plain'</p>
</td></tr>
<tr><td><code id="rt_request_handler_+3A_on_suspect_content">on_suspect_content</code></td>
<td>
<p>request state handler for content that seems to be
something else than a robots.txt file (usually a JSON, XML or HTML)</p>
</td></tr>
<tr><td><code id="rt_request_handler_+3A_warn">warn</code></td>
<td>
<p>suppress warnings</p>
</td></tr>
<tr><td><code id="rt_request_handler_+3A_encoding">encoding</code></td>
<td>
<p>The text encoding to assume if no encoding is provided in the
headers of the response</p>
</td></tr>
</table>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 4.
</p>
<p>An object of class <code>list</code> of length 4.
</p>
<p>An object of class <code>list</code> of length 4.
</p>
<p>An object of class <code>list</code> of length 2.
</p>
<p>An object of class <code>list</code> of length 3.
</p>
<p>An object of class <code>list</code> of length 2.
</p>
<p>An object of class <code>list</code> of length 4.
</p>
<p>An object of class <code>list</code> of length 4.
</p>


<h3>Value</h3>

<p>a list with three items following the following schema: <br /> <code>
  list( rtxt = "", problems = list( "redirect" = list( status_code = 301 ),
  "domain" = list(from_url = "...", to_url = "...") ) ) </code>
</p>

<hr>
<h2 id='sanitize_path'>making paths uniform</h2><span id='topic+sanitize_path'></span>

<h3>Description</h3>

<p>making paths uniform
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sanitize_path(path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sanitize_path_+3A_path">path</code></td>
<td>
<p>path to be sanitized</p>
</td></tr>
</table>


<h3>Value</h3>

<p>sanitized path
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
