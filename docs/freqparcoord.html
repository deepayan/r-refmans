<!DOCTYPE html><html><head><title>Help for package freqparcoord</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {freqparcoord}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#freqparcoord'>
<p>Frequency-based parallel coordinates.</p></a></li>
<li><a href='#mlb'>
<p>Major Leage Baseball player data set.</p></a></li>
<li><a href='#newadult'>
<p>UCI adult income data set, adapted</p></a></li>
<li><a href='#oliveoils'>
<p>Italian olive oils data set.</p></a></li>
<li><a href='#posjitter'>
<p>Add positive jitter.</p></a></li>
<li><a href='#prgeng'>
<p>Silicon Valley programmers and engineers</p></a></li>
<li><a href='#regdiag'>
<p>Diagnosing regression model fit using parallel coordinates.</p></a></li>
<li><a href='#rmixmvnorm'>
<p>Random vectors from mixtures of multivariate normal</p>
distributions.</a></li>
<li><a href='#smoothz'>
<p>Smoothing functions.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Norm Matloff &lt;normmatloff@gmail.com&gt; and Yingkang Xie 
        &lt;yingkang.xie@gmail.com&gt; </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Norm Matloff &lt;normmatloff@gmail.com&gt;</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-01-15</td>
</tr>
<tr>
<td>Title:</td>
<td>Novel Methods for Parallel Coordinates</td>
</tr>
<tr>
<td>Description:</td>
<td>New approaches to parallel coordinates plots for
   multivariate data visualization, including applications to clustering,
   outlier hunting and regression diagnostics.  Includes general functions
   for multivariate nonparametric density and regression estimation, 
   using parallel computation.  </td>
</tr>
<tr>
<td>Depends:</td>
<td>parallel, ggplot2, GGally, FNN, mvtnorm</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mgcv</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>no</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-01-17 04:19:08 UTC; nm</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-01-17 10:59:33</td>
</tr>
</table>
<hr>
<h2 id='freqparcoord'>
Frequency-based parallel coordinates.
</h2><span id='topic+freqparcoord'></span>

<h3>Description</h3>

<p>A novel approach to the parallel coordinates method for visualizing many
variables at once.
</p>
<p>(a) Addresses the screen-clutter problem in parallel coordinates, by only
plotting the &quot;most typical&quot; cases, meaning those with the highest estimated
multivariate density values.  This makes it easier to discern relations
between variables, especially those whose axes are &quot;distant&quot; from each
other.  
</p>
<p>(b) One can also plot the &quot;least typical&quot; cases, i.e. those with the
lowest density values, in order to find outliers.  
</p>
<p>(c) One can plot only cases that are &quot;local maxima&quot; in terms of density,
as a means of performing clustering.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freqparcoord(x,m,dispcols=1:ncol(x),grpvar=NULL,
      method="maxdens",faceting="vert",k=50,klm=5*k,
            keepidxs=NULL,plotidxs=FALSE,cls=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="freqparcoord_+3A_x">x</code></td>
<td>
<p>The data, in data frame or matrix form.  If there are
indicator </p>
</td></tr>
<tr><td><code id="freqparcoord_+3A_m">m</code></td>
<td>
<p>Number of lines to plot for each group.  A negative value in
conjunction with <code>method</code> being &quot;maxdens&quot; indicates that the
lowest-density lines are to be plotted.  If <code>method</code> is
&quot;locmax&quot;, <code>m</code> is forced to 1.</p>
</td></tr>
<tr><td><code id="freqparcoord_+3A_dispcols">dispcols</code></td>
<td>
<p>Numbers of the columns of <code>x</code> to be displayed.</p>
</td></tr>
<tr><td><code id="freqparcoord_+3A_grpvar">grpvar</code></td>
<td>
<p>Column number for the grouping variable, if any (if none, 
all the data is treated as a single group); vector or factor.  Must
not be in <code>dispcols</code>.  If
<code>method</code> is &quot;locmax&quot;, <code>grpvar</code> is forced to NULL</p>
</td></tr>
<tr><td><code id="freqparcoord_+3A_method">method</code></td>
<td>
<p>What to display:  &quot;maxdens&quot; for plotting the most
(or least) typical lines, &quot;locmax&quot; for cluster hunting, or 
&quot;randsamp&quot; for plotting a random sample of lines.</p>
</td></tr>
<tr><td><code id="freqparcoord_+3A_faceting">faceting</code></td>
<td>
<p>How to display groups, if present.  Use &quot;vert&quot; for
vertical stacking of group plots, &quot;horiz&quot; for horizontal ones, or
&quot;none&quot; to draw all lines in one plot, color-coding by group.</p>
</td></tr>
<tr><td><code id="freqparcoord_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to use for density estimation.</p>
</td></tr>
<tr><td><code id="freqparcoord_+3A_klm">klm</code></td>
<td>
<p>If method is &quot;locmax&quot;, number of nearest neighbors to 
use for finding local maxima for cluster hunting.  Generally needs
to be much larger than <code>k</code>, to avoid &quot;noise fitting.&quot;</p>
</td></tr>
<tr><td><code id="freqparcoord_+3A_keepidxs">keepidxs</code></td>
<td>
<p>If not NULL, the indices of the rows of <code>x</code> that 
are plotted will be stored in a component <code>idxs</code> of the
return value.  The rows themselves will be in a component
<code>xdisp</code>, ordered by <code>x[,dispcols[1]</code>.]</p>
</td></tr>
<tr><td><code id="freqparcoord_+3A_plotidxs">plotidxs</code></td>
<td>
<p>If TRUE, lines in the display will be annotated 
with their case numbers, i.e. their row numbers within <code>x</code>.  
Use only with small values of <code>m</code>, as overplotting may occur.</p>
</td></tr>
<tr><td><code id="freqparcoord_+3A_cls">cls</code></td>
<td>
<p>Cluster, if any (see the <code>parallel</code> package) for
parallel computation.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>In general, a parallel coordinates plot draws each data point as a
polygonal line.  Say for example we have variables Height, Weight and
Age (inches, pounds, years).  The vertical axes are drawn, one for each
variable.  Then each point, &quot;connects the dots&quot; on the vertical axes.
For instance, the point (70, 160, 28) would be represented as a
segmented line connecting 70 on the Height axis, 160 on the Weight axis
and 28 on the Age axis.  See for example <code>parcoord</code> in the
<span class="pkg">MASS</span> package.
</p>
<p>The problem with the parallel coordinates method is screen clutter&ndash;too
many lines filling the screen. The treatment here avoids this problem by
plotting only the lines having the highest estimated multivariate
density (or variants discussed below).
</p>
<p>If <code>method</code> = &quot;maxdens&quot;, the <code>m</code> most frequent (<code>m</code>
positive) or least frequent (<code>m</code> negative) rows of <code>x</code> will be
plotted from each group, where frequency is measured by density value
(the nongroup case being considered one group).   
</p>
<p>If  <code>method = "locmax"</code>, the rows having the property that their
density value is highest in their <code>klm</code>-neighborhood will be plotted.  
</p>
<p>Otherwise, <code>m</code> random rows will be displayed.  
</p>
<p>The lines will be color-coded according to density value.  Density
values are computed separately within groups.
</p>
<p>If <code>cls</code> is non-null, the computation will be done in parallel.
See <a href="#topic+knndens">knndens</a>.
</p>
<p>The data is centered and scaled using <code>scale</code> before analysis,
including before any grouping operations.  Thus the selected rows are
still plotted on the scale of the entire data set; for instance, a
vertical axis value of 0 corresponds to the mean of the given variable.
If some variable is constant, scaling is impossible, and an error
message, &quot;arguments imply differing number of rows: 0, 1,&quot; will appear.
In such case, try a larger value of <code>m</code>. 
</p>
<p>Density estimation is done through the k-Nearest Neighbor method, in the
function <code>smoothz</code>.  (Due to use above-mentioned use of
<code>scale</code>, this is meaningful even if some variables are of the
indicator/dummy type, i.e. 1-0 valued to indicate the presence or
absence of some trait.  This way such variables are comparable to the
continuous ones in the distance compuations.) For any point, the k
nearest data points are found, requiring powers of distances in a
denominator.  With large, discrete data, the denominator may be 0.  In
such cases, it is recommended that you apply <code>jitter</code> or (from
this package) <code>posjitter</code>.  The same visual patterns will emerge. 
</p>
<p>As with any exploratory tool, the user should experiment with the values
of the arguments, especially the <code>klm</code> argument with the method
&quot;locmax&quot;.  
</p>
<p>Note that with long-tailed distributions, the scaled data will be
disproportionately negative.  Thus the magnitude of the scaled variables
should be viewed relative to each other, rather than to 0.
</p>
<p>If you use too large a value for <code>k</code>, it may be larger than some
group size, generating an error message like &quot;k should be less than
sample size.&quot;  If so, try a smaller <code>k</code>.  If a plot would contain
only one line, this may cause a problem with some graphics systems.
</p>


<h3>Value</h3>

<p>Object of type &quot;gg&quot; (<span class="pkg">ggplot2</span> object), with components <code>idxs</code>
and <code>xdisp</code> added if <code>keepidxs</code> is not NULL (see argument
<code>keepidxs</code> above).
</p>


<h3>Author(s)</h3>

<p>Norm Matloff &lt;matloff@cs.ucdavis.edu&gt; and Yingkang Xie
&lt;yingkang.xie@gmail.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># baseball player data courtesy of UCLA Stat. Dept., www.socr.ucla.edu
data(mlb)

# plot baseball data, broken down by position category (infield,
# outfield, etc.); plot the 5 higest-density values in each group
freqparcoord(mlb,5,4:6,7,method="maxdens")
# we see that the most typical pitchers are tall and young, while the
# catchers are short and heavy

# same, but no grouping
freqparcoord(mlb,5,4:6,method="maxdens")

# find the outliers, 1 for each position 
freqparcoord(mlb,-1,4:6,7)
# for instance we see an infielder of average height and weight, but
# extremely high age, worth looking into

# do the same, but also plot and retain the indices of the rows being
# plotted, and the rows themselves
p &lt;- freqparcoord(mlb,-1,4:6,7,keepidxs=4,plotidxs=TRUE)
p
p$idxs
p$xdisp
# ah, that outlier infielder was case number 674,
# Julio Franco, 48 years old!

# olive oil data courtesy of Dr. Martin Theus
data(oliveoils)
olv &lt;- oliveoils

# there are 9 olive-oil producing areas of Italy, named Area here
# check whether the area groups have distinct patterns (yes)
freqparcoord(olv,1,3:10,1,k=15)

# same check but looking at within-group variation (turns out that some
# variables are more diverse in some areas than others)
freqparcoord(olv,25,3:10,1,k=15)
# yes, definitely, e.g. wide variation in stearic in Sicily

# look at it without stacking the groups
freqparcoord(olv,25,3:10,1,faceting="none",k=15)
# prettier this way, with some patterns just as discernible

## Not run: 
# programmers and engineers in Silicon Valley, 2000 census
data(prgeng)
pg &lt;- prgeng

# compare men and women
freqparcoord(pg,10,dispcols=c(1,3,8),grpvar=7,faceting="horiz")
# men seem to fall into 2 subgroups, one with very low wages; let's get 
# a printout of the plotted points, grouped by gender
p &lt;-
   freqparcoord(pg,10,dispcols=c(1,3,8),grpvar=7,faceting="horiz",keepidxs=7);
p$xdisp
# ah, there are some wages like $3000; delete those and look again;
pg1 &lt;- pg[pg$wageinc &gt;= 40000 &amp; pg$wkswrkd &gt;= 48,]
freqparcoord(pg1,50,dispcols=c(1,3,8),grpvar=7,faceting="horiz",keepidxs=7)
# the women seem to fall in 2 age groups, but not the men, worth further 
# analysis 
# note that all have the same education, a bachelor's degree, the 
# most frequent level

# generate some simulated data with clusters at (0,0), (1,2) and (3,3),
# and see whether "locmax" (clustering) picks them up
cv &lt;- 0.5*diag(2)
x &lt;- rmixmvnorm(10000,2,3,list(c(0,0),c(1,2),c(3,3)),list(cv,cv,cv))
p &lt;- freqparcoord(x,m=1,method="locmax",keepidxs=1,k=50,klm=800)
p$xdisp  # worked well in this case, centers near (0,0), (1,2), (3,3)

# see how well outlier detection works
x &lt;- rmixmvnorm(10000,2,3,list(c(0,0),c(1,2),c(8,8)),list(cv,cv,cv),
   wts=c(0.49,0.49,0.02))
# most of the outliers should be out toward (8,8)
p &lt;- freqparcoord(x,m=-10,keepidxs=1)
p$xdisp

## End(Not run)

</code></pre>

<hr>
<h2 id='mlb'>
Major Leage Baseball player data set.
</h2><span id='topic+mlb'></span>

<h3>Description</h3>

<p>Heights, weights, ages etc. of major league baseball players.  A new
variable has been added, consolidating positions into Infielders,
Outfielders, Catchers and Pitchers.
</p>
<p>Included here with the permission of the UCLA Statistics Department.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mlb); mlb
</code></pre>

<hr>
<h2 id='newadult'>
UCI adult income data set, adapted
</h2><span id='topic+newadult'></span>

<h3>Description</h3>

<p>This data set is adapted from
the Adult data from the UCI Machine Learning Repository,
which was in turn adapted from Census data on adult incomes and other 
demographic variables.  The UCI data is used here with permission 
from Ronny Kohavi.
</p>
<p>The variables are:
</p>

<ul>
<li> <p><code>gt50</code>, which converts the original <code>&gt;50K</code> variable
to an indicator variable; 1 for income greater than $50,000, else 0
</p>
</li>
<li> <p><code>edu</code>, which converts a set of education levels to
approximate number of years of schooling
</p>
</li>
<li> <p><code>age</code>
</p>
</li>
<li> <p><code>gender</code>, 1 for male, 0 for female
</p>
</li>
<li> <p><code>mar</code>, 1 for married, 0 for single
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(newadult); newadult
</code></pre>

<hr>
<h2 id='oliveoils'>
Italian olive oils data set.
</h2><span id='topic+oliveoils'></span>

<h3>Description</h3>

<p>Italian olive oils data set, as used in <em>Graphics of Large 
Datasets: Visualizing a  Million</em>, by  Antony Unwin, Martin Theus and 
Heike Hofmann, Springer, 2006.  Included here with permission of Dr. 
Martin Theus.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(oliveoils); oliveoils
</code></pre>

<hr>
<h2 id='posjitter'>
Add positive jitter.
</h2><span id='topic+posjitter'></span>

<h3>Description</h3>

<p>Similar to <a href="base.html#topic+jitter">jitter</a>, but only generating values in (0,1).  A
typical example of use is for an age variable, which in many data sets
is truncated to the lowest integer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posjitter(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="posjitter_+3A_x">x</code></td>
<td>
<p>Vector to which jitter is to be added.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vector <code>x + runif(length(x))</code>.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff &lt;matloff@cs.ucdavis.edu&gt; 
</p>

<hr>
<h2 id='prgeng'>
Silicon Valley programmers and engineers
</h2><span id='topic+prgeng'></span>

<h3>Description</h3>

<p>This data set is adapted from the 2000 Census (5% sample, person
records).  It is restricted to programmers and engineers in the 
Silicon Valley area.
</p>
<p>The variable codes, e.g. occupational codes, are available from the Census
Bureau, at
<a href="http://www.census.gov/prod/cen2000/doc/pums.pdf">http://www.census.gov/prod/cen2000/doc/pums.pdf</a>.
(Short code lists are given in the record layout, but longer ones are in
the appendix Code Lists.)
</p>
<p>The variables are:
</p>

<ul>
<li><p><code>age</code>, with a U(0,1) variate added for jitter
</p>
</li>
<li><p><code>cit</code>, citizenship; 1-4 code various categories of
citizens; 5 means noncitizen (including permanent residents
</p>
</li>
<li><p><code>educ</code>: 01-09 code no college; 10-12 means some college;
13 is a bachelor's degree, 14 a master's, 15 a professiona deal and
16 is a doctorate
</p>
</li>
<li><p><code>engl</code>, English proficiency
</p>
</li>
<li><p><code>occ</code>, occupation
</p>
</li>
<li><p><code>birth</code>, place of birth
</p>
</li>
<li><p><code>wageinc</code>, wage income
</p>
</li>
<li><p><code>wkswrkd</code>, number of weeks worked
</p>
</li>
<li><p><code>yrentry</code>, year of entry to the U.S. (0 for natives)
</p>
</li>
<li><p><code>powpuma</code>, location of work 
</p>
</li>
<li><p><code>gender</code>, 1 for male, 2 for female
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(prgeng); prgeng
</code></pre>

<hr>
<h2 id='regdiag'>
Diagnosing regression model fit using parallel coordinates.
</h2><span id='topic+regdiag'></span><span id='topic+regdiagbas'></span>

<h3>Description</h3>

<p>Performs parametric regression model fit diagnostics, based on
<a href="#topic+freqparcoord">freqparcoord</a>.  One axis is the &quot;divergences,&quot; the differences
beween the parametric and nonparametric estimates of the population
regression function, while the other axes are the predictor variables.
Note that the divergences are NOT the parametric model residuals, e.g.
differences between fitted model values and response (&quot;Y&quot;) values.  
</p>
<p>The question addressed is, &quot;In what regions of predictor space is the
parametric fit poorer?&quot;  To answer that, the divergences are
grouped into upper and lower tails; e.g. if <code>tail</code> is set to 0.10,
we find the data points that have divergences in the lower and upper
10%, then plot both groups, as well as the middle.
</p>
<p>The parallel coordinates plot then can be used to identify regions in
which the parametric model tends to either under- or overpredict the
response, thus indicating possible addition of interaction or
polynomial terms.
</p>
<p>Furthermore, in the case for <code>regdiag</code> in which an <code>lm</code> object
is input, the adjusted R-squared value for the parametric model and the
R-squared value from the nonparametric fit are computed.  If the
nonparametric value is substantially larger than the parametric one,
this is an indication of some deficiency in the parametric model, thus
providing some quantitative information on whether inclusion of 
interaction and/or polynomial terms may be useful. 
</p>
<p>The term <em>regression</em> is used in the sense of condtional mean
response given the predictors.  Thus parametric classification models
such as the logistic may also be used, with the regression function
being the condtional probability of response = 1, given the predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regdiag(regout, tail=0.10, k=NULL, m=5,
      checkna = TRUE, cls = NULL, nchunks = length(cls))
regdiagbas(preds, resp, parest, tail=0.10, k=NULL, m=5,
      checkna = TRUE, cls = NULL, nchunks = length(cls))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regdiag_+3A_regout">regout</code></td>
<td>
<p>Output of <code>lm</code> or <code>glm</code></p>
</td></tr>
<tr><td><code id="regdiag_+3A_preds">preds</code></td>
<td>
<p>Matrix of predictor values.</p>
</td></tr>
<tr><td><code id="regdiag_+3A_resp">resp</code></td>
<td>
<p>Vector of response values.</p>
</td></tr>
<tr><td><code id="regdiag_+3A_parest">parest</code></td>
<td>
<p>Parametric model estimates of the population
regression function at the predictor data points.</p>
</td></tr>
<tr><td><code id="regdiag_+3A_tail">tail</code></td>
<td>
<p>Proportion of most negative and most positive divergences
to use in grouping.</p>
</td></tr>
<tr><td><code id="regdiag_+3A_k">k</code></td>
<td>
<p>See <a href="#topic+freqparcoord">freqparcoord</a>.</p>
</td></tr>
<tr><td><code id="regdiag_+3A_m">m</code></td>
<td>
<p>See <a href="#topic+freqparcoord">freqparcoord</a>.</p>
</td></tr>
<tr><td><code id="regdiag_+3A_checkna">checkna</code></td>
<td>
<p>See <a href="#topic+freqparcoord">freqparcoord</a>.</p>
</td></tr>
<tr><td><code id="regdiag_+3A_cls">cls</code></td>
<td>
<p>See <a href="#topic+freqparcoord">freqparcoord</a>.</p>
</td></tr>
<tr><td><code id="regdiag_+3A_nchunks">nchunks</code></td>
<td>
<p>See <a href="#topic+freqparcoord">freqparcoord</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>The population regression function (including the case of a probability
function in a classification problem) is estimated nonparametrically at the
observation points, using <a href="#topic+knnreg">knnreg</a>.
</p>
<p>The nonparametric estimates are subtracted from the parametric ones,
yielding the divergences.  A frequency-parallel coordinates plot is
displayed as described above. 
</p>
<p>The R-squared values are available in the situation noted earlier.  The
nonparametric R-squared value is calculated as the squared correlation
between estimated regression value and the response value.
</p>
<p>It is possible that in one of the tail groups the response value is
constant, in which case an error message appears.  If so, try a larger
value of <code>tail</code>.
</p>


<h3>Value</h3>

<p>An object of type &quot;gg&quot; (a <span class="pkg">ggplot2</span> object, displays when printed), 
with new components added:
</p>

<ul>
<li><p>The nonparametric regression estimates, in <code>nonparest</code>.
</p>
</li>
<li><p>In the case of a linear model specified via <code>regout</code>, the
adjusted R-squared value for the parametric model, in <code>paramr2</code>,
and <code>nonparamr2</code>, the R-squared value from the nonparametric 
fit.  The latter is the squared correlation between predicted and 
actual response values 
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Norm Matloff &lt;matloff@cs.ucdavis.edu&gt; and Yingkang Xie
&lt;yingkang.xie@gmail.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mlb)

lmout &lt;- lm(mlb$Weight ~ mlb$Height + mlb$Age)
p &lt;- regdiag(lmout,0.10,k=50,m=25)
p
# taller, older players are overpredicted, with shorter, younger players
# underpredicted; suggests that adding quadratic terms for Height, Age
# may help in the tails
# let's compare the R-squared values
p$paramr2 
p$nonparamr2 
# not much difference (param. model a bit better), possibly due to 
# small sample size 

# doing it "the long way" (showing use without an lm/glm object)
parest &lt;- lmout$fitted.values
regdiagbas(mlb[c("Height","Age")], mlb$Weight,parest,0.10,k=50,m=25)

data(prgeng)
pg &lt;- prgeng
pg1 &lt;- pg[pg$wageinc &gt;= 40000 &amp; pg$wkswrkd &gt;= 48,]
l1 &lt;- lm(wageinc ~ age+educ+sex,data=pg1)
p &lt;- regdiag(l1)
p
p$paramr2
p$nonparamr2
# young men's wages underpredicted, older women overpredicted; both
# R-squared values low, but nonpar is about 27% higher, indicating room
# for improvement; interaction and polynomial terms may help

## Not run: 
data(newadult)
g1 &lt;- glm(gt50 ~ edu + age + gender + mar, data=newadult, family=binomial)
regdiag(g1)
# parametric model underpredicts older highly-educated married men,
# and overpredicts young female lesser-educated singles; might try adding 
# interaction terms 

## End(Not run)
</code></pre>

<hr>
<h2 id='rmixmvnorm'>
Random vectors from mixtures of multivariate normal
distributions.
</h2><span id='topic+rmixmvnorm'></span>

<h3>Description</h3>

<p>Generates random vectors from mixtures of multivariate normal
distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmixmvnorm(n,dm,nmix,means,covs,wts=rep(1/nmix,nmix)) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmixmvnorm_+3A_n">n</code></td>
<td>
<p>Number of random vectors to generate.</p>
</td></tr>
<tr><td><code id="rmixmvnorm_+3A_dm">dm</code></td>
<td>
<p>Dimension i.e. length of each random vector.</p>
</td></tr>
<tr><td><code id="rmixmvnorm_+3A_nmix">nmix</code></td>
<td>
<p>Number of components in the mixture.</p>
</td></tr>
<tr><td><code id="rmixmvnorm_+3A_means">means</code></td>
<td>
<p>Mean vectors of the MV normal distributions; an R list
of <code>nmix</code> vectors of length <code>dm</code> each</p>
</td></tr>
<tr><td><code id="rmixmvnorm_+3A_covs">covs</code></td>
<td>
<p>Covariance matrices of the MV normal distributions; an R
list of <code>nmix</code> , each <code>d</code>m x <code>dm</code>.</p>
</td></tr>
<tr><td><code id="rmixmvnorm_+3A_wts">wts</code></td>
<td>
<p>Mixture probabilities.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>n</code> by <code>dm</code> matrix of random vectors of length <code>dm</code>,
grouped by MV normal distribution of origin.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff &lt;matloff@cs.ucdavis.edu&gt; 
</p>

<hr>
<h2 id='smoothz'>
Smoothing functions.
</h2><span id='topic+smoothz'></span><span id='topic+smoothzpred'></span><span id='topic+knnreg'></span><span id='topic+knndens'></span>

<h3>Description</h3>

<p>Routines for k-Nearest Neighbor density and regression estimation,
optionally using parallel computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothz(z,sf,k,checkna=TRUE,cls=NULL,nchunks=length(cls),scalefirst=FALSE)
smoothzpred(newx,oldx,oldxregest,checkna=TRUE,cls=NULL,nchunks=length(cls))
knnreg(data,k) 
knndens(data,k) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothz_+3A_z">z</code></td>
<td>
<p>The data, in data frame or matrix form.  In the regression
case, the response variable is assumed to be in the last column.</p>
</td></tr>
<tr><td><code id="smoothz_+3A_sf">sf</code></td>
<td>
<p>Smoothing function (unquoted), <code>knnreg</code> for regression or
<code>knndens</code> for density estimation.</p>
</td></tr>
<tr><td><code id="smoothz_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors.</p>
</td></tr>
<tr><td><code id="smoothz_+3A_nchunks">nchunks</code></td>
<td>
<p>Number of chunks to break the computation into.</p>
</td></tr>
<tr><td><code id="smoothz_+3A_newx">newx</code></td>
<td>
<p>New X data to predict from</p>
</td></tr>
<tr><td><code id="smoothz_+3A_oldx">oldx</code></td>
<td>
<p>X-variable values in the training set.</p>
</td></tr>
<tr><td><code id="smoothz_+3A_oldxregest">oldxregest</code></td>
<td>
<p>Estimated regression values in the training set.</p>
</td></tr>
<tr><td><code id="smoothz_+3A_checkna">checkna</code></td>
<td>
<p>If TRUE, remove any row having at least one NA value.</p>
</td></tr>
<tr><td><code id="smoothz_+3A_cls">cls</code></td>
<td>
<p>Cluster to use (see the <code>parallel</code> package) for
parallel computation.</p>
</td></tr>
<tr><td><code id="smoothz_+3A_data">data</code></td>
<td>
<p>Data to be smoothed.</p>
</td></tr>
<tr><td><code id="smoothz_+3A_scalefirst">scalefirst</code></td>
<td>
<p>Apply <a href="base.html#topic+scale">scale</a> to the data before smoothing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The smoothed values are calculated at the input data points
(needed in this form for another application).  So, for instance, the
i-th value of the output of <code>smoothz</code> in the regression case is the
estimated regression function at the i-th row of <code>z</code>.
</p>
<p>The density estimates are not mormalized to having total
hypervolume equal to 1.0.
</p>
<p>In the case of non-null <code>nchunks</code>, smoothing is done within-chunk
only.  The smoothed value at a point will be computed only from its
neighbors in the point's chunk.
</p>
<p>The <code>smoothzpred</code> function applies only to the regression case.
It is assumed that <code>smoothz</code> has been previously called on
<code>oldx</code>, yielding regression function estimates <code>oldxregest</code> at
those points.  The <code>smoothzpred</code> function then finds, for each
point <code>newx[i]</code>, the closest point <code>oldx[j]</code> in <code>oldx</code>, and
uses the corresponding value <code>oldxregest[j]</code> as the predicted value
at <code>newx[i]</code>.
</p>


<h3>Value</h3>

<p>Vector of smoothed values, or in the case of <code>smoothzpred</code>,
vector of predicted Y values for <code>newx</code>.  
</p>


<h3>Author(s)</h3>

<p>Norm Matloff &lt;matloff@cs.ucdavis.edu&gt; and Yingkang Xie
&lt;yingkang.xie@gmail.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# programmers and engineers in Silicon Valley, 2000 census, age 25-65
data(prgeng)
pg &lt;- prgeng
pg1 &lt;- pg[pg$age &gt;= 25 &amp; pg$age &lt;= 65,]
estreg &lt;- smoothz(pg1[,c(1,8)],sf=knnreg,k=100)
age &lt;- pg1[,1]
p &lt;- ggplot(data.frame(age,estreg))
p + geom_smooth(aes(x=age,y=estreg))
# peak earnings appear to occur around age 45

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
