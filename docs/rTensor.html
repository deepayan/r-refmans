<!DOCTYPE html><html><head><title>Help for package rTensor</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rTensor}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#rTensor-package'><p>Tools for tensor analysis and decomposition</p></a></li>
<li><a href='#+5B-methods'><p>Extract or Replace Subtensors</p></a></li>
<li><a href='#as.tensor'><p>Tensor Conversion</p></a></li>
<li><a href='#cp'><p>Canonical Polyadic Decomposition</p></a></li>
<li><a href='#cs_fold'><p>Column Space Folding of Matrix</p></a></li>
<li><a href='#cs_unfold-methods'><p>Tensor Column Space Unfolding</p></a></li>
<li><a href='#dim-methods'><p>Mode Getter for Tensor</p></a></li>
<li><a href='#fnorm-methods'><p>Tensor Frobenius Norm</p></a></li>
<li><a href='#fold'><p>General Folding of Matrix</p></a></li>
<li><a href='#hadamard_list'><p>List hadamard Product</p></a></li>
<li><a href='#head-methods'><p>Head for Tensor</p></a></li>
<li><a href='#hosvd'><p>(Truncated-)Higher-order SVD</p></a></li>
<li><a href='#initialize-methods'><p>Initializes a Tensor instance</p></a></li>
<li><a href='#innerProd-methods'><p>Tensors Inner Product</p></a></li>
<li><a href='#k_fold'><p>k-mode Folding of Matrix</p></a></li>
<li><a href='#k_unfold-methods'><p>Tensor k-mode Unfolding</p></a></li>
<li><a href='#khatri_rao'><p>Khatri-Rao Product</p></a></li>
<li><a href='#khatri_rao_list'><p>List Khatri-Rao Product</p></a></li>
<li><a href='#kronecker_list'><p>List Kronecker Product</p></a></li>
<li><a href='#load_orl'><p>ORL Database of Faces</p></a></li>
<li><a href='#matvec-methods'><p>Tensor Matvec Unfolding</p></a></li>
<li><a href='#modeMean-methods'><p>Tensor Mean Across Single Mode</p></a></li>
<li><a href='#modeSum-methods'><p>Tensor Sum Across Single Mode</p></a></li>
<li><a href='#mpca'><p>Multilinear Principal Components Analysis</p></a></li>
<li><a href='#Ops-methods'><p>Conformable elementwise operators for Tensor</p></a></li>
<li><a href='#plot_orl'><p>Function to plot the ORL Database of Faces</p></a></li>
<li><a href='#print-methods'><p>Print for Tensor</p></a></li>
<li><a href='#pvd'><p>Population Value Decomposition</p></a></li>
<li><a href='#rand_tensor'><p>Tensor with Random Entries</p></a></li>
<li><a href='#rs_fold'><p>Row Space Folding of Matrix</p></a></li>
<li><a href='#rs_unfold-methods'><p>Tensor Row Space Unfolding</p></a></li>
<li><a href='#show-methods'><p>Show for Tensor</p></a></li>
<li><a href='#t_mult'><p>Tensor Multiplication (T-MULT)</p></a></li>
<li><a href='#t_svd'><p>Tensor Singular Value Decomposition</p></a></li>
<li><a href='#t_svd_reconstruct'><p>Reconstruct Tensor From TSVD</p></a></li>
<li><a href='#t-methods'><p>Tensor Transpose</p></a></li>
<li><a href='#tail-methods'><p>Tail for Tensor</p></a></li>
<li><a href='#Tensor-class'><p>S4 Class for a Tensor</p></a></li>
<li><a href='#tperm-methods'><p>Mode Permutation for Tensor</p></a></li>
<li><a href='#ttl'><p>Tensor Times List</p></a></li>
<li><a href='#ttm'><p>Tensor Times Matrix (m-Mode Product)</p></a></li>
<li><a href='#tucker'><p>Tucker Decomposition</p></a></li>
<li><a href='#unfold-methods'><p>Tensor Unfolding</p></a></li>
<li><a href='#unmatvec'><p>Unmatvec Folding of Matrix</p></a></li>
<li><a href='#vec-methods'><p>Tensor Vec</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools for Tensor Analysis and Decomposition</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4.8</td>
</tr>
<tr>
<td>Author:</td>
<td>James Li and Jacob Bien and Martin Wells</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Koki Tsuyuzaki &lt;k.t.the-answer@hotmail.co.jp&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A set of tools for creation, manipulation, and modeling
    of tensors with arbitrary number of modes. A tensor in the context of data
    analysis is a multidimensional array. rTensor does this by providing a S4
    class 'Tensor' that wraps around the base 'array' class. rTensor
    provides common tensor operations as methods, including matrix unfolding,
    summing/averaging across modes, calculating the Frobenius norm, and taking
    the inner product between two tensors. Familiar array operations are
    overloaded, such as index subsetting via '[' and element-wise operations.
    rTensor also implements various tensor decomposition, including CP, GLRAM,
    MPCA, PVD, and Tucker. For tensors with 3 modes, rTensor also implements
    transpose, t-product, and t-SVD, as defined in Kilmer et al. (2013). Some
    auxiliary functions include the Khatri-Rao product, Kronecker product, and
    the Hadamard product for a list of matrices.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-05-14</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/rikenbit/rTensor">https://github.com/rikenbit/rTensor</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-05-14 04:43:12 UTC; root</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-05-15 06:20:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='rTensor-package'>Tools for tensor analysis and decomposition</h2><span id='topic+rTensor'></span><span id='topic+rTensor-package'></span>

<h3>Description</h3>

<p>This package is centered around the <code><a href="#topic+Tensor-class">Tensor-class</a></code>, which defines a S4 class for tensors of arbitrary number of modes. A vignette and/or a possible paper will be included in a future release of this package.
</p>


<h3>Details</h3>

<p>This page will summarize the full functionality of this package. Note that since all the methods associated with S4 class <code><a href="#topic+Tensor-class">Tensor-class</a></code> are documented there, we will not duplicate it here.
</p>
<p>The remaining functions can be split into two groups: the first is a set of tensor decompositions, and the second is a set of helper functions that are useful in tensor manipulation.
</p>
<p>rTensor implements the following tensor decompositions: </p>

<dl>
<dt><code><a href="#topic+cp">cp</a></code></dt><dd><p>Canonical Polyadic (CP) decomposition</p>
</dd>
<dt><code><a href="#topic+tucker">tucker</a></code></dt><dd><p>General Tucker decomposition</p>
</dd>
<dt><code><a href="#topic+mpca">mpca</a></code></dt><dd><p>Multilinear Principal Component Analysis; note that for 3-Tensors this is also known as Generalized Low Rank Approximation of Matrices(GLRAM)</p>
</dd>
<dt><code><a href="#topic+hosvd">hosvd</a></code></dt><dd><p>(Truncated-)Higher-order singular value decomposition</p>
</dd>
<dt><code><a href="#topic+t_svd">t_svd</a></code></dt><dd><p>Tensor singular value decomposition; 3-Tensors only; also note that there is an asociated reconstruction function <code><a href="#topic+t_svd_reconstruct">t_svd_reconstruct</a></code></p>
</dd>
<dt><code><a href="#topic+pvd">pvd</a></code></dt><dd><p>Population value decomposition of images; 3-Tensors only</p>
</dd>
</dl>

<p>rTensor also provides a set functions for tensors multiplication: </p>

<dl>
<dt><code><a href="#topic+ttm">ttm</a></code></dt><dd><p>Tensor times matrix, aka m-mode product</p>
</dd>
<dt><code><a href="#topic+ttl">ttl</a></code></dt><dd><p>Tensor times list (of matrices)</p>
</dd>
<dt><code><a href="#topic+t_mult">t_mult</a></code></dt><dd><p>Tensor product based on block circulant unfolding; only implemented for a pair of 3-Tensors</p>
</dd>
</dl>

<p>...as well as for matrices: </p>

<dl>
<dt><code><a href="#topic+hadamard_list">hadamard_list</a></code></dt><dd><p>Computes the Hadamard (element-wise) product of a list of matrices</p>
</dd>
<dt><code><a href="#topic+kronecker_list">kronecker_list</a></code></dt><dd><p>Computes the Kronecker product of a list of matrices</p>
</dd>
<dt><code><a href="#topic+khatri_rao">khatri_rao</a></code></dt><dd><p>Computes the Khatri-Rao product of two matrices</p>
</dd>
<dt><code><a href="#topic+khatri_rao_list">khatri_rao_list</a></code></dt><dd><p>Computes the Khatri-Rao product of a list of matrices</p>
</dd>
<dt><code><a href="#topic+fold">fold</a></code></dt><dd><p>General folding of a matrix into a tensor</p>
</dd>
<dt><code><a href="#topic+k_fold">k_fold</a></code></dt><dd><p>Inverse operation for <code><a href="#topic+k_unfold">k_unfold</a></code></p>
</dd>
<dt><code><a href="#topic+unmatvec">unmatvec</a></code></dt><dd><p>Inverse operation for <code><a href="#topic+matvec">matvec</a></code></p>
</dd>
</dl>

<p>For more information on any of the functions, please consult the individual man pages.
</p>


<h3>Author(s)</h3>

<p>James Li <a href="mailto:jamesyili@gmail.com">jamesyili@gmail.com</a>, Jacob Bien, and Martin T. Wells
</p>

<hr>
<h2 id='+5B-methods'>Extract or Replace Subtensors</h2><span id='topic++5B-methods'></span><span id='topic++5B+2CTensor-method'></span><span id='topic+extract+2CTensor-method'></span><span id='topic++5B+3C-+2CTensor-method'></span>

<h3>Description</h3>

<p>Extends '[' and '[&lt;-' from the base array class for the Tensor class. Works exactly as it would for the base 'array' class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Tensor'
x[i, j, ..., drop = TRUE]

## S4 replacement method for signature 'Tensor'
x[i, j, ...] &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B5B-methods_+3A_x">x</code></td>
<td>
<p>Tensor to be subset</p>
</td></tr>
<tr><td><code id="+2B5B-methods_+3A_i">i</code>, <code id="+2B5B-methods_+3A_j">j</code>, <code id="+2B5B-methods_+3A_...">...</code></td>
<td>
<p>indices that specify the extents of the sub-tensor</p>
</td></tr>
<tr><td><code id="+2B5B-methods_+3A_drop">drop</code></td>
<td>
<p>whether or not to reduce the number of modes to exclude those that have '1' as the mode</p>
</td></tr>
<tr><td><code id="+2B5B-methods_+3A_value">value</code></td>
<td>
<p>either vector, matrix, or array that will replace the subtensor</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>x[i,j,...,drop=TRUE]</code>
</p>


<h3>Value</h3>

<p>an object of class Tensor
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
tnsr[1,2,3]
tnsr[3,1,]
tnsr[,,5]
tnsr[,,5,drop=FALSE]

tnsr[1,2,3] &lt;- 3; tnsr[1,2,3]
tnsr[3,1,] &lt;- rep(0,5); tnsr[3,1,]
tnsr[,2,] &lt;- matrix(0,nrow=3,ncol=5); tnsr[,2,]
</code></pre>

<hr>
<h2 id='as.tensor'>Tensor Conversion</h2><span id='topic+as.tensor'></span>

<h3>Description</h3>

<p>Create a <code><a href="#topic+Tensor-class">Tensor-class</a></code> object from an <code>array</code>, <code>matrix</code>, or <code>vector</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.tensor(x, drop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.tensor_+3A_x">x</code></td>
<td>
<p>an instance of <code>array</code>, <code>matrix</code>, or <code>vector</code></p>
</td></tr>
<tr><td><code id="as.tensor_+3A_drop">drop</code></td>
<td>
<p>whether or not modes of 1 should be dropped</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="#topic+Tensor-class">Tensor-class</a></code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#From vector
vec &lt;- runif(3); vecT &lt;- as.tensor(vec); vecT
#From matrix
mat &lt;- matrix(runif(2*3),nrow=2,ncol=3)
matT &lt;- as.tensor(mat); matT
#From array
indices &lt;- c(2,3,4)
arr &lt;- array(runif(prod(indices)), dim = indices)
arrT &lt;- as.tensor(arr); arrT
</code></pre>

<hr>
<h2 id='cp'>Canonical Polyadic Decomposition</h2><span id='topic+cp'></span>

<h3>Description</h3>

<p>Canonical Polyadic (CP) decomposition of a tensor, aka CANDECOMP/PARAFRAC. Approximate a K-Tensor using a sum of <code>num_components</code> rank-1 K-Tensors. A rank-1 K-Tensor can be written as an outer product of K vectors. There are a total of <code>num_compoents *tnsr@num_modes</code> vectors in the output, stored in <code>tnsr@num_modes</code> matrices, each with <code>num_components</code> columns. This is an iterative algorithm, with two possible stopping conditions: either relative error in Frobenius norm has gotten below <code>tol</code>, or the <code>max_iter</code> number of iterations has been reached. For more details on CP decomposition, consult Kolda and Bader (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cp(tnsr, num_components = NULL, max_iter = 25, tol = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cp_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor with K modes</p>
</td></tr>
<tr><td><code id="cp_+3A_num_components">num_components</code></td>
<td>
<p>the number of rank-1 K-Tensors to use in approximation</p>
</td></tr>
<tr><td><code id="cp_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum number of iterations if error stays above <code>tol</code></p>
</td></tr>
<tr><td><code id="cp_+3A_tol">tol</code></td>
<td>
<p>relative Frobenius norm error tolerance</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the Alternating Least Squares (ALS) estimation procedure. A progress bar is included to help monitor operations on large tensors.
</p>


<h3>Value</h3>

<p>a list containing the following </p>

<dl>
<dt><code>lambdas</code></dt><dd><p>a vector of normalizing constants, one for each component</p>
</dd>
<dt><code>U</code></dt><dd><p>a list of matrices - one for each mode - each matrix with <code>num_components</code> columns</p>
</dd>
<dt><code>conv</code></dt><dd><p>whether or not <code>resid</code> &lt; <code>tol</code> by the last iteration</p>
</dd>
<dt><code>norm_percent</code></dt><dd><p>the percent of Frobenius norm explained by the approximation</p>
</dd>
<dt><code>est</code></dt><dd><p>estimate of <code>tnsr</code> after compression</p>
</dd>
<dt><code>fnorm_resid</code></dt><dd><p>the Frobenius norm of the error <code>fnorm(est-tnsr)</code></p>
</dd>
<dt><code>all_resids</code></dt><dd><p>vector containing the Frobenius norm of error for all the iterations</p>
</dd>
</dl>



<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tucker">tucker</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### How to retrieve faces_tnsr from figshare
# faces_tnsr &lt;- load_orl()
# subject &lt;- faces_tnsr[,,14,]
dummy_faces_tnsr &lt;- rand_tensor(c(92,112,40,10))
subject &lt;- dummy_faces_tnsr[,,14,]
cpD &lt;- cp(subject, num_components=3)
cpD$conv
cpD$norm_percent
plot(cpD$all_resids)
</code></pre>

<hr>
<h2 id='cs_fold'>Column Space Folding of Matrix</h2><span id='topic+cs_fold'></span>

<h3>Description</h3>

<p>DEPRECATED. Please see <code><a href="#topic+unmatvec">unmatvec</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cs_fold(mat, m = NULL, modes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cs_fold_+3A_mat">mat</code></td>
<td>
<p>matrix to be folded</p>
</td></tr>
<tr><td><code id="cs_fold_+3A_m">m</code></td>
<td>
<p>the mode corresponding to cs_unfold</p>
</td></tr>
<tr><td><code id="cs_fold_+3A_modes">modes</code></td>
<td>
<p>the original modes of the tensor</p>
</td></tr>
</table>

<hr>
<h2 id='cs_unfold-methods'>Tensor Column Space Unfolding</h2><span id='topic+cs_unfold-methods'></span><span id='topic+cs_unfold'></span><span id='topic+cs_unfold+2CTensor-method'></span>

<h3>Description</h3>

<p>DEPRECATED. Please see <code><a href="#topic+matvec-methods">matvec-methods</a></code> and <code><a href="#topic+unfold-methods">unfold-methods</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cs_unfold(tnsr, m)

## S4 method for signature 'Tensor'
cs_unfold(tnsr, m = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cs_unfold-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor instance</p>
</td></tr>
<tr><td><code id="cs_unfold-methods_+3A_m">m</code></td>
<td>
<p>mode to be unfolded on</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cs_unfold(tnsr,m=NULL)</code>
</p>

<hr>
<h2 id='dim-methods'>Mode Getter for Tensor</h2><span id='topic+dim-methods'></span><span id='topic+dim+2CTensor-method'></span>

<h3>Description</h3>

<p>Return the vector of modes from a tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Tensor'
dim(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dim-methods_+3A_x">x</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dim(x)</code>
</p>


<h3>Value</h3>

<p>an integer vector of the modes associated with <code>x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
dim(tnsr)
</code></pre>

<hr>
<h2 id='fnorm-methods'>Tensor Frobenius Norm</h2><span id='topic+fnorm-methods'></span><span id='topic+fnorm'></span><span id='topic+fnorm+2CTensor-method'></span>

<h3>Description</h3>

<p>Returns the Frobenius norm of the Tensor instance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fnorm(tnsr)

## S4 method for signature 'Tensor'
fnorm(tnsr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fnorm-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fnorm(tnsr)</code>
</p>


<h3>Value</h3>

<p>numeric Frobenius norm of <code>x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
fnorm(tnsr)
</code></pre>

<hr>
<h2 id='fold'>General Folding of Matrix</h2><span id='topic+fold'></span>

<h3>Description</h3>

<p>General folding of a matrix into a Tensor. This is designed to be the inverse function to <code><a href="#topic+unfold-methods">unfold-methods</a></code>, with the same ordering of the indices. This amounts to following: if we were to unfold a Tensor using a set of <code>row_idx</code> and <code>col_idx</code>, then we can fold the resulting matrix back into the original Tensor using the same <code>row_idx</code> and <code>col_idx</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fold(mat, row_idx = NULL, col_idx = NULL, modes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fold_+3A_mat">mat</code></td>
<td>
<p>matrix to be folded into a Tensor</p>
</td></tr>
<tr><td><code id="fold_+3A_row_idx">row_idx</code></td>
<td>
<p>the indices of the modes that are mapped onto the row space</p>
</td></tr>
<tr><td><code id="fold_+3A_col_idx">col_idx</code></td>
<td>
<p>the indices of the modes that are mapped onto the column space</p>
</td></tr>
<tr><td><code id="fold_+3A_modes">modes</code></td>
<td>
<p>the modes of the output Tensor</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses <code>aperm</code> as the primary workhorse.
</p>


<h3>Value</h3>

<p>Tensor object with modes given by <code>modes</code>
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+unfold-methods">unfold-methods</a></code>, <code><a href="#topic+k_fold">k_fold</a></code>, <code><a href="#topic+unmatvec">unmatvec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- new("Tensor",3L,c(3L,4L,5L),data=runif(60))
matT3&lt;-unfold(tnsr,row_idx=2,col_idx=c(3,1))
identical(fold(matT3,row_idx=2,col_idx=c(3,1),modes=c(3,4,5)),tnsr)
</code></pre>

<hr>
<h2 id='hadamard_list'>List hadamard Product</h2><span id='topic+hadamard_list'></span>

<h3>Description</h3>

<p>Returns the hadamard (element-wise) product from a list of matrices or vectors. Commonly used for n-mode products and various Tensor decompositions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hadamard_list(L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hadamard_list_+3A_l">L</code></td>
<td>
<p>list of matrices or vectors</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix that is the hadamard product
</p>


<h3>Note</h3>

<p>The modes/dimensions of each element in the list must match.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kronecker_list">kronecker_list</a></code>, <code><a href="#topic+khatri_rao_list">khatri_rao_list</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lizt &lt;- list('mat1' = matrix(runif(40),ncol=4), 
'mat2' = matrix(runif(40),ncol=4),
'mat3' = matrix(runif(40),ncol=4))
dim(hadamard_list(lizt))
</code></pre>

<hr>
<h2 id='head-methods'>Head for Tensor</h2><span id='topic+head-methods'></span><span id='topic+head+2CTensor-method'></span>

<h3>Description</h3>

<p>Extend head for Tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Tensor'
head(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="head-methods_+3A_x">x</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
<tr><td><code id="head-methods_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed into head()</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>head(x,...)</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tail-methods">tail-methods</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
head(tnsr)
</code></pre>

<hr>
<h2 id='hosvd'>(Truncated-)Higher-order SVD</h2><span id='topic+hosvd'></span>

<h3>Description</h3>

<p>Higher-order SVD of a K-Tensor. Write the K-Tensor as a (m-mode) product of a core Tensor (possibly smaller modes) and K orthogonal factor matrices. Truncations can be specified via <code>ranks</code> (making them smaller than the original modes of the K-Tensor will result in a truncation). For the mathematical details on HOSVD, consult Lathauwer et. al. (2000).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hosvd(tnsr, ranks = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hosvd_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor with K modes</p>
</td></tr>
<tr><td><code id="hosvd_+3A_ranks">ranks</code></td>
<td>
<p>a vector of desired modes in the output core tensor, default is <code>tnsr@modes</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>A progress bar is included to help monitor operations on large tensors.
</p>


<h3>Value</h3>

<p>a list containing the following:</p>

<dl>
<dt><code>Z</code></dt><dd><p>core tensor with modes speficied by <code>ranks</code></p>
</dd>
<dt><code>U</code></dt><dd><p>a list of orthogonal matrices, one for each mode</p>
</dd>
<dt><code>est</code></dt><dd><p>estimate of <code>tnsr</code> after compression</p>
</dd>
<dt><code>fnorm_resid</code></dt><dd><p>the Frobenius norm of the error <code>fnorm(est-tnsr)</code> - if there was no truncation, then this is on the order of mach_eps * fnorm. </p>
</dd>
</dl>



<h3>Note</h3>

<p>The length of <code>ranks</code> must match <code>tnsr@num_modes</code>.
</p>


<h3>References</h3>

<p>L. Lathauwer, B.Moor, J. Vanderwalle &quot;A multilinear singular value decomposition&quot;. Journal of Matrix Analysis and Applications 2000.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tucker">tucker</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor(c(6,7,8))
hosvdD &lt;- hosvd(tnsr)
plot(hosvdD$fnorm_resid)
hosvdD2 &lt;- hosvd(tnsr,ranks=c(3,3,4))
plot(hosvdD2$fnorm_resid)
</code></pre>

<hr>
<h2 id='initialize-methods'>Initializes a Tensor instance</h2><span id='topic+initialize-methods'></span><span id='topic+initialize+2CTensor-method'></span>

<h3>Description</h3>

<p>Not designed to be called by the user. Use <code>as.tensor</code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Tensor'
initialize(.Object, num_modes = NULL, modes = NULL,
  data = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="initialize-methods_+3A_.object">.Object</code></td>
<td>
<p>the tensor object</p>
</td></tr>
<tr><td><code id="initialize-methods_+3A_num_modes">num_modes</code></td>
<td>
<p>number of modes of the tensor</p>
</td></tr>
<tr><td><code id="initialize-methods_+3A_modes">modes</code></td>
<td>
<p>modes of the tensor</p>
</td></tr>
<tr><td><code id="initialize-methods_+3A_data">data</code></td>
<td>
<p>can be vector, matrix, or array</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code>as.tensor</code>
</p>

<hr>
<h2 id='innerProd-methods'>Tensors Inner Product</h2><span id='topic+innerProd-methods'></span><span id='topic+innerProd'></span><span id='topic+innerProd+2CTensor+2CTensor-method'></span>

<h3>Description</h3>

<p>Returns the inner product between two Tensors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>innerProd(tnsr1, tnsr2)

## S4 method for signature 'Tensor,Tensor'
innerProd(tnsr1, tnsr2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="innerProd-methods_+3A_tnsr1">tnsr1</code></td>
<td>
<p>first Tensor instance</p>
</td></tr>
<tr><td><code id="innerProd-methods_+3A_tnsr2">tnsr2</code></td>
<td>
<p>second Tensor instance</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>innerProd(tnsr1,tnsr2)</code>
</p>


<h3>Value</h3>

<p>inner product between <code>x1</code> and <code>x2</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr1 &lt;- rand_tensor()
tnsr2 &lt;- rand_tensor()
innerProd(tnsr1,tnsr2)
</code></pre>

<hr>
<h2 id='k_fold'>k-mode Folding of Matrix</h2><span id='topic+k_fold'></span>

<h3>Description</h3>

<p>k-mode folding of a matrix into a Tensor. This is the inverse funtion to <code>k_unfold</code> in the m mode. In particular, <code>k_fold(k_unfold(tnsr, m),m,getModes(tnsr))</code> will result in the original Tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_fold(mat, m = NULL, modes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k_fold_+3A_mat">mat</code></td>
<td>
<p>matrix to be folded into a Tensor</p>
</td></tr>
<tr><td><code id="k_fold_+3A_m">m</code></td>
<td>
<p>the index of the mode that is mapped onto the row indices</p>
</td></tr>
<tr><td><code id="k_fold_+3A_modes">modes</code></td>
<td>
<p>the modes of the output Tensor</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a wrapper function to <code><a href="#topic+fold">fold</a></code>.
</p>


<h3>Value</h3>

<p>Tensor object with modes given by <code>modes</code>
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+k_unfold-methods">k_unfold-methods</a></code>, <code><a href="#topic+fold">fold</a></code>, <code><a href="#topic+unmatvec">unmatvec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- new("Tensor",3L,c(3L,4L,5L),data=runif(60))
matT2&lt;-k_unfold(tnsr,m=2)
identical(k_fold(matT2,m=2,modes=c(3,4,5)),tnsr)
</code></pre>

<hr>
<h2 id='k_unfold-methods'>Tensor k-mode Unfolding</h2><span id='topic+k_unfold-methods'></span><span id='topic+k_unfold'></span><span id='topic+k_unfold+2CTensor-method'></span>

<h3>Description</h3>

<p>Unfolding of a tensor by mapping the kth mode (specified through parameter <code>m</code>), and all other modes onto the column space. This the most common type of unfolding operation for Tucker decompositions and its variants. Also known as k-mode matricization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_unfold(tnsr, m)

## S4 method for signature 'Tensor'
k_unfold(tnsr, m = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k_unfold-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
<tr><td><code id="k_unfold-methods_+3A_m">m</code></td>
<td>
<p>the index of the mode to unfold on</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>k_unfold(tnsr,m=NULL)</code>
</p>


<h3>Value</h3>

<p>matrix with <code>x@modes[m]</code> rows and <code>prod(x@modes[-m])</code> columns
</p>


<h3>References</h3>

<p>T. Kolda and B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+matvec-methods">matvec-methods</a></code> and <code><a href="#topic+unfold-methods">unfold-methods</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
matT2&lt;-rs_unfold(tnsr,m=2)
</code></pre>

<hr>
<h2 id='khatri_rao'>Khatri-Rao Product</h2><span id='topic+khatri_rao'></span>

<h3>Description</h3>

<p>Returns the Khatri-Rao (column-wise Kronecker) product of two matrices. If the inputs are vectors then this is the same as the Kronecker product.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>khatri_rao(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="khatri_rao_+3A_x">x</code></td>
<td>
<p>first matrix</p>
</td></tr>
<tr><td><code id="khatri_rao_+3A_y">y</code></td>
<td>
<p>second matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix that is the Khatri-Rao product
</p>


<h3>Note</h3>

<p>The number of columns must match in the two inputs.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+kronecker">kronecker</a></code>, <code><a href="#topic+khatri_rao_list">khatri_rao_list</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dim(khatri_rao(matrix(runif(12),ncol=4),matrix(runif(12),ncol=4)))
</code></pre>

<hr>
<h2 id='khatri_rao_list'>List Khatri-Rao Product</h2><span id='topic+khatri_rao_list'></span>

<h3>Description</h3>

<p>Returns the Khatri-Rao product from a list of matrices or vectors. Commonly used for n-mode products and various Tensor decompositions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>khatri_rao_list(L, reverse = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="khatri_rao_list_+3A_l">L</code></td>
<td>
<p>list of matrices or vectors</p>
</td></tr>
<tr><td><code id="khatri_rao_list_+3A_reverse">reverse</code></td>
<td>
<p>whether or not to reverse the order</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix that is the Khatri-Rao product
</p>


<h3>Note</h3>

<p>The number of columns must match in every element of the input list.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+khatri_rao">khatri_rao</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>smalllizt &lt;- list('mat1' = matrix(runif(12),ncol=4), 
'mat2' = matrix(runif(12),ncol=4),
'mat3' = matrix(runif(12),ncol=4))
dim(khatri_rao_list(smalllizt))
</code></pre>

<hr>
<h2 id='kronecker_list'>List Kronecker Product</h2><span id='topic+kronecker_list'></span>

<h3>Description</h3>

<p>Returns the Kronecker product from a list of matrices or vectors. Commonly used for n-mode products and various Tensor decompositions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kronecker_list(L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kronecker_list_+3A_l">L</code></td>
<td>
<p>list of matrices or vectors</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix that is the Kronecker product
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hadamard_list">hadamard_list</a></code>, <code><a href="#topic+khatri_rao_list">khatri_rao_list</a></code>, <code><a href="base.html#topic+kronecker">kronecker</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>smalllizt &lt;- list('mat1' = matrix(runif(12),ncol=4), 
'mat2' = matrix(runif(12),ncol=4),
'mat3' = matrix(runif(12),ncol=4))
dim(kronecker_list(smalllizt))
</code></pre>

<hr>
<h2 id='load_orl'>ORL Database of Faces</h2><span id='topic+load_orl'></span>

<h3>Description</h3>

<p>A dataset containing pictures of 40 individuals under 10 different lightings. Each image has 92 x 112 pixels. Structured as a 4-tensor with modes 92 x 112 x 40 x 10. The data is now stored in figshare <a href="https://ndownloader.figshare.com/files/28005669">https://ndownloader.figshare.com/files/28005669</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_orl()
</code></pre>


<h3>Format</h3>

<p>A Tensor object with modes 92 x 112 x 40 x 10. The first two modes correspond to the image pixels, the third mode corresponds to the individual, and the last mode correpsonds to the lighting.</p>


<h3>Source</h3>

<p><a href="https://www.kaggle.com/kasikrit/att-database-of-faces">https://www.kaggle.com/kasikrit/att-database-of-faces</a>
</p>


<h3>References</h3>

<p>AT&amp;T Laboratories Cambridge. <a href="https://www.kaggle.com/kasikrit/att-database-of-faces">https://www.kaggle.com/kasikrit/att-database-of-faces</a>
</p>
<p>F. Samaria, A. Harter, &quot;Parameterisation of a Stochastic Model for Human Face Identification&quot;. IEEE Workshop on Applications of Computer Vision 1994.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot_orl">plot_orl</a></code>
</p>

<hr>
<h2 id='matvec-methods'>Tensor Matvec Unfolding</h2><span id='topic+matvec-methods'></span><span id='topic+matvec'></span><span id='topic+matvec+2CTensor-method'></span>

<h3>Description</h3>

<p>For 3-tensors only. Stacks the slices along the third mode. This is the prevalent unfolding for T-SVD and T-MULT based on block circulant matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matvec(tnsr)

## S4 method for signature 'Tensor'
matvec(tnsr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matvec-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>matvec(tnsr)</code>
</p>


<h3>Value</h3>

<p>matrix with <code>prod(x@modes[-m])</code> rows and <code>x@modes[m]</code> columns
</p>


<h3>References</h3>

<p>M. Kilmer, K. Braman, N. Hao, and R. Hoover, &quot;Third-order tensors as operators on matrices: a theoretical and computational framework with applications in imaging&quot;. SIAM Journal on Matrix Analysis and Applications 2013.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+k_unfold-methods">k_unfold-methods</a></code> and <code><a href="#topic+unfold-methods">unfold-methods</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor(c(2,3,4))
matT1&lt;- matvec(tnsr)
</code></pre>

<hr>
<h2 id='modeMean-methods'>Tensor Mean Across Single Mode</h2><span id='topic+modeMean-methods'></span><span id='topic+modeMean'></span><span id='topic+modeMean+2CTensor-method'></span>

<h3>Description</h3>

<p>Given a mode for a K-tensor, this returns the K-1 tensor resulting from taking the mean across that particular mode.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modeMean(tnsr, m, drop)

## S4 method for signature 'Tensor'
modeMean(tnsr, m = NULL, drop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modeMean-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
<tr><td><code id="modeMean-methods_+3A_m">m</code></td>
<td>
<p>the index of the mode to average across</p>
</td></tr>
<tr><td><code id="modeMean-methods_+3A_drop">drop</code></td>
<td>
<p>whether or not mode m should be dropped</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>modeMean(tnsr,m=NULL,drop=FALSE)</code>
</p>


<h3>Value</h3>

<p>K-1 or K Tensor, where <code>K = x@num_modes</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+modeSum">modeSum</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
modeMean(tnsr,1,drop=TRUE)
</code></pre>

<hr>
<h2 id='modeSum-methods'>Tensor Sum Across Single Mode</h2><span id='topic+modeSum-methods'></span><span id='topic+modeSum'></span><span id='topic+modeSum+2CTensor-method'></span>

<h3>Description</h3>

<p>Given a mode for a K-tensor, this returns the K-1 tensor resulting from summing across that particular mode.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modeSum(tnsr, m, drop)

## S4 method for signature 'Tensor'
modeSum(tnsr, m = NULL, drop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modeSum-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
<tr><td><code id="modeSum-methods_+3A_m">m</code></td>
<td>
<p>the index of the mode to sum across</p>
</td></tr>
<tr><td><code id="modeSum-methods_+3A_drop">drop</code></td>
<td>
<p>whether or not mode m should be dropped</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>modeSum(tnsr,m=NULL,drop=FALSE)</code>
</p>


<h3>Value</h3>

<p>K-1 or K tensor, where <code>K = x@num_modes</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+modeMean">modeMean</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
modeSum(tnsr,3,drop=TRUE)
</code></pre>

<hr>
<h2 id='mpca'>Multilinear Principal Components Analysis</h2><span id='topic+mpca'></span>

<h3>Description</h3>

<p>This is basically the Tucker decomposition of a K-Tensor, <code><a href="#topic+tucker">tucker</a></code>, with one of the modes uncompressed. If K = 3, then this is also known as the Generalized Low Rank Approximation of Matrices (GLRAM). This implementation assumes that the last mode is the measurement mode and hence uncompressed. This is an iterative algorithm, with two possible stopping conditions: either relative error in Frobenius norm has gotten below <code>tol</code>, or the <code>max_iter</code> number of iterations has been reached. For more details on the MPCA of tensors, consult Lu et al. (2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpca(tnsr, ranks = NULL, max_iter = 25, tol = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mpca_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor with K modes</p>
</td></tr>
<tr><td><code id="mpca_+3A_ranks">ranks</code></td>
<td>
<p>a vector of the compressed modes of the output core Tensor, this has length K-1</p>
</td></tr>
<tr><td><code id="mpca_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum number of iterations if error stays above <code>tol</code></p>
</td></tr>
<tr><td><code id="mpca_+3A_tol">tol</code></td>
<td>
<p>relative Frobenius norm error tolerance</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the Alternating Least Squares (ALS) estimation procedure. A progress bar is included to help monitor operations on large tensors.
</p>


<h3>Value</h3>

<p>a list containing the following:</p>

<dl>
<dt><code>Z_ext</code></dt><dd><p>the extended core tensor, with the first K-1 modes given by <code>ranks</code></p>
</dd>
<dt><code>U</code></dt><dd><p>a list of K-1 orthgonal factor matrices - one for each compressed mode, with the number of columns of the matrices given by <code>ranks</code></p>
</dd>
<dt><code>conv</code></dt><dd><p>whether or not <code>resid</code> &lt; <code>tol</code> by the last iteration</p>
</dd>
<dt><code>est</code></dt><dd><p>estimate of <code>tnsr</code> after compression</p>
</dd>
<dt><code>norm_percent</code></dt><dd><p>the percent of Frobenius norm explained by the approximation</p>
</dd>
<dt><code>fnorm_resid</code></dt><dd><p>the Frobenius norm of the error <code>fnorm(est-tnsr)</code></p>
</dd>
<dt><code>all_resids</code></dt><dd><p>vector containing the Frobenius norm of error for all the iterations</p>
</dd>
</dl>



<h3>Note</h3>

<p>The length of <code>ranks</code> must match <code>tnsr@num_modes-1</code>.
</p>


<h3>References</h3>

<p>H. Lu, K. Plataniotis, A. Venetsanopoulos, &quot;Mpca: Multilinear principal component analysis of tensor objects&quot;. IEEE Trans. Neural networks, 2008.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tucker">tucker</a></code>, <code><a href="#topic+hosvd">hosvd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### How to retrieve faces_tnsr from figshare
# faces_tnsr &lt;- load_orl()
# subject &lt;- faces_tnsr[,,21,]
dummy_faces_tnsr &lt;- rand_tensor(c(92,112,40,10))
subject &lt;- dummy_faces_tnsr[,,21,]
mpcaD &lt;- mpca(subject, ranks=c(10, 10))
mpcaD$conv
mpcaD$norm_percent
plot(mpcaD$all_resids)
</code></pre>

<hr>
<h2 id='Ops-methods'>Conformable elementwise operators for Tensor</h2><span id='topic+Ops-methods'></span><span id='topic+Ops+2CTensor+2CTensor-method'></span><span id='topic+Ops+2CTensor+2Carray-method'></span><span id='topic+Ops+2CTensor+2Cnumeric-method'></span><span id='topic+Ops+2Carray+2CTensor-method'></span><span id='topic+Ops+2Cnumeric+2CTensor-method'></span>

<h3>Description</h3>

<p>Overloads elementwise operators for tensors, arrays, and vectors that are conformable (have the same modes).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Tensor,Tensor'
Ops(e1, e2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ops-methods_+3A_e1">e1</code></td>
<td>
<p>left-hand object</p>
</td></tr>
<tr><td><code id="Ops-methods_+3A_e2">e2</code></td>
<td>
<p>right-hand object</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor(c(3,4,5))
tnsr2 &lt;- rand_tensor(c(3,4,5))
tnsrsum &lt;- tnsr + tnsr2
tnsrdiff &lt;- tnsr - tnsr2
tnsrelemprod &lt;- tnsr * tnsr2
tnsrelemquot &lt;- tnsr / tnsr2
for (i in 1:3L){
for (j in 1:4L){
	for (k in 1:5L){
		stopifnot(tnsrsum@data[i,j,k]==tnsr@data[i,j,k]+tnsr2@data[i,j,k])
		stopifnot(tnsrdiff@data[i,j,k]==(tnsr@data[i,j,k]-tnsr2@data[i,j,k]))
		stopifnot(tnsrelemprod@data[i,j,k]==tnsr@data[i,j,k]*tnsr2@data[i,j,k])
		stopifnot(tnsrelemquot@data[i,j,k]==tnsr@data[i,j,k]/tnsr2@data[i,j,k])
}
}
}
</code></pre>

<hr>
<h2 id='plot_orl'>Function to plot the ORL Database of Faces</h2><span id='topic+plot_orl'></span>

<h3>Description</h3>

<p>A wrapper function to image() to allow easy visualization of faces_tnsr, the ORL Face Dataset.  The data is now stored in figshare <a href="https://ndownloader.figshare.com/files/28005669">https://ndownloader.figshare.com/files/28005669</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_orl(subject = 1, condition = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_orl_+3A_subject">subject</code></td>
<td>
<p>which subject to plot (1-40)</p>
</td></tr>
<tr><td><code id="plot_orl_+3A_condition">condition</code></td>
<td>
<p>which lighting condition (1-10)</p>
</td></tr>
</table>


<h3>References</h3>

<p>AT&amp;T Laboratories Cambridge. <a href="https://www.kaggle.com/kasikrit/att-database-of-faces">https://www.kaggle.com/kasikrit/att-database-of-faces</a>
</p>
<p>F. Samaria, A. Harter, &quot;Parameterisation of a Stochastic Model for Human Face Identification&quot;. IEEE Workshop on Applications of Computer Vision 1994.
</p>

<hr>
<h2 id='print-methods'>Print for Tensor</h2><span id='topic+print-methods'></span><span id='topic+print+2CTensor-method'></span>

<h3>Description</h3>

<p>Extend print for Tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Tensor'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print-methods_+3A_x">x</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
<tr><td><code id="print-methods_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed into print()</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>print(x,...)</code>
</p>


<h3>See Also</h3>

<p><code><a href="methods.html#topic+show">show</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
print(tnsr)
</code></pre>

<hr>
<h2 id='pvd'>Population Value Decomposition</h2><span id='topic+pvd'></span>

<h3>Description</h3>

<p>The default Population Value Decomposition (PVD) of a series of 2D images. Constructs population-level matrices P, V, and D to account for variances within as well as across the images. Structurally similar to Tucker (<code><a href="#topic+tucker">tucker</a></code>) and GLRAM (<code><a href="#topic+mpca">mpca</a></code>), but retains crucial differences. Requires <code>2*n3 + 2</code> parameters to specified the final ranks of P, V, and D, where n3 is the third mode (how many images are in the set). Consult Crainiceanu et al. (2013) for the construction and rationale behind the PVD model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pvd(tnsr, uranks = NULL, wranks = NULL, a = NULL, b = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pvd_+3A_tnsr">tnsr</code></td>
<td>
<p>3-Tensor with the third mode being the measurement mode</p>
</td></tr>
<tr><td><code id="pvd_+3A_uranks">uranks</code></td>
<td>
<p>ranks of the U matrices</p>
</td></tr>
<tr><td><code id="pvd_+3A_wranks">wranks</code></td>
<td>
<p>ranks of the W matrices</p>
</td></tr>
<tr><td><code id="pvd_+3A_a">a</code></td>
<td>
<p>rank of <code>P = U%*%t(U)</code></p>
</td></tr>
<tr><td><code id="pvd_+3A_b">b</code></td>
<td>
<p>rank of <code>D = W%*%t(W)</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The PVD is not an iterative method, but instead relies on <code>n3 + 2</code>separate PCA decompositions. The third mode is for how many images are in the set.
</p>


<h3>Value</h3>

<p>a list containing the following:</p>

<dl>
<dt><code>P</code></dt><dd><p>population-level matrix <code>P = U%*%t(U)</code>, where U is constructed by stacking the truncated left eigenvectors of slicewise PCA along the third mode</p>
</dd>
<dt><code>V</code></dt><dd><p>a list of image-level core matrices</p>
</dd>
<dt><code>D</code></dt><dd><p>population-leve matrix <code>D = W%*%t(W)</code>, where W is constructed by stacking the truncated right eigenvectors of slicewise PCA along the third mode</p>
</dd>
<dt><code>est</code></dt><dd><p>estimate of <code>tnsr</code> after compression</p>
</dd>
<dt><code>norm_percent</code></dt><dd><p>the percent of Frobenius norm explained by the approximation</p>
</dd>
<dt><code>fnorm_resid</code></dt><dd><p>the Frobenius norm of the error <code>fnorm(est-tnsr)</code></p>
</dd>
</dl>



<h3>References</h3>

<p>C. Crainiceanu, B. Caffo, S. Luo, V. Zipunnikov, N. Punjabi, &quot;Population value decomposition: a framework for the analysis of image populations&quot;. Journal of the American Statistical Association, 2013.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### How to retrieve faces_tnsr from figshare
# faces_tnsr &lt;- load_orl()
# subject &lt;- faces_tnsr[,,8,]
dummy_faces_tnsr &lt;- rand_tensor(c(92,112,40,10))
subject &lt;- dummy_faces_tnsr[,,8,]
pvdD &lt;- pvd(subject, uranks=rep(46,10), wranks=rep(56,10), a=46, b=56)
plot(pvdD$fnorm_resid)
</code></pre>

<hr>
<h2 id='rand_tensor'>Tensor with Random Entries</h2><span id='topic+rand_tensor'></span>

<h3>Description</h3>

<p>Generate a Tensor with specified modes with iid normal(0,1) entries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rand_tensor(modes = c(3, 4, 5), drop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rand_tensor_+3A_modes">modes</code></td>
<td>
<p>the modes of the output Tensor</p>
</td></tr>
<tr><td><code id="rand_tensor_+3A_drop">drop</code></td>
<td>
<p>whether or not modes equal to 1 should be dropped</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor object with modes given by <code>modes</code>
</p>


<h3>Note</h3>

<p>Default <code>rand_tensor()</code> generates a 3-Tensor with modes <code>c(3,4,5)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rand_tensor()
rand_tensor(c(4,4,4))
rand_tensor(c(10,2,1),TRUE)
</code></pre>

<hr>
<h2 id='rs_fold'>Row Space Folding of Matrix</h2><span id='topic+rs_fold'></span>

<h3>Description</h3>

<p>DEPRECATED. Please see <code><a href="#topic+k_fold">k_fold</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rs_fold(mat, m = NULL, modes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rs_fold_+3A_mat">mat</code></td>
<td>
<p>matrix to be folded</p>
</td></tr>
<tr><td><code id="rs_fold_+3A_m">m</code></td>
<td>
<p>the mode corresponding to rs_unfold</p>
</td></tr>
<tr><td><code id="rs_fold_+3A_modes">modes</code></td>
<td>
<p>the original modes of the tensor</p>
</td></tr>
</table>

<hr>
<h2 id='rs_unfold-methods'>Tensor Row Space Unfolding</h2><span id='topic+rs_unfold-methods'></span><span id='topic+rs_unfold'></span><span id='topic+rs_unfold+2CTensor-method'></span>

<h3>Description</h3>

<p>DEPRECATED. Please see <code><a href="#topic+k_unfold-methods">k_unfold-methods</a></code> and <code><a href="#topic+unfold-methods">unfold-methods</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rs_unfold(tnsr, m)

## S4 method for signature 'Tensor'
rs_unfold(tnsr, m = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rs_unfold-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor instance</p>
</td></tr>
<tr><td><code id="rs_unfold-methods_+3A_m">m</code></td>
<td>
<p>mode to be unfolded on</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>rs_unfold(tnsr,m=NULL)</code>
</p>

<hr>
<h2 id='show-methods'>Show for Tensor</h2><span id='topic+show-methods'></span><span id='topic+show+2CTensor-method'></span>

<h3>Description</h3>

<p>Extend show for Tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Tensor'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show-methods_+3A_object">object</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>show(object)</code>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+print">print</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
tnsr
</code></pre>

<hr>
<h2 id='t_mult'>Tensor Multiplication (T-MULT)</h2><span id='topic+t_mult'></span>

<h3>Description</h3>

<p>Implements T-MULT based on block circulant matrices (Kilmer et al. 2013) for 3-tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_mult(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_mult_+3A_x">x</code></td>
<td>
<p>a 3-tensor</p>
</td></tr>
<tr><td><code id="t_mult_+3A_y">y</code></td>
<td>
<p>another 3-tensor</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the Fast Fourier Transform (FFT) speed up suggested by Kilmer et al. 2013 instead of explicitly constructing the block circulant matrix. For the mathematical details of T-MULT, see Kilmer et al. (2013).
</p>


<h3>Value</h3>

<p>tensor product between <code>x</code> and <code>y</code>
</p>


<h3>Note</h3>

<p>This only works (so far) between 3-Tensors.
</p>


<h3>References</h3>

<p>M. Kilmer, K. Braman, N. Hao, and R. Hoover, &quot;Third-order tensors as operators on matrices: a theoretical and computational framework with applications in imaging&quot;. SIAM Journal on Matrix Analysis and Applications 2013.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- new("Tensor",3L,c(3L,4L,5L),data=runif(60))
tnsr2 &lt;- new("Tensor",3L,c(4L,3L,5L),data=runif(60))
t_mult(tnsr, tnsr2)
</code></pre>

<hr>
<h2 id='t_svd'>Tensor Singular Value Decomposition</h2><span id='topic+t_svd'></span>

<h3>Description</h3>

<p>TSVD for a 3-Tensor. Constructs 3-Tensors <code>U, S, V</code> such that <code>tnsr = t_mult(t_mult(U,S),t(V))</code>. <code>U</code> and <code>V</code> are orthgonal 3-Tensors with orthogonality defined in Kilmer et al. (2013), and <code>S</code> is a 3-Tensor consists of facewise diagonal matrices. For more details on the TSVD, consult Kilmer et al. (2013).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_svd(tnsr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_svd_+3A_tnsr">tnsr</code></td>
<td>
<p>3-Tensor to decompose via TSVD</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the following:</p>

<dl>
<dt><code>U</code></dt><dd><p>the left orthgonal 3-Tensor</p>
</dd>
<dt><code>V</code></dt><dd><p>the right orthgonal 3-Tensor</p>
</dd>
<dt><code>S</code></dt><dd><p>the middle 3-Tensor consisting of face-wise diagonal matrices</p>
</dd>
</dl>



<h3>Note</h3>

<p>Computation involves complex values, but if the inputs are real, then the outputs are also real. Some loss of precision occurs in the truncation of the imaginary components during the FFT and inverse FFT.
</p>


<h3>References</h3>

<p>M. Kilmer, K. Braman, N. Hao, and R. Hoover, &quot;Third-order tensors as operators on matrices: a theoretical and computational framework with applications in imaging&quot;. SIAM Journal on Matrix Analysis and Applications 2013.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+t_mult">t_mult</a></code>, <code><a href="#topic+t_svd_reconstruct">t_svd_reconstruct</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
tsvdD &lt;- t_svd(tnsr)
</code></pre>

<hr>
<h2 id='t_svd_reconstruct'>Reconstruct Tensor From TSVD</h2><span id='topic+t_svd_reconstruct'></span>

<h3>Description</h3>

<p>Reconstruct the original 3-Tensor after it has been decomposed into <code>U, S, V</code> via <code><a href="#topic+t_svd">t_svd</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_svd_reconstruct(L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_svd_reconstruct_+3A_l">L</code></td>
<td>
<p>list that is an output from <code><a href="#topic+t_svd">t_svd</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 3-Tensor
</p>


<h3>See Also</h3>

<p><code><a href="#topic+t_svd">t_svd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor(c(10,10,10))
tsvdD &lt;- t_svd(tnsr)
1 - fnorm(t_svd_reconstruct(tsvdD)-tnsr)/fnorm(tnsr)
</code></pre>

<hr>
<h2 id='t-methods'>Tensor Transpose</h2><span id='topic+t-methods'></span><span id='topic+t+2CTensor-method'></span>

<h3>Description</h3>

<p>Implements the tensor transpose based on block circulant matrices (Kilmer et al. 2013) for 3-tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Tensor'
t(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t-methods_+3A_x">x</code></td>
<td>
<p>a 3-tensor</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>t(x)</code>
</p>


<h3>Value</h3>

<p>tensor transpose of <code>x</code>
</p>


<h3>References</h3>

<p>M. Kilmer, K. Braman, N. Hao, and R. Hoover, &quot;Third-order tensors as operators on matrices: a theoretical and computational framework with applications in imaging&quot;. SIAM Journal on Matrix Analysis and Applications 2013.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
identical(t(tnsr)@data[,,1],t(tnsr@data[,,1]))
identical(t(tnsr)@data[,,2],t(tnsr@data[,,5]))
identical(t(t(tnsr)),tnsr)
</code></pre>

<hr>
<h2 id='tail-methods'>Tail for Tensor</h2><span id='topic+tail-methods'></span><span id='topic+tail+2CTensor-method'></span>

<h3>Description</h3>

<p>Extend tail for Tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Tensor'
tail(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tail-methods_+3A_x">x</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
<tr><td><code id="tail-methods_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed into tail()</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>tail(x,...)</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+head-methods">head-methods</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
tail(tnsr)
</code></pre>

<hr>
<h2 id='Tensor-class'>S4 Class for a Tensor</h2><span id='topic+Tensor-class'></span><span id='topic+Tensor'></span>

<h3>Description</h3>

<p>An S4 class for a tensor with arbitrary number of modes. The Tensor class extends the base 'array' class to include additional tensor manipulation (folding, unfolding, reshaping, subsetting) as well as a formal class definition that enables more explicit tensor algebra.
</p>


<h3>Details</h3>

<p>This can be seen as a wrapper class to the base <code>array</code> class. While it is possible to create an instance using <code>new</code>, it is also possible to do so by passing the data into <code><a href="#topic+as.tensor">as.tensor</a></code>.
</p>
<p>Each slot of a Tensor instance can be obtained using <code>@</code>.
</p>
<p>The following methods are overloaded for the Tensor class: <code><a href="#topic+dim-methods">dim-methods</a></code>, <code><a href="#topic+head-methods">head-methods</a></code>, <code><a href="#topic+tail-methods">tail-methods</a></code>, <code><a href="#topic+print-methods">print-methods</a></code>, <code><a href="#topic+show-methods">show-methods</a></code>,  element-wise array operations, array subsetting (extract via &lsquo;[&rsquo;), array subset replacing (replace via &lsquo;[&lt;-&rsquo;), and <code><a href="#topic+tperm-methods">tperm-methods</a></code>, which is a wrapper around the base <code>aperm</code> method.
</p>
<p>To sum across any one mode of a tenor, use the function <code><a href="#topic+modeSum-methods">modeSum-methods</a></code>. To compute the mean across any one mode, use <code><a href="#topic+modeMean-methods">modeMean-methods</a></code>.
</p>
<p>You can always unfold any Tensor into a matrix, and the <code><a href="#topic+unfold-methods">unfold-methods</a></code>, <code><a href="#topic+k_unfold-methods">k_unfold-methods</a></code>, and <code><a href="#topic+matvec-methods">matvec-methods</a></code> methods are for that purpose. The output can be kept as a Tensor with 2 modes or a <code>matrix</code> object. The vectorization function is also provided as <code>vec</code>. See the attached vignette for a visualization of the different unfoldings.
</p>
<p>Conversion from <code>array</code>/<code>matrix</code> to Tensor is facilitated via <code><a href="#topic+as.tensor">as.tensor</a></code>. To convert from a Tensor instance, simply invoke <code>@data</code>.
</p>
<p>The Frobenius norm of the Tensor is given by <code><a href="#topic+fnorm-methods">fnorm-methods</a></code>, while the inner product between two Tensors (of equal modes) is given by <code><a href="#topic+innerProd-methods">innerProd-methods</a></code>. You can also sum through any one mode to obtain the K-1 Tensor sum using <code><a href="#topic+modeSum-methods">modeSum-methods</a></code>. <code><a href="#topic+modeMean-methods">modeMean-methods</a></code> provides similar functionality to obtain the K-1 Tensor mean. These are primarily meant to be used internally but may be useful in doing statistics with Tensors.
</p>
<p>For Tensors with 3 modes, we also overloaded <code>t</code> (transpose) defined by Kilmer et.al (2013). See <code><a href="#topic+t-methods">t-methods</a></code>.
</p>
<p>To create a Tensor with i.i.d. random normal(0, 1) entries, see <code><a href="#topic+rand_tensor">rand_tensor</a></code>.

</p>


<h3>Slots</h3>


<dl>
<dt>num_modes</dt><dd><p>number of modes (integer)</p>
</dd>
<dt>modes</dt><dd><p>vector of modes (integer), aka sizes/extents/dimensions</p>
</dd>
<dt>data</dt><dd><p>actual data of the tensor, which can be 'array' or 'vector'</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>[</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>[&lt;-</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>matvec</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>dim</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>fnorm</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>head</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>initialize</dt><dd><p><code>signature(.Object = "Tensor")</code>: ... </p>
</dd>
<dt>innerProd</dt><dd><p><code>signature(tnsr1 = "Tensor", tnsr2 = "Tensor")</code>: ... </p>
</dd>
<dt>modeMean</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>modeSum</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>Ops</dt><dd><p><code>signature(e1 = "array", e2 = "Tensor")</code>: ... </p>
</dd>
<dt>Ops</dt><dd><p><code>signature(e1 = "numeric", e2 = "Tensor")</code>: ... </p>
</dd>
<dt>Ops</dt><dd><p><code>signature(e1 = "Tensor", e2 = "array")</code>: ... </p>
</dd>
<dt>Ops</dt><dd><p><code>signature(e1 = "Tensor", e2 = "numeric")</code>: ... </p>
</dd>
<dt>Ops</dt><dd><p><code>signature(e1 = "Tensor", e2 = "Tensor")</code>: ... </p>
</dd>
<dt>print</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>k_unfold</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>show</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>t</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>tail</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>unfold</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ... </p>
</dd>
<dt>tperm</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ...</p>
</dd>
<dt>image</dt><dd><p><code>signature(tnsr = "Tensor")</code>: ...</p>
</dd>
</dl>



<h3>Note</h3>

<p>All of the decompositions and regression models in this package require a Tensor input.
</p>


<h3>Author(s)</h3>

<p>James Li <a href="mailto:jamesyili@gmail.com">jamesyili@gmail.com</a>
</p>


<h3>References</h3>

<p>James Li, Jacob Bien, Martin T. Wells (2018). rTensor: An R Package for Multidimensional Array (Tensor) Unfolding, Multiplication, and Decomposition. Journal of Statistical Software, 87(10), 1-31. URL http://www.jstatsoft.org/v087/i10/.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.tensor">as.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
class(tnsr)
tnsr
print(tnsr)
dim(tnsr)
tnsr@num_modes
tnsr@data
</code></pre>

<hr>
<h2 id='tperm-methods'>Mode Permutation for Tensor</h2><span id='topic+tperm-methods'></span><span id='topic+tperm'></span><span id='topic+tperm+2CTensor-method'></span>

<h3>Description</h3>

<p>Overloads <code>aperm</code> for Tensor class for convenience.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tperm(tnsr, perm, ...)

## S4 method for signature 'Tensor'
tperm(tnsr, perm, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tperm-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
<tr><td><code id="tperm-methods_+3A_perm">perm</code></td>
<td>
<p>the new permutation of the current modes</p>
</td></tr>
<tr><td><code id="tperm-methods_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed into <code>aperm</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>tperm(tnsr,perm=NULL,...)</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor(c(3,4,5))
dim(tperm(tnsr,perm=c(2,1,3)))
dim(tperm(tnsr,perm=c(1,3,2)))
</code></pre>

<hr>
<h2 id='ttl'>Tensor Times List</h2><span id='topic+ttl'></span>

<h3>Description</h3>

<p>Contracted (m-Mode) product between a Tensor of arbitrary number of modes and a list of matrices. The result is folded back into Tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttl(tnsr, list_mat, ms = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ttl_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor object with K modes</p>
</td></tr>
<tr><td><code id="ttl_+3A_list_mat">list_mat</code></td>
<td>
<p>a list of matrices</p>
</td></tr>
<tr><td><code id="ttl_+3A_ms">ms</code></td>
<td>
<p>a vector of modes to contract on (order should match the order of <code>list_mat</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Performs <code>ttm</code> repeated for a single Tensor and a list of matrices on multiple modes. For instance, suppose we want to do multiply a Tensor object <code>tnsr</code> with three matrices <code>mat1</code>, <code>mat2</code>, <code>mat3</code> on modes 1, 2, and 3. We could do <code>ttm(ttm(ttm(tnsr,mat1,1),mat2,2),3)</code>, or we could do <code>ttl(tnsr,list(mat1,mat2,mat3),c(1,2,3))</code>. The order of the matrices in the list should obviously match the order of the modes. This is a common operation for various Tensor decompositions such as CP and Tucker. For the math on the m-Mode Product, see Kolda and Bader (2009).
</p>


<h3>Value</h3>

<p>Tensor object with K modes
</p>


<h3>Note</h3>

<p>The returned Tensor does not drop any modes equal to 1.
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ttm">ttm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- new("Tensor",3L,c(3L,4L,5L),data=runif(60))
lizt &lt;- list('mat1' = matrix(runif(30),ncol=3), 
'mat2' = matrix(runif(40),ncol=4),
'mat3' = matrix(runif(50),ncol=5))
ttl(tnsr,lizt,ms=c(1,2,3))
</code></pre>

<hr>
<h2 id='ttm'>Tensor Times Matrix (m-Mode Product)</h2><span id='topic+ttm'></span>

<h3>Description</h3>

<p>Contracted (m-Mode) product between a Tensor of arbitrary number of modes and a matrix. The result is folded back into Tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttm(tnsr, mat, m = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ttm_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor object with K modes</p>
</td></tr>
<tr><td><code id="ttm_+3A_mat">mat</code></td>
<td>
<p>input matrix with same number columns as the <code>m</code>th mode of <code>tnsr</code></p>
</td></tr>
<tr><td><code id="ttm_+3A_m">m</code></td>
<td>
<p>the mode to contract on</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By definition, <code>rs_unfold(ttm(tnsr,mat),m) = mat%*%rs_unfold(tnsr,m)</code>, so the number of columns in <code>mat</code> must match the <code>m</code>th mode of <code>tnsr</code>. For the math on the m-Mode Product, see Kolda and Bader (2009).
</p>


<h3>Value</h3>

<p>a Tensor object with K modes
</p>


<h3>Note</h3>

<p>The <code>m</code>th mode of <code>tnsr</code> must match the number of columns in <code>mat</code>. By default, the returned Tensor does not drop any modes equal to 1.
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ttl">ttl</a></code>, <code><a href="#topic+rs_unfold-methods">rs_unfold-methods</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- new("Tensor",3L,c(3L,4L,5L),data=runif(60))
mat &lt;- matrix(runif(50),ncol=5)
ttm(tnsr,mat,m=3)
</code></pre>

<hr>
<h2 id='tucker'>Tucker Decomposition</h2><span id='topic+tucker'></span>

<h3>Description</h3>

<p>The Tucker decomposition of a tensor. Approximates a K-Tensor using a n-mode product of a core tensor (with modes specified by <code>ranks</code>) with orthogonal factor matrices. If there is no truncation in one of the modes, then this is the same as the MPCA, <code><a href="#topic+mpca">mpca</a></code>. If there is no truncation in all the modes (i.e. <code>ranks = tnsr@modes</code>), then this is the same as the HOSVD, <code><a href="#topic+hosvd">hosvd</a></code>. This is an iterative algorithm, with two possible stopping conditions: either relative error in Frobenius norm has gotten below <code>tol</code>, or the <code>max_iter</code> number of iterations has been reached. For more details on the Tucker decomposition, consult Kolda and Bader (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tucker(tnsr, ranks = NULL, max_iter = 25, tol = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tucker_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor with K modes</p>
</td></tr>
<tr><td><code id="tucker_+3A_ranks">ranks</code></td>
<td>
<p>a vector of the modes of the output core Tensor</p>
</td></tr>
<tr><td><code id="tucker_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum number of iterations if error stays above <code>tol</code></p>
</td></tr>
<tr><td><code id="tucker_+3A_tol">tol</code></td>
<td>
<p>relative Frobenius norm error tolerance</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the Alternating Least Squares (ALS) estimation procedure also known as Higher-Order Orthogonal Iteration (HOOI). Intialized using a (Truncated-)HOSVD. A progress bar is included to help monitor operations on large tensors.
</p>


<h3>Value</h3>

<p>a list containing the following:</p>

<dl>
<dt><code>Z</code></dt><dd><p>the core tensor, with modes specified by <code>ranks</code></p>
</dd>
<dt><code>U</code></dt><dd><p>a list of orthgonal factor matrices - one for each mode, with the number of columns of the matrices given by <code>ranks</code></p>
</dd>
<dt><code>conv</code></dt><dd><p>whether or not <code>resid</code> &lt; <code>tol</code> by the last iteration</p>
</dd>
<dt><code>est</code></dt><dd><p>estimate of <code>tnsr</code> after compression</p>
</dd>
<dt><code>norm_percent</code></dt><dd><p>the percent of Frobenius norm explained by the approximation</p>
</dd>
<dt><code>fnorm_resid</code></dt><dd><p>the Frobenius norm of the error <code>fnorm(est-tnsr)</code></p>
</dd>
<dt><code>all_resids</code></dt><dd><p>vector containing the Frobenius norm of error for all the iterations</p>
</dd>
</dl>



<h3>Note</h3>

<p>The length of <code>ranks</code> must match <code>tnsr@num_modes</code>.
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hosvd">hosvd</a></code>, <code><a href="#topic+mpca">mpca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor(c(4,4,4,4))
tuckerD &lt;- tucker(tnsr,ranks=c(2,2,2,2))
tuckerD$conv 
tuckerD$norm_percent
plot(tuckerD$all_resids)
</code></pre>

<hr>
<h2 id='unfold-methods'>Tensor Unfolding</h2><span id='topic+unfold-methods'></span><span id='topic+unfold'></span><span id='topic+unfold+2CTensor-method'></span>

<h3>Description</h3>

<p>Unfolds the tensor into a matrix, with the modes in <code>rs</code> onto the rows and modes in <code>cs</code> onto the columns. Note that <code>c(rs,cs)</code> must have the same elements (order doesn't matter) as <code>x@modes</code>. Within the rows and columns, the order of the unfolding is determined by the order of the modes. This convention is consistent with Kolda and Bader (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unfold(tnsr, row_idx, col_idx)

## S4 method for signature 'Tensor'
unfold(tnsr, row_idx = NULL, col_idx = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unfold-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
<tr><td><code id="unfold-methods_+3A_row_idx">row_idx</code></td>
<td>
<p>the indices of the modes to map onto the row space</p>
</td></tr>
<tr><td><code id="unfold-methods_+3A_col_idx">col_idx</code></td>
<td>
<p>the indices of the modes to map onto the column space</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For Row Space Unfolding or m-mode Unfolding, see <code><a href="#topic+rs_unfold-methods">rs_unfold-methods</a></code>. For Column Space Unfolding or matvec, see <code><a href="#topic+cs_unfold-methods">cs_unfold-methods</a></code>.
</p>
<p><code><a href="#topic+vec-methods">vec-methods</a></code> returns the vectorization of the tensor.
</p>
<p><code>unfold(tnsr,row_idx=NULL,col_idx=NULL)</code>
</p>


<h3>Value</h3>

<p>matrix with <code>prod(row_idx)</code> rows and <code>prod(col_idx)</code> columns
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+k_unfold-methods">k_unfold-methods</a></code> and <code><a href="#topic+matvec-methods">matvec-methods</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
matT3&lt;-unfold(tnsr,row_idx=2,col_idx=c(3,1))
</code></pre>

<hr>
<h2 id='unmatvec'>Unmatvec Folding of Matrix</h2><span id='topic+unmatvec'></span>

<h3>Description</h3>

<p>The inverse operation to <code><a href="#topic+matvec-methods">matvec-methods</a></code>, turning a matrix into a Tensor. For a full account of matrix folding/unfolding operations, consult Kolda and Bader (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unmatvec(mat, modes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unmatvec_+3A_mat">mat</code></td>
<td>
<p>matrix to be folded into a Tensor</p>
</td></tr>
<tr><td><code id="unmatvec_+3A_modes">modes</code></td>
<td>
<p>the modes of the output Tensor</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Tensor object with modes given by <code>modes</code>
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+matvec-methods">matvec-methods</a></code>, <code><a href="#topic+fold">fold</a></code>, <code><a href="#topic+k_fold">k_fold</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- new("Tensor",3L,c(3L,4L,5L),data=runif(60))
matT1&lt;-matvec(tnsr)
identical(unmatvec(matT1,modes=c(3,4,5)),tnsr)
</code></pre>

<hr>
<h2 id='vec-methods'>Tensor Vec</h2><span id='topic+vec-methods'></span><span id='topic+vec'></span><span id='topic+vec+2CTensor-method'></span>

<h3>Description</h3>

<p>Turns the tensor into a single vector, following the convention that earlier indices vary slower than later indices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vec(tnsr)

## S4 method for signature 'Tensor'
vec(tnsr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vec-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>vec(tnsr)</code>
</p>


<h3>Value</h3>

<p>vector with length <code>prod(x@modes)</code>
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor(c(4,5,6,7))
vec(tnsr)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
