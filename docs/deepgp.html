<!DOCTYPE html><html><head><title>Help for package deepgp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {deepgp}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ALC'><p>Active Learning Cohn for Sequential Design</p></a></li>
<li><a href='#continue'><p>Continues MCMC sampling</p></a></li>
<li><a href='#crps'><p>Calculates CRPS</p></a></li>
<li><a href='#deepgp-package'><p>Package deepgp</p></a></li>
<li><a href='#fit_one_layer'><p>MCMC sampling for one layer GP</p></a></li>
<li><a href='#fit_three_layer'><p>MCMC sampling for three layer deep GP</p></a></li>
<li><a href='#fit_two_layer'><p>MCMC sampling for two layer deep GP</p></a></li>
<li><a href='#IMSE'><p>Integrated Mean-Squared (prediction) Error for Sequential Design</p></a></li>
<li><a href='#plot'><p>Plots object from <code>deepgp</code> package</p></a></li>
<li><a href='#predict'><p>Predict posterior mean and variance/covariance</p></a></li>
<li><a href='#rmse'><p>Calculates RMSE</p></a></li>
<li><a href='#score'><p>Calculates score</p></a></li>
<li><a href='#sq_dist'><p>Calculates squared pairwise distances</p></a></li>
<li><a href='#trim'><p>Trim/Thin MCMC iterations</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Deep Gaussian Processes using MCMC</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-04</td>
</tr>
<tr>
<td>Author:</td>
<td>Annie S. Booth &lt;annie_booth@ncsu.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Annie S. Booth &lt;annie_booth@ncsu.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs Bayesian posterior inference for deep Gaussian processes following 
    Sauer, Gramacy, and Higdon (2023, &lt;<a href="https://arxiv.org/abs/2012.08015">arXiv:2012.08015</a>&gt;).  See Sauer (2023,
    <a href="http://hdl.handle.net/10919/114845">http://hdl.handle.net/10919/114845</a>) for comprehensive methodological details and
    <a href="https://bitbucket.org/gramacylab/deepgp-ex/">https://bitbucket.org/gramacylab/deepgp-ex/</a> for a variety of coding examples. 
    Models are trained through
    MCMC including elliptical slice sampling of latent Gaussian layers and Metropolis-Hastings
    sampling of kernel hyperparameters.  Vecchia-approximation for faster computation is implemented
    following Sauer, Cooper, and Gramacy (2022, &lt;<a href="https://arxiv.org/abs/2204.02904">arXiv:2204.02904</a>&gt;).  Downstream tasks include
    sequential design through active learning Cohn/integrated mean squared error (ALC/IMSE; Sauer, 
    Gramacy, and Higdon, 2023), optimization through expected improvement (EI; 
    Gramacy, Sauer, and Wycoff, 2021 &lt;<a href="https://arxiv.org/abs/2112.07457">arXiv:2112.07457</a>&gt;), and contour location through entropy
    (Sauer, 2023).  Models extend up to three layers deep; a one layer model is equivalent to 
    typical Gaussian process regression.  Incorporates OpenMP and SNOW parallelization and 
    utilizes C/C++ under the hood.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Imports:</td>
<td>grDevices, graphics, stats, doParallel, foreach, parallel,
GpGp, Matrix, Rcpp, mvtnorm, FNN</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo,</td>
</tr>
<tr>
<td>Suggests:</td>
<td>interp, knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-07 19:18:44 UTC; anniesauer</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-07 20:50:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='ALC'>Active Learning Cohn for Sequential Design</h2><span id='topic+ALC'></span><span id='topic+ALC.gp'></span><span id='topic+ALC.dgp2'></span><span id='topic+ALC.dgp3'></span>

<h3>Description</h3>

<p>Acts on a <code>gp</code>, <code>dgp2</code>, or <code>dgp3</code> object. 
Current version requires squared exponential covariance 
(<code>cov = "exp2"</code>).  Calculates ALC over the input locations 
<code>x_new</code> using specified reference grid.  If no reference grid is 
specified, <code>x_new</code> is used as the reference.  Optionally utilizes 
SNOW parallelization.  User should 
select the point with the highest ALC to add to the design.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ALC(object, x_new, ref, cores)

## S3 method for class 'gp'
ALC(object, x_new = NULL, ref = NULL, cores = 1)

## S3 method for class 'dgp2'
ALC(object, x_new = NULL, ref = NULL, cores = 1)

## S3 method for class 'dgp3'
ALC(object, x_new = NULL, ref = NULL, cores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ALC_+3A_object">object</code></td>
<td>
<p>object of class <code>gp</code>, <code>dgp2</code>, or <code>dgp3</code></p>
</td></tr>
<tr><td><code id="ALC_+3A_x_new">x_new</code></td>
<td>
<p>matrix of possible input locations, if object has been run 
through <code>predict</code> the previously stored <code>x_new</code> is used</p>
</td></tr>
<tr><td><code id="ALC_+3A_ref">ref</code></td>
<td>
<p>optional reference grid for ALC approximation, if <code>ref = NULL</code> 
then <code>x_new</code> is used</p>
</td></tr>
<tr><td><code id="ALC_+3A_cores">cores</code></td>
<td>
<p>number of cores to utilize in parallel, by default no 
parallelization is used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Not yet implemented for Vecchia-approximated fits.
</p>
<p>All iterations in the object are used in the calculation, so samples 
should be burned-in.  Thinning the samples using <code>trim</code> will
speed up computation.  This function may be used in two ways:
</p>

<ul>
<li><p> Option 1: called on an object with only MCMC iterations, in 
which case <code>x_new</code> must be specified
</p>
</li>
<li><p> Option 2: called on an object that has been predicted over, in 
which case the <code>x_new</code> from <code>predict</code> is used
</p>
</li></ul>

<p>In Option 2,  it is recommended to set <code>store_latent = TRUE</code> for 
<code>dgp2</code> and <code>dgp3</code> objects so
latent mappings do not have to be re-calculated.  Through <code>predict</code>, 
the user may specify a mean mapping (<code>mean_map = TRUE</code>) or a full 
sample from the MVN distribution over <code>w_new</code> 
(<code>mean_map = FALSE</code>).  When the object has not yet been predicted
over (Option 1), the mean mapping is used.
</p>
<p>SNOW parallelization reduces computation time but requires more memory
storage.  C code derived from the &quot;laGP&quot; package (Robert B Gramacy and 
Furong Sun).
</p>


<h3>Value</h3>

<p>list with elements:
</p>

<ul>
<li> <p><code>value</code>: vector of ALC values, indices correspond to <code>x_new</code>
</p>
</li>
<li> <p><code>time</code>: computation time in seconds
</p>
</li></ul>



<h3>References</h3>

<p>Sauer, A., Gramacy, R.B., &amp; Higdon, D. (2023). Active learning for deep 
Gaussian process surrogates. *Technometrics, 65,* 4-18.  arXiv:2012.08015
<br /><br />
Seo, S, M Wallat, T Graepel, and K Obermayer. 2000. Gaussian Process Regression:
Active Data Selection and Test Point Rejection. In Mustererkennung 2000, 
2734. New York, NY: SpringerVerlag.<br /><br />
Gramacy, RB and F Sun. (2016). laGP: Large-Scale Spatial Modeling via Local 
Approximate Gaussian Processes in R. <em>Journal of Statistical Software 
72</em> (1), 1-46. doi:10.18637/jss.v072.i01
</p>


<h3>Examples</h3>

<pre><code class='language-R'># --------------------------------------------------------
# Example 1: toy step function, runs in less than 5 seconds
# --------------------------------------------------------

f &lt;- function(x) {
    if (x &lt;= 0.4) return(-1)
    if (x &gt;= 0.6) return(1)
    if (x &gt; 0.4 &amp; x &lt; 0.6) return(10*(x-0.5))
}

x &lt;- seq(0.05, 0.95, length = 7)
y &lt;- sapply(x, f)
x_new &lt;- seq(0, 1, length = 100)

# Fit model and calculate ALC
fit &lt;- fit_two_layer(x, y, nmcmc = 100, cov = "exp2")
fit &lt;- trim(fit, 50)
fit &lt;- predict(fit, x_new, cores = 1, store_latent = TRUE)
alc &lt;- ALC(fit)


# --------------------------------------------------------
# Example 2: damped sine wave
# --------------------------------------------------------

f &lt;- function(x) {
    exp(-10*x) * (cos(10*pi*x - 1) + sin(10*pi*x - 1)) * 5 - 0.2
}

# Training data
x &lt;- seq(0, 1, length = 30)
y &lt;- f(x) + rnorm(30, 0, 0.05)

# Testing data
xx &lt;- seq(0, 1, length = 100)
yy &lt;- f(xx)

plot(xx, yy, type = "l")
points(x, y, col = 2)

# Conduct MCMC (can replace fit_two_layer with fit_one_layer/fit_three_layer)
fit &lt;- fit_two_layer(x, y, D = 1, nmcmc = 2000, cov = "exp2")
plot(fit)
fit &lt;- trim(fit, 1000, 2)

# Option 1 - calculate ALC from MCMC iterations
alc &lt;- ALC(fit, xx)

# Option 2 - calculate ALC after predictions
fit &lt;- predict(fit, xx, cores = 1, store_latent = TRUE)
alc &lt;- ALC(fit)

# Visualize fit
plot(fit)
par(new = TRUE) # overlay ALC
plot(xx, alc$value, type = 'l', lty = 2, 
     axes = FALSE, xlab = '', ylab = '')

# Select next design point
x_new &lt;- xx[which.max(alc$value)]


</code></pre>

<hr>
<h2 id='continue'>Continues MCMC sampling</h2><span id='topic+continue'></span><span id='topic+continue.gp'></span><span id='topic+continue.dgp2'></span><span id='topic+continue.dgp3'></span><span id='topic+continue.gpvec'></span><span id='topic+continue.dgp2vec'></span><span id='topic+continue.dgp3vec'></span>

<h3>Description</h3>

<p>Acts on a <code>gp</code>, <code>gpvec</code>, <code>dgp2</code>, 
<code>dgp2vec</code>, <code>dgp3</code>, or <code>dgp3vec</code> object.  
Continues MCMC sampling of hyperparameters and hidden layers using 
settings from the original object.  Appends new samples to existing
samples.  When <code>vecchia = TRUE</code>, this function provides the option
to update Vecchia ordering/conditioning sets based on latent layer
warpings through the specification of <code>re_approx = TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>continue(object, new_mcmc, verb, re_approx, ...)

## S3 method for class 'gp'
continue(object, new_mcmc = 1000, verb = TRUE, ...)

## S3 method for class 'dgp2'
continue(object, new_mcmc = 1000, verb = TRUE, ...)

## S3 method for class 'dgp3'
continue(object, new_mcmc = 1000, verb = TRUE, ...)

## S3 method for class 'gpvec'
continue(object, new_mcmc = 1000, verb = TRUE, re_approx = FALSE, ...)

## S3 method for class 'dgp2vec'
continue(object, new_mcmc = 1000, verb = TRUE, re_approx = FALSE, ...)

## S3 method for class 'dgp3vec'
continue(object, new_mcmc = 1000, verb = TRUE, re_approx = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="continue_+3A_object">object</code></td>
<td>
<p>object from <code>fit_one_layer</code>, <code>fit_two_layer</code>, or 
<code>fit_three_layer</code></p>
</td></tr>
<tr><td><code id="continue_+3A_new_mcmc">new_mcmc</code></td>
<td>
<p>number of new MCMC iterations to conduct and append</p>
</td></tr>
<tr><td><code id="continue_+3A_verb">verb</code></td>
<td>
<p>logical indicating whether to print iteration progress</p>
</td></tr>
<tr><td><code id="continue_+3A_re_approx">re_approx</code></td>
<td>
<p>logical indicating whether to re-randomize the ordering 
and update Vecchia nearest-neighbor conditioning sets (only for fits 
with <code>vecchia = TRUE</code>)</p>
</td></tr>
<tr><td><code id="continue_+3A_...">...</code></td>
<td>
<p>N/A</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code>fit_one_layer</code>, <code>fit_two_layer</code>, or 
<code>fit_three_layer</code> for details on MCMC.  The resulting 
object will have <code>nmcmc</code> equal to the previous <code>nmcmc</code> plus 
<code>new_mcmc</code>.  It is recommended to start an MCMC fit then 
investigate trace plots to assess burn-in.  The primary use of this 
function is to gather more MCMC iterations in order to obtain burned-in 
samples.
</p>
<p>Specifying <code>re_approx = TRUE</code> updates random orderings and 
nearest-neighbor conditioning sets (only for <code>vecchia = TRUE</code> 
fits).  In one-layer, there is no latent warping but the Vecchia 
approximation is still re-randomized and nearest-neighbors are adjusted 
accordingly.  In two- and three-layers, the latest samples of hidden 
layers are used to update nearest-neighbors.  If you update the 
Vecchia approximation, you should later remove previous samples 
(updating the approximation effectively starts a new chain).  When 
<code>re_approx = FALSE</code> the previous orderings and conditioning sets 
are used (maintaining the continuity of the previous chain).
</p>


<h3>Value</h3>

<p>object of the same class with the new iterations appended
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See "fit_two_layer" for an example

</code></pre>

<hr>
<h2 id='crps'>Calculates CRPS</h2><span id='topic+crps'></span>

<h3>Description</h3>

<p>Calculates continuous ranked probability score (lower CRPS indicate
better fits, better uncertainty quantification).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crps(y, mu, s2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crps_+3A_y">y</code></td>
<td>
<p>response vector</p>
</td></tr>
<tr><td><code id="crps_+3A_mu">mu</code></td>
<td>
<p>predicted mean</p>
</td></tr>
<tr><td><code id="crps_+3A_s2">s2</code></td>
<td>
<p>predicted point-wise variances</p>
</td></tr>
</table>


<h3>References</h3>

<p>Gneiting, T, and AE Raftery. 2007. Strictly Proper Scoring Rules, Prediction, 
and Estimation. <em>Journal of the American Statistical Association 102</em> 
(477), 359-378.
</p>

<hr>
<h2 id='deepgp-package'>Package deepgp</h2><span id='topic+deepgp-package'></span>

<h3>Description</h3>

<p>Performs Bayesian posterior inference for deep Gaussian processes following 
Sauer, Gramacy, and Higdon (2023, &lt;arXiv:2012.08015&gt;).  See Sauer (2023, 
&lt;http://hdl.handle.net/10919/114845&gt;) for comprehensive methodological details and 
&lt;https://bitbucket.org/gramacylab/deepgp-ex/&gt; for a variety of coding examples. 
Models are trained through MCMC including elliptical slice sampling of latent 
Gaussian layers and Metropolis-Hastings sampling of kernel hyperparameters.  
Vecchia-approximation for faster computation is implemented following 
Sauer, Cooper, and Gramacy (2022, &lt;arXiv:2204.02904&gt;).  Downstream tasks 
include sequential design through active learning Cohn/integrated mean squared 
error (ALC/IMSE; Sauer, Gramacy, and Higdon, 2023), optimization through 
expected improvement (EI; Gramacy, Sauer, and Wycoff, 2021 &lt;arXiv:2112.07457&gt;), 
and contour location through entrop(Sauer, 2023).  Models extend up to three 
layers deep; a one layer model is equivalent to typical Gaussian process 
regression.  Incorporates OpenMP and SNOW parallelization and utilizes 
C/C++ under the hood.
</p>


<h3>Important Functions</h3>


<ul>
<li> <p><code><a href="#topic+fit_one_layer">fit_one_layer</a></code>: conducts MCMC sampling of 
hyperparameters for a one layer GP
</p>
</li>
<li> <p><code><a href="#topic+fit_two_layer">fit_two_layer</a></code>: conducts MCMC sampling of 
hyperparameters and hidden layer for a two layer deep GP
</p>
</li>
<li> <p><code><a href="#topic+fit_three_layer">fit_three_layer</a></code>: conducts MCMC sampling of 
hyperparameters and hidden layers for a three layer deep GP
</p>
</li>
<li> <p><code><a href="#topic+continue">continue</a></code>: collects additional MCMC samples
</p>
</li>
<li> <p><code><a href="#topic+trim">trim</a></code>: cuts off burn-in and optionally thins 
samples
</p>
</li>
<li> <p><code><a href="#topic+predict">predict</a></code>: calculates posterior mean and 
variance over a set of input locations (optionally calculates EI or entropy)
</p>
</li>
<li> <p><code><a href="#topic+plot">plot</a></code>: produces trace plots, hidden layer 
plots, and posterior predictive plots
</p>
</li>
<li> <p><code><a href="#topic+ALC">ALC</a></code>: calculates active learning Cohn over 
set of input locations using reference grid
</p>
</li>
<li> <p><code><a href="#topic+IMSE">IMSE</a></code>: calculates integrated mean-squared error
over set of input locations
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Annie S. Booth <a href="mailto:annie_booth@ncsu.edu">annie_booth@ncsu.edu</a>
</p>


<h3>References</h3>

<p>Sauer, A. (2023). Deep Gaussian process surrogates for computer experiments. 
*Ph.D. Dissertation, Department of Statistics, Virginia Polytechnic Institute and State University.*
<br /><br />
Sauer, A., Gramacy, R.B., &amp; Higdon, D. (2023). Active learning for deep 
Gaussian process surrogates. *Technometrics, 65,* 4-18.  arXiv:2012.08015
<br /><br />
Sauer, A., Cooper, A., &amp; Gramacy, R. B. (2022). Vecchia-approximated deep Gaussian 
processes for computer experiments. 
*Journal of Computational and Graphical Statistics,* 1-14.  arXiv:2204.02904
<br /><br />
Gramacy, R. B., Sauer, A. &amp; Wycoff, N. (2022). Triangulation candidates for Bayesian 
optimization.  *Advances in Neural Information Processing Systems (NeurIPS), 35,* 
35933-35945.  arXiv:2112.07457
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See "fit_one_layer", "fit_two_layer", "fit_three_layer", 
# "ALC", or "IMSE" for examples
# Examples of real-world implementations are available at: 
# https://bitbucket.org/gramacylab/deepgp-ex/

</code></pre>

<hr>
<h2 id='fit_one_layer'>MCMC sampling for one layer GP</h2><span id='topic+fit_one_layer'></span>

<h3>Description</h3>

<p>Conducts MCMC sampling of hyperparameters for a one layer 
GP.  Length scale parameter <code>theta</code> governs 
the strength of the correlation and nugget parameter <code>g</code> 
governs noise.  In Matern covariance, <code>v</code> governs smoothness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_one_layer(
  x,
  y,
  nmcmc = 10000,
  sep = FALSE,
  verb = TRUE,
  g_0 = 0.01,
  theta_0 = 0.1,
  true_g = NULL,
  settings = NULL,
  cov = c("matern", "exp2"),
  v = 2.5,
  vecchia = FALSE,
  m = min(25, length(y) - 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_one_layer_+3A_x">x</code></td>
<td>
<p>vector or matrix of input locations</p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_y">y</code></td>
<td>
<p>vector of response values</p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_nmcmc">nmcmc</code></td>
<td>
<p>number of MCMC iterations</p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_sep">sep</code></td>
<td>
<p>logical indicating whether to use separable (<code>sep = TRUE</code>)
or isotropic (<code>sep = FALSE</code>) lengthscales</p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_verb">verb</code></td>
<td>
<p>logical indicating whether to print iteration progress</p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_g_0">g_0</code></td>
<td>
<p>initial value for <code>g</code></p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_theta_0">theta_0</code></td>
<td>
<p>initial value for <code>theta</code></p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_true_g">true_g</code></td>
<td>
<p>if true nugget is known it may be specified here (set to a 
small value to make fit deterministic).  Note - values that are too 
small may cause numerical issues in matrix inversions.</p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_settings">settings</code></td>
<td>
<p>hyperparameters for proposals and priors (see details)</p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_cov">cov</code></td>
<td>
<p>covariance kernel, either Matern or squared exponential 
(<code>"exp2"</code>)</p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_v">v</code></td>
<td>
<p>Matern smoothness parameter (only used if <code>cov = "matern"</code>)</p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_vecchia">vecchia</code></td>
<td>
<p>logical indicating whether to use Vecchia approximation</p>
</td></tr>
<tr><td><code id="fit_one_layer_+3A_m">m</code></td>
<td>
<p>size of Vecchia conditioning sets (only used if 
<code>vecchia = TRUE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Utilizes Metropolis Hastings sampling of the length scale and
nugget parameters with proposals and priors controlled by 
<code>settings</code>.  When <code>true_g</code> is set to a specific value, the 
nugget is not estimated.  When <code>vecchia = TRUE</code>, all calculations 
leverage the Vecchia approximation with specified conditioning set size 
<code>m</code>.  Vecchia approximation is only implemented for 
<code>cov = "matern"</code>.
</p>
<p>NOTE on OpenMP: The Vecchia implementation relies on OpenMP parallelization
for efficient computation.  This function will produce a warning message 
if the package was installed without OpenMP (this is the default for 
CRAN packages installed on Apple machines).  To set up OpenMP 
parallelization, download the package source code and install 
using the gcc/g++ compiler.  
</p>
<p>Proposals for <code>g</code> and <code>theta</code> follow a uniform sliding window 
scheme, e.g. 
</p>
<p><code>g_star &lt;- runif(1, l * g_t / u, u * g_t / l)</code>, 
</p>
<p>with defaults <code>l = 1</code> and <code>u = 2</code> provided in <code>settings</code>.
To adjust these, set <code>settings = list(l = new_l, u = new_u)</code>.
Priors on <code>g</code> and <code>theta</code> follow Gamma distributions with 
shape parameters (<code>alpha</code>) and rate parameters (<code>beta</code>) 
controlled within the <code>settings</code> list object.  Defaults are
</p>

<ul>
<li> <p><code>settings$alpha$g &lt;- 1.5</code>
</p>
</li>
<li> <p><code>settings$beta$g &lt;- 3.9</code>
</p>
</li>
<li> <p><code>settings$alpha$theta &lt;- 1.5</code>
</p>
</li>
<li> <p><code>settings$beta$theta &lt;- 3.9 / 1.5</code>
</p>
</li></ul>

<p>These priors are designed for <code>x</code> scaled 
to [0, 1] and <code>y</code> scaled to have mean 0 and variance 1.  These may
be adjusted using the <code>settings</code> input.
</p>
<p>The output object of class <code>gp</code> is designed for use with 
<code>continue</code>, <code>trim</code>, and <code>predict</code>.
</p>


<h3>Value</h3>

<p>a list of the S3 class <code>gp</code> or <code>gpvec</code> with elements:
</p>

<ul>
<li> <p><code>x</code>: copy of input matrix
</p>
</li>
<li> <p><code>y</code>: copy of response vector
</p>
</li>
<li> <p><code>nmcmc</code>: number of MCMC iterations
</p>
</li>
<li> <p><code>settings</code>: copy of proposal/prior settings
</p>
</li>
<li> <p><code>v</code>: copy of Matern smoothness parameter (<code>v = 999</code> 
indicates <code>cov = "exp2"</code>)
</p>
</li>
<li> <p><code>g</code>: vector of MCMC samples for <code>g</code>
</p>
</li>
<li> <p><code>theta</code>: vector of MCMC samples for <code>theta</code>
</p>
</li>
<li> <p><code>tau2</code>: vector of MLE estimates for <code>tau2</code> 
(scale parameter)
</p>
</li>
<li> <p><code>ll</code>: vector of MVN log likelihood for each Gibbs iteration
</p>
</li>
<li> <p><code>time</code>: computation time in seconds
</p>
</li></ul>



<h3>References</h3>

<p>Sauer, A. (2023). Deep Gaussian process surrogates for computer experiments. 
*Ph.D. Dissertation, Department of Statistics, Virginia Polytechnic Institute and State University.*
<br /><br />
Sauer, A., Gramacy, R.B., &amp; Higdon, D. (2023). Active learning for deep 
Gaussian process surrogates. *Technometrics, 65,* 4-18.  arXiv:2012.08015
<br /><br />
Sauer, A., Cooper, A., &amp; Gramacy, R. B. (2022). Vecchia-approximated deep 
Gaussian processes for computer experiments. 
*Journal of Computational and Graphical Statistics,* 1-14.  arXiv:2204.02904
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Examples of real-world implementations are available at: 
# https://bitbucket.org/gramacylab/deepgp-ex/

# G function (https://www.sfu.ca/~ssurjano/gfunc.html)
f &lt;- function(xx, a = (c(1:length(xx)) - 1) / 2) { 
    new1 &lt;- abs(4 * xx - 2) + a
    new2 &lt;- 1 + a
    prod &lt;- prod(new1 / new2)
    return((prod - 1) / 0.86)
}

# Training data
d &lt;- 1 
n &lt;- 20
x &lt;- matrix(runif(n * d), ncol = d)
y &lt;- apply(x, 1, f)

# Testing data
n_test &lt;- 100
xx &lt;- matrix(runif(n_test * d), ncol = d)
yy &lt;- apply(xx, 1, f)

plot(xx[order(xx)], yy[order(xx)], type = "l")
points(x, y, col = 2)

# Example 1: full model (nugget fixed)
fit &lt;- fit_one_layer(x, y, nmcmc = 2000, true_g = 1e-6)
plot(fit)
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1)
plot(fit)

# Example 2: full model (nugget estimated, EI calculated)
fit &lt;- fit_one_layer(x, y, nmcmc = 2000)
plot(fit) 
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1, EI = TRUE)
plot(fit)
par(new = TRUE) # overlay EI
plot(xx[order(xx)], fit$EI[order(xx)], type = 'l', lty = 2, 
      axes = FALSE, xlab = '', ylab = '')
      
# Example 3: Vecchia approximated model
fit &lt;- fit_one_layer(x, y, nmcmc = 2000, vecchia = TRUE, m = 10) 
plot(fit)
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1)
plot(fit)


</code></pre>

<hr>
<h2 id='fit_three_layer'>MCMC sampling for three layer deep GP</h2><span id='topic+fit_three_layer'></span>

<h3>Description</h3>

<p>Conducts MCMC sampling of hyperparameters, hidden layer 
<code>z</code>, and hidden layer <code>w</code> for a three layer deep GP.  
Separate length scale parameters <code>theta_z</code>, 
<code>theta_w</code>, and <code>theta_y</code> govern the correlation 
strength of the inner layer, middle layer, and outer layer respectively.  
Nugget parameter <code>g</code> governs noise on the outer layer.  In Matern 
covariance, <code>v</code> governs smoothness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_three_layer(
  x,
  y,
  nmcmc = 10000,
  D = ifelse(is.matrix(x), ncol(x), 1),
  verb = TRUE,
  w_0 = NULL,
  z_0 = NULL,
  g_0 = 0.01,
  theta_y_0 = 0.1,
  theta_w_0 = 0.1,
  theta_z_0 = 0.1,
  true_g = NULL,
  settings = NULL,
  cov = c("matern", "exp2"),
  v = 2.5,
  vecchia = FALSE,
  m = min(25, length(y) - 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_three_layer_+3A_x">x</code></td>
<td>
<p>vector or matrix of input locations</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_y">y</code></td>
<td>
<p>vector of response values</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_nmcmc">nmcmc</code></td>
<td>
<p>number of MCMC iterations</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_d">D</code></td>
<td>
<p>integer designating dimension of hidden layers, defaults to 
dimension of <code>x</code></p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_verb">verb</code></td>
<td>
<p>logical indicating whether to print iteration progress</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_w_0">w_0</code></td>
<td>
<p>initial value for hidden layer <code>w</code> (must be matrix 
of dimension <code>nrow(x)</code> by <code>D</code> or  dimension 
<code>nrow(x) - 1</code> by <code>D</code>).  Defaults to the identity mapping.</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_z_0">z_0</code></td>
<td>
<p>initial value for hidden layer <code>z</code> (must be matrix 
of dimension <code>nrow(x)</code> by <code>D</code> or  dimension 
<code>nrow(x) - 1</code> by <code>D</code>).  Defaults to the identity mapping.</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_g_0">g_0</code></td>
<td>
<p>initial value for <code>g</code></p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_theta_y_0">theta_y_0</code></td>
<td>
<p>initial value for <code>theta_y</code> (length scale of outer 
layer)</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_theta_w_0">theta_w_0</code></td>
<td>
<p>initial value for <code>theta_w</code> (length scale of middle 
layer), may be single value or vector of length <code>D</code></p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_theta_z_0">theta_z_0</code></td>
<td>
<p>initial value for <code>theta_z</code> (length scale of inner 
layer), may be single value or vector of length <code>D</code></p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_true_g">true_g</code></td>
<td>
<p>if true nugget is known it may be specified here (set to a 
small value to make fit deterministic).  Note - values that are too 
small may cause numerical issues in matrix inversions.</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_settings">settings</code></td>
<td>
<p>hyperparameters for proposals and priors (see details)</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_cov">cov</code></td>
<td>
<p>covariance kernel, either Matern or squared exponential 
(<code>"exp2"</code>)</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_v">v</code></td>
<td>
<p>Matern smoothness parameter (only used if <code>cov = "matern"</code>)</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_vecchia">vecchia</code></td>
<td>
<p>logical indicating whether to use Vecchia approximation</p>
</td></tr>
<tr><td><code id="fit_three_layer_+3A_m">m</code></td>
<td>
<p>size of Vecchia conditioning sets (only used if 
<code>vecchia = TRUE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>pmx = TRUE</code> option not yet implemented for three-layer DGP.
</p>
<p>Maps inputs <code>x</code> through hidden layer <code>z</code> then hidden
layer <code>w</code> to outputs <code>y</code>.  Conducts sampling of the hidden 
layers using Elliptical Slice sampling.  Utilizes Metropolis Hastings 
sampling of the length scale and nugget parameters with proposals and 
priors controlled by <code>settings</code>.  When <code>true_g</code> is set to a 
specific value, the nugget is not estimated.  When 
<code>vecchia = TRUE</code>, all calculations leverage the Vecchia 
approximation with specified conditioning set size <code>m</code>.  Vecchia 
approximation is only implemented for <code>cov = "matern"</code>.
</p>
<p>NOTE on OpenMP: The Vecchia implementation relies on OpenMP parallelization
for efficient computation.  This function will produce a warning message 
if the package was installed without OpenMP (this is the default for 
CRAN packages installed on Apple machines).  To set up OpenMP 
parallelization, download the package source code and install 
using the gcc/g++ compiler.  
</p>
<p>Proposals for <code>g</code>, 
<code>theta_y</code>, <code>theta_w</code>, and <code>theta_z</code> follow a uniform 
sliding window scheme, e.g.
</p>
<p><code>g_star &lt;- runif(1, l * g_t / u, u * g_t / l)</code>,
</p>
<p>with defaults <code>l = 1</code> and <code>u = 2</code> provided in <code>settings</code>.
To adjust these, set <code>settings = list(l = new_l, u = new_u)</code>.  
Priors on <code>g</code>, <code>theta_y</code>, <code>theta_w</code>, and <code>theta_z</code> 
follow Gamma distributions with shape parameters (<code>alpha</code>) and rate 
parameters (<code>beta</code>) controlled within the <code>settings</code> list 
object.  Defaults are 
</p>

<ul>
<li> <p><code>settings$alpha$g &lt;- 1.5</code>
</p>
</li>
<li> <p><code>settings$beta$g &lt;- 3.9</code>
</p>
</li>
<li> <p><code>settings$alpha$theta_z &lt;- 1.5</code>
</p>
</li>
<li> <p><code>settings$beta$theta_z &lt;- 3.9 / 4</code>
</p>
</li>
<li> <p><code>settings$alpha$theta_w &lt;- 1.5</code>
</p>
</li>
<li> <p><code>settings$beta$theta_w &lt;- 3.9 / 12</code>
</p>
</li>
<li> <p><code>settings$alpha$theta_y &lt;- 1.5</code>
</p>
</li>
<li> <p><code>settings$beta$theta_y &lt;- 3.9 / 6</code>
</p>
</li></ul>

<p>These priors are designed for <code>x</code> scaled to [0, 1] and <code>y</code> 
scaled to have mean 0 and variance 1.  These may be adjusted using the 
<code>settings</code> input.
</p>
<p>In the current version, the three-layer does not have any equivalent
setting for <code>pmx = TRUE</code> as in <code>fit_two_layer</code>.
</p>
<p>When <code>w_0 = NULL</code> and/or <code>z_0 = NULL</code>, the hidden layers are 
initialized at <code>x</code> (i.e. the identity mapping).  The default prior 
mean of the inner hidden layer <code>z</code> is zero, but may be adjusted to <code>x</code> 
using <code>settings = list(z_prior_mean = x)</code>.  The prior mean of the
middle hidden layer <code>w</code> is set at zero is is not user adjustable.
If <code>w_0</code> and/or <code>z_0</code> is of dimension <code>nrow(x) - 1</code> by 
<code>D</code>, the final row is predicted using kriging. This is helpful in 
sequential design when adding a new input location and starting the MCMC 
at the place where the previous MCMC left off.
</p>
<p>The output object of class <code>dgp3</code> or <code>dgp3vec</code> is designed for 
use with <code>continue</code>, <code>trim</code>, and <code>predict</code>.
</p>


<h3>Value</h3>

<p>a list of the S3 class <code>dgp3</code> or <code>dgp3vec</code> with elements:
</p>

<ul>
<li> <p><code>x</code>: copy of input matrix
</p>
</li>
<li> <p><code>y</code>: copy of response vector
</p>
</li>
<li> <p><code>nmcmc</code>: number of MCMC iterations
</p>
</li>
<li> <p><code>settings</code>: copy of proposal/prior settings
</p>
</li>
<li> <p><code>v</code>: copy of Matern smoothness parameter (<code>v = 999</code> 
indicates <code>cov = "exp2"</code>) 
</p>
</li>
<li> <p><code>g</code>: vector of MCMC samples for <code>g</code>
</p>
</li>
<li> <p><code>theta_y</code>: vector of MCMC samples for <code>theta_y</code> (length 
scale of outer layer)
</p>
</li>
<li> <p><code>theta_w</code>: matrix of MCMC samples for <code>theta_w</code> (length 
scale of middle layer)
</p>
</li>
<li> <p><code>theta_z</code>: matrix of MCMC samples for <code>theta_z</code> (length 
scale of inner layer)
</p>
</li>
<li> <p><code>tau2</code>: vector of MLE estimates for <code>tau2</code> (scale 
parameter of outer layer)
</p>
</li>
<li> <p><code>w</code>: list of MCMC samples for middle hidden layer <code>w</code>
</p>
</li>
<li> <p><code>z</code>: list of MCMC samples for inner hidden layer <code>z</code>
</p>
</li>
<li> <p><code>ll</code>: vector of MVN log likelihood of the outer layer 
for reach Gibbs iteration
</p>
</li>
<li> <p><code>time</code>: computation time in seconds
</p>
</li></ul>



<h3>References</h3>

<p>Sauer, A. (2023). Deep Gaussian process surrogates for computer experiments. 
*Ph.D. Dissertation, Department of Statistics, Virginia Polytechnic Institute and State University.*
<br /><br />
Sauer, A., Gramacy, R.B., &amp; Higdon, D. (2023). Active learning for deep 
Gaussian process surrogates. *Technometrics, 65,* 4-18.  arXiv:2012.08015
<br /><br />
Sauer, A., Cooper, A., &amp; Gramacy, R. B. (2022). Vecchia-approximated deep 
Gaussian processes for computer experiments. 
*Journal of Computational and Graphical Statistics,* 1-14.  arXiv:2204.02904
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Examples of real-world implementations are available at: 
# https://bitbucket.org/gramacylab/deepgp-ex/

# G function (https://www.sfu.ca/~ssurjano/gfunc.html)
f &lt;- function(xx, a = (c(1:length(xx)) - 1) / 2) { 
    new1 &lt;- abs(4 * xx - 2) + a
    new2 &lt;- 1 + a
    prod &lt;- prod(new1 / new2)
    return((prod - 1) / 0.86)
}

# Training data
d &lt;- 2
n &lt;- 30
x &lt;- matrix(runif(n * d), ncol = d)
y &lt;- apply(x, 1, f)

# Testing data
n_test &lt;- 100
xx &lt;- matrix(runif(n_test * d), ncol = d)
yy &lt;- apply(xx, 1, f)

i &lt;- interp::interp(xx[, 1], xx[, 2], yy)
image(i, col = heat.colors(128))
contour(i, add = TRUE)
points(x)

# Example 1: full model (nugget estimated)
fit &lt;- fit_three_layer(x, y, nmcmc = 2000)
plot(fit)
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1)
plot(fit)

# Example 2: Vecchia approximated model (nugget fixed)
# (Vecchia approximation is faster for larger data sizes)
fit &lt;- fit_three_layer(x, y, nmcmc = 2000, vecchia = TRUE, 
                       m = 10, true_g = 1e-6)
plot(fit) 
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1)
plot(fit)


</code></pre>

<hr>
<h2 id='fit_two_layer'>MCMC sampling for two layer deep GP</h2><span id='topic+fit_two_layer'></span>

<h3>Description</h3>

<p>Conducts MCMC sampling of hyperparameters and hidden layer 
<code>w</code> for a two layer deep GP.  Separate length scale 
parameters <code>theta_w</code> and <code>theta_y</code> govern the correlation 
strength of the hidden layer and outer layer respectively.  Nugget 
parameter <code>g</code> governs noise on the outer layer.  In Matern 
covariance, <code>v</code> governs smoothness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_two_layer(
  x,
  y,
  nmcmc = 10000,
  D = ifelse(is.matrix(x), ncol(x), 1),
  pmx = FALSE,
  verb = TRUE,
  w_0 = NULL,
  g_0 = 0.01,
  theta_y_0 = 0.1,
  theta_w_0 = 0.1,
  true_g = NULL,
  settings = NULL,
  cov = c("matern", "exp2"),
  v = 2.5,
  vecchia = FALSE,
  m = min(25, length(y) - 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_two_layer_+3A_x">x</code></td>
<td>
<p>vector or matrix of input locations</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_y">y</code></td>
<td>
<p>vector of response values</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_nmcmc">nmcmc</code></td>
<td>
<p>number of MCMC iterations</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_d">D</code></td>
<td>
<p>integer designating dimension of hidden layer, defaults to 
dimension of <code>x</code></p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_pmx">pmx</code></td>
<td>
<p>&quot;prior mean X&quot;, logical indicating whether W should have prior
mean of X (<code>TRUE</code>, requires <code>D = ncol(X)</code>) or prior 
mean zero (<code>FALSE</code>).  <code>pmx = TRUE</code> is recommended for
higher dimensions.  May be numeric, in which case the specified
argument is used as the scale (<code>tau2</code>) in the latent <code>w</code>
layer (default is 1).  Small values encourage identity mappings.</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_verb">verb</code></td>
<td>
<p>logical indicating whether to print iteration progress</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_w_0">w_0</code></td>
<td>
<p>initial value for hidden layer <code>w</code> (must be matrix 
of dimension <code>nrow(x)</code> by <code>D</code> or  dimension 
<code>nrow(x) - 1</code> by <code>D</code>).  Defaults to the identity mapping.</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_g_0">g_0</code></td>
<td>
<p>initial value for <code>g</code></p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_theta_y_0">theta_y_0</code></td>
<td>
<p>initial value for <code>theta_y</code> (length scale of outer 
layer)</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_theta_w_0">theta_w_0</code></td>
<td>
<p>initial value for <code>theta_w</code> (length scale of inner 
layer), may be single value or vector of length <code>D</code></p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_true_g">true_g</code></td>
<td>
<p>if true nugget is known it may be specified here (set to a 
small value to make fit deterministic).  Note - values that are too 
small may cause numerical issues in matrix inversions.</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_settings">settings</code></td>
<td>
<p>hyperparameters for proposals and priors (see details)</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_cov">cov</code></td>
<td>
<p>covariance kernel, either Matern or squared exponential 
(<code>"exp2"</code>)</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_v">v</code></td>
<td>
<p>Matern smoothness parameter (only used if <code>cov = "matern"</code>)</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_vecchia">vecchia</code></td>
<td>
<p>logical indicating whether to use Vecchia approximation</p>
</td></tr>
<tr><td><code id="fit_two_layer_+3A_m">m</code></td>
<td>
<p>size of Vecchia conditioning sets (only used if 
<code>vecchia = TRUE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Maps inputs <code>x</code> through hidden layer <code>w</code> to outputs 
<code>y</code>.  Conducts sampling of the hidden layer using Elliptical 
Slice sampling.  Utilizes Metropolis Hastings sampling of the length 
scale and nugget parameters with proposals and priors controlled by 
<code>settings</code>.  When <code>true_g</code> is set to a specific value, the 
nugget is not estimated.  When <code>vecchia = TRUE</code>, all calculations
leverage the Vecchia approximation with specified conditioning set size
<code>m</code>.  Vecchia approximation is only implemented for 
<code>cov = "matern"</code>.
</p>
<p>NOTE on OpenMP: The Vecchia implementation relies on OpenMP parallelization
for efficient computation.  This function will produce a warning message 
if the package was installed without OpenMP (this is the default for 
CRAN packages installed on Apple machines).  To set up OpenMP 
parallelization, download the package source code and install 
using the gcc/g++ compiler.  
</p>
<p>Proposals for <code>g</code>, <code>theta_y</code>, and 
<code>theta_w</code> follow a uniform sliding window scheme, e.g.
</p>
<p><code>g_star &lt;- runif(1, l * g_t / u, u * g_t / l)</code>, 
</p>
<p>with defaults <code>l = 1</code> and <code>u = 2</code> provided in <code>settings</code>.
To adjust these, set <code>settings = list(l = new_l, u = new_u)</code>.    
Priors on <code>g</code>, <code>theta_y</code>, and <code>theta_w</code> follow Gamma 
distributions with shape parameters (<code>alpha</code>) and rate parameters 
(<code>beta</code>) controlled within the <code>settings</code> list object.  
Defaults are
</p>

<ul>
<li> <p><code>settings$alpha$g &lt;- 1.5</code>
</p>
</li>
<li> <p><code>settings$beta$g &lt;- 3.9</code>
</p>
</li>
<li> <p><code>settings$alpha$theta_w &lt;- 1.5</code>
</p>
</li>
<li> <p><code>settings$beta$theta_w &lt;- 3.9 / 4</code>
</p>
</li>
<li> <p><code>settings$alpha$theta_y &lt;- 1.5</code>
</p>
</li>
<li> <p><code>settings$beta$theta_y &lt;- 3.9 / 6</code>
</p>
</li></ul>

<p>These priors are designed for <code>x</code> scaled to 
[0, 1] and <code>y</code> scaled to have mean 0 and variance 1.  These may be 
adjusted using the <code>settings</code> input.
</p>
<p>When <code>w_0 = NULL</code>, the hidden layer is initialized at <code>x</code> 
(i.e. the identity mapping).  If <code>w_0</code> is of dimension 
<code>nrow(x) - 1</code> by <code>D</code>, the final row is predicted using kriging. 
This is helpful in sequential design when adding a new input location 
and starting the MCMC at the place where the previous MCMC left off.
</p>
<p>The output object of class <code>dgp2</code> or <code>dgp2vec</code> is designed for 
use with <code>continue</code>, <code>trim</code>, and <code>predict</code>.
</p>


<h3>Value</h3>

<p>a list of the S3 class <code>dgp2</code> or <code>dgp2vec</code> with elements:
</p>

<ul>
<li> <p><code>x</code>: copy of input matrix
</p>
</li>
<li> <p><code>y</code>: copy of response vector
</p>
</li>
<li> <p><code>nmcmc</code>: number of MCMC iterations
</p>
</li>
<li> <p><code>settings</code>: copy of proposal/prior settings
</p>
</li>
<li> <p><code>v</code>: copy of Matern smoothness parameter (<code>v = 999</code> 
indicates <code>cov = "exp2"</code>) 
</p>
</li>
<li> <p><code>g</code>: vector of MCMC samples for <code>g</code>
</p>
</li>
<li> <p><code>theta_y</code>: vector of MCMC samples for <code>theta_y</code> (length
scale of outer layer)
</p>
</li>
<li> <p><code>theta_w</code>: matrix of MCMC samples for <code>theta_w</code> (length 
scale of inner layer)
</p>
</li>
<li> <p><code>tau2</code>: vector of MLE estimates for <code>tau2</code> (scale 
parameter of outer layer)
</p>
</li>
<li> <p><code>w</code>: list of MCMC samples for hidden layer <code>w</code>
</p>
</li>
<li> <p><code>ll</code>: vector of MVN log likelihood of the outer layer 
for reach Gibbs iteration
</p>
</li>
<li> <p><code>time</code>: computation time in seconds
</p>
</li></ul>



<h3>References</h3>

<p>Sauer, A. (2023). Deep Gaussian process surrogates for computer experiments. 
*Ph.D. Dissertation, Department of Statistics, Virginia Polytechnic Institute and State University.*
<br /><br />
Sauer, A., Gramacy, R.B., &amp; Higdon, D. (2023). Active learning for deep 
Gaussian process surrogates. *Technometrics, 65,* 4-18.  arXiv:2012.08015
<br /><br />
Sauer, A., Cooper, A., &amp; Gramacy, R. B. (2022). Vecchia-approximated deep 
Gaussian processes for computer experiments. 
*Journal of Computational and Graphical Statistics,* 1-14.  arXiv:2204.02904
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Examples of real-world implementations are available at: 
# https://bitbucket.org/gramacylab/deepgp-ex/

# G function (https://www.sfu.ca/~ssurjano/gfunc.html)
f &lt;- function(xx, a = (c(1:length(xx)) - 1) / 2) { 
    new1 &lt;- abs(4 * xx - 2) + a
    new2 &lt;- 1 + a
    prod &lt;- prod(new1 / new2)
    return((prod - 1) / 0.86)
}

# Training data
d &lt;- 1 
n &lt;- 20
x &lt;- matrix(runif(n * d), ncol = d)
y &lt;- apply(x, 1, f)

# Testing data
n_test &lt;- 100
xx &lt;- matrix(runif(n_test * d), ncol = d)
yy &lt;- apply(xx, 1, f)

plot(xx[order(xx)], yy[order(xx)], type = "l")
points(x, y, col = 2)

# Example 1: full model (nugget estimated, using continue)
fit &lt;- fit_two_layer(x, y, nmcmc = 1000)
plot(fit)
fit &lt;- continue(fit, 1000) 
plot(fit) 
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1)
plot(fit, hidden = TRUE)

# Example 2: Vecchia approximated model
# (Vecchia approximation is faster for larger data sizes)
fit &lt;- fit_two_layer(x, y, nmcmc = 2000, vecchia = TRUE, m = 10)
plot(fit) 
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1)
plot(fit, hidden = TRUE)

# Example 3: Vecchia approximated model (re-approximated after burn-in)
fit &lt;- fit_two_layer(x, y, nmcmc = 1000, vecchia = TRUE, m = 10)
fit &lt;- continue(fit, 1000, re_approx = TRUE)
plot(fit)
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1)
plot(fit, hidden = TRUE)


</code></pre>

<hr>
<h2 id='IMSE'>Integrated Mean-Squared (prediction) Error for Sequential Design</h2><span id='topic+IMSE'></span><span id='topic+IMSE.gp'></span><span id='topic+IMSE.dgp2'></span><span id='topic+IMSE.dgp3'></span>

<h3>Description</h3>

<p>Acts on a <code>gp</code>, <code>dgp2</code>, or <code>dgp3</code> object.
Current version requires squared exponential covariance
(<code>cov = "exp2"</code>).  Calculates IMSE over the input locations 
<code>x_new</code>.  Optionally utilizes SNOW parallelization.  User should 
select the point with the lowest IMSE to add to the design.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IMSE(object, x_new, cores)

## S3 method for class 'gp'
IMSE(object, x_new = NULL, cores = 1)

## S3 method for class 'dgp2'
IMSE(object, x_new = NULL, cores = 1)

## S3 method for class 'dgp3'
IMSE(object, x_new = NULL, cores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IMSE_+3A_object">object</code></td>
<td>
<p>object of class <code>gp</code>, <code>dgp2</code>, or <code>dgp3</code></p>
</td></tr>
<tr><td><code id="IMSE_+3A_x_new">x_new</code></td>
<td>
<p>matrix of possible input locations, if object has been run 
through <code>predict</code> the previously stored <code>x_new</code> is used</p>
</td></tr>
<tr><td><code id="IMSE_+3A_cores">cores</code></td>
<td>
<p>number of cores to utilize in parallel, by default no 
parallelization is used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Not yet implemented for Vecchia-approximated fits.
</p>
<p>All iterations in the object are used in the calculation, so samples
should be burned-in.  Thinning the samples using <code>trim</code> will speed 
up computation.  This function may be used in two ways:
</p>

<ul>
<li><p> Option 1: called on an object with only MCMC iterations, in 
which case <code>x_new</code> must be specified
</p>
</li>
<li><p> Option 2: called on an object that has been predicted over, in 
which case the <code>x_new</code> from <code>predict</code> is used
</p>
</li></ul>

<p>In Option 2, it is recommended to set <code>store_latent = TRUE</code> for 
<code>dgp2</code> and <code>dgp3</code> objects so latent mappings do not have to 
be re-calculated.  Through <code>predict</code>, the user may
specify a mean mapping (<code>mean_map = TRUE</code>) or a full sample from 
the MVN distribution over <code>w_new</code> (<code>mean_map = FALSE</code>).  When 
the object has not yet been predicted over (Option 1), the mean mapping 
is used.
</p>
<p>SNOW parallelization reduces computation time but requires more memory storage.
</p>


<h3>Value</h3>

<p>list with elements:
</p>

<ul>
<li> <p><code>value</code>: vector of IMSE values, indices correspond to <code>x_new</code>
</p>
</li>
<li> <p><code>time</code>: computation time in seconds
</p>
</li></ul>



<h3>References</h3>

<p>Sauer, A., Gramacy, R.B., &amp; Higdon, D. (2023). Active learning for
deep Gaussian process surrogates. *Technometrics, 65,* 4-18.  arXiv:2012.08015
<br /><br />
Binois, M, J Huang, RB Gramacy, and M Ludkovski. 2019. &quot;Replication or Exploration? 
Sequential Design for Stochastic Simulation Experiments.&quot; <em>Technometrics 
61</em>, 7-23. Taylor &amp; Francis. doi:10.1080/00401706.2018.1469433
</p>


<h3>Examples</h3>

<pre><code class='language-R'># --------------------------------------------------------
# Example 1: toy step function, runs in less than 5 seconds
# --------------------------------------------------------

f &lt;- function(x) {
    if (x &lt;= 0.4) return(-1)
    if (x &gt;= 0.6) return(1)
    if (x &gt; 0.4 &amp; x &lt; 0.6) return(10*(x-0.5))
}

x &lt;- seq(0.05, 0.95, length = 7)
y &lt;- sapply(x, f)
x_new &lt;- seq(0, 1, length = 100)

# Fit model and calculate IMSE
fit &lt;- fit_one_layer(x, y, nmcmc = 100, cov = "exp2")
fit &lt;- trim(fit, 50)
fit &lt;- predict(fit, x_new, cores = 1, store_latent = TRUE)
imse &lt;- IMSE(fit)


# --------------------------------------------------------
# Example 2: Higdon function
# --------------------------------------------------------

f &lt;- function(x) {
    i &lt;- which(x &lt;= 0.48)
    x[i] &lt;- 2 * sin(pi * x[i] * 4) + 0.4 * cos(pi * x[i] * 16)
    x[-i] &lt;- 2 * x[-i] - 1
    return(x)
}

# Training data
x &lt;- seq(0, 1, length = 30)
y &lt;- f(x) + rnorm(30, 0, 0.05)

# Testing data
xx &lt;- seq(0, 1, length = 100)
yy &lt;- f(xx)

plot(xx, yy, type = "l")
points(x, y, col = 2)

# Conduct MCMC (can replace fit_three_layer with fit_one_layer/fit_two_layer)
fit &lt;- fit_three_layer(x, y, D = 1, nmcmc = 2000, cov = "exp2")
plot(fit)
fit &lt;- trim(fit, 1000, 2)

# Option 1 - calculate IMSE from only MCMC iterations
imse &lt;- IMSE(fit, xx)

# Option 2 - calculate IMSE after predictions
fit &lt;- predict(fit, xx, cores = 1, store_latent = TRUE)
imse &lt;- IMSE(fit)

# Visualize fit
plot(fit)
par(new = TRUE) # overlay IMSE
plot(xx, imse$value, col = 2, type = 'l', lty = 2, 
     axes = FALSE, xlab = '', ylab = '')

# Select next design point
x_new &lt;- xx[which.min(imse$value)]


</code></pre>

<hr>
<h2 id='plot'>Plots object from <code>deepgp</code> package</h2><span id='topic+plot'></span><span id='topic+plot.gp'></span><span id='topic+plot.gpvec'></span><span id='topic+plot.dgp2'></span><span id='topic+plot.dgp2vec'></span><span id='topic+plot.dgp3'></span><span id='topic+plot.dgp3vec'></span>

<h3>Description</h3>

<p>Acts on a <code>gp</code>, <code>gpvec</code>, <code>dgp2</code>, <code>dgp2vec</code>,
<code>dgp3</code>, or <code>dgp3vec</code> object.  
Generates trace plots for outer log likelihood, length scale,
and nugget hyperparameters.
Generates plots of hidden layers for one-dimensional inputs.  Generates
plots of the posterior mean and estimated 90% prediction intervals for 
one-dimensional inputs; generates heat maps of the posterior mean and 
point-wise variance for two-dimensional inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gp'
plot(x, trace = NULL, predict = NULL, ...)

## S3 method for class 'gpvec'
plot(x, trace = NULL, predict = NULL, ...)

## S3 method for class 'dgp2'
plot(x, trace = NULL, hidden = NULL, predict = NULL, ...)

## S3 method for class 'dgp2vec'
plot(x, trace = NULL, hidden = NULL, predict = NULL, ...)

## S3 method for class 'dgp3'
plot(x, trace = NULL, hidden = NULL, predict = NULL, ...)

## S3 method for class 'dgp3vec'
plot(x, trace = NULL, hidden = NULL, predict = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_+3A_x">x</code></td>
<td>
<p>object of class <code>gp</code>, <code>gpvec</code>, <code>dgp2</code>, 
<code>dgp2vec</code>, <code>dgp3</code>, or <code>dgp3vec</code></p>
</td></tr>
<tr><td><code id="plot_+3A_trace">trace</code></td>
<td>
<p>logical indicating whether to generate trace plots (default is
TRUE if the object has not been through <code>predict</code>)</p>
</td></tr>
<tr><td><code id="plot_+3A_predict">predict</code></td>
<td>
<p>logical indicating whether to generate posterior predictive 
plot (default is TRUE if the object has been through <code>predict</code>)</p>
</td></tr>
<tr><td><code id="plot_+3A_...">...</code></td>
<td>
<p>N/A</p>
</td></tr>
<tr><td><code id="plot_+3A_hidden">hidden</code></td>
<td>
<p>logical indicating whether to generate plots of hidden layers
(two or three layer only, default is FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Trace plots are useful in assessing burn-in.  Hidden layer plots 
are colored on a gradient - red lines represent earlier iterations and 
yellow lines represent later iterations - to help assess burn-in of the 
hidden layers.  These plots are meant to help in model fitting and 
visualization.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See "fit_one_layer", "fit_two_layer", or "fit_three_layer"
# for an example

</code></pre>

<hr>
<h2 id='predict'>Predict posterior mean and variance/covariance</h2><span id='topic+predict'></span><span id='topic+predict.gp'></span><span id='topic+predict.dgp2'></span><span id='topic+predict.dgp3'></span><span id='topic+predict.gpvec'></span><span id='topic+predict.dgp2vec'></span><span id='topic+predict.dgp3vec'></span>

<h3>Description</h3>

<p>Acts on a <code>gp</code>, <code>dgp2</code>, or <code>dgp3</code> object.
Calculates posterior mean and variance/covariance over specified input 
locations.  Optionally calculates expected improvement (EI) or entropy 
over candidate inputs.  Optionally utilizes SNOW parallelization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gp'
predict(
  object,
  x_new,
  lite = TRUE,
  return_all = FALSE,
  EI = FALSE,
  entropy_limit = NULL,
  cores = 1,
  ...
)

## S3 method for class 'dgp2'
predict(
  object,
  x_new,
  lite = TRUE,
  store_latent = FALSE,
  mean_map = TRUE,
  return_all = FALSE,
  EI = FALSE,
  entropy_limit = NULL,
  cores = 1,
  ...
)

## S3 method for class 'dgp3'
predict(
  object,
  x_new,
  lite = TRUE,
  store_latent = FALSE,
  mean_map = TRUE,
  return_all = FALSE,
  EI = FALSE,
  entropy_limit = NULL,
  cores = 1,
  ...
)

## S3 method for class 'gpvec'
predict(
  object,
  x_new,
  m = object$m,
  lite = TRUE,
  return_all = FALSE,
  entropy_limit = NULL,
  cores = 1,
  ...
)

## S3 method for class 'dgp2vec'
predict(
  object,
  x_new,
  m = object$m,
  lite = TRUE,
  store_latent = FALSE,
  mean_map = TRUE,
  return_all = FALSE,
  entropy_limit = NULL,
  cores = 1,
  ...
)

## S3 method for class 'dgp3vec'
predict(
  object,
  x_new,
  m = object$m,
  lite = TRUE,
  store_latent = FALSE,
  mean_map = TRUE,
  return_all = FALSE,
  entropy_limit = NULL,
  cores = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p>object from <code>fit_one_layer</code>, <code>fit_two_layer</code>, or 
<code>fit_three_layer</code> with burn-in already removed</p>
</td></tr>
<tr><td><code id="predict_+3A_x_new">x_new</code></td>
<td>
<p>matrix of predictive input locations</p>
</td></tr>
<tr><td><code id="predict_+3A_lite">lite</code></td>
<td>
<p>logical indicating whether to calculate only point-wise 
variances (<code>lite = TRUE</code>) or full covariance 
(<code>lite = FALSE</code>)</p>
</td></tr>
<tr><td><code id="predict_+3A_return_all">return_all</code></td>
<td>
<p>logical indicating whether to return mean and point-wise
variance prediction for ALL samples (only available for <code>lite = TRUE</code>)</p>
</td></tr>
<tr><td><code id="predict_+3A_ei">EI</code></td>
<td>
<p>logical indicating whether to calculate expected improvement 
(for minimizing the response)</p>
</td></tr>
<tr><td><code id="predict_+3A_entropy_limit">entropy_limit</code></td>
<td>
<p>optional limit state for entropy calculations (separating
passes and failures), default value of <code>NULL</code> bypasses entropy
calculations</p>
</td></tr>
<tr><td><code id="predict_+3A_cores">cores</code></td>
<td>
<p>number of cores to utilize in parallel</p>
</td></tr>
<tr><td><code id="predict_+3A_...">...</code></td>
<td>
<p>N/A</p>
</td></tr>
<tr><td><code id="predict_+3A_store_latent">store_latent</code></td>
<td>
<p>logical indicating whether to store and return mapped 
values of latent layers (two or three layer models only)</p>
</td></tr>
<tr><td><code id="predict_+3A_mean_map">mean_map</code></td>
<td>
<p>logical indicating whether to map hidden layers using 
conditional mean (<code>mean_map = TRUE</code>) or using a random sample
from the full MVN distribution (two or three layer models only),
<code>mean_map = FALSE</code> is not yet implemented for fits with 
<code>vecchia = TRUE</code></p>
</td></tr>
<tr><td><code id="predict_+3A_m">m</code></td>
<td>
<p>size of Vecchia conditioning sets (only for fits with 
<code>vecchia = TRUE</code>), defaults to the <code>m</code> used for MCMC</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All iterations in the object are used for prediction, so samples 
should be burned-in.  Thinning the samples using <code>trim</code> will speed 
up computation.  Posterior moments are calculated using conditional 
expectation and variance.  As a default, only point-wise variance is 
calculated.  Full covariance may be calculated using <code>lite = FALSE</code>. 
</p>
<p>Expected improvement is calculated with the goal of minimizing the 
response.  See Chapter 7 of Gramacy (2020) for details.  Entropy is 
calculated based on two classes separated by the specified limit.  
See Sauer (2023, Chapter 3) for details.
</p>
<p>SNOW parallelization reduces computation time but requires 
more memory storage.
</p>


<h3>Value</h3>

<p>object of the same class with the following additional elements:
</p>

<ul>
<li> <p><code>x_new</code>: copy of predictive input locations
</p>
</li>
<li> <p><code>mean</code>: predicted posterior mean, indices correspond to 
<code>x_new</code> locations
</p>
</li>
<li> <p><code>s2</code>: predicted point-wise variances, indices correspond to 
<code>x_new</code> locations (only returned when <code>lite = TRUE</code>)
</p>
</li>
<li> <p><code>mean_all</code>: predicted posterior mean for each sample (column
indices), only returned when <code>return_all = TRUE</code>
</p>
</li>
<li> <p><code>s2_all</code> predicted point-wise variances for each sample (column
indices), only returned when <code>return-all = TRUE</code>
</p>
</li>
<li> <p><code>Sigma</code>: predicted posterior covariance, indices correspond to 
<code>x_new</code> locations (only returned when <code>lite = FALSE</code>)
</p>
</li>
<li> <p><code>EI</code>: vector of expected improvement values, indices correspond 
to <code>x_new</code> locations (only returned when <code>EI = TRUE</code>)
</p>
</li>
<li> <p><code>entropy</code>: vector of entropy values, indices correspond to 
<code>x_new</code> locations (only returned when <code>entropy_limit</code> is
numeric)
</p>
</li>
<li> <p><code>w_new</code>: list of hidden layer mappings (only returned when 
<code>store_latent = TRUE</code>), list index corresponds to iteration and 
row index corresponds to <code>x_new</code> location (two or three layer 
models only)
</p>
</li>
<li> <p><code>z_new</code>: list of hidden layer mappings (only returned when 
<code>store_latent = TRUE</code>), list index corresponds to iteration and 
row index corresponds to <code>x_new</code> location (three layer models only) 
</p>
</li></ul>

<p>Computation time is added to the computation time of the existing object.
</p>


<h3>References</h3>

<p>Sauer, A. (2023). Deep Gaussian process surrogates for computer experiments. 
*Ph.D. Dissertation, Department of Statistics, Virginia Polytechnic Institute and State University.*
<br /><br />
Sauer, A, RB Gramacy, and D Higdon. 2020. &quot;Active Learning for Deep Gaussian 
Process Surrogates.&quot; <em>Technometrics, to appear;</em> arXiv:2012.08015. 
<br /><br />
Sauer, A, A Cooper, and RB Gramacy. 2022. &quot;Vecchia-approximated Deep Gaussian
Processes for Computer Experiments.&quot; <em>pre-print on arXiv:2204.02904</em> 
<br /><br />   
Gramacy, RB. <em>Surrogates: Gaussian Process Modeling, Design, and 
Optimization for the Applied Sciences</em>. Chapman Hall, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See "fit_one_layer", "fit_two_layer", or "fit_three_layer"
# for an example

</code></pre>

<hr>
<h2 id='rmse'>Calculates RMSE</h2><span id='topic+rmse'></span>

<h3>Description</h3>

<p>Calculates root mean square error (lower RMSE indicate better 
fits).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmse(y, mu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmse_+3A_y">y</code></td>
<td>
<p>response vector</p>
</td></tr>
<tr><td><code id="rmse_+3A_mu">mu</code></td>
<td>
<p>predicted mean</p>
</td></tr>
</table>

<hr>
<h2 id='score'>Calculates score</h2><span id='topic+score'></span>

<h3>Description</h3>

<p>Calculates score, proportional to the multivariate normal log
likelihood.  Higher scores indicate better fits.  Only 
applicable to noisy data.  Requires full covariance matrix 
(e.g. <code>predict</code> with <code>lite = FALSE</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score(y, mu, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_+3A_y">y</code></td>
<td>
<p>response vector</p>
</td></tr>
<tr><td><code id="score_+3A_mu">mu</code></td>
<td>
<p>predicted mean</p>
</td></tr>
<tr><td><code id="score_+3A_sigma">sigma</code></td>
<td>
<p>predicted covariance</p>
</td></tr>
</table>


<h3>References</h3>

<p>Gneiting, T, and AE Raftery. 2007. Strictly Proper Scoring Rules, Prediction, 
and Estimation. <em>Journal of the American Statistical Association 102</em> 
(477), 359-378.
</p>

<hr>
<h2 id='sq_dist'>Calculates squared pairwise distances</h2><span id='topic+sq_dist'></span>

<h3>Description</h3>

<p>Calculates squared pairwise euclidean distances using C.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sq_dist(X1, X2 = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sq_dist_+3A_x1">X1</code></td>
<td>
<p>matrix of input locations</p>
</td></tr>
<tr><td><code id="sq_dist_+3A_x2">X2</code></td>
<td>
<p>matrix of second input locations (if <code>NULL</code>, distance is 
calculated between <code>X1</code> and itself)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>C code derived from the &quot;laGP&quot; package (Robert B Gramacy and 
Furong Sun).
</p>


<h3>Value</h3>

<p>symmetric matrix of squared euclidean distances
</p>


<h3>References</h3>

<p>Gramacy, RB and F Sun. (2016). laGP: Large-Scale Spatial Modeling via Local 
Approximate Gaussian Processes in R. <em>Journal of Statistical 
Software 72</em> (1), 1-46. doi:10.18637/jss.v072.i01
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(0, 1, length = 10)
d2 &lt;- sq_dist(x)

</code></pre>

<hr>
<h2 id='trim'>Trim/Thin MCMC iterations</h2><span id='topic+trim'></span><span id='topic+trim.gp'></span><span id='topic+trim.gpvec'></span><span id='topic+trim.dgp2'></span><span id='topic+trim.dgp2vec'></span><span id='topic+trim.dgp3'></span><span id='topic+trim.dgp3vec'></span>

<h3>Description</h3>

<p>Acts on a <code>gp</code>, <code>gpvec</code>, <code>dgp2</code>, <code>dgp2vec</code>,
<code>dgp3vec</code>, or <code>dgp3</code> object.
Removes the specified number of MCMC iterations (starting at the first 
iteration).  After these samples are removed, the remaining samples are
optionally thinned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trim(object, burn, thin)

## S3 method for class 'gp'
trim(object, burn, thin = 1)

## S3 method for class 'gpvec'
trim(object, burn, thin = 1)

## S3 method for class 'dgp2'
trim(object, burn, thin = 1)

## S3 method for class 'dgp2vec'
trim(object, burn, thin = 1)

## S3 method for class 'dgp3'
trim(object, burn, thin = 1)

## S3 method for class 'dgp3vec'
trim(object, burn, thin = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trim_+3A_object">object</code></td>
<td>
<p>object from <code>fit_one_layer</code>, <code>fit_two_layer</code>, or 
<code>fit_three_layer</code></p>
</td></tr>
<tr><td><code id="trim_+3A_burn">burn</code></td>
<td>
<p>integer specifying number of iterations to cut off as burn-in</p>
</td></tr>
<tr><td><code id="trim_+3A_thin">thin</code></td>
<td>
<p>integer specifying amount of thinning (<code>thin = 1</code> keeps all 
iterations, <code>thin = 2</code> keeps every other iteration, 
<code>thin = 10</code> keeps every tenth iteration, etc.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The resulting object will have <code>nmcmc</code> equal to the previous 
<code>nmcmc</code> minus <code>burn</code> divided by <code>thin</code>.  It is 
recommended to start an MCMC fit then investigate trace plots to assess 
burn-in.  Once burn-in has been achieved, use this function to remove 
the starting iterations.  Thinning reduces the size of the resulting 
object while accounting for the high correlation between consecutive 
iterations.
</p>


<h3>Value</h3>

<p>object of the same class with the selected iterations removed
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See "fit_one_layer", "fit_two_layer", or "fit_three_layer"
# for an example

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
