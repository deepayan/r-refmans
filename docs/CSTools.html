<!DOCTYPE html><html><head><title>Help for package CSTools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {CSTools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#AdamontQQCorr'><p>AdamontQQCorr computes quantile-quantile correction of seasonal or decadal</p>
forecast data using weather types</a></li>
<li><a href='#Analogs'><p>Analogs based on large scale fields.</p></a></li>
<li><a href='#as.s2dv_cube'><p>Conversion of 'startR_array' or 'list' objects to 's2dv_cube'</p></a></li>
<li><a href='#BEI_EMWeighting'><p>Computing the weighted ensemble means for SFSs.</p></a></li>
<li><a href='#BEI_PDFBest'><p>Computing the Best Index PDFs combining Index PDFs from two SFSs</p></a></li>
<li><a href='#BEI_ProbsWeighting'><p>Computing the weighted tercile probabilities for SFSs.</p></a></li>
<li><a href='#BEI_TercilesWeighting'><p>Computing the weighted terciles for SFSs.</p></a></li>
<li><a href='#BEI_Weights'><p>Computing the weights for SFSs using the Best Index PDFs.</p></a></li>
<li><a href='#BiasCorrection'><p>Bias Correction based on the mean and standard deviation adjustment</p></a></li>
<li><a href='#Calibration'><p>Forecast Calibration</p></a></li>
<li><a href='#CategoricalEnsCombination'><p>Make categorical forecast based on a multi-model forecast with potential for</p>
calibrate</a></li>
<li><a href='#CST_AdamontAnalog'><p>CST_AdamontAnalog finds analogous data in the reference dataset to experiment</p>
data based on weather types</a></li>
<li><a href='#CST_AdamontQQCorr'><p>CST_AdamontQQCorr computes quantile-quantile correction of seasonal or</p>
decadal forecast data using weather types</a></li>
<li><a href='#CST_Analogs'><p>Downscaling using Analogs based on large scale fields.</p></a></li>
<li><a href='#CST_AnalogsPredictors'><p>AEMET Downscaling</p>
Precipitation and maximum and minimum temperature downscaling method
based on analogs: synoptic situations and significant predictors.</a></li>
<li><a href='#CST_Anomaly'><p>Anomalies relative to a climatology along selected dimension with or without</p>
cross-validation</a></li>
<li><a href='#CST_BEI_Weighting'><p>Weighting SFSs of a CSTools object.</p></a></li>
<li><a href='#CST_BiasCorrection'><p>Bias Correction based on the mean and standard deviation adjustment</p></a></li>
<li><a href='#CST_Calibration'><p>Forecast Calibration</p></a></li>
<li><a href='#CST_CategoricalEnsCombination'><p>Make categorical forecast based on a multi-model forecast with potential for</p>
calibrate</a></li>
<li><a href='#CST_ChangeDimNames'><p>Change the name of one or more dimensions for an object of class s2dv_cube</p></a></li>
<li><a href='#CST_DynBiasCorrection'><p>Performing a Bias Correction conditioned by the dynamical</p>
properties of the data.</a></li>
<li><a href='#CST_EnsClustering'><p>Ensemble clustering</p></a></li>
<li><a href='#CST_InsertDim'><p>Add a named dimension to an object of class s2dv_cube</p></a></li>
<li><a href='#CST_Load'><p>CSTools Data Retreival Function</p></a></li>
<li><a href='#CST_MergeDims'><p>Function to  Merge Dimensions</p></a></li>
<li><a href='#CST_MultiEOF'><p>EOF analysis of multiple variables</p></a></li>
<li><a href='#CST_MultiMetric'><p>Multiple Metrics applied in Multiple Model Anomalies</p></a></li>
<li><a href='#CST_MultivarRMSE'><p>Multivariate Root Mean Square Error (RMSE)</p></a></li>
<li><a href='#CST_ProxiesAttractor'><p>Computing two dinamical proxies of the attractor in s2dv_cube.</p></a></li>
<li><a href='#CST_QuantileMapping'><p>Quantile Mapping for seasonal or decadal forecast data</p></a></li>
<li><a href='#CST_RainFARM'><p>RainFARM stochastic precipitation downscaling of a CSTools object</p></a></li>
<li><a href='#CST_RegimesAssign'><p>Function for matching a field of anomalies with</p>
a set of maps used as a reference (e.g. clusters obtained from the WeatherRegime function)</a></li>
<li><a href='#CST_RFSlope'><p>RainFARM spectral slopes from a CSTools object</p></a></li>
<li><a href='#CST_RFTemp'><p>Temperature downscaling of a CSTools object using lapse rate</p>
correction or a reference field</a></li>
<li><a href='#CST_RFWeights'><p>Compute climatological weights for RainFARM stochastic precipitation downscaling</p></a></li>
<li><a href='#CST_SaveExp'><p>Save objects of class 's2dv_cube' to data in NetCDF format</p></a></li>
<li><a href='#CST_SplitDim'><p>Function to Split Dimension</p></a></li>
<li><a href='#CST_Start'><p>CSTools data retrieval function using Start</p></a></li>
<li><a href='#CST_Subset'><p>Subset an object of class s2dv_cube</p></a></li>
<li><a href='#CST_WeatherRegimes'><p>Function for Calculating the Cluster analysis</p></a></li>
<li><a href='#DynBiasCorrection'><p>Performing a Bias Correction conditioned by the dynamical</p>
properties of the data.</a></li>
<li><a href='#EnsClustering'><p>Ensemble clustering</p></a></li>
<li><a href='#lonlat_prec'><p>Sample Of Experimental Precipitation Data In Function Of Longitudes And Latitudes</p></a></li>
<li><a href='#lonlat_prec_st'><p>Sample Of Experimental Precipitation Data In Function Of Longitudes And Latitudes with Start</p></a></li>
<li><a href='#lonlat_temp'><p>Sample Of Experimental And Observational Climate Data In Function Of Longitudes And Latitudes</p></a></li>
<li><a href='#lonlat_temp_st'><p>Sample Of Experimental And Observational Climate Data In Function Of Longitudes And Latitudes with Start</p></a></li>
<li><a href='#MergeDims'><p>Function to Split Dimension</p></a></li>
<li><a href='#MultiEOF'><p>EOF analysis of multiple variables starting from an array (reduced</p>
version)</a></li>
<li><a href='#MultiMetric'><p>Multiple Metrics applied in Multiple Model Anomalies</p></a></li>
<li><a href='#PDFIndexHind'><p>Computing the Index PDFs for a dataset of SFSs for a hindcats period.</p></a></li>
<li><a href='#PlotCombinedMap'><p>Plot Multiple Lon-Lat Variables In a Single Map According to a Decision Function</p></a></li>
<li><a href='#PlotForecastPDF'><p>Plot one or multiple ensemble forecast pdfs for the same event</p></a></li>
<li><a href='#PlotMostLikelyQuantileMap'><p>Plot Maps of Most Likely Quantiles</p></a></li>
<li><a href='#PlotPDFsOLE'><p>Plotting two probability density gaussian functions and the optimal linear</p>
estimation (OLE) as result of combining them.</a></li>
<li><a href='#PlotTriangles4Categories'><p>Function to convert any 3-d numerical array to a grid of coloured triangles.</p></a></li>
<li><a href='#PlotWeeklyClim'><p>Plots the observed weekly means and climatology of a timeseries data</p></a></li>
<li><a href='#Predictability'><p>Computing scores of predictability using two dynamical proxies</p>
based on dynamical systems theory.</a></li>
<li><a href='#print.s2dv_cube'><p>Print method for s2dv_cube objects</p></a></li>
<li><a href='#ProxiesAttractor'><p>Computing two dinamical proxies of the attractor.</p></a></li>
<li><a href='#QuantileMapping'><p>Quantile Mapping for seasonal or decadal forecast data</p></a></li>
<li><a href='#RainFARM'><p>RainFARM stochastic precipitation downscaling (reduced version)</p></a></li>
<li><a href='#RegimesAssign'><p>Function for matching a field of anomalies with</p>
a set of maps used as a reference (e.g. clusters obtained from the WeatherRegime function).</a></li>
<li><a href='#RF_Weights'><p>Compute climatological weights for RainFARM stochastic precipitation downscaling</p></a></li>
<li><a href='#RFSlope'><p>RainFARM spectral slopes from an array (reduced version)</p></a></li>
<li><a href='#RFTemp'><p>Temperature downscaling of a CSTools object using lapse rate</p>
correction (reduced version)</a></li>
<li><a href='#s2dv_cube'><p>Creation of a 's2dv_cube' object</p></a></li>
<li><a href='#SaveExp'><p>Save a multidimensional array with metadata to data in NetCDF format</p></a></li>
<li><a href='#SplitDim'><p>Function to Split Dimension</p></a></li>
<li><a href='#training_analogs'><p>AEMET Training</p>
Training method (pre-downscaling) based on analogs:
synoptic situations and significant predictors.</a></li>
<li><a href='#WeatherRegime'><p>Function for Calculating the Cluster analysis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Assessing Skill of Climate Forecasts on Seasonal-to-Decadal
Timescales</td>
</tr>
<tr>
<td>Version:</td>
<td>5.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Exploits dynamical seasonal forecasts in order to provide
    information relevant to stakeholders at the seasonal timescale. The package
    contains process-based methods for forecast calibration, bias correction,
    statistical and stochastic downscaling, optimal forecast combination and
    multivariate verification, as well as basic and advanced tools to obtain
    tailored products. This package was developed in the context of the 'ERA4CS' 
    project 'MEDSCOPE' and the 'H2020 S2S4E' project and includes contributions from 
    'ArticXchange' project founded by 'EU-PolarNet 2'.
    'Pérez-Zanón et al. (2022) &lt;<a href="https://doi.org/10.5194%2Fgmd-15-6115-2022">doi:10.5194/gmd-15-6115-2022</a>&gt;'.
    'Doblas-Reyes et al. (2005) &lt;<a href="https://doi.org/10.1111%2Fj.1600-0870.2005.00104.x">doi:10.1111/j.1600-0870.2005.00104.x</a>&gt;'.
    'Mishra et al. (2018) &lt;<a href="https://doi.org/10.1007%2Fs00382-018-4404-z">doi:10.1007/s00382-018-4404-z</a>&gt;'.
    'Sanchez-Garcia et al. (2019) &lt;<a href="https://doi.org/10.5194%2Fasr-16-165-2019">doi:10.5194/asr-16-165-2019</a>&gt;'.
    'Straus et al. (2007) &lt;<a href="https://doi.org/10.1175%2FJCLI4070.1">doi:10.1175/JCLI4070.1</a>&gt;'.
    'Terzago et al. (2018) &lt;<a href="https://doi.org/10.5194%2Fnhess-18-2825-2018">doi:10.5194/nhess-18-2825-2018</a>&gt;'.
    'Torralba et al. (2017) &lt;<a href="https://doi.org/10.1175%2FJAMC-D-16-0204.1">doi:10.1175/JAMC-D-16-0204.1</a>&gt;'.
    'D'Onofrio et al. (2014) &lt;<a href="https://doi.org/10.1175%2FJHM-D-13-096.1">doi:10.1175/JHM-D-13-096.1</a>&gt;'.
    'Verfaillie et al. (2017) &lt;<a href="https://doi.org/10.5194%2Fgmd-10-4257-2017">doi:10.5194/gmd-10-4257-2017</a>&gt;'.
    'Van Schaeybroeck et al. (2019) &lt;<a href="https://doi.org/10.1016%2FB978-0-12-812372-0.00010-8">doi:10.1016/B978-0-12-812372-0.00010-8</a>&gt;'.
    'Yiou et al. (2013) &lt;<a href="https://doi.org/10.1007%2Fs00382-012-1626-3">doi:10.1007/s00382-012-1626-3</a>&gt;'.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), maps, qmap, easyVerification</td>
</tr>
<tr>
<td>Imports:</td>
<td>s2dv, startR, rainfarmr, multiApply (&ge; 2.1.1), ClimProjDiags,
ncdf4, plyr, abind, data.table, reshape2, ggplot2,
RColorBrewer, graphics, grDevices, stats, utils, verification,
lubridate, scales, easyNCDF</td>
</tr>
<tr>
<td>Suggests:</td>
<td>zeallot, testthat, knitr, markdown, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-25 15:15:30 UTC; vagudets</td>
</tr>
<tr>
<td>Author:</td>
<td>Nuria Perez-Zanon <a href="https://orcid.org/0000-0001-8568-3071"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Louis-Philippe Caron
    <a href="https://orcid.org/0000-0001-5221-0147"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Carmen Alvarez-Castro
    <a href="https://orcid.org/0000-0002-9958-010X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Lauriane Batte [aut],
  Carlos Delgado [aut],
  Jost von Hardenberg
    <a href="https://orcid.org/0000-0002-5312-8070"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Llorenç LLedo [aut],
  Nicolau Manubens [aut],
  Lluís Palma [aut],
  Eroteida Sanchez-Garcia [aut],
  Bert van Schaeybroeck [aut],
  Veronica Torralba [aut],
  Deborah Verfaillie [aut],
  Eva Rifà [ctb],
  Filippo Cali Quaglia [ctb],
  Maria M. Chaves-Montero [ctb],
  Chihchung Chou [ctb],
  Nicola Cortesi [ctb],
  Susanna Corti [ctb],
  Paolo Davini [ctb],
  Gildas Dayon [ctb],
  Marta Dominguez [ctb],
  Federico Fabiano [ctb],
  Ignazio Giuntoli [ctb],
  Raul Marcos [ctb],
  Paola Marson [ctb],
  Niti Mishra [ctb],
  Jesus Peña [ctb],
  Francesc Roura-Adserias [ctb],
  Silvia Terzago [ctb],
  Danila Volpi [ctb],
  An-Chi Ho [ctb],
  Victoria Agudetse [cre],
  BSC-CNS [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Victoria Agudetse &lt;victoria.agudetse@bsc.es&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-25 16:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='AdamontQQCorr'>AdamontQQCorr computes quantile-quantile correction of seasonal or decadal
forecast data using weather types</h2><span id='topic+AdamontQQCorr'></span>

<h3>Description</h3>

<p>This function computes a quantile mapping based on weather types
for experiment data (typically a hindcast) onto reference <code>obs</code>,
typically provided by reanalysis data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AdamontQQCorr(
  exp,
  wt_exp,
  obs,
  wt_obs,
  corrdims = c("member", "sdate", "ftime"),
  londim = "lon",
  latdim = "lat",
  regrid = FALSE,
  NN = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AdamontQQCorr_+3A_exp">exp</code></td>
<td>
<p>Array with named dimensions (such as <code>$data</code> array of 
experiment data from an object of class <code>s2dv_cube</code>).</p>
</td></tr>
<tr><td><code id="AdamontQQCorr_+3A_wt_exp">wt_exp</code></td>
<td>
<p>Corresponding weather types (same dimensions as <code>exp</code> but
lat/lon).</p>
</td></tr>
<tr><td><code id="AdamontQQCorr_+3A_obs">obs</code></td>
<td>
<p>Array with named dimensions with reference data (can also be 
<code>$data</code> array of class <code>s2dv_cube</code>). lat/lon dimensions can differ 
from <code>exp</code> if non rectilinear latlon grids are used, in which case 
regrid should be set to TRUE and .NearestNeighbors <code>NN</code> output should 
be provided.</p>
</td></tr>
<tr><td><code id="AdamontQQCorr_+3A_wt_obs">wt_obs</code></td>
<td>
<p>Corresponding weather types (same dimensions as <code>obs</code> but
lat/lon).</p>
</td></tr>
<tr><td><code id="AdamontQQCorr_+3A_corrdims">corrdims</code></td>
<td>
<p>List of dimensions in <code>exp</code> for which quantile mapping 
correction is applied.</p>
</td></tr>
<tr><td><code id="AdamontQQCorr_+3A_londim">londim</code></td>
<td>
<p>Character name of longitude dimension in <code>exp</code> and 
<code>obs</code>.</p>
</td></tr>
<tr><td><code id="AdamontQQCorr_+3A_latdim">latdim</code></td>
<td>
<p>Character name of latitude dimension in <code>exp</code> and 
<code>obs</code>.</p>
</td></tr>
<tr><td><code id="AdamontQQCorr_+3A_regrid">regrid</code></td>
<td>
<p>(optional) Boolean indicating whether .NearestNeighbors
regridding is needed.</p>
</td></tr>
<tr><td><code id="AdamontQQCorr_+3A_nn">NN</code></td>
<td>
<p>(optional, if regrid = TRUE) List (output from .NearestNeighbors)
maps (nlat, nlon) onto (nlat_o, nlon_o).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An array (such as <code>$data</code> array from an object of class 
<code>s2dv_cube</code>) with named dimensions, containing experiment data on the 
lat/lon grid of <code>obs</code> array, corrected by quantile mapping depending on 
the weather types <code>wt_exp</code>
</p>


<h3>Author(s)</h3>

<p>Paola Marson, <a href="mailto:paola.marson@meteo.fr">paola.marson@meteo.fr</a> for PROSNOW version
</p>
<p>Lauriane Batté, <a href="mailto:lauriane.batte@meteo.fr">lauriane.batte@meteo.fr</a> for CSTools adaptation
</p>


<h3>Examples</h3>

<pre><code class='language-R'>wt_exp &lt;- c(1,1,2,3,3,2,2,1,1,2,2,3)
dim(wt_exp) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 3)
wt_obs &lt;- c(3,3,1,2,2,2,2,1,3,1,1,2) 
dim(wt_obs) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 3)
exp &lt;- 1 : c(1 * 1 * 4 * 3 * 4 * 4)
dim(exp) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 3,
             lat = 4, lon = 4)
obs &lt;- 101 : c(100 + 1 * 1 * 4 * 3 * 4 * 4)
dim(obs) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 3,
             lat = 4, lon = 4)
exp_corr &lt;- AdamontQQCorr(exp = exp, wt_exp = wt_exp, 
                         obs = obs, wt_obs = wt_obs, 
                         corrdims = c('dataset', 'member', 'sdate', 'ftime'))
</code></pre>

<hr>
<h2 id='Analogs'>Analogs based on large scale fields.</h2><span id='topic+Analogs'></span>

<h3>Description</h3>

<p>This function perform a downscaling using Analogs. To compute 
the analogs, the function search for days with similar large scale conditions
to downscaled fields in the local scale. The large scale and the local scale 
regions are defined by the user. The large scale is usually given by 
atmospheric circulation as sea level pressure or geopotential height (Yiou 
et al, 2013) but the function gives the possibility to use another field. The
local scale will be usually given by precipitation or temperature fields, but
might be another variable. 
The analogs function will find the best analogs based in three criterias:
(1) Minimum Euclidean distance in the large scale pattern (i.e. SLP)
(2) Minimum Euclidean distance in the large scale pattern (i.e. SLP) 
and minimum Euclidean distance in the local scale pattern (i.e. SLP). 
(3) Minimum Euclidean distance in the large scale pattern (i.e. SLP), 
minimum distance in the local scale pattern (i.e. SLP) and highest 
correlation in the local variable to downscale (i.e Precipitation).
The search of analogs must be done in the longest dataset posible. This is 
important since it is necessary to have a good representation of the 
possible states of the field in the past, and therefore, to get better 
analogs. Once the search of the analogs is complete, and in order to used the 
three criterias the user can select a number of analogs , using parameter 
'nAnalogs' to restrict 
the selection of the best analogs in a short number of posibilities, the best
ones. This function has not constrains of specific regions, variables to 
downscale, or data to be used (seasonal forecast data, climate projections 
data, reanalyses data). The regrid into a finner scale is done interpolating 
with CST_Start. Then, this interpolation is corrected selecting the analogs in
the large and local scale in based of the observations. The function is an 
adapted version of the method of Yiou et al 2013.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Analogs(
  expL,
  obsL,
  time_obsL,
  time_expL = NULL,
  lonL = NULL,
  latL = NULL,
  expVar = NULL,
  obsVar = NULL,
  sdate_dim = "sdate",
  criteria = "Large_dist",
  excludeTime = NULL,
  lonVar = NULL,
  latVar = NULL,
  region = NULL,
  nAnalogs = NULL,
  AnalogsInfo = FALSE,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Analogs_+3A_expl">expL</code></td>
<td>
<p>An array of N named dimensions containing the experimental field
on the large scale for which the analog is aimed. This field is used to in 
all the criterias. If parameter 'expVar' is not provided, the function will
return the expL analog. The element 'data' in the 's2dv_cube' object must 
have, at least, latitudinal and longitudinal dimensions. The object is 
expect to be already subset for the desired large scale region. Latitudinal 
dimension accepted names: 'lat', 'lats', 'latitude', 'y', 'j', 'nav_lat'. 
Longitudinal dimension accepted names: 'lon', 'lons','longitude', 'x', 'i', 
'nav_lon'.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_obsl">obsL</code></td>
<td>
<p>An array of N named dimensions containing the observational field 
on the large scale. The element 'data' in the 's2dv_cube' object must have 
the same latitudinal and longitudinal dimensions as parameter 'expL' and a 
single temporal dimension with the maximum number of available observations.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_time_obsl">time_obsL</code></td>
<td>
<p>A character string indicating the date of the observations 
in the format &quot;dd-mm-yyyy&quot;. Reference time to search for analogs. It must 
have time dimensions.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_time_expl">time_expL</code></td>
<td>
<p>An array of N named dimensions (coinciding with time 
dimensions in expL) of character string(s) indicating the date(s) of the 
experiment in the format &quot;dd-mm-yyyy&quot;. Time(s) to find the analogs. If it 
is not an scalar it must have named dimensions.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_lonl">lonL</code></td>
<td>
<p>A vector containing the longitude of parameter 'expL'.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_latl">latL</code></td>
<td>
<p>A vector containing the latitude of parameter 'expL'.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_expvar">expVar</code></td>
<td>
<p>An array of N named dimensions containing the experimental 
field on the local scale, usually a different variable to the parameter 
'expL'. If it is not NULL (by default, NULL), the returned field by this 
function will be the analog of parameter 'expVar'.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_obsvar">obsVar</code></td>
<td>
<p>An array of N named dimensions containing the field of the 
same variable as the passed in parameter 'expVar' for the same region.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. By default, it is set to 'sdate'.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_criteria">criteria</code></td>
<td>
<p>A character string indicating the criteria to be used for the
selection of analogs:
</p>
<ul>
<li><p>Large_dist minimum Euclidean distance in the large scale pattern; 
</p>
</li>
<li><p>Local_dist minimum Euclidean distance in the large scale pattern 
and minimum Euclidean distance in the local scale pattern; and
</p>
</li>
<li><p>Local_cor minimum Euclidean distance in the large scale pattern, 
minimum Euclidean distance in the local scale pattern and highest 
correlation in the local variable to downscale.</p>
</li></ul>
</td></tr>
<tr><td><code id="Analogs_+3A_excludetime">excludeTime</code></td>
<td>
<p>An array of N named dimensions (coinciding with time 
dimensions in expL) of character string(s) indicating the date(s) of the 
observations in the format &quot;dd/mm/yyyy&quot; to be excluded during the search of 
analogs. It can be NULL but if expL is not a forecast (time_expL contained 
in time_obsL), by default time_expL will be removed during the search of 
analogs.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_lonvar">lonVar</code></td>
<td>
<p>A vector containing the longitude of parameter 'expVar'.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_latvar">latVar</code></td>
<td>
<p>A vector containing the latitude of parameter 'expVar'.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_region">region</code></td>
<td>
<p>A vector of length four indicating the minimum longitude, 
the maximum longitude, the minimum latitude and the maximum latitude.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_nanalogs">nAnalogs</code></td>
<td>
<p>Number of Analogs to be selected to apply the criterias 
'Local_dist' or 'Local_cor'. This is not the necessary the number of analogs 
that the user can get, but the number of events with minimum distance in 
which perform the search of the best Analog. The default value for the 
'Large_dist' criteria is 1, for 'Local_dist' and 'Local_cor' criterias must
be greater than 1 in order to match with the first criteria, if nAnalogs is
NULL for 'Local_dist' and 'Local_cor' the default value will be set at the 
length of 'time_obsL'. If AnalogsInfo is FALSE the function returns just 
the best analog.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_analogsinfo">AnalogsInfo</code></td>
<td>
<p>A logical value. If it is TRUE it returns a list
with two elements: 1) the downscaled field and
2) the AnalogsInfo which contains: a) the number of the best 
analogs, b) the corresponding value of the metric used in the selected 
criteria (distance values for Large_dist and Local_dist,correlation values 
for Local_cor), c)dates of the analogs). The analogs are listed in 
decreasing order, the first one is the best analog (i.e if the selected 
criteria is Local_cor the best analog will be the one with highest 
correlation, while for Large_dist criteria the best analog will be the day 
with minimum Euclidean distance). Set to FALSE to get a single analog, the 
best analog, for instance for downscaling.</p>
</td></tr>
<tr><td><code id="Analogs_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to use in parallel computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An array with the dowscaled values of the best analogs for the criteria 
selected. If 'AnalogsInfo' is set to TRUE it returns a list with an array 
of the dowsncaled field and the analogs information in elements 'analogs', 
'metric' and 'dates'.
</p>


<h3>Author(s)</h3>

<p>M. Carmen Alvarez-Castro, <a href="mailto:carmen.alvarez-castro@cmcc.it">carmen.alvarez-castro@cmcc.it</a>
</p>
<p>Maria M. Chaves-Montero, <a href="mailto:mariadm.chaves@cmcc.it">mariadm.chaves@cmcc.it</a>
</p>
<p>Veronica Torralba, <a href="mailto:veronica.torralba@cmcc.it">veronica.torralba@cmcc.it</a>
</p>
<p>Nuria Perez-Zanon <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>References</h3>

<p>Yiou, P., T. Salameh, P. Drobinski, L. Menut, R. Vautard,
and M. Vrac, 2013 : Ensemble reconstruction of the atmospheric column 
from surface pressure using analogues.  Clim. Dyn., 41, 1419-1437. 
<a href="mailto:pascal.yiou@lsce.ipsl.fr">pascal.yiou@lsce.ipsl.fr</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: Downscaling using criteria 'Large_dist' and a single variable:
expSLP &lt;- rnorm(1:20)
dim(expSLP) &lt;- c(lat = 4, lon = 5)
obsSLP &lt;- c(rnorm(1:180), expSLP * 1.2)
dim(obsSLP) &lt;- c(time = 10, lat = 4, lon = 5)
time_obsSLP &lt;- paste(rep("01", 10), rep("01", 10), 1994 : 2003, sep = "-")
dim(time_obsSLP) &lt;- c(time = 10)
downscale_field &lt;- Analogs(expL = expSLP, obsL = obsSLP, 
                          time_obsL = time_obsSLP,time_expL = "01-01-1994")

# Example 2: Downscaling using criteria 'Large_dist' and 2 variables:
obs.pr &lt;- c(rnorm(1:200) * 0.001)
dim(obs.pr) &lt;- dim(obsSLP)
downscale_field &lt;- Analogs(expL = expSLP, obsL = obsSLP, obsVar = obs.pr,
                          time_obsL = time_obsSLP, time_expL = "01-01-1994")

# Example 3: Downscaling using criteria 'Local_dist' and 2 variables:
# analogs of local scale using criteria 2
region = c(lonmin = -1 ,lonmax = 2, latmin = 30, latmax = 33)
Local_scale &lt;- Analogs(expL = expSLP, obsL = obsSLP, time_obsL = time_obsSLP,
                      obsVar = obs.pr, criteria = "Local_dist", 
                      lonL = seq(-1, 5, 1.5),latL = seq(30, 35, 1.5), 
                      region = region,time_expL = "01-10-2000", 
                      nAnalogs = 10, AnalogsInfo = TRUE)

# Example 4: Downscaling using criteria 'Local_cor' and 2 variables:
exp.pr &lt;- c(rnorm(1:20) * 0.001)
dim(exp.pr) &lt;- dim(expSLP)
Local_scalecor &lt;- Analogs(expL = expSLP, obsL = obsSLP, time_obsL = time_obsSLP,
                         obsVar = obs.pr, expVar = exp.pr,
                         criteria = "Local_cor", lonL = seq(-1, 5, 1.5),
                         time_expL = "01-10-2000", latL = seq(30, 35, 1.5),
                         lonVar = seq(-1, 5, 1.5), latVar = seq(30, 35, 1.5), 
                         nAnalogs = 8, region = region, AnalogsInfo = FALSE)

# Example 5: List of best analogs in the three criterias Large_dist, 
Large_scale &lt;- Analogs(expL = expSLP, obsL = obsSLP, time_obsL = time_obsSLP,
                      criteria = "Large_dist", time_expL = "01-10-2000",
                      nAnalogs = 7, AnalogsInfo = TRUE)
Local_scale &lt;- Analogs(expL = expSLP, obsL = obsSLP, time_obsL = time_obsSLP,
                      time_expL = "01-10-2000", criteria = "Local_dist",
                      lonL = seq(-1, 5, 1.5), latL = seq(30, 35, 1.5), 
                      nAnalogs = 7, region = region, AnalogsInfo = TRUE)
Local_scalecor &lt;- Analogs(expL = expSLP, obsL = obsSLP, time_obsL = time_obsSLP,
                         obsVar = obsSLP, expVar = expSLP, 
                         time_expL = "01-10-2000", criteria = "Local_cor", 
                         lonL = seq(-1, 5, 1.5), latL = seq(30, 35, 1.5),
                         lonVar = seq(-1, 5, 1.5), latVar = seq(30, 35, 1.5),
                         nAnalogs = 7, region = region, 
                         AnalogsInfo = TRUE)
</code></pre>

<hr>
<h2 id='as.s2dv_cube'>Conversion of 'startR_array' or 'list' objects to 's2dv_cube'</h2><span id='topic+as.s2dv_cube'></span>

<h3>Description</h3>

<p>This function converts data loaded using Start function from startR package or 
Load from s2dv into an 's2dv_cube' object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.s2dv_cube(object, remove_attrs_coords = FALSE, remove_null = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.s2dv_cube_+3A_object">object</code></td>
<td>
<p>An object of class 'startR_array' generated from function 
<code>Start</code> from startR package or a list output from function <code>Load</code> 
from s2dv package. Any other object class will not be accepted.</p>
</td></tr>
<tr><td><code id="as.s2dv_cube_+3A_remove_attrs_coords">remove_attrs_coords</code></td>
<td>
<p>A logical value indicating whether to remove the 
attributes of the coordinates (TRUE) or not (FALSE). The default value is 
FALSE.</p>
</td></tr>
<tr><td><code id="as.s2dv_cube_+3A_remove_null">remove_null</code></td>
<td>
<p>Optional. A logical value indicating whether to remove the
elements that are NULL (TRUE) or not (FALSE) of the output object. It is 
only used when the object is an output from function <code>Load</code>. The 
default value is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an 's2dv_cube' object to be easily used with 
functions with the prefix <code>CST</code> from CSTools and CSIndicators packages. 
The object is mainly a list with the following elements:<br />
</p>

<ul>
<li><p>'data', array with named dimensions;
</p>
</li>
<li><p>'dims', named vector of the data dimensions;
</p>
</li>
<li><p>'coords', list of named vectors with the coordinates corresponding to 
the dimensions of the data parameter;
</p>
</li>
<li><p>'attrs', named list with elements:
</p>

<ul>
<li><p>'Dates', array with named temporal dimensions of class 'POSIXct' 
from time values in the data;
</p>
</li>
<li><p>'Variable', has the following components:
</p>

<ul>
<li><p>'varName', character vector of the short variable name. It is  
usually specified in the parameter 'var' from the functions 
Start and Load;
</p>
</li>
<li><p>'metadata', named list of elements with variable metadata. 
They can be from coordinates variables (e.g. longitude) or 
main variables (e.g. 'var');
</p>
</li></ul>


</li>
<li><p>'Datasets', character strings indicating the names of the 
datasets;
</p>
</li>
<li><p>'source_files', a vector of character strings with complete paths 
to all the found files involved in loading the data;
</p>
</li>
<li><p>'when', a time stamp of the date issued by the Start() or Load() 
call to obtain the data;
</p>
</li>
<li><p>'load_parameters', it contains the components used in the 
arguments to load the data from Start() or Load() functions.
</p>
</li></ul>


</li></ul>



<h3>Author(s)</h3>

<p>Perez-Zanon Nuria, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>
<p>Nicolau Manubens, <a href="mailto:nicolau.manubens@bsc.es">nicolau.manubens@bsc.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+s2dv_cube">s2dv_cube</a></code>, <code><a href="#topic+CST_Start">CST_Start</a></code>, 
<code><a href="startR.html#topic+Start">Start</a></code> and <code><a href="#topic+CST_Load">CST_Load</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Example 1: convert an object from startR::Start function to 's2dv_cube'
library(startR)
repos &lt;- '/esarchive/exp/ecmwf/system5_m1/monthly_mean/$var$_f6h/$var$_$sdate$.nc'
data &lt;- Start(dat = repos,
             var = 'tas',
             sdate = c('20170101', '20180101'),
             ensemble = indices(1:5),
             time = 'all',
             latitude = indices(1:5),
             longitude = indices(1:5),
             return_vars = list(latitude = 'dat', longitude = 'dat', time = 'sdate'),
             retrieve = TRUE)
data &lt;- as.s2dv_cube(data)
# Example 2: convert an object from s2dv::Load function to 's2dv_cube'
startDates &lt;- c('20001101', '20011101', '20021101',
               '20031101', '20041101', '20051101')
data &lt;- Load(var = 'tas', exp = 'system5c3s', 
            nmember = 2, sdates = startDates,
            leadtimemax = 3, latmin = 10, latmax = 30,
            lonmin = -10, lonmax = 10, output = 'lonlat')
data &lt;- as.s2dv_cube(data)

## End(Not run)
</code></pre>

<hr>
<h2 id='BEI_EMWeighting'>Computing the weighted ensemble means for SFSs.</h2><span id='topic+BEI_EMWeighting'></span>

<h3>Description</h3>

<p>This function implements the computation to obtain the weighted
ensemble means for SFSs using a normalized weights array,
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BEI_EMWeighting(var_exp, aweights, time_dim_name = "time", memb_dim = "member")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BEI_EMWeighting_+3A_var_exp">var_exp</code></td>
<td>
<p>Variable (e.g. precipitation, temperature, NAO index)
array from a SFS with at least dimensions (time, member) for a spatially 
aggregated variable or dimensions (time, member, lat, lon) for a spatial 
variable, as 'time' the spatial dimension by default.</p>
</td></tr>
<tr><td><code id="BEI_EMWeighting_+3A_aweights">aweights</code></td>
<td>
<p>Normalized weights array with at least dimensions 
(time, member), when 'time' is the temporal dimension as default.</p>
</td></tr>
<tr><td><code id="BEI_EMWeighting_+3A_time_dim_name">time_dim_name</code></td>
<td>
<p>A character string indicating the name of the 
temporal dimension, by default 'time'.</p>
</td></tr>
<tr><td><code id="BEI_EMWeighting_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the 
member dimension, by default 'member'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>BEI_EMWeighting() returns an array with at least one or three 
dimensions depending if the variable is spatially aggregated variable 
(as e.g. NAO index)(time) or it is spatial variable (as e.g. precipitation 
or temperature) (time, lat, lon), containing the ensemble means computing
with weighted members.
</p>


<h3>Author(s)</h3>

<p>Eroteida Sanchez-Garcia - AEMET, <a href="mailto:esanchezg@aemet.es">esanchezg@aemet.es</a>
</p>


<h3>References</h3>

<p>Regionally improved seasonal forecast of precipitation through Best
estimation of winter NAO, Sanchez-Garcia, E. et al.,
Adv. Sci. Res., 16, 165174, 2019, doi: <a href="https://doi.org/10.5194/asr-16-165-2019">10.5194/asr-16-165-2019</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1 
var_exp &lt;- 1 : (2 * 3 * 4)
dim(var_exp) &lt;- c(time = 2, dataset = 3, member = 4)
aweights &lt;- runif(24, min = 0.001, max = 0.999)
dim(aweights) &lt;- c(time = 2, dataset = 3, member = 4)
res &lt;- BEI_EMWeighting(var_exp, aweights)

# Example 2 
var_exp &lt;- 1 : (2 * 4 * 2 * 3)
dim(var_exp) &lt;- c(time = 2, member = 4, lat = 2, lon = 3)
aweights &lt;- c(0.2, 0.1, 0.3, 0.4, 0.1, 0.2, 0.4, 0.3)
dim(aweights) &lt;- c(time = 2, member = 4)
res &lt;- BEI_EMWeighting(var_exp, aweights)

</code></pre>

<hr>
<h2 id='BEI_PDFBest'>Computing the Best Index PDFs combining Index PDFs from two SFSs</h2><span id='topic+BEI_PDFBest'></span>

<h3>Description</h3>

<p>This function implements the computation to obtain the index 
Probability Density Functions (PDFs) (e.g. NAO index) obtained to combining 
the Index PDFs for two Seasonal Forecast Systems (SFSs), the Best Index 
estimation (see Sanchez-Garcia, E. et al (2019), 
doi: <a href="https://doi.org/10.5194/asr-16-165-2019">10.5194/asr-16-165-2019</a> for more details about the 
methodology applied to estimate the Best Index).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BEI_PDFBest(
  index_obs,
  index_hind1,
  index_hind2,
  index_fcst1 = NULL,
  index_fcst2 = NULL,
  method_BC = "none",
  time_dim_name = "time",
  na.rm = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BEI_PDFBest_+3A_index_obs">index_obs</code></td>
<td>
<p>Index (e.g. NAO index) array from an observational database
or reanalysis with at least a temporal dimension (by default 'time'), 
which must be greater than 2.</p>
</td></tr>
<tr><td><code id="BEI_PDFBest_+3A_index_hind1">index_hind1</code></td>
<td>
<p>Index (e.g. NAO index) array from a SFS (named SFS1)
with at least two dimensions (time , member) or (time, statistic). 
The temporal dimension, by default 'time', must be greater than 2. 
The dimension 'member' must be greater than 1. The dimension 'statistic' 
must be equal to 2, for containing the two paramenters of a normal 
distribution (mean and sd) representing the ensemble of a SFS. It is not 
possible to have the dimension 'member' and 'statistic' 
at the same time.</p>
</td></tr>
<tr><td><code id="BEI_PDFBest_+3A_index_hind2">index_hind2</code></td>
<td>
<p>Index (e.g. NAO index) array from a SFS (named SFS2)
with at least two dimensions (time , member) or (time, statistic). 
The temporal dimension, by default 'time', must be greater than 2. 
The dimension 'member' must be greater than 1. 
The dimension 'statistic' must be equal to 2, for containing the two 
paramenters of a normal distribution (mean and sd) representing the ensemble 
of a SFS. It is not possible to have the dimension 'member' and  'statistic' 
together.</p>
</td></tr>
<tr><td><code id="BEI_PDFBest_+3A_index_fcst1">index_fcst1</code></td>
<td>
<p>(optional, default = NULL) Index (e.g. NAO index) array 
from forescating of SFS1 with at least two dimensions (time , member) or 
(time, statistic). The temporal dimension, by default 'time', must be equal 
to 1, the forecast year target. The dimension 'member' must be greater than 
1. The dimension 'statistic' must be equal to 2, for containing the two 
paramenters of a normal distribution (mean and sd) representing the ensemble 
of a SFS. It is not possible to have the dimension 'member' and  'statistic' 
together.</p>
</td></tr>
<tr><td><code id="BEI_PDFBest_+3A_index_fcst2">index_fcst2</code></td>
<td>
<p>(optional, default = NULL) Index (e.g. NAO index) array 
from forescating of SFS2 with at least two dimensions (time , member) or 
(time, statistic). The temporal dimension, by default 'time', must be equal 
to 1, the forecast year target. The dimension 'member' must be greater than 
1. The dimension 'statistic' must be equal to 2, for containing the two 
paramenters of a normal distribution (mean and sd) representing the ensemble 
of a SFS. It is not possible to have the dimension 'member' and  'statistic' 
together.</p>
</td></tr>
<tr><td><code id="BEI_PDFBest_+3A_method_bc">method_BC</code></td>
<td>
<p>A character vector of maximun length 2 indicating the bias 
correction methodology to be applied on each SFS. If it is 'none' or any of 
its elements is 'none', the bias correction won't be applied. Available 
methods developped are &quot;ME&quot; (a bias correction scheme based on the mean 
error or bias between observation and predictions to correct the predicted 
values), and &quot;LMEV&quot; (a bias correction scheme based on a linear model using 
ensemble variance of index as predictor). (see Sanchez-Garcia, E. et al 
(2019), doi: <a href="https://doi.org/10.5194/asr-16-165-2019">10.5194/asr-16-165-2019</a> for more details).</p>
</td></tr>
<tr><td><code id="BEI_PDFBest_+3A_time_dim_name">time_dim_name</code></td>
<td>
<p>A character string indicating the name of the temporal 
dimension, by default 'time'.</p>
</td></tr>
<tr><td><code id="BEI_PDFBest_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical (default = FALSE). Should missing values be removed?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>BEI_PDFBest() returns an array with the parameters that caracterize
the PDFs, with at least a temporal dimension, by default 'time' and dimension 
'statistic' equal to 2. The firt statistic is the parameter 'mean' of the PDF 
for the best estimation combining the two SFSs PDFs. The second statistic is 
the parameter 'standard deviation' of the PDF for the best estimation 
combining the two SFSs PDFs. If index_fcst1 and/or index_fcst2 are null, 
returns the values for hindcast period. Otherwise, it returns the values for a 
forecast year.
</p>


<h3>Author(s)</h3>

<p>Eroteida Sanchez-Garcia - AEMET, <a href="mailto:esanchezg@aemet.es">esanchezg@aemet.es</a>
</p>


<h3>References</h3>

<p>Regionally improved seasonal forecast of precipitation through 
Best estimation of winter NAO, Sanchez-Garcia, E. et al.,
Adv. Sci. Res., 16, 165174, 2019, doi: <a href="https://doi.org/10.5194/asr-16-165-2019">10.5194/asr-16-165-2019</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1 for the BEI_PDFBest function
index_obs&lt;- rnorm(10, sd = 3)
dim(index_obs) &lt;- c(time = 5, season = 2)
index_hind1 &lt;- rnorm(40, mean = 0.2, sd = 3)
dim(index_hind1) &lt;- c(time = 5, member = 4, season = 2)
index_hind2 &lt;- rnorm(60, mean = -0.5, sd = 4)
dim(index_hind2) &lt;- c(time = 5, member = 6, season = 2)
index_fcst1 &lt;- rnorm(16, mean = 0.2, sd = 3)
dim(index_fcst1) &lt;- c(time = 1, member = 8, season = 2)
index_fcst2 &lt;- rnorm(18, mean = -0.5, sd = 4)
dim(index_fcst2) &lt;- c(time = 1, member = 9, season = 2)
method_BC &lt;- 'ME'
res &lt;- BEI_PDFBest(index_obs, index_hind1, index_hind2, index_fcst1, 
index_fcst2, method_BC)  
# Example 2 for the BEI_PDFBest function
index_obs&lt;- rnorm(10, sd = 3)
dim(index_obs) &lt;- c(time = 5, season = 2)
index_hind1 &lt;- rnorm(40, mean = 0.2, sd = 3)
dim(index_hind1) &lt;- c(time = 5, member = 4, season = 2)
index_hind2 &lt;- rnorm(60, mean = -0.5, sd = 4)
dim(index_hind2) &lt;- c(time = 5, member = 6, season = 2)
index_fcst1 &lt;- rnorm(16, mean = 0.2, sd = 3)
dim(index_fcst1) &lt;- c(time = 1, member = 8, season = 2)
index_fcst2 &lt;- rnorm(18, mean = -0.5, sd = 4)
dim(index_fcst2) &lt;- c(time = 1, member = 9, season = 2)
method_BC &lt;- c('LMEV', 'ME')
res &lt;- BEI_PDFBest(index_obs, index_hind1, index_hind2, index_fcst1, 
                   index_fcst2, method_BC) 
</code></pre>

<hr>
<h2 id='BEI_ProbsWeighting'>Computing the weighted tercile probabilities for SFSs.</h2><span id='topic+BEI_ProbsWeighting'></span>

<h3>Description</h3>

<p>This function implements the computation to obtain the tercile
probabilities for a weighted variable for SFSs using a normalized weights array,
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BEI_ProbsWeighting(
  var_exp,
  aweights,
  terciles,
  time_dim_name = "time",
  memb_dim = "member"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BEI_ProbsWeighting_+3A_var_exp">var_exp</code></td>
<td>
<p>Variable (e.g. precipitation, temperature, NAO index)
array from a SFS with at least dimensions (time, member) for a spatially 
aggregated variable or dimensions (time, member, lat, lon) for a spatial 
variable, as 'time' the spatial dimension by default.</p>
</td></tr>
<tr><td><code id="BEI_ProbsWeighting_+3A_aweights">aweights</code></td>
<td>
<p>Normalized weights array with at least dimensions 
(time, member), when 'time' is the temporal dimension as default.</p>
</td></tr>
<tr><td><code id="BEI_ProbsWeighting_+3A_terciles">terciles</code></td>
<td>
<p>A numeric array with at least one dimension 'tercil' equal to 
2, the first element is the lower tercil for a hindcast period, and the second  
element is the upper tercile.</p>
</td></tr>
<tr><td><code id="BEI_ProbsWeighting_+3A_time_dim_name">time_dim_name</code></td>
<td>
<p>A character string indicating the name of the 
temporal dimension, by default 'time'.</p>
</td></tr>
<tr><td><code id="BEI_ProbsWeighting_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the 
member dimension, by default 'member'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>BEI_ProbsWeighting() returns an array with at least two or four 
dimensions depending if the variable is a spatially aggregated variable 
(as e.g. NAO index)(time, tercil) or it is spatial variable (as e.g. 
precipitation or temperature)(time, tercile, lat, lon), containing the 
terciles probabilities computing with weighted members.
The first tercil is the lower tercile, the second is the normal tercile and
the third is the upper tercile.
</p>


<h3>Author(s)</h3>

<p>Eroteida Sanchez-Garcia - AEMET, <a href="mailto:esanchezg@aemet.es">esanchezg@aemet.es</a>
</p>


<h3>References</h3>

<p>Regionally improved seasonal forecast of precipitation through Best
estimation of winter NAO, Sanchez-Garcia, E. et al.,
Adv. Sci. Res., 16, 165174, 2019, doi: <a href="https://doi.org/10.5194/asr-16-165-2019">10.5194/asr-16-165-2019</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1 
var_exp &lt;- 1 : (2 * 4)
dim(var_exp) &lt;- c(time = 2, member = 4)
aweights &lt;- c(0.2, 0.1, 0.3, 0.4, 0.1, 0.2, 0.4, 0.3)
dim(aweights) &lt;- c(time = 2, member = 4)
terciles &lt;- c(2.5,5)
dim(terciles) &lt;- c(tercil = 2)
res &lt;- BEI_ProbsWeighting(var_exp, aweights, terciles)

# Example 2 
var_exp &lt;- rnorm(48, 50, 9)
dim(var_exp) &lt;- c(time = 2, member = 4, lat = 2, lon = 3)
aweights &lt;- c(0.2, 0.1, 0.3, 0.4, 0.1, 0.2, 0.4, 0.3)
dim(aweights) &lt;- c(time = 2, member = 4)
terciles &lt;- rep(c(48,50), 2*3)
dim(terciles) &lt;- c(tercil = 2, lat = 2, lon = 3)
res &lt;- BEI_ProbsWeighting(var_exp, aweights, terciles)
</code></pre>

<hr>
<h2 id='BEI_TercilesWeighting'>Computing the weighted terciles for SFSs.</h2><span id='topic+BEI_TercilesWeighting'></span>

<h3>Description</h3>

<p>This function implements the computation to obtain the terciles
for a weighted variable for SFSs using a normalized weights array,
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BEI_TercilesWeighting(
  var_exp,
  aweights,
  time_dim_name = "time",
  memb_dim = "member"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BEI_TercilesWeighting_+3A_var_exp">var_exp</code></td>
<td>
<p>Variable (e.g. precipitation, temperature, NAO index)
array from a SFS with at least dimensions (time, member) for a spatially 
aggregated variable or dimensions (time, member, lat, lon) for a spatial 
variable, as 'time' the spatial dimension by default.</p>
</td></tr>
<tr><td><code id="BEI_TercilesWeighting_+3A_aweights">aweights</code></td>
<td>
<p>Normalized weights array with at least dimensions 
(time, member), when 'time' is the temporal dimension as default.</p>
</td></tr>
<tr><td><code id="BEI_TercilesWeighting_+3A_time_dim_name">time_dim_name</code></td>
<td>
<p>A character string indicating the name of the 
temporal dimension, by default 'time'.</p>
</td></tr>
<tr><td><code id="BEI_TercilesWeighting_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the 
member dimension, by default 'member'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>BEI_TercilesWeighting() returns an array with at least one 
dimension depending if the variable is a spatially aggregated variable 
(as e.g. NAO index)(tercil) or it is spatial variable (as e.g. 
precipitation or temperature)(tercil, lat, lon), containing the 
terciles computing with weighted members.
The first tercil is the lower tercile, the second is the upper tercile.
</p>


<h3>Author(s)</h3>

<p>Eroteida Sanchez-Garcia - AEMET, <a href="mailto:esanchezg@aemet.es">esanchezg@aemet.es</a>
</p>


<h3>References</h3>

<p>Regionally improved seasonal forecast of precipitation through Best
estimation of winter NAO, Sanchez-Garcia, E. et al.,
Adv. Sci. Res., 16, 165174, 2019, doi: <a href="https://doi.org/10.5194/asr-16-165-2019">10.5194/asr-16-165-2019</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1 
var_exp &lt;- 1 : (2 * 4)
dim(var_exp) &lt;- c(time = 2, member = 4)
aweights&lt;- c(0.2, 0.1, 0.3, 0.4, 0.1, 0.2, 0.4, 0.3)
dim(aweights) &lt;- c(time = 2, member = 4)
res &lt;- BEI_TercilesWeighting(var_exp, aweights)

# Example 2 
var_exp &lt;- rnorm(48, 50, 9)
dim(var_exp) &lt;- c(time = 2, member = 4, lat = 2, lon = 3)
aweights&lt;- c(0.2, 0.1, 0.3, 0.4, 0.1, 0.2, 0.4, 0.3)
dim(aweights) &lt;- c(time = 2, member = 4)
res &lt;- BEI_TercilesWeighting(var_exp, aweights)
</code></pre>

<hr>
<h2 id='BEI_Weights'>Computing the weights for SFSs using the Best Index PDFs.</h2><span id='topic+BEI_Weights'></span>

<h3>Description</h3>

<p>This function implements the computation to obtain the
normalized weights for each member of each Seasonal Forecast Systems (SFS) 
or dataset using the Probability Density Functions (PDFs) indicated by the 
parameter 'pdf_weight' (for instance the Best Index estimation obtained 
using the 'PDFBest' function). The weight of each member is proportional to
the probability of its index calculated with the PDF &quot;pdf_weight&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BEI_Weights(index_weight, pdf_weight, time_dim_name = "time")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BEI_Weights_+3A_index_weight">index_weight</code></td>
<td>
<p>Index (e.g. NAO index) array, from a dataset of SFSs
for a period of years, with at least dimensions 'member'. 
Additional dimensions, for instance, a temporal dimension as 'time', 
must have the same lenght in both parameters, 'index_weight' and 
'pdf_weight'.</p>
</td></tr>
<tr><td><code id="BEI_Weights_+3A_pdf_weight">pdf_weight</code></td>
<td>
<p>Statistics array to define a Gaussian PDF with at least 
dimensions 'statistic'. The firt statistic is the parameter 'mean' of the PDF 
and the second statistic is the parameter 'standard deviation' of the PDF.</p>
</td></tr>
<tr><td><code id="BEI_Weights_+3A_time_dim_name">time_dim_name</code></td>
<td>
<p>A character string indicating the name of the temporal 
dimension, by default 'time'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>BEI_Weights() returns a normalized weights array with the same 
dimensions that index_weight.
</p>


<h3>Author(s)</h3>

<p>Eroteida Sanchez-Garcia - AEMET, <a href="mailto:esanchezg@aemet.es">esanchezg@aemet.es</a>
</p>


<h3>References</h3>

<p>Regionally improved seasonal forecast of precipitation through 
Best estimation of winter NAO, Sanchez-Garcia, E. et al.,
Adv. Sci. Res., 16, 165174, 2019, doi: <a href="https://doi.org/10.5194/asr-16-165-2019">10.5194/asr-16-165-2019</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example for the BEI_Weights function
index_weight &lt;- 1 : (10 * 3 * 5 * 1)
dim(index_weight) &lt;- c(sdate = 10, dataset = 3, member = 5, season = 1)
pdf_weight &lt;- 1 : (10 * 3 * 2 * 1)
dim(pdf_weight) &lt;- c(sdate = 10, dataset = 3, statistic = 2, season = 1)
res &lt;- BEI_Weights(index_weight, pdf_weight)
dim(res)
# sdate   dataset    member season
#    10         3         5      1

</code></pre>

<hr>
<h2 id='BiasCorrection'>Bias Correction based on the mean and standard deviation adjustment</h2><span id='topic+BiasCorrection'></span>

<h3>Description</h3>

<p>This function applies the simple bias adjustment technique 
described in Torralba et al. (2017). The adjusted forecasts have an equivalent 
standard deviation and mean to that of the reference dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BiasCorrection(
  exp,
  obs,
  exp_cor = NULL,
  na.rm = FALSE,
  memb_dim = "member",
  sdate_dim = "sdate",
  dat_dim = NULL,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BiasCorrection_+3A_exp">exp</code></td>
<td>
<p>A multidimensional array with named dimensions containing the 
seasonal forecast experiment data with at least time and member dimensions.</p>
</td></tr>
<tr><td><code id="BiasCorrection_+3A_obs">obs</code></td>
<td>
<p>A multidimensional array with named dimensions containing the 
observed data with at least time dimension.</p>
</td></tr>
<tr><td><code id="BiasCorrection_+3A_exp_cor">exp_cor</code></td>
<td>
<p>A multidimensional array with named dimensions containing the 
seasonal forecast experiment to be corrected with at least time and member 
dimension. If it is NULL, the 'exp' forecast will be corrected. If there is 
only one corrected dataset, it should not have dataset dimension. If there 
is a corresponding corrected dataset for each 'exp' forecast, the dataset 
dimension must have the same length as in 'exp'. The default value is NULL.</p>
</td></tr>
<tr><td><code id="BiasCorrection_+3A_na.rm">na.rm</code></td>
<td>
<p>A logical value indicating whether missing values should be 
stripped before the computation proceeds, by default it is set to FALSE.</p>
</td></tr>
<tr><td><code id="BiasCorrection_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the member 
dimension. By default, it is set to 'member'.</p>
</td></tr>
<tr><td><code id="BiasCorrection_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. By default, it is set to 'sdate'.</p>
</td></tr>
<tr><td><code id="BiasCorrection_+3A_dat_dim">dat_dim</code></td>
<td>
<p>A character string indicating the name of dataset dimension. 
The length of this dimension can be different between 'exp' and 'obs'. 
The default value is NULL.</p>
</td></tr>
<tr><td><code id="BiasCorrection_+3A_ncores">ncores</code></td>
<td>
<p>An integer that indicates the number of cores for parallel 
computations using multiApply function. The default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An array containing the bias corrected forecasts with the dimensions 
nexp, nobs and same dimensions as in the 'exp' object. nexp is the number of 
experiment (i.e., 'dat_dim' in exp), and nobs is the number of observation 
(i.e., 'dat_dim' in obs). If dat_dim is NULL, nexp and nobs are omitted. If 
'exp_cor' is provided the returned array will be with the same dimensions as 
'exp_cor'.
</p>


<h3>Author(s)</h3>

<p>Verónica Torralba, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>
</p>


<h3>References</h3>

<p>Torralba, V., F.J. Doblas-Reyes, D. MacLeod, I. Christel and M. 
Davis (2017). Seasonal climate prediction: a new source of information for the 
management of wind energy resources. Journal of Applied Meteorology and 
Climatology, 56, 1231-1247, doi: <a href="https://doi.org/10.1175/JAMC-D-16-0204.1">10.1175/JAMC-D-16-0204.1</a>. (CLIM4ENERGY, 
EUPORIAS, NEWA, RESILIENCE, SPECS)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mod1 &lt;- 1 : (1 * 3 * 4 * 5 * 6 * 7)
dim(mod1) &lt;- c(dataset = 1, member = 3, sdate = 4, time = 5, lat = 6, lon = 7)
obs1 &lt;- 1 : (1 * 1 * 4 * 5 * 6 * 7)
dim(obs1) &lt;- c(dataset = 1, member = 1, sdate = 4, time = 5, lat = 6, lon = 7)
a &lt;- BiasCorrection(exp = mod1, obs = obs1)
</code></pre>

<hr>
<h2 id='Calibration'>Forecast Calibration</h2><span id='topic+Calibration'></span>

<h3>Description</h3>

<p>Five types of member-by-member bias correction can be performed. 
The <code>"bias"</code> method corrects the bias only, the <code>"evmos"</code> method 
applies a variance inflation technique to ensure the correction of the bias 
and the correspondence of variance between forecast and observation (Van 
Schaeybroeck and Vannitsem, 2011). The ensemble calibration methods 
<code>"mse_min"</code> and <code>"crps_min"</code> correct the bias, the overall forecast 
variance and the ensemble spread as described in Doblas-Reyes et al. (2005) 
and Van Schaeybroeck and Vannitsem (2015), respectively. While the 
<code>"mse_min"</code> method minimizes a constrained mean-squared error using three 
parameters, the <code>"crps_min"</code> method features four parameters and 
minimizes the Continuous Ranked Probability Score (CRPS). The 
<code>"rpc-based"</code> method adjusts the forecast variance ensuring that the 
ratio of predictable components (RPC) is equal to one, as in Eade et al. 
(2014). Both in-sample or our out-of-sample (leave-one-out cross 
validation) calibration are possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Calibration(
  exp,
  obs,
  exp_cor = NULL,
  cal.method = "mse_min",
  eval.method = "leave-one-out",
  multi.model = FALSE,
  na.fill = TRUE,
  na.rm = TRUE,
  apply_to = NULL,
  alpha = NULL,
  memb_dim = "member",
  sdate_dim = "sdate",
  dat_dim = NULL,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Calibration_+3A_exp">exp</code></td>
<td>
<p>A multidimensional array with named dimensions (at least 'sdate' 
and 'member') containing the seasonal hindcast experiment data. The hindcast 
is used to calibrate the forecast in case the forecast is provided; if not, 
the same hindcast will be calibrated instead.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_obs">obs</code></td>
<td>
<p>A multidimensional array with named dimensions (at least 'sdate') 
containing the observed data.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_exp_cor">exp_cor</code></td>
<td>
<p>An optional multidimensional array with named dimensions (at 
least 'sdate' and 'member') containing the seasonal forecast experiment 
data. If the forecast is provided, it will be calibrated using the hindcast 
and observations; if not, the hindcast will be calibrated instead. If there  
is only one corrected dataset, it should not have dataset dimension. If there 
is a corresponding corrected dataset for each 'exp' forecast, the dataset 
dimension must have the same length as in 'exp'. The default value is NULL.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_cal.method">cal.method</code></td>
<td>
<p>A character string indicating the calibration method used, 
can be either <code>bias</code>, <code>evmos</code>, <code>mse_min</code>, <code>crps_min</code> 
or <code>rpc-based</code>. Default value is <code>mse_min</code>.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_eval.method">eval.method</code></td>
<td>
<p>A character string indicating the sampling method used, 
can be either <code>in-sample</code> or <code>leave-one-out</code>. Default value is 
the <code>leave-one-out</code> cross validation. In case the forecast is 
provided, any chosen eval.method is over-ruled and a third option is 
used.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_multi.model">multi.model</code></td>
<td>
<p>A boolean that is used only for the <code>mse_min</code> 
method. If multi-model ensembles or ensembles of different sizes are used, 
it must be set to <code>TRUE</code>. By default it is <code>FALSE</code>. Differences 
between the two approaches are generally small but may become large when 
using small ensemble sizes. Using multi.model when the calibration method 
is <code>bias</code>, <code>evmos</code> or <code>crps_min</code> will not affect the result.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_na.fill">na.fill</code></td>
<td>
<p>A boolean that indicates what happens in case calibration is 
not possible or will yield unreliable results. This happens when three or 
less forecasts-observation pairs are available to perform the training phase
of the calibration. By default <code>na.fill</code> is set to true such that NA 
values will be returned. If <code>na.fill</code> is set to false, the uncorrected 
data will be returned.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_na.rm">na.rm</code></td>
<td>
<p>A boolean that indicates whether to remove the NA values or 
not. The default value is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_apply_to">apply_to</code></td>
<td>
<p>A character string that indicates whether to apply the 
calibration to all the forecast (<code>"all"</code>) or only to those where the 
correlation between the ensemble mean and the observations is statistically 
significant (<code>"sign"</code>). Only useful if <code>cal.method == "rpc-based"</code>.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_alpha">alpha</code></td>
<td>
<p>A numeric value indicating the significance level for the 
correlation test. Only useful if <code>cal.method == "rpc-based" &amp; apply_to == 
"sign"</code>.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the member 
dimension. By default, it is set to 'member'.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. By default, it is set to 'sdate'.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_dat_dim">dat_dim</code></td>
<td>
<p>A character string indicating the name of dataset dimension. 
The length of this dimension can be different between 'exp' and 'obs'. 
The default value is NULL.</p>
</td></tr>
<tr><td><code id="Calibration_+3A_ncores">ncores</code></td>
<td>
<p>An integer that indicates the number of cores for parallel 
computation using multiApply function. The default value is NULL (one core).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Both the <code>na.fill</code> and <code>na.rm</code> parameters can be used to 
indicate how the function has to handle the NA values. The <code>na.fill</code> 
parameter checks whether there are more than three forecast-observations pairs 
to perform the computation. In case there are three or less pairs, the 
computation is not carried out, and the value returned by the function depends 
on the value of this parameter (either NA if <code>na.fill == TRUE</code> or the 
uncorrected value if <code>na.fill == TRUE</code>). On the other hand, <code>na.rm</code> 
is used to indicate the function whether to remove the missing values during 
the computation of the parameters needed to perform the calibration.
</p>


<h3>Value</h3>

<p>An array containing the calibrated forecasts with the dimensions 
nexp, nobs and same dimensions as in the 'exp' array. nexp is the number of 
experiment (i.e., 'dat_dim' in exp), and nobs is the number of observation 
(i.e., 'dat_dim' in obs). If dat_dim is NULL, nexp and nobs are omitted. 
If 'exp_cor' is provided the returned array will be with the same dimensions as 
'exp_cor'.
</p>


<h3>Author(s)</h3>

<p>Verónica Torralba, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>
</p>
<p>Bert Van Schaeybroeck, <a href="mailto:bertvs@meteo.be">bertvs@meteo.be</a>
</p>


<h3>References</h3>

<p>Doblas-Reyes F.J, Hagedorn R, Palmer T.N. The rationale behind the 
success of multi-model ensembles in seasonal forecasting-II calibration and 
combination. Tellus A. 2005;57:234-252. doi:10.1111/j.1600-0870.2005.00104.x
</p>
<p>Eade, R., Smith, D., Scaife, A., Wallace, E., Dunstone, N., 
Hermanson, L., &amp; Robinson, N. (2014). Do seasonal-to-decadal climate 
predictions underestimate the predictability of the read world? Geophysical 
Research Letters, 41(15), 5620-5628. doi: <a href="https://doi.org/10.1002/2014GL061146">10.1002/2014GL061146</a>
</p>
<p>Van Schaeybroeck, B., &amp; Vannitsem, S. (2011). Post-processing 
through linear regression. Nonlinear Processes in Geophysics, 18(2), 
147. doi: <a href="https://doi.org/10.5194/npg-18-147-2011">10.5194/npg-18-147-2011</a>
</p>
<p>Van Schaeybroeck, B., &amp; Vannitsem, S. (2015). Ensemble 
post-processing using member-by-member approaches: theoretical aspects. 
Quarterly Journal of the Royal Meteorological Society, 141(688), 807-818.  
doi: <a href="https://doi.org/10.1002/qj.2397">10.1002/qj.2397</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CST_Start">CST_Start</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mod1 &lt;- 1 : (1 * 3 * 4 * 5 * 6 * 7)
dim(mod1) &lt;- c(dataset = 1, member = 3, sdate = 4, ftime = 5, lat = 6, lon = 7)
obs1 &lt;- 1 : (1 * 1 * 4 * 5 * 6 * 7)
dim(obs1) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 5, lat = 6, lon = 7)
a &lt;- Calibration(exp = mod1, obs = obs1)

</code></pre>

<hr>
<h2 id='CategoricalEnsCombination'>Make categorical forecast based on a multi-model forecast with potential for 
calibrate</h2><span id='topic+CategoricalEnsCombination'></span>

<h3>Description</h3>

<p>This function converts a multi-model ensemble forecast into a 
categorical forecast by giving the probability for each category. Different 
methods are available to combine the different ensemble forecasting models 
into probabilistic categorical forecasts. 
</p>
<p>See details in ?CST_CategoricalEnsCombination
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CategoricalEnsCombination(fc, obs, cat.method, eval.method, amt.cat, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CategoricalEnsCombination_+3A_fc">fc</code></td>
<td>
<p>A multi-dimensional array with named dimensions containing the 
seasonal forecast experiment data in the element named <code>$data</code>. The 
amount of forecasting models is equal to the size of the <code>dataset</code> 
dimension of the data array. The amount of members per model may be 
different. The  size of the <code>member</code> dimension of the data array is 
equal to the maximum of the ensemble members among the models. Models with 
smaller ensemble sizes have residual indices of <code>member</code> dimension in 
the data array filled with NA values.</p>
</td></tr>
<tr><td><code id="CategoricalEnsCombination_+3A_obs">obs</code></td>
<td>
<p>A multidimensional array with named dimensions containing the 
observed data in the element named <code>$data</code>.</p>
</td></tr>
<tr><td><code id="CategoricalEnsCombination_+3A_cat.method">cat.method</code></td>
<td>
<p>Method used to produce the categorical forecast, can be 
either <code>pool</code>, <code>comb</code>, <code>mmw</code> or <code>obs</code>. The method pool 
assumes equal weight for all ensemble members while the method comb assumes 
equal weight for each model. The weighting method is descirbed in 
Rajagopalan et al. (2002), Robertson et al. (2004) and Van Schaeybroeck and 
Vannitsem (2019). Finally, the <code>obs</code> method classifies the observations
into the different categories and therefore contains only 0 and 1 values.</p>
</td></tr>
<tr><td><code id="CategoricalEnsCombination_+3A_eval.method">eval.method</code></td>
<td>
<p>Is the sampling method used, can be either 
<code>"in-sample"</code> or <code>"leave-one-out"</code>. Default value is the 
<code>"leave-one-out"</code> cross validation.</p>
</td></tr>
<tr><td><code id="CategoricalEnsCombination_+3A_amt.cat">amt.cat</code></td>
<td>
<p>Is the amount of categories. Equally-sized quantiles will be 
calculated based on the amount of categories.</p>
</td></tr>
<tr><td><code id="CategoricalEnsCombination_+3A_...">...</code></td>
<td>
<p>Other parameters to be passed on to the calibration procedure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An array containing the categorical forecasts in the element called 
<code>$data</code>. The first two dimensions of the returned object are named 
dataset and member and are both of size one. An additional dimension named 
category is introduced and is of size amt.cat.
</p>


<h3>Author(s)</h3>

<p>Bert Van Schaeybroeck, <a href="mailto:bertvs@meteo.be">bertvs@meteo.be</a>
</p>


<h3>References</h3>

<p>Rajagopalan, B., Lall, U., &amp; Zebiak, S. E. (2002). Categorical 
climate forecasts through regularization and optimal combination of multiple 
GCM ensembles. Monthly Weather Review, 130(7), 1792-1811.
</p>
<p>Robertson, A. W., Lall, U., Zebiak, S. E., &amp; Goddard, L. (2004). 
Improved combination of multiple atmospheric GCM ensembles for seasonal 
prediction. Monthly Weather Review, 132(12), 2732-2744.
</p>
<p>Van Schaeybroeck, B., &amp; Vannitsem, S. (2019). Postprocessing of 
Long-Range Forecasts. In Statistical Postprocessing of Ensemble Forecasts (pp. 267-290).
</p>

<hr>
<h2 id='CST_AdamontAnalog'>CST_AdamontAnalog finds analogous data in the reference dataset to experiment
data based on weather types</h2><span id='topic+CST_AdamontAnalog'></span><span id='topic+AdamontAnalog'></span>

<h3>Description</h3>

<p>This function searches for analogs in a reference dataset for 
experiment data, based on corresponding weather types. The experiment data is
typically a hindcast, observations are typically provided by reanalysis data.
</p>
<p>This function searches for analogs in a reference dataset for 
experiment data, based on corresponding weather types. The experiment data is
typically a hindcast, observations are typically provided by reanalysis data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_AdamontAnalog(
  exp,
  obs,
  wt_exp,
  wt_obs,
  nanalogs,
  method = "pattcorr",
  thres = NULL,
  search_obsdims = c("member", "sdate", "ftime"),
  londim = "lon",
  latdim = "lat"
)

AdamontAnalog(
  exp,
  obs,
  wt_exp,
  wt_obs,
  nanalogs = 5,
  method = "pattcorr",
  thres = NULL,
  search_obsdims = c("member", "sdate", "ftime"),
  londim = "lon",
  latdim = "lat"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_AdamontAnalog_+3A_exp">exp</code></td>
<td>
<p>A multidimensional array with named dimensions containing the 
experiment data.</p>
</td></tr>
<tr><td><code id="CST_AdamontAnalog_+3A_obs">obs</code></td>
<td>
<p>A multidimensional array with named dimensions containing the 
reference data. Note that lat/lon dimensions need to be the same as 
<code>exp</code>.</p>
</td></tr>
<tr><td><code id="CST_AdamontAnalog_+3A_wt_exp">wt_exp</code></td>
<td>
<p>Corresponding weather types (same dimensions as <code>exp$data</code> 
but lat/lon).</p>
</td></tr>
<tr><td><code id="CST_AdamontAnalog_+3A_wt_obs">wt_obs</code></td>
<td>
<p>Corresponding weather types (same dimensions as <code>obs$data</code>
but lat/lon).</p>
</td></tr>
<tr><td><code id="CST_AdamontAnalog_+3A_nanalogs">nanalogs</code></td>
<td>
<p>Integer defining the number of analog values to return
(default: 5).</p>
</td></tr>
<tr><td><code id="CST_AdamontAnalog_+3A_method">method</code></td>
<td>
<p>A character string indicating the method used for analog
definition. It can be:
</p>

<ul>
<li><p>'pattcorr': pattern correlation.
</p>
</li>
<li><p>'rain1' (for precip patterns): rain occurrence consistency.
</p>
</li>
<li><p>'rain01' (for precip patterns): rain occurrence/non occurrence 
consistency
</p>
</li></ul>
</td></tr>
<tr><td><code id="CST_AdamontAnalog_+3A_thres">thres</code></td>
<td>
<p>Real number indicating the threshold to define rain 
occurrence/non occurrence in rain (0)1.</p>
</td></tr>
<tr><td><code id="CST_AdamontAnalog_+3A_search_obsdims">search_obsdims</code></td>
<td>
<p>List of dimensions in <code>obs</code> along which analogs are
searched for.</p>
</td></tr>
<tr><td><code id="CST_AdamontAnalog_+3A_londim">londim</code></td>
<td>
<p>Name of longitude dimension.</p>
</td></tr>
<tr><td><code id="CST_AdamontAnalog_+3A_latdim">latdim</code></td>
<td>
<p>Name of latitude dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>analog_vals An object of class <code>s2dv_cube</code> containing 
nanalogs analog values for each value of <code>exp</code> input data.
</p>
<p>analog_vals An array containing nanalogs analog values.
</p>


<h3>Author(s)</h3>

<p>Paola Marson, <a href="mailto:paola.marson@meteo.fr">paola.marson@meteo.fr</a> for PROSNOW version
</p>
<p>Lauriane Batté, <a href="mailto:lauriane.batte@meteo.fr">lauriane.batte@meteo.fr</a> for CSTools adaptation
</p>


<h3>Examples</h3>

<pre><code class='language-R'>wt_exp &lt;- sample(1:3, 15*6*3, replace = TRUE)
dim(wt_exp) &lt;- c(dataset = 1, member = 15, sdate = 6, ftime = 3)
wt_obs &lt;- sample(1:3, 6*3, replace = TRUE)
dim(wt_obs) &lt;- c(dataset = 1, member = 1, sdate = 6, ftime = 3)
exp &lt;- NULL
exp$data &lt;- 1 : c(1 * 15 * 6 * 3 * 8 * 8)
dim(exp$data) &lt;- c(dataset = 1, member = 15, sdate = 6, ftime = 3,
                  lat = 8, lon = 8)
class(exp) &lt;- 's2dv_cube'
obs &lt;- NULL
obs$data &lt;- 101 : c(100 + 1 * 1 * 6 * 3 * 8 * 8)
dim(obs$data) &lt;- c(dataset = 1, member = 1, sdate = 6, ftime = 3,
                  lat = 8, lon = 8)
class(obs) &lt;- 's2dv_cube'
analog_vals &lt;- CST_AdamontAnalog(exp = exp, obs = obs, wt_exp = wt_exp, 
                                wt_obs = wt_obs, nanalogs = 2)
wt_exp &lt;- sample(1:3, 15*6*3, replace = TRUE)
dim(wt_exp) &lt;- c(dataset = 1, member = 15, sdate = 6, ftime = 3)
wt_obs &lt;- sample(1:3, 6*3, replace = TRUE)
dim(wt_obs) &lt;- c(dataset = 1, member = 1, sdate = 6, ftime = 3)
exp &lt;- 1 : c(1 * 15 * 6 * 3 * 8 * 8)
dim(exp) &lt;- c(dataset = 1, member = 15, sdate = 6, ftime = 3, lat = 8, lon = 8)
obs &lt;- 101 : c(100 + 1 * 1 * 6 * 3 * 8 * 8)
dim(obs) &lt;- c(dataset = 1, member = 1, sdate = 6, ftime = 3, lat = 8, lon = 8)
analog_vals &lt;- AdamontAnalog(exp = exp, obs = obs, wt_exp = wt_exp, 
                            wt_obs = wt_obs, nanalogs = 2)
</code></pre>

<hr>
<h2 id='CST_AdamontQQCorr'>CST_AdamontQQCorr computes quantile-quantile correction of seasonal or 
decadal forecast data using weather types</h2><span id='topic+CST_AdamontQQCorr'></span>

<h3>Description</h3>

<p>This function computes a quantile mapping based on weather types 
for experiment data (typically a hindcast) onto reference <code>obs</code>,
typically provided by reanalysis data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_AdamontQQCorr(
  exp,
  wt_exp,
  obs,
  wt_obs,
  corrdims = c("member", "sdate", "ftime"),
  londim = "lon",
  latdim = "lat"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_AdamontQQCorr_+3A_exp">exp</code></td>
<td>
<p>Experiment data an object of class <code>s2dv_cube</code>.</p>
</td></tr>
<tr><td><code id="CST_AdamontQQCorr_+3A_wt_exp">wt_exp</code></td>
<td>
<p>Corresponding weather types (same dimensions as <code>exp$data</code>
but lat/lon).</p>
</td></tr>
<tr><td><code id="CST_AdamontQQCorr_+3A_obs">obs</code></td>
<td>
<p>Reference data, also of class <code>s2dv_cube</code>. lat/lon dimensions
can differ from <code>exp</code> if non rectilinear latlon grids are used, 
in which case regrid should be set to TRUE and .NearestNeighbors <code>NN</code>
output should be provided.</p>
</td></tr>
<tr><td><code id="CST_AdamontQQCorr_+3A_wt_obs">wt_obs</code></td>
<td>
<p>Corresponding weather types (same dimensions as <code>obs</code> but
lat/lon).</p>
</td></tr>
<tr><td><code id="CST_AdamontQQCorr_+3A_corrdims">corrdims</code></td>
<td>
<p>List of dimensions in <code>exp</code> for which quantile mapping
correction is applied.</p>
</td></tr>
<tr><td><code id="CST_AdamontQQCorr_+3A_londim">londim</code></td>
<td>
<p>Character name of longitude dimension in <code>exp</code> and 
<code>obs</code>.</p>
</td></tr>
<tr><td><code id="CST_AdamontQQCorr_+3A_latdim">latdim</code></td>
<td>
<p>Character name of latitude dimension in <code>exp</code> and 
<code>obs</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>s2dv_cube</code> containing experiment data on the
lat/lon grid of <code>obs</code> input data, corrected by quantile mapping 
depending on the weather types <code>wt_exp</code>.
</p>


<h3>Author(s)</h3>

<p>Lauriane Batté, <a href="mailto:lauriane.batte@meteo.fr">lauriane.batte@meteo.fr</a>
</p>
<p>Paola Marson, <a href="mailto:paola.marson@meteo.fr">paola.marson@meteo.fr</a>
</p>
<p>Gildas Dayon, <a href="mailto:gildas.dayon@meteo.fr">gildas.dayon@meteo.fr</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>wt_exp &lt;- c(1,1,2,3,3,2,2,1,1,2,2,3)
dim(wt_exp) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 3)
wt_obs &lt;- c(3,3,1,2,2,2,2,1,3,1,1,2) 
dim(wt_obs) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 3)
exp &lt;- NULL
exp$data &lt;-  1 : c(1 * 1 * 4 * 3 * 4 * 4)
dim(exp$data) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 3,
                  lat = 4, lon = 4)
class(exp) &lt;- 's2dv_cube'
obs &lt;- NULL
obs$data &lt;-  101 : c(100 + 1 * 1 * 4 * 3 * 4 * 4)
dim(obs$data) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 3,
                  lat = 4, lon = 4)
class(obs) &lt;- 's2dv_cube'
exp_corr &lt;- CST_AdamontQQCorr(exp = exp, wt_exp = wt_exp, 
                             obs = obs, wt_obs = wt_obs, 
                             corrdims = c('dataset','member','sdate','ftime'))
</code></pre>

<hr>
<h2 id='CST_Analogs'>Downscaling using Analogs based on large scale fields.</h2><span id='topic+CST_Analogs'></span>

<h3>Description</h3>

<p>This function perform a downscaling using Analogs. To compute 
the analogs, the function search for days with similar large scale conditions
to downscaled fields to a local scale. The large scale and the local scale 
regions are defined by the user. The large scale is usually given by 
atmospheric circulation as sea level pressure or geopotential height 
(Yiou et al, 2013) but the function gives the possibility to use another 
field. The local scale will be usually given by precipitation or temperature 
fields, but might be another variable.The analogs function will find the best
analogs based in Minimum Euclidean distance in the large scale pattern
(i.e.SLP).
</p>
<p>The search of analogs must be done in the longest dataset posible. This is 
important since it is necessary to have a good representation of the 
possible states of the field in the past, and therefore, to get better 
analogs. 
This function has not constrains of specific regions, variables to downscale,
or data to be used (seasonal forecast data, climate projections data, 
reanalyses data). The regrid into a finner scale is done interpolating with 
CST_Start. Then, this interpolation is corrected selecting the analogs in the 
large and local scale in based of the observations. The function is an 
adapted version of the method of Yiou et al 2013. For an advanced search of 
Analogs (multiple Analogs, different criterias, further information from the
metrics and date of the selected Analogs) use the'Analog'
function within 'CSTools' package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_Analogs(
  expL,
  obsL,
  expVar = NULL,
  obsVar = NULL,
  sdate_dim = "sdate",
  region = NULL,
  criteria = "Large_dist",
  excludeTime = NULL,
  time_expL = NULL,
  time_obsL = NULL,
  nAnalogs = NULL,
  AnalogsInfo = FALSE,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_Analogs_+3A_expl">expL</code></td>
<td>
<p>An 's2dv_cube' object containing the experimental field on the 
large scale for which the analog is aimed. This field is used to in all the
criterias. If parameter 'expVar' is not provided, the function will return 
the expL analog. The element 'data' in the 's2dv_cube' object must have, at
least, latitudinal and longitudinal dimensions. The object is expect to be 
already subset for the desired large scale region. Latitudinal dimension 
accepted names: 'lat', 'lats', 'latitude', 'y', 'j', 'nav_lat'. Longitudinal 
dimension accepted names: 'lon', 'lons','longitude', 'x', 'i', 'nav_lon'.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_obsl">obsL</code></td>
<td>
<p>An 's2dv_cube' object containing the observational field on the
large scale. The element 'data' in the 's2dv_cube' object must have the same 
latitudinal and longitudinal dimensions as parameter 'expL' and a temporal 
dimension with the maximum number of available observations.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_expvar">expVar</code></td>
<td>
<p>An 's2dv_cube' object containing the experimental field on the
local scale, usually a different variable to the parameter 'expL'. If it is 
not NULL (by default, NULL), the returned field by this function will be the 
analog of parameter 'expVar'.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_obsvar">obsVar</code></td>
<td>
<p>An 's2dv_cube' containing the field of the same variable as the 
passed in parameter 'expVar' for the same region.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. By default, it is set to 'sdate'.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_region">region</code></td>
<td>
<p>A vector of length four indicating the minimum longitude, 
the maximum longitude, the minimum latitude and the maximum latitude.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_criteria">criteria</code></td>
<td>
<p>A character string indicating the criteria to be used for the
selection of analogs:
</p>

<ul>
<li><p>Large_dist minimum Euclidean distance in the large scale pattern; 
</p>
</li>
<li><p>Local_dist minimum Euclidean distance in the large scale pattern 
and minimum Euclidean distance in the local scale pattern; and
</p>
</li>
<li><p>Local_cor minimum Euclidean distance in the large scale pattern, 
minimum Euclidean distance in the local scale pattern and highest 
correlation in the local variable to downscale.</p>
</li></ul>

<p>Criteria 'Large_dist' is recommended for CST_Analogs, for an advanced use of 
the criterias 'Local_dist' and 'Local_cor' use 'Analogs' function.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_excludetime">excludeTime</code></td>
<td>
<p>An array of N named dimensions (coinciding with time 
dimensions in expL)of character string(s) indicating the date(s) of the 
observations in the format &quot;dd/mm/yyyy&quot; to be excluded during the search of 
analogs. It can be NULL but if expL is not a forecast (time_expL contained in
time_obsL), by default time_expL will be removed during the search of analogs.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_time_expl">time_expL</code></td>
<td>
<p>A character string indicating the date of the experiment 
in the same format than time_obsL (i.e. &quot;yyyy-mm-dd&quot;). By default it is NULL 
and dates are taken from element <code>$attrs$Dates</code> from expL.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_time_obsl">time_obsL</code></td>
<td>
<p>A character string indicating the date of the observations 
in the date format (i.e. &quot;yyyy-mm-dd&quot;). By default it is NULL and dates are 
taken from element <code>$attrs$Dates</code> from obsL. It must have time 
dimensions.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_nanalogs">nAnalogs</code></td>
<td>
<p>Number of Analogs to be selected to apply the criterias 
'Local_dist' or 'Local_cor'. This is not the necessary the number of analogs 
that the user can get, but the number of events with minimum distance in 
which perform the search of the best Analog. The default value for the 
'Large_dist' criteria is 1, for 'Local_dist' and 'Local_cor' criterias must
be greater than 1 in order to match with the first criteria, if nAnalogs is
NULL for 'Local_dist' and 'Local_cor' the default value will be set at the 
length of 'time_obsL'. If AnalogsInfo is FALSE the function returns just 
the best analog.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_analogsinfo">AnalogsInfo</code></td>
<td>
<p>A logical value. TRUE to get a list with two elements: 
1) the downscaled field and 2) the AnalogsInfo which contains: 
a) the number of the best analogs, b) the corresponding value of the metric 
used in the selected criteria (distance values for Large_dist and Local_dist, 
correlation values for Local_cor), c)dates of the analogs). The analogs are 
listed in decreasing order, the first one is the best analog (i.e if the 
selected criteria is Local_cor the best analog will be the one with highest 
correlation, while for Large_dist criteria the best analog will be the day 
with minimum Euclidean distance). Set to FALSE to get a single analog, the 
best analog, for instance for downscaling.</p>
</td></tr>
<tr><td><code id="CST_Analogs_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to use in parallel computation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An 's2dv_cube' object containing an array with the dowscaled values of 
the best analogs in element 'data'. If 'AnalogsInfo' is TRUE, 'data' is a list 
with an array of the downscaled fields and the analogs information in 
elements 'analogs', 'metric' and 'dates'.
</p>


<h3>Author(s)</h3>

<p>M. Carmen Alvarez-Castro, <a href="mailto:carmen.alvarez-castro@cmcc.it">carmen.alvarez-castro@cmcc.it</a>
</p>
<p>Maria M. Chaves-Montero, <a href="mailto:mariadm.chaves@cmcc.it">mariadm.chaves@cmcc.it</a>
</p>
<p>Veronica Torralba, <a href="mailto:veronica.torralba@cmcc.it">veronica.torralba@cmcc.it</a>
</p>
<p>Nuria Perez-Zanon <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>References</h3>

<p>Yiou, P., T. Salameh, P. Drobinski, L. Menut, R. Vautard,
and M. Vrac, 2013 : Ensemble reconstruction of the atmospheric column 
from surface pressure using analogues.  Clim. Dyn., 41, 1419-1437. 
<a href="mailto:pascal.yiou@lsce.ipsl.fr">pascal.yiou@lsce.ipsl.fr</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CST_Start">CST_Start</a></code>, <code><a href="startR.html#topic+Start">Start</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>expL &lt;- rnorm(1:200)
dim(expL) &lt;- c(member = 10, lat = 4, lon = 5)
obsL &lt;- c(rnorm(1:180), expL[1, , ]*1.2)
dim(obsL) &lt;- c(time = 10, lat = 4, lon = 5)
time_obsL &lt;- as.POSIXct(paste(rep("01", 10), rep("01", 10), 1994:2003, sep = "-"), 
                       format = "%d-%m-%y")
dim(time_obsL) &lt;- c(time = 10)
time_expL &lt;- time_obsL[1]
dim(time_expL) &lt;- c(time = 1)
lon &lt;-  seq(-1, 5, 1.5)
lat &lt;- seq(30, 35, 1.5)
coords &lt;- list(lon = seq(-1, 5, 1.5), lat = seq(30, 35, 1.5))
attrs_expL &lt;- list(Dates = time_expL)
attrs_obsL &lt;- list(Dates = time_obsL)
expL &lt;- list(data = expL, coords = coords, attrs = attrs_expL)
obsL &lt;- list(data = obsL, coords = coords, attrs = attrs_obsL)
class(expL) &lt;- 's2dv_cube'
class(obsL) &lt;- 's2dv_cube'
region &lt;- c(min(lon), max(lon), min(lat), max(lat))
downscaled_field &lt;- CST_Analogs(expL = expL, obsL = obsL, region = region)

</code></pre>

<hr>
<h2 id='CST_AnalogsPredictors'>AEMET Downscaling   
Precipitation and maximum and minimum temperature downscaling method 
based on analogs: synoptic situations and significant predictors.</h2><span id='topic+CST_AnalogsPredictors'></span>

<h3>Description</h3>

<p>This function downscales low resolution precipitation data (e.g. 
from Seasonal Forecast Models) through the association with an observational 
high resolution (HR) dataset (AEMET 5 km gridded data of daily precipitation 
(Peral et al., 2017)) and a collection of predictors and past synoptic 
situations similar to estimated day. The method uses three domains: 
</p>

<ul>
<li><p>Peninsular Spain and Balearic Islands domain (5 km resolution): HR precipitation 
and the downscaling result domain.
</p>
</li>
<li><p>Synoptic domain (low resolution, e.g. 1.5º x 1.5º): it should be 
centered over Iberian Peninsula and cover enough extension to detect 
as much synoptic situations as possible.
</p>
</li>
<li><p>Extended domain (low resolution, e.g. 1.5º x 1.5º): it should have the 
same resolution as synoptic domain. It is used for SLP Seasonal 
Forecast Models.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>CST_AnalogsPredictors(
  exp,
  slp,
  obs,
  lon,
  lat,
  slp_lon,
  slp_lat,
  var_name,
  hr_obs,
  tdates,
  ddates,
  restrain,
  dim_name_longitude = "lon",
  dim_name_latitude = "lat",
  dim_name_time = "time"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_AnalogsPredictors_+3A_exp">exp</code></td>
<td>
<p>List of arrays with downscaled period seasonal forecast data. The 
list has to contain model atmospheric variables (instantaneous 12h data) 
that must be indentify by parenthesis name. For precipitation:
</p>

<ul>
<li><p>u component of wind at 500 hPa (u500_mod) in m/s.
</p>
</li>
<li><p>v component of wind at 500 hPa (v500_mod) in m/s.
</p>
</li>
<li><p>temperature at 500 hPa (t500_mod) in K.
</p>
</li>
<li><p>temperature at 850 hPa (t850_mod) in K.
</p>
</li>
<li><p>specific humidity at 700 hPa (q700_mod) in g/kg. 
</p>
</li></ul>

<p>For temperature:
</p>

<ul>
<li><p>u component of wind at 500 hPa (u500_mod) in m/s.
</p>
</li>
<li><p>v component of wind at 500 hPa (v500_mod) in m/s.
</p>
</li>
<li><p>temperature at 500 hPa (t500_mod) in K.
</p>
</li>
<li><p>temperature at 700 hPa (t700_mod) in K. 
</p>
</li>
<li><p>temperature at 850 hPa (t850_mod) in K.
</p>
</li>
<li><p>specific humidity at 700 hPa (q700_mod) in g/kg. 
</p>
</li>
<li><p>2 meters temperature (tm2m_mod) in K.
</p>
</li></ul>

<p>The arrays must have at least three dimensions with names 'lon', 'lat' and 
'time'. (lon = gridpoints of longitude, lat = gridpoints of latitude, 
time = number of downscaling days) Seasonal forecast variables must have the 
same resolution and domain as reanalysis variables ('obs' parameter, below).</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_slp">slp</code></td>
<td>
<p>Array with atmospheric seasonal forecast model sea level pressure
(instantaneous 12h data) that must be indentify as 'slp' (hPa). It has the  
same resolution as 'exp' and 'obs' paremeters but with an extended domain.
This domain contains extra degrees (most in the north and west part) compare  
to synoptic domain. The array must have at least three dimensions with
names 'lon', 'lat' and 'time'.</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_obs">obs</code></td>
<td>
<p>List of arrays with training period reanalysis data. 
The list has to contain reanalysis atmospheric variables (instantaneous 
12h data) that must be indentify by parenthesis name. For precipitation:
</p>

<ul>
<li><p>u component of wind at 500 hPa (u500) in m/s.
</p>
</li>
<li><p>v component of wind at 500 hPa (v500) in m/s.
</p>
</li>
<li><p>temperature at 500 hPa (t500) in K.
</p>
</li>
<li><p>temperature at 850 hPa (t850) in K.
</p>
</li>
<li><p>sea level pressure (slp) in hPa.
</p>
</li>
<li><p>specific humidity at 700 hPa (q700) in g/kg.
</p>
</li></ul>

<p>For maximum and minimum temperature:
</p>

<ul>
<li><p>u component of wind at 500 hPa (u500) in m/s.
</p>
</li>
<li><p>v component of wind at 500 hPa (v500) in m/s.
</p>
</li>
<li><p>temperature at 500 hPa (t500) in K.
</p>
</li>
<li><p>temperature at 700 hPa (t700) in K.
</p>
</li>
<li><p>temperature at 850 hPa (t850) in K.
</p>
</li>
<li><p>sea level pressure (slp) in hPa.
</p>
</li>
<li><p>specific humidity at 700 hPa (q700) in g/kg
</p>
</li>
<li><p>2 meters temperature (tm2m) in K
</p>
</li></ul>

<p>The arrays must have at least three dimensions with names 'lon', 'lat' and 
'time'.</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_lon">lon</code></td>
<td>
<p>Vector of the synoptic longitude (from (-180º) to 180º), 
The vector must go from west to east. The same as for the training function.</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_lat">lat</code></td>
<td>
<p>Vector of the synoptic latitude. The vector must go from north to 
south. The same as for the training function.</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_slp_lon">slp_lon</code></td>
<td>
<p>Vector of the extended longitude (from (-180º) to 180º), 
The vector must go from west to east. The same as for the training function.</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_slp_lat">slp_lat</code></td>
<td>
<p>Vector of the extended latitude. The vector must go from north 
to south. The same as for the training function.</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_var_name">var_name</code></td>
<td>
<p>Variable name to downscale. There are two options: 'prec' for
precipitation and 'temp' for maximum and minimum temperature.</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_hr_obs">hr_obs</code></td>
<td>
<p>Local path of HR observational files (maestro and pcp/tmx-tmn). 
For precipitation and temperature can be downloaded from the following link:
<a href="https://www.aemet.es/en/serviciosclimaticos/cambio_climat/datos_diarios?w=2">https://www.aemet.es/en/serviciosclimaticos/cambio_climat/datos_diarios?w=2</a> 
respetively. Maestro file (maestro_red_hr_SPAIN.txt) has gridpoint (nptos), 
longitude (lon), latitude (lat) and altitude (alt) in columns (vector 
structure). Data file (pcp/tmx/tmn_red_SPAIN_1951-201903.txt) includes 5km 
resolution spanish daily data (precipitation or maximum and minimum 
temperature from january 1951 to june 2020. See README file for more 
information. IMPORTANT!: HR observational period must be the same as for 
reanalysis variables. It is assumed that the training period is smaller than 
the HR original one (1951-2019), so it is needed to make a new ascii file 
with the new period and the same structure as original, specifying the 
training dates in the name 
(e.g. 'pcp_red_SPAIN_19810101-19961231.txt' for '19810101-19961231' period).</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_tdates">tdates</code></td>
<td>
<p>Training period dates in format YYYYMMDD(start)-YYYYMMDD(end) 
(e.g. 19810101-20181231).</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_ddates">ddates</code></td>
<td>
<p>Downscaling period dates in format YYYYMMDD(start)-YYYYMMDD(end) 
(e.g. 20191001-20200331).</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_restrain">restrain</code></td>
<td>
<p>Output (list of matrix) obtained from 'training_analogs' 
function. For precipitation, 'restrain' object must contains um, vm, nger, 
gu92, gv92, gu52, gv52, neni, vdmin, vref, ccm, lab_pred and cor_pred 
variables. For maximum and minimum temperature, 'restrain' object must 
contains um, vm, insol, neni, vdmin y vref. See 'AnalogsPred_train.R' for 
more information.</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_dim_name_longitude">dim_name_longitude</code></td>
<td>
<p>A character string indicating the name of the 
longitude dimension, by default 'longitude'.</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_dim_name_latitude">dim_name_latitude</code></td>
<td>
<p>A character string indicating the name of the 
latitude dimension, by default 'latitude'.</p>
</td></tr>
<tr><td><code id="CST_AnalogsPredictors_+3A_dim_name_time">dim_name_time</code></td>
<td>
<p>A character string indicating the name of the time 
dimension, by default 'time'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix with seasonal forecast precipitation (mm) or maximum and 
minimum temperature (dozens of ºC) in a 5km x 5km regular grid over peninsular 
Spain and Balearic Islands. The resulted matrices have two dimensions 
('ddates' x 'nptos').(ddates = number of downscaling days and nptos = number 
of 'hr_obs' gridpoints).
</p>


<h3>Author(s)</h3>

<p>Marta Dominguez Alonso - AEMET, <a href="mailto:mdomingueza@aemet.es">mdomingueza@aemet.es</a>
</p>
<p>Nuria Perez-Zanon - BSC, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>

<hr>
<h2 id='CST_Anomaly'>Anomalies relative to a climatology along selected dimension with or without 
cross-validation</h2><span id='topic+CST_Anomaly'></span>

<h3>Description</h3>

<p>This function computes the anomalies relative to a climatology 
computed along the selected dimension (usually starting dates or forecast 
time) allowing the application or not of crossvalidated climatologies. The 
computation is carried out independently for experimental and observational 
data products.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_Anomaly(
  exp = NULL,
  obs = NULL,
  dim_anom = "sdate",
  cross = FALSE,
  memb_dim = "member",
  memb = TRUE,
  dat_dim = c("dataset", "member"),
  filter_span = NULL,
  ftime_dim = "ftime",
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_Anomaly_+3A_exp">exp</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by <code>CST_Start</code> 
function, containing the seasonal forecast experiment data in the element 
named <code>$data</code>.</p>
</td></tr>
<tr><td><code id="CST_Anomaly_+3A_obs">obs</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by <code>CST_Start</code> 
function, containing the observed data in the element named <code>$data</code>.</p>
</td></tr>
<tr><td><code id="CST_Anomaly_+3A_dim_anom">dim_anom</code></td>
<td>
<p>A character string indicating the name of the dimension 
along which the climatology will be computed. The default value is 'sdate'.</p>
</td></tr>
<tr><td><code id="CST_Anomaly_+3A_cross">cross</code></td>
<td>
<p>A logical value indicating whether cross-validation should be 
applied or not. Default = FALSE.</p>
</td></tr>
<tr><td><code id="CST_Anomaly_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the member 
dimension. It must be one dimension in 'exp' and 'obs'. If there is no 
member dimension, set NULL. The default value is 'member'.</p>
</td></tr>
<tr><td><code id="CST_Anomaly_+3A_memb">memb</code></td>
<td>
<p>A logical value indicating whether to subtract the climatology 
based on the individual members (TRUE) or the ensemble mean over all
members (FALSE) when calculating the anomalies. The default value is TRUE.</p>
</td></tr>
<tr><td><code id="CST_Anomaly_+3A_dat_dim">dat_dim</code></td>
<td>
<p>A character vector indicating the name of the dataset and 
member dimensions. If there is no dataset dimension, it can be NULL.
The default value is &quot;c('dataset', 'member')&quot;.</p>
</td></tr>
<tr><td><code id="CST_Anomaly_+3A_filter_span">filter_span</code></td>
<td>
<p>A numeric value indicating the degree of smoothing. This 
option is only available if parameter <code>cross</code> is set to FALSE.</p>
</td></tr>
<tr><td><code id="CST_Anomaly_+3A_ftime_dim">ftime_dim</code></td>
<td>
<p>A character string indicating the name of the temporal 
dimension where the smoothing with 'filter_span' will be applied. It cannot
be NULL if 'filter_span' is provided. The default value is 'ftime'.</p>
</td></tr>
<tr><td><code id="CST_Anomaly_+3A_ncores">ncores</code></td>
<td>
<p>An integer indicating the number of cores to use for parallel 
computation. The default value is NULL. It will be used only when 
'filter_span' is not NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two S3 objects, 'exp' and 'obs', of the class 
's2dv_cube', containing experimental and date-corresponding observational 
anomalies, respectively. These 's2dv_cube's can be ingested by other functions 
in CSTools.
</p>


<h3>Author(s)</h3>

<p>Perez-Zanon Nuria, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>
<p>Pena Jesus, <a href="mailto:jesus.pena@bsc.es">jesus.pena@bsc.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="s2dv.html#topic+Ano_CrossValid">Ano_CrossValid</a></code>, <code><a href="s2dv.html#topic+Clim">Clim</a></code> and 
<code><a href="#topic+CST_Start">CST_Start</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- 1 : (2 * 3 * 4 * 5 * 6 * 7)
dim(mod) &lt;- c(dataset = 2, member = 3, sdate = 4, ftime = 5, lat = 6, lon = 7)
obs &lt;- 1 : (1 * 1 * 4 * 5 * 6 * 7)
dim(obs) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 5, lat = 6, lon = 7)
lon &lt;- seq(0, 30, 5)
lat &lt;- seq(0, 25, 5)
coords &lt;- list(lon = lon, lat = lat)
exp &lt;- list(data = mod, coords = coords)
obs &lt;- list(data = obs, coords = coords)
attr(exp, 'class') &lt;- 's2dv_cube'
attr(obs, 'class') &lt;- 's2dv_cube'

anom &lt;- CST_Anomaly(exp = exp, obs = obs, cross = FALSE, memb = TRUE)

</code></pre>

<hr>
<h2 id='CST_BEI_Weighting'>Weighting SFSs of a CSTools object.</h2><span id='topic+CST_BEI_Weighting'></span>

<h3>Description</h3>

<p>Function to apply weights to a 's2dv_cube' object.  
It could return a weighted ensemble mean (deterministic output) or
the terciles probabilities (probabilistic output) for Seasonal Forecast 
Systems (SFSs).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_BEI_Weighting(
  var_exp,
  aweights,
  terciles = NULL,
  type = "ensembleMean",
  time_dim_name = "time",
  memb_dim = "member"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_BEI_Weighting_+3A_var_exp">var_exp</code></td>
<td>
<p>An object of the class 's2dv_cube' containing the variable
(e.g. precipitation, temperature, NAO index) array.
The var_exp object is expected to have an element named <code>$data</code> with
at least a temporal dimension and a dimension named 'member'.</p>
</td></tr>
<tr><td><code id="CST_BEI_Weighting_+3A_aweights">aweights</code></td>
<td>
<p>Normalized weights array with at least dimensions 
(time, member), when 'time' is the temporal dimension as default. 
When 'aweights' parameter has any other dimensions (as e.g. 'lat') and 
'var_exp' parameter has also the same dimension, they must be equals.</p>
</td></tr>
<tr><td><code id="CST_BEI_Weighting_+3A_terciles">terciles</code></td>
<td>
<p>A numeric array with at least one dimension 'tercil' equal to 
2, the first element is the lower tercil for a hindcast period, and the second  
element is the upper tercile. By default is NULL, the terciles are computed  
from var_exp data.</p>
</td></tr>
<tr><td><code id="CST_BEI_Weighting_+3A_type">type</code></td>
<td>
<p>A character string indicating the type of output. 
If 'type' =  'probs', the function returns, in the element data from 
'var_exp' parameter, an array with at least two 
or four dimensions depending if the variable is spatially aggregated variable 
(as e.g. NAO index), dimension (time, tercil) or it is spatial variable 
(as e.g. precipitation or temperature), dimension (time, tercile, lat, lon), 
containing the terciles probabilities computing with weighted members.
The first tercil is the lower tercile, the second is the normal tercile and
the third is the upper tercile. If 'type' =  'ensembleMean', the function 
returns, in the element data from 'var_exp' parameter, an array with at 
least one or three dimensions depending if the variable is a spatially 
aggregated variable (as e.g. NAO index)(time) or it is spatial variable (as 
e.g. precipitation or temperature) (time, lat, lon), containing the ensemble 
means computing with weighted members.</p>
</td></tr>
<tr><td><code id="CST_BEI_Weighting_+3A_time_dim_name">time_dim_name</code></td>
<td>
<p>A character string indicating the name of the 
temporal dimension, by default 'time'.</p>
</td></tr>
<tr><td><code id="CST_BEI_Weighting_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the 
member dimension, by default 'member'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>CST_BEI_Weighting() returns a CSTools object (i.e., of the
class 's2dv_cube').
This object has at least an element named <code>$data</code>
with at least a temporal dimension (and dimension 'tercil' when the output 
are tercile probabilities), containing the ensemble means computing with 
weighted members or probabilities of terciles.
</p>


<h3>Author(s)</h3>

<p>Eroteida Sanchez-Garcia - AEMET, <a href="mailto:esanchezg@aemet.es">esanchezg@aemet.es</a>
</p>


<h3>References</h3>

<p>Regionally improved seasonal forecast of precipitation through
Best estimation of winter NAO, Sanchez-Garcia, E. et al.,
Adv. Sci. Res., 16, 165174, 2019, doi: <a href="https://doi.org/10.5194/asr-16-165-2019">10.5194/asr-16-165-2019</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var_exp &lt;- 1 : (2 * 4 * 3 * 2)
dim(var_exp) &lt;- c(time = 2, member = 4, lat = 3, lon = 2)
aweights &lt;- c(0.2, 0.1, 0.3, 0.4, 0.1, 0.2, 0.4, 0.3, 0.1, 0.2, 0.4, 0.4, 0.1, 
             0.2, 0.4, 0.2)
dim(aweights) &lt;- c(time = 2, member = 4, dataset = 2)
var_exp &lt;- list(data = var_exp)
class(var_exp) &lt;- 's2dv_cube'
res_CST &lt;- CST_BEI_Weighting(var_exp, aweights)
</code></pre>

<hr>
<h2 id='CST_BiasCorrection'>Bias Correction based on the mean and standard deviation adjustment</h2><span id='topic+CST_BiasCorrection'></span>

<h3>Description</h3>

<p>This function applies the simple bias adjustment technique 
described in Torralba et al. (2017). The adjusted forecasts have an equivalent 
standard deviation and mean to that of the reference dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_BiasCorrection(
  exp,
  obs,
  exp_cor = NULL,
  na.rm = FALSE,
  memb_dim = "member",
  sdate_dim = "sdate",
  dat_dim = NULL,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_BiasCorrection_+3A_exp">exp</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by <code>CST_Start</code> 
function, containing the seasonal forecast experiment data in the element 
named <code>$data</code> with at least time and member dimensions.</p>
</td></tr>
<tr><td><code id="CST_BiasCorrection_+3A_obs">obs</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by <code>CST_Start</code> 
function, containing the observed data in the element named <code>$data</code> 
with at least time dimension.</p>
</td></tr>
<tr><td><code id="CST_BiasCorrection_+3A_exp_cor">exp_cor</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by 
<code>CST_Start</code> function, containing the seasonal forecast experiment to be 
corrected with at least time dimension. If it is NULL, the 'exp' forecast 
will be corrected. If there is only one corrected dataset, it should not  
have dataset dimension. If there is a corresponding corrected dataset for  
each 'exp' forecast, the dataset dimension must have the same length as in 
'exp'. The default value is NULL.</p>
</td></tr>
<tr><td><code id="CST_BiasCorrection_+3A_na.rm">na.rm</code></td>
<td>
<p>A logical value indicating whether missing values should be 
stripped before the computation proceeds, by default it is set to FALSE.</p>
</td></tr>
<tr><td><code id="CST_BiasCorrection_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the member 
dimension. By default, it is set to 'member'.</p>
</td></tr>
<tr><td><code id="CST_BiasCorrection_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. By default, it is set to 'sdate'.</p>
</td></tr>
<tr><td><code id="CST_BiasCorrection_+3A_dat_dim">dat_dim</code></td>
<td>
<p>A character string indicating the name of dataset dimension. 
The length of this dimension can be different between 'exp' and 'obs'. 
The default value is NULL.</p>
</td></tr>
<tr><td><code id="CST_BiasCorrection_+3A_ncores">ncores</code></td>
<td>
<p>An integer that indicates the number of cores for parallel 
computations using multiApply function. The default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>s2dv_cube</code> containing the bias corrected 
forecasts with the dimensions nexp, nobs and same dimensions as in the 'exp' 
object. nexp is the number of experiment (i.e., 'dat_dim' in exp), and nobs is 
the number of observation (i.e., 'dat_dim' in obs). If dat_dim is NULL, nexp 
and nobs are omitted. If 'exp_cor' is provided the returned array will be with 
the same dimensions as 'exp_cor'.
</p>


<h3>Author(s)</h3>

<p>Verónica Torralba, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>
</p>


<h3>References</h3>

<p>Torralba, V., F.J. Doblas-Reyes, D. MacLeod, I. Christel and M. 
Davis (2017). Seasonal climate prediction: a new source of information for 
the management of wind energy resources. Journal of Applied Meteorology and 
Climatology, 56, 1231-1247, doi: <a href="https://doi.org/10.1175/JAMC-D-16-0204.1">10.1175/JAMC-D-16-0204.1</a>. (CLIM4ENERGY, 
EUPORIAS, NEWA, RESILIENCE, SPECS)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mod1 &lt;- 1 : (1 * 3 * 4 * 5 * 6 * 7)
dim(mod1) &lt;- c(dataset = 1, member = 3, sdate = 4, time = 5, lat = 6, lon = 7)
obs1 &lt;- 1 : (1 * 1 * 4 * 5 * 6 * 7)
dim(obs1) &lt;- c(dataset = 1, member = 1, sdate = 4, time = 5, lat = 6, lon = 7)
lon &lt;- seq(0, 30, 5)
lat &lt;- seq(0, 25, 5)
coords &lt;- list(lat = lat, lon = lon)
exp &lt;- list(data = mod1, coords = coords)
obs &lt;- list(data = obs1, coords = coords)
attr(exp, 'class') &lt;- 's2dv_cube'
attr(obs, 'class') &lt;- 's2dv_cube'
a &lt;- CST_BiasCorrection(exp = exp, obs = obs)
</code></pre>

<hr>
<h2 id='CST_Calibration'>Forecast Calibration</h2><span id='topic+CST_Calibration'></span>

<h3>Description</h3>

<p>Five types of member-by-member bias correction can be performed. 
The <code>"bias"</code> method corrects the bias only, the <code>"evmos"</code> method 
applies a variance inflation technique to ensure the correction of the bias 
and the correspondence of variance between forecast and observation (Van 
Schaeybroeck and Vannitsem, 2011). The ensemble calibration methods 
<code>"mse_min"</code> and <code>"crps_min"</code> correct the bias, the overall forecast 
variance and the ensemble spread as described in Doblas-Reyes et al. (2005) 
and Van Schaeybroeck and Vannitsem (2015), respectively. While the 
<code>"mse_min"</code> method minimizes a constrained mean-squared error using three 
parameters, the <code>"crps_min"</code> method features four parameters and 
minimizes the Continuous Ranked Probability Score (CRPS). The 
<code>"rpc-based"</code> method adjusts the forecast variance ensuring that the 
ratio of predictable components (RPC) is equal to one, as in Eade et al. 
(2014). It is equivalent to function <code>Calibration</code> but for objects 
of class <code>s2dv_cube</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_Calibration(
  exp,
  obs,
  exp_cor = NULL,
  cal.method = "mse_min",
  eval.method = "leave-one-out",
  multi.model = FALSE,
  na.fill = TRUE,
  na.rm = TRUE,
  apply_to = NULL,
  alpha = NULL,
  memb_dim = "member",
  sdate_dim = "sdate",
  dat_dim = NULL,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_Calibration_+3A_exp">exp</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by <code>CST_Start</code> 
function with at least 'sdate' and 'member' dimensions, containing the  
seasonal hindcast experiment data in the element named <code>data</code>. The 
hindcast is used to calibrate the forecast in case the forecast is provided; 
if not, the same hindcast will be calibrated instead.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_obs">obs</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by <code>CST_Start</code> 
function with at least 'sdate' dimension, containing the observed data in 
the element named <code>$data</code>.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_exp_cor">exp_cor</code></td>
<td>
<p>An optional object of class <code>s2dv_cube</code> as returned by 
<code>CST_Start</code> function with at least 'sdate' and 'member' dimensions, 
containing the seasonal forecast experiment data in the element named 
<code>data</code>. If the forecast is provided, it will be calibrated using the 
hindcast and observations; if not, the hindcast will be calibrated instead. 
If there is only one corrected dataset, it should not have dataset dimension. 
If there is a corresponding corrected dataset for each 'exp' forecast, the 
dataset dimension must have the same length as in 'exp'. The default value 
is NULL.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_cal.method">cal.method</code></td>
<td>
<p>A character string indicating the calibration method used, 
can be either <code>bias</code>, <code>evmos</code>, <code>mse_min</code>, <code>crps_min</code> or 
<code>rpc-based</code>. Default value is <code>mse_min</code>.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_eval.method">eval.method</code></td>
<td>
<p>A character string indicating the sampling method used, it 
can be either <code>in-sample</code> or <code>leave-one-out</code>. Default value is the 
<code>leave-one-out</code> cross validation. In case the forecast is provided, any 
chosen eval.method is over-ruled and a third option is used.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_multi.model">multi.model</code></td>
<td>
<p>A boolean that is used only for the <code>mse_min</code> 
method. If multi-model ensembles or ensembles of different sizes are used, 
it must be set to <code>TRUE</code>. By default it is <code>FALSE</code>. Differences 
between the two approaches are generally small but may become large when 
using small ensemble sizes. Using multi.model when the calibration method is 
<code>bias</code>, <code>evmos</code> or <code>crps_min</code> will not affect the result.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_na.fill">na.fill</code></td>
<td>
<p>A boolean that indicates what happens in case calibration is 
not possible or will yield unreliable results. This happens when three or 
less forecasts-observation pairs are available to perform the training phase 
of the calibration. By default <code>na.fill</code> is set to true such that NA 
values will be returned. If <code>na.fill</code> is set to false, the uncorrected 
data will be returned.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_na.rm">na.rm</code></td>
<td>
<p>A boolean that indicates whether to remove the NA values or not. 
The default value is <code>TRUE</code>. See Details section for further 
information about its use and compatibility with <code>na.fill</code>.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_apply_to">apply_to</code></td>
<td>
<p>A character string that indicates whether to apply the 
calibration to all the forecast (<code>"all"</code>) or only to those where the 
correlation between the ensemble mean and the observations is statistically
significant (<code>"sign"</code>). Only useful if <code>cal.method == "rpc-based"</code>.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_alpha">alpha</code></td>
<td>
<p>A numeric value indicating the significance level for the 
correlation test. Only useful if <code>cal.method == "rpc-based" &amp; apply_to 
== "sign"</code>.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the member dimension.
By default, it is set to 'member'.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. By default, it is set to 'sdate'.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_dat_dim">dat_dim</code></td>
<td>
<p>A character string indicating the name of dataset dimension. 
The length of this dimension can be different between 'exp' and 'obs'. 
The default value is NULL.</p>
</td></tr>
<tr><td><code id="CST_Calibration_+3A_ncores">ncores</code></td>
<td>
<p>An integer that indicates the number of cores for parallel 
computations using multiApply function. The default value is one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Both the <code>na.fill</code> and <code>na.rm</code> parameters can be used to 
indicate how the function has to handle the NA values. The <code>na.fill</code> 
parameter checks whether there are more than three forecast-observations pairs 
to perform the computation. In case there are three or less pairs, the 
computation is not carried out, and the value returned by the function depends 
on the value of this parameter (either NA if <code>na.fill == TRUE</code> or the 
uncorrected value if <code>na.fill == TRUE</code>). On the other hand, <code>na.rm</code> 
is used to indicate the function whether to remove the missing values during 
the computation of the parameters needed to perform the calibration.
</p>


<h3>Value</h3>

<p>An object of class <code>s2dv_cube</code> containing the calibrated 
forecasts in the element <code>data</code> with the dimensions nexp, nobs and same 
dimensions as in the 'exp' object. nexp is the number of experiment 
(i.e., 'dat_dim' in exp), and nobs is the number of observation (i.e., 
'dat_dim' in obs). If dat_dim is NULL, nexp and nobs are omitted. If 'exp_cor' 
is provided the returned array will be with the same dimensions as 'exp_cor'.
</p>


<h3>Author(s)</h3>

<p>Verónica Torralba, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>
</p>
<p>Bert Van Schaeybroeck, <a href="mailto:bertvs@meteo.be">bertvs@meteo.be</a>
</p>


<h3>References</h3>

<p>Doblas-Reyes F.J, Hagedorn R, Palmer T.N. The rationale behind the 
success of multi-model ensembles in seasonal forecasting-II calibration and 
combination. Tellus A. 2005;57:234-252. doi: <a href="https://doi.org/10.1111/j.1600-0870.2005.00104.x">10.1111/j.1600-0870.2005.00104.x</a>
</p>
<p>Eade, R., Smith, D., Scaife, A., Wallace, E., Dunstone, N., 
Hermanson, L., &amp; Robinson, N. (2014). Do seasonal-to-decadal climate 
predictions underestimate the predictability of the read world? Geophysical 
Research Letters, 41(15), 5620-5628. doi: <a href="https://doi.org/10.1002/2014GL061146">10.1002/2014GL061146</a>
</p>
<p>Van Schaeybroeck, B., &amp; Vannitsem, S. (2011). Post-processing 
through linear regression. Nonlinear Processes in Geophysics, 18(2), 
147. doi: <a href="https://doi.org/10.5194/npg-18-147-2011">10.5194/npg-18-147-2011</a>
</p>
<p>Van Schaeybroeck, B., &amp; Vannitsem, S. (2015). Ensemble 
post-processing using member-by-member approaches: theoretical aspects. 
Quarterly Journal of the Royal Meteorological Society, 141(688), 807-818.  
doi: <a href="https://doi.org/10.1002/qj.2397">10.1002/qj.2397</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CST_Start">CST_Start</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1:
mod1 &lt;- 1 : (1 * 3 * 4 * 5 * 6 * 7)
dim(mod1) &lt;- c(dataset = 1, member = 3, sdate = 4, ftime = 5, lat = 6, lon = 7)
obs1 &lt;- 1 : (1 * 1 * 4 * 5 * 6 * 7)
dim(obs1) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 5, lat = 6, lon = 7)
lon &lt;- seq(0, 30, 5)
lat &lt;- seq(0, 25, 5)
coords &lt;- list(lat = lat, lon = lon)
exp &lt;- list(data = mod1, coords = coords)
obs &lt;- list(data = obs1, coords = coords)
attr(exp, 'class') &lt;- 's2dv_cube'
attr(obs, 'class') &lt;- 's2dv_cube'
a &lt;- CST_Calibration(exp = exp, obs = obs, cal.method = "mse_min", eval.method = "in-sample")

# Example 2:
mod1 &lt;- 1 : (1 * 3 * 4 * 5 * 6 * 7)
mod2 &lt;- 1 : (1 * 3 * 1 * 5 * 6 * 7)
dim(mod1) &lt;- c(dataset = 1, member = 3, sdate = 4, ftime = 5, lat = 6, lon = 7)
dim(mod2) &lt;- c(dataset = 1, member = 3, sdate = 1, ftime = 5, lat = 6, lon = 7)
obs1 &lt;- 1 : (1 * 1 * 4 * 5 * 6 * 7)
dim(obs1) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 5, lat = 6, lon = 7)
lon &lt;- seq(0, 30, 5)
lat &lt;- seq(0, 25, 5)
coords &lt;- list(lat = lat, lon = lon)
exp &lt;- list(data = mod1, coords = coords)
obs &lt;- list(data = obs1, coords = coords)
exp_cor &lt;- list(data = mod2, lat = lat, lon = lon)
attr(exp, 'class') &lt;- 's2dv_cube'
attr(obs, 'class') &lt;- 's2dv_cube'
attr(exp_cor, 'class') &lt;- 's2dv_cube'
a &lt;- CST_Calibration(exp = exp, obs = obs, exp_cor = exp_cor, cal.method = "evmos")

</code></pre>

<hr>
<h2 id='CST_CategoricalEnsCombination'>Make categorical forecast based on a multi-model forecast with potential for 
calibrate</h2><span id='topic+CST_CategoricalEnsCombination'></span>

<h3>Description</h3>

<p>This function converts a multi-model ensemble forecast into a 
categorical forecast by giving the probability for each category. Different 
methods are available to combine the different ensemble forecasting models 
into probabilistic categorical forecasts. 
</p>
<p>Motivation: Beyond the short range, the unpredictable component of weather 
predictions becomes substantial due to the chaotic nature of the earth system. 
Therefore, predictions can mostly be skillful when used in a probabilistic 
sense. In practice this is done using ensemble forecasts. It is then common to
convert the ensemble forecasts to occurence probabilities for different 
categories. These categories typically are taken as terciles from 
climatolgical distributions. For instance for temperature, there is a cold, 
normal and warm class. Commonly multiple ensemble forecasting systems are 
available but some models may be more competitive than others for the 
variable, region and user need under consideration. Therefore, when 
calculating the category probabilities, the ensemble members of the different 
forecasting system may be differently weighted. Such weighting is typically 
done by comparison of the ensemble forecasts with observations. 
</p>
<p>Description of the tool: The tool considers all forecasts (all members from 
all forecasting systems) and converts them into occurrence probabilities of 
different categories. The amount of categories can be changed and are taken as 
the climatological quantiles (e.g. terciles), extracted from the observational 
data. The methods that are available to combine the ensemble forecasting 
models into probabilistic categorical forecasts are: 1) ensemble pooling where 
all ensemble members of all ensemble systems are weighted equally, 
2) model combination where each model system is weighted equally, and,
3) model weighting. 
The model weighting method is described in Rajagopalan et al. (2002),
Robertson et al. 2004 and Van Schaeybroeck and Vannitsem (2019). More 
specifically, this method uses different weights for the occurence probability 
predicted by the available models and by a climatological model and optimizes 
the weights by minimizing the ignorance score. Finally, the function can also 
be used to categorize the observations in the categorical quantiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_CategoricalEnsCombination(
  exp,
  obs,
  cat.method = "pool",
  eval.method = "leave-one-out",
  amt.cat = 3,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_CategoricalEnsCombination_+3A_exp">exp</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by <code>CST_Load</code> 
function, containing the seasonal forecast experiment data in the element 
named <code>$data</code>. The amount of forecasting models is equal to the size of 
the <code>dataset</code> dimension of the data array. The amount of members per 
model may be different. The  size of the <code>member</code> dimension of the data
array is equal to the maximum of the ensemble members among the models. 
Models with smaller ensemble sizes have residual indices of <code>member</code> 
dimension in the data array filled with NA values.</p>
</td></tr>
<tr><td><code id="CST_CategoricalEnsCombination_+3A_obs">obs</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by <code>CST_Load</code> 
function, containing the observed data in the element named <code>$data</code>.</p>
</td></tr>
<tr><td><code id="CST_CategoricalEnsCombination_+3A_cat.method">cat.method</code></td>
<td>
<p>Method used to produce the categorical forecast, can be 
either <code>pool</code>, <code>comb</code>, <code>mmw</code> or <code>obs</code>. The method pool 
assumes equal weight for all ensemble members while the method comb assumes 
equal weight for each model. The weighting method is descirbed in 
Rajagopalan et al. (2002), Robertson et al. (2004) and Van Schaeybroeck and 
Vannitsem (2019). Finally, the <code>obs</code> method classifies the observations 
into the different categories and therefore contains only 0 and 1 values.</p>
</td></tr>
<tr><td><code id="CST_CategoricalEnsCombination_+3A_eval.method">eval.method</code></td>
<td>
<p>Is the sampling method used, can be either 
<code>"in-sample"</code> or <code>"leave-one-out"</code>. Default value is the 
<code>"leave-one-out"</code> cross validation.</p>
</td></tr>
<tr><td><code id="CST_CategoricalEnsCombination_+3A_amt.cat">amt.cat</code></td>
<td>
<p>Is the amount of categories. Equally-sized quantiles will be 
calculated based on the amount of categories.</p>
</td></tr>
<tr><td><code id="CST_CategoricalEnsCombination_+3A_...">...</code></td>
<td>
<p>other parameters to be passed on to the calibration procedure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>s2dv_cube</code> containing the categorical 
forecasts in the element called <code>$data</code>. The first two dimensions of the 
returned object are named dataset and member and are both of size one. An 
additional dimension named category is introduced and is of size amt.cat.
</p>


<h3>Author(s)</h3>

<p>Bert Van Schaeybroeck, <a href="mailto:bertvs@meteo.be">bertvs@meteo.be</a>
</p>


<h3>References</h3>

<p>Rajagopalan, B., Lall, U., &amp; Zebiak, S. E. (2002). Categorical 
climate forecasts through regularization and optimal combination of multiple 
GCM ensembles. Monthly Weather Review, 130(7), 1792-1811.
</p>
<p>Robertson, A. W., Lall, U., Zebiak, S. E., &amp; Goddard, L. (2004). 
Improved combination of multiple atmospheric GCM ensembles for seasonal 
prediction. Monthly Weather Review, 132(12), 2732-2744.
</p>
<p>Van Schaeybroeck, B., &amp; Vannitsem, S. (2019). Postprocessing of 
Long-Range Forecasts. In Statistical Postprocessing of Ensemble Forecasts (pp. 267-290).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mod1 &lt;- 1 : (2 * 2* 4 * 5 * 2 * 2)
dim(mod1) &lt;- c(dataset = 2, member = 2, sdate = 4, ftime = 5, lat = 2, lon = 2)
mod1[2, 1, , , , ] &lt;- NA
datasets &lt;- c("MF", "UKMO")
obs1 &lt;- 1 : (1 * 1 * 4 * 5 * 2 * 2)
dim(obs1) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 5, lat = 2, lon = 2)
lon &lt;- seq(0, 30, 5)
lat &lt;- seq(0, 25, 5)
coords &lt;- list(lat = lat, lon = lon)
attrs &lt;- list(Datasets = datasets)
exp &lt;- list(data = mod1, coords = coords, attrs = attrs)
obs &lt;- list(data = obs1, coords = coords)
attr(exp, 'class') &lt;- 's2dv_cube'
attr(obs, 'class') &lt;- 's2dv_cube'
a &lt;- CST_CategoricalEnsCombination(exp = exp, obs = obs, amt.cat = 3, 
                                  cat.method = "mmw") 
</code></pre>

<hr>
<h2 id='CST_ChangeDimNames'>Change the name of one or more dimensions for an object of class s2dv_cube</h2><span id='topic+CST_ChangeDimNames'></span>

<h3>Description</h3>

<p>Change the names of the dimensions specified in 'original_names' to the names
in 'new_names'. The coordinate names and the dimensions of any attributes
are also modified accordingly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_ChangeDimNames(data, original_names, new_names)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_ChangeDimNames_+3A_data">data</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> whose dimension names
should be changed.</p>
</td></tr>
<tr><td><code id="CST_ChangeDimNames_+3A_original_names">original_names</code></td>
<td>
<p>A single character string or a vector indicating the 
dimensions to be renamed.</p>
</td></tr>
<tr><td><code id="CST_ChangeDimNames_+3A_new_names">new_names</code></td>
<td>
<p>A single character string or a vector indicating the new
dimension names, in the same order as the dimensions in 'original_names'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>s2dv_cube</code> with similar data, coordinates and 
attributes as the <code>data</code> input, but with modified dimension names.
</p>


<h3>Author(s)</h3>

<p>Agudetse Roures Victoria, <a href="mailto:victoria.agudetse@bsc.es">victoria.agudetse@bsc.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with sample data:
# Check original dimensions and coordinates
lonlat_temp$exp$dims
names(lonlat_temp$exp$coords)
dim(lonlat_temp$exp$attrs$Dates)
# Change 'dataset' to 'dat' and 'ftime' to 'time'
exp &lt;- CST_ChangeDimNames(lonlat_temp$exp,
                         original_names = c("dataset", "ftime"),
                         new_names = c("dat", "time"))
# Check new dimensions and coordinates
exp$dims
names(exp$coords)
dim(exp$attrs$Dates)

</code></pre>

<hr>
<h2 id='CST_DynBiasCorrection'>Performing a Bias Correction conditioned by the dynamical
properties of the data.</h2><span id='topic+CST_DynBiasCorrection'></span>

<h3>Description</h3>

<p>This function perform a bias correction conditioned by the 
dynamical properties of the dataset. This function internally uses the functions 
'Predictability' to divide in terciles the two dynamical proxies 
computed with 'CST_ProxiesAttractor'. A bias correction
between the model and the observations is performed using the division into
terciles of the local dimension 'dim' and inverse of the persistence 'theta'.
For instance, model values with lower 'dim' will be corrected with observed 
values with lower 'dim', and the same for theta. The function gives two options
of bias correction: one for 'dim' and/or one for 'theta'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_DynBiasCorrection(
  exp,
  obs,
  method = "QUANT",
  wetday = FALSE,
  proxy = "dim",
  quanti,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_DynBiasCorrection_+3A_exp">exp</code></td>
<td>
<p>An s2v_cube object with the experiment data.</p>
</td></tr>
<tr><td><code id="CST_DynBiasCorrection_+3A_obs">obs</code></td>
<td>
<p>An s2dv_cube object with the reference data.</p>
</td></tr>
<tr><td><code id="CST_DynBiasCorrection_+3A_method">method</code></td>
<td>
<p>A character string indicating the method to apply bias 
correction among these ones: &quot;PTF&quot;,&quot;RQUANT&quot;,&quot;QUANT&quot;,&quot;SSPLIN&quot;.</p>
</td></tr>
<tr><td><code id="CST_DynBiasCorrection_+3A_wetday">wetday</code></td>
<td>
<p>Logical indicating whether to perform wet day correction 
or not OR a numeric threshold below which all values are set to zero (by 
default is set to 'FALSE').</p>
</td></tr>
<tr><td><code id="CST_DynBiasCorrection_+3A_proxy">proxy</code></td>
<td>
<p>A character string indicating the proxy for local dimension
'dim' or inverse of persistence 'theta' to apply the dynamical 
conditioned bias correction method.</p>
</td></tr>
<tr><td><code id="CST_DynBiasCorrection_+3A_quanti">quanti</code></td>
<td>
<p>A number lower than 1 indicating the quantile to perform 
the computation of local dimension and theta.</p>
</td></tr>
<tr><td><code id="CST_DynBiasCorrection_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to use in parallel computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dynbias An s2dvcube object with a bias correction performed 
conditioned by local dimension 'dim' or inverse of persistence 'theta'.
</p>


<h3>Author(s)</h3>

<p>Carmen Alvarez-Castro, <a href="mailto:carmen.alvarez-castro@cmcc.it">carmen.alvarez-castro@cmcc.it</a>
</p>
<p>Maria M. Chaves-Montero, <a href="mailto:mdm.chaves-montero@cmcc.it">mdm.chaves-montero@cmcc.it</a>
</p>
<p>Veronica Torralba, <a href="mailto:veronica.torralba@cmcc.it">veronica.torralba@cmcc.it</a>
</p>
<p>Davide Faranda, <a href="mailto:davide.faranda@lsce.ipsl.fr">davide.faranda@lsce.ipsl.fr</a>
</p>


<h3>References</h3>

<p>Faranda, D., Alvarez-Castro, M.C., Messori, G., Rodriguez, D., 
and Yiou, P. (2019). The hammam effect or how a warm ocean enhances large 
scale atmospheric predictability.Nature Communications, 10(1), 1316. 
doi: <a href="https://doi.org/10.1038/s41467-019-09305-8">10.1038/s41467-019-09305-8</a>&quot;
</p>
<p>Faranda, D., Gabriele Messori and Pascal Yiou. (2017).
Dynamical proxies of North Atlantic predictability and extremes. 
Scientific Reports, 7-41278, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>expL &lt;- rnorm(1:2000)
dim(expL) &lt;- c(time = 100, lat = 4, lon = 5)
obsL &lt;- c(rnorm(1:1980), expL[1, , ] * 1.2)
dim(obsL) &lt;- c(time = 100, lat = 4, lon = 5)
time_obsL &lt;- as.POSIXct(paste(rep("01", 100), rep("01", 100), 1920:2019, sep = "-"), 
                       format = "%d-%m-%y")
time_expL &lt;- as.POSIXct(paste(rep("01", 100), rep("01", 100), 1929:2019, sep = "-"), 
                       format = "%d-%m-%y")
lon &lt;- seq(-1, 5, 1.5)
lat &lt;- seq(30, 35, 1.5)
# qm = 0.98 #'too high for this short dataset, it is possible that doesn't
# get the requirement, in that case it would be necessary select a lower qm
# for instance qm = 0.60
expL &lt;- s2dv_cube(data = expL, coords = list(lon = lon, lat = lat),
                 Dates = time_expL)
obsL &lt;- s2dv_cube(data = obsL, coords = list(lon = lon, lat = lat),
                 Dates = time_obsL)
# to use DynBiasCorrection
dynbias1 &lt;- DynBiasCorrection(exp = expL$data, obs = obsL$data, proxy= "dim",
                             quanti = 0.6)
# to use CST_DynBiasCorrection
dynbias2 &lt;- CST_DynBiasCorrection(exp = expL, obs = obsL, proxy= "dim",
                                 quanti = 0.6)

</code></pre>

<hr>
<h2 id='CST_EnsClustering'>Ensemble clustering</h2><span id='topic+CST_EnsClustering'></span>

<h3>Description</h3>

<p>This function performs a clustering on members/starting dates
and returns a number of scenarios, with representative members for each of 
them. The clustering is performed in a reduced EOF space.
</p>
<p>Motivation:
Ensemble forecasts give a probabilistic insight of average weather conditions
on extended timescales, i.e. from sub-seasonal to seasonal and beyond.
With large ensembles, it is often an advantage to be able to group members
according to similar characteristics and to select the most representative 
member for each cluster. This can be useful to characterize the most probable 
forecast scenarios in a multi-model (or single model) ensemble prediction.  
This approach, applied at a regional level, can also be used to identify the 
subset of ensemble members that best represent the full range of possible 
solutions for downscaling applications. The choice of the ensemble members is 
made flexible in order to meet the requirements of specific (regional) climate 
information products, to be tailored for different regions and user needs. 
</p>
<p>Description of the tool:
EnsClustering is a cluster analysis tool, based on the k-means algorithm, for 
ensemble predictions. The aim is to group ensemble members according to 
similar characteristics and to select the most representative member for each 
cluster. The user chooses which feature of the data is used to group the 
ensemble members by clustering: time mean, maximum, a certain percentile 
(e.g., 75
time period. For each ensemble member this value is computed at each grid 
point, obtaining N lat-lon maps, where N is the number of ensemble members.
The anomaly is computed subtracting the ensemble mean of these maps to each of 
the single maps. The anomaly is therefore computed with respect to the 
ensemble members (and not with respect to the time) and the Empirical 
Orthogonal Function (EOF) analysis is applied to these anomaly maps. Regarding 
the EOF analysis, the user can choose either how many Principal Components 
(PCs) to retain or the percentage of explained variance to keep. After 
reducing dimensionality via EOF analysis, k-means analysis is applied using 
the desired subset of PCs. 
</p>
<p>The major final outputs are the classification in clusters, i.e. which member 
belongs to which cluster (in k-means analysis the number k of clusters needs 
to be defined prior to the analysis) and the most representative member for 
each cluster, which is the closest member to the cluster centroid. Other 
outputs refer to the statistics of clustering: in the PC space, the minimum 
and the maximum distance between a member in a cluster and the cluster 
centroid (i.e. the closest and the furthest member), the intra-cluster 
standard deviation for each cluster (i.e. how much the cluster is compact).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_EnsClustering(
  exp,
  time_moment = "mean",
  numclus = NULL,
  lon_lim = NULL,
  lat_lim = NULL,
  variance_explained = 80,
  numpcs = NULL,
  time_dim = NULL,
  time_percentile = 90,
  cluster_dim = "member",
  verbose = F
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_EnsClustering_+3A_exp">exp</code></td>
<td>
<p>An object of the class 's2dv_cube', containing the variables to be 
analysed. The element 'data' in the 's2dv_cube' object must have, at
least, spatial and temporal dimensions. Latitudinal dimension accepted 
names: 'lat', 'lats', 'latitude', 'y', 'j', 'nav_lat'. Longitudinal 
dimension accepted names: 'lon', 'lons','longitude', 'x', 'i', 'nav_lon'.</p>
</td></tr>
<tr><td><code id="CST_EnsClustering_+3A_time_moment">time_moment</code></td>
<td>
<p>Decides the moment to be applied to the time dimension. Can 
be either 'mean' (time mean), 'sd' (standard deviation along time) or 'perc' 
(a selected percentile on time). If 'perc' the keyword 'time_percentile' is 
also used.</p>
</td></tr>
<tr><td><code id="CST_EnsClustering_+3A_numclus">numclus</code></td>
<td>
<p>Number of clusters (scenarios) to be calculated. If set to NULL 
the number of ensemble members divided by 10 is used, with a minimum of 2 
and a maximum of 8.</p>
</td></tr>
<tr><td><code id="CST_EnsClustering_+3A_lon_lim">lon_lim</code></td>
<td>
<p>List with the two longitude margins in 'c(-180,180)' format.</p>
</td></tr>
<tr><td><code id="CST_EnsClustering_+3A_lat_lim">lat_lim</code></td>
<td>
<p>List with the two latitude margins.</p>
</td></tr>
<tr><td><code id="CST_EnsClustering_+3A_variance_explained">variance_explained</code></td>
<td>
<p>variance (percentage) to be explained by the set of 
EOFs. Defaults to 80. Not used if numpcs is specified.</p>
</td></tr>
<tr><td><code id="CST_EnsClustering_+3A_numpcs">numpcs</code></td>
<td>
<p>Number of EOFs retained in the analysis (optional).</p>
</td></tr>
<tr><td><code id="CST_EnsClustering_+3A_time_dim">time_dim</code></td>
<td>
<p>String or character array with name(s) of dimension(s) over 
which to compute statistics. If omitted c(&quot;ftime&quot;, &quot;sdate&quot;, &quot;time&quot;) are 
searched in this order.</p>
</td></tr>
<tr><td><code id="CST_EnsClustering_+3A_time_percentile">time_percentile</code></td>
<td>
<p>Set the percentile in time you want to analyse (used 
for 'time_moment = &quot;perc&quot;).</p>
</td></tr>
<tr><td><code id="CST_EnsClustering_+3A_cluster_dim">cluster_dim</code></td>
<td>
<p>Dimension along which to cluster. Typically &quot;member&quot; or 
&quot;sdate&quot;. This can also be a list like c(&quot;member&quot;, &quot;sdate&quot;).</p>
</td></tr>
<tr><td><code id="CST_EnsClustering_+3A_verbose">verbose</code></td>
<td>
<p>Logical for verbose output</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements <code>$cluster</code> (cluster assigned for each 
member), <code>$freq</code> (relative frequency of each cluster), 
<code>$closest_member</code> (representative member for each cluster), 
<code>$repr_field</code> (list of fields for each representative member), 
<code>composites</code> (list of mean fields for each cluster), <code>$lon</code> 
(selected longitudes of output fields), <code>$lat</code> (selected longitudes of 
output fields).
</p>


<h3>Author(s)</h3>

<p>Federico Fabiano - ISAC-CNR, <a href="mailto:f.fabiano@isac.cnr.it">f.fabiano@isac.cnr.it</a>
</p>
<p>Ignazio Giuntoli - ISAC-CNR, <a href="mailto:i.giuntoli@isac.cnr.it">i.giuntoli@isac.cnr.it</a>
</p>
<p>Danila Volpi - ISAC-CNR, <a href="mailto:d.volpi@isac.cnr.it">d.volpi@isac.cnr.it</a>
</p>
<p>Paolo Davini - ISAC-CNR, <a href="mailto:p.davini@isac.cnr.it">p.davini@isac.cnr.it</a>
</p>
<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat_exp &lt;- array(abs(rnorm(1152))*275, dim = c(dataset = 1, member = 4, 
                                              sdate = 6, ftime = 3, 
                                              lat = 4, lon = 4))
lon &lt;- seq(0, 3)
lat &lt;- seq(48, 45)
coords &lt;- list(lon = lon, lat = lat)
exp &lt;- list(data = dat_exp, coords = coords)
attr(exp, 'class') &lt;- 's2dv_cube'
res &lt;- CST_EnsClustering(exp = exp, numclus = 3,
                        cluster_dim = c("sdate"))

</code></pre>

<hr>
<h2 id='CST_InsertDim'>Add a named dimension to an object of class s2dv_cube</h2><span id='topic+CST_InsertDim'></span>

<h3>Description</h3>

<p>Insert an extra dimension into an array at position 'posdim' with length 
'lendim'. The array in <code>data</code> repeats along the new dimension.
The dimensions, coordinates and attributes are modified accordingly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_InsertDim(data, posdim, lendim, name, values = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_InsertDim_+3A_data">data</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> to which the additional
dimension should be added.</p>
</td></tr>
<tr><td><code id="CST_InsertDim_+3A_posdim">posdim</code></td>
<td>
<p>An integer indicating the position of the new dimension.</p>
</td></tr>
<tr><td><code id="CST_InsertDim_+3A_lendim">lendim</code></td>
<td>
<p>An integer indicating the length of the new dimension.</p>
</td></tr>
<tr><td><code id="CST_InsertDim_+3A_name">name</code></td>
<td>
<p>A character string indicating the name for the new dimension.</p>
</td></tr>
<tr><td><code id="CST_InsertDim_+3A_values">values</code></td>
<td>
<p>A vector containing the values of the new dimension and any
relevant attributes. If NULL, a sequence of integers from 1 to lendim will
be added.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>s2dv_cube</code> with similar data, coordinates and 
attributes as the <code>data</code> input, but with an additional dimension.
</p>


<h3>Author(s)</h3>

<p>Agudetse Roures Victoria, <a href="mailto:victoria.agudetse@bsc.es">victoria.agudetse@bsc.es</a>
</p>


<h3>See Also</h3>

<p><a href="s2dv.html#topic+InsertDim">InsertDim</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example with sample data:
# Check original dimensions and coordinates
lonlat_temp$exp$dims
names(lonlat_temp$exp$coords)
# Add 'variable' dimension
exp &lt;- CST_InsertDim(lonlat_temp$exp,
                    posdim = 2,
                    lendim = 1,
                    name = "variable",
                    values = c("tas"))
# Check new dimensions and coordinates
exp$dims
exp$coords$variable

</code></pre>

<hr>
<h2 id='CST_Load'>CSTools Data Retreival Function</h2><span id='topic+CST_Load'></span>

<h3>Description</h3>

<p>This function aggregates, subsets and retrieves sub-seasonal, seasonal, decadal or climate projection data from NetCDF files in a local file system or on remote OPeNDAP servers, and arranges it for easy application of the CSTools functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_Load(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_Load_+3A_...">...</code></td>
<td>
<p>Parameters that are automatically forwarded to the 's2dv::Load' function. See details in '?s2dv::Load'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It receives any number of parameters ('...') that are automatically forwarded to the 's2dv::Load' function. See details in '?s2dv::Load'.
</p>
<p>It is recommended to use this function in combination with the 'zeallot::&quot;
</p>


<h3>Value</h3>

<p>A list with one or two S3 objects, named 'exp' and 'obs', of the class 's2dv_cube', containing experimental and date-corresponding observational data, respectively. These 's2dv_cube's can be ingested by other functions in CSTools. If the parameter &lsquo;exp' in the call to 'CST_Load' is set to 'NULL', then only the &rsquo;obs' component is returned, and viceversa.
</p>


<h3>Author(s)</h3>

<p>Nicolau Manubens, <a href="mailto:nicolau.manubens@bsc.es">nicolau.manubens@bsc.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(zeallot)
startDates &lt;- c('20001101', '20011101', '20021101', 
                '20031101', '20041101', '20051101')
c(exp, obs) %&lt;-% 
  CST_Load(
    var = 'tas', 
    exp = 'system5c3s', 
    obs = 'era5', 
    nmember = 15,
    sdates = startDates,
    leadtimemax = 3,
    latmin = 27, latmax = 48,
    lonmin = -12, lonmax = 40,
    output = 'lonlat',
    nprocs = 1
  )

## End(Not run)

</code></pre>

<hr>
<h2 id='CST_MergeDims'>Function to  Merge Dimensions</h2><span id='topic+CST_MergeDims'></span>

<h3>Description</h3>

<p>This function merges two dimensions of the array <code>data</code> in a 
's2dv_cube' object into one. The user can select the dimensions to merge and 
provide the final name of the dimension. The user can select to remove NA 
values or keep them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_MergeDims(
  data,
  merge_dims = c("ftime", "monthly"),
  rename_dim = NULL,
  na.rm = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_MergeDims_+3A_data">data</code></td>
<td>
<p>An 's2dv_cube' object</p>
</td></tr>
<tr><td><code id="CST_MergeDims_+3A_merge_dims">merge_dims</code></td>
<td>
<p>A character vector indicating the names of the dimensions to 
merge.</p>
</td></tr>
<tr><td><code id="CST_MergeDims_+3A_rename_dim">rename_dim</code></td>
<td>
<p>a character string indicating the name of the output 
dimension. If left at NULL, the first dimension name provided in parameter 
<code>merge_dims</code> will be used.</p>
</td></tr>
<tr><td><code id="CST_MergeDims_+3A_na.rm">na.rm</code></td>
<td>
<p>A logical indicating if the NA values should be removed or not.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nuria Perez-Zanon, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- 1 : c(2 * 3 * 4 * 5 * 6 * 7)
dim(data) &lt;- c(time = 7, lat = 2, lon = 3, monthly = 4, member = 6,
              dataset = 5, var = 1)
data[2,,,,,,] &lt;- NA
data[c(3,27)] &lt;- NA
data &lt;- list(data = data)
class(data) &lt;- 's2dv_cube'
new_data &lt;- CST_MergeDims(data, merge_dims = c('time', 'monthly'))
new_data &lt;- CST_MergeDims(data, merge_dims = c('lon', 'lat'), rename_dim = 'grid')
new_data &lt;- CST_MergeDims(data, merge_dims = c('time', 'monthly'), na.rm = TRUE)
</code></pre>

<hr>
<h2 id='CST_MultiEOF'>EOF analysis of multiple variables</h2><span id='topic+CST_MultiEOF'></span>

<h3>Description</h3>

<p>This function performs EOF analysis over multiple variables,
accepting in input a list of CSTools objects. Based on Singular Value 
Decomposition. For each field the EOFs are computed and the corresponding PCs 
are standardized (unit variance, zero mean); the minimum number of principal 
components needed to reach the user-defined variance is retained. The function 
weights the input data for the latitude cosine square root.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_MultiEOF(
  datalist,
  lon_dim = "lon",
  lat_dim = "lat",
  time_dim = "ftime",
  sdate_dim = "sdate",
  var_dim = "var",
  neof_max = 40,
  neof_composed = 5,
  minvar = 0.6,
  lon_lim = NULL,
  lat_lim = NULL,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_MultiEOF_+3A_datalist">datalist</code></td>
<td>
<p>A list of objects of the class 's2dv_cube', containing the 
variables to be analysed. Each data object in the list is expected to have 
an element named <code>$data</code> with at least two spatial dimensions named 
&quot;lon&quot; and &quot;lat&quot;, a dimension &quot;ftime&quot; and a dimension &quot;sdate&quot;. Latitudinal 
dimension accepted names: 'lat', 'lats', 'latitude', 'y', 'j', 'nav_lat'. 
Longitudinal dimension accepted names: 'lon', 'lons','longitude', 'x', 'i', 
'nav_lon'. NAs can exist but it should be consistent along 'time_dim'. That 
is, if one grid point has NAs for each variable, all the time steps at this 
point should be NAs.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_lon_dim">lon_dim</code></td>
<td>
<p>A character string indicating the name of the longitudinal 
dimension. By default, it is set to 'lon'.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_lat_dim">lat_dim</code></td>
<td>
<p>A character string indicating the name of the latitudinal 
dimension. By default, it is set to 'lat'.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_time_dim">time_dim</code></td>
<td>
<p>A character string indicating the name of the temporal 
dimension. By default, it is set to 'time'.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. By default, it is set to 'sdate'.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_var_dim">var_dim</code></td>
<td>
<p>A character string indicating the name of the variable 
dimension. By default, it is set to 'var'.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_neof_max">neof_max</code></td>
<td>
<p>Maximum number of single eofs considered in the first 
decomposition.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_neof_composed">neof_composed</code></td>
<td>
<p>Number of composed eofs to return in output.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_minvar">minvar</code></td>
<td>
<p>Minimum variance fraction to be explained in first decomposition.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_lon_lim">lon_lim</code></td>
<td>
<p>Vector with longitudinal range limits for the EOF calculation 
for all input variables.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_lat_lim">lat_lim</code></td>
<td>
<p>Vector with latitudinal range limits for the EOF calculation 
for all input variables.</p>
</td></tr>
<tr><td><code id="CST_MultiEOF_+3A_ncores">ncores</code></td>
<td>
<p>An integer indicating the number of cores to use for parallel 
computation. The default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>coeff</code></td>
<td>

<p>An 's2dv_cube' with the data element being an array of principal components 
with dimensions 'time_dim', 'sdate_dim', number of eof, rest of the 
dimensions of 'data' except 'lon_dim' and 'lat_dim'. 
</p>
</td></tr>
<tr><td><code>variance</code></td>
<td>

<p>An 's2dv_cube' with the data element being an array of explained variances 
with dimensions 'eof' and the rest of the dimensions of 'data' except 
'time_dim', 'sdate_dim', 'lon_dim' and 'lat_dim'.
</p>
</td></tr>
<tr><td><code>eof_pattern</code></td>
<td>

<p>An 's2dv_cube' with the data element being an array of EOF patterns obtained 
by regression with dimensions: 'eof' and the rest of the dimensions of 
'data' except 'time_dim' and 'sdate_dim'.
</p>
</td></tr>
<tr><td><code>mask</code></td>
<td>

<p>An 's2dv_cube' with the data element being an array of the mask with 
dimensions ('lon_dim', 'lat_dim', rest of the dimensions of 'data' except 
'time_dim'). It is made from 'data', 1 for the positions that 'data' has 
value and NA for the positions that 'data' has NA. It is used to replace NAs 
with 0s for EOF calculation and mask the result with NAs again after the 
calculation.
</p>
</td></tr>
<tr><td><code>coordinates</code></td>
<td>

<p>Longitudinal and latitudinal coordinates vectors.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>
<p>Paolo Davini - ISAC-CNR, <a href="mailto:p.davini@isac.cnr.it">p.davini@isac.cnr.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>seq &lt;- 1 : (2 * 3 * 4 * 5 * 6 * 8)
mod1 &lt;- sin( 0.7 + seq )^2 + cos( seq ^ 2 * 1.22  )
dim(mod1) &lt;- c(dataset = 2, member = 3, sdate = 4, ftime = 5, lat = 6, 
              lon = 8)
mod2 &lt;- sin( seq * 2 ) ^ 3 + cos( seq ^ 2 )
dim(mod2) &lt;- c(dataset = 2, member = 3, sdate = 4, ftime = 5, lat = 6, 
              lon = 8)
lon &lt;- seq(0, 35, 5)
lat &lt;- seq(0, 25, 5)
exp1 &lt;- list(data = mod1, coords = list(lat = lat, lon = lon))
exp2 &lt;- list(data = mod2, coords = list(lat = lat, lon = lon))
attr(exp1, 'class') &lt;- 's2dv_cube'
attr(exp2, 'class') &lt;- 's2dv_cube'
d = as.POSIXct(c("2017/01/01", "2017/01/02", "2017/01/03", "2017/01/04", 
                "2017/01/05", "2018/01/01", "2018/01/02", "2018/01/03",
                "2018/01/04", "2018/01/05", "2019/01/01", "2019/01/02", 
                "2019/01/03", "2019/01/04", "2019/01/05", "2020/01/01", 
                "2020/01/02", "2020/01/03", "2020/01/04", "2020/01/05"))
exp1$attrs$Dates = d
exp2$attrs$Dates = d

cal &lt;- CST_MultiEOF(datalist = list(exp1, exp2), neof_composed = 2)
</code></pre>

<hr>
<h2 id='CST_MultiMetric'>Multiple Metrics applied in Multiple Model Anomalies</h2><span id='topic+CST_MultiMetric'></span>

<h3>Description</h3>

<p>This function calculates correlation (Anomaly Correlation 
Coefficient; ACC), root mean square error (RMS) and the root mean square error 
skill score (RMSSS) of individual anomaly models and multi-models mean (if 
desired) with the observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_MultiMetric(
  exp,
  obs,
  metric = "correlation",
  multimodel = TRUE,
  time_dim = "ftime",
  memb_dim = "member",
  sdate_dim = "sdate"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_MultiMetric_+3A_exp">exp</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by 
<code>CST_Anomaly</code> function, containing the anomaly of the seasonal forecast 
experiments data in the element named <code>$data</code>.</p>
</td></tr>
<tr><td><code id="CST_MultiMetric_+3A_obs">obs</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> as returned by 
<code>CST_Anomaly</code> function, containing the anomaly of observed data in the 
element named <code>$data</code>.</p>
</td></tr>
<tr><td><code id="CST_MultiMetric_+3A_metric">metric</code></td>
<td>
<p>A character string giving the metric for computing the maximum 
skill. This must be one of the strings 'correlation', 'rms', 'rmsss' and 
'rpss'. If 'rpss' is chossen the terciles probabilities are evaluated.</p>
</td></tr>
<tr><td><code id="CST_MultiMetric_+3A_multimodel">multimodel</code></td>
<td>
<p>A logical value indicating whether a Multi-Model Mean should 
be computed.</p>
</td></tr>
<tr><td><code id="CST_MultiMetric_+3A_time_dim">time_dim</code></td>
<td>
<p>Name of the temporal dimension where a mean will be applied. 
It can be NULL, the default value is 'ftime'.</p>
</td></tr>
<tr><td><code id="CST_MultiMetric_+3A_memb_dim">memb_dim</code></td>
<td>
<p>Name of the member dimension. It can be NULL, the default 
value is 'member'.</p>
</td></tr>
<tr><td><code id="CST_MultiMetric_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>Name of the start date dimension or a dimension name 
identifiying the different forecast. It can be NULL, the default value is 
'sdate'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>s2dv_cube</code> containing the statistics of the 
selected metric in the element <code>$data</code> which is a list of arrays: for the
metric requested and others for statistics about its signeificance. The arrays 
have two dataset dimensions equal to the 'dataset' dimension in the 
<code>exp$data</code> and <code>obs$data</code> inputs. If <code>multimodel</code> is TRUE, the 
first position in the first 'nexp' dimension correspons to the Multi-Model Mean.
</p>


<h3>Author(s)</h3>

<p>Mishra Niti, <a href="mailto:niti.mishra@bsc.es">niti.mishra@bsc.es</a>
</p>
<p>Perez-Zanon Nuria, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>References</h3>

<p>Mishra, N., Prodhomme, C., &amp; Guemas, V. (n.d.). Multi-Model Skill 
Assessment of Seasonal Temperature and Precipitation Forecasts over Europe, 
29-31. doi: <a href="https://doi.org/10.1007/s00382-018-4404-z">10.1007/s00382-018-4404-z</a>
</p>


<h3>See Also</h3>

<p><code><a href="s2dv.html#topic+Corr">Corr</a></code>, <code><a href="s2dv.html#topic+RMS">RMS</a></code>, 
<code><a href="s2dv.html#topic+RMSSS">RMSSS</a></code> and <code><a href="#topic+CST_Load">CST_Load</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- rnorm(2*2*4*5*2*2)
dim(mod) &lt;- c(dataset = 2, member = 2, sdate = 4, ftime = 5, lat = 2, lon = 2)
obs &lt;- rnorm(1*1*4*5*2*2)
dim(obs) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 5, lat = 2, lon = 2)
lon &lt;- seq(0, 30, 5)
lat &lt;- seq(0, 25, 5)
coords &lt;- list(lat = lat, lon = lon)
exp &lt;- list(data = mod, coords = coords)
obs &lt;- list(data = obs, coords = coords)
attr(exp, 'class') &lt;- 's2dv_cube'
attr(obs, 'class') &lt;- 's2dv_cube'
a &lt;- CST_MultiMetric(exp = exp, obs = obs)
</code></pre>

<hr>
<h2 id='CST_MultivarRMSE'>Multivariate Root Mean Square Error (RMSE)</h2><span id='topic+CST_MultivarRMSE'></span>

<h3>Description</h3>

<p>This function calculates the RMSE from multiple variables, as the 
mean of each variable's RMSE scaled by its observed standard deviation. 
Variables can be weighted based on their relative importance (defined by the 
user).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_MultivarRMSE(
  exp,
  obs,
  weight = NULL,
  memb_dim = "member",
  dat_dim = "dataset",
  sdate_dim = "sdate",
  ftime_dim = "ftime"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_MultivarRMSE_+3A_exp">exp</code></td>
<td>
<p>A list of objects, one for each variable, of class <code>s2dv_cube</code> 
as returned by <code>CST_Anomaly</code> function, containing the anomaly of the 
seasonal forecast experiment data in the element named <code>$data</code>.</p>
</td></tr>
<tr><td><code id="CST_MultivarRMSE_+3A_obs">obs</code></td>
<td>
<p>A list of objects, one for each variable (in the same order than 
the input in 'exp') of class <code>s2dv_cube</code> as returned by 
<code>CST_Anomaly</code> function, containing the observed anomaly data in the 
element named <code>$data</code>.</p>
</td></tr>
<tr><td><code id="CST_MultivarRMSE_+3A_weight">weight</code></td>
<td>
<p>(optional) A vector of weight values to assign to each variable. 
If no weights are defined, a value of 1 is assigned to every variable.</p>
</td></tr>
<tr><td><code id="CST_MultivarRMSE_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the member 
dimension. It must be one dimension in 'exp' and 'obs'. The default value is 
'member'.</p>
</td></tr>
<tr><td><code id="CST_MultivarRMSE_+3A_dat_dim">dat_dim</code></td>
<td>
<p>A character string indicating the name of the dataset 
dimension. It must be one dimension in 'exp' and 'obs'. If there is no 
dataset dimension, it can be NULL. The default value is 'dataset'.</p>
</td></tr>
<tr><td><code id="CST_MultivarRMSE_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. It must be one dimension in 'exp' and 'obs'. The default value is 
'sdate'.</p>
</td></tr>
<tr><td><code id="CST_MultivarRMSE_+3A_ftime_dim">ftime_dim</code></td>
<td>
<p>A character string indicating the name of the forecast time 
dimension. It must be one dimension in 'exp' and 'obs'. The default value is 
'ftime'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>s2dv_cube</code> containing the RMSE in the element 
<code>$data</code> which is an array with two datset dimensions equal to the 
'dataset' dimension in the <code>exp$data</code> and <code>obs$data</code> inputs. An 
array with dimensions: c(number of exp, number of obs, 1 (the multivariate 
RMSE value), number of lat, number of lon)
</p>


<h3>Author(s)</h3>

<p>Deborah Verfaillie, <a href="mailto:deborah.verfaillie@bsc.es">deborah.verfaillie@bsc.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="s2dv.html#topic+RMS">RMS</a></code> and <code><a href="#topic+CST_Load">CST_Load</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with 2 variables
mod1 &lt;- abs(rnorm(1 * 3 * 4 * 5 * 6 * 7))
mod2 &lt;- abs(rnorm(1 * 3 * 4 * 5 * 6 * 7))
dim(mod1) &lt;- c(dataset = 1, member = 3, sdate = 4, ftime = 5, lat = 6, lon = 7)
dim(mod2) &lt;- c(dataset = 1, member = 3, sdate = 4, ftime = 5, lat = 6, lon = 7)
obs1 &lt;- abs(rnorm(1 * 1 * 4 * 5 * 6 * 7))
obs2 &lt;- abs(rnorm(1 * 1 * 4 * 5 * 6 * 7))
dim(obs1) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 5, lat = 6, lon = 7)
dim(obs2) &lt;- c(dataset = 1, member = 1, sdate = 4, ftime = 5, lat = 6, lon = 7)
lon &lt;- seq(0, 30, 5)
lat &lt;- seq(0, 25, 5)
coords &lt;- list(lat = lat, lon = lon)
exp1 &lt;- list(data = mod1, coords = coords, 
            attrs = list(Datasets = "EXP1", source_files = "file1", 
                         Variable = list(varName = 'pre')))
exp2 &lt;- list(data = mod2, coords = coords, 
            attrs = list(Datasets = "EXP2", source_files = "file2", 
                         Variable = list(varName = 'tas')))
obs1 &lt;- list(data = obs1, coords = coords, 
            attrs = list(Datasets = "OBS1", source_files = "file1", 
                         Variable = list(varName = 'pre')))
obs2 &lt;- list(data = obs2, coords = coords, 
            attrs = list(Datasets = "OBS2", source_files = "file2", 
                         Variable = list(varName = 'tas')))
attr(exp1, 'class') &lt;- 's2dv_cube'
attr(exp2, 'class') &lt;- 's2dv_cube'
attr(obs1, 'class') &lt;- 's2dv_cube'
attr(obs2, 'class') &lt;- 's2dv_cube'
anom1 &lt;- CST_Anomaly(exp1, obs1, cross = TRUE, memb = TRUE)
anom2 &lt;- CST_Anomaly(exp2, obs2, cross = TRUE, memb = TRUE)
ano_exp &lt;- list(anom1$exp, anom2$exp)
ano_obs &lt;- list(anom1$obs, anom2$obs)
a &lt;- CST_MultivarRMSE(exp = ano_exp, obs = ano_obs, weight = c(1, 2))
</code></pre>

<hr>
<h2 id='CST_ProxiesAttractor'>Computing two dinamical proxies of the attractor in s2dv_cube.</h2><span id='topic+CST_ProxiesAttractor'></span>

<h3>Description</h3>

<p>This function computes two dinamical proxies of the attractor: 
The local dimension (d) and the inverse of the persistence (theta) for an
's2dv_cube' object.
These two parameters will be used as a condition for the computation of 
dynamical scores to measure predictability and to compute bias correction 
conditioned  by the dynamics with the function DynBiasCorrection Function 
based on the matlab code (davide.faranda@lsce.ipsl.fr) used in
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_ProxiesAttractor(data, quanti, ncores = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_ProxiesAttractor_+3A_data">data</code></td>
<td>
<p>An s2dv_cube object with the data to create the attractor. Must be 
a matrix with the timesteps in nrow and the grids in ncol(dat(time,grids)</p>
</td></tr>
<tr><td><code id="CST_ProxiesAttractor_+3A_quanti">quanti</code></td>
<td>
<p>A number lower than 1 indicating the quantile to perform the 
computation of local dimension and theta.</p>
</td></tr>
<tr><td><code id="CST_ProxiesAttractor_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to use in parallel computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dim and theta
</p>


<h3>Author(s)</h3>

<p>Carmen Alvarez-Castro, <a href="mailto:carmen.alvarez-castro@cmcc.it">carmen.alvarez-castro@cmcc.it</a>
</p>
<p>Maria M. Chaves-Montero, <a href="mailto:mdm.chaves-montero@cmcc.it">mdm.chaves-montero@cmcc.it</a>
</p>
<p>Veronica Torralba, <a href="mailto:veronica.torralba@cmcc.it">veronica.torralba@cmcc.it</a>
</p>
<p>Davide Faranda, <a href="mailto:davide.faranda@lsce.ipsl.fr">davide.faranda@lsce.ipsl.fr</a>
</p>


<h3>References</h3>

<p>Faranda, D., Alvarez-Castro, M.C., Messori, G., Rodriguez, D., 
and Yiou, P. (2019). The hammam effect or how a warm ocean enhances large 
scale atmospheric predictability. Nature Communications, 10(1), 1316. 
doi: <a href="https://doi.org/10.1038/s41467-019-09305-8">10.1038/s41467-019-09305-8</a>&quot;
</p>
<p>Faranda, D., Gabriele Messori and Pascal Yiou. (2017).
Dynamical proxies of North Atlantic predictability and extremes. 
Scientific Reports, 7-41278, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: Computing the attractor using simple s2dv data
obs &lt;- rnorm(2 * 3 * 4 * 8 * 8)
dim(obs) &lt;- c(dataset = 1, member = 2, sdate = 3, ftime = 4, lat = 8, lon = 8)
lon &lt;- seq(10, 13.5, 0.5)
lat &lt;- seq(40, 43.5, 0.5)
coords &lt;- list(lon = lon, lat = lat)
data &lt;- list(data = obs, coords = coords)
class(data) &lt;- "s2dv_cube"
attractor &lt;- CST_ProxiesAttractor(data = data, quanti = 0.6)
</code></pre>

<hr>
<h2 id='CST_QuantileMapping'>Quantile Mapping for seasonal or decadal forecast data</h2><span id='topic+CST_QuantileMapping'></span>

<h3>Description</h3>

<p>This function is a wrapper of fitQmap and doQmap from package
'qmap' to be applied on the object of class 's2dv_cube'. The quantile mapping
adjustment between an experiment, typically a hindcast, and observation is 
applied to the experiment itself or to a provided forecast.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_QuantileMapping(
  exp,
  obs,
  exp_cor = NULL,
  sdate_dim = "sdate",
  memb_dim = "member",
  window_dim = NULL,
  method = "QUANT",
  na.rm = FALSE,
  ncores = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_QuantileMapping_+3A_exp">exp</code></td>
<td>
<p>An object of class <code>s2dv_cube</code>.</p>
</td></tr>
<tr><td><code id="CST_QuantileMapping_+3A_obs">obs</code></td>
<td>
<p>An object of class <code>s2dv_cube</code>.</p>
</td></tr>
<tr><td><code id="CST_QuantileMapping_+3A_exp_cor">exp_cor</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> in which the quantile 
mapping correction should be applied. If it is not specified, the correction
is applied in object 'exp'.</p>
</td></tr>
<tr><td><code id="CST_QuantileMapping_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the dimension name in which 
cross-validation would be applied when exp_cor is not provided. 'sdate' by 
default.</p>
</td></tr>
<tr><td><code id="CST_QuantileMapping_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the dimension name where
ensemble members are stored in the experimental arrays. It can be NULL if 
there is no ensemble member dimension. It is set as 'member' by default.</p>
</td></tr>
<tr><td><code id="CST_QuantileMapping_+3A_window_dim">window_dim</code></td>
<td>
<p>A character string indicating the dimension name where 
samples have been stored. It can be NULL (default) in case all samples are 
used.</p>
</td></tr>
<tr><td><code id="CST_QuantileMapping_+3A_method">method</code></td>
<td>
<p>A character string indicating the method to be used:'PTF', 
'DIST', 'RQUANT', 'QUANT', 'SSPLIN'. By default, the empirical quantile 
mapping 'QUANT' is used.</p>
</td></tr>
<tr><td><code id="CST_QuantileMapping_+3A_na.rm">na.rm</code></td>
<td>
<p>A logical value indicating if missing values should be removed
(FALSE by default).</p>
</td></tr>
<tr><td><code id="CST_QuantileMapping_+3A_ncores">ncores</code></td>
<td>
<p>An integer indicating the number of cores for parallel 
computation using multiApply function. The default value is NULL (1).</p>
</td></tr>
<tr><td><code id="CST_QuantileMapping_+3A_...">...</code></td>
<td>
<p>Additional parameters to be used by the method choosen. See qmap 
package for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>s2dv_cube</code> containing the experimental data
after applying the quantile mapping correction.
</p>


<h3>Author(s)</h3>

<p>Nuria Perez-Zanon, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="qmap.html#topic+fitQmap">fitQmap</a></code> and <code><a href="qmap.html#topic+doQmap">doQmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Use synthetic data
exp &lt;- NULL
exp$data &lt;- 1 : c(1 * 3 * 5 * 4 * 3 * 2)
dim(exp$data) &lt;- c(dataset = 1, member = 3, sdate = 5, ftime = 4,
                  lat = 3, lon = 2)
class(exp) &lt;- 's2dv_cube'
obs &lt;- NULL
obs$data &lt;- 101 : c(100 + 1 * 1 * 5 * 4 * 3 * 2)
dim(obs$data) &lt;- c(dataset = 1, member = 1, sdate = 5, ftime = 4,
                  lat = 3, lon = 2)
class(obs) &lt;- 's2dv_cube'
res &lt;- CST_QuantileMapping(exp, obs)

</code></pre>

<hr>
<h2 id='CST_RainFARM'>RainFARM stochastic precipitation downscaling of a CSTools object</h2><span id='topic+CST_RainFARM'></span>

<h3>Description</h3>

<p>This function implements the RainFARM stochastic precipitation
downscaling method and accepts a CSTools object (an object of the class 
's2dv_cube' as provided by 'CST_Load') as input.
Adapted for climate downscaling and including orographic correction
as described in Terzago et al. 2018.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_RainFARM(
  data,
  weights = 1,
  slope = 0,
  nf,
  kmin = 1,
  nens = 1,
  fglob = FALSE,
  fsmooth = TRUE,
  nprocs = 1,
  time_dim = NULL,
  verbose = FALSE,
  drop_realization_dim = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_RainFARM_+3A_data">data</code></td>
<td>
<p>An object of the class 's2dv_cube' as returned by 'CST_Load', 
containing the spatial precipitation fields to downscale.
The data object is expected to have an element named <code>$data</code> with at 
least two spatial dimensions named &quot;lon&quot; and &quot;lat&quot; and one or more 
dimensions over which to compute average spectral slopes (unless specified 
with parameter <code>slope</code>), which can be specified by parameter 
<code>time_dim</code>. The number of longitudes and latitudes in the input data is 
expected to be even and the same. If not the function will perform a 
subsetting to ensure this condition.</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_weights">weights</code></td>
<td>
<p>Matrix with climatological weights which can be obtained using
the <code>CST_RFWeights</code> function. If <code>weights = 1.</code> (default) no  
weights are used. The names of these dimensions must be at least 'lon' and 
'lat'.</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_slope">slope</code></td>
<td>
<p>Prescribed spectral slope. The default is <code>slope = 0.</code>
meaning that the slope is determined automatically over the dimensions 
specified by <code>time_dim</code>. A 1D array with named dimension can be 
provided (see details and examples).</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_nf">nf</code></td>
<td>
<p>Refinement factor for downscaling (the output resolution is 
increased by this factor).</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_kmin">kmin</code></td>
<td>
<p>First wavenumber for spectral slope (default: <code>kmin = 1</code>).</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_nens">nens</code></td>
<td>
<p>Number of ensemble members to produce (default: <code>nens = 1</code>).</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_fglob">fglob</code></td>
<td>
<p>Logical to conserve global precipitation over the domain 
(default: FALSE).</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_fsmooth">fsmooth</code></td>
<td>
<p>Logical to conserve precipitation with a smoothing kernel 
(default: TRUE).</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_nprocs">nprocs</code></td>
<td>
<p>The number of parallel processes to spawn for the use for 
parallel computation in multiple cores. (default: 1)</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_time_dim">time_dim</code></td>
<td>
<p>String or character array with name(s) of dimension(s)
(e.g. &quot;ftime&quot;, &quot;sdate&quot;, &quot;member&quot; ...) over which to compute spectral slopes.
If a character array of dimension names is provided, the spectral slopes
will be computed as an average over all elements belonging to those 
dimensions. If omitted one of c(&quot;ftime&quot;, &quot;sdate&quot;, &quot;time&quot;) is searched and 
the first one with more than one element is chosen.</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_verbose">verbose</code></td>
<td>
<p>Logical for verbose output (default: FALSE).</p>
</td></tr>
<tr><td><code id="CST_RainFARM_+3A_drop_realization_dim">drop_realization_dim</code></td>
<td>
<p>Logical to remove the &quot;realization&quot; stochastic 
ensemble dimension, needed for saving data through function CST_SaveData 
(default: FALSE) with the following behaviour if set to TRUE:
</p>

<ol>
<li><p>if <code>nens == 1</code>: the dimension is dropped;
</p>
</li>
<li><p>if <code>nens &gt; 1</code> and a &quot;member&quot; dimension exists: the &quot;realization&quot; 
and &quot;member&quot; dimensions are compacted (multiplied) and the resulting 
dimension is named &quot;member&quot;;
</p>
</li>
<li><p>if <code>nens &gt; 1</code> and a &quot;member&quot; dimension does not exist: the 
&quot;realization&quot; dimension is renamed to &quot;member&quot;.
</p>
</li></ol>
</td></tr>
</table>


<h3>Details</h3>

<p>Wether parameter 'slope' and 'weights' presents seasonality 
dependency, a dimension name should match between these parameters and the 
input data in parameter 'data'. See example 2 below where weights and slope 
vary with 'sdate' dimension.
</p>


<h3>Value</h3>

<p>CST_RainFARM() returns a downscaled CSTools object (i.e., of the 
class 's2dv_cube'). If <code>nens &gt; 1</code> an additional dimension named 
&quot;realization&quot; is added to the <code>$data</code> array after the &quot;member&quot; dimension 
(unless <code>drop_realization_dim = TRUE</code> is specified). The ordering of the 
remaining dimensions in the <code>$data</code> element of the input object is 
maintained.
</p>


<h3>Author(s)</h3>

<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>


<h3>References</h3>

<p>Terzago, S. et al. (2018). NHESS 18(11), 2825-2840.
doi: <a href="https://doi.org/10.5194/nhess-18-2825-2018">10.5194/nhess-18-2825-2018</a>;
D'Onofrio et al. (2014), J of Hydrometeorology 15, 830-843; Rebora et. al. 
(2006), JHM 7, 724.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: using CST_RainFARM for a CSTools object
nf &lt;- 8   # Choose a downscaling by factor 8
exp &lt;- 1 : (2 * 3 * 4 * 8 * 8)
dim(exp) &lt;- c(dataset = 1, member = 2, sdate = 3, ftime = 4, lat = 8, lon = 8)
lon &lt;- seq(10, 13.5, 0.5)
lat &lt;- seq(40, 43.5, 0.5)
coords &lt;- list(lon = lon, lat = lat)
data &lt;- list(data = exp, coords = coords)
class(data) &lt;- 's2dv_cube'
# Create a test array of weights
ww &lt;- array(1., dim = c(lon = 8 * nf, lat = 8 * nf))
res &lt;- CST_RainFARM(data, nf = nf, weights = ww, nens = 3, time_dim = 'ftime')
</code></pre>

<hr>
<h2 id='CST_RegimesAssign'>Function for matching a field of anomalies with 
a set of maps used as a reference (e.g. clusters obtained from the WeatherRegime function)</h2><span id='topic+CST_RegimesAssign'></span>

<h3>Description</h3>

<p>This function performs the matching between a field of anomalies 
and a set of maps which will be used as a reference. The anomalies will be 
assigned to the reference map for which the minimum Eucledian distance 
(method =’distance’) or highest spatial correlation (method = 'ACC') is 
obtained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_RegimesAssign(
  data,
  ref_maps,
  method = "distance",
  composite = FALSE,
  memb = FALSE,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_RegimesAssign_+3A_data">data</code></td>
<td>
<p>An 's2dv_cube' object.</p>
</td></tr>
<tr><td><code id="CST_RegimesAssign_+3A_ref_maps">ref_maps</code></td>
<td>
<p>An 's2dv_cube' object as the output of CST_WeatherRegimes.</p>
</td></tr>
<tr><td><code id="CST_RegimesAssign_+3A_method">method</code></td>
<td>
<p>Whether the matching will be performed in terms of minimum 
distance (default = 'distance') or the maximum spatial correlation 
(method = 'ACC') between the maps.</p>
</td></tr>
<tr><td><code id="CST_RegimesAssign_+3A_composite">composite</code></td>
<td>
<p>A logical parameter indicating if the composite maps are 
computed or not (default = FALSE).</p>
</td></tr>
<tr><td><code id="CST_RegimesAssign_+3A_memb">memb</code></td>
<td>
<p>A logical value indicating whether to compute composites for 
separate members (default FALSE) or as unique ensemble (TRUE). This option 
is only available for when parameter 'composite' is set to TRUE and the data 
object has a dimension named 'member'.</p>
</td></tr>
<tr><td><code id="CST_RegimesAssign_+3A_ncores">ncores</code></td>
<td>
<p>The number of multicore threads to use for parallel computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements <code>$data</code> (a 's2dv_cube' object containing 
the composites cluster=1,..,K for case (*1) or only k=1 for any specific 
cluster, i.e., case (*2)) (only when composite = 'TRUE') and <code>$statistics</code> 
that includes <code>$pvalue</code> (array with the same structure as <code>$data</code> 
containing the pvalue of the composites obtained through a t-test that 
accounts for the serial dependence of the data with the same structure as 
Composite.)(only when composite = 'TRUE'), <code>$cluster</code> (array with the 
same dimensions as data (except latitude and longitude which are removed) 
indicating the ref_maps to which each point is allocated.), <code>$frequency</code> 
(A vector of integers (from k=1,...k n reference maps) indicating the 
percentage of assignations corresponding to each map.).
</p>


<h3>Author(s)</h3>

<p>Verónica Torralba - BSC, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>
</p>


<h3>References</h3>

<p>Torralba, V. (2019) Seasonal climate prediction for the wind 
energy sector: methods and tools for the development of a climate service. 
Thesis. Available online: <a href="https://eprints.ucm.es/56841/">https://eprints.ucm.es/56841/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- array(abs(rnorm(1280, 282.7, 6.4)), dim = c(dataset = 2, member = 2, 
                                                   sdate = 3, ftime = 3, 
                                                   lat = 4, lon = 4))
coords &lt;- list(lon = seq(0, 3), lat = seq(47, 44))
exp &lt;- list(data = data, coords = coords)
class(exp) &lt;- 's2dv_cube'
regimes &lt;- CST_WeatherRegimes(data = exp, EOFs = FALSE, 
                             ncenters = 4)
res1 &lt;- CST_RegimesAssign(data = exp, ref_maps = regimes, 
                         composite = FALSE)
</code></pre>

<hr>
<h2 id='CST_RFSlope'>RainFARM spectral slopes from a CSTools object</h2><span id='topic+CST_RFSlope'></span>

<h3>Description</h3>

<p>This function computes spatial spectral slopes from a CSTools 
object to be used for RainFARM stochastic precipitation downscaling method and 
accepts a CSTools object (of the class 's2dv_cube') as input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_RFSlope(data, kmin = 1, time_dim = NULL, ncores = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_RFSlope_+3A_data">data</code></td>
<td>
<p>An object of the class 's2dv_cube', containing the spatial 
precipitation fields to downscale. The data object is expected to have an 
element named <code>$data</code> with at least two spatial dimensions named &quot;lon&quot; 
and &quot;lat&quot; and one or more dimensions over which to average these slopes, 
which can be specified by parameter <code>time_dim</code>.</p>
</td></tr>
<tr><td><code id="CST_RFSlope_+3A_kmin">kmin</code></td>
<td>
<p>First wavenumber for spectral slope (default <code>kmin=1</code>).</p>
</td></tr>
<tr><td><code id="CST_RFSlope_+3A_time_dim">time_dim</code></td>
<td>
<p>String or character array with name(s) of dimension(s) (e.g. 
&quot;ftime&quot;, &quot;sdate&quot;, &quot;member&quot; ...) over which to compute spectral slopes. If a 
character array of dimension names is provided, the spectral slopes will be 
computed as an average over all elements belonging to those dimensions. If 
omitted one of c(&quot;ftime&quot;, &quot;sdate&quot;, &quot;time&quot;)  is searched and the first one 
with more than one element is chosen.</p>
</td></tr>
<tr><td><code id="CST_RFSlope_+3A_ncores">ncores</code></td>
<td>
<p>Is an integer that indicates the number of cores for parallel 
computations using multiApply function. The default value is one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>CST_RFSlope() returns spectral slopes using the RainFARM convention
(the logarithmic slope of k*|A(k)|^2 where A(k) are the spectral amplitudes).
The returned array has the same dimensions as the <code>exp</code> element of the 
input object, minus the dimensions specified by <code>lon_dim</code>, 
<code>lat_dim</code> and <code>time_dim</code>.
</p>


<h3>Author(s)</h3>

<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>exp &lt;- 1 : (2 * 3 * 4 * 8 * 8)
dim(exp) &lt;- c(dataset = 1, member = 2, sdate = 3, ftime = 4, lat = 8, lon = 8)
lon &lt;- seq(10, 13.5, 0.5)
lat &lt;- seq(40, 43.5, 0.5)
coords &lt;- list(lon = lon, lat = lat)
data &lt;- list(data = exp, coords = coords)
class(data) &lt;- 's2dv_cube'
slopes &lt;- CST_RFSlope(data)
</code></pre>

<hr>
<h2 id='CST_RFTemp'>Temperature downscaling of a CSTools object using lapse rate
correction or a reference field</h2><span id='topic+CST_RFTemp'></span>

<h3>Description</h3>

<p>This function implements a simple lapse rate correction of a
temperature field (an object of class 's2dv_cube' as provided by
'CST_Load') as input.
The input lon grid must be increasing (but can be modulo 360).
The input lat grid can be irregularly spaced (e.g. a Gaussian grid)
The output grid can be irregularly spaced in lon and/or lat.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_RFTemp(
  data,
  oro,
  xlim = NULL,
  ylim = NULL,
  lapse = 6.5,
  lon_dim = "lon",
  lat_dim = "lat",
  time_dim = NULL,
  nolapse = FALSE,
  verbose = FALSE,
  compute_delta = FALSE,
  method = "bilinear",
  delta = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_RFTemp_+3A_data">data</code></td>
<td>
<p>An object of the class 's2dv_cube' as returned by 'CST_Load',
containing the temperature fields to downscale. The data object is expected 
to have an element named <code>$data</code> with at least two spatial dimensions 
named &quot;lon&quot; and &quot;lat&quot;. (these default names can be changed with the 
<code>lon_dim</code> and <code>lat_dim</code> parameters).</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_oro">oro</code></td>
<td>
<p>An object of the class 's2dv_cube' as returned by 'CST_Load',
containing fine scale orography (in meters). The destination downscaling 
area must be contained in the orography field.</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_xlim">xlim</code></td>
<td>
<p>Vector with longitude bounds for downscaling; the full input
field is downscaled if 'xlim' and 'ylim' are not specified.</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_ylim">ylim</code></td>
<td>
<p>Vector with latitude bounds for downscaling</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_lapse">lapse</code></td>
<td>
<p>Float with environmental lapse rate</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_lon_dim">lon_dim</code></td>
<td>
<p>String with name of longitude dimension</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_lat_dim">lat_dim</code></td>
<td>
<p>String with name of latitude dimension</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_time_dim">time_dim</code></td>
<td>
<p>A vector of character string indicating the name of temporal 
dimension. By default, it is set to NULL and it considers &quot;ftime&quot;, &quot;sdate&quot; 
and &quot;time&quot; as temporal dimensions.</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_nolapse">nolapse</code></td>
<td>
<p>Logical, if true 'oro' is interpreted as a fine-scale
climatology and used directly for bias correction.</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_verbose">verbose</code></td>
<td>
<p>Logical if to print diagnostic output.</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_compute_delta">compute_delta</code></td>
<td>
<p>Logical if true returns only a delta to be used for
out-of-sample forecasts. Returns an object of the class 's2dv_cube',
containing a delta. Activates 'nolapse = TRUE'.</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_method">method</code></td>
<td>
<p>String indicating the method used for interpolation:
&quot;nearest&quot; (nearest neighbours followed by smoothing with a circular
uniform weights kernel), &quot;bilinear&quot; (bilinear interpolation)
The two methods provide similar results, but nearest is slightly better
provided that the fine-scale grid is correctly centered as a subdivision
of the large-scale grid.</p>
</td></tr>
<tr><td><code id="CST_RFTemp_+3A_delta">delta</code></td>
<td>
<p>An object of the class 's2dv_cube', containing a delta
to be applied to the downscaled input data. Activates 'nolapse = TRUE'.
The grid of this object must coincide with that of the required output.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>CST_RFTemp() returns a downscaled CSTools object (i.e., of the class 
's2dv_cube').
</p>


<h3>Author(s)</h3>

<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>


<h3>References</h3>

<p>Method described in ERA4CS MEDSCOPE milestone M3.2: 
High-quality climate prediction data available to WP4 here: 
<a href="https://www.medscope-project.eu/the-project/deliverables-reports/">https://www.medscope-project.eu/the-project/deliverables-reports/</a>
and in H2020 ECOPOTENTIAL Deliverable No. 8.1:
High resolution (1-10 km) climate, land use and ocean change scenarios available 
here: <a href="https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e5b6cd2324&amp;appId=PPGMS">https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e5b6cd2324&amp;appId=PPGMS</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate simple synthetic data and downscale by factor 4
t &lt;- rnorm(7 * 6 * 2 * 3 * 4)*10 + 273.15 + 10
dim(t) &lt;- c(dataset = 1, member = 2, sdate = 3, ftime = 4, lat = 6, lon = 7)
lon &lt;- seq(3, 9, 1)
lat &lt;- seq(42, 47, 1)
coords &lt;- list(lat = lat, lon = lon)
exp &lt;- list(data = t, coords = coords)
attr(exp, 'class') &lt;- 's2dv_cube'
o &lt;- runif(29*29)*3000
dim(o) &lt;- c(lats = 29, lons = 29)
lon &lt;- seq(3, 10, 0.25)
lat &lt;- seq(41, 48, 0.25)
coords &lt;- list(lat = lat, lon = lon)
oro &lt;- list(data = o, coords = coords)
attr(oro, 'class') &lt;- 's2dv_cube'
res &lt;- CST_RFTemp(data = exp, oro = oro, xlim = c(4,8), ylim = c(43, 46), 
                 lapse = 6.5, time_dim = 'ftime',
                 lon_dim = 'lon', lat_dim = 'lat')
</code></pre>

<hr>
<h2 id='CST_RFWeights'>Compute climatological weights for RainFARM stochastic precipitation downscaling</h2><span id='topic+CST_RFWeights'></span>

<h3>Description</h3>

<p>Compute climatological (&quot;orographic&quot;) weights from a fine-scale 
precipitation climatology file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_RFWeights(
  climfile,
  nf,
  lon,
  lat,
  varname = NULL,
  fsmooth = TRUE,
  lonname = "lon",
  latname = "lat",
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_RFWeights_+3A_climfile">climfile</code></td>
<td>
<p>Filename of a fine-scale precipitation climatology. The file 
is expected to be in NetCDF format and should contain at least one 
precipitation field. If several fields at different times are provided,
a climatology is derived by time averaging. Suitable climatology files could 
be for example a fine-scale precipitation climatology from a high-resolution 
regional climate model (see e.g. Terzago et al. 2018), a local 
high-resolution gridded climatology from observations, or a reconstruction 
such as those which can be downloaded from the WORLDCLIM 
(<a href="https://www.worldclim.org">https://www.worldclim.org</a>) or CHELSA (<a href="https://chelsa-climate.org/">https://chelsa-climate.org/</a>) 
websites. The latter data will need to be converted to NetCDF format before 
being used (see for example the GDAL tools (<a href="https://gdal.org/">https://gdal.org/</a>). It  
could also be an 's2dv_cube' object.</p>
</td></tr>
<tr><td><code id="CST_RFWeights_+3A_nf">nf</code></td>
<td>
<p>Refinement factor for downscaling (the output resolution is 
increased by this factor).</p>
</td></tr>
<tr><td><code id="CST_RFWeights_+3A_lon">lon</code></td>
<td>
<p>Vector of longitudes.</p>
</td></tr>
<tr><td><code id="CST_RFWeights_+3A_lat">lat</code></td>
<td>
<p>Vector of latitudes. The number of longitudes and latitudes is 
expected to be even and the same. If not the function will perform a 
subsetting to ensure this condition.</p>
</td></tr>
<tr><td><code id="CST_RFWeights_+3A_varname">varname</code></td>
<td>
<p>Name of the variable to be read from <code>climfile</code>.</p>
</td></tr>
<tr><td><code id="CST_RFWeights_+3A_fsmooth">fsmooth</code></td>
<td>
<p>Logical to use smooth conservation (default) or large-scale 
box-average conservation.</p>
</td></tr>
<tr><td><code id="CST_RFWeights_+3A_lonname">lonname</code></td>
<td>
<p>A character string indicating the name of the longitudinal 
dimension set as 'lon' by default.</p>
</td></tr>
<tr><td><code id="CST_RFWeights_+3A_latname">latname</code></td>
<td>
<p>A character string indicating the name of the latitudinal 
dimension set as 'lat' by default.</p>
</td></tr>
<tr><td><code id="CST_RFWeights_+3A_ncores">ncores</code></td>
<td>
<p>An integer that indicates the number of cores for parallel 
computations using multiApply function. The default value is one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class 's2dv_cube' containing in matrix <code>data</code> the 
weights with dimensions (lon, lat).
</p>


<h3>Author(s)</h3>

<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>


<h3>References</h3>

<p>Terzago, S., Palazzi, E., &amp; von Hardenberg, J. (2018).
Stochastic downscaling of precipitation in complex orography: 
A simple method to reproduce a realistic fine-scale climatology.
Natural Hazards and Earth System Sciences, 18(11),
2825-2840. doi: <a href="https://doi.org/10.5194/nhess-18-2825-2018">10.5194/nhess-18-2825-2018</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create weights to be used with the CST_RainFARM() or RainFARM() functions
# using an external random data in the form of 's2dv_cube'.
obs &lt;- rnorm(2 * 3 * 4 * 8 * 8)
dim(obs) &lt;- c(dataset = 1, member = 2, sdate = 3, ftime = 4, lat = 8, lon = 8)
lon &lt;- seq(10, 13.5, 0.5)
lat &lt;- seq(40, 43.5, 0.5)
coords &lt;- list(lon = lon, lat = lat)
data &lt;- list(data = obs, coords = coords)
class(data) &lt;- "s2dv_cube"
res &lt;- CST_RFWeights(climfile = data, nf = 3, lon, lat, lonname = 'lon', 
                    latname = 'lat', fsmooth = TRUE)
</code></pre>

<hr>
<h2 id='CST_SaveExp'>Save objects of class 's2dv_cube' to data in NetCDF format</h2><span id='topic+CST_SaveExp'></span>

<h3>Description</h3>

<p>This function allows to divide and save a object of class 
's2dv_cube' into a NetCDF file, allowing to reload the saved data using 
<code>CST_Start</code> or <code>CST_Load</code> functions. It also allows to save any 
's2dv_cube' object that follows the NetCDF attributes conventions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_SaveExp(
  data,
  destination = "./",
  startdates = NULL,
  sdate_dim = "sdate",
  ftime_dim = "time",
  memb_dim = "member",
  dat_dim = "dataset",
  var_dim = "var",
  drop_dims = NULL,
  single_file = FALSE,
  extra_string = NULL,
  global_attrs = NULL,
  units_hours_since = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_SaveExp_+3A_data">data</code></td>
<td>
<p>An object of class <code>s2dv_cube</code>.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_destination">destination</code></td>
<td>
<p>A character string containing the directory name in which 
to save the data. NetCDF file for each starting date are saved into the 
folder tree: 'destination/Dataset/variable/'. By default the function 
saves the data into the working directory.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_startdates">startdates</code></td>
<td>
<p>A vector of dates that will be used for the filenames 
when saving the data in multiple files (single_file = FALSE). It must be a 
vector of the same length as the start date dimension of data. It must be a 
vector of class <code>Dates</code>, <code>'POSIXct'</code> or character with lenghts 
between 1 and 10. If it is NULL, the coordinate corresponding the the start 
date dimension or the first Date of each time step will be used as the name 
of the files. It is NULL by default.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. By default, it is set to 'sdate'. It can be NULL if there is no
start date dimension.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_ftime_dim">ftime_dim</code></td>
<td>
<p>A character string indicating the name of the forecast time
dimension. If 'Dates' are used, it can't be NULL. If there is no forecast 
time dimension, 'Dates' will be set to NULL and will not be used. By 
default, it is set to 'time'.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the member 
dimension. It can be NULL if there is no member dimension. By default, it is
set to 'member'.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_dat_dim">dat_dim</code></td>
<td>
<p>A character string indicating the name of dataset dimension. 
It can be NULL if there is no dataset dimension. By default, it is set to 
'dataset'.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_var_dim">var_dim</code></td>
<td>
<p>A character string indicating the name of variable dimension. 
It can be NULL if there is no variable dimension. By default, it is set to 
'var'.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_drop_dims">drop_dims</code></td>
<td>
<p>(optional) A vector of character strings indicating the 
dimension names of length 1 that need to be dropped in order that they don't 
appear in the netCDF file. Only is allowed to drop dimensions that are not 
used in the computation. The dimensions used in the computation are the ones 
specified in: sdate_dim, ftime_dim, dat_dim, var_dim and memb_dim. It is 
NULL by default.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_single_file">single_file</code></td>
<td>
<p>A logical value indicating if all object is saved in a 
single file (TRUE) or in multiple files (FALSE). When it is FALSE, 
the array is separated for datasets, variable and start date. When there are 
no specified time dimensions, the data will be saved in a single file by 
default. The output file name when 'single_file' is TRUE is a character 
string containing: '&lt;var&gt;_&lt;first_sdate&gt;_&lt;last_sdate&gt;.nc'; when it is FALSE, 
it is '&lt;var&gt;_&lt;sdate&gt;.nc'. It is FALSE by default.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_extra_string">extra_string</code></td>
<td>
<p>(Optional) A character string to be included as part of 
the file name, for instance, to identify member or realization. When 
single_file is TRUE, the 'extra_string' will substitute all the default 
file name; when single_file is FALSE, the 'extra_string' will be added 
in the file name as: '&lt;var&gt;_&lt;extra_string&gt;_&lt;sdate&gt;.nc'. It is NULL by 
default.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_global_attrs">global_attrs</code></td>
<td>
<p>(Optional) A list with elements containing the global 
attributes to be saved in the NetCDF.</p>
</td></tr>
<tr><td><code id="CST_SaveExp_+3A_units_hours_since">units_hours_since</code></td>
<td>
<p>(Optional) A logical value only available for the 
case: 'Dates' have forecast time and start date dimension, 'single_file' is 
TRUE and 'time_bounds' are not used. When it is TRUE, it saves the forecast 
time with units of 'hours since'; if it is FALSE, the time units will be a 
number of time steps with its corresponding frequency (e.g. n days, n months 
or n hours). It is FALSE by default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Multiple or single NetCDF files containing the data array.<br />
</p>
<table>
<tr><td><code>single_file is TRUE</code></td>
<td>

<p>All data is saved in a single file located in the specified destination  
path with the following name (by default): 
'&lt;variable_name&gt;_&lt;first_sdate&gt;_&lt;last_sdate&gt;.nc'. Multiple variables
are saved separately in the same file. The forecast time units 
are calculated from each start date (if sdate_dim is not NULL) or from 
the time step. If 'units_hours_since' is TRUE, the forecast time units 
will be 'hours since &lt;each start date&gt;'. If 'units_hours_since' is FALSE, 
the forecast time units are extracted from the frequency of the time steps 
(hours, days, months); if no frequency is found, the units will be ’hours 
since’. When the time units are 'hours since' the time ateps are assumed to 
be equally spaced.
</p>
</td></tr>
<tr><td><code>single_file is FALSE</code></td>
<td>

<p>The data array is subset and stored into multiple files. Each file 
contains the data subset for each start date, variable and dataset. Files 
with different variables and datasets are stored in separated directories 
within the following directory tree: 'destination/Dataset/variable/'. 
The name of each file will be by default: '&lt;variable_name&gt;_&lt;sdate&gt;.nc'. 
The forecast time units are calculated from each start date (if sdate_dim 
is not NULL) or from the time step. The forecast time units will be 'hours 
since &lt;each start date&gt;'.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Perez-Zanon Nuria, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="startR.html#topic+Start">Start</a></code>, <code><a href="#topic+as.s2dv_cube">as.s2dv_cube</a></code> and 
<code><a href="#topic+s2dv_cube">s2dv_cube</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data &lt;- lonlat_temp_st$exp
CST_SaveExp(data = data, ftime_dim = 'ftime', var_dim = 'var', 
           dat_dim = 'dataset', sdate_dim = 'sdate')

## End(Not run)

</code></pre>

<hr>
<h2 id='CST_SplitDim'>Function to Split Dimension</h2><span id='topic+CST_SplitDim'></span>

<h3>Description</h3>

<p>This function split a dimension in two. The user can select the 
dimension to split and provide indices indicating how to split that dimension 
or dates and the frequency expected (monthly or by day, month and year). The 
user can also provide a numeric frequency indicating the length of each 
division.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_SplitDim(
  data,
  split_dim = "time",
  indices = NULL,
  freq = "monthly",
  new_dim_name = NULL,
  insert_ftime = NULL,
  ftime_dim = "time",
  sdate_dim = "sdate",
  return_indices = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_SplitDim_+3A_data">data</code></td>
<td>
<p>A 's2dv_cube' object</p>
</td></tr>
<tr><td><code id="CST_SplitDim_+3A_split_dim">split_dim</code></td>
<td>
<p>A character string indicating the name of the dimension to 
split. It is set as 'time' by default.</p>
</td></tr>
<tr><td><code id="CST_SplitDim_+3A_indices">indices</code></td>
<td>
<p>A vector of numeric indices or dates. If left at NULL, the 
dates provided in the s2dv_cube object (element Dates) will be used.</p>
</td></tr>
<tr><td><code id="CST_SplitDim_+3A_freq">freq</code></td>
<td>
<p>A character string indicating the frequency: by 'day', 'month' and 
'year' or 'monthly' (by default). 'month' identifies months between 1 and 12 
independently of the year they belong to, while 'monthly' differenciates 
months from different years.</p>
</td></tr>
<tr><td><code id="CST_SplitDim_+3A_new_dim_name">new_dim_name</code></td>
<td>
<p>A character string indicating the name of the new 
dimension.</p>
</td></tr>
<tr><td><code id="CST_SplitDim_+3A_insert_ftime">insert_ftime</code></td>
<td>
<p>An integer indicating the number of time steps to add at 
the begining of the time series.</p>
</td></tr>
<tr><td><code id="CST_SplitDim_+3A_ftime_dim">ftime_dim</code></td>
<td>
<p>A character string indicating the name of the forecast time
dimension. It is set as 'time' by default.</p>
</td></tr>
<tr><td><code id="CST_SplitDim_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. It is set as 'sdate' by default.</p>
</td></tr>
<tr><td><code id="CST_SplitDim_+3A_return_indices">return_indices</code></td>
<td>
<p>A logical value that if it is TRUE, the indices 
used in splitting the dimension will be returned. It is FALSE by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parameter 'insert_ftime' has been included for the case of using 
daily data, requiring split the temporal dimensions by months (or similar) and 
the first lead time doesn't correspondt to the 1st day of the month. In this 
case, the insert_ftime could be used, to get a final output correctly 
organized. E.g.: leadtime 1 is the 2nd of November and the input time series 
extend to the 31st of December. When requiring split by month with 
<code>inset_ftime = 1</code>, the 'monthly' dimension of length two will indicate 
the month (position 1 for November and position 2 for December), dimension 
'time' will be length 31. For November, the position 1 and 31 will be NAs, 
while from positon 2 to 30 will be filled with the data provided. This allows 
to select correctly days trhough time dimension.
</p>


<h3>Author(s)</h3>

<p>Nuria Perez-Zanon, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- 1 : 20
dim(data) &lt;- c(time = 10, lat = 2)
data &lt;-list(data = data)
class(data) &lt;- 's2dv_cube'
indices &lt;- c(rep(1,5), rep(2,5))
new_data &lt;- CST_SplitDim(data, indices = indices)
time &lt;- c(seq(ISOdate(1903, 1, 1), ISOdate(1903, 1, 4), "days"),
         seq(ISOdate(1903, 2, 1), ISOdate(1903, 2, 4), "days"),
         seq(ISOdate(1904, 1, 1), ISOdate(1904, 1, 2), "days"))
data &lt;- list(data = data$data, Dates = time)
class(data) &lt;- 's2dv_cube'
new_data &lt;- CST_SplitDim(data, indices = time)
new_data &lt;- CST_SplitDim(data, indices = time, freq = 'day')
new_data &lt;- CST_SplitDim(data, indices = time, freq = 'month')
new_data &lt;- CST_SplitDim(data, indices = time, freq = 'year')
</code></pre>

<hr>
<h2 id='CST_Start'>CSTools data retrieval function using Start</h2><span id='topic+CST_Start'></span>

<h3>Description</h3>

<p>This function aggregates, subsets and retrieves sub-seasonal, seasonal, 
decadal or climate projection data from NetCDF files in a local file system 
and arranges it for easy application of the CSTools functions. It calls the 
function <code>Start</code> from startR, which is an R package started at BSC with
the aim to develop a tool that allows the user to automatically process large 
multidimensional distributed data sets. Then, the output is transformed into 
's2dv_cube' object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_Start(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_Start_+3A_...">...</code></td>
<td>
<p>Parameters that are automatically forwarded to the 'startR::Start' 
function. See details in '?startR::Start'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It receives any number of parameters ('...') that are automatically forwarded 
to the 'startR::Start' function. See details in '?startR::Start'. The 
auxiliary functions used to define dimensions need to be called within the 
startR namespace (e.g. startR::indices(), startR::values(), startR::Sort(), 
startR::CircularSort(), startR::CDORemapper(), ...).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
 sdates &lt;- c('20101101', '20111101', '20121101')
 latmin &lt;- 44
 latmax &lt;- 47
 lonmin &lt;- 6
 lonmax &lt;- 9
 data &lt;- CST_Start(dat = path,
                   var = 'prlr',
                   ensemble = indices(1:6),
                   sdate = sdates,
                   time = 121:151,
                   latitude = values(list(latmin, latmax)),
                   longitude = values(list(lonmin, lonmax)),
                   synonims = list(longitude = c('lon', 'longitude'),
                                   latitude = c('lat', 'latitude')),
                   return_vars = list(time = 'sdate',
                                      longitude = NULL, latitude = NULL),
                   retrieve = FALSE)

## End(Not run) 

</code></pre>

<hr>
<h2 id='CST_Subset'>Subset an object of class s2dv_cube</h2><span id='topic+CST_Subset'></span>

<h3>Description</h3>

<p>This function allows to subset (i.e. slice, take a chunk of) the data inside
an object of class <code>s2dv_cube</code> and modify the dimensions, coordinates and
attributes accordingly, removing any variables, time steps and spatial
coordinates that are dropped when subsetting. It ensures that the information
inside the s2dv_cube remains coherent with the data it contains.<br /><br />
As in the function <code>Subset</code> from the ClimProjDiags package, the 
dimensions to subset along can be specified via the parameter <code>along</code> 
either with integer indices or by their name.<br /><br />
There are additional ways to adjust which dimensions are dropped in the 
resulting object: either to drop all, to drop none, to drop only the ones that
have been sliced or to drop only the ones that have not been sliced.<br /><br />
The <code>load_parameters</code> and <code>when</code> attributes of the original cube
are preserved. The <code>source_files</code> attribute is subset along the
<code>var_dim</code> and <code>dat_dim</code> dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_Subset(x, along, indices, drop = FALSE, var_dim = NULL, dat_dim = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_Subset_+3A_x">x</code></td>
<td>
<p>An object of class <code>s2dv_cube</code> to be sliced.</p>
</td></tr>
<tr><td><code id="CST_Subset_+3A_along">along</code></td>
<td>
<p>A vector with references to the dimensions to take the subset 
from: either integers or dimension names.</p>
</td></tr>
<tr><td><code id="CST_Subset_+3A_indices">indices</code></td>
<td>
<p>A list of indices to take from each dimension specified in 
'along'. If a single dimension is specified in 'along', it can be directly
provided as an integer or a vector.</p>
</td></tr>
<tr><td><code id="CST_Subset_+3A_drop">drop</code></td>
<td>
<p>Whether to drop all the dimensions of length 1 in the resulting 
array, none, only those that are specified in 'along', or only those that 
are not specified in 'along'. The possible values are: 'all' or TRUE, 'none'
or FALSE, 'selected', and 'non-selected'. The default value is FALSE.</p>
</td></tr>
<tr><td><code id="CST_Subset_+3A_var_dim">var_dim</code></td>
<td>
<p>A chatacter string indicating the name of the variable
dimension. The default value is NULL.</p>
</td></tr>
<tr><td><code id="CST_Subset_+3A_dat_dim">dat_dim</code></td>
<td>
<p>A character string indicating the name of dataset dimension.
The default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>s2dv_cube</code> with similar data, coordinates and 
attributes as the <code>x</code> input, but with trimmed or dropped dimensions.
</p>


<h3>Author(s)</h3>

<p>Agudetse Roures Victoria, <a href="mailto:victoria.agudetse@bsc.es">victoria.agudetse@bsc.es</a>
</p>


<h3>See Also</h3>

<p><a href="ClimProjDiags.html#topic+Subset">Subset</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example with sample data:
# Check original dimensions and coordinates
lonlat_temp$exp$dims
names(lonlat_temp$exp$coords)
# Subset the s2dv_cube
exp_subset &lt;- CST_Subset(lonlat_temp$exp,
                        along = c("lat", "lon"),
                        indices = list(1:10, 1:10),
                        drop = 'non-selected')
# Check new dimensions and coordinates
exp_subset$dims
names(exp_subset$coords)

</code></pre>

<hr>
<h2 id='CST_WeatherRegimes'>Function for Calculating the Cluster analysis</h2><span id='topic+CST_WeatherRegimes'></span>

<h3>Description</h3>

<p>This function computes the weather regimes from a cluster 
analysis. It is applied on the array <code>data</code> in a 's2dv_cube' object. The 
dimensionality of this object can be also reduced by using PCs obtained from 
the application of the #'EOFs analysis to filter the dataset. The cluster 
analysis can be performed with the traditional k-means or those methods
included in the hclust (stats package).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CST_WeatherRegimes(
  data,
  ncenters = NULL,
  EOFs = TRUE,
  neofs = 30,
  varThreshold = NULL,
  method = "kmeans",
  iter.max = 100,
  nstart = 30,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CST_WeatherRegimes_+3A_data">data</code></td>
<td>
<p>An 's2dv_cube' object.</p>
</td></tr>
<tr><td><code id="CST_WeatherRegimes_+3A_ncenters">ncenters</code></td>
<td>
<p>Number of clusters to be calculated with the clustering 
function.</p>
</td></tr>
<tr><td><code id="CST_WeatherRegimes_+3A_eofs">EOFs</code></td>
<td>
<p>Whether to compute the EOFs (default = 'TRUE') or not (FALSE) to 
filter the data.</p>
</td></tr>
<tr><td><code id="CST_WeatherRegimes_+3A_neofs">neofs</code></td>
<td>
<p>Number of modes to be kept (default = 30).</p>
</td></tr>
<tr><td><code id="CST_WeatherRegimes_+3A_varthreshold">varThreshold</code></td>
<td>
<p>Value with the percentage of variance to be explained by 
the PCs. Only sufficient PCs to explain this much variance will be used in 
the clustering.</p>
</td></tr>
<tr><td><code id="CST_WeatherRegimes_+3A_method">method</code></td>
<td>
<p>Different options to estimate the clusters. The most traditional 
approach is the k-means analysis (default=’kmeans’) but the function also 
support the different methods included in the hclust . These methods are:
&quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot; (= UPGMA), &quot;mcquitty&quot; 
(= WPGMA), &quot;median&quot; (= WPGMC) or &quot;centroid&quot; (= UPGMC). For more details 
about these methods see the hclust function documentation included in the 
stats package.</p>
</td></tr>
<tr><td><code id="CST_WeatherRegimes_+3A_iter.max">iter.max</code></td>
<td>
<p>Parameter to select the maximum number of iterations allowed 
(Only if method='kmeans' is selected).</p>
</td></tr>
<tr><td><code id="CST_WeatherRegimes_+3A_nstart">nstart</code></td>
<td>
<p>Parameter for the cluster analysis determining how many random 
sets to choose (Only if method='kmeans' is selected).</p>
</td></tr>
<tr><td><code id="CST_WeatherRegimes_+3A_ncores">ncores</code></td>
<td>
<p>The number of multicore threads to use for parallel computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements <code>$data</code> (a 's2dv_cube' object containing 
the composites cluster = 1,..,K for case (*1) or only k = 1 for any specific 
cluster, i.e., case (*2)) and <code>$statistics</code> that includes <code>$pvalue</code> 
(array with the same structure as <code>$data</code> containing the pvalue of the 
composites obtained through a t-test that accounts for the serial dependence.),
<code>cluster</code> (A matrix or vector with integers (from 1:k) indicating the 
cluster to which each time step is allocated.), <code>persistence</code> (Percentage 
of days in a month/season before a cluster is replaced for a new one (only if 
method=’kmeans’ has been selected.)), <code>frequency</code> (Percentage of days in 
a month/season belonging to each cluster (only if method=’kmeans’ has been 
selected).),
</p>


<h3>Author(s)</h3>

<p>Verónica Torralba - BSC, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>
</p>


<h3>References</h3>

<p>Cortesi, N., V., Torralba, N., González-Reviriego, A., Soret, and 
F.J., Doblas-Reyes (2019). Characterization of European wind speed variability 
using weather regimes. Climate Dynamics,53, 4961–4976, 
doi: <a href="https://doi.org/10.1007/s00382-019-04839-5">10.1007/s00382-019-04839-5</a>.
</p>
<p>Torralba, V. (2019) Seasonal climate prediction for the wind 
energy sector: methods and tools for the development of a climate service. 
Thesis. Available online: <a href="https://eprints.ucm.es/56841/">https://eprints.ucm.es/56841/</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- array(abs(rnorm(1280, 283.7, 6)), dim = c(dataset = 2, member = 2, 
                                                 sdate = 3, ftime = 3, 
                                                 lat = 4, lon = 4))
coords &lt;- list(lon = seq(0, 3), lat = seq(47, 44))
obs &lt;- list(data = data, coords = coords)
class(obs) &lt;- 's2dv_cube'

res1 &lt;- CST_WeatherRegimes(data = obs, EOFs = FALSE, ncenters = 4)
res2 &lt;- CST_WeatherRegimes(data = obs, EOFs = TRUE, ncenters = 3)

</code></pre>

<hr>
<h2 id='DynBiasCorrection'>Performing a Bias Correction conditioned by the dynamical
properties of the data.</h2><span id='topic+DynBiasCorrection'></span>

<h3>Description</h3>

<p>This function perform a bias correction conditioned by the 
dynamical properties of the dataset. This function used the functions 
'CST_Predictability' to divide in terciles the two dynamical proxies 
computed with 'CST_ProxiesAttractor'. A bias correction
between the model and the observations is performed using the division into
terciles of the local dimension 'dim' and inverse of the persistence 'theta'.
For instance, model values with lower 'dim' will be corrected with observed 
values with lower 'dim', and the same for theta. The function gives two options
of bias correction: one for 'dim' and/or one for 'theta'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DynBiasCorrection(
  exp,
  obs,
  method = "QUANT",
  wetday = FALSE,
  proxy = "dim",
  quanti,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DynBiasCorrection_+3A_exp">exp</code></td>
<td>
<p>A multidimensional array with named dimensions with the 
experiment data.</p>
</td></tr>
<tr><td><code id="DynBiasCorrection_+3A_obs">obs</code></td>
<td>
<p>A multidimensional array with named dimensions with the 
observation data.</p>
</td></tr>
<tr><td><code id="DynBiasCorrection_+3A_method">method</code></td>
<td>
<p>A character string indicating the method to apply bias 
correction among these ones:
&quot;PTF&quot;, &quot;RQUANT&quot;, &quot;QUANT&quot;, &quot;SSPLIN&quot;.</p>
</td></tr>
<tr><td><code id="DynBiasCorrection_+3A_wetday">wetday</code></td>
<td>
<p>Logical indicating whether to perform wet day correction 
or not OR a numeric threshold below which all values are set to zero (by 
default is set to 'FALSE').</p>
</td></tr>
<tr><td><code id="DynBiasCorrection_+3A_proxy">proxy</code></td>
<td>
<p>A character string indicating the proxy for local dimension 
'dim' or inverse of persistence 'theta' to apply the dynamical conditioned 
bias correction method.</p>
</td></tr>
<tr><td><code id="DynBiasCorrection_+3A_quanti">quanti</code></td>
<td>
<p>A number lower than 1 indicating the quantile to perform the 
computation of local dimension and theta.</p>
</td></tr>
<tr><td><code id="DynBiasCorrection_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to use in parallel computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A multidimensional array with named dimensions with a bias correction 
performed conditioned by local dimension 'dim' or inverse of persistence 'theta'.
</p>


<h3>Author(s)</h3>

<p>Carmen Alvarez-Castro, <a href="mailto:carmen.alvarez-castro@cmcc.it">carmen.alvarez-castro@cmcc.it</a>
</p>
<p>Maria M. Chaves-Montero, <a href="mailto:mdm.chaves-montero@cmcc.it">mdm.chaves-montero@cmcc.it</a>
</p>
<p>Veronica Torralba, <a href="mailto:veronica.torralba@cmcc.it">veronica.torralba@cmcc.it</a>
</p>
<p>Davide Faranda, <a href="mailto:davide.faranda@lsce.ipsl.fr">davide.faranda@lsce.ipsl.fr</a>
</p>


<h3>References</h3>

<p>Faranda, D., Alvarez-Castro, M.C., Messori, G., Rodriguez, D., 
and Yiou, P. (2019). The hammam effect or how a warm ocean enhances large 
scale atmospheric predictability.Nature Communications, 10(1), 1316. 
doi: <a href="https://doi.org/10.1038/s41467-019-09305-8">10.1038/s41467-019-09305-8</a>&quot;
</p>
<p>Faranda, D., Gabriele Messori and Pascal Yiou. (2017).
Dynamical proxies of North Atlantic predictability and extremes. 
Scientific Reports, 7-41278, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>expL &lt;- rnorm(1:2000)
dim (expL) &lt;- c(time =100,lat = 4, lon = 5)
obsL &lt;- c(rnorm(1:1980),expL[1,,]*1.2)
dim (obsL) &lt;- c(time = 100,lat = 4, lon = 5)
dynbias &lt;- DynBiasCorrection(exp = expL, obs = obsL, method='QUANT',
                            proxy= "dim", quanti = 0.6)
</code></pre>

<hr>
<h2 id='EnsClustering'>Ensemble clustering</h2><span id='topic+EnsClustering'></span>

<h3>Description</h3>

<p>This function performs a clustering on members/starting dates
and returns a number of scenarios, with representative members for each of 
them. The clustering is performed in a reduced EOF space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EnsClustering(
  data,
  lat,
  lon,
  time_moment = "mean",
  numclus = NULL,
  lon_lim = NULL,
  lat_lim = NULL,
  variance_explained = 80,
  numpcs = NULL,
  time_percentile = 90,
  time_dim = NULL,
  cluster_dim = "member",
  verbose = T
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EnsClustering_+3A_data">data</code></td>
<td>
<p>A matrix of dimensions 'dataset member sdate ftime lat lon' 
containing the variables to be analysed. Latitudinal dimension accepted 
names: 'lat', 'lats', 'latitude', 'y', 'j', 'nav_lat'. Longitudinal 
dimension accepted names: 'lon', 'lons','longitude', 'x', 'i', 'nav_lon'.</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_lat">lat</code></td>
<td>
<p>Vector of latitudes.</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_lon">lon</code></td>
<td>
<p>Vector of longitudes.</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_time_moment">time_moment</code></td>
<td>
<p>Decides the moment to be applied to the time dimension. Can 
be either 'mean' (time mean), 'sd' (standard deviation along time) or 'perc' 
(a selected percentile on time). If 'perc' the keyword 'time_percentile' is 
also used.</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_numclus">numclus</code></td>
<td>
<p>Number of clusters (scenarios) to be calculated. If set to NULL
the number of ensemble members divided by 10 is used, with a minimum of 2 
and a maximum of 8.</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_lon_lim">lon_lim</code></td>
<td>
<p>List with the two longitude margins in 'c(-180,180)' format.</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_lat_lim">lat_lim</code></td>
<td>
<p>List with the two latitude margins.</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_variance_explained">variance_explained</code></td>
<td>
<p>variance (percentage) to be explained by the set of 
EOFs. Defaults to 80. Not used if numpcs is specified.</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_numpcs">numpcs</code></td>
<td>
<p>Number of EOFs retained in the analysis (optional).</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_time_percentile">time_percentile</code></td>
<td>
<p>Set the percentile in time you want to analyse (used 
for 'time_moment = &quot;perc&quot;).</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_time_dim">time_dim</code></td>
<td>
<p>String or character array with name(s) of dimension(s) over 
which to compute statistics. If omitted c(&quot;ftime&quot;, &quot;sdate&quot;, &quot;time&quot;) are 
searched in this order.</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_cluster_dim">cluster_dim</code></td>
<td>
<p>Dimension along which to cluster. Typically &quot;member&quot; or 
&quot;sdate&quot;. This can also be a list like c(&quot;member&quot;, &quot;sdate&quot;).</p>
</td></tr>
<tr><td><code id="EnsClustering_+3A_verbose">verbose</code></td>
<td>
<p>Logical for verbose output</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements <code>$cluster</code> (cluster assigned for each member),
<code>$freq</code> (relative frequency of each cluster), <code>$closest_member</code>
(representative member for each cluster), <code>$repr_field</code> (list of fields for
each representative member), <code>composites</code> (list of mean fields for each 
cluster), <code>$lon</code> (selected longitudes of output fields), <code>$lat</code> 
(selected longitudes of output fields).
</p>


<h3>Author(s)</h3>

<p>Federico Fabiano - ISAC-CNR, <a href="mailto:f.fabiano@isac.cnr.it">f.fabiano@isac.cnr.it</a>
</p>
<p>Ignazio Giuntoli - ISAC-CNR, <a href="mailto:i.giuntoli@isac.cnr.it">i.giuntoli@isac.cnr.it</a>
</p>
<p>Danila Volpi - ISAC-CNR, <a href="mailto:d.volpi@isac.cnr.it">d.volpi@isac.cnr.it</a>
</p>
<p>Paolo Davini - ISAC-CNR, <a href="mailto:p.davini@isac.cnr.it">p.davini@isac.cnr.it</a>
</p>
<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>exp &lt;- array(abs(rnorm(1152))*275, dim = c(dataset = 1, member = 4, 
                                          sdate = 6, ftime = 3, 
                                          lat = 4, lon = 4))
lon &lt;- seq(0, 3)
lat &lt;- seq(48, 45)
res &lt;- EnsClustering(exp, lat = lat, lon = lon, numclus = 2,
                    cluster_dim = c("member", "dataset", "sdate"))

</code></pre>

<hr>
<h2 id='lonlat_prec'>Sample Of Experimental Precipitation Data In Function Of Longitudes And Latitudes</h2><span id='topic+lonlat_prec'></span>

<h3>Description</h3>

<p>This sample data set contains a small cutout of gridded seasonal precipitation
forecast data from the Copernicus Climate Change ECMWF-System 5 forecast 
system, to be used to demonstrate downscaling. Specifically, for the 'pr' 
(precipitation) variable, for the first 6 forecast ensemble members, daily
values, for all 31 days in March following the forecast starting dates in 
November of years 2010 to 2012, for a small 4x4 pixel cutout in a region in
the North-Western Italian Alps (44N-47N, 6E-9E). The data resolution is 1 
degree.
</p>


<h3>Details</h3>

<p>The 'CST_Load' call used to generate the data set in the infrastructure of 
the Marconi machine at CINECA is shown next, working on files which were 
extracted from forecast data available in the MEDSCOPE internal archive.
</p>
<pre>
library(CSTools)
infile &lt;- list(path = paste0('/esarchive/exp/ecmwf/system5c3s/daily_mean/',
                             '$VAR_NAME$_s0-24h/$VAR_NAME$_$START_DATE$.nc'))
lonlat_prec &lt;- CST_Load('prlr', exp = list(infile), obs = NULL,
                        sdates = c('20101101', '20111101', '20121101'),
                        leadtimemin = 121, leadtimemax = 151,
                        latmin = 44, latmax = 47,
                        lonmin = 6, lonmax = 9,
                        nmember = 6,
                        storefreq = "daily", sampleperiod = 1,
                        output = "lonlat"
                       )
</pre>


<h3>Author(s)</h3>

<p>Jost von Hardenberg <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>
<p>An-Chi Ho <a href="mailto:an.ho@bsc.es">an.ho@bsc.es</a>
</p>

<hr>
<h2 id='lonlat_prec_st'>Sample Of Experimental Precipitation Data In Function Of Longitudes And Latitudes with Start</h2><span id='topic+lonlat_prec_st'></span>

<h3>Description</h3>

<p>This sample data set contains a small cutout of gridded seasonal precipitation
forecast data from the Copernicus Climate Change ECMWF-System 5 forecast 
system, to be used to demonstrate downscaling. Specifically, for the 'pr' 
(precipitation) variable, for the first 6 forecast ensemble members, daily
values, for all 31 days in March following the forecast starting dates in 
November of years 2010 to 2012, for a small 4x4 pixel cutout in a region in
the North-Western Italian Alps (44N-47N, 6E-9E). The data resolution is 1 
degree.
</p>


<h3>Details</h3>

<p>The 'CST_Start' call used to generate the data set in the infrastructure of 
the Marconi machine at CINECA is shown next, working on files which were 
extracted from forecast data available in the MEDSCOPE internal archive.
</p>
<pre>
 path &lt;- paste0('/esarchive/exp/ecmwf/system5c3s/daily_mean/',
                '$var$_s0-24h/$var$_$sdate$.nc')
 sdates = c('20101101', '20111101', '20121101')
 latmin &lt;- 44
 latmax &lt;- 47
 lonmin &lt;- 6
 lonmax &lt;- 9

 lonlat_prec_st &lt;- CST_Start(dataset = path,
                             var = 'prlr',
                             member = startR::indices(1:6),
                             sdate = sdates,
                             ftime = 121:151,
                             lat = startR::values(list(latmin, latmax)),
                             lat_reorder = startR::Sort(decreasing = TRUE),
                             lon = startR::values(list(lonmin, lonmax)),
                             lon_reorder = startR::CircularSort(0, 360),
                             synonims = list(lon = c('lon', 'longitude'),
                                             lat = c('lat', 'latitude'),
                                             ftime = c('time', 'ftime'),
                                             member = c('member', 'ensemble')),
                              return_vars = list(ftime = 'sdate',
                                                 lon = NULL, lat = NULL),
                              retrieve = TRUE)
</pre>


<h3>Author(s)</h3>

<p>Jost von Hardenberg <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>
<p>An-Chi Ho <a href="mailto:an.ho@bsc.es">an.ho@bsc.es</a>
</p>

<hr>
<h2 id='lonlat_temp'>Sample Of Experimental And Observational Climate Data In Function Of Longitudes And Latitudes</h2><span id='topic+lonlat_temp'></span>

<h3>Description</h3>

<p>This sample data set contains gridded seasonal forecast and corresponding 
observational data from the Copernicus Climate Change ECMWF-System 5 forecast
system, and from the Copernicus Climate Change ERA-5 reconstruction. 
Specifically, for the 'tas' (2-meter temperature) variable, for the 15 first 
forecast ensemble members, monthly averaged, for the 3 first forecast time 
steps (lead months 1 to 4) of the November start dates of 2000 to 2005, for 
the Mediterranean region (27N-48N, 12W-40E). The data was generated on (or 
interpolated onto, for the reconstruction) a rectangular regular grid of size
360 by 181.
</p>


<h3>Details</h3>

<p>It is recommended to use the data set as follows:
</p>
<pre>
require(zeallot)
c(exp, obs) 
</pre>
<p>The 'CST_Load' call used to generate the data set in the infrastructure of 
the Earth Sciences Department of the Barcelona Supercomputing Center is shown
next. Note that 'CST_Load' internally calls 's2dv::Load', which would require 
a configuration file (not provided here) expressing the distribution of the 
'system5c3s' and 'era5' NetCDF files in the file system.
</p>
<pre>
library(CSTools)
require(zeallot)

startDates &lt;- c('20001101', '20011101', '20021101',
                '20031101', '20041101', '20051101')

lonlat_temp &lt;- 
  CST_Load(
    var = 'tas', 
    exp = 'system5c3s', 
    obs = 'era5', 
    nmember = 15,
    sdates = startDates,
    leadtimemax = 3,
    latmin = 27, latmax = 48,
    lonmin = -12, lonmax = 40, 
    output = 'lonlat',
    nprocs = 1
  )
</pre>


<h3>Author(s)</h3>

<p>Nicolau Manubens <a href="mailto:nicolau.manubens@bsc.es">nicolau.manubens@bsc.es</a>
</p>

<hr>
<h2 id='lonlat_temp_st'>Sample Of Experimental And Observational Climate Data In Function Of Longitudes And Latitudes with Start</h2><span id='topic+lonlat_temp_st'></span>

<h3>Description</h3>

<p>This sample data set contains gridded seasonal forecast and corresponding 
observational data from the Copernicus Climate Change ECMWF-System 5 forecast
system, and from the Copernicus Climate Change ERA-5 reconstruction. 
Specifically, for the 'tas' (2-meter temperature) variable, for the 15 first 
forecast ensemble members, monthly averaged, for the 3 first forecast time 
steps (lead months 1 to 4) of the November start dates of 2000 to 2005, for 
the Mediterranean region (27N-48N, 12W-40E). The data was generated on (or 
interpolated onto, for the reconstruction) a rectangular regular grid of size
360 by 181.
</p>


<h3>Details</h3>

<p>The 'CST_Start' call used to generate the data set in the infrastructure of 
the Earth Sciences Department of the Barcelona Supercomputing Center is shown
next. Note that 'CST_Start' internally calls 'startR::Start' and then uses 
'as.s2dv_cube' that converts the 'startR_array' into 's2dv_cube'.
</p>
<pre>
 lonlat_temp_st &lt;- NULL
 repos_exp &lt;- paste0('/esarchive/exp/ecmwf/system5c3s/monthly_mean/',
                     '$var$_f6h/$var$_$sdate$.nc')
 sdates &lt;- sapply(2000:2005, function(x) paste0(x, '1101'))
 lonmax &lt;- 40
 lonmin &lt;- -12
 latmax &lt;- 48
 latmin &lt;- 27
 lonlat_temp_st$exp &lt;- CST_Start(dataset = repos_exp,
                                 var = 'tas',
                                 member = startR::indices(1:15),
                                 sdate = sdates,
                                 ftime = startR::indices(1:3),
                                 lat = startR::values(list(latmin, latmax)),
                                 lat_reorder = startR::Sort(decreasing = TRUE), 
                                 lon = startR::values(list(lonmin, lonmax)),
                                 lon_reorder = startR::CircularSort(0, 360),
                                 synonims = list(lon = c('lon', 'longitude'),
                                                 lat = c('lat', 'latitude'),
                                                 member = c('member', 'ensemble'),
                                                 ftime = c('ftime', 'time')),
                                 return_vars = list(lat = NULL, 
                                                    lon = NULL, ftime = 'sdate'),
                                                    retrieve = TRUE)
 
 dates &lt;- c(paste0(2000, c(11, 12)), paste0(2001, c('01', 11, 12)),
            paste0(2002, c('01', 11, 12)),  paste0(2003, c('01', 11, 12)),
            paste0(2004, c('01', 11, 12)),  paste0(2005, c('01', 11, 12)), 200601)
 dates &lt;- sapply(dates, function(x) {paste0(x, '01')})
 dates &lt;- as.POSIXct(dates, format = '
 dim(dates) &lt;- c(ftime = 3, sdate = 6)

 dates &lt;- t(dates)
 names(dim(dates)) &lt;- c('sdate', 'ftime')

 path.obs &lt;- '/esarchive/recon/ecmwf/era5/monthly_mean/$var$_f1h-r1440x721cds/$var$_$date$.nc'
 lonlat_temp_st$obs &lt;- CST_Start(dataset = path.obs,
                                 var = 'tas',
                                 date = unique(format(dates, '
                                 ftime = startR::values(dates), 
                                 ftime_across = 'date',
                                 ftime_var = 'ftime',
                                 merge_across_dims = TRUE,
                                 split_multiselected_dims = TRUE,
                                 lat = startR::values(list(latmin, latmax)),
                                 lat_reorder = startR::Sort(decreasing = TRUE),
                                 lon = startR::values(list(lonmin, lonmax)),
                                 lon_reorder = startR::CircularSort(0, 360),
                                 synonims = list(lon = c('lon', 'longitude'),
                                                 lat = c('lat', 'latitude'),
                                                 ftime = c('ftime', 'time')),
                                 transform = startR::CDORemapper,
                                 transform_extra_cells = 2,
                                 transform_params = list(grid = 'r360x181',
                                                         method = 'conservative'),
                                 transform_vars = c('lat', 'lon'),
                                 return_vars = list(lon = NULL,
                                                    lat = NULL,
                                                    ftime = 'date'),
                                 retrieve = TRUE)

 library(lubridate)
 dates_exp &lt;- lonlat_temp_st$exp$attrs$Dates
 lonlat_temp_st$exp$attrs$Dates &lt;- floor_date(ymd_hms(dates_exp), unit = "months")
 dim(lonlat_temp_st$exp$attrs$Dates) &lt;- dim(dates_exp)
 
 dates_obs &lt;- lonlat_temp_st$obs$attrs$Dates
 lonlat_temp_st$obs$attrs$Dates &lt;- floor_date(ymd_hms(dates_obs), unit = "months")
 dim(lonlat_temp_st$obs$attrs$Dates) &lt;- dim(dates_obs)

</pre>


<h3>Author(s)</h3>

<p>Nicolau Manubens <a href="mailto:nicolau.manubens@bsc.es">nicolau.manubens@bsc.es</a>
</p>

<hr>
<h2 id='MergeDims'>Function to Split Dimension</h2><span id='topic+MergeDims'></span>

<h3>Description</h3>

<p>This function merges two dimensions of an array into one. The 
user can select the dimensions to merge and provide the final name of the 
dimension. The user can select to remove NA values or keep them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MergeDims(
  data,
  merge_dims = c("time", "monthly"),
  rename_dim = NULL,
  na.rm = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MergeDims_+3A_data">data</code></td>
<td>
<p>An n-dimensional array with named dimensions</p>
</td></tr>
<tr><td><code id="MergeDims_+3A_merge_dims">merge_dims</code></td>
<td>
<p>A character vector indicating the names of the dimensions to 
merge.</p>
</td></tr>
<tr><td><code id="MergeDims_+3A_rename_dim">rename_dim</code></td>
<td>
<p>A character string indicating the name of the output 
dimension. If left at NULL, the first dimension name provided in parameter 
<code>merge_dims</code> will be used.</p>
</td></tr>
<tr><td><code id="MergeDims_+3A_na.rm">na.rm</code></td>
<td>
<p>A logical indicating if the NA values should be removed or not.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nuria Perez-Zanon, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- 1 : 20
dim(data) &lt;- c(time = 10, lat = 2)
new_data &lt;- MergeDims(data, merge_dims = c('time', 'lat'))
</code></pre>

<hr>
<h2 id='MultiEOF'>EOF analysis of multiple variables starting from an array (reduced 
version)</h2><span id='topic+MultiEOF'></span>

<h3>Description</h3>

<p>This function performs EOF analysis over multiple variables, 
accepting in input an array with a dimension <code>"var"</code> for each variable to 
analyse. Based on Singular Value Decomposition. For each field the EOFs are 
computed and the corresponding PCs are standardized (unit variance, zero mean); 
the minimum number of principal components needed to reach the user-defined 
variance is retained. The function weights the input data for the latitude 
cosine square root.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultiEOF(
  data,
  lon,
  lat,
  dates,
  time = NULL,
  lon_dim = "lon",
  lat_dim = "lat",
  time_dim = "ftime",
  sdate_dim = "sdate",
  var_dim = "var",
  neof_max = 40,
  neof_composed = 5,
  minvar = 0.6,
  lon_lim = NULL,
  lat_lim = NULL,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultiEOF_+3A_data">data</code></td>
<td>
<p>A multidimensional array with dimension <code>"var"</code>, containing 
the variables to be analysed. The other diemnsions follow the same structure
as the <code>"exp"</code> element of a 's2dv_cube' object. Latitudinal 
dimension accepted names: 'lat', 'lats', 'latitude', 'y', 'j', 'nav_lat'. 
Longitudinal dimension accepted names: 'lon', 'lons','longitude', 'x', 'i', 
'nav_lon'. NAs can exist but it should be consistent along 'time_dim'. That 
is, if one grid point has NAs for each variable, all the time steps at this 
point should be NAs.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_lon">lon</code></td>
<td>
<p>Vector of longitudes.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_lat">lat</code></td>
<td>
<p>Vector of latitudes.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_dates">dates</code></td>
<td>
<p>Vector or matrix of dates in POSIXct format.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_time">time</code></td>
<td>
<p>Deprecated parameter, it has been substituted by 'dates'. It will 
be removed in the next release.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_lon_dim">lon_dim</code></td>
<td>
<p>A character string indicating the name of the longitudinal 
dimension. By default, it is set to 'lon'.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_lat_dim">lat_dim</code></td>
<td>
<p>A character string indicating the name of the latitudinal 
dimension. By default, it is set to 'lat'.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_time_dim">time_dim</code></td>
<td>
<p>A character string indicating the name of the temporal 
dimension. By default, it is set to 'time'.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. By default, it is set to 'sdate'.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_var_dim">var_dim</code></td>
<td>
<p>A character string indicating the name of the variable 
dimension. By default, it is set to 'var'.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_neof_max">neof_max</code></td>
<td>
<p>Maximum number of single eofs considered in the first 
decomposition.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_neof_composed">neof_composed</code></td>
<td>
<p>Number of composed eofs to return in output.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_minvar">minvar</code></td>
<td>
<p>Minimum variance fraction to be explained in first decomposition.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_lon_lim">lon_lim</code></td>
<td>
<p>Vector with longitudinal range limits for the calculation for 
all input variables.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_lat_lim">lat_lim</code></td>
<td>
<p>Vector with latitudinal range limits for the calculation for 
all input variables.</p>
</td></tr>
<tr><td><code id="MultiEOF_+3A_ncores">ncores</code></td>
<td>
<p>An integer indicating the number of cores to use for parallel 
computation. The default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>coeff</code></td>
<td>

<p>An array of principal components with dimensions 'time_dim', 'sdate_dim', 
number of eof, rest of the dimensions of 'data' except 'lon_dim' and 
'lat_dim'. 
</p>
</td></tr>
<tr><td><code>variance</code></td>
<td>

<p>An array of explained variances with dimensions 'eof' and the rest of the 
dimensions of 'data' except 'time_dim', 'sdate_dim', 'lon_dim' and 
'lat_dim'.
</p>
</td></tr>
<tr><td><code>eof_pattern</code></td>
<td>

<p>An array of EOF patterns obtained by regression with dimensions: 'eof' and 
the rest of the dimensions of 'data' except 'time_dim' and 'sdate_dim'.
</p>
</td></tr>
<tr><td><code>mask</code></td>
<td>

<p>An array of the mask with dimensions ('lon_dim', 'lat_dim', rest of the 
dimensions of 'data' except 'time_dim'). It is made from 'data', 1 for the 
positions that 'data' has value and NA for the positions that 'data' has NA. 
It is used to replace NAs with 0s for EOF calculation and mask the result 
with NAs again after the calculation.
</p>
</td></tr>
<tr><td><code>coordinates</code></td>
<td>

<p>Longitudinal and latitudinal coordinates vectors.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>
<p>Paolo Davini - ISAC-CNR, <a href="mailto:p.davini@isac.cnr.it">p.davini@isac.cnr.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>exp &lt;- array(runif(1280)*280, dim = c(dataset = 2, member = 2, sdate = 3, 
                                     ftime = 3, lat = 4, lon = 4, var = 1))
lon &lt;- seq(0, 3)
lat &lt;- seq(47, 44)
dates &lt;- c("2000-11-01", "2000-12-01", "2001-01-01", "2001-11-01", 
          "2001-12-01", "2002-01-01", "2002-11-01", "2002-12-01", "2003-01-01")
Dates &lt;- as.POSIXct(dates, format = "%Y-%m-%d")
dim(Dates) &lt;- c(ftime = 3, sdate = 3)
cal &lt;- MultiEOF(data = exp, lon = lon, lat = lat, dates = Dates)
</code></pre>

<hr>
<h2 id='MultiMetric'>Multiple Metrics applied in Multiple Model Anomalies</h2><span id='topic+MultiMetric'></span>

<h3>Description</h3>

<p>This function calculates correlation (Anomaly Correlation 
Coefficient; ACC), root mean square error (RMS) and the root mean square error 
skill score (RMSSS) of individual anomaly models and multi-models mean (if 
desired) with the observations on arrays with named dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultiMetric(
  exp,
  obs,
  metric = "correlation",
  multimodel = TRUE,
  time_dim = "ftime",
  memb_dim = "member",
  sdate_dim = "sdate"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultiMetric_+3A_exp">exp</code></td>
<td>
<p>A multidimensional array with named dimensions.</p>
</td></tr>
<tr><td><code id="MultiMetric_+3A_obs">obs</code></td>
<td>
<p>A multidimensional array with named dimensions.</p>
</td></tr>
<tr><td><code id="MultiMetric_+3A_metric">metric</code></td>
<td>
<p>A character string giving the metric for computing the maximum 
skill. This must be one of the strings 'correlation', 'rms' or 'rmsss.</p>
</td></tr>
<tr><td><code id="MultiMetric_+3A_multimodel">multimodel</code></td>
<td>
<p>A logical value indicating whether a Multi-Model Mean should 
be computed.</p>
</td></tr>
<tr><td><code id="MultiMetric_+3A_time_dim">time_dim</code></td>
<td>
<p>Name of the temporal dimension where a mean will be applied. 
It can be NULL, the default value is 'ftime'.</p>
</td></tr>
<tr><td><code id="MultiMetric_+3A_memb_dim">memb_dim</code></td>
<td>
<p>Name of the member dimension. It can be NULL, the default 
value is 'member'.</p>
</td></tr>
<tr><td><code id="MultiMetric_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>Name of the start date dimension or a dimension name 
identifiying the different forecast. It can be NULL, the default value is 
'sdate'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of arrays containing the statistics of the selected metric in 
the element <code>$data</code> which is a list of arrays: for the metric requested 
and others for statistics about its signeificance. The arrays have two dataset 
dimensions equal to the 'dataset' dimension in the <code>exp$data</code> and 
<code>obs$data</code> inputs. If <code>multimodel</code> is TRUE, the greatest position in 
the first dimension correspons to the Multi-Model Mean.
</p>


<h3>Author(s)</h3>

<p>Mishra Niti, <a href="mailto:niti.mishra@bsc.es">niti.mishra@bsc.es</a>
</p>
<p>Perez-Zanon Nuria, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>References</h3>

<p>Mishra, N., Prodhomme, C., &amp; Guemas, V. (n.d.). Multi-Model Skill 
Assessment of Seasonal Temperature and Precipitation Forecasts over Europe, 
29-31. doi: <a href="https://doi.org/10.1007/s00382-018-4404-z">10.1007/s00382-018-4404-z</a>
</p>


<h3>See Also</h3>

<p><code><a href="s2dv.html#topic+Corr">Corr</a></code>, <code><a href="s2dv.html#topic+RMS">RMS</a></code>, 
<code><a href="s2dv.html#topic+RMSSS">RMSSS</a></code> and <code><a href="#topic+CST_Load">CST_Load</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>exp &lt;- array(rnorm(2*2*4*5*2*2), 
            dim = c(dataset = 2, member = 2, sdate = 4, ftime = 5, lat = 2, 
                    lon = 2))
obs &lt;- array(rnorm(1*1*4*5*2*2),
            dim = c(dataset = 1, member = 1, sdate = 4, ftime = 5, lat = 2, 
                    lon = 2))
res &lt;- MultiMetric(exp = exp, obs = obs)
</code></pre>

<hr>
<h2 id='PDFIndexHind'>Computing the Index PDFs for a dataset of SFSs for a hindcats period.</h2><span id='topic+PDFIndexHind'></span>

<h3>Description</h3>

<p>This function implements the computation to obtain the index PDFs
(e.g. NAO index) to improve the index estimate from SFSs for a hindcast period.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PDFIndexHind(
  index_hind,
  index_obs,
  method = "ME",
  time_dim_name = "time",
  na.rm = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PDFIndexHind_+3A_index_hind">index_hind</code></td>
<td>
<p>Index (e.g. NAO index) array from SFSs
with at least two dimensions (time , member) or (time, statistic). 
The temporal dimension, by default 'time', must be greater than 2. 
The dimension 'member' must be greater than 1. 
The dimension 'statistic' must be equal to 2, for containing the two 
paramenters of a normal distribution (mean and sd) representing the ensemble 
of a SFS. It is not possible to have the dimension 'member' and  'statistic' 
together.</p>
</td></tr>
<tr><td><code id="PDFIndexHind_+3A_index_obs">index_obs</code></td>
<td>
<p>Index (e.g. NAO index) array from an observational database
or reanalysis with at least a temporal dimension (by default 'time'), 
which must be greater than 2.</p>
</td></tr>
<tr><td><code id="PDFIndexHind_+3A_method">method</code></td>
<td>
<p>A character string indicating which methodology is applied
to compute the PDFs. One of &quot;ME&quot; (default) or &quot;LMEV&quot;.</p>
</td></tr>
<tr><td><code id="PDFIndexHind_+3A_time_dim_name">time_dim_name</code></td>
<td>
<p>A character string indicating the name of the temporal 
dimension, by default 'time'.</p>
</td></tr>
<tr><td><code id="PDFIndexHind_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical (default = FALSE). Should missing values be removed?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An array with at least two dimensions (time, statistic = 4). The firt 
statistic is the parameter 'mean' of the PDF with not bias corrected.
The second statistic is the parameter 'standard deviation' of the PDF with not 
bias corrected. The third statistic is the parameter 'mean' of the PDF with 
bias corrected. The fourth statistic is the parameter 'standard deviation' of 
the PDF with bias corrected.
</p>


<h3>Author(s)</h3>

<p>Eroteida Sanchez-Garcia - AEMET, <a href="mailto:esanchezg@aemet.es">esanchezg@aemet.es</a>
</p>


<h3>References</h3>

<p>Regionally improved seasonal forecast of precipitation through Best
estimation of winter NAO, Sanchez-Garcia, E. et al.,
Adv. Sci. Res., 16, 165174, 2019, doi: <a href="https://doi.org/10.5194/asr-16-165-2019">10.5194/asr-16-165-2019</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example for the PDFIndexHind function
# Example 1 
index_obs &lt;- 1 : (5 * 3 ) 
dim(index_obs) &lt;- c(time = 5, season = 3)
index_hind &lt;- 1 : (5 * 4 * 3)
dim(index_hind) &lt;- c(time = 5, member = 4, season = 3)
res &lt;- PDFIndexHind(index_hind, index_obs)
dim(res)
# time statistic  season
#   5         4        3
# Example 2
index_obs &lt;- 1 : (5 * 3) 
dim(index_obs) &lt;- c(time = 5, season = 3)
index_hind &lt;- 1 : (5 * 2 * 3)
dim(index_hind) &lt;- c(time = 5, statistic = 2, season = 3)
res &lt;- PDFIndexHind(index_hind, index_obs)
</code></pre>

<hr>
<h2 id='PlotCombinedMap'>Plot Multiple Lon-Lat Variables In a Single Map According to a Decision Function</h2><span id='topic+PlotCombinedMap'></span>

<h3>Description</h3>

<p>Plot a number a two dimensional matrices with (longitude, 
latitude) dimensions on a single map with the cylindrical equidistant 
latitude and longitude projection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotCombinedMap(
  maps,
  lon,
  lat,
  map_select_fun,
  display_range,
  map_dim = "map",
  brks = NULL,
  cols = NULL,
  bar_limits = NULL,
  triangle_ends = c(F, F),
  col_inf = NULL,
  col_sup = NULL,
  col_unknown_map = "white",
  mask = NULL,
  col_mask = "grey",
  dots = NULL,
  bar_titles = NULL,
  legend_scale = 1,
  cex_bar_titles = 1.5,
  plot_margin = NULL,
  bar_extra_margin = c(2, 0, 2, 0),
  fileout = NULL,
  width = 8,
  height = 5,
  size_units = "in",
  res = 100,
  drawleg = T,
  return_leg = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotCombinedMap_+3A_maps">maps</code></td>
<td>
<p>List of matrices to plot, each with (longitude, latitude) 
dimensions, or 3-dimensional array with the dimensions (longitude, latitude, 
map). Dimension names are required.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_lon">lon</code></td>
<td>
<p>Vector of longitudes. Must match the length of the corresponding 
dimension in 'maps'.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_lat">lat</code></td>
<td>
<p>Vector of latitudes. Must match the length of the corresponding 
dimension in 'maps'.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_map_select_fun">map_select_fun</code></td>
<td>
<p>Function that selects, for each grid point, which value 
to take among all the provided maps. This function receives as input a 
vector of values for a same grid point for all the provided maps, and must 
return a single selected value (not its index!) or NA. For example, the 
<code>min</code> and <code>max</code> functions are accepted.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_display_range">display_range</code></td>
<td>
<p>Range of values to be displayed for all the maps. This 
must be a numeric vector c(range min, range max). The values in the 
parameter 'maps' can go beyond the limits specified in this range. If the 
selected value for a given grid point (according to 'map_select_fun') falls 
outside the range, it will be coloured with 'col_unknown_map'.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_map_dim">map_dim</code></td>
<td>
<p>Optional name for the dimension of 'maps' along which the 
multiple maps are arranged. Only applies when 'maps' is provided as a 
3-dimensional array. Takes the value 'map' by default.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_brks">brks</code></td>
<td>
<p>Colour levels to be sent to PlotEquiMap. This parameter is 
optional and adjusted automatically by the function.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_cols">cols</code></td>
<td>
<p>List of vectors of colours to be sent to PlotEquiMap for the 
colour bar of each map. This parameter is optional and adjusted 
automatically by the function (up to 5 maps). The colours provided for each 
colour bar will be automatically interpolated to match the number of breaks. 
Each item in this list can be named, and the name will be used as title for 
the corresponding colour bar (equivalent to the parameter 'bar_titles').</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_bar_limits">bar_limits</code></td>
<td>
<p>Parameter from s2dv::ColorBar. Vector of two numeric values 
with the extremes of the range of values represented in the colour bar. If 
'var_limits' go beyond this interval, the drawing of triangle extremes is 
triggered at the corresponding sides, painted in 'col_inf' and 'col_sup'. 
Either of them can be set as NA and will then take as value the 
corresponding extreme in 'var_limits' (hence a triangle end won't be 
triggered for these sides). Takes as default the extremes of 'brks' if 
available, else the same values as 'var_limits'.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_triangle_ends">triangle_ends</code></td>
<td>
<p>Parameter from s2dv::ColorBar. Vector of two logical 
elements, indicating whether to force the drawing of triangle ends at each 
of the extremes of the colour bar. This choice is automatically made from 
the provided 'brks', 'bar_limits', 'var_limits', 'col_inf' and 'col_sup', 
but the behaviour can be manually forced to draw or not to draw the triangle 
ends with this parameter. If 'cols' is provided, 'col_inf' and 'col_sup' 
will take priority over 'triangle_ends' when deciding whether to draw the 
triangle ends or not.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_col_inf">col_inf</code></td>
<td>
<p>Parameter from s2dv::ColorBar. Colour to fill the inferior 
triangle end with. Useful if specifying colours manually with parameter 
'cols', to specify the colour and to trigger the drawing of the lower 
extreme triangle, or if 'cols' is not specified, to replace the colour 
automatically generated by ColorBar().</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_col_sup">col_sup</code></td>
<td>
<p>Parameter from s2dv::ColorBar. Colour to fill the superior 
triangle end with. Useful if specifying colours manually with parameter 
'cols', to specify the colour and to trigger the drawing of the upper 
extreme triangle, or if 'cols' is not specified, to replace the colour 
automatically generated by ColorBar().</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_col_unknown_map">col_unknown_map</code></td>
<td>
<p>Colour to use to paint the grid cells for which a map 
is not possible to be chosen according to 'map_select_fun' or for those 
values that go beyond 'display_range'. Takes the value 'white' by default.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_mask">mask</code></td>
<td>
<p>Optional numeric array with dimensions (latitude, longitude), with 
values in the range [0, 1], indicating the opacity of the mask over each 
grid point. Cells with a 0 will result in no mask, whereas cells with a 1 
will result in a totally opaque superimposed pixel coloured in 'col_mask'.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_col_mask">col_mask</code></td>
<td>
<p>Colour to be used for the superimposed mask (if specified in 
'mask'). Takes the value 'grey' by default.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_dots">dots</code></td>
<td>
<p>Array of same dimensions as 'var' or with dimensions 
c(n, dim(var)), where n is the number of dot/symbol layers to add to the 
plot. A value of TRUE at a grid cell will draw a dot/symbol on the 
corresponding square of the plot. By default all layers provided in 'dots' 
are plotted with dots, but a symbol can be specified for each of the 
layers via the parameter 'dot_symbol'.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_bar_titles">bar_titles</code></td>
<td>
<p>Optional vector of character strings providing the titles to 
be shown on top of each of the colour bars.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_legend_scale">legend_scale</code></td>
<td>
<p>Scale factor for the size of the colour bar labels. Takes 
1 by default.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_cex_bar_titles">cex_bar_titles</code></td>
<td>
<p>Scale factor for the sizes of the bar titles. Takes 1.5 
by default.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_plot_margin">plot_margin</code></td>
<td>
<p>Numeric vector of length 4 for the margin sizes in the 
following order: bottom, left, top, and right. If not specified, use the 
default of par(&quot;mar&quot;), c(5.1, 4.1, 4.1, 2.1). Used as 'margin_scale' in 
s2dv::PlotEquiMap.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_bar_extra_margin">bar_extra_margin</code></td>
<td>
<p>Parameter from s2dv::ColorBar. Extra margins to be 
added around the colour bar, in the format c(y1, x1, y2, x2). The units are 
margin lines. Takes rep(0, 4) by default.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_fileout">fileout</code></td>
<td>
<p>File where to save the plot. If not specified (default) a 
graphics device will pop up. Extensions allowed: eps/ps, jpeg, png, pdf, bmp 
and tiff</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_width">width</code></td>
<td>
<p>File width, in the units specified in the parameter size_units 
(inches by default). Takes 8 by default.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_height">height</code></td>
<td>
<p>File height, in the units specified in the parameter size_units 
(inches by default). Takes 5 by default.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot in. 
Inches ('in') by default. See ?Devices and the creator function of the 
corresponding device.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See ?Devices 
and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_drawleg">drawleg</code></td>
<td>
<p>Where to draw the common colour bar. Can take values TRUE, 
FALSE or:<br />
'up', 'u', 'U', 'top', 't', 'T', 'north', 'n', 'N'<br />
'down', 'd', 'D', 'bottom', 'b', 'B', 'south', 's', 'S' (default)<br />
'right', 'r', 'R', 'east', 'e', 'E'<br />
'left', 'l', 'L', 'west', 'w', 'W'</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_return_leg">return_leg</code></td>
<td>
<p>A logical value indicating if the color bars information 
should be returned by the function. If TRUE, the function doesn't plot the
color bars but still creates the layout with color bar areas, and the 
arguments for GradientCatsColorBar() or ColorBar() will be returned. It is
convenient for users to adjust the color bars manually. The default is 
FALSE, the color bars will be plotted directly.</p>
</td></tr>
<tr><td><code id="PlotCombinedMap_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed on to <code>PlotEquiMap</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicolau Manubens, <a href="mailto:nicolau.manubens@bsc.es">nicolau.manubens@bsc.es</a>
</p>
<p>Veronica Torralba, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>
</p>


<h3>See Also</h3>

<p><code>PlotCombinedMap</code> and <code>PlotEquiMap</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simple example
x &lt;- array(1:(20 * 10), dim = c(lat = 10, lon = 20)) / 200
a &lt;- x * 0.6
b &lt;- (1 - x) * 0.6
c &lt;- 1 - (a + b)
lons &lt;- seq(0, 359.5, length = 20)
lats &lt;- seq(-89.5, 89.5, length = 10)
## Not run: 
PlotCombinedMap(list(a, b, c), lons, lats, 
               toptitle = 'Maximum map',
               map_select_fun = max,
               display_range = c(0, 1),
               bar_titles = paste('% of belonging to', c('a', 'b', 'c')), 
               brks = 20, width = 12, height = 10)

## End(Not run)

Lon &lt;- c(0:40, 350:359)
Lat &lt;- 51:26
data &lt;- rnorm(51 * 26 * 3)
dim(data) &lt;- c(map = 3, lon = 51, lat = 26)
mask &lt;-  sample(c(0,1), replace = TRUE, size = 51 * 26)
dim(mask) &lt;- c(lat = 26, lon = 51)
## Not run: 
PlotCombinedMap(data, lon = Lon, lat = Lat, map_select_fun = max,
               display_range = range(data), mask = mask,
               width = 14, height = 10) 

## End(Not run)

</code></pre>

<hr>
<h2 id='PlotForecastPDF'>Plot one or multiple ensemble forecast pdfs for the same event</h2><span id='topic+PlotForecastPDF'></span>

<h3>Description</h3>

<p>This function plots the probability distribution function of 
several ensemble forecasts. Separate panels are used to plot forecasts valid 
or initialized at different times or by different models or even at different 
locations. Probabilities for tercile categories are computed, plotted in 
colors and annotated. An asterisk marks the tercile with higher probabilities. 
Probabilities for extreme categories (above P90 and below P10) can also be 
included as hatched areas. Individual ensemble members can be plotted as 
jittered points. The observed value is optionally shown as a diamond.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotForecastPDF(
  fcst,
  tercile.limits,
  extreme.limits = NULL,
  obs = NULL,
  plotfile = NULL,
  title = "Set a title",
  var.name = "Varname (units)",
  fcst.names = NULL,
  add.ensmemb = c("above", "below", "no"),
  color.set = c("ggplot", "s2s4e", "hydro", "vitigeoss"),
  memb_dim = "member"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotForecastPDF_+3A_fcst">fcst</code></td>
<td>
<p>A dataframe or array containing all the ensember members for each 
forecast. If <code>'fcst'</code> is an array, it should have two labelled 
dimensions, and one of them should be <code>'members'</code>. If <code>'fcsts'</code> is 
a data.frame, each column shoul be a separate forecast, with the rows beeing 
the different ensemble members.</p>
</td></tr>
<tr><td><code id="PlotForecastPDF_+3A_tercile.limits">tercile.limits</code></td>
<td>
<p>An array or vector with P33 and P66 values that define 
the tercile categories for each panel. Use an array of dimensions 
(nforecasts,2) to  define different terciles for each forecast panel, or a 
vector with two elements to reuse the same tercile limits for all forecast 
panels.</p>
</td></tr>
<tr><td><code id="PlotForecastPDF_+3A_extreme.limits">extreme.limits</code></td>
<td>
<p>(optional) An array or vector with P10 and P90 values 
that define the extreme categories for each panel. Use an array of 
(nforecasts,2) to define different extreme limits for each forecast panel, 
or a vector with two elements to reuse the same tercile limits for all 
forecast panels. (Default: extreme categories are not shown).</p>
</td></tr>
<tr><td><code id="PlotForecastPDF_+3A_obs">obs</code></td>
<td>
<p>(optional) A vector providing the observed values for each forecast 
panel or a single value that will be reused for all forecast panels. 
(Default: observation is not shown).</p>
</td></tr>
<tr><td><code id="PlotForecastPDF_+3A_plotfile">plotfile</code></td>
<td>
<p>(optional) A filename (pdf, png...) where the plot will be 
saved. (Default: the plot is not saved).</p>
</td></tr>
<tr><td><code id="PlotForecastPDF_+3A_title">title</code></td>
<td>
<p>A string with the plot title.</p>
</td></tr>
<tr><td><code id="PlotForecastPDF_+3A_var.name">var.name</code></td>
<td>
<p>A string with the variable name and units.</p>
</td></tr>
<tr><td><code id="PlotForecastPDF_+3A_fcst.names">fcst.names</code></td>
<td>
<p>(optional) An array of strings with the titles of each 
individual forecast.</p>
</td></tr>
<tr><td><code id="PlotForecastPDF_+3A_add.ensmemb">add.ensmemb</code></td>
<td>
<p>Either to add the ensemble members <code>'above'</code> (default) 
or <code>'below'</code> the pdf, or not (<code>'no'</code>).</p>
</td></tr>
<tr><td><code id="PlotForecastPDF_+3A_color.set">color.set</code></td>
<td>
<p>A selection of predefined color sets: use <code>'ggplot'</code> 
(default) for blue/green/red, <code>'s2s4e'</code> for blue/grey/orange, 
<code>'hydro'</code> for yellow/gray/blue (suitable for precipitation and 
inflows) or the <code>"vitigeoss"</code> color set.</p>
</td></tr>
<tr><td><code id="PlotForecastPDF_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the member 
dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object containing the plot.
</p>


<h3>Author(s)</h3>

<p>Llorenç Lledó <a href="mailto:llledo@bsc.es">llledo@bsc.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fcsts &lt;- data.frame(fcst1 = rnorm(10), fcst2 = rnorm(10, 0.5, 1.2))
PlotForecastPDF(fcsts,c(-1,1))
</code></pre>

<hr>
<h2 id='PlotMostLikelyQuantileMap'>Plot Maps of Most Likely Quantiles</h2><span id='topic+PlotMostLikelyQuantileMap'></span>

<h3>Description</h3>

<p>This function receives as main input (via the parameter 
<code>probs</code>) a collection of longitude-latitude maps, each containing the 
probabilities (from 0 to 1) of the different grid cells of belonging to a 
category. As many categories as maps provided as inputs are understood to 
exist. The maps of probabilities must be provided on a common rectangular 
regular grid, and a vector with the longitudes and a vector with the latitudes 
of the grid must be provided. The input maps can be provided in two forms, 
either as a list of multiple two-dimensional arrays (one for each category) or 
as a three-dimensional array, where one of the dimensions corresponds to the 
different categories.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotMostLikelyQuantileMap(
  probs,
  lon,
  lat,
  cat_dim = "bin",
  bar_titles = NULL,
  col_unknown_cat = "white",
  drawleg = T,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotMostLikelyQuantileMap_+3A_probs">probs</code></td>
<td>
<p>A list of bi-dimensional arrays with the named dimensions 
'latitude' (or 'lat') and 'longitude' (or 'lon'), with equal size and in the 
same order, or a single tri-dimensional array with an additional dimension 
(e.g. 'bin') for the different categories. The arrays must contain 
probability values between 0 and 1, and the probabilities for all categories 
of a grid cell should not exceed 1 when added.</p>
</td></tr>
<tr><td><code id="PlotMostLikelyQuantileMap_+3A_lon">lon</code></td>
<td>
<p>A numeric vector with the longitudes of the map grid, in the same 
order as the values along the corresponding dimension in <code>probs</code>.</p>
</td></tr>
<tr><td><code id="PlotMostLikelyQuantileMap_+3A_lat">lat</code></td>
<td>
<p>A numeric vector with the latitudes of the map grid, in the same 
order as the values along the corresponding dimension in <code>probs</code>.</p>
</td></tr>
<tr><td><code id="PlotMostLikelyQuantileMap_+3A_cat_dim">cat_dim</code></td>
<td>
<p>The name of the dimension along which the different categories 
are stored in <code>probs</code>. This only applies if <code>probs</code> is provided in 
the form of 3-dimensional array. The default expected name is 'bin'.</p>
</td></tr>
<tr><td><code id="PlotMostLikelyQuantileMap_+3A_bar_titles">bar_titles</code></td>
<td>
<p>Vector of character strings with the names to be drawn on 
top of the color bar for each of the categories. As many titles as 
categories provided in <code>probs</code> must be provided.</p>
</td></tr>
<tr><td><code id="PlotMostLikelyQuantileMap_+3A_col_unknown_cat">col_unknown_cat</code></td>
<td>
<p>Character string with a colour representation of the 
colour to be used to paint the cells for which no category can be clearly 
assigned. Takes the value 'white' by default.</p>
</td></tr>
<tr><td><code id="PlotMostLikelyQuantileMap_+3A_drawleg">drawleg</code></td>
<td>
<p>Where to draw the common colour bar. Can take values TRUE, 
FALSE or:<br />
'up', 'u', 'U', 'top', 't', 'T', 'north', 'n', 'N'<br />
'down', 'd', 'D', 'bottom', 'b', 'B', 'south', 's', 'S' (default)<br />
'right', 'r', 'R', 'east', 'e', 'E'<br />
'left', 'l', 'L', 'west', 'w', 'W'</p>
</td></tr>
<tr><td><code id="PlotMostLikelyQuantileMap_+3A_...">...</code></td>
<td>
<p>Additional parameters to be sent to <code>PlotCombinedMap</code> and 
<code>PlotEquiMap</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Veronica Torralba, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>, Nicolau Manubens, 
<a href="mailto:nicolau.manubens@bsc.es">nicolau.manubens@bsc.es</a>
</p>


<h3>See Also</h3>

<p><code>PlotCombinedMap</code> and <code>PlotEquiMap</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simple example
x &lt;- array(1:(20 * 10), dim = c(lat = 10, lon = 20)) / 200
a &lt;- x * 0.6
b &lt;- (1 - x) * 0.6
c &lt;- 1 - (a + b)
lons &lt;- seq(0, 359.5, length = 20)
lats &lt;- seq(-89.5, 89.5, length = 10)
## Not run: 
PlotMostLikelyQuantileMap(list(a, b, c), lons, lats, 
                         toptitle = 'Most likely tercile map',
                         bar_titles = paste('% of belonging to', c('a', 'b', 'c')), 
                         brks = 20, width = 10, height = 8)

## End(Not run)

# More complex example
n_lons &lt;- 40
n_lats &lt;- 20
n_timesteps &lt;- 100
n_bins &lt;- 4

# 1. Generation of sample data
lons &lt;- seq(0, 359.5, length = n_lons)
lats &lt;- seq(-89.5, 89.5, length = n_lats)

# This function builds a 3-D gaussian at a specified point in the map.
make_gaussian &lt;- function(lon, sd_lon, lat, sd_lat) {
 w &lt;- outer(lons, lats, function(x, y) dnorm(x, lon, sd_lon) * dnorm(y, lat, sd_lat))
 min_w &lt;- min(w)
 w &lt;- w - min_w
 w &lt;- w / max(w)
 w &lt;- t(w)
 names(dim(w)) &lt;- c('lat', 'lon')
 w
}

# This function generates random time series (with values ranging 1 to 5) 
# according to 2 input weights.
gen_data &lt;- function(w1, w2, n) {
 r &lt;- sample(1:5, n, 
             prob = c(.05, .9 * w1, .05, .05, .9 * w2), 
             replace = TRUE)
 r &lt;- r + runif(n, -0.5, 0.5)
 dim(r) &lt;- c(time = n)
 r
}

# We build two 3-D gaussians.
w1 &lt;- make_gaussian(120, 80, 20, 30)
w2 &lt;- make_gaussian(260, 60, -10, 40)

# We generate sample data (with dimensions time, lat, lon) according 
# to the generated gaussians
sample_data &lt;- multiApply::Apply(list(w1, w2), NULL, 
                                gen_data, n = n_timesteps)$output1

# 2. Binning sample data
prob_thresholds &lt;- 1:n_bins / n_bins
prob_thresholds &lt;- prob_thresholds[1:(n_bins - 1)]
thresholds &lt;- quantile(sample_data, prob_thresholds)

binning &lt;- function(x, thresholds) {
 n_samples &lt;- length(x)
 n_bins &lt;- length(thresholds) + 1

 thresholds &lt;- c(thresholds, max(x))
 result &lt;- 1:n_bins
 lower_threshold &lt;- min(x) - 1
 for (i in 1:n_bins) {
   result[i] &lt;- sum(x &gt; lower_threshold &amp; x &lt;= thresholds[i]) / n_samples
   lower_threshold &lt;- thresholds[i]
 }

 dim(result) &lt;- c(bin = n_bins)
 result
}

bins &lt;- multiApply::Apply(sample_data, 'time', binning, thresholds)$output1

# 3. Plotting most likely quantile/bin
## Not run: 
PlotMostLikelyQuantileMap(bins, lons, lats, 
                         toptitle = 'Most likely quantile map',
                         bar_titles = paste('% of belonging to', letters[1:n_bins]),
                         mask = 1 - (w1 + w2 / max(c(w1, w2))), 
                         brks = 20, width = 10, height = 8)

## End(Not run)
</code></pre>

<hr>
<h2 id='PlotPDFsOLE'>Plotting two probability density gaussian functions and the optimal linear 
estimation (OLE) as result of combining them.</h2><span id='topic+PlotPDFsOLE'></span>

<h3>Description</h3>

<p>This function plots two probability density gaussian functions 
and the optimal linear estimation (OLE) as result of combining them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotPDFsOLE(
  pdf_1,
  pdf_2,
  nsigma = 3,
  legendPos = "bottom",
  legendSize = 1,
  plotfile = NULL,
  width = 30,
  height = 15,
  units = "cm",
  dpi = 300
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotPDFsOLE_+3A_pdf_1">pdf_1</code></td>
<td>
<p>A numeric array with a dimension named 'statistic', containg 
two parameters: mean' and 'standard deviation' of the first gaussian pdf 
to combining.</p>
</td></tr>
<tr><td><code id="PlotPDFsOLE_+3A_pdf_2">pdf_2</code></td>
<td>
<p>A numeric array with a dimension named 'statistic', containg 
two parameters: mean' and 'standard deviation' of the second gaussian pdf 
to combining.</p>
</td></tr>
<tr><td><code id="PlotPDFsOLE_+3A_nsigma">nsigma</code></td>
<td>
<p>(optional) A numeric value for setting the limits of X axis. 
(Default nsigma = 3).</p>
</td></tr>
<tr><td><code id="PlotPDFsOLE_+3A_legendpos">legendPos</code></td>
<td>
<p>(optional) A character value for setting the position of the
legend (&quot;bottom&quot;, &quot;top&quot;, &quot;right&quot; or &quot;left&quot;)(Default 'bottom').</p>
</td></tr>
<tr><td><code id="PlotPDFsOLE_+3A_legendsize">legendSize</code></td>
<td>
<p>(optional) A numeric value for setting the size of the 
legend text. (Default 1.0).</p>
</td></tr>
<tr><td><code id="PlotPDFsOLE_+3A_plotfile">plotfile</code></td>
<td>
<p>(optional) A filename where the plot will be saved. 
(Default: the plot is not saved).</p>
</td></tr>
<tr><td><code id="PlotPDFsOLE_+3A_width">width</code></td>
<td>
<p>(optional) A numeric value indicating the plot width in 
units (&quot;in&quot;, &quot;cm&quot;, or &quot;mm&quot;). (Default width = 30).</p>
</td></tr>
<tr><td><code id="PlotPDFsOLE_+3A_height">height</code></td>
<td>
<p>(optional) A numeric value indicating the plot height. 
(Default height = 15).</p>
</td></tr>
<tr><td><code id="PlotPDFsOLE_+3A_units">units</code></td>
<td>
<p>(optional) A character value indicating the plot size 
unit. (Default units = 'cm').</p>
</td></tr>
<tr><td><code id="PlotPDFsOLE_+3A_dpi">dpi</code></td>
<td>
<p>(optional) A numeric value indicating the plot resolution.
(Default dpi = 300).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>PlotPDFsOLE() returns a ggplot object containing the plot.
</p>


<h3>Author(s)</h3>

<p>Eroteida Sanchez-Garcia - AEMET, <a href="mailto:esanchezg@aemet.es">esanchezg@aemet.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1
pdf_1 &lt;- c(1.1,0.6)
attr(pdf_1, "name") &lt;- "NAO1"
dim(pdf_1) &lt;-  c(statistic = 2)
pdf_2 &lt;- c(1,0.5)
attr(pdf_2, "name") &lt;- "NAO2"
dim(pdf_2) &lt;-  c(statistic = 2)

PlotPDFsOLE(pdf_1, pdf_2)
</code></pre>

<hr>
<h2 id='PlotTriangles4Categories'>Function to convert any 3-d numerical array to a grid of coloured triangles.</h2><span id='topic+PlotTriangles4Categories'></span>

<h3>Description</h3>

<p>This function converts a 3-d numerical data array into a coloured 
grid with triangles. It is useful for a slide or article to present tabular 
results as colors instead of numbers. This can be used to compare the outputs 
of two or four categories (e.g. modes  of variability, clusters, or forecast 
systems).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotTriangles4Categories(
  data,
  brks = NULL,
  cols = NULL,
  toptitle = NULL,
  sig_data = NULL,
  pch_sig = 18,
  col_sig = "black",
  cex_sig = 1,
  xlab = TRUE,
  ylab = TRUE,
  xlabels = NULL,
  xtitle = NULL,
  ylabels = NULL,
  ytitle = NULL,
  legend = TRUE,
  lab_legend = NULL,
  cex_leg = 1,
  col_leg = "black",
  cex_axis = 1.5,
  mar = c(5, 4, 0, 0),
  fileout = NULL,
  size_units = "px",
  res = 100,
  figure.width = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotTriangles4Categories_+3A_data">data</code></td>
<td>
<p>Array with three named dimensions: 'dimx', 'dimy', 'dimcat', 
containing the values to be displayed in a coloured image with triangles.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_brks">brks</code></td>
<td>
<p>A vector of the color bar intervals. The length must be one more 
than the parameter 'cols'. Use ColorBar() to generate default values.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_cols">cols</code></td>
<td>
<p>A vector of valid colour identifiers for color bar. The length
must be one less than the parameter 'brks'. Use ColorBar() to generate 
default values.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_toptitle">toptitle</code></td>
<td>
<p>A string of the title of the grid. Set NULL as default.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_sig_data">sig_data</code></td>
<td>
<p>Logical array with the same dimensions as 'data' to add layers 
to the plot. A value of TRUE at a grid cell will draw a dot/symbol on the 
corresponding triangle of the plot. Set NULL as default.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_pch_sig">pch_sig</code></td>
<td>
<p>Symbol to be used to represent sig_data. Takes 18 
(diamond) by default. See 'pch' in par() for additional accepted options.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_col_sig">col_sig</code></td>
<td>
<p>Colour of the symbol to represent sig_data.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_cex_sig">cex_sig</code></td>
<td>
<p>Parameter to increase/reduce the size of the symbols used 
to represent sig_data.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_xlab">xlab</code></td>
<td>
<p>A logical value (TRUE) indicating if xlabels should be plotted</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_ylab">ylab</code></td>
<td>
<p>A logical value (TRUE) indicating if ylabels should be plotted</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_xlabels">xlabels</code></td>
<td>
<p>A vector of labels of the x-axis The length must be 
length of the col of parameter 'data'. Set the sequence from 1 to the 
length of the row of parameter 'data' as default.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_xtitle">xtitle</code></td>
<td>
<p>A string of title of the x-axis. Set NULL as default.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_ylabels">ylabels</code></td>
<td>
<p>A vector of labels of the y-axis The length must be 
length of the row of parameter 'data'. Set the sequence from 1 to the 
length of the row of parameter 'data' as default.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_ytitle">ytitle</code></td>
<td>
<p>A string of title of the y-axis. Set NULL as default.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_legend">legend</code></td>
<td>
<p>A logical value to decide to draw the color bar legend or not. 
Set TRUE as default.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_lab_legend">lab_legend</code></td>
<td>
<p>A vector of labels indicating what is represented in each 
category (i.e. triangle). Set the sequence from 1 to the length of 
the categories (2 or 4).</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_cex_leg">cex_leg</code></td>
<td>
<p>A number to indicate the increase/reductuion of the lab_legend
used to represent sig_data.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_col_leg">col_leg</code></td>
<td>
<p>Color of the legend (triangles).</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_cex_axis">cex_axis</code></td>
<td>
<p>A number to indicate the increase/reduction of the axis labels.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_mar">mar</code></td>
<td>
<p>A numerical vector of the form c(bottom, left, top, right) which 
gives the number of lines of margin to be specified on the four sides of the 
plot.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_fileout">fileout</code></td>
<td>
<p>A string of full directory path and file name indicating where 
to save the plot. If not specified (default), a graphics device will pop up.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_size_units">size_units</code></td>
<td>
<p>A string indicating the units of the size of the device 
(file or window) to plot in. Set 'px' as default. See ?Devices and the 
creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_res">res</code></td>
<td>
<p>A positive number indicating resolution of the device (file or 
window) to plot in. See ?Devices and the creator function of the 
corresponding device.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_figure.width">figure.width</code></td>
<td>
<p>a numeric value to control the width of the plot.</p>
</td></tr>
<tr><td><code id="PlotTriangles4Categories_+3A_...">...</code></td>
<td>
<p>The additional parameters to be passed to function ColorBar() in 
s2dv for color legend creation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A figure in popup window by default, or saved to the specified path.
</p>


<h3>Author(s)</h3>

<p>History:<br />
1.0  -  2020-10  (V.Torralba, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>)  -  Original code
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with random data
arr1 &lt;- array(runif(n = 4 * 5 * 4, min = -1, max = 1), dim = c(4,5,4))
names(dim(arr1)) &lt;- c('dimx', 'dimy', 'dimcat')
arr2 &lt;- array(TRUE, dim = dim(arr1))
arr2[which(arr1 &lt; 0.3)] &lt;- FALSE
PlotTriangles4Categories(data = arr1,
                        cols = c('white','#fef0d9','#fdd49e','#fdbb84','#fc8d59'),
                        brks = c(-1, 0, 0.1, 0.2, 0.3, 0.4), 
                        lab_legend = c('NAO+', 'BL','AR','NAO-'), 
                        xtitle = "Target month", ytitle = "Lead time",
                        xlabels = c("Jan", "Feb", "Mar", "Apr"))
</code></pre>

<hr>
<h2 id='PlotWeeklyClim'>Plots the observed weekly means and climatology of a timeseries data</h2><span id='topic+PlotWeeklyClim'></span>

<h3>Description</h3>

<p>This function plots the observed weekly means and climatology of 
a timeseries data using ggplot package. It compares the weekly climatology in 
a specified period (reference period) to the observed conditions during the 
target period analyzed in the case study.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotWeeklyClim(
  data,
  first_date,
  ref_period,
  last_date = NULL,
  data_years = NULL,
  time_dim = "time",
  sdate_dim = "sdate",
  ylim = NULL,
  title = NULL,
  subtitle = NULL,
  ytitle = NULL,
  legend = TRUE,
  palette = "Blues",
  fileout = NULL,
  device = NULL,
  width = 8,
  height = 6,
  units = "in",
  dpi = 300
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotWeeklyClim_+3A_data">data</code></td>
<td>
<p>A multidimensional array with named dimensions with at least sdate 
and time dimensions containing observed daily data. It can also be a 
dataframe with computed percentiles as input for ggplot. If it's a 
dataframe, it must contain the following column names: 'week', 'clim', 
'p10', 'p90', 'p33', 'p66', 'week_mean', 'day' and 'data'.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_first_date">first_date</code></td>
<td>
<p>The first date of the observed values of timeseries. It can 
be of class 'Date', 'POSIXct' or a character string in the format 
'yyyy-mm-dd'. If parameter 'data_years' is not provided, it must be a date
included in the reference period.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_ref_period">ref_period</code></td>
<td>
<p>A vector of numeric values indicating the years of the 
reference period. If parameter 'data_years' is not specified, it must 
be of the same length of dimension 'sdate_dim' of parameter 'data'.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_last_date">last_date</code></td>
<td>
<p>Optional parameter indicating the last date of the target 
period of the daily timeseries. It can be of class 'Date', 'POSIXct' or a 
character string in the format 'yyyy-mm-dd'. If it is NULL, the last date of 
the daily timeseries will be set as the last date of 'data'. As the data is 
plotted by weeks, only full groups of 7 days will be plotted. If the last 
date of the timeseries is not a multiple of 7 days, the last week will 
not be plotted.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_data_years">data_years</code></td>
<td>
<p>A vector of numeric values indicating the years of the 
data. It must be of the same length of dimension 'sdate_dim' of parameter 
'data'. It is optional, if not specified, all the years will be used as the 
target period.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_time_dim">time_dim</code></td>
<td>
<p>A character string indicating the daily time dimension name. 
The default value is 'time'.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the start year dimension name. 
The default value is 'sdate'.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_ylim">ylim</code></td>
<td>
<p>A numeric vector of length two providing limits of the scale. 
Use NA to refer to the existing minimum or maximum. For more information, 
see 'ggplot2' documentation of 'scale_y_continuous' parameter.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_title">title</code></td>
<td>
<p>The text for the top title of the plot. It is NULL by default.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_subtitle">subtitle</code></td>
<td>
<p>The text for the subtitle of the plot. It is NULL bu default.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_ytitle">ytitle</code></td>
<td>
<p>Character string to be drawn as y-axis title. It is NULL by 
default.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_legend">legend</code></td>
<td>
<p>A logical value indicating whether a legend should be included 
in the plot. If it is TRUE or NA, the legend will be included. If it is 
FALSE, the legend will not be included. It is TRUE by default.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_palette">palette</code></td>
<td>
<p>A palette name from the R Color Brewer’s package. The default 
value is 'Blues'.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_fileout">fileout</code></td>
<td>
<p>A character string indicating the file name where to save the 
plot. If not specified (default) a graphics device will pop up.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_device">device</code></td>
<td>
<p>A character string indicating the device to use. Can either be 
a device function (e.g. png), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex), 
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_width">width</code></td>
<td>
<p>A numeric value of the plot width in units (&quot;in&quot;, &quot;cm&quot;, &quot;mm&quot;, or 
&quot;px&quot;). It is set to 8 by default.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_height">height</code></td>
<td>
<p>A numeric value of the plot height in units (&quot;in&quot;, &quot;cm&quot;, &quot;mm&quot;, 
or &quot;px&quot;). It is set to 6 by default.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_units">units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot in. 
Inches (’in’) by default.</p>
</td></tr>
<tr><td><code id="PlotWeeklyClim_+3A_dpi">dpi</code></td>
<td>
<p>A numeric value of the plot resolution. It is set to 300 by 
default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object containing the plot.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- array(rnorm(49*20*3, 274), dim = c(time = 49, sdate = 20, member = 3))
PlotWeeklyClim(data = data, first_date = '2002-08-09', 
              last_date = '2002-09-15', ref_period = 2010:2019, 
              data_years = 2000:2019, time_dim = 'time', sdate_dim = 'sdate',
              title = "Observed weekly means and climatology", 
              subtitle = "Target years: 2010 to 2019", 
              ytitle = paste0('tas', " (", "deg.C", ")"))

</code></pre>

<hr>
<h2 id='Predictability'>Computing scores of predictability using two dynamical proxies 
based on dynamical systems theory.</h2><span id='topic+Predictability'></span>

<h3>Description</h3>

<p>This function divides in terciles the two dynamical proxies 
computed with CST_ProxiesAttractor or ProxiesAttractor. These terciles will
be used to measure the predictability of the system in dyn_scores. When the
local dimension 'dim' is small and the inverse of persistence 'theta' is 
small the predictability is high, and viceversa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Predictability(dim, theta, ncores = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Predictability_+3A_dim">dim</code></td>
<td>
<p>An array of N named dimensions containing the local dimension as 
the output of CST_ProxiesAttractor or ProxiesAttractor.</p>
</td></tr>
<tr><td><code id="Predictability_+3A_theta">theta</code></td>
<td>
<p>An array of N named dimensions containing the inverse of the 
persistence 'theta' as the output of CST_ProxiesAttractor or 
ProxiesAttractor.</p>
</td></tr>
<tr><td><code id="Predictability_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to use in parallel computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of length 2:
</p>

<ul>
<li><p>'pred.dim', a list of two lists 'qdim' and 'pos.d'. The 'qdim' list
contains values of local dimension 'dim' divided by terciles: 
d1: lower tercile (high predictability), 
d2: middle tercile, 
d3: higher tercile (low predictability)
The 'pos.d' list contains the position of each tercile in parameter 
'dim'.
</p>
</li>
<li><p>'pred.theta', a list of two lists 'qtheta' and 'pos.t'. 
The 'qtheta' list contains values of the inverse of persistence 'theta' 
divided by terciles: 
th1: lower tercile (high predictability), 
th2: middle tercile, 
th3: higher tercile (low predictability)
The 'pos.t' list contains the position of each tercile in parameter 
'theta'.
</p>
</li></ul>

<p>dyn_scores values from 0 to 1. A dyn_score of 1 indicates the highest
predictability.
</p>


<h3>Author(s)</h3>

<p>Carmen Alvarez-Castro, <a href="mailto:carmen.alvarez-castro@cmcc.it">carmen.alvarez-castro@cmcc.it</a>
</p>
<p>Maria M. Chaves-Montero, <a href="mailto:mdm.chaves-montero@cmcc.it">mdm.chaves-montero@cmcc.it</a>
</p>
<p>Veronica Torralba, <a href="mailto:veronica.torralba@cmcc.it">veronica.torralba@cmcc.it</a>
</p>
<p>Davide Faranda, <a href="mailto:davide.faranda@lsce.ipsl.fr">davide.faranda@lsce.ipsl.fr</a>
</p>


<h3>References</h3>

<p>Faranda, D., Alvarez-Castro, M.C., Messori, G., Rodriguez, D., 
and Yiou, P. (2019). The hammam effect or how a warm ocean enhances large 
scale atmospheric predictability.Nature Communications, 10(1), 1316. 
doi: <a href="https://doi.org/10.1038/s41467-019-09305-8">10.1038/s41467-019-09305-8</a>&quot;
</p>
<p>Faranda, D., Gabriele Messori and Pascal Yiou. (2017).
Dynamical proxies of North Atlantic predictability and extremes. 
Scientific Reports, 7-41278, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Creating an example of matrix dat(time,grids):
m &lt;- matrix(rnorm(2000) * 10, nrow = 50, ncol = 40)
names(dim(m)) &lt;- c('time', 'grid')
# imposing a threshold
 quanti &lt;-  0.90
# computing dyn_scores from parameters dim and theta of the attractor
attractor &lt;- ProxiesAttractor(dat = m, quanti = 0.60)
predyn &lt;- Predictability(dim = attractor$dim, theta = attractor$theta)
</code></pre>

<hr>
<h2 id='print.s2dv_cube'>Print method for s2dv_cube objects</h2><span id='topic+print.s2dv_cube'></span>

<h3>Description</h3>

<p>This is an S3 method of the generic 'print' for the class 's2dv_cube'. When 
you will call 'print' on an 's2dv_cube' object, this method will display the 
content of the object in a clear and informative way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 's2dv_cube'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.s2dv_cube_+3A_x">x</code></td>
<td>
<p>An 's2dv_cube' object.</p>
</td></tr>
<tr><td><code id="print.s2dv_cube_+3A_...">...</code></td>
<td>
<p>Additional arguments of print function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The object will be displayed following 's2dv_cube' class conventions. The 
top-level elements are: 'Data', a multidimensional array containing the 
object's data; 'Dimensions', the dimensions of the array; 'Coordinates', the 
array coordinates that match its dimensions, explicit coordinates have an 
asterisk (*) at the beginning while index coordinates do not; and 
'Attributes', which contains all the metadata of the object. For more 
information about the 's2dv_cube', see <code>s2dv_cube()</code> and 
<code>as.s2dv_cube()</code> functions.
</p>

<hr>
<h2 id='ProxiesAttractor'>Computing two dinamical proxies of the attractor.</h2><span id='topic+ProxiesAttractor'></span>

<h3>Description</h3>

<p>This function computes two dinamical proxies of the attractor: 
The local dimension (d) and the inverse of the persistence (theta). 
These two parameters will be used as a condition for the computation of dynamical 
scores to measure predictability and to compute bias correction conditioned by
the dynamics with the function DynBiasCorrection.  
Funtion based on the matlab code (davide.faranda@lsce.ipsl.fr) used in:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ProxiesAttractor(data, quanti, ncores = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ProxiesAttractor_+3A_data">data</code></td>
<td>
<p>A multidimensional array with named dimensions to create the 
attractor. It requires a temporal dimension named 'time' and spatial 
dimensions called 'lat' and 'lon', or 'latitude' and 'longitude' or 'grid'.</p>
</td></tr>
<tr><td><code id="ProxiesAttractor_+3A_quanti">quanti</code></td>
<td>
<p>A number lower than 1 indicating the quantile to perform the 
computation of local dimension and theta</p>
</td></tr>
<tr><td><code id="ProxiesAttractor_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to use in parallel computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dim and theta
</p>


<h3>Author(s)</h3>

<p>Carmen Alvarez-Castro, <a href="mailto:carmen.alvarez-castro@cmcc.it">carmen.alvarez-castro@cmcc.it</a>
</p>
<p>Maria M. Chaves-Montero, <a href="mailto:mdm.chaves-montero@cmcc.it">mdm.chaves-montero@cmcc.it</a>
</p>
<p>Veronica Torralba, <a href="mailto:veronica.torralba@cmcc.it">veronica.torralba@cmcc.it</a>
</p>
<p>Davide Faranda, <a href="mailto:davide.faranda@lsce.ipsl.fr">davide.faranda@lsce.ipsl.fr</a>
</p>


<h3>References</h3>

<p>Faranda, D., Alvarez-Castro, M.C., Messori, G., Rodriguez, D., and 
Yiou, P. (2019). The hammam effect or how a warm ocean enhances large scale 
atmospheric predictability. Nature Communications, 10(1), 1316. 
doi: <a href="https://doi.org/10.1038/s41467-019-09305-8">10.1038/s41467-019-09305-8</a>&quot;
</p>
<p>Faranda, D., Gabriele Messori and Pascal Yiou. (2017).
Dynamical proxies of North Atlantic predictability and extremes. 
Scientific Reports, 7-41278, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: Computing the attractor using simple data
# Creating an example of matrix data(time,grids):
mat &lt;- array(rnorm(36 * 40), c(time = 36, grid = 40)) 
qm &lt;- 0.90 # imposing a threshold
Attractor &lt;- ProxiesAttractor(data = mat, quanti = qm)
# to plot the result
time = c(1:length(Attractor$theta))
plot(time, Attractor$dim, xlab = 'time', ylab = 'd',
    main = 'local dimension', type = 'l')
</code></pre>

<hr>
<h2 id='QuantileMapping'>Quantile Mapping for seasonal or decadal forecast data</h2><span id='topic+QuantileMapping'></span>

<h3>Description</h3>

<p>This function is a wrapper of fitQmap and doQmap from package
'qmap' to be applied on multi-dimensional arrays. The quantile mapping 
adjustment between an experiment, typically a hindcast, and observation is 
applied to the experiment itself or to a provided forecast.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QuantileMapping(
  exp,
  obs,
  exp_cor = NULL,
  sdate_dim = "sdate",
  memb_dim = "member",
  window_dim = NULL,
  method = "QUANT",
  na.rm = FALSE,
  ncores = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QuantileMapping_+3A_exp">exp</code></td>
<td>
<p>A multidimensional array with named dimensions containing the 
hindcast.</p>
</td></tr>
<tr><td><code id="QuantileMapping_+3A_obs">obs</code></td>
<td>
<p>A multidimensional array with named dimensions containing the 
reference dataset.</p>
</td></tr>
<tr><td><code id="QuantileMapping_+3A_exp_cor">exp_cor</code></td>
<td>
<p>A multidimensional array with named dimensions in which the
quantile mapping correction should be applied. If it is not specified, the 
correction is applied on object 'exp'.</p>
</td></tr>
<tr><td><code id="QuantileMapping_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the dimension name in which
cross-validation would be applied when exp_cor is not provided. 'sdate' by
default.</p>
</td></tr>
<tr><td><code id="QuantileMapping_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the dimension name where
ensemble members are stored in the experimental arrays. It can be NULL if 
there is no ensemble member dimension. It is set as 'member' by default.</p>
</td></tr>
<tr><td><code id="QuantileMapping_+3A_window_dim">window_dim</code></td>
<td>
<p>A character string indicating the dimension name where 
samples have been stored. It can be NULL (default) in case all samples are
used.</p>
</td></tr>
<tr><td><code id="QuantileMapping_+3A_method">method</code></td>
<td>
<p>A character string indicating the method to be used: 'PTF',
'DIST', 'RQUANT', 'QUANT', 'SSPLIN'. By default, the empirical quantile 
mapping 'QUANT' is used.</p>
</td></tr>
<tr><td><code id="QuantileMapping_+3A_na.rm">na.rm</code></td>
<td>
<p>A logical value indicating if missing values should be removed
(FALSE by default).</p>
</td></tr>
<tr><td><code id="QuantileMapping_+3A_ncores">ncores</code></td>
<td>
<p>An integer indicating the number of cores for parallel 
computation using multiApply function. The default value is NULL (1).</p>
</td></tr>
<tr><td><code id="QuantileMapping_+3A_...">...</code></td>
<td>
<p>Additional parameters to be used by the method choosen. See qmap 
package for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An array containing the experimental data after applying the quantile
mapping correction.
</p>


<h3>Author(s)</h3>

<p>Nuria Perez-Zanon, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="qmap.html#topic+fitQmap">fitQmap</a></code> and <code><a href="qmap.html#topic+doQmap">doQmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Use synthetic data
exp &lt;- 1 : c(1 * 3 * 5 * 4 * 3 * 2)
dim(exp) &lt;- c(dataset = 1, member = 3, sdate = 5, ftime = 4, 
             lat = 3, lon = 2)
obs &lt;- 101 : c(100 + 1 * 1 * 5 * 4 * 3 * 2)
dim(obs) &lt;- c(dataset = 1, member = 1, sdate = 5, ftime = 4,
             lat = 3, lon = 2)
res &lt;- QuantileMapping(exp, obs)

</code></pre>

<hr>
<h2 id='RainFARM'>RainFARM stochastic precipitation downscaling (reduced version)</h2><span id='topic+RainFARM'></span>

<h3>Description</h3>

<p>This function implements the RainFARM stochastic precipitation downscaling method
and accepts in input an array with named dims (&quot;lon&quot;, &quot;lat&quot;)
and one or more dimension (such as &quot;ftime&quot;, &quot;sdate&quot; or &quot;time&quot;)
over which to average automatically determined spectral slopes.
Adapted for climate downscaling and including orographic correction.
References:
Terzago, S. et al. (2018). NHESS 18(11), 2825-2840. doi: <a href="https://doi.org/10.5194/nhess-18-2825-2018">10.5194/nhess-18-2825-2018</a>,
D'Onofrio et al. (2014), J of Hydrometeorology 15, 830-843; Rebora et. al. 
(2006), JHM 7, 724.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RainFARM(
  data,
  lon,
  lat,
  nf,
  weights = 1,
  nens = 1,
  slope = 0,
  kmin = 1,
  fglob = FALSE,
  fsmooth = TRUE,
  nprocs = 1,
  time_dim = NULL,
  lon_dim = "lon",
  lat_dim = "lat",
  drop_realization_dim = FALSE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RainFARM_+3A_data">data</code></td>
<td>
<p>Precipitation array to downscale. The input array is expected to 
have at least two dimensions named &quot;lon&quot; and &quot;lat&quot; by default (these default 
names can be changed with the <code>lon_dim</code> and <code>lat_dim</code> parameters)
and one or more dimensions over which to average these slopes, which can be 
specified by parameter <code>time_dim</code>. The number of longitudes and 
latitudes in the input data is expected to be even and the same. If not
the function will perform a subsetting to ensure this condition.</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_lon">lon</code></td>
<td>
<p>Vector or array of longitudes.</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_lat">lat</code></td>
<td>
<p>Vector or array of latitudes.</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_nf">nf</code></td>
<td>
<p>Refinement factor for downscaling (the output resolution is 
increased by this factor).</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_weights">weights</code></td>
<td>
<p>Multi-dimensional array with climatological weights which can 
be obtained using the <code>CST_RFWeights</code> function. If <code>weights = 1.</code> 
(default) no weights are used. The names of these dimensions must be at 
least the same longitudinal and latitudinal dimension names as data.</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_nens">nens</code></td>
<td>
<p>Number of ensemble members to produce (default: <code>nens = 1</code>).</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_slope">slope</code></td>
<td>
<p>Prescribed spectral slope. The default is <code>slope = 0.</code>
meaning that the slope is determined automatically over the dimensions 
specified by <code>time_dim</code>. A 1D array with named dimension can be 
provided (see details and examples).</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_kmin">kmin</code></td>
<td>
<p>First wavenumber for spectral slope (default: <code>kmin = 1</code>).</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_fglob">fglob</code></td>
<td>
<p>Logical to conseve global precipitation over the domain 
(default: FALSE).</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_fsmooth">fsmooth</code></td>
<td>
<p>Logical to conserve precipitation with a smoothing kernel 
(default: TRUE).</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_nprocs">nprocs</code></td>
<td>
<p>The number of parallel processes to spawn for the use for 
parallel computation in multiple cores. (default: 1)</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_time_dim">time_dim</code></td>
<td>
<p>String or character array with name(s) of time dimension(s)
(e.g. &quot;ftime&quot;, &quot;sdate&quot;, &quot;time&quot; ...) over which to compute spectral slopes.
If a character array of dimension names is provided, the spectral slopes
will be computed over all elements belonging to those dimensions.
If omitted one of c(&quot;ftime&quot;, &quot;sdate&quot;, &quot;time&quot;) is searched and the first one 
with more than one element is chosen.</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_lon_dim">lon_dim</code></td>
<td>
<p>Name of lon dimension (&quot;lon&quot; by default).</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_lat_dim">lat_dim</code></td>
<td>
<p>Name of lat dimension (&quot;lat&quot; by default).</p>
</td></tr>
<tr><td><code id="RainFARM_+3A_drop_realization_dim">drop_realization_dim</code></td>
<td>
<p>Logical to remove the &quot;realization&quot; stochastic 
ensemble dimension (default: FALSE) with the following behaviour if set to 
TRUE:
</p>

<ol>
<li><p>if <code>nens == 1</code>: the dimension is dropped;
</p>
</li>
<li><p>if <code>nens &gt; 1</code> and a &quot;member&quot; dimension exists: the &quot;realization&quot; 
and &quot;member&quot; dimensions are compacted (multiplied) and the resulting 
dimension is named &quot;member&quot;;
</p>
</li>
<li><p>if <code>nens &gt; 1</code> and a &quot;member&quot; dimension does not exist: the 
&quot;realization&quot; dimension is renamed to &quot;member&quot;.
</p>
</li></ol>
</td></tr>
<tr><td><code id="RainFARM_+3A_verbose">verbose</code></td>
<td>
<p>logical for verbose output (default: FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Wether parameter 'slope' and 'weights' presents seasonality 
dependency, a dimension name should match between these parameters and the 
input data in parameter 'data'. See example 2 below where weights and slope 
vary with 'sdate' dimension.
</p>


<h3>Value</h3>

<p>RainFARM() Returns a list containing the fine-scale longitudes, 
latitudes and the sequence of <code>nens</code> downscaled fields. If 
<code>nens &gt; 1</code> an additional dimension named &quot;realization&quot; is added to the 
output array after the &quot;member&quot; dimension (if it exists and unless 
<code>drop_realization_dim = TRUE</code> is specified). The ordering of the 
remaining dimensions in the <code>exp</code> element of the input object is 
maintained.
</p>


<h3>Author(s)</h3>

<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example for the 'reduced' RainFARM function 
nf &lt;- 8   # Choose a downscaling by factor 8
exp &lt;- 1 : (2 * 3 * 4 * 8 * 8)
dim(exp) &lt;- c(dataset = 1, member = 2, sdate = 3, ftime = 4, lat = 8, lon = 8)
lon &lt;- seq(10, 13.5, 0.5)
lat &lt;- seq(40, 43.5, 0.5)
# Create a test array of weights
ww &lt;- array(1., dim = c(lon = 8 * nf, lat = 8 * nf))
res &lt;- RainFARM(data = exp, lon = lon, lat = lat, nf = nf, 
               weights = ww, nens = 3, time_dim = 'ftime')
</code></pre>

<hr>
<h2 id='RegimesAssign'>Function for matching a field of anomalies with 
a set of maps used as a reference (e.g. clusters obtained from the WeatherRegime function).</h2><span id='topic+RegimesAssign'></span>

<h3>Description</h3>

<p>This function performs the matching between a field of anomalies 
and a set of maps which will be used as a reference. The anomalies will be 
assigned to the reference map for which the minimum Eucledian distance 
(method = 'distance') or highest spatial correlation (method = 'ACC') is 
obtained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RegimesAssign(
  data,
  ref_maps,
  lat,
  method = "distance",
  composite = FALSE,
  memb = FALSE,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RegimesAssign_+3A_data">data</code></td>
<td>
<p>An array containing anomalies with named dimensions: dataset, 
member, sdate, ftime, lat and lon.</p>
</td></tr>
<tr><td><code id="RegimesAssign_+3A_ref_maps">ref_maps</code></td>
<td>
<p>Array with 3-dimensions ('lon', 'lat', 'cluster') containing 
the maps/clusters that will be used as a reference for the matching.</p>
</td></tr>
<tr><td><code id="RegimesAssign_+3A_lat">lat</code></td>
<td>
<p>A vector of latitudes corresponding to the positions provided in 
data and ref_maps.</p>
</td></tr>
<tr><td><code id="RegimesAssign_+3A_method">method</code></td>
<td>
<p>Whether the matching will be performed in terms of minimum 
distance (default = 'distance') or the maximum spatial correlation 
(method = 'ACC') between the maps.</p>
</td></tr>
<tr><td><code id="RegimesAssign_+3A_composite">composite</code></td>
<td>
<p>A logical parameter indicating if the composite maps are 
computed or not (default = FALSE).</p>
</td></tr>
<tr><td><code id="RegimesAssign_+3A_memb">memb</code></td>
<td>
<p>A logical value indicating whether to compute composites for 
separate members (default FALSE) or as unique ensemble (TRUE). This option 
is only available for when parameter 'composite' is set to TRUE and the data 
object has a dimension named 'member'.</p>
</td></tr>
<tr><td><code id="RegimesAssign_+3A_ncores">ncores</code></td>
<td>
<p>The number of multicore threads to use for parallel computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements <code>$composite</code> (3-d array (lon, lat, k) 
containing the composites k = 1,..,K for case (*1) or only k = 1 for any specific 
cluster, i.e., case (*2)) (only if composite = 'TRUE'), <code>$pvalue</code> (array 
with the same structure as <code>$composite</code> containing the pvalue of the 
composites obtained through a t-test that accounts for the serial dependence 
of the data with the same structure as Composite.) (only if composite='TRUE'),
<code>$cluster</code> (array with the same dimensions as data (except latitude and 
longitude which are removed) indicating the ref_maps to which each point is 
allocated.), <code>$frequency</code> (A vector of integers (from k = 1, ... k n 
reference maps) indicating the percentage of assignations corresponding to 
each map.),
</p>


<h3>Author(s)</h3>

<p>Verónica Torralba - BSC, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>
</p>


<h3>References</h3>

<p>Torralba, V. (2019) Seasonal climate prediction for the wind 
energy sector: methods and tools for the development of a climate service. 
Thesis. Available online: <a href="https://eprints.ucm.es/56841/">https://eprints.ucm.es/56841/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- array(abs(rnorm(1280, 282.7, 6.4)), dim = c(dataset = 2, member = 2, 
                                                   sdate = 3, ftime = 3, 
                                                   lat = 4, lon = 4))
regimes &lt;- WeatherRegime(data = data, lat = seq(47, 44),
                        EOFs = FALSE, ncenters = 4)$composite
res1 &lt;- RegimesAssign(data = data, ref_maps = drop(regimes), 
                     lat = seq(47, 44), composite = FALSE)
</code></pre>

<hr>
<h2 id='RF_Weights'>Compute climatological weights for RainFARM stochastic precipitation downscaling</h2><span id='topic+RF_Weights'></span>

<h3>Description</h3>

<p>Compute climatological (&quot;orographic&quot;) weights from a fine-scale 
precipitation climatology file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RF_Weights(
  zclim,
  latin,
  lonin,
  nf,
  lat,
  lon,
  fsmooth = TRUE,
  lonname = "lon",
  latname = "lat",
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RF_Weights_+3A_zclim">zclim</code></td>
<td>
<p>A multi-dimensional array with named dimension containing at 
least one precipiation field with spatial dimensions.</p>
</td></tr>
<tr><td><code id="RF_Weights_+3A_latin">latin</code></td>
<td>
<p>A vector indicating the latitudinal coordinates corresponding to 
the <code>zclim</code> parameter.</p>
</td></tr>
<tr><td><code id="RF_Weights_+3A_lonin">lonin</code></td>
<td>
<p>A vector indicating the longitudinal coordinates corresponding to 
the <code>zclim</code> parameter.</p>
</td></tr>
<tr><td><code id="RF_Weights_+3A_nf">nf</code></td>
<td>
<p>Refinement factor for downscaling (the output resolution is 
increased by this factor).</p>
</td></tr>
<tr><td><code id="RF_Weights_+3A_lat">lat</code></td>
<td>
<p>Vector of latitudes. The number of longitudes and latitudes is 
expected to be even and the same. If not the function will perform a 
subsetting to ensure this condition.</p>
</td></tr>
<tr><td><code id="RF_Weights_+3A_lon">lon</code></td>
<td>
<p>Vector of longitudes.</p>
</td></tr>
<tr><td><code id="RF_Weights_+3A_fsmooth">fsmooth</code></td>
<td>
<p>Logical to use smooth conservation (default) or large-scale 
box-average conservation.</p>
</td></tr>
<tr><td><code id="RF_Weights_+3A_lonname">lonname</code></td>
<td>
<p>A character string indicating the name of the longitudinal 
dimension set as 'lon' by default.</p>
</td></tr>
<tr><td><code id="RF_Weights_+3A_latname">latname</code></td>
<td>
<p>A character string indicating the name of the latitudinal 
dimension set as 'lat' by default.</p>
</td></tr>
<tr><td><code id="RF_Weights_+3A_ncores">ncores</code></td>
<td>
<p>An integer that indicates the number of cores for parallel 
computations using multiApply function. The default value is one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class 's2dv_cube' containing in matrix <code>data</code> the 
weights with dimensions (lon, lat).
</p>


<h3>Author(s)</h3>

<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>


<h3>References</h3>

<p>Terzago, S., Palazzi, E., &amp; von Hardenberg, J. (2018).
Stochastic downscaling of precipitation in complex orography: 
A simple method to reproduce a realistic fine-scale climatology.
Natural Hazards and Earth System Sciences, 18(11),
2825-2840. doi: <a href="https://doi.org/10.5194/nhess-18-2825-2018">10.5194/nhess-18-2825-2018</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- array(1:2500, c(lat = 50, lon = 50))
res &lt;- RF_Weights(a, seq(0.1 ,5, 0.1), seq(0.1 ,5, 0.1), 
                 nf = 5, lat = 1:5, lon = 1:5)
</code></pre>

<hr>
<h2 id='RFSlope'>RainFARM spectral slopes from an array (reduced version)</h2><span id='topic+RFSlope'></span>

<h3>Description</h3>

<p>This function computes spatial spectral slopes from an array,
to be used for RainFARM stochastic precipitation downscaling method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RFSlope(
  data,
  kmin = 1,
  time_dim = NULL,
  lon_dim = "lon",
  lat_dim = "lat",
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RFSlope_+3A_data">data</code></td>
<td>
<p>Array containing the spatial precipitation fields to downscale.
The input array is expected to have at least two dimensions named &quot;lon&quot; and 
&quot;lat&quot; by default (these default names can be changed with the <code>lon_dim</code> 
and <code>lat_dim</code> parameters) and one or more dimensions over which to 
average the slopes, which can be specified by parameter <code>time_dim</code>.</p>
</td></tr>
<tr><td><code id="RFSlope_+3A_kmin">kmin</code></td>
<td>
<p>First wavenumber for spectral slope (default <code>kmin=1</code>).</p>
</td></tr>
<tr><td><code id="RFSlope_+3A_time_dim">time_dim</code></td>
<td>
<p>String or character array with name(s) of dimension(s)
(e.g. &quot;ftime&quot;, &quot;sdate&quot;, &quot;member&quot; ...) over which to compute spectral slopes.
If a character array of dimension names is provided, the spectral slopes
will be computed as an average over all elements belonging to those dimensions.
If omitted one of c(&quot;ftime&quot;, &quot;sdate&quot;, &quot;time&quot;)  is searched and the first one
with more than one element is chosen.</p>
</td></tr>
<tr><td><code id="RFSlope_+3A_lon_dim">lon_dim</code></td>
<td>
<p>Name of lon dimension (&quot;lon&quot; by default).</p>
</td></tr>
<tr><td><code id="RFSlope_+3A_lat_dim">lat_dim</code></td>
<td>
<p>Name of lat dimension (&quot;lat&quot; by default).</p>
</td></tr>
<tr><td><code id="RFSlope_+3A_ncores">ncores</code></td>
<td>
<p>is an integer that indicates the number of cores for parallel 
computations using multiApply function. The default value is one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>RFSlope() returns spectral slopes using the RainFARM convention
(the logarithmic slope of k*|A(k)|^2 where A(k) are the spectral amplitudes).
The returned array has the same dimensions as the input array,
minus the dimensions specified by <code>lon_dim</code>, <code>lat_dim</code> and <code>time_dim</code>.
</p>


<h3>Author(s)</h3>

<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example for the 'reduced' RFSlope function 
# Create a test array with dimension 8x8 and 20 timesteps, 
# 3 starting dates and 20 ensemble members.
pr &lt;- 1:(4*3*8*8*20)
dim(pr) &lt;- c(ensemble = 4, sdate = 3, lon = 8, lat = 8, ftime = 20)
# Compute the spectral slopes ignoring the wavenumber
# corresponding to the largest scale (the box)
slopes &lt;- RFSlope(pr, kmin = 2, time_dim = 'ftime')
</code></pre>

<hr>
<h2 id='RFTemp'>Temperature downscaling of a CSTools object using lapse rate
correction (reduced version)</h2><span id='topic+RFTemp'></span>

<h3>Description</h3>

<p>This function implements a simple lapse rate correction of a
temperature field (a multidimensional array) as input.
The input lon grid must be increasing (but can be modulo 360).
The input lat grid can be irregularly spaced (e.g. a Gaussian grid)
The output grid can be irregularly spaced in lon and/or lat.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RFTemp(
  data,
  lon,
  lat,
  oro,
  lonoro,
  latoro,
  xlim = NULL,
  ylim = NULL,
  lapse = 6.5,
  lon_dim = "lon",
  lat_dim = "lat",
  time_dim = NULL,
  nolapse = FALSE,
  verbose = FALSE,
  compute_delta = FALSE,
  method = "bilinear",
  delta = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RFTemp_+3A_data">data</code></td>
<td>
<p>Temperature array to downscale. The input array is expected to 
have at least two dimensions named &quot;lon&quot; and &quot;lat&quot; by default (these default 
names can be changed with the <code>lon_dim</code> and <code>lat_dim</code> parameters).</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_lon">lon</code></td>
<td>
<p>Vector or array of longitudes.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_lat">lat</code></td>
<td>
<p>Vector or array of latitudes.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_oro">oro</code></td>
<td>
<p>Array containing fine-scale orography (in m). The destination 
downscaling area must be contained in the orography field.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_lonoro">lonoro</code></td>
<td>
<p>Vector or array of longitudes corresponding to the fine orography.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_latoro">latoro</code></td>
<td>
<p>Vector or array of latitudes corresponding to the fine orography.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_xlim">xlim</code></td>
<td>
<p>Vector with longitude bounds for downscaling; the full input field 
is downscaled if 'xlim' and 'ylim' are not specified.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_ylim">ylim</code></td>
<td>
<p>Vector with latitude bounds for downscaling.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_lapse">lapse</code></td>
<td>
<p>Float with environmental lapse rate.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_lon_dim">lon_dim</code></td>
<td>
<p>String with name of longitude dimension.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_lat_dim">lat_dim</code></td>
<td>
<p>String with name of latitude dimension.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_time_dim">time_dim</code></td>
<td>
<p>A vector of character string indicating the name of temporal 
dimension. By default, it is set to NULL and it considers &quot;ftime&quot;, &quot;sdate&quot; 
and &quot;time&quot; as temporal dimensions.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_nolapse">nolapse</code></td>
<td>
<p>Logical, if true 'oro' is interpreted as a fine-scale 
climatology and used directly for bias correction.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_verbose">verbose</code></td>
<td>
<p>Logical if to print diagnostic output.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_compute_delta">compute_delta</code></td>
<td>
<p>Logical if true returns only a delta to be used for
out-of-sample forecasts.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_method">method</code></td>
<td>
<p>String indicating the method used for interpolation:
&quot;nearest&quot; (nearest neighbours followed by smoothing with a circular
uniform weights kernel), &quot;bilinear&quot; (bilinear interpolation)
The two methods provide similar results, but nearest is slightly better
provided that the fine-scale grid is correctly centered as a subdivision
of the large-scale grid.</p>
</td></tr>
<tr><td><code id="RFTemp_+3A_delta">delta</code></td>
<td>
<p>Matrix containing a delta to be applied to the downscaled
input data. The grid of this matrix is supposed to be same as that of
the required output field.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>CST_RFTemp() returns a downscaled CSTools object.
</p>
<p>RFTemp() returns a list containing the fine-scale
longitudes, latitudes and the downscaled fields.
</p>


<h3>Author(s)</h3>

<p>Jost von Hardenberg - ISAC-CNR, <a href="mailto:j.vonhardenberg@isac.cnr.it">j.vonhardenberg@isac.cnr.it</a>
</p>


<h3>References</h3>

<p>Method described in ERA4CS MEDSCOPE milestone M3.2: 
High-quality climate prediction data available to WP4 here:
<a href="https://www.medscope-project.eu/the-project/deliverables-reports/">https://www.medscope-project.eu/the-project/deliverables-reports/</a>
and in H2020 ECOPOTENTIAL Deliverable No. 8.1:
High resolution (1-10 km) climate, land use and ocean change scenarios here: 
<a href="https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e5b6cd2324&amp;appId=PPGMS">https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e5b6cd2324&amp;appId=PPGMS</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate simple synthetic data and downscale by factor 4
t &lt;- rnorm(7 * 6 * 4 * 3) * 10 + 273.15 + 10
dim(t) &lt;- c(sdate = 3, ftime = 4, lat = 6, lon = 7)
lon &lt;- seq(3, 9, 1)
lat &lt;- seq(42, 47, 1)
o &lt;- runif(29 * 29) * 3000
dim(o) &lt;- c(lat = 29, lon = 29)
lono &lt;- seq(3, 10, 0.25)
lato &lt;- seq(41, 48, 0.25)
res &lt;- RFTemp(t, lon, lat, o, lono, lato, xlim = c(4, 8), ylim = c(43, 46),
             lapse = 6.5, time_dim = 'ftime')
</code></pre>

<hr>
<h2 id='s2dv_cube'>Creation of a 's2dv_cube' object</h2><span id='topic+s2dv_cube'></span>

<h3>Description</h3>

<p>This function allows to create an 's2dv_cube' object by passing 
information through its parameters. This function will be needed if the data 
hasn't been loaded using CST_Start or has been transformed with other methods. 
An 's2dv_cube' object has many different components including metadata. This 
function will allow to create 's2dv_cube' objects even if not all elements 
are defined and for each expected missed parameter a warning message will be 
returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>s2dv_cube(
  data,
  coords = NULL,
  varName = NULL,
  metadata = NULL,
  Datasets = NULL,
  Dates = NULL,
  when = NULL,
  source_files = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="s2dv_cube_+3A_data">data</code></td>
<td>
<p>A multidimensional array with named dimensions, typically with 
dimensions: dataset, member, sdate, time, lat and lon.</p>
</td></tr>
<tr><td><code id="s2dv_cube_+3A_coords">coords</code></td>
<td>
<p>A list of named vectors with the coordinates corresponding to 
the dimensions of the data parameter. If any coordinate has dimensions, they 
will be set as NULL. If any coordinate is not provided, it is set as an 
index vector with the values from 1 to the length of the corresponding 
dimension. The attribute 'indices' indicates wether the coordinate is an 
index vector (TRUE) or not (FALSE).</p>
</td></tr>
<tr><td><code id="s2dv_cube_+3A_varname">varName</code></td>
<td>
<p>A character string indicating the abbreviation of the variable 
name.</p>
</td></tr>
<tr><td><code id="s2dv_cube_+3A_metadata">metadata</code></td>
<td>
<p>A named list where each element is a variable containing the
corresponding information. The information can be contained in a list of 
lists for each variable.</p>
</td></tr>
<tr><td><code id="s2dv_cube_+3A_datasets">Datasets</code></td>
<td>
<p>Character strings indicating the names of the dataset. It 
there are multiple datasets it can be a vector of its names or a list of
lists with additional information.</p>
</td></tr>
<tr><td><code id="s2dv_cube_+3A_dates">Dates</code></td>
<td>
<p>A POSIXct array of time dimensions containing the Dates.</p>
</td></tr>
<tr><td><code id="s2dv_cube_+3A_when">when</code></td>
<td>
<p>A time stamp of the date when the data has been loaded. This 
parameter is also found in Load() and Start() functions output.</p>
</td></tr>
<tr><td><code id="s2dv_cube_+3A_source_files">source_files</code></td>
<td>
<p>A vector of character strings with complete paths to all 
the found files involved in loading the data.</p>
</td></tr>
<tr><td><code id="s2dv_cube_+3A_...">...</code></td>
<td>
<p>Additional elements to be added in the object. They will be 
stored in the end of 'attrs' element. Multiple elements are accepted.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of class 's2dv_cube' with the following 
elements in the structure:<br />
</p>

<ul>
<li><p>'data', array with named dimensions;
</p>
</li>
<li><p>'dims', named vector of the data dimensions;
</p>
</li>
<li><p>'coords', list of named vectors with the coordinates corresponding to 
the dimensions of the data parameter;
</p>
</li>
<li><p>'attrs', named list with elements:
</p>

<ul>
<li><p>'Dates', array with named temporal dimensions of class 'POSIXct' from 
time values in the data;
</p>
</li>
<li><p>'Variable', has the following components:
</p>

<ul>
<li><p>'varName', with the short name of the loaded variable as specified 
in the parameter 'var';
</p>
</li>
<li><p>'metadata', named list of elements with variable metadata. 
They can be from coordinates variables (e.g. longitude) or 
main variables (e.g. 'var');
</p>
</li></ul>


</li>
<li><p>'Datasets', character strings indicating the names of the dataset;
</p>
</li>
<li><p>'source_files', a vector of character strings with complete paths 
to all the found files involved in loading the data;
</p>
</li>
<li><p>'when', a time stamp of the date issued by the Start() or Load() 
call to obtain the data;
</p>
</li>
<li><p>'load_parameters', it contains the components used in the 
arguments to load the data from Start() or Load() functions.
</p>
</li></ul>


</li></ul>



<h3>Author(s)</h3>

<p>Perez-Zanon Nuria, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="s2dv.html#topic+Load">Load</a></code> and <code><a href="#topic+CST_Start">CST_Start</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>exp_original &lt;- 1:100
dim(exp_original) &lt;- c(lat = 2, time = 10, lon = 5)
exp1 &lt;- s2dv_cube(data = exp_original)
class(exp1)
coords &lt;- list(lon = seq(-10, 10, 5), lat = c(45, 50))
exp2 &lt;- s2dv_cube(data = exp_original, coords = coords) 
class(exp2)
metadata &lt;- list(tas = list(level = '2m'))
exp3 &lt;- s2dv_cube(data = exp_original, coords = coords,
                 varName = 'tas', metadata = metadata)
class(exp3)
Dates = as.POSIXct(paste0(rep("01", 10), rep("01", 10), 1990:1999), format = "%d%m%Y")
dim(Dates) &lt;- c(time = 10)
exp4 &lt;- s2dv_cube(data = exp_original, coords = coords,
                 varName = 'tas', metadata = metadata,
                 Dates = Dates)  
class(exp4)
exp5 &lt;- s2dv_cube(data = exp_original, coords = coords,
                 varName = 'tas', metadata = metadata,
                 Dates = Dates, when = "2019-10-23 19:15:29 CET")  
class(exp5)
exp6 &lt;- s2dv_cube(data = exp_original, coords = coords,
                 varName = 'tas', metadata = metadata,
                 Dates = Dates,
                 when = "2019-10-23 19:15:29 CET", 
                 source_files = c("/path/to/file1.nc", "/path/to/file2.nc"))
class(exp6)
exp7 &lt;- s2dv_cube(data = exp_original, coords = coords,
                 varName = 'tas', metadata = metadata,
                 Dates = Dates,
                 when = "2019-10-23 19:15:29 CET", 
                 source_files = c("/path/to/file1.nc", "/path/to/file2.nc"),
                 Datasets = list(
                   exp1 = list(InitializationsDates = list(Member_1 = "01011990", 
                                                           Members = "Member_1"))))  
class(exp7)
dim(exp_original) &lt;- c(dataset = 1, member = 1, time = 10, lat = 2, lon = 5)
exp8 &lt;- s2dv_cube(data = exp_original, coords = coords,
                 varName = 'tas', metadata = metadata,
                 Dates = Dates, original_dates = Dates)  
class(exp8)
</code></pre>

<hr>
<h2 id='SaveExp'>Save a multidimensional array with metadata to data in NetCDF format</h2><span id='topic+SaveExp'></span>

<h3>Description</h3>

<p>This function allows to save a data array with metadata into a 
NetCDF file, allowing to reload the saved data using <code>Start</code> function 
from StartR package. If the original 's2dv_cube' object has been created from 
<code>CST_Load()</code>, then it can be reloaded with <code>Load()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SaveExp(
  data,
  destination = "./",
  coords = NULL,
  Dates = NULL,
  time_bounds = NULL,
  startdates = NULL,
  varname = NULL,
  metadata = NULL,
  Datasets = NULL,
  sdate_dim = "sdate",
  ftime_dim = "time",
  memb_dim = "member",
  dat_dim = "dataset",
  var_dim = "var",
  drop_dims = NULL,
  single_file = FALSE,
  extra_string = NULL,
  global_attrs = NULL,
  units_hours_since = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SaveExp_+3A_data">data</code></td>
<td>
<p>A multi-dimensional array with named dimensions.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_destination">destination</code></td>
<td>
<p>A character string indicating the path where to store the 
NetCDF files.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_coords">coords</code></td>
<td>
<p>A named list with elements of the coordinates corresponding to 
the dimensions of the data parameter. The names and length of each element 
must correspond to the names of the dimensions. If any coordinate is not 
provided, it is set as an index vector with the values from 1 to the length 
of the corresponding dimension.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_dates">Dates</code></td>
<td>
<p>A named array of dates with the corresponding sdate and forecast 
time dimension. If there is no sdate_dim, you can set it to NULL. 
It must have ftime_dim dimension.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_time_bounds">time_bounds</code></td>
<td>
<p>(Optional) A list of two arrays of dates containing 
the lower (first array) and the upper (second array) time bounds 
corresponding to Dates. Each array must have the same dimensions as Dates.
If 'Dates' parameter is NULL, 'time_bounds' are not used. It is NULL by 
default.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_startdates">startdates</code></td>
<td>
<p>A vector of dates that will be used for the filenames 
when saving the data in multiple files (single_file = FALSE). It must be a 
vector of the same length as the start date dimension of data. It must be a 
vector of class <code>Dates</code>, <code>'POSIXct'</code> or character with lenghts 
between 1 and 10. If it is NULL, the coordinate corresponding the the start 
date dimension or the first Date of each time step will be used as the name 
of the files. It is NULL by default.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_varname">varname</code></td>
<td>
<p>A character string indicating the name of the variable to be 
saved.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_metadata">metadata</code></td>
<td>
<p>A named list where each element is a variable containing the
corresponding information. The information must be contained in a list of 
lists for each variable.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_datasets">Datasets</code></td>
<td>
<p>A vector of character string indicating the names of the 
datasets.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_sdate_dim">sdate_dim</code></td>
<td>
<p>A character string indicating the name of the start date 
dimension. By default, it is set to 'sdate'. It can be NULL if there is no
start date dimension.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_ftime_dim">ftime_dim</code></td>
<td>
<p>A character string indicating the name of the forecast time
dimension. By default, it is set to 'time'. It can be NULL if there is no 
forecast time dimension.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_memb_dim">memb_dim</code></td>
<td>
<p>A character string indicating the name of the member 
dimension. By default, it is set to 'member'. It can be NULL if there is no 
member dimension.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_dat_dim">dat_dim</code></td>
<td>
<p>A character string indicating the name of dataset dimension. 
By default, it is set to 'dataset'. It can be NULL if there is no dataset  
dimension.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_var_dim">var_dim</code></td>
<td>
<p>A character string indicating the name of variable dimension. 
By default, it is set to 'var'. It can be NULL if there is no variable  
dimension.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_drop_dims">drop_dims</code></td>
<td>
<p>(optional) A vector of character strings indicating the 
dimension names of length 1 that need to be dropped in order that they don't 
appear in the netCDF file. Only is allowed to drop dimensions that are not 
used in the computation. The dimensions used in the computation are the ones 
specified in: sdate_dim, ftime_dim, dat_dim, var_dim and memb_dim. It is 
NULL by default.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_single_file">single_file</code></td>
<td>
<p>A logical value indicating if all object is saved in a 
single file (TRUE) or in multiple files (FALSE). When it is FALSE, 
the array is separated for datasets, variable and start date. When there are 
no specified time dimensions, the data will be saved in a single file by 
default. The output file name when 'single_file' is TRUE is a character 
string containing: '&lt;var&gt;_&lt;first_sdate&gt;_&lt;last_sdate&gt;.nc'; when it is FALSE, 
it is '&lt;var&gt;_&lt;sdate&gt;.nc'. It is FALSE by default.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_extra_string">extra_string</code></td>
<td>
<p>(Optional) A character string to be included as part of 
the file name, for instance, to identify member or realization. When 
single_file is TRUE, the 'extra_string' will substitute all the default 
file name; when single_file is FALSE, the 'extra_string' will be added 
in the file name as: '&lt;var&gt;_&lt;extra_string&gt;_&lt;sdate&gt;.nc'. It is NULL by 
default.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_global_attrs">global_attrs</code></td>
<td>
<p>(Optional) A list with elements containing the global 
attributes to be saved in the NetCDF.</p>
</td></tr>
<tr><td><code id="SaveExp_+3A_units_hours_since">units_hours_since</code></td>
<td>
<p>(Optional) A logical value only available for the 
case: Dates have forecast time and start date dimension, single_file is 
TRUE and 'time_bounds' is NULL. When it is TRUE, it saves the forecast time 
with units of 'hours since'; if it is FALSE, the time units will be a number 
of time steps with its corresponding frequency (e.g. n days, n months or n 
hours). It is FALSE by default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Multiple or single NetCDF files containing the data array.<br />
</p>
<table>
<tr><td><code>single_file is TRUE</code></td>
<td>

<p>All data is saved in a single file located in the specified destination  
path with the following name (by default): 
'&lt;variable_name&gt;_&lt;first_sdate&gt;_&lt;last_sdate&gt;.nc'. Multiple variables
are saved separately in the same file. The forecast time units 
are calculated from each start date (if sdate_dim is not NULL) or from 
the time step. If 'units_hours_since' is TRUE, the forecast time units 
will be 'hours since &lt;each start date&gt;'. If 'units_hours_since' is FALSE, 
the forecast time units are extracted from the frequency of the time steps 
(hours, days, months); if no frequency is found, the units will be ’hours 
since’. When the time units are 'hours since' the time ateps are assumed to 
be equally spaced.
</p>
</td></tr>
<tr><td><code>single_file is FALSE</code></td>
<td>

<p>The data array is subset and stored into multiple files. Each file 
contains the data subset for each start date, variable and dataset. Files 
with different variables and datasets are stored in separated directories 
within the following directory tree: 'destination/Dataset/variable/'. 
The name of each file will be by default: '&lt;variable_name&gt;_&lt;sdate&gt;.nc'. 
The forecast time units are calculated from each start date (if sdate_dim 
is not NULL) or from the time step. The forecast time units will be 'hours 
since &lt;each start date&gt;'.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Perez-Zanon Nuria, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data &lt;- lonlat_temp_st$exp$data
lon &lt;- lonlat_temp_st$exp$coords$lon
lat &lt;- lonlat_temp_st$exp$coords$lat
coords &lt;- list(lon = lon, lat = lat)
Datasets &lt;- lonlat_temp_st$exp$attrs$Datasets
varname &lt;- 'tas'
Dates &lt;- lonlat_temp_st$exp$attrs$Dates
metadata &lt;- lonlat_temp_st$exp$attrs$Variable$metadata
SaveExp(data = data, coords = coords, Datasets = Datasets, varname = varname, 
       Dates = Dates, metadata = metadata, single_file = TRUE, 
       ftime_dim = 'ftime', var_dim = 'var', dat_dim = 'dataset')

## End(Not run)

</code></pre>

<hr>
<h2 id='SplitDim'>Function to Split Dimension</h2><span id='topic+SplitDim'></span>

<h3>Description</h3>

<p>This function split a dimension in two. The user can select the 
dimension to split and provide indices indicating how to split that dimension 
or dates and the frequency expected (monthly or by day, month and year). The 
user can also provide a numeric frequency indicating the length of each division.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SplitDim(
  data,
  split_dim = "time",
  indices,
  freq = "monthly",
  new_dim_name = NULL,
  dates = NULL,
  return_indices = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SplitDim_+3A_data">data</code></td>
<td>
<p>An n-dimensional array with named dimensions.</p>
</td></tr>
<tr><td><code id="SplitDim_+3A_split_dim">split_dim</code></td>
<td>
<p>A character string indicating the name of the dimension to 
split.</p>
</td></tr>
<tr><td><code id="SplitDim_+3A_indices">indices</code></td>
<td>
<p>A vector of numeric indices or dates.</p>
</td></tr>
<tr><td><code id="SplitDim_+3A_freq">freq</code></td>
<td>
<p>A character string indicating the frequency: by 'day', 'month' and 
'year' or 'monthly' (by default). 'month' identifies months between 1 and 12 
independetly of the year they belong to, while 'monthly' differenciates 
months from different years. Parameter 'freq' can also be numeric indicating 
the length in which to subset the dimension.</p>
</td></tr>
<tr><td><code id="SplitDim_+3A_new_dim_name">new_dim_name</code></td>
<td>
<p>A character string indicating the name of the new 
dimension.</p>
</td></tr>
<tr><td><code id="SplitDim_+3A_dates">dates</code></td>
<td>
<p>An optional parameter containing an array of dates of class 
'POSIXct' with the corresponding time dimensions of 'data'. It is NULL 
by default.</p>
</td></tr>
<tr><td><code id="SplitDim_+3A_return_indices">return_indices</code></td>
<td>
<p>A logical value that if it is TRUE, the indices 
used in splitting the dimension will be returned. It is FALSE by default.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nuria Perez-Zanon, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- 1 : 20
dim(data) &lt;- c(time = 10, lat = 2)
indices &lt;- c(rep(1,5), rep(2,5))
new_data &lt;- SplitDim(data, indices = indices)
time &lt;- c(seq(ISOdate(1903, 1, 1), ISOdate(1903, 1, 4), "days"),
         seq(ISOdate(1903, 2, 1), ISOdate(1903, 2, 4), "days"),
         seq(ISOdate(1904, 1, 1), ISOdate(1904, 1, 2), "days"))
new_data &lt;- SplitDim(data, indices = time)
new_data &lt;- SplitDim(data, indices = time, freq = 'day')
new_data &lt;- SplitDim(data, indices = time, freq = 'month')
new_data &lt;- SplitDim(data, indices = time, freq = 'year')
</code></pre>

<hr>
<h2 id='training_analogs'>AEMET Training   
Training method (pre-downscaling) based on analogs: 
synoptic situations and significant predictors.</h2><span id='topic+training_analogs'></span>

<h3>Description</h3>

<p>This function caracterizes the synoptic situations in a past 
period based on low resolution reanalysis data (e.g, ERAInterim 1.5º x 1.5º) 
and an observational high resolution (HR) dataset (AEMET 5 km gridded daily 
precipitation and maximum and minimum temperature) (Peral et al., 2017)). 
The method uses three domains:
</p>

<ul>
<li><p>peninsular Spain and Balearic Islands domain (5 km resolution): HR domain
</p>
</li>
<li><p>synoptic domain (low resolution): it should be centered over Iberian   
Peninsula and cover enough extension to detect as much synoptic 
situations as possible.
</p>
</li>
<li><p>extended domain (low resolution): it is an extension of the synoptic 
domain. It is used for 'slp_ext' parameter (see 'slp_lon' and 'slp_lat' 
below).
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>training_analogs(
  pred,
  slp_ext,
  lon,
  lat,
  slp_lon,
  slp_lat,
  var,
  HR_path,
  tdates
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="training_analogs_+3A_pred">pred</code></td>
<td>
<p>List of matrix reanalysis data in a synoptic domain. The list has 
to contain reanalysis atmospheric variables (instantaneous 12h data) that 
must be indentify by parenthesis name. For precipitation:
</p>

<ul>
<li><p>u component of wind at 500 hPa (u500) in m/s
</p>
</li>
<li><p>v component of wind at 500 hPa (v500) in m/s
</p>
</li>
<li><p>temperature at 500 hPa (t500) in K
</p>
</li>
<li><p>temperature at 850 hPa (t850) in K
</p>
</li>
<li><p>temperature at 1000 hPa (t1000) in K
</p>
</li>
<li><p>geopotential height at 500 hPa (z500) in m
</p>
</li>
<li><p>geopotential height at 1000 hPa (z1000) in m
</p>
</li>
<li><p>sea level pressure (slp) in hPa
</p>
</li>
<li><p>specific humidity at 700 hPa (q700) in g/kg
</p>
</li></ul>

<p>For maximum and minimum temperature:
</p>

<ul>
<li><p>temperature at 1000 hPa (t1000) in K
</p>
</li>
<li><p>sea level pressure (slp) in hPa
</p>
</li></ul>

<p>All matrix must have [time,gridpoint] dimensions.
(time = number of training days, gridpoint = number of synoptic gridpoints).</p>
</td></tr>
<tr><td><code id="training_analogs_+3A_slp_ext">slp_ext</code></td>
<td>
<p>Matrix with atmospheric reanalysis sea level pressure
(instantaneous 12h data)(hPa). It has the same resolution as 'pred' parameter 
but with an extended domain. This domain contains extra degrees (most in the 
north and west part) compare to synoptic domain. The matrix must have 
[time,gridpoint] dimensions. (time = number of training days, 
gridpoint = number of extended gridpoints).</p>
</td></tr>
<tr><td><code id="training_analogs_+3A_lon">lon</code></td>
<td>
<p>Vector of the synoptic longitude (from (-180º) to 180º), 
The vector must go from west to east.</p>
</td></tr>
<tr><td><code id="training_analogs_+3A_lat">lat</code></td>
<td>
<p>Vector of the synoptic latitude. The vector must go from north to 
south.</p>
</td></tr>
<tr><td><code id="training_analogs_+3A_slp_lon">slp_lon</code></td>
<td>
<p>Vector of the extended longitude (from (-180º) to 180º).
The vector must go from west to east.</p>
</td></tr>
<tr><td><code id="training_analogs_+3A_slp_lat">slp_lat</code></td>
<td>
<p>Vector of the extended latitude. The vector must go from north 
to south.</p>
</td></tr>
<tr><td><code id="training_analogs_+3A_var">var</code></td>
<td>
<p>Variable name to downscale. There are two options: 'prec' for
precipitation and 'temp' for maximum and minimum temperature.</p>
</td></tr>
<tr><td><code id="training_analogs_+3A_hr_path">HR_path</code></td>
<td>
<p>Local path of HR observational files (maestro and pcp/tmx-tmn). 
For precipitation and temperature can be downloaded from the following link:
<a href="https://www.aemet.es/en/serviciosclimaticos/cambio_climat/datos_diarios?w=2">https://www.aemet.es/en/serviciosclimaticos/cambio_climat/datos_diarios?w=2</a> 
respetively. Maestro file (maestro_red_hr_SPAIN.txt) has gridpoint (nptos), 
longitude (lon), latitude (lat) and altitude (alt) in columns (vector 
structure). Data file (pcp/tmx/tmn_red_SPAIN_1951-201903.txt) includes 5km 
resolution spanish daily data (precipitation or maximum and minimum 
temperature from january 1951 to june 2020. See README file for more 
information. IMPORTANT!: HR observational period must be the same as for 
reanalysis variables. It is assumed that the training period is smaller than 
the HR original one (1951-2020), so it is needed to make a new ascii file 
with the new period and the same structure as original, specifying the 
training dates ('tdates' parameter) in the name (e.g. 
'pcp_red_SPAIN_19810101-19961231.txt' for '19810101-19961231' period).</p>
</td></tr>
<tr><td><code id="training_analogs_+3A_tdates">tdates</code></td>
<td>
<p>Training period dates in format YYYYMMDD(start)-YYYYMMDD(end) 
(e.g. 19810101-19961231).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix list (e.g. restrain) as a result of characterize the past 
synoptic situations and the significant predictors needed to downscale 
seasonal forecast variables. For precipitation the output includes:
</p>

<ul>
<li><p>'um': u component of geostrophic wind in all period (numeric matrix 
with [time, gridpoint] dimensions).
</p>
</li>
<li><p>'vm': v component of geostrophic wind in all period (numeric matrix 
with [time,gridpoint] dimensions).
</p>
</li>
<li><p>'nger': number of synoptic situations (integer).
</p>
</li>
<li><p>'gu92': u component of geostrophic wind for each synoptic situation 
(numeric matrix with [nger,gridpoint] dimensions).
</p>
</li>
<li><p>'gv92': v component of geostrophic wind for each synoptic situation 
(numeric matrix with [nger, gridpoint] dimensions).
</p>
</li>
<li><p>'gu52': u component of wind at 500 hPa for each synotic situation 
(numeric matrix with [nger, gridpoint] dimensions).
</p>
</li>
<li><p>'gv52': v component of wind at 500 hPa for each synotic situation 
(numeric matrix with [nger, gridpoint] dimensions).
</p>
</li>
<li><p>'neni': number of reference centers where predictors are calculated 
(integer).
</p>
</li>
<li><p>'vdmin': minimum distances between each HR gridpoint and the four 
nearest synoptic gridpoints (numeric matrix with [nptos,4] dimensions) 
(nptos = number of HR gridpoints).
</p>
</li>
<li><p>'vref': four nearest synoptic gridpoints to each HR gridpoint (integer 
matrix with [nptos, 4] dimensions).
</p>
</li>
<li><p>'ccm': multiple correlation coeficients (numeric matrix with [nger, nptos] 
dimensions) indices:
</p>

<ul>
<li><p>'lab_pred': numeric labels of selected predictors (integer matrix 
with [nger,nptos,11,1] dimensions).
</p>
</li>
<li><p>'cor_pred': partial correlation of selected predictors (numeric 
matrix with [nger,nptos,11,2] dimensions).
</p>
</li></ul>


</li></ul>

<p>For maximum and minimum temperature the output includes:
</p>

<ul>
<li><p>'um': u component of geostrophic wind in all training period (numeric 
matrix with [time,gridpoint] dimensions).
</p>
</li>
<li><p>'vm': v component of geostrophic wind in all training period (numeric 
matrix with [time,gridpoint] dimensions).
</p>
</li>
<li><p>'insol': insolation in all training period (numeric vector with [time] 
dimension).
</p>
</li>
<li><p>'neni': number of reference centers where predictors are calculated 
(integer).
</p>
</li>
<li><p>'vdmin': minimum distances between each HR gridpoint and the four 
nearest synoptic gridpoints (numeric matrix with [nptos,4] dimensions) 
(nptos = number of HR gridpoints).
</p>
</li>
<li><p>'vref': four nearest synoptic gridpoints to each HR gridpoint (integer 
matrix with [nptos,4] dimensions).
</p>
</li></ul>

<p>The output can directly use as argument to 'CST_AnalogsPredictors' function 
(e.g. resdowns &lt;- CST_AnalogsPredictors(...,restrain)).
</p>


<h3>Author(s)</h3>

<p>Marta Dominguez Alonso - AEMET, <a href="mailto:mdomingueza@aemet.es">mdomingueza@aemet.es</a>
</p>
<p>Nuria Perez-Zanon - BSC, <a href="mailto:nuria.perez@bsc.es">nuria.perez@bsc.es</a>
</p>

<hr>
<h2 id='WeatherRegime'>Function for Calculating the Cluster analysis</h2><span id='topic+WeatherRegime'></span>

<h3>Description</h3>

<p>This function computes the weather regimes from a cluster analysis.
It can be applied over the dataset with dimensions c(year/month, month/day, 
lon, lat), or by using PCs obtained from the application of the EOFs analysis 
to filter the dataset. The cluster analysis can be performed with the 
traditional k-means or those methods included in the hclust (stats package).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WeatherRegime(
  data,
  ncenters = NULL,
  EOFs = TRUE,
  neofs = 30,
  varThreshold = NULL,
  lon = NULL,
  lat = NULL,
  method = "kmeans",
  iter.max = 100,
  nstart = 30,
  ncores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WeatherRegime_+3A_data">data</code></td>
<td>
<p>An array containing anomalies with named dimensions with at least 
start date 'sdate', forecast time 'ftime', latitude 'lat' and longitude 
'lon'.</p>
</td></tr>
<tr><td><code id="WeatherRegime_+3A_ncenters">ncenters</code></td>
<td>
<p>Number of clusters to be calculated with the clustering 
function.</p>
</td></tr>
<tr><td><code id="WeatherRegime_+3A_eofs">EOFs</code></td>
<td>
<p>Whether to compute the EOFs (default = 'TRUE') or not (FALSE) to 
filter the data.</p>
</td></tr>
<tr><td><code id="WeatherRegime_+3A_neofs">neofs</code></td>
<td>
<p>Number of modes to be kept only if EOFs = TRUE has been selected. 
(default = 30).</p>
</td></tr>
<tr><td><code id="WeatherRegime_+3A_varthreshold">varThreshold</code></td>
<td>
<p>Value with the percentage of variance to be explained by 
the PCs. Only sufficient PCs to explain this much variance will be used in 
the clustering.</p>
</td></tr>
<tr><td><code id="WeatherRegime_+3A_lon">lon</code></td>
<td>
<p>Vector of longitudes.</p>
</td></tr>
<tr><td><code id="WeatherRegime_+3A_lat">lat</code></td>
<td>
<p>Vector of latitudes.</p>
</td></tr>
<tr><td><code id="WeatherRegime_+3A_method">method</code></td>
<td>
<p>Different options to estimate the clusters. The most traditional 
approach is the k-means analysis (default=’kmeans’) but the function also 
support the different methods included in the hclust . These methods are:
&quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot; (= UPGMA), &quot;mcquitty&quot; 
(= WPGMA), &quot;median&quot; (= WPGMC) or &quot;centroid&quot; (= UPGMC). For more details 
about these methods see the hclust function documentation included in the 
stats package.</p>
</td></tr>
<tr><td><code id="WeatherRegime_+3A_iter.max">iter.max</code></td>
<td>
<p>Parameter to select the maximum number of iterations allowed 
(Only if method = 'kmeans' is selected).</p>
</td></tr>
<tr><td><code id="WeatherRegime_+3A_nstart">nstart</code></td>
<td>
<p>Parameter for the cluster analysis determining how many random 
sets to choose (Only if method='kmeans' is selected).</p>
</td></tr>
<tr><td><code id="WeatherRegime_+3A_ncores">ncores</code></td>
<td>
<p>The number of multicore threads to use for parallel computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements <code>$composite</code> (array with at least 3-d ('lat', 
'lon', 'cluster') containing the composites k = 1,..,K for case (*1) or only k = 1 
for any specific cluster, i.e., case (*2)), <code>pvalue</code> (array with at least 
3-d ('lat','lon','cluster') with the pvalue of the composites obtained through 
a t-test that accounts for the serial dependence of the data with the same 
structure as Composite.), <code>cluster</code> (A matrix or vector with integers 
(from 1:k) indicating the cluster to which each time step is allocated.),
<code>persistence</code> (Percentage of days in a month/season before a cluster is 
replaced for a new one (only if method=’kmeans’ has been selected.)),
<code>frequency</code> (Percentage of days in a month/season belonging to each 
cluster (only if method=’kmeans’ has been selected).),
</p>


<h3>Author(s)</h3>

<p>Verónica Torralba - BSC, <a href="mailto:veronica.torralba@bsc.es">veronica.torralba@bsc.es</a>
</p>


<h3>References</h3>

<p>Cortesi, N., V., Torralba, N., González-Reviriego, A., Soret, and 
F.J., Doblas-Reyes (2019). Characterization of European wind speed variability 
using weather regimes. Climate Dynamics,53, 4961–4976, 
doi: <a href="https://doi.org/10.1007/s00382-019-04839-5">10.1007/s00382-019-04839-5</a>.
</p>
<p>Torralba, V. (2019) Seasonal climate prediction for the wind 
energy sector: methods and tools for the development of a climate service. 
Thesis. Available online: <a href="https://eprints.ucm.es/56841/">https://eprints.ucm.es/56841/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- array(abs(rnorm(1280, 283.7, 6)), dim = c(dataset = 2, member = 2,  
                                                 sdate = 3, ftime = 3, 
                                                 lat = 4, lon = 4))
lat &lt;- seq(47, 44)
res &lt;- WeatherRegime(data = data, lat = lat,  
                    EOFs = FALSE, ncenters = 4)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
