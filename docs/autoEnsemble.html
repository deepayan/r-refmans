<!DOCTYPE html><html lang="en"><head><title>Help for package autoEnsemble</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {autoEnsemble}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#autoEnsemble'><p>Automatically Trains H2O Models and Builds a Stacked Ensemble Model</p></a></li>
<li><a href='#ensemble'><p>Builds Stacked Ensemble Model from H2O Models</p></a></li>
<li><a href='#evaluate'><p>Evaluate H2O Model(s) Performance</p></a></li>
<li><a href='#h2o.get_ids'><p>h2o.get_ids</p></a></li>
<li><a href='#modelSelection'><p>Selects Diverse Top-Performing Models for Stacking an Ensemble Model</p></a></li>
<li><a href='#stopping_criteria'><p>Stopping Criteria for Ending the Search</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Automated Stacked Ensemble Classifier for Severe Class Imbalance</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0),</td>
</tr>
<tr>
<td>Description:</td>
<td>A stacking solution for modeling imbalanced and severely skewed data. It automates the process of building homogeneous or heterogeneous stacked ensemble models by selecting "best" models according to different criteria. In doing so, it strategically searches for and selects diverse, high-performing base-learners to construct ensemble models optimized for skewed data. This package is particularly useful for addressing class imbalance in datasets, ensuring robust and effective model outcomes through advanced ensemble strategies which aim to stabilize the model, reduce its overfitting, and further improve its generalizability.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>h2o (&ge; 3.34.0.0), h2otools (&ge; 0.3), curl (&ge; 4.3.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/haghish/autoEnsemble">https://github.com/haghish/autoEnsemble</a>,
<a href="https://www.sv.uio.no/psi/english/people/academic/haghish/">https://www.sv.uio.no/psi/english/people/academic/haghish/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/haghish/autoEnsemble/issues">https://github.com/haghish/autoEnsemble/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-20 09:48:25 UTC; haghish</td>
</tr>
<tr>
<td>Author:</td>
<td>E. F. Haghish [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>E. F. Haghish &lt;haghish@hotmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-20 11:50:13 UTC</td>
</tr>
</table>
<hr>
<h2 id='autoEnsemble'>Automatically Trains H2O Models and Builds a Stacked Ensemble Model</h2><span id='topic+autoEnsemble'></span>

<h3>Description</h3>

<p>Automatically trains various algorithms to build base-learners and then
automatically creates a stacked ensemble model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoEnsemble(
  x,
  y,
  training_frame,
  validation_frame = NULL,
  nfolds = 10,
  balance_classes = TRUE,
  max_runtime_secs = NULL,
  max_runtime_secs_per_model = NULL,
  max_models = NULL,
  sort_metric = "AUCPR",
  include_algos = c("GLM", "DeepLearning", "DRF", "XGBoost", "GBM"),
  save_models = FALSE,
  directory = paste("autoEnsemble", format(Sys.time(), "%d-%m-%y-%H:%M")),
  ...,
  newdata = NULL,
  family = "binary",
  strategy = c("search"),
  model_selection_criteria = c("auc", "aucpr", "mcc", "f2"),
  min_improvement = 1e-05,
  max = NULL,
  top_rank = seq(0.01, 0.99, 0.01),
  stop_rounds = 3,
  reset_stop_rounds = TRUE,
  stop_metric = "auc",
  seed = -1,
  verbatim = FALSE,
  startH2O = FALSE,
  nthreads = NULL,
  max_mem_size = NULL,
  min_mem_size = NULL,
  ignore_config = FALSE,
  bind_to_localhost = FALSE,
  insecure = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="autoEnsemble_+3A_x">x</code></td>
<td>
<p>Vector. Predictor column names or indices.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_y">y</code></td>
<td>
<p>Character. The response column name or index.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_training_frame">training_frame</code></td>
<td>
<p>An H2OFrame containing the training data.
Default is <code>h2o.getFrame("hmda.train.hex")</code>.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_validation_frame">validation_frame</code></td>
<td>
<p>An H2OFrame for early stopping. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_nfolds">nfolds</code></td>
<td>
<p>Integer. Number of folds for cross-validation.
Default is 10.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_balance_classes">balance_classes</code></td>
<td>
<p>Logical. Specify whether to oversample the minority
classes to balance the class distribution; only applicable to classification</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_max_runtime_secs">max_runtime_secs</code></td>
<td>
<p>Integer. This argument specifies the maximum time that
the AutoML process will run for in seconds.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_max_runtime_secs_per_model">max_runtime_secs_per_model</code></td>
<td>
<p>Maximum runtime in seconds dedicated to each
individual model training process.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_max_models">max_models</code></td>
<td>
<p>Maximum number of models to build in the AutoML training (passed to autoML)</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_sort_metric">sort_metric</code></td>
<td>
<p>Metric to sort the leaderboard by (passed to autoML).
For binomial classification
choose between &quot;AUC&quot;, &quot;AUCPR&quot;, &quot;logloss&quot;, &quot;mean_per_class_error&quot;,
&quot;RMSE&quot;, &quot;MSE&quot;. For regression choose between &quot;mean_residual_deviance&quot;,
&quot;RMSE&quot;, &quot;MSE&quot;, &quot;MAE&quot;, and &quot;RMSLE&quot;. For multinomial classification choose
between &quot;mean_per_class_error&quot;, &quot;logloss&quot;, &quot;RMSE&quot;, &quot;MSE&quot;. Default is
&quot;AUTO&quot;. If set to &quot;AUTO&quot;, then &quot;AUC&quot; will be used for binomial classification,
&quot;mean_per_class_error&quot; for multinomial classification, and
&quot;mean_residual_deviance&quot; for regression.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_include_algos">include_algos</code></td>
<td>
<p>Vector of character strings naming the algorithms to
restrict to during the model-building phase. this argument
is passed to autoML.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_save_models">save_models</code></td>
<td>
<p>Logical. if TRUE, the models trained will be stored locally</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_directory">directory</code></td>
<td>
<p>path to a local directory to store the trained models</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_...">...</code></td>
<td>
<p>parameters to be passed to autoML algorithm in h2o package</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_newdata">newdata</code></td>
<td>
<p>h2o frame (data.frame). the data.frame must be already uploaded
on h2o server (cloud). when specified, this dataset will be used
for evaluating the models. if not specified, model performance
on the training dataset will be reported.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_family">family</code></td>
<td>
<p>model family. currently only <code>"binary"</code> classification models
are supported.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_strategy">strategy</code></td>
<td>
<p>character. the current available strategies are <code>"search"</code>
(default) and <code>"top"</code>. The <code>"search"</code> strategy searches
for the best combination of top-performing diverse models
whereas the <code>"top"</code> strategy is more simplified and just
combines the specified of top-performing diverse models without
examining the possibility of improving the model by searching for
larger number of models that can further improve the model. generally,
the <code>"search"</code> strategy is preferable, unless the computation
runtime is too large and optimization is not possible.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_model_selection_criteria">model_selection_criteria</code></td>
<td>
<p>character, specifying the performance metrics that
should be taken into consideration for model selection. the default are
<code>"c('auc', 'aucpr', 'mcc', 'f2')"</code>. other possible criteria are
<code>"'f1point5', 'f3', 'f4', 'f5', 'kappa', 'mean_per_class_error', 'gini', 'accuracy'"</code>,
which are also provided by the <code>"evaluate"</code> function.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_min_improvement">min_improvement</code></td>
<td>
<p>numeric. specifies the minimum improvement in model
evaluation metric to qualify further optimization search.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_max">max</code></td>
<td>
<p>integer. specifies maximum number of models for each criteria to be extracted. the
default value is the <code>"top_rank"</code> percentage for each model selection
criteria.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_top_rank">top_rank</code></td>
<td>
<p>numeric vector. specifies percentage of the top models taht
should be selected. if the strategy is <code>"search"</code>, the
algorithm searches for the best best combination of the models
from top ranked models to the bottom. however, if the strategy
is <code>"top"</code>, only the first value of the vector is used
(default value is top 1%).</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_stop_rounds">stop_rounds</code></td>
<td>
<p>integer. number of stoping rounds, in case the model stops
improving</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_reset_stop_rounds">reset_stop_rounds</code></td>
<td>
<p>logical. if TRUE, everytime the model improves the
stopping rounds penalty is resets to 0.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_stop_metric">stop_metric</code></td>
<td>
<p>character. model stopping metric. the default is <code>"auc"</code>,
but <code>"aucpr"</code> and <code>"mcc"</code> are also available.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_seed">seed</code></td>
<td>
<p>random seed (recommended)</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_verbatim">verbatim</code></td>
<td>
<p>logical. if TRUE, it reports additional information about the
progress of the model training, particularly used for debugging.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_starth2o">startH2O</code></td>
<td>
<p>Logical. if TRUE, h2o server will be initiated.</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_nthreads">nthreads</code></td>
<td>
<p>arguments to be passed to h2o.init()</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_max_mem_size">max_mem_size</code></td>
<td>
<p>arguments to be passed to h2o.init()</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_min_mem_size">min_mem_size</code></td>
<td>
<p>arguments to be passed to h2o.init()</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_ignore_config">ignore_config</code></td>
<td>
<p>arguments to be passed to h2o.init()</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_bind_to_localhost">bind_to_localhost</code></td>
<td>
<p>arguments to be passed to h2o.init()</p>
</td></tr>
<tr><td><code id="autoEnsemble_+3A_insecure">insecure</code></td>
<td>
<p>arguments to be passed to h2o.init()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list including the ensemble model and the top-rank models that were
used in the model
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
# load the required libraries for building the base-learners and the ensemble models
library(h2o)
library(autoEnsemble)

# initiate the h2o server
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)

# upload data to h2o cloud
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)

### H2O provides 2 types of grid search for tuning the models, which are
### AutoML and Grid. Below, I tune 2 set of model grids and use them both
### for building the ensemble, just to set an example ...

#######################################################
### PREPARE AutoML Grid (takes a couple of minutes)
#######################################################
# run AutoML to tune various models (GLM, GBM, XGBoost, DRF, DeepLearning) for 120 seconds
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 120,
                 include_algos=c("DRF","GLM", "XGBoost", "GBM", "DeepLearning"),

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

#######################################################
### PREPARE H2O Grid (takes a couple of minutes)
#######################################################
# make sure equal number of "nfolds" is specified for different grids
grid &lt;- h2o.grid(algorithm = "gbm", y = y, training_frame = prostate,
                 hyper_params = list(ntrees = seq(1,50,1)),
                 grid_id = "ensemble_grid",

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, fold_assignment = "Modulo", nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

#######################################################
### PREPARE ENSEMBLE MODEL
#######################################################

### get the models' IDs from the AutoML and grid searches.
### this is all that is needed before building the ensemble,
### i.e., to specify the model IDs that should be evaluated.

ids    &lt;- c(h2o.get_ids(aml), h2o.get_ids(grid))
top    &lt;- ensemble(models = ids, training_frame = prostate, strategy = "top")
search &lt;- ensemble(models = ids, training_frame = prostate, strategy = "search")

#######################################################
### EVALUATE THE MODELS
#######################################################
h2o.auc(aml@leader)                          # best model identified by h2o.automl
h2o.auc(h2o.getModel(grid@model_ids[[1]]))   # best model identified by grid search
h2o.auc(top$model).                          # ensemble model with 'top' search strategy
h2o.auc(search$model).                       # ensemble model with 'search' search strategy


## End(Not run)
</code></pre>

<hr>
<h2 id='ensemble'>Builds Stacked Ensemble Model from H2O Models</h2><span id='topic+ensemble'></span>

<h3>Description</h3>

<p>Multiple trained H2O models are stacked to create an ensemble
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ensemble(
  models,
  training_frame,
  newdata = NULL,
  family = "binary",
  strategy = c("search"),
  model_selection_criteria = c("auc", "aucpr", "mcc", "f2"),
  min_improvement = 1e-05,
  max = NULL,
  top_rank = seq(0.01, 0.99, 0.01),
  stop_rounds = 3,
  reset_stop_rounds = TRUE,
  stop_metric = "auc",
  seed = -1,
  verbatim = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ensemble_+3A_models">models</code></td>
<td>
<p>H2O search grid or AutoML grid or a character vector of H2O model IDs.
the <code>"h2o.get_ids"</code> function from <code>"h2otools"</code> can
retrieve the IDs from grids.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_training_frame">training_frame</code></td>
<td>
<p>h2o training frame (data.frame) for model training</p>
</td></tr>
<tr><td><code id="ensemble_+3A_newdata">newdata</code></td>
<td>
<p>h2o frame (data.frame). the data.frame must be already uploaded
on h2o server (cloud). when specified, this dataset will be used
for evaluating the models. if not specified, model performance
on the training dataset will be reported.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_family">family</code></td>
<td>
<p>model family. currently only <code>"binary"</code> classification models
are supported.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_strategy">strategy</code></td>
<td>
<p>character. the current available strategies are <code>"search"</code>
(default) and <code>"top"</code>. The <code>"search"</code> strategy searches
for the best combination of top-performing diverse models
whereas the <code>"top"</code> strategy is more simplified and just
combines the specified of top-performing diverse models without
examining the possibility of improving the model by searching for
larger number of models that can further improve the model. generally,
the <code>"search"</code> strategy is preferable, unless the computation
runtime is too large and optimization is not possible.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_model_selection_criteria">model_selection_criteria</code></td>
<td>
<p>character, specifying the performance metrics that
should be taken into consideration for model selection. the default are
<code>"c('auc', 'aucpr', 'mcc', 'f2')"</code>. other possible criteria are
<code>"'f1point5', 'f3', 'f4', 'f5', 'kappa', 'mean_per_class_error', 'gini', 'accuracy'"</code>,
which are also provided by the <code>"evaluate"</code> function.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_min_improvement">min_improvement</code></td>
<td>
<p>numeric. specifies the minimum improvement in model
evaluation metric to qualify further optimization search.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_max">max</code></td>
<td>
<p>integer. specifies maximum number of models for each criteria to be extracted. the
default value is the <code>"top_rank"</code> percentage for each model selection
criteria.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_top_rank">top_rank</code></td>
<td>
<p>numeric vector. specifies percentage of the top models taht
should be selected. if the strategy is <code>"search"</code>, the
algorithm searches for the best best combination of the models
from top ranked models to the bottom. however, if the strategy
is <code>"top"</code>, only the first value of the vector is used
(default value is top 1%).</p>
</td></tr>
<tr><td><code id="ensemble_+3A_stop_rounds">stop_rounds</code></td>
<td>
<p>integer. number of stoping rounds, in case the model stops
improving</p>
</td></tr>
<tr><td><code id="ensemble_+3A_reset_stop_rounds">reset_stop_rounds</code></td>
<td>
<p>logical. if TRUE, every time the model improves the
stopping rounds penalty is resets to 0.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_stop_metric">stop_metric</code></td>
<td>
<p>character. model stopping metric. the default is <code>"auc"</code>,
but <code>"aucpr"</code> and <code>"mcc"</code> are also available.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_seed">seed</code></td>
<td>
<p>random seed (recommended)</p>
</td></tr>
<tr><td><code id="ensemble_+3A_verbatim">verbatim</code></td>
<td>
<p>logical. if TRUE, it reports additional information about the
progress of the model training, particularly used for debugging.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list including the ensemble model and the top-rank models that were
used in the model
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
# load the required libraries for building the base-learners and the ensemble models
library(h2o)
library(autoEnsemble)

# initiate the h2o server
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)

# upload data to h2o cloud
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)

### H2O provides 2 types of grid search for tuning the models, which are
### AutoML and Grid. Below, I tune 2 set of model grids and use them both
### for building the ensemble, just to set an example ...

#######################################################
### PREPARE AutoML Grid (takes a couple of minutes)
#######################################################
# run AutoML to tune various models (GLM, GBM, XGBoost, DRF, DeepLearning) for 120 seconds
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 120,
                 include_algos=c("DRF","GLM", "XGBoost", "GBM", "DeepLearning"),

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

#######################################################
### PREPARE H2O Grid (takes a couple of minutes)
#######################################################
# make sure equal number of "nfolds" is specified for different grids
grid &lt;- h2o.grid(algorithm = "gbm", y = y, training_frame = prostate,
                 hyper_params = list(ntrees = seq(1,50,1)),
                 grid_id = "ensemble_grid",

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, fold_assignment = "Modulo", nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

#######################################################
### PREPARE ENSEMBLE MODEL
#######################################################

### get the models' IDs from the AutoML and grid searches.
### this is all that is needed before building the ensemble,
### i.e., to specify the model IDs that should be evaluated.

ids    &lt;- c(h2o.get_ids(aml), h2o.get_ids(grid))
top    &lt;- ensemble(models = ids, training_frame = prostate, strategy = "top")
search &lt;- ensemble(models = ids, training_frame = prostate, strategy = "search")

#######################################################
### EVALUATE THE MODELS
#######################################################
h2o.auc(aml@leader)                          # best model identified by h2o.automl
h2o.auc(h2o.getModel(grid@model_ids[[1]]))   # best model identified by grid search
h2o.auc(top$model).                          # ensemble model with 'top' search strategy
h2o.auc(search$model).                       # ensemble model with 'search' search strategy


## End(Not run)
</code></pre>

<hr>
<h2 id='evaluate'>Evaluate H2O Model(s) Performance</h2><span id='topic+evaluate'></span>

<h3>Description</h3>

<p>Multiple model performance metrics are computed for each model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evaluate(id, newdata = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evaluate_+3A_id">id</code></td>
<td>
<p>a character vector of H2O model IDs retrieved from H2O Grid search
or AutoML random search. the <code>"h2o.get_ids"</code> function from
<code>"h2otools"</code> can retrieve the IDs from grids.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_newdata">newdata</code></td>
<td>
<p>h2o frame (data.frame). the data.frame must be already uploaded
on h2o server (cloud). when specified, this dataset will be used
for evaluating the models. if not specified, model performance
on the training dataset will be reported.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_...">...</code></td>
<td>
<p>arguments to be passed to <code>"h2o.performance"</code> from H2O package</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame of various model performance metrics for each model
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(h2o)
library(h2otools) #for h2o.get_ids() function
library(autoEnsemble)

# initiate the H2O server to train a grid of models
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)

# Run a grid search or AutoML search
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 30,
                  seed = 2023, nfolds = 10, keep_cross_validation_predictions = TRUE)

# get the model IDs from the H2O Grid search or H2O AutoML Grid
ids &lt;- h2otools::h2o.get_ids(aml)

# evaluate all the models and return a dataframe
evals &lt;- evaluate(id = ids)

## End(Not run)
</code></pre>

<hr>
<h2 id='h2o.get_ids'>h2o.get_ids</h2><span id='topic+h2o.get_ids'></span>

<h3>Description</h3>

<p>extracts the model IDs from H2O AutoML object or H2O grid
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h2o.get_ids(automl)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="h2o.get_ids_+3A_automl">automl</code></td>
<td>
<p>a h2o <code>"AutoML"</code> grid object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of trained models' names (IDs)
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(h2o)
library(autoEnsemble)
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 30)

# get the model IDs
ids &lt;- h2o.get_ids(aml)

## End(Not run)
</code></pre>

<hr>
<h2 id='modelSelection'>Selects Diverse Top-Performing Models for Stacking an Ensemble Model</h2><span id='topic+modelSelection'></span>

<h3>Description</h3>

<p>Multiple model performance metrics are computed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelSelection(
  eval,
  family = "binary",
  top_rank = 0.01,
  max = NULL,
  model_selection_criteria = c("auc", "aucpr", "mcc", "f2")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="modelSelection_+3A_eval">eval</code></td>
<td>
<p>an object of class <code>"ensemble.eval"</code> which is provided
by 'evaluate' function. this object is a data.frame, including
several performance metrics for the evaluated models.</p>
</td></tr>
<tr><td><code id="modelSelection_+3A_family">family</code></td>
<td>
<p>model family. currently only <code>"binary"</code> classification models
are supported.</p>
</td></tr>
<tr><td><code id="modelSelection_+3A_top_rank">top_rank</code></td>
<td>
<p>numeric. what percentage of the top model should be selected?
the default value is top 1% models.</p>
</td></tr>
<tr><td><code id="modelSelection_+3A_max">max</code></td>
<td>
<p>integer. specifies maximum number of models for each criteria to be extracted. the
default value is the <code>"top_rank"</code> percentage for each model selection
criteria.</p>
</td></tr>
<tr><td><code id="modelSelection_+3A_model_selection_criteria">model_selection_criteria</code></td>
<td>
<p>character, specifying the performance metrics that
should be taken into consideration for model selection. the default are
<code>"c('auc', 'aucpr', 'mcc', 'f2')"</code>. other possible criteria are
<code>"'f1point5', 'f3', 'f4', 'f5', 'kappa', 'mean_per_class_error', 'gini', 'accuracy'"</code>,
which are also provided by the <code>"evaluate"</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of F-Measures for different thresholds or the highest F-Measure value
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(h2o)
library(h2otools) #for h2o.get_ids() function
library(h2oEnsemble)

# initiate the H2O server to train a grid of models
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)

# Run a grid search or AutoML search
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 30,
                  seed = 2023, nfolds = 10, keep_cross_validation_predictions = TRUE)

# get the model IDs from the H2O Grid search or H2O AutoML Grid
ids &lt;- h2otools::h2o.get_ids(aml)

# evaluate all the models and return a dataframe
evals &lt;- evaluate(id = ids)

# perform model selection (up to top 10% of each criteria)
select &lt;- modelSelection(eval = evals, top_rank = 0.1))

## End(Not run)
</code></pre>

<hr>
<h2 id='stopping_criteria'>Stopping Criteria for Ending the Search</h2><span id='topic+stopping_criteria'></span>

<h3>Description</h3>

<p>Defines criteria for ending the optimization search
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stopping_criteria(
  df,
  round,
  stop,
  min_improvement,
  stop_rounds = 3,
  reset_stop_rounds = TRUE,
  stop_metric = "auc"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stopping_criteria_+3A_df">df</code></td>
<td>
<p>data.frame. includes the metrics of ensemblem model performance</p>
</td></tr>
<tr><td><code id="stopping_criteria_+3A_round">round</code></td>
<td>
<p>integer. the current round of optimization</p>
</td></tr>
<tr><td><code id="stopping_criteria_+3A_stop">stop</code></td>
<td>
<p>integer. current round of stopping penalty</p>
</td></tr>
<tr><td><code id="stopping_criteria_+3A_min_improvement">min_improvement</code></td>
<td>
<p>numeric. specifies the minimum improvement in model
evaluation metric to qualify further optimization search.</p>
</td></tr>
<tr><td><code id="stopping_criteria_+3A_stop_rounds">stop_rounds</code></td>
<td>
<p>integer. number of stoping rounds, in case the model stops
improving</p>
</td></tr>
<tr><td><code id="stopping_criteria_+3A_reset_stop_rounds">reset_stop_rounds</code></td>
<td>
<p>logical. if TRUE, everytime the model improves the
stopping rounds penalty is resets to 0.</p>
</td></tr>
<tr><td><code id="stopping_criteria_+3A_stop_metric">stop_metric</code></td>
<td>
<p>character. model stopping metric. the default is <code>"auc"</code>,
but <code>"aucpr"</code> and <code>"mcc"</code> are also available.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of F-Measures for different thresholds or the highest F-Measure value
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
