<!DOCTYPE html><html><head><title>Help for package amap</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {amap}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#acp'><p>Principal component analysis</p></a></li>
<li><a href='#acpgen'><p>Generalised principal component analysis</p></a></li>
<li><a href='#acprob'><p>Robust principal component analysis</p></a></li>
<li><a href='#afc'><p>Correspondance factorial analysis.</p></a></li>
<li><a href='#burt'><p>Compute burt table from a factor dataframe.</p></a></li>
<li><a href='#diss'><p>Compute a dissimilarity matrix</p></a></li>
<li><a href='#Dist'><p>Distance Matrix Computation</p></a></li>
<li><a href='#hcluster'><p>Hierarchical Clustering</p></a></li>
<li><a href='#Kmeans'>
<p>K-Means Clustering</p></a></li>
<li><a href='#lubisch'><p>Dataset Lubischew</p></a></li>
<li><a href='#plot'><p>Graphics for Principal component Analysis</p></a></li>
<li><a href='#pop'><p>Optimal Partition (classification).</p></a></li>
<li><a href='#VarRob'><p>Robust variance</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.8-19</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-10-25</td>
</tr>
<tr>
<td>Title:</td>
<td>Another Multidimensional Analysis Package</td>
</tr>
<tr>
<td>Author:</td>
<td>Antoine Lucas</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Antoine Lucas &lt;antoinelucas@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>Biobase</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools for Clustering and Principal Component Analysis
        (With robust methods, and parallelized functions).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL]</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-10-27 20:39:27 UTC; antoine</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-10-28 06:55:22 UTC</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
</table>
<hr>
<h2 id='acp'>Principal component analysis</h2><span id='topic+acp'></span><span id='topic+pca'></span><span id='topic+print.acp'></span>

<h3>Description</h3>

<p>Principal component analysis</p>


<h3>Usage</h3>

<pre><code class='language-R'>acp(x,center=TRUE,reduce=TRUE,wI=rep(1,nrow(x)),wV=rep(1,ncol(x)))
pca(x,center=TRUE,reduce=TRUE,wI=rep(1,nrow(x)),wV=rep(1,ncol(x)))
## S3 method for class 'acp'
print(x, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="acp_+3A_x">x</code></td>
<td>
<p>Matrix  / data frame</p>
</td></tr>
<tr><td><code id="acp_+3A_center">center</code></td>
<td>
<p>a logical value indicating whether we center data</p>
</td></tr>
<tr><td><code id="acp_+3A_reduce">reduce</code></td>
<td>
<p>a logical value indicating whether we &quot;reduce&quot; data i.e. 
divide each column by standard deviation</p>
</td></tr>
<tr><td><code id="acp_+3A_wi">wI</code>, <code id="acp_+3A_wv">wV</code></td>
<td>
<p>weigth vector for individuals / variables</p>
</td></tr>
<tr><td><code id="acp_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function offer a variant of <code><a href="stats.html#topic+princomp">princomp</a></code> and
<code><a href="stats.html#topic+prcomp">prcomp</a></code> functions, with a slightly different
graphic representation (see <code><a href="#topic+plot.acp">plot.acp</a></code>).
</p>


<h3>Value</h3>

<p>An object of class <b>acp</b> 
The object is a list with components:
</p>
<table>
<tr><td><code>sdev</code></td>
<td>
<p>the standard deviations of the principal components.</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>the matrix of variable loadings (i.e., a matrix
whose columns contain the eigenvectors).  This is of class
<code>"loadings"</code>: see <code><a href="stats.html#topic+loadings">loadings</a></code> for its <code>print</code>
method.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>if <code>scores = TRUE</code>, the scores of the supplied
data on the principal components.</p>
</td></tr>
<tr><td><code>eig</code></td>
<td>
<p>Eigen values</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Antoine Lucas</p>


<h3>See Also</h3>

<p><a href="#topic+plot.acp">plot.acp</a>,<a href="#topic+acpgen">acpgen</a>, <a href="stats.html#topic+princomp">princomp</a> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lubisch)
lubisch &lt;- lubisch[,-c(1,8)]
p &lt;- acp(lubisch)
plot(p)
</code></pre>

<hr>
<h2 id='acpgen'>Generalised principal component analysis</h2><span id='topic+acpgen'></span><span id='topic+K'></span><span id='topic+W'></span>

<h3>Description</h3>

<p>Generalised principal component analysis</p>


<h3>Usage</h3>

<pre><code class='language-R'>acpgen(x,h1,h2,center=TRUE,reduce=TRUE,kernel="gaussien")
K(u,kernel="gaussien")
W(x,h,D=NULL,kernel="gaussien")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="acpgen_+3A_x">x</code></td>
<td>
<p>Matrix or data frame</p>
</td></tr>
<tr><td><code id="acpgen_+3A_h">h</code></td>
<td>
<p>Scalar: bandwidth of the Kernel</p>
</td></tr>
<tr><td><code id="acpgen_+3A_h1">h1</code></td>
<td>
<p>Scalar: bandwidth of the Kernel for W</p>
</td></tr>
<tr><td><code id="acpgen_+3A_h2">h2</code></td>
<td>
<p>Scalar: bandwidth of the Kernel for U</p>
</td></tr>
<tr><td><code id="acpgen_+3A_kernel">kernel</code></td>
<td>
<p>The kernel used. This must be one of '&quot;gaussien&quot;', 
'&quot;quartic&quot;', '&quot;triweight&quot;', '&quot;epanechikov&quot;' , 
'&quot;cosinus&quot;' or '&quot;uniform&quot;' </p>
</td></tr> 
<tr><td><code id="acpgen_+3A_center">center</code></td>
<td>
<p>A logical value indicating whether we center data</p>
</td></tr>
<tr><td><code id="acpgen_+3A_reduce">reduce</code></td>
<td>
<p>A logical value indicating whether we &quot;reduce&quot; data i.e. 
divide each column by standard deviation</p>
</td></tr>
<tr><td><code id="acpgen_+3A_d">D</code></td>
<td>
<p>A product scalar matrix / une matrice de produit scalaire</p>
</td></tr>
<tr><td><code id="acpgen_+3A_u">u</code></td>
<td>
<p>Vector</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>acpgen</code> compute generalised pca. i.e. spectral analysis of 
<code class="reqn">U_n . W_n^{-1}</code>, and project <code class="reqn">X_i</code> with
<code class="reqn">W_n^{-1}</code> on the principal vector sub-spaces.
</p>
<p><code class="reqn">X_i</code> a column vector of <code class="reqn">p</code> variables of individu  <code class="reqn">i</code> 
(input data)
</p>
<p><code>W</code> compute estimation of noise in the variance. 
</p>
<p style="text-align: center;"><code class="reqn">W_n=\frac{\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}K(||X_i-X_j||_{V_n^{-1}}/h)(X_i-X_j)(X_i-X_j)'}{\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}K(||X_i-X_j||_{V_n^{-1}}/h)}</code>
</p>

<p>with <code class="reqn">V_n</code> variance estimation;
</p>
<p><code>U</code> compute robust variance. <code class="reqn">U_n^{-1} = S_n^{-1} - 1/h V_n^{-1}</code>
</p>
<p style="text-align: center;"><code class="reqn">S_n=\frac{\sum_{i=1}^{n}K(||X_i||_{V_n^{-1}}/h)(X_i-\mu_n)(X_i-\mu_n)'}{\sum_{i=1}^nK(||X_i||_{V_n^{-1}}/h)}</code>
</p>

<p>with <code class="reqn">\mu_n</code> estimator of the mean.
</p>
<p><code>K</code> compute   kernel, i.e.
</p>
<p>gaussien: </p>
<p style="text-align: center;"><code class="reqn">\frac{1}{\sqrt{2\pi}}  e^{-u^2/2}</code>
</p>

<p>quartic: </p>
<p style="text-align: center;"><code class="reqn">\frac{15}{16}(1-u^2)^2 I_{|u|\leq 1}  </code>
</p>

<p>triweight: </p>
<p style="text-align: center;"><code class="reqn">\frac{35}{32}(1-u^2)^3 I_{|u|\leq 1}  </code>
</p>

<p>epanechikov: </p>
<p style="text-align: center;"><code class="reqn">\frac{3}{4}(1-u^2) I_{|u|\leq 1}  </code>
</p>

<p>cosinus: </p>
<p style="text-align: center;"><code class="reqn">\frac{\pi}{4}\cos(\frac{\pi}{2}u) I_{|u|\leq 1}  </code>
</p>



<h3>Value</h3>

<p>An object of class <b>acp</b> 
The object is a list with components:
</p>
<table>
<tr><td><code>sdev</code></td>
<td>
<p>the standard deviations of the principal components.</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>the matrix of variable loadings (i.e., a matrix
whose columns contain the eigenvectors).  This is of class
<code>"loadings"</code>: see <code><a href="stats.html#topic+loadings">loadings</a></code> for its <code>print</code>
method.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>if <code>scores = TRUE</code>, the scores of the supplied
data on the principal components.</p>
</td></tr>
<tr><td><code>eig</code></td>
<td>
<p>Eigen values</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Antoine Lucas</p>


<h3>References</h3>

<p>H. Caussinus, M. Fekri, S. Hakam and A. Ruiz-Gazen, <em>A monitoring
display of multivariate outliers</em> Computational Statistics &amp; Data
Analysis,
Volume 44, Issues 1-2, 28 October 2003, Pages 237-252
</p>
<p>Caussinus, H and Ruiz-Gazen, A. (1993): <em>Projection Pursuit and
Generalized Principal Component Analyses, in New Directions in
Statistical Data Analysis and Robustness</em> (eds.  Morgenthaler et
al.), pp. 35-46. Birk\&quot;auser Verlag Basel.
</p>
<p>Caussinus, H. and Ruiz-Gazen, A. (1995). <em>Metrics for Finding Typical
Structures by Means of Principal Component Analysis. In Data Science
and its Applications</em> (eds Y. Escoufier and C. Hayashi),
pp. 177-192. Tokyo: Academic Press.
</p>
<p>Antoine Lucas and Sylvain Jasson, <em>Using amap and ctc Packages
for Huge Clustering</em>, R News, 2006, vol 6, issue 5 pages 58-60.
</p>


<h3>See Also</h3>

<p><a href="#topic+acp">acp</a> <a href="#topic+acprob">acprob</a> <a href="stats.html#topic+princomp">princomp</a> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lubisch)
lubisch &lt;- lubisch[,-c(1,8)]
p &lt;- acpgen(lubisch,h1=1,h2=1/sqrt(2))
plot(p,main='ACP robuste des individus')

# See difference with acp

p &lt;- princomp(lubisch)
class(p)&lt;- "acp"

</code></pre>

<hr>
<h2 id='acprob'>Robust principal component analysis</h2><span id='topic+acprob'></span>

<h3>Description</h3>

<p>Robust principal component analysis</p>


<h3>Usage</h3>

<pre><code class='language-R'>acprob(x,h,center=TRUE,reduce=TRUE,kernel="gaussien")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="acprob_+3A_x">x</code></td>
<td>
<p>Matrix  / data frame</p>
</td></tr>
<tr><td><code id="acprob_+3A_h">h</code></td>
<td>
<p>Scalar: bandwidth of the Kernel</p>
</td></tr>
<tr><td><code id="acprob_+3A_kernel">kernel</code></td>
<td>
<p>The kernel used. This must be one of '&quot;gaussien&quot;', 
'&quot;quartic&quot;', '&quot;triweight&quot;', '&quot;epanechikov&quot;' , 
'&quot;cosinus&quot;' or '&quot;uniform&quot;' </p>
</td></tr> 
<tr><td><code id="acprob_+3A_center">center</code></td>
<td>
<p>A logical value indicating whether we center data</p>
</td></tr>
<tr><td><code id="acprob_+3A_reduce">reduce</code></td>
<td>
<p>A logical value indicating whether we &quot;reduce&quot; data i.e. 
divide each column by standard deviation</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>acpgen</code>  compute robust pca. i.e. spectral analysis of a robust
variance instead of usual variance. Robust variance: see
<code><a href="#topic+varrob">varrob</a></code>
</p>


<h3>Value</h3>

<p>An object of class <b>acp</b> 
The object is a list with components:
</p>
<table>
<tr><td><code>sdev</code></td>
<td>
<p>the standard deviations of the principal components.</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>the matrix of variable loadings (i.e., a matrix
whose columns contain the eigenvectors).  This is of class
<code>"loadings"</code>: see <code><a href="stats.html#topic+loadings">loadings</a></code> for its <code>print</code>
method.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>if <code>scores = TRUE</code>, the scores of the supplied
data on the principal components.</p>
</td></tr>
<tr><td><code>eig</code></td>
<td>
<p>Eigen values</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Antoine Lucas</p>


<h3>References</h3>

<p>H. Caussinus, M. Fekri, S. Hakam and A. Ruiz-Gazen, <em>A monitoring
display of multivariate outliers</em> Computational Statistics &amp; Data
Analysis,
Volume 44, Issues 1-2, 28 October 2003, Pages 237-252
</p>
<p>Caussinus, H and Ruiz-Gazen, A. (1993): <em>Projection Pursuit and
Generalized Principal Component Analyses, in New Directions in
Statistical Data Analysis and Robustness</em> (eds.  Morgenthaler et
al.), pp. 35-46. Birk\&quot;auser Verlag Basel.
</p>
<p>Caussinus, H. and Ruiz-Gazen, A. (1995). <em>Metrics for Finding Typical
Structures by Means of Principal Component Analysis. In Data Science
and its Applications</em> (eds Y. Escoufier and C. Hayashi),
pp. 177-192. Tokyo: Academic Press.
</p>
<p>Antoine Lucas and Sylvain Jasson, <em>Using amap and ctc Packages
for Huge Clustering</em>, R News, 2006, vol 6, issue 5 pages 58-60.
</p>


<h3>See Also</h3>

<p><a href="stats.html#topic+princomp">princomp</a> <a href="#topic+acpgen">acpgen</a></p>

<hr>
<h2 id='afc'>Correspondance factorial analysis.</h2><span id='topic+afc'></span>

<h3>Description</h3>

<p>Compute an acp on a contingency table tacking into account weight
of rows and columns
</p>


<h3>Usage</h3>

<pre><code class='language-R'>afc(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="afc_+3A_x">x</code></td>
<td>
<p>A contingency table, or a result of function <code>burt</code> or
<code>matlogic</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Antoine Lucas</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
color &lt;- as.factor(c('blue','red','red','blue','red'))
size &lt;- as.factor(c('large','large','small','medium','large'))
x &lt;- data.frame(color,size)

afc.1 &lt;- afc(burt(x))
afc.2 &lt;- afc(matlogic(x))

plotAll(afc.1)
plotAll(afc.2)

## End(Not run)
</code></pre>

<hr>
<h2 id='burt'>Compute burt table from a factor dataframe.</h2><span id='topic+burt'></span><span id='topic+matlogic'></span>

<h3>Description</h3>



<p>matlogic returns for all variables a matrix of logical values
for each levels. burt is defined as t(matlogic).matlogic
</p>


<h3>Usage</h3>

<pre><code class='language-R'>burt(x)
matlogic(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="burt_+3A_x">x</code></td>
<td>
<p>A dataframe that contents only factors</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Antoine Lucas</p>


<h3>Examples</h3>

<pre><code class='language-R'>color &lt;- as.factor(c('blue','red','red','blue','red'))
size &lt;- as.factor(c('large','large','small','medium','large'))
x &lt;- data.frame(color,size)

matlogic(x)
##  color.blue color.red size.large size.medium size.small
##1          1         0          1           0          0
##2          0         1          1           0          0
##3          0         1          0           0          1
##4          1         0          0           1          0
##5          0         1          1           0          0

burt(x)
##              color.blue color.red size.large size.medium size.small
##  color.blue           2         0          1           1          0
##  color.red            0         3          2           0          1
##  size.large           1         2          3           0          0
##  size.medium          1         0          0           1          0
##  size.small           0         1          0           0          1
</code></pre>

<hr>
<h2 id='diss'>Compute a dissimilarity matrix</h2><span id='topic+diss'></span>

<h3>Description</h3>

<p>Compute a dissimilarity matrix from a data set (containing only factors).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diss(x, w=rep(1,ncol(x)) )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diss_+3A_x">x</code></td>
<td>
<p>A matrix or data frame containing only factors.</p>
</td></tr>
<tr><td><code id="diss_+3A_w">w</code></td>
<td>
<p>A vector of weight, by default each variable has got same weight</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Case of N individuals described by P categorical variables:
each element (i,j) of the signed similarities array is computed
by sommation over the P variables of the contributions of each
variable, multiplied by the weight of the variable. The contribution
of a given categorical variable is +1 if the individual i and j
are in the same class, and is -1 if they are not.
</p>


<h3>Value</h3>

<p>A dissimilarity matrix.
</p>


<h3>Author(s)</h3>

<p>Antoine Lucas</p>


<h3>See Also</h3>

 <p><code><a href="#topic+Dist">Dist</a></code>, <code><a href="#topic+pop">pop</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;-
matrix(c(1,1,1,1,1
        ,1,2,1,2,1
        ,2,3,2,3,2
        ,2,4,3,3,2
        ,1,2,4,2,1
        ,2,3,2,3,1),ncol=5,byrow=TRUE)

diss(data)

## With weights
diss(data,w=c(1,1,2,2,3))

</code></pre>

<hr>
<h2 id='Dist'>Distance Matrix Computation</h2><span id='topic+Dist'></span>

<h3>Description</h3>

<p>This function computes and returns the distance matrix computed by
using the specified distance measure to compute the distances between
the rows of a data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Dist(x, method = "euclidean", nbproc = 2, diag = FALSE, upper = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Dist_+3A_x">x</code></td>
<td>
<p>numeric matrix or (data frame) or an object of class
&quot;exprSet&quot;.
Distances between the rows of
<code>x</code> will be computed.</p>
</td></tr>
<tr><td><code id="Dist_+3A_method">method</code></td>
<td>
<p>the distance measure to be used. This must be one of
<code>"euclidean"</code>, <code>"maximum"</code>, <code>"manhattan"</code>,
<code>"canberra"</code>, <code>"binary"</code>, <code>"pearson"</code>,
<code>"abspearson"</code>,  <code>"correlation"</code>,
<code>"abscorrelation"</code>, <code>"spearman"</code> or <code>"kendall"</code>.
Any unambiguous substring can be given.</p>
</td></tr>
<tr><td><code id="Dist_+3A_nbproc">nbproc</code></td>
<td>
<p>integer, Number of subprocess for parallelization</p>
</td></tr>
<tr><td><code id="Dist_+3A_diag">diag</code></td>
<td>
<p>logical value indicating whether the diagonal of the
distance matrix should be printed by <code>print.dist</code>.</p>
</td></tr>
<tr><td><code id="Dist_+3A_upper">upper</code></td>
<td>
<p>logical value indicating whether the upper triangle of the
distance matrix should be printed by <code>print.dist</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Available distance measures are (written for two vectors <code class="reqn">x</code> and
<code class="reqn">y</code>):
</p>

<dl>
<dt><code>euclidean</code>:</dt><dd><p>Usual square distance between the two
vectors (2 norm).</p>
</dd>
<dt><code>maximum</code>:</dt><dd><p>Maximum distance between two components of <code class="reqn">x</code>
and <code class="reqn">y</code> (supremum norm)</p>
</dd>
<dt><code>manhattan</code>:</dt><dd><p>Absolute distance between the two vectors
(1 norm).</p>
</dd>
<dt><code>canberra</code>:</dt><dd><p><code class="reqn">\sum_i |x_i - y_i| / |x_i + y_i|</code>.  Terms with zero numerator and
denominator are omitted from the sum and treated as if the values
were missing.
</p>
</dd>
<dt><code>binary</code>:</dt><dd><p>(aka <em>asymmetric binary</em>): The vectors
are regarded as binary bits, so non-zero elements are &lsquo;on&rsquo; and zero
elements are &lsquo;off&rsquo;.  The distance is the <em>proportion</em> of
bits in which only one is on amongst those in which at least one is on.</p>
</dd>
<dt><code>pearson</code>:</dt><dd><p>Also named &quot;not centered Pearson&quot;
<code class="reqn">1 - \frac{\sum_i x_i y_i}{\sqrt{\sum_i x_i^2 %
	   \sum_i y_i^2}}</code>.
</p>
</dd>
<dt><code>abspearson</code>:</dt><dd><p>Absolute Pearson
<code class="reqn">1 - \left| \frac{\sum_i x_i y_i}{\sqrt{\sum_i x_i^2 %
	   \sum_i y_i^2}} \right| </code>.
</p>
</dd>
<dt><code>correlation</code>:</dt><dd><p>Also named &quot;Centered Pearson&quot;
<code class="reqn">1 - corr(x,y)</code>.  
</p>
</dd>
<dt><code>abscorrelation</code>:</dt><dd><p>Absolute correlation
<code class="reqn">1 - | corr(x,y) |</code>
with
</p>
<p><code class="reqn"> corr(x,y) = \frac{\sum_i x_i y_i -\frac1n \sum_i x_i \sum_i%
y_i}{% frac: 2nd part
\sqrt{\left(\sum_i x_i^2 -\frac1n \left( \sum_i x_i \right)^2 %
\right)%
\left( \sum_i  y_i^2 -\frac1n \left( \sum_i y_i \right)^2 %
\right)}  }</code>.  
</p>
</dd>
<dt><code>spearman</code>:</dt><dd><p>Compute a distance based on rank.
<code class="reqn">\sum(d_i^2)</code> where <code class="reqn">d_i</code> is the difference
in rank between <code class="reqn">x_i</code> and <code class="reqn">y_i</code>.
</p>
<p><code>Dist(x,method="spearman")[i,j] =</code>
</p>
<p><code>cor.test(x[i,],x[j,],method="spearman")$statistic</code>
</p>
</dd>
<dt><code>kendall</code>:</dt><dd><p>Compute a distance based on rank.
<code class="reqn">\sum_{i,j} K_{i,j}(x,y)</code> with <code class="reqn">K_{i,j}(x,y)</code>
is 0 if <code class="reqn">x_i, x_j</code> in same order as <code class="reqn">y_i,y_j</code>,
1 if not.
</p>
</dd>
</dl>

<p>Missing values are allowed, and are excluded from all computations
involving the rows within which they occur.  If some columns are
excluded in calculating a Euclidean, Manhattan or Canberra distance,
the sum is scaled up proportionally to the number of columns used.
If all pairs are excluded when calculating a particular distance,
the value is <code>NA</code>.
</p>
<p>The functions <code>as.matrix.dist()</code> and <code>as.dist()</code> can be used
for conversion between objects of class <code>"dist"</code> and conventional
distance matrices and vice versa.
</p>


<h3>Value</h3>

<p>An object of class <code>"dist"</code>.
</p>
<p>The lower triangle of the distance matrix stored by columns in a
vector, say <code>do</code>. If <code>n</code> is the number of
observations, i.e., <code>n &lt;- attr(do, "Size")</code>, then
for <code class="reqn">i &lt; j &lt;= n</code>, the dissimilarity between (row) i and j is
<code>do[n*(i-1) - i*(i-1)/2 + j-i]</code>.
The length of the vector is <code class="reqn">n*(n-1)/2</code>, i.e., of order <code class="reqn">n^2</code>.
</p>
<p>The object has the following attributes (besides <code>"class"</code> equal
to <code>"dist"</code>):
</p>
<table>
<tr><td><code>Size</code></td>
<td>
<p>integer, the number of observations in the dataset.</p>
</td></tr>
<tr><td><code>Labels</code></td>
<td>
<p>optionally, contains the labels, if any, of the
observations of the dataset.</p>
</td></tr>
<tr><td><code>Diag</code>, <code>Upper</code></td>
<td>
<p>logicals corresponding to the arguments <code>diag</code>
and <code>upper</code> above, specifying how the object should be printed.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>optionally, the <code><a href="base.html#topic+call">call</a></code> used to create the
object.</p>
</td></tr>
<tr><td><code>methods</code></td>
<td>
<p>optionally, the distance method used; resulting form
<code><a href="stats.html#topic+dist">dist</a>()</code>, the (<code><a href="base.html#topic+match.arg">match.arg</a>()</code>ed) <code>method</code>
argument.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Multi-thread (parallelisation) is disable on Windows.</p>


<h3>References</h3>

<p>Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979)
<em>Multivariate Analysis.</em> London: Academic Press.
</p>
<p>Wikipedia
<a href="https://en.wikipedia.org/wiki/Kendall_tau_distance">https://en.wikipedia.org/wiki/Kendall_tau_distance</a>
</p>


<h3>See Also</h3>

<p><code><a href="cluster.html#topic+daisy">daisy</a></code> in the &lsquo;<span class="file">cluster</span>&rsquo; package with more
possibilities in the case of <em>mixed</em> (contiuous / categorical)
variables.
<code><a href="stats.html#topic+dist">dist</a></code>  <code><a href="#topic+hcluster">hcluster</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(100), nrow=5)
Dist(x)
Dist(x, diag = TRUE)
Dist(x, upper = TRUE)


## compute dist with 8 threads
Dist(x,nbproc=8)


Dist(x,method="abscorrelation")
Dist(x,method="kendall")

</code></pre>

<hr>
<h2 id='hcluster'>Hierarchical Clustering</h2><span id='topic+hcluster'></span><span id='topic+hclusterpar'></span>

<h3>Description</h3>

<p>Hierarchical cluster analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hcluster(x, method = "euclidean", diag = FALSE, upper = FALSE,
         link = "complete", members = NULL, nbproc = 2,
         doubleprecision = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hcluster_+3A_x">x</code></td>
<td>

<p>A numeric matrix of data, or an object that can be coerced to such a
matrix (such as a numeric vector or a data frame with all numeric
columns). Or an object of class &quot;exprSet&quot;.
</p>
</td></tr>
<tr><td><code id="hcluster_+3A_method">method</code></td>
<td>
<p>the distance measure to be used. This must be one of
<code>"euclidean"</code>, <code>"maximum"</code>, <code>"manhattan"</code>,
<code>"canberra"</code>, <code>"binary"</code>, <code>"pearson"</code>,
<code>"abspearson"</code>,  <code>"correlation"</code>,
<code>"abscorrelation"</code>, <code>"spearman"</code> or <code>"kendall"</code>.
Any unambiguous substring can be given.</p>
</td></tr>
<tr><td><code id="hcluster_+3A_diag">diag</code></td>
<td>
<p>logical value indicating whether the diagonal of the
distance matrix should be printed by <code>print.dist</code>.</p>
</td></tr>
<tr><td><code id="hcluster_+3A_upper">upper</code></td>
<td>
<p>logical value indicating whether the upper triangle of the
distance matrix should be printed by <code>print.dist</code>.</p>
</td></tr>
<tr><td><code id="hcluster_+3A_link">link</code></td>
<td>
<p>the agglomeration method to be used. This should
be (an unambiguous abbreviation of) one of
<code>"ward"</code>, <code>"single"</code>, <code>"complete"</code>,
<code>"average"</code>, <code>"mcquitty"</code>, <code>"median"</code> or
<code>"centroid"</code>,<code>"centroid2"</code>.</p>
</td></tr>
<tr><td><code id="hcluster_+3A_members">members</code></td>
<td>
<p><code>NULL</code> or a vector with length size of <code>d</code>.</p>
</td></tr>
<tr><td><code id="hcluster_+3A_nbproc">nbproc</code></td>
<td>
<p>integer, number of subprocess for parallelization [Linux
&amp; Mac only]</p>
</td></tr>
<tr><td><code id="hcluster_+3A_doubleprecision">doubleprecision</code></td>
<td>
<p>True: use of double precision for distance
matrix computation; False: use simple precision</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a mix of function <code>hclust</code> and function
<code>dist</code>. <code>hcluster(x, method = "euclidean",link = "complete")
     = hclust(dist(x, method = "euclidean"),method = "complete"))</code>   
It use twice less memory, as it doesn't store distance matrix.
</p>
<p>For more details, see documentation of <code>hclust</code> and <code>Dist</code>.
</p>


<h3>Value</h3>

<p>An object of class <b>hclust</b> which describes the
tree produced by the clustering process.
The object is a list with components:
</p>
<table>
<tr><td><code>merge</code></td>
<td>
<p>an <code class="reqn">n-1</code> by 2 matrix.
Row <code class="reqn">i</code> of <code>merge</code> describes the merging of clusters
at step <code class="reqn">i</code> of the clustering.
If an element <code class="reqn">j</code> in the row is negative,
then observation <code class="reqn">-j</code> was merged at this stage.
If <code class="reqn">j</code> is positive then the merge
was with the cluster formed at the (earlier) stage <code class="reqn">j</code>
of the algorithm.
Thus negative entries in <code>merge</code> indicate agglomerations
of singletons, and positive entries indicate agglomerations
of non-singletons.</p>
</td></tr>
<tr><td><code>height</code></td>
<td>
<p>a set of <code class="reqn">n-1</code> non-decreasing real values.
The clustering <em>height</em>: that is, the value of
the criterion associated with the clustering
<code>method</code> for the particular agglomeration.</p>
</td></tr>
<tr><td><code>order</code></td>
<td>
<p>a vector giving the permutation of the original
observations suitable for plotting, in the sense that a cluster
plot using this ordering and matrix <code>merge</code> will not have
crossings of the branches.</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>
<p>labels for each of the objects being clustered.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call which produced the result.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the cluster method that has been used.</p>
</td></tr>
<tr><td><code>dist.method</code></td>
<td>
<p>the distance that has been used to create <code>d</code>
(only returned if the distance object has a <code>"method"</code>
attribute).</p>
</td></tr>
</table>
<p>There is a <code><a href="base.html#topic+print">print</a></code> and a <code><a href="graphics.html#topic+plot">plot</a></code> method for
<code>hclust</code> objects.
The <code>plclust()</code> function is basically the same as the plot method,
<code>plot.hclust</code>, primarily for back compatibility with S-plus. Its
extra arguments are not yet implemented.
</p>


<h3>Note</h3>

<p>Multi-thread (parallelisation) is disable on Windows.</p>


<h3>Author(s)</h3>

<p>The <code>hcluster</code> function is based on C code adapted from Cran
Fortran routine
by Antoine Lucas.
</p>


<h3>References</h3>

<p>Antoine Lucas and Sylvain Jasson, <em>Using amap and ctc Packages
for Huge Clustering</em>, R News, 2006, vol 6, issue 5 pages 58-60.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Dist">Dist</a></code>,
<code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="stats.html#topic+kmeans">kmeans</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(USArrests)
hc &lt;- hcluster(USArrests,link = "ave")
plot(hc)
plot(hc, hang = -1)

## Do the same with centroid clustering and squared Euclidean distance,
## cut the tree into ten clusters and reconstruct the upper part of the
## tree from the cluster centers.
hc &lt;- hclust(dist(USArrests)^2, "cen")
memb &lt;- cutree(hc, k = 10)
cent &lt;- NULL
for(k in 1:10){
  cent &lt;- rbind(cent, colMeans(USArrests[memb == k, , drop = FALSE]))
}
hc1 &lt;- hclust(dist(cent)^2, method = "cen", members = table(memb))
opar &lt;- par(mfrow = c(1, 2))
plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
par(opar)


## other combinaison are possible

hc &lt;- hcluster(USArrests,method = "euc",link = "ward", nbproc= 1,
doubleprecision = TRUE)
hc &lt;- hcluster(USArrests,method = "max",link = "single", nbproc= 2,
doubleprecision = TRUE)
hc &lt;- hcluster(USArrests,method = "man",link = "complete", nbproc= 1,
doubleprecision = TRUE)
hc &lt;- hcluster(USArrests,method = "can",link = "average", nbproc= 2,
doubleprecision = TRUE)
hc &lt;- hcluster(USArrests,method = "bin",link = "mcquitty", nbproc= 1,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "pea",link = "median", nbproc= 2,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "abspea",link = "median", nbproc= 2,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "cor",link = "centroid", nbproc= 1,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "abscor",link = "centroid", nbproc= 1,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "spe",link = "complete", nbproc= 2,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "ken",link = "complete", nbproc= 2,
doubleprecision = FALSE)



</code></pre>

<hr>
<h2 id='Kmeans'>
K-Means Clustering
</h2><span id='topic+Kmeans'></span>

<h3>Description</h3>

<p>Perform k-means clustering on a data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Kmeans(x, centers, iter.max = 10, nstart = 1,
         method = "euclidean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Kmeans_+3A_x">x</code></td>
<td>

<p>A numeric matrix of data, or an object that can be coerced to such a
matrix (such as a numeric vector or a data frame with all numeric
columns). Or an object of class &quot;exprSet&quot;.    
</p>
</td></tr>
<tr><td><code id="Kmeans_+3A_centers">centers</code></td>
<td>

<p>Either the number of clusters or a set of initial cluster centers.
If the first, a random set of rows in <code>x</code> are chosen as the initial
centers.
</p>
</td></tr>
<tr><td><code id="Kmeans_+3A_iter.max">iter.max</code></td>
<td>

<p>The maximum number of iterations allowed.
</p>
</td></tr>
<tr><td><code id="Kmeans_+3A_nstart">nstart</code></td>
<td>
<p>If <code>centers</code> is a number, how many random sets
should be chosen?</p>
</td></tr>
<tr><td><code id="Kmeans_+3A_method">method</code></td>
<td>
<p>the distance measure to be used. This must be one of
<code>"euclidean"</code>, <code>"maximum"</code>, <code>"manhattan"</code>,
<code>"canberra"</code>, <code>"binary"</code>, <code>"pearson"</code> ,
<code>"abspearson"</code> , <code>"abscorrelation"</code>,
<code>"correlation"</code>, <code>"spearman"</code> or <code>"kendall"</code>.
Any unambiguous substring can be given.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data given by <code>x</code> is clustered by the k-means algorithm.
When this terminates, all cluster centres are at the mean of
their Voronoi sets (the set of data points which are nearest to
the cluster centre).
</p>
<p>The algorithm of Lloyd&ndash;Forgy is used; method=&quot;euclidean&quot; should
return same result as with function <a href="stats.html#topic+kmeans">kmeans</a>.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>cluster</code></td>
<td>

<p>A vector of integers indicating the cluster to which each point is
allocated.
</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centres.</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>The within-cluster sum of square distances for each cluster.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of points in each cluster.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>An objective: to allow NA values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hcluster">hcluster</a></code>,<code><a href="stats.html#topic+kmeans">kmeans</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## a 2-dimensional example
x &lt;- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(x) &lt;- c("x", "y")
(cl &lt;- Kmeans(x, 2))
plot(x, col = cl$cluster)
points(cl$centers, col = 1:2, pch = 8, cex=2)

## random starts do help here with too many clusters
(cl &lt;- Kmeans(x, 5, nstart = 25))
plot(x, col = cl$cluster)
points(cl$centers, col = 1:5, pch = 8)


Kmeans(x, 5,nstart = 25, method="abscorrelation")


</code></pre>

<hr>
<h2 id='lubisch'>Dataset Lubischew</h2><span id='topic+lubisch'></span>

<h3>Description</h3>

<p>Lubischew data (1962): 74 insects, 6 morphologic size. 3
supposed classes</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(lubisch)
</code></pre>

<hr>
<h2 id='plot'>Graphics for Principal component Analysis</h2><span id='topic+plot.acp'></span><span id='topic+biplot.acp'></span><span id='topic+plot2'></span><span id='topic+plotAll'></span>

<h3>Description</h3>

<p>Graphics for Principal component Analysis</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'acp'
plot(x,i=1,j=2,text=TRUE,label='Composants',col='darkblue',
main='Individuals PCA',variables=TRUE,individual.label=NULL,...)
## S3 method for class 'acp'
biplot(x,i=1,j=2,label='Composants',col='darkblue',length=0.1,
main='Variables PCA',circle=TRUE,...)
plot2(x,pourcent=FALSE,eigen=TRUE,label='Comp.',col='lightgrey',
main='Scree Graph',ylab='Eigen Values')
plotAll(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_+3A_x">x</code></td>
<td>
<p>Result of acp or princomp</p>
</td></tr>
<tr><td><code id="plot_+3A_i">i</code></td>
<td>
<p>X axis</p>
</td></tr>
<tr><td><code id="plot_+3A_j">j</code></td>
<td>
<p>Y axis</p>
</td></tr>
<tr><td><code id="plot_+3A_text">text</code></td>
<td>
<p>a logical value indicating whether we use text or points for plot</p>
</td></tr>
<tr><td><code id="plot_+3A_pourcent">pourcent</code></td>
<td>
<p>a logical value indicating whether we use pourcentage of 
values</p>
</td></tr>
<tr><td><code id="plot_+3A_eigen">eigen</code></td>
<td>
<p>a logical value indicating whether we use eigen values or
standard deviation</p>
</td></tr>
<tr><td><code id="plot_+3A_label">label</code></td>
<td>
<p>label for X and Y axis</p>
</td></tr>
<tr><td><code id="plot_+3A_individual.label">individual.label</code></td>
<td>
<p>labels naming individuals</p>
</td></tr>
<tr><td><code id="plot_+3A_col">col</code></td>
<td>
<p>Color of plot</p>
</td></tr>
<tr><td><code id="plot_+3A_main">main</code></td>
<td>
<p>Title of graphic</p>
</td></tr>
<tr><td><code id="plot_+3A_ylab">ylab</code></td>
<td>
<p>Y label</p>
</td></tr>
<tr><td><code id="plot_+3A_length">length</code></td>
<td>
<p>length of arrows</p>
</td></tr>
<tr><td><code id="plot_+3A_variables">variables</code>, <code id="plot_+3A_circle">circle</code></td>
<td>
<p>a logical value indicating whether we display
circle or variables</p>
</td></tr>
<tr><td><code id="plot_+3A_...">...</code></td>
<td>
<p>cex, pch, and other options; see points.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Graphics: 
</p>
<p><code>plot.acp</code> PCA for lines (individuals)
</p>
<p><code>plot.acp</code> PCA for columns (variables)
</p>
<p><code>plot2</code> Eigen values diagram (Scree Graph)
</p>
<p><code>plotAll</code> Plot both 3 graphs
</p>


<h3>Author(s)</h3>

<p>Antoine Lucas</p>


<h3>See Also</h3>

<p><a href="#topic+acpgen">acpgen</a>,<a href="#topic+acprob">acprob</a>, <a href="stats.html#topic+princomp">princomp</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lubisch)
lubisch &lt;- lubisch[,-c(1,8)]
p &lt;- acp(lubisch)
plotAll(p)
</code></pre>

<hr>
<h2 id='pop'>Optimal Partition (classification).</h2><span id='topic+pop'></span>

<h3>Description</h3>

<p>Classification: Computing an Optimal Partition
from Weighted Categorical Variables
or from an Array of Signed Similarities.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pop(x,fmbvr=TRUE,triabs=TRUE,allsol=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pop_+3A_x">x</code></td>
<td>
<p>A dissimilarity matrix</p>
</td></tr>
<tr><td><code id="pop_+3A_fmbvr">fmbvr</code></td>
<td>
<p>Logical, TRUE: look for the exact solution</p>
</td></tr>
<tr><td><code id="pop_+3A_triabs">triabs</code></td>
<td>
<p>Logical, TRUE: try to init with absolute values</p>
</td></tr>
<tr><td><code id="pop_+3A_allsol">allsol</code></td>
<td>
<p>Logical, TRUE all solutions, FALSE only one solution</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michel Petitjean,
<a href="http://petitjeanmichel.free.fr/itoweb.petitjean.class.html">http://petitjeanmichel.free.fr/itoweb.petitjean.class.html</a>
</p>
<p>R port by Antoine Lucas,
</p>


<h3>References</h3>

<p>Theory is explained at <a href="http://petitjeanmichel.free.fr/itoweb.petitjean.class.html">http://petitjeanmichel.free.fr/itoweb.petitjean.class.html</a>
</p>
<p>Marcotorchino F.
<em>Agr\'egation des similarit\'es en classification automatique.</em>
Th\'ese de Doctorat d'Etat en Math\'ematiques,
Universit\'e Paris VI, 25 June 1981.
</p>
<p>Petitjean M.
<em>Agr\'egation des similarit\'es: une solution oubli\'ee.</em>
RAIRO Oper. Res. 2002,36[1],101-108.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## pop from a data matrix
data &lt;-
matrix(c(1,1,1,1,1
        ,1,2,1,2,1
        ,2,3,2,3,2
        ,2,4,3,3,2
        ,1,2,4,2,1
        ,2,3,2,3,1),ncol=5,byrow=TRUE)



pop(diss(data))


## pop from a dissimilarity matrix

d &lt;-2 * matrix(c(9,  8,  5,  7,  7,  2
,  8,  9,  2,  5,  1,  7
,  5,  2,  9,  8,  7,  1
,  7,  5,  8,  9,  3,  2
,  7,  1,  7,  3,  9,  6
,  2,  7,  1,  2,  6,  9),ncol=6,byrow=TRUE) - 9

pop(d)


## Not run: 
d &lt;- 2 * matrix(c(57, 15, 11, 32,  1, 34,  4,  6, 17,  7
, 15, 57, 27, 35, 27, 27, 20, 24, 30, 15
, 11, 27, 57, 25, 25, 20, 34, 25, 17, 15
, 32, 35, 25, 57, 22, 44, 13, 22, 30, 11
,  1, 27, 25, 22, 57, 21, 28, 43, 20, 13
, 34, 27, 20, 44, 21, 57, 18, 27, 21,  8
,  4, 20, 34, 13, 28, 18, 57, 31, 28, 13
,  6, 24, 25, 22, 43, 27, 31, 57, 30, 15
, 17, 30, 17, 30, 20, 21, 28, 30, 57, 12
,  7, 15, 15, 11, 13,  8, 13, 15, 12, 57),ncol=10,byrow=TRUE) - 57

pop(d)



## End(Not run)

</code></pre>

<hr>
<h2 id='VarRob'>Robust variance</h2><span id='topic+varrob'></span>

<h3>Description</h3>

<p>Compute a robust variance</p>


<h3>Usage</h3>

<pre><code class='language-R'>varrob(x,h,D=NULL,kernel="gaussien")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VarRob_+3A_x">x</code></td>
<td>
<p>Matrix  / data frame</p>
</td></tr>
<tr><td><code id="VarRob_+3A_h">h</code></td>
<td>
<p>Scalar: bandwidth of the Kernel</p>
</td></tr>
<tr><td><code id="VarRob_+3A_kernel">kernel</code></td>
<td>
<p>The kernel used. This must be one of '&quot;gaussien&quot;', 
'&quot;quartic&quot;', '&quot;triweight&quot;', '&quot;epanechikov&quot;' , 
'&quot;cosinus&quot;' or '&quot;uniform&quot;' </p>
</td></tr> 
<tr><td><code id="VarRob_+3A_d">D</code></td>
<td>
<p>A product scalar matrix / une matrice de produit scalaire</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>U</code> compute robust variance. <code class="reqn">U_n^{-1} = S_n^{-1} - 1/h V_n^{-1}</code>
</p>
<p style="text-align: center;"><code class="reqn">S_n=\frac{\sum_{i=1}^{n}K(||X_i||_{V_n^{-1}}/h)(X_i-\mu_n)(X_i-\mu_n)'}{\sum_{i=1}^nK(||X_i||_{V_n^{-1}}/h)}</code>
</p>

<p>with <code class="reqn">\mu_n</code> estimator of the mean.
</p>
<p><code>K</code> compute a kernel.
</p>


<h3>Value</h3>

<p>A matrix
</p>


<h3>Author(s)</h3>

<p>Antoine Lucas</p>


<h3>References</h3>

<p>H. Caussinus, S. Hakam, A. Ruiz-Gazen
Projections revelatrices controlees: groupements et structures
diverses.
2002, to appear in Rev. Statist. Appli.
</p>


<h3>See Also</h3>

<p><a href="#topic+acp">acp</a> <a href="stats.html#topic+princomp">princomp</a> </p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
