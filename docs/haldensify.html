<!DOCTYPE html><html lang="en"><head><title>Help for package haldensify</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {haldensify}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#confint.ipw_haldensify'><p>Confidence Intervals for IPW Estimates of the Causal Effects of Stochatic</p>
Shift Interventions</a></li>
<li><a href='#cv_haldensify'><p>HAL Conditional Density Estimation in a Cross-validation Fold</p></a></li>
<li><a href='#dcar_selector'><p>IPW Estimator Selector via Projection of Efficient Influence Function</p></a></li>
<li><a href='#fit_haldensify'><p>Fit Conditional Density Estimation for a Sequence of HAL Models</p></a></li>
<li><a href='#format_long_hazards'><p>Generate Augmented (Long Format) Data for Pooled Hazards Regression</p></a></li>
<li><a href='#gcv_selector'><p>IPW Estimator Selector via Global Cross-Validation</p></a></li>
<li><a href='#haldensify'><p>Cross-validated HAL Conditional Density Estimation</p></a></li>
<li><a href='#ipw_shift'><p>IPW Estimates of the Causal Effects of Stochatic Shift Interventions</p></a></li>
<li><a href='#make_bins'><p>Histogram Binning Procedures for Pooled Hazards Regression</p></a></li>
<li><a href='#map_hazard_to_density'><p>Map Predicted Hazard to Predicted Density for a Single Observation</p></a></li>
<li><a href='#plateau_selector'><p>IPW Estimator Selector Using Lepski's Plateau Method for the MSE</p></a></li>
<li><a href='#plot.haldensify'><p>Plot Method for HAL Conditional Density Estimates</p></a></li>
<li><a href='#predict.haldensify'><p>Prediction Method for HAL Conditional Density Estimation</p></a></li>
<li><a href='#print.haldensify'><p>Print: Highly Adaptive Lasso Conditional Density Estimates</p></a></li>
<li><a href='#print.ipw_haldensify'><p>Print: IPW Estimates of the Causal Effects of Stochatic Shift Interventions</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Highly Adaptive Lasso Conditional Density Estimation</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.3</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nima Hejazi &lt;nh@nimahejazi.org&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An algorithm for flexible conditional density estimation based on
    application of pooled hazard regression to an artificial repeated measures
    dataset constructed by discretizing the support of the outcome variable. To
    facilitate non/semi-parametric estimation of the conditional density, the
    highly adaptive lasso, a nonparametric regression function shown to reliably
    estimate a large class of functions at a fast convergence rate, is utilized.
    The pooled hazards data augmentation formulation implemented was first
    described by Díaz and van der Laan (2011) &lt;<a href="https://doi.org/10.2202%2F1557-4679.1356">doi:10.2202/1557-4679.1356</a>&gt;. To
    complement the conditional density estimation utilities, tools for efficient
    nonparametric inverse probability weighted (IPW) estimation of the causal
    effects of stochastic shift interventions (modified treatment policies),
    directly utilizing the density estimation technique for construction of the
    generalized propensity score, are provided. These IPW estimators utilize
    undersmoothing (sieve estimation) of the conditional density estimators in
    order to achieve the non/semi-parametric efficiency bound.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, utils, dplyr, tibble, ggplot2, data.table, matrixStats,
future.apply, assertthat, hal9001 (&ge; 0.4.1), origami (&ge;
1.0.3), rsample, rlang, scales, Rdpack</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, stringr, covr, future</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/nhejazi/haldensify">https://github.com/nhejazi/haldensify</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/nhejazi/haldensify/issues">https://github.com/nhejazi/haldensify/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-02-09 21:48:47 UTC; nsh</td>
</tr>
<tr>
<td>Author:</td>
<td>Nima Hejazi <a href="https://orcid.org/0000-0002-7127-2789"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph],
  David Benkeser <a href="https://orcid.org/0000-0002-1019-8343"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Mark van der Laan <a href="https://orcid.org/0000-0003-1432-5511"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ths],
  Rachael Phillips <a href="https://orcid.org/0000-0002-8474-591X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-02-09 22:20:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='confint.ipw_haldensify'>Confidence Intervals for IPW Estimates of the Causal Effects of Stochatic
Shift Interventions</h2><span id='topic+confint.ipw_haldensify'></span>

<h3>Description</h3>

<p>Confidence Intervals for IPW Estimates of the Causal Effects of Stochatic
Shift Interventions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ipw_haldensify'
confint(object, parm = seq_len(object$psi), level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="confint.ipw_haldensify_+3A_object">object</code></td>
<td>
<p>An object of class <code>ipw_haldensify</code>, produced by invoking
the function <code><a href="#topic+ipw_shift">ipw_shift</a></code>, for which a confidence interval is to
be computed.</p>
</td></tr>
<tr><td><code id="confint.ipw_haldensify_+3A_parm">parm</code></td>
<td>
<p>A <code>numeric</code> vector indicating indices of <code>object$est</code>
for which to return confidence intervals.</p>
</td></tr>
<tr><td><code id="confint.ipw_haldensify_+3A_level">level</code></td>
<td>
<p>A <code>numeric</code> indicating the nominal level of the confidence
interval to be computed.</p>
</td></tr>
<tr><td><code id="confint.ipw_haldensify_+3A_...">...</code></td>
<td>
<p>Other arguments. Not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Compute confidence intervals for estimates produced by
<code><a href="#topic+ipw_shift">ipw_shift</a></code>.
</p>


<h3>Value</h3>

<p>A named <code>numeric</code> vector containing the parameter estimate from
a <code>ipw_haldensify</code> object, alongside lower/upper Wald-style confidence
intervals at a specified coverage level.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate data
n_obs &lt;- 50
W1 &lt;- rbinom(n_obs, 1, 0.6)
W2 &lt;- rbinom(n_obs, 1, 0.2)
A &lt;- rnorm(n_obs, (2 * W1 - W2 - W1 * W2), 2)
Y &lt;- rbinom(n_obs, 1, plogis(3 * A + W1 + W2 - W1 * W2))

# fit the IPW estimator
est_ipw_shift &lt;- ipw_shift(
  W = cbind(W1, W2), A = A, Y = Y,
  delta = 0.5, n_bins = 3L, cv_folds = 2L,
  lambda_seq = exp(seq(-1, -10, length = 100L)),
  # arguments passed to hal9001::fit_hal()
  max_degree = 1,
  # ...continue arguments for IPW
  undersmooth_type = "gcv"
)
confint(est_ipw_shift)
</code></pre>

<hr>
<h2 id='cv_haldensify'>HAL Conditional Density Estimation in a Cross-validation Fold</h2><span id='topic+cv_haldensify'></span>

<h3>Description</h3>

<p>HAL Conditional Density Estimation in a Cross-validation Fold
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_haldensify(
  fold,
  long_data,
  wts = rep(1, nrow(long_data)),
  lambda_seq = exp(seq(-1, -13, length = 1000L)),
  smoothness_orders = 0L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv_haldensify_+3A_fold">fold</code></td>
<td>
<p>Object specifying cross-validation folds as generated by a call
to <code><a href="origami.html#topic+make_folds">make_folds</a></code>.</p>
</td></tr>
<tr><td><code id="cv_haldensify_+3A_long_data">long_data</code></td>
<td>
<p>A <code>data.table</code> or <code>data.frame</code> object containing
the data in long format, as given in Díaz I, van der Laan MJ (2011).
&ldquo;Super learner based conditional density estimation with application to marginal structural models.&rdquo;
<em>International Journal of Biostatistics</em>, <b>7</b>(1), 1&ndash;20.,
as produced by <code><a href="#topic+format_long_hazards">format_long_hazards</a></code>.</p>
</td></tr>
<tr><td><code id="cv_haldensify_+3A_wts">wts</code></td>
<td>
<p>A <code>numeric</code> vector of observation-level weights, matching in
its length the number of records present in the long format data. Default
is to weight all observations equally.</p>
</td></tr>
<tr><td><code id="cv_haldensify_+3A_lambda_seq">lambda_seq</code></td>
<td>
<p>A <code>numeric</code> sequence of values of the regularization
parameter of Lasso regression; passed to <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code>.</p>
</td></tr>
<tr><td><code id="cv_haldensify_+3A_smoothness_orders">smoothness_orders</code></td>
<td>
<p>A <code>integer</code> indicating the smoothness of the
HAL basis functions; passed to <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code>. The default
is set to zero, for indicator basis functions.</p>
</td></tr>
<tr><td><code id="cv_haldensify_+3A_...">...</code></td>
<td>
<p>Additional (optional) arguments of <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code>
that may be used to control fitting of the HAL regression model. Possible
choices include <code>use_min</code>, <code>reduce_basis</code>, <code>return_lasso</code>,
and <code>return_x_basis</code>, but this list is not exhaustive. Consult the
documentation of <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code> for complete details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimates the conditional density of A|W for a subset of the full
set of observations based on the inputted structure of the cross-validation
folds. This is a helper function intended to be used to select the optimal
value of the penalization parameter for the highly adaptive lasso estimates
of the conditional hazard (via <code><a href="origami.html#topic+cross_validate">cross_validate</a></code>). The
</p>


<h3>Value</h3>

<p>A <code>list</code>, containing density predictions, observations IDs,
observation-level weights, and cross-validation indices for conditional
density estimation on a single fold of the overall data.
</p>

<hr>
<h2 id='dcar_selector'>IPW Estimator Selector via Projection of Efficient Influence Function</h2><span id='topic+dcar_selector'></span>

<h3>Description</h3>

<p>IPW Estimator Selector via Projection of Efficient Influence Function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcar_selector(
  W,
  A,
  Y,
  delta = 0,
  gn_pred_natural,
  gn_pred_shifted,
  Qn_pred_natural,
  Qn_pred_shifted
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dcar_selector_+3A_w">W</code></td>
<td>
<p>A <code>matrix</code>, <code>data.frame</code>, or similar containing a set of
baseline covariates.</p>
</td></tr>
<tr><td><code id="dcar_selector_+3A_a">A</code></td>
<td>
<p>A <code>numeric</code> vector corresponding to a exposure variable. The
parameter of interest is defined as a location shift of this quantity.</p>
</td></tr>
<tr><td><code id="dcar_selector_+3A_y">Y</code></td>
<td>
<p>A <code>numeric</code> vector of the observed outcomes.</p>
</td></tr>
<tr><td><code id="dcar_selector_+3A_delta">delta</code></td>
<td>
<p>A <code>numeric</code> value indicating the shift in the exposure to
be used in defining the target parameter. This is defined with respect to
the scale of the exposure (A).</p>
</td></tr>
<tr><td><code id="dcar_selector_+3A_gn_pred_natural">gn_pred_natural</code></td>
<td>
<p>A <code>matrix</code> of conditional density estimates of
the exposure mechanism g(A|W) along a grid of the regularization parameter,
at the natural (i.e., observed) values of the exposure.</p>
</td></tr>
<tr><td><code id="dcar_selector_+3A_gn_pred_shifted">gn_pred_shifted</code></td>
<td>
<p>A <code>matrix</code> of conditional density estimates of
the exposure mechanism g(A+delta|W) along a grid of the regularization
parameter, at the shifted (i.e., counterfactual) values of the exposure.</p>
</td></tr>
<tr><td><code id="dcar_selector_+3A_qn_pred_natural">Qn_pred_natural</code></td>
<td>
<p>A <code>numeric</code> of the outcome mechanism estimate at
the natural (i.e., observed) values of the exposure. HAL regression is used
for the estimate, with the regularization term chosen by cross-validation.</p>
</td></tr>
<tr><td><code id="dcar_selector_+3A_qn_pred_shifted">Qn_pred_shifted</code></td>
<td>
<p>A <code>numeric</code> of the outcome mechanism estimate at
the shifted (i.e., counterfactual) values of the exposure. HAL regression
is used for the estimate, with the regularization term chosen by
cross-validation.</p>
</td></tr>
</table>

<hr>
<h2 id='fit_haldensify'>Fit Conditional Density Estimation for a Sequence of HAL Models</h2><span id='topic+fit_haldensify'></span>

<h3>Description</h3>

<p>Fit Conditional Density Estimation for a Sequence of HAL Models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_haldensify(
  A,
  W,
  wts = rep(1, length(A)),
  grid_type = "equal_range",
  n_bins = round(c(0.5, 1, 1.5, 2) * sqrt(length(A))),
  cv_folds = 5L,
  lambda_seq = exp(seq(-1, -13, length = 1000L)),
  smoothness_orders = 0L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit_haldensify_+3A_a">A</code></td>
<td>
<p>The <code>numeric</code> vector of observed values.</p>
</td></tr>
<tr><td><code id="fit_haldensify_+3A_w">W</code></td>
<td>
<p>A <code>data.frame</code>, <code>matrix</code>, or similar giving the values of
baseline covariates (potential confounders) for the observed units. These
make up the conditioning set for the conditional density estimate.</p>
</td></tr>
<tr><td><code id="fit_haldensify_+3A_wts">wts</code></td>
<td>
<p>A <code>numeric</code> vector of observation-level weights. The default
is to weight all observations equally.</p>
</td></tr>
<tr><td><code id="fit_haldensify_+3A_grid_type">grid_type</code></td>
<td>
<p>A <code>character</code> indicating the strategy to be used in
creating bins along the observed support of <code>A</code>. For bins of equal
range, use <code>"equal_range"</code>; consult the documentation of
<code><a href="ggplot2.html#topic+cut_interval">cut_interval</a></code> for more information. To ensure each
bin has the same number of observations, use <code>"equal_mass"</code>; consult
the documentation of <code><a href="ggplot2.html#topic+cut_number">cut_number</a></code> for details.</p>
</td></tr>
<tr><td><code id="fit_haldensify_+3A_n_bins">n_bins</code></td>
<td>
<p>This <code>numeric</code> value indicates the number(s) of bins into
which the support of <code>A</code> is to be divided. As with <code>grid_type</code>,
multiple values may be specified, in which case cross-validation will be
used to choose the optimal number of bins. The default sets the candidate
choices of the number of bins based on heuristics tested in simulation.</p>
</td></tr>
<tr><td><code id="fit_haldensify_+3A_cv_folds">cv_folds</code></td>
<td>
<p>A <code>numeric</code> indicating the number of cross-validation
folds to be used in fitting the sequence of HAL conditional density models.</p>
</td></tr>
<tr><td><code id="fit_haldensify_+3A_lambda_seq">lambda_seq</code></td>
<td>
<p>A <code>numeric</code> sequence of values of the regularization
parameter of Lasso regression; passed to <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code>.</p>
</td></tr>
<tr><td><code id="fit_haldensify_+3A_smoothness_orders">smoothness_orders</code></td>
<td>
<p>A <code>integer</code> indicating the smoothness of the
HAL basis functions; passed to <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code>. The default
is set to zero, for indicator basis functions.</p>
</td></tr>
<tr><td><code id="fit_haldensify_+3A_...">...</code></td>
<td>
<p>Additional (optional) arguments of <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code>
that may be used to control fitting of the HAL regression model. Possible
choices include <code>use_min</code>, <code>reduce_basis</code>, <code>return_lasso</code>,
and <code>return_x_basis</code>, but this list is not exhaustive. Consult the
documentation of <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code> for complete details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of the conditional density of A|W via a cross-validated
highly adaptive lasso, used to estimate the conditional hazard of failure
in a given bin over the support of A.
</p>


<h3>Value</h3>

<p>A <code>list</code>, containing density predictions for the sequence of
fitted HAL models; the index and value of the L1 regularization parameter
minimizing the density loss; and the sequence of empirical risks for the
sequence of fitted HAL models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate data: W ~ U[-4, 4] and A|W ~ N(mu = W, sd = 0.5)
n_train &lt;- 50
w &lt;- runif(n_train, -4, 4)
a &lt;- rnorm(n_train, w, 0.5)
# fit cross-validated HAL-based density estimator of A|W
haldensify_cvfit &lt;- fit_haldensify(
  A = a, W = w, n_bins = 10L, lambda_seq = exp(seq(-1, -10, length = 100)),
  # the following arguments are passed to hal9001::fit_hal()
  max_degree = 3, reduce_basis = 1 / sqrt(length(a))
)
</code></pre>

<hr>
<h2 id='format_long_hazards'>Generate Augmented (Long Format) Data for Pooled Hazards Regression</h2><span id='topic+format_long_hazards'></span>

<h3>Description</h3>

<p>Generate Augmented (Long Format) Data for Pooled Hazards Regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_long_hazards(
  A,
  W,
  wts = rep(1, length(A)),
  grid_type = c("equal_range", "equal_mass"),
  n_bins = NULL,
  breaks = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="format_long_hazards_+3A_a">A</code></td>
<td>
<p>The <code>numeric</code> vector or similar of the observed values of an
intervention for a group of observational units of interest.</p>
</td></tr>
<tr><td><code id="format_long_hazards_+3A_w">W</code></td>
<td>
<p>A <code>data.frame</code>, <code>matrix</code>, or similar giving the values of
baseline covariates (potential confounders) for the observed units whose
observed intervention values are provided in the previous argument.</p>
</td></tr>
<tr><td><code id="format_long_hazards_+3A_wts">wts</code></td>
<td>
<p>A <code>numeric</code> vector of observation-level weights. The default
is to weight all observations equally.</p>
</td></tr>
<tr><td><code id="format_long_hazards_+3A_grid_type">grid_type</code></td>
<td>
<p>A <code>character</code> indicating the strategy (or strategies)
to be used in creating bins along the observed support of the intervention
<code>A</code>. For bins of equal range, use &quot;equal_range&quot;; consult documentation
of <code><a href="ggplot2.html#topic+cut_interval">cut_interval</a></code> for more information. To ensure each
bin has the same number of points, use &quot;equal_mass&quot;; consult documentation
of <code><a href="ggplot2.html#topic+cut_number">cut_number</a></code> for details.</p>
</td></tr>
<tr><td><code id="format_long_hazards_+3A_n_bins">n_bins</code></td>
<td>
<p>Only used if <code>grid_type</code> is set to <code>"equal_range"</code>
or <code>"equal_mass"</code>. This <code>numeric</code> value indicates the number(s)
of bins into which the support of <code>A</code> is to be divided.</p>
</td></tr>
<tr><td><code id="format_long_hazards_+3A_breaks">breaks</code></td>
<td>
<p>A <code>numeric</code> vector of break points to be used in dividing
up the support of <code>A</code>. This is passed through the <code>...</code> argument
to <code><a href="base.html#topic+cut.default">cut.default</a></code> by <code><a href="ggplot2.html#topic+cut_interval">cut_interval</a></code>
or <code><a href="ggplot2.html#topic+cut_number">cut_number</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generates an augmented (long format, or repeated measures) dataset
that includes multiple records for each observation, a single record for
each discretized bin up to and including the bin in which a given observed
value of A falls. Such bins are derived from selecting break points over
the support of A. This repeated measures dataset is suitable for estimating
the hazard of failing in a particular bin over A using a highly adaptive
lasso (or other) classification model.
</p>


<h3>Value</h3>

<p>A <code>list</code> containing the break points used in dividing the
support of <code>A</code> into discrete bins, the length of each bin, and the
reformatted data. The reformatted data is a <code>data.table</code> of
repeated measures data, with an indicator for which bin an observation
fails in, the bin ID, observation ID, values of <code>W</code> for each given
observation, and observation-level weights.
</p>

<hr>
<h2 id='gcv_selector'>IPW Estimator Selector via Global Cross-Validation</h2><span id='topic+gcv_selector'></span>

<h3>Description</h3>

<p>IPW Estimator Selector via Global Cross-Validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gcv_selector(
  W,
  A,
  Y,
  delta = 0,
  gn_pred_natural,
  gn_pred_shifted,
  Qn_pred_natural,
  Qn_pred_shifted
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gcv_selector_+3A_w">W</code></td>
<td>
<p>A <code>matrix</code>, <code>data.frame</code>, or similar containing a set of
baseline covariates.</p>
</td></tr>
<tr><td><code id="gcv_selector_+3A_a">A</code></td>
<td>
<p>A <code>numeric</code> vector corresponding to a exposure variable. The
parameter of interest is defined as a location shift of this quantity.</p>
</td></tr>
<tr><td><code id="gcv_selector_+3A_y">Y</code></td>
<td>
<p>A <code>numeric</code> vector of the observed outcomes.</p>
</td></tr>
<tr><td><code id="gcv_selector_+3A_delta">delta</code></td>
<td>
<p>A <code>numeric</code> value indicating the shift in the exposure to
be used in defining the target parameter. This is defined with respect to
the scale of the exposure (A).</p>
</td></tr>
<tr><td><code id="gcv_selector_+3A_gn_pred_natural">gn_pred_natural</code></td>
<td>
<p>A <code>matrix</code> of conditional density estimates of
the exposure mechanism g(A|W) along a grid of the regularization parameter,
at the natural (observed, actual) values of the exposure.</p>
</td></tr>
<tr><td><code id="gcv_selector_+3A_gn_pred_shifted">gn_pred_shifted</code></td>
<td>
<p>A <code>matrix</code> of conditional density estimates of
the exposure mechanism g(A+delta|W) along a grid of the regularization
parameter, at the shifted (counterfactual) values of the exposure.</p>
</td></tr>
<tr><td><code id="gcv_selector_+3A_qn_pred_natural">Qn_pred_natural</code></td>
<td>
<p>A <code>numeric</code> of the outcome mechanism estimate at
the natural (i.e., observed) values of the exposure. HAL regression is used
for the estimate, with the regularization term chosen by cross-validation.</p>
</td></tr>
<tr><td><code id="gcv_selector_+3A_qn_pred_shifted">Qn_pred_shifted</code></td>
<td>
<p>A <code>numeric</code> of the outcome mechanism estimate at
the shifted (i.e., counterfactual) values of the exposure. HAL regression
is used for the estimate, with the regularization term chosen by
cross-validation.</p>
</td></tr>
</table>

<hr>
<h2 id='haldensify'>Cross-validated HAL Conditional Density Estimation</h2><span id='topic+haldensify'></span>

<h3>Description</h3>

<p>Cross-validated HAL Conditional Density Estimation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>haldensify(
  A,
  W,
  wts = rep(1, length(A)),
  grid_type = "equal_range",
  n_bins = round(c(0.5, 1, 1.5, 2) * sqrt(length(A))),
  cv_folds = 5L,
  lambda_seq = exp(seq(-1, -13, length = 1000L)),
  smoothness_orders = 0L,
  hal_basis_list = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="haldensify_+3A_a">A</code></td>
<td>
<p>The <code>numeric</code> vector observed values.</p>
</td></tr>
<tr><td><code id="haldensify_+3A_w">W</code></td>
<td>
<p>A <code>data.frame</code>, <code>matrix</code>, or similar giving the values of
baseline covariates (potential confounders) for the observed units. These
make up the conditioning set for the density estimate. For estimation of a
marginal density, specify a constant <code>numeric</code> vector or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="haldensify_+3A_wts">wts</code></td>
<td>
<p>A <code>numeric</code> vector of observation-level weights. The default
is to weight all observations equally.</p>
</td></tr>
<tr><td><code id="haldensify_+3A_grid_type">grid_type</code></td>
<td>
<p>A <code>character</code> indicating the strategy to be used in
creating bins along the observed support of <code>A</code>. For bins of equal
range, use <code>"equal_range"</code>; consult the documentation of
<code><a href="ggplot2.html#topic+cut_interval">cut_interval</a></code> for more information. To ensure each
bin has the same number of observations, use <code>"equal_mass"</code>; consult
the documentation of <code><a href="ggplot2.html#topic+cut_number">cut_number</a></code> for details. The
default is <code>"equal_range"</code> since this has been found to provide better
performance in simulation experiments; however, both types may be specified
(i.e., <code>c("equal_range", "equal_mass")</code>) together, in which case
cross-validation will be used to select the optimal binning strategy.</p>
</td></tr>
<tr><td><code id="haldensify_+3A_n_bins">n_bins</code></td>
<td>
<p>This <code>numeric</code> value indicates the number(s) of bins into
which the support of <code>A</code> is to be divided. As with <code>grid_type</code>,
multiple values may be specified, in which case cross-validation will be
used to choose the optimal number of bins. The default sets the candidate
choices of the number of bins based on heuristics tested in simulation.</p>
</td></tr>
<tr><td><code id="haldensify_+3A_cv_folds">cv_folds</code></td>
<td>
<p>A <code>numeric</code> indicating the number of cross-validation
folds to be used in fitting the sequence of HAL conditional density models.</p>
</td></tr>
<tr><td><code id="haldensify_+3A_lambda_seq">lambda_seq</code></td>
<td>
<p>A <code>numeric</code> sequence of values of the regularization
parameter of Lasso regression; passed to <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code> via
its argument <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="haldensify_+3A_smoothness_orders">smoothness_orders</code></td>
<td>
<p>A <code>integer</code> indicating the smoothness of the
HAL basis functions; passed to <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code>. The default
is set to zero, for indicator basis functions.</p>
</td></tr>
<tr><td><code id="haldensify_+3A_hal_basis_list">hal_basis_list</code></td>
<td>
<p>A <code>list</code> consisting of a preconstructed set of
HAL basis functions, as produced by <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code>. The
default of <code>NULL</code> results in creating such a set of basis functions.
When specified, this is passed directly to the HAL model fitted upon the
augmented (repeated measures) data structure, resulting in a much lowered
computational cost. This is useful, for example, in fitting HAL conditional
density estimates with external cross-validation or bootstrap samples.</p>
</td></tr>
<tr><td><code id="haldensify_+3A_...">...</code></td>
<td>
<p>Additional (optional) arguments of <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code>
that may be used to control fitting of the HAL regression model. Possible
choices include <code>use_min</code>, <code>reduce_basis</code>, <code>return_lasso</code>,
and <code>return_x_basis</code>, but this list is not exhaustive. Consult the
documentation of <code><a href="hal9001.html#topic+fit_hal">fit_hal</a></code> for complete details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of the conditional density A|W through using the highly
adaptive lasso to estimate the conditional hazard of failure in a given
bin over the support of A. Cross-validation is used to select the optimal
value of the penalization parameters, based on minimization of the weighted
log-likelihood loss for a density.
</p>


<h3>Value</h3>

<p>Object of class <code>haldensify</code>, containing a fitted
<code>hal9001</code> object; a vector of break points used in binning <code>A</code>
over its support <code>W</code>; sizes of the bins used in each fit; the tuning
parameters selected by cross-validation; the full sequence (in lambda) of
HAL models for the CV-selected number of bins and binning strategy; and
the range of <code>A</code>.
</p>


<h3>Note</h3>

<p>Parallel evaluation of the cross-validation procedure to select tuning
parameters for density estimation may be invoked via the framework exposed
in the <span class="pkg">future</span> ecosystem. Specifically, set <code><a href="future.html#topic+plan">plan</a></code>
for <code><a href="future.apply.html#topic+future_mapply">future_mapply</a></code> to be used internally.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate data: W ~ U[-4, 4] and A|W ~ N(mu = W, sd = 0.5)
set.seed(429153)
n_train &lt;- 50
w &lt;- runif(n_train, -4, 4)
a &lt;- rnorm(n_train, w, 0.5)
# learn relationship A|W using HAL-based density estimation procedure
haldensify_fit &lt;- haldensify(
  A = a, W = w, n_bins = 10L, lambda_seq = exp(seq(-1, -10, length = 100)),
  # the following arguments are passed to hal9001::fit_hal()
  max_degree = 3, reduce_basis = 1 / sqrt(length(a))
)
</code></pre>

<hr>
<h2 id='ipw_shift'>IPW Estimates of the Causal Effects of Stochatic Shift Interventions</h2><span id='topic+ipw_shift'></span>

<h3>Description</h3>

<p>IPW Estimates of the Causal Effects of Stochatic Shift Interventions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ipw_shift(
  W,
  A,
  Y,
  delta,
  n_bins = make_bins(A, "hist"),
  cv_folds = 10L,
  lambda_seq,
  ...,
  bin_type = c("equal_range", "equal_mass"),
  trim_density = FALSE,
  undersmooth_type = c("dcar", "plateau", "gcv", "all"),
  bootstrap = FALSE,
  n_boot = 1000L
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ipw_shift_+3A_w">W</code></td>
<td>
<p>A <code>matrix</code>, <code>data.frame</code>, or similar containing a set of
baseline covariates.</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_a">A</code></td>
<td>
<p>A <code>numeric</code> vector corresponding to a exposure variable. The
parameter of interest is defined as a location shift of this quantity.</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_y">Y</code></td>
<td>
<p>A <code>numeric</code> vector of the observed outcomes.</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_delta">delta</code></td>
<td>
<p>A <code>numeric</code> value indicating the shift in the exposure to
be used in defining the target parameter. This is defined with respect to
the scale of the exposure (A).</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_n_bins">n_bins</code></td>
<td>
<p>A <code>numeric</code>, scalar or vector, indicating the number of
bins into which the support of A is to be partitioned for constructing
conditional density estimates.</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_cv_folds">cv_folds</code></td>
<td>
<p>A <code>numeric</code> giving the number of folds to be used for
cross-validation. Note that this form of sample splitting is used for the
selection of tuning parameters by empirical risk minimization, not for the
estimation of nuisance parameters (i.e., to relax regularity conditions).</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_lambda_seq">lambda_seq</code></td>
<td>
<p>A <code>numeric</code> sequence of the regularization parameter
(L1 norm of HAL coefficients) to be used in fitting HAL models.</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_...">...</code></td>
<td>
<p>Additional arguments for model fitting to be passed directly to
<code><a href="#topic+haldensify">haldensify</a></code>.</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_bin_type">bin_type</code></td>
<td>
<p>A <code>character</code> indicating the strategy to be used in
creating bins along the observed support of <code>A</code>. For bins of equal
range, use <code>"equal_range"</code>; to ensure each bin has the same number of
observations, use instead <code>"equal_mass"</code>. For more information, see
documentation of <code>grid_type</code> in <code><a href="#topic+haldensify">haldensify</a></code>.</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_trim_density">trim_density</code></td>
<td>
<p>A <code>logical</code> indicating whether estimates of the
conditional density should be trimmed. Refer to the <code>trim</code> argument of
the <code>predict</code> method of <code>haldensify</code> for details. The default is
<code>FALSE</code> since propensity score truncation can lead to estimation bias.</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_undersmooth_type">undersmooth_type</code></td>
<td>
<p>A <code>character</code> indicating the selection strategy
to be used in identifying an efficent IPW estimator. The choices include
<code>"gcv"</code> for global cross-validation, <code>"dcar"</code> for solving the
IPW representation of the EIF through, and <code>"plateau"</code> for an approach
that balances changes in the parameter estimate and its mean squared error,
based on Lepski's method. The option <code>"all"</code> produces results based on
all three selection strategies, sharing redundant computation between each.</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_bootstrap">bootstrap</code></td>
<td>
<p>A <code>logical</code> indicating whether the estimator variance
should be approximated using the nonparametric bootstrap. The default is
<code>FALSE</code>, in which case the empirical variances of the IPW estimating
function and the EIF are used for for estimator selection and for variance
estimation, respectively. When set to <code>TRUE</code>, the bootstrap variance
is used for both of these purposes instead. Note that the bootstrap is very
computationally intensive and scales relatively poorly.</p>
</td></tr>
<tr><td><code id="ipw_shift_+3A_n_boot">n_boot</code></td>
<td>
<p>A <code>numeric</code> giving the number of bootstrap re-samples to
be used in computing the mean squared error as part of the plateau selector
criterion. Ignored when <code>undersmooth_type</code> is not <code>"plateau"</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># simulate data
n_obs &lt;- 50
W1 &lt;- rbinom(n_obs, 1, 0.6)
W2 &lt;- rbinom(n_obs, 1, 0.2)
A &lt;- rnorm(n_obs, (2 * W1 - W2 - W1 * W2), 2)
Y &lt;- rbinom(n_obs, 1, plogis(3 * A + W1 + W2 - W1 * W2))

# fit the IPW estimator
est_ipw_shift &lt;- ipw_shift(
  W = cbind(W1, W2), A = A, Y = Y,
  delta = 0.5, n_bins = 3L, cv_folds = 2L,
  lambda_seq = exp(seq(-1, -10, length = 100L)),
  # arguments passed to hal9001::fit_hal()
  max_degree = 1,
  # ...continue arguments for IPW
  undersmooth_type = "gcv"
)
</code></pre>

<hr>
<h2 id='make_bins'>Histogram Binning Procedures for Pooled Hazards Regression</h2><span id='topic+make_bins'></span>

<h3>Description</h3>

<p>Histogram Binning Procedures for Pooled Hazards Regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_bins(grid_var, grid_type = c("hist", "scaled"), max_bins = 30L)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="make_bins_+3A_grid_var">grid_var</code></td>
<td>
<p>The <code>numeric</code> vector over which histogram-based binning
is to be performed.</p>
</td></tr>
<tr><td><code id="make_bins_+3A_grid_type">grid_type</code></td>
<td>
<p>A <code>character</code> indicating the choice of binning rule,
with <code>"hist"</code> corresponding to the use of several rules proposed for
optimal histogram construction and <code>"scaled"</code> corresponding to the use
of various pre-set multiples of the square root of the sample size.</p>
</td></tr>
<tr><td><code id="make_bins_+3A_max_bins">max_bins</code></td>
<td>
<p>A <code>numeric</code> indicating the maximum number of bins that
are allowed in the grid for building the histogram based discretization.</p>
</td></tr>
</table>

<hr>
<h2 id='map_hazard_to_density'>Map Predicted Hazard to Predicted Density for a Single Observation</h2><span id='topic+map_hazard_to_density'></span>

<h3>Description</h3>

<p>Map Predicted Hazard to Predicted Density for a Single Observation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>map_hazard_to_density(hazard_pred_single_obs)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="map_hazard_to_density_+3A_hazard_pred_single_obs">hazard_pred_single_obs</code></td>
<td>
<p>A <code>numeric</code> vector of predicted hazard of
failure in a given bin (under a given partitioning of the support) for a
single observational unit based on a long format data structure (from
<code><a href="#topic+format_long_hazards">format_long_hazards</a></code>). This is the probability that a given
value falls in a corresponding bin, given that it has not yet failed
(fallen in a preceding bin), as per Díaz I, van der Laan MJ (2011).
&ldquo;Super learner based conditional density estimation with application to marginal structural models.&rdquo;
<em>International Journal of Biostatistics</em>, <b>7</b>(1), 1&ndash;20..</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a single observation, map a predicted hazard of failure (as an
occurrence in a particular bin, under a given partitioning of the support)
to a density.
</p>


<h3>Value</h3>

<p>A <code>matrix</code> composed of a single row and a number of columns
specified by the grid of penalization parameters used in fitting of the
highly adaptive lasso. This is the predicted conditional density for a
given observation, re-mapped from the hazard scale.
</p>

<hr>
<h2 id='plateau_selector'>IPW Estimator Selector Using Lepski's Plateau Method for the MSE</h2><span id='topic+plateau_selector'></span>

<h3>Description</h3>

<p>IPW Estimator Selector Using Lepski's Plateau Method for the MSE
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plateau_selector(
  W,
  A,
  Y,
  delta = 0,
  gn_pred_natural,
  gn_pred_shifted,
  gn_fit_haldensify,
  Qn_pred_natural,
  Qn_pred_shifted,
  cv_folds = 10L,
  gcv_mult = 50L,
  bootstrap = FALSE,
  n_boot = 1000L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plateau_selector_+3A_w">W</code></td>
<td>
<p>A <code>matrix</code>, <code>data.frame</code>, or similar containing a set of
baseline covariates.</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_a">A</code></td>
<td>
<p>A <code>numeric</code> vector corresponding to a exposure variable. The
parameter of interest is defined as a location shift of this quantity.</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_y">Y</code></td>
<td>
<p>A <code>numeric</code> vector of the observed outcomes.</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_delta">delta</code></td>
<td>
<p>A <code>numeric</code> value indicating the shift in the exposure to
be used in defining the target parameter. This is defined with respect to
the scale of the exposure (A).</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_gn_pred_natural">gn_pred_natural</code></td>
<td>
<p>A <code>matrix</code> of conditional density estimates of
the exposure mechanism g(A|W) along a grid of the regularization parameter,
at the natural (observed, actual) values of the exposure.</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_gn_pred_shifted">gn_pred_shifted</code></td>
<td>
<p>A <code>matrix</code> of conditional density estimates of
the exposure mechanism g(A+delta|W) along a grid of the regularization
parameter, at the shifted (counterfactual) values of the exposure.</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_gn_fit_haldensify">gn_fit_haldensify</code></td>
<td>
<p>An object of class <code>haldensify</code> of the fitted
conditional density model for the natural exposure mechanism. This should
be the fit object returned by <code><a href="#topic+haldensify">haldensify</a>[haldensify]</code> as part
of a call to <code><a href="#topic+ipw_shift">ipw_shift</a></code>.</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_qn_pred_natural">Qn_pred_natural</code></td>
<td>
<p>A <code>numeric</code> of the outcome mechanism estimate at
the natural (i.e., observed) values of the exposure. HAL regression is used
for the estimate, with the regularization term chosen by cross-validation.</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_qn_pred_shifted">Qn_pred_shifted</code></td>
<td>
<p>A <code>numeric</code> of the outcome mechanism estimate at
the shifted (i.e., counterfactual) values of the exposure. HAL regression
is used for the estimate, with the regularization term chosen by
cross-validation.</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_cv_folds">cv_folds</code></td>
<td>
<p>A <code>numeric</code> giving the number of folds to be used for
cross-validation. Note that this form of sample splitting is used for the
selection of tuning parameters by empirical risk minimization, not for the
estimation of nuisance parameters (i.e., to relax regularity conditions).</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_gcv_mult">gcv_mult</code></td>
<td>
<p>TODO</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_bootstrap">bootstrap</code></td>
<td>
<p>A <code>logical</code> indicating whether the estimator variance
should be approximated using the nonparametric bootstrap. The default is
<code>FALSE</code>, in which case the empirical variances of the IPW estimating
function and the EIF are used for for estimator selection and for variance
estimation, respectively. When set to <code>TRUE</code>, the bootstrap variance
is used for both of these purposes instead. Note that the bootstrap is very
computationally intensive and scales relatively poorly.</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_n_boot">n_boot</code></td>
<td>
<p>A <code>numeric</code> giving the number of bootstrap re-samples to
be used in computing the plateau estimator selection criterion. The default
uses 1000 bootstrap samples, though it may be appropriate to use fewer such
samples for experimentation purposes. This is ignored when <code>bootstrap</code>
is set to <code>FALSE</code> (its default).</p>
</td></tr>
<tr><td><code id="plateau_selector_+3A_...">...</code></td>
<td>
<p>Additional arguments for model fitting to be passed directly to
<code><a href="#topic+haldensify">haldensify</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='plot.haldensify'>Plot Method for HAL Conditional Density Estimates</h2><span id='topic+plot.haldensify'></span>

<h3>Description</h3>

<p>Plot Method for HAL Conditional Density Estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'haldensify'
plot(x, ..., type = c("risk", "density"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.haldensify_+3A_x">x</code></td>
<td>
<p>Object of class <code>haldensify</code>, containing conditional density
estimates, as produced by <code><a href="#topic+haldensify">haldensify</a></code>.</p>
</td></tr>
<tr><td><code id="plot.haldensify_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed <code>plot</code>, currently ignored.</p>
</td></tr>
<tr><td><code id="plot.haldensify_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of plot to be produced.
Options include visualizing the empirical risks of the conditional density
estimators across a grid of values of the regularization parameter and a
plot of the estimated conditional density (based on the estimator selected
by cross-validation). The latter has yet to be implemented.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>ggplot</code> containing a plot of the desired
<code>type</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate data: W ~ U[-4, 4] and A|W ~ N(mu = W, sd = 0.5)
n_train &lt;- 50
w &lt;- runif(n_train, -4, 4)
a &lt;- rnorm(n_train, w, 0.5)
# learn relationship A|W using HAL-based density estimation procedure
haldensify_fit &lt;- haldensify(
  A = a, W = w, n_bins = 3,
  lambda_seq = exp(seq(-1, -10, length = 50)),
  # the following arguments are passed to hal9001::fit_hal()
  max_degree = 3, reduce_basis = 0.1
)
plot(haldensify_fit)
</code></pre>

<hr>
<h2 id='predict.haldensify'>Prediction Method for HAL Conditional Density Estimation</h2><span id='topic+predict.haldensify'></span>

<h3>Description</h3>

<p>Prediction Method for HAL Conditional Density Estimation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'haldensify'
predict(
  object,
  ...,
  new_A,
  new_W,
  trim = TRUE,
  trim_min = NULL,
  lambda_select = c("cv", "undersmooth", "all")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.haldensify_+3A_object">object</code></td>
<td>
<p>An object of class <code><a href="#topic+haldensify">haldensify</a></code>, containing the
results of fitting the highly adaptive lasso for conditional density
estimation, as produced by a call to <code><a href="#topic+haldensify">haldensify</a></code>.</p>
</td></tr>
<tr><td><code id="predict.haldensify_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>predict</code> as necessary.</p>
</td></tr>
<tr><td><code id="predict.haldensify_+3A_new_a">new_A</code></td>
<td>
<p>The <code>numeric</code> vector or similar of the observed values for
which a conditional density estimate is to be generated.</p>
</td></tr>
<tr><td><code id="predict.haldensify_+3A_new_w">new_W</code></td>
<td>
<p>A <code>data.frame</code>, <code>matrix</code>, or similar giving the
values of baseline covariates (potential confounders) for the conditioning
set of the observed values <code>A</code>.</p>
</td></tr>
<tr><td><code id="predict.haldensify_+3A_trim">trim</code></td>
<td>
<p>A <code>logical</code> indicating whether estimates of the conditional
density below the value indicated in <code>trim_min</code> should be truncated.
The default value of <code>TRUE</code> enforces truncation of any values below
the cutoff specified in <code>trim_min</code> and similarly truncates predictions
for any of <code>new_A</code> falling outside of the training support.</p>
</td></tr>
<tr><td><code id="predict.haldensify_+3A_trim_min">trim_min</code></td>
<td>
<p>A <code>numeric</code> indicating the minimum allowed value of the
resultant density predictions. Any predicted density values below this
tolerance threshold are set to the indicated minimum. The default is to use
a scaled inverse square root of the sample size of the prediction set,
i.e., 5/sqrt(n)/log(n) (another notable choice is 1/sqrt(n)). If there are
observations in the prediction set with values of <code>new_A</code> outside of
the support of the training set (i.e., provided in the argument <code>A</code> to
<code><a href="#topic+haldensify">haldensify</a></code>), their predictions are similarly truncated.</p>
</td></tr>
<tr><td><code id="predict.haldensify_+3A_lambda_select">lambda_select</code></td>
<td>
<p>A <code>character</code> indicating whether to return the
predicted density for the value of the regularization parameter chosen by
the global cross-validation selector or whether to return an undersmoothed
sequence (which starts with the cross-validation selector's choice but also
includes all values in the sequence that are less restrictive). The default
is <code>"cv"</code> for the global cross-validation selector. Setting the choice
to <code>"undersmooth"</code> returns a matrix of predicted densities, with each
column corresponding to a value of the regularization parameter less than
or equal to the choice made by the global cross-validation selector. When
<code>"all"</code> is set, predictions are returned for the full sequence of the
regularization parameter on which the HAL model <code>object</code> was fitted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method for computing and extracting predictions of the conditional
density estimates based on the highly adaptive lasso estimator, returned as
an S3 object of class <code>haldensify</code> from <code><a href="#topic+haldensify">haldensify</a></code>.
</p>


<h3>Value</h3>

<p>A <code>numeric</code> vector of predicted conditional density values from
a fitted <code>haldensify</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate data: W ~ U[-4, 4] and A|W ~ N(mu = W, sd = 0.5)
n_train &lt;- 50
w &lt;- runif(n_train, -4, 4)
a &lt;- rnorm(n_train, w, 0.5)
# HAL-based density estimator of A|W
haldensify_fit &lt;- haldensify(
  A = a, W = w, n_bins = 10L, lambda_seq = exp(seq(-1, -10, length = 100)),
  # the following arguments are passed to hal9001::fit_hal()
  max_degree = 3, reduce_basis = 1 / sqrt(length(a))
)
# predictions to recover conditional density of A|W
new_a &lt;- seq(-4, 4, by = 0.1)
new_w &lt;- rep(0, length(new_a))
pred_dens &lt;- predict(haldensify_fit, new_A = new_a, new_W = new_w)
</code></pre>

<hr>
<h2 id='print.haldensify'>Print: Highly Adaptive Lasso Conditional Density Estimates</h2><span id='topic+print.haldensify'></span>

<h3>Description</h3>

<p>Print: Highly Adaptive Lasso Conditional Density Estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'haldensify'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.haldensify_+3A_x">x</code></td>
<td>
<p>An object of class <code>haldensify</code>.</p>
</td></tr>
<tr><td><code id="print.haldensify_+3A_...">...</code></td>
<td>
<p>Other options (not currently used).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>print</code> method for objects of class <code>haldensify</code>
</p>


<h3>Value</h3>

<p>None. Called for the side effect of printing an informative summary
of slots of objects of class <code>haldensify</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate data: W ~ U[-4, 4] and A|W ~ N(mu = W, sd = 0.5)
set.seed(429153)
n_train &lt;- 50
w &lt;- runif(n_train, -4, 4)
a &lt;- rnorm(n_train, w, 0.5)

# learn relationship A|W using HAL-based density estimation procedure
haldensify_fit &lt;- haldensify(
  A = a, W = w, n_bins = c(3, 5),
  lambda_seq = exp(seq(-1, -15, length = 50L)),
  max_degree = 3, reduce_basis = 0.1
)
print(haldensify_fit)
</code></pre>

<hr>
<h2 id='print.ipw_haldensify'>Print: IPW Estimates of the Causal Effects of Stochatic Shift Interventions</h2><span id='topic+print.ipw_haldensify'></span>

<h3>Description</h3>

<p>Print: IPW Estimates of the Causal Effects of Stochatic Shift Interventions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ipw_haldensify'
print(x, ..., ci_level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.ipw_haldensify_+3A_x">x</code></td>
<td>
<p>An object of class <code>ipw_haldensify</code>.</p>
</td></tr>
<tr><td><code id="print.ipw_haldensify_+3A_...">...</code></td>
<td>
<p>Other options (not currently used).</p>
</td></tr>
<tr><td><code id="print.ipw_haldensify_+3A_ci_level">ci_level</code></td>
<td>
<p>A <code>numeric</code> indicating the level of the confidence
interval to be computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>print</code> method for objects of class <code>ipw_haldensify</code>
</p>


<h3>Value</h3>

<p>None. Called for the side effect of printing an informative summary
of slots of objects of class <code>ipw_haldensify</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate data
n_obs &lt;- 50
W1 &lt;- rbinom(n_obs, 1, 0.6)
W2 &lt;- rbinom(n_obs, 1, 0.2)
A &lt;- rnorm(n_obs, (2 * W1 - W2 - W1 * W2), 2)
Y &lt;- rbinom(n_obs, 1, plogis(3 * A + W1 + W2 - W1 * W2))

# fit the IPW estimator
est_ipw_shift &lt;- ipw_shift(
  W = cbind(W1, W2), A = A, Y = Y,
  delta = 0.5, n_bins = 3L, cv_folds = 2L,
  lambda_seq = exp(seq(-1, -10, length = 100L)),
  # arguments passed to hal9001::fit_hal()
  max_degree = 1,
  # ...continue arguments for IPW
  undersmooth_type = "gcv"
)
print(est_ipw_shift)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
