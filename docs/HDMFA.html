<!DOCTYPE html><html><head><title>Help for package HDMFA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {HDMFA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#alpha_PCA'>
<p>Statistical Inference for High-Dimensional Matrix-Variate Factor Model</p></a></li>
<li><a href='#KMHFA'>
<p>Estimating the Pair of Factor Numbers via Eigenvalue Ratios or Rank Minimization.</p></a></li>
<li><a href='#KPCA'>
<p>Estimating the Pair of Factor Numbers via Eigenvalue Ratios Corresponding to <code class="reqn">\alpha</code>-PCA</p></a></li>
<li><a href='#KPE'>
<p>Estimating the Pair of Factor Numbers via Eigenvalue Ratios Corresponding to PE</p></a></li>
<li><a href='#MHFA'>
<p>Matrix Huber Factor Analysis</p></a></li>
<li><a href='#PE'>
<p>Projected Estimation for Large-Dimensional Matrix Factor Models</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>High-Dimensional Matrix Factor Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Yong He [aut],
  Changwei Zhao [aut],
  Ran Zhao [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ran Zhao &lt;Zhaoran@mail.sdu.edu.cn&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>High-dimensional matrix factor models have drawn much attention in view of the fact that observations are usually well structured to be an array such as in macroeconomics and finance. In addition, data often exhibit heavy-tails and thus it is also important to develop robust procedures. We aim to address this issue by replacing the least square loss with Huber loss function. We propose two algorithms to do robust factor analysis by considering the Huber loss. One is based on minimizing the Huber loss of the idiosyncratic error's Frobenius norm, which leads to a weighted iterative projection approach to compute and learn the parameters and thereby named as Robust-Matrix-Factor-Analysis (RMFA), see the details in He et al. (2023)&lt;<a href="https://doi.org/10.1080%2F07350015.2023.2191676">doi:10.1080/07350015.2023.2191676</a>&gt;. The other one is based on minimizing the element-wise Huber loss, which can be solved by an iterative Huber regression algorithm (IHR), see the details in He et al. (2023) &lt;<a href="https://doi.org/10.48550/arXiv.2306.03317">doi:10.48550/arXiv.2306.03317</a>&gt;. In this package, we also provide the algorithm for alpha-PCA by Chen &amp; Fan (2021) &lt;<a href="https://doi.org/10.1080%2F01621459.2021.1970569">doi:10.1080/01621459.2021.1970569</a>&gt;, the Projected estimation (PE) method by Yu et al. (2022)&lt;<a href="https://doi.org/10.1016%2Fj.jeconom.2021.04.001">doi:10.1016/j.jeconom.2021.04.001</a>&gt;. In addition, the methods for determining the pair of factor numbers are also given.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, RSpectra</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-20 03:14:41 UTC; 赵冉</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-20 07:32:46 UTC</td>
</tr>
</table>
<hr>
<h2 id='alpha_PCA'>
Statistical Inference for High-Dimensional Matrix-Variate Factor Model
</h2><span id='topic+alpha_PCA'></span>

<h3>Description</h3>

<p>This function is to fit the matrix factor model via the <code class="reqn">\alpha</code>-PCA method by conducting eigen-analysis of a weighted average of the sample mean and the column (row) sample covariance matrix through a hyper-parameter <code class="reqn">\alpha</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alpha_PCA(X, m1, m2, alpha = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alpha_PCA_+3A_x">X</code></td>
<td>

<p>Input an array with <code class="reqn">T \times p_1 \times p_2</code>, where <code class="reqn">T</code> is the sample size, <code class="reqn">p_1</code> is the the row dimension of each matrix observation and <code class="reqn">p_2</code> is the the column dimension of each matrix observation.
</p>
</td></tr>
<tr><td><code id="alpha_PCA_+3A_m1">m1</code></td>
<td>

<p>A positive integer indicating the row factor numbers.
</p>
</td></tr>
<tr><td><code id="alpha_PCA_+3A_m2">m2</code></td>
<td>

<p>A positive integer indicating the column factor numbers.
</p>
</td></tr>
<tr><td><code id="alpha_PCA_+3A_alpha">alpha</code></td>
<td>

<p>A hyper-parameter balancing the information of the first and second moments    (<code class="reqn">\alpha \geq -1</code> ). The default is 0.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the matrix factor models, Chen &amp; Fan (2021) propose an estimation procedure, i.e. <code class="reqn">\alpha</code>-PCA. The method aggregates the information in both first and second moments and extract it via a spectral method. In detail, for observations <code class="reqn">\bold{X}_t, t=1,2,\cdots,T</code>, define 
</p>
<p style="text-align: center;"><code class="reqn">\hat{\bold{M}}_R = \frac{1}{p_1 p_2} \left( (1+\alpha) \bar{\bold{X}} \bar{\bold{X}}^\top + \frac{1}{T} \sum_{t=1}^T (\bold{X}_t - \bar{\bold{X}}) (\bold{X}_t - \bar{\bold{X}})^\top \right),</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{\bold{M}}_C = \frac{1}{p_1 p_2} \left( (1+\alpha) \bar{\bold{X}}^\top \bar{\bold{X}} + \frac{1}{T} \sum_{t=1}^T (\bold{X}_t - \bar{\bold{X}})^\top (\bold{X}_t - \bar{\bold{X}}) \right),</code>
</p>
 
<p>where <code class="reqn">\alpha \in</code> [-1,<code class="reqn">+\infty</code>], <code class="reqn">\bar{\bold{X}} = \frac{1}{T} \sum_{t=1}^T \bold{X}_t</code>, <code class="reqn">\frac{1}{T} \sum_{t=1}^T (\bold{X}_t - \bar{\bold{X}}) (\bold{X}_t - \bar{\bold{X}})^\top</code> and <code class="reqn">\frac{1}{T} \sum_{t=1}^T (\bold{X}_t - \bar{\bold{X}})^\top (\bold{X}_t - \bar{\bold{X}})</code> are the sample row and column covariance matrix, respectively. The loading matrices <code class="reqn">\bold{R}</code> and <code class="reqn">\bold{C}</code> are estimated as <code class="reqn">\sqrt{p_1}</code> times the top <code class="reqn">k_1</code> eigenvectors of <code class="reqn">\hat{\bold{M}}_R</code> and <code class="reqn">\sqrt{p_2}</code> times the top <code class="reqn">k_2</code> eigenvectors of <code class="reqn">\hat{\bold{M}}_C</code>, respectively. For details, see Chen &amp; Fan (2021). 
</p>


<h3>Value</h3>

<p>The return value is a list. In this list, it contains the following:
</p>
<table>
<tr><td><code>F</code></td>
<td>
<p>The estimated factor matrix of dimension <code class="reqn">T \times m_1\times m_2</code>.</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>The estimated row loading matrix of dimension <code class="reqn">p_1\times m_1</code>, satisfying <code class="reqn">\bold{R}^\top\bold{R}=p_1\bold{I}_{m_1}</code>.</p>
</td></tr>
<tr><td><code>C</code></td>
<td>
<p>The estimated column loading matrix of dimension <code class="reqn">p_2\times m_2</code>, satisfying <code class="reqn">\bold{C}^\top\bold{C}=p_2\bold{I}_{m_2}</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yong He, Changwei Zhao, Ran Zhao.
</p>


<h3>References</h3>

<p>Chen, E. Y., &amp; Fan, J. (2021). Statistical inference for high-dimensional matrix-variate factor models. Journal of the American Statistical Association, 1-18.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   set.seed(11111)
   T=20;p1=20;p2=20;k1=3;k2=3
   R=matrix(runif(p1*k1,min=-1,max=1),p1,k1)
   C=matrix(runif(p2*k2,min=-1,max=1),p2,k2)
   X=array(0,c(T,p1,p2))
   Y=X;E=Y
   F=array(0,c(T,k1,k2))
   for(t in 1:T){
     F[t,,]=matrix(rnorm(k1*k2),k1,k2)
     E[t,,]=matrix(rnorm(p1*p2),p1,p2)
     Y[t,,]=R%*%F[t,,]%*%t(C)
   }
   X=Y+E
   
   #Estimate the factor matrices and loadings
   fit=alpha_PCA(X, k1, k2, alpha = 0)
   Rhat=fit$R 
   Chat=fit$C
   Fhat=fit$F
   
   #Estimate the common component
   CC=array(0,c(T,p1,p2))
   for (t in 1:T){
   CC[t,,]=Rhat%*%Fhat[t,,]%*%t(Chat)
   }
   CC
</code></pre>

<hr>
<h2 id='KMHFA'>
Estimating the Pair of Factor Numbers via Eigenvalue Ratios or Rank Minimization.
</h2><span id='topic+KMHFA'></span>

<h3>Description</h3>

<p>The function is to estimate the pair of factor numbers via eigenvalue-ratio corresponding to RMFA method or rank minimization and eigenvalue-ratio corresponding to Iterative Huber Regression (IHR). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMHFA(X, W1 = NULL, W2 = NULL, kmax, method, max_iter = 100, c = 1e-04, ep = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KMHFA_+3A_x">X</code></td>
<td>

<p>Input an array with <code class="reqn">T \times p_1 \times p_2</code>, where <code class="reqn">T</code> is the sample size, <code class="reqn">p_1</code> is the the row dimension of each matrix observation and <code class="reqn">p_2</code> is the the column dimension of each matrix observation.
</p>
</td></tr>
<tr><td><code id="KMHFA_+3A_w1">W1</code></td>
<td>

<p>Only if <code>method="E_RM"</code> or <code>method="E_ER"</code>, the inital value of row loadings matrix. The default is NULL, which is randomly chosen and all entries from a standard normal distribution.
</p>
</td></tr>
<tr><td><code id="KMHFA_+3A_w2">W2</code></td>
<td>

<p>Only if <code>method="E_RM"</code> or <code>method="E_ER"</code>, the inital value of column loadings matrix. The default is NULL, which is randomly chosen and all entries from a standard normal distribution.
</p>
</td></tr>
<tr><td><code id="KMHFA_+3A_kmax">kmax</code></td>
<td>

<p>The user-supplied maximum factor numbers. Here it means the upper bound of the number of row factors and column factors.
</p>
</td></tr>
<tr><td><code id="KMHFA_+3A_method">method</code></td>
<td>

<p>Character string, specifying the type of the estimation method to be used.
</p>

<dl>
<dt><code>"P",</code></dt><dd><p>the robust iterative eigenvalue-ratio based on RMFA</p>
</dd>
<dt><code>"E_RM",</code></dt><dd><p>the rank-minimization based on IHR</p>
</dd>
<dt><code>"E_ER",</code></dt><dd><p>the eigenvalue-ratio based on IHR</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="KMHFA_+3A_max_iter">max_iter</code></td>
<td>

<p>Only if <code>method="E_RM"</code> or <code>method="E_ER"</code>, the maximum number of iterations in the iterative Huber regression algorithm. The default is 100.
</p>
</td></tr>
<tr><td><code id="KMHFA_+3A_c">c</code></td>
<td>

<p>A constant to avoid vanishing denominators. The default is <code class="reqn">10^{-4}</code>.
</p>
</td></tr>
<tr><td><code id="KMHFA_+3A_ep">ep</code></td>
<td>

<p>Only if <code>method="E_RM"</code> or <code>method="E_ER"</code>, the stopping critetion parameter in the iterative Huber regression algorithm. The default is <code class="reqn">10^{-4} \times Tp_1 p_2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>method="P"</code>, the number of factors <code class="reqn">k_1</code> and <code class="reqn">k_2</code> are estimated by </p>
<p style="text-align: center;"><code class="reqn">\hat{k}_1 = \arg \max_{j \leq k_{max}} \frac{\lambda _j (\bold{M}_c^w)}{\lambda _{j+1} (\bold{M}_c^w)}, \hat{k}_2 = \arg \max_{j \leq k_{max}} \frac{\lambda _j (\bold{M}_r^w)}{\lambda _{j+1} (\bold{M}_r^w)},</code>
</p>
<p> where <code class="reqn">k_{max}</code> is a predetermined value larger than <code class="reqn">k_1</code> and <code class="reqn">k_2</code>. <code class="reqn">\lambda _j(\cdot)</code> is the j-th largest eigenvalue of a nonnegative definitive matrix. See the function <code><a href="#topic+MHFA">MHFA</a></code> for the definition of <code class="reqn">\bold{M}_c^w</code> and <code class="reqn">\bold{M}_r^w</code>. For details, see He et al. (2023).
</p>
<p>Define <code class="reqn">D=\min({\sqrt{Tp_1}},\sqrt{Tp_2},\sqrt{p_1 p_2})</code>,
</p>
<p style="text-align: center;"><code class="reqn">\hat{\bold{\Sigma}}_1=\frac{1}{T}\sum_{t=1}^T\hat{\bold{F}}_t \hat{\bold{F}}_t^\top, \hat{\bold{\Sigma}}_2=\frac{1}{T}\sum_{t=1}^T\hat{\bold{F}}_t^\top \hat{\bold{F}}_t,</code>
</p>

<p>where <code class="reqn">\hat{\bold{F}}_t, t=1, \dots, T</code> is estimated by IHR under the number of factor is <code class="reqn">k_{max}</code>.
</p>
<p>If <code>method="E_RM"</code>, the number of factors <code class="reqn">k_1</code> and <code class="reqn">k_2</code> are estimated by </p>
<p style="text-align: center;"><code class="reqn">\hat{k}_1=\sum_{i=1}^{k_{max}}I\left(\mathrm{diag}(\hat{\bold{\Sigma}}_1)&gt;P_1\right), \hat{k}_2=\sum_{j=1}^{k_{max}}I\left(\mathrm{diag}(\hat{\bold{\Sigma}}_2) &gt; P_2\right),</code>
</p>

<p>where <code class="reqn">I</code> is the indicator function. In practice, <code class="reqn">P_1</code> is set as <code class="reqn">\max \left(\mathrm{diag}(\hat{\bold{\Sigma}}_1)\right) \cdot D^{-2/3}</code>, <code class="reqn">P_2</code> is set as <code class="reqn">\max \left(\mathrm{diag}(\hat{\bold{\Sigma}}_2)\right) \cdot D^{-2/3}</code>.
</p>
<p>If <code>method="E_ER"</code>, the number of factors <code class="reqn">k_1</code> and <code class="reqn">k_2</code> are estimated by </p>
<p style="text-align: center;"><code class="reqn">\hat{k}_1 = \arg \max_{i \leq k_{max}} \frac{\lambda _i (\hat{\bold{\Sigma}}_1)}{\lambda _{i+1} (\hat{\bold{\Sigma}}_1)+cD^{-2}}, \hat{k}_2 = \arg \max_{j \leq k_{max}} \frac{\lambda _j (\hat{\bold{\Sigma}}_2)}{\lambda _{j+1} (\hat{\bold{\Sigma}}_2)+cD^{-2}}.</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>\eqn{k_1}</code></td>
<td>
<p>The estimated row factor number.</p>
</td></tr>
<tr><td><code>\eqn{k_2}</code></td>
<td>
<p>The estimated column factor number.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yong He, Changwei Zhao, Ran Zhao.
</p>


<h3>References</h3>

<p>He, Y., Kong, X., Yu, L., Zhang, X., &amp; Zhao, C. (2023). Matrix factor analysis: From least squares to iterative projection. Journal of Business &amp; Economic Statistics, 1-26.
</p>
<p>He, Y., Kong, X. B., Liu, D., &amp; Zhao, R. (2023). Robust Statistical Inference for Large-dimensional Matrix-valued Time Series via Iterative Huber Regression. &lt;arXiv:2306.03317&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(11111)
   T=20;p1=20;p2=20;k1=3;k2=3
   R=matrix(runif(p1*k1,min=-1,max=1),p1,k1)
   C=matrix(runif(p2*k2,min=-1,max=1),p2,k2)
   X=array(0,c(T,p1,p2))
   Y=X;E=Y
   F=array(0,c(T,k1,k2))
   for(t in 1:T){
     F[t,,]=matrix(rnorm(k1*k2),k1,k2)
     E[t,,]=matrix(rnorm(p1*p2),p1,p2)
     Y[t,,]=R%*%F[t,,]%*%t(C)
   }
   X=Y+E
   
   KMHFA(X, kmax=6, method="P")
   
   KMHFA(X, W1 = NULL, W2 = NULL, 6, "E_RM")
   KMHFA(X, W1 = NULL, W2 = NULL, 6, "E_ER")
   
</code></pre>

<hr>
<h2 id='KPCA'>
Estimating the Pair of Factor Numbers via Eigenvalue Ratios Corresponding to <code class="reqn">\alpha</code>-PCA
</h2><span id='topic+KPCA'></span>

<h3>Description</h3>

<p>The function is to estimate the pair of factor numbers via eigenvalue ratios corresponding to <code class="reqn">\alpha</code>-PCA. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KPCA(X, kmax, alpha = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KPCA_+3A_x">X</code></td>
<td>

<p>Input an array with <code class="reqn">T \times p_1 \times p_2</code>, where <code class="reqn">T</code> is the sample size, <code class="reqn">p_1</code> is the the row dimension of each matrix observation and <code class="reqn">p_2</code> is the the column dimension of each matrix observation.
</p>
</td></tr>
<tr><td><code id="KPCA_+3A_kmax">kmax</code></td>
<td>

<p>The user-supplied maximum factor numbers. Here it means the upper bound of the number of row factors and column factors.
</p>
</td></tr>
<tr><td><code id="KPCA_+3A_alpha">alpha</code></td>
<td>

<p>A hyper-parameter balancing the information of the first and second moments    (<code class="reqn">\alpha \geq -1</code> ). The default is 0.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code><a href="#topic+KPCA">KPCA</a></code> uses the eigenvalue-ratio idea to estimate the number of factors. In details, the number of factors <code class="reqn">k_1</code> is estimated by
</p>
<p style="text-align: center;"><code class="reqn">\hat{k}_1 = \arg \max_{j \leq k_{max}} \frac{\lambda _j (\hat{\bold{M}}_R)}{\lambda _{j+1} (\hat{\bold{M}}_R)},</code>
</p>
 
<p>where <code class="reqn">k_{max}</code> is a given upper bound. <code class="reqn">k_2</code> is defined similarly with respect to <code class="reqn">\hat{\bold{M}}_C</code>. See the function <code><a href="#topic+alpha_PCA">alpha_PCA</a></code> for the definition of <code class="reqn">\hat{\bold{M}}_R</code> and <code class="reqn">\hat{\bold{M}}_C</code>. For more details, see Chen &amp; Fan (2021).
</p>


<h3>Value</h3>

<table>
<tr><td><code>\eqn{k_1}</code></td>
<td>
<p>The estimated row factor number.</p>
</td></tr>
<tr><td><code>\eqn{k_2}</code></td>
<td>
<p>The estimated column factor number.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yong He, Changwei Zhao, Ran Zhao.
</p>


<h3>References</h3>

<p>Chen, E. Y., &amp; Fan, J. (2021). Statistical inference for high-dimensional matrix-variate factor models. Journal of the American Statistical Association, 1-18.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   set.seed(11111)
   T=20;p1=20;p2=20;k1=3;k2=3
   R=matrix(runif(p1*k1,min=-1,max=1),p1,k1)
   C=matrix(runif(p2*k2,min=-1,max=1),p2,k2)
   X=array(0,c(T,p1,p2))
   Y=X;E=Y
   F=array(0,c(T,k1,k2))
   for(t in 1:T){
     F[t,,]=matrix(rnorm(k1*k2),k1,k2)
     E[t,,]=matrix(rnorm(p1*p2),p1,p2)
     Y[t,,]=R%*%F[t,,]%*%t(C)
   }
   X=Y+E
   
   KPCA(X, 8, alpha = 0)
</code></pre>

<hr>
<h2 id='KPE'>
Estimating the Pair of Factor Numbers via Eigenvalue Ratios Corresponding to PE
</h2><span id='topic+KPE'></span>

<h3>Description</h3>

<p>The function is to estimate the pair of factor numbers via eigenvalue ratios corresponding to PE method. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KPE(X, kmax, c = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KPE_+3A_x">X</code></td>
<td>

<p>Input an array with <code class="reqn">T \times p_1 \times p_2</code>, where <code class="reqn">T</code> is the sample size, <code class="reqn">p_1</code> is the the row dimension of each matrix observation and <code class="reqn">p_2</code> is the the column dimension of each matrix observation.
</p>
</td></tr>
<tr><td><code id="KPE_+3A_kmax">kmax</code></td>
<td>

<p>The user-supplied maximum factor numbers. Here it means the upper bound of the number of row factors and column factors.
</p>
</td></tr>
<tr><td><code id="KPE_+3A_c">c</code></td>
<td>

<p>A constant to avoid vanishing denominators. The default is 0.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code><a href="#topic+KPE">KPE</a></code> uses the eigenvalue-ratio idea to estimate the number of factors. 
First, obtain the initial estimators <code class="reqn">\hat{\bold{R}}</code> and <code class="reqn">\hat{\bold{C}}</code>. Second, define 
</p>
<p style="text-align: center;"><code class="reqn">\hat{\bold{Y}}_t=\frac{1}{p_2}\bold{X}_t\hat{\bold{C}}, \hat{\bold{Z}}_t=\frac{1}{p_1}\bold{X}_t^\top\hat{\bold{R}},</code>
</p>
<p> and 
</p>
<p style="text-align: center;"><code class="reqn">\tilde{\bold{M}}_1=\frac{1}{Tp_1}\hat{\bold{Y}}_t\hat{\bold{Y}}_t^\top, \tilde{\bold{M}}_2=\frac{1}{Tp_2}\sum_{t=1}^T\hat{\bold{Z}}_t\hat{\bold{Z}}_t^\top,</code>
</p>
<p> the number of factors <code class="reqn">k_1</code> is estimated by
</p>
<p style="text-align: center;"><code class="reqn">\hat{k}_1 = \arg \max_{j \leq k_{max}} \frac{\lambda_j (\tilde{\bold{M}}_1)}{\lambda _{j+1} (\tilde{\bold{M}}_1)},</code>
</p>

<p>where <code class="reqn">k_{max}</code> is a predetermined upper bound for <code class="reqn">k_1</code>. The estimation of <code class="reqn">k_2</code> is defined similarly with respect to <code class="reqn">\tilde{\bold{M}}_2</code>. 
For details, see Yu et al. (2022).
</p>


<h3>Value</h3>

<table>
<tr><td><code>\eqn{k_1}</code></td>
<td>
<p>The estimated row factor number.</p>
</td></tr>
<tr><td><code>\eqn{k_2}</code></td>
<td>
<p>The estimated column factor number.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yong He, Changwei Zhao, Ran Zhao.
</p>


<h3>References</h3>

<p>Yu, L., He, Y., Kong, X., &amp; Zhang, X. (2022). Projected estimation for large-dimensional matrix factor models. Journal of Econometrics, 229(1), 201-217.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   set.seed(11111)
   T=20;p1=20;p2=20;k1=3;k2=3
   R=matrix(runif(p1*k1,min=-1,max=1),p1,k1)
   C=matrix(runif(p2*k2,min=-1,max=1),p2,k2)
   X=array(0,c(T,p1,p2))
   Y=X;E=Y
   F=array(0,c(T,k1,k2))
   for(t in 1:T){
     F[t,,]=matrix(rnorm(k1*k2),k1,k2)
     E[t,,]=matrix(rnorm(p1*p2),p1,p2)
     Y[t,,]=R%*%F[t,,]%*%t(C)
   }
   X=Y+E
   
   KPE(X, 8, c = 0)
</code></pre>

<hr>
<h2 id='MHFA'>
Matrix Huber Factor Analysis
</h2><span id='topic+MHFA'></span>

<h3>Description</h3>

<p>This function is to fit the matrix factor models via the Huber loss. We propose two algorithms to do robust factor analysis. One is based on minimizing the Huber loss of the idiosyncratic error's Frobenius norm, which leads to a weighted iterative projection approach to compute and learn the parameters and thereby named as Robust-Matrix-Factor-Analysis (RMFA). The other one is based on minimizing the element-wise Huber loss, which can be solved by an iterative Huber regression algorithm (IHR).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MHFA(X, W1=NULL, W2=NULL, m1, m2, method, max_iter = 100, ep = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MHFA_+3A_x">X</code></td>
<td>

<p>Input an array with <code class="reqn">T \times p_1 \times p_2</code>, where <code class="reqn">T</code> is the sample size, <code class="reqn">p_1</code> is the the row dimension of each matrix observation and <code class="reqn">p_2</code> is the the column dimension of each matrix observation.
</p>
</td></tr>
<tr><td><code id="MHFA_+3A_w1">W1</code></td>
<td>

<p>Only if <code>method="E"</code>, the inital value of row loadings matrix. The default is NULL, which is randomly chosen and all entries from a standard normal distribution.
</p>
</td></tr>
<tr><td><code id="MHFA_+3A_w2">W2</code></td>
<td>

<p>Only if <code>method="E"</code>, the inital value of column loadings matrix. The default is NULL, which is randomly chosen and all entries from a standard normal distribution.
</p>
</td></tr>
<tr><td><code id="MHFA_+3A_m1">m1</code></td>
<td>

<p>A positive integer indicating the row factor numbers.
</p>
</td></tr>
<tr><td><code id="MHFA_+3A_m2">m2</code></td>
<td>

<p>A positive integer indicating the column factor numbers.
</p>
</td></tr>
<tr><td><code id="MHFA_+3A_method">method</code></td>
<td>

<p>Character string, specifying the type of the estimation method to be used.
</p>

<dl>
<dt><code>"P",</code></dt><dd><p>indicates minimizing the Huber loss of the idiosyncratic error's Frobenius norm. (RMFA)</p>
</dd>
<dt><code>"E",</code></dt><dd><p>indicates minimizing the elementwise Huber loss. (IHR)</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="MHFA_+3A_max_iter">max_iter</code></td>
<td>

<p>Only if <code>method="E"</code>, the maximum number of iterations in the iterative Huber regression algorithm. The default is 100.
</p>
</td></tr>
<tr><td><code id="MHFA_+3A_ep">ep</code></td>
<td>

<p>Only if <code>method="E"</code>, the stopping critetion parameter in the iterative Huber regression algorithm. The default is <code class="reqn">10^{-4} \times Tp_1 p_2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the matrix factor models, He et al. (2021) propose a weighted iterative projection approach to compute and learn the parameters by minimizing the Huber loss function of the idiosyncratic error's Frobenius norm. In details, for observations <code class="reqn">\bold{X}_t, t=1,2,\cdots,T</code>, define 
</p>
<p style="text-align: center;"><code class="reqn">\bold{M}_c^w = \frac{1}{Tp_2} \sum_{t=1}^T w_t \bold{X}_t \bold{C} \bold{C}^\top \bold{X}_t^\top, \bold{M}_r^w = \frac{1}{Tp_1} \sum_{t=1}^T w_t \bold{X}_t^\top \bold{R} \bold{R}^\top \bold{X}_t.</code>
</p>
 
<p>The estimators of loading matrics <code class="reqn">\hat{\bold{R}}</code> and <code class="reqn">\hat{\bold{C}}</code> are calculated by <code class="reqn">\sqrt{p_1}</code> times the leading <code class="reqn">k_1</code> eigenvectors of <code class="reqn">\bold{M}_c^w</code> and <code class="reqn">\sqrt{p_2}</code> times the leading <code class="reqn">k_2</code> eigenvectors of <code class="reqn">\bold{M}_r^w</code>.
And </p>
<p style="text-align: center;"><code class="reqn">\hat{\bold{F}}_t=\frac{1}{p_1 p_2}\hat{\bold{R}}^\top \bold{X}_t \hat{\bold{C}}.</code>
</p>
<p> For details, see He et al. (2023). 
</p>
<p>The other one is based on minimizing the element-wise Huber loss. Define 
</p>
<p style="text-align: center;"><code class="reqn">M_{i,Tp_2}(\bold{r}, \bold{F}_t, \bold{C})=\frac{1}{Tp_2} \sum_{t=1}^{T} \sum_{j=1}^{p_2} H_\tau \left(x_{t,ij}-\bold{r}_i^\top\bold{F}_t\bold{c}_j \right),</code>
</p>

<p style="text-align: center;"><code class="reqn">M_{i,Tp_1}(\bold{R}, \bold{F}_t, \bold{c})=\frac{1}{Tp_1}\sum_{t=1}^T\sum_{i=1}^{p_1} H_\tau \left(x_{t,ij}-\bold{r}_i^\top\bold{F}_t\bold{c}_j\right),</code>
</p>

<p style="text-align: center;"><code class="reqn">M_{t,p_1 p_2}(\bold{R}, \mathrm{vec}(\bold{F}), \bold{C})=\frac{1}{p_1 p_2} \sum_{i=1}^{p_1}\sum_{j=1}^{p_2} H_\tau \left(x_{t,ij}-(\bold{c}_j \otimes \bold{r}_i)^\top \mathrm{vec}(\bold{F})\right).</code>
</p>
<p> This can be seen as Huber regression as each time optimizing one argument while keeping the other two fixed.
</p>


<h3>Value</h3>

<p>The return value is a list. In this list, it contains the following:
</p>
<table>
<tr><td><code>F</code></td>
<td>
<p>The estimated factor matrix of dimension <code class="reqn">T \times m_1\times m_2</code>.</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>The estimated row loading matrix of dimension <code class="reqn">p_1\times m_1</code>, satisfying <code class="reqn">\bold{R}^\top\bold{R}=p_1\bold{I}_{m_1}</code>.</p>
</td></tr>
<tr><td><code>C</code></td>
<td>
<p>The estimated column loading matrix of dimension <code class="reqn">p_2\times m_2</code>, satisfying <code class="reqn">\bold{C}^\top\bold{C}=p_2\bold{I}_{m_2}</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yong He, Changwei Zhao, Ran Zhao.
</p>


<h3>References</h3>

<p>He, Y., Kong, X., Yu, L., Zhang, X., &amp; Zhao, C. (2023). Matrix factor analysis: From least squares to iterative projection. Journal of Business &amp; Economic Statistics, 1-26.
</p>
<p>He, Y., Kong, X. B., Liu, D., &amp; Zhao, R. (2023). Robust Statistical Inference for Large-dimensional Matrix-valued Time Series via Iterative Huber Regression. &lt;arXiv:2306.03317&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(11111)
   T=20;p1=20;p2=20;k1=3;k2=3
   R=matrix(runif(p1*k1,min=-1,max=1),p1,k1)
   C=matrix(runif(p2*k2,min=-1,max=1),p2,k2)
   X=array(0,c(T,p1,p2))
   Y=X;E=Y
   F=array(0,c(T,k1,k2))
   for(t in 1:T){
     F[t,,]=matrix(rnorm(k1*k2),k1,k2)
     E[t,,]=matrix(rnorm(p1*p2),p1,p2)
     Y[t,,]=R%*%F[t,,]%*%t(C)
   }
   X=Y+E
   
   #Estimate the factor matrices and loadings by RMFA
   fit1=MHFA(X, m1=3, m2=3, method="P")
   Rhat1=fit1$R 
   Chat1=fit1$C
   Fhat1=fit1$F
   
   #Estimate the factor matrices and loadings by IHR
   fit2=MHFA(X, W1=NULL, W2=NULL, 3, 3, "E")
   Rhat2=fit2$R 
   Chat2=fit2$C
   Fhat2=fit2$F
   
   #Estimate the common component by RMFA
   CC1=array(0,c(T,p1,p2))
   for (t in 1:T){
   CC1[t,,]=Rhat1%*%Fhat1[t,,]%*%t(Chat1)
   }
   CC1
   
   #Estimate the common component by IHR
   CC2=array(0,c(T,p1,p2))
   for (t in 1:T){
   CC2[t,,]=Rhat2%*%Fhat2[t,,]%*%t(Chat2)
   }
   CC2
</code></pre>

<hr>
<h2 id='PE'>
Projected Estimation for Large-Dimensional Matrix Factor Models 
</h2><span id='topic+PE'></span>

<h3>Description</h3>

<p>This function is to fit the matrix factor model via the PE method by projecting the observation matrix onto the row or column factor space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PE(X, m1, m2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PE_+3A_x">X</code></td>
<td>

<p>Input an array with <code class="reqn">T \times p_1 \times p_2</code>, where <code class="reqn">T</code> is the sample size, <code class="reqn">p_1</code> is the the row dimension of each matrix observation and <code class="reqn">p_2</code> is the the column dimension of each matrix observation.
</p>
</td></tr>
<tr><td><code id="PE_+3A_m1">m1</code></td>
<td>

<p>A positive integer indicating the row factor numbers.
</p>
</td></tr>
<tr><td><code id="PE_+3A_m2">m2</code></td>
<td>

<p>A positive integer indicating the column factor numbers.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the matrix factor models, Yu et al. (2022) propose a projection estimation method to estimate the model parameters. In details, for observations <code class="reqn">\bold{X}_t, t=1,2,\cdots,T</code>, the data matrix is projected to a lower dimensional space by setting 
</p>
<p style="text-align: center;"><code class="reqn">\bold{Y}_t = \frac{1}{p_2} \bold{X}_t \bold{C}.</code>
</p>

<p>Given <code class="reqn">\bold{Y}_t</code>, define </p>
<p style="text-align: center;"><code class="reqn"> \bold{M}_1 = \frac{1}{Tp_1} \sum_{t=1}^T \bold{Y}_t \bold{Y}_t^\top, </code>
</p>
<p> and then the row factor loading matrix <code class="reqn">\bold{R}</code> can be estimated by <code class="reqn">\sqrt{p_1}</code> times the leading <code class="reqn">k_1</code> eigenvectors of <code class="reqn">\bold{M}_1</code>. However, the projection matrix <code class="reqn">\bold{C}</code> is unavailable in practice. A natural solution is to replace it with a consistent initial estimator. The column factor loading matrix <code class="reqn">\bold{C}</code> can be similarly estimated by projecting <code class="reqn">\bold{X}_t</code> onto the space of <code class="reqn">\bold{C}</code> with <code class="reqn">\bold{R}</code>. See Yu et al. (2022) for the detailed algorithm.
</p>


<h3>Value</h3>

<p>The return value is a list. In this list, it contains the following:
</p>
<table>
<tr><td><code>F</code></td>
<td>
<p>The estimated factor matrix of dimension <code class="reqn">T \times m_1\times m_2</code>.</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>The estimated row loading matrix of dimension <code class="reqn">p_1\times m_1</code>, satisfying <code class="reqn">\bold{R}^\top\bold{R}=p_1\bold{I}_{m_1}</code>.</p>
</td></tr>
<tr><td><code>C</code></td>
<td>
<p>The estimated column loading matrix of dimension <code class="reqn">p_2\times m_2</code>, satisfying <code class="reqn">\bold{C}^\top\bold{C}=p_2\bold{I}_{m_2}</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yong He, Changwei Zhao, Ran Zhao.
</p>


<h3>References</h3>

<p>Yu, L., He, Y., Kong, X., &amp; Zhang, X. (2022). Projected estimation for large-dimensional matrix factor models. Journal of Econometrics, 229(1), 201-217.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   set.seed(11111)
   T=20;p1=20;p2=20;k1=3;k2=3
   R=matrix(runif(p1*k1,min=-1,max=1),p1,k1)
   C=matrix(runif(p2*k2,min=-1,max=1),p2,k2)
   X=array(0,c(T,p1,p2))
   Y=X;E=Y
   F=array(0,c(T,k1,k2))
   for(t in 1:T){
     F[t,,]=matrix(rnorm(k1*k2),k1,k2)
     E[t,,]=matrix(rnorm(p1*p2),p1,p2)
     Y[t,,]=R%*%F[t,,]%*%t(C)
   }
   X=Y+E
   
   #Estimate the factor matrices and loadings
   fit=PE(X, k1, k2)
   Rhat=fit$R 
   Chat=fit$C
   Fhat=fit$F
   
   #Estimate the common component
   CC=array(0,c(T,p1,p2))
   for (t in 1:T){
   CC[t,,]=Rhat%*%Fhat[t,,]%*%t(Chat)
   }
   CC
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
