<!DOCTYPE html><html><head><title>Help for package FKSUM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FKSUM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#FKSUM-package'>
<p>Fast Exact Kernel Smoothing</p></a></li>
<li><a href='#bin_wts'><p>Compute discrete bin weights</p></a></li>
<li><a href='#cbin_alloc'><p>Allocation of points to bins</p></a></li>
<li><a href='#df_ica'><p>Gradient of projection index for independent component analysis.</p></a></li>
<li><a href='#df_ppr'><p>Gradient of the projection index for projection pursuit regression</p></a></li>
<li><a href='#dksum'><p>Kernel derivative sums</p></a></li>
<li><a href='#f_ica'><p>Projection index for independent component analysis.</p></a></li>
<li><a href='#f_ppr'><p>Projection index for projection pursuit regression</p></a></li>
<li><a href='#fancy_PPR_initialisation'><p>Initialisation for PPR based on Ridge LM after GAM type smoothing</p></a></li>
<li><a href='#fk_density'><p>Fast univariate kernel density estimation</p></a></li>
<li><a href='#fk_dfmdh'><p>Gradient of projection index for finding minimum density hyperplanes</p></a></li>
<li><a href='#fk_fmdh'><p>Projection index for finding minimum density hyperplanes</p></a></li>
<li><a href='#fk_ICA'><p>Independent component analysis with sample entropy estimated via kernel density</p></a></li>
<li><a href='#fk_is_minim_md'><p>Check if MDH constraints are active</p></a></li>
<li><a href='#fk_loc_lin'><p>Local linear regression estimator</p></a></li>
<li><a href='#fk_md'><p>C++ code for evaluating mimimum density hyperplane from projected data</p></a></li>
<li><a href='#fk_md_b'><p>Minimum density hyperplane orthogonal to a vector</p></a></li>
<li><a href='#fk_md_dp'><p>C++ code for evaluating partial gradient of mimimum density hyperplane w.r.t. projected data</p></a></li>
<li><a href='#fk_mdh'><p>Minimum density hyperplanes</p></a></li>
<li><a href='#fk_NW'><p>Nadaraya-Watson regression estimator</p></a></li>
<li><a href='#fk_ppr'><p>Projection pursuit regression with local linear kernel smoother</p></a></li>
<li><a href='#fk_regression'><p>Fast univariate kernel regression</p></a></li>
<li><a href='#fk_sum'><p>Fast Exact Kernel Sum Evaluation</p></a></li>
<li><a href='#h_Gauss_to_K'><p>Bandwidth conversion from Gaussian</p></a></li>
<li><a href='#h_K_to_Gauss'><p>Bandwidth conversion to Gaussian</p></a></li>
<li><a href='#kLLreg'><p>Leave-one-out regression smoother</p></a></li>
<li><a href='#kndksum'><p>Kernel and kernel derivative sums</p></a></li>
<li><a href='#ksum'><p>Kernel sums</p></a></li>
<li><a href='#norm_const_K'><p>Normalising constant for kernels in FKSUM</p></a></li>
<li><a href='#norm_K'><p>The L2 norm of a kernel</p></a></li>
<li><a href='#plot_kernel'><p>Plot the shape of a kernel function implemented in FKSUM based on its vector of beta coefficients</p></a></li>
<li><a href='#plot.fk_density'><p>Plot method for class fk_density</p></a></li>
<li><a href='#plot.fk_ICA'><p>Plot method for class fk_ICA</p></a></li>
<li><a href='#plot.fk_mdh'><p>Plot method for class fk_mdh</p></a></li>
<li><a href='#plot.fk_ppr'><p>Plot method for class fk_ppr</p></a></li>
<li><a href='#plot.fk_regression'><p>Plot method for class fk_regression</p></a></li>
<li><a href='#predict.fk_ppr'><p>Predict method for class fk_ppr</p></a></li>
<li><a href='#predict.fk_regression'><p>Predict method for class fk_regression</p></a></li>
<li><a href='#print.fk_density'><p>Print method for class fk_density</p></a></li>
<li><a href='#print.fk_ICA'><p>Print method for class fk_ICA</p></a></li>
<li><a href='#print.fk_mdh'><p>Print method for class fk_mdh</p></a></li>
<li><a href='#print.fk_ppr'><p>Print method for class fk_ppr</p></a></li>
<li><a href='#print.fk_regression'><p>Print method for class fk_regression</p></a></li>
<li><a href='#roughness_K'><p>Kernel roughness</p></a></li>
<li><a href='#sm_bin_wts'><p>Compute smoothed bin weights</p></a></li>
<li><a href='#var_K'><p>Variance of a kernel</p></a></li>
<li><a href='#whiten'><p>Whitening (standardising) a data matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast Kernel Sums</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Author:</td>
<td>David P. Hofmeyr</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David P. Hofmeyr &lt;dhofmeyr@sun.ac.za&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements the method of Hofmeyr, D.P. (2021) &lt;<a href="https://doi.org/10.1109%2FTPAMI.2019.2930501">doi:10.1109/TPAMI.2019.2930501</a>&gt; for fast evaluation of univariate kernel smoothers based on recursive computations.
  Applications to the basic problems of density and regression function estimation are provided, as well as some projection pursuit methods
  for which the objective is based on non-parametric functionals of the projected density, or conditional density of a response given projected
  covariates.
  The package is accompanied by an instructive paper in the Journal of Statistical Software &lt;<a href="https://doi.org/10.18637%2Fjss.v101.i03">doi:10.18637/jss.v101.i03</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>Rcpp (&ge; 0.12.16)</td>
</tr>
<tr>
<td>Imports:</td>
<td>rARPACK, MASS, Matrix</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-15 16:01:51 UTC; david</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-15 16:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='FKSUM-package'>
Fast Exact Kernel Smoothing
</h2><span id='topic+FKSUM-package'></span><span id='topic+FKSUM'></span>

<h3>Description</h3>

<p>Uses recursive expressions to compute exact univariate kernel smoothers in log-linear time based on the method described by Hofmeyr (2021).   The main general purpose function is fk_sum() which computes exact (or binned approximations of) weighted sums of kernels, or their derivatives.
Standard smoothing problems such as density estimation and regression can be addressed directly using this function, or using the purpose-built
functions fk_density() and fk_regression. Projection pursuit algorithms based on minimum entropy (ICA, Hyvarinen and Oja 2000), minimum density cluster separation
(MDH, Pavlidis et al. 2016) and regression-type losses (PPR, Friedman and Stuetzle 1981) are implemented in the functions fk_ICA(), fk_ppr() and fk_mdh()
respectively.
The package is accompanied by an instructive paper in the Journal of Statistical
Software (see below for reference).
</p>


<h3>Details</h3>

<p>Package:  FKSUM
</p>
<p>Type: Package
</p>
<p>Title:  Fast Kernel Sums
</p>
<p>Version:  0.1.3
</p>
<p>Depends:  Rcpp (&gt;= 0.12.16)
</p>
<p>License:  GPL-3
</p>
<p>LazyData: yes
</p>
<p>Imports:  rARPACK, MASS
</p>
<p>LinkingTo: Rcpp, RcppArmadillo
</p>


<h3>Author(s)</h3>

<p>David Hofmeyr[aut, cre]
</p>
<p>Maintainer: David P. Hofmeyr &lt;dhofmeyr@sun.ac.za&gt;
</p>


<h3>References</h3>

<p>Hofmeyr, D.P. (2022) &quot;Fast kernel smoothing in R with applications to projection
pursuit&quot;, <em>Journal of Statistical Software</em>, 101(3), 1-33.
</p>
<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>
<p>Hyvarinen, A., and Oja, E. (2000) &quot;Independent component analysis: algorithms and applications.&quot; <em>Neural networks</em>, 13(4), 411–430.
</p>
<p>Friedman, J.H., and Stuetzle, W. (1981) &quot;Projection pursuit regression.&quot; <em>Journal of the American statistical Association</em>, 76(376), 817–823.
</p>
<p>Pavlidis, N.G., Hofmeyr, D.P., and Tasoulis, S.K. (2016) &quot;Minimum density hyperplanes.&quot; <em>Journal of Machine Learning Research</em>, 17(156), 1–33.
</p>

<hr>
<h2 id='bin_wts'>Compute discrete bin weights</h2><span id='topic+bin_wts'></span>

<h3>Description</h3>

<p>Computes total weight from an initial weight vector falling into regular bins, based on corresponding location of sample points.
Not intended for general use.
</p>

<hr>
<h2 id='cbin_alloc'>Allocation of points to bins</h2><span id='topic+cbin_alloc'></span>

<h3>Description</h3>

<p>Finds the bin into which each of a set of sample points falls. Not intended for alternative use.
</p>

<hr>
<h2 id='df_ica'>Gradient of projection index for independent component analysis.</h2><span id='topic+df_ica'></span>

<h3>Description</h3>

<p>Computes the gradient of the objective evaluating sample entropy of a data set projected onto a given vector.
Only used within optimisation of projection. Not intended for alternative use.
</p>

<hr>
<h2 id='df_ppr'>Gradient of the projection index for projection pursuit regression</h2><span id='topic+df_ppr'></span>

<h3>Description</h3>

<p>Computes the gradient of the loss of a smoother fit to projected covariates and corresponding response values.
Only used within optimisation of projection. Not intended for alternative use.
</p>

<hr>
<h2 id='dksum'>Kernel derivative sums</h2><span id='topic+dksum'></span>

<h3>Description</h3>

<p>Fast computation of kernel derivative sums. Called by multiple functions exported by the package, but not
intended for alternative use.
</p>

<hr>
<h2 id='f_ica'>Projection index for independent component analysis.</h2><span id='topic+f_ica'></span>

<h3>Description</h3>

<p>Computes the sample entropy of a data set projected onto a given vector. Only used within optimisation of projection. Not
intended for alternative use.
</p>

<hr>
<h2 id='f_ppr'>Projection index for projection pursuit regression</h2><span id='topic+f_ppr'></span>

<h3>Description</h3>

<p>Computes the loss of a smoother fit to projected covariates and corresponding response values.
Only used within optimisation of projection. Not intended for alternative use.
</p>

<hr>
<h2 id='fancy_PPR_initialisation'>Initialisation for PPR based on Ridge LM after GAM type smoothing</h2><span id='topic+fancy_PPR_initialisation'></span>

<h3>Description</h3>

<p>Computes an initial projection for PPR by replacing each covariate by the smoothed response against it. Then fit a
standard ridge estimator to the transformed data. Not intended for general use.
</p>

<hr>
<h2 id='fk_density'>Fast univariate kernel density estimation</h2><span id='topic+fk_density'></span>

<h3>Description</h3>

<p>Uses recursive formulation for kernel sums as described in Hofmeyr (2021) to evaluate kernel estimate of the density exactly. Binning approximation also available for faster computation if needed. Default is exact evaluation on a grid, but evaluation at an arbitrary collection of points is possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fk_density(x, h = 'Silverman', h_adjust = 1, beta = NULL, from = NULL,
              to = NULL, ngrid = 1000, nbin = NULL, x_eval = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fk_density_+3A_x">x</code></td>
<td>
<p>vector of sample points.</p>
</td></tr>
<tr><td><code id="fk_density_+3A_h">h</code></td>
<td>
<p>(optional) bandwidth to be used in estimate. Can be either positive numeric, or one of &quot;Silverman&quot; for Silverman's rule of thumb (Silverman, 1986) or &quot;mlcv&quot; for maximum pseudo-likelihood cross validation bandwidth. Default is Silverman's heuristic. Exact &quot;mlcv&quot; will be time consuming for samples of more than millions of points. Binning approximation for &quot;mlcv&quot; bandwidth needs at least 10000 bins for reasonable accuracy, and even more if density has very sharp features.</p>
</td></tr>
<tr><td><code id="fk_density_+3A_h_adjust">h_adjust</code></td>
<td>
<p>(optional) positive numeric. Final bandwidth will be h*h_adjust. Default value is 1. R's base function density uses Silverman's heuristic with h_adjust approximately 0.85.</p>
</td></tr>
<tr><td><code id="fk_density_+3A_beta">beta</code></td>
<td>
<p>(optional) numeric vector of kernel coefficients. See Hofmeyr (2019) for details. The
default is the smooth order one kernel described in the paper.</p>
</td></tr>
<tr><td><code id="fk_density_+3A_from">from</code></td>
<td>
<p>(optional) lower end of evaluation interval if evaluation on a grid is desired. Default is min(x)-6*h</p>
</td></tr>
<tr><td><code id="fk_density_+3A_to">to</code></td>
<td>
<p>(optional) upper end of evaluation interval if evaluation on a grid is desired. Default is max(x)+6*h</p>
</td></tr>
<tr><td><code id="fk_density_+3A_ngrid">ngrid</code></td>
<td>
<p>(optional) integer number of grid points for evaluation. Default is 1000.</p>
</td></tr>
<tr><td><code id="fk_density_+3A_nbin">nbin</code></td>
<td>
<p>(optional) integer number of bins if binning estimator is to be used. The default is to compute the exact density on a grid of 1000 points.</p>
</td></tr>
<tr><td><code id="fk_density_+3A_x_eval">x_eval</code></td>
<td>
<p>(optional) vector of evaluation points. The default if both ngrid and nbin are set to NULL is evaluation at the sample points themselves. If another specific set of points is required then ngrid must be set to null and x_eval supplied. Evaluation at arbitrary x_eval using binned approximation is also possible, in which case nbin and x_eval must both be supplied.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list with fields
</p>
<table>
<tr><td><code>$x</code></td>
<td>
<p>the vector of points at which the density is estimated.</p>
</td></tr>
<tr><td><code>$y</code></td>
<td>
<p>the estimated density values.</p>
</td></tr>
<tr><td><code>$h</code></td>
<td>
<p>the value of the bandwidth.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>
<p>Silverman, B. (1986) <em>Density estimation for statistics and data analysis</em>, volume 26. CRC press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>op &lt;- par(no.readonly = TRUE)

set.seed(1)

### generate a bimodal Gaussian mixture sample with 100000 points

n1 &lt;- rbinom(1, 100000, .5)

x &lt;- c(rnorm(n1), rnorm(100000-n1)/4+2)



# --------- Example 1: Grid evaluation --------------#
### evaluate exact and binned approximation on a
### grid and plot along with true density. We can
### use both Silverman's heuristic and the "mlcv" estimate.

xs &lt;- seq(-5, 3.5, length = 1000)

ftrue &lt;- dnorm(xs)/2 + dnorm(xs, 2, 1/4)/2

fhat &lt;- fk_density(x, from = -5, to = 3.5)

fhat_bin &lt;- fk_density(x, nbin = 1000, from = -5, to = 3.5)

par(mfrow = c(2, 2))
plot(xs, ftrue, type = 'l', lwd = 4, col = rgb(.7, .7, .7),
    xlab = 'x', ylab = 'f(x)',
    main = 'Exact evaluation, Silverman bandwidth')
lines(fhat, lty = 2)

plot(xs, ftrue, type = 'l', lwd = 4, col = rgb(.7, .7, .7),
    xlab = 'x', ylab = 'f(x)',
    main = 'Binned approximation, Silverman bandwidth')
lines(fhat_bin, lty = 2)




fhat &lt;- fk_density(x, from = -5, to = 3.5, h = 'mlcv')

fhat_bin &lt;- fk_density(x, nbin = 1000, from = -5,
    to = 3.5, h = 'mlcv')

plot(xs, ftrue, type = 'l', lwd = 4, col = rgb(.7, .7, .7),
    xlab = 'x', ylab = 'f(x)',
    main = 'Exact evaluation, MLCV bandwidth')
lines(fhat, lty = 2)

plot(xs, ftrue, type = 'l', lwd = 4, col = rgb(.7, .7, .7),
    xlab = 'x', ylab = 'f(x)',
    main = 'Binned approximation, MLCV bandwidth')
lines(fhat_bin, lty = 2)


par(op)


# --------- Example 2: Evaluation at sample --------------#
### evaluate exact and binned approximation at the sample.
### Note that the output will be in the order of the original
### sample, and also the number of points will be large. It is
### not advisable, therefore, to simply plot these. We instead
### compute the mean squared deviations from the true density

ftrue &lt;- sapply(x, function(xi) dnorm(xi)/2 + dnorm(xi, 2, 1/4)/2)

fhat &lt;- fk_density(x, ngrid = NULL)

fhat_bin &lt;- fk_density(x, nbin = 1000, x_eval = x)


mean((ftrue-fhat$y)^2)

mean((ftrue-fhat_bin$y)^2)


### now for MLCV bandwidth

fhat &lt;- fk_density(x, h = 'mlcv', ngrid = NULL)

fhat_bin &lt;- fk_density(x, nbin = 1000,
    h = 'mlcv', x_eval = x)


mean((ftrue-fhat$y)^2)

mean((ftrue-fhat_bin$y)^2)

</code></pre>

<hr>
<h2 id='fk_dfmdh'>Gradient of projection index for finding minimum density hyperplanes</h2><span id='topic+fk_dfmdh'></span>

<h3>Description</h3>

<p>Computes the gradient of the objective based on minimum integrated density of a hyperplane orthogonal to a given projection vector.
Only used within optimisation of projection. Not intended for alternative use.
</p>

<hr>
<h2 id='fk_fmdh'>Projection index for finding minimum density hyperplanes</h2><span id='topic+fk_fmdh'></span>

<h3>Description</h3>

<p>Computes the minimum integrated density orthogonal to a given projection vector. Only used within optimisation of projection. Not
intended for alternative use.
</p>

<hr>
<h2 id='fk_ICA'>Independent component analysis with sample entropy estimated via kernel density</h2><span id='topic+fk_ICA'></span>

<h3>Description</h3>

<p>Performs minimum entropy projection pursuit
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fk_ICA(X, ncomp = 1, beta = c(.25, .25), hmult = 1.5, it = 20, nbin = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fk_ICA_+3A_x">X</code></td>
<td>
<p>numeric data matrix (num_data x num_dimensions).</p>
</td></tr>
<tr><td><code id="fk_ICA_+3A_ncomp">ncomp</code></td>
<td>
<p>integer number of indpenedent components to extract.</p>
</td></tr>
<tr><td><code id="fk_ICA_+3A_beta">beta</code></td>
<td>
<p>numeric vector of kernel coefficients. See Hofmeyr (2019) for details. The
default is the smooth order one kernel described in the paper.</p>
</td></tr>
<tr><td><code id="fk_ICA_+3A_hmult">hmult</code></td>
<td>
<p>positive numeric. The bandwidth in the kernel density is set to hmult multiplied by Silverman's rule of thumb value, which is based on the AMISE minimiser when the underlying distribution is Gaussian.</p>
</td></tr>
<tr><td><code id="fk_ICA_+3A_it">it</code></td>
<td>
<p>integer maximum number of iterations. The default is 20.</p>
</td></tr>
<tr><td><code id="fk_ICA_+3A_nbin">nbin</code></td>
<td>
<p>integer number of bins if binning estimator is to be used. The default is to compute the exact entropy estimate from the kde.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list with fields
</p>
<table>
<tr><td><code>$X</code></td>
<td>
<p>the data matrix given as argument.</p>
</td></tr>
<tr><td><code>$K</code></td>
<td>
<p>the pre-whitening matrix.</p>
</td></tr>
<tr><td><code>$W</code></td>
<td>
<p>the estimated un-mixing matrix.</p>
</td></tr>
<tr><td><code>$S</code></td>
<td>
<p>the estimated source matrix, S = sweep(X,2,colMeans(X),'-')%*%K%*%W.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>op &lt;- par(no.readonly = TRUE)

set.seed(1)

### Generate a set of data with 5 non-Gaussian components
### and 20 noise (Gaussian) components. The non-Gaussian
### components are cubed Gaussian, exponential, uniform,
### t3 and bimodal Gaussian mixture.

errdim = 10
ndat = 2000
X = cbind(rnorm(ndat)^3/5, rexp(ndat), runif(ndat)*sqrt(12)*2, rt(ndat, 3),
    c(rnorm(ndat/2), rexp(ndat/2)+2), matrix(rnorm(errdim*ndat), ncol = errdim))

### Generate a random mixing matrix and mixed data XX

R = matrix(rnorm((5+errdim)*(5+errdim)), ncol = 5+errdim)
XX = X%*%R

### Apply fk_ICA to XX to extract components.
### Note that ordering of extracted components is
### in some sense arbitrary w.r.t. their generation

model &lt;- fk_ICA(XX, ncol(XX))

par(mfrow = c(1, 5))

for(i in 1:5) plot(density(model$S[,i]))

par(op)
</code></pre>

<hr>
<h2 id='fk_is_minim_md'>Check if MDH constraints are active</h2><span id='topic+fk_is_minim_md'></span>

<h3>Description</h3>

<p>Called during processing of fk_mdh(). Not intended for alternative use.
</p>

<hr>
<h2 id='fk_loc_lin'>Local linear regression estimator</h2><span id='topic+fk_loc_lin'></span>

<h3>Description</h3>

<p>Called by the function fk_regression(). Not intended for alternative use.
</p>

<hr>
<h2 id='fk_md'>C++ code for evaluating mimimum density hyperplane from projected data</h2><span id='topic+fk_md'></span>

<h3>Description</h3>

<p>Called during processing of fk_mdh(). Not intended for alternative use.
</p>

<hr>
<h2 id='fk_md_b'>Minimum density hyperplane orthogonal to a vector</h2><span id='topic+fk_md_b'></span>

<h3>Description</h3>

<p>Finds the location of the minimum density hyperplane orthogonal to a given vector. Not intended for alternative use.
</p>

<hr>
<h2 id='fk_md_dp'>C++ code for evaluating partial gradient of mimimum density hyperplane w.r.t. projected data</h2><span id='topic+fk_md_dp'></span>

<h3>Description</h3>

<p>Called during processing of fk_mdh(). Not intended for alternative use.
</p>

<hr>
<h2 id='fk_mdh'>Minimum density hyperplanes</h2><span id='topic+fk_mdh'></span>

<h3>Description</h3>

<p>Estimates minimum density hyperplanes for clustering using projection pursuit, based on the method of Pavlidis et al. (2016)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fk_mdh(X, v0 = NULL, hmult = 1, beta = c(.25,.25), alphamax = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fk_mdh_+3A_x">X</code></td>
<td>
<p>numeric data matrix (num_data x num_dimensions).</p>
</td></tr>
<tr><td><code id="fk_mdh_+3A_v0">v0</code></td>
<td>
<p>(optional) vector representing the initial projection direction. Default is the first principal direction.</p>
</td></tr>
<tr><td><code id="fk_mdh_+3A_hmult">hmult</code></td>
<td>
<p>(optional) positive numeric. The bandwidth in the kernel density is set to hmult multiplied by Silverman's rule of thumb value,
which is based on the AMISE minimiser when the underlying distribution is Gaussian.</p>
</td></tr>
<tr><td><code id="fk_mdh_+3A_beta">beta</code></td>
<td>
<p>(optional) numeric vector of kernel coefficients. The default is the smooth order one kernel described by Hofmeyr (2019).</p>
</td></tr>
<tr><td><code id="fk_mdh_+3A_alphamax">alphamax</code></td>
<td>
<p>(optional) maximum/final (scaled) distance of the optimal hyperplane from the mean of the data.
The default is alphamax = 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list with fields
</p>
<table>
<tr><td><code>$v</code></td>
<td>
<p>the optimal projection vector.</p>
</td></tr>
<tr><td><code>$b</code></td>
<td>
<p>the location of the minimum density hyperplane orthogonal to v.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Pavlidis N.G., Hofmeyr D.P., Tasoulis S.K. (2016) &quot;Minimum Density Hyperplanes&quot;, <em>Journal of Machine Learning Research</em>, 17(156), 1–33.
</p>
<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>op &lt;- par(no.readonly = TRUE)

set.seed(1)

### Generate data from a simple 10 component mixture model in 10 dimensions:
### Determine means of components

mu &lt;- matrix(runif(100), ncol = 10)

### Determine scales of components (diagonal elements of covariance)

covs &lt;- matrix(rexp(100), ncol = 10)/10

### Determine cluster indicator matrix

I &lt;- t(rmultinom(2000, 1, 1:10))

### Determine mean and residual matrix

M &lt;- I%*%mu

R &lt;- matrix(rnorm(20000), 2000, 10)*(I%*%covs)

### Data is given by the sum of these

X &lt;- M + R

### Find the minimum density hyperplane separator and plot
### the projected data as well as the PCA plot for comparison

mdh &lt;- fk_mdh(X)

par(mfrow = c(2, 2))

plot(X%*%mdh$v, X%*%eigen(cov(X))$vectors[,2], xlab = 'mdh vector',
    ylab = '2nd principal component', main = 'MDH solution')
abline(v = mdh$b, col = 2)
plot(X%*%eigen(cov(X))$vectors[,1:2], xlab = '1st principal component',
    ylab = '2nd principal component', main = 'PCA plot')

plot(fk_density(X%*%mdh$v))
abline(v = mdh$b, col = 2)
plot(fk_density(X%*%eigen(cov(X))$vectors[,1]))

par(op)
</code></pre>

<hr>
<h2 id='fk_NW'>Nadaraya-Watson regression estimator</h2><span id='topic+fk_NW'></span>

<h3>Description</h3>

<p>Called by the function fk_regression(). Not intended for alternative use.
</p>

<hr>
<h2 id='fk_ppr'>Projection pursuit regression with local linear kernel smoother</h2><span id='topic+fk_ppr'></span>

<h3>Description</h3>

<p>Generates a projection pursuit regression model for covariates X and response y
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fk_ppr(X, y, nterms=1, hmult=1, betas=NULL, loss=NULL,
    dloss=NULL, initialisation="lm", type = "loc-lin")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fk_ppr_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) of covariates.</p>
</td></tr>
<tr><td><code id="fk_ppr_+3A_y">y</code></td>
<td>
<p>a numeric vector of responses.</p>
</td></tr>
<tr><td><code id="fk_ppr_+3A_nterms">nterms</code></td>
<td>
<p>The number of terms in the additive regression model. The default is a single term.</p>
</td></tr>
<tr><td><code id="fk_ppr_+3A_betas">betas</code></td>
<td>
<p>The coefficients in the expression of the kernel. See Hofmeyr (2019) for details. The
default is the smooth order one kernel described in the paper.</p>
</td></tr>
<tr><td><code id="fk_ppr_+3A_hmult">hmult</code></td>
<td>
<p>The bandwidth in the kernel smoother is set to eigen(cov(X))values[1]^.5/nrow(X)^.2*hmult during projection pursuit. The final value is based on leave-one-out cross validation on the optimal projection.</p>
</td></tr>
<tr><td><code id="fk_ppr_+3A_loss">loss</code></td>
<td>
<p>The (additive) loss function to be used. Allows for generalised responses by specifying an appropriate likelihood/deviance function for the loss. Note: loss is to be minimised, so deviance or negative log-likelihood would be appropriate. If specified then must be a function of y and hy (the fitted values, yhat), returning a vector of &quot;errors&quot;&quot; which are added as the total loss. The default is the squared error.</p>
</td></tr>
<tr><td><code id="fk_ppr_+3A_dloss">dloss</code></td>
<td>
<p>The derivative of the loss function. Also takes arguments y and hy, and returns the vector of partial derivatives of the loss w.r.t. hy.</p>
</td></tr>
<tr><td><code id="fk_ppr_+3A_initialisation">initialisation</code></td>
<td>
<p>Method use to initialise the projection vectors. Must be one of &quot;lm&quot; and &quot;random&quot;, or a function taking only arguments X and y, and returning a vector of length ncol(X). The default is &quot;lm&quot;, which is a simple linear model with a small ridge to ensure a solution. &quot;random&quot; uses random initialisation.</p>
</td></tr>
<tr><td><code id="fk_ppr_+3A_type">type</code></td>
<td>
<p>The type of regression estimator. Must be one of &quot;loc-lin&quot; for local linear regression, or &quot;NW&quot; for the Nadaray
Watson, or local constant regression. The default is local linear regression.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list with class fk_ppr containing
</p>
<table>
<tr><td><code>$mu</code></td>
<td>
<p>The estimated (global) mean of the response.</p>
</td></tr>
<tr><td><code>$mu_X</code></td>
<td>
<p>The vector of estimated means of the covariates.</p>
</td></tr>
<tr><td><code>$y</code></td>
<td>
<p>The responses given as argument to the function.</p>
</td></tr>
<tr><td><code>$X</code></td>
<td>
<p>The covariates given as argument to the function.</p>
</td></tr>
<tr><td><code>$hs</code></td>
<td>
<p>A vector of bandwidths used for each term in the model.</p>
</td></tr>
<tr><td><code>$vs</code></td>
<td>
<p>A matrix whose rows are the projection vectors.</p>
</td></tr>
<tr><td><code>$fitted</code></td>
<td>
<p>The fitted values on each projection. Note that these are based on the residuals used to fit that component of the model, and not the original y values. $fitted is used for prediction on test data.</p>
</td></tr>
<tr><td><code>$beta</code></td>
<td>
<p>The beta coefficients in the kernel formulation.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Friedman, J., and Stuetzle, W. (1981) &quot;Projection pursuit regression.&quot; <em>Journal of the American statistical Association</em> <b>76</b>.376.
</p>
<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>op &lt;- par(no.readonly = TRUE)

set.seed(2)

### Generate a set of data

X = matrix(rnorm(10000), ncol = 10)

### Generate some "true" projection vectors

beta1 = (runif(10)&gt;.5)*rnorm(10)
beta2 = (runif(10)&gt;.5)*rnorm(10)

### Generate responses, dependent on X%*%beta1 and X%*%beta2

y = 1 + X%*%beta1 + ((X%*%beta2)&gt;2)*(X%*%beta2-2)*10
y = y + (X%*%beta1)*(X%*%beta2)/5 + rnorm(1000)

### Fit a PPR model with 2 terms on a sample of the data

train_ids = sample(1:1000, 500)

model = fk_ppr(X[train_ids,], y[train_ids], nterms = 2)

### Predict on left out data, and compute
### estimated coefficient of determination

yhat = predict(model, X[-train_ids,])

MSE = mean((yhat-y[-train_ids])^2)
Var = mean((y[-train_ids]-mean(y[-train_ids]))^2)

1-MSE/Var


#################################################

### Add some "outliers" in the training data and fit
### the model again, as well as one with an absolute loss

y[train_ids] = y[train_ids] + (runif(500)&lt;.05)*(rnorm(500)*100)

model1 &lt;- fk_ppr(X[train_ids,], y[train_ids], nterms = 2)

model2 &lt;- fk_ppr(X[train_ids,], y[train_ids], nterms = 2,
    loss = function(y, hy) abs(y-hy),
    dloss = function(y, hy) sign(hy-y))

### Plot the resulting components in the model on the test data

par(mar = c(2, 2, 2, 2))
par(mfrow = c(2, 2))

plot(X[-train_ids,]%*%model1$vs[1,], y[-train_ids])
plot(X[-train_ids,]%*%model1$vs[2,], y[-train_ids])
plot(X[-train_ids,]%*%model2$vs[1,], y[-train_ids])
plot(X[-train_ids,]%*%model2$vs[2,], y[-train_ids])

par(op)

### Estimate comparative estimated coefficients of determination

MSE1 = mean((predict(model1, X[-train_ids,])-y[-train_ids])^2)
MSE2 = mean((predict(model2, X[-train_ids,])-y[-train_ids])^2)
Var = mean((y[-train_ids]-mean(y[-train_ids]))^2)


1-MSE1/Var
1-MSE2/Var
</code></pre>

<hr>
<h2 id='fk_regression'>Fast univariate kernel regression</h2><span id='topic+fk_regression'></span>

<h3>Description</h3>

<p>Uses recursive formulation for kernel sums as described in Hofmeyr (2021) for exact evaluation of kernel regression estimate.  Both local linear and Nadaraya-Watson (Watson, 1964; Nadaraya, 1964) estimators are available. Binning approximation also available for faster computation if needed. Default is exact evaluation on a grid, but evaluation at sample points is also possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fk_regression(x, y, h = 'amise', beta = NULL, from = NULL,
    to = NULL, ngrid = 1000, nbin = NULL, type = 'loc-lin')
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fk_regression_+3A_x">x</code></td>
<td>
<p>vector of covariates.</p>
</td></tr>
<tr><td><code id="fk_regression_+3A_y">y</code></td>
<td>
<p>vector of response values.</p>
</td></tr>
<tr><td><code id="fk_regression_+3A_h">h</code></td>
<td>
<p>(optional) bandwidth to be used in estimate. Can be either positive numeric, or one of &quot;amise&quot; for a rough estimate of the asymptotic mean integrated square error minimiser, or &quot;cv&quot; for leave-one-out cross validation error minimiser based on squared error. Default is &quot;amise&quot;. Cross validation not available for binning approximation.</p>
</td></tr>
<tr><td><code id="fk_regression_+3A_beta">beta</code></td>
<td>
<p>(optional) numeric vector of kernel coefficients. See Hofmeyr (2019) for details. The
default is the smooth order one kernel described in the paper.</p>
</td></tr>
<tr><td><code id="fk_regression_+3A_from">from</code></td>
<td>
<p>(optional) lower end of evaluation interval if evaluation on a grid is desired. Default is min(x)</p>
</td></tr>
<tr><td><code id="fk_regression_+3A_to">to</code></td>
<td>
<p>(optional) upper end of evaluation interval if evaluation on a grid is desired. Default is max(x)</p>
</td></tr>
<tr><td><code id="fk_regression_+3A_ngrid">ngrid</code></td>
<td>
<p>(optional) integer number of grid points for evaluation. Default is 1000.</p>
</td></tr>
<tr><td><code id="fk_regression_+3A_nbin">nbin</code></td>
<td>
<p>(optional) integer number of bins if binning estimator is to be used. The default is to compute the exact density on a grid of 1000 points.</p>
</td></tr>
<tr><td><code id="fk_regression_+3A_type">type</code></td>
<td>
<p>(optional) one of &quot;loc-lin&quot; and &quot;NW&quot; if local linear or Nadaraya-Watson is desired, respectively. Default is local linear estimator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list with fields
</p>
<table>
<tr><td><code>$x</code></td>
<td>
<p>the vector of points at which the regression function is estimated.</p>
</td></tr>
<tr><td><code>$y</code></td>
<td>
<p>the estimated function values.</p>
</td></tr>
<tr><td><code>$h</code></td>
<td>
<p>the value of the bandwidth.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>
<p>Nadaraya, E.A. (1964) &quot;On estimating regression.&quot; <em>Theory of Probability and Its Applications</em>,9(1), 141–142.
</p>
<p>Watson, G.S. (1964) &quot;Smooth regression analysis.&quot; <em>Sankhya: The Indian Journal of Statistics</em>, Series A, pp. 359–372.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>op &lt;- par(no.readonly = TRUE)

set.seed(1)

n &lt;- 2000
x &lt;- rbeta(n, 2, 2) * 10
y &lt;- 3 * sin(2 * x) + 10 * (x &gt; 5) * (x - 5)
y &lt;- y + rnorm(n) + (rgamma(n, 2, 2) - 1) * (abs(x - 5) + 3)

xs &lt;- seq(min(x), max(x), length = 1000)
ftrue &lt;- 3 * sin(2 * xs) + 10 * (xs &gt; 5) * (xs - 5)

fhat_loc_lin &lt;- fk_regression(x, y)
fhat_NW &lt;- fk_regression(x, y, type = 'NW')

par(mfrow = c(2, 2))
plot(fhat_loc_lin, main = 'Local linear estimator with amise bandwidth')

plot(fhat_NW, main = 'NW estimator with amise bandwidth')


fhat_loc_lin &lt;- fk_regression(x, y, h = 'cv')
fhat_NW &lt;- fk_regression(x, y, type = 'NW', h = 'cv')

plot(fhat_loc_lin, main = 'Local linear estimator with cv bandwidth')

plot(fhat_NW, main = 'NW estimator with cv bandwidth')

par(op)
</code></pre>

<hr>
<h2 id='fk_sum'>Fast Exact Kernel Sum Evaluation</h2><span id='topic+fk_sum'></span>

<h3>Description</h3>

<p>Computes exact (and binned approximations of) kernel and kernel derivative sums with arbitrary weights/coefficients. Computation is based on the method of Hofmeyr (2021).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fk_sum(x, omega, h, x_eval = NULL, beta = c(.25,.25),
    nbin = NULL, type = "ksum")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fk_sum_+3A_x">x</code></td>
<td>
<p>numeric vector of sample points.</p>
</td></tr>
<tr><td><code id="fk_sum_+3A_omega">omega</code></td>
<td>
<p>numeric vector of weights.</p>
</td></tr>
<tr><td><code id="fk_sum_+3A_h">h</code></td>
<td>
<p>numeric bandwidth (must be strictly positive).</p>
</td></tr>
<tr><td><code id="fk_sum_+3A_x_eval">x_eval</code></td>
<td>
<p>vector of evaluation points. Default is to evaluate at the sample points themselves.</p>
</td></tr>
<tr><td><code id="fk_sum_+3A_beta">beta</code></td>
<td>
<p>numeric vector of kernel coefficients. Default is c(.25, .25); the smooth order 1 kernel.</p>
</td></tr>
<tr><td><code id="fk_sum_+3A_nbin">nbin</code></td>
<td>
<p>integer number of bins for binning approximation. Default is to compute the exact sums.</p>
</td></tr>
<tr><td><code id="fk_sum_+3A_type">type</code></td>
<td>
<p>one of &quot;ksum&quot;: returns the kernel sums, &quot;dksum&quot;: returns the kernel derivative sums and &quot;both&quot;: returns a matrix cbind(ksum, dksum).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector (if type%in%c(&quot;ksum&quot;,&quot;dksum&quot;)) of kernel sums, or kernel derivative sums. A matrix (if type == &quot;both&quot;) with kernel sums in its first column and kernel derivative sums in its second column.
</p>


<h3>References</h3>

<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Compute density estimates directly with
### kernel sums and constant normalising weights

set.seed(1)
n &lt;- 150000
num_Gauss &lt;- rbinom(1, n, 2 / 3)
x &lt;- c(rnorm(num_Gauss), rexp(n - num_Gauss) + 1)

hs &lt;- seq(.025, .1, length = 5)
xeval &lt;- seq(-4, 8, length = 1000)
ftrue &lt;- 2 / 3 * dnorm(xeval) + 1 / 3 * dexp(xeval - 1)
plot(xeval, ftrue, lwd = 6, col = rgb(.8, .8, .8), xlab = "x",
  ylab = "f(x)", type = "l")

for(i in 1:5) lines(xeval, fk_sum(x, rep(1 / hs[i] / n, n), hs[i],
    x_eval = xeval), lty = i)
</code></pre>

<hr>
<h2 id='h_Gauss_to_K'>Bandwidth conversion from Gaussian</h2><span id='topic+h_Gauss_to_K'></span>

<h3>Description</h3>

<p>A naive and simple, but useful way of transforming a bandwidth for use with the Gaussian kernel to an appropriate value to be used
with a kernel implemented in the FKSUM package. The transformation if based on the relative values for the kernels for the AMISE minimal
bandwidth for the purpose of density estimation. The transformation has been useful for other problems as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h_Gauss_to_K(h, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h_Gauss_to_K_+3A_h">h</code></td>
<td>
<p>positive numeric bandwidth value appropriate for use with the Gaussian kernel.</p>
</td></tr>
<tr><td><code id="h_Gauss_to_K_+3A_beta">beta</code></td>
<td>
<p>numeric vector of kernel coefficients.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>positive numeric value, the bandwidth to be used for estimation using the kernel with coefficients beta.
</p>


<h3>References</h3>

<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Use existing bandwidth selection with the package

### generate sample from bimodal Gaussian mixture with varying scale

x &lt;- c(rnorm(100000), rnorm(100000)/4 + 2)

### estimate bandwidth using bw.SJ

bwsj &lt;- h_Gauss_to_K(bw.SJ(x), c(.25,.25))

xs &lt;- seq(-3, 4, length = 1000)
plot(xs, dnorm(xs)/2+dnorm(xs,2,1/4)/2, type = 'l',
    col = rgb(.7, .7, .7), lwd = 4)

lines(fk_density(x, h = bwsj), lty = 2, lwd = 2)

</code></pre>

<hr>
<h2 id='h_K_to_Gauss'>Bandwidth conversion to Gaussian</h2><span id='topic+h_K_to_Gauss'></span>

<h3>Description</h3>

<p>A naive and simple, but useful way of transforming a bandwidth for use with a kernel implemented in FKSUM to an appropriate value to be
used with the Gaussian kernel. The transformation if based on the relative values for the kernels for the AMISE minimal
bandwidth for the purpose of density estimation. The transformation has been useful for other problems as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h_K_to_Gauss(h, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h_K_to_Gauss_+3A_h">h</code></td>
<td>
<p>positive numeric bandwidth value appropriate for use with the FKSUM kernel with coefficients beta to an appropriate value
for use with the Gaussian kernel.</p>
</td></tr>
<tr><td><code id="h_K_to_Gauss_+3A_beta">beta</code></td>
<td>
<p>numeric vector of kernel coefficients.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>positive numeric value, the bandwidth to be used for estimation using the Gaussian kernel.
</p>


<h3>References</h3>

<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Use the package for data driven bandwidth for use with
### the Gaussian kernel in existing implementations

### generate sample from bimodal Gaussian mixture with varying scale

x &lt;- c(rnorm(100000), rnorm(100000)/4 + 2)

### estimate bandwidth with the package using MLCV and convert

bwml &lt;- h_K_to_Gauss(fk_density(x, h = 'mlcv',
    beta = c(.25,.25))$h, c(.25,.25))

bwml_binned &lt;- h_K_to_Gauss(fk_density(x, h = 'mlcv',
    beta = c(.25,.25), nbin = 10000)$h, c(.25,.25))

xs &lt;- seq(-3, 4, length = 1000)
plot(xs, dnorm(xs)/2+dnorm(xs,2,1/4)/2, type = 'l',
    lwd = 4, col = rgb(.7, .7, .7))

lines(density(x, bw = bwml), lty = 2, lwd = 2)

lines(density(x, bw = bwml_binned), lty = 3, lwd = 2)
</code></pre>

<hr>
<h2 id='kLLreg'>Leave-one-out regression smoother</h2><span id='topic+kLLreg'></span>

<h3>Description</h3>

<p>Called during processing of fk_ppr(). Not intended for alternative use.
</p>

<hr>
<h2 id='kndksum'>Kernel and kernel derivative sums</h2><span id='topic+kndksum'></span>

<h3>Description</h3>

<p>Fast computation of both kernel sums and kernel derivative sums. Called by multiple functions exported by the package, but not
intended for alternative use.
</p>

<hr>
<h2 id='ksum'>Kernel sums</h2><span id='topic+ksum'></span>

<h3>Description</h3>

<p>Fast computation of kernel sums. Called by multiple functions exported by the package, but not
intended for alternative use.
</p>

<hr>
<h2 id='norm_const_K'>Normalising constant for kernels in FKSUM</h2><span id='topic+norm_const_K'></span>

<h3>Description</h3>

<p>Computes the normalising constant for kernels implemented in the FKSUM package, to convert kernel with arbitrary beta coefficients to one which is a probability density. That is the kernel with coefficients equal to beta/norm_const_beta(beta) has unit integral
</p>


<h3>Usage</h3>

<pre><code class='language-R'>norm_const_K(beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="norm_const_K_+3A_beta">beta</code></td>
<td>
<p>numeric vector of positive coefficients.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>positive numeric normalising constant
</p>


<h3>References</h3>

<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>

<hr>
<h2 id='norm_K'>The L2 norm of a kernel</h2><span id='topic+norm_K'></span>

<h3>Description</h3>

<p>Computes the L2 norm of a kernel implemented in FKSUM, based on its coefficients. NB: the coefficients will first be normalised so that
the kernel represents a density function. Equivalent to sqrt(roughness_K())
</p>


<h3>Usage</h3>

<pre><code class='language-R'>norm_K(beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="norm_K_+3A_beta">beta</code></td>
<td>
<p>numeric vector of positive coefficients.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>positive numeric representing the L2 norm of the kernel with coefficients beta/norm_const(beta).
</p>


<h3>References</h3>

<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>

<hr>
<h2 id='plot_kernel'>Plot the shape of a kernel function implemented in FKSUM based on its vector of beta coefficients</h2><span id='topic+plot_kernel'></span>

<h3>Description</h3>

<p>Plots the kernel function for a given set of beta coefficients. NB: coefficients will be normalised so that the kernel describes
a probability density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_kernel(beta, type = 'l', ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_kernel_+3A_beta">beta</code></td>
<td>
<p>positive numeric vector of kernel coefficients.</p>
</td></tr>
<tr><td><code id="plot_kernel_+3A_type">type</code></td>
<td>
<p>(optional) plot type, as in base plot() function. The default is a line plot.</p>
</td></tr>
<tr><td><code id="plot_kernel_+3A_...">...</code></td>
<td>
<p>(optional) any additional arguments accepted by base plot() function.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Plot order 4 smooth kernel

plot_kernel(1/factorial(0:4))

### Use a different line style

plot_kernel(1/factorial(0:4), lty = 2)

</code></pre>

<hr>
<h2 id='plot.fk_density'>Plot method for class fk_density</h2><span id='topic+plot.fk_density'></span>

<h3>Description</h3>

<p>Plot method for kernel density estimate obtained with function fk_density()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_density'
plot(x, main = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fk_density_+3A_x">x</code></td>
<td>
<p>an object of class fk_density, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="plot.fk_density_+3A_main">main</code></td>
<td>
<p>(optional) main title for plot.</p>
</td></tr>
<tr><td><code id="plot.fk_density_+3A_...">...</code></td>
<td>
<p>(optional) further plotting parameters.</p>
</td></tr>
</table>

<hr>
<h2 id='plot.fk_ICA'>Plot method for class fk_ICA</h2><span id='topic+plot.fk_ICA'></span>

<h3>Description</h3>

<p>Plot method for Independent Component Analysis model obtained with function fk_ICA()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_ICA'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fk_ICA_+3A_x">x</code></td>
<td>
<p>an object of class fk_ICA, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="plot.fk_ICA_+3A_...">...</code></td>
<td>
<p>(optional) further plotting parameters.</p>
</td></tr>
</table>

<hr>
<h2 id='plot.fk_mdh'>Plot method for class fk_mdh</h2><span id='topic+plot.fk_mdh'></span>

<h3>Description</h3>

<p>Plot method for minimum density hyperplane solution obtained with function fk_mdh()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_mdh'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fk_mdh_+3A_x">x</code></td>
<td>
<p>an object of class fk_mdh, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="plot.fk_mdh_+3A_...">...</code></td>
<td>
<p>(optional) further plotting parameters.</p>
</td></tr>
</table>

<hr>
<h2 id='plot.fk_ppr'>Plot method for class fk_ppr</h2><span id='topic+plot.fk_ppr'></span>

<h3>Description</h3>

<p>Plot method for projection pursuit regression model fit with function fk_ppr()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_ppr'
plot(x, term = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fk_ppr_+3A_x">x</code></td>
<td>
<p>an object of class fk_ppr, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="plot.fk_ppr_+3A_term">term</code></td>
<td>
<p>(optional) the term in the additive regression model to be visualised. If omitted then
the residuals and fitted values for the full model are shown.</p>
</td></tr>
<tr><td><code id="plot.fk_ppr_+3A_...">...</code></td>
<td>
<p>(optional) further arguments passed to or from other methods.</p>
</td></tr>
</table>

<hr>
<h2 id='plot.fk_regression'>Plot method for class fk_regression</h2><span id='topic+plot.fk_regression'></span>

<h3>Description</h3>

<p>Plot method for kernel regression estimate obtained with function fk_regression()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_regression'
plot(x, main = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fk_regression_+3A_x">x</code></td>
<td>
<p>an object of class fk_regression, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="plot.fk_regression_+3A_main">main</code></td>
<td>
<p>(optional) main title for plot.</p>
</td></tr>
<tr><td><code id="plot.fk_regression_+3A_...">...</code></td>
<td>
<p>(optional) further plotting parameters.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.fk_ppr'>Predict method for class fk_ppr</h2><span id='topic+predict.fk_ppr'></span>

<h3>Description</h3>

<p>Standard prediction method for regression models, specific to outputs from the fk_ppr() function. See help(fk_ppr) for more details on the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_ppr'
predict(object, Xtest = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.fk_ppr_+3A_object">object</code></td>
<td>
<p>an object of class fk_ppr, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="predict.fk_ppr_+3A_xtest">Xtest</code></td>
<td>
<p>(optional) matrix of test data on which predictions are to be made. If omitted then
fitted values from training data are returned.</p>
</td></tr>
<tr><td><code id="predict.fk_ppr_+3A_...">...</code></td>
<td>
<p>(optional) further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions for Xtest.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>op &lt;- par(no.readonly = TRUE)

set.seed(2)

### Generate a set of data

X = matrix(rnorm(10000), ncol = 10)

### Generate some "true" projection vectors

beta1 = (runif(10)&gt;.5)*rnorm(10)
beta2 = (runif(10)&gt;.5)*rnorm(10)

### Generate responses, dependent on X%*%beta1 and X%*%beta2

y = 1 + X%*%beta1 + ((X%*%beta2)&gt;2)*(X%*%beta2-2)*10
y = y + (X%*%beta1)*(X%*%beta2)/5 + rnorm(1000)

### Fit a PPR model with 2 terms on a sample of the data

train_ids = sample(1:1000, 500)

model = fk_ppr(X[train_ids,], y[train_ids], nterms = 2)

### Predict on left out data, and compute
### estimated coefficient of determination

yhat = predict(model, X[-train_ids,])

MSE = mean((yhat-y[-train_ids])^2)
Var = mean((y[-train_ids]-mean(y[-train_ids]))^2)

1-MSE/Var


#################################################

### Add some "outliers" in the training data and fit
### the model again, as well as one with an absolute loss

y[train_ids] = y[train_ids] + (runif(500)&lt;.05)*(rnorm(500)*100)

model1 &lt;- fk_ppr(X[train_ids,], y[train_ids], nterms = 2)

model2 &lt;- fk_ppr(X[train_ids,], y[train_ids], nterms = 2,
    loss = function(y, hy) abs(y-hy),
    dloss = function(y, hy) sign(hy-y))

### Plot the resulting components in the model on the test data

par(mar = c(2, 2, 2, 2))
par(mfrow = c(2, 2))

plot(X[-train_ids,]%*%model1$vs[1,], y[-train_ids])
plot(X[-train_ids,]%*%model1$vs[2,], y[-train_ids])
plot(X[-train_ids,]%*%model2$vs[1,], y[-train_ids])
plot(X[-train_ids,]%*%model2$vs[2,], y[-train_ids])

par(op)

### estimate comparative estimated coefficients of determination

MSE1 = mean((predict(model1, X[-train_ids,])-y[-train_ids])^2)
MSE2 = mean((predict(model2, X[-train_ids,])-y[-train_ids])^2)
Var = mean((y[-train_ids]-mean(y[-train_ids]))^2)


1-MSE1/Var
1-MSE2/Var
</code></pre>

<hr>
<h2 id='predict.fk_regression'>Predict method for class fk_regression</h2><span id='topic+predict.fk_regression'></span>

<h3>Description</h3>

<p>Predict method for kernel regression. Evaluates the fitted regression function at a set of evaluation/test points
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_regression'
predict(object, xtest = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.fk_regression_+3A_object">object</code></td>
<td>
<p>an object of class fk_regression, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="predict.fk_regression_+3A_xtest">xtest</code></td>
<td>
<p>(optional) vector of evaluation/test points. If omitted then the fitted values at the original sample are returned.</p>
</td></tr>
<tr><td><code id="predict.fk_regression_+3A_...">...</code></td>
<td>
<p>(optional) further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of fitted/predicted values of the regression function.</p>

<hr>
<h2 id='print.fk_density'>Print method for class fk_density</h2><span id='topic+print.fk_density'></span>

<h3>Description</h3>

<p>Print method for kernel density estimate obtained with function fk_density()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_density'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fk_density_+3A_x">x</code></td>
<td>
<p>an object of class fk_density, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="print.fk_density_+3A_...">...</code></td>
<td>
<p>(optional) further arguments passed to or from other methods.</p>
</td></tr>
</table>

<hr>
<h2 id='print.fk_ICA'>Print method for class fk_ICA</h2><span id='topic+print.fk_ICA'></span>

<h3>Description</h3>

<p>Print method for Independent Component Analysis model fit with function fk_ICA()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_ICA'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fk_ICA_+3A_x">x</code></td>
<td>
<p>an object of class fk_ICA, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="print.fk_ICA_+3A_...">...</code></td>
<td>
<p>(optional) further arguments passed to or from other methods.</p>
</td></tr>
</table>

<hr>
<h2 id='print.fk_mdh'>Print method for class fk_mdh</h2><span id='topic+print.fk_mdh'></span>

<h3>Description</h3>

<p>Print method for minimum density hyperplane solution obtained with function fk_mdh()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_mdh'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fk_mdh_+3A_x">x</code></td>
<td>
<p>an object of class fk_mdh, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="print.fk_mdh_+3A_...">...</code></td>
<td>
<p>(optional) further arguments passed to or from other methods.</p>
</td></tr>
</table>

<hr>
<h2 id='print.fk_ppr'>Print method for class fk_ppr</h2><span id='topic+print.fk_ppr'></span>

<h3>Description</h3>

<p>Print method for projection pursuit regression model fit with function fk_ppr()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_ppr'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fk_ppr_+3A_x">x</code></td>
<td>
<p>an object of class fk_ppr, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="print.fk_ppr_+3A_...">...</code></td>
<td>
<p>(optional) further arguments passed to or from other methods.</p>
</td></tr>
</table>

<hr>
<h2 id='print.fk_regression'>Print method for class fk_regression</h2><span id='topic+print.fk_regression'></span>

<h3>Description</h3>

<p>Print method for kernel regression estimate obtained with function fk_regression()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fk_regression'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fk_regression_+3A_x">x</code></td>
<td>
<p>an object of class fk_regression, output from the function of the same name.</p>
</td></tr>
<tr><td><code id="print.fk_regression_+3A_...">...</code></td>
<td>
<p>(optional) further arguments passed to or from other methods.</p>
</td></tr>
</table>

<hr>
<h2 id='roughness_K'>Kernel roughness</h2><span id='topic+roughness_K'></span>

<h3>Description</h3>

<p>Computes the squared L2 norm, also known as the roughness, of a kernel implemented in FKSUM based on its beta coefficients. NB: the
coefficients will first be normalised so that the kernel represents a density function. Equivalent to norm_K()^2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roughness_K(beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roughness_K_+3A_beta">beta</code></td>
<td>
<p>numeric vector of positive coefficients.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>positive numeric representing the squared L2 norm, or roughness of the kernel with coefficients beta/norm_const(beta).
</p>


<h3>References</h3>

<p>Hofmeyr, D.P. (2021) &quot;Fast exact evaluation of univariate kernel sums&quot;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>

<hr>
<h2 id='sm_bin_wts'>Compute smoothed bin weights</h2><span id='topic+sm_bin_wts'></span>

<h3>Description</h3>

<p>Computes total weight from an initial weight vector falling into regular bins, based on corresponding location of sample points.
Individual points are shared between pairs of adjacent bins to produce slightly smoother estimation. Not intended for general use.
</p>

<hr>
<h2 id='var_K'>Variance of a kernel</h2><span id='topic+var_K'></span>

<h3>Description</h3>

<p>Computes the variance of the random variable whose density is given by a kernel implemented in FKSUM, with coefficients beta.
NB: coefficients will first be normalised so that the kernel is a density function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_K(beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_K_+3A_beta">beta</code></td>
<td>
<p>positive numeric vector of kernel coefficients.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A positive numeric value representing the variance of the random variable with density given by the kernel.
</p>

<hr>
<h2 id='whiten'>Whitening (standardising) a data matrix</h2><span id='topic+whiten'></span>

<h3>Description</h3>

<p>Transforms data matrix by centering, projecting onto its first k principal components, and standardising the resulting components. The output is a zero mean, identity covariance data matrix. Only used for pre-processing prior to ICA. Not intended for alternative access.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
