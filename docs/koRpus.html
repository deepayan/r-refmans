<!DOCTYPE html><html><head><title>Help for package koRpus</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {koRpus}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ARI'><p>Readability: Automated Readability Index (ARI)</p></a></li>
<li><a href='#available.koRpus.lang'><p>List available language packages</p></a></li>
<li><a href='#bormuth'><p>Readability: Bormuth's Mean Cloze and Grade Placement</p></a></li>
<li><a href='#C.ld'><p>Lexical diversity: Herdan's C</p></a></li>
<li><a href='#clozeDelete'><p>Transform text into cloze test format</p></a></li>
<li><a href='#coleman'><p>Readability: Coleman's Formulas</p></a></li>
<li><a href='#coleman.liau'><p>Readability: Coleman-Liau Index</p></a></li>
<li><a href='#correct.tag'><p>Methods to correct koRpus objects</p></a></li>
<li><a href='#cTest'><p>Transform text into C-Test-like format</p></a></li>
<li><a href='#CTTR'><p>Lexical diversity: Carroll's corrected TTR (CTTR)</p></a></li>
<li><a href='#dale.chall'><p>Readability: Dale-Chall Readability Formula</p></a></li>
<li><a href='#danielson.bryan'><p>Readability: Danielson-Bryan</p></a></li>
<li><a href='#dickes.steiwer'><p>Readability: Dickes-Steiwer Handformel</p></a></li>
<li><a href='#docTermMatrix'><p>Generate a document-term matrix</p></a></li>
<li><a href='#DRP'><p>Readability: Degrees of Reading Power (DRP)</p></a></li>
<li><a href='#ELF'><p>Readability: Fang's Easy Listening Formula (ELF)</p></a></li>
<li><a href='#farr.jenkins.paterson'><p>Readability: Farr-Jenkins-Paterson Index</p></a></li>
<li><a href='#filterByClass'><p>Remove word classes</p></a></li>
<li><a href='#flesch'><p>Readability: Flesch Readability Ease</p></a></li>
<li><a href='#flesch.kincaid'><p>Readability: Flesch-Kincaid Grade Level</p></a></li>
<li><a href='#FOG'><p>Readability: Gunning FOG Index</p></a></li>
<li><a href='#FORCAST'><p>Readability: FORCAST Index</p></a></li>
<li><a href='#freq.analysis'><p>Analyze word frequencies</p></a></li>
<li><a href='#fucks'><p>Readability: Fucks' Stilcharakteristik</p></a></li>
<li><a href='#get.kRp.env'><p>Get koRpus session settings</p></a></li>
<li><a href='#guess.lang'><p>Guess language a text is written in</p></a></li>
<li><a href='#gutierrez'><p>Readability: Gutiérrez <em>Fórmula de comprensibilidad</em></p></a></li>
<li><a href='#harris.jacobson'><p>Readability: Harris-Jacobson indices</p></a></li>
<li><a href='#HDD'><p>Lexical diversity: HD-D (vocd-d)</p></a></li>
<li><a href='#hyphen,kRp.text-method'><p>Automatic hyphenation</p></a></li>
<li><a href='#install.koRpus.lang'><p>Install language support packages</p></a></li>
<li><a href='#jumbleWords'><p>Produce jumbled words</p></a></li>
<li><a href='#K.ld'><p>Lexical diversity: Yule's K</p></a></li>
<li><a href='#koRpus-deprecated'><p>Deprecated object classes</p></a></li>
<li><a href='#koRpus-package'><p>Text Analysis with Emphasis on POS Tagging, Readability, and Lexical Diversity</p></a></li>
<li><a href='#kRp.cluster'><p>Work in (early) progress. Probably don't even look at it. Consider it pure magic that is not to be tempered with.</p></a></li>
<li><a href='#kRp.corp.freq,-class'><p>S4 Class kRp.corp.freq</p></a></li>
<li><a href='#kRp.lang,-class'><p>S4 Class kRp.lang</p></a></li>
<li><a href='#kRp.POS.tags'><p>Get elaborated word tag definitions</p></a></li>
<li><a href='#kRp.readability,-class'><p>S4 Class kRp.readability</p></a></li>
<li><a href='#kRp.text,-class'><p>S4 Class kRp.text</p></a></li>
<li><a href='#kRp.TTR,-class'><p>S4 Class kRp.TTR</p></a></li>
<li><a href='#lex.div'><p>Analyze lexical diversity</p></a></li>
<li><a href='#lex.div.num'><p>Calculate lexical diversity</p></a></li>
<li><a href='#linsear.write'><p>Readability: Linsear Write Index</p></a></li>
<li><a href='#LIX'><p>Readability: Bj\&quot;ornsson's L\&quot;asbarhetsindex (LIX)</p></a></li>
<li><a href='#maas'><p>Lexical diversity: Maas' indices</p></a></li>
<li><a href='#MATTR'><p>Lexical diversity: Moving-Average Type-Token Ratio (MATTR)</p></a></li>
<li><a href='#MSTTR'><p>Lexical diversity: Mean Segmental Type-Token Ratio (MSTTR)</p></a></li>
<li><a href='#MTLD'><p>Lexical diversity: Measure of Textual Lexical Diversity (MTLD)</p></a></li>
<li><a href='#nWS'><p>Readability: Neue Wiener Sachtextformeln</p></a></li>
<li><a href='#pasteText'><p>Paste koRpus objects</p></a></li>
<li><a href='#plot'><p>Plot method for objects of class kRp.text</p></a></li>
<li><a href='#query'><p>A method to get information out of koRpus objects</p></a></li>
<li><a href='#R.ld'><p>Lexical diversity: Guiraud's R</p></a></li>
<li><a href='#read.BAWL'><p>Import BAWL-R data</p></a></li>
<li><a href='#read.corp.celex'><p>Import Celex data</p></a></li>
<li><a href='#read.corp.custom'><p>Import custom corpus data</p></a></li>
<li><a href='#read.corp.LCC'><p>Import LCC data</p></a></li>
<li><a href='#readability'><p>Measure readability</p></a></li>
<li><a href='#readability.num'><p>Calculate readability</p></a></li>
<li><a href='#readTagged'><p>Import already tagged texts</p></a></li>
<li><a href='#RIX'><p>Readability: Anderson's Readability Index (RIX)</p></a></li>
<li><a href='#S.ld'><p>Lexical diversity: Summer's S</p></a></li>
<li><a href='#segment.optimizer'><p>A function to optimize MSTTR segment sizes</p></a></li>
<li><a href='#set.kRp.env'><p>A function to set information on your koRpus environment</p></a></li>
<li><a href='#set.lang.support'><p>Add support for new languages</p></a></li>
<li><a href='#show,kRp.lang-method'><p>Show methods for koRpus objects</p></a></li>
<li><a href='#SMOG'><p>Readability: Simple Measure of Gobbledygook (SMOG)</p></a></li>
<li><a href='#spache'><p>Readability: Spache Formula</p></a></li>
<li><a href='#split_by_doc_id'><p>Turn a multi-document kRp.text object into a list of kRp.text objects</p></a></li>
<li><a href='#strain'><p>Readability: Strain Index</p></a></li>
<li><a href='#summary'><p>Summary methods for koRpus objects</p></a></li>
<li><a href='#taggedText'><p>Getter/setter methods for koRpus objects</p></a></li>
<li><a href='#textFeatures'><p>Extract text features for authorship analysis</p></a></li>
<li><a href='#textTransform'><p>Letter case transformation</p></a></li>
<li><a href='#tokenize'><p>A simple tokenizer</p></a></li>
<li><a href='#traenkle.bailer'><p>Readability: Traenkle-Bailer Formeln</p></a></li>
<li><a href='#treetag'><p>A method to call TreeTagger</p></a></li>
<li><a href='#TRI'><p>Readability: Kuntzsch's Text-Redundanz-Index</p></a></li>
<li><a href='#TTR'><p>Lexical diversity: Type-Token Ratio</p></a></li>
<li><a href='#tuldava'><p>Readability: Tuldava's Text Difficulty Formula</p></a></li>
<li><a href='#types'><p>Get types and tokens of a given text</p></a></li>
<li><a href='#U.ld'><p>Lexical diversity: Uber Index (U)</p></a></li>
<li><a href='#wheeler.smith'><p>Readability: Wheeler-Smith Score</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Text Analysis with Emphasis on POS Tagging, Readability, and
Lexical Diversity</td>
</tr>
<tr>
<td>Description:</td>
<td>A set of tools to analyze texts. Includes, amongst others, functions for
          automatic language detection, hyphenation, several indices of lexical diversity
          (e.g., type token ratio, HD-D/vocd-D, MTLD) and readability (e.g., Flesch,
          SMOG, LIX, Dale-Chall). Basic import functions for language corpora are also
          provided, to enable frequency analyses (supports Celex and Leipzig Corpora
          Collection file formats) and measures like tf-idf. Note: For full functionality
          a local installation of TreeTagger is recommended. It is also recommended to
          not load this package directly, but by loading one of the available language
          support packages from the 'l10n' repository
          <a href="https://undocumeantit.github.io/repos/l10n/">https://undocumeantit.github.io/repos/l10n/</a>. 'koRpus' also includes a plugin
          for the R GUI and IDE RKWard, providing graphical dialogs for its basic
          features. The respective R package 'rkward' cannot be installed directly from a
          repository, as it is a part of RKWard. To make full use of this feature, please
          install RKWard from <a href="https://rkward.kde.org">https://rkward.kde.org</a> (plugins are detected
          automatically). Due to some restrictions on CRAN, the full package sources are
          only available from the project homepage. To ask for help, report bugs, request
          features, or discuss the development of the package, please subscribe to the
          koRpus-dev mailing list (<a href="https://korpusml.reaktanz.de">https://korpusml.reaktanz.de</a>).</td>
</tr>
<tr>
<td>Author:</td>
<td>Meik Michalke [aut, cre],
  Earl Brown [ctb],
  Alberto Mirisola [ctb],
  Alexandre Brulet [ctb],
  Laura Hauser [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Meik Michalke &lt;meik.michalke@hhu.de&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0),sylly (&ge; 0.1-6)</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table,methods,Matrix</td>
</tr>
<tr>
<td>Enhances:</td>
<td>rkward</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat,tm,SnowballC,shiny,knitr,rmarkdown,koRpus.lang.de,koRpus.lang.en,koRpus.lang.es,koRpus.lang.fr,koRpus.lang.it,koRpus.lang.nl,koRpus.lang.pt,koRpus.lang.ru</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://reaktanz.de/?c=hacking&amp;s=koRpus">https://reaktanz.de/?c=hacking&amp;s=koRpus</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/unDocUMeantIt/koRpus/issues">https://github.com/unDocUMeantIt/koRpus/issues</a></td>
</tr>
<tr>
<td>Additional_repositories:</td>
<td><a href="https://undocumeantit.github.io/repos/l10n">https://undocumeantit.github.io/repos/l10n</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Version:</td>
<td>0.13-8</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-05-17</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Collate:</td>
<td>'01_class_01_kRp.text.R' '02_method_filterByClass.R'
'koRpus-internal.R' '00_environment.R' '01_class_02_kRp.TTR.R'
'01_class_03_kRp.corp.freq.R' '01_class_04_kRp.lang.R'
'01_class_05_kRp.readability.R'
'01_class_81_kRp.connection_union.R'
'02_method_get_set_kRp.text.R'
'01_class_90_deprecated_classes.R' '02_method_cTest.R'
'02_method_clozeDelete.R' '02_method_correct.R'
'02_method_docTermMatrix.R' '02_method_freq.analysis.R'
'02_method_hyphen.R' '02_method_jumbleWords.R'
'02_method_lex.div.R' '02_method_pasteText.R'
'02_method_plot.kRp.text.R' '02_method_query.R'
'02_method_read.corp.custom.R' '02_method_readTagged.R'
'02_method_readability.R' '02_method_show.kRp.lang.R'
'02_method_show.kRp.TTR.R' '02_method_show.kRp.corp.freq.R'
'02_method_show.kRp.readability.R' '02_method_show.kRp.text.R'
'02_method_split_by_doc_id.R' '02_method_summary.kRp.lang.R'
'02_method_summary.kRp.TTR.R'
'02_method_summary.kRp.readability.R'
'02_method_summary.kRp.text.R' '02_method_textTransform.R'
'02_method_tokenize.R' '02_method_treetag.R'
'02_method_types_tokens.R' 'available.koRpus.lang.R'
'get.kRp.env.R' 'guess.lang.R' 'install.koRpus.lang.R'
'kRp.POS.tags.R' 'kRp.cluster.R'
'koRpus-internal.freq.analysis.R' 'koRpus-internal.import.R'
'koRpus-internal.lexdiv.formulae.R'
'koRpus-internal.rdb.formulae.R'
'koRpus-internal.rdb.params.grades.R'
'koRpus-internal.read.corp.custom.R' 'koRpus-package.R'
'lex.div.num.R' 'read.BAWL.R' 'read.corp.LCC.R'
'read.corp.celex.R' 'readability.num.R' 'segment.optimizer.R'
'set.kRp.env.R' 'set.lang.support.R' 'textFeatures.R'
'wrapper_functions_lex.div.R' 'wrapper_functions_readability.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-05-17 16:26:12 UTC; m</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-05-17 21:50:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='ARI'>Readability: Automated Readability Index (ARI)</h2><span id='topic+ARI'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ARI(txt.file, parameters = c(asl = 0.5, awl = 4.71, const = 21.43), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ARI_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="ARI_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="ARI_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Automated Readability Index (ARI). In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>If <code>parameters="NRI"</code>,
the simplified parameters from the Navy Readability Indexes are used, if set to
<code>ARI="simple"</code>, the simplified formula is calculated.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>DuBay, W.H. (2004). <em>The Principles of Readability</em>. Costa Mesa: Impact Information.
WWW: <a href="http://www.impact-information.com/impactinfo/readability02.pdf">http://www.impact-information.com/impactinfo/readability02.pdf</a>; 22.03.2011.
</p>
<p>Smith, E.A. &amp; Senter,
R.J. (1967). <em>Automated readability index</em>. AMRL-TR-66-22. Wright-Paterson AFB, Ohio: Aerospace Medical Division.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ARI(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='available.koRpus.lang'>List available language packages</h2><span id='topic+available.koRpus.lang'></span>

<h3>Description</h3>

<p>Get a list of all currently available language packages for koRpus from the official
l10n repository.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>available.koRpus.lang(repos = "https://undocumeantit.github.io/repos/l10n/")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="available.koRpus.lang_+3A_repos">repos</code></td>
<td>
<p>The URL to additional repositories to query. You should probably leave this to the
default, but if you would like to use a third party repository, you're free to do so. The
value is temporarily appended to the repos currently returned by <code>getOption("repos")</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>koRpus' language support is modular by design, meaning you can (and must) load
an extension package for each language you want to work with in a given session.
These language support packages are named <code>koRpus.lang.**</code>, where <code>**</code>
is replaced by a valid language identifier (like <code>en</code> for English or <code>de</code>
for German). See <code><a href="#topic+set.lang.support">set.lang.support</a></code> for more details.
</p>
<p>This function downloads the package list from (also) the official localization repository
for koRpus and lists all currently available language packages that you could install
and load. Apart from than it does not download or install anything.
</p>
<p>You can install the packages by either calling the convenient wrapper function
<code><a href="#topic+install.koRpus.lang">install.koRpus.lang</a></code>, or
<code><a href="utils.html#topic+install.packages">install.packages</a></code> (see examples).
</p>


<h3>Value</h3>

<p>Returns an invisible character vector with all available language packages.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+install.koRpus.lang">install.koRpus.lang</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# see all available language packages
available.koRpus.lang()

# install support for German
install.koRpus.lang("de")
# alternatively, you could call install.packages directly
install.packages("koRpus.lang.de", repos="https://undocumeantit.github.io/repos/l10n/")

## End(Not run)
</code></pre>

<hr>
<h2 id='bormuth'>Readability: Bormuth's Mean Cloze and Grade Placement</h2><span id='topic+bormuth'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bormuth(txt.file, word.list, clz=35,
   meanc=c(const=0.886593, awl=0.08364, afw=0.161911,
     asl1=0.021401, asl2=0.000577, asl3=0.000005),
   grade=c(const=4.275, m1=12.881, m2=34.934, m3=20.388,
     c1=26.194, c2=2.046, c3=11.767, mc1=44.285, mc2=97.62,
     mc3=59.538), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bormuth_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="bormuth_+3A_word.list">word.list</code></td>
<td>
<p>A vector or matrix (with exactly one column) which defines familiar words. For valid results
the long Dale-Chall list with 3000 words should be used.</p>
</td></tr>
<tr><td><code id="bormuth_+3A_clz">clz</code></td>
<td>
<p>Integer, the cloze criterion score in percent.</p>
</td></tr>
<tr><td><code id="bormuth_+3A_meanc">meanc</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for Mean Cloze calculation.</p>
</td></tr>
<tr><td><code id="bormuth_+3A_grade">grade</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for Grade Placement calculation.
If omitted, Grade Placement will not be calculated.</p>
</td></tr>
<tr><td><code id="bormuth_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates Bormuth's Mean Cloze and estimted grade placement. In contrast to
<code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  bormuth(tagged.text, word.list=new.dale.chall.wl)

## End(Not run)
</code></pre>

<hr>
<h2 id='C.ld'>Lexical diversity: Herdan's C</h2><span id='topic+C.ld'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>C.ld(txt, char = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="C.ld_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="C.ld_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="C.ld_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates Herdan's C. In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the C value,
and characteristics are
off by default.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
C.ld(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='clozeDelete'>Transform text into cloze test format</h2><span id='topic+clozeDelete'></span><span id='topic+clozeDelete+2CkRp.text-method'></span>

<h3>Description</h3>

<p>If you feed a tagged text object to this function, its text will be transformed into
a format used for cloze deletion tests. That is,
by default every fifth word (or as specified by
<code>every</code>) will be replaced by a line. You can also set an offset value to specify where
to begin.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clozeDelete(obj, ...)

## S4 method for signature 'kRp.text'
clozeDelete(obj, every = 5, offset = 0, replace.by = "_", fixed = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clozeDelete_+3A_obj">obj</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
<tr><td><code id="clozeDelete_+3A_...">...</code></td>
<td>
<p>Additional arguments to the method (as described in this document).</p>
</td></tr>
<tr><td><code id="clozeDelete_+3A_every">every</code></td>
<td>
<p>Integer numeric,
setting the frequency of words to be manipulated. By default,
every fifth word is being transformed.</p>
</td></tr>
<tr><td><code id="clozeDelete_+3A_offset">offset</code></td>
<td>
<p>Either an integer numeric,
sets the number of words to offset the transformations. Or the
special keyword <code>"all"</code>,
which will cause the method to iterate through all possible offset values
and not return an object, but print the results (including the list with changed words).</p>
</td></tr>
<tr><td><code id="clozeDelete_+3A_replace.by">replace.by</code></td>
<td>
<p>Character, will be used as the replacement for the removed words.</p>
</td></tr>
<tr><td><code id="clozeDelete_+3A_fixed">fixed</code></td>
<td>
<p>Integer numberic,
defines the length of the replacement (<code>replace.by</code> will
be repeated this much times). If set to 0,
the replacement wil be as long as the replaced word.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The option <code>offset="all"</code> will not return one single object,
but print the results after iterating
through all possible offset values.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> with the added feature <code>diff</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  tokenized.obj &lt;- clozeDelete(tokenized.obj)
  pasteText(tokenized.obj)

  # diff stats are now part of the object
  hasFeature(tokenized.obj)
  diffText(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='coleman'>Readability: Coleman's Formulas</h2><span id='topic+coleman'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coleman(
  txt.file,
  hyphen = NULL,
  parameters = c(syll = 1),
  clz1 = c(word = 1.29, const = 38.45),
  clz2 = c(word = 1.16, sntc = 1.48, const = 37.95),
  clz3 = c(word = 1.07, sntc = 1.18, pron = 0.76, const = 34.02),
  clz4 = c(word = 1.04, sntc = 1.06, pron = 0.56, prep = 0.36, const = 26.01),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coleman_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="coleman_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="coleman_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for all formulas of the index.</p>
</td></tr>
<tr><td><code id="coleman_+3A_clz1">clz1</code></td>
<td>
<p>A numeric vector with named magic numbers for the first formula.</p>
</td></tr>
<tr><td><code id="coleman_+3A_clz2">clz2</code></td>
<td>
<p>A numeric vector with named magic numbers for the second formula.</p>
</td></tr>
<tr><td><code id="coleman_+3A_clz3">clz3</code></td>
<td>
<p>A numeric vector with named magic numbers for the third formula.</p>
</td></tr>
<tr><td><code id="coleman_+3A_clz4">clz4</code></td>
<td>
<p>A numeric vector with named magic numbers for the fourth formula.</p>
</td></tr>
<tr><td><code id="coleman_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the four readability formulas by Coleman. In contrast to
<code><a href="#topic+readability">readability</a></code>, which by default calculates all possible
indices, this function will only calculate the index value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
coleman(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='coleman.liau'>Readability: Coleman-Liau Index</h2><span id='topic+coleman.liau'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coleman.liau(
  txt.file,
  ecp = c(const = 141.8401, char = 0.21459, sntc = 1.079812),
  grade = c(ecp = -27.4004, const = 23.06395),
  short = c(awl = 5.88, spw = 29.6, const = 15.8),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coleman.liau_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="coleman.liau_+3A_ecp">ecp</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the cloze percentage estimate.</p>
</td></tr>
<tr><td><code id="coleman.liau_+3A_grade">grade</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters to calculate grade equvalent for ECP values.</p>
</td></tr>
<tr><td><code id="coleman.liau_+3A_short">short</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the short form of the formula.</p>
</td></tr>
<tr><td><code id="coleman.liau_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Coleman-Liau index. In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
coleman.liau(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='correct.tag'>Methods to correct koRpus objects</h2><span id='topic+correct.tag'></span><span id='topic+correct.tag+2CkRp.text-method'></span>

<h3>Description</h3>

<p>The method <code>correct.tag</code> can be used to alter objects of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correct.tag(
  obj,
  row,
  tag = NULL,
  lemma = NULL,
  check.token = NULL,
  quiet = TRUE
)

## S4 method for signature 'kRp.text'
correct.tag(
  obj,
  row,
  tag = NULL,
  lemma = NULL,
  check.token = NULL,
  quiet = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correct.tag_+3A_obj">obj</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
<tr><td><code id="correct.tag_+3A_row">row</code></td>
<td>
<p>Integer, the row number of the entry to be changed. Can be an integer vector
to change several rows in one go.</p>
</td></tr>
<tr><td><code id="correct.tag_+3A_tag">tag</code></td>
<td>
<p>A character string with a valid POS tag to replace the current tag entry.
If <code>NULL</code> (the default) the entry remains unchanged.</p>
</td></tr>
<tr><td><code id="correct.tag_+3A_lemma">lemma</code></td>
<td>
<p>A character string naming the lemma to to replace the current lemma entry.
If <code>NULL</code> (the default) the entry remains unchanged.</p>
</td></tr>
<tr><td><code id="correct.tag_+3A_check.token">check.token</code></td>
<td>
<p>A character string naming the token you expect to be in this row.
If not <code>NULL</code>, <code>correct</code> will stop with an error if this values don't match.</p>
</td></tr>
<tr><td><code id="correct.tag_+3A_quiet">quiet</code></td>
<td>
<p>If <code>FALSE</code>, messages about all applied changes are shown.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although automatic POS tagging and lemmatization are remarkably accurate,
the algorithms do ususally produce
some errors. If you want to correct for these flaws, this method can be of help,
because it might prevent you from
introducing new errors. That is,
it will do some sanitiy checks before the object is actually manipulated and returned.
</p>
<p><code>correct.tag</code> will read the <code>lang</code> slot from the given object and check whether the <code>tag</code>
provided is actually valid. If so,
it will not only change the <code>tag</code> field in the object, but also update
<code>wclass</code> and <code>desc</code> accordingly.
</p>
<p>If <code>check.token</code> is set it must also match <code>token</code> in the given row(s). Note that no check is done on the lemmata.
</p>


<h3>Value</h3>

<p>An object of the same class as <code>obj</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.text-class">kRp.text</a></code>, <code><a href="#topic+treetag">treetag</a></code>,
<code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  tokenized.obj &lt;- correct.tag(tokenized.obj, row=6, tag="NN")
} else {}
</code></pre>

<hr>
<h2 id='cTest'>Transform text into C-Test-like format</h2><span id='topic+cTest'></span><span id='topic+cTest+2CkRp.text-method'></span>

<h3>Description</h3>

<p>If you feed a tagged text object to this function, its text will be transformed into
a format used for C-Tests:
</p>

<ul>
<li><p>the first and last sentence will be left untouched (except if the <code>start</code>
and <code>stop</code> values of the <code>intact</code> parameter are changed
</p>
</li>
<li><p>of all other sentences, the second half of every 2nd word (or as specified by
<code>every</code>) will be replaced by a line
</p>
</li>
<li><p>words must have at least <code>min.length</code> characters, otherwise they are
skipped
</p>
</li>
<li><p>words an uneven number of characters will be replaced after the next character,
i.e., a word with five characters will keep the first three and have the last two
replaced
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>cTest(obj, ...)

## S4 method for signature 'kRp.text'
cTest(
  obj,
  every = 2,
  min.length = 3,
  intact = c(start = 1, end = 1),
  replace.by = "_"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cTest_+3A_obj">obj</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
<tr><td><code id="cTest_+3A_...">...</code></td>
<td>
<p>Additional arguments to the method (as described in this document).</p>
</td></tr>
<tr><td><code id="cTest_+3A_every">every</code></td>
<td>
<p>Integer numeric,
setting the frequency of words to be manipulated. By default,
every other word is being transformed.</p>
</td></tr>
<tr><td><code id="cTest_+3A_min.length">min.length</code></td>
<td>
<p>Integer numeric,
sets the minimum length of words to be considered (in letters).</p>
</td></tr>
<tr><td><code id="cTest_+3A_intact">intact</code></td>
<td>
<p>Named vector with the elements <code>start</code> and <code>end</code>. both must be integer values
and define, which sentences are to be left untouched,
counted in sentences from beginning and end of the text.
The default is to ignore the first and last sentence.</p>
</td></tr>
<tr><td><code id="cTest_+3A_replace.by">replace.by</code></td>
<td>
<p>Character, will be used as the replacement for the removed word halves.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> with the added feature <code>diff</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  tokenized.obj &lt;- cTest(tokenized.obj)
  pasteText(tokenized.obj)

  # diff stats are now part of the object
  hasFeature(tokenized.obj)
  diffText(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='CTTR'>Lexical diversity: Carroll's corrected TTR (CTTR)</h2><span id='topic+CTTR'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CTTR(txt, char = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CTTR_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="CTTR_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="CTTR_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates Carroll's corrected TTR (CTTR). In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the CTTR value,
and characteristics are
off by default.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
CTTR(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='dale.chall'>Readability: Dale-Chall Readability Formula</h2><span id='topic+dale.chall'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dale.chall(
  txt.file,
  word.list,
  parameters = c(const = 64, dword = 0.95, asl = 0.69),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dale.chall_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="dale.chall_+3A_word.list">word.list</code></td>
<td>
<p>A vector or matrix (with exactly one column) which defines familiar words. For valid results
the long Dale-Chall list with about 3000 words should be used.</p>
</td></tr>
<tr><td><code id="dale.chall_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="dale.chall_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the New Dale-Chall Readability Formula. In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>If <code>parameters="PSK"</code>, the parameters by Powers-Sumner-Kearl (1958) are used, and if
<code>parameters="old"</code>, the original parameters by Dale-Chall (1948), respectively.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dale.chall(tagged.text, word.list=new.dale.chall.wl)

## End(Not run)
</code></pre>

<hr>
<h2 id='danielson.bryan'>Readability: Danielson-Bryan</h2><span id='topic+danielson.bryan'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>danielson.bryan(
  txt.file,
  db1 = c(cpb = 1.0364, cps = 0.0194, const = 0.6059),
  db2 = c(const = 131.059, cpb = 10.364, cps = 0.194),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="danielson.bryan_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="danielson.bryan_+3A_db1">db1</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the first formula (regression).</p>
</td></tr>
<tr><td><code id="danielson.bryan_+3A_db2">db2</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the second formula (cloze equivalent).</p>
</td></tr>
<tr><td><code id="danielson.bryan_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the two Danielson-Bryan formulas. In contrast to
<code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  danielson.bryan(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='dickes.steiwer'>Readability: Dickes-Steiwer Handformel</h2><span id='topic+dickes.steiwer'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dickes.steiwer(
  txt.file,
  parameters = c(const = 235.95993, awl = 73.021, asl = 12.56438, ttr = 50.03293),
  case.sens = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dickes.steiwer_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="dickes.steiwer_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="dickes.steiwer_+3A_case.sens">case.sens</code></td>
<td>
<p>Logical, whether types should be counted case sensitive.</p>
</td></tr>
<tr><td><code id="dickes.steiwer_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the shortcut formula by Dickes-Steiwer. In contrast to
<code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  dickes.steiwer(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='docTermMatrix'>Generate a document-term matrix</h2><span id='topic+docTermMatrix'></span><span id='topic+docTermMatrix+2Cdata.frame-method'></span><span id='topic+docTermMatrix+2C-methods'></span><span id='topic+docTermMatrix+2CkRp.text-method'></span>

<h3>Description</h3>

<p>Returns a sparse document-term matrix calculated from a given TIF[1] compliant token data frame
or object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>. You can also
calculate the term frequency inverted document frequency value (tf-idf) for each term.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>docTermMatrix(obj, terms = "token", case.sens = FALSE, tfidf = FALSE, ...)

## S4 method for signature 'data.frame'
docTermMatrix(obj, terms = "token", case.sens = FALSE,
      tfidf = FALSE)

## S4 method for signature 'kRp.text'
docTermMatrix(obj, terms = "token", case.sens = FALSE, tfidf = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="docTermMatrix_+3A_obj">obj</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
or a TIF[1] compliant token data frame.</p>
</td></tr>
<tr><td><code id="docTermMatrix_+3A_terms">terms</code></td>
<td>
<p>A character string defining the <code>tokens</code> column to be used for calculating the matrix.</p>
</td></tr>
<tr><td><code id="docTermMatrix_+3A_case.sens">case.sens</code></td>
<td>
<p>Logical, whether terms should be counted case sensitive.</p>
</td></tr>
<tr><td><code id="docTermMatrix_+3A_tfidf">tfidf</code></td>
<td>
<p>Logical,
if <code>TRUE</code> calculates term frequency&ndash;inverse document frequency (tf-idf)
values instead of absolute frequency.</p>
</td></tr>
<tr><td><code id="docTermMatrix_+3A_...">...</code></td>
<td>
<p>Additional arguments depending on the particular method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is usually more interesting if done with more than one single text. If you're interested
in full corpus analysis, the <code>tm.plugin.koRpus</code> package should be worth checking out.
Alternatively, a data frame with multiple <code>doc_id</code> entries can be used.
</p>
<p>See the examples to learn how to limit the analysis to desired word classes.
</p>


<h3>Value</h3>

<p>A sparse matrix of class <code><a href="Matrix.html#topic+dgCMatrix-class">dgCMatrix</a></code>.
</p>


<h3>References</h3>

<p>[1] Text Interchange Formats (<a href="https://github.com/ropensci/tif">https://github.com/ropensci/tif</a>)
[2] tm.plugin.koRpus: https://CRAN.R-project.org/package=tm.plugin.koRpus
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  # of course this makes more sense with a corpus of
  # multiple texts, see the tm.plugin.koRpus[2] package
  # for that
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  # get the document-term frequencies in a sparse matrix
  myDTMatrix &lt;- docTermMatrix(tokenized.obj)

  # combine with filterByClass() to, e.g.,  exclude all punctuation
  myDTMatrix &lt;- docTermMatrix(filterByClass(tokenized.obj))

  # instead of absolute frequencies, get the tf-idf values
  myDTMatrix &lt;- docTermMatrix(
    filterByClass(tokenized.obj),
    tfidf=TRUE
  )
} else {}
</code></pre>

<hr>
<h2 id='DRP'>Readability: Degrees of Reading Power (DRP)</h2><span id='topic+DRP'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DRP(txt.file, word.list, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DRP_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="DRP_+3A_word.list">word.list</code></td>
<td>
<p>A vector or matrix (with exactly one column) which defines familiar words. For valid results
the long Dale-Chall list with 3000 words should be used.</p>
</td></tr>
<tr><td><code id="DRP_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Degrees of Reading Power,
using the Bormuth Mean Cloze Score. In contrast to
<code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  DRP(tagged.text, word.list=new.dale.chall.wl)

## End(Not run)
</code></pre>

<hr>
<h2 id='ELF'>Readability: Fang's Easy Listening Formula (ELF)</h2><span id='topic+ELF'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ELF(txt.file, hyphen = NULL, parameters = c(syll = 1), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ELF_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="ELF_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="ELF_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="ELF_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates Fang's Easy Listening Formula (ELF). In contrast to
<code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>DuBay, W.H. (2004). <em>The Principles of Readability</em>. Costa Mesa: Impact Information.
WWW: <a href="http://www.impact-information.com/impactinfo/readability02.pdf">http://www.impact-information.com/impactinfo/readability02.pdf</a>; 22.03.2011.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ELF(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='farr.jenkins.paterson'>Readability: Farr-Jenkins-Paterson Index</h2><span id='topic+farr.jenkins.paterson'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>farr.jenkins.paterson(
  txt.file,
  hyphen = NULL,
  parameters = c(const = -31.517, asl = 1.015, monsy = 1.599),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="farr.jenkins.paterson_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="farr.jenkins.paterson_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="farr.jenkins.paterson_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index, or <code>"PSK"</code>.</p>
</td></tr>
<tr><td><code id="farr.jenkins.paterson_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Farr-Jenkins-Paterson index, a simplified version of Flesch Reading Ease.
In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>If <code>parameters="PSK"</code>, the revised parameters by Powers-Sumner-Kearl (1958) are used.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>Farr, J.N., Jenkins, J.J. &amp; Paterson,
D.G. (1951). Simplification of Flesch Reading Ease formula. <em>Journal of Applied Psychology</em>, 35(5), 333&ndash;337.
</p>
<p>Powers, R.D, Sumner, W.A, &amp; Kearl,
B.E. (1958). A recalculation of four adult readability formulas, <em>Journal of Educational Psychology</em>, 49(2), 99&ndash;105.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+flesch">flesch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
farr.jenkins.paterson(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='filterByClass'>Remove word classes</h2><span id='topic+filterByClass'></span><span id='topic+filterByClass+2CkRp.text-method'></span>

<h3>Description</h3>

<p>This method strips off defined word classes of tagged text objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filterByClass(txt, ...)

## S4 method for signature 'kRp.text'
filterByClass(
  txt,
  corp.rm.class = "nonpunct",
  corp.rm.tag = c(),
  as.vector = FALSE,
  update.desc = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filterByClass_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
<tr><td><code id="filterByClass_+3A_...">...</code></td>
<td>
<p>Additional options, currently unused.</p>
</td></tr>
<tr><td><code id="filterByClass_+3A_corp.rm.class">corp.rm.class</code></td>
<td>
<p>A character vector with word classes which should be removed. The default value
<code>"nonpunct"</code> has special meaning and will cause the result of
<code>kRp.POS.tags(lang, tags=c("punct","sentc"), list.classes=TRUE)</code> to be used.
Another valid value is &quot;stopword&quot; to remove all detected stopwords.</p>
</td></tr>
<tr><td><code id="filterByClass_+3A_corp.rm.tag">corp.rm.tag</code></td>
<td>
<p>A character vector with valid POS tags which should be removed.</p>
</td></tr>
<tr><td><code id="filterByClass_+3A_as.vector">as.vector</code></td>
<td>
<p>Logical. If <code>TRUE</code>,
results will be returned as a character vector containing only the text parts
which survived the filtering.</p>
</td></tr>
<tr><td><code id="filterByClass_+3A_update.desc">update.desc</code></td>
<td>
<p>Logical. If <code>TRUE</code>,
the <code>desc</code> slot of the tagged object will be fully recalculated
using the filtered text. If <code>FALSE</code>,
the <code>desc</code> slot will be copied from the original object.
Finally, if <code>NULL</code>, the <code>desc</code> slot remains empty.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of the input class. If <code>as.vector=TRUE</code>, returns only a character vector.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  filterByClass(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='flesch'>Readability: Flesch Readability Ease</h2><span id='topic+flesch'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flesch(
  txt.file,
  hyphen = NULL,
  parameters = c(const = 206.835, asl = 1.015, asw = 84.6),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flesch_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="flesch_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="flesch_+3A_parameters">parameters</code></td>
<td>
<p>Either a numeric vector with named magic numbers,
defining the relevant parameters for the index, or
a valid character string naming a preset for implemented languages (<code>"de"</code>,
<code>"es"</code>, <code>"es-s"</code>,
<code>"nl"</code>, <code>"nl-b"</code>, <code>"fr"</code>).</p>
</td></tr>
<tr><td><code id="flesch_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Flesch Readability Ease index. In contrast to
<code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the Flesch RE value.
</p>
<p>Certain internationalisations of the parameters are also implemented. They can be used by setting
<code>parameters</code> to <code>"es"</code> (Fernandez-Huerta),  <code>"es-s"</code> (Szigriszt),
<code>"nl"</code> (Douma),
<code>"nl-b"</code> (Brouwer), <code>"de"</code> (Amstad) or <code>"fr"</code> (Kandel-Moles).
If <code>parameters="PSK"</code>, the revised parameters by Powers-Sumner-Kearl (1958) are used
to calculate a grade level.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+flesch.kincaid">flesch.kincaid</a></code> for grade levels,
<code><a href="#topic+farr.jenkins.paterson">farr.jenkins.paterson</a></code> for a simplified Flesch formula.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
flesch(german.tagged.text, parameters="de")

## End(Not run)
</code></pre>

<hr>
<h2 id='flesch.kincaid'>Readability: Flesch-Kincaid Grade Level</h2><span id='topic+flesch.kincaid'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flesch.kincaid(
  txt.file,
  hyphen = NULL,
  parameters = c(asl = 0.39, asw = 11.8, const = 15.59),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flesch.kincaid_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="flesch.kincaid_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="flesch.kincaid_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="flesch.kincaid_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Flesch-Kincaid grade level. In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
flesch.kincaid(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='FOG'>Readability: Gunning FOG Index</h2><span id='topic+FOG'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FOG(
  txt.file,
  hyphen = NULL,
  parameters = list(syll = 3, const = 0.4, suffix = c("es", "ed", "ing")),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FOG_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="FOG_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="FOG_+3A_parameters">parameters</code></td>
<td>
<p>A list with named magic numbers and a vector with verb suffixes,
defining the relevant parameters for the index,
or one of <code>"PSK"</code> or <code>"NRI"</code>.</p>
</td></tr>
<tr><td><code id="FOG_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Gunning FOG index. In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>If <code>parameters="PSK"</code>, the revised parameters by Powers-Sumner-Kearl (1958) are used,
and
if <code>parameters="NRI"</code>, the simplified parameters from the Navy Readability Indexes,
respectively.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>DuBay, W.H. (2004). <em>The Principles of Readability</em>. Costa Mesa: Impact Information.
WWW: <a href="http://www.impact-information.com/impactinfo/readability02.pdf">http://www.impact-information.com/impactinfo/readability02.pdf</a>; 22.03.2011.
</p>
<p>Powers, R.D, Sumner, W.A, &amp; Kearl,
B.E. (1958). A recalculation of four adult readability formulas,
<em>Journal of Educational Psychology</em>, 49(2), 99&ndash;105.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
FOG(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='FORCAST'>Readability: FORCAST Index</h2><span id='topic+FORCAST'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FORCAST(
  txt.file,
  hyphen = NULL,
  parameters = c(syll = 1, mult = 0.1, const = 20),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FORCAST_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="FORCAST_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="FORCAST_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index, or <code>"RGL"</code>.</p>
</td></tr>
<tr><td><code id="FORCAST_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the FORCAST index (both grade level and reading age). In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>If <code>parameters="RGL"</code>, the parameters for the precise Reading Grade Level are used.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>Klare, G.R. (1975). Assessing readability. <em>Reading Research Quarterly</em>, 10(1),
62&ndash;102.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
FORCAST(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='freq.analysis'>Analyze word frequencies</h2><span id='topic+freq.analysis'></span><span id='topic+freq.analysis+2CkRp.text-method'></span>

<h3>Description</h3>

<p>The function <code>freq.analysis</code> analyzes texts regarding frequencies of tokens,
word classes etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freq.analysis(txt.file, ...)

## S4 method for signature 'kRp.text'
freq.analysis(
  txt.file,
  corp.freq = NULL,
  desc.stat = TRUE,
  corp.rm.class = "nonpunct",
  corp.rm.tag = c()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="freq.analysis_+3A_txt.file">txt.file</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
<tr><td><code id="freq.analysis_+3A_...">...</code></td>
<td>
<p>Additional options for the generic.</p>
</td></tr>
<tr><td><code id="freq.analysis_+3A_corp.freq">corp.freq</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>.</p>
</td></tr>
<tr><td><code id="freq.analysis_+3A_desc.stat">desc.stat</code></td>
<td>
<p>Logical,
whether an updated descriptive statistical analysis should be conducted.</p>
</td></tr>
<tr><td><code id="freq.analysis_+3A_corp.rm.class">corp.rm.class</code></td>
<td>
<p>A character vector with word classes which should be ignored for frequency analysis. The default value
<code>"nonpunct"</code> has special meaning and will cause the result of
<code>kRp.POS.tags(lang, tags=c("punct","sentc"), list.classes=TRUE)</code> to be used.</p>
</td></tr>
<tr><td><code id="freq.analysis_+3A_corp.rm.tag">corp.rm.tag</code></td>
<td>
<p>A character vector with POS tags which should be ignored for frequency analysis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It adds new columns with frequency information to the <code>tokens</code> data frame of the input data,
describing how often the particular token is used in the additionally provided corpus frequency object.
</p>
<p>To get the results, you can use <code>taggedText</code> to get the <code>tokens</code> slot,
<code>describe</code> to get
the raw descriptive statistics (only updated if <code>desc.stat=TRUE</code>),
and <code>corpusFreq</code> to get
the data from the added <code>freq</code> feature.
</p>
<p>If <code>corp.freq</code> provides appropriate idf values for the types in <code>txt.file</code>, the
term frequency&ndash;inverse document frequency statistic (tf-idf) will also be computed.
Missing idf values will result in <code>NA</code>.
</p>


<h3>Value</h3>

<p>An updated object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> with the added feature <code>freq</code>,
which is a list with information on the word frequencies of the analyzed text.
Use <code><a href="#topic+corpusFreq">corpusFreq</a></code> to get that slot.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get.kRp.env">get.kRp.env</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  # call freq.analysis() on a tokenized text
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  # the token slot before frequency analysis
  head(taggedText(tokenized.obj))

  # instead of data from a larger corpus, we'll
  # use the token frequencies of the text itself
  tokenized.obj &lt;- freq.analysis(
    tokenized.obj,
    corp.freq=read.corp.custom(tokenized.obj)
  )
  # compare the columns after the anylsis
  head(taggedText(tokenized.obj))

  # the object now has further statistics in a
  # new feature slot called freq
  hasFeature(tokenized.obj)
  corpusFreq(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='fucks'>Readability: Fucks' Stilcharakteristik</h2><span id='topic+fucks'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fucks(txt.file, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fucks_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="fucks_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates Fucks' Stilcharakteristik (&quot;characteristics of style&quot;; Fucks, 1955,
as cited in Briest, 1974).
In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>Briest, W. (1974). Kann man Verständlichkeit messen? <em>Zeitschrift für Phonetik,
Sprachwissenschaft und Kommunikationsforschung</em>, 27, 543&ndash;563.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  fucks(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='get.kRp.env'>Get koRpus session settings</h2><span id='topic+get.kRp.env'></span>

<h3>Description</h3>

<p>The function <code>get.kRp.env</code> returns information on your session environment regarding the koRpus package,
e.g.
where your local TreeTagger installation resides, if it was set before using
<code><a href="#topic+set.kRp.env">set.kRp.env</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get.kRp.env(..., errorIfUnset = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get.kRp.env_+3A_...">...</code></td>
<td>
<p>Named parameters to get from the koRpus environment. Valid arguments are:
</p>

<dl>
<dt>TT.cmd</dt><dd><p> Logical, whether the set tagger command should be returned.</p>
</dd>
<dt>lang</dt><dd><p> Logical, whether the set language should be returned.</p>
</dd>
<dt>TT.options</dt><dd><p> Logical,
whether the set TT.options for <code>treetag</code> should be returned.</p>
</dd>
<dt>hyph.cache.file</dt><dd><p> Logical,
whether the set hyphenation cache file for <code>hyphen</code> should be returned.</p>
</dd>
<dt>add.desc</dt><dd><p> Logical,
whether tag descriptions should be added directly to tagged text objects.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="get.kRp.env_+3A_errorifunset">errorIfUnset</code></td>
<td>
<p>Logical, if <code>TRUE</code> and the desired property is not set at all,
the function will fail with an error message.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the most part,
<code>get.kRp.env</code> is a convenient wrapper for <code><a href="base.html#topic+getOption">getOption</a></code>.
</p>


<h3>Value</h3>

<p>A character string or list, possibly including:
</p>
<table>
<tr><td><code>TT.cmd</code></td>
<td>
<p>Path information for the TreeTagger command</p>
</td></tr>
<tr><td><code>lang</code></td>
<td>
<p>The specified language</p>
</td></tr>
<tr><td><code>TT.options</code></td>
<td>
<p>A list with options for <code>treetag</code></p>
</td></tr>
<tr><td><code>hyph.cache.file</code></td>
<td>
<p>The specified hyphenation cache file for <code>hyphen</code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+set.kRp.env">set.kRp.env</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.kRp.env(lang="en")
get.kRp.env(lang=TRUE)
</code></pre>

<hr>
<h2 id='guess.lang'>Guess language a text is written in</h2><span id='topic+guess.lang'></span>

<h3>Description</h3>

<p>This function tries to guess the language a text is written in.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>guess.lang(
  txt.file,
  udhr.path,
  comp.length = 300,
  keep.udhr = FALSE,
  quiet = TRUE,
  in.mem = TRUE,
  format = "file"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="guess.lang_+3A_txt.file">txt.file</code></td>
<td>
<p>A character vector pointing to the file with the text to be analyzed.</p>
</td></tr>
<tr><td><code id="guess.lang_+3A_udhr.path">udhr.path</code></td>
<td>
<p>A character string,
either pointing to the directory where you unzipped the translations of the
Universal Declaration of Human Rights, or to the ZIP file containing them.</p>
</td></tr>
<tr><td><code id="guess.lang_+3A_comp.length">comp.length</code></td>
<td>
<p>Numeric value,
giving the number of characters to be used of <code>txt</code> to estimate the language.</p>
</td></tr>
<tr><td><code id="guess.lang_+3A_keep.udhr">keep.udhr</code></td>
<td>
<p>Logical,
whether all the UDHR translations should be kept in the resulting object.</p>
</td></tr>
<tr><td><code id="guess.lang_+3A_quiet">quiet</code></td>
<td>
<p>Logical. If <code>FALSE</code>, short status messages will be shown.</p>
</td></tr>
<tr><td><code id="guess.lang_+3A_in.mem">in.mem</code></td>
<td>
<p>Logical. If <code>TRUE</code>,
the gzip compression will remain in memory (using <code>memCompress</code>), which
is probably the faster method. Otherwise temporary files are created and automatically removed on exit.</p>
</td></tr>
<tr><td><code id="guess.lang_+3A_format">format</code></td>
<td>
<p>Either &quot;file&quot; or &quot;obj&quot;. If the latter,
<code>txt.file</code> is not interpreted as a file path but the text to analyze itself.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To accomplish the task, the method described by Benedetto,
Caglioti &amp; Loreto (2002) is used, utilizing both
gzip compression and tranlations of the Universal Declaration of Human Rights[1]. The latter holds the world
record for being translated into the most different languages, and is publicly available.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.lang-class">kRp.lang</a></code>.
</p>


<h3>Note</h3>

<p>For this implementation the documents provided by the &quot;UDHR in Unicode&quot; project[2] have been used.
Their translations are <em>not part of this package</em> and must be downloaded seperately to use <code>guess.lang</code>!
You need the ZIP archive containing <em>all the plain text files</em> from <a href="https://unicode.org/udhr/downloads.html">https://unicode.org/udhr/downloads.html</a>.
</p>


<h3>References</h3>

<p>Benedetto, D., Caglioti, E. &amp; Loreto,
V. (2002). Language trees and zipping. <em>Physical Review Letters</em>, 88(4), 048702.
</p>
<p>[1] <a href="https://www.ohchr.org/EN/UDHR/Pages/UDHRIndex.aspx">https://www.ohchr.org/EN/UDHR/Pages/UDHRIndex.aspx</a>
</p>
<p>[2] <a href="https://unicode.org/udhr/">https://unicode.org/udhr/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # using the still zipped bulk file
  guess.lang(
    file.path("~","data","some.txt"),
    udhr.path=file.path("~","data","udhr_txt.zip")
  )
  # using the unzipped UDHR archive
  guess.lang(
    file.path("~","data","some.txt"),
    udhr.path=file.path("~","data","udhr_txt")
  )

## End(Not run)
</code></pre>

<hr>
<h2 id='gutierrez'>Readability: Gutiérrez <em>Fórmula de comprensibilidad</em></h2><span id='topic+gutierrez'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gutierrez(txt.file, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gutierrez_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="gutierrez_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates Gutiérrez de Polini's <em>Fórmula de comprensibilidad</em> (Gutiérrez, 1972,
as cited in Fernández, 2016).
In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>Fernández, A. M. (2016,
November 30). <em>Fórmula de comprensibilidad de Gutiérrez de Polini</em>.
<a href="https://legible.es/blog/comprensibilidad-gutierrez-de-polini/">https://legible.es/blog/comprensibilidad-gutierrez-de-polini/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  gutierrez(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='harris.jacobson'>Readability: Harris-Jacobson indices</h2><span id='topic+harris.jacobson'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>harris.jacobson(
  txt.file,
  word.list,
  parameters = c(char = 6),
  hj1 = c(dword = 0.094, asl = 0.168, const = 0.502),
  hj2 = c(dword = 0.14, asl = 0.153, const = 0.56),
  hj3 = c(asl = 0.158, lword = 0.055, const = 0.355),
  hj4 = c(dword = 0.07, asl = 0.125, lword = 0.037, const = 0.497),
  hj5 = c(dword = 0.118, asl = 0.134, lword = 0.032, const = 0.424),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="harris.jacobson_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="harris.jacobson_+3A_word.list">word.list</code></td>
<td>
<p>A vector or matrix (with exactly one column) which defines familiar words. For valid results
the short Harris-Jacobson word list for grades 1 and 2 (english) should be used.</p>
</td></tr>
<tr><td><code id="harris.jacobson_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for all formulas of the index.</p>
</td></tr>
<tr><td><code id="harris.jacobson_+3A_hj1">hj1</code></td>
<td>
<p>A numeric vector with named magic numbers for the first of the formulas.</p>
</td></tr>
<tr><td><code id="harris.jacobson_+3A_hj2">hj2</code></td>
<td>
<p>A numeric vector with named magic numbers for the second of the formulas.</p>
</td></tr>
<tr><td><code id="harris.jacobson_+3A_hj3">hj3</code></td>
<td>
<p>A numeric vector with named magic numbers for the third of the formulas.</p>
</td></tr>
<tr><td><code id="harris.jacobson_+3A_hj4">hj4</code></td>
<td>
<p>A numeric vector with named magic numbers for the fourth of the formulas.</p>
</td></tr>
<tr><td><code id="harris.jacobson_+3A_hj5">hj5</code></td>
<td>
<p>A numeric vector with named magic numbers for the fifth of the formulas.</p>
</td></tr>
<tr><td><code id="harris.jacobson_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the revised Harris-Jacobson readability formulas (1 to 5),
as described in their paper for the
18th Annual Meeting of the College Reading Association (Harris &amp; Jacobson,
1974). In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index values.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>Harris, A.J. &amp; Jacobson,
M.D. (1974). Revised Harris-Jacobson readability formulas. In <em>18th Annual Meeting of the College Reading Association</em>, Bethesda.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
harris.jacobson(tagged.text, word.list=harris.jacobson.wl)

## End(Not run)
</code></pre>

<hr>
<h2 id='HDD'>Lexical diversity: HD-D (vocd-d)</h2><span id='topic+HDD'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HDD(txt, rand.sample = 42, char = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HDD_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="HDD_+3A_rand.sample">rand.sample</code></td>
<td>
<p>An integer value,
how many tokens should be assumed to be drawn for calculating HD-D.</p>
</td></tr>
<tr><td><code id="HDD_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="HDD_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates HD-D, an idealized version of vocd-d (see McCarthy &amp; Jarvis,
2007). In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the HD-D value,
and characteristics are
off by default.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>References</h3>

<p>McCarthy, P.M. &amp; Jarvis,
S. (2007). vocd: A theoretical and empirical evaluation. <em>Language Testing</em>, 24(4), 459&ndash;488.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
HDD(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='hyphen+2CkRp.text-method'>Automatic hyphenation</h2><span id='topic+hyphen+2CkRp.text-method'></span><span id='topic+hyphen'></span><span id='topic+hyphen_df+2CkRp.text-method'></span><span id='topic+hyphen_c+2CkRp.text-method'></span>

<h3>Description</h3>

<p>These methods implement word hyphenation, based on Liang's algorithm.
For details, please refer to the documentation for the generic
<code><a href="sylly.html#topic+hyphen">hyphen</a></code> method in the <code>sylly</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'kRp.text'
hyphen(
  words,
  hyph.pattern = NULL,
  min.length = 4,
  rm.hyph = TRUE,
  corp.rm.class = "nonpunct",
  corp.rm.tag = c(),
  quiet = FALSE,
  cache = TRUE,
  as = "kRp.hyphen",
  as.feature = FALSE
)

## S4 method for signature 'kRp.text'
hyphen_df(
  words,
  hyph.pattern = NULL,
  min.length = 4,
  rm.hyph = TRUE,
  quiet = FALSE,
  cache = TRUE
)

## S4 method for signature 'kRp.text'
hyphen_c(
  words,
  hyph.pattern = NULL,
  min.length = 4,
  rm.hyph = TRUE,
  quiet = FALSE,
  cache = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hyphen+2B2CkRp.text-method_+3A_words">words</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
or a character vector with words to be hyphenated.</p>
</td></tr>
<tr><td><code id="hyphen+2B2CkRp.text-method_+3A_hyph.pattern">hyph.pattern</code></td>
<td>
<p>Either an object of class <code><a href="sylly.html#topic+kRp.hyph.pat-class">kRp.hyph.pat</a></code>,
or
a valid character string naming the language of the patterns to be used. See details.</p>
</td></tr>
<tr><td><code id="hyphen+2B2CkRp.text-method_+3A_min.length">min.length</code></td>
<td>
<p>Integer,
number of letters a word must have for considering a hyphenation. <code>hyphen</code> will
not split words after the first or before the last letter,
so values smaller than 4 are not useful.</p>
</td></tr>
<tr><td><code id="hyphen+2B2CkRp.text-method_+3A_rm.hyph">rm.hyph</code></td>
<td>
<p>Logical,
whether appearing hyphens in words should be removed before pattern matching.</p>
</td></tr>
<tr><td><code id="hyphen+2B2CkRp.text-method_+3A_corp.rm.class">corp.rm.class</code></td>
<td>
<p>A character vector with word classes which should be ignored. The default value
<code>"nonpunct"</code> has special meaning and will cause the result of
<code>kRp.POS.tags(lang, tags=c("punct","sentc"),
      list.classes=TRUE)</code> to be used. Relevant only if <code>words</code>
is a valid koRpus object.</p>
</td></tr>
<tr><td><code id="hyphen+2B2CkRp.text-method_+3A_corp.rm.tag">corp.rm.tag</code></td>
<td>
<p>A character vector with POS tags which should be ignored. Relevant only if <code>words</code>
is a valid koRpus object.</p>
</td></tr>
<tr><td><code id="hyphen+2B2CkRp.text-method_+3A_quiet">quiet</code></td>
<td>
<p>Logical. If <code>FALSE</code>, short status messages will be shown.</p>
</td></tr>
<tr><td><code id="hyphen+2B2CkRp.text-method_+3A_cache">cache</code></td>
<td>
<p>Logical. <code>hyphen()</code> can cache results to speed up the process. If this option is set to <code>TRUE</code>,
the
current cache will be queried and new tokens also be added. Caches are language-specific and reside in an environment,
i.e., they are cleaned at the end of a session. If you want to save these for later use,
see the option <code>hyph.cache.file</code>
in <code><a href="#topic+set.kRp.env">set.kRp.env</a></code>.</p>
</td></tr>
<tr><td><code id="hyphen+2B2CkRp.text-method_+3A_as">as</code></td>
<td>
<p>A character string defining the class of the object to be returned. Defaults to <code>"kRp.hyphen"</code>,
but can also be
set to <code>"data.frame"</code> or <code>"numeric"</code>,
returning only the central <code>data.frame</code> or the numeric vector of counted syllables,
respectively. For the latter two options,
you can alternatively use the shortcut methods <code>hyphen_df</code> or  <code>hyphen_c</code>.
Ignored if <code>as.feature=TRUE</code>.</p>
</td></tr>
<tr><td><code id="hyphen+2B2CkRp.text-method_+3A_as.feature">as.feature</code></td>
<td>
<p>Logical,
whether the output should be just the analysis results or the input object with
the results added as a feature. Use <code><a href="#topic+corpusHyphen">corpusHyphen</a></code> to get the results from such an aggregated object.
If set to <code>TRUE</code>, <code>as="kRp.hyphen"</code> is automatically set,
overwriting other setting of <code>as</code> with a warning.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="sylly.html#topic+kRp.hyphen-class">kRp.hyphen</a></code>,
<code>data.frame</code> or a numeric vector,
depending on the values of the <code>as</code> and <code>as.feature</code> arguments.
</p>


<h3>References</h3>

<p>Liang, F.M. (1983). <em>Word Hy-phen-a-tion by Com-put-er</em>.
Dissertation, Stanford University, Dept. of Computer Science.
</p>
<p>[1] <a href="http://tug.ctan.org/tex-archive/language/hyph-utf8/tex/generic/hyph-utf8/patterns/">http://tug.ctan.org/tex-archive/language/hyph-utf8/tex/generic/hyph-utf8/patterns/</a>
</p>
<p>[2] <a href="http://www.ctan.org/tex-archive/macros/latex/base/lppl.txt">http://www.ctan.org/tex-archive/macros/latex/base/lppl.txt</a>
</p>


<h3>See Also</h3>

<p><code><a href="sylly.html#topic+read.hyph.pat">read.hyph.pat</a></code>,
<code><a href="sylly.html#topic+manage.hyph.pat">manage.hyph.pat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  # call hyphen on a given english word
  # "quiet=TRUE" suppresses the progress bar
  hyphen(
    "interference",
    hyph.pattern="en",
    quiet=TRUE
  )

  # call hyphen() on a tokenized text
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  # language definition is defined in the object
  # if you call hyphen() without arguments,
  # you will get its results directly
  hyphen(tokenized.obj)

  # alternatively, you can also store those results as a
  # feature in the object itself
  tokenized.obj &lt;- hyphen(
    tokenized.obj,
    as.feature=TRUE
  )
  # results are now part of the object
  hasFeature(tokenized.obj)
  corpusHyphen(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='install.koRpus.lang'>Install language support packages</h2><span id='topic+install.koRpus.lang'></span>

<h3>Description</h3>

<p>This is a wrapper for <code><a href="utils.html#topic+install.packages">install.packages</a></code>,
making it more
convenient to install additional language support packages for koRpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install.koRpus.lang(
  lang,
  repos = "https://undocumeantit.github.io/repos/l10n/",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="install.koRpus.lang_+3A_lang">lang</code></td>
<td>
<p>Character vector,
one or more valid language identifiers (like <code>en</code> for English or <code>de</code>
for German).</p>
</td></tr>
<tr><td><code id="install.koRpus.lang_+3A_repos">repos</code></td>
<td>
<p>The URL to additional repositories to query. You should probably leave this to the
default, but if you would like to use a third party repository, you're free to do so. The
value is temporarily appended to the repos currently returned by <code>getOption("repos")</code>.</p>
</td></tr>
<tr><td><code id="install.koRpus.lang_+3A_...">...</code></td>
<td>
<p>Additional options for <code>install.packages</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a list of currently available language packages see <code><a href="#topic+available.koRpus.lang">available.koRpus.lang</a></code>.
See <code><a href="#topic+set.lang.support">set.lang.support</a></code> for more details on koRpus' language support in general.
</p>


<h3>Value</h3>

<p>Does not return any useful objects,
just calls <code><a href="utils.html#topic+install.packages">install.packages</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+install.packages">install.packages</a></code>,
<code><a href="#topic+available.koRpus.lang">available.koRpus.lang</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# install support for German
install.koRpus.lang("de")
# load the package
library("koRpus.lang.de")

## End(Not run)
</code></pre>

<hr>
<h2 id='jumbleWords'>Produce jumbled words</h2><span id='topic+jumbleWords'></span><span id='topic+jumbleWords+2CkRp.text-method'></span><span id='topic+jumbleWords+2Ccharacter-method'></span>

<h3>Description</h3>

<p>This method either takes a character vector or objects inheriting class <code>kRp.text</code>
(i.e., text tokenized by <code>koRpus</code>),
and jumbles the words. This usually means that the
first and last letter of each word is left intact,
while all characters inbetween are being
randomized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jumbleWords(words, ...)

## S4 method for signature 'kRp.text'
jumbleWords(words, min.length = 3, intact = c(start = 1, end = 1))

## S4 method for signature 'character'
jumbleWords(words, min.length = 3, intact = c(start = 1, end = 1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jumbleWords_+3A_words">words</code></td>
<td>
<p>Either a character vector or an object inheriting from class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
<tr><td><code id="jumbleWords_+3A_...">...</code></td>
<td>
<p>Additional options, currently unused.</p>
</td></tr>
<tr><td><code id="jumbleWords_+3A_min.length">min.length</code></td>
<td>
<p>An integer value,
defining the minimum word length. Words with less characters
will not be changed. Grapheme clusters are counted as one.</p>
</td></tr>
<tr><td><code id="jumbleWords_+3A_intact">intact</code></td>
<td>
<p>A named vector with the two integer values named <code>start</code> and <code>stop</code>.
These define how many characters of each relevant words will be left unchanged at its start
and its end, respectively.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Depending on the class of <code>words</code>, either a character vector or an object of class
<code><a href="#topic+kRp.text-class">kRp.text</a></code> with the added feature <code>diff</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  tokenized.obj &lt;- jumbleWords(tokenized.obj)
  pasteText(tokenized.obj)

  # diff stats are now part of the object
  hasFeature(tokenized.obj)
  diffText(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='K.ld'>Lexical diversity: Yule's K</h2><span id='topic+K.ld'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>K.ld(txt, char = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="K.ld_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="K.ld_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="K.ld_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates Yule's K. In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the K value,
and characteristics are
off by default.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
K.ld(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='koRpus-deprecated'>Deprecated object classes</h2><span id='topic+koRpus-deprecated'></span><span id='topic+kRp.filter.wclass'></span><span id='topic+kRp_tagged'></span><span id='topic+kRp.tagged-class'></span><span id='topic+kRp_txt_freq'></span><span id='topic+kRp.txt.freq-class'></span><span id='topic+kRp_txt_trans'></span><span id='topic+kRp.txt.trans-class'></span><span id='topic+kRp_analysis'></span><span id='topic+kRp.analysis-class'></span><span id='topic+kRp.text.paste'></span><span id='topic+read.tagged'></span><span id='topic+kRp.text.transform'></span>

<h3>Description</h3>

<p>These classes are no longer used by the <code>koRpus</code> package and will be removed in a later version.
They are kept here for the time being so you can still load old objects and convert them into new objects using the
<code>fixObject</code> method.
</p>
<p>These functions will be removed soon and should no longer ne used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kRp.filter.wclass(...)

kRp.text.paste(...)

read.tagged(...)

kRp.text.transform(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="koRpus-deprecated_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to the replacement of the function</p>
</td></tr>
</table>


<h3>Slots</h3>


<dl>
<dt><code>lang</code></dt><dd><p>A character string,
naming the language that is assumed for the tokenized text in this object.</p>
</dd>
<dt><code>desc</code></dt><dd><p>Descriptive statistics of the tagged text.</p>
</dd>
<dt><code>TT.res</code></dt><dd><p>Results of the called tokenizer and POS tagger. The data.frame usually has eleven columns:
</p>

<dl>
<dt><code>doc_id</code>:</dt><dd><p>Factor, optional document identifier.</p>
</dd>
<dt><code>token</code>:</dt><dd><p>Character, the tokenized text.</p>
</dd>
<dt><code>tag</code>:</dt><dd><p>Factor, POS tags for each token.</p>
</dd>
<dt><code>lemma</code>:</dt><dd><p>Character, lemma for each token.</p>
</dd>
<dt><code>lttr</code>:</dt><dd><p>Integer, number of letters.</p>
</dd>
<dt><code>wclass</code>:</dt><dd><p>Factor, word class.</p>
</dd>
<dt><code>desc</code>:</dt><dd><p>Factor, a short description of the POS tag.</p>
</dd>
<dt><code>stop</code>:</dt><dd><p>Logical, <code>TRUE</code> if token is a stopword.</p>
</dd>
<dt><code>stem</code>:</dt><dd><p>Character, stemmed token.</p>
</dd>
<dt><code>idx</code>:</dt><dd><p>Integer, index number of token in this document.</p>
</dd>
<dt><code>sntc</code>:</dt><dd><p>Integer, number of sentence in this document.</p>
</dd>
</dl>

<p>This data.frame structure adheres to the &quot;Text Interchange Formats&quot; guidelines set out by rOpenSci[1].</p>
</dd>
<dt><code>freq.analysis</code></dt><dd><p>A list with information on the word frequencies of the analyzed text.</p>
</dd>
<dt><code>diff</code></dt><dd><p>A list with mostly atomic vectors,
describing the amount of diffences between both text variants (percentage):
</p>

<dl>
<dt><code>all.tokens</code>:</dt><dd><p>Percentage of all tokens, including punctuation,
that were altered.</p>
</dd>
<dt><code>words</code>:</dt><dd><p>Percentage of altered words only.</p>
</dd>
<dt><code>all.chars</code>:</dt><dd><p>Percentage of all characters, including punctuation,
that were altered.</p>
</dd>
<dt><code>letters</code>:</dt><dd><p>Percentage of altered letters in words only.</p>
</dd>
<dt><code>transfmt</code>:</dt><dd><p>Character vector documenting the transformation(s) done to the tokens.</p>
</dd>
<dt><code>transfmt.equal</code>:</dt><dd><p>Data frame documenting which token was changed in which transformational step. Only available if more than one transformation was done.</p>
</dd>
<dt><code>transfmt.normalize</code>:</dt><dd><p>A list documenting steps of normalization that were done to the object,
one element per transformation.
Each entry holds the name of the method, the query parameters,
and the effective replacement value.</p>
</dd>
</dl>
</dd>
<dt><code>lex.div</code></dt><dd><p>Information on lexical diversity</p>
</dd>
</dl>


<h3>S4 Class <code>kRp.tagged</code></h3>

<p>This was used for objects returned by <code><a href="#topic+treetag">treetag</a></code> or <code><a href="#topic+tokenize">tokenize</a></code>.
It was replaced by <code><a href="#topic+kRp.text-class">kRp.text</a></code>.
</p>


<h3>S4 Class <code>kRp.txt.freq</code></h3>

<p>This was used for objects returned by <code><a href="#topic+freq.analysis">freq.analysis</a></code>.
It was replaced by <code><a href="#topic+kRp.text-class">kRp.text</a></code>.
</p>


<h3>S4 Class <code>kRp.txt.trans</code></h3>

<p>This was used for objects returned by <code><a href="#topic+textTransform">textTransform</a></code>,
<code><a href="#topic+clozeDelete">clozeDelete</a></code>,
<code><a href="#topic+cTest">cTest</a></code>, and <code><a href="#topic+jumbleWords">jumbleWords</a></code>.
It was replaced by <code><a href="#topic+kRp.text-class">kRp.text</a></code>.
</p>


<h3>S4 Class <code>kRp.analysis</code></h3>

<p>This was used for objects returned by <code>kRp.text.analysis</code>.
The function is also deprecated,
functionality can be replicated by combining <code>treetag</code>,<code>freq.analysis</code> and <code>lex.div</code>.
</p>


<h3>References</h3>

<p>[1] Text Interchange Formats (<a href="https://github.com/ropensci/tif">https://github.com/ropensci/tif</a>)
</p>

<hr>
<h2 id='koRpus-package'>Text Analysis with Emphasis on POS Tagging, Readability, and Lexical Diversity</h2><span id='topic+koRpus'></span><span id='topic+koRpus-package'></span>

<h3>Description</h3>

<p>A set of tools to analyze texts. Includes, amongst others, functions for
          automatic language detection, hyphenation, several indices of lexical diversity
          (e.g., type token ratio, HD-D/vocd-D, MTLD) and readability (e.g., Flesch,
          SMOG, LIX, Dale-Chall). Basic import functions for language corpora are also
          provided, to enable frequency analyses (supports Celex and Leipzig Corpora
          Collection file formats) and measures like tf-idf. Note: For full functionality
          a local installation of TreeTagger is recommended. It is also recommended to
          not load this package directly, but by loading one of the available language
          support packages from the 'l10n' repository
          &lt;https://undocumeantit.github.io/repos/l10n/&gt;. 'koRpus' also includes a plugin
          for the R GUI and IDE RKWard, providing graphical dialogs for its basic
          features. The respective R package 'rkward' cannot be installed directly from a
          repository, as it is a part of RKWard. To make full use of this feature, please
          install RKWard from &lt;https://rkward.kde.org&gt; (plugins are detected
          automatically). Due to some restrictions on CRAN, the full package sources are
          only available from the project homepage. To ask for help, report bugs, request
          features, or discuss the development of the package, please subscribe to the
          koRpus-dev mailing list (&lt;https://korpusml.reaktanz.de&gt;).
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> koRpus</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.13-8</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2021-05-17</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 3.0.0),sylly (&gt;= 0.1-6)</td>
</tr>
<tr>
 <td style="text-align: left;">
Enhances: </td><td style="text-align: left;"> rkward</td>
</tr>
<tr>
 <td style="text-align: left;">
Encoding: </td><td style="text-align: left;"> UTF-8</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 3)</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
URL: </td><td style="text-align: left;"> https://reaktanz.de/?c=hacking&amp;s=koRpus</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Meik Michalke [aut, cre], Earl Brown [ctb], Alberto Mirisola [ctb], Alexandre
          Brulet [ctb], Laura Hauser [ctb]
</p>
<p>Maintainer: Meik Michalke &lt;meik.michalke@hhu.de&gt;
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://reaktanz.de/?c=hacking&amp;s=koRpus">https://reaktanz.de/?c=hacking&amp;s=koRpus</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/unDocUMeantIt/koRpus/issues">https://github.com/unDocUMeantIt/koRpus/issues</a>
</p>
</li></ul>


<hr>
<h2 id='kRp.cluster'>Work in (early) progress. Probably don't even look at it. Consider it pure magic that is not to be tempered with.</h2><span id='topic+kRp.cluster'></span>

<h3>Description</h3>

<p>In some future release,
this might evolve into a function to help comparing several texts by features like average
sentece length, word length, lexical diversity,
and so forth. The idea behind it is to conduct a cluster analysis,
to discover which texts out of several are similar to (or very different from) each other. This can be useful,
e.g., if
you need texts for an experiment which are different in content,
but similar regarding syntactic features, like
listed above.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kRp.cluster(txts, lang, TT.path, TT.preset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kRp.cluster_+3A_txts">txts</code></td>
<td>
<p>A character vector with paths to texts to analyze.</p>
</td></tr>
<tr><td><code id="kRp.cluster_+3A_lang">lang</code></td>
<td>
<p>A character string with a valid Language identifier.</p>
</td></tr>
<tr><td><code id="kRp.cluster_+3A_tt.path">TT.path</code></td>
<td>
<p>A character string, path to TreeTagger installation.</p>
</td></tr>
<tr><td><code id="kRp.cluster_+3A_tt.preset">TT.preset</code></td>
<td>
<p>A character string naming the TreeTagger preset to use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is included in this package not really to be used, but to maybe inspire you,
to toy around with the code and help me to
come up with something useful in the end...
</p>

<hr>
<h2 id='kRp.corp.freq+2C-class'>S4 Class kRp.corp.freq</h2><span id='topic+kRp.corp.freq+2C-class'></span><span id='topic+kRp_corp_freq'></span><span id='topic+kRp.corp.freq-class'></span>

<h3>Description</h3>

<p>This class is used for objects that are returned by <code><a href="#topic+read.corp.LCC">read.corp.LCC</a></code> and <code><a href="#topic+read.corp.celex">read.corp.celex</a></code>.
</p>


<h3>Details</h3>

<p>The slot <code>meta</code> simply contains all information from the &quot;meta.txt&quot; of the LCC[1] data and remains
empty for data from a Celex[2] DB.
</p>


<h3>Slots</h3>


<dl>
<dt><code>meta</code></dt><dd><p>Metadata on the corpora (see details).</p>
</dd>
<dt><code>words</code></dt><dd><p>Absolute word frequencies. It has at least the following columns:
</p>

<dl>
<dt><code>num</code>:</dt><dd><p>Some word ID from the DB, integer</p>
</dd>
<dt><code>word</code>:</dt><dd><p>The word itself</p>
</dd>
<dt><code>lemma</code>:</dt><dd><p>The lemma of the word</p>
</dd>
<dt><code>tag</code>:</dt><dd><p>A part-of-speech tag</p>
</dd>
<dt><code>wclass</code>:</dt><dd><p>The word class</p>
</dd>
<dt><code>lttr</code>:</dt><dd><p>The number of characters</p>
</dd>
<dt><code>freq</code>:</dt><dd><p>The frequency of that word in the corpus DB</p>
</dd>
<dt><code>pct</code>:</dt><dd><p>Percentage of appearance in DB</p>
</dd>
<dt><code>pmio</code>:</dt><dd><p>Appearance per million words in DB</p>
</dd>
<dt><code>log10</code>:</dt><dd><p>Base 10 logarithm of word frequency</p>
</dd>
<dt><code>rank.avg</code>:</dt><dd><p>Rank in corpus data, <code><a href="base.html#topic+rank">rank</a></code> ties method &quot;average&quot;</p>
</dd>
<dt><code>rank.min</code>:</dt><dd><p>Rank in corpus data, <code><a href="base.html#topic+rank">rank</a></code> ties method &quot;min&quot;</p>
</dd>
<dt><code>rank.rel.avg</code>:</dt><dd><p>Relative rank, i.e. percentile of <code>"rank.avg"</code></p>
</dd>
<dt><code>rank.rel.min</code>:</dt><dd><p>Relative rank, i.e. percentile of <code>"rank.min"</code></p>
</dd>
<dt><code>inDocs</code>:</dt><dd><p>The absolute number of documents in the corpus containing the word</p>
</dd>
<dt><code>idf</code>:</dt><dd><p>The inverse document frequency</p>
</dd>
</dl>

<p>The slot might have additional columns, depending on the input material.</p>
</dd>
<dt><code>desc</code></dt><dd><p>Descriptive information. It contains six numbers from the <code>meta</code> information,
for convenient accessibility:
</p>

<dl>
<dt><code>tokens</code>:</dt><dd><p>Number of running word forms</p>
</dd>
<dt><code>types</code>:</dt><dd><p>Number of distinct word forms</p>
</dd>
<dt><code>words.p.sntc</code>:</dt><dd><p>Average sentence length in words</p>
</dd>
<dt><code>chars.p.sntc</code>:</dt><dd><p>Average sentence length in characters</p>
</dd>
<dt><code>chars.p.wform</code>:</dt><dd><p>Average word form length</p>
</dd>
<dt><code>chars.p.word</code>:</dt><dd><p>Average running word length</p>
</dd>
</dl>

<p>The slot might have additional columns, depending on the input material.</p>
</dd>
<dt><code>bigrams</code></dt><dd><p>A data.frame listing all tokens that co-occurred next to each other in the corpus:
</p>

<dl>
<dt><code>token1</code>:</dt><dd><p>The first token</p>
</dd>
<dt><code>token2</code>:</dt><dd><p>The second token that appeared right next to the first</p>
</dd>
<dt><code>freq</code>:</dt><dd><p>How often the co-occurrance was present</p>
</dd>
<dt><code>sig</code>:</dt><dd><p>Log-likelihood significance of the co-occurrende</p>
</dd>
</dl>
</dd>
<dt><code>cooccur</code></dt><dd><p>Similar to <code>bigrams</code>,
but listing co-occurrences anywhere in one sentence:
</p>

<dl>
<dt><code>token1</code>:</dt><dd><p>The first token</p>
</dd>
<dt><code>token2</code>:</dt><dd><p>The second token that appeared in the same sentence</p>
</dd>
<dt><code>freq</code>:</dt><dd><p>How often the co-occurrance was present</p>
</dd>
<dt><code>sig</code>:</dt><dd><p>Log-likelihood significance of the co-occurrende</p>
</dd>
</dl>
</dd>
<dt><code>caseSens</code></dt><dd><p>A single logical value,
whether the frequency statistics were calculated case sensitive
or not.</p>
</dd>
</dl>


<h3>Contructor function</h3>

<p>Should you need to manually generate objects of this class (which should rarely be the case),
the contructor function 
<code>kRp_corp_freq(...)</code> can be used instead of
<code>new("kRp.corp.freq", ...)</code>.
</p>


<h3>References</h3>

<p>[1] <a href="https://wortschatz.uni-leipzig.de/en/download/">https://wortschatz.uni-leipzig.de/en/download/</a>
[2] <a href="http://celex.mpi.nl">http://celex.mpi.nl</a>
</p>

<hr>
<h2 id='kRp.lang+2C-class'>S4 Class kRp.lang</h2><span id='topic+kRp.lang+2C-class'></span><span id='topic+kRp_lang'></span><span id='topic+kRp.lang-class'></span>

<h3>Description</h3>

<p>This class is used for objects that are returned by <code><a href="#topic+guess.lang">guess.lang</a></code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>lang</code></dt><dd><p>A character string,
naming the language (by its ISO 639-3 identifier) that was estimated for the analized text in this object.</p>
</dd>
<dt><code>lang.name</code></dt><dd><p>A character string, full name of the estimated language.</p>
</dd>
<dt><code>txt</code></dt><dd><p>A character string containing the analized part of the text.</p>
</dd>
<dt><code>txt.full</code></dt><dd><p>A character string containing the full text.</p>
</dd>
<dt><code>udhr</code></dt><dd><p>A data.frame with full analysis results for each language tried.</p>
</dd>
</dl>


<h3>Contructor function</h3>

<p>Should you need to manually generate objects of this class (which should rarely be the case),
the contructor function 
<code>kRp_lang(...)</code> can be used instead of
<code>new("kRp.lang", ...)</code>.
</p>

<hr>
<h2 id='kRp.POS.tags'>Get elaborated word tag definitions</h2><span id='topic+kRp.POS.tags'></span>

<h3>Description</h3>

<p>This function can be used to get a set of part-of-speech (POS) tags for a given language. These tag sets should conform
with the ones used by TreeTagger.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kRp.POS.tags(
  lang = get.kRp.env(lang = TRUE),
  list.classes = FALSE,
  list.tags = FALSE,
  tags = c("words", "punct", "sentc")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kRp.POS.tags_+3A_lang">lang</code></td>
<td>
<p>A character string defining a language (see details for valid choices).</p>
</td></tr>
<tr><td><code id="kRp.POS.tags_+3A_list.classes">list.classes</code></td>
<td>
<p>Logical,
if <code>TRUE</code> only the known word classes for the chosen language will me returned.</p>
</td></tr>
<tr><td><code id="kRp.POS.tags_+3A_list.tags">list.tags</code></td>
<td>
<p>Logical,
if <code>TRUE</code> only the POS tags for the chosen language will me returned.</p>
</td></tr>
<tr><td><code id="kRp.POS.tags_+3A_tags">tags</code></td>
<td>
<p>A character vector with at least one of &quot;words&quot;, &quot;punct&quot; or &quot;sentc&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use <code><a href="#topic+available.koRpus.lang">available.koRpus.lang</a></code> to get a list of all supported languages. Language
support packages must be installed an loaded to be usable with <code>kRp.POS.tags</code>.
For the internal tokenizer a small subset of tags is also defined,
available through <code>lang="kRp"</code>.
Finally,
the Universal POS Tags[1] are automatically appended if no matching tag was already defined.
If you don't know the language your text was written in,
the function <code><a href="#topic+guess.lang">guess.lang</a></code>
should be able to detect it.
</p>
<p>With the element <code>tags</code> you can specify if you want all tag definitions, or a subset,
e.g. tags only for punctuation and
sentence endings (that is,
you need to call for both &quot;punct&quot; and &quot;sentc&quot; to get all punctuation tags).
</p>
<p>The function is not so much intended to be used directly,
but it is called by several other functions internally. However,
it can still be useful to directly examine available POS tags.
</p>


<h3>Value</h3>

<p>If <code>list.classes=FALSE</code> and <code>list.tags=FALSE</code> returns a matrix with word tag definitions of the given language.
The matrix has three columns:
</p>

<dl>
<dt><code>tag</code>:</dt><dd><p>Word tag</p>
</dd>
<dt><code>class</code>:</dt><dd><p>Respective word class</p>
</dd>
<dt><code>desc</code>:</dt><dd><p>&quot;Human readable&quot; description of what the tag stands for</p>
</dd>
</dl>

<p>Otherwise a vector with the known word classes or POS tags for the chosen language (and probably tag subset) will be returned.
If both <code>list.classes</code> and <code>list.tags</code> are <code>TRUE</code>,
still only the POS tags will be returned.
</p>


<h3>References</h3>

<p>[1] <a href="https://universaldependencies.org/u/pos/index.html">https://universaldependencies.org/u/pos/index.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get.kRp.env">get.kRp.env</a></code>,
<code><a href="#topic+available.koRpus.lang">available.koRpus.lang</a></code>,
<code><a href="#topic+install.koRpus.lang">install.koRpus.lang</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  tags.internal &lt;- kRp.POS.tags("kRp")
  tags.en &lt;- kRp.POS.tags("en")
} else {}
</code></pre>

<hr>
<h2 id='kRp.readability+2C-class'>S4 Class kRp.readability</h2><span id='topic+kRp.readability+2C-class'></span><span id='topic+kRp_readability'></span><span id='topic+kRp.readability-class'></span>

<h3>Description</h3>

<p>This class is used for objects that are returned by <code><a href="#topic+readability">readability</a></code> and its wrapper functions
(e.g., <code>Flesch</code>, <code>FOG</code> or <code>LIX</code>).
</p>


<h3>Slots</h3>


<dl>
<dt><code>lang</code></dt><dd><p>A character string,
naming the language that is assumed for the text in this object.</p>
</dd>
<dt><code>tokens</code></dt><dd><p>The tokenized and POS-tagged text. See <code><a href="#topic+kRp.text-class">kRp.text</a></code> for details.</p>
</dd>
<dt><code>desc</code></dt><dd><p>Descriptive measures which were computed from the text:
</p>

<dl>
<dt><code>sentences</code>:</dt><dd><p>Number of sentences.</p>
</dd>
<dt><code>words</code>:</dt><dd><p>Number of words.</p>
</dd>
<dt><code>letters</code>:</dt><dd><p>Named vector with total number of letters (<code>"all"</code>) and possibly several entries called <code>"l&lt;digit&gt;"</code>,
giving the number of words
with <code>&lt;digit&gt;</code> letters.</p>
</dd>
<dt><code>all.chars</code>:</dt><dd><p>Number of all characters, including spaces.</p>
</dd>
<dt><code>syllables</code>:</dt><dd><p>Named vector with the number of syllables,
simlar to <code>letters</code>, but entries are called <code>"s&lt;digit&gt;"</code> (<code>NA</code> if hyphenation was skipped).</p>
</dd>
<dt><code>lttr.distrib</code>:</dt><dd><p>Distribution of letters: Absolute numbers, cumulative sum,
inversed cumulative sum, percent, cumulative percent, and inversed cumulative percent.</p>
</dd>
<dt><code>syll.distrib</code>:</dt><dd><p>Distribution of syllables (see <code>lttr.distrib</code>,
<code>NA</code> if hyphenation was skipped).</p>
</dd>
<dt><code>syll.uniq.distrib</code>:</dt><dd><p>Distribution of unique syllables (see <code>lttr.distrib</code>,
<code>NA</code> if hyphenation was skipped).</p>
</dd>
<dt><code>punct</code>:</dt><dd><p>Number of punctuation characters.</p>
</dd>
<dt><code>conjunctions</code>:</dt><dd><p>Number of conjunctions.</p>
</dd>
<dt><code>prepositions</code>:</dt><dd><p>Number of prepositions.</p>
</dd>
<dt><code>pronouns</code>:</dt><dd><p>Number of pronouns.</p>
</dd>
<dt><code>foreign</code>:</dt><dd><p>Number of foreign words.</p>
</dd>
<dt><code>TTR</code>:</dt><dd><p>Type-token ratio.</p>
</dd>
<dt><code>avg.sentc.length</code>:</dt><dd><p>Average number of words per sentence.</p>
</dd>
<dt><code>avg.word.length</code>:</dt><dd><p>Average number of characters per word.</p>
</dd>
<dt><code>avg.syll.word</code>:</dt><dd><p>Average number of syllables per word (<code>NA</code> if hyphenation was skipped).</p>
</dd>
<dt><code>sntc.per.word</code>:</dt><dd><p>Number of sentences per word.</p>
</dd>
<dt><code>sntc.per100</code>:</dt><dd><p>Number of sentences per 100 words.</p>
</dd>
<dt><code>lett.per100</code>:</dt><dd><p>Number of letters per 100 words.</p>
</dd>
<dt><code>syll.per100</code>:</dt><dd><p>Number of syllables per 100 words (<code>NA</code> if hyphenation was skipped).</p>
</dd>
<dt><code>FOG.hard.words</code>:</dt><dd><p>Number of hard words,
counted according to FOG (<code>NULL</code> if measure was not computed).</p>
</dd>
<dt><code>Bormuth.NOL</code>:</dt><dd><p>Number of words not on the Bormuth word list (<code>NULL</code> if measure was not computed).</p>
</dd>
<dt><code>Dale.Chall.NOL</code>:</dt><dd><p>Number of words not on the Dale-Chall word list (<code>NULL</code> if measure was not computed).</p>
</dd>
<dt><code>Harris.Jacobson.NOL</code>:</dt><dd><p>Number of words not on the Harris-Jacobson word list (<code>NULL</code> if measure was not computed).</p>
</dd>
<dt><code>Spache.NOL</code>:</dt><dd><p>Number of words not on the Spache word list (<code>NULL</code> if measure was not computed).</p>
</dd>
</dl>
</dd>
<dt><code>hyphen</code></dt><dd><p>The hyphenated text that was actually analyzed (i.e. without certain word classes,
if they were to be removed).</p>
</dd>
<dt><code>param</code></dt><dd><p>Relevant parameters of the given analysis,
as given to the function call. See <code><a href="#topic+readability">readability</a></code>
for detailed onformation.</p>
</dd>
<dt><code>ARI</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the ARI level. NA if not calculated.</p>
</dd>
<dt><code>ARI.NRI</code></dt><dd><p>See &quot;ARI&quot;.</p>
</dd>
<dt><code>ARI.simple</code></dt><dd><p>See &quot;ARI&quot;.</p>
</dd>
<dt><code>Bormuth</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of Bormuth's Mean Cloze and grade level. NA if not calculated.</p>
</dd>
<dt><code>Coleman</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the four Coleman formulas. NA if not calculated.</p>
</dd>
<dt><code>Coleman.Liau</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Coleman-Liau index. NA if not calculated.</p>
</dd>
<dt><code>Dale.Chall</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Dale-Chall Readability Formula. NA if not calculated.</p>
</dd>
<dt><code>Dale.Chall.PSK</code></dt><dd><p>See &quot;Dale.Chall&quot;.</p>
</dd>
<dt><code>Dale.Chall.old</code></dt><dd><p>See &quot;Dale.Chall&quot;.</p>
</dd>
<dt><code>Danielson.Bryan</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Danielson-Bryan Formula. NA if not calculated.</p>
</dd>
<dt><code>Dickes.Steiwer</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of Dickes-Steiwer's shortcut formula. NA if not calculated.</p>
</dd>
<dt><code>DRP</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Degrees of Reading Power. NA if not calculated.</p>
</dd>
<dt><code>ELF</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Easy Listening Formula. NA if not calculated.</p>
</dd>
<dt><code>Farr.Jenkins.Paterson</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Farr-Jenkins-Paterson index. NA if not calculated.</p>
</dd>
<dt><code>Farr.Jenkins.Paterson.PSK</code></dt><dd><p>See &quot;Farr.Jenkins.Paterson&quot;.</p>
</dd>
<dt><code>Flesch</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of Flesch Reading Ease. NA if not calculated.</p>
</dd>
<dt><code>Flesch.PSK</code></dt><dd><p>See &quot;Flesch&quot;.</p>
</dd>
<dt><code>Flesch.Brouwer</code></dt><dd><p>See &quot;Flesch&quot;.</p>
</dd>
<dt><code>Flesch.Szigriszt</code></dt><dd><p>See &quot;Flesch&quot;.</p>
</dd>
<dt><code>Flesch.de</code></dt><dd><p>See &quot;Flesch&quot;.</p>
</dd>
<dt><code>Flesch.es</code></dt><dd><p>See &quot;Flesch&quot;.</p>
</dd>
<dt><code>Flesch.fr</code></dt><dd><p>See &quot;Flesch&quot;.</p>
</dd>
<dt><code>Flesch.nl</code></dt><dd><p>See &quot;Flesch&quot;.</p>
</dd>
<dt><code>Flesch.Kincaid</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Flesch-Kincaid Grade Level. NA if not calculated.</p>
</dd>
<dt><code>FOG</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings, a list of proper nouns,
combined words and verbs that were not counted as hard words
(<code>"dropped"</code>), the considered number of hard words,
and the calculated value of Gunning's FOG index. NA if not calculated.</p>
</dd>
<dt><code>FOG.PSK</code></dt><dd><p>See &quot;FOG&quot;.</p>
</dd>
<dt><code>FOG.NRI</code></dt><dd><p>See &quot;FOG&quot;.</p>
</dd>
<dt><code>FORCAST</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the FORCAST grade level. NA if not calculated.</p>
</dd>
<dt><code>FORCAST.RGL</code></dt><dd><p>See &quot;FORCAST&quot;.</p>
</dd>
<dt><code>Fucks</code></dt><dd><p>The calculated value of Fucks' Stilcharakteristik. NA if not calculated.</p>
</dd>
<dt><code>Gutierrez</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Gutierrez index. NA if not calculated.</p>
</dd>
<dt><code>Harris.Jacobson</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Harris-Jacobson index.
the word list used, all words not found on the list, the percentage of difficult words,
the percentage of long words,
as well as HJ1 to HJ5 for the five indices. NA if not calculated.</p>
</dd>
<dt><code>Linsear.Write</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Linsear Write index. NA if not calculated.</p>
</dd>
<dt><code>LIX</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the LIX index. NA if not calculated.</p>
</dd>
<dt><code>RIX</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the RIX index. NA if not calculated.</p>
</dd>
<dt><code>SMOG</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the SMOG grade level. NA if not calculated.</p>
</dd>
<dt><code>SMOG.de</code></dt><dd><p>See &quot;SMOG&quot;.</p>
</dd>
<dt><code>SMOG.C</code></dt><dd><p>See &quot;SMOG&quot;.</p>
</dd>
<dt><code>SMOG.simple</code></dt><dd><p>See &quot;SMOG&quot;.</p>
</dd>
<dt><code>Spache</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Spache formula. NA if not calculated.</p>
</dd>
<dt><code>Spache.old</code></dt><dd><p>See &quot;Spache&quot;.</p>
</dd>
<dt><code>Strain</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Strain index. NA if not calculated.</p>
</dd>
<dt><code>Traenkle.Bailer</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings,
percentages of prepositions and conjunctions,
and the calculated values of both Tr\&quot;ankle-Bailer formulae. NA if not calculated.</p>
</dd>
<dt><code>TRI</code></dt><dd><p>The calculated value of Kuntzsch' Text-Redundanz-Index. NA if not calculated.</p>
</dd>
<dt><code>Tuldava</code></dt><dd><p>The calculated value of the Tuldava text difficulty formula. NA if not calculated.</p>
</dd>
<dt><code>Wheeler.Smith</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Wheeler-Smith index. NA if not calculated.</p>
</dd>
<dt><code>Wheeler.Smith.de</code></dt><dd><p>See &quot;Wheeler.Smith&quot;</p>
</dd>
<dt><code>Wiener.STF</code></dt><dd><p>The &quot;flavour&quot; of the parameter settings and the calculated value of the Wiener Sachtextformel. NA if not calculated.</p>
</dd>
</dl>


<h3>Contructor function</h3>

<p>Should you need to manually generate objects of this class (which should rarely be the case),
the contructor function 
<code>kRp_readability(...)</code> can be used instead of
<code>new("kRp.readability", ...)</code>.
</p>

<hr>
<h2 id='kRp.text+2C-class'>S4 Class kRp.text</h2><span id='topic+kRp.text+2C-class'></span><span id='topic+kRp_text'></span><span id='topic+kRp.text-class'></span>

<h3>Description</h3>

<p>This class is used for objects that are returned by <code><a href="#topic+treetag">treetag</a></code> or <code><a href="#topic+tokenize">tokenize</a></code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>lang</code></dt><dd><p>A character string,
naming the language that is assumed for the tokenized text in this object.</p>
</dd>
<dt><code>desc</code></dt><dd><p>Descriptive statistics of the tagged text.</p>
</dd>
<dt><code>tokens</code></dt><dd><p>Results of the called tokenizer and POS tagger. The data.frame usually has eleven columns:
</p>

<dl>
<dt><code>doc_id</code>:</dt><dd><p>Factor, optional document identifier.</p>
</dd>
<dt><code>token</code>:</dt><dd><p>Character, the tokenized text.</p>
</dd>
<dt><code>tag</code>:</dt><dd><p>Factor, POS tags for each token.</p>
</dd>
<dt><code>lemma</code>:</dt><dd><p>Character, lemma for each token.</p>
</dd>
<dt><code>lttr</code>:</dt><dd><p>Integer, number of letters.</p>
</dd>
<dt><code>wclass</code>:</dt><dd><p>Factor, word class.</p>
</dd>
<dt><code>desc</code>:</dt><dd><p>Factor, a short description of the POS tag.</p>
</dd>
<dt><code>stop</code>:</dt><dd><p>Logical, <code>TRUE</code> if token is a stopword.</p>
</dd>
<dt><code>stem</code>:</dt><dd><p>Character, stemmed token.</p>
</dd>
<dt><code>idx</code>:</dt><dd><p>Integer, index number of token in this document.</p>
</dd>
<dt><code>sntc</code>:</dt><dd><p>Integer, number of sentence in this document.</p>
</dd>
</dl>

<p>This data.frame structure adheres to the &quot;Text Interchange Formats&quot; guidelines set out by rOpenSci[1].</p>
</dd>
<dt><code>features</code></dt><dd><p>A named logical vector,
indicating which features are available in this object's <code>feat_list</code> slot.
Common features are listed in the description of the <code>feat_list</code> slot.</p>
</dd>
<dt><code>feat_list</code></dt><dd><p>A named list with optional analysis results or other content as used by the defined <code>features</code>:
</p>

<ul>
<li><p><code>hyphen</code> A named list of objects of class <code><a href="sylly.html#topic+kRp.hyphen-class">kRp.hyphen</a></code>.
</p>
</li>
<li><p><code>readability</code> A named list of objects of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>
</li>
<li><p><code>lex_div</code> A named list of objects of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>
</li>
<li><p><code>freq</code> A list with additional results of <code><a href="#topic+freq.analysis">freq.analysis</a></code>.
</p>
</li>
<li><p><code>corp_freq</code> An object of class <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>,
e.g., results of a call to
<code><a href="#topic+read.corp.custom">read.corp.custom</a></code>.
</p>
</li>
<li><p><code>diff</code> Additional results of calls to a method like <code><a href="#topic+textTransform">textTransform</a></code>.
</p>
</li>
<li><p><code>doc_term_matrix</code> A sparse document-term matrix,
as produced by <code><a href="#topic+docTermMatrix">docTermMatrix</a></code>.
</p>
</li></ul>

<p>See the <code><a href="#topic+kRp.text_get-methods">getter and setter methods</a></code> for easy access to these sub-slots.
There can actually be any number of additional features,
the above is just a list of those already defined by this package.</p>
</dd>
</dl>


<h3>Contructor function</h3>

<p>Should you need to manually generate objects of this class (which should rarely be the case),
the contructor function 
<code>kRp_text(...)</code> can be used instead of
<code>new("kRp.text", ...)</code>.
</p>


<h3>Note</h3>

<p>There is also <code>as()</code> methods to transform objects from other koRpus classes into kRp.text.
</p>


<h3>References</h3>

<p>[1] Text Interchange Formats (<a href="https://github.com/ropensci/tif">https://github.com/ropensci/tif</a>)
</p>

<hr>
<h2 id='kRp.TTR+2C-class'>S4 Class kRp.TTR</h2><span id='topic+kRp.TTR+2C-class'></span><span id='topic+kRp_TTR'></span><span id='topic+kRp.TTR-class'></span>

<h3>Description</h3>

<p>This class is used for objects that are returned by <code><a href="#topic+lex.div">lex.div</a></code> and its wrapper functions
(like <code>TTR</code>, <code>MSTTR</code>, <code>MTLD</code>, etc.).
</p>


<h3>Slots</h3>


<dl>
<dt><code>param</code></dt><dd><p>Relevant parameters of the given analysis,
as given to the function call, see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</dd>
<dt><code>tt</code></dt><dd><p>The analyzed text in tokenized form, with eight elements (&quot;tokens&quot;,
&quot;types&quot;, &quot;lemmas&quot;, &quot;type.in.txt&quot;, &quot;type.in.result&quot;, &quot;num.tokens&quot;, &quot;num.types&quot;, &quot;num.lemmas&quot;).</p>
</dd>
<dt><code>TTR</code></dt><dd><p>Value of the classic type-token ratio. NA if not calculated.</p>
</dd>
<dt><code>MSTTR</code></dt><dd><p>Mean segmental type-token ratio, including the actual &quot;MSTTR&quot;,
TTR values of each segment (&quot;TTR.seg&quot;),
and the number of dropped words due to segment size (&quot;dropped&quot;). NA if not calculated.</p>
</dd>
<dt><code>MATTR</code></dt><dd><p>Moving-average type-token ratio, including the actual &quot;MATTR&quot;,
TTR values of each window (&quot;TTR.win&quot;),
and standard deviation of TTRs (&quot;sd&quot;). NA if not calculated.</p>
</dd>
<dt><code>C.ld</code></dt><dd><p>Herdan's C. NA if not calculated.</p>
</dd>
<dt><code>R.ld</code></dt><dd><p>Guiraud's R. NA if not calculated.</p>
</dd>
<dt><code>CTTR</code></dt><dd><p>Carroll's CTTR. NA if not calculated.</p>
</dd>
<dt><code>U.ld</code></dt><dd><p>Uber Index. NA if not calculated.</p>
</dd>
<dt><code>S.ld</code></dt><dd><p>Summer's S. NA if not calculated.</p>
</dd>
<dt><code>K.ld</code></dt><dd><p>Yule's K. NA if not calculated.</p>
</dd>
<dt><code>Maas</code></dt><dd><p>Maas' a. NA if not calculated.</p>
</dd>
<dt><code>lgV0</code></dt><dd><p>Maas' <code class="reqn">\lg{V_0}</code>. NA if not calculated.</p>
</dd>
<dt><code>lgeV0</code></dt><dd><p>Maas' <code class="reqn">\lg{}_{e}{V_0}</code>. NA if not calculated.</p>
</dd>
<dt><code>Maas.grw</code></dt><dd><p>Maas' relative type growth <code class="reqn">V'</code>. NA if not calculated.</p>
</dd>
<dt><code>HDD</code></dt><dd><p>The actual HD-D value (&quot;HDD&quot;),
a vector with the probabilies for each type (&quot;type.probs&quot;), a &quot;summary&quot;
on these probabilities and their standard deviation &quot;sd&quot;.</p>
</dd>
<dt><code>MTLD</code></dt><dd><p>Measure of textual lexical diversity, including the actual &quot;MTLD&quot;,
two matrices with detailed
information on forward and backward factorization (&quot;all.forw&quot; &amp; &quot;all.back&quot;),
a named vector holding both calculated
factors values (&quot;factors&quot;),
and a named list with information on the number or tokens in each factor, both
forward and backward,
as well as their mean and standard deviation (&quot;lengths&quot;). NA if not calculated.</p>
</dd>
<dt><code>MTLDMA</code></dt><dd><p>Moving-average MTLD, including the actual &quot;MTLDMA&quot;,
its standard deviation, a list (&quot;all&quot;) with detailed
information on factorization, the step size,
and a named list with information on the number or tokens in each factor,
as well as their mean and standard deviation (&quot;lengths&quot;). NA if not calculated.</p>
</dd>
<dt><code>TTR.char</code></dt><dd><p>TTR values, starting with the first steplength of tokens,
then adding the next one, progressing until
the whole text is analyzed. The matrix has two colums,
one for the respective step (&quot;token&quot;) and one for the actual values
(&quot;value&quot;). Can be used to plot TTR characteristic curves. NA if not calculated.</p>
</dd>
<dt><code>MATTR.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using MATTR algorithm. NA if not calculated.</p>
</dd>
<dt><code>C.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using Herdan's C algorithm. NA if not calculated.</p>
</dd>
<dt><code>R.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using Guiraud's R algorithm. NA if not calculated.</p>
</dd>
<dt><code>CTTR.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using Carroll's CTTR algorithm. NA if not calculated.</p>
</dd>
<dt><code>U.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using the Uber Index algorithm. NA if not calculated.</p>
</dd>
<dt><code>S.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using Summer's S algorithm. NA if not calculated.</p>
</dd>
<dt><code>K.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using Yule's K algorithm. NA if not calculated.</p>
</dd>
<dt><code>Maas.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using Maas' a algorithm. NA if not calculated.</p>
</dd>
<dt><code>lgV0.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using Maas' <code class="reqn">\lg{V_0}</code> algorithm. NA if not calculated.</p>
</dd>
<dt><code>lgeV0.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using Maas' <code class="reqn">\lg{}_{e}{V_0}</code> algorithm. NA if not calculated.</p>
</dd>
<dt><code>HDD.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using the HD-D algorithm. NA if not calculated.</p>
</dd>
<dt><code>MTLD.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using the MTLD algorithm. NA if not calculated.</p>
</dd>
<dt><code>MTLDMA.char</code></dt><dd><p>Equivalent to TTR.char,
but calculated using the moving-average MTLD algorithm. NA if not calculated.</p>
</dd>
</dl>


<h3>Contructor function</h3>

<p>Should you need to manually generate objects of this class (which should rarely be the case),
the contructor function 
<code>kRp_TTR(...)</code> can be used instead of
<code>new("kRp.TTR", ...)</code>.
</p>

<hr>
<h2 id='lex.div'>Analyze lexical diversity</h2><span id='topic+lex.div'></span><span id='topic+lex.div+2CkRp.text-method'></span><span id='topic+lex.div+2Ccharacter-method'></span><span id='topic+lex.div+2Cmissing-method'></span><span id='topic++5B+2CkRp.TTR+2CANY+2CANY+2CANY-method'></span><span id='topic++5B+2CkRp.TTR+2CANY-method'></span><span id='topic++5B+5B+2CkRp.TTR-method'></span><span id='topic++5B+5B+2CkRp.TTR+2CANY-method'></span>

<h3>Description</h3>

<p>These methods analyze the lexical diversity/complexity of a text corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lex.div(txt, ...)

## S4 method for signature 'kRp.text'
lex.div(
  txt,
  segment = 100,
  factor.size = 0.72,
  min.tokens = 9,
  MTLDMA.steps = 1,
  rand.sample = 42,
  window = 100,
  case.sens = FALSE,
  lemmatize = FALSE,
  detailed = FALSE,
  measure = c("TTR", "MSTTR", "MATTR", "C", "R", "CTTR", "U", "S", "K", "Maas", "HD-D",
    "MTLD", "MTLD-MA"),
  char = c("TTR", "MATTR", "C", "R", "CTTR", "U", "S", "K", "Maas", "HD-D", "MTLD",
    "MTLD-MA"),
  char.steps = 5,
  log.base = 10,
  force.lang = NULL,
  keep.tokens = FALSE,
  type.index = FALSE,
  corp.rm.class = "nonpunct",
  corp.rm.tag = c(),
  as.feature = FALSE,
  quiet = FALSE
)

## S4 method for signature 'character'
lex.div(
  txt,
  segment = 100,
  factor.size = 0.72,
  min.tokens = 9,
  MTLDMA.steps = 1,
  rand.sample = 42,
  window = 100,
  case.sens = FALSE,
  lemmatize = FALSE,
  detailed = FALSE,
  measure = c("TTR", "MSTTR", "MATTR", "C", "R", "CTTR", "U", "S", "K", "Maas", "HD-D",
    "MTLD", "MTLD-MA"),
  char = c("TTR", "MATTR", "C", "R", "CTTR", "U", "S", "K", "Maas", "HD-D", "MTLD",
    "MTLD-MA"),
  char.steps = 5,
  log.base = 10,
  force.lang = NULL,
  keep.tokens = FALSE,
  type.index = FALSE,
  corp.rm.class = "nonpunct",
  corp.rm.tag = c(),
  quiet = FALSE
)

## S4 method for signature 'missing'
lex.div(txt, measure)

## S4 method for signature 'kRp.TTR,ANY,ANY,ANY'
x[i]

## S4 method for signature 'kRp.TTR'
x[[i]]
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lex.div_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
containing the tagged text to be analyzed.
If <code>txt</code> is of class character, it is assumed to be the raw text to be analyzed.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_...">...</code></td>
<td>
<p>Only used for the method generic.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_segment">segment</code></td>
<td>
<p>An integer value for MSTTR,
defining how many tokens should form one segment.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_factor.size">factor.size</code></td>
<td>
<p>A real number between 0 and 1, defining the MTLD factor size.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_min.tokens">min.tokens</code></td>
<td>
<p>An integer value,
how many tokens a full factor must at least have to be considered for the MTLD-MA result.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_mtldma.steps">MTLDMA.steps</code></td>
<td>
<p>An integer value for MTLD-MA,
defining the step size for the moving window, in tokens. The original proposal
uses an incremet of 1. If you increase this value, computation will be faster,
but your value can only remain a good estimate if
the text is long enough.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_rand.sample">rand.sample</code></td>
<td>
<p>An integer value,
how many tokens should be assumed to be drawn for calculating HD-D.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_window">window</code></td>
<td>
<p>An integer value for MATTR,
defining how many tokens the moving window should include.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_case.sens">case.sens</code></td>
<td>
<p>Logical, whether types should be counted case sensitive.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_lemmatize">lemmatize</code></td>
<td>
<p>Logical,
whether analysis should be carried out on the lemmatized tokens rather than all running word forms.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_detailed">detailed</code></td>
<td>
<p>Logical,
whether full details of the analysis should be calculated. This currently affects MTLD and MTLD-MA, defining
if all factors should be kept in the object. This slows down calculations considerably.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_measure">measure</code></td>
<td>
<p>A character vector defining the measures which should be calculated. Valid elements are <code>"TTR"</code>,
<code>"MSTTR"</code>,
<code>"MATTR"</code>, <code>"C"</code>, <code>"R"</code>, <code>"CTTR"</code>, <code>"U"</code>, <code>"S"</code>, <code>"K"</code>,
<code>"Maas"</code>, <code>"HD-D"</code>, <code>"MTLD"</code>
and <code>"MTLD-MA"</code>. You can also set it to <code>"validation"</code> to get information on the current status of validation.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_char">char</code></td>
<td>
<p>A character vector defining whether data for plotting characteristic curves should be calculated. Valid elements are 
<code>"TTR"</code>, <code>"MATTR"</code>, <code>"C"</code>, <code>"R"</code>, <code>"CTTR"</code>, <code>"U"</code>,
<code>"S"</code>, <code>"K"</code>, <code>"Maas"</code>, <code>"HD-D"</code>,
<code>"MTLD"</code> and <code>"MTLD-MA"</code>.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_char.steps">char.steps</code></td>
<td>
<p>An integer value defining the step size for characteristic curves,
in tokens.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_log.base">log.base</code></td>
<td>
<p>A numeric value defining the base of the logarithm. See <code><a href="base.html#topic+log">log</a></code> for details.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_force.lang">force.lang</code></td>
<td>
<p>A character string defining the language to be assumed for the text,
by force. See details.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_keep.tokens">keep.tokens</code></td>
<td>
<p>Logical. If <code>TRUE</code>,
all raw tokens and types will be preserved in the resulting object, in a slot called 
<code>tt</code>. For the types, also their frequency in the analyzed text will be listed.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_type.index">type.index</code></td>
<td>
<p>Logical. If <code>TRUE</code>,
the <code>tt</code> slot will contain two named lists of all types with the indices where that particular
type is to be found in the original tagged text (<code>type.in.txt</code>) or the list of tokens in these results (<code>type.in.result</code>),
respectively.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_corp.rm.class">corp.rm.class</code></td>
<td>
<p>A character vector with word classes which should be dropped. The default value
<code>"nonpunct"</code> has special meaning and will cause the result of
<code>kRp.POS.tags(lang, tags=c("punct","sentc"), list.classes=TRUE)</code> to be used.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_corp.rm.tag">corp.rm.tag</code></td>
<td>
<p>A character vector with POS tags which should be dropped.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_as.feature">as.feature</code></td>
<td>
<p>Logical,
whether the output should be just the analysis results or the input object with
the results added as a feature. Use <code><a href="#topic+corpusLexDiv">corpusLexDiv</a></code>
to get the results from such an aggregated object.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_quiet">quiet</code></td>
<td>
<p>Logical. If <code>FALSE</code>, short status messages will be shown.
<code>TRUE</code> will also suppress all potential warnings regarding the validation status of measures.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_x">x</code></td>
<td>
<p>An object of class <code>kRp.TTR</code>.</p>
</td></tr>
<tr><td><code id="lex.div_+3A_i">i</code></td>
<td>
<p>Defines the row selector (<code>[</code>) or the name to match (<code>[[</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>lex.div</code> calculates a variety of proposed indices for lexical diversity. In the following formulae,
<code class="reqn">N</code> refers to
the total number of tokens, and <code class="reqn">V</code> to the number of types:
</p>

<dl>
<dt><code>"TTR"</code>:</dt><dd><p>The ordinary <em>Type-Token Ratio</em>: </p>
<p style="text-align: center;"><code class="reqn">TTR = \frac{V}{N}</code>
</p>

<p>Wrapper function: <code><a href="#topic+TTR">TTR</a></code></p>
</dd>
<dt><code>"MSTTR"</code>:</dt><dd><p>For the <em>Mean Segmental Type-Token Ratio</em> (sometimes referred to as <em>Split TTR</em>) tokens are split up into 
segments of the given size,
TTR for each segment is calculated and the mean of these values returned. Tokens at the end which do 
not make a full segment are ignored. The number of dropped tokens is reported.
</p>
<p>Wrapper function: <code><a href="#topic+MSTTR">MSTTR</a></code></p>
</dd>
<dt><code>"MATTR"</code>:</dt><dd><p>The <em>Moving-Average Type-Token Ratio</em> (Covington &amp; McFall,
2010) calculates TTRs for a defined number of tokens
(called the &quot;window&quot;),
starting at the beginning of the text and moving this window over the text, until the last token is reached.
The mean of these TTRs is the MATTR.
</p>
<p>Wrapper function: <code><a href="#topic+MATTR">MATTR</a></code></p>
</dd>
<dt><code>"C"</code>:</dt><dd><p>Herdan's <em>C</em> (Herdan, 1960, as cited in Tweedie &amp; Baayen,
1998; sometimes referred to as <em>LogTTR</em>): </p>
<p style="text-align: center;"><code class="reqn">C = \frac{\lg{V}}{\lg{N}}</code>
</p>
</dd>
</dl>
<p>Wrapper function: <code><a href="#topic+C.ld">C.ld</a></code>
</p>
<dl>
<dt><code>"R"</code>:</dt><dd><p>Guiraud's <em>Root TTR</em> (Guiraud, 1954,
as cited in Tweedie &amp; Baayen, 1998): </p>
<p style="text-align: center;"><code class="reqn">R = \frac{V}{\sqrt{N}}</code>
</p>
</dd>
</dl>
<p>Wrapper function: <code><a href="#topic+R.ld">R.ld</a></code>
</p>
<dl>
<dt><code>"CTTR"</code>:</dt><dd><p>Carroll's <em>Corrected TTR</em>: </p>
<p style="text-align: center;"><code class="reqn">CTTR = \frac{V}{\sqrt{2N}}</code>
</p>
</dd>
</dl>
<p>Wrapper function: <code><a href="#topic+CTTR">CTTR</a></code>
</p>
<dl>
<dt><code>"U"</code>:</dt><dd><p>Dugast's <em>Uber Index</em>  (Dugast, 1978,
as cited in Tweedie &amp; Baayen, 1998): </p>
<p style="text-align: center;"><code class="reqn">U = \frac{(\lg{N})^2}{\lg{N} - \lg{V}}</code>
</p>
</dd>
</dl>
<p>Wrapper function: <code><a href="#topic+U.ld">U.ld</a></code>
</p>
<dl>
<dt><code>"S"</code>:</dt><dd><p>Summer's index: </p>
<p style="text-align: center;"><code class="reqn">S = \frac{\lg{\lg{V}}}{\lg{\lg{N}}}</code>
</p>
</dd>
</dl>
<p>Wrapper function: <code><a href="#topic+S.ld">S.ld</a></code>
</p>
<dl>
<dt><code>"K"</code>:</dt><dd><p>Yule's <em>K</em>  (Yule, 1944, as cited in Tweedie &amp; Baayen,
1998) is calculated by: </p>
<p style="text-align: center;"><code class="reqn">K = 10^4 \times \frac{(\sum_{X=1}^{X}{{f_X}X^2}) - N}{N^2}</code>
</p>

<p>where <code class="reqn">N</code> is the number of tokens,
<code class="reqn">X</code> is a vector with the frequencies of each type, and <code class="reqn">f_X</code> is
the frequencies for each X.
</p>
<p>Wrapper function: <code><a href="#topic+K.ld">K.ld</a></code></p>
</dd>
<dt><code>"Maas"</code>:</dt><dd><p>Maas' indices (<code class="reqn">a</code>,
<code class="reqn">\lg{V_0}</code> &amp; <code class="reqn">\lg{}_{e}{V_0}</code>): </p>
<p style="text-align: center;"><code class="reqn">a^2 = \frac{\lg{N} - \lg{V}}{\lg{N}^2}</code>
</p>

<p style="text-align: center;"><code class="reqn">\lg{V_0} = \frac{\lg{V}}{\sqrt{1 - \frac{\lg{V}}{\lg{N}}^2}}</code>
</p>

<p>Earlier versions (<code>koRpus</code> &lt; 0.04-12) reported <code class="reqn">a^2</code>,
and not <code class="reqn">a</code>. The measure was derived from a formula by M\&quot;uller (1969, as cited in Maas, 1972).
<code class="reqn">\lg{}_{e}{V_0}</code> is equivalent to <code class="reqn">\lg{V_0}</code>,
only with <code class="reqn">e</code> as the base for the logarithms. Also calculated are <code class="reqn">a</code>, <code class="reqn">\lg{V_0}</code> (both not the same
as before) and <code class="reqn">V'</code> as measures of relative vocabulary growth while the text progresses. To calculate these measures,
the first half of the text and the full text
will be examined (see Maas, 1972, p. 67 ff. for details).
</p>
<p>Wrapper function: <code><a href="#topic+maas">maas</a></code></p>
</dd>
<dt><code>"MTLD"</code>:</dt><dd><p>For the <em>Measure of Textual Lexical Diversity</em> (McCarthy &amp; Jarvis,
2010) so called factors are counted. Each factor is a subsequent stream of 
tokens which ends (and is then counted as a full factor) when the TTR value falls below the given factor size. The value of
remaining partial factors is estimated by the ratio of their current TTR to the factor size threshold. The MTLD is the total number 
of tokens divided by the number of factors. The procedure is done twice,
both forward and backward for all tokens, and the mean of 
both calculations is the final MTLD result.
</p>
<p>Wrapper function: <code><a href="#topic+MTLD">MTLD</a></code></p>
</dd>
<dt><code>"MTLD-MA"</code>:</dt><dd><p>The <em>Moving-Average Measure of Textual Lexical Diversity</em> (Jarvis,
no year) combines factor counting and a moving
window similar to MATTR: After each full factor the the next one is calculated from one token after the last starting point. This is repeated
until the end of text is reached for the first time. The average of all full factor lengths is the final MTLD-MA result. Factors below the
<code>min.tokens</code> threshold are dropped.
</p>
<p>Wrapper function: <code><a href="#topic+MTLD">MTLD</a></code></p>
</dd>
<dt><code>"HD-D"</code>:</dt><dd><p>The <em>HD-D</em> value can be interpreted as the idealized version of <em>vocd-D</em> (see McCarthy &amp; Jarvis,
2007). For each type,
the probability is computed (using the hypergeometric distribution) of drawing it at least one time when drawing randomly a certain
number of tokens from the text &ndash; 42 by default. The sum of these probabilities make up the HD-D value. The sum of probabilities relative to
the drawn sample size (ATTR) is also reported.
</p>
<p>Wrapper function: <code><a href="#topic+HDD">HDD</a></code></p>
</dd>
</dl>

<p>By default, if the text has to be tagged yet,
the language definition is queried by calling <code>get.kRp.env(lang=TRUE)</code> 
internally.
Or, if <code>txt</code> has already been tagged,
by default the language definition of that tagged object is read
and used. Set <code>force.lang=get.kRp.env(lang=TRUE)</code> or to any other valid value,
if you want to forcibly overwrite this
default behaviour,
and only then. See <code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code> for all supported languages.
</p>


<h3>Value</h3>

<p>Depending on <code>as.feature</code>,
either an object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>,
or an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> with the added feature <code>lex_div</code> containing it.
</p>


<h3>References</h3>

<p>Covington, M.A. &amp; McFall,
J.D. (2010). Cutting the Gordian Knot: The Moving-Average Type-Token Ratio (MATTR). 
<em>Journal of Quantitative Linguistics</em>, 17(2), 94&ndash;100.
</p>
<p>Maas, H.-D.,
(1972). \&quot;Uber den Zusammenhang zwischen Wortschatzumfang und L\&quot;ange eines Textes. <em>Zeitschrift f\&quot;ur 
Literaturwissenschaft und Linguistik</em>, 2(8), 73&ndash;96.
</p>
<p>McCarthy, P.M. &amp; Jarvis,
S. (2007). vocd: A theoretical and empirical evaluation. <em>Language Testing</em>, 24(4), 459&ndash;488.
</p>
<p>McCarthy, P.M. &amp; Jarvis, S. (2010). MTLD, vocd-D,
and HD-D: A validation study of sophisticated approaces to lexical diversity 
assessment. <em>Behaviour Research Methods</em>, 42(2), 381&ndash;392.
</p>
<p>Tweedie. F.J. &amp; Baayen,
R.H. (1998). How Variable May a Constant Be? Measures of Lexical Richness in Perspective.
<em>Computers and the Humanities</em>, 32(5), 323&ndash;352.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  # call lex.div() on a tokenized text
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  # if you call lex.div() without arguments,
  # you will get its results directly
  ld.results &lt;- lex.div(tokenized.obj, char=c())

  # there are [ and [[ methods for these objects
  ld.results[["MSTTR"]]

  # alternatively, you can also store those results as a
  # feature in the object itself
  tokenized.obj &lt;- lex.div(
    tokenized.obj,
    char=c(),
    as.feature=TRUE
  )
  # results are now part of the object
  hasFeature(tokenized.obj)
  corpusLexDiv(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='lex.div.num'>Calculate lexical diversity</h2><span id='topic+lex.div.num'></span>

<h3>Description</h3>

<p>This function is a stripped down version of <code><a href="#topic+lex.div">lex.div</a></code>. It does not analyze text,
but takes the numbers of tokens and types directly to calculate measures for which this information is sufficient:
</p>

<ul>
<li> <p><code>"TTR"</code>The classic <em>Type-Token Ratio</em>
</p>
</li>
<li> <p><code>"C"</code>Herdan's <em>C</em>
</p>
</li>
<li> <p><code>"R"</code>Guiraud's <em>Root TTR</em>
</p>
</li>
<li> <p><code>"CTTR"</code>Carroll's <em>Corrected TTR</em>
</p>
</li>
<li> <p><code>"U"</code>Dugast's <em>Uber Index</em>
</p>
</li>
<li> <p><code>"S"</code>Summer's index
</p>
</li>
<li> <p><code>"Maas"</code> Maas' (<code class="reqn">a^2</code>)
</p>
</li></ul>

<p>See <code><a href="#topic+lex.div">lex.div</a></code> for further details on the formulae.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lex.div.num(
  num.tokens,
  num.types,
  measure = c("TTR", "C", "R", "CTTR", "U", "S", "Maas"),
  log.base = 10,
  quiet = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lex.div.num_+3A_num.tokens">num.tokens</code></td>
<td>
<p>Numeric, the number of tokens.</p>
</td></tr>
<tr><td><code id="lex.div.num_+3A_num.types">num.types</code></td>
<td>
<p>Numeric, the number of types.</p>
</td></tr>
<tr><td><code id="lex.div.num_+3A_measure">measure</code></td>
<td>
<p>A character vector defining the measures to calculate.</p>
</td></tr>
<tr><td><code id="lex.div.num_+3A_log.base">log.base</code></td>
<td>
<p>A numeric value defining the base of the logarithm. See <code><a href="base.html#topic+log">log</a></code> for details.</p>
</td></tr>
<tr><td><code id="lex.div.num_+3A_quiet">quiet</code></td>
<td>
<p>Logical. If <code>FALSE</code>, short status messages will be shown.
<code>TRUE</code> will also suppress all potential warnings regarding the validation status of measures.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>References</h3>

<p>Maas, H.-D.,
(1972). \&quot;Uber den Zusammenhang zwischen Wortschatzumfang und L\&quot;ange eines Textes. <em>Zeitschrift f\&quot;ur
Literaturwissenschaft und Linguistik</em>, 2(8), 73&ndash;96.
</p>
<p>Tweedie. F.J. &amp; Baayen,
R.H. (1998). How Variable May a Constant Be? Measures of Lexical Richness in Perspective.
<em>Computers and the Humanities</em>, 32(5), 323&ndash;352.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lex.div">lex.div</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lex.div.num(
  num.tokens=104,
  num.types=43
)
</code></pre>

<hr>
<h2 id='linsear.write'>Readability: Linsear Write Index</h2><span id='topic+linsear.write'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linsear.write(
  txt.file,
  hyphen = NULL,
  parameters = c(short.syll = 2, long.syll = 3, thrs = 20),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linsear.write_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="linsear.write_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="linsear.write_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="linsear.write_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the Linsear Write index. In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
linsear.write(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='LIX'>Readability: Bj\&quot;ornsson's L\&quot;asbarhetsindex (LIX)</h2><span id='topic+LIX'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LIX(txt.file, parameters = c(char = 6, const = 100), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LIX_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="LIX_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="LIX_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the readability index (&quot;l\&quot;asbarhetsindex&quot;) by Bj\&quot;ornsson. In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>Anderson,
J. (1981). Analysing the readability of english and non-english texts in the classroom with Lix. In
<em>Annual Meeting of the Australian Reading Association</em>, Darwin, Australia.
</p>
<p>Anderson,
J. (1983). Lix and Rix: Variations on a little-known readability index. <em>Journal of Reading</em>, 26(6), 490&ndash;496.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  LIX(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='maas'>Lexical diversity: Maas' indices</h2><span id='topic+maas'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maas(txt, char = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maas_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="maas_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="maas_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates Maas' indices (<code class="reqn">a^2</code> &amp; <code class="reqn">\lg{V_0}</code>). In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the index values,
and characteristics are
off by default.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
maas(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='MATTR'>Lexical diversity: Moving-Average Type-Token Ratio (MATTR)</h2><span id='topic+MATTR'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MATTR(txt, window = 100, char = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MATTR_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="MATTR_+3A_window">window</code></td>
<td>
<p>An integer value for MATTR,
defining how many tokens the moving window should include.</p>
</td></tr>
<tr><td><code id="MATTR_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="MATTR_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the moving-average type-token ratio (MATTR). In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the MATTR value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>References</h3>

<p>Covington, M.A. &amp; McFall,
J.D. (2010). Cutting the Gordian Knot: The Moving-Average Type-Token Ratio (MATTR).
<em>Journal of Quantitative Linguistics</em>, 17(2), 94&ndash;100.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
MATTR(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='MSTTR'>Lexical diversity: Mean Segmental Type-Token Ratio (MSTTR)</h2><span id='topic+MSTTR'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MSTTR(txt, segment = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MSTTR_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="MSTTR_+3A_segment">segment</code></td>
<td>
<p>An integer value, defining how many tokens should form one segment.</p>
</td></tr>
<tr><td><code id="MSTTR_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the mean segmental type-token ratio (MSTTR). In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the MSTTR value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
MSTTR(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='MTLD'>Lexical diversity: Measure of Textual Lexical Diversity (MTLD)</h2><span id='topic+MTLD'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MTLD(
  txt,
  factor.size = 0.72,
  min.tokens = 9,
  detailed = FALSE,
  char = FALSE,
  MA = FALSE,
  steps = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MTLD_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="MTLD_+3A_factor.size">factor.size</code></td>
<td>
<p>A real number between 0 and 1, defining the MTLD factor size.</p>
</td></tr>
<tr><td><code id="MTLD_+3A_min.tokens">min.tokens</code></td>
<td>
<p>An integer value,
how many tokens a full factor must at least have to be considered for the MTLD-MA result.</p>
</td></tr>
<tr><td><code id="MTLD_+3A_detailed">detailed</code></td>
<td>
<p>Logical,
whether full details of the analysis should be calculated. It defines
if all factors should be kept in the object. This slows down calculations considerably.</p>
</td></tr>
<tr><td><code id="MTLD_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="MTLD_+3A_ma">MA</code></td>
<td>
<p>Logical,
defining whether the newer moving-average algorithm (MTLD-MA) should be calculated.</p>
</td></tr>
<tr><td><code id="MTLD_+3A_steps">steps</code></td>
<td>
<p>An integer value for MTLD-MA, defining the step size for the moving window,
in tokens. The original proposal
uses an incremet of 1. If you increase this value, computation will be faster,
but your value can only remain a good estimate if
the text is long enough.</p>
</td></tr>
<tr><td><code id="MTLD_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the measure of textual lexical diversity (MTLD; see McCarthy &amp; Jarvis,
2010). In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the MTLD value,
and characteristics are
off by default.
</p>
<p>If you set <code>MA=TRUE</code>,
the newer MTLD-MA (moving-average method) is used instead of the classic MTLD.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>References</h3>

<p>McCarthy, P. M. &amp; Jarvis, S. (2010). MTLD, vocd-D,
and HD-D: A validation study of sophisticated approaces to lexical diversity assessment.
<em>Behaviour Research Methods</em>, 42(2), 381&ndash;392.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
MTLD(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='nWS'>Readability: Neue Wiener Sachtextformeln</h2><span id='topic+nWS'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nWS(
  txt.file,
  hyphen = NULL,
  parameters = c(ms.syll = 3, iw.char = 6, es.syll = 1),
  nws1 = c(ms = 19.35, sl = 0.1672, iw = 12.97, es = 3.27, const = 0.875),
  nws2 = c(ms = 20.07, sl = 0.1682, iw = 13.73, const = 2.779),
  nws3 = c(ms = 29.63, sl = 0.1905, const = 1.1144),
  nws4 = c(ms = 27.44, sl = 0.2656, const = 1.693),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nWS_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="nWS_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="nWS_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for all formulas of the index.</p>
</td></tr>
<tr><td><code id="nWS_+3A_nws1">nws1</code></td>
<td>
<p>A numeric vector with named magic numbers for the first of the formulas.</p>
</td></tr>
<tr><td><code id="nWS_+3A_nws2">nws2</code></td>
<td>
<p>A numeric vector with named magic numbers for the second of the formulas.</p>
</td></tr>
<tr><td><code id="nWS_+3A_nws3">nws3</code></td>
<td>
<p>A numeric vector with named magic numbers for the third of the formulas.</p>
</td></tr>
<tr><td><code id="nWS_+3A_nws4">nws4</code></td>
<td>
<p>A numeric vector with named magic numbers for the fourth of the formulas.</p>
</td></tr>
<tr><td><code id="nWS_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the new Wiener Sachtextformeln (formulas 1 to 4). In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index values.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>Bamberger, R. &amp; Vanecek,
E. (1984). <em>Lesen&ndash;Verstehen&ndash;Lernen&ndash;Schreiben</em>. Wien: Jugend und Volk.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
nWS(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='pasteText'>Paste koRpus objects</h2><span id='topic+pasteText'></span><span id='topic+pasteText+2CkRp.text-method'></span>

<h3>Description</h3>

<p>Paste the text in koRpus objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pasteText(txt, ...)

## S4 method for signature 'kRp.text'
pasteText(
  txt,
  replace = c(hon.kRp = "", hoff.kRp = "\n\n", p.kRp = "\n\n")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pasteText_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
<tr><td><code id="pasteText_+3A_...">...</code></td>
<td>
<p>Additional options, currently unused.</p>
</td></tr>
<tr><td><code id="pasteText_+3A_replace">replace</code></td>
<td>
<p>A named character vector to define replacements for <code>koRpus</code>' internal headline and paragraph tags.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes objects of class <code>kRp.text</code> and pastes only the actual text as is.
</p>


<h3>Value</h3>

<p>An atomic character vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  tokenized.obj &lt;- jumbleWords(tokenized.obj)
  pasteText(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='plot'>Plot method for objects of class kRp.text</h2><span id='topic+plot'></span><span id='topic+plot+2CkRp.text+2Cmissing-method'></span>

<h3>Description</h3>

<p>Plot method for S4 objects of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
plots the frequencies of tagged word classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot(x, y, ...)

## S4 method for signature 'kRp.text,missing'
plot(x, what = "wclass", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_+3A_x">x</code></td>
<td>
<p>An object of class <code>kRp.text</code></p>
</td></tr>
<tr><td><code id="plot_+3A_y">y</code></td>
<td>
<p>From the generic <code>plot</code> function, ignored for koRpus class objects.</p>
</td></tr>
<tr><td><code id="plot_+3A_...">...</code></td>
<td>
<p>Any other argument suitable for plot()</p>
</td></tr>
<tr><td><code id="plot_+3A_what">what</code></td>
<td>
<p>Character string, valid options are:
</p>

<dl>
<dt><code>"wclass"</code>:</dt><dd><p>Barplot of distribution of word classes</p>
</dd>
<dt><code>"letters"</code>:</dt><dd><p>Line plot of distribution of word length in letters</p>
</dd>
</dl>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.text-class">kRp.text</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  plot(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='query'>A method to get information out of koRpus objects</h2><span id='topic+query'></span><span id='topic+query+2CkRp.corp.freq-method'></span><span id='topic+query+2CkRp.text-method'></span><span id='topic+query+2Cdata.frame-method'></span>

<h3>Description</h3>

<p>The method <code>query</code> returns query information from objects of classes <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code> and
<code><a href="#topic+kRp.text-class">kRp.text</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query(obj, ...)

## S4 method for signature 'kRp.corp.freq'
query(
  obj,
  var = NULL,
  query,
  rel = "eq",
  as.df = TRUE,
  ignore.case = TRUE,
  perl = FALSE,
  regexp_var = "word"
)

## S4 method for signature 'kRp.text'
query(
  obj,
  var,
  query,
  rel = "eq",
  as.df = TRUE,
  ignore.case = TRUE,
  perl = FALSE,
  regexp_var = "token"
)

## S4 method for signature 'data.frame'
query(
  obj,
  var,
  query,
  rel = "eq",
  as.df = TRUE,
  ignore.case = TRUE,
  perl = FALSE,
  regexp_var = "token"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_+3A_obj">obj</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>, or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="query_+3A_...">...</code></td>
<td>
<p>Optional arguments, see above.</p>
</td></tr>
<tr><td><code id="query_+3A_var">var</code></td>
<td>
<p>A character string naming a variable in the object (i.e., colname). If set to
<code>"regexp"</code>, <code>grepl</code> is called on the column specified by <code>regexp_var</code>.</p>
</td></tr>
<tr><td><code id="query_+3A_query">query</code></td>
<td>
<p>A character vector (for words), regular expression,
or single number naming values to be matched in the variable.
Can also be a vector of two numbers to query a range of frequency data,
or a list of named lists for multiple queries (see
&quot;Query lists&quot; section in details).</p>
</td></tr>
<tr><td><code id="query_+3A_rel">rel</code></td>
<td>
<p>A character string defining the relation of the queried value and desired results.
Must either be <code>"eq"</code> (equal, the default), <code>"gt"</code> (greater than),
<code>"ge"</code> (greater of equal),
<code>"lt"</code> (less than) or <code>"le"</code> (less or equal). If <code>var="word"</code>,
is always interpreted as <code>"eq"</code></p>
</td></tr>
<tr><td><code id="query_+3A_as.df">as.df</code></td>
<td>
<p>Logical, if <code>TRUE</code>, returns a data.frame, otherwise an object of
the input class. Ignored if <code>obj</code> is a data frame already.</p>
</td></tr>
<tr><td><code id="query_+3A_ignore.case">ignore.case</code></td>
<td>
<p>Logical, passed through to <code>grepl</code> if <code>var="regexp"</code>.</p>
</td></tr>
<tr><td><code id="query_+3A_perl">perl</code></td>
<td>
<p>Logical, passed through to <code>grepl</code> if <code>var="regexp"</code>.</p>
</td></tr>
<tr><td><code id="query_+3A_regexp_var">regexp_var</code></td>
<td>
<p>A character string naming the column to query if <code>var="regexp"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>kRp.corp.freq:</em> Depending on the setting of the <code>var</code> parameter,
will return entries with a matching character (<code>var="word"</code>),
or all entries of the desired frequency (see the examples). A special case is the need for a range of frequencies,
which can be achieved by providing a nomerical vector of two values as the <code>query</code> value,
for start and end of
the range, respectively. In these cases,
if <code>rel</code> is set to <code>"gt"</code> or <code>"lt"</code>,
the given range borders are excluded, otherwise they will be included as true matches.
</p>
<p><em>kRp.text:</em> <code>var</code> can be any of the variables in slot <code>tokens</code>. If <code>rel="num"</code>,
a vector with the row numbers in which the query was found is returned.
</p>


<h3>Value</h3>

<p>Depending on the arguments, might include whole objects, lists, single values etc.
</p>


<h3>Query lists</h3>

<p>You can combine an arbitrary number of queries in a simple way by providing a list of named lists to the
<code>query</code> parameter, where each list contains one query request. In each list,
the first element name represents the
<code>var</code> value of the request,
and its value is taken as the <code>query</code> argument. You can also assign <code>rel</code>, 
<code>ignore.case</code> and <code>perl</code> for each request individually, and if you don't,
the settings of the main query call are 
taken as default (<code>as.df</code> only applies to the final query). The filters will be applied in the order given,
i.e., the
second query will be made to the results of the first.
</p>
<p>This method calls <code><a href="base.html#topic+subset">subset</a></code>,
which might actually be even more flexible if you need more control.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>, <code><a href="base.html#topic+subset">subset</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  en_corp &lt;- read.corp.custom(
    tokenized.obj,
    caseSens=FALSE
  )

  # look up frequencies for the word "winner"
  query(en_corp, var="word", query="winner")

  # show all entries with a frequency of exactly 3 in the corpus
  query(en_corp, "freq", 3)

  # now, which tokens appear more than 40000 times in a million?
  query(en_corp, "pmio", 40000, "gt")

  # example for a range request: tokens with a log10 between 4.2 and 4.7
  # (including these two values)
  query(en_corp, "log10", c(4.2, 4.7))
  # (and without them)
  query(en_corp, "log10", c(4.2, 4.7), "gt")

  # example for a list of queries: get words with a frequency between
  # 10000 and 25000 per million and at least four letters
  query(en_corp, query=list(
    list(pmio=c(10000, 25000)),
    list(lttr=4, rel="ge"))
  )

  # get all instances of "the" in a tokenized text object
  query(tokenized.obj, "token", "the")
} else {}
</code></pre>

<hr>
<h2 id='R.ld'>Lexical diversity: Guiraud's R</h2><span id='topic+R.ld'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>R.ld(txt, char = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="R.ld_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="R.ld_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="R.ld_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates Guiraud's R. In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the R value,
and characteristics are
off by default.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
R.ld(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='read.BAWL'>Import BAWL-R data</h2><span id='topic+read.BAWL'></span>

<h3>Description</h3>

<p>Read the Berlin Affective Word List &ndash; Reloaded (V\&quot;o, Conrad, Kuchinke, Hartfeld,
Hofmann &amp; Jacobs, 2009; [1]) into a valid object of class
<code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.BAWL(csv, fileEncoding = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.BAWL_+3A_csv">csv</code></td>
<td>
<p>A character string, path to the BAWL-R in CSV2 format.</p>
</td></tr>
<tr><td><code id="read.BAWL_+3A_fileencoding">fileEncoding</code></td>
<td>
<p>A character string naming the encoding of the file, if necessary.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To use this function,
you must first export the BAWL-R list into CSV format: Use comma for decimal values and semicolon as value separator
(often referred to as CSV2). Once you have successfully imported the word list,
you can use the object to perform frequency analysis.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>.
</p>


<h3>References</h3>

<p>V\&quot;o, M. L.-H., Conrad, M., Kuchinke, L., Hartfeld, K., Hofmann, M.F. &amp; Jacobs,
A.M. (2009).
The Berlin Affective Word List Reloaded (BAWL-R). <em>Behavior Research Methods</em>,
41(2), 534&ndash;538.
doi: 10.3758/BRM.41.2.534
</p>
<p>[1] <a href="https://www.ewi-psy.fu-berlin.de/einrichtungen/arbeitsbereiche/allgpsy/Download/BAWL/index.html">https://www.ewi-psy.fu-berlin.de/einrichtungen/arbeitsbereiche/allgpsy/Download/BAWL/index.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>,
<code><a href="#topic+query">query</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
bawl.corp &lt;- read.BAWL(
  file.path("~","mydata","valence","BAWL-R.csv")
)

# you can now use query() now to create subsets of the word list,
# e.g., only nound with 5 letters and an valence rating of &gt;= 1
bawl.stimulus &lt;- query(bawl.corp,
  query=list(
    list(wclass="noun"),
    list(lttr=5),
    list("EMO_MEAN"=1, rel="ge")
  )
)

## End(Not run)
</code></pre>

<hr>
<h2 id='read.corp.celex'>Import Celex data</h2><span id='topic+read.corp.celex'></span>

<h3>Description</h3>

<p>Read data from Celex[1] formatted corpora.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.corp.celex(
  celex.path,
  running.words,
  fileEncoding = "ISO_8859-1",
  n = -1,
  caseSens = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.corp.celex_+3A_celex.path">celex.path</code></td>
<td>
<p>A character string, path to a frequency file in Celex format to read.</p>
</td></tr>
<tr><td><code id="read.corp.celex_+3A_running.words">running.words</code></td>
<td>
<p>An integer value,
number of running words in the Celex data corpus to be read.</p>
</td></tr>
<tr><td><code id="read.corp.celex_+3A_fileencoding">fileEncoding</code></td>
<td>
<p>A character string naming the encoding of the Celex files.</p>
</td></tr>
<tr><td><code id="read.corp.celex_+3A_n">n</code></td>
<td>
<p>An integer value defining how many lines of data should be read if <code>format="flatfile"</code>. Reads all at -1.</p>
</td></tr>
<tr><td><code id="read.corp.celex_+3A_casesens">caseSens</code></td>
<td>
<p>Logical,
if <code>FALSE</code> forces all frequency statistics to be calculated regardless of the tokens' case.
Otherwise, if the imported database supports it,
you will get different frequencies for the same tokens in different
cases (e.\,g., &quot;one&quot; and &quot;One&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>.
</p>


<h3>References</h3>

<p>[1] <a href="http://celex.mpi.nl">http://celex.mpi.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
my.Celex.data &lt;- read.corp.celex(
  file.path("~","mydata","Celex","GERMAN","GFW","GFW.CD"),
  running.words=5952000
)
freq.analysis(
  tokenized.obj,
  corp.freq=my.Celex.data
)

## End(Not run)
</code></pre>

<hr>
<h2 id='read.corp.custom'>Import custom corpus data</h2><span id='topic+read.corp.custom'></span><span id='topic+read.corp.custom+2CkRp.text-method'></span>

<h3>Description</h3>

<p>Read data from a custom corpus into a valid object of class <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.corp.custom(corpus, caseSens = TRUE, log.base = 10, ...)

## S4 method for signature 'kRp.text'
read.corp.custom(
  corpus,
  caseSens = TRUE,
  log.base = 10,
  dtm = docTermMatrix(obj = corpus, case.sens = caseSens),
  as.feature = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.corp.custom_+3A_corpus">corpus</code></td>
<td>
<p>An object of class <code>kRp.text</code> (then the column <code>"token"</code> of the <code>tokens</code> slot is used).</p>
</td></tr>
<tr><td><code id="read.corp.custom_+3A_casesens">caseSens</code></td>
<td>
<p>Logical. If <code>FALSE</code>,
all tokens will be matched in their lower case form.</p>
</td></tr>
<tr><td><code id="read.corp.custom_+3A_log.base">log.base</code></td>
<td>
<p>A numeric value defining the base of the logarithm used for inverse document frequency (idf). See
<code><a href="base.html#topic+log">log</a></code> for details.</p>
</td></tr>
<tr><td><code id="read.corp.custom_+3A_...">...</code></td>
<td>
<p>Additional options for methods of the generic.</p>
</td></tr>
<tr><td><code id="read.corp.custom_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix of the <code>corpus</code> object as generated by <code><a href="#topic+docTermMatrix">docTermMatrix</a></code>.
This argument merely exists for cases where you want to re-use an already existing matrix.
By default, it is being created from the <code>corpus</code> object.</p>
</td></tr>
<tr><td><code id="read.corp.custom_+3A_as.feature">as.feature</code></td>
<td>
<p>Logical,
whether the output should be just the analysis results or the input object with
the results added as a feature. Use <code><a href="#topic+corpusCorpFreq">corpusCorpFreq</a></code>
to get the results from such an aggregated object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The methods should enable you to perform a basic text corpus frequency analysis. That is,
not just to
import analysis results like LCC files,
but to import the corpus material itself. The resulting object
is of class <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>,
so it can be used for frequency analysis by
other functions and methods of this package.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>.
</p>
<p>Depending on <code>as.feature</code>,
either an object of class <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>,
or an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> with the added feature <code>corp_freq</code> containing it.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  # call read.corp.custom() on a tokenized text
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  # if you call read.corp.custom() without arguments,
  # you will get its results directly
  en_corp &lt;- read.corp.custom(
    tokenized.obj,
    caseSens=FALSE
  )

  # alternatively, you can also store those results as a
  # feature in the object itself
  tokenized.obj &lt;- read.corp.custom(
    tokenized.obj,
    caseSens=FALSE,
    as.feature=TRUE
  )
  # results are now part of the object
  hasFeature(tokenized.obj)
  corpusCorpFreq(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='read.corp.LCC'>Import LCC data</h2><span id='topic+read.corp.LCC'></span>

<h3>Description</h3>

<p>Read data from LCC[1] formatted corpora (Quasthoff, Richter &amp; Biemann, 2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.corp.LCC(
  LCC.path,
  format = "flatfile",
  fileEncoding = "UTF-8",
  n = -1,
  keep.temp = FALSE,
  prefix = NULL,
  bigrams = FALSE,
  cooccurence = FALSE,
  caseSens = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.corp.LCC_+3A_lcc.path">LCC.path</code></td>
<td>
<p>A character string,
either path to a .tar/.tar.gz/.zip file in LCC format (flatfile),
or the path to the directory with the unpacked archive.</p>
</td></tr>
<tr><td><code id="read.corp.LCC_+3A_format">format</code></td>
<td>
<p>Either &quot;flatfile&quot; or &quot;MySQL&quot;, depending on the type of LCC data.</p>
</td></tr>
<tr><td><code id="read.corp.LCC_+3A_fileencoding">fileEncoding</code></td>
<td>
<p>A character string naming the encoding of the LCC files. Old zip archives used &quot;ISO_8859-1&quot;.
This option will only influence the reading of meta information,
as the actual database encoding is derived from
there.</p>
</td></tr>
<tr><td><code id="read.corp.LCC_+3A_n">n</code></td>
<td>
<p>An integer value defining how many lines of data should be read if <code>format="flatfile"</code>. Reads all at -1.</p>
</td></tr>
<tr><td><code id="read.corp.LCC_+3A_keep.temp">keep.temp</code></td>
<td>
<p>Logical. If <code>LCC.path</code> is a tarred/zipped archive,
setting <code>keep.temp=TRUE</code> will keep
the temporarily unpacked files for further use. By default all temporary files will be removed when
the function ends.</p>
</td></tr>
<tr><td><code id="read.corp.LCC_+3A_prefix">prefix</code></td>
<td>
<p>Character string,
giving the prefix for the file names in the archive. Needed for newer LCC tar archives
if they are already decompressed (autodetected if <code>LCC.path</code> points to the tar archive directly).</p>
</td></tr>
<tr><td><code id="read.corp.LCC_+3A_bigrams">bigrams</code></td>
<td>
<p>Logical, whether infomration on bigrams should be imported.
This is <code>FALSE</code> by default, because it might make the objects quite large.
Note that this will only work in <code>n = -1</code> because otherwise the tokens cannot be looked up.</p>
</td></tr>
<tr><td><code id="read.corp.LCC_+3A_cooccurence">cooccurence</code></td>
<td>
<p>Logical, like <code>bigrams</code>,
but for information on co-occurences of tokens in a sentence.</p>
</td></tr>
<tr><td><code id="read.corp.LCC_+3A_casesens">caseSens</code></td>
<td>
<p>Logical,
if <code>FALSE</code> forces all frequency statistics to be calculated regardless of the tokens' case.
Otherwise, if the imported database supports it,
you will get different frequencies for the same tokens in different
cases (e.\,g., &quot;one&quot; and &quot;One&quot;).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The LCC database can either be unpacked or still a .tar/.tar.gz/.zip archive. If the latter is the case,
then
all necessary files will be extracted to a temporal location automatically,
and by default removed again
when the function has finished reading from it.
</p>
<p>Newer LCC archives no longer feature the <code>*-meta.txt</code> file,
resulting in less meta informtion in the object.
In these cases, the total number of tokens is calculated as the sum of types' frequencies.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>.
</p>


<h3>Note</h3>

<p>Please note that MySQL support is not implemented yet.
</p>


<h3>References</h3>

<p>Quasthoff, U., Richter, M. &amp; Biemann,
C. (2006). Corpus Portal for Search in Monolingual Corpora, In
<em>Proceedings of the Fifth International Conference on Language Resources and Evaluation</em>,
Genoa, 1799&ndash;1802.
</p>
<p>[1] <a href="https://wortschatz.uni-leipzig.de/en/download/">https://wortschatz.uni-leipzig.de/en/download/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# old format .zip archive
my.LCC.data &lt;- read.corp.LCC(
  file.path("~","mydata","corpora","de05_3M.zip")
)
# new format tar archive
my.LCC.data &lt;- read.corp.LCC(
  file.path("~","mydata","corpora","rus_web_2002_300K-text.tar")
)
# in case the tar archive was already unpacked
my.LCC.data &lt;- read.corp.LCC(
  file.path("~","mydata","corpora","rus_web_2002_300K-text"),
  prefix="rus_web_2002_300K-"
)
freq.analysis(
  tokenized.obj,
  corp.freq=my.LCC.data
)

## End(Not run)
</code></pre>

<hr>
<h2 id='readability'>Measure readability</h2><span id='topic+readability'></span><span id='topic+readability+2CkRp.text-method'></span><span id='topic+readability+2Cmissing-method'></span><span id='topic++5B+2CkRp.readability+2CANY+2CANY+2CANY-method'></span><span id='topic++5B+2CkRp.readability+2CANY-method'></span><span id='topic++5B+5B+2CkRp.readability-method'></span><span id='topic++5B+5B+2CkRp.readability+2CANY-method'></span>

<h3>Description</h3>

<p>These methods calculate several readability indices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readability(txt.file, ...)

## S4 method for signature 'kRp.text'
readability(
  txt.file,
  hyphen = NULL,
  index = c("ARI", "Bormuth", "Coleman", "Coleman.Liau", "Dale.Chall",
    "Danielson.Bryan", "Dickes.Steiwer", "DRP", "ELF", "Farr.Jenkins.Paterson", "Flesch",
    "Flesch.Kincaid", "FOG", "FORCAST", "Fucks", "Gutierrez", "Harris.Jacobson",
    "Linsear.Write", "LIX", "nWS", "RIX", "SMOG", "Spache", "Strain", "Traenkle.Bailer",
    "TRI", "Tuldava", "Wheeler.Smith"),
  parameters = list(),
  word.lists = list(Bormuth = NULL, Dale.Chall = NULL, Harris.Jacobson = NULL, Spache =
    NULL),
  fileEncoding = "UTF-8",
  sentc.tag = "sentc",
  nonword.class = "nonpunct",
  nonword.tag = c(),
  quiet = FALSE,
  keep.input = NULL,
  as.feature = FALSE
)

## S4 method for signature 'missing'
readability(txt.file, index)

## S4 method for signature 'kRp.readability,ANY,ANY,ANY'
x[i]

## S4 method for signature 'kRp.readability'
x[[i]]
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readability_+3A_txt.file">txt.file</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
<tr><td><code id="readability_+3A_...">...</code></td>
<td>
<p>Additional arguments for the generics.</p>
</td></tr>
<tr><td><code id="readability_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class <code><a href="sylly.html#topic+kRp.hyphen-class">kRp.hyphen</a></code>. If <code>NULL</code>,
the text will be hyphenated automatically. All syllable handling will
be skipped automatically if it's not needed for the selected indices.</p>
</td></tr>
<tr><td><code id="readability_+3A_index">index</code></td>
<td>
<p>A character vector,
indicating which indices should actually be computed. If set to <code>"all"</code>, then all available indices
will be tried (meaning all variations of all measures). If set to <code>"fast"</code>,
a subset of the default values is used that is
known to compute fast (currently,
this only excludes &quot;FOG&quot;). You can also set it to <code>"validation"</code> to get information on the current
status of validation.</p>
</td></tr>
<tr><td><code id="readability_+3A_parameters">parameters</code></td>
<td>
<p>A list with named magic numbers,
defining the relevant parameters for each index. If none are given,
the default values are used.</p>
</td></tr>
<tr><td><code id="readability_+3A_word.lists">word.lists</code></td>
<td>
<p>A named list providing the word lists for indices which need one. If <code>NULL</code> or missing,
the indices will be
skipped and a warning is giving. Actual word lists can be provided as either a vector (or matrix or data.frame with only one column),
or as a file name, where this file must contain one word per line. Alternatively,
you can provide the number of words which are not
on the list, directly.</p>
</td></tr>
<tr><td><code id="readability_+3A_fileencoding">fileEncoding</code></td>
<td>
<p>A character string defining the character encoding of the <code>word.lists</code> in case they are provided as files,
like <code>"Latin1"</code> or <code>"UTF-8"</code>.</p>
</td></tr>
<tr><td><code id="readability_+3A_sentc.tag">sentc.tag</code></td>
<td>
<p>A character vector with POS tags which indicate a sentence ending. The default value <code>"sentc"</code> has special meaning and
will cause the result of <code>kRp.POS.tags(lang, tags="sentc",
      list.tags=TRUE)</code> to be used.</p>
</td></tr>
<tr><td><code id="readability_+3A_nonword.class">nonword.class</code></td>
<td>
<p>A character vector with word classes which should be ignored for readability analysis. The default value
<code>"nonpunct"</code> has special meaning and will cause the result of <code>kRp.POS.tags(lang,
      tags=c("punct","sentc"), list.classes=TRUE)</code>
to be used. Will only be of consequence if <code>hyphen</code> is not set!</p>
</td></tr>
<tr><td><code id="readability_+3A_nonword.tag">nonword.tag</code></td>
<td>
<p>A character vector with POS tags which should be ignored for readability analysis. Will only be
of consequence if <code>hyphen</code> is not set!</p>
</td></tr>
<tr><td><code id="readability_+3A_quiet">quiet</code></td>
<td>
<p>Logical. If <code>FALSE</code>, short status messages will be shown.
<code>TRUE</code> will also suppress all potential warnings regarding the validation status of measures.</p>
</td></tr>
<tr><td><code id="readability_+3A_keep.input">keep.input</code></td>
<td>
<p>Logical. If <code>FALSE</code>,
neither the object provided by (or generated from) <code>txt.file</code> nor
<code>hyphen</code> will be kept in the output object. By default (<code>NULL</code>) they are kept if the input was not already of the needed object class
(e.g., <code>kRp.text</code>) or missing,
to allow for re-use without the need to tag or hyphenate the text again.
If <code>TRUE</code>, they are always kept. In cases where you want smaller object sizes,
set this to <code>FALSE</code> to always drop these slots.</p>
</td></tr>
<tr><td><code id="readability_+3A_as.feature">as.feature</code></td>
<td>
<p>Logical,
whether the output should be just the analysis results or the input object with
the results added as a feature. Use <code><a href="#topic+corpusReadability">corpusReadability</a></code>
to get the results from such an aggregated object.</p>
</td></tr>
<tr><td><code id="readability_+3A_x">x</code></td>
<td>
<p>An object of class <code>kRp.readability</code>.</p>
</td></tr>
<tr><td><code id="readability_+3A_i">i</code></td>
<td>
<p>Defines the row selector (<code>[</code>) or the name to match (<code>[[</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the following formulae, <code class="reqn">W</code> stands for the number of words,
<code class="reqn">St</code> for the number of sentences, <code class="reqn">C</code> for the number of
characters (usually meaning letters), <code class="reqn">Sy</code> for the number of syllables,
<code class="reqn">W_{3Sy}</code> for the number of words with at least three syllables,
<code class="reqn">W_{&lt;3Sy}</code> for the number of words with less than three syllables, <code class="reqn">W^{1Sy}</code>
for words with exactly one syllable,
<code class="reqn">W_{6C}</code> for the number of words with at least six letters, and <code class="reqn">W_{-WL}</code> for the number
of words which are not on a certain word list (explained where needed).
</p>

<dl>
<dt><code>"ARI"</code>:</dt><dd><p><em>Automated Readability Index</em>:
</p>
<p style="text-align: center;"><code class="reqn">ARI = 0.5 \times \frac{W}{St} + 4.71 \times \frac{C}{W} - 21.43</code>
</p>

<p>If <code>parameters</code> is set to <code>ARI="NRI"</code>,
the revised parameters from the Navy Readability Indexes are used:
</p>
<p style="text-align: center;"><code class="reqn">ARI_{NRI} = 0.4 \times \frac{W}{St} + 6 \times \frac{C}{W} - 27.4</code>
</p>

<p>If <code>parameters</code> is set to <code>ARI="simple"</code>,
the simplified formula is calculated:
</p>
<p style="text-align: center;"><code class="reqn">ARI_{simple} = \frac{W}{St} + 9 \times \frac{C}{W}</code>
</p>

<p>Wrapper function: <code><a href="#topic+ARI">ARI</a></code>
</p>
</dd>
<dt><code>"Bormuth"</code>:</dt><dd><p><em>Bormuth Mean Cloze</em> &amp; Grade Placement:
</p>
<p style="text-align: center;"><code class="reqn">
     B_{MC} = 0.886593 - \left( 0.08364 \times \frac{C}{W} \right) +  0.161911 \times \left(\frac{W_{-WL}}{W} \right)^3
     </code>
</p>

<p style="text-align: center;"><code class="reqn">
      - 0.21401 \times \left(\frac{W}{St} \right) + 0.000577 \times \left(\frac{W}{St} \right)^2
     </code>
</p>

<p style="text-align: center;"><code class="reqn">
      - 0.000005 \times \left(\frac{W}{St} \right)^3
     </code>
</p>

<p><strong>Note:</strong> This index needs the long Dale-Chall list of 3000 familiar (english) words to compute <code class="reqn">W_{-WL}</code>. That is,
you must have a copy of
this word list and provide it via the <code>word.lists=list(Bormuth=&lt;your.list&gt;)</code> parameter!
</p>
<p style="text-align: center;"><code class="reqn">
     B_{GP} = 4.275 + 12.881 \times B_{MC} - (34.934 \times B_{MC}^2) + (20.388 \times B_{MC}^3)
     </code>
</p>

<p style="text-align: center;"><code class="reqn">
      + (26.194C - 2.046 C_{CS}^2) - (11.767 C_{CS}^3) - (44.285 \times B_{MC} \times C_{CS})
     </code>
</p>

<p style="text-align: center;"><code class="reqn">
      + (97.620 \times (B_{MC} \times C_{CS})^2) - (59.538 \times (B_{MC} \times C_{CS})^3)</code>
</p>

<p>Where <code class="reqn">C_{CS}</code> represents the cloze criterion score (35% by default).
</p>
<p>Wrapper function: <code><a href="#topic+bormuth">bormuth</a></code>
</p>
</dd>
<dt><code>"Coleman"</code>:</dt><dd><p><em>Coleman's Readability Formulas</em>:
</p>
<p style="text-align: center;"><code class="reqn">C_1 = 1.29 \times \left( \frac{100 \times W^{1Sy}}{W} \right) - 38.45</code>
</p>

<p style="text-align: center;"><code class="reqn">C_2 = 1.16 \times \left( \frac{100 \times W^{1Sy}}{W} \right) + 1.48 \times \left( \frac{100 \times St}{W} \right) - 37.95</code>
</p>

<p style="text-align: center;"><code class="reqn">C_3 = 1.07 \times \left( \frac{100 \times W^{1Sy}}{W} \right) + 1.18 \times \left( \frac{100 \times St}{W} \right)
       + 0.76 \times \left( \frac{100 \times W_{pron}}{W} \right) - 34.02</code>
</p>

<p style="text-align: center;"><code class="reqn">C_4 = 1.04 \times \left( \frac{100 \times W^{1Sy}}{W} \right) + 1.06 \times \left( \frac{100 \times St}{W} \right) \\
       + 0.56 \times \left( \frac{100 \times W_{pron}}{W} \right) - 0.36  \times \left( \frac{100 \times W_{prep}}{W} \right) - 26.01</code>
</p>

<p>Where <code class="reqn">W_{pron}</code> is the number of pronouns,
and <code class="reqn">W_{prep}</code> the number of prepositions.
</p>
<p>Wrapper function: <code><a href="#topic+coleman">coleman</a></code>
</p>
</dd>
<dt><code>"Coleman.Liau"</code>:</dt><dd><p>First estimates cloze percentage,
then calculates grade equivalent:
</p>
<p style="text-align: center;"><code class="reqn">CL_{ECP} = 141.8401 - 0.214590 \times \frac{100 \times C}{W} + 1.079812 \times \frac{100 \times St}{W}</code>
</p>

<p style="text-align: center;"><code class="reqn">CL_{grade} = -27.4004 \times \frac{CL_{ECP}}{100} + 23.06395</code>
</p>

<p>The short form is also calculated:
</p>
<p style="text-align: center;"><code class="reqn">CL_{short} = 5.88 \times \frac{C}{W} - 29.6 \times \frac{St}{W} - 15.8</code>
</p>

<p>Wrapper function: <code><a href="#topic+coleman.liau">coleman.liau</a></code>
</p>
</dd>
<dt><code>"Dale.Chall"</code>:</dt><dd><p><em>New Dale-Chall Readability Formula</em>. By default the revised formula (1995) is calculated:
</p>
<p style="text-align: center;"><code class="reqn">DC_{new} = 64 - 0.95 \times{} \frac{100 \times{} W_{-WL}}{W} - 0.69 \times{} \frac{W}{St} </code>
</p>

<p>This will result in a cloze score which is then looked up in a grading table. If <code>parameters</code> is set to <code>Dale.Chall="old"</code>,
the original formula (1948) is used:
</p>
<p style="text-align: center;"><code class="reqn">DC_{old} = 0.1579 \times{} \frac{100 \times{} W_{-WL}}{W} + 0.0496 \times{} \frac{W}{St} + 3.6365 </code>
</p>

<p>If <code>parameters</code> is set to <code>Dale.Chall="PSK"</code>,
the revised parameters by Powers-Sumner-Kearl (1958) are used:
</p>
<p style="text-align: center;"><code class="reqn">DC_{PSK} =  0.1155 \times{} \frac{100 \times{} W_{-WL}}{W} + 0.0596  \times{} \frac{W}{St} + 3.2672 </code>
</p>

<p><strong>Note:</strong> This index needs the long Dale-Chall list of 3000 familiar (english) words to compute <code class="reqn">W_{-WL}</code>. That is,
you must have a copy of
this word list and provide it via the <code>word.lists=list(Dale.Chall=&lt;your.list&gt;)</code> parameter!
</p>
<p>Wrapper function: <code><a href="#topic+dale.chall">dale.chall</a></code>
</p>
</dd>
<dt><code>"Danielson.Bryan"</code>:</dt><dd>
<p style="text-align: center;"><code class="reqn">DB_1 = \left( 1.0364 \times \frac{C}{Bl} \right) + \left( 0.0194 \times \frac{C}{St} \right) - 0.6059</code>
</p>

<p style="text-align: center;"><code class="reqn">DB_2 = 131.059 - \left( 10.364 \times \frac{C}{Bl} \right) - \left( 0.194 \times \frac{C}{St} \right)</code>
</p>

<p>Where <code class="reqn">Bl</code> means blanks between words,
which is not really counted in this implementation, but estimated
by <code class="reqn">words - 1</code>. <code class="reqn">C</code> is interpreted as literally all characters.
</p>
<p>Wrapper function: <code><a href="#topic+danielson.bryan">danielson.bryan</a></code>
</p>
</dd>
<dt><code>"Dickes.Steiwer"</code>:</dt><dd><p><em>Dickes-Steiwer Handformel</em>:
</p>
<p style="text-align: center;"><code class="reqn">DS = 235.95993 - \left( 73.021 \times \frac{C}{W} \right) - \left(12.56438 \times \frac{W}{St} \right) - \left(50.03293 \times TTR \right)</code>
</p>

<p>Where <code class="reqn">TTR</code> refers to the type-token ratio,
which will be calculated case-insensitive by default.
</p>
<p>Wrapper function: <code><a href="#topic+dickes.steiwer">dickes.steiwer</a></code>
</p>
</dd>
<dt><code>"DRP"</code>:</dt><dd><p><em>Degrees of Reading Power</em>. Uses the Bormuth Mean Cloze Score:
</p>
<p style="text-align: center;"><code class="reqn">DRP = (1 - B_{MC}) \times 100</code>
</p>

<p>This formula itself has no parameters.
<strong>Note:</strong> The Bormuth index needs the long Dale-Chall list of 3000 familiar (english) words to compute <code class="reqn">W_{-WL}</code>.
That is,
you must have a copy of this word list and provide it via the <code>word.lists=list(Bormuth=&lt;your.list&gt;)</code> parameter!
Wrapper function: <code><a href="#topic+DRP">DRP</a></code>
</p>
</dd>
<dt><code>"ELF"</code>:</dt><dd><p>Fang's <em>Easy Listening Formula</em>:
</p>
<p style="text-align: center;"><code class="reqn">ELF = \frac{W_{2Sy}}{St}</code>
</p>

<p>Wrapper function: <code><a href="#topic+ELF">ELF</a></code>
</p>
</dd>
<dt><code>"Farr.Jenkins.Paterson"</code>:</dt><dd><p>A simplified version of Flesch Reading Ease:
</p>
<p style="text-align: center;"><code class="reqn">FJP = -31.517 - 1.015 \times \frac{W}{St} + 1.599 \times \frac{W^{1Sy}}{W}</code>
</p>

<p>If <code>parameters</code> is set to <code>Farr.Jenkins.Paterson="PSK"</code>,
the revised parameters by Powers-Sumner-Kearl (1958) are used:
</p>
<p style="text-align: center;"><code class="reqn">FJP_{PSK} = 8.4335 + 0.0923 \times \frac{W}{St} - 0.0648 \times \frac{W^{1Sy}}{W}</code>
</p>

<p>Wrapper function: <code><a href="#topic+farr.jenkins.paterson">farr.jenkins.paterson</a></code>
</p>
</dd>
<dt><code>"Flesch"</code>:</dt><dd><p><em>Flesch Reading Ease</em>:
</p>
<p style="text-align: center;"><code class="reqn">F_{EN} = 206.835 - 1.015 \times \frac{W}{St} - 84.6 \times \frac{Sy}{W}</code>
</p>

<p>Certain internationalisations of the parameters are also implemented. They can be used by setting
the <code>Flesch</code> parameter to one of the following language abbreviations.
</p>
<p><code>"de"</code> (Amstad's Verständlichkeitsindex):
</p>
<p style="text-align: center;"><code class="reqn">F_{DE} = 180 - \frac{W}{St} - 58.5 \times \frac{Sy}{W}</code>
</p>

<p><code>"es"</code> (Fernandez-Huerta):
</p>
<p style="text-align: center;"><code class="reqn">F_{ES} = 206.835 - 1.02 \times \frac{W}{St} - 60 \times \frac{Sy}{W}</code>
</p>

<p><code>"es-s"</code> (Szigriszt):
</p>
<p style="text-align: center;"><code class="reqn">F_{ES S} = 206.835 - \frac{W}{St} - 62.3 \times \frac{Sy}{W}</code>
</p>

<p><code>"nl"</code> (Douma):
</p>
<p style="text-align: center;"><code class="reqn">F_{NL} = 206.835 - 0.93 \times \frac{W}{St} - 77 \times \frac{Sy}{W}</code>
</p>

<p><code>"nl-b"</code> (Brouwer Leesindex):
</p>
<p style="text-align: center;"><code class="reqn">F_{NL B} = 195 - 2 \times \frac{W}{St} - 67 \times \frac{Sy}{W}</code>
</p>

<p><code>"fr"</code> (Kandel-Moles):
</p>
<p style="text-align: center;"><code class="reqn">F_{FR} = 209 - 1.15 \times \frac{W}{St} - 68 \times \frac{Sy}{W}</code>
</p>

<p>If <code>parameters</code> is set to <code>Flesch="PSK"</code>,
the revised parameters by Powers-Sumner-Kearl (1958) are used
to calculate a grade level:
</p>
<p style="text-align: center;"><code class="reqn">F_{PSK} = 0.0778 \times \frac{W}{St} + 4.55 \times \frac{Sy}{W} - 2.2029</code>
</p>

<p>Wrapper function: <code><a href="#topic+flesch">flesch</a></code>
</p>
</dd>
<dt><code>"Flesch.Kincaid"</code>:</dt><dd><p><em>Flesch-Kincaid Grade Level</em>:
</p>
<p style="text-align: center;"><code class="reqn">FK = 0.39 \times \frac{W}{St} + 11.8 \times \frac{Sy}{W} - 15.59</code>
</p>

<p>Wrapper function: <code><a href="#topic+flesch.kincaid">flesch.kincaid</a></code>
</p>
</dd>
<dt><code>"FOG"</code>:</dt><dd><p>Gunning <em>Frequency of Gobbledygook</em>:
</p>
<p style="text-align: center;"><code class="reqn">FOG = 0.4 \times \left( \frac{W}{St} + \frac{100 \times W_{3Sy}}{W} \right)</code>
</p>

<p>If <code>parameters</code> is set to <code>FOG="PSK"</code>,
the revised parameters by Powers-Sumner-Kearl (1958) are used:
</p>
<p style="text-align: center;"><code class="reqn">FOG_{PSK} = 3.0680 + \left( 0.0877 \times \frac{W}{St} \right) + \left(0.0984 \times \frac{100 \times W_{3Sy}}{W} \right)</code>
</p>

<p>If <code>parameters</code> is set to <code>FOG="NRI"</code>,
the new FOG count from the Navy Readability Indexes is used:
</p>
<p style="text-align: center;"><code class="reqn">FOG_{new} = \frac{\frac{W_{&lt;3Sy} + (3 * W_{3Sy})}{\frac{100 \times St}{W}} - 3}{2}</code>
</p>

<p>If the text was POS-tagged accordingly,
proper nouns and combinations of only easy words will not be counted as hard words,
and the syllables of verbs ending in &quot;-ed&quot;,
&quot;-es&quot; or &quot;-ing&quot; will be counted without these suffixes.
</p>
<p>Due to the need to re-hyphenate combined words after splitting them up,
this formula takes considerably longer to compute than most others.
If will be omitted if you set <code>index="fast"</code> instead of the default.
</p>
<p>Wrapper function: <code><a href="#topic+FOG">FOG</a></code>
</p>
</dd>
<dt><code>"FORCAST"</code>:</dt><dd>
<p style="text-align: center;"><code class="reqn">FORCAST = 20 - \frac{W^{1Sy} \times \frac{150}{W}}{10}</code>
</p>

<p>If <code>parameters</code> is set to <code>FORCAST="RGL"</code>,
the parameters for the precise reading grade level are used (see Klare, 1975, pp. 84&ndash;85):
</p>
<p style="text-align: center;"><code class="reqn">FORCAST_{RGL} = 20.43 - 0.11 \times W^{1Sy} \times \frac{150}{W}</code>
</p>

<p>Wrapper function: <code><a href="#topic+FORCAST">FORCAST</a></code>
</p>
</dd>
<dt><code>"Fucks"</code>:</dt><dd><p>Fucks' <em>Stilcharakteristik</em> (Fucks, 1955,
as cited in Briest, 1974):
</p>
<p style="text-align: center;"><code class="reqn">Fucks = \frac{Sy}{W} \times \frac{W}{St}</code>
</p>

<p>This simple formula has no parameters.
</p>
<p>Wrapper function: <code><a href="#topic+fucks">fucks</a></code>
</p>
</dd>
<dt><code>"Gutierrez"</code>:</dt><dd><p>Gutiérrez de Polini's <em>Fórmula de comprensibilidad</em> (Gutiérrez,
1972, as cited in Fernández, 2016)
for Spanish:
</p>
<p style="text-align: center;"><code class="reqn">Gutierrez = 95.2 - \frac{9.7 \times C}{W} - \frac{0.35 \times W}{St}</code>
</p>

<p>Wrapper function: <code><a href="#topic+gutierrez">gutierrez</a></code>
</p>
</dd>
<dt><code>"Harris.Jacobson"</code>:</dt><dd><p><em>Revised Harris-Jacobson Readability Formulas</em> (Harris &amp; Jacobson,
1974):
For primary-grade material:
</p>
<p style="text-align: center;"><code class="reqn">HJ_1 = 0.094 \times \frac{100 \times{} W_{-WL}}{W} + 0.168 \times \frac{W}{St} + 0.502</code>
</p>

<p>For material above third grade:
</p>
<p style="text-align: center;"><code class="reqn">HJ_2 = 0.140 \times \frac{100 \times{} W_{-WL}}{W} + 0.153 \times \frac{W}{St} + 0.560</code>
</p>

<p>For material below forth grade:
</p>
<p style="text-align: center;"><code class="reqn">HJ_3 = 0.158 \times \frac{W}{St} + 0.055 \times \frac{100 \times{} W_{6C}}{W} + 0.355</code>
</p>

<p>For material below forth grade:
</p>
<p style="text-align: center;"><code class="reqn">HJ_4 = 0.070 \times \frac{100 \times{} W_{-WL}}{W} + 0.125 \times \frac{W}{St} + 0.037 \times \frac{100 \times{} W_{6C}}{W} + 0.497</code>
</p>

<p>For material above third grade:
</p>
<p style="text-align: center;"><code class="reqn">HJ_5 = 0.118 \times \frac{100 \times{} W_{-WL}}{W} + 0.134 \times \frac{W}{St} + 0.032 \times \frac{100 \times{} W_{6C}}{W} + 0.424</code>
</p>

<p><strong>Note:</strong> This index needs the short Harris-Jacobson word list for grades 1 and 2 (english) to compute <code class="reqn">W_{-WL}</code>. That is,
you must have a copy of
this word list and provide it via the <code>word.lists=list(Harris.Jacobson=&lt;your.list&gt;)</code> parameter!
</p>
<p>Wrapper function: <code><a href="#topic+harris.jacobson">harris.jacobson</a></code>
</p>
</dd>
<dt><code>"Linsear.Write"</code> (O'Hayre, undated, see Klare, 1975, p. 85):</dt><dd>
<p style="text-align: center;"><code class="reqn">LW_{raw} = \frac{100 - \frac{100 \times W_{&lt;3Sy}}{W} + \left( 3 \times \frac{100 \times W_{3Sy}}{W} \right)}{\frac{100 \times St}{W}} </code>
</p>

<p style="text-align: center;"><code class="reqn">LW(LW_{raw} \leq 20) = \frac{LW_{raw} - 2}{2}</code>
</p>

<p style="text-align: center;"><code class="reqn">LW(LW_{raw} &gt; 20) = \frac{LW_{raw}}{2}</code>
</p>

<p>Wrapper function: <code><a href="#topic+linsear.write">linsear.write</a></code>
</p>
</dd>
<dt><code>"LIX"</code></dt><dd><p>Björnsson's <em>Läsbarhetsindex</em>. Originally proposed for Swedish texts,
calculated by:
</p>
<p style="text-align: center;"><code class="reqn">LIX = \frac{W}{St} + \frac{100 \times{} W_{7C}}{W}</code>
</p>

<p>Texts with a LIX &lt; 25 are considered very easy, around 40 normal,
and &gt; 55 very difficult to read.
</p>
<p>Wrapper function: <code><a href="#topic+LIX">LIX</a></code>
</p>
</dd>
<dt><code>"nWS"</code>:</dt><dd><p><em>Neue Wiener Sachtextformeln</em> (Bamberger &amp; Vanecek, 1984):
</p>
<p style="text-align: center;"><code class="reqn">nWS_1 = 19.35 \times \frac{W_{3Sy}}{W} + 0.1672 \times \frac{W}{St} + 12.97 \times \frac{W_{6C}}{W} - 3.27 \times \frac{W^{1Sy}}{W} - 0.875</code>
</p>

<p style="text-align: center;"><code class="reqn">nWS_2 = 20.07 \times \frac{W_{3Sy}}{W} + 0.1682 \times \frac{W}{St} + 13.73 \times \frac{W_{6C}}{W} - 2.779</code>
</p>

<p style="text-align: center;"><code class="reqn">nWS_3 = 29.63 \times \frac{W_{3Sy}}{W} + 0.1905 \times \frac{W}{St} - 1.1144</code>
</p>

<p style="text-align: center;"><code class="reqn">nWS_4 = 27.44 \times \frac{W_{3Sy}}{W} + 0.2656 \times \frac{W}{St} - 1.693</code>
</p>

<p>Wrapper function: <code><a href="#topic+nWS">nWS</a></code>
</p>
</dd>
<dt><code>"RIX"</code></dt><dd><p>Anderson's <em>Readability Index</em>. A simplified version of LIX:
</p>
<p style="text-align: center;"><code class="reqn">RIX = \frac{W_{7C}}{St}</code>
</p>

<p>Texts with a RIX &lt; 1.8 are considered very easy, around 3.7 normal,
and &gt; 7.2 very difficult to read.
</p>
<p>Wrapper function: <code><a href="#topic+RIX">RIX</a></code>
</p>
</dd>
<dt><code>"SMOG"</code>:</dt><dd><p><em>Simple Measure of Gobbledygook</em>. By default calculates formula D by McLaughlin (1969):
</p>
<p style="text-align: center;"><code class="reqn">SMOG = 1.043 \times \sqrt{W_{3Sy} \times \frac{30}{St}} + 3.1291</code>
</p>

<p>If <code>parameters</code> is set to <code>SMOG="C"</code>, formula C will be calculated:
</p>
<p style="text-align: center;"><code class="reqn">SMOG_{C} = 0.9986 \times \sqrt{W_{3Sy} \times \frac{30}{St} + 5} + 2.8795</code>
</p>

<p>If <code>parameters</code> is set to <code>SMOG="simple"</code>, the simplified formula is used:
</p>
<p style="text-align: center;"><code class="reqn">SMOG_{simple} = \sqrt{W_{3Sy} \times \frac{30}{St}} + 3</code>
</p>

<p>If <code>parameters</code> is set to <code>SMOG="de"</code>,
the formula adapted to German texts (&quot;Qu&quot;, Bamberger &amp; Vanecek, 1984, p. 78) is used:
</p>
<p style="text-align: center;"><code class="reqn">SMOG_{de} = \sqrt{W_{3Sy} \times \frac{30}{St}} - 2</code>
</p>

<p>Wrapper function: <code><a href="#topic+SMOG">SMOG</a></code>
</p>
</dd>
<dt><code>"Spache"</code>:</dt><dd><p><em>Spache Revised Formula (1974)</em>:
</p>
<p style="text-align: center;"><code class="reqn">Spache = 0.121 \times \frac{W}{St} + 0.082 \times{} \frac{100 \times{} W_{-WL}}{W} + 0.659</code>
</p>

<p>If <code>parameters</code> is set to <code>Spache="old"</code>, the original parameters (Spache,
1953) are used:
</p>
<p style="text-align: center;"><code class="reqn">Spache_{old} = 0.141 \times \frac{W}{St} + 0.086 \times{} \frac{100 \times{} W_{-WL}}{W} + 0.839</code>
</p>

<p><strong>Note:</strong> The revised index needs the revised Spache word list (see Klare, 1975,
p. 73), and the old index the short Dale-Chall list of
769 familiar (english) words to compute <code class="reqn">W_{-WL}</code>. That is,
you must have a copy of this word list and provide it via the
<code>word.lists=list(Spache=&lt;your.list&gt;)</code> parameter!
</p>
<p>Wrapper function: <code><a href="#topic+spache">spache</a></code>
</p>
</dd>
<dt><code>"Strain"</code>:</dt><dd><p><em>Strain Index</em>. This index was proposed in [1]:
</p>
<p style="text-align: center;"><code class="reqn">S = Sy \times{} \frac{1}{St / 3} \times{} \frac{1}{10}</code>
</p>

<p>Wrapper function: <code><a href="#topic+strain">strain</a></code>
</p>
</dd>
<dt><code>"Traenkle.Bailer"</code>:</dt><dd><p><em>Tränkle-Bailer Formeln</em>. These two formulas were the result of a re-examination of the ones proposed
by Dickes-Steiwer. They try to avoid the usage of the type-token ratio,
which is dependent on text length (Tränkle &amp; Bailer, 1984):
</p>
<p style="text-align: center;"><code class="reqn">TB1 = 224.6814 - \left(79.8304 \times \frac{C}{W} \right) - \left(12.24032 \times \frac{W}{St} \right) - \left(1.292857 \times \frac{100 \times{} W_{prep}}{W} \right)</code>
</p>

<p style="text-align: center;"><code class="reqn">TB2 = 234.1063 - \left(96.11069 \times \frac{C}{W} \right) - \left(2.05444 \times \frac{100 \times{} W_{prep}}{W} \right) - \left(1.02805 \times \frac{100 \times{} W_{conj}}{W} \right)</code>
</p>

<p>Where <code class="reqn">W_{prep}</code> refers to the number of prepositions,
and <code class="reqn">W_{conj}</code> to the number of conjunctions.
</p>
<p>Wrapper function: <code><a href="#topic+traenkle.bailer">traenkle.bailer</a></code>
</p>
</dd>
<dt><code>"TRI"</code>:</dt><dd><p>Kuntzsch's <em>Text-Redundanz-Index</em>. Intended mainly for German newspaper comments.
</p>
<p style="text-align: center;"><code class="reqn">TRI = \left(0.449 \times W^{1Sy}\right) - \left(2.467 \times Ptn\right) - \left(0.937 \times Frg\right) - 14.417</code>
</p>

<p>Where <code class="reqn">Ptn</code> is the number of punctuation marks and <code class="reqn">Frg</code> the number of foreign words.
</p>
<p>Wrapper function: <code><a href="#topic+TRI">TRI</a></code>
</p>
</dd>
<dt><code>"Tuldava"</code>:</dt><dd><p>Tuldava's <em>Text Difficulty Formula</em>. Supposed to be rather independent of specific languages (Grzybek,
2010).
</p>
<p style="text-align: center;"><code class="reqn">TD = \frac{Sy}{W} \times ln\left( \frac{W}{St} \right)</code>
</p>

<p>Wrapper function: <code><a href="#topic+tuldava">tuldava</a></code>
</p>
</dd>
<dt><code>"Wheeler.Smith"</code>:</dt><dd><p>Intended for english texts in primary grades 1&ndash;4 (Wheeler &amp; Smith,
1954):
</p>
<p style="text-align: center;"><code class="reqn">WS = \frac{W}{St} \times \frac{10 \times{} W_{2Sy}}{W}</code>
</p>

<p>If <code>parameters</code> is set to <code>Wheeler.Smith="de"</code>,
the calculation stays the same, but grade placement
is done according to Bamberger &amp; Vanecek (1984), that is for german texts.
</p>
<p>Wrapper function: <code><a href="#topic+wheeler.smith">wheeler.smith</a></code>
</p>
</dd>
</dl>

<p>By default, if the text has to be tagged yet,
the language definition is queried by calling <code>get.kRp.env(lang=TRUE)</code> internally.
Or, if <code>txt</code> has already been tagged,
by default the language definition of that tagged object is read
and used. Set <code>force.lang=get.kRp.env(lang=TRUE)</code> or to any other valid value,
if you want to forcibly overwrite this
default behaviour,
and only then. See <code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code> for all supported languages.
</p>


<h3>Value</h3>

<p>Depending on <code>as.feature</code>,
either an object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>,
or an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> with the added feature <code>readability</code> containing it.
</p>


<h3>Note</h3>

<p>To get a printout of the default parameters like they're set if no other parameters are specified,
call <code>readability(parameters="dput")</code>.
In case you want to provide different parameters,
you must provide a complete set for an index, or special parameters that are
mentioned in the index descriptions above (e.g., &quot;PSK&quot;, if appropriate).
</p>


<h3>References</h3>

<p>Anderson,
J. (1981). Analysing the readability of english and non-english texts in the classroom with Lix. In
<em>Annual Meeting of the Australian Reading Association</em>, Darwin, Australia.
</p>
<p>Anderson,
J. (1983). Lix and Rix: Variations on a little-known readability index. <em>Journal of Reading</em>, 26(6), 490&ndash;496.
</p>
<p>Bamberger, R. &amp; Vanecek,
E. (1984). <em>Lesen&ndash;Verstehen&ndash;Lernen&ndash;Schreiben</em>. Wien: Jugend und Volk.
</p>
<p>Briest, W. (1974). Kann man Verständlichkeit messen? <em>Zeitschrift für Phonetik,
Sprachwissenschaft und Kommunikationsforschung</em>, 27, 543&ndash;563.
</p>
<p>Coleman, M. &amp; Liau,
T.L. (1975). A computer readability formula designed for machine scoring, <em>Journal of Applied Psychology</em>, 60(2), 283&ndash;284.
</p>
<p>Dickes, P. &amp; Steiwer,
L. (1977). Ausarbeitung von Lesbarkeitsformeln für die deutsche Sprache.
<em>Zeitschrift für Entwicklungspsychologie und Pädagogische Psychologie</em>, 9(1),
20&ndash;28.
</p>
<p>DuBay,
W.H. (2004). <em>The Principles of Readability</em>. Costa Mesa: Impact Information.
WWW: <a href="http://www.impact-information.com/impactinfo/readability02.pdf">http://www.impact-information.com/impactinfo/readability02.pdf</a>; 22.03.2011.
</p>
<p>Farr, J.N., Jenkins, J.J. &amp; Paterson,
D.G. (1951). Simplification of Flesch Reading Ease formula. <em>Journal of Applied Psychology</em>, 35(5), 333&ndash;337.
</p>
<p>Fernández, A. M. (2016,
November 30). <em>Fórmula de comprensibilidad de Gutiérrez de Polini</em>.
<a href="https://legible.es/blog/comprensibilidad-gutierrez-de-polini/">https://legible.es/blog/comprensibilidad-gutierrez-de-polini/</a>
</p>
<p>Flesch, R. (1948). A new readability yardstick. <em>Journal of Applied Psychology</em>,
32(3), 221&ndash;233.
</p>
<p>Grzybek, P. (2010). Text difficulty and the Arens-Altmann law. In Peter Grzybek,
Emmerich Kelih, Ján Mačutek (Eds.),
<em>Text and Language. Structures &ndash; Functions &ndash; Interrelations. Quantitative Perspectives</em>. Wien: Praesens,
57&ndash;70.
</p>
<p>Harris, A.J. &amp; Jacobson,
M.D. (1974). Revised Harris-Jacobson readability formulas. In <em>18th Annual Meeting of the College Reading Association</em>, Bethesda.
</p>
<p>Klare, G.R. (1975). Assessing readability. <em>Reading Research Quarterly</em>, 10(1),
62&ndash;102.
</p>
<p>McLaughlin,
G.H. (1969). SMOG grading &ndash; A new readability formula. <em>Journal of Reading</em>, 12(8), 639&ndash;646.
</p>
<p>Powers, R.D, Sumner, W.A, &amp; Kearl,
B.E. (1958). A recalculation of four adult readability formulas, <em>Journal of Educational Psychology</em>, 49(2), 99&ndash;105.
</p>
<p>Smith, E.A. &amp; Senter,
R.J. (1967). <em>Automated readability index</em>. AMRL-TR-66-22. Wright-Paterson AFB, Ohio: Aerospace Medical Division.
</p>
<p>Spache,
G. (1953). A new readability formula for primary-grade reading materials. <em>The Elementary School Journal</em>, 53, 410&ndash;413.
</p>
<p>Tränkle, U. &amp; Bailer,
H. (1984). Kreuzvalidierung und Neuberechnung von Lesbarkeitsformeln für die deutsche Sprache.
<em>Zeitschrift für Entwicklungspsychologie und Pädagogische Psychologie</em>, 16(3),
231&ndash;244.
</p>
<p>Wheeler, L.R. &amp; Smith,
E.H. (1954). A practical readability formula for the classroom teacher in the primary grades. <em>Elementary English</em>,
31, 397&ndash;399.
</p>
<p>[1] <a href="https://strainindex.wordpress.com/2007/09/25/hello-world/">https://strainindex.wordpress.com/2007/09/25/hello-world/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  # call readability() on a tokenized text
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  # if you call readability() without arguments,
  # you will get its results directly
  rdb.results &lt;- readability(tokenized.obj)

  # there are [ and [[ methods for these objects
  rdb.results[["ARI"]]

  # alternatively, you can also store those results as a
  # feature in the object itself
  tokenized.obj &lt;- readability(
    tokenized.obj,
    as.feature=TRUE
  )
  # results are now part of the object
  hasFeature(tokenized.obj)
  corpusReadability(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='readability.num'>Calculate readability</h2><span id='topic+readability.num'></span>

<h3>Description</h3>

<p>This function is a stripped down version of <code><a href="#topic+readability">readability</a></code>. It does not analyze text,
but directly takes the values used by the formulae to calculate the readability measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readability.num(
  txt.features = list(sentences = NULL, words = NULL, letters = c(all = 0, l5 = 0, l6 =
    0), syllables = c(all = 0, s1 = 0, s2 = 0), punct = NULL, all.chars = NULL,
    prepositions = NULL, conjunctions = NULL, pronouns = NULL, foreign = NULL, TTR =
    NULL, FOG.hard.words = NULL, Bormuth.NOL = NULL, Dale.Chall.NOL = NULL,
    Harris.Jacobson.NOL = NULL, Spache.NOL = NULL, lang = character()),
  index = c("ARI", "Bormuth", "Coleman", "Coleman.Liau", "Dale.Chall",
    "Danielson.Bryan", "Dickes.Steiwer", "DRP", "ELF", "Farr.Jenkins.Paterson", "Flesch",
    "Flesch.Kincaid", "FOG", "FORCAST", "Fucks", "Harris.Jacobson", "Linsear.Write",
    "LIX", "nWS", "RIX", "SMOG", "Spache", "Strain", "Traenkle.Bailer", "TRI", "Tuldava",
    "Wheeler.Smith"),
  parameters = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readability.num_+3A_txt.features">txt.features</code></td>
<td>
<p>A named list with statistical information on the text,
or an object of class <code>kRp.readability</code>
(only its <code>desc</code> slot will then be used). Valid values are:
</p>

<dl>
<dt><code>sentences</code>:</dt><dd><p>The number of sentences.</p>
</dd>
<dt><code>words</code>:</dt><dd><p>The number of words.</p>
</dd>
<dt><code>letters</code>:</dt><dd><p>A named vector providing the number of letters. Must contain a value called <code>"all"</code>,
the total number of letters, and several values called <code>"l&lt;digit&gt;"</code>,
giving the number of words
with <code>&lt;digit&gt;</code> letters. To calculate all implemented measures with default parameters,
you need
at least the values <code>"l5"</code> (words with five <em>or less</em> letters) and <code>"l6"</code> (words with six letters).</p>
</dd>
<dt><code>syllables</code>:</dt><dd><p>Similar to <code>letters</code>,
but providing the number of syllables.  Must contain a value called <code>"all"</code>,
the total number of syllables, and several values called <code>"s&lt;digit&gt;"</code>,
giving the number of words
with <code>&lt;digit&gt;</code> syllables. To calculate all implemented measures with default parameters,
you need
at least the values <code>"s1"</code> and <code>"s2"</code>.
Only needed to calculate measures which need syllable count (see <code><a href="#topic+readability">readability</a></code>).</p>
</dd>
<dt><code>punct</code>:</dt><dd><p>The number of punctuation characters. Only needed to calculate <code>"TRI"</code>.</p>
</dd>
<dt><code>all.chars</code>:</dt><dd><p>The number of all characters (including spaces). Only needed to calculate <code>Danielson.Bryan</code>.</p>
</dd>
<dt><code>prepositions</code>:</dt><dd><p>The number of prepositions. Only needed to calculate <code>"Coleman"</code> and <code>"Traenkle.Bailer"</code>.</p>
</dd>
<dt><code>conjunctions</code>:</dt><dd><p>The number of conjunctions. Only needed to calculate <code>"Traenkle.Bailer"</code>.</p>
</dd>
<dt><code>pronouns</code>:</dt><dd><p>The number of pronouns. Only needed to calculate <code>"Coleman"</code>.</p>
</dd>
<dt><code>foreign</code>:</dt><dd><p>The number of foreign words. Only needed to calculate <code>"TRI"</code>.</p>
</dd>
<dt><code>TTR</code>:</dt><dd><p>The type-token ratio. Only needed to calculate <code>"Dickes.Steiwer"</code>.</p>
</dd>
<dt><code>FOG.hard.words</code>:</dt><dd><p>The number of hard words,
counted according to FOG. Only needed to calculate <code>"FOG"</code>.</p>
</dd>
<dt><code>Bormuth.NOL</code>:</dt><dd><p>Number of words not on the Bormuth word list. Only needed to calculate <code>"Bormuth"</code>.</p>
</dd>
<dt><code>Dale.Chall.NOL</code>:</dt><dd><p>Number of words not on the Dale-Chall word list. Only needed to calculate <code>"Dale.Chall"</code>.</p>
</dd>
<dt><code>Harris.Jacobson.NOL</code>:</dt><dd><p>Number of words not on the Harris-Jacobson word list. Only needed to calculate <code>"Harris.Jacobson"</code>.</p>
</dd>
<dt><code>Spache.NOL</code>:</dt><dd><p>Number of words not on the Spache word list. Only needed to calculate <code>"Spache"</code>.</p>
</dd>
<dt><code>lang</code>:</dt><dd><p>A character string defining the language, if needed.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="readability.num_+3A_index">index</code></td>
<td>
<p>A character vector, indicating which indices should actually be computed.</p>
</td></tr>
<tr><td><code id="readability.num_+3A_parameters">parameters</code></td>
<td>
<p>A named list with magic numbers,
defining the relevant parameters for each index. If none are given,
the default values are used.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
test.features &lt;- list(
  sentences=18,
  words=556,
  letters=c(
    all=2918,
    l1=19,
    l2=92,
    l3=74,
    l4=80,
    l5=51,
    l6=49
  ),
  syllables=c(
    all=974,
    s1=316,
    s2=116
  ),
  punct=78,
  all.chars=3553,
  prepositions=74,
  conjunctions=18,
  pronouns=9,
  foreign=0,
  TTR=0.5269784,
  Bormuth.NOL=192,
  Dale.Chall.NOL=192,
  Harris.Jacobson.NOL=240,
  Spache.NOL=240,
  lang="en"
)

# should not calculate FOG, because FOG.hard.words is missing:
readability.num(test.features, index="all")

## End(Not run)
</code></pre>

<hr>
<h2 id='readTagged'>Import already tagged texts</h2><span id='topic+readTagged'></span><span id='topic+readTagged+2Cmatrix-method'></span><span id='topic+readTagged+2Cdata.frame-method'></span><span id='topic+readTagged+2CkRp.connection-method'></span><span id='topic+readTagged+2Ccharacter-method'></span>

<h3>Description</h3>

<p>This method can be used on text files or matrices containing already tagged text material,
e.g. the results of
TreeTagger[1].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readTagged(file, ...)

## S4 method for signature 'matrix'
readTagged(
  file,
  lang = "kRp.env",
  tagger = "TreeTagger",
  apply.sentc.end = TRUE,
  sentc.end = c(".", "!", "?", ";", ":"),
  stopwords = NULL,
  stemmer = NULL,
  rm.sgml = TRUE,
  doc_id = NA,
  add.desc = "kRp.env",
  mtx_cols = c(token = "token", tag = "tag", lemma = "lemma")
)

## S4 method for signature 'data.frame'
readTagged(
  file,
  lang = "kRp.env",
  tagger = "TreeTagger",
  apply.sentc.end = TRUE,
  sentc.end = c(".", "!", "?", ";", ":"),
  stopwords = NULL,
  stemmer = NULL,
  rm.sgml = TRUE,
  doc_id = NA,
  add.desc = "kRp.env",
  mtx_cols = c(token = "token", tag = "tag", lemma = "lemma")
)

## S4 method for signature 'kRp.connection'
readTagged(
  file,
  lang = "kRp.env",
  encoding = getOption("encoding"),
  tagger = "TreeTagger",
  apply.sentc.end = TRUE,
  sentc.end = c(".", "!", "?", ";", ":"),
  stopwords = NULL,
  stemmer = NULL,
  rm.sgml = TRUE,
  doc_id = NA,
  add.desc = "kRp.env"
)

## S4 method for signature 'character'
readTagged(
  file,
  lang = "kRp.env",
  encoding = getOption("encoding"),
  tagger = "TreeTagger",
  apply.sentc.end = TRUE,
  sentc.end = c(".", "!", "?", ";", ":"),
  stopwords = NULL,
  stemmer = NULL,
  rm.sgml = TRUE,
  doc_id = NA,
  add.desc = "kRp.env"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readTagged_+3A_file">file</code></td>
<td>
<p>Either a matrix, a connection or a character vector. If the latter,
that must be a valid path to a file,
containing the previously analyzed text. If it is a matrix,
it must contain three columns named &quot;token&quot;, &quot;tag&quot;, and &quot;lemma&quot;,
and except for these three columns all others are ignored.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_...">...</code></td>
<td>
<p>Additional options, currently unused.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_lang">lang</code></td>
<td>
<p>A character string naming the language of the analyzed corpus. See <code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>
for all supported languages.
If set to <code>"kRp.env"</code> this is got from <code><a href="#topic+get.kRp.env">get.kRp.env</a></code>.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_tagger">tagger</code></td>
<td>
<p>The software which was used to tokenize and tag the text. Currently,
&quot;TreeTagger&quot; and &quot;manual&quot; are the only
supported values. If &quot;manual&quot;,
you must also adjust the values of <code>mtx_cols</code> to define the columns to be imported.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_apply.sentc.end">apply.sentc.end</code></td>
<td>
<p>Logical,
whethter the tokens defined in <code>sentc.end</code> should be searched and set to a sentence ending tag.
You could call this a compatibility mode to make sure you get the results you would get if you called
<code><a href="#topic+treetag">treetag</a></code> on the original file.
If set to <code>FALSE</code>, the tags will be imported as they are.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_sentc.end">sentc.end</code></td>
<td>
<p>A character vector with tokens indicating a sentence ending. This adds to given results,
it doesn't replace them.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_stopwords">stopwords</code></td>
<td>
<p>A character vector to be used for stopword detection. Comparison is done in lower case. You can also simply set 
<code>stopwords=tm::stopwords("en")</code> to use the english stopwords provided by the <code>tm</code> package.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_stemmer">stemmer</code></td>
<td>
<p>A function or method to perform stemming. For instance,
you can set <code>stemmer=Snowball::SnowballStemmer</code> if you
have the <code>Snowball</code> package installed (or <code>SnowballC::wordStem</code>). As of now,
you cannot provide further arguments to
this function.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_rm.sgml">rm.sgml</code></td>
<td>
<p>Logical, whether SGML tags should be ignored and removed from output.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_doc_id">doc_id</code></td>
<td>
<p>Character string,
optional identifier of the particular document. Will be added to the <code>desc</code> slot.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_add.desc">add.desc</code></td>
<td>
<p>Logical. If <code>TRUE</code>,
the tag description (column <code>"desc"</code> of the data.frame) will be added directly
to the resulting object. If set to <code>"kRp.env"</code> this is fetched from <code><a href="#topic+get.kRp.env">get.kRp.env</a></code>. Only needed if <code>tag=TRUE</code>.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_mtx_cols">mtx_cols</code></td>
<td>
<p>Character vector with exactly three elements named &quot;token&quot;, &quot;tag&quot;,
and &quot;lemma&quot;,
the values of which must match the respective column names of the matrix provided via <code>file</code>.
It is possible to set <code>lemma=NA</code> if the tagged results only provide token and tag.
This argument is ignored unless <code>tagger="manual"</code> and data is provided as either a matrix or data frame.</p>
</td></tr>
<tr><td><code id="readTagged_+3A_encoding">encoding</code></td>
<td>
<p>A character string defining the character encoding of the input file,
like  <code>"Latin1"</code> or <code>"UTF-8"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the value of <code>lang</code> must match a valid language supported by <code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>.
It will also get stored in the resulting object and might be used by other functions at a later point.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>. If <code>debug=TRUE</code>,
prints internal variable settings and
attempts to return the original output if the TreeTagger system call in a matrix.
</p>


<h3>References</h3>

<p>Schmid, H. (1994). Probabilistic part-of-speec tagging using decision trees. In
<em>International Conference on New Methods in Language Processing</em>, Manchester, UK,
44&ndash;49.
</p>
<p>[1] <a href="https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/">https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+treetag">treetag</a></code>,
<code><a href="#topic+freq.analysis">freq.analysis</a></code>,
<code><a href="#topic+get.kRp.env">get.kRp.env</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # call method on a connection
  text_con &lt;- file("~/my.data/tagged_speech.txt", "r")
  tagged_results &lt;- readTagged(text_con, lang="en")
  close(text_con)

  # call it on the file directly
  tagged_results &lt;- readTagged("~/my.data/tagged_speech.txt", lang="en")
  
  # import the results of RDRPOSTagger, using the "manual" tagger feature
  sample_text &lt;- c("Dies ist ein kurzes Beispiel. Es ergibt wenig Sinn.")
  tagger &lt;- RDRPOSTagger::rdr_model(language="German", annotation="POS")
  tagged_rdr &lt;- RDRPOSTagger::rdr_pos(tagger, x=sample_text)
  tagged_results &lt;- readTagged(
    tagged_rdr,
    lang="de",
    tagger="manual",
    mtx_cols=c(token="token", tag="pos", lemma=NA)
  )

## End(Not run)
</code></pre>

<hr>
<h2 id='RIX'>Readability: Anderson's Readability Index (RIX)</h2><span id='topic+RIX'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RIX(txt.file, parameters = c(char = 6), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RIX_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="RIX_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="RIX_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the Readability Index (RIX) by Anderson,
which is a simplified version of the l\&quot;asbarhetsindex (LIX)
by Bj\&quot;ornsson. In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>Anderson,
J. (1981). Analysing the readability of english and non-english texts in the classroom with Lix. In
<em>Annual Meeting of the Australian Reading Association</em>, Darwin, Australia.
</p>
<p>Anderson,
J. (1983). Lix and Rix: Variations on a little-known readability index. <em>Journal of Reading</em>, 26(6), 490&ndash;496.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  RIX(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='S.ld'>Lexical diversity: Summer's S</h2><span id='topic+S.ld'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>S.ld(txt, char = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="S.ld_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="S.ld_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="S.ld_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates Summer's S. In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the S value,
and characteristics are
off by default.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
S.ld(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='segment.optimizer'>A function to optimize MSTTR segment sizes</h2><span id='topic+segment.optimizer'></span>

<h3>Description</h3>

<p>This function calculates an optimized segment size for <code><a href="#topic+MSTTR">MSTTR</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>segment.optimizer(txtlgth, segment = 100, range = 20, favour.min = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="segment.optimizer_+3A_txtlgth">txtlgth</code></td>
<td>
<p>Integer value, size of text in tokens.</p>
</td></tr>
<tr><td><code id="segment.optimizer_+3A_segment">segment</code></td>
<td>
<p>Integer value, start value of the segment size.</p>
</td></tr>
<tr><td><code id="segment.optimizer_+3A_range">range</code></td>
<td>
<p>Integer value,
range around <code>segment</code> to search for better fitting sizes.</p>
</td></tr>
<tr><td><code id="segment.optimizer_+3A_favour.min">favour.min</code></td>
<td>
<p>Logical,
whether as a last ressort smaller or larger segment sizes should be
prefered, if in doubt.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When calculating the mean segmental type-token ratio (MSTTR), tokens are divided into
segments of a given size and analyzed. If at the end text is left over which won't fill another
full segment, it is discarded,
i.e. information is lost. For interpretation it is debatable
which is worse: Dropping more or less actual token material,
or variance in segment size between
analyzed texts. If you'd prefer the latter, this function might prove helpful.
</p>
<p>Starting with a given text length, segment size and range to investigate,
<code>segment.optimizer</code>
iterates through possible segment values. It returns the segment size which would drop the fewest
tokens (zero, if you're lucky). Should more than one value fulfill this demand,
the one nearest to
the segment start value is taken. In cases,
where still two values are equally far away from the
start value,
it depends on the setting of <code>favour.min</code> if the smaller or larger segment size
is returned.
</p>


<h3>Value</h3>

<p>A numeric vector with two elements:
</p>
<table>
<tr><td><code>seg</code></td>
<td>
<p>The optimized segment size</p>
</td></tr>
<tr><td><code>drop</code></td>
<td>
<p>The number of tokens that would be dropped using this segment size</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+lex.div">lex.div</a></code>, <code><a href="#topic+MSTTR">MSTTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>segment.optimizer(2014, favour.min=FALSE)
</code></pre>

<hr>
<h2 id='set.kRp.env'>A function to set information on your koRpus environment</h2><span id='topic+set.kRp.env'></span>

<h3>Description</h3>

<p>The function <code>set.kRp.env</code> can be called before any of the analysing functions. It writes information
on your session environment regarding the koRpus package,
e.g. path to a local TreeTagger installation,
to your global <code><a href="base.html#topic+.Options">.Options</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set.kRp.env(..., validate = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set.kRp.env_+3A_...">...</code></td>
<td>
<p>Named parameters to set in the koRpus environment. Valid arguments are:
</p>

<dl>
<dt>TT.cmd</dt><dd><p> A character string pointing to the tagger command you want to use for basic text analysis,
or <code>"manual"</code> if
you want to set <code>TT.options</code> as well. Set to <code>"tokenize"</code> to use <code><a href="#topic+tokenize">tokenize</a></code>.</p>
</dd>
<dt>lang</dt><dd><p> A character string specifying a valid language.</p>
</dd>
<dt>TT.options</dt><dd><p> A list with arguments to be used as <code>TT.options</code> by <code><a href="#topic+treetag">treetag</a></code>.</p>
</dd>
<dt>hyph.cache.file</dt><dd><p> A character string specifying a path to a file to use for storing already hyphenated data,
used by
<code><a href="#topic+hyphen">hyphen</a></code>.</p>
</dd>
<dt>add.desc</dt><dd><p> A logical value,
whether tag descriptions should be added directly to tagged text objects.</p>
</dd>
</dl>

<p>To explicitly unset a value again, set it to an empty character string (e.g.,
<code>lang=""</code>).</p>
</td></tr>
<tr><td><code id="set.kRp.env_+3A_validate">validate</code></td>
<td>
<p>Logical,
if <code>TRUE</code> given paths will be checked for actual availablity, and the function will fail if files can't be found.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To get the current settings, the function <code><a href="#topic+get.kRp.env">get.kRp.env</a></code>
should be used. For the most part, <code>set.kRp.env</code> is a convenient wrapper for
<code><a href="base.html#topic+options">options</a></code>. To permanently set some defaults, you could also add
respective <code>options</code> calls to an <code><a href="base.html#topic+.Rprofile">.Rprofile</a></code> file.
</p>
<p>Note that you can also suppress the startup message informing about <code><a href="#topic+available.koRpus.lang">available.koRpus.lang</a></code>
and <code><a href="#topic+install.koRpus.lang">install.koRpus.lang</a></code> by adding <code>noStartupMessage=TRUE</code> to the options in <code>.Rprofile</code>.
</p>


<h3>Value</h3>

<p>Returns an invisible <code>NULL</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get.kRp.env">get.kRp.env</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.kRp.env(lang="en")
get.kRp.env(lang=TRUE)
## Not run: 
set.kRp.env(
  TT.cmd=file.path("~","bin","treetagger","cmd","tree-tagger-german"),
  lang="de"
)

# example for setting permanent default values in an .Rprofile file
options(
  koRpus=list(
    TT.cmd="manual",
    TT.options=list(
      path=file.path("~","bin","treetagger"),
      preset="de"),
    lang="de",
    noStartupMessage=TRUE
  )
)
# be aware that setting a permamnent default language without loading
# the respective language support package might trigger errors

## End(Not run)
</code></pre>

<hr>
<h2 id='set.lang.support'>Add support for new languages</h2><span id='topic+set.lang.support'></span>

<h3>Description</h3>

<p>You can use this function to add new languages to be used with <code>koRpus</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set.lang.support(target, value, merge = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set.lang.support_+3A_target">target</code></td>
<td>
<p>One of &quot;kRp.POS.tags&quot;, &quot;treetag&quot;, or &quot;hyphen&quot;,
depending on what support is to be added.</p>
</td></tr>
<tr><td><code id="set.lang.support_+3A_value">value</code></td>
<td>
<p>A named list that upholds exactly the structure defined here for its respective <code>target</code>.</p>
</td></tr>
<tr><td><code id="set.lang.support_+3A_merge">merge</code></td>
<td>
<p>Logical,
only relevant for the &quot;kRp.POS.tags&quot; target. This argument controls whether <code>value</code>
will completely replace an already present tagset definition,
or merge all given tags (i.e., replace 
single tags with an updated definition or add new tags).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Language support in this package is designed to be extended easily. You could call it modular,
although it's actually more &quot;environemntal&quot;, but nevermind.
</p>
<p>To add full new language support, say for Xyzedish,
you basically have to call this function
three times (or at least twice, see hyphen section below) with different targets.
If you would like to re-use this language support,
you should consider making it a package.
</p>
<p>Be it a package or a script,
it should contain all three calls to this function. If it succeeds,
it will fill an internal environment with the information you have defined.
</p>
<p>The function <code>set.language.support()</code> gets called three times because there's three
functions of koRpus that need language support:
</p>

<ul>
<li><p> treetag() needs the preset information from its own start scripts
</p>
</li>
<li><p> kRp.POS.tags() needs to learn all possible POS tags that TreeTagger uses for the given
language
</p>
</li>
<li><p> hyphen() needs to know which language pattern tests are available as data files (which
you must provide also)
</p>
</li></ul>

<p>All the calls follow the same pattern &ndash; first,
you name one of the three targets explained above,
and second,
you provide a named list as the <code>value</code> for the respective <code>target</code> function.
</p>


<h3>&quot;treetag&quot;</h3>

<p>The presets for the treetag() function are basically what the shell (GNU/Linux,
MacOS) and batch
(Win) scripts define that come with TreeTagger. Look for scripts called
&quot;$TREETAGGER/cmd/tree-tagger-xyzedish&quot; and &quot;$TREETAGGER\cmd\tree-tagger-xyzedish.bat&quot;,
figure out which call resembles which call and then define them in set.lang.support(&quot;treetag&quot;)
accordingly.
</p>
<p>Have a look at the commented template in your <code>koRpus</code> installation directory for an elaborate
example.
</p>


<h3>&quot;kRp.POS.tags&quot;</h3>

<p>If Xyzedish is supported by TreeTagger,
you should find a tagset definition for the language on its
homepage. treetag() needs to know <em>all</em> POS tags that TreeTagger might return,
otherwise you
will get a self-explaining error message as soon as an unknown tag appears. Notice that this can
still happen after you implemented the full documented tag set: sometimes the contributed TreeTagger
parameter files added their own tags, e.g.,
for special punctuation. So please test your tag set well.
</p>
<p>As you can see in the template file,
you will also have to add a global word class and an explaination
for each tag. The former is especially important for further steps like frequency analysis.
</p>
<p>Again,
please have a look at the commented template and/or existing language support files in the
package sources, most of it should be almost self-explaining.
</p>


<h3>&quot;hyphen&quot;</h3>

<p>Using the target &quot;hyphen&quot; will cause a call to the equivalent of this function in the <code>sylly</code> package.
See the documentation of its <code><a href="sylly.html#topic+set.hyph.support">set.hyph.support</a></code> function for details.
</p>


<h3>Packaging</h3>

<p>If you would like to create a proper language support package,
you should only include the &quot;treetag&quot; and
&quot;kRp.POS.tags&quot; calls,
and the hyphenation patterns should be loaded as a dependency to a package called
<code>sylly.xx</code>. You can generate such a sylly package rather quickly by using the private function
<code>sylly:::sylly_langpack()</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>hyph_pat_yxz &lt;- sylly::kRp_hyph_pat(
  lang = "xy",
  pattern = matrix(
    c(
      ".im5b", ".in1", ".in3d",
      ".imb", ".in", ".ind",
      "0050", "001", "0030"
    ),
    nrow=3,
    dimnames= list(
      NULL,
      c("orig", "char", "nums")
    )
  )
)
set.lang.support(
  target="hyphen",
  value=list("xyz"=hyph_pat_yxz)
)
</code></pre>

<hr>
<h2 id='show+2CkRp.lang-method'>Show methods for koRpus objects</h2><span id='topic+show+2CkRp.lang-method'></span><span id='topic+show+2C-methods'></span><span id='topic+show+2CkRp.TTR-method'></span><span id='topic+show+2CkRp.corp.freq-method'></span><span id='topic+show+2CkRp.readability-method'></span><span id='topic+show+2CkRp.text-method'></span>

<h3>Description</h3>

<p>Show methods for S4 objects of classes
<code><a href="#topic+kRp.lang-class">kRp.lang</a></code>,
<code><a href="#topic+kRp.readability-class">kRp.readability</a></code>,
<code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code> or
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'kRp.lang'
show(object)

## S4 method for signature 'kRp.TTR'
show(object)

## S4 method for signature 'kRp.corp.freq'
show(object)

## S4 method for signature 'kRp.readability'
show(object)

## S4 method for signature 'kRp.text'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show+2B2CkRp.lang-method_+3A_object">object</code></td>
<td>
<p>An object of class <code>kRp.lang</code>, <code>kRp.readability</code>,
<code>kRp.corp.freq</code>, or <code>kRp.TTR</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.lang-class">kRp.lang</a></code>,
<code><a href="#topic+kRp.readability-class">kRp.readability</a></code>,
<code><a href="#topic+kRp.corp.freq-class">kRp.corp.freq</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  guess.lang("/home/user/data/some.txt", udhr.path="/home/user/data/udhr_txt/")

## End(Not run)
## Not run: 
MTLD(tagged.txt)

## End(Not run)
## Not run: 
flesch(tagged.txt)

## End(Not run)
</code></pre>

<hr>
<h2 id='SMOG'>Readability: Simple Measure of Gobbledygook (SMOG)</h2><span id='topic+SMOG'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SMOG(
  txt.file,
  hyphen = NULL,
  parameters = c(syll = 3, sqrt = 1.043, fact = 30, const = 3.1291, sqrt.const = 0),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SMOG_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="SMOG_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="SMOG_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="SMOG_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the Simple Measure of Gobbledygook (SMOG). In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>By default calculates formula D by McLaughlin (1969).
If <code>parameters</code> is set to <code>SMOG="C"</code>, formula C will be calculated.
If <code>parameters</code> is set to <code>SMOG="simple"</code>, the simplified formula is used, and
if <code>parameters="de"</code>, the formula adapted to German texts (&quot;Qu&quot;, Bamberger &amp; Vanecek,
1984, p. 78).
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>Bamberger, R. &amp; Vanecek,
E. (1984). <em>Lesen&ndash;Verstehen&ndash;Lernen&ndash;Schreiben</em>. Wien: Jugend und Volk.
</p>
<p>McLaughlin,
G.H. (1969). SMOG grading &ndash; A new readability formula. <em>Journal of Reading</em>, 12(8), 639&ndash;646.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
SMOG(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='spache'>Readability: Spache Formula</h2><span id='topic+spache'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spache(
  txt.file,
  word.list,
  parameters = c(asl = 0.121, dword = 0.082, const = 0.659),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spache_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="spache_+3A_word.list">word.list</code></td>
<td>
<p>A vector or matrix (with exactly one column) which defines familiar words. For valid results
the short Dale-Chall list with 769 easy words should be used.</p>
</td></tr>
<tr><td><code id="spache_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="spache_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Spache Formula. In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>By default the revised Spache formula is calculated. If <code>parameters="old"</code>,
the original
parameters are used.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
spache(tagged.text, word.list=spache.revised.wl)

## End(Not run)
</code></pre>

<hr>
<h2 id='split_by_doc_id'>Turn a multi-document kRp.text object into a list of kRp.text objects</h2><span id='topic+split_by_doc_id'></span><span id='topic+split_by_doc_id+2CkRp.text-method'></span><span id='topic+split_by_doc_id+2C-methods'></span><span id='topic+split_by_doc_id+2CkRp.corpus-method'></span>

<h3>Description</h3>

<p>For some analysis steps it might be important to have individual tagged texts
instead of one large corpus object. This method produces just that.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_by_doc_id(obj, keepFeatures = TRUE)

## S4 method for signature 'kRp.text'
split_by_doc_id(obj, keepFeatures = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="split_by_doc_id_+3A_obj">obj</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
<tr><td><code id="split_by_doc_id_+3A_keepfeatures">keepFeatures</code></td>
<td>
<p>Either logical, whether to keep all features or drop them,
or a character vector
of names of features to keep if present.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of objects of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.
Elements are named by their <code>doc_id</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
myCorpusList &lt;- split_by_doc_id(myCorpus)

## End(Not run)
</code></pre>

<hr>
<h2 id='strain'>Readability: Strain Index</h2><span id='topic+strain'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strain(txt.file, hyphen = NULL, parameters = c(sent = 3, const = 10), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strain_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="strain_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="strain_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="strain_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the Strain index. In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
strain(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='summary'>Summary methods for koRpus objects</h2><span id='topic+summary'></span><span id='topic+summary+2CkRp.lang-method'></span><span id='topic+summary+2CkRp.TTR-method'></span><span id='topic+summary+2CkRp.readability-method'></span><span id='topic+summary+2CkRp.text-method'></span>

<h3>Description</h3>

<p>Summary method for S4 objects of classes
<code><a href="#topic+kRp.lang-class">kRp.lang</a></code>,
<code><a href="#topic+kRp.readability-class">kRp.readability</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>, or
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summary(object, ...)

## S4 method for signature 'kRp.lang'
summary(object)

## S4 method for signature 'kRp.TTR'
summary(object, flat = FALSE)

## S4 method for signature 'kRp.readability'
summary(object, flat = FALSE)

## S4 method for signature 'kRp.text'
summary(object, index = NA, feature = NULL, flat = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary_+3A_object">object</code></td>
<td>
<p>An object of class, <code>kRp.lang</code>, <code>kRp.readability</code>, 
<code>kRp.text</code>, or <code>kRp.TTR</code>.</p>
</td></tr>
<tr><td><code id="summary_+3A_...">...</code></td>
<td>
<p>Further options, depending on the object class.</p>
</td></tr>
<tr><td><code id="summary_+3A_flat">flat</code></td>
<td>
<p>Logical, if <code>TRUE</code> and <code>feature="lex_div"</code> or <code>"readability"</code>,
a named vector of main
results is returned. For objects containig more than one <code>doc_id</code>,
defaults to <code>TRUE</code> automatically and
returns a data frame with named rows.</p>
</td></tr>
<tr><td><code id="summary_+3A_index">index</code></td>
<td>
<p>Either a vector indicating which rows should be considered as transformed for the statistics,
or the name of a particular transformation that was previously done to the object,
if more than one transformation was applied.
If <code>NA</code>, all rows where <code>"equal"</code> is <code>FALSE</code> are used.
Only valid for objects providing a <code>diff</code> feature.</p>
</td></tr>
<tr><td><code id="summary_+3A_feature">feature</code></td>
<td>
<p>A character string naming a feature present in the object,
to trigger a summary regarding that feature.
Currently only <code>"freq"</code>, <code>"lex_div"</code>, and <code>"readability"</code> are implemented.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.lang-class">kRp.lang</a></code>,
<code><a href="#topic+kRp.readability-class">kRp.readability</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
summary(guess.lang("/home/user/data/some.txt", udhr.path="/home/user/data/udhr_txt/"))

## End(Not run)
# code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  ld.results &lt;- lex.div(tokenized.obj, char=c())
  summary(ld.results)
  summary(ld.results, flat=TRUE)
} else {}
# code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  rdb.results &lt;- readability(tokenized.obj, index="fast")
  summary(rdb.results)
  summary(rdb.results, flat=TRUE)
} else {}
# code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  # this will look more useful when you
  # can use treetag() instead of tokenize()
  summary(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='taggedText'>Getter/setter methods for koRpus objects</h2><span id='topic+taggedText'></span><span id='topic+taggedText+2CkRp.text-method'></span><span id='topic+taggedText+2C-methods'></span><span id='topic+taggedText+3C-'></span><span id='topic+taggedText+3C-+2CkRp.text-method'></span><span id='topic+taggedText+3C-+2C-methods'></span><span id='topic+doc_id'></span><span id='topic+doc_id+2CkRp.text-method'></span><span id='topic+doc_id+2C-methods'></span><span id='topic+hasFeature'></span><span id='topic+hasFeature+2CkRp.text-method'></span><span id='topic+hasFeature+2C-methods'></span><span id='topic+hasFeature+3C-'></span><span id='topic+hasFeature+3C-+2CkRp.text-method'></span><span id='topic+hasFeature+3C-+2C-methods'></span><span id='topic+feature'></span><span id='topic+feature+2CkRp.text-method'></span><span id='topic+feature+2C-methods'></span><span id='topic+feature+3C-'></span><span id='topic+feature+3C-+2CkRp.text-method'></span><span id='topic+feature+3C-+2C-methods'></span><span id='topic+corpusReadability'></span><span id='topic+corpusReadability+2CkRp.text-method'></span><span id='topic+corpusReadability+2C-methods'></span><span id='topic+corpusReadability+3C-'></span><span id='topic+corpusReadability+3C-+2CkRp.text-method'></span><span id='topic+corpusReadability+3C-+2C-methods'></span><span id='topic+corpusHyphen'></span><span id='topic+corpusHyphen+2CkRp.text-method'></span><span id='topic+corpusHyphen+2C-methods'></span><span id='topic+corpusHyphen+3C-'></span><span id='topic+corpusHyphen+3C-+2CkRp.text-method'></span><span id='topic+corpusHyphen+3C-+2C-methods'></span><span id='topic+corpusLexDiv'></span><span id='topic+corpusLexDiv+2CkRp.text-method'></span><span id='topic+corpusLexDiv+2C-methods'></span><span id='topic+corpusLexDiv+3C-'></span><span id='topic+corpusLexDiv+3C-+2CkRp.text-method'></span><span id='topic+corpusLexDiv+3C-+2C-methods'></span><span id='topic+corpusFreq'></span><span id='topic+corpusFreq+2CkRp.text-method'></span><span id='topic+corpusFreq+2C-methods'></span><span id='topic+corpusFreq+3C-'></span><span id='topic+corpusFreq+3C-+2CkRp.text-method'></span><span id='topic+corpusFreq+3C-+2C-methods'></span><span id='topic+corpusCorpFreq'></span><span id='topic+corpusCorpFreq+2CkRp.text-method'></span><span id='topic+corpusCorpFreq+2C-methods'></span><span id='topic+corpusCorpFreq+3C-'></span><span id='topic+corpusCorpFreq+3C-+2CkRp.text-method'></span><span id='topic+corpusCorpFreq+3C-+2C-methods'></span><span id='topic+corpusStopwords'></span><span id='topic+corpusStopwords+2CkRp.text-method'></span><span id='topic+corpusStopwords+2C-methods'></span><span id='topic+corpusStopwords+3C-'></span><span id='topic+corpusStopwords+3C-+2CkRp.text-method'></span><span id='topic+corpusStopwords+3C-+2C-methods'></span><span id='topic++5B+2CkRp.text+2CANY+2CANY+2CANY-method'></span><span id='topic++5B+2C-methods'></span><span id='topic++5B+2CkRp.text+2CANY+2CANY-method'></span><span id='topic++5B+3C-+2CkRp.text+2CANY+2CANY+2CANY-method'></span><span id='topic++5B+3C-+2C-methods'></span><span id='topic++5B+5B+2CkRp.text-method'></span><span id='topic++5B+5B+2C-methods'></span><span id='topic++5B+5B+2CkRp.text+2CANY-method'></span><span id='topic++5B+5B+3C-+2CkRp.text-method'></span><span id='topic++5B+5B+3C-+2C-methods'></span><span id='topic++5B+5B+3C-+2CkRp.text+2CANY+2CANY-method'></span><span id='topic+describe+2CkRp.text-method'></span><span id='topic+describe+2C-methods'></span><span id='topic+describe+3C-+2CkRp.text-method'></span><span id='topic+describe+3C-+2C-methods'></span><span id='topic+language+2CkRp.text-method'></span><span id='topic+language+2C-methods'></span><span id='topic+language+3C-+2CkRp.text-method'></span><span id='topic+language+3C-+2C-methods'></span><span id='topic+diffText'></span><span id='topic+diffText+2CkRp.text-method'></span><span id='topic+diffText+2C-methods'></span><span id='topic+diffText+3C-'></span><span id='topic+diffText+3C-+2CkRp.text-method'></span><span id='topic+diffText+3C-+2C-methods'></span><span id='topic+originalText'></span><span id='topic+originalText+2CkRp.text-method'></span><span id='topic+originalText+2C-methods'></span><span id='topic+is.taggedText'></span><span id='topic+is.kRp.text'></span><span id='topic+fixObject'></span><span id='topic+fixObject+2CkRp.text-method'></span><span id='topic+fixObject+2C-methods'></span><span id='topic+tif_as_tokens_df'></span><span id='topic+tif_as_tokens_df+2CkRp.text-method'></span><span id='topic+tif_as_tokens_df+2C-methods'></span><span id='topic+fixObject+2CkRp.tagged-method'></span><span id='topic+fixObject+2CkRp.txt.freq-method'></span><span id='topic+fixObject+2CkRp.txt.trans-method'></span><span id='topic+fixObject+2CkRp.analysis-method'></span>

<h3>Description</h3>

<p>These methods should be used to get or set values of tagged text objects
generated by koRpus functions like <code><a href="#topic+treetag">treetag</a></code> or <code><a href="#topic+tokenize">tokenize</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>taggedText(obj, add.desc = FALSE, doc_id = FALSE)

## S4 method for signature 'kRp.text'
taggedText(obj, add.desc = FALSE, doc_id = FALSE)

taggedText(obj) &lt;- value

## S4 replacement method for signature 'kRp.text'
taggedText(obj) &lt;- value

doc_id(obj, ...)

## S4 method for signature 'kRp.text'
doc_id(obj, has_id = NULL)

hasFeature(obj, feature = NULL, ...)

## S4 method for signature 'kRp.text'
hasFeature(obj, feature = NULL)

hasFeature(obj, feature) &lt;- value

## S4 replacement method for signature 'kRp.text'
hasFeature(obj, feature) &lt;- value

feature(obj, feature, ...)

## S4 method for signature 'kRp.text'
feature(obj, feature, doc_id = NULL)

feature(obj, feature) &lt;- value

## S4 replacement method for signature 'kRp.text'
feature(obj, feature) &lt;- value

corpusReadability(obj, ...)

## S4 method for signature 'kRp.text'
corpusReadability(obj, doc_id = NULL)

corpusReadability(obj) &lt;- value

## S4 replacement method for signature 'kRp.text'
corpusReadability(obj) &lt;- value

corpusHyphen(obj, ...)

## S4 method for signature 'kRp.text'
corpusHyphen(obj, doc_id = NULL)

corpusHyphen(obj) &lt;- value

## S4 replacement method for signature 'kRp.text'
corpusHyphen(obj) &lt;- value

corpusLexDiv(obj, ...)

## S4 method for signature 'kRp.text'
corpusLexDiv(obj, doc_id = NULL)

corpusLexDiv(obj) &lt;- value

## S4 replacement method for signature 'kRp.text'
corpusLexDiv(obj) &lt;- value

corpusFreq(obj, ...)

## S4 method for signature 'kRp.text'
corpusFreq(obj)

corpusFreq(obj) &lt;- value

## S4 replacement method for signature 'kRp.text'
corpusFreq(obj) &lt;- value

corpusCorpFreq(obj, ...)

## S4 method for signature 'kRp.text'
corpusCorpFreq(obj)

corpusCorpFreq(obj) &lt;- value

## S4 replacement method for signature 'kRp.text'
corpusCorpFreq(obj) &lt;- value

corpusStopwords(obj, ...)

## S4 method for signature 'kRp.text'
corpusStopwords(obj)

corpusStopwords(obj) &lt;- value

## S4 replacement method for signature 'kRp.text'
corpusStopwords(obj) &lt;- value

## S4 method for signature 'kRp.text,ANY,ANY,ANY'
x[i, j, ..., drop = TRUE]

## S4 replacement method for signature 'kRp.text,ANY,ANY,ANY'
x[i, j, ...] &lt;- value

## S4 method for signature 'kRp.text'
x[[i, doc_id = NULL, ...]]

## S4 replacement method for signature 'kRp.text'
x[[i, doc_id = NULL, ...]] &lt;- value

## S4 method for signature 'kRp.text'
describe(obj, doc_id = NULL, simplify = TRUE, ...)

## S4 replacement method for signature 'kRp.text'
describe(obj, doc_id = NULL, ...) &lt;- value

## S4 method for signature 'kRp.text'
language(obj)

## S4 replacement method for signature 'kRp.text'
language(obj) &lt;- value

diffText(obj, doc_id = NULL)

## S4 method for signature 'kRp.text'
diffText(obj, doc_id = NULL)

diffText(obj) &lt;- value

## S4 replacement method for signature 'kRp.text'
diffText(obj) &lt;- value

originalText(obj)

## S4 method for signature 'kRp.text'
originalText(obj)

is.taggedText(obj)

is.kRp.text(obj)

fixObject(obj, doc_id = NA)

## S4 method for signature 'kRp.text'
fixObject(obj, doc_id = NA)

tif_as_tokens_df(tokens)

## S4 method for signature 'kRp.text'
tif_as_tokens_df(tokens)

## S4 method for signature 'kRp.tagged'
fixObject(obj, doc_id = NA)

## S4 method for signature 'kRp.txt.freq'
fixObject(obj, doc_id = NA)

## S4 method for signature 'kRp.txt.trans'
fixObject(obj, doc_id = NA)

## S4 method for signature 'kRp.analysis'
fixObject(obj, doc_id = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="taggedText_+3A_obj">obj</code></td>
<td>
<p>An arbitrary <code>R</code> object.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_add.desc">add.desc</code></td>
<td>
<p>Logical,
determines whether the <code>desc</code> column should be re-written with descriptions
for all POS tags.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_doc_id">doc_id</code></td>
<td>
<p>Logical (except for <code>fixObject</code>, <code>feature</code>, and <code>[[/[[&lt;-</code>),
if <code>TRUE</code> the <code>doc_id</code> column will be a factor with the respective value
of the <code>desc</code> slot, i.\,e.,
the document ID will be preserved in the data.frame. If used with <code>fixObject</code>, can be a character string
to set the document ID manually (the default <code>NA</code> will preserve existing values and not overwrite them). If used with <code>feature</code> or <code>[[/[[&lt;-</code>,
a character vector to limit the scope to one or more particular document IDs.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_value">value</code></td>
<td>
<p>The new value to replace the current with.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_...">...</code></td>
<td>
<p>Additional arguments for the generics.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_has_id">has_id</code></td>
<td>
<p>A character vector with <code>doc_id</code>s to look for in the object. The return value
is then a logical vector of the same length,
indicating if the respective id was found or not.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_feature">feature</code></td>
<td>
<p>Character string naming the feature to look for. The return value is logical if a single feature
name is given. If <code>feature=NULL</code>, a character vector is returned,
naming all features found in the object.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_x">x</code></td>
<td>
<p>An object of class <code>kRp.text</code> or <code>kRp.hyphen</code>.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_i">i</code></td>
<td>
<p>Defines the row selector (<code>[</code>) or the name to match (<code>[[</code>).</p>
</td></tr>
<tr><td><code id="taggedText_+3A_j">j</code></td>
<td>
<p>Defines the column selector.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_drop">drop</code></td>
<td>
<p>Logical,
whether the result should be coerced to the lowest possible dimension. See <code><a href="base.html#topic+Extract">[</a></code> for more details.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_simplify">simplify</code></td>
<td>
<p>Logical, if <code>TRUE</code> and the result is a list oft length one (i.e.,
just a single <code>doc_id</code>),
returns the contents of the single list entry.</p>
</td></tr>
<tr><td><code id="taggedText_+3A_tokens">tokens</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><code>taggedText()</code> returns the <code>tokens</code> slot.
</p>
</li>
<li><p><code>doc_id()</code> Returns a character vector of all <code>doc_id</code> values in the object.
</p>
</li>
<li><p><code>describe()</code> returns the <code>desc</code> slot.
</p>
</li>
<li><p><code>language()</code> returns the <code>lang</code> slot.
</p>
</li>
<li><p><code>[</code>/<code>[[</code> Can be used as a shortcut to index the results of <code>taggedText()</code>.
</p>
</li>
<li><p><code>fixObject</code> returns the same object upgraded to the object structure of this package version (e.g.,
new columns, changed names, etc.).
</p>
</li>
<li><p><code>hasFeature()</code> returns <code>TRUE</code> or codeFALSE,
depending on whether the requested feature is present or not.
</p>
</li>
<li><p><code>feature()</code> returns the list entry of the <code>feat_list</code> slot for the requested feature.
</p>
</li>
<li><p><code>corpusReadability()</code> returns the list of <code>kRp.readability</code> objects,
see <code><a href="#topic+readability">readability</a></code>.
</p>
</li>
<li><p><code>corpusHyphen()</code> returns the list of <code>kRp.hyphen</code> objects,
see <code><a href="#topic+hyphen">hyphen</a></code>.
</p>
</li>
<li><p><code>corpusLexDiv()</code> returns the list of <code>kRp.TTR</code> objects,
see <code><a href="#topic+lex.div">lex.div</a></code>.
</p>
</li>
<li><p><code>corpusFreq()</code> returns the frequency analysis data from the <code>feat_list</code> slot,
see <code><a href="#topic+freq.analysis">freq.analysis</a></code>.
</p>
</li>
<li><p><code>corpusCorpFreq()</code> returns the <code>kRp.corp.freq</code> object of the <code>feat_list</code> slot,
see for example <code><a href="#topic+read.corp.custom">read.corp.custom</a></code>.
</p>
</li>
<li><p><code>corpusStopwords()</code> returns the number of stopwords found in each text (if analyzed) from the <code>feat_list</code> slot.
</p>
</li>
<li><p><code>tif_as_tokens_df</code> returns the <code>tokens</code> slot in a TIF[1] compliant format,
i.e., <code>doc_id</code> is not a factor but a character vector.
</p>
</li>
<li><p><code>originalText()</code> similar to <code>taggedText()</code>,
but reverts any transformations back to the original text before returning the <code>tokens</code> slot.
Only works if the object has the feature <code>diff</code>, see examples.
</p>
</li>
<li><p><code>diffText()</code> returns the <code>diff</code> slot, if present.
</p>
</li></ul>



<h3>References</h3>

<p>[1] Text Interchange Formats (<a href="https://github.com/ropensci/tif">https://github.com/ropensci/tif</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )

  doc_id(tokenized.obj)

  describe(tokenized.obj)

  language(tokenized.obj)

  taggedText(tokenized.obj)
  tokenized.obj[["token"]]
  tokenized.obj[1:3, "token"]

  tif_as_tokens_df(tokenized.obj)

  # example for originalText()
  tokenized.obj &lt;- jumbleWords(tokenized.obj)
  # now compare the jumbled words to the original
  tokenized.obj[["token"]]
  originalText(tokenized.obj)[["token"]]
} else {}
</code></pre>

<hr>
<h2 id='textFeatures'>Extract text features for authorship analysis</h2><span id='topic+textFeatures'></span>

<h3>Description</h3>

<p>This function combines several of <code>koRpus</code>' methods to extract the 9-Feature Set for
authorship detection (Brannon, Afroz &amp; Greenstadt, 2011; Brannon &amp; Greenstadt, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textFeatures(text, hyphen = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textFeatures_+3A_text">text</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>. Can
also be a list of these objects, if you want to analyze more than one text at once.</p>
</td></tr>
<tr><td><code id="textFeatures_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class <code><a href="sylly.html#topic+kRp.hyphen-class">kRp.hyphen</a></code>,
if <code>text</code> has
already been hyphenated. If <code>text</code> is a list and <code>hyphen</code> is not <code>NULL</code>,
it must
also be a list with one object for each text, in the same order.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame:
</p>

<dl>
<dt>uniqWd</dt><dd><p>Number of unique words (tokens)</p>
</dd>
<dt>cmplx</dt><dd><p>Complexity (TTR)</p>
</dd>
<dt>sntCt</dt><dd><p>Sentence count</p>
</dd>
<dt>sntLen</dt><dd><p>Average sentence length</p>
</dd>
<dt>syllCt</dt><dd><p>Average syllable count</p>
</dd>
<dt>charCt</dt><dd><p>Character count (all characters, including spaces)</p>
</dd>
<dt>lttrCt</dt><dd><p>Letter count (without spaces, punctuation and digits)</p>
</dd>
<dt>FOG</dt><dd><p>Gunning FOG index</p>
</dd>
<dt>flesch</dt><dd><p>Flesch Reading Ease index</p>
</dd>
</dl>



<h3>References</h3>

<p>Brennan, M., Afroz, S., &amp; Greenstadt,
R. (2011). Deceiving authorship detection. Presentation
at <em>28th Chaos Communication Congress (28C3)</em>, Berlin, Germany.
Brennan, M. &amp; Greenstadt,
R. (2009). Practical Attacks Against Authorship Recognition Techniques. In
<em>Proceedings of the Twenty-First Conference on Innovative Applications of Artificial Intelligence (IAAI)</em>,
Pasadena, CA.
Tweedie, F.J., Singh, S., &amp; Holmes,
D.I. (1996). Neural Network Applications in Stylometry: The Federalist Papers.
<em>Computers and the Humanities</em>, 30, 1&ndash;10.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  textFeatures(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='textTransform'>Letter case transformation</h2><span id='topic+textTransform'></span><span id='topic+textTransform+2CkRp.text-method'></span>

<h3>Description</h3>

<p>Transforms text in koRpus objects token by token.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textTransform(txt, ...)

## S4 method for signature 'kRp.text'
textTransform(
  txt,
  scheme,
  p = 0.5,
  paste = FALSE,
  var = "wclass",
  query = "fullstop",
  method = "replace",
  replacement = ".",
  f = NA,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textTransform_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.</p>
</td></tr>
<tr><td><code id="textTransform_+3A_...">...</code></td>
<td>
<p>Parameters passed to <code><a href="#topic+query">query</a></code> to find matching tokens. Relevant only if <code>scheme="normalize"</code>.</p>
</td></tr>
<tr><td><code id="textTransform_+3A_scheme">scheme</code></td>
<td>
<p>One of the following character strings:
</p>

<ul>
<li> <p><code>"minor"</code> Start each word with a lowercase letter.
</p>
</li>
<li> <p><code>"all.minor"</code> Forces all letters into lowercase.
</p>
</li>
<li> <p><code>"major"</code> Start each word with a uppercase letter.
</p>
</li>
<li> <p><code>"all.major"</code> Forces all letters into uppercase.
</p>
</li>
<li> <p><code>"random"</code> Randomly start words with uppercase or lowercase letters.
</p>
</li>
<li> <p><code>"de.norm"</code> German norm: All names,
nouns and sentence beginnings start with an uppercase letter,
anything else with a lowercase letter.
</p>
</li>
<li> <p><code>"de.inv"</code> Inversion of <code>"de.norm"</code>.
</p>
</li>
<li> <p><code>"eu.norm"</code> Usual European cases: Only names and sentence beginnings start with an uppercase letter,
anything else with a lowercase letter.
</p>
</li>
<li> <p><code>"eu.inv"</code> Inversion of <code>"eu.norm"</code>.
</p>
</li>
<li> <p><code>"normalize"</code> Replace all tokens matching <code>query</code> in column <code>var</code> according to <code>method</code> (see below).
</p>
</li></ul>
</td></tr>
<tr><td><code id="textTransform_+3A_p">p</code></td>
<td>
<p>Numeric value between 0 and 1. Defines the probability for upper case letters (relevant only
if <code>scheme="random"</code>).</p>
</td></tr>
<tr><td><code id="textTransform_+3A_paste">paste</code></td>
<td>
<p>Logical, see value section.</p>
</td></tr>
<tr><td><code id="textTransform_+3A_var">var</code></td>
<td>
<p>A character string naming a variable in the object (i.e.,
colname). See <code><a href="#topic+query">query</a></code> for details.
Relevant only if <code>scheme="normalize"</code>.</p>
</td></tr>
<tr><td><code id="textTransform_+3A_query">query</code></td>
<td>
<p>A character vector (for words), regular expression,
or single number naming values to be matched in the variable.
See <code><a href="#topic+query">query</a></code> for details. Relevant only if <code>scheme="normalize"</code>.</p>
</td></tr>
<tr><td><code id="textTransform_+3A_method">method</code></td>
<td>
<p>One of the following character strings:
</p>

<ul>
<li> <p><code>"shortest"</code> Replace all matches with the shortest value found.
</p>
</li>
<li> <p><code>"longest"</code> Replace all matches with the longest value found.
</p>
</li>
<li> <p><code>"replace"</code> Replace all matches with the token given via <code>replacement</code>.
</p>
</li>
<li> <p><code>"function"</code> Replace all matches with the result of the function provided by <code>f</code> (see section Function for details).
</p>
</li></ul>

<p>In case of <code>"shortest"</code> and <code>"longest"</code>,
if multiple values of the same length are found, the (first) most prevalent one is being used.
The actual replacement value is documented in the <code>diff</code> slot of the object,
as a list called <code>transfmt.normalize</code>.
Relevant only if <code>scheme="normalize"</code>.</p>
</td></tr>
<tr><td><code id="textTransform_+3A_replacement">replacement</code></td>
<td>
<p>Character string defining the exact token to replace all query matches with.
Relevant only if <code>scheme="normalize"</code> and <code>method="replace"</code>.</p>
</td></tr>
<tr><td><code id="textTransform_+3A_f">f</code></td>
<td>
<p>A function to calculate the replacement for all query matches.
Relevant only if <code>scheme="normalize"</code> and <code>method="function"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method is mainly intended to produce text material for experiments.
</p>


<h3>Value</h3>

<p>By default an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> with the added feature <code>diff</code> is returned.
It provides a list with mostly atomic vectors,
describing the amount of diffences between both text variants (percentage):
</p>

<dl>
<dt><code>all.tokens</code>:</dt><dd><p>Percentage of all tokens, including punctuation,
that were altered.</p>
</dd>
<dt><code>words</code>:</dt><dd><p>Percentage of altered words only.</p>
</dd>
<dt><code>all.chars</code>:</dt><dd><p>Percentage of all characters, including punctuation,
that were altered.</p>
</dd>
<dt><code>letters</code>:</dt><dd><p>Percentage of altered letters in words only.</p>
</dd>
<dt><code>transfmt</code>:</dt><dd><p>Character vector documenting the transformation(s) done to the tokens.</p>
</dd>
<dt><code>transfmt.equal</code>:</dt><dd><p>Data frame documenting which token was changed in which transformational step. Only available if more than one transformation was done.</p>
</dd>
<dt><code>transfmt.normalize</code>:</dt><dd><p>A list documenting steps of normalization that were done to the object,
one element per transformation.
Each entry holds the name of the method, the query parameters,
and the effective replacement value.</p>
</dd>
</dl>

<p>If <code>paste=TRUE</code>,
returns an atomic character vector (via <code><a href="#topic+pasteText">pasteText</a></code>).
</p>


<h3>Function</h3>

<p>You can dynamically calculate the replacement value for the <code>"normalize"</code> scheme by setting <code>method="function"</code> and
providing a function object as <code>f</code>. The function you provide must support the following arguments:
</p>

<ul>
<li> <p><code>tokens</code> The original tokens slot of the <code>txt</code> object (see <code><a href="#topic+taggedText">taggedText</a></code>).
</p>
</li>
<li> <p><code>match</code> A logical vector,
indicating for each row of <code>tokens</code> whether it's a query match or not.
</p>
</li></ul>

<p>You can then use these arguments in your function body to calculate the replacement,
e.g. <code>tokens[match,"token"]</code> to get all relevant tokens.
The return value of the function will be used as the replacement for all matched tokens. You probably want to make sure it's a character vecor
of length one or of the same length as all matches.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )
  tokenized.obj &lt;- textTransform(
    tokenized.obj,
    scheme="random"
  )
  pasteText(tokenized.obj)

  # diff stats are now part of the object
  hasFeature(tokenized.obj)
  diffText(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='tokenize'>A simple tokenizer</h2><span id='topic+tokenize'></span><span id='topic+tokenize+2Ccharacter-method'></span><span id='topic+tokenize+2CkRp.connection-method'></span>

<h3>Description</h3>

<p>This tokenizer can be used to try replace TreeTagger. Its results are not as detailed when it comes to word classes,
and no
lemmatization is done. However, for most cases this should suffice.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize(
  txt,
  format = "file",
  fileEncoding = NULL,
  split = "[[:space:]]",
  ign.comp = "-",
  heuristics = "abbr",
  heur.fix = list(pre = c("\u2019", "'"), suf = c("\u2019", "'")),
  abbrev = NULL,
  tag = TRUE,
  lang = "kRp.env",
  sentc.end = c(".", "!", "?", ";", ":"),
  detect = c(parag = FALSE, hline = FALSE),
  clean.raw = NULL,
  perl = FALSE,
  stopwords = NULL,
  stemmer = NULL,
  doc_id = NA,
  add.desc = "kRp.env",
  ...
)

## S4 method for signature 'character'
tokenize(
  txt,
  format = "file",
  fileEncoding = NULL,
  split = "[[:space:]]",
  ign.comp = "-",
  heuristics = "abbr",
  heur.fix = list(pre = c("\u2019", "'"), suf = c("\u2019", "'")),
  abbrev = NULL,
  tag = TRUE,
  lang = "kRp.env",
  sentc.end = c(".", "!", "?", ";", ":"),
  detect = c(parag = FALSE, hline = FALSE),
  clean.raw = NULL,
  perl = FALSE,
  stopwords = NULL,
  stemmer = NULL,
  doc_id = NA,
  add.desc = "kRp.env"
)

## S4 method for signature 'kRp.connection'
tokenize(
  txt,
  format = NA,
  fileEncoding = NULL,
  split = "[[:space:]]",
  ign.comp = "-",
  heuristics = "abbr",
  heur.fix = list(pre = c("\u2019", "'"), suf = c("\u2019", "'")),
  abbrev = NULL,
  tag = TRUE,
  lang = "kRp.env",
  sentc.end = c(".", "!", "?", ";", ":"),
  detect = c(parag = FALSE, hline = FALSE),
  clean.raw = NULL,
  perl = FALSE,
  stopwords = NULL,
  stemmer = NULL,
  doc_id = NA,
  add.desc = "kRp.env"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_+3A_txt">txt</code></td>
<td>
<p>Either an open connection,
the path to directory with txt files to read and tokenize, or a vector object
already holding the text corpus.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_format">format</code></td>
<td>
<p>Either &quot;file&quot; or &quot;obj&quot;,
depending on whether you want to scan files or analyze the given object.
Ignored if <code>txt</code> is a connection.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_fileencoding">fileEncoding</code></td>
<td>
<p>A character string naming the encoding of all files.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_split">split</code></td>
<td>
<p>A regular expression to define the basic split method. Should only need refinement
for languages that don't separate words by space.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_ign.comp">ign.comp</code></td>
<td>
<p>A character vector defining punctuation which might be used in composita that should 
not be split.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_heuristics">heuristics</code></td>
<td>
<p>A vector to indicate if the tokenizer should use some heuristics. Can be none,
one or several of the following:
</p>

<ul>
<li><p><code>"abbr"</code>Assume that &quot;letter-dot-letter-dot&quot; combinations are abbreviations and leave them intact.
</p>
</li>
<li><p><code>"suf"</code>Try to detect possesive suffixes like &quot;'s&quot;,
or shorting suffixes like &quot;'ll&quot; and treat them as one token
</p>
</li>
<li><p><code>"pre"</code>Try to detect prefixes like &quot;s'&quot; or &quot;l'&quot; and treat them as one token
</p>
</li></ul>

<p>Earlier releases used the names <code>"en"</code> and <code>"fr"</code> instead of <code>"suf"</code> and <code>"pre"</code>. They are still working,
that is
<code>"en"</code> is equivalent to <code>"suf"</code>,
whereas <code>"fr"</code> is now equivalent to both <code>"suf"</code> and <code>"pre"</code> (and not only
<code>"pre"</code> as in the past, which was missing the use of suffixes in French).</p>
</td></tr>
<tr><td><code id="tokenize_+3A_heur.fix">heur.fix</code></td>
<td>
<p>A list with the named vectors <code>pre</code> and <code>suf</code>. These will be used if <code>heuristics</code> were
set to use one of the presets that try to detect pre- and/or suffixes. Change them if you document uses other
characters than the ones defined by default.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_abbrev">abbrev</code></td>
<td>
<p>Path to a text file with abbreviations to take care of,
one per line. Note that
this file must have the same encoding as defined by <code>fileEncoding</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_tag">tag</code></td>
<td>
<p>Logical. If <code>TRUE</code>,
the text will be rudimentarily tagged and returned as an object
of class <code>kRp.text</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_lang">lang</code></td>
<td>
<p>A character string naming the language of the analyzed text. If set to <code>"kRp.env"</code> this is fetched from <code><a href="#topic+get.kRp.env">get.kRp.env</a></code>. Only needed if <code>tag=TRUE</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_sentc.end">sentc.end</code></td>
<td>
<p>A character vector with tokens indicating a sentence ending. Only needed if <code>tag=TRUE</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_detect">detect</code></td>
<td>
<p>A named logical vector,
indicating by the setting of <code>parag</code> and <code>hline</code> whether <code>tokenize</code> should try
to detect paragraphs and headlines.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_clean.raw">clean.raw</code></td>
<td>
<p>A named list of character values,
indicating replacements that should globally be made to the text prior to tokenizing it.
This is applied after the text was converted into UTF-8 internally. In the list,
the name of each element represents a pattern which
is replaced by its value if met in the text. Since this is done by calling <code><a href="base.html#topic+gsub">gsub</a></code>,
regular expressions are basically
supported. See the <code>perl</code> attribute, too.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_perl">perl</code></td>
<td>
<p>Logical,
only relevant if <code>clean.raw</code> is not <code>NULL</code>. If <code>perl=TRUE</code>, this is forwarded to <code><a href="base.html#topic+gsub">gsub</a></code>
to allow for perl-like regular expressions in <code>clean.raw</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_stopwords">stopwords</code></td>
<td>
<p>A character vector to be used for stopword detection. Comparison is done in lower case. You can also simply set 
<code>stopwords=tm::stopwords("en")</code> to use the english stopwords provided by the <code>tm</code> package.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_stemmer">stemmer</code></td>
<td>
<p>A function or method to perform stemming. For instance,
you can set <code>SnowballC::wordStem</code> if you have
the <code>SnowballC</code> package installed. As of now,
you cannot provide further arguments to this function.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_doc_id">doc_id</code></td>
<td>
<p>Character string,
optional identifier of the particular document. Will be added to the <code>desc</code> slot, and as a factor to the <code>"doc_id"</code> column
of the <code>tokens</code> slot. If <code>NA</code>,
the document name will be used (for <code>format="obj"</code> a random name).</p>
</td></tr>
<tr><td><code id="tokenize_+3A_add.desc">add.desc</code></td>
<td>
<p>Logical. If <code>TRUE</code>,
the tag description (column <code>"desc"</code> of the data.frame) will be added directly
to the resulting object. If set to <code>"kRp.env"</code> this is fetched from <code><a href="#topic+get.kRp.env">get.kRp.env</a></code>. Only needed if <code>tag=TRUE</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_...">...</code></td>
<td>
<p>Only used for the method generic.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>tokenize</code> can try to guess what's a headline and where a paragraph was inserted (via the <code>detect</code> parameter).
A headline is assumed if a line of text without sentence ending punctuation is found,
a paragraph if two blocks of text
are separated by space. This will add extra tags into the text: &quot;&lt;kRp.h&gt;&quot; (headline starts),
&quot;&lt;/kRp.h&gt;&quot; (headline ends)
and &quot;&lt;kRp.p/&gt;&quot; (paragraph),
respectively. This can be useful in two cases: &quot;&lt;/kRp.h&gt;&quot; will be treated like a sentence ending,
which gives you more control for automatic analyses. And adding to that,
<code><a href="#topic+pasteText">pasteText</a></code>
can replace these tags, which probably preserves more of the original layout.
</p>


<h3>Value</h3>

<p>If <code>tag=FALSE</code>, a character vector with the tokenized text. If <code>tag=TRUE</code>,
returns an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )

  ## character manipulation
  # this is useful if you know of problematic characters in your
  # raw text files, but don't want to touch them directly. you
  # don't have to, as you can substitute them, even using regular
  # expressions. a simple example: replace all single quotes by
  # double quotes througout the text:
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en",
    clean.raw=list("'"='\"')
  )

  # now replace all occurrances of the letter A followed
  # by two digits with the letter B, followed by the same
  # two digits:
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en",
    clean.raw=list("(A)([[:digit:]]{2})"="B\\2"),
    perl=TRUE
  )

  ## enabling stopword detection and stemming
  if(all(
    requireNamespace("tm", quietly=TRUE),
    requireNamespace("SnowballC", quietly=TRUE)
  )){
    # if you also installed the packages tm and Snowball,
    # you can use some of their features with koRpus:
    tokenized.obj &lt;- tokenize(
      txt=sample_file,
      lang="en",
      stopwords=tm::stopwords("en"),
      stemmer=SnowballC::wordStem
    )

    # removing all stopwords now is simple:
    tokenized.noStopWords &lt;- filterByClass(tokenized.obj, "stopword")
  } else {}
} else {}
</code></pre>

<hr>
<h2 id='traenkle.bailer'>Readability: Traenkle-Bailer Formeln</h2><span id='topic+traenkle.bailer'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>traenkle.bailer(
  txt.file,
  TB1 = c(const = 224.6814, awl = 79.8304, asl = 12.24032, prep = 1.292857),
  TB2 = c(const = 234.1063, awl = 96.11069, prep = 2.05444, conj = 1.02805),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="traenkle.bailer_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="traenkle.bailer_+3A_tb1">TB1</code></td>
<td>
<p>A numeric vector with named magic numbers for the first of the formulas.</p>
</td></tr>
<tr><td><code id="traenkle.bailer_+3A_tb2">TB2</code></td>
<td>
<p>A numeric vector with named magic numbers for the second of the formulas.</p>
</td></tr>
<tr><td><code id="traenkle.bailer_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the two formulae by Tr\&quot;ankle-Bailer,
which are based on the Dickes-Steiwer formulae.
In contrast to <code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index values.
</p>
<p>This formula doesn't need syllable count.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  traenkle.bailer(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='treetag'>A method to call TreeTagger</h2><span id='topic+treetag'></span><span id='topic+treetag+2Ccharacter-method'></span><span id='topic+treetag+2CkRp.connection-method'></span>

<h3>Description</h3>

<p>This method calls a local installation of TreeTagger[1] to tokenize and POS tag the given text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>treetag(
  file,
  treetagger = "kRp.env",
  rm.sgml = TRUE,
  lang = "kRp.env",
  apply.sentc.end = TRUE,
  sentc.end = c(".", "!", "?", ";", ":"),
  encoding = NULL,
  TT.options = NULL,
  debug = FALSE,
  TT.tknz = TRUE,
  format = "file",
  stopwords = NULL,
  stemmer = NULL,
  doc_id = NA,
  add.desc = "kRp.env",
  ...
)

## S4 method for signature 'character'
treetag(
  file,
  treetagger = "kRp.env",
  rm.sgml = TRUE,
  lang = "kRp.env",
  apply.sentc.end = TRUE,
  sentc.end = c(".", "!", "?", ";", ":"),
  encoding = NULL,
  TT.options = NULL,
  debug = FALSE,
  TT.tknz = TRUE,
  format = "file",
  stopwords = NULL,
  stemmer = NULL,
  doc_id = NA,
  add.desc = "kRp.env"
)

## S4 method for signature 'kRp.connection'
treetag(
  file,
  treetagger = "kRp.env",
  rm.sgml = TRUE,
  lang = "kRp.env",
  apply.sentc.end = TRUE,
  sentc.end = c(".", "!", "?", ";", ":"),
  encoding = NULL,
  TT.options = NULL,
  debug = FALSE,
  TT.tknz = TRUE,
  format = NA,
  stopwords = NULL,
  stemmer = NULL,
  doc_id = NA,
  add.desc = "kRp.env"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="treetag_+3A_file">file</code></td>
<td>
<p>Either a connection or a character vector, valid path to a file,
containing the text to be analyzed.
If <code>file</code> is a connection, its contents will be written to a temporary file,
since TreeTagger can't read from
R connection objects.</p>
</td></tr>
<tr><td><code id="treetag_+3A_treetagger">treetagger</code></td>
<td>
<p>A character vector giving the TreeTagger script to be called. If set to <code>"kRp.env"</code> this is got from <code><a href="#topic+get.kRp.env">get.kRp.env</a></code>.
Only if set to <code>"manual"</code>,
it is assumend not to be a wrapper script that can work the given text file, but that you would like
to manually tweak options for tokenizing and POS tagging yourself. In that case,
you need to provide a full set of options with the <code>TT.options</code>
parameter.</p>
</td></tr>
<tr><td><code id="treetag_+3A_rm.sgml">rm.sgml</code></td>
<td>
<p>Logical, whether SGML tags should be ignored and removed from output</p>
</td></tr>
<tr><td><code id="treetag_+3A_lang">lang</code></td>
<td>
<p>A character string naming the language of the analyzed corpus. See <code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code> and
<code><a href="#topic+available.koRpus.lang">available.koRpus.lang</a></code>for all supported languages. If set to <code>"kRp.env"</code> this is
fetched from <code><a href="#topic+get.kRp.env">get.kRp.env</a></code>.</p>
</td></tr>
<tr><td><code id="treetag_+3A_apply.sentc.end">apply.sentc.end</code></td>
<td>
<p>Logical,
whethter the tokens defined in <code>sentc.end</code> should be searched and set to a sentence ending tag.</p>
</td></tr>
<tr><td><code id="treetag_+3A_sentc.end">sentc.end</code></td>
<td>
<p>A character vector with tokens indicating a sentence ending. This adds to TreeTaggers results,
it doesn't really replace them.</p>
</td></tr>
<tr><td><code id="treetag_+3A_encoding">encoding</code></td>
<td>
<p>A character string defining the character encoding of the input file,
like  <code>"Latin1"</code> or <code>"UTF-8"</code>. If <code>NULL</code>,
the encoding will either be taken from a preset (if defined in <code>TT.options</code>),
or fall back to <code>""</code>. Hence you can overwrite the preset encoding with this parameter.</p>
</td></tr>
<tr><td><code id="treetag_+3A_tt.options">TT.options</code></td>
<td>
<p>A list of options to configure how TreeTagger is called. You have two basic choices: Either you choose one of the pre-defined presets
or you give a full set of valid options:
</p>

<ul>
<li> <p><code>path</code> Mandatory: The absolute path to the TreeTagger root directory. That is where its subfolders <code>bin</code>,
<code>cmd</code> and <code>lib</code> are located.
</p>
</li>
<li> <p><code>preset</code> Optional: If you choose one of the pre-defined presets of one of the available language packages (like <code>"de"</code> for German,
see
<code><a href="#topic+available.koRpus.lang">available.koRpus.lang</a></code> for details),
you can omit all the following elements,
because they will be filled with defaults. Of course this only makes sense if you have a
working default installation. Note that since koRpus 0.07-1,
UTF-8 is the global default encoding.
</p>
</li>
<li> <p><code>tokenizer</code> Mandatory: A character string,
naming the tokenizer to be called. Interpreted relative to <code>path/cmd/</code>.
</p>
</li>
<li> <p><code>tknz.opts</code> Optional: A character string with the options to hand over to the tokenizer. You don't need to specify &quot;-a&quot;
if <code>abbrev</code> is given. If <code>TT.tknz=FALSE</code>,
you can pass configurational options to <code><a href="#topic+tokenize">tokenize</a></code>
by provinding them as a named list (instead of a character string) here.
</p>
</li>
<li> <p><code>pre.tagger</code> Optional: A character string with code to be run before the tagger. This code is used as-is,
so you need
make sure it includes the needed pipe symbols.
</p>
</li>
<li> <p><code>tagger</code> Mandatory: A character string,
naming the tagger-command to be called. Interpreted relative to <code>path/bin/</code>.
</p>
</li>
<li> <p><code>abbrev</code> Optional: A character string,
naming the abbreviation list to be used. Interpreted relative to <code>path/lib/</code>.
</p>
</li>
<li> <p><code>params</code> Mandatory: A character string,
naming the parameter file to be used. Interpreted relative to <code>path/lib/</code>.
</p>
</li>
<li> <p><code>lexicon</code> Optional: A character string,
naming the lexicon file to be used. Interpreted relative to <code>path/lib/</code>.
</p>
</li>
<li> <p><code>lookup</code> Optional: A character string,
naming the lexicon lookup command. Interpreted relative to <code>path/cmd/</code>.
</p>
</li>
<li> <p><code>filter</code> Optional: A character string,
naming the output filter to be used. Interpreted relative to <code>path/cmd/</code>.
</p>
</li>
<li> <p><code>no.unknown</code> Optional: Logical,
can be used to toggle the <code>"-no-unknown"</code> option of TreeTagger (defaults to <code>FALSE</code>).
</p>
</li>
<li> <p><code>splitter</code> Optional: A character string,
naming the splitter to be called (before the tokenizer). Interpreted relative to <code>path/cmd/</code>.
</p>
</li>
<li> <p><code>splitter.opts</code> Optional: A character string with the options to hand over to the splitter.
</p>
</li></ul>

<p>You can also set these options globally using <code><a href="#topic+set.kRp.env">set.kRp.env</a></code>,
and then force <code>treetag</code> to use them by setting <code>TT.options="kRp.env"</code> here. Note: 
If you use the <code>treetagger</code> setting from kRp.env and it's set to <code>TT.cmd="manual"</code>,
<code>treetag</code> will treat <code>TT.options=NULL</code> like <code>TT.options="kRp.env"</code> 
automatically.</p>
</td></tr>
<tr><td><code id="treetag_+3A_debug">debug</code></td>
<td>
<p>Logical. Especially in cases where the presets wouldn't work as expected,
this switch can be used to examine the values <code>treetag</code>
is assuming.</p>
</td></tr>
<tr><td><code id="treetag_+3A_tt.tknz">TT.tknz</code></td>
<td>
<p>Logical,
if <code>FALSE</code> TreeTagger's tokenzier script will be replaced by <code>koRpus</code>' function <code><a href="#topic+tokenize">tokenize</a></code>.
To accomplish this,
its results will be written to a temporal file which is automatically deleted afterwards (if <code>debug=FALSE</code>). Note that
this option only has an effect if <code>treetagger="manual"</code>.</p>
</td></tr>
<tr><td><code id="treetag_+3A_format">format</code></td>
<td>
<p>Either &quot;file&quot; or &quot;obj&quot;,
depending on whether you want to scan files or analyze the text in a given object, like
a character vector. If the latter,
it will be written to a temporary file (see <code>file</code>).</p>
</td></tr>
<tr><td><code id="treetag_+3A_stopwords">stopwords</code></td>
<td>
<p>A character vector to be used for stopword detection. Comparison is done in lower case. You can also simply set 
<code>stopwords=tm::stopwords("en")</code> to use the english stopwords provided by the <code>tm</code> package.</p>
</td></tr>
<tr><td><code id="treetag_+3A_stemmer">stemmer</code></td>
<td>
<p>A function or method to perform stemming. For instance,
you can set <code>SnowballC::wordStem</code> if you have
the <code>SnowballC</code> package installed. As of now,
you cannot provide further arguments to this function.</p>
</td></tr>
<tr><td><code id="treetag_+3A_doc_id">doc_id</code></td>
<td>
<p>Character string,
optional identifier of the particular document. Will be added to the <code>desc</code> slot, and as a factor to the <code>"doc_id"</code> column
of the <code>tokens</code> slot. If <code>NA</code>,
the document name will be used (for <code>format="obj"</code> a random name).</p>
</td></tr>
<tr><td><code id="treetag_+3A_add.desc">add.desc</code></td>
<td>
<p>Logical. If <code>TRUE</code>,
the tag description (column <code>"desc"</code> of the data.frame) will be added directly
to the resulting object. If set to <code>"kRp.env"</code> this is fetched from <code><a href="#topic+get.kRp.env">get.kRp.env</a></code>.</p>
</td></tr>
<tr><td><code id="treetag_+3A_...">...</code></td>
<td>
<p>Only used for the method generic.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the value of <code>lang</code> must match a valid language supported by <code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>.
It will also get stored in the resulting object and might be used by other functions at a later point.
E.g., <code>treetag</code> is being called by <code><a href="#topic+freq.analysis">freq.analysis</a></code>,
which
will by default query this language definition,
unless explicitly told otherwise. The rationale behind this
is to comfortably make it possible to have tokenized and POS tagged objects of various languages around
in your workspace, and not worry about that too much.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>. If <code>debug=TRUE</code>,
prints internal variable settings and attempts to return the
original output if the TreeTagger system call in a matrix.
</p>


<h3>Author(s)</h3>

<p>m.eik michalke <a href="mailto:meik.michalke@hhu.de">meik.michalke@hhu.de</a>,
support for various laguages was contributed by Earl Brown (Spanish), Alberto Mirisola (Italian) and
Alexandre Brulet (French).
</p>


<h3>References</h3>

<p>Schmid, H. (1994). Probabilistic part-of-speec tagging using decision trees. In
<em>International Conference on New Methods in Language Processing</em>, Manchester, UK,
44&ndash;49.
</p>
<p>[1] <a href="https://www.cis.lmu.de/~schmid/tools/TreeTagger/">https://www.cis.lmu.de/~schmid/tools/TreeTagger/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+freq.analysis">freq.analysis</a></code>,
<code><a href="#topic+get.kRp.env">get.kRp.env</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
## Not run: 
# first way to invoke POS tagging, using a built-in preset:
tagged.results &lt;- treetag(
  sample_file,
  treetagger="manual",
  lang="en",
  TT.options=list(
    path=file.path("~","bin","treetagger"),
    preset="en"
  )
)
# second way, use one of the batch scripts that come with TreeTagger:
tagged.results &lt;- treetag(
  sample_file,
  treetagger=file.path("~","bin","treetagger","cmd","tree-tagger-english"),
  lang="en"
)
# third option, set the above batch script in an environment object first:
set.kRp.env(
  TT.cmd=file.path("~","bin","treetagger","cmd","tree-tagger-english"),
  lang="en"
)
tagged.results &lt;- treetag(
  sample_file
)

# after tagging, use the resulting object with other functions in this package:
readability(tagged.results)
lex.div(tagged.results)

## enabling stopword detection and stemming
# if you also installed the packages tm and SnowballC,
# you can use some of their features with koRpus:
set.kRp.env(
  TT.cmd="manual",
  lang="en",
  TT.options=list(
    path=file.path("~","bin","treetagger"),
    preset="en"
  )
)
tagged.results &lt;- treetag(
  sample_file,
  stopwords=tm::stopwords("en"),
  stemmer=SnowballC::wordStem
)

# removing all stopwords now is simple:
tagged.noStopWords &lt;- filterByClass(
  tagged.results,
  "stopword"
)

## End(Not run)
</code></pre>

<hr>
<h2 id='TRI'>Readability: Kuntzsch's Text-Redundanz-Index</h2><span id='topic+TRI'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TRI(
  txt.file,
  hyphen = NULL,
  parameters = c(syll = 1, word = 0.449, pnct = 2.467, frgn = 0.937, const = 14.417),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TRI_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="TRI_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="TRI_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="TRI_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates Kuntzsch's Text-Redundanz-Index (text redundancy index). In contrast to
<code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  TRI(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='TTR'>Lexical diversity: Type-Token Ratio</h2><span id='topic+TTR'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TTR(txt, char = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TTR_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="TTR_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="TTR_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the classic type-token ratio (TTR). In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the TTR value,
and characteristics are
off by default.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
TTR(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='tuldava'>Readability: Tuldava's Text Difficulty Formula</h2><span id='topic+tuldava'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuldava(
  txt.file,
  hyphen = NULL,
  parameters = c(syll = 1, word1 = 1, word2 = 1, sent = 1),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuldava_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="tuldava_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="tuldava_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="tuldava_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates Tuldava's Text Difficulty Formula. In contrast to
<code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>Note</h3>

<p>This index originally has no parameter weights. To be able the use weights anyway,
each parameter of the formula
is available and its weight set to 1 by default.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  tuldava(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='types'>Get types and tokens of a given text</h2><span id='topic+types'></span><span id='topic+tokens'></span><span id='topic+types+2CkRp.TTR-method'></span><span id='topic+tokens+2CkRp.TTR-method'></span><span id='topic+types+2CkRp.text-method'></span><span id='topic+tokens+2CkRp.text-method'></span><span id='topic+types+2Ccharacter-method'></span><span id='topic+tokens+2Ccharacter-method'></span>

<h3>Description</h3>

<p>These methods return character vectors that return all types or tokens of a given text,
where text can either be a character
vector itself, a previosly tokenized/tagged koRpus object,
or an object of class <code>kRp.TTR</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>types(txt, ...)

tokens(txt, ...)

## S4 method for signature 'kRp.TTR'
types(txt, stats = FALSE)

## S4 method for signature 'kRp.TTR'
tokens(txt)

## S4 method for signature 'kRp.text'
types(
  txt,
  case.sens = FALSE,
  lemmatize = FALSE,
  corp.rm.class = "nonpunct",
  corp.rm.tag = c(),
  stats = FALSE
)

## S4 method for signature 'kRp.text'
tokens(
  txt,
  case.sens = FALSE,
  lemmatize = FALSE,
  corp.rm.class = "nonpunct",
  corp.rm.tag = c()
)

## S4 method for signature 'character'
types(
  txt,
  case.sens = FALSE,
  lemmatize = FALSE,
  corp.rm.class = "nonpunct",
  corp.rm.tag = c(),
  stats = FALSE,
  lang = NULL
)

## S4 method for signature 'character'
tokens(
  txt,
  case.sens = FALSE,
  lemmatize = FALSE,
  corp.rm.class = "nonpunct",
  corp.rm.tag = c(),
  lang = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="types_+3A_txt">txt</code></td>
<td>
<p>An object of either class <code><a href="#topic+kRp.text-class">kRp.text</a></code> or
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>, or a character vector.</p>
</td></tr>
<tr><td><code id="types_+3A_...">...</code></td>
<td>
<p>Only used for the method generic.</p>
</td></tr>
<tr><td><code id="types_+3A_stats">stats</code></td>
<td>
<p>Logical,
whether statistics on the length in characters and frequency of types in the text should also be returned.</p>
</td></tr>
<tr><td><code id="types_+3A_case.sens">case.sens</code></td>
<td>
<p>Logical, whether types should be counted case sensitive.
This option is available for tagged text and character input only.</p>
</td></tr>
<tr><td><code id="types_+3A_lemmatize">lemmatize</code></td>
<td>
<p>Logical,
whether analysis should be carried out on the lemmatized tokens rather than all running word forms.
This option is available for tagged text and character input only.</p>
</td></tr>
<tr><td><code id="types_+3A_corp.rm.class">corp.rm.class</code></td>
<td>
<p>A character vector with word classes which should be dropped. The default value
<code>"nonpunct"</code> has special meaning and will cause the result of
<code>kRp.POS.tags(lang, tags=c("punct","sentc"), list.classes=TRUE)</code> to be used.
This option is available for tagged text and character input only.</p>
</td></tr>
<tr><td><code id="types_+3A_corp.rm.tag">corp.rm.tag</code></td>
<td>
<p>A character vector with POS tags which should be dropped.
This option is available for tagged text and character input only.</p>
</td></tr>
<tr><td><code id="types_+3A_lang">lang</code></td>
<td>
<p>Set the language of a text,
see the <code>force.lang</code> option of <code><a href="#topic+lex.div">lex.div</a></code>.
This option is available for character input only.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector. For<code>types</code> and <code>stats=TRUE</code> a data.frame containing all types,
their length (characters)
and frequency. The <code>types</code> result is always sorted by frequency,
with more frequent types coming first.
</p>


<h3>Note</h3>

<p>If the input is of class <code>kRp.TTR</code>,
the result will only be useful if <code>lex.div</code> or
the respective wrapper function was called with <code>keep.tokens=TRUE</code>. Similarily,
<code>lemmatize</code> can only work
properly if the input is a tagged text object with lemmata or you've properly set up the enviroment via <code>set.kRp.env</code>.
Calling these methods on <code>kRp.TTR</code> objects is just returning the respective part of its <code>tt</code> slot.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>,
<code><a href="#topic+lex.div">lex.div</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># code is only run when the english language package can be loaded
if(require("koRpus.lang.en", quietly = TRUE)){
  sample_file &lt;- file.path(
    path.package("koRpus"), "examples", "corpus", "Reality_Winner.txt"
  )
  tokenized.obj &lt;- tokenize(
    txt=sample_file,
    lang="en"
  )

  types(tokenized.obj)
  tokens(tokenized.obj)
} else {}
</code></pre>

<hr>
<h2 id='U.ld'>Lexical diversity: Uber Index (U)</h2><span id='topic+U.ld'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+lex.div">lex.div</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>U.ld(txt, char = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="U.ld_+3A_txt">txt</code></td>
<td>
<p>An object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code> containing the tagged text to be analyzed.</p>
</td></tr>
<tr><td><code id="U.ld_+3A_char">char</code></td>
<td>
<p>Logical,
defining whether data for plotting characteristic curves should be calculated.</p>
</td></tr>
<tr><td><code id="U.ld_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+lex.div">lex.div</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the Uber Index (U). In contrast to
<code><a href="#topic+lex.div">lex.div</a></code>,
which by default calculates all possible measures and
their progressing characteristics, this function will only calculate the U value,
and characteristics are
off by default.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kRp.POS.tags">kRp.POS.tags</a></code>,
<code><a href="#topic+kRp.text-class">kRp.text</a></code>,
<code><a href="#topic+kRp.TTR-class">kRp.TTR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
U.ld(tagged.text)

## End(Not run)
</code></pre>

<hr>
<h2 id='wheeler.smith'>Readability: Wheeler-Smith Score</h2><span id='topic+wheeler.smith'></span>

<h3>Description</h3>

<p>This is just a convenient wrapper function for <code><a href="#topic+readability">readability</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wheeler.smith(txt.file, hyphen = NULL, parameters = c(syll = 2), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wheeler.smith_+3A_txt.file">txt.file</code></td>
<td>
<p>Either an object of class <code><a href="#topic+kRp.text-class">kRp.text</a></code>,
a character vector which must be be
a valid path to a file containing the text to be analyzed,
or a list of text features. If the latter, calculation
is done by <code><a href="#topic+readability.num">readability.num</a></code>.</p>
</td></tr>
<tr><td><code id="wheeler.smith_+3A_hyphen">hyphen</code></td>
<td>
<p>An object of class kRp.hyphen. If <code>NULL</code>,
the text will be hyphenated automatically.</p>
</td></tr>
<tr><td><code id="wheeler.smith_+3A_parameters">parameters</code></td>
<td>
<p>A numeric vector with named magic numbers,
defining the relevant parameters for the index.</p>
</td></tr>
<tr><td><code id="wheeler.smith_+3A_...">...</code></td>
<td>
<p>Further valid options for the main function,
see <code><a href="#topic+readability">readability</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the Wheeler-Smith Score. In contrast to
<code><a href="#topic+readability">readability</a></code>,
which by default calculates all possible indices,
this function will only calculate the index value.
</p>
<p>If <code>parameters="de"</code>, the calculation stays the same, but grade placement
is done according to Bamberger &amp; Vanecek (1984), that is for german texts.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="#topic+kRp.readability-class">kRp.readability</a></code>.
</p>


<h3>References</h3>

<p>Bamberger, R. &amp; Vanecek,
E. (1984). <em>Lesen&ndash;Verstehen&ndash;Lernen&ndash;Schreiben</em>. Wien: Jugend und Volk.
</p>
<p>Wheeler, L.R. &amp; Smith,
E.H. (1954). A practical readability formula for the classroom teacher in the primary grades. <em>Elementary English</em>,
31, 397&ndash;399.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  wheeler.smith(tagged.text)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
