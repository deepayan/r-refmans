<!DOCTYPE html><html><head><title>Help for package moreparty</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {moreparty}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BivariateAssoc'>
<p>Bivariate association measures for supervised learning tasks.</p></a></li>
<li><a href='#ctree-module'>
<p>Shiny module to build and analyse conditional inference trees</p></a></li>
<li><a href='#EasyTreeVarImp'>
<p>Variable importance for conditional inference trees.</p></a></li>
<li><a href='#fastcforest'>
<p>Parallelized conditional inference random forest</p></a></li>
<li><a href='#fastvarImp'>
<p>Variable importance for conditional inference random forests</p></a></li>
<li><a href='#fastvarImpAUC'><p>Variable importance (with AUC performance measure) for conditional inference random forests</p></a></li>
<li><a href='#FeatureSelection'>
<p>Feature selection for conditional random forests.</p></a></li>
<li><a href='#GetAleData'><p>Accumulated Local Effects for a conditional random forest.</p></a></li>
<li><a href='#GetCtree'>
<p>Gets a tree from a conditional random forest</p></a></li>
<li><a href='#GetInteractionStrength'><p>Strength of interactions</p></a></li>
<li><a href='#GetPartialData'><p>Partial dependence for a conditional random forest.</p></a></li>
<li><a href='#GetSplitStats'>
<p>Permutation tests results for each split in a conditional tree.</p></a></li>
<li><a href='#ggForestEffects'><p>Dot plot of covariates effects</p></a></li>
<li><a href='#ggVarImp'><p>Dot plot of variable importance</p></a></li>
<li><a href='#ictree'>
<p>An interactive app for conditional inference trees</p></a></li>
<li><a href='#NiceTreePlot'>
<p>Plots conditional inference trees.</p></a></li>
<li><a href='#NodesInfo'>
<p>Informations about terminal nodes</p></a></li>
<li><a href='#NodeTreePlot'>
<p>Plots the results of each node of a conditional inference tree</p></a></li>
<li><a href='#Outliers'><p>Computes outliers</p></a></li>
<li><a href='#PerfsBinClassif'>
<p>Performance measures for binary classification tasks</p></a></li>
<li><a href='#PerfsRegression'>
<p>Performance measures for regressions</p></a></li>
<li><a href='#Prototypes'>
<p>Prototypes of groups</p></a></li>
<li><a href='#SurrogateTree'>
<p>Surrogate tree for conditional inference random forests</p></a></li>
<li><a href='#titanic'><p>Titanic dataset</p></a></li>
<li><a href='#TreeStab'>
<p>Stability assessment of conditional inference trees</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Toolbox for Conditional Inference Trees and Random Forests</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), party</td>
</tr>
<tr>
<td>Imports:</td>
<td>partykit, varImp, plyr, foreach, measures, methods, MASS, iml,
pdp, vip (&ge; 0.4.1), ggplot2, rlang, shiny, shinyWidgets,
rclipboard, DT, datamods, phosphoricons</td>
</tr>
<tr>
<td>Suggests:</td>
<td>doParallel, knitr, rmarkdown, rmdformats, descriptio,
RColorBrewer, caret, pROC, dplyr, e1071</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Author:</td>
<td>Nicolas Robette</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nicolas Robette &lt;nicolas.robette@uvsq.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Additions to 'party' and 'partykit' packages : tools for the interpretation of forests (surrogate trees, prototypes, etc.), feature selection (see Gregorutti et al (2017) &lt;<a href="https://doi.org/10.48550/arXiv.1310.5726">doi:10.48550/arXiv.1310.5726</a>&gt;, Hapfelmeier and Ulm (2013) &lt;<a href="https://doi.org/10.1016%2Fj.csda.2012.09.020">doi:10.1016/j.csda.2012.09.020</a>&gt;, Altmann et al (2010) &lt;<a href="https://doi.org/10.1093%2Fbioinformatics%2Fbtq134">doi:10.1093/bioinformatics/btq134</a>&gt;) and parallelized versions of conditional forest and variable importance functions. Also modules and a shiny app for conditional inference trees.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-22 12:42:47 UTC; nicolas</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-22 14:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='BivariateAssoc'>
Bivariate association measures for supervised learning tasks.
</h2><span id='topic+BivariateAssoc'></span>

<h3>Description</h3>

<p>Computes bivariate association measures between a response and predictor variables (and, optionnaly, between every pairs of predictor variables.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BivariateAssoc(Y, X, xx = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BivariateAssoc_+3A_y">Y</code></td>
<td>
<p>the response variable</p>
</td></tr>
<tr><td><code id="BivariateAssoc_+3A_x">X</code></td>
<td>
<p>the predictor variables</p>
</td></tr>
<tr><td><code id="BivariateAssoc_+3A_xx">xx</code></td>
<td>
<p>whether the association measures should be computed for couples of predictor variables (default) or not. With a lot of predictors, consider setting xx to FALSE (for reasons of computation time).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each pair of variable, a permutation test is computed, following the framework used in conditional inference trees to choose a splitting variable. This test produces a p-value, transformed as -log(1-p) for reasons of comparison stability. The function also computes a &quot;standard&quot; association measure : kenddal's tau correlation for pairs of numeric variables, Cramer's V for pairs of factors and eta-squared for pairs numeric-factor.</p>


<h3>Value</h3>

<p>A list of the following items :
</p>
<table>
<tr><td><code>YX</code></td>
<td>
<p>: a table with the association measures between the response and predictor variables</p>
</td></tr>
<tr><td><code>XX</code></td>
<td>
<p>: a table with the association measures between every couples of predictor variables</p>
</td></tr>
</table>
<p>In each table :
</p>
<table>
<tr><td><code>measure</code></td>
<td>
<p>: name of the &quot;standard&quot; association measure</p>
</td></tr>
<tr><td><code>assoc</code></td>
<td>
<p>: value of the &quot;standard&quot; association measure</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>: p-value from the permutation test</p>
</td></tr>
<tr><td><code>criterion</code></td>
<td>
<p>: p-value from the permutation test transformed as -log(1-p), which serves to sort rows</p>
</td></tr>
</table>


<h3>Note</h3>

<p>see also https://stats.stackexchange.com/questions/171301/interpreting-ctree-partykit-output-in-r
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Hothorn T, Hornik K, Van De Wiel MA, Zeileis A. &quot;A lego system for conditional inference&quot;. <em>The American Statistician</em>. 60:257–263, 2006.
</p>
<p>Hothorn T, Hornik K, Zeileis A. &quot;Unbiased Recursive Partitioning: A Conditional Inference Framework&quot;. <em>Journal of Computational and Graphical Statistics</em>, 15(3):651-674, 2006.
</p>


<h3>See Also</h3>

<p><code>ctree</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  BivariateAssoc(iris2$Species,iris2[,1:4])
</code></pre>

<hr>
<h2 id='ctree-module'>
Shiny module to build and analyse conditional inference trees
</h2><span id='topic+ctree-module'></span><span id='topic+ctreeUI'></span><span id='topic+ctreeServer'></span>

<h3>Description</h3>

<p>The module builds a conditional inference trees according to several parameter inputs. Then it plots the tree and computes performance measures, variable importance, checks the stability and return the code to reproduce the analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ctreeUI(id)

ctreeServer(id, data, name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ctree-module_+3A_id">id</code></td>
<td>
<p>Module id. See <code><a href="shiny.html#topic+callModule">shiny::callModule()</a></code>.</p>
</td></tr>
<tr><td><code id="ctree-module_+3A_data">data</code></td>
<td>
<p><code><a href="shiny.html#topic+reactive">shiny::reactive()</a></code> function returning a <code>data.frame</code> to use for the analyses.</p>
</td></tr>
<tr><td><code id="ctree-module_+3A_name">name</code></td>
<td>
<p><code><a href="shiny.html#topic+reactive">shiny::reactive()</a></code> function returning a <code>character</code> string representing <code>data</code> name.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Hothorn T, Hornik K, Van De Wiel MA, Zeileis A. &quot;A lego system for conditional inference&quot;. <em>The American Statistician</em>. 60:257–263, 2006.
</p>
<p>Hothorn T, Hornik K, Zeileis A. &quot;Unbiased Recursive Partitioning: A Conditional Inference Framework&quot;. <em>Journal of Computational and Graphical Statistics</em>, 15(3):651-674, 2006.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ictree">ictree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(shiny)
library(moreparty)

data(titanic)

ui &lt;- fluidPage(
  titlePanel("Conditional inference trees"),
  ctreeUI(id = "ctree_app")
)

server &lt;- function(input, output, session) {
  rv &lt;- reactiveValues(
    data = titanic,
    name = deparse(substitute(titanic))
  )
  ctreeServer(id = "ctree_app", reactive(rv$data), reactive(rv$name))
}

if (interactive())
  shinyApp(ui, server)
</code></pre>

<hr>
<h2 id='EasyTreeVarImp'>
Variable importance for conditional inference trees.
</h2><span id='topic+EasyTreeVarImp'></span>

<h3>Description</h3>

<p>Variable importance for <code>partykit</code> conditional inference trees, using various performance measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EasyTreeVarImp(ct, nsim = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EasyTreeVarImp_+3A_ct">ct</code></td>
<td>
<p>A tree of class <code>constparty</code> (as returned by <code>ctree</code> from <code>partykit</code> package).</p>
</td></tr>
<tr><td><code id="EasyTreeVarImp_+3A_nsim">nsim</code></td>
<td>
<p>Integer specifying the number of Monte Carlo replications to perform. Default is 1. If nsim &gt; 1, the results from each replication are simply averaged together.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the response variable is a factor, AUC (if response is binary), accuracy, balanced accuracy and true predictions by class are used. If the response is numeric, r-squared and Kendall's tau are used.
</p>


<h3>Value</h3>

<p>A data frame of variable importances, with variables as rows and performance measures as columns.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Hothorn T, Hornik K, Van De Wiel MA, Zeileis A. &quot;A lego system for conditional inference&quot;. <em>The American Statistician</em>. 60:257–263, 2006.
</p>
<p>Hothorn T, Hornik K, Zeileis A. &quot;Unbiased Recursive Partitioning: A Conditional Inference Framework&quot;. <em>Journal of Computational and Graphical Statistics</em>, 15(3):651-674, 2006.</p>


<h3>See Also</h3>

<p><code>ctree</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.ct = partykit::ctree(Species ~ ., data = iris2)
  EasyTreeVarImp(iris.ct, nsim = 1)
</code></pre>

<hr>
<h2 id='fastcforest'>
Parallelized conditional inference random forest
</h2><span id='topic+fastcforest'></span>

<h3>Description</h3>

<p>Parallelized version of <code>cforest</code> function from <code>party</code> package, which is an implementation of the random forest and bagging ensemble algorithms utilizing conditional inference trees as base learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fastcforest(formula, data = list(), subset = NULL, weights = NULL,
            controls = party::cforest_unbiased(),
            xtrafo = ptrafo, ytrafo = ptrafo, scores = NULL,
            parallel = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fastcforest_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of the model to be fit. Note that symbols like <code>:</code> and <code>-</code> will not work and the tree will make use of all variables listed on the rhs of <code>formula</code>
</p>
</td></tr>
<tr><td><code id="fastcforest_+3A_data">data</code></td>
<td>

<p>a data frame containing the variables in the model
</p>
</td></tr>
<tr><td><code id="fastcforest_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in the fitting process
</p>
</td></tr>
<tr><td><code id="fastcforest_+3A_weights">weights</code></td>
<td>

<p>an optional vector of weights to be used in the fitting process. Non-negative integer valued weights are allowed as well as non-negative real weights. Observations are sampled (with or without replacement) according to probabilities <code>weights / sum(weights)</code>. The fraction of observations to be sampled (without replacement) is computed based on the sum of the weights if all weights are integer-valued and based on the number of weights greater zero else. Alternatively, <code>weights</code> can be a double matrix defining case weights for all <code>ncol(weights)</code> trees in the forest directly. This requires more storage but gives the user more control.
</p>
</td></tr>
<tr><td><code id="fastcforest_+3A_controls">controls</code></td>
<td>

<p>an object of class <code><a href="party.html#topic+ForestControl-class">ForestControl-class</a></code>, which can be obtained using <code><a href="party.html#topic+cforest_control">cforest_control</a></code> (and its convenience interfaces <code>cforest_unbiased</code> and <code>cforest_classical</code>).
</p>
</td></tr>
<tr><td><code id="fastcforest_+3A_xtrafo">xtrafo</code></td>
<td>

<p>a function to be applied to all input variables. By default, the <code><a href="party.html#topic+ptrafo">ptrafo</a></code> function is applied.
</p>
</td></tr>
<tr><td><code id="fastcforest_+3A_ytrafo">ytrafo</code></td>
<td>

<p>a function to be applied to all response variables. By default, the <code><a href="party.html#topic+ptrafo">ptrafo</a></code> function is applied.
</p>
</td></tr>
<tr><td><code id="fastcforest_+3A_scores">scores</code></td>
<td>

<p>an optional named list of scores to be attached to ordered factors
</p>
</td></tr>
<tr><td><code id="fastcforest_+3A_parallel">parallel</code></td>
<td>

<p>Logical indicating whether or not to run <code>fastcforest</code> in parallel using a backend provided by the <code>foreach</code> package. Default is <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="party.html#topic+cforest">cforest</a></code> documentation for details.
The code for parallelization is inspired by <a href="https://stackoverflow.com/questions/36272816/train-a-cforest-in-parallel">https://stackoverflow.com/questions/36272816/train-a-cforest-in-parallel</a>
</p>


<h3>Value</h3>

<p>An object of class <code><a href="party.html#topic+RandomForest-class">RandomForest-class</a></code>.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Leo Breiman (2001). Random Forests. <em>Machine Learning</em>, 45(1), 5&ndash;32.
</p>
<p>Torsten Hothorn, Berthold Lausen, Axel Benner and Martin Radespiel-Troeger
(2004). Bagging Survival Trees. <em>Statistics in Medicine</em>, <b>23</b>(1), 77&ndash;91.
</p>
<p>Torsten Hothorn, Peter Buhlmann, Sandrine Dudoit, Annette Molinaro
and Mark J. van der Laan (2006a). Survival Ensembles. <em>Biostatistics</em>,
<b>7</b>(3), 355&ndash;373.
</p>
<p>Torsten Hothorn, Kurt Hornik and Achim Zeileis (2006b). Unbiased
Recursive Partitioning: A Conditional Inference Framework.
<em>Journal of Computational and Graphical Statistics</em>, <b>15</b>(3),
651&ndash;674.  Preprint available from
<a href="https://www.zeileis.org/papers/Hothorn+Hornik+Zeileis-2006.pdf">https://www.zeileis.org/papers/Hothorn+Hornik+Zeileis-2006.pdf</a>
</p>
<p>Carolin Strobl, Anne-Laure Boulesteix, Achim Zeileis and Torsten Hothorn (2007).
Bias in Random Forest Variable Importance Measures: Illustrations, Sources and
a Solution. <em>BMC Bioinformatics</em>, <b>8</b>, 25.
<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25">https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25</a>
</p>
<p>Carolin Strobl, James Malley and Gerhard Tutz (2009).
An Introduction to Recursive Partitioning: Rationale, Application, and Characteristics of
Classification and Regression Trees, Bagging, and Random forests.
<em>Psychological Methods</em>, <b>14</b>(4), 323&ndash;348.
</p>


<h3>See Also</h3>

<p><code><a href="party.html#topic+cforest">cforest</a></code>, <code><a href="#topic+fastvarImp">fastvarImp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## classification
  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species=="versicolor")
  iris.cf = fastcforest(Species~., data=iris2, parallel=FALSE)
</code></pre>

<hr>
<h2 id='fastvarImp'>
Variable importance for conditional inference random forests
</h2><span id='topic+fastvarImp'></span>

<h3>Description</h3>

<p>Parallelized version of <code>varImp</code> function from <code>varImp</code> package, which computes the variable importance for arbitrary measures from the <code>measures</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fastvarImp(object, mincriterion = 0, conditional = FALSE,
           threshold = 0.2, nperm = 1, OOB = TRUE,
           pre1.0_0 = conditional, measure = "multiclass.Brier",
           parallel = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fastvarImp_+3A_object">object</code></td>
<td>

<p>An object as returned by <code>cforest</code> (or <code>fastcforest</code>).
</p>
</td></tr>
<tr><td><code id="fastvarImp_+3A_mincriterion">mincriterion</code></td>
<td>

<p>The value of the test statistic or 1 - p-value that must be exceeded in order to include a split in the computation of the importance. The default mincriterion = 0 guarantees that all splits are included.
</p>
</td></tr>
<tr><td><code id="fastvarImp_+3A_conditional">conditional</code></td>
<td>

<p>a logical determining whether unconditional or conditional computation of the importance is performed.
</p>
</td></tr>
<tr><td><code id="fastvarImp_+3A_threshold">threshold</code></td>
<td>

<p>The threshold value for (1 - p-value) of the association between the variable of interest and a covariate, which must be exceeded inorder to include the covariate in the conditioning scheme for the variable of interest (only relevant if conditional = TRUE). A threshold value of zero includes all covariates.
</p>
</td></tr>
<tr><td><code id="fastvarImp_+3A_nperm">nperm</code></td>
<td>

<p>The number of permutations performed.
</p>
</td></tr>
<tr><td><code id="fastvarImp_+3A_oob">OOB</code></td>
<td>

<p>A logical determining whether the importance is computed from the out-of-bag sample or the learning sample (not suggested).
</p>
</td></tr>
<tr><td><code id="fastvarImp_+3A_pre1.0_0">pre1.0_0</code></td>
<td>

<p>Prior to party version 1.0-0, the actual data values were permuted according to the original permutation importance suggested by Breiman (2001). Now the assignments to child nodes of splits in the variable of interest are permuted as described by Hapfelmeier et al. (2012), which allows for missing values in the explanatory variables and is more efficient wrt memory consumption and computing time. This method does not apply to conditional variable importances.
</p>
</td></tr>
<tr><td><code id="fastvarImp_+3A_measure">measure</code></td>
<td>

<p>The name of the measure of the <code>measures</code> package that should be used for the variable importance calculation.
</p>
</td></tr>
<tr><td><code id="fastvarImp_+3A_parallel">parallel</code></td>
<td>

<p>Logical indicating whether or not to run <code>fastvarImp</code> in parallel using a backend provided by the <code>foreach</code> package. Default is <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="fastvarImp_+3A_...">...</code></td>
<td>

<p>Further arguments (like positive or negative class) that are needed by the measure.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The code is adapted from <code><a href="varImp.html#topic+varImp">varImp</a></code> function in <code><a href="varImp.html#topic+varImp">varImp</a></code> package.
</p>


<h3>Value</h3>

<p>Vector with computed permutation importance for each variable.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>See Also</h3>

<p><code><a href="varImp.html#topic+varImp">varImp</a></code>, <code><a href="#topic+fastvarImpAUC">fastvarImpAUC</a></code>, <code><a href="party.html#topic+cforest">cforest</a></code>, <code><a href="#topic+fastcforest">fastcforest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2,
            control = party::cforest_unbiased(mtry = 2, ntree = 50))
  fastvarImp(object = iris.cf, measure='ACC', parallel=FALSE)          
</code></pre>

<hr>
<h2 id='fastvarImpAUC'>Variable importance (with AUC performance measure) for conditional inference random forests</h2><span id='topic+fastvarImpAUC'></span>

<h3>Description</h3>

<p>Computes the variable importance regarding the AUC. Bindings are not taken into account in the AUC definition as they did
not provide as good results as the version without bindings in the paper of Janitza <em>et al.</em> (2013).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fastvarImpAUC(object, mincriterion = 0, conditional = FALSE,
            threshold = 0.2, nperm = 1, OOB = TRUE,
            pre1.0_0 = conditional,
            parallel = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fastvarImpAUC_+3A_object">object</code></td>
<td>
<p>An object as returned by <code>cforest</code> (or <code>fastcforest</code>).</p>
</td></tr>
<tr><td><code id="fastvarImpAUC_+3A_mincriterion">mincriterion</code></td>
<td>
<p>The value of the test statistic or 1 - p-value that must be exceeded in order to include a
split in the computation of the importance. The default mincriterion = 0 guarantees that all splits are included.</p>
</td></tr>
<tr><td><code id="fastvarImpAUC_+3A_conditional">conditional</code></td>
<td>
<p>The value of the test statistic or 1 - p-value that must be exceeded in order to include a split
in the computation of the importance. The default mincriterion = 0 guarantees that all splits are included.</p>
</td></tr>
<tr><td><code id="fastvarImpAUC_+3A_threshold">threshold</code></td>
<td>
<p>The threshold value for (1 - p-value) of the association between the variable of interest and a
covariate, which must be exceeded inorder to include the covariate in the conditioning scheme for the variable of
interest (only relevant if conditional = TRUE). A threshold value of zero includes all covariates.</p>
</td></tr>
<tr><td><code id="fastvarImpAUC_+3A_nperm">nperm</code></td>
<td>
<p>The number of permutations performed.</p>
</td></tr>
<tr><td><code id="fastvarImpAUC_+3A_oob">OOB</code></td>
<td>
<p>A logical determining whether the importance is computed from the out-of-bag sample or the learning
sample (not suggested).</p>
</td></tr>
<tr><td><code id="fastvarImpAUC_+3A_pre1.0_0">pre1.0_0</code></td>
<td>
<p>Prior to party version 1.0-0, the actual data values were permuted according to the original
permutation importance suggested by Breiman (2001). Now the assignments to child nodes of splits in the variable
of interest are permuted as described by Hapfelmeier et al. (2012), which allows for missing values in the
explanatory variables and is more efficient wrt memory consumption and computing time. This method does not
apply to conditional variable importances.</p>
</td></tr>
<tr><td><code id="fastvarImpAUC_+3A_parallel">parallel</code></td>
<td>
<p>Logical indicating whether or not to run <code>fastvarImpAUC</code> in parallel using a backend provided by the <code>foreach</code> package. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For using the original AUC definition and multiclass AUC you can use the <code>fastvarImp</code> function and specify the particular measure.
The code is adapted from <code>varImpAUC</code> function in <code>varImp</code> package.
</p>


<h3>Value</h3>

<p>Vector with computed permutation importance for each variable.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Janitza, S., Strobl, C. &amp; Boulesteix, A.-L. An AUC-based permutation variable
importance measure for random forests. <em>BMC Bioinform.</em> 14, 119 (2013).
</p>


<h3>See Also</h3>

<p><code>varImpAUC</code>, <code>fastvarImp</code>, <code>cforest</code>, <code>fastcforest</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2,
            control = party::cforest_unbiased(mtry = 2, ntree = 50))
  fastvarImpAUC(object = iris.cf, parallel = FALSE)
</code></pre>

<hr>
<h2 id='FeatureSelection'>
Feature selection for conditional random forests.
</h2><span id='topic+FeatureSelection'></span>

<h3>Description</h3>

<p>Performs feature selection for a conditional random forest model. Four approaches are available : non-recursive feature elimination (NRFE), recursive feature elimination (RFE), permutation test approach with permuted response (Altmann et al, 2010), permutation test approach with permuted predictors (Hapfelmeier et Ulm, 2013).</p>


<h3>Usage</h3>

<pre><code class='language-R'>FeatureSelection(Y, X, method = 'NRFE', ntree = 1000, measure = NULL,
                 nperm = 30, alpha = 0.05, distrib = 'approx',
                 parallel = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FeatureSelection_+3A_y">Y</code></td>
<td>
<p>response vector. Must be of class <code>factor</code> or <code>numeric</code></p>
</td></tr>
<tr><td><code id="FeatureSelection_+3A_x">X</code></td>
<td>
<p>matrix or data frame containing the predictors</p>
</td></tr>
<tr><td><code id="FeatureSelection_+3A_method">method</code></td>
<td>
<p>method for feature selection. Should be 'NRFE' (non-recursive feature elimination, default), 'RFE' (recursive feature elimination), 'ALT' (permutation of response) or 'HAPF' (permutation of predictors)</p>
</td></tr>
<tr><td><code id="FeatureSelection_+3A_ntree">ntree</code></td>
<td>
<p>number of trees contained in a forest</p>
</td></tr>
<tr><td><code id="FeatureSelection_+3A_measure">measure</code></td>
<td>
<p>the name of the measure of the <code>measures</code> package that should be used for error and variable importance calculations.</p>
</td></tr>
<tr><td><code id="FeatureSelection_+3A_nperm">nperm</code></td>
<td>
<p>number of permutations. Only for 'ALT' and 'HAPF' methods.</p>
</td></tr>
<tr><td><code id="FeatureSelection_+3A_alpha">alpha</code></td>
<td>
<p>alpha level for permutation tests. Only for 'ALT' and 'HAPF' methods.</p>
</td></tr>
<tr><td><code id="FeatureSelection_+3A_distrib">distrib</code></td>
<td>
<p>the null distribution of the variable importance can be approximated by its asymptotic distribution (<code>"asympt"</code>) or via Monte Carlo resampling (<code>"approx"</code>, default). Only for 'ALT' and 'HAPF' methods.</p>
</td></tr>
<tr><td><code id="FeatureSelection_+3A_parallel">parallel</code></td>
<td>
<p>Logical indicating whether or not to run <code>fastvarImp</code> in parallel using a backend provided by the <code>foreach</code> package. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="FeatureSelection_+3A_...">...</code></td>
<td>
<p>Further arguments (like positive or negative class) that are needed by the measure.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To be developed soon !
</p>


<h3>Value</h3>

<p>A list with the following elements :
</p>
<table>
<tr><td><code>selection.0se</code></td>
<td>
<p>selected variables with the 0 standard error rule</p>
</td></tr>
<tr><td><code>forest.0se</code></td>
<td>
<p>forest corresponding the variables selected with the 0 standard error rule</p>
</td></tr>
<tr><td><code>oob.error.0se</code></td>
<td>
<p>OOB error of the forest with 0 standard error rule</p>
</td></tr>
<tr><td><code>selection.1se</code></td>
<td>
<p>selected variables with the 1 standard error rule</p>
</td></tr>
<tr><td><code>forest.1se</code></td>
<td>
<p>forest corresponding the variables selected with the 1 standard error rule</p>
</td></tr>
<tr><td><code>oob.error.1se</code></td>
<td>
<p>OOB error of the forest with 1 standard error rule</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The code is adapted from Hapfelmeier &amp; Ulm (2013).
</p>
<p>Only works for regression and binary classification.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>B. Gregorutti, B. Michel, and P. Saint Pierre. &quot;Correlation and variable importance in random forests&quot;. arXiv:1310.5726, 2017.
</p>
<p>A. Hapfelmeier and K. Ulm. &quot;A new variable selection approach using random forests&quot;. <em>Computational Statistics and Data Analysis</em>, 60:50–69, 2013.
</p>
<p>A. Altmann, L. Toloşi, O. Sander et T. Lengauer. &quot;Permutation importance: a corrected feature importance measure&quot;. <em>Bioinformatics</em>, 26(10):1340-1347, 2010.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  featsel &lt;- FeatureSelection(iris2$Species, iris2[,1:4], measure='ACC', ntree=200)
  featsel$selection.0se
  featsel$selection.1se
</code></pre>

<hr>
<h2 id='GetAleData'>Accumulated Local Effects for a conditional random forest.</h2><span id='topic+GetAleData'></span>

<h3>Description</h3>

<p>Computes the Accumulated Local Effects for several covariates in a conditional random forest and gathers them into a single data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GetAleData(object, xnames=NULL, order=1, grid.size=20, parallel=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GetAleData_+3A_object">object</code></td>
<td>
<p>An object as returned by <code><a href="party.html#topic+cforest">cforest</a></code> (or <code><a href="#topic+fastcforest">fastcforest</a></code>).</p>
</td></tr>
<tr><td><code id="GetAleData_+3A_xnames">xnames</code></td>
<td>
<p>A character vector of the covariates for which to compute the Accumulated Local Effects. If NULL (default), ALE are computed for all the covariates in the model. Should be of length 2 for 2nd order ALE.</p>
</td></tr>
<tr><td><code id="GetAleData_+3A_order">order</code></td>
<td>
<p>An integer indicating whether to compute 1st order ALE (1, default) or 2nd order ALE (2).</p>
</td></tr>
<tr><td><code id="GetAleData_+3A_grid.size">grid.size</code></td>
<td>
<p>The size of the grid for evaluating the predictions. Default is 20.</p>
</td></tr>
<tr><td><code id="GetAleData_+3A_parallel">parallel</code></td>
<td>
<p>Logical indicating whether or not to run the function in parallel using a backend provided by the <code><a href="foreach.html#topic+foreach">foreach</a></code> package. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The computation of Accumulated Local Effects uses <code><a href="iml.html#topic+FeatureEffect">FeatureEffect</a></code> function from <code><a href="iml.html#topic+iml">iml</a></code> package for each covariate. The results are then gathered and reshaped into a friendly data frame format.
</p>


<h3>Value</h3>

<p>A data frame with covariates, their categories and their accumulated local effects.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette</p>


<h3>References</h3>

<p>Apley, D. W., Zhu J. &quot;Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models&quot;. arXiv:1612.08468v2, 2019.
</p>
<p>Molnar, Christoph. &quot;Interpretable machine learning. A Guide for Making Black Box Models Explainable&quot;, 2019.
https://christophm.github.io/interpretable-ml-book/.</p>


<h3>See Also</h3>

<p><code><a href="iml.html#topic+FeatureEffect">FeatureEffect</a></code>,<code><a href="#topic+GetPartialData">GetPartialData</a></code>,<code><a href="#topic+GetInteractionStrength">GetInteractionStrength</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## Not run: 
  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2, 
              controls = party::cforest_unbiased(mtry=2, ntree=50))
  GetAleData(iris.cf)

## End(Not run)</code></pre>

<hr>
<h2 id='GetCtree'>
Gets a tree from a conditional random forest
</h2><span id='topic+GetCtree'></span>

<h3>Description</h3>

<p>This function gets the ith tree from a conditional random forest as produced by <code>cforest</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GetCtree(object, k = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GetCtree_+3A_object">object</code></td>
<td>
<p>An object as returned by <code>cforest</code> (or <code>fastcforest</code>).</p>
</td></tr>
<tr><td><code id="GetCtree_+3A_k">k</code></td>
<td>
<p>The index of the tree to get from the forest. Default is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tree of class <code>BinaryTree</code>, as returned by <code>ctree</code> from <code>party</code> package.
</p>


<h3>Note</h3>

<p>Code taken from https://stackoverflow.com/questions/19924402/cforest-prints-empty-tree
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2,
            control = party::cforest_unbiased(mtry = 2, ntree = 50))
  plot(GetCtree(iris.cf))
</code></pre>

<hr>
<h2 id='GetInteractionStrength'>Strength of interactions</h2><span id='topic+GetInteractionStrength'></span>

<h3>Description</h3>

<p>Computes the strength of second order interactions for covariates in a conditional random forest.</p>


<h3>Usage</h3>

<pre><code class='language-R'>GetInteractionStrength(object, xnames=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GetInteractionStrength_+3A_object">object</code></td>
<td>
<p>An object as returned by <code><a href="party.html#topic+cforest">cforest</a></code> (or <code><a href="#topic+fastcforest">fastcforest</a></code>).</p>
</td></tr>
<tr><td><code id="GetInteractionStrength_+3A_xnames">xnames</code></td>
<td>
<p>character vector. The names of the variables for which to measure the strength of second order interactions. If NULL (default), all covariates are included.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with pairs of variable names and the strength of the interaction between them.
</p>


<h3>Note</h3>

<p>This function calls <code>vint</code> function from an old version of <code><a href="vip.html#topic+vip">vip</a></code> package for each interaction. The results are then gathered and reshaped into a friendly data frame format.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette</p>


<h3>References</h3>

<p>Greenwell, B. M., Boehmke, B. C., and McCarthy, A. J.: A Simple and Effective Model-Based Variable Importance Measure. arXiv preprint arXiv:1805.04755 (2018).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GetPartialData">GetPartialData</a></code>,<code><a href="#topic+GetAleData">GetAleData</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## Not run: 
  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2, 
              controls = party::cforest_unbiased(mtry=2, ntree=50))
  GetInteractionStrength(iris.cf)
  
## End(Not run)
</code></pre>

<hr>
<h2 id='GetPartialData'>Partial dependence for a conditional random forest.</h2><span id='topic+GetPartialData'></span>

<h3>Description</h3>

<p>Computes the partial dependence for several covariates in a conditional random forest and gathers them into a single data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GetPartialData(object, xnames=NULL, ice = FALSE, center = FALSE,
               grid.resolution = NULL, quantiles = TRUE, probs = 1:9/10,
               trim.outliers = FALSE, which.class = 1L, prob = TRUE,
               pred.fun = NULL, parallel = FALSE, paropts = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GetPartialData_+3A_object">object</code></td>
<td>

<p>An object as returned by <code><a href="party.html#topic+cforest">cforest</a></code> (or <code><a href="#topic+fastcforest">fastcforest</a></code>).
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_xnames">xnames</code></td>
<td>

<p>A character vector of the covariates for which to compute the partial dependence. If NULL (default), partial dependence is computed for all the covariates in the model.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_ice">ice</code></td>
<td>

<p>Logical indicating whether or not to compute individual conditional expectation (ICE) curves. Default is FALSE. See Goldstein et al. (2014) for details.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_center">center</code></td>
<td>

<p>Logical indicating whether or not to produce centered ICE curves (c-ICE curves). Only used when ice = TRUE. Default is FALSE. See Goldstein et al. (2014) for details.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_grid.resolution">grid.resolution</code></td>
<td>

<p>Integer giving the number of equally spaced points to use for the continuous variables listed in <code>xnames</code>. If left NULL, it will default to the minimum between 51 and the number of unique data points for each of the continuous independent variables listed in <code>xnames</code>.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_quantiles">quantiles</code></td>
<td>

<p>Logical indicating whether or not to use the sample quantiles of the continuous predictors listed in <code>xnames</code>. If <code>quantiles = TRUE</code> and <code>grid.resolution = NULL</code> (default), the sample quantiles will be used to generate the grid of joint values for which the partial dependence is computed.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_probs">probs</code></td>
<td>

<p>Numeric vector of probabilities with values in [0,1]. (Values up to 2e-14 outside that range are accepted and moved to the nearby endpoint.) Default is <code>1:9/10</code> which corresponds to the deciles of the predictor variables. These specify which quantiles to use for the continuous predictors listed in <code>xnames</code> when <code>quantiles = TRUE</code>.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_trim.outliers">trim.outliers</code></td>
<td>

<p>Logical indicating whether or not to trim off outliers from the continuous predictors listed in <code>xnames</code> (using the simple boxplot method) before generating the grid of joint values for which the partial dependence is computed. Default is FALSE.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_which.class">which.class</code></td>
<td>

<p>Integer specifying which column of the matrix of predicted probabilities to use as the &quot;focus&quot; class. Default is to use the first class. Only used for classification problems.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_prob">prob</code></td>
<td>

<p>Logical indicating whether or not partial dependence for classification problems should be returned on the probability scale, rather than the centered logit. If FALSE, the partial dependence function is on a scale similar to the logit. Default is TRUE.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_pred.fun">pred.fun</code></td>
<td>

<p>Optional prediction function that requires two arguments: <code>object</code> and <code>newdata</code>. If specified, then the function must return a single prediction or a vector of predictions (i.e., not a matrix or data frame). Default is NULL.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_parallel">parallel</code></td>
<td>

<p>Logical indicating whether or not to run <code><a href="pdp.html#topic+partial">partial</a></code> in parallel using a backend provided by the <code><a href="foreach.html#topic+foreach">foreach</a></code> package. Default is FALSE.
</p>
</td></tr>
<tr><td><code id="GetPartialData_+3A_paropts">paropts</code></td>
<td>

<p>List containing additional options to be passed onto <code><a href="foreach.html#topic+foreach">foreach</a></code> when <code>parallel = TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The computation of partial dependence uses <code><a href="pdp.html#topic+partial">partial</a></code> function from <code><a href="pdp.html#topic+pdp">pdp</a></code> package for each covariate. The results are then gathered and reshaped into a friendly data frame format.</p>


<h3>Value</h3>

<p>A data frame with covariates, their categories and their partial dependence effects.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette</p>


<h3>References</h3>

<p>J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29: 1189-1232, 2001.
</p>
<p>Goldstein, A., Kapelner, A., Bleich, J., and Pitkin, E., Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation. (2014) Journal of Computational and Graphical Statistics, 24(1): 44-65, 2015.
</p>


<h3>See Also</h3>

<p><code><a href="pdp.html#topic+partial">partial</a></code>,<code><a href="#topic+GetAleData">GetAleData</a></code>,<code><a href="#topic+GetInteractionStrength">GetInteractionStrength</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2, 
              controls = party::cforest_unbiased(mtry=2, ntree=50))
  GetPartialData(iris.cf)
</code></pre>

<hr>
<h2 id='GetSplitStats'>
Permutation tests results for each split in a conditional tree.
</h2><span id='topic+GetSplitStats'></span>

<h3>Description</h3>

<p>This function displays the results of the variable selection process for each split of a conditional tree, i.e. the p-values from permutation tests of independence between every predictor and the dependent variable. This may help to assess the stability of the tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GetSplitStats(ct)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GetSplitStats_+3A_ct">ct</code></td>
<td>
<p>A tree of class <code>constparty</code> (as returned by <code>ctree</code> from <code>partykit</code> package).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ratio index represents the ratio between the association test result for the splitting variable and the association test result for another candidate variable for splitting. It is always greater than 1. The closer it is to 1, the tighter the competition for the splitting variable, and therefore the more potentially unstable the node concerned. Conversely, the higher the ratio, the more the splitting variable has dominated the competition, and the more stable the node is likely to be.
</p>


<h3>Value</h3>

<p>A list of two elements :
</p>
<table>
<tr><td><code>details</code></td>
<td>
<p>a list of data frames (one for each inner node), with one row per candidate variable, and test statistic and p-value of the permutation test of independence, criterion (equal to log(1-p)) and ratio (criterion/max(criterion) as columns. Variables are sorted by decreasing degree of association with the dependent variable.</p>
</td></tr>
<tr><td><code>summary</code></td>
<td>
<p>a data frame with one row per inner node and 5 variables : the mode id, the splitting variable, the best candidate to split among the other variables, the ratio of the criterion of the splitting variable divided by the criterion of the best variable among the others.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>see also https://stats.stackexchange.com/questions/171301/interpreting-ctree-partykit-output-in-r
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Hothorn T, Hornik K, Van De Wiel MA, Zeileis A. &quot;A lego system for conditional inference&quot;. <em>The American Statistician</em>. 60:257–263, 2006.
</p>
<p>Hothorn T, Hornik K, Zeileis A. &quot;Unbiased Recursive Partitioning: A Conditional Inference Framework&quot;. <em>Journal of Computational and Graphical Statistics</em>, 15(3):651-674, 2006.</p>


<h3>See Also</h3>

<p><code>ctree</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.ct = partykit::ctree(Species ~ ., data = iris2)
  GetSplitStats(iris.ct)
</code></pre>

<hr>
<h2 id='ggForestEffects'>Dot plot of covariates effects</h2><span id='topic+ggForestEffects'></span>

<h3>Description</h3>

<p>Plots the effects (partial dependence or accumulated local effects) of the covariates of a supervised learning model in a single a dot plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggForestEffects(dt, vline=0, xlabel="", ylabel="", main="")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggForestEffects_+3A_dt">dt</code></td>
<td>
<p>data frame. Must have three columns : one with the names of the covariates (named &quot;var&quot;), one with the names of the categories of the covariates (named &quot;cat&quot;), one with the values of the effects (named &quot;value&quot;). Typically the result of GetAleData or GetPartialData functions.</p>
</td></tr>
<tr><td><code id="ggForestEffects_+3A_vline">vline</code></td>
<td>
<p>numeric. Coordinate on the x axis where a vertical line is added.</p>
</td></tr>
<tr><td><code id="ggForestEffects_+3A_xlabel">xlabel</code></td>
<td>
<p>character. Title of the x axis.</p>
</td></tr>
<tr><td><code id="ggForestEffects_+3A_ylabel">ylabel</code></td>
<td>
<p>character. Title of the y axis.</p>
</td></tr>
<tr><td><code id="ggForestEffects_+3A_main">main</code></td>
<td>
<p>character. Title of the plot.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>There should be no duplicated categories. If it is the case, duplicated categories have to be renamed in <code>dt</code> prior to running <code>ggForestEffects</code>.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette</p>


<h3>References</h3>

<p>Apley, D. W., Zhu J. &quot;Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models&quot;. arXiv:1612.08468v2, 2019.
</p>
<p>Molnar, Christoph. &quot;Interpretable machine learning. A Guide for Making Black Box Models Explainable&quot;, 2019.
https://christophm.github.io/interpretable-ml-book/.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GetAleData">GetAleData</a></code>, <code><a href="#topic+GetPartialData">GetPartialData</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## Not run: 
  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2, controls = cforest_unbiased(mtry=2))
  ale &lt;- GetAleData(iris.cf)
  ale$cat &lt;- paste(ale$var,ale$cat,sep='_')  # to avoid duplicated categories
  ggForestEffects(ale)

## End(Not run)</code></pre>

<hr>
<h2 id='ggVarImp'>Dot plot of variable importance</h2><span id='topic+ggVarImp'></span>

<h3>Description</h3>

<p>Plots the importance of the covariates of a supervised learning model in a dot plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggVarImp(importance, sort=TRUE, xlabel="Importance", ylabel="Variable", main="")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggVarImp_+3A_importance">importance</code></td>
<td>
<p>numeric vector. The vector of the importances of the covariates. Should be a named vector.</p>
</td></tr>
<tr><td><code id="ggVarImp_+3A_sort">sort</code></td>
<td>
<p>logical. Whether the vector of importances should be sorted or not. Default is TRUE.</p>
</td></tr>
<tr><td><code id="ggVarImp_+3A_xlabel">xlabel</code></td>
<td>
<p>character. Title of the x axis.</p>
</td></tr>
<tr><td><code id="ggVarImp_+3A_ylabel">ylabel</code></td>
<td>
<p>character. Title of the y axis.</p>
</td></tr>
<tr><td><code id="ggVarImp_+3A_main">main</code></td>
<td>
<p>character. Title of the plot.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicolas Robette</p>


<h3>See Also</h3>

<p><code><a href="varImp.html#topic+varImp">varImp</a></code>,<code><a href="varImp.html#topic+varImpAUC">varImpAUC</a></code>,<code><a href="#topic+fastvarImp">fastvarImp</a></code>,<code><a href="#topic+fastvarImpAUC">fastvarImpAUC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2,
            control = party::cforest_unbiased(mtry = 2, ntree = 50))
  imp &lt;- fastvarImpAUC(object = iris.cf, parallel = FALSE)
  ggVarImp(imp)
</code></pre>

<hr>
<h2 id='ictree'>
An interactive app for conditional inference trees
</h2><span id='topic+ictree'></span>

<h3>Description</h3>

<p>This function launches a shiny app in a web browser in order to build and analyse conditional inference trees.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ictree(treedata = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ictree_+3A_treedata">treedata</code></td>
<td>
<p>The data frame to be used in the app. If NULL (default), a module is launched to import data from a file or from the global environment.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Hothorn T, Hornik K, Van De Wiel MA, Zeileis A. &quot;A lego system for conditional inference&quot;. <em>The American Statistician</em>. 60:257–263, 2006.
</p>
<p>Hothorn T, Hornik K, Zeileis A. &quot;Unbiased Recursive Partitioning: A Conditional Inference Framework&quot;. <em>Journal of Computational and Graphical Statistics</em>, 15(3):651-674, 2006.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ctree-module">ctree-module</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (interactive()) {
ictree(iris)
}
</code></pre>

<hr>
<h2 id='NiceTreePlot'>
Plots conditional inference trees.
</h2><span id='topic+NiceTreePlot'></span>

<h3>Description</h3>

<p>Plots a <code>partykit</code> conditional inference tree in a pretty and simple way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NiceTreePlot(ct, inner_plots = FALSE, cex = 0.8, justmin = 15)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NiceTreePlot_+3A_ct">ct</code></td>
<td>
<p>A tree of class <code>constparty</code> (as returned by <code>ctree</code> from <code>partykit</code> package).</p>
</td></tr>
<tr><td><code id="NiceTreePlot_+3A_inner_plots">inner_plots</code></td>
<td>
<p>Logical. If TRUE, plots are displayed at each inner node. Default is FALSE.</p>
</td></tr>
<tr><td><code id="NiceTreePlot_+3A_cex">cex</code></td>
<td>
<p>Numerical value. Multiplier applied to fontsize. Default is 0.8.</p>
</td></tr>
<tr><td><code id="NiceTreePlot_+3A_justmin">justmin</code></td>
<td>
<p>Numerical value. Minimum average edge label length to employ justification (see <code>panelfunctions</code> documentation from <code>partykit</code> package)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Hothorn T, Hornik K, Van De Wiel MA, Zeileis A. &quot;A lego system for conditional inference&quot;. <em>The American Statistician</em>. 60:257–263, 2006.
</p>
<p>Hothorn T, Hornik K, Zeileis A. &quot;Unbiased Recursive Partitioning: A Conditional Inference Framework&quot;. <em>Journal of Computational and Graphical Statistics</em>, 15(3):651-674, 2006.</p>


<h3>See Also</h3>

<p><code>ctree</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.ct = partykit::ctree(Species ~ ., data = iris2)
  NiceTreePlot(iris.ct, inner_plots = TRUE)
</code></pre>

<hr>
<h2 id='NodesInfo'>
Informations about terminal nodes
</h2><span id='topic+NodesInfo'></span>

<h3>Description</h3>

<p>Retrieves informations about terminal nodes of a conditional inference tree : node id, rule set, frequency, prediction or class probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NodesInfo(ct)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NodesInfo_+3A_ct">ct</code></td>
<td>
<p>A tree of class <code>constparty</code> (as returned by <code>ctree</code> from <code>partykit</code> package).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Hothorn T, Hornik K, Van De Wiel MA, Zeileis A. &quot;A lego system for conditional inference&quot;. <em>The American Statistician</em>. 60:257–263, 2006.
</p>
<p>Hothorn T, Hornik K, Zeileis A. &quot;Unbiased Recursive Partitioning: A Conditional Inference Framework&quot;. <em>Journal of Computational and Graphical Statistics</em>, 15(3):651-674, 2006.</p>


<h3>See Also</h3>

<p><code>ctree</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.ct = partykit::ctree(Species ~ ., data = iris2)
  NodesInfo(iris.ct)
</code></pre>

<hr>
<h2 id='NodeTreePlot'>
Plots the results of each node of a conditional inference tree
</h2><span id='topic+NodeTreePlot'></span>

<h3>Description</h3>

<p>Plots the results of each node of a <code>partykit</code> conditional inference tree with boxplots (regression) or lollipops (binary classification) .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NodeTreePlot(ct)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NodeTreePlot_+3A_ct">ct</code></td>
<td>
<p>A tree of class <code>constparty</code> (as returned by <code>ctree</code> from <code>partykit</code> package).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 object.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Hothorn T, Hornik K, Van De Wiel MA, Zeileis A. &quot;A lego system for conditional inference&quot;. <em>The American Statistician</em>. 60:257–263, 2006.
</p>
<p>Hothorn T, Hornik K, Zeileis A. &quot;Unbiased Recursive Partitioning: A Conditional Inference Framework&quot;. <em>Journal of Computational and Graphical Statistics</em>, 15(3):651-674, 2006.</p>


<h3>See Also</h3>

<p><code>ctree</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.ct = partykit::ctree(Species ~ ., data = iris2)
  NodeTreePlot(iris.ct)
</code></pre>

<hr>
<h2 id='Outliers'>Computes outliers</h2><span id='topic+Outliers'></span>

<h3>Description</h3>

<p>Computes outlierness scores and detects outliers.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  Outliers(prox, cls=NULL, data=NULL, threshold=10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Outliers_+3A_prox">prox</code></td>
<td>
<p>a proximity matrix (a square matrix with 1 on the diagonal and values between 0 and 1 in the off-diagonal positions).</p>
</td></tr>
<tr><td><code id="Outliers_+3A_cls">cls</code></td>
<td>
<p>Factor. The classes the rows in the proximity matrix belong to. If NULL (default), all data are assumed to come from the same class.</p>
</td></tr>
<tr><td><code id="Outliers_+3A_data">data</code></td>
<td>
<p>A data frame of variables to describe the outliers (optional).</p>
</td></tr>
<tr><td><code id="Outliers_+3A_threshold">threshold</code></td>
<td>
<p>Numeric. The value of outlierness above which an observation is considered an outlier. Default is 10.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The outlierness score of a case is computed as n / sum(squared proximity), normalized by
subtracting the median and divided by the MAD, within each class.
</p>


<h3>Value</h3>

<p>A list with the following elements :
</p>
<table>
<tr><td><code>scores</code></td>
<td>
<p>numeric vector containing the outlierness scores</p>
</td></tr>
<tr><td><code>outliers</code></td>
<td>
<p>numeric vector of indexes of the outliers, or a data frame with the outliers and their characteristics</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The code is adapted from <code>outlier</code> function in <code>randomForest</code> package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2,
            control = party::cforest_unbiased(mtry = 2, ntree = 50))
  prox=proximity(iris.cf)
  Outliers(prox, iris2$Species, iris2[,1:4])
</code></pre>

<hr>
<h2 id='PerfsBinClassif'>
Performance measures for binary classification tasks
</h2><span id='topic+PerfsBinClassif'></span>

<h3>Description</h3>

<p>Computes various performance measures for binary classification tasks : true positive rate, true negative rate, accuracy, balanced accuracy, area under curve (AUC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PerfsBinClassif(pred, actual)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PerfsBinClassif_+3A_pred">pred</code></td>
<td>
<p>numerical vector of predicted values</p>
</td></tr>
<tr><td><code id="PerfsBinClassif_+3A_actual">actual</code></td>
<td>
<p>numerical vector of actual values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of performance measures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(titanic)
  titanic &lt;- titanic[complete.cases(titanic),]
  model &lt;- partykit::ctree(Survived ~ Sex + Pclass, data = titanic)
  pred &lt;- predict(model, type = "prob")[,"Yes"]
  PerfsBinClassif(pred, titanic$Survived)
</code></pre>

<hr>
<h2 id='PerfsRegression'>
Performance measures for regressions
</h2><span id='topic+PerfsRegression'></span>

<h3>Description</h3>

<p>Computes various performance measures for regression tasks : sum of the squared errors (SSE), mean squared errors (MSE), root mean squared errors (RMSE), coefficient of determination (R2), Kendall's rank correlation (tau).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PerfsRegression(pred, actual)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PerfsRegression_+3A_pred">pred</code></td>
<td>
<p>numerical vector of predicted values</p>
</td></tr>
<tr><td><code id="PerfsRegression_+3A_actual">actual</code></td>
<td>
<p>numerical vector of actual values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of performance measures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(titanic)
  titanic &lt;- titanic[complete.cases(titanic),]
  model &lt;- partykit::ctree(Age ~ Sex + Pclass, data = titanic)
  pred &lt;- predict(model)
  PerfsRegression(pred, titanic$Age)
</code></pre>

<hr>
<h2 id='Prototypes'>
Prototypes of groups
</h2><span id='topic+Prototypes'></span>

<h3>Description</h3>

<p>Prototypes are &lsquo;representative&rsquo; cases of a group of data points, given the similarity matrix among the points. They are very similar to medoids.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Prototypes(label, x, prox, nProto = 5, nNbr = floor((min(table(label)) - 1)/nProto))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Prototypes_+3A_label">label</code></td>
<td>

<p>the response variable. Should be a factor.
</p>
</td></tr>
<tr><td><code id="Prototypes_+3A_x">x</code></td>
<td>

<p>matrix or data frame of predictor variables.
</p>
</td></tr>
<tr><td><code id="Prototypes_+3A_prox">prox</code></td>
<td>

<p>the proximity (or similarity) matrix, assumed to be symmetric with 1 on the diagonal and in [0, 1] off the diagonal (the order of row/column must match that of x)
</p>
</td></tr>
<tr><td><code id="Prototypes_+3A_nproto">nProto</code></td>
<td>

<p>number of prototypes to compute for each value of the response variables.
</p>
</td></tr>
<tr><td><code id="Prototypes_+3A_nnbr">nNbr</code></td>
<td>

<p>number of nearest neighbors used to find the prototypes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each case in x, the nNbr nearest neighors are found. Then, for each class, the case that has most neighbors of that class is identified. The prototype for that class is then the medoid of these neighbors (coordinate-wise medians for numerical variables and modes for categorical variables). One then remove the neighbors used and iterate the first steps to find a second prototype, etc.
</p>


<h3>Value</h3>

<p>A list of data frames with prototypes. The number of data frames is equal to the number of classes of the response variable.
</p>


<h3>Note</h3>

<p>The code is an extension of <code>classCenter</code> function in <code>randomForest</code> package.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Random Forests, by Leo Breiman and Adele Cutler
https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#prototype
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2,
            control = party::cforest_unbiased(mtry = 2, ntree = 50))
  prox=proximity(iris.cf)
  Prototypes(iris2$Species,iris2[,1:4],prox)
</code></pre>

<hr>
<h2 id='SurrogateTree'>
Surrogate tree for conditional inference random forests</h2><span id='topic+SurrogateTree'></span>

<h3>Description</h3>

<p>Builds a surrogate tree to approximate a conditional random forest model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SurrogateTree(object, mincriterion = 0.95, maxdepth = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SurrogateTree_+3A_object">object</code></td>
<td>
<p>An object as returned by <code>cforest</code> (or <code>fastcforest</code>).</p>
</td></tr>
<tr><td><code id="SurrogateTree_+3A_mincriterion">mincriterion</code></td>
<td>
<p>the value of the test statistic (for <code>testtype == "Teststatistic"</code>),
or 1 - p-value (for other values of <code>testtype</code>) that
must be exceeded in order to implement a split.</p>
</td></tr>
<tr><td><code id="SurrogateTree_+3A_maxdepth">maxdepth</code></td>
<td>
<p>maximum depth of the tree. Default is 3.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A global surrogate model is an interpretable model that is trained to approximate the predictions of a black box model (see Molnar 2019). Here a conditional inference tree is build to approximate the prediction of a conditional inference random forest. Practically, the surrogate tree takes the forest predictions as response and the same predictors as the forest.
</p>


<h3>Value</h3>

<p>A list withe following items :
</p>
<table>
<tr><td><code>tree</code></td>
<td>
<p>The surrogate tree, of class <code>party</code></p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>The R squared of a linear regression with random forests prediction as dependent variable and surrogate tree prediction as predictor</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The surrogate tree is built using <code>ctree</code> from <code>partykit</code> package.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Molnar, Christoph. &quot;Interpretable machine learning. A Guide for Making Black Box Models Explainable&quot;, 2019.
https://christophm.github.io/interpretable-ml-book/.</p>


<h3>See Also</h3>

<p><code>cforest</code>, <code>ctree</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.cf = party::cforest(Species ~ ., data = iris2,
            control = party::cforest_unbiased(mtry = 2, ntree = 50))
  surro &lt;- SurrogateTree(iris.cf)
  surro$r.squared
  plot(surro$tree)
</code></pre>

<hr>
<h2 id='titanic'>Titanic dataset</h2><span id='topic+titanic'></span>

<h3>Description</h3>

<p>A dataset describing the passengers of the Titanic and their survival
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("titanic")</code></pre>


<h3>Format</h3>

<p>A data frame with 1309 observations and the following 5 variables.
</p>

<dl>
<dt><code>Survived</code></dt><dd><p>Factor. Whether one survived or not</p>
</dd>
<dt><code>Pclass</code></dt><dd><p>Factor. Passenger class</p>
</dd>
<dt><code>Sex</code></dt><dd><p>Factor. Sex</p>
</dd>
<dt><code>Age</code></dt><dd><p>Numeric vector. Age</p>
</dd>
<dt><code>Embarked</code></dt><dd><p>Factor. Port of embarkation</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data(titanic)
str(titanic)
</code></pre>

<hr>
<h2 id='TreeStab'>
Stability assessment of conditional inference trees
</h2><span id='topic+TreeStab'></span>

<h3>Description</h3>

<p>Assesses the stability of conditional inference trees through the partition of observations in the terminal nodes and the frequency of the variables used for splits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TreeStab(ct, B = 20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TreeStab_+3A_ct">ct</code></td>
<td>
<p>A tree of class <code>constparty</code> (as returned by <code>ctree</code> from <code>partykit</code> package).</p>
</td></tr>
<tr><td><code id="TreeStab_+3A_b">B</code></td>
<td>
<p>Numerical value. The number of bootstrap replications. Default is 20.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The study of splitting variables used in the original tree and in bootstrap trees in directly inspired from the approach implemented in <code>stablelearner</code> package.
The other side of this functions also uses bootstrap trees, this time to compute the Jaccard index of concordance between partitions, to assess the stability of the partition of observations in the terminal nodes of the tree.
</p>


<h3>Value</h3>

<p>A list of two elements :
</p>
<table>
<tr><td><code>partition</code></td>
<td>
<p>average Jaccard index of concordance between the partition (terminal nodes) of ct and the partitions of bootstrap trees</p>
</td></tr>
<tr><td><code>variables</code></td>
<td>
<p>a data frame with splitting variables in rows and two statistics in columns : their frequency of use in the tree vs in the bootstrap trees, and </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Hothorn T, Hornik K, Van De Wiel MA, Zeileis A. &quot;A lego system for conditional inference&quot;. <em>The American Statistician</em>. 60:257–263, 2006.
</p>
<p>Hothorn T, Hornik K, Zeileis A. &quot;Unbiased Recursive Partitioning: A Conditional Inference Framework&quot;. <em>Journal of Computational and Graphical Statistics</em>, 15(3):651-674, 2006.
</p>
<p>Philipp M, Zeileis A, Strobl C (2016). &quot;A Toolkit for Stability Assessment of Tree-Based Learners&quot;. In A. Colubi, A. Blanco, and C. Gatu (Eds.), <em>Proceedings of COMPSTAT 2016 - 22nd International Conference on Computational Statistics</em> (pp. 315-325). The International Statistical Institute/International Association for Statistical Computing. Preprint available at https://EconPapers.RePEc.org/RePEc:inn:wpaper:2016-11
</p>


<h3>See Also</h3>

<p><code>ctree</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species == "versicolor")
  iris.ct = partykit::ctree(Species ~ ., data = iris2)
  TreeStab(iris.ct, B = 10)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
