<!DOCTYPE html><html><head><title>Help for package transforEmotion</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {transforEmotion}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calculate_moving_average'><p>Calculate the moving average for a time series</p></a></li>
<li><a href='#conda_check'><p>Check if the &quot;transforEmotion&quot; conda environment exists</p></a></li>
<li><a href='#dlo_dynamics'><p>Dynamics function of the DLO model</p></a></li>
<li><a href='#emotions'><p>Emotions Data</p></a></li>
<li><a href='#emoxicon_scores'><p>Emoxicon Scores</p></a></li>
<li><a href='#emphasize'><p>Generate and emphasize sudden jumps in emotion scores</p></a></li>
<li><a href='#generate_observables'><p>Generate observable emotion scores data from latent variables</p></a></li>
<li><a href='#generate_q'><p>Generate a matrix of Dynamic Error values for the DLO simulation</p></a></li>
<li><a href='#image_scores'><p>Calculate image scores based on OpenAI CLIP model</p></a></li>
<li><a href='#MASS_mvrnorm'><p>Multivariate Normal (Gaussian) Distribution</p></a></li>
<li><a href='#neo_ipip_extraversion'><p>NEO-PI-R IPIP Extraversion Item Descriptions</p></a></li>
<li><a href='#nlp_scores'><p>Natural Language Processing Scores</p></a></li>
<li><a href='#plot_sim_emotions'><p>Plot the latent or the observable emotion scores.</p></a></li>
<li><a href='#punctuate'><p>Punctuation Removal for Text</p></a></li>
<li><a href='#setup_miniconda'><p>Install Miniconda and activate the transforEmotion environment</p></a></li>
<li><a href='#setup_modules'><p>Install Necessary Python Modules</p></a></li>
<li><a href='#simulate_video'><p>Simulate latent and observed emotion scores for a single &quot;video&quot;</p></a></li>
<li><a href='#stop_words'><p>Stop Words from the <em>tm</em> Package</p></a></li>
<li><a href='#tinytrolls'><p>Russian Trolls Data - Small Version</p></a></li>
<li><a href='#transforEmotion-package'><p>transforEmotion&ndash;package</p></a></li>
<li><a href='#transformer_scores'><p>Sentiment Analysis Scores</p></a></li>
<li><a href='#video_scores'><p>Run FER on YouTube video</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Sentiment Analysis for Text, Image and Video using Transformer
Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-08</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Aleksandar Tomašević &lt;atomashevic@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements sentiment analysis using huggingface <a href="https://huggingface.co">https://huggingface.co</a> transformer zero-shot classification model pipelines for text and image data. The default text pipeline is Cross-Encoder's DistilRoBERTa <a href="https://huggingface.co/cross-encoder/nli-distilroberta-base">https://huggingface.co/cross-encoder/nli-distilroberta-base</a> and default image/video pipeline is Open AI's CLIP  <a href="https://huggingface.co/openai/clip-vit-base-patch32">https://huggingface.co/openai/clip-vit-base-patch32</a>. All other zero-shot classification model pipelines can be implemented using their model name from <a href="https://huggingface.co/models?pipeline_tag=zero-shot-classification">https://huggingface.co/models?pipeline_tag=zero-shot-classification</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3.0)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>reticulate, pbapply, googledrive, LSAfun, dplyr, remotes,
Matrix</td>
</tr>
<tr>
<td>Suggests:</td>
<td>markdown, knitr, rmarkdown, rstudioapi, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-09 11:04:25 UTC; aleksandar</td>
</tr>
<tr>
<td>Author:</td>
<td>Alexander Christensen
    <a href="https://orcid.org/0000-0002-9798-7037"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Hudson Golino <a href="https://orcid.org/0000-0002-1601-1447"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Aleksandar Tomašević
    <a href="https://orcid.org/0000-0003-4863-6051"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-09 12:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='calculate_moving_average'>Calculate the moving average for a time series</h2><span id='topic+calculate_moving_average'></span>

<h3>Description</h3>

<p>This function calculates the moving average for a time series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_moving_average(data, window_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calculate_moving_average_+3A_data">data</code></td>
<td>
<p>Matrix or Data frame.
The time series data</p>
</td></tr>
<tr><td><code id="calculate_moving_average_+3A_window_size">window_size</code></td>
<td>
<p>Numeric integer.
The size of the moving average window.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix or Data frame containing the moving average values.
</p>

<hr>
<h2 id='conda_check'>Check if the &quot;transforEmotion&quot; conda environment exists</h2><span id='topic+conda_check'></span>

<h3>Description</h3>

<p>This function checks if the &quot;transforEmotion&quot; conda environment exists by
running the command &quot;conda env list&quot; and searching for the environment name
in the output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conda_check()
</code></pre>


<h3>Value</h3>

<p>A logical value indicating whether the &quot;transforEmotion&quot; conda
environment exists.
</p>

<hr>
<h2 id='dlo_dynamics'>Dynamics function of the DLO model</h2><span id='topic+dlo_dynamics'></span>

<h3>Description</h3>

<p>This function calculates the dynamics of a system using the DLO (Damped Linear Oscillator) model based on Equation 1 (Ollero et al., 2023).
The DLO model is a second-order differential equation that describes the behavior of a damped harmonic oscillator.
The function takes in the current state of the system, the derivative of the state, the damping coefficient, the time step,
and the values of the eta and zeta parameters. It returns the updated derivative of the state.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlo_dynamics(x, dxdt, q, dt, eta, zeta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dlo_dynamics_+3A_x">x</code></td>
<td>
<p>Numeric.
The current state of the system (value of the latent score).</p>
</td></tr>
<tr><td><code id="dlo_dynamics_+3A_dxdt">dxdt</code></td>
<td>
<p>Numeric.
The derivative of the state (rate of change of the latent score).</p>
</td></tr>
<tr><td><code id="dlo_dynamics_+3A_q">q</code></td>
<td>
<p>Numeric. 
The damping coefficient.</p>
</td></tr>
<tr><td><code id="dlo_dynamics_+3A_dt">dt</code></td>
<td>
<p>Numeric.
The time step.</p>
</td></tr>
<tr><td><code id="dlo_dynamics_+3A_eta">eta</code></td>
<td>
<p>Numeric. 
The eta parameter of the DLO model.</p>
</td></tr>
<tr><td><code id="dlo_dynamics_+3A_zeta">zeta</code></td>
<td>
<p>Numeric. 
The zeta parameter of the DLO model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector containing the updated derivative of the state.
</p>


<h3>References</h3>

<p>Ollero, M. J. F., Estrada, E., Hunter, M. D., &amp; Cáncer, P. F. (2023).
Characterizing affect dynamics with a damped linear oscillator model: Theoretical considerations and recommendations for individual-level applications. 
<em>Psychological Methods</em>. 
<a href="https://doi.org/10.1037/met0000615">doi:10.1037/met0000615</a>
</p>

<hr>
<h2 id='emotions'>Emotions Data</h2><span id='topic+emotions'></span>

<h3>Description</h3>

<p>A matrix containing words (n = 175,592) and the emotion category most frequently associated with each word.
This dataset is a modified version of the 'DepecheMood++' lexicon developed by
Araque, Gatti, Staiano, and Guerini (2018). For proper scoring, text should not be
stemmed prior to using this lexicon. This version of the lexicon does not
rely on part of speech tagging.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(emotions)
</code></pre>


<h3>Format</h3>

<p>A data frame with 175,592 rows and 9 columns.
</p>

<dl>
<dt>word</dt><dd><p>An entry in the lexicon, in English</p>
</dd>
<dt>AFRAID, AMUSED, ANGRY, ANNOYED, DONT_CARE, HAPPY, INSPIRED, SAD</dt><dd><p>The emotional category. All emotions contain either a 0 or 1. If the
category is most likely to be associated with the word, it recieves a 1, otherwise, 0.
Words are only associated with one category.</p>
</dd>
</dl>



<h3>References</h3>

<p>Araque, O., Gatti, L., Staiano, J., and Guerini, M. (2018).
DepecheMood++: A bilingual emotion lexicon built through simple yet powerful techniques.
<em>ArXiv</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("emotions")

</code></pre>

<hr>
<h2 id='emoxicon_scores'>Emoxicon Scores</h2><span id='topic+emoxicon_scores'></span>

<h3>Description</h3>

<p>A bag-of-words approach for computing emotions in text data using
the lexicon compiled by Araque, Gatti, Staiano, and Guerini (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emoxicon_scores(text, lexicon, exclude)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emoxicon_scores_+3A_text">text</code></td>
<td>
<p>Matrix or data frame.
A data frame containing texts to be scored (one text per row)</p>
</td></tr>
<tr><td><code id="emoxicon_scores_+3A_lexicon">lexicon</code></td>
<td>
<p>The lexicon used to score the words. The default is the <code><a href="#topic+emotions">emotions</a></code> dataset,
a modification of the lexicon developed by Araque, Gatti, Staiano, and Guerini (2018).
To use the raw lexicon from Araque et. al (2018) containing the original probability weights, use the <code><a href="stats.html#topic+weights">weights</a></code> dataset.
If another custom lexicon is used, the first column of the lexicon should contain the terms
and the subsequent columns contain the scoring categories.</p>
</td></tr>
<tr><td><code id="emoxicon_scores_+3A_exclude">exclude</code></td>
<td>
<p>A vector listing terms that should be excluded from the lexicon.
Words specified in <code>exclude</code> will not
influence document scoring. Users should consider excluding 'red herring' words
that are more closely related to the topics of the documents,
rather than the documents' emotional content.
For example, the words &quot;clinton&quot; and &quot;trump&quot; are present in the lexicon and are both associated with the emotion 'AMUSED'.
Excluding these words when analyzing political opinions may produce more accurate results.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Tara Valladares &lt;tls8vx at virginia.edu&gt; and Hudson F. Golino &lt;hfg9s at virginia.edu&gt;
</p>


<h3>References</h3>

<p>Araque, O., Gatti, L., Staiano, J., and Guerini, M. (2018).
DepecheMood++: A bilingual emotion lexicon built through simple yet powerful techniques.
<em>ArXiv</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+emotions">emotions</a></code>, where we describe how we modified the original DepecheMood++ lexicon.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Obtain "emotions" data
data("emotions")

# Obtain "tinytrolls" data
data("tinytrolls")

## Not run: 
# Obtain emoxicon scores for first 10 tweets
emotions_tinytrolls &lt;- emoxicon_scores(text = tinytrolls$content, lexicon = emotions)

## End(Not run)

</code></pre>

<hr>
<h2 id='emphasize'>Generate and emphasize sudden jumps in emotion scores</h2><span id='topic+emphasize'></span>

<h3>Description</h3>

<p>This function generates and emphasizes the effect of strong emotions expressions during the period where the derivative of the latent variable is high. The observable value of the strongest emotion from the positive or negative group will spike in the next k time steps. The probability of this happening is p at each time step in which the derivative of the latent variable is greater than 0.2. The jump is proportionate to the derivative of the latent variable and the sum of the observable values of the other emotions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emphasize(data, num_observables, num_steps, k = 10, p = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emphasize_+3A_data">data</code></td>
<td>
<p>Data frame.
The data frame containing the latent and observable variables created by the <code>simulate_video</code> function.</p>
</td></tr>
<tr><td><code id="emphasize_+3A_num_observables">num_observables</code></td>
<td>
<p>Numeric integer.
The number of observable variables per latent factor.</p>
</td></tr>
<tr><td><code id="emphasize_+3A_num_steps">num_steps</code></td>
<td>
<p>Numeric integer.
The number of time steps used in the simulation.</p>
</td></tr>
<tr><td><code id="emphasize_+3A_k">k</code></td>
<td>
<p>Numeric integer.
The mumber of time steps to emphasize the effect of strong emotions on future emotions (default is 10). Alternatively: the length of a strong emotional episode.</p>
</td></tr>
<tr><td><code id="emphasize_+3A_p">p</code></td>
<td>
<p>Numeric.
The probability of the strongest emotion being emphasized in the next k time steps (default is 0.5).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing the updated observable variables.
</p>

<hr>
<h2 id='generate_observables'>Generate observable emotion scores data from latent variables</h2><span id='topic+generate_observables'></span>

<h3>Description</h3>

<p>Function to generate observable data from 2 latent variables (negative and positive affect).
The function takes in the latent variable scores, the number of time steps, the number of observable variables per latent factor,
and the measurement error variance. It returns a matrix of observable data.
The factor loadings are not the same for all observable variables. They have uniform random noise added to them (between -0.15 and 0.15).
The loadings are scaled so that the sum of the loadings for each latent factor is 2, to introduce a ceiling effect and to differentiate the dynamics of specific emotions. This is further empahsized by adding small noise to the measurement error variance for each observed variable (between -0.01 and 0.01).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_observables(X, num_steps, num_obs, error, loadings = 0.8)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_observables_+3A_x">X</code></td>
<td>
<p>Matrix or Data frame.
The (num_steps X 2) matrix of latent variable scores.</p>
</td></tr>
<tr><td><code id="generate_observables_+3A_num_steps">num_steps</code></td>
<td>
<p>Numeric integer.
Number of time steps.</p>
</td></tr>
<tr><td><code id="generate_observables_+3A_num_obs">num_obs</code></td>
<td>
<p>Numeric integer.
The number of observable variables per latent factor.</p>
</td></tr>
<tr><td><code id="generate_observables_+3A_error">error</code></td>
<td>
<p>Numeric.
Measurement error variance.</p>
</td></tr>
<tr><td><code id="generate_observables_+3A_loadings">loadings</code></td>
<td>
<p>Numeric (default = 0.8).
The default initial loading of the latent variable on the observable variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A (num_steps X num_obs) Matrix or Data frame containing the observable variables.
</p>

<hr>
<h2 id='generate_q'>Generate a matrix of Dynamic Error values for the DLO simulation</h2><span id='topic+generate_q'></span>

<h3>Description</h3>

<p>This function generates a matrix of Dynamic Error values (q) for the DLO simulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_q(num_steps, sigma_q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_q_+3A_num_steps">num_steps</code></td>
<td>
<p>Numeric integer.
The number of time steps used in the simulation.</p>
</td></tr>
<tr><td><code id="generate_q_+3A_sigma_q">sigma_q</code></td>
<td>
<p>Numeric. 
Standard deviation of the Dynamic Error/</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A (num_steps X 3) matrix of Dynamic Error values for neutral, negative and positive emotion latent score.
</p>

<hr>
<h2 id='image_scores'>Calculate image scores based on OpenAI CLIP model</h2><span id='topic+image_scores'></span>

<h3>Description</h3>

<p>This function takes an image file and a vector of classes as input and calculates the scores for each class using the OpenAI CLIP model.
Primary use of the function is to calculate FER scores - Facial Expession Detectection of emotions based on detected facial expression in images. In case there are more than one face in the image, the function will return the scores of the face selected using the face_selection parameter.
If there is no face in the image, the function will return NA for all classes.
Function uses reticulate to call the Python functions in the image.py file. If you run this package/function for the first time it will take some time for the package to setup a functioning Python virtual enviroment in the background. This includes installing Python libraries for facial recognition and emotion detection in text, images and video. Please be patient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>image_scores(image, classes, face_selection = "largest")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="image_scores_+3A_image">image</code></td>
<td>
<p>The path to the image file or URL of the image.</p>
</td></tr>
<tr><td><code id="image_scores_+3A_classes">classes</code></td>
<td>
<p>A character vector of classes to classify the image into.</p>
</td></tr>
<tr><td><code id="image_scores_+3A_face_selection">face_selection</code></td>
<td>
<p>The method to select the face in the image. Can be &quot;largest&quot; or &quot;left&quot; or &quot;right&quot;. Default is &quot;largest&quot; and will select the largest face in the image. &quot;left&quot; and &quot;right&quot; will select the face on the far left or the far right side of the image. Face_selection method is irrelevant if there is only one face in the image.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing the scores for each class.
</p>


<h3>Author(s)</h3>

<p>Aleksandar Tomašević &lt;atomashevic@gmail.com&gt;
</p>

<hr>
<h2 id='MASS_mvrnorm'>Multivariate Normal (Gaussian) Distribution</h2><span id='topic+MASS_mvrnorm'></span>

<h3>Description</h3>

<p>This function generates a random sample from the multivariate normal distribution with mean mu and covariance matrix Sigma.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MASS_mvrnorm(n = 1, mu, Sigma, tol = 1e-06, empirical = FALSE, EISPACK = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MASS_mvrnorm_+3A_n">n</code></td>
<td>
<p>Numeric integer.
The number of observations to generate.</p>
</td></tr>
<tr><td><code id="MASS_mvrnorm_+3A_mu">mu</code></td>
<td>
<p>Numeric vector.
The mean vector of the multivariate normal distribution.</p>
</td></tr>
<tr><td><code id="MASS_mvrnorm_+3A_sigma">Sigma</code></td>
<td>
<p>Numeric matrix.
The covariance matrix of the multivariate normal distribution.</p>
</td></tr>
<tr><td><code id="MASS_mvrnorm_+3A_tol">tol</code></td>
<td>
<p>Numeric.
Tolerance for checking the positive definiteness of the covariance matrix.</p>
</td></tr>
<tr><td><code id="MASS_mvrnorm_+3A_empirical">empirical</code></td>
<td>
<p>Logical.
Whether to return the empirical covariance matrix.</p>
</td></tr>
<tr><td><code id="MASS_mvrnorm_+3A_eispack">EISPACK</code></td>
<td>
<p>Logical.
Whether to use the EISPACK routine instead of the LINPACK routine.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A (n X p) matrix of random observations from the multivariate normal distribution.
Updated: 26.10.2023.
</p>

<hr>
<h2 id='neo_ipip_extraversion'>NEO-PI-R IPIP Extraversion Item Descriptions</h2><span id='topic+neo_ipip_extraversion'></span>

<h3>Description</h3>

<p>A list (length = 6) of the NEO-PI-R IPIP item descriptions
(https://ipip.ori.org/newNEOFacetsKey.htm). Each vector within
the 6 list elements contains the item descriptions for the
respective Extraversion facets &ndash; friendliness, gregariousness,
assertiveness, activity_level, excitement_seeking, and
cheerfulness
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(neo_ipip_extraversion)
</code></pre>


<h3>Format</h3>

<p>A list (length = 6)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("neo_ipip_extraversion")

</code></pre>

<hr>
<h2 id='nlp_scores'>Natural Language Processing Scores</h2><span id='topic+nlp_scores'></span>

<h3>Description</h3>

<p>Natural Language Processing using word embeddings to compute
semantic similarities (cosine; see
<code><a href="LSAfun.html#topic+costring">costring</a></code>) of text and specified classes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlp_scores(
  text,
  classes,
  semantic_space = c("baroni", "cbow", "cbow_ukwac", "en100", "glove", "tasa"),
  preprocess = TRUE,
  remove_stop = TRUE,
  keep_in_env = TRUE,
  envir = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlp_scores_+3A_text">text</code></td>
<td>
<p>Character vector or list.
Text in a vector or list data format</p>
</td></tr>
<tr><td><code id="nlp_scores_+3A_classes">classes</code></td>
<td>
<p>Character vector.
Classes to score the text</p>
</td></tr>
<tr><td><code id="nlp_scores_+3A_semantic_space">semantic_space</code></td>
<td>
<p>Character vector.
The semantic space used to compute the distances between words
(more than one allowed). Here's a list of the semantic spaces:
</p>

<dl>
<dt><code>"baroni"</code></dt><dd><p>Combination of British National Corpus, ukWaC corpus, and a 2009
Wikipedia dump. Space created using continuous bag of words algorithm
using a context window size of 11 words (5 left and right)
and 400 dimensions. Best word2vec model according to
Baroni, Dinu, &amp; Kruszewski (2014)</p>
</dd>
<dt><code>"cbow"</code></dt><dd><p>Combination of British National Corpus, ukWaC corpus, and a 2009
Wikipedia dump. Space created using continuous bag of words algorithm with
a context window size of 5 (2 left and right) and 300 dimensions</p>
</dd>
<dt><code>"cbow_ukwac"</code></dt><dd><p>ukWaC corpus with the continuous bag of words algorithm with
a context window size of 5 (2 left and right) and 400 dimensions</p>
</dd>
<dt><code>"en100"</code></dt><dd><p>Combination of British National Corpus, ukWaC corpus, and a 2009
Wikipedia dump. 100,000 most frequent words. Uses moving window model
with a size of 5 (2 to the left and right). Positive pointwise mutual
information and singular value decomposition was used to reduce the
space to 300 dimensions</p>
</dd>
<dt><code>"glove"</code></dt><dd><p><a href="https://dumps.wikimedia.org/">Wikipedia 2014 dump</a> and <a href="https://catalog.ldc.upenn.edu/LDC2011T07">Gigaword 5</a> with 400,000
words (300 dimensions). Uses co-occurrence of words in text
documents (uses cosine similarity)</p>
</dd>
<dt><code>"tasa"</code></dt><dd><p>Latent Semantic Analysis space from TASA corpus all (300 dimensions).Uses co-occurrence of words in text documents (uses cosine similarity)</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="nlp_scores_+3A_preprocess">preprocess</code></td>
<td>
<p>Boolean.
Should basic preprocessing be applied?
Includes making lowercase, keeping only alphanumeric characters,
removing escape characters, removing repeated characters,
and removing white space.
Defaults to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nlp_scores_+3A_remove_stop">remove_stop</code></td>
<td>
<p>Boolean.
Should <code><a href="#topic+stop_words">stop_words</a></code>
be removed?
Defaults to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nlp_scores_+3A_keep_in_env">keep_in_env</code></td>
<td>
<p>Boolean.
Whether the classifier should be kept in your global environment.
Defaults to <code>TRUE</code>.
By keeping the classifier in your environment, you can skip
re-loading the classifier every time you run this function.
<code>TRUE</code> is recommended</p>
</td></tr>
<tr><td><code id="nlp_scores_+3A_envir">envir</code></td>
<td>
<p>Numeric.
Environment for the classifier to be saved for repeated use.
Defaults to the global environment</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns semantic distances for the text classes
</p>


<h3>Author(s)</h3>

<p>Alexander P. Christensen &lt;alexpaulchristensen@gmail.com&gt;
</p>


<h3>References</h3>

<p>Baroni, M., Dinu, G., &amp; Kruszewski, G. (2014).
Don't count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.
In <em>Proceedings of the 52nd annual meting of the association for computational linguistics</em> (pp. 238-247).
</p>
<p>Landauer, T.K., &amp; Dumais, S.T. (1997).
A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge.
<em>Psychological Review</em>, <em>104</em>, 211-240.
</p>
<p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014).
GloVe: Global vectors for word representation.
In <em>Proceedings of the 2014 conference on empirical methods in natural language processing</em> (pp. 1532-1543).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data
data(neo_ipip_extraversion)

# Example text 
text &lt;- neo_ipip_extraversion$friendliness[1:5]

## Not run: 
# GloVe
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 )
)

# Baroni
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 semantic_space = "baroni"
)
 
# CBOW
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 semantic_space = "cbow"
)

# CBOW + ukWaC
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 semantic_space = "cbow_ukwac"
)

# en100
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 semantic_space = "en100"
)

# tasa
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 semantic_space = "tasa"
)

## End(Not run)

</code></pre>

<hr>
<h2 id='plot_sim_emotions'>Plot the latent or the observable emotion scores.</h2><span id='topic+plot_sim_emotions'></span>

<h3>Description</h3>

<p>Function to plot the latent or the observable emotion scores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_sim_emotions(df, mode = "latent", title = " ")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_sim_emotions_+3A_df">df</code></td>
<td>
<p>Data frame.
The data frame containing the latent and observable variables created by the <code>simulate_video</code> function.</p>
</td></tr>
<tr><td><code id="plot_sim_emotions_+3A_mode">mode</code></td>
<td>
<p>Character.
The mode of the plot. Can be either 'latent', 'positive' or 'negative'.</p>
</td></tr>
<tr><td><code id="plot_sim_emotions_+3A_title">title</code></td>
<td>
<p>Character.
The title of the plot. Default is an empty title, ' '.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot of the latent or the observable emotion scores.
</p>

<hr>
<h2 id='punctuate'>Punctuation Removal for Text</h2><span id='topic+punctuate'></span>

<h3>Description</h3>

<p>Keeps the punctuations you want and removes the punctuations you don't
</p>


<h3>Usage</h3>

<pre><code class='language-R'>punctuate(
  text,
  allowPunctuations = c("-", "?", "'", "\"", ";", ",", ".", "!")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="punctuate_+3A_text">text</code></td>
<td>
<p>Character vector or list.
Text in a vector or list data format</p>
</td></tr>
<tr><td><code id="punctuate_+3A_allowpunctuations">allowPunctuations</code></td>
<td>
<p>Character vector.
Punctuations that should be allowed in the text.
Defaults to common punctuations in English text</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Coarsely removes punctuations from text. Keeps general punctuations
that are used in most English language text. Apostrophes are much trickier.
For example, not allowing &quot;'&quot; will remove apostrophes from contractions
like &quot;can't&quot; becoming &quot;cant&quot;
</p>


<h3>Value</h3>

<p>Returns text with only the allowed punctuations
</p>


<h3>Author(s)</h3>

<p>Alexander P. Christensen &lt;alexpaulchristensen@gmail.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data
data(neo_ipip_extraversion)

# Example text 
text &lt;- neo_ipip_extraversion$friendliness

# Keep only periods
punctuate(text, allowPunctuations = c("."))

</code></pre>

<hr>
<h2 id='setup_miniconda'>Install Miniconda and activate the transforEmotion environment</h2><span id='topic+setup_miniconda'></span>

<h3>Description</h3>

<p>Installs miniconda and activates the transforEmotion environment
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setup_miniconda()
</code></pre>


<h3>Details</h3>

<p>Installs miniconda using <code><a href="reticulate.html#topic+install_miniconda">install_miniconda</a></code> and activates the transforEmotion environment using <code><a href="reticulate.html#topic+use_condaenv">use_condaenv</a></code>. If the transforEmotion environment does not exist, it will be created using <code><a href="reticulate.html#topic+conda_create">conda_create</a></code>.
</p>


<h3>Author(s)</h3>

<p>Alexander P. Christensen &lt;alexpaulchristensen@gmail.com&gt;
Aleksandar Tomašević &lt;atomashevic@gmail.com&gt;
</p>

<hr>
<h2 id='setup_modules'>Install Necessary Python Modules</h2><span id='topic+setup_modules'></span>

<h3>Description</h3>

<p>Installs modules to compute <code><a href="#topic+transformer_scores">transformer_scores</a></code>. These include
</p>

<ul>
<li><p>pytorch
</p>
</li>
<li><p>torchvison
</p>
</li>
<li><p>torchaudio
</p>
</li>
<li><p>tensorflow
</p>
</li>
<li><p>transformers
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>setup_modules()
</code></pre>


<h3>Details</h3>

<p>Installs modules for miniconda using <code><a href="reticulate.html#topic+conda_install">conda_install</a></code>
</p>


<h3>Author(s)</h3>

<p>Alexander P. Christensen &lt;alexpaulchristensen@gmail.com&gt;
</p>

<hr>
<h2 id='simulate_video'>Simulate latent and observed emotion scores for a single &quot;video&quot;</h2><span id='topic+simulate_video'></span>

<h3>Description</h3>

<p>This function simulates emotions in a video using the DLO model implemented as continuous time state space model. The function takes in several parameters, including the time step, number of steps, number of observables, and various model parameters. It returns a data frame containing the simulated emotions and their derivatives, as well as smoothed versions of the observables.
The initial state of the video is always the same. Neutral score is 0.5 and both positive and negative emotion score is 0.25. 
To simulate more realistic time series, there is an option of including a sudden jump in the emotion scores. This is done by emphasizing the effect of the dominant emotion during the period where the derivative of the latent variable is high. The observable value of the strongest emotion from the positive or negative group will spike in the next k time step (emph.dur). The probability of this happening is p at each time step in which the derivative of the latent variable is greater than 0.2. The jump is proportionate to the derivative of the latent variable and the sum of the observable values of the other emotions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_video(
  dt,
  num_steps,
  num_observables,
  eta_n,
  zeta_n,
  eta,
  zeta,
  sigma_q,
  sd_observable,
  loadings,
  window_size,
  emph = FALSE,
  emph.dur = 10,
  emph.prob = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_video_+3A_dt">dt</code></td>
<td>
<p>Numeric real.
The time step for the simulation (in minutes).</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_num_steps">num_steps</code></td>
<td>
<p>Numeric real.
Total length of the video (in minutes).</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_num_observables">num_observables</code></td>
<td>
<p>Numeric integer. 
The number of observables to generate per factor. Total number of observables generated is 2 x num_observables.</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_eta_n">eta_n</code></td>
<td>
<p>Numeric.
The eta parameter for the neutral state.</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_zeta_n">zeta_n</code></td>
<td>
<p>Numeric.
The zeta parameter for the neutral state.</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_eta">eta</code></td>
<td>
<p>Numeric.
The eta parameter for the positive and negative emotions.</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_zeta">zeta</code></td>
<td>
<p>Numeric.
The zeta parameter for the positive and negative emotions.</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_sigma_q">sigma_q</code></td>
<td>
<p>Numeric.
The standard deviation of Dynamic Error of the q(t) function.</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_sd_observable">sd_observable</code></td>
<td>
<p>Numeric.
The standard deviation of the measurement error.</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_loadings">loadings</code></td>
<td>
<p>Numeric (default = 0.8).
The default initial loading of the latent variable on the observable variable.</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_window_size">window_size</code></td>
<td>
<p>Numeric integer.
The window size for smoothing the observables.</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_emph">emph</code></td>
<td>
<p>Logical.
Whether to emphasize the effect of dominant emotion (default is FALSE).</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_emph.dur">emph.dur</code></td>
<td>
<p>Numeric integer.
The duration of the emphasis (default is 10).</p>
</td></tr>
<tr><td><code id="simulate_video_+3A_emph.prob">emph.prob</code></td>
<td>
<p>Numeric.
The probability of the dominant emotion being emphasized (default is 0.5).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame (num_steps X (6 + num_observables)) containing the latent scores for neutral score, positive emotions, negative emotions and their derivatives, as well as smoothed versions of the observables.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>simulate_video(dt = 0.01, num_steps = 50, num_observables = 4, 
               eta_n = 0.5, zeta_n = 0.5,
               eta = 0.5, zeta = 0.5,
               sigma_q = 0.1, sd_observable = 0.1,
               loadings = 0.8, window_size = 10)
</code></pre>

<hr>
<h2 id='stop_words'>Stop Words from the <em>tm</em> Package</h2><span id='topic+stop_words'></span>

<h3>Description</h3>

<p>174 English stop words in the <em>tm</em> package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(stop_words)
</code></pre>


<h3>Format</h3>

<p>A vector (length = 174)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("stop_words")

</code></pre>

<hr>
<h2 id='tinytrolls'>Russian Trolls Data - Small Version</h2><span id='topic+tinytrolls'></span>

<h3>Description</h3>

<p>A matrix containing a smaller subset of tweets from the <code>trolls</code> dataset, useful for test purposes.
There are approximately 20,000 tweets from 50 authors.
This dataset includes only authored tweets by each account; retweets, reposts, and repeated tweets have been removed.
The original data was provided by FiveThirtyEight and Clemson University researchers Darren Linvill and Patrick Warren.
For more information, visit https://github.com/fivethirtyeight/russian-troll-tweets
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(tinytrolls)
</code></pre>


<h3>Format</h3>

<p>A data frame with 22,143 rows and 6 columns.
</p>

<dl>
<dt>content</dt><dd><p>A tweet.</p>
</dd>
<dt>author</dt><dd><p>The name of the handle that authored the tweet.</p>
</dd>
<dt>publish_date</dt><dd><p>The date the tweet was published on.</p>
</dd>
<dt>followers</dt><dd><p>How many followers the handle had at the time of posting.</p>
</dd>
<dt>updates</dt><dd><p>How many interactions (including likes, tweets, retweets) the post garnered.</p>
</dd>
<dt>account_type</dt><dd><p>Left or Right</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data(tinytrolls)


</code></pre>

<hr>
<h2 id='transforEmotion-package'>transforEmotion&ndash;package</h2><span id='topic+transforEmotion'></span><span id='topic+transforEmotion-package'></span>

<h3>Description</h3>

<p>Implements sentiment and emotion analysis using <a href="https://huggingface.co">huggingface</a> transformer
zero-shot classification model pipelines on text and image data. The default text pipeline is
<a href="https://huggingface.co/cross-encoder/nli-distilroberta-base">Cross-Encoder's DistilRoBERTa</a> and default image/video pipeline
is <a href="https://huggingface.co/openai/clip-vit-base-patch32">Open AI's CLIP</a>. All other zero-shot classification model pipelines can be implemented using
their model name from <a href="https://huggingface.co/models?pipeline_tag=zero-shot-classification">https://huggingface.co/models?pipeline_tag=zero-shot-classification</a>.
</p>


<h3>Author(s)</h3>

<p>Alexander P. Christensen &lt;alexpaulchristensen@gmail.com&gt;, Hudson Golino &lt;hfg9s@virginia.edu&gt; and Aleksandar Tomasevic &lt;atomashevic@ff.uns.ac.rs&gt;
</p>


<h3>References</h3>

<p>Yin, W., Hay, J., &amp; Roth, D. (2019).
Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach.
arXiv preprint arXiv:1909.00161.
</p>

<hr>
<h2 id='transformer_scores'>Sentiment Analysis Scores</h2><span id='topic+transformer_scores'></span>

<h3>Description</h3>

<p>Uses sentiment analysis pipelines from <a href="https://huggingface.co">huggingface</a>
to compute probabilities that the text corresponds to the specified classes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transformer_scores(
  text,
  classes,
  multiple_classes = FALSE,
  transformer = c("cross-encoder-roberta", "cross-encoder-distilroberta",
    "facebook-bart"),
  preprocess = FALSE,
  keep_in_env = TRUE,
  envir = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transformer_scores_+3A_text">text</code></td>
<td>
<p>Character vector or list.
Text in a vector or list data format</p>
</td></tr>
<tr><td><code id="transformer_scores_+3A_classes">classes</code></td>
<td>
<p>Character vector.
Classes to score the text</p>
</td></tr>
<tr><td><code id="transformer_scores_+3A_multiple_classes">multiple_classes</code></td>
<td>
<p>Boolean.
Whether the text can belong to multiple true classes.
Defaults to <code>FALSE</code>.
Set to <code>TRUE</code> to get scores with multiple classes</p>
</td></tr>
<tr><td><code id="transformer_scores_+3A_transformer">transformer</code></td>
<td>
<p>Character.
Specific zero-shot sentiment analysis transformer
to be used. Default options:
</p>

<dl>
<dt><code>"cross-encoder-roberta"</code></dt><dd><p>Uses <a href="https://huggingface.co/cross-encoder/nli-roberta-base">Cross-Encoder's Natural Language Interface RoBERTa Base</a>
zero-shot classification model trained on the
<a href="https://nlp.stanford.edu/projects/snli/">Stanford Natural Language Inference</a>
(SNLI) corpus and 
<a href="https://huggingface.co/datasets/multi_nli">MultiNLI</a> datasets</p>
</dd>
<dt><code>"cross-encoder-distilroberta"</code></dt><dd><p>Uses <a href="https://huggingface.co/cross-encoder/nli-distilroberta-base">Cross-Encoder's Natural Language Interface DistilRoBERTa Base</a>
zero-shot classification model trained on the
<a href="https://nlp.stanford.edu/projects/snli/">Stanford Natural Language Inference</a>
(SNLI) corpus and 
<a href="https://huggingface.co/datasets/multi_nli">MultiNLI</a> datasets. The DistilRoBERTa
is intended to be a smaller, more lightweight version of <code>"cross-encoder-roberta"</code>,
that sacrifices some accuracy for much faster speed (see 
<a href="https://www.sbert.net/docs/pretrained_cross-encoders.html#nli">https://www.sbert.net/docs/pretrained_cross-encoders.html#nli</a>)</p>
</dd>
<dt><code>"facebook-bart"</code></dt><dd><p>Uses <a href="https://huggingface.co/facebook/bart-large-mnli">Facebook's BART Large</a>
zero-shot classification model trained on the
<a href="https://huggingface.co/datasets/multi_nli">Multi-Genre Natural Language
Inference</a> (MultiNLI) dataset</p>
</dd>
</dl>

<p>Defaults to <code>"cross-encoder-distilroberta"</code>
</p>
<p>Also allows any zero-shot classification models with a pipeline
from <a href="https://huggingface.co/models?pipeline_tag=zero-shot-classification">huggingface</a>
to be used by using the specified name (e.g., <code>"typeform/distilbert-base-uncased-mnli"</code>; see Examples)</p>
</td></tr>
<tr><td><code id="transformer_scores_+3A_preprocess">preprocess</code></td>
<td>
<p>Boolean.
Should basic preprocessing be applied?
Includes making lowercase, keeping only alphanumeric characters,
removing escape characters, removing repeated characters,
and removing white space.
Defaults to <code>FALSE</code>.
Transformers generally are OK without preprocessing and handle
many of these functions internally, so setting to <code>TRUE</code>
will not change performance much</p>
</td></tr>
<tr><td><code id="transformer_scores_+3A_keep_in_env">keep_in_env</code></td>
<td>
<p>Boolean.
Whether the classifier should be kept in your global environment.
Defaults to <code>TRUE</code>.
By keeping the classifier in your environment, you can skip
re-loading the classifier every time you run this function.
<code>TRUE</code> is recommended</p>
</td></tr>
<tr><td><code id="transformer_scores_+3A_envir">envir</code></td>
<td>
<p>Numeric.
Environment for the classifier to be saved for repeated use.
Defaults to the global environment</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns probabilities for the text classes
</p>


<h3>Author(s)</h3>

<p>Alexander P. Christensen &lt;alexpaulchristensen@gmail.com&gt;
</p>


<h3>References</h3>

<p># BART <br />
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ... &amp; Zettlemoyer, L. (2019).
Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
<em>arXiv preprint arXiv:1910.13461</em>.
</p>
<p># RoBERTa <br />
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... &amp; Stoyanov, V. (2019).
Roberta: A robustly optimized bert pretraining approach.
<em>arXiv preprint arXiv:1907.11692</em>.
</p>
<p># Zero-shot classification <br />
Yin, W., Hay, J., &amp; Roth, D. (2019).
Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach.
<em>arXiv preprint arXiv:1909.00161</em>.
</p>
<p># MultiNLI dataset <br />
Williams, A., Nangia, N., &amp; Bowman, S. R. (2017).
A broad-coverage challenge corpus for sentence understanding through inference.
<em>arXiv preprint arXiv:1704.05426</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data
data(neo_ipip_extraversion)

# Example text 
text &lt;- neo_ipip_extraversion$friendliness[1:5]

## Not run: 
# Cross-Encoder DistilRoBERTa
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 )
)

# Facebook BART Large
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 transformer = "facebook-bart"
)

# Directly from huggingface: typeform/distilbert-base-uncased-mnli
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 transformer = "typeform/distilbert-base-uncased-mnli"
)

## End(Not run)

</code></pre>

<hr>
<h2 id='video_scores'>Run FER on YouTube video</h2><span id='topic+video_scores'></span>

<h3>Description</h3>

<p>This function retrieves FER scores a specific number of frames extracted from YouTube video. It uses Python libraries for facial recognition and emotion detection in text, images, and videos.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>video_scores(
  video,
  classes,
  nframes = 100,
  face_selection = "largest",
  start = 0,
  end = -1,
  uniform = FALSE,
  ffreq = 15,
  save_video = FALSE,
  save_frames = FALSE,
  save_dir = "temp/",
  video_name = "temp"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="video_scores_+3A_video">video</code></td>
<td>
<p>The URL of the YouTube video to analyze.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_classes">classes</code></td>
<td>
<p>A character vector specifying the classes to analyze.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_nframes">nframes</code></td>
<td>
<p>The number of frames to analyze in the video. Default is 100.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_face_selection">face_selection</code></td>
<td>
<p>The method for selecting faces in the video. Options are &quot;largest&quot;, &quot;left&quot;, or &quot;right&quot;. Default is &quot;largest&quot;.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_start">start</code></td>
<td>
<p>The start time of the video range to analyze. Default is 0.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_end">end</code></td>
<td>
<p>The end time of the video range to analyze. Default is -1 and this means that video won't be cut. If end is a positive number greater than start, the video will be cut from start to end.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_uniform">uniform</code></td>
<td>
<p>Logical indicating whether to uniformly sample frames from the video. Default is FALSE.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_ffreq">ffreq</code></td>
<td>
<p>The frame frequency for sampling frames from the video. Default is 15.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_save_video">save_video</code></td>
<td>
<p>Logical indicating whether to save the analyzed video. Default is FALSE.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_save_frames">save_frames</code></td>
<td>
<p>Logical indicating whether to save the analyzed frames. Default is FALSE.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_save_dir">save_dir</code></td>
<td>
<p>The directory to save the analyzed frames. Default is &quot;temp/&quot;.</p>
</td></tr>
<tr><td><code id="video_scores_+3A_video_name">video_name</code></td>
<td>
<p>The name of the analyzed video. Default is &quot;temp&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result object containing the analyzed video scores.
</p>


<h3>Author(s)</h3>

<p>Aleksandar Tomašević &lt;atomashevic@gmail.com&gt;
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
